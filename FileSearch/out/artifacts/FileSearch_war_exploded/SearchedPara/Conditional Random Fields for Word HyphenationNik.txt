 The task that we investigate is learning to split words into parts that are conventionally agreed to be individual written units. In many languages, it is acceptable to separate these units with hyphens, but it is not acceptable to split words arbitrarily. Another way of stating the task is that we want to learn to predict for each letter in a word whether or not it is permissible for the letter to be followed by a hyphen. This means that we tag each letter with either 1, for hyphen allowed following this letter, or 0, for hyphen not allowed after this letter.
The hyphenation task is also called ortho-graphic syllabification (Bartlett et al., 2008). It is an important issue in real-world text processing, as described further in Section 2 below. It is also useful as a preprocessing step to improve letter-to-phoneme conversion, and more generally for text-to-speech conversion. In the well-known NETtalk system, for example, syllable boundaries are an input to the neural network in addition to letter identities (Sejnowski and Rosenberg, 1988). Of course, orthographic syllabification is not a fun-damental scientific problem in linguistics. Nev-ertheless, it is a difficult engineering task that is worth studying for both practical and intellectual reasons.

The goal in performing hyphenation is to pre-dict a sequence of 0/1 values as a function of a se-quence of input characters. This sequential predic-tion task is significantly different from a standard (non-sequential) supervised learning task. There are at least three important differences that make sequence prediction difficult. First, the set of all possible sequences of labels is an exponentially large set of possible outputs. Second, different in-puts have different lengths, so it is not obvious how to represent every input by a vector of the same fixed length, as is almost universal in su-pervised learning. Third and most important, too much information is lost if we learn a traditional classifier that makes a prediction for each letter separately. Even if the traditional classifier is a function of the whole input sequence, this remains true. In order to achieve high accuracy, correla-tions between neighboring predicted labels must be taken into account.

Learning to predict a sequence of output labels, given a sequence of input data items, is an instance of a structured learning problem. In general, struc-tured learning means learning to predict outputs that have internal structure. This structure can be modeled; to achieve high predictive accuracy, when there are dependencies between parts of an output, it must be modeled. Research on struc-tured learning has been highly successful, with sequence classification as its most important and successful subfield, and with conditional random fields (CRFs) as the most influential approach to learning sequence classifiers. In the present paper, we show that CRFs can achieve extremely good performance on the hyphenation task. The earliest software for automatic hyphenation was implemented for RCA 301 computers, and used by the Palm Beach Post-Tribune and Los An-geles Times newspapers in 1962. These were two different systems. The Florida system had a dic-tionary of 30,000 words; words not in the dictio-nary were hyphenated after the third, fifth, or sev-enth letter, because the authors observed that this was correct for many words. The California sys-tem (Friedlander, 1968) used a collection of rules based on the rules stated in a version of Webster X  X  dictionary. The earliest hyphenation software for a language other than English may have been a rule-based program for Finnish first used in 1964 (Jarvi, 2009).

The first formal description of an algorithm for hyphenation was in a patent application submit-ted in 1964 (Damerau, 1964). Other early pub-lications include (Ocker, 1971; Huyser, 1976). The hyphenation algorithm that is by far the most widely used now is due to Liang (Liang, 1983). Although this method is well-known now as the one used in T E X and its derivatives, the first ver-sion of T E X used a different, simpler method. Liang X  X  method was used also in troff and groff , which were the main original competitors of T E X, and is part of many contemporary software products, supposedly including Microsoft Word. Any major improvement over Liang X  X  method is therefore of considerable practical and commer-cial importance.

Over the years, various machine learning meth-ods have been applied to the hyphenation task. However, none have achieved high accuracy. One paper that presents three different learning meth-ods is (van den Bosch et al., 1995). The lowest per-letter test error rate reported is about 2%. Neu-ral networks have been used, but also without great success. For example, the authors of (Kristensen and Langmyhr, 2001) found that the T E X method is a better choice for hyphenating Norwegian.
The highest accuracy achieved until now for the hyphenation task is by (Bartlett et al., 2008), who use a large-margin structured learning approach. Our work is similar, but was done fully indepen-dently. The accuracy we achieve is slightly higher: word-level accuracy of 96.33% compared to their 95.65% for English. Moreover, (Bartlett et al., 2008) do not address the issue that false positive hyphens are worse mistakes than false negative hy-phens, which we address below. Also, they report that training on 14,000 examples requires about an hour, compared to 6.2 minutes for our method on 65,828 words. Perhaps more important for large-scale publishing applications, our system is about six times faster at syllabifying new text. The speed comparison is fair because the computer we use is slightly slower than the one they used.

Methods inspired by nonstatistical natural lan-guage processing research have also been pro-posed for the hyphenation task, in particular (Bouma, 2003; Tsalidis et al., 2004; Woestenburg, 2006; Haralambous, 2006). However, the methods for Dutch presented in (Bouma, 2003) were found to have worse performance than T E X. Moreover, our experimental results below show that the com-mercial software of (Woestenburg, 2006) allows hyphens incorrectly almost three times more often than T E X.

In general, a dictionary based approach has zero errors for words in the dictionary, but fails to work for words not included in it. A rule-based ap-proach requires an expert to define manually the rules and exceptions for each language, which is laborious work. Furthermore, for languages such as English where hyphenation does not system-atically follow general rules, such an approach does not have good results. A pattern-learning ap-proach, like that of T E X, infers patterns from a training list of hyphenated words, and then uses these patterns to hyphenate text. Although useful patterns are learned automatically, both the T E X learning algorithm and the learned patterns must be hand-tuned to perform well (Liang, 1983).
Liang X  X  method is implemented in a program named PATGEN , which takes as input a training set of hyphenated words, and outputs a collection of interacting hyphenation patterns. The standard pattern collections are named hyphen.tex for American English, ukhyphen.tex for British English, and nehyph96.tex for Dutch. The precise details of how different versions of T E X and L A T E X use these pattern collections to do hy-phenation in practice are unclear. At a minimum, current variants of T E X improve hyphenation ac-curacy by disallowing hyphens in the first and last two or three letters of every word, regardless of what the PATGEN patterns recommend.
Despite the success of Liang X  X  method, incor-rect hyphenations remain an issue with T E X and its current variants and competitors. For instance, incorrect hyphenations are common in the Wall Street Journal , which has the highest circulation of any newspaper in the U.S. An example is the hyphenation of the word  X  X udden X  in this extract: It is the case that most hyphenation mistakes in the Wall Street Journal and other media are for proper nouns such as  X  X etflix X  that do not appear in stan-dard dictionaries, or in compound words such as  X  X udden-acceleration X  above. A linear-chain conditional random field (Lafferty et al., 2001) is a way to use a log-linear model for the sequence prediction task. We use the bar notation for sequences, so  X  x means a sequence of variable length. Specifically, let  X  x be a sequence of n letters and let  X  y be a corresponding sequence of n tags. Define the log-linear model p (  X  y |  X  x ; w ) = The index j ranges over a large set of feature-functions. Each such function F j is a sum along the output sequence for i = 1 to i = n : where each function f j is a 0/1 indicator function that picks out specific values for neighboring tags y i  X  1 and y i and a particular substring of  X  x . The denominator Z (  X  x, w ) is a normalizing constant: where the outer sum is over all possible labelings  X  y of the input sequence  X  x . Training a CRF means finding a weight vector w that gives the best pos-sible predictions for each training example  X  x .

The software we use as an implementation of conditional random fields is named CRF++ (Kudo, 2007). This implementation offers fast training since it uses L-BFGS (Nocedal and Wright, 1999), a state-of-the-art quasi-Newton method for large optimization problems. We adopt the default pa-rameter settings of CRF++, so no development set or tuning set is needed in our work.

We define indicator functions f j that depend on substrings of the input word, and on whether or not a hyphen is legal after the current and/or the previous letter. The substrings are of length 2 to 5, covering up to 4 letters to the left and right of the current letter. From all possible indicator func-tions we use only those that involve a substring that occurs at least once in the training data.
As an example, consider the word hy-phen-ate . For this word  X  x = hyphenate and  X  y = 010001000 . Suppose i = 3 so p is the current letter. Then exactly two functions f j that depend on substrings of length 2 have value 1: I ( y i  X  1 = 1 and y i = 0 and x 2 x 3 = yp ) = 1 , I ( y i  X  1 = 1 and y i = 0 and x 3 x 4 = ph ) = 1 . All other similar functions have value 0: I ( y i  X  1 = 1 and y i = 1 and x 2 x 3 = yp ) = 0 ,
I ( y i  X  1 = 1 and y i = 0 and x 2 x 3 = yq ) = 0 , and so on. There are similar indicator functions for substrings up to length 5. In total, 2,916,942 dif-ferent indicator functions involve a substring that appears at least once in the English dataset.
One finding of our work is that it is prefer-able to use a large number of low-level features, that is patterns of specific letters, rather than a smaller number of higher-level features such as consonant-vowel patterns. This finding is consis-tent with an emerging general lesson about many natural language processing tasks: the best perfor-mance is achieved with models that are discrimi-native, that are trained on as large a dataset as pos-sible, and that have a very large number of param-eters but are regularized (Halevy et al., 2009).
When evaluating the performance of a hyphen-ation algorithm, one should not just count how many words are hyphenated in exactly the same way as in a reference dictionary. One should also measure separately how many legal hyphens are actually predicted, versus how many predicted hy-phens are in fact not legal. Errors of the sec-ond type are false positives. For any hyphenation method, a false positive hyphen is a more serious mistake than a false negative hyphen, i.e. a hyphen allowed by the lexicon that the method fails to identify. The standard Viterbi algorithm for mak-ing predictions from a trained CRF is not tuned to minimize false positives. To address this difficulty, we use the forward-backward algorithm (Sha and Pereira, 2003; Culotta and McCallum, 2004) to es-timate separately for each position the probability of a hyphen at that position. Then, we only allow a hyphen if this probability is over a high threshold such as 0.9.

Each hyphenation corresponds to one path through a graph that defines all 2 k  X  1 hyphenations that are possible for a word of length k . The over-all probability of a hyphen at any given location is the sum of the weights of all paths that do have a hyphen at this position, divided by the sum of the weights of all paths. The forward-backward algorithm uses the sum operator to compute the weight of a set of paths, instead of the max op-erator to compute the weight of a single highest-weight path. In order to compute the weight of all paths that contain a hyphen at a specific location, weight 0 is assigned to all paths that do not have a hyphen at this location. We start with the lexicon for English published by the Dutch Centre for Lexical Information at http://www.mpi.nl/world/celex . We download all English word forms with legal hy-phenation points indicated by hyphens. These include plurals of nouns, conjugated forms of verbs, and compound words such as  X  X ff-line X . We separate the components of compound words and phrases, leading to 204,466 words, of which 68,744 are unique. In order to eliminate abbrevia-tions and proper names which may not be English, we remove all words that are not fully lower-case. In particular, we exclude words that contain capi-tal letters, apostrophes, and/or periods. This leaves 66,001 words.

Among these words, 86 have two different hy-phenations, and one has three hyphenations. For most of the 86 words with alternative hyphen-ations, these alternatives exist because different meanings of the words have different pronuncia-tions, and the different pronunciations have differ-ent boundaries between syllables. This fact im-plies that no algorithm that operates on words in isolation can be a complete solution for the hy-phenation task. 1
We exclude the few words that have two or more different hyphenations from the dataset. Finally, we obtain 65,828 spellings. These have 550,290 letters and 111,228 hyphens, so the average is 8.36 letters and 1.69 hyphens per word. Informal in-spection suggests that the 65,828 spellings contain no mistakes. However, about 1000 words follow British as opposed to American spelling.

The Dutch dataset of 293,681 words is created following the same procedure as for the English dataset, except that all entries from CELEX that are compound words containing dashes are dis-carded instead of being split into parts, since many of these are not in fact Dutch words. 2 We use ten-fold cross validation for the experi-ments. In order to measure accuracy, we com-pute the confusion matrix for each method, and from this we compute error rates. We report both word-level and letter-level error rates. The word-level error rate is the fraction of words on which a method makes at least one mistake. The letter-level error rate is the fraction of letters for which the method predicts incorrectly whether or not a hyphen is legal after this letter. Table 1 explains the terminology that we use in presenting our re-sults. Precision, recall, and F1 can be computed easily from the reported confusion matrices.
As an implementation of Liang X  X  method we use T E X Hyphenator in Java software available at http://texhyphj.sourceforge.net .
 We evaluate this algorithm on our entire English and Dutch datasets using the appropriate language pattern files, and not allowing a hyphen to be placed between the first lefthyphenmin and last righthyphenmin letters of each word. For sets of each fold of cross-validation.
 English the default values are 2 and 3 respectively. For Dutch the default values are both 2.

The hyphenation patterns used by TeXHyphen-ator, which are those currently used by essentially all variants of T E X, may not be optimal for our new English and Dutch datasets. Therefore, we also do experiments with the PATGEN tool (Liang and Breitenlohner, 2008). These are learning ex-periments so we also use ten-fold cross validation in the same way as with CRF++. Specifically, we create a pattern file from 90% of the dataset us-ing PATGEN , and then hyphenate the remaining 10% of the dataset using Liang X  X  algorithm and the learned pattern file.

The PATGEN tool has many user-settable pa-rameters. As is the case with many machine learn-ing methods, no strong guidance is available for choosing values for these parameters. For En-glish we use the parameters reported in (Liang, 1983). For Dutch we use the parameters reported in (Tutelaers, 1999). Preliminary informal exper-iments found that these parameters work better than alternatives. We also disallow hyphens in the first two letters of every word, and the last three letters for English, or last two for Dutch.

We also evaluate the TALO commercial soft-ware (Woestenburg, 2006). We know of one other commercial hyphenation application, which is named Dashes. 3 Unfortunately we do not have access to it for evaluation. We also cannot do a precise comparison with the method of (Bartlett et al., 2008). We do know that their training set was also derived from CELEX, and their maximum reported accuracy is slightly lower. Specifically, for English our word-level accuracy ( X  X wer X ) is 96.33% while their best ( X  X A X ) is 95.65%. In Table 2 and Table 3 we report the performance of the different methods on the English and Dutch datasets respectively. Figure 1 shows how the er-ror rate is affected by increasing the CRF proba-bility threshold for each language.

Figure 1 shows confidence intervals for the er-ror rates. These are computed as follows. For a single Bernoulli trial the mean is p and the vari-ance is p (1  X  p ) . If N such trials are taken, then the observed success rate f = S/N is a random variable with mean p and variance p (1  X  p ) /N . For large N , the distribution of the random vari-able f approaches the normal distribution. Hence we can derive a confidence interval for p using the formula where for a 95% confidence interval, i.e. for c = 0 . 95 , we set z = 1 . 96 . All differences between rows in Table 2 are significant, with one exception: the serious error rates for PATGEN and TALO are not statistically significantly different. A similar conclusion applies to Table 3.
 For the English language, the CRF using the Viterbi path has overall error rate of 0.84%, com-pared to 6.81% for the T E X algorithm using Amer-ican English patterns, which is eight times worse. However, the serious error rate for the CRF is less good: 0.41% compared to 0.24%. This weak-ness is remedied by predicting that a hyphen is al-lowable only if it has high probability. Figure 1 shows that the CRF can use a probability thresh-old up to 0.99, and still have lower overall error rate than the T E X algorithm. Fixing the probabil-ity threshold at 0.99, the CRF serious error rate is 0.04% (224 false positives) compared to 0.24% (1343 false positives) for the T E X algorithm.
For the English language, TALO yields overall error rate 1.99% with serious error rate 0.72%, so the standard CRF using the Viterbi path is better on both measures. The dominance of the CRF method can be increased further by using a prob-ability threshold. Figure 1 shows that the CRF can use a probability threshold up to 0.94, and still have lower overall error rate than TALO. Us-ing this threshold, the CRF serious error rate is 0.12% (657 false positives) compared to 0.72% (3970 false positives) for TALO.

For the Dutch language, the standard CRF us-ing the Viterbi path has overall error rate 0.08%, compared to 0.81% for the T E X algorithm. The serious error rate for the CRF is 0.04% while for T E
X it is 0.18%. Figure 1 shows that any probabil-ity threshold for the CRF of 0.99 or below yields lower error rates than the T E X algorithm. Using the threshold 0.99, the CRF has serious error rate only 0.005%.

For the Dutch language, the TALO method has overall error rate 0.61%. The serious error rate for TALO is 0.11%. The CRF dominance can again be increased via a high probability thresh-old. Figure 1 shows that this threshold can range up to 0.98, and still give lower overall error rate than TALO. Using the 0.98 threshold, the CRF has serious error rate 0.006% (206 false positives); in comparison the serious error rate of TALO is 0.11% (3638 false positives).

For both languages, PATGEN has higher serious letter-level and word-level error rates than T E X us-ing the existing pattern files. This is expected since the pattern collections included in T E X distribu-tions have been tuned over the years to minimize objectionable errors. The difference is especially pronounced for American English, for which the standard pattern collection has been manually im-proved over more than two decades by many peo-ple (Beeton, 2002). Initially, Liang optimized this pattern collection extensively by upweighting the most common words and by iteratively adding exception words found by testing the algorithm against a large dictionary from an unknown pub-lisher (Liang, 1983).

One can tune PATGEN to yield either better overall error rate, or better serious error rate, but not both simultaneously, compared to the T E X al-gorithm using the existing pattern files for both languages. For the English dataset, if we use Liang X  X  parameters for PATGEN as reported in (Sojka and Sevecek, 1995), we obtain overall er-ror rate of 6.05% and serious error rate of 0.85%. It is possible that the specific patterns used in T E X implementations today have been tuned by hand to be better than anything the PATGEN software is capable of. This section presents empirical results following two experimental designs that are less standard, but that may be more appropriate for the hyphen-ation task.

First, the experimental design used above has an issue shared by many CELEX-based tagging or transduction evaluations: words are randomly divided into training and test sets without be-ing grouped by stem. This means that a method can get credit for hyphenating  X  X ccents X  correctly, when  X  X ccent X  appears in the training data. There-fore, we do further experiments where the folds for evaluation are divided by stem, and not by word; that is, all versions of a base form of a word appear in the same fold. Stemming uses the English and Dutch versions of the Porter stem-mer (Porter, 1980). 4 The 65,828 English words in our dictionary produce 27,100 unique stems, while the 293,681 Dutch words produce 169,693 unique stems. The results of these experiments are shown in Tables 4 and 5.

The main evaluation in the previous section is based on a list of unique words, which means that in the results each word is equally weighted. Be-cause cross validation is applied, errors are always measured on testing subsets that are disjoint from the corresponding training subsets. Hence, the accuracy achieved can be interpreted as the per-formance expected when hyphenating unknown words, i.e. rare future words.

However, in real documents common words appear repeatedly. Therefore, the second less-standard experimental design for which we report results restricts attention to the most common En-glish words. Specifically, we consider the top 4000 words that make up about three quarters of all word appearances in the American National Corpus, which consists of 18,300,430 words from written texts of all genres. 5 From the 4,471 most frequent words in this list, if we omit the words not in our dataset of 89,019 hyphenated English words from CELEX, we get 4,000 words. The words that are omitted are proper names, contrac-tions, incomplete words containing apostrophes, and abbreviations such as DNA. These 4,000 most frequent words make up 74.93% of the whole cor-pus.

We evaluate the following methods on the 4000 words: Liang X  X  method using the American pat-terns file hyphen.tex , Liang X  X  method using the patterns derived from PATGEN when trained on the whole English dataset, our CRF trained on the whole English dataset, and the same CRF with a probability threshold of 0.9. Results are shown in Table 6. In summary, T E X and PATGEN make serious errors on 43 and 113 of the 4000 words, respectively. With a threshold of 0.9, the CRF ap-proach makes zero serious errors on these words. Table 7 shows the speed of the alternative meth-ods for the English dataset. The column  X  X ea-tures/Patterns X  in the table reports the number of feature-functions used for the CRF, or the number of patterns used for the T E X algorithm. Overall, the CRF approach is about ten times slower than the T E X algorithm, but its performance is still ac-ceptable on a standard personal computer. All ex-periments use a machine having a Pentium 4 CPU at 3.20GHz and 2GB memory. Moreover, infor-mal experiments show that CRF training would be about eight times faster if we used CRFSGD rather than CRF++ (Bottou, 2008).

From a theoretical perspective, both methods have almost-constant time complexity per word if they are implemented using appropriate data struc-tures. In T E X, hyphenation patterns are stored in a data structure that is a variant of a trie. The CRF software uses other data structures and op-timizations that allow a word to be hyphenated in time that is almost independent of the number of feature-functions used. Finding allowable places in words to insert hy-phens is a real-world problem that is still not fully solved in practice. The main contribu-tion of this paper is a hyphenation method that is clearly more accurate than the currently used Knuth/Liang method. The new method is an ap-Table 7: Timings for the English dataset (training and testing on the whole dataset that consists of 65,828 words). plication of CRFs, which are a major advance of recent years in machine learning. We hope that the method proposed here is adopted in practice, since the number of serious errors that it makes is about a sixfold improvement over what is cur-rently in use. A second contribution of this pa-per is to provide training sets for hyphenation in English and Dutch, so other researchers can, we hope, soon invent even more accurate methods. A third contribution of our work is a demonstration that current CRF methods can be used straightfor-wardly for an important application and outper-form state-of-the-art commercial and open-source software; we hope that this demonstration acceler-ates the widespread use of CRFs.

