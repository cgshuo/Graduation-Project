 2 Institute for Translational Sciences, 3 Dept. of Preventative Medicine and Community Health, In applications such as active learning [1, 2, 3, 4], disease/fault diagnosis [5, 6, 7], toxic chemical identification [8], computer vision [9, 10] or the adaptive traveling salesman problem [11], one often encounters the problem of identifying an unknown object while minimizing the number of binary objects and a set Q = { q 1 ,  X  X  X  ,q N } of N distinct subsets of  X  known as queries. An unknown from Q as possible, where a query q  X  Q returns a value 1 if  X   X  q , and 0 otherwise. For example, active diagnosis, objects may correspond to faults, and queries to alarms. This problem has been Q are finite, and the queries are noiseless.
 The goal in object identification is to construct an optimal binary decision tree, where each internal node in the tree is associated with a query from Q , and each leaf node corresponds to an object from  X  . Optimality is often with respect to the expected depth of the leaf node corresponding to the unknown object  X  . In general the determination of an optimal tree is NP-complete [13]. Hence, various greedy algorithms [5, 14] have been proposed to obtain a suboptimal binary decision tree. A well studied algorithm for this problem is known as the splitting algorithm [5] or generalized binary search (GBS) [1, 2]. This is the greedy algorithm which selects a query that most evenly divides the probability mass of the remaining objects [1, 2, 5, 15]. GBS assumes that the end goal is to rapidly identify individual objects. However, in applications such as disease diagnosis, where  X  is a collection of possible diseases, it may only be necessary to identify the intervention or response to an object, rather than the object itself. In these prob-lems, the object set  X  is partitioned into groups and it is only necessary to identify the group to which the unknown object belongs. We note below that GBS is not necessarily efficient for group identification.
 To address this problem, we first present a new interpretation of GBS from a coding-theoretic per-spective by viewing the problem of object identification as constrained source coding. Specifically, we present an exact formula for the expected number of queries required to identify an unknown object in terms of Shannon entropy of the prior distribution  X  , and show that GBS is a top-down algorithm that greedily minimizes this cost function. Then, we extend this framework to the problem of group identification and derive a natural extension of GBS for this problem.
 We also extend the coding theoretic framework to the problem of object (or group) identification where the cost of identifying an object grows exponentially in the number of queries, i.e., the cost a scenario arises have been discussed earlier in the context of source coding [16], random search trees [17] and design of alphabetic codes [18], for which efficient optimal or greedy algorithms have been presented. In the context of object/group identification, the exponential cost function has certain advantages in terms of avoiding deep trees (which is crucial in time-critical applications) and being more robust to misspecification of the prior probabilities. However, there does not exist an algorithm to the best of our knowledge that constructs a good suboptimal decision tree for the problem of object/group identification with exponential costs. Once again, we show below that GBS is not necessarily efficient for minimizing the exponential cost function, and propose an improved greedy algorithm that generalizes GBS. 1.1 Notation We denote an object identification problem by a pair ( B ,  X ) where B is a known M  X  N binary query from the set Q at each of its internal nodes, with the leaf nodes terminating in the objects from  X  . For a decision tree with L leaves, the leaf nodes are indexed by the set L = { 1 ,  X  X  X  ,L } and the the set of queries that have been performed along the path from the root node up to that node. An for the rows corresponding to  X  i and  X  are same over the columns corresponding to queries in Q a . to the objects in  X  a that respond 0 and 1 to the query at node  X  a  X , respectively. We denote by  X  lim We begin by noting that object identification reduces to the standard source coding problem [19] in the special case when Q is complete , meaning, for any S  X   X  there exists a query q  X  Q such that either q = S or  X  \ q = S . Here, the problem of constructing an optimal binary decision tree is equivalent to constructing optimal variable length binary prefix codes, for which there exists an efficient optimal algorithm known as the Huffman algorithm [20]. It is also known that the expected length of any binary prefix code (i.e., expected depth of any binary decision tree) is bounded below by the Shannon entropy of the prior distribution  X  [19].
 For the problem of object identification, where Q is not complete, the entropy lower bound is still valid, but Huffman coding cannot be implemented. In this case, GBS is a greedy, top-down al-gorithm that is analogous to Shannon-Fano coding [21, 22]. We now show that GBS is actually greedily minimizing the expected number of queries required to identify an object. First, we define a parameter called the reduction factor on the binary matrix/tree combination that provides a useful quantification on the expected number of queries required to identify an object. reduction factor at any internal node  X  a  X  in the tree is defined by  X  a = max {  X   X  of decision trees that can uniquely identify all the objects in the set  X  . We assume that the rows of tree. Then the expected number of queries required to identify an unknown object using a tree (or, the expected depth of a tree) is L 1 ( X ) = P i  X  i d i . Note that the cost function depends on both  X  and d = ( d 1 ,  X  X  X  ,d M ) . However, we do not show the dependence on d explicitly. Theorem 1. For any T  X  X  ( B ,  X ) , the expected number of queries required to identify an unknown object is given by Theorems 1, 2 and 3 are special cases of Theorem 4, whose proof is sketched in the Appendix. Complete proofs are given in the Supplemental Material. Since H (  X  a )  X  1 , this theorem recovers the result that L 1 ( X ) is bounded below by the Shannon entropy H ( X ) . It presents the exact formula for the gap in this lower bound. It also follows from the above result that a tree attains the entropy minimizing L 1 ( X ) can be formulated as the following optimization problem: T ( B ,  X ) . As mentioned earlier, finding a global optimal solution for this optimization problem is NP-complete [13]. Instead, we may take a top down approach and minimize the objective function by minimizing the term C a :=  X   X  a [1  X  H (  X  a )] at each internal node, starting from the root node. Note that the only term that depends on the query chosen at node  X  a  X  in this cost function is  X  a . Hence the algorithm reduces to minimizing  X  a (i.e., choosing a split as balanced as possible) at each internal node a  X  X  .
 In other words, greedy minimization of (2) is equivalent to GBS. In the next section, we show how this framework can be extended to derive greedy algorithms for the problems of group identification and object identification with exponential costs. 3.1 Group Identification In group identification 1 , the goal is not to determine the unknown object  X   X   X  , rather the group to which it belongs, in as few queries as possible. Here, in addition to B and  X  , the group labels for the objects are also provided, where the groups are assumed to be disjoint.
 y = k } . It is important to note here that the group identification problem cannot be simply reduced within a group need not respond the same to each query. For instance, consider the toy example shown in Figure 1 where the objects  X  1 , X  2 and  X  3 belonging to group 1 cannot be collapsed into a single meta object as these objects respond differently to queries q 1 and q 3 .
 In this context, we also note that GBS can fail to produce a good solution for a group identification problem as it does not take the group labels into consideration while choosing queries. Once again, consider the toy example shown in Figure 1 where query q 2 is sufficient to identify the group of an unknown object, whereas GBS requires 2 queries to identify the group when the unknown object is either  X  2 or  X  4 . Here, we propose a natural extension of GBS to the problem of group identification. Note that when constructing a tree for group identification, a greedy, top-down algorithm terminates splitting when all the objects at the node belong to the same group. Hence, a tree constructed in this fashion can have multiple objects ending in the same leaf node and multiple leaves ending in the same group. For a tree with L leaves, we denote by L k  X  L = { 1 ,  X  X  X  ,L } the set of leaves that terminate in group k . Similar to  X  k  X   X  , we denote by  X  k a  X   X  a the set of objects belonging to we define a new parameter called the group reduction factor for each group k  X  X  1 ,  X  X  X  ,K } at each internal node.
 Definition 2 (Group reduction factor) . Let T be a decision tree constructed on a group identification  X  node j  X  L . Let random variable X denote the number of queries required to identify the group of an unknown object  X  . Then, the expected number of queries required to identify the group of an unknown object using the given tree is equal to Theorem 2. For any T  X  X  ( B ,  X  , y ) , the expected number of queries required to identify the group of an unknown object is given by where  X  y = (  X   X  1 ,  X  X  X  , X   X  K ) denotes the probability distribution of the object groups induced by the labels y and H (  X  ) denotes the Shannon entropy.
 Note that the term in the summation in (4) is non-negative. Hence, the above result implies that L ( X ) is bounded below by the Shannon entropy of the probability distribution of the groups. It to  X  k a = 1 for all groups at every internal node.
 Using this result, the problem of finding a decision tree with minimum L 1 ( X ) can be formulated as: This optimization problem being a generalized version of that in (2) is NP-complete. Hence, we may take a top-down approach and minimize the objective function greedily by minimizing the term  X  a [1  X  H (  X  a ) + P to minimizing C a := 1  X  H (  X  a ) + P K k =1  X   X  k a  X  Group-GBS (GGBS)
Initialize: L = { root node } , Q root =  X  while some a  X  X  has more than one group
Choose query q  X  = arg min q  X  Q \ Q Form child nodes l ( a ) ,r ( a )
Replace  X  a  X  with l ( a ) ,r ( a ) in L end Figure 3: Greedy algorithm for group identifi-cation group) while the term P k  X   X  k a  X  its child nodes. This algorithm, which we refer to as Group Generalized Binary Search (GGBS), is summarized in Figure 3. Finally, as an interesting connection with greedy decision-tree algorithms for multi-class classification, it can be shown that GGBS is equivalent to the decision-tree splitting algorithm used in the C 4 . 5 software package, based on the entropy impurity measure [25]. 3.2 Exponential Costs this cost function reduces to the average depth and worst case depth, respectively. That is, As mentioned in Section 2, GBS is tailored to minimize L 1 ( X ) , and hence may not produce a good suboptimal solution for the exponential cost function with  X  &gt; 1 . Thus, we derive an extension of GBS for the problem of exponential costs. Here, we use a result by Campbell [26] which states that the exponential cost L  X  ( X ) of any tree T is bounded below by the  X  -R  X  enyi entropy, given by H problem and derive an explicit formula for the gap in this lower bound. We then use this formula to derive a family of greedy algorithms that minimize the exponential cost function L  X  ( X ) for  X  &gt; 1 . Note that the entropy bound reduces to the Shannon entropy H ( X ) and log 2 M , in the limiting cases where  X  tends to 1 and  X  , respectively.
 Theorem 3. For any  X  &gt; 1 and any T  X  X  ( B ,  X ) , the exponential cost L  X  ( X ) is given by  X  where d a denotes the depth of any internal node  X  a  X  in the tree,  X  a denotes the set of objects that reach node  X  a  X ,  X   X  a = P Campbell X  X  lower bound. This result suggests a top-down greedy approach to minimize L  X  ( X ) , which is to minimize the term (  X   X  1)  X  d a  X  X   X  ( X  a ) +  X   X  l ( a )  X  each internal node, starting from the root node. Noting that the terms that depend on the query chosen at node  X  a  X  are  X   X  L X  X   X  opital X  X  rule that in the limiting case where  X   X  1 ,  X  -GBS reduces to GBS, and in the case where  X   X  X  X  ,  X  -GBS reduces to GBS with uniform prior  X  i = 1 /M . The latter algorithm is GBS but with the true prior  X  replaced by a uniform distribution. Figure 5: Beta distribution over the range [0 . 5 , 1] 3.3 Group Identification with Exponential Costs Finally, we complete our discussion by considering the problem of group identification with expo-and to max j  X  X  d j , i.e., the worst case depth of the tree, in the case where  X   X  X  X  . group of an object is given by  X  where  X  y = (  X   X  1 ,  X  X  X  , X   X  K ) denotes the probability distribution of the object groups induced by Note that the definition of D  X  ( X  a ) in this theorem is a generalization of that in Theorem 3. As mentioned earlier, Theorems 1-3 are special cases of the above theorem, where Theorem 2 follows as  X   X  1 and Theorem 1 follows when each group is of size one in addition. This result also implies a top-down, greedy algorithm to minimize L  X  ( X ) , which is to choose a query that minimizes C in the case where  X   X  X  X  , this reduces to choosing a query that minimizes the maximum number of groups in the child nodes [27]. We compare the performance of the proposed algorithms to that of GBS on synthetic data generated using different random data models. 4.1 Group Identification For fixed M = |  X  | and N = | Q | , we consider a random data model where each query q  X  Q is as its response to query q , whereas, when it is close to 1 , most of the objects within a group are is equally likely to exhibit 0 or 1 as its response to the query, where a group response corresponds for a query q , the object responses to query q (i.e., the binary column of 0  X  X  and 1  X  X  corresponding to query q in B ) are generated as follows 1 . Flip a fair coin to generate a Bernoulli random variable, x 3 . For each object in group k , assign b k as the object response to q with probability  X  w ( q ) lowing the above procedure for each query.
 We compare the performances of GBS and GGBS on random datasets generated using the above model. We demonstrate the results on datasets of size N = 200 ( # of queries) and M = 400 ( # of objects), where we randomly partitioned the objects into 15 groups and assumed a uniform prior on the objects. For each dataset, the correlation parameters are drawn from independent beta  X  different values of  X  . Note that  X  = 1 corresponds to a uniform distribution, while, for  X  &lt; 1 the distribution is right skewed and for  X  &gt; 1 the distribution is left skewed.
 Figure 6 compares the mean value of the cost function L 1 ( X ) for GBS and GGBS over 100 randomly generated datasets, for each value of (  X  w , X  b ) . This shows the improved performance of GGBS over GBS in group identification. Especially, note that GGBS achieves performance close to the entropy bound as  X  w decreases. This is due to the increased number of queries with  X  w ( q ) close to 1 in the in group identification, but can be overlooked by GBS. 4.2 Object Identification with Exponential Costs We consider the same random data model as above where we set K = M , i.e., each group is comprised of one object. Thus, the only correlation parameter that determines the structure of the dataset is  X  b ( q ) ,q  X  Q . Figure 7 demonstrates the improved performance of  X  -GBS over standard GBS, and GBS with uniform prior, over a range of  X  values, for a dataset generated using the above to the average value of the cost function L  X  ( X ) as a function of  X  over 100 repetitions. In each randomly permuting the objects. Note that in the special case when  X  = 0 , this reduces to the uniform distribution and as  X  increases, it tends to a skewed distribution with most of the probability mass concentrated on few objects.
 ent values of  X , X  . In all our experiments, we observed  X  -GBS to be consistently performing better than both the standard GBS, and GBS with uniform prior. In addition, the performance of  X  -GBS has been observed to be very close to that of the entropy bound. Finally, Figure 7 also reflects that  X  -GBS converges to GBS as  X   X  1 , and to GBS with uniform prior as  X   X  X  X  . In this paper, we show that generalized binary search (GBS) is a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. We then use this inter-pretation to extend GBS in two ways. First, we consider the case where the objects are partitioned into groups, and the goal is to identify only the group of the unknown object. Second, we consider the problem where the cost of identifying an object grows exponentially in the number of queries. The algorithms are derived in a common framework. In particular, we prove the exact formulas for the cost function in each case that close the gap between previously known lower bounds related to Shannon and R  X  enyi entropy. These exact formulas are then optimized in a greedy, top-down manner to construct a decision tree. We demonstrate the improved performance of the proposed algorithms over GBS through simulations. An important open question and the direction of our future work is to relate these greedy algorithms to the global optimizer of their respective cost functions. Acknowledgements G. Bellala and C. Scott were supported in part by NSF Awards No. 0830490 and 0953135. S. Bhavnani was supported in part by CDC/NIOSH grant No. R21OH009441. Define two new functions e L  X  and e H  X  as  X  -R  X  enyi entropy H  X  ( X  y ) as where we use the definition of  X  , i.e.,  X  = 1 1+log where d a denotes the depth of internal node  X  a  X  in the tree T . Similarly, we note from (6b) and Lemma 2 that Finally, the result follows from (7) and (8) above.
 P of the objects at that node.
 Lemma 2. The function e H  X  can be decomposed over the internal nodes in a tree T , as internal node a  X  X  .
 The above two lemmas can be proved using induction over subtrees rooted at any internal node  X  a  X  in the tree. The details may be found in the Supplemental Material.
