 In addition to the main content, most web pages also con-tain navigation panels, advertisements and copyright and disclaimer notices. This additional content, which is also known as noise, is typically not related to the main sub-ject and may hamper the performance of web data mining, and hence needs to be removed properly. In this paper, we present Content Extraction via Text Density (CETD) X  a fast, accurate and general method for extracting content from diverse web pages, and using DOM (Document Objec-t Model) node text density to preserve the original struc-ture. For this purpose, we introduce two concepts to mea-sure the importance of nodes: Text Density and Composite Text Density. In order to extract content intact, we propose a technique called DensitySum to replace Data Smoothing. The approach was evaluated with the CleanEval benchmark and with randomly selected pages from well-known websites, where various web domains and styles are tested. The aver-age F 1 -scores with our method were 8.79% higher than the best scores among several alternative methods.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; H.3.1 [ Content Analysis and Indexing ]: Ab-stracting methods Algorithms, Experimentation Content Extraction, Text Density, Composite Text Density, DensitySum
The explosive growth of the Internet has produced a huge number of information sources, and its influence continues to increase. Thus, web data mining has become an important and popular technique for discovering useful information or knowledge. Such research requires main content ( e.g. , an ar-ticle text) from the web to be gathered, processed and stored quickly and efficiently. However, the main content in web pages is often accompanied by a large amount of addition-al content such as navigation menus, banner advertisements and copyright notices. Although such information is expect-ed by website owners and benefits user browsing, it is unre-lated to the topic of the web pages and not simple enough for computer programs to parse. Thus this information can be treated as noise, hampering information gathering and web mining.

In order to improve the performance of web mining and in-formation retrieval from web pages, content extraction tech-niques have been developed to remove such noise. Generally, content extraction improves performance, and is essential for many real world applications.

Traditionally, building text corpora was a very expensive and time-consuming process. By automatically download-ing textual data from the web, extremely large corpora can be built in a short period, at relatively low cost. There-fore, the idea of Web as Corpus has been very attractive for many researchers in Natural Language Processing and relat-ed areas. In order to prepare web data for use as a corpus, ACL-SIGWAC held the first CleanEval competition during the summer of 2007 [23].

Additionally, as Pocket-sized devices with small screens such as PDAs and mobile phones have become ubiquitous, adapting web pages for small screens has become an increas-ingly important and challenging task [3, 8].

However, extracting the main content from the web pages has become more difficult and nontrivial. As early as 2005, Gibson et al. [14] estimated the noise to account for around 40-50% of the data on the web, and predicted correctly that this ratio would go on increasing. Meanwhile, web page lay-out has become much more complex than before, especially with the development and widespread use of the Cascad-ing Style Sheets (CSS) technique. We find that most of the recent web pages use the style sheets and &lt; div &gt; or &lt; s-pan &gt; tags for structural information to replace structural tags within a web page. Many early content extraction tech-niques failed to keep up with these changes and performed poorly, because recent web pages no longer include particu-used before.

In addition, nowadays most additional web page content, especially advertisements ( e.g. , Google AdSense), are gen-erated dynamically, which means template detection algo-r ithms perform poorly. Moreover, they may also break easily due to the changes in page structure.

In this paper, we propose a highly effective content extrac-tion algorithm to extract the main content from web pages. It can not only extract the main content, but also preserve its original structure information.

Our content extraction technique is based on the following observation: in a typical web page, the noise is usually highly formatted and contain less text and brief sentences, whereas the content is commonly simply formatted and contain more text but far less hyperlinks than in the noise. Furthermore, the content is usually an integral part of a web page and maintains the integrity of the structure, i.e. belongs to an ancestor node in the DOM tree.

First, we propose two measures for the importance of the tags in the web pages: Text Density and Composite Text Density . Once an HTML document is parsed and represent-ed by a DOM tree, we calculate the text density for each node. Higher text density implies the node is more likely to represent a tag with content-text within the web page. In the case of noise the opposite applies. Furthermore, we extend the Text Density to the Composite Text Density by adding statistical information about hyperlinks. To solve the problem of losing low text density nodes in content, a tai-lored technique called DensitySum was designed to extract integral content. The results show that it is a fast, accurate and general content extraction algorithm which outperform-s many current content extraction algorithms on large and varied data sets.

One difference between our approach and current methods is that we make no assumptions about the specific structure of an input web page, nor do we look for particular HTML cues. Another is that we can preserve the original struc-ture of the input pages since all operations are performed on their DOM trees. Most existing approaches remove all tags from the HTML document and just output the text of the contents. They are not very user-friendly, and cannot be used to extract structured data from the web pages. In contrast, our method can output cleaned HTML documents instead of unformatted text.

The rest of the paper is organized as follows: after briefly reviewing related work in section 2, we propose two different definitions for text density in section 3. Next, we describe how to choose the threshold and the algorithm to extract content. Then, we describe our evaluation setup and com-pare the performance of our approach with other content extraction methods, and discuss the results. Finally we of-fer our conclusions and plans for future research. The term Content Extraction (CE) was introduced by Rahman et al. in 2001 [25]. In the last decade, extrac-tion of content from web pages has been studied intensively and numerous methods have been developed.

In the early days of content extraction, some handcrafted web scrapers (such as NoDoSE [2] or XWRAP [5]) extract-ed article text embedded in a common template from web pages by looking for some HTML cues, using regular ex-pressions. These were written in a traditional programming language or with some specialized tools designed for con-tent extraction. The biggest advantage of these methods was their accuracy. Obviously, the disadvantage lies in the fact that different regular expressions need to be manually created for each website. Still, even individual websites em-ploy multiple structures; furthermore, these websites may also change structures or layouts over time. All the above situations mean such approaches require constant updating.
Kushmerick [20] and Davison [9] proposed machine learn-ing mechanisms to recognize banner advertisements, redun-dant and irrelevant links in web pages. However, these tech-niques cannot be put to general use because they require a large set of manual-labeled training data and domain knowl-edge to generate classification rules.

The Vision-based Page Segmentation (VIPS) technique was introduced by Cai et al. [6] in 2003 to divide a web page into a tree, where the nodes are visually grouped blocks. Based on the VIPS method, Song et al. [26] presented an approach to rank block importance for web pages through learning algorithms using spatial features (such as position or size) and content features (such as the number of images and links); and Fernandes et al. [12] developed another way to compute block importance of a web page by means of assigning weights to classes of structurally similar blocks. However, VIPS must partially render a page in order to analyze it. Additionally, if external style sheets are used, they should also be retrieved. Therefore, compared to other techniques, VIPS is resource intensive.
 A different approach for content extraction is Template Detection (TD) algorithms [4, 21, 28, 7, 18] in which collec-tions of documents based on the same template are used to learn a common structure. Bar-Yossef et al. presented an approach to automatically detect templates from the largest pagelet , i.e. self-contained regions in a web page [4]. Lin et al. partitioned a page with &lt; table &gt; tags, and identified re-dundant blocks using an entropy measure over a set of word-based features [21]. In order to improve the performance of web page clustering and classification, Yi et al. introduced a site style tree (SST) structure that labels DOM nodes with similar styles across pages as uninformative [28]. Chen et al. combined template detection and removal with the in-dex building process in large scale search engines to increase accuracy and speed. They segmented pages into blocks and clustered them based upon their styles and positions; then similar clusters among different pages were identified as part of the template [7].

In general, template detection algorithms identify the con-tent by removing identical parts found in all web pages. This is an accurate approach but has been found to be too bur-densome. The reason is that models need to be built for each website. This means pages in each site should share the same template. Furthermore, these methods often incorrect-ly assume that uninformative segments are largely repeated across pages (this is clearly not the case for varying text ad-vertisements or lists of related articles) and any updates of the layout or structure may result in the template X  X  failure.
Conversely, in the CleanEval shared task, only a few pages are available from the same site, thus requiring a more gen-eral approach. The winner of the CleanEval task split pages by their tags into a sequence of blocks and then labeled each block as  X  X ontent X  or  X  X oise X  using conditional random fields with a number of block-level features [23].

There are sets of content extraction approaches based on statistical information of web pages. In 2001, Finn et al. introduced the Body Text Extraction (BTE) algorithm to improve the accuracy of the content X  X  classifier for digital libraries [13]. They interpreted an HTML document as a s equence of word and tag tokens, and then extracted con-tent by identifying a single, continuous region which contains the most words and the least HTML tags. To overcome the restriction of BTE in discovering only a single continuous block of text, Pinto et al. [24] extended this method to con-struct Document Slope Curves (DSC), in which a windowing technique is used to locate document regions in which word tokens are more frequent than tag tokens. They used this technique to improve performance and efficiency for answer-ing questions with web data in their QuASM system.
Mantratzis et al. presented an approach named Link Quo-ta Filter (LQF) to identify link lists and navigation elements by identifying DOM elements which have a high ratio of text residing in hyperlink anchors [22]. It can be applied to con-tent extraction by removing the resulting link blocks from the document. The drawback of this method is that it relies on Structure elements , and it can only identify hyperlink-type noise.
 Debnath et al. proposed the FeatureExtractor (FE) and KFeatureExtractor (KFE) techniques based on block seg-mentation of the HTML document [10, 11]. Each block is analyzed for particular features such as the amount of text, the presence of images and script code. Content text is ex-tracted by selecting blocks that correspond best to a desired feature, e.g. the presence of most text.

The Content Code Blurring (CCB) algorithm was intro-duced by Gottron in 2008 [16]. Content regions are detect-ed in homogeneously formatted source code character se-quences.
 Weninger et al. introduced the Content Extraction via Tag Ratios (CETR) algorithm, a method to extract content text from diverse web pages using the HTML document X  X  tag ratios [27]. The approach computes tag ratios on a line-by-line basis and then clusters the resulting histogram into content and noise areas. This is a laconic and efficient al-gorithm, however vulnerable to the page X  X  source code style changes.

Kohlsch  X  utter et al. developed a simple, yet effective tech-nique to classify individual text elements from a web page [19]. They analyzed a small set of shallow text features, which were theoretically grounded by stochastic text generation processes from Quantitative Linguistics, for boilerplate de-tection. Their study provides theoretical support for the method in this paper.

Gupta et al. attempted to combine different heuristics into one system called the Crunch framework [17]. They demonstrated that a well-chosen combination of differen-t content extraction algorithms can provide better results than a single approach on its own. Since Crunch, sever-al new content extraction algorithms have been developed. Then Gottron developed CombineE framework, a more re-cent ensemble method, which made it easier to configure ensembles of content extraction algorithms [15].
Let X  X  take the news article from The New York Times 1 shown in Figure 1 as an example. This page is typical of the web: the banner, navigation and advertisements take up about half space on the page while the content of the page is confined to a relatively small space. http://www.nytimes.com Figure 1: The New York Times web page article
T o extract content from this web page, we take advantage of typical text features of content and noise. It was found that the noise in web pages is usually highly formatted and contains less text and brief sentences. On the other hand, the content is commonly lengthy and simply formatted. The example in Figure 1 also supports this observation.
Document Object Model (DOM) [1] is a standardized, platform and language independent interface for accessing and updating content, structure and style of documents. Each HTML page corresponds to a DOM tree where tags are internal nodes and the detailed text and images are leaf nodes.
 Example 1. Below is a brief segment of HTML code from Figure 1. 1. 8.
Example 1 shows a segment of HTML code from Figure 1, and Figure 2 shows the DOM tree of Example 1.

Notice that our study of HTML web pages begins from the &lt;body&gt; tag since all the viewable parts in a web page are within the scope of &lt;body&gt; .
South Korea. . .
Once an HTML document has been parsed and is repre-sented by a DOM tree, the number of characters and tags that each node contains can be figured out. Then, such statistical information can be added to the node.
Furthermore, we can compute the ratios of the number of characters to the number of tags per node. Now, we define the Text Density , the basis of our method, as follows:
Definition 1. If i is a tag (corresponding to an element node in DOM) in a web page, then the tag i  X  X  Text Density ( T D i ) is the ratio of its CharNumber to its TagNumber: w here C i is the number of all characters under i , T i is the number of all tags under i . Note that if T i is 0, it should be set to 1.

T D i is a measure of the density of each node X  X  text in a web page. It assigns high values for nodes that common-ly contain long and simply formatted text and low values for highly formatted nodes containing less, brief text. It is useful for determining whether a part of a web page is mean-ingful or not. Clearly, content in a web page will be assigned relatively high density.

Before performing the computation, script , comment and style tags are removed from the DOM tree because such in-formation is not visible and would likely skew the results if included in computation. In the likely case where the num-ber of tags under a particular node is 0, the density is set to the number of characters the node contains. The Compute-Density algorithm is described as Algorithm 1 where N is a DOM node being computed.

Computing the text density is a recursive task as evident from the simplicity of Algorithm 1. Example 2 below shows the text density for each node of Example 1.

Example 2. The text density for the five tags in Exam-ple 1 are computed as follows: 1. &lt;div "main"&gt; : Chars=85 , Tags=4 , Density=21.25 2. &lt;div "article"&gt; : Chars=85 , Tags=3 , Density=28.33 3. &lt;div "articleHeadline"&gt; : Chars=46 , Tags=1 , Density=46 4. &lt;div "articleBody"&gt; : Chars=39 , Tags=1 , Density=39 5. &lt;a&gt; : Chars=15 , Tags=1 , Density=15 Algorithm 1 P seudocode of ComputeDensity(N) 1: I NPUT : N 2: OUTPUT : N 3: for all child node C in N do 4: ComputeDensity ( C ) 5: end for 6: N.CharN umber  X  CountChar ( N ) 7: N.T agN umber  X  CountT ag ( N ) 8: if N.T agN umber == 0 then 9: N.T agN umber  X  1 10: end if 11: N.Density  X  N.CharN umber/N.T agN umber
Figure 3 shows the resulting density-histogram for the p age in Figure 1. It can be seen that there are nodes with a relatively high text density. Intuitively, we can take the high text density portion as the web page X  X  content. Figure 3: Text density for each node from NYtimes w eb page
With further study, we find that most of the noise in the web pages consists of hyperlinks. The example web page in Figure 1 supports this observation.

Based on the above discussion, we calculate additional statistical information per node as below:
According to the four pieces of statistical information men-tioned above, we redefine the Text Density . In order to dis-tinguish it from the Text Density defined above, we call it Composite Text Density . Unless otherwise specified, we use Text Density to refer to these two densities.

Definition 2. If i is a tag (corresponding to an elemen-t node in DOM) in a web page, then its Composite Text Density ( CT D i ) is: w here C i is the number of all characters under i , T i is the number of all tags under i , LC i is the number of all hyperlink characters under i ,  X  L C i is the number of all non-hyperlink characters under i , LT i is the number of all hyperlink tags under i , LC b is the number of all hyperlink characters under the &lt;body&gt; tag and C b is the number of all characters under the &lt;body&gt; tag. Note that when denominators of the formula are 0, set them to 1.

In Definition 2, C i LC perlink texts in i ; accordingly, T i LT portion of hyperlink tags. When tag i contains numerous non-hyperlink characters and few hyperlinks, they will as-sign high values to it, and vice versa. Meanwhile, C i would get a low value in this case, and high otherwise. The containing lengthy and homogeneously formatted text from getting an extremely high values, or nodes which contain brief text ( e.g. , news headlines or one sentence paragraphs) from getting extremely low values.

We argue that a node with too many hyperlinks and less text is less important, thus getting a low density value; and a node that contains much non-hyperlink text and few hyper-links is more important, and receives a high density value. Clearly, a node X  X  density will be zero if there are only hyper-links in its subtree. In another extreme case, all tags of a web page which have no hyperlinks will get an infinite den-sity. Thus, we would classify such a page as not containing noise.

Compared to Figure 3, Composite Text Density, shown in Figure 4, is better suited for classification because of the more obvious differences between sections. These two cal-culation methods of Text Density are discussed further in Section 5. Figure 4: Composite text density for each node from N Ytimes web page
In this section we describe the technique used to extract the content. The idea behind this approach is to determine a threshold t which divides nodes into content or noise sec-tions. Simply, any node with text density greater than or equal to t should be labeled as content; and any node should be labeled as noise if its text density is less than t . The prob-lem then becomes a matter of finding the best value of t and a way to extracting the content completely.
Intuitively, since the &lt;body&gt; tag is the root node that we compute, therefore it should contains both content and noise. This means that it contains more text than noise, and more hyperlinks than meaningful content. Hence, its text density should be an intermediate value sufficient to distinguish between the two. Empirically, we set the &lt;body&gt; tag X  X  text density as the threshold in our study. It must be noted that this criterion is slightly modified in section 4.2.
It is easy to see that some nodes X  text density is abnormal-ly different from the surrounding nodes in Figure 3 and 4. For instance, pictures, hyperlinks, the byline or dateline of news articles, or very short paragraphs in the main article and references of an article may have abnormally low tex-t density; conversely, some noise nodes ( e.g. , copyright or disclaimer information) may get an abnormally high text density. The authors of [19] also noted this fact. Therefore, many content nodes may be lost if we simply label a node as content or noise just according to the threshold; and some noise nodes may be retained.

To solve this problem, we propose a technique called Den-sitySum . It is known that Data Smoothing can help with this problem to a certain extent by smoothing outlying peak-s and valleys, increasing cohesiveness within sections and differences between sections. In general, although data s-moothing can achieve good results, it may still lose some low-density content nodes; meanwhile, some noisy nodes may be retained because of text density increase.

DensitySum, is based on the observation that a content block of a web page belongs to an ancestor node in the struc-ture of DOM; and as mentioned before, the text density of content nodes is much higher than that of noise nodes. Therefore, the content block ( i.e. , a node in DOM) will get a peak value if we add up its children X  X  text densities. Here, we define DensitySum as below:
Definition 3. If N is a tag (corresponding to an element node in DOM) in a web page and i is a child of N , then N  X  X  DensitySum is the sum of all its children X  X  text densities:
Where C is the set of N  X  X  children and T extDensity i is the text density of tag i .
 Note that T extDensity here is just a general term; we use Text Density or Composite Text Density in practice.
For a simple case, if there exists just one content block, we identify the content by looking for the node under the &lt;body&gt; tag with the maximum DensitySum, then mark it as content. Afterwards, we extract the content by keeping its ancestors and subtrees.

Note that in many cases, the web page contains more than one content block. Therefore, for each node in which the text density is greater than the threshold, we apply the same method described above to extract content.

Moreover, in some web pages, the content block X  X  text density may be less than the &lt;body&gt;  X  X  text density. This block will be lost if we simply use the &lt;body&gt;  X  X  text densi-ty as threshold. To resolve this problem, the algorithm first finds the maximum DensitySum tag in the whole page, with-out thresholding. Next, we set the minimum text density of the node in the path from the maximum DensitySum tag to &lt;body&gt; tag as threshold. The simple process of content extraction using DensitySum can be seen in Algorithm 2. Algorithm 2 P seudocode of ExtractContent(N) 1: I NPUT : N 2: if N.T extDensity &gt; = threshold then 3: T  X  F indM axDensitySumT ag ( N ) 4: M arkContent ( T ) 5: for all child node C in N do 6: ExtractContent ( C ) 7: end for 8: end if
T he program is implemented using Tidy 2 and RapidXml 3 in C++. Since there are several types of errors ( e.g. , non-standard tags, incorrect encoding of HTML files) in web pages, parsing these web pages directly would fail. We use Tidy to correct such errors, and output them as standard X-HTML documents. Afterwards, the XHTML documents are parsed into DOM trees using RapidXml. Before text den-sities are computed, the algorithm removes invisible parts of an HTML document: scripts , style definitions and comments . Given their similar roles in a webpage, buttons and drop-down lists are treated as hyperlinks.

Three distinct algorithms are implemented. The first is the Text Density with DensitySum method, hereafter re-ferred to as CETD-DS. The second is the Composite Text Density with Data Smoothing method, which, hereafter re-ferred to as CECTD-S; here it is simply weighted averag-ing with sibling nodes. The last one is the Composite Text Density with DensitySum method, hereafter referred to as CECTD-DS. We use Content Extraction via Text Density (CETD) to refer to the three algorithms collectively.
In this section we describe experiments on real world data from various web sites to demonstrate the effectiveness of our method. Data sets used in our experiments and evaluation measures are described first. We then present and discuss our experimental results.
In our experiments we use data from two sources: (1) development and evaluation data sets from the CleanEval competition, and (2) the data sets we gathered from several web sites.

CleanEval: CleanEval is a shared task and competitive evaluation on the topic of cleaning arbitrary web pages [23]. Besides extracting content, the original CleanEval competi-tion also asked participants to annotate the structure of the web pages: identify lists, paragraphs and headers. In this paper, we just focus on extracting content from arbitrary web pages. This data set includes four divisions: a develop-ment set and an evaluation set in both English and Chinese languages which are all hand-labeled. It is a diverse data set, only a few pages are used from each site, and the sites use various styles and structures.

However, we found some errors in the data of CleanEval X  X  gold standard in the experiment, for instance, garbled char-acters or texts clearly not part of the content. Therefore, we manually extracted the main content of these pages using a http://tidy.sourceforge.net/ http://rapidxml.sourceforge.n et/ web browser and saved it into text files (gold standard) in UTF-8. In our experiments, we only use the English data set, and do not distinguish between development and evalua-tion documents since our approach does not require training.
CETD: In order to evaluate our methods on varied sources, we produced a data set (which can be obtained from the author X  X  website 4 ). This data set is separated into two non-overlapping sets. (1) The Big 5 : Ars Technica, BBC, Ya-hoo!, New York Times, Wikipedia, and (2) the Chaos data set chosen randomly from Google News and the best-known blog platforms such as WordPress and Blogger. For our pur-poses we arbitrarily selected 100 pages from each of the Big 5 and 200 pages from Chaos . All these pages X  contents were labeled manually using a web browser and saved as UTF-8 text files to serve as the gold standard.
Standard metrics were used to evaluate and compare the performance of different approaches. Specifically, precision, recall and F 1 -scores were calculated by comparing the results of each method against the hand-labeled gold standard. Let a be the text in the extraction result and b be the text in the gold standard. Precision, recall and F 1 -scores then follow from: where LCS ( a, b ) is the longest common subsequence be-tween a and b . It is important to note that every word in the document is considered to be distinct even if two word-s are lexically the same. However, other methods, such as CETR, treated a and b as a bag of words, i.e. , two words are considered the same if they are lexically the same. The bag of words measurement is more lenient, thus the scores of these methods may be further inflated.

CleanEval uses a different metric to evaluate the partic-ipants X  performance. The scoring method is based on the Levenshtein Distance from the output of an extraction al-gorithm to the gold standard text (the number of insertions and removals of words necessary to align the gold standard text with the output of the extraction algorithm; substitu-tions are not allowed). The full formula is: where alignmentLength ( a, b ) is the number of operations (insert, remove, or align) required to align two word se-quences. The Levenshtein distance is relatively expensive to compute, taking O ( | a | X | b | ) time, which can be prohibitively large when | a | and/or | b | are sufficiently large. We find that our data sets frequently include documents which are that large ( i.e. , size greater than 10,000 words). CleanEval X  X  s-coring script takes an extremely long time to return results, or even an  X  X ut of memory X  error.

CleanEval X  X  metric can be transformed into the following formula:
Score ( a, b ) = LCS ( a, b ) .length http://disnet.cs.bit.edu.cn/ Therefore, we implemented a test program using L ongest Common Subsequence instead. In our experiments, it per-formed very well, and is fast and robust.
In order to evaluate the performance of our methods, we compare them with several other content extraction algo-rithms.
 Several other algorithms (BTE, DSC, FE, KFE, LQ, C-CB) described in Section 2 have been implemented in Java for the Combine E framework [15]. In addition, CETR was also implemented in Java [27]. Evaluation was performed by providing each HTML document as input to each algorithm and collecting the results.
All results are collected by calculating the average of each metrics over all examples.
 Table 1 presents the results of the Content Extraction via Text Density with DensitySum (CETD-DS) method when given the task of extracting contents from the CleanEval, Big 5 and the Chaos data sets.
 Table 1: Results for CETD-DS on various domains Source Precision Recall F 1 Score CleanEval-Eng 92.96% 94.52% 93.73% 88.96% NYTimes 98.38% 95.84% 97.09% 94.42% Yahoo! 83.16% 85.90% 84.51% 72.84% Wikipedia 98.32% 97.22% 97.77% 95.76% BBC 84.39% 95.21% 89.48% 80.66% Ars Technica 97.81% 98.85% 98.33% 96.71% Chaos 92.23% 94.99% 93.59% 88.76%
Table 2 presents the results of Content Extraction vi-a Composite Text Density with DensitySum (CECTD-DS) method.
 Table 2: Results for CECTD-DS on various domains Source Precision Recall F 1 Score CleanEval-Eng 95.87% 97.15% 96.51% 93.87% NYTimes 99.69% 98.16% 98.92% 97.86% Yahoo! 84.59% 93.99% 89.04% 80.71% Wikipedia 98.25% 92.77% 95.43% 91.49% BBC 86.15% 97.95% 91.67% 84.44% Ars Technica 98.04% 99.51% 98.76% 97.57% Chaos 96.21% 96.10% 96.15% 93.47%
Table 1 and 2 clearly show that these two methods perfor-m very well, especially CECTD-DS, which performs better than CETD-DS on the broader corpora. This shows that Composite Text Density is more suitable than Text Density as a measure of the importance of a tag in web pages. Interestingly, the CETD-DS outperforms CECTD-DS for Wikipedia, especially in recall. The reason for this phe-nomenon is the high density of in-text hyperlinks and low noise in Wikipedia pages. For the Composite Text Density method, these hyperlink tags in contents may assign their ancestor tags to a low density value, even lower than the threshold. However, the Text Density method could be un-affected.
 Table 3 presents the results of Content Extraction via Composite Text Density with Smoothing (CECTD-S) method. Table 3: Results for CECTD-S on various domains Source Precision Recall F 1 Score CleanEval-Eng 90.35% 92.60% 91.46% 87.24% NYTimes 96.72% 96.56% 96.64% 94.41% Yahoo! 80.33% 93.34% 86.35% 76.16% Wikipedia 98.02% 97.61% 97.81% 95.75% BBC 82.55% 93.77% 87.80% 79.65% Ars Technica 94.61% 93.56% 94.08% 91.65% Chaos 89.64% 92.86% 91.22% 86.17%
Clearly, Table 2 and 3 show that CECTD-DS performs f ar better than CECTD-S for most data sets, again except the Wikipedia site. They demonstrate that DensitySum for deals more effectively than data smoothing with the situa-tion where content nodes X  text density is abnormally lower than the threshold, and noisy nodes X  text density is abnor-mally higher than threshold.

Overall, these results show that the CECTD-DS perform-s far better than CETD-DS and CECTD-S. It is exciting that the CleanEval scores are higher than the winner of CleanEval competition, which only scored 84.1% on the En-glish data set [23].

The results show that the precision of all three methods is relatively lower when applied to Yahoo! and BBC (com-pared with other sources). For Yahoo!, it is probably be-cause its web pages contain user comments after each arti-cle; and these comments X  structure hierarchies are very deep and separate from the content. That is why our method cannot extract the whole comments block. It is easy to see more precise results from sources such as Ars Technica which hides comments by default, and NYTimes which does not accept comments at all. As for BBC, the reason is that there are hidden disclaimers at the bottom of each page, with very long text. These disclaimers, which should not be computed because they are not visible in the browser, are included in the results.
In order to show the effectiveness of our methods, we com-pare the above performance with the alternative approaches described earlier in this section. Table 4 presents the results with the best approach for each data source in bold.
Interestingly, the DSC algorithm does not perform better than the BTE algorithm, even though it actually extends the BTE algorithm. We believe this is due to the window-ing technique which improves the precision, but reduces the recall rate of the DSC algorithm.

It is noteworthy that CETR is very similar to our ap-proach, whereby content is extracted by tag ratios. How-ever, CETR loses the structural information of web pages, since the tag ratios are computed on a line-by-line basis. The results in Table 4, 5 and 6 show that the Composite Text Density and DensitySum methods outperform CETR.
Table 5 and 6 show that other methods typically achieve either a high recall or a high precision but rarely both. Our CETD performs more consistently than other algorithms with overall high scores in both metrics, and CECTD-DS is the best performing algorithm in most data sets with the average F 1 -score 8.79% higher than the best score among other approaches..
The results show that CETD is an effective and robust content extraction algorithm, which performs relatively well even on non-news web corpora with considerable diversity, such as the CleanEval data set. Note that CETD does not require manual-labeled training examples since it is a com-pletely unsupervised algorithm.
 The results also show that CECTD-DS outperforms CETD-DS and CECTD-S on broader corpora. It shows the Com-posite Text Density and DensitySum techniques are more effective than Text Density and Data Smoothing.

For practical purposes, users usually value recall over pre-cision as a performance metric. Although, CECTD-DS can achieve a very high recall rate, users can still see a marked increase in recall and a sharp decrease in precision by reduc-ing the threshold. This precision/recall tradeoff is shown in Figure 5. We set the threshold in actual use as the base value, and use a coefficient  X  to adjust the threshold value in this experiment. Note that the threshold must be lower than the text density of the &lt;body&gt; tag, otherwise the whole page will be viewed as noise. When  X  = 0, the recall is al-ways 100% because all nodes in DOM are included. For the Yahoo! domain, shown in Figure 5, a good tradeoff might be  X  = 0 . 9. Finding a good threshold value is difficult. In cur-rent experiments, we set  X  = 1, which can already achieve very good results on various domains. Figure 5: Precision, Recall and F 1 t radeoff for Ya-hoo! as the threshold coefficient (  X  ) increases
In contrast to most other methods, which just output un-formatted text, content with complete structure information can be obtained by CETD because all operations are done in the DOM tree. Therefore, CETD can be easily integrated with other applications.

Many current methods only extract the most probable content section. However, there is no rule that a web page may only have a single content section. There are many web pages, especially blogs where content is separated by horizontal lines or other delimiters. CETD is not affected by multiple content sections.

Despite many advantages of our algorithm, there are some weaknesses. It does not perform well with some site cate-gories, such as video and picture sites. The contents of such sites are videos or pictures as well as the comments under them. Therefore, our findings do not hold for these sites. For Youtube, CETD can only extract comments. Another situation where CETD does not perform well is portal home pages. These pages usually contain a vast array of menus and news titles or short news descriptions, and most of these are hyperlinks. CETD has problems discerning the content section(s) of these types of web pages. However, in gen-eral, these pages have no topic. Therefore, extracting the contents of these pages does not make much sense.
In this paper, we have proposed a method for extracting the contents from web pages by Text Density, based on the observation that the content text is usually lengthy and sim-ply formatted, while noise is usually highly formatted and contain less text with brief sentences. Observing that noise contains many more hyperlinks than meaningful content, we extended Text Density to Composite Text Density by adding statistical information about hyperlinks. In order to extract the content completely, we proposed the DensitySum tech-nique instead of Data Smoothing. The effectiveness of the CETD algorithm has been demonstrated. The results show that CETD performs better than several other content ex-traction algorithms.

In addition to its effectiveness, the greatest strength of this algorithm over other methods is the simplicity of its concept and implementation. Furthermore, this algorithm just re-quires a web page as input, and then returns a cleaned page without adjusting parameters, training and building classi-fier models. CETD can also retain the original structure information of the web pages, which can then be utilized for other applications, such as small screen devices.
In the research for this paper, we found that HTML Tidy was not sufficiently robust since it may sometimes cause some pages to not be properly parsed. In the future, the WebKit 5 Kernel will be incorporated to parse web pages, so that pages rendered normally by a browser can be parsed correctly.

Another area for further investigation is to identify the hidden elements of the pages. In some sites, some of the non-content part ( e.g. , BBC disclaimers) is not visible. We may achieve better results if we can remove these non-visible elements before the computation of text density, since these elements are meaningless for the end user but treated as non-tag text in the algorithm.

Intuitively, the meaningful content always occupies the center of the screen. Obviously, a web page can be parti-tioned into multiple segments or blocks, and usually the im-portance of those blocks on a page is not equivalent. There-fore, spatial information ( e.g. , position, size) will be added to the density measure to further enhance the performance.
Finally, in this paper, we focused on the comparison be-tween the content extraction methods based on statistical information of web pages. VIPS, on the other hand, is a vi-sual information based method which cannot be compared directly as it outputs a set of page segments rather than ex-http://webkit.org/ tracted text. It can be deduced that CETD performs better t han VIPS since CETR outperform VIPS in [27]. In a future study, we will do more experiments to verify this.
The authors sincerely thank Tim Weninger, the author of CETR, for the fruitful discussions and data access. We also thank Thomas Gottron, author of the CombineE frame-work, for some of the implementations used in this work; and the CleanEval team for providing a standard evaluation da-ta set. This work is funded by NSFC (Grant Nos.60873237 and 61003168), Beijing Natural Science Foundation (Grant No.4092037), Excellent Researcher Award Program and Ba-sic Research Foundation of Beijing Institute of Technology. [1] W3C document object model. Website, 2009. [2] B. Adelberg. Nodose X  X  tool for semi-automatically [3] S. Baluja. Browsing on small screens: recasting [4] Z. Bar-Yossef and S. Rajagopalan. Template detection [5] O. Buyukkokten, H. Garcia-Molina, and A. Paepcke. [6] D. Cai, S. Yu, J. Wen, and W. Ma. Extracting content [7] L. Chen, S. Ye, and X. Li. Template detection for [8] Y. Chen, P. Fankhauser, and H.-J. Zhang. Detecting [9] B. D. Davison. Recognizing nepotistic links on the [10] S. Debnath, P. Mitra, and C. L. Giles. Automatic [11] S. Debnath, P. Mitra, and C. L. Giles. Identifying [12] D. Fernandes, E. S. de Moura, B. Ribeiro-Neto, A. S. [13] A. Finn, N. Kushmerick, and B. Smyth. Fact or [14] D. Gibson, K. Punera, and A. Tomkins. The volume [15] T. Gottron. Combining content extraction heuristics: [16] T. Gottron. Content code blurring: A new approach [17] S. Gupta, G. Kaiser, and S. Stolfo. Extracting context [18] H. Kao, S. Lin, J. Ho, and M. Chen. Mining web [19] C. Kohlsch  X  utter, P. Fankhauser, and W. Nejdl. [20] N. Kushmerick. Learning to remove internet [21] S. Lin and J. Ho. Discovering informative content [22] C. Mantratzis, M. Orgun, and S. Cassidy. Separating [23] M. Marek, P. Pecina, , and M. Spousta. Web page [24] D. Pinto, M. Branstein, R. Coleman, W. B. Croft, [25] A. F. R. Rahman, H. Alam, and R. Hartono. Content [26] R. Song, H. Liu, J. Wen, and W. Ma. Learning block [27] T. Weninger, W. H. Hsu, and J. Han. Cetr -content [28] L. Yi, B. Liu, and X. Li. Eliminating noisy
