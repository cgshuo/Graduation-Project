 Dept. of Computer Science low-dimensional representation. Well-known examples of t his theme include principal component analysis, manifold learning algorithms and their many vari ants [1 X 4].
 a low-dimensional representation Z of the covariates X and predicting well the response variable Y and related techniques [5 X 8]. SDR seeks a low-dimensional Z which captures all the dependency between X and Y . This is ensured by requiring conditional independence amo ng the three vari-making strong assumptions about the distribution of X . Therefore, KDR has been found especially suitable for high-dimensional problems in machine learnin g and computer vision [8, 10, 11]. based on a specific parametric regression function. By explo iting the SDR and KDR frameworks, on the other hand, we can cast the unsupervised learning prob lem within a general nonparametric measures of independence.
 We refer to this approach as  X  X nsupervised kernel dimension ality reduction X  (UKDR). As we will some interesting analytical links of the UKDR approach to st ochastic neighbor embedding (SNE) and t -distributed SNE ( t -SNE) [14, 15].
 The paper is organized as follows. In Section 2, we review the SDR framework and discuss how directions in Section 5.
 Notation Random variables are denoted with upper-case characters su ch as X and Y . To refer to their specific values, if vectorial, we use bold lower-case s uch as x and x element of x . Matrices are in bold upper-case such as M . there have been a growing interest in computing measures of i ndependence in Reproducing Kernel the resulting independence measures attain minimum values when random variables are indepen-dent. These methods were originally developed in the contex t of independent component analy-selection, and dimensionality reduction [7, 8, 18 X 21].
 describe briefly kernel-based measures of (conditional) in dependence, focusing on how they are applied to supervised dimensionality reduction. 2.1 Kernel dimension reduction for supervised learning Y , provides side information about the covariates, X  X  X . In a basic version of this problem we seek a linear projection B  X  R D  X  M to project X from D-dimensional space to a M -dimensional subspace. We would like the low-dimensional coordinates Z = B  X  X to be as predictive about Y regressing Y .
 the subject of a large literature [22]. In particular, SDR se eks a projection B such that, conditional independence in RKHS spaces [7]. Concretely, w e map the two variables X and Y to the RKHS spaces F and G induced by two positive semidefinite kernels K and K C the conditional covariance operator C B The conditional covariance operator has an important prope rty: for any projection B , C B C operators as a contrast function to estimate B .
 Concretely, with N samples drawn from P ( X, Y ) , we compute the corresponding kernel matrices K where G matrix. It should be chosen such that when N  X  +  X  ,  X  B for kernel dimensionality reduction: it is clear from context, we use  X  J The optimization functional in eq. (3) is not the only way to i mplement the KDR idea. Indeed, another kernel-based measure of independence that can be op timized in the KDR context is the the cross-covariance operator C norm of C the empirical estimate of HSIC is given by (up to a multiplica tive constant): where K this independence measure to dimensionality reduction, we seek a projection B which maximizes  X  J
XY ( B  X  X, Y ) with X , proposition.
 Proposition 1. Let N  X  +  X  and  X  uniformly on the unit sphere. If  X  Therefore, under these conditions it is equivalent to minim ize  X  J  X  J
XY ( B  X  X , Y ) Proof The proof is sketched in the supplementary material. Note th at assuming the norm of X is the overall scale.
 We note that while the two measures are asymptotically equiv alent, they have different computa-tional complexity X  X omputing  X  J trices.
 The HSIC measure  X  J the angles between (vectorized) kernel matrices K Trace [ K by assigning cluster labels Y so that the two kernel matrices are maximally aligned. The HS IC measure has also been used for similar tasks [18]. While both  X  J unsupervised dimensionality reduction, which is the direc tion that we pursue here. In unsupervised dimensionality reduction, the low-dimens ional representation Z can be viewed as neural nets.
 is a linear projection of X . We then consider nonlinear approaches. 3.1 Linear unsupervised kernel dimension reduction is a copy of X and Z = B  X  X  X  R M is the low-dimensional representation of X . Following the framework of SDR and KDR in section 2, we seek B such that X  X  X  X   X  X | B  X  X . Such B  X  X thus captures all information in X in order to construct itself (i.e.,  X  X ).
 the following kernel-based measure of independence where G alternatively maximize the corresponding HSIC measure of d ependence between B  X  X and X mized or maximized. 3.2 Nonlinear unsupervised kernel dimension reduction UKDR. The main idea is to find a linear subspace embedding of no nlinearly transformed X . Let  X  J ( B  X  h ( X ) , X ) .
 Radial Basis Network (RBN). In the spirit of neural network autoencoder, one obvious cho ice of samples from X . For a sample x where x Random Sparse Feature (RSF). In this approach we draw D  X  H elements of W from a multi-element of h ( X ) as where w of h RSF ( X ) , a property that can be computationally advantageous.
 Our choice of random matrix W is motivated by earlier work in neural networks with infinite num-26]. In particular, in the limit of H  X  +  X  , the transformed X induces an RKHS space with the arccos kernel: h RSF ( u )  X  h RSF ( v ) = 1  X  1 / X  cos  X  1 ( u  X  v / k u kk v k ) [26]. Nonparametric. We have also experimented with a setup where Z is not constrained to any para-with the solutions from the other nonlinear methods, the fina l solution is generally better. 3.3 Choice of kernels choice is a universal kernel, in particular the Gaussian ker nel: K B Random walk kernel over X . Given N observations, { x transformed x walk from x The matrix P with elements of p theless, a simple transformation K the values of p the similarity between x Cauchy kernel for B  X  X . A Cauchy kernel is a positive semidefinite kernel and is given by We define K Gaussian kernel in the transformed space  X  ( B  X  X ) such that  X  ( x Supplementary Material. 3.4 Numerical optimization sure. The techniques constrain the projection matrix B to lie on the Grassman-Stiefel manifold (d) are 1D embeddings by UKDR. They differ in terms of how the e mbeddings are constrained (see UKDR makes fewer mistakes in (c) and no mistakes in (d).
 B  X  B = I [28]. While the optimization is nonconvex, our optimizatio n algorithm works quite well in practice.
 trix needs to be computed. Standard tricks X  X uch as chunking  X  X or handling large kernel matrices computing the search direction from the gradient needs a QR d ecomposition which depends cu-quadratic on D and linearly on M , the dimensionality of the low-dimensional space. One simp le strategy is to use PCA as a preprocessing step to obtain a mode rate D . We compare the performance of our proposed methods for unsup ervised kernel dimension reduction section, we have used the independence measure  X  J 4.1 Synthetic example Fig. 1(a). We use t -SNE and our proposed method to yield 1D embeddings of these d ata points, correspond to the inner ring.
 corresponding to the  X  X onparametric embedding X  presented in section 3. 4.2 Images of handwritten digits Our second data set is a set of 2007 images of USPS handwritten digits [20]. Each image has 256 3 and 5 are often indistinguishable from each other. We refer to this dataset as  X  X SPS-500. X  USPS-500. Fig. 2 displays a 2D embedding of the 500 images. The colors en code digit categories The second row was generated with our UKDR method with Gaussi an kernels for both the low-dimensional coordinates Z and X . The difference between the three embeddings is whether Z is constrained as a linear projection of the original X (linear UKDR), an RBN-transformed X (RBN UKDR), or a Random Sparse Feature transform of X (RSF UKDR). The Gaussian kernel band-UKDR yields reasonably good clusters of the data, RBN UKDR an d RSF UKDR yield significantly improved clusterings. Indeed, the quality of the embedding s is on par with that of t -SNE. In the third row of the figure, the embedding Z is constrained to be RSF UKDR. However, instead one, outperforming t -SNE by significantly increasing the gap of digit 1 and 4 from t he others. row. Embeddings by UKDR are shown in the bottom two panels.
 options for handling high-dimensional data for nonlinear U KDR.
 USPS-2007: visualization and classification. In Fig. 3, we compare the embeddings of t -SNE and unsupervised KDR on the full USPS 2007 data set. The data set h as many easily confusable pairs UKDR framework, using an RBN transformation to parameteriz e the embedding performs slightly better than using the RSF transformation. techniques.
 outperforms both t -SNE and PCA significantly. As the dimensionality goes up, t -SNE starts to our method is highly desirable when the target dimensionali ty is very much constrained. Figure 3: Embeddings of the USPS-2007 data set by our nonline ar UKDR approach and by t -SNE. classifiers suggests that the embedding by nonlinear UKDR is of higher quality. by optimizing independence measures computed in a reproduc ing kernel Hilbert space. We study show that our method yield meaningful and appealing cluster ing patterns of data. When used for classification, it also leads to significantly lower misclas sification.
 This work is partially supported by NSF Grant IIS-0957742 an d DARPA N10AP20019. F.S. also Scholar Program.

