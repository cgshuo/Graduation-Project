 Novelty detection (ND) or one-class classi fication involves learning data descrip-tion of normal data to build a model that can detect any divergence from nor-mality [9]. Data description can be used f or outlier detectio n to detect abnormal samples from a data set. Data description is also used for a classification problem where one class is well sampled while other classes are severely undersampled. In real-world applications, collecting the normal data is cheap and easy while the abnormal data is expensive and is not available in several situations [14]. For instance, in case of machine fault detection, the normal data under the nor-mal operation is easy to obtain while in faulty situation the machine is required to devastate completely. Therefore one-class classification is more difficult than conventional two-class classification because the decision boundary of one-class classification is mainly constructed from samples of only the normal class and hence it is hard to decide how strict decision boundary should be. ND is widely applied to many application domains such as network intrusion, currency valida-tion, user verification in computer systems, medical diagnosis [3], and machine fault detection [16].

There are two main approaches to solving the data description problem which are density estimation approach [1][2][12] and kernel based approach [13][14][20]. In density estimation approach, the task of data description is solved by esti-mating a probability density of a data set [11]. This approach requires a large number of training samples for estimation, in practice the training data is not insufficient and hence does not represent the complete density distribution. The estimation will mainly focus on modeling the high density areas and can result in a bad data description [14]. Kernel-based approach aims at determining the boundaries of the training set rather than at estimating the probability density. The training data is mapped from the input space into a higher dimensional feature space via a kernel function. Support Vector Machine (SVM) is one of the well-known kernel-based methods which constructs an optimal hyperplane between two classes by focusing on the training samples close to the edge of the class descriptors [17]. These training samples are called support vectors. In One-Class Support Vector Machine (OCSVM), a hyperplane is determined to separate the normal data such that the margin between the hyperplane and outliers is maximized [13]. Support Vector Data Description (SVDD) is a new SVM learning method for one-class classification [14]. A hyperspherically shaped boundary around the normal data set is constructed to separate this set from abnormal data. The volume of this data description is minimized to reduce the chance of accepting abnormal data. S VDD has been proven as one of the best methods for one-class classification problems [19].

Some extensions to SVDD have been proposed to improve the margins of the hyperspherically shaped boundary. The first extension is Small Sphere and Large Margin (SSLM) [20] which proposes to surround the normal data in this optimal hypersphere such that the margin X  X istance from outliers to the hypersphere, is maximized. This SSLM approach is helpful for parameter selection and provides very good detection results on a numbe r of real data sets. We have recently proposed a further extension to SSLM which is called Small Sphere and Two Large Margins (SS2LM) [7]. This SS2LM aims at maximising the margin between the surface of the hypersphere and abnormal data and the margin between that surface and the normal data while the volume of this data description is being minimised.

Other extensions to SVDD regarding data distribution have also been pro-posed. The first extension is to apply SVDD to multi-class classification problems [5]. Several class-specific hyperspheres that each encloses all data samples from one class but excludes all data samples fro m other classes. The second extension is for one-class classification which proposes to use a number of hyperspheres to decribe the normal data set [19]. Normal data samples may have some dis-tinctive distributions so they will locate in different regions in the feature space and hence if the single hypersphere in SV DD is used to enclose all normal data, it will also enclose abnormal data samples resulting a high false positive error rate. However this work was not presen ted in detail, the proposed method is heuristic and there is no proof provided to show that the multi-sphere approach can provide a better data description.

We propose in this paper a new and more detailed multi-hypersphere ap-proach to SVDD. A set of hyperspheres is proposed to describe the normal data set assuming that normal data samples have distinctive data distributions. We formulate the optimisation problem for multi-sphere SVDD and prove how SVDD parameters are obtained through solving this problem. An iterative al-gorithm is also proposed for building data descriptors, and we also prove that the classification error will be reduced after each iteration. Experimental re-sults on 28 well-known data sets show that the proposed method provides lower classification error rates comparing with the standard single-sphere SVDD. Let x i , i =1 ,...,p be normal data points with label y i =+1and x i , i = p + 1 ,...,n be abnormal data points (outliers) with label y i =  X  1. SVDD [14] aims at determining an optimal hypersphere to include all normal data points while abnormal data points are outside this hypersphere. The optimisation problem is as follows subject to is vector of slack variables,  X  ( . ) is a kernel function, and c is centre of the hypersphere.

For classifying an unknown data point x , the following decision function is f ( x ) = +1 or abnormal if f ( x )=  X  1. 3.1 Problem Formulation Consider a set of m hyperspheres S j ( c j ,R j )with center c j and radius R j , j = 1 ,...,m . This hypershere set is a good data description of the normal data set data set and the sum of all radii m j =1 R 2 j should be minimised. the membership representing degree of belonging of data point x i to hypersphere S . The optimisation problem of multi-sphere SVDD can be formulated as follows subject to such that u i 0 j 0 =1and u i 0 j =0, j = j 0 .

Minimising the function in (3) over variables R , c and  X  subject to (4) will determine radii and centres of hyperspheres and slack variables if the matrix U is given. On the other hand, the matrix U will be determined if radii and centres of hyperspheres are given. Therefore an iterative algorithm will be applied to find the complete solution. The algorithm consists of two alternative steps: 1) Calculate radii and centres of hyperspheres and slack variables, and 2) Calculate membership U .

We present in the next sections the iterative algorithm and prove that the clas-sification error in the current iteration will be smaller than that in the previous iteration.

Forclassifyingadatapoint x , the following decision function is used The unknown data point x is normal if f ( x ) = +1 or abnormal if f ( x )=  X  1. This decision function implies that the mapping of a normal data point has to be in one of the hyperspheres and that the mapping of an abnormal data point has to be outside all of those hyperspheres. The following theorem is used to consider the relation of slack variables to data points classified. Theorem 1. Assume that ( R, c,  X  ) is a solution of the optimisation problem in (3), x i , i  X  X  1 , 2 ,...,n } is the i-th data point. Proof. From (4) we have  X  i =max 0 , ||  X  ( x i )  X  c k || 2  X  R 2 k ,if x i is normal, and  X  ij =max 0 ,R 2 j  X  X |  X  ( x i )  X  c j || 2 ,if x i is abnormal. The following empirical error can be defined for a data point x i :
Referring to Theorem 1, it is easy to prove that n i =1  X  i is an upper bound of i =1 error ( i ). 3.2 Calculating Radii, Centres and Slack Variables The Lagrange function for the optimisation problem in in (3) subject to (4) is as follows where s ( i )  X  X  1 ,...,m } is index of the hypersphere to which data point x i belongs and satisfies u is ( i ) =1and u ij =0  X  j = s ( i ).

Setting derivatives of L ( R, c,  X ,  X ,  X  )with respect to primal variables to 0, we obtain
To get the dual form, we substitute (8)-(15) to the Lagrange function in (7) and obtain the following:
The result in (16) shows that the optimisation problem in (3) is equivalent to m individual optimisation problems as follows subject to
After solving all of these individual optimization problems, we can calculate the updating radii R =[ R j ] and centres c =[ c j ], j =1 ,...,m using the equations in SVDD. 3.3 Calculating Membership U We use radii and centres of hyperspheres to update the membership matrix U . The following algorithm is proposed: 3.4 Iterative Learning Process The proposed iterative learning process for multi-sphere SVDD will run two al-ternative steps until a convergence is reached as follows
We can prove that the classification error in the current iteration will be smaller than that in the previous iteration through the following key theorem. Theorem 2. Let ( R , c ,  X  , U )and( R , c ,  X  , U ) be solutions at the previous iteration and current iteration, respectively. The following inequality holds Proof. We prove that ( R, c,  X , U ) is a feasible solution at current iteration. Case 1 : x i is normal and misclassified. Hence (21) is reasonable due to ( R, c,  X , U ) is solution at the previous step. Case 2 : x i is normal and correctly classified.
 Case 3 : x i is abnormal.
 It is seen that results in our conclusion. We performed our experiments on 28 we ll-known data sets related to machine fault detection and bioinformatics. These data sets were originally balanced data sets and some of them contain several classes. For each data set, we picked up a class at a time and divided the data set of this class into two equal subsets. One subset was used as training set and the other one with data sets of other classes were used for testing. We repeated dividing a data set ten times and calculated the average classification rates. We also compared our multi-sphere SVDD method with SVDD and OCSVM. The classification rate acc is measured as [6] where acc + and acc  X  are the classification accuracy on normal and abnormal data, respectively.

The popular RBF kernel function K ( x, x )= e  X   X  || x  X  x || 2 was used in our ex-For SVDD and multi-sphere SVDD, the trade-off parameter C 1 was searched the ratio C 2 /C 1 belonged to For OCSVM, the parameter  X  was searched in { 0 . 1 k : k =1 ,..., 9 } . For multi-sphere SVDD, the number of hyperspheres was changed from 1 to 10 and 50 iterations were applied to each training.
 Table 2 presents classification results for OCSVM, SVDD, and multi-sphere SVDD (MS-SVDD). Those results over 28 data sets show that MS-SVDD always performs better than SVDD. The reason is that SVDD is regarded as a special case of MS-SVDD when the number of hyperspheres is 1. MS-SVDD provides the highest accuracies for data sets e xcept for Colon cancer and Biomed data sets. For some cases, MS-SVDD obtains the same result as SVDD. This could be explained as only one distribution for those data sets. Our new model seems to attain the major improvement for the larger data sets. It is quite obvious since the large data sets could have different distributions and can be described by different hyperspheres. We have proposed a new multiple hypersphere approach to solving one-class classification problem using support vector data description. A data set is de-scribed by a set of hyperspheres. This is an incremental learning process and we can prove theoretically that the error rate obtained in current iteration is less than that in previous iteration. We have made comparison of our proposed method with support vector data description and one-class support vector ma-chine. Experimental results have shown that our proposed method provided better performance than those two methods over 28 well-known data sets.
