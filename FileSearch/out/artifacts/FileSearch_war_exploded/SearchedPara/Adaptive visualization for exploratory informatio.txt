 1. Introduction
Today X  X  information environment is getting more and more complicated each day. How to locate relevant information in this complexity and to distill valuable information is one of the biggest challenges. In the past, the most successful informa-tion access tools were directory services and web-based search engines. Despite their success, they have limitations to deal with different user search conditions and the problem of users X  lack of the knowledge or contextual awareness to formulate queries or navigate complex information spaces ( Bates, 1989; White, Kules, &amp; Bederson, 2005, 2006 ).

One of the promising ideas that can address this problem is personalized search ( Micarelli, Gasparetti, Sciarrone, &amp; Gauch, 2007; Pitkow et al., 2002 ). Unlike traditional search, it tries to avoid the one-size-fits-all strategy and understand user differences  X  different user interests or different contexts  X  and then provides more customized results to meet specific user interests under different contexts. In order to accomplish this goal, personalized search systems usually adopt user models,
Micarelli, 2007 ). Utilizing user models, dynamic contexts can be integrated into personalized retrieval models.  X 
Over the last 20 years, the concept of personalized search has been explored in a number of projects. Multiple studies have demonstrated the benefits of this technology. However, the problem is that the majority of them focused on the classic low-interactive search model: a search engine receives a query from users and then returns the most relevant document sur-rogates in a ranked list. In this case, the user interaction with the system is limited to issuing a query and examining the systems and personalized systems. Currently a low-interactive approach falls behind the state-of-the-art research in both fields. For more than a decade, a volume of research in the field of information retrieval was devoted to an alternative ap-proach to handle the growing complexity of search tasks known as exploratory search systems ( Marchionini, 2006 ). Explor-atory search systems focused on supporting more complicated search tasks in case where traditional search systems were less than adequate. Exploratory search stresses the importance of highly interactive user interfaces, so that users can learn and understand their problems more thoroughly while actively interacting with the system. Going beyond the simple look-up search activities can produce better results.

As these approaches explored by personalized and exploratory search systems are not contradictory, we believe that they can re-enforce each other. We argue that the effectiveness of personalized search systems may be increased by allowing users to interact with the system and learn/investigate the problem in order to reach the final goal. The value of combining positive sides of personalization and interactivity has already been demonstrated in some other types of personalized sys-tems. While classic personalized systems focused on sophisticated approaches to modeling user interests, goals, and other features by observing a relatively low flow of user actions, the modern trend is to engage users of adaptive systems in a ri-cher interaction and to use the increased diversity and volume of user activities for more reliable user modeling. One of the earliest examples of this trend was the MetadDoc system ( Boyle &amp; Encarnacion, 1994 ). MetaDoc replaced classic one-shot page content adaptation with so-called adaptive stretchtext that allowed users to expand or collapse content fragments. This increased the volume of information that the system received from the user and allowed MetaDoc to build more reliable user models and offer better personalization.

The challenge that we address in this paper is how to develop personalized interactive search systems that can incor-porate recent developments from both contributing fields. On one hand, we want to develop systems that can offer use-ful personalization for modern interactive search approaches such as used in many exploratory search systems. On the other hand, we want to focus on creating user interactions that can improve the opportunities for more reliable user modeling.

In our previous work, we have investigated several ideas for this personalized interactive search. Our systems allowed users to have more control over the personalization process and we explored options to help users view and manipulate the user models ( Ahn, Brusilovsky, Grady, He, &amp; Syn, 2007 ) as well as to control the impact of the user models on search because they were still based on a traditional static ranked list approach that offered little support for exploratory search tasks. In this paper, we extend the previous attempts by exploring personalization approaches for a more interactive search scenario. A relevance-based visualization framework called VIBE (Visual Information Browsing Environment) ( Olsen, Korfh-age, Sochats, Spring, &amp; Williams, 1993 ) was chosen as an example of a highly-interactive search approach.
VIBE makes use of reference points called POIs (Points of Interest) and can position documents according to their simi-larity ratios to the POIs. To introduce a search-focused personalization to VIBE, we defined user queries and user model key-words as POIs and provided means to organize them spatially in the visualization. Users can explore this personalized visual space easily and search for relevant information within it. The resulting personalized visual search system was called Adap-tive VIBE. By combining personalization and visualization-based exploratory search, we expect Adaptive VIBE could solve the problems in traditional non-personalized and static information retrieval and the complexity issue of personalized search.
We conducted a user study in order to prove the value of Adaptive VIBE and to investigate its properties to facilitate future improvements. This paper introduces the details about the Adaptive VIBE system features and discusses the user study re-sults. The next section explores related studies regarding visual exploration for information retrieval and adaptive visuali-zation. Section 3 introduces the Adaptive VIBE system and the implementation details. Sections 4 and 5 explain the design and the result of the user study. Section 6 summarizes and discusses the implications of the results. The last section concludes this paper and presents future plans. 2. Related studies
We propose a technology that combines the knowledge from several fields, particularly these three broad categories: (1) information retrieval supported by exploratory visualization, (2) personalized search, and (3) adaptive visualization. 2.1. Visual exploration for information retrieval
Exploratory search is a non-static information retrieval approach that focuses on users X  ability to control, learn, and dis-cover information during the interaction with the system. It emphasizes on iterative user interfaces and understands the information retrieval process as a learning or investigatory process rather than a simple lookup search ( Marchionini, 2006 ). The target space and the nature of the problem of exploratory search is uncertain ( White et al., 2005 ), so every exploratory search system has an interactive user interface as the core component in order to implement the iterative explo-ration through the uncertain search space.

Each system reveals different visualization strategy. The systems such as Flamenco ( Hearst et al., 2002 ), Relation Browser traditional lists, grids, tables, etc. Other approaches more actively utilize visualization methods for stronger presentations.
Even before the exploratory search concept was established, several visualization algorithms were devised for search result presentation. FeatureLens ( Don et al., 2007 ) visualized the inner structure of a huge text and help users mine meaningful context, which was not easily comprehensible. Several approaches attempted to map the search results on a two-dimen-1991; Rivadeneira &amp; Bederson, 2003 ).
 There are approaches that emphasize the visual representation and the exploratory search mechanism at the same time.
For example, TileBars ( Hearst, 1995 ) introduced graphical snippets that showed the distribution of keywords along with the
Liu, 2008 ) integrated text analysis techniques and visualization for search result analysis. 2.2. Personalized search
Personalized search is defined as an attempt to provide individualized collections of documents to the users, based on some form of models representing their needs and the context of their activities ( Micarelli et al., 2007 ). The personalized search results are tailored to the preferences, tastes, backgrounds and knowledge of the user who expressed them in the form of queries.
 Pitkow et al. (2002) described two methods for personalized searching: (1) query augmentation and (2) result processing.
In the query augmentation stage, the user query is extended by the system considering the context of the user search. After the search engine retrieves documents based on this augmented query, the search result is examined and modified again to better reflect user context. Pretschner and Gauch (1999) and Micarelli et al. (2007) provided similar schemes: (1) re-ranking, (2) filtering, and (3) query expansion.

Personalized search systems can realize these techniques based on the core component called  X  X  X ser models. X  X  A user mod-el is a representation of information about an individual user that is essential to the adaptive system to provide the adap-movements as inputs for implicit feedback. Loboda, Brusilovsky, and Brunstein (2011) inferred word relevance from eye-movements of readers too. In addition to the feedback method, personalized search systems can be classified according to the number of involved people: individual or collective ( Hearst, 2009 ).

The user models can cover different temporal user interests. Long-term user models represent users X  global or general interests whereas short-term user models represent more specific and shorter time frames of user interests. Some user mod-els, which take care of a very short-term task and try to accumulate information about a specific user task, are called task models ( Vassileva, 1996 ).

Many personalized search engines adopted the re-ranking or the filtering method ( Arezki, Poncelet, Dray, &amp; Pearson,
Sugiyama, Hatano, &amp; Yoshikawa, 2004; Teevan, Dumais, &amp; Horvitz, 2005, 2008 ) while query expansion or augmentation is 2002; Shen et al., 2005 ).

Personalized searchtechniqueshavebeenappliedto web-based commercial searchenginesas well. Google hastriedseveral personalized searching approaches by re-ranking search results according to users X  explicit feedback ( Kamvar &amp; Mayer, 2007;
Miller, 2011 ). Jeh and Widom (2003) suggested a scalable algorithm to personalize graph-based searching such as PageRank, so that the personalization can be applied to the web-scale. Agichtein, Brill, and Dumais (2006) evaluated web-scale personalized search reranking results and found a significant search accuracy increase. Microsoft Bing also added personalized search fea-media encouragedthe major searchenginesto embrace group-based personalization. Facebook introduceda feature called  X  X  X n-stant Personalization X  X  1 that can tailor search results based on the preferences of individual users or their friends. 2.3. Adaptive visualization
Adaptive visualization is an attempt to improve visualization by incorporating adaptation, i.e., the ability to change visu-alization depending on various user features that can be explicitly provided or inferred from the trace of user actions.
Through adaptation, users can modify the way in which the system visualizes a collection of elements (or documents) ( Rous-posed. They can be categorized into four groups: (1) visualization method adaptation, (2) visual structure adaptation, (3) adaptive annotations, and (4) user model visualization.

The first visualization method adaptation group prepares multiple visualization methods and provides them selectively according to different user characteristics. ERST (External Representation Selection Tutor) provided a selection of informa-tion display formats (plot chart, table, pie chart, sector graph, bar chart, Euler diagram) mapped to users background knowl-edge of external representations (KER) and task types ( Grawemeyer &amp; Cox, 2005 ).

The visual structure adaptation methods adapt the structures either by varying the visualization layouts or by providing easy exploration methods. CVI and RF-Cones ( Teraoka &amp; Maruyama, 1997 ) tried to help users to navigate the problem space with dynamically changing view points and similarity-based layouts. WIVI ( Lehmann, Schwanecke, &amp; D X rner, 2010 ) pro-vided an adaptive navigation system for Wikipedia articles. Opinion Space ( Bitton, 2009 ) let users easily see where their opinions were located among high-dimensional survey attributes. Roussinov and Ramsey X  X  (1998) multi-level SOM (Self
Organizing Map) helped users to explore multi-level maps that were adaptively regenerated following users X  exploration commands.

Using the visual elements such as colors or icons, some adaptive annotation approaches focused more on a specific part of visualizations. ADVISE (ADaptive VIsualization for Education) 2D ( Brusilovsky, Ahn, Dumitriu, &amp; Yudelson, 2006 ) imple-mented this approach based on the well-known Force Directed Placement (FDP) visualization. QuizVIBE ( Ahn, Brusilovsky, VIBE visualization, where the C language quizzes were displayed according to their similarities to the C language concepts.
Gansner, Hu, Kobourov, and Volinsky (2009) used the FDP visualization to adaptively visualize TV programs with color-based and social annotation of educational content by adapting foreground/background colors of icons and cells. Lighthouse ( Leu-ski &amp; Allan, 2004 ) introduced an interesting adaptive search visualization mechanism. The estimated relevancy calculated through user feedback was marked on the document icons and textual titles using different colors and lengths of the col-ored-shades.

The last group attempts to show the contents of the user models to the users and even let them edit the user models, so that the users could control the user model contents. YourNews ( Ahn et al., 2007 ) explored an on-line news filtering system that was equipped with a user model viewer/editor. TaskSieve ( Ahn et al., 2008 ) continued to examine the potential of open user models but it focused more on the query and the user model fusion interface, rather than the keyword level user model exploration. IntrospectiveView ( Bakalov, K X nig-Ries, Nauerz, &amp; Welsch, 2010 ) visualized concepts in ontologies in a circle and used different levels of colors and font sizes according to user interests. MyExperiences ( Kump, Seifert, Beham, Linds-taedt, &amp; Ley, 2010 ) visualized the open learner model (OLM) in order to permit the adaptive learning system users to see and construct their user models. The learner model was represented as a tree structure using the Treemap algorithm.
Despite the diversity of these adaptive visualization strategies, most of them still remain static. They lack the interactivity to explore the objects that helps users X  learning and understanding of complicated information needs. The lack of interactiv-ity can be problematic in adaptive systems because it can make harder to understand the hidden links between the adap-tation components (e.g. documents and query) ( Golovchinsky, 2009 ). In comparison, Adaptive VIBE supports high-level interactivity for personalized search through adaptive exploratory visualization. The following section introduces its key idea and features. 3. Adaptive VIBE system
We introduce an adaptive visualization based information retrieval system called Adaptive VIBE. It combines exploratory search, personalized search, and adaptive visualization. It is based on Olsen et al. X  X  (1993) VIBE visualization algorithm. VIBE is a reference point-based spatial visualization, which shows the target objects according to their similarities to special ref-erence points called POIs. Usually, they represent important keywords or concepts for search visualization, like the land-marks on a geographic map represent important places. Adaptive VIBE was extended to add personalization to the original VIBE system. This section introduces the original VIBE and describes how we extended it to implement Adaptive
VIBE. Adaptive VIBE replaced ranked lists of a text-based personalized information retrieval system called TaskSieve (Sec-tion 4.2 ), which was built for our previous study ( Ahn et al., 2008 ) and which acted as a baseline system in this paper.
The detailed list of Adaptive VIBE visualization features and their advantages are presented at the end of the section. 3.1. The original VIBE
VIBE ( Olsen et al., 1993 ) visualizes the target objects (e.g. documents) according to their similarity ratios to the reference points (e.g. query keywords) called POIs. If a document is more similar to POI P
P , and the closeness is determined by the document-to-POI similarity ratio. For example, if a document has similarities of the way from P b on the line connecting those two POIs, because it is twice as similar to P (1993) for more details.

Users can drag POIs anywhere they want on the screen and the document locations are updated keeping their similarity ratios to the POIs. That is, they can see that the similar documents are following the movement of the POIs. The more similar the documents are, the more they follow the POIs. Therefore, users can easily discover which documents are more similar to a certain POI by their locations and can also learn the degree of similarity by the documents X  moving distances.
It can display many more documents than the ranked-lists. In contrast to the usual web search engine ranked lists that usually show only 10 X 20 documents per page, VIBE can show hundreds of documents at the same time on a single screen without scrolling. Users can easily locate related documents to a specific POI without being lost in the middle of hundreds of documents, by examining those documents closer to the POI. 3.2. Adaptive VIBE  X  idea
Fig. 1 is a screenshot of Adaptive VIBE. The prototype in Fig. 1 shares the internal personalized search engine with the text-based personalized search system TaskSieve. However, it utilizes the VIBE visualization instead of the textual ranked list. In the current implementation, users can switch between the two systems using a tab interface.

The idea behind the VIBE adaptation is straightforward  X  add the user model POIs in addition to the query POIs and then visually separate those two groups using special layouts . The queries and user models are core components commonly found in personalized search systems. Usually, text-based information retrieval systems display the results in a ranked list format, with the query highlighted in titles and summaries. This method works well with static search systems but can be problem-atic with personalized search systems due to the following reasons: 1. If the number of the keywords to be highlighted increases, simple snippets cannot display them at once and will produce 2. It is hard to show the document-keyword relationships if the number of keywords increases. Users can read 2 X 3 high-
In order to address these problems, Adaptive VIBE provides a method to represent the keywords on a single visualiza-tion plane and separates them spatially. It defines the user model and the query keywords as two POI groups, and the retrieved documents as the objects to be allocated according to the similarities to the POIs. By separating the user queries and the user model spatially, it can separate the target document space as well. The documents that are more similar to the user model will be located closer to the user model POIs and the ones more similar to the queries will be located clo-ser to the query POIs.

Fig. 2 is an example of the Adaptive VIBE visualization. In this example, a query were a traditional search system, it would return a mixture of documents about various nuclear weapon topics. For example, anti-nuclear weapon activities, Iranian nuclear weapon development program, Russia-U.S. nuclear weapon cut issues, etc.
However, by tracking the current users X  past activities and interests, Adaptive VIBE constructed a user model that contains the terms korea , janap , north , torpedo , and shell , which suggests that the user was specifically interested in the North Korean nuclear weapon issues.

Given this information, a documents set that includes a significant number of North Korean nuclear weapon issues are retrieved, using the user query ( nuclearweapon ) and the user model ( the tracked user interest better than non-personalized searching, it still contains the general nuclear weapon documents because the query and the user model terms were OR X  X d. Adaptive VIBE can help users easily locate the documents more similar to their user models. In Fig. 2 , two components  X  the query and the user model  X  are displayed as POIs (yellow and blue discs respectively) and the retrieved documents are displayed as squares. The query and user model POIs are separated into two groups painted in different colors (yellow and blue). The similarities between the documents and the keywords are easily seen by examining their spatial proximity to those POIs. There are some documents describing the general nuclear weapon topic (e.g. the squares on closer to the yellow POIs) but we also note that some documents focus on a very specific event about North Korean nuclear weapons by examining the document locations around the
POI. The effect of north can be noticed from the documents that are placed lower right from the though its effect is smaller.

This example clearly shows the advantage of Adaptive VIBE. It adds context to the traditional information retrieval visu-alization. Non-adaptive systems cannot distinguish the current context of the search task  X  North Korean nuclear weapon development program  X  from the general nuclear weapon issue. Without the context of the current user X  X  interests, the sys-tem might have mixed other nuclear weapons issues (e.g. Iranian nuclear weapon) and the current user X  X  interest. By visu-alizing the user model (interests in North Korean nuclear weapons), it could visualize relevant documents more clearly that are closer in context to the user interests.

In Adaptive VIBE, this advantage was achieved by incorporating user models as POIs and spatially separating it from the other POIs (i.e. query POIs). Traditional VIBE systems usually placed the POIs equivalently in a circle. However, by spatially separating two POI groups using the non-circular layouts as in the previous example, we could also expect the separation of documents according to their contexts. Therefore, we added two more layouts to the circular layout of POIs, named as Hemi-sphere and Parallel. They are compared in Fig. 3 .
 3.3. Adaptive VIBE  X  implementation
The Adaptive VIBE visualization was implemented as a Java applet and runs within web browsers. The VIBE visualization occupies the largest space within the system ( Fig. 1 ), so that it can display as many documents as possible and allow users to explore and analyze them.

Users begin the initial search by entering their queries according to their tasks to be accomplished. Initially, this step is identical to the conventional non-personalized searching. However, users can annotate and collect some relevant passages from the document snippets or from the fulltexts by drag-selecting the passages and clicking the  X  X  X elevant X  X  button. Then the passages are saved to a special area labeled as  X  X  X ask Model Notes X  X  or a shoebox. It is a place where users can easily store clippings of short text fragments considered to be important and acts as a user feedback mechanism for personalization.
The system observes the shoebox, instantly analyzes its content, and re-constructs the task model. The task model is dis-played to the users in the visualization as well as in the term-cloud format. This process is done on the fly whenever the shoebox is updated.

As mentioned earlier, Adaptive VIBE replaces the textual ranked-lists with adaptive visualization. The underlying infor-mation retrieval and personalized ranking algorithms are identical between the two systems but Adaptive VIBE has more robust user interface to better support the exploratory search mechanism. 1. The HTML-based ranked-lists were replaced by the Adaptive VIBE Java applet. It means that the list of documents are 2. Users can open the full-text from the visualization. 3. A list of documents can still be displayed in a separate panel, but they are activated from within the visualization.
Users can manipulate the visualization, interact with it, and then select one or multiple documents that may contain information relevant to their search. The document list is shown in the box below so that the user could do the next iteration of searching by annotating/saving the newly-found relevant passages. The existence of this textual document list is impor-tant because users do not want to use graphical representations alone for navigation ( Lai, Huang, Nguyen, &amp; Huang, 2007; Lehmann et al., 2010 ).

When the contents of the user model are updated by accumulating the relevant passages in the shoebox, the user model can be updated with a new set of user model POIs. This can be done automatically whenever the user model contents change or by manually clicking on a button. Users can decide the importance of the user model to their own query by choosing a tab from  X  X  X elevant to Query, X  X   X  X  X oth, X  X  or  X  X  X ask Model. X  X 
Fig. 4 illustrates the process described thus far. User queries and user models (except in the first round, where the user model is empty) work together to retrieve candidate documents. Adaptive VIBE visualizes the documents with the query and the user model information in the same space ( but separated spatially ) and let the user discover relevant documents. Users are expected to sort out relevant documents more easily and efficiently, thanks to the adaptively visualized document space (Section 3.2 ) and the interactive exploratory features (Section 3.3.2 ). They then extract the required information from the documents and store them in the shoebox. The change to the shoebox is reflected in the user model on the fly, which returns a new set of documents to be analyzed in the next iteration. 3.3.1. User modeling in Adaptive VIBE The process of how the user model was created in the Adaptive VIBE framework was described in the previous section.
User annotates text fragments from retrieved documents in the notebook. From the notebook, important keywords are se-lected and the user models are updated on the fly.

Adaptive VIBE was integrated into TaskSieve, so it shares the same user model keyword selection process. The keyword selection from the notebook utilizes a variation of the classic term frequency (TF). We assumed that the keywords appearing frequently in the notebook were more important than the less frequent keywords. We chose TF instead of TF-IDF because sometimes low IDF terms were important to represent more general aspects of user interests. Therefore, we defined a note frequency (NF) as follows.

All keywords included in the notebook except stopwords are first stemmed. They are then sorted by their NF scores and the top N = 300 keywords are selected to be used in the user models. The selected keywords build the user models. The user models are used in two ways: (1) to filter and rerank the initial search results and (2) to be displayed to the users (open user model).

As can be seen in Fig. 1 (upper right hand side box), the open user model shows the top N most frequent keywords in the notebook. The most frequent keywords are painted in bigger and bolder fonts. Along with this cloud-format open user mod-el, Adaptive VIBE displays the user model contents as POIs in the VIBE visualization, which is the core idea of Adaptive VIBE.
Users can distinguish the user model POIs from the query POIs by their locations in the Adaptive VIBE layouts (Hemisphere and Parallel) or by different colors (blue for user model POIs and yellow for query POIs). Users can organize the user model
POIs using the feature called POI Dock ( Fig. 6 ), where they can temporarily disable the POIs. Please see the next section for the details of this feature. 3.3.2. Interactive features Along with the core functionality of Adaptive VIBE, we added more features to empower the adaptive visualization in the
Adaptive VIBE environment. These core and additional interactive features of Adaptive VIBE were primarily intended to sup-port users in interacting and exploring the search space and to engage user intelligence.

Dynamic movement of documents following POIs . Even though the original VIBE supported moving POIs by mouse dragging, it could not show the in-between steps of the movements. It could only show them jumping from the starting point to the finishing point of a movement. Consequently, the document locations were not updated dynamically.
VIBE implementation, the POIs can be dragged naturally, so that the users can observe the dynamic movements of POIs (and the following documents). Fig. 5 illustrates the dynamic movements of POIs supported by our VIBE implementation.
POI Dock for visual user model manipulation . Sometimes, POIs need to be disabled for various reasons. Therefore, user visualization. It can be re-enabled when dragged from the dock. This POI-docking feature has se eral advantages. First, users can have control over the user model POIs recommended automatically by the system. If a user th nks that a POI does not precisely reflect her interest, she can simply disable it into the dock. The second advantage comes fr m the characteristics of the user models that can be defined separately according to the topics or the time range they cover Gauch et al. (2007) . The user model of Adaptive VIBE assumes one single task per each session and supports just one user model. Even though one big task can be sub-divided into several sub-tasks, it is overkill to define multiple user models for each of hem. At the same time, because the sub-tasks are correlated to each other under the umbrella of the parent task, it is not wis to simply divide them.
Therefore, instead of defining multiple user models, users can use the POI dock feature so that they c n disable some POIs that are not relevant to the current sub-task, but might be relevant to other sub-tasks.

Similarity overlay disc . The basic VIBE algorithm places documents closer to the related POIs and can help users under-stand the relatedness between the POIs and the documents. When it is unclear, they can move a POI and observe the doc-uments following the POI. The more the documents follow, the more they are related to the POI. However, users may want to get that information while they keep the initial positions of the POIs. At the same time, they may need to know the inverse relationship: that is, to discover more related POIs from a specific document. Fig. 7 shows a feature supporting this need. In
Fig. 7 a, the user moves the mouse cursor over the POI  X  X  X IVE X  X  and the red-to-blue spectrum colored discs are overlaid on top of the related documents. The size of the disks is proportional to the relatedness. Fig. 7 b shows the reverse usage  X  finding POIs from a document.

Document filter . Adaptive VIBE aims to overcome the limitations of ranked lists and displays hundreds of documents at the same time. Even though it increases the probability of recovering useful information, it can sometimes produce severe clutter on a small area as well. Therefore, we provided a simple mechanism to hide documents by adjusting the lower and the upper thresholds of POI-to-document similarities. A double slider widget was used for this purpose ( Fig. 8 ). It lets users decide the similarity range of documents to be displayed for a specific POI. For example, users can set the range as 0.8 X 1.0 for a POI  X  X  X ISASTER X  X  in order to display only the documents that are very similar to the keyword or concept of  X  X  X ISASTER. X  X 
Marquee selection . In our previous study ( Ahn &amp; Brusilovsky, 2009 ), we found that the relevant documents could be lo-cated geometrically closer to the user models in the Adaptive VIBE visualization, using simulations based on log data of a text-based personalized search study. Based on this finding, we added a spatial marquee tool in order to let users easily ac-cess relevant documents located in a specific position. In Fig. 9 (above), the current user was interested in the documents that might have information about the  X  X  X URSK X  X  and/or  X  X  X ISASTER X  X  and decided to examine the contents of the documents.
S/he therefore drew a rectangular region (displayed dim) and then the documents in that rectangle were automatically se-lected (squares in red borders). The list of selected documents was displayed ( Fig. 9 , below) and the user could read docu-ment summaries or open fulltexts by clicking the titles.

Visual relevance annotation on document icons . Even though our approach is purely a two-dimensional visualization, we can still make use of the ranks (or the relevance scores) of the documents, which are calculated using the query and the user models. This information was added to the Adaptive VIBE visualization in order to provide users with additional infor-mation about the top N documents estimated by the personalization engine. The blue squares shown in Fig. 1 are the top 10 documents in the retrieved set, which have the top 10 relevance scores calculated by the system.
 3.4. Advantages of Adaptive VIBE
The Adaptive VIBE system has several advantages over traditional ranked-list systems. By adopting the strong reference-based 2D document visualization method, it can show a much larger number of documents and fully exploits the screen space. It encodes important information about personalization as POIs, so that users can quickly locate information they search for. Below is the summary of expected advantages of Adaptive VIBE. 1. Using the 2D document visualization, it can show hundreds of documents without changing screens, which will make 2. Using the VIBE visualization scheme, it can show richer information regarding the keyword-to-document relatedness 3. By separating the query and the user model space in the visualization, it can help users instantly understand which doc-4. User study design In order to test whether Adaptive VIBE provides the expected advantages, we conducted an experiment-based user study.
We recruited 33 participants from the University of Pittsburgh and Carnegie Mellon University and asked them to perform tasks using both Adaptive VIBE and a non-visualized baseline system. We designed the systems to be used by experienced users rather than casual users. Therefore, we recruited the participants who met the following criteria. 1. Language issues  X  The participants should read large amounts of news stories, so native English speakers or those with 2. Information search ability  X  They should have strong information retrieval skills. We required the subjects to have 3. Education  X  In order to qualify as experienced information retrieval system users, we recruited participants enrolled in
The average age was 26.1 with 6.5 standard deviation and the average subjective confidence level in search was 8.09 out of 10. Twenty-seven were graduate students and seven were undergraduate students. The performance of two systems was compared using the log data of the systems and questionnaires asking subjective reactions of the participants. The log data recorded user actions and system outputs: user queries, retrieved results, user notes in the shoebox, user model content (keywords with weights), and user actions on the baseline and the experimental system user interface widgets. 4.1. Hypothesis
The objective of this study is to understand what advantages Adaptive VIBE could provide to users and to test the effec-tiveness of the adaptive visualization for information retrieval. Therefore, we defined the hypothesis as follows.
H: The adaptive visualization-based information retrieval framework will produce better results than the baseline text-
In order to test this hypothesis, we compared the performance of Adaptive VIBE and a baseline system. For the baseline, we selected a text-based personalized information retrieval system called TaskSieve ( Ahn et al., 2008 ). TaskSieve is intro-duced in the next section (Section 4.2 ).

We can compare the systems with regards to two aspects: (1) system performance and (2) user performance. The system performance is the ability of a system to provide relevant information to the users. It can be measured by observing the out-with the help of each system. In order to measure the performance, we adopted two metrics: (1) precision and (2) diversity.
Precision means the ratio of relevant documents in the retrieved documents. Diversity is defined as an ability to find more diverse information that may not be covered by others. Therefore, the hypothesis above is specified as the following two sub-hypotheses.

H-1: The Adaptive VIBE (experimental) system will result in better system performance than TaskSieve (baseline), in terms
H-2: The Adaptive VIBE system will guide the users better in terms of user performance than TaskSieve, in terms of precision
We can define several measures that represent the precision and the diversity. They are presented in Table 1 . Marquee selection precision and Top-10 document precision represent the document precision of marquee selected documents (Sec-system performance measures. Open document precision and note precision represent the precision of documents opened by users and the notes annotated by users respectively. The opened documents and user notes depend more strongly on user actions and they were classified as user performance.

Navigation depth forms a foundation of diversity. If a user navigates deeper in the low ranked documents, the chance to locate more diverse set of relevant documents will increase. We analyzed how many diverse sub-topics were discovered by observing the user opened documents and the user annotations. Lastly, we calculated the productivity by counting the high quality notes made by users. Detailed formal definitions are presented in Sections 5.3.1, 5.5.1 and 5.6.1 . 4.2. TaskSieve  X  the baseline system
We used a system called TaskSieve ( Fig. 10 ) as a baseline for comparison with Adaptive VIBE. It is a text-based person-alized information retrieval system with a visual user model. It allows users to mediate between different methods for mix-ing the effects of the user query and the user model. Users can select one of three ranking methods that consider: (1) query only, (2) user model only, and (3) query and user model at the same time with equal weights, using a simple tab-based interface.

It was designed to support information analysts and to adapt to short-term tasks. It has a  X  X  X otebook X  X  or a shoebox just like Adaptive VIBE (Section 3.3 ) as a user feedback mechanism. TaskSieve displays the user model and user query informa-tion in the document summaries as well. The document summaries of TaskSieve are noted as Task-Infused Snippets, which highlight the task model terms as well as the query terms in different colors. Moreover, the sentences to be displayed in the task-infused snippets are selected adaptively, considering the user-selected task model weights. 4.3. Data collection
We used the same test collection for the baseline and the experimental systems developed by He et al. (2008) . They ex-panded the TDT4 (Topic Detection and Tracking) English test corpus, in order to support the evaluation of task-based infor-mation exploration. It contains 28,390 English documents published between October 2000 to January 2001. We chose to use this corpus for the current study, because it enriched the original 18 TDT4 topics into so-called GALE (Global Autonomous
Language Exploitation) topics to resemble the tasks performed by intelligence analysts. The tasks modeled in the corpus aimed to be realistic, complex, and dynamic. Each GALE topic contains an overarching task theme and up to 10 different but related sub-tasks ( Fig. 11 ). The groundtruth information per each topic was added by human annotators at the passage level with three different degrees of relevance (highly, slightly, and not relevant). At least two annotators marked each scenario.

Out of 18 topics, we selected three for the current study. In order to reduce the bias caused by the different complexity levels of the topics, we devised a measure to consider the distribution of groundtruth information in the corpus. In some topics, the answers were concentrated in a small number of documents whereas the answers were dispersed across many documents in other topics. The former case would be easier because there is a chance for users to pin-point a specific group of relevant documents, without exploring larger document spaces. We defined the standard deviation of relevant passage count per document as a pseudo topic complexity measure (Eq. (2) ). The complexity was calculated for the 18 topics and the three topics with equivalent standard deviations were selected. Table 2 shows the complexity score of these three tasks and their topic descriptions.
 4.4. Study procedure
In order to test the hypothesis, we ran two search sessions per each participant. In each session, either the baseline or the experimental system was used to complete the task. The order of the systems and the topics was randomized by the Latin
Square Design in order to reduce any possible learning or fatigue effects on the statistical analysis. They were asked to read a one-page introductory statement to the experiment, and to complete an entry questionnaire asking about their search expe-rience and consumption of news.

Fifty minute training sessions were given to the participants in which the experiment coordinator explained every feature supported by the systems and let the participants solve a real task as training. The training task was chosen from the TDT4 collection as the experiment tasks (40004  X  X  X ussian Submarine Kursk sank X  X ). It was exactly same with the real tasks (4000, 40021, and 40048) in terms of the structure. No real task was given to the participants before starting the main sessions. The training session was especially important for Adaptive VIBE because it was new to the participants and most of them were completely unfamiliar with its visual exploration feature.

After each search session, the participants were asked to fill out a post-task questionnaire asking about their subjective experience on each system. When the search sessions were finished, they participated in a 10-min exit interview. Fig. 12 illustrates the procedure.
 5. Results analysis
The user study results analysis is comprised of three broader stages: (1) analysis of user activities, (2) system performance analysis, and (3) user performance analysis. The first stage provides some descriptive statistics that can provide a basic understanding about the user activities. The second stage provides the performance analysis of the system outputs. That is, whether the systems returned good results to the users. Adaptive VIBE X  X  internal personalization engine generates ranked lists first and then visualizes the documents on the 2D VIBE visualization plane. Even though users mainly see the visuali-zation, it is important to examine the performance of the ranked lists along with the visualization because the ranked lists work as the foundation to generate the adaptive visualization.

After the result is presented to the users, the users interact with it. They explore the visualization space, examine the spa-tial information, check the document summaries, and click-open the document they find potentially relevant. When they find relevant text passages, they annotate them to store in the shoebox. At this stage, users can make full use of the core interactive exploratory features. Therefore, the user performance evaluation is even more important than the system perfor-mance evaluation. We evaluated the user performance by checking the documents they opened and the notebook contents they maintained. These measures can identify the performance of user activities during the interaction with the personalized search systems. Fig. 13 summarizes the performance analysis process. 5.1. User feedback analysis
This section reports the analysis of users X  subjective feedback collected from the post-search questionnaires. Except for the open question to collect the user comments, all questions used a five point Likert scale (1 X 5), where the users could se- X  X  X xtremely X  X  respectively in order to correctly indicate the meaning of each point. Even though points 2 and 4 were not tex-tually labeled, users were instructed that they could select 2 or 4 as well.

The participants were asked to report their overall positive impressions on the performance of the systems. Even though there was a risk of over-simplification by asking a single question on this matter, it was still a good measure when combined with other more specific questions in order to capture the big picture. Table 3 compares the answers from the subjects. The best possible score was 5 (from the five point Likert scale). The distribution of the scores was not normal (Shapiro X  X ilk nor-participants were more disposed towards the baseline than the experimental system. The larger standard deviations of the experimental system (over 0.9) may be worth noting. During the open interviews, many participants stated that they found the experimental system unfamiliar and complicated at first sight but they eventually became familiar with it through the training. This larger variance can be understood as reflecting their mixed feelings on the new system.
Table 4 compares the relative positive reactions towards either the baseline and the experimental systems. It summarizes how many subjects preferred the baseline to the experimental system. Even though more people preferred the baseline (18), there were a good number of participants (9) who gave the same ratings to both systems. Therefore, about 45% (9 + 6) of the participants who acted as surrogates of experts with more experience showed equivalent or higher positive reactions to-wards the experimental system despite their greater complexity. 5.2. Analysis of user activities 5.2.1. Visualization feature usage statistics
The ability to move POIs and examine the related documents on the fly is the core feature of the VIBE visualization. From the system log recorded during the user study, we were able to count the number of user manipulations (moves) of the POIs ( Table 5 ). The POI manipulation frequency of the query and the user model were about the same. Even though the partic-ipants were informed about the importance of the user models, they seem to be naturally inclined to controlling their own representation of information needs by manipulating the query POIs. However, it should not be underestimated that they still showed almost the equivalent interests in the user model POIs as well. 5.2.2. Page navigation analysis
Even though Adaptive VIBE does not have pages, it is worth examining the page navigation pattern of the participants using TaskSieve (where page navigation exists). Table 6 shows the frequency of the page accesses during the user study.
Almost all (96%) of the actions were done in the first page. It is well known that users usually navigate one or two pages in the casual web search ( Jansen, Spink, &amp; Saracevic, 2000 ) and the users of personalized systems tend to have a bias towards assuming that they found everything they need after checking the first or second pages and then abandon any efforts at dee-iors. The related analysis about the navigation depth is presented in Section 5.5.3 . 5.3. System performance analysis  X  ranked list performance
This section compares the performance of ranked lists generated by two systems. Using the personalized search results, the experimental system can produce the adaptive visualizations. Therefore, it is essential to compare the ranked lists first, and then we can move onto the next stage. We analyzed the precision of marquee-selected documents to test the precision of Adaptive VIBE X  X  ranked lists and compared the average precision of top 10 documents over the two systems, using Pre-cision@10 and NDCG@10 metrics. 5.3.1. Measures
We used several precision measures to compare the performance of the baseline and the experimental systems. The most traditional one was Precision@10, which means the average precision of the top 10 documents retrieved by the personalized search systems. We defined the rank as 10 based on the fact that users usually check the first 1 or 2 pages and do not go to lower ranked items (Section 5.2.2 ). The second measure was NDCG (Normalized Discounted Cumulative Gain) at rank 10, which calculates the usefulness, or gain, of a document based on its position in the result list ( J X rvelin &amp; Kek X l X inen, 2002 ). We could use these rank-based measures for the experimental system as well, because it generates the ranked lists internally and the top 10 high scoring documents are color-encoded in the visualization. The measures are defined as follows.
 With the experimental system, we could calculate the precision of documents, selected by users using the marquee tools. It means the fraction of the number of relevant documents and the number of selected documents (Eq. (5) ). 5.3.2. Precision of marquee-selected and top-10 documents
Table 7 compares the precision of the retrieved documents in the baseline as the precision at rank 10 and the precision of the marquee-selected documents. There was no statistical difference between the systems. However, given the fact that the marquee selection has a higher likelihood to include noise (non-relevant documents) within the visualization, it is worth noting the efficiency of the tool which supports rich exploration within the document space.

Precision@10 and NDCG@10 were used to compare the ranked lists generated by the two systems. Adaptive VIBE gener-ates ranked lists first and then visualizes using VIBE as shown in Section 3.3 . Therefore this analysis can examine the sys-tems X  ability to generate high quality document sets (ranked lists) whether or not they are visualized.

Table 8 compares the scores of the two systems. The statistical test showed no difference in terms of Precision@10, whereas NDCG@10 showed significant difference between the baseline and VIBE (Wilcoxon signed rank test, p = 0.0289).
This result is encouraging because it suggests that the Adaptive VIBE visualizations were able to provide more relevant doc-uments than the baseline system X  X  ranked-lists. This improvement could be the result of the higher quality of the user mod-els. The user models can be developed and enriched during the cycles of the user-system interactions and contribute to the higher quality of search results.

The initial ranked lists of better quality would have led to better visualizations. At the same time, the Adaptive VIBE prototype shows the top 10 highly scored document icons in different shades. The document with the highest score is the darkest and the last document is the lightest, but still distinguished from the lower rank documents. Users could have used the document colors as reliable cues leading to the relevant information. 5.4. System performance analysis  X  visualization performance
Using the documents retrieved by the personalized search engine, the experimental system can generate the adaptive visualizations. In the previous section, we showed that Adaptive VIBE could produce better search results than the baseline, in terms of the top 10 highly scored documents. The next question is the quality of the visualizations that were generated from these results.

We can answer the question by examining the separation of the relevant and non-relevant document clusters in the visu-alization. At the same time, our intent is to check if the user models were still able to attract the relevant documents. We have observed this effect in our previous study ( Ahn &amp; Brusilovsky, 2009 ), where we have simulated the adaptive visualiza-tions, using the log data extracted from the TaskSieve study ( Ahn et al., 2008 ). The expanded TDT4 dataset with groundtruth (Section 4.3 ) was used for the simulation, so we could calculate the relevance of the documents within the simulated visu-alizations. The simulation has limitations due to the artificiality of the data used. This section presents the results from the real user study. It analyzes the actual visualizations seen by users during the user study.

Table 9 compares the x -coordinates in the visualizations generated during the user study. As in the previous study, the average location of the relevant documents was closer to the user model than the query in the Adaptive VIBE visualizations.
The higher x -coordinates indicates that they are closer to the user models because the user models are placed in the right hand side of the visualization. The difference in the locations between the relevant and non-relevant document clusters was statistically significant (paired Wilcoxon rank sum test, p &lt; 0.001). 5.5. User performance analysis  X  document access
When the documents are presented to the users through ranked-lists (baseline) or visualization (experimental system), they examine the provided information and open a document to read the fulltext. This  X  X  X pening X  X  action is based on the sys-tem performances analyzed in the previous sections, rather than random activities. Therefore, by analyzing the document access behaviors of the users, we can learn how well each system supported the users. 5.5.1. Measures
The most common document access method is to open the document by clicking the document icons or titles and to read the fulltext. We can measure the precision of the opened documents as follows. If a system could provide an effective ranked list or visualization and they were accompanied by proper cues, users will be more likely to open relevant documents. The analysis results are presented in Section 5.5.2 .

Along with the relevance of the opened documents, we need to investigate the depth of document ranks or the diversity of documents that user opened. It is due to the nature of the visualization. Unlike ranked lists where users need to click sev-eral times to see the low ranked documents, Adaptive VIBE shows 100 X 200 documents at the same time regardless of their ranks in the original ranked lists. Therefore, we need to analyze if the users could benefit from this feature in terms of the diversity of the opened documents and discover relevant information hidden in lower ranks. We define the navigation depth and the diversity as follows and the analysis results are presented in Section 5.5.3
As can be seen in Eq. (8) , we measure the diversity by the number of discovered relevant sub-topics. It looks similar to the traditional recall measure, but it is topic-based, rather than document-based. Because the sub-topic information is not pro-classify the documents into the sub-topics. 5.5.2. Precision of opened documents
Table 10 compares the average open precision of the baseline and the experimental systems. In order to clarify the com-parison when the user model is working, the document opening actions taking place after the user models were built were counted. Even though the average precision of the baseline is slightly higher, there was no significant difference (Wilcoxon signed rank test). It should be noted that the performance of the baseline TaskSieve system is almost perfect (0.950) and it was very difficult to improve upon.

This is due to the nature of the personalized system and its users. The personalized systems tend to concentrate more relevant documents in the first or second pages and the users tend to examine those top ranked documents only (See Sec-tion 5.2.1 ). However, we should note that the subjects of the experimental system had more freedom to explore the docu-ment space and they had more chances to make mistakes, even though they were provided with quality results from the system. In fact, there were hundreds of documents spread in the visualization and a single click on a non-relevant document by mistake could lead to the large drop in open precision. 5.5.3. Depth of document navigation and diversity of open documents
Even though the open precision could not show any improvement in the experimental systems, it was not necessarily discouraging. It gave us a hint that the increased degree of freedom could benefit the experimental systems. One possible benefit was the depth of the navigation.

Table 11 illustrates the rank of the opened documents under various conditions. Here, we can clearly see that there were differences between the systems. The average rank of opened documents in the baseline was around 4, whereas the exper-imental system went down to around rank 20. Even when the relevant documents were solely counted (the  X  X  X elevant X  X  col-umns), the experimental system marked around rank 10, whereas TaskSieve users found the relevant documents above rank 4. Using the experimental system, the users could have more freely examined low rank documents and then have chosen relevant ones. The rank of the noted documents  X  from which the user notes were annotated  X  shows similar results (the  X  X  X ote Rank X  X  column).

The navigation depth analysis is expanded even further by examining sub-topics (going beyond the task topics: 40009, 40048, and 40021 for this experiment) found by the subjects. The goal of this analysis is to calculate the proportion of the discovered sub-topics among the entire set of relevant sub-topics from the groundtruth information. The procedure is as follows: 1. Locate the relevant documents from the groundtruth per each topic (40009, 40048, and 40021). 2. Identify the sub-topics from the relevant document sets. 3. Assign each opened document to the sub-topics found in the previous step. 4. Compare the number of the discovered (assigned) sub-topics.

For Step 2 and 3, LDA ( Blei et al., 2003 ) was used to detect latent topics from the text and cluster the documents around them. Therefore, we could first apply the LDA algorithm to the relevant documents and extract the sub-topics from them (Step 2). In Step 3, the list of documents opened by the subjects were divided into each sub-topic. Because
LDA requires the specification of the number of sub-topics to be estimated from the corpus and the real values are not known, two values were tried for each topic, k = 15 and 20. We chose these values assuming that the number of sub-topics will be greater than the number of questions per topic. Because the topic 40021 has the maximum number of questions ( N = 13), we tried k = 15 and k = 20. However, we should note that this limitation of LDA might limit the accuracy of the analysis.

Table 12 lists the number of sub-topics discovered by the subjects in terms of their document opening actions. It can be seen that experimental system always found more sub-topics than the baseline. The difference may look rather small (less than 4) but a single difference in sub-topic discovery can mean a great deal in the given problem domain. 5.6. User performance analysis  X  note annotation
After receiving the search results and studying the documents included in the output, the participants were asked to save text fragments that contained answers to the tasks into the shoebox as the final report of their search tasks. Both the baseline and the experimental systems support this feature as part of user model construction and the example annotations are shown in Figs. 1 and 10 . By analyzing the annotations saved in the shoebox, we can evaluate the overall performance of the baseline and the experimental systems. 5.6.1. Measures
Three measures were employed to evaluate the quality of the user annotations. The first one is the precision of the anno-tations. Unlike document precision, where the relevance of individual document access actions were regarded as 1 or 0, note precision can give a precision value 0 X 1 per each note (Eq. (9) ).

It calculates the precision of a passage against the ground truth, where the overlap _ length is the character length of the common text chunk between a user X  X  selection and the ground truth; weight is the weight of the ground truth combining the two annotators X  mark-ups, where the weight can be one of five levels: 0, 0.25, 0.5, 1, 1.25, 2; miss _ length is the character length of the part of the passage that has no overlap with the ground truth. Here the 0.5 associated with miss _ length is the penalty ( He et al., 2008 ).

The second measure is the diversity of notes. The same procedure with the document level diversity (Section 5.6.3 ) was used for the notes too. The last measure is productivity. If the diversity increases, it could lead to higher productivity. The productivity is defined as the number of found answers (and saved to the notebook) in a given time (Eq. (10) ). Because every participant was allowed to use just 20 min per topic, the simple count can be used as a productivity measure. 5.6.2. Note precision analysis
Table 14 a shows the average note precisions for each of the two systems. According to the Kruskal Wallis rank sum test, there was no significant difference. Even though the average precision of Adaptive VIBE did not show improvement, the overall effectiveness of Adaptive VIBE can be better analyzed in conjunction with productivity. That is, the final products (annotations) made with Adaptive VIBE produced more output than TaskSieve with the equivalent quality. The details are discussed in the next section.
 5.6.3. Note diversity and productivity analysis
As the previous section (Section 5.5.3 ) suggests, the experimental systems were able to help users to examine more di-note level. Table 13 is the latent sub-topic analysis undertaken for the documents where the notes were saved by the sub-jects. Note that the same analysis in the previous section was done for the  X  X  X ocument opening X  X  actions. This analysis is to examine whether the clusters found by the opening action were actually used for the final product, the user notes. The re-sults show that the experiment system could find more sub-topics within the annotations in four out of six cases (corre-sponding numbers were underlined in Table 13 ). The baseline could best the experimental system in only one case (Topic = 40021, k = 20). The participants found the same number of sub-topics in the remaining case (Topic = 40009, k = 15).
With the increased diversity shown in the previous section, we can expect that the experimental system could enjoy a higher level of productivity as well. The productivity means the amount of relevant information collected in the given time frame. Table 14 b shows the number of notes annotated by the participants within each system.

It shows that the experimental system collected more notes (476) than the baseline (451). The number of high-precision notes, which counts the notes with precision greater than 0.9, provides a consistent result (397 versus 343). Even though the experimental system X  X  mean note precision could not improve on the baseline (Section 5.6.2 ), the productivity analysis shows that it collected more data. In order to understand precisely what happened, the notes collected by the two systems were sorted into decreasing order and plotted on the same graph ( Fig. 14 ).

It shows the change of average precision of the N high-precision notes and compares the average precision scores of the two systems when the subjects made N high-precision notes. Both are dropping as they collect more passages but the base-line drops more sharply. The experimental system shows a higher precision until N reaches 405. The right end point N = 476 is the total number of annotations made using the experimental system and it is where the overall average precision in Table 14 a was calculated.

Subjects using the baseline made 451 annotations, so a new average precision at N = 451 (where the note counts of both systems are same) was calculated ( Table 14 c). The experimental system shows a significantly higher average precision (Wilcoxon signed rank test, p &lt; 0.001). 6. Summary and discussion
In this study, we examined the effectiveness of the adaptive visualization-based exploratory information retrieval system called Adaptive VIBE. It was done in an experiment that compared Adaptive VIBE with a traditional text-based personalized system. We created an experimental design that allowed us to compare the systems on two levels (stages)  X  (1) system per-formance and (2) user performance  X  and defined two hypotheses and test measures accordingly (Section 4.1 and Table 1 ).
The experimental system showed improved system and user performance over the baseline. Table 15 summarizes the mea-sures that showed higher performance with the experimental system. 6.1. Higher precision of search results in the system level
First of all, we assumed that the experimental system would be able to generate personalized ranked lists with higher precision. The analysis was consistent with our assumption. The marquee tool that was designed to help users to select doc-uments spatially showed an almost equal precision level compared to ranked lists.

The top 10 documents retrieved by the experimental system were of a better quality than the baseline is. NDCG@10 was used for measuring the quality of the retrieved documents and the experimental system produced a higher NDCG for the top 10 documents than the baseline. Note that NDCG gives higher scores to the high ranked relevant documents. Therefore, the higher NDCG score within the top 10 ranked documents means that the experimental system promoted relevant documents more strongly within highly ranked documents.

One of the likely sources of this performance improvement could be the improved quality of user models in the exper-imental system. Its fundamental search ability was identical to the baseline. Both systems used the query and the user model in order to retrieve documents. However, the experimental system had more power to adaptively visualize the search results and to help users interactively explore the adaptive visualization. Therefore, within the cycle of personalized searching and user interaction, it could build better user models than the baseline system. 6.2. Visualization performance of Adaptive VIBE
While the VIBE visualization component operates on top of the personalized search results, it is not the ranked list, but the adaptive visualization generated by Adaptive VIBE that the users see and manipulate during their search process. There-fore, we need to examine the evidence that can confirm the visualization quality at the second stage of system performance analysis. From the log data of the user study, we could calculate the locations of relevant and non-relevant documents within the visualizations shown to the participants. By analyzing the document locations, we could confirm the following user mod-el effects.
 1. Adaptive VIBE visualizations could separate the relevant and non-relevant documents visually. 2. The relevant document clusters gathered closer to the user model in the visualization. 6.3. User performance  X  diversity and productivity beyond over-fitting
The results discussed above focused more on the system X  X  role in presenting the right information in the right way. A good text-based system would place a number of documents at the very top of the ranked lists. The Adaptive VIBE system, work-ing perfectly in our scenario, would place the relevant documents just beside the user models.

However, users X  information needs are not that simple. Particularly, the tasks required in this user study were designed to navigate through complicated paths of exploration. Those who try to undertake these tasks usually try different queries, study the initial results returned from the system, learn more about the problem, and try another hint against the system. They may need to answer a question in order to complete the next task when the structure of the task is multi-faceted. Therefore, any system that is optimized too much for a specific task or a sub-task may result in the  X  X  X ver-fitting X  X  problem.
This is one of the problems that the personalized information access systems can face. We have experienced it before, when we provided users with an editable open user model ( Ahn et al., 2007 ). There, the user model over-fitted due to user inter-vention and eventually deteriorated the system X  X  performance.

A personalized information retrieval system with an over-fitted user model can bring up a small number of highly precise results in the high rank. Nevertheless, there is always a risk that valuable information will be missed. Therefore, two vari-ables were investigated in order to examine the aspects that the simple precisions cannot cover: Diversity and Productivity .
Diversity is defined as how much relevant information is discovered by the participants. We first checked the depth of navigation in the ranked lists. The results clearly showed that the participants navigated much deeper into the resulting doc-uments  X  that were relevant to the questions of the three tasks they undertook  X  than the text-based system. This result contrasts with the findings in Keane et al. (2008) , which found that the search engine users were easily biased toward the top of the ranked lists. The second measure used for investigating the diversity was the  X  X  X iscovered sub-topic X  X  counts.
A statistical topic detection algorithm was applied to the test corpus and we found that the visualization systems could dis-cover more sub-topics than the baseline at the annotation level as well as the document access level than the baseline. One of the design principles of Adaptive VIBE was to employ the exploratory searching scheme. We wanted a system that could encourage its users to explore the search space and learn within the process, rather than a simple look-up search system.
The ability to discover diverse documents, annotations, and topics using Adaptive VIBE was an ample evidence that our expectations were met.

Even though the note precision of the experimental system was not higher than the baseline X  X , we showed that it made more high precision annotations. In terms of the annotations that are the final products of the task-solving, the productivity can better reflect the reality. That is, how much high quality information was collected during the given time. It is a combi-nation of the quality and quantity of the user annotations. We found that the experimental system users produced more annotations that were relevant than the baseline. Along with the diversity, the higher productivity of the experimental sys-tem proves that it was capable of better assisting users. 6.4. Limitations
Our work can be considered as only the first step in exploring the value of personalization in a more interactive explor-atory search context such as that offered by the VIBE approach to interactive visualization of search results. We introduced a specific approach to make VIBE visualization adaptive by applying the user model terms as POIs and compared this approach and to focus on the main ideas introduced in the paper, we had to use relatively simple solutions for some components of the experimental and baseline systems. Below we want to revisit these decisions and discuss to what extent they limit the gen-erality of the results.

The most visible simplification is the use of explicit relevance judgement: the user is expected to explicitly add a page or a fragment to the shoebox for the system to consider it relevant. This approach allowed us to avoid the additional burden of distilling relevant content. While the  X  X  X hoebox X  X  approach to explicit relevance judgment is considered to be realistic in sev-might not be suitable for less prepared users who might not be capable of providing explicit judgements and might have problems with managing their  X  X  X hoebox. X  X  A successful personalized search system for a more general user audience should contemporary work on implicit indicators explored a number of approaches to increase the quality of relevance judgement modern approaches to distill relevance fragments might still be lower than in a system with explicit relevance judgement due to additional noise, but the decrease will likely be small and it will equally affect both experimental and baseline systems.

Another simplification is the use of a relatively simple user interest model in the form of a term vector updated by relevance feedback. While this model is more advanced than models used in early personalized search systems (since it maintains separate term vectors for each user task), it is still less sophisticated than user interest models explored in more recent work on personalized search. A more sophisticated model can include concept drift and forgetting as suggested in
White, Bennett, and Dumais (2010), Hassan, Jones, and Klinkner (2010), Sugiyama et al. (2004) and/or it could model the representations of user models by adopting conceptual entities ( Ahn &amp; Brusilovsky, 2010 ) or by improving term weighting using the techniques such as lexical chaining ( Barzilay &amp; Elhadad, 1997 ). The use of more advanced user models will likely lead to the improved quality of the personalization. However, as in the case above, the change will likely equally affect both experimental and baseline systems.

In a more general sense, the focus on experienced users which is evident from the design of both the system and the experiment, can be considered as an potential limitation of both the approach explored in this paper and the personalized interactive search in general. While we argue that more interactive search in general  X  and more interactive search person-alization in particular  X  is beneficial to users, our work and a number of other research projects depend on relatively pro-fessional users (college educated, with solid search experience) to prove it. The fact that interactive personalized search works well for this category of users does not necessary mean that it will work well for regular,  X  X  X aive X  X  users who form the majority of search engine clients. In fact, the repeated failure of more sophisticated search approaches (including search visualization) on the mass market indicates that increased interactivity and its associated increased system complexity might not benefit less prepared users. As our own research demonstrated, a misuse of interactive features stemming from users X  inability to understand more complicated systems might even harm their performance ( Ahn et al., 2007 ). This evi-dence hints that the classic ranked list search with traditional non-interactive personalization might be the best approach required to determine to what extent this approach can benefit users with different levels of preparedness. 7. Conclusions
This paper presents an adaptive visualization method called Adaptive VIBE. It was designed to overcome the limitations of the ranked-list based static personalized search user interfaces and to maximize interactivity using visualization techniques.
It attempts to provide an exploratory search environment where the system understands users X  search contexts and visual-izes the search result adaptively to them.

We corroborated the effectiveness of Adaptive VIBE and analyzed its characteristics through a user study. The perfor-mance of Adaptive VIBE has been explored from two prospects (1) system performance and (2) user performance. According to the analysis, Adaptive VIBE could construct higher precision document sets compared to the baseline text-based person-alized search system; in addition, the visualization could separate relevant documents from non-relevant documents. The personalized visualization also positively impacted several aspects of user work. Users of Adaptive VIBE were able to access more diverse information. They discovered hidden documents in the lower ranks of the retrieval lists, explored more sub-topics, and produced more diverse final reports compared to those using the the baseline system. At the same time, it is important to note that the increased precision on the system side has not resulted in an increased precision of user selec-tions. This result hints that a better performance of a more complex personalized visualization system might have been counterbalanced by its higher complexity. A more sophisticated study is required to carefully measure the balance of system power and increased complexity in adaptive visualization systems.

On the other hand we could speculate that the comparative performance of Adaptive VIBE against the baseline TaskSieve system was affected by the use of a shoebox-based approach to user modeling. While the presence of shoebox simplifies the construction of the user modeling component of the system, it also does not allow the system to differentiate the intensity of user interest in different phrased placed into the shoebox. The resulting  X  X  X latter X  X  user model diminishes the potential power of Adaptive VIBE, which has a unique ability to visualize varying level of user interest in user model components. It might be likely that the use of a more advanced user modeling approach will increase the advantage of the Adaptive VIBE.
Taken together, the results of our study provide a good ground to recommend the Adaptive VIBE system as a useful com-plementary exploratory search tool that can remedy some shortcomings of traditional low-interactive personalized search systems based on the ranked lists. This tool might offer several advantages to end users dealing with complex search tasks. However, it is not a silver bullet that can replace the more familiar ranked list approach.

We plan to extend the current user study in order to determine the individual factors and learning effects in a long-term multi-session study such as MILCs (Multi-dimensional In-depth Long-term Case studies) ( Shneiderman &amp; Plaisant, 2006 ). A more generalized Adaptive VIBE version that is integrated into an accessible search framework could be used for this purpose.
 References
