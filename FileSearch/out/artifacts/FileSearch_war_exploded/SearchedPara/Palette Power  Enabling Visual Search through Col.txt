 With the explosion of mobile devices with cameras, online search has moved beyond text to other modalities like images, voice, and writing. For many applications like Fashion, image-based search offers a compelling interface as compared to text forms by better capturing the visual attributes. In this paper we present a simple and fast search algorithm that uses color as the main feature for building visual search. We show that low level cues such as color can be used to quantify image similarity and also to discriminate among products with different visual appearances. We demonstrate the effectiveness of our approach through a mobile shopping appli-cation 1 . Our approach outperforms several other state-of-the-art image retrieval algorithms for large scale image data.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Search Engine, Image Search, Visual Search, e-Commerce
Recent advances in mobile devices, especially smartphones and tablets, has redefined the dynamics of commerce. The ability to shop anywhere and anytime has allowed users to bridge the gap between offline and online stores. It has also led to newer shop-ping trends where users browse for goods in offline stores and use online stores to find the best deals [23, 1]. Recent statistics sug-gest that among the top online shopping categories, 58% of smart-phone users research electronic goods in-store but purchase them online [1, 6]. A similar trend has been observed in other categories such as shoes (41%) and apparel (39%). However, searching for a product among the massive collection of items remains a major eBay Fashion App available at https://itunes.apple.com/us/app/ebay-fashion/id378358380?mt=8 and eBay image swatch is the feature indexing millions of real world fashion images bottleneck for this shopping experience. Although recent work in text retrieval has addressed some of these issues, categories such as fashion continue to present a huge challenge. This is particularly contributed by the following:  X  Most of the items in such categories lack useful product speci- X  The notion of relevance in these categories is mostly visual  X  Mobile shopping experience requires a fast and low memory Many product categories sold online can be described exactly by a limited set of well-defined attributes. For instance, digital cameras can be described accurately by their model name, frame size, and pixel count. On the other hand, it is difficult to describe women X  X  dresses in terms of such textual attributes. Although basic visual properties, such as color, can be specified by the sellers, a cursory data analysis shows that a large fraction of fashion items listed online lack this information. For example, Figure 1 shows the color and pattern distribution of men X  X  neck tie collections on an e-commerce website where this metadata is un-specified for around 50% of the items. Also, for some of the visually complex patterns (such as 19.8% items tagged as multi-color), there is not much tex-tual information about the color content of the items. We believe this reinforces our aforementioned observation regarding the diffi-culty of using text to describe items that are fundamentally defined by their visual attributes, and highlights the importance of using their image information to index them more accurately.

Consider the history of web search as well. While text based features played a crucial role in the early days, in the form of tf-idf scores, now increasing importance is associated with several other kinds of signals and features that are more semantic and percep-tual in nature. In similar vain, image search is also going through a transition where it becomes more important than ever to really un-derstand the perception of users and try to capture the same in any visual task such as image matching, search, or retrieval. As is to be expected, one of the first things that attracts a viewer X  X  attention is the color distribution. Features such as patterns, textures, or styles are only secondary in nature. To this end, the goal of our work is to really focus on color as the primary feature and see how far we can take image search.

In a way, the goal of this work is to see how far something ex-tremely simple can be taken, rather than try to benefit from more complex features. Surprisingly, simplicity actually goes a long way, reinforcing our hypothesis that in the context of images, per-ceptual features should be the starting point, rather than just a com-ponent. To this end, our paper focuses on the image search and re-trieval application and is constrained to only capturing an image X  X  features in color, and specifically a bag-of-color -i.e. just the color distribution. A dual goal, however, is for us to build an application that actually is practical and usable at a large scale; to this end, the concern that a lot of images, specifically for shopping, are taken with low-end cameras on cell phones is an additional difficulty.
Using image information to index items for a mobile shopping experience is challenging for two main reasons. The quality of these images can have a lot of variance, especially since most of them are captured by amateur users. Secondly, being an online experience, the indexing and retrieval system must be computa-tionally very fast, with a low memory foot-print. These constraints limit the types of information that can be extracted from product images, and the algorithms that can be used to analyze this infor-mation. In this paper we address some of these issues by describ-ing a simple and fast visual search algorithm that demonstrates the power of low-level features, specially color, for online shopping of fashion related items.

This paper presents our techniques that go into the fully devel-oped and live system at eBay being used on mobile devices under eBay Fashion. The app is live and functional and is accessible to everyone in the US and can be used to search and shop for fash-ion clothing inventory on eBay. The app can be found on the Ap-ple app store through eBay Fashion App 2 ; the feature indexing real world fashion images is called eBay Image Swatch ; it indexes and searches a large number of images, in the order of millions. The process starts with a user taking a picture (from their cell phone camera), or using a stored image on the phone, of any color combi-nation desired (and typically of a dress, or tshirt or piece of cloth-ing); this is then searched for using our algorithm, on the millions of images indexed by our system, and the best matches are returned.
The remainder of the paper is organized as follows: Section 2 discusses related work on image search. Section 3 describes the main insights drawn from our data and the high-level algorithm for our color based search engine. Section 4 highlights the distance measure used in the algorithm, while Section 5 talks about the in-dexing scheme used for scalability. Section 6 discusses in detail our evaluation and experimental validation. Finally, Section 7 con-cludes the paper and discusses future work.
 Figure 1: (L) Color and (R) pattern distribution of men X  X  neck-tie; Nearly
The task of visual search has been studied in great detail in Com-puter Vision community. Most of the existing techniques can be
Available at https://itunes.apple.com/us/app/ebay-fashion/id378358380?mt=8 Figure 2: Sample color spaces; Comparing the shape of decision regions broadly categorized into  X  X eature Based Approaches" and  X  X ag-of-Words Based Approaches". Feature Based approaches employ traditional paradigm of extracting low-level image features and use techniques such as histogram distance, euclidean distance etc. to compute image similarity. The most commonly used features are:  X  Color Features: Most systems using color features utilize color  X  Texture Features: Haralick et al. [11] proposed elementary tex- X  Shape Features: One of the earliest systems using shape fea-
Bag-of-Word (BoW) Based Approaches: This technique refers to representing an image by an orderless collection of local fea-tures [9]. A basic BoW method clusters local features into dif-ferent vocabulary groups to generate a  X  X odeBook". Each image is represented over this codebook using a histogram of vocabulary counts. Finally, histogram distances are used to compute similarity score between two images. Several different types of local fea-tures that utilize this particular image representation include Scale Invariant Feature Transform (SIFT) [18], Speeded Up Robust Fea-tures (SURF) [2] or Histogram of Oriented Gradients (HOG) [5]. Figure 3: Lighting Variation induces high variance in RGB values even Spatial Pyramid matching techniques [17] have also been proposed that define a fixed hierarchy of rectangular windows for capturing  X  X erceptually salient features" of the image. However, one of the major drawbacks of these approaches is the long training time to learn a visual vocabulary. Since online products usually have short life-span and continuous high volume of inventory updates, such long training times cannot be afforded.

Such advances in image search techniques have also enabled several consumer facing applications. Some of the most promi-nent technologies known today are Google Goggles [13], Bing Vi-sual Search [15]. However, these technologies provide generic im-age search capabilities which may not be useful for a specific do-main such as fashion. There are also customized vertical based image search techniques for fashion such as GILT Groupe X  X  app for iPhone [14] but they only use dominant color information for matching and hence cannot effectively support multi-colored query images. The objective of this work is to thoroughly analyze one specific vertical that is fashion and design a search algorithm geared towards solving fashion-specific image matching problems.
Designing a visual or image search engine involves multiple steps such as finding salient regions in images (i.e. object localization), computing representative visual properties (feature extraction) and searching the entire repository for similar images (matching). A number of factors affect the performance of system including color distribution, texture, pattern, and shape information that may reveal the clothing style. Each of these phases present a new challenge in terms of scalability and efficiency. To understand the bottleneck of each component and find a possible solution, we choose images of the fashion product family from one of the largest e-commerce marketplaces, and analyze them for possible solutions. Following sub-sections describe insights learned from this data analysis.
Online user-generated image content for commerce often vary in quality. In a peer-to-peer commerce marketplace, casual sellers may not be motivated or skilled to take professional quality pictures as opposed to professional sellers dealing with large sales volume. Most of such low-quality images suffer from poor lighting, low contrast, and cluttered background that makes the task of image matching more difficult (Figure 4). Therefore finding the relevant region in the image (object localization) becomes a very important task. One approach would be to apply a State-of-the-art image seg-mentation algorithm to remove the background. However, typical image segmentation algorithms operate at pixel levels and may take several seconds to process a good resolution query image which is unacceptable in our constrained settings. Figure 4: Sample images with challenging background; Figures (a) and
To solve this problem, we take insights from our data and build an offline spatial prior for foreground (dress) and background (clut-ter) pixels. Firstly, a black-box segmentation algorithm is used (GrabCut [22]) to automatically remove the background for im-ages in the women X  X  dress category. Images with poor confidence for background removal are ignored. The segmentation provides a mask or an outline of foreground pixels. We take the sample mean of RGB pixel values at corresponding mask locations and visualize this for each style of dress in our inventory (Figure 5). As shown, the mask outlines have a close resemblance to the dress style shown on the right (Off-Shoulder, Long Sleeve, Halter, Cap Sleeve, 3/4 Sleeve) illustrating that a large number of dresses re-spond well to the segmentation algorithm. Further, the mean RGB image in all the styles occupy the center of the image which pro-vides a strong clue as to how users tend to center the dress before they take pictures. We use this insight to design a segmentation mask that samples the center of the image for foreground pixels and assumes pixels falling outside of center-window to be back-ground. One obvious failure case of this technique is shown in figure 6, where the image consists of multiple views of the dress and hence such a sampling would lead to false segmentation. How-ever, since most of these images occur in our online inventory and tend to have simpler background, we can afford to apply black-box segmentation algorithms to find dress regions. Moreover, a smaller portion of such images allows us to keep a good indexing rate.
We emphasize the use of color for image retrieval. Therefore the choice of color representation is important for extraction of color distribution. Typically, images can be represented by differ-ent color spaces such as HSV, HSL, HSI, Lab, LUV, YCbCr. Each color space has its own set of strengths and weaknesses. These color spaces map pixel values to 2-dimensional chrominance space, and a single luminance/brightness channel that captures most of the lighting variations. Figure 5: Spatial prior for various women X  X  dress styles. We segment the Figure 6: (Left) Spatial prior failure case, (Right) Addressed using Back-
Visualization of some of these color spaces are shown in Fig-ure 2. Once color space is chosen, it needs to be sampled in or-der to get the color histogram. The simplest form of sampling is uniform sampling, forming a rectangular grid, where each axis is divided equally. Each axis may have different number of bins, but along each axis, the bins have equal width. As shown in Figure 2 only HSV color space has a uniform color distribution which is best suited for uniform sampling. Choosing any other color space with such a sampling scheme will lead to color bias. For e.g. Uni-form sampling with YCbCr will lead to good matching for red and pink but fewer matches for other colors since these colors domi-nate the color space. Even in cases with single color, there may be variations due to illumination changes Figure 3 shows an ex-ample, whereas even though the sampled region (shown in red) is perceived as a single color, multiple shades appear due to lighting variations (examples: shadows, attenuation of strength of illumi-nation over space). The histogram of maximum value of R, G, B shows a wide variation, instead of having a single peak. These sub-tle changes can be easily perceived by humans but are difficult for RGB based color models. Using HSV color space provides us with an added advantage as it mirrors the human perception of color, thereby providing a better way to handle illumination variations.
Another important aspect of color matching is color confusion which increases as saturation decreases. To understand this, let us Figure 7: We use a binning scheme for the Hue-Saturation-Value (HSV) Figure 8: Illustration of stacked 1D histogram for multi-colored dress: look at the mathematical formulae for hue, saturation and value channels of HSV space as shown below:
This formula lets us visualize HSV space in an alternative form as shown in Figure 7. Notice that for small values of V, all three values R, G, B will be similar. The same is true for small values of saturation, since maximum value and minimum values of R, G, and B will be similar. This means that, due to noise, each of R, G, and B may be dominant spuriously. Thus, hue will have discontinuous values (note the conditional assignment for hue). In other words, hue is not reliable when saturation is low. When saturation is low, the color will lack vividness and look grayish. To overcome this issue, we consider all pixels with saturation less than 6% as  X  X ray" pixels and bin them separately along with rest of the H, S,V com-ponents. Figure 8 shows an example of a stacked 1D histogram containing all the bins along with their respective weights.
Given an RGB swatch, we first convert it into HSV space and build a color histogram in each dimension. Since sampling in full 3D space as cross-product will create a sparse histogram matrix, we treat each channel (Hue, Saturation, Value) separately. Uniformly spaced bins along Hue( nH ), Saturation( nS ) and Value( nV ) axes are stacked to form 1D histograms. This produces relatively dense and more reliable histogram, which is much smaller than the full 3D sample scheme ( nH + nS + nV vs. nH  X  nS  X  nV ). This has an added advantage of having smaller memory requirement and faster matching. Weights that emphasize the Hue channel are ap-Figure 9: The figure shows our overall system overview. The indexing plied. For example, in our fashion inventory, 24 , 8 , 8 and 8 are chosen as the number of bins for H, S, V and Gray channels re-spectively. Their respective weights are set to 0 . 4 , 0 . 2 , 0 . 1 , 0 . 3 . Finally, we normalize the entire bin such that the ratio of color pix-els to gray pixels is encoded in the stacked histogram. A similar process is repeated for an input query image. To compute similari-ties between query and candidate histograms, Hellinger distance is utilized (Equation 2).
 where N is the number of bins, H 1 and H 2 represent the query image histogram and index image histogram respectively. Figure 9 outlines the overall system architecture of our proposed method.
Given the scale of images we deal with, and the fact that image query search results need to be returned in the order of millisec-onds, it is impractical to assume the backend algorithm can perform a flat search at runtime. With tens of millions of images to compare the query image against, it is necessary to design efficient retrieval techniques for online processing.
Indexing schemes have been heavily researched in a variety of different contexts such as traditional search engines. The scale at which we wish to admit image searches has grown rapidly in the past couple of years. Notice, in our context both the query and the result sets are images. We contrast the vocabulary and intent in a few kinds of search tasks:  X  Keyword-Document search: This is the most widely researched  X  Document-Document search: While in the case of retrieving  X  Keyword-Image search: The dictionary of query and results  X  Image-Image search: The challenges here are different from
Parallel this with traditional and very well-studied text search, where efficient techniques have been developed for retrieving rele-vant documents corresponding to keyword queries. Unfortunately, though, the same techniques do not directly adapt to image search given that their indexing schemes are largely text-based. In the following section, we describe a methodology that scales well for image search and retrieval.

We now present a backend clustering and indexing scheme that admits efficient image similarity retrieval for online queries. This helps us overcome the naive linear scan look-up and obtain a tun-able parameter that can trade-off time complexity without much loss in retrieval accuracy.

We begin by describing the simple clustering based approach that helps significantly speed up query run time complexity for im-age retrieval. The algorithm is based on computing and storing a backend k -center clustering, and then at query time. At run time, the query X  X  distance is computed to each of the k centers of the k clusters. Subsequently, the query is compared with all points in the cluster corresponding to the nearest cluster center, and the top matches are returned. While this approach has potentially several benefits, for the purposes of this paper, the focus is entirely on ob-taining nearly results as good as the naive approach, but with a significantly enhanced time complexity. Below we describe the no-tion of k -center clustering objective, a 2 -approximation algorithm (which is folklore) and present the proof and time complexity for completeness. This is followed by experimental validation of this method for our context. k -center objective . The goal of the k -center clustering algo-rithm, given a set S of n points in a metric space, is identify a set of k centers (and an allocation of each of the remaining points to its nearest center) in order to minimize the maximum diameter of the clusters. Specifically the diameter of a cluster is measured as the maximum inter-point distance, over all points in a cluster.
Let us use d ( a, b ) to denote the distance between any two points a and b . Mathematically stated, the goal is to find a set C of centers, with | C | = k , in order to minimize max c  X  C max s 1 ,s Here we slightly abuse notation to use c as a set as well (including all points in the cluster corresponding to this center). Optimizing this objective is a well-known NP-hard problem [12] but there is a very simple greedy algorithm that achieves a constant factor ap-proximation and runs in O ( nk ) time.

Algorithm description ( G REEDY -A LGORITHM ). The algorithm proceeds by greedily picking k centers as follows. The first center is picked arbitrarily, call this c 1 . The second center, c the farthest point from the remaining n  X  1 points. After, i centers have been picked, the next center c i +1 is picked as the point (from among the remaining ( n  X  i ) points) as the one farthest from the al-ready picked centers -here the distance of a point to a set of points (or a set of centers) is measured as the distance to the nearest of the points. Therefore, let C i denote the set of first i centers picked, the next center c i +1 is picked as arg max c i +1  X  S in this manner. Finally, the allocation of all n points to their respec-tive centers is done by picking the nearest center independently for each point.

T HEOREM 5.1 ([12]). G REEDY -A LGORITHM achieves a 2-factor approximation for the k -center objective.

P ROOF . Let D be the objective cost generated by the greedy al-the optimal set of cluster centers say C  X  , and let the associated objective cost be D  X  . Consider a hypothetical scenario where the algorithm generated ( k + 1) center points instead of k by the same greedy process, and call the last center c k +1 . Then by pigeonhole principle, at least two of c 1 , c 2 , ...., c k , c k +1 cluster in optimal clustering C  X  . Let these two centers be c Therefore, the optimal cost D  X  is at least d ( c i , c j that the distance from any point in S to its nearest center in C is at most d ( c k +1 , C ) because of the greedy process, which is at most d ( c i , c j ) since c k +1 was the last hypothetical center to be picked. Further, by triangle inequality (given it is a metric space), the di-ameter of any cluster in C is at most twice d ( c i , c j these two bounds, it follows that D  X  2  X  D  X  which completes the proof.

Retrieval. Once the k -center clustering has been pre-computed, given a query image, the retrieval algorithm is really simple: rather than compute the distance of the query to all n points, we compute the distance to each of the k centers. Subsequently, we pick the nearest center, and only consider points allocated to this center. The distance of the query is then computed to each point corresponding to this center X  X  cluster, and the top results are returned.
Time Complexity. The time complexity of the clustering phase is O ( nk 2 ) . This is easy to see as to choose i -th center, one consid-ers the distance of each of the ( n  X  i + 1) remaining points to the ( i  X  1) already picked centers. Therefore, the time complexity is of the order of n + 2 n + 3 n + .... + kn which is O ( nk
The time complexity of the retrieval phase varies depending on the size and distribution of all clusters, but is expected to be of the order of O ( k + n k ) . This is because the distance of the query point is first computed with each of the k centers. Then, assuming a roughly equal distribution of points to centers, the query point is subsequently compared with O ( n k ) points. Notice that for k  X   X  n , the query complexity is only O ( improvement over the naive query complexity of O ( n ) .

As a comment, the above approach is intuitive and may work well even when using a distance that is not necessarily a metric. The theoretical approximation guarantees however hold only for a metric space. Also one can incorporate several additional heuristics to improve performance, such as comparing with points in a small constant number of clusters rather than just one cluster (depending on the center distances). Further, it is possible to explore other clustering algorithms such as k -means. We pick k -center for its simplicity and as it scales efficiently to a large number of points, and present experimental results in the next section as a validation of its performance.
One of the main motivations of our work is to improve the search experience for online fashion shopping which is predominantly vi-sual in nature. For our experiments, we created a dataset of nearly Figure 10: Precision@k evaluation for different distance metrics at rank Figure 11: Two query examples where matched pattens found in the Top-4 1 million images by taking a set of random snapshots from a large e-commerce website. In this data collection, we focus only on 6 major categories for women X  X  clothing: Dresses, Tops &amp; Blouses, Coats &amp; Jackets, Skirts, Sweaters, and T-Shirts. For evaluating the system performance, we developed it as a mobile application and deployed it for 15 users who scanned 1600 query images over a pe-riod of 30 days. Since annotating such a large number of queries, each for 1 million images was infeasible, we randomly sampled 40 query images for our evaluation. 3 human judges were shown the top-10 matches for every query image and were asked to rate it on a scale of 0 -4 ( 0 marking non-relevant matches). Finally, judge-ments from all human judges was collated and average rating per query-match pair was recorded as the final relevance.

We use this human judgement to compare different distance met-rics for histogram matching. 4 distance metrics: Hellinger, Corre-lation, Intersection and Chi-Square were evaluated. Figure 10 il-lustrates the relative performance of each of these metrics and as shown, Hellinger Distance outperforms all other distance metrics which supports our use of this metric in our matching algorithm.
We also show how our proposed algorithm fares in retrieving fashion images containing distinct pattern. Figure 11 shows two such examples of plaid pattern as query and top-4 results returned by our system. As shown, even without any explicit pattern infor-mation encoded, our algorithm is able to retrieve matching patterns from the inventory.
In this section, we describe our experimental evaluation of k -center based indexing technique. For a large scale inventory, flat Figure 12: Plot of % overlap between indexed and non-indexed results Figure 13: Plot of speedup distribution using indexed approach for matching across millions of images can be prohibitively expensive. Our results in this section show that the suggested indexing and retrieval approach not only provides substantial speedups over flat matching but also ensures high overlap with results from flat match-ing approach. Due to space constraints, we only outline a sampling of the plots here.

Figure 12 and 13 show % overlap with flat matching results and speedup against flat matching results respectively when using k -center based indexing strategy for cluster size k = 50 . As can be seen, our proposed approach obtains a minimum of 10 x speedup for all queries in our database while maintaining an average overlap of around 60% for top-20 matches.

We also study the effect of varying the cluster size. Figure 14 and 15 illustrate the trade-off that can be obtained between the speedup and overlap ratio as k scales. Specifically, we observe a linear speedup in matching time at almost no performance degra-dation.
Though our system is primarily designed for matching soft goods such as fashion, we also experimented to benchmark its generaliza-tion to other commerce categories containing rigid goods such as Camera, Toys and Sports. Examples of this dataset is shown in Figure 17. For each query image, approximately 15 true match-ing images are collected. 5 are transformed versions of the query Figure 14: Plot of median % overlap between indexed and non-indexed Figure 15: Plot of median speedup using indexed results for varying clus-image with randomly chosen transformation parameters for Gaus-sian Blur, Perspective Distortion, Rotation etc. Figure 18 shows the sizes of each categories of this dataset.

For comparison, we baseline our results with State-of-the-art de-scriptors SIFT [2] and SURF [18]. Figure 19 shows the perfor-mance of each category in the dataset, where our method outper-forms both SIFT and SURF for all three categories in terms of Mean Average Precision (mAP) [21].
 Figure 16 and Figure 20 show examples of matching results for Toys, Camera and Sports respectively. Figure 16(a) and Figure 20(b) show that our system successfully retrieves images with dif-ferent transformations, such as shape, view angle and blurring. Be-sides, as shown in (b) of both figures, due to the center spatial prior, our algorithm successfully avoids the noisy information either from the complex background or from the image headers, which may be commonly seen in online shopping images.
We further extend our quantitative evaluation to a more gen-eral INRIA Holidays dataset. This dataset [16] consists of images from different categories such as natural scenes, monuments, build-ings with varying resolutions and perspective. Each category con-tains 500 unique images where the first image is used as query and the rest as matching. We compared our results with the state-of-art bag-of-words (BOF) based method [9]. Figure 21 shows the comparison between our proposed algorithm and the BoW base-line algorithm. We show the baseline performance for two dif-ferent vocabulary sizes: k = 2000 (BOF2000) and k = 20000 (BOF20000) that were reported in [16]. For the proposed algo-rithm we discard the spatial prior and use all pixels for computing the histogram since the dataset does not contain any strong spa-tial prior. We also experiment with different number of bins and set the size of Hue histogram to nH = 36 . Weights are also ad-justed accordingly to be 0 . 5 , 0 . 25 , 0 . 15 , 0 . 1 for H, S, V for color Figure 17: Examples of one collection of the online commerce data that pixels and V for gray pixels, respectively. This is done to account for larger color variation obtained by sampling from the whole im-age instead of center. The mAP result of our algorithm is denoted as color-A in Figure 21. To utilize spatial information, we further divide each image into 5 patches (left, right, top, bottom, center) and compute their individual similarity. These similarities are av-eraged to generate a final matching score. The mAP result of this extension is denoted as color-B in Figure 21. Note that by only using 60 features( 36 + 8 + 8 + 8 = 60 ), our algorithm outperforms the baseline result BOF2000. By dividing the entire image into sev-eral small patches to include some spatial information, our result (color-B) even outperforms the result by using 200000 visual words (BOF200000).
Using 24 GB RAM, Intel Xeon E 5630 , 2 . 53 GHz, the average feature extraction costs 10 ms per image, and retrieval about 80 ms, (i.e. average time required for matching the a query image with all the images in the 1 million online commerce image dataset). Each feature vector consists of 49 float values totaling to 196 bytes of memory. Matching server for 1 million items takes 190 MB of RAM to load feature indices and return query results in real-time. Figure 18: Data distribution of one collection of the online commerce Figure 19: mAP (Mean Average Precision) comparison between the pro-Figure 22 shows the average time used for indexing and matching each image for different categories. Our speed metrics show that our system is at par with the leading online search engines. Fur-thermore, our low memory footprint indicates the scalability of the system to a larger number of categories.
In this paper, we presented a color based visual search engine for fashion. Text-based search has been extensively studied in the lit-erature over the last two decades. As the web evolves from text to more sophisticated content including billions of images, it is cru-cial to re-evaluate and design new systems robust enough to deal with modern applications such as image search. Take for example the application of searching for clothing items in an online search engine. The task at hand is of a visual nature where users would like or even expect engines to be able to return results from their inventory very similar to a dress that they have a picture to. This happens often when someone takes a picture of a dress from a cell phone, either of another person, or in a store, and would like to be able to look up prices or stock availability from online stores im-mediately in seconds. We design a massive system specific to this application and introduce techniques of computer vision that may go beyond this domain. Our system is live and open to use by any-one under the eBay Fashion App and leverages the eBay inventory using techniques presented in this paper.

Our algorithm is motivated from insights into real-world fashion data that enables us to design simple yet efficient object localization and color matching technique. Our main findings from this work are as follows:  X  Spatial priors can be extremely useful in localizing objects in  X  By using only color information, we can also get a reasonable Figure 20: Camera and Sports examples of retrieval result for online Figure 21: mAP (Mean Average Precision) comparison for the proposed  X  It is important to separate the chromatic (color) content of the  X  We present an indexing scheme that is particularly suited for Using these insights as designing principals of our visual search en-gine, we compare the results of our system to other approaches. We Figure 22: Averaged computing time for each image used for online com-also illustrate successful matching results for distinctive clothing patterns. Furthermore, we baseline our performance with state-of-art image retrieval methods such as SIFT, SURF and Bag-of-Words techniques and illustrate our low computational cost advantages.
Our future work focuses on extending the proposed method to incorporate non-uniform bins and explore tf-idf based vector space models for matching. It would also be very interesting to design and implement image based search applications for other genres beyond fashion and understand the technical challenges specific to these domains.
