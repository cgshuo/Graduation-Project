 Evgeniy Gabrilovich gabr@cs.technion.ac.il Shaul Markovitch shaulm@cs.technion.ac.il Text categorization deals with assigning category la-bels to natural language documents. Categories come from a fixed set of labels, and each document may be assigned one or more categories. The absolute major-ity of works in the field employ the so-called  X  X ag of words X  approach and use plain language words as fea-tures (Sebastiani, 2002). Using a bag of words usually leads to an explosion in the number of features, so that even moderately-sized test collections often have thou-sands or even tens of thousands of features. In such high-dimensional spaces, feature selection (FS) is often necessary to reduce noise and avoid overfitting. Prior studies found support vector machines (SVM) and K -Nearest Neighbor (KNN) to be the best performing al-gorithms for text categorization (Dumais et al., 1998; Yang &amp; Liu, 1999).
 Joachims (1998) found that support vector machines are very robust even in the presence of numerous fea-tures, and further observed that the multitude of text features are indeed useful for text categorization. To substantiate this claim, Joachims used a Naive Bayes classifier with feature sets of increasing size, where fea-tures were first ordered by their discriminative capac-ity (using the information gain criterion), and then the most informative features were removed . The clas-sifier trained on the remaining  X  X ow-utility X  features performed markedly better than random labeling of documents with categories, thus implying that all fea-tures are relevant and should be used. These find-ings were later corroborated in more recent studies (Brank et al., 2002; Rogati &amp; Yang, 2002) that ob-served either no improvement or even small degrada-tion of SVM performance after feature selection. On the 20 Newsgroups collection (Lang, 1995), which is one of the standard text categorization datasets, fea-ture selection significantly degrades the accuracy of SVM classification (Bekkerman, 2003) due to a very large and diversified vocabulary of newsgroup post-ings. Consequently, many later works using SVMs did not perform any feature selection at all (Leopold &amp; Kindermann, 2002; Lewis et al., 2004).
 In this paper we describe a class of text categorization problems that are characterized by many redundant features. The corresponding datasets were collected in the course of our prior work (Davidov et al., 2004), where we proposed a methodology for parameterized generation of labeled datasets for text categorization based on the Open Directory Project (ODP). In our present work we use a subset of 100 datasets whose categorization difficulty (as measured by baseline SVM accuracy) is evenly distributed from very easy to very hard. We observed that even though the datasets dif-fer significantly in their difficulty, many of them are comprised of categories that can be told apart using a small number of words. For example, consider dis-tinguishing the documents about Boulder, Colorado, from those about Dallas, Texas. A few proper names of local landmarks and a handful of words describing local industries and other peculiarities often suffice to distinguish texts about the two cities. Given these discriminators, other words add little differentiation power, and are therefore redundant . As we show in Section 3, support vector machines X  X hich are usu-ally quite robust in the presence of many features X  X o not fare well when a few good discriminators are vastly outnumbered by features with little additional differ-entiation power .
 We further demonstrate that on such datasets C4.5 significantly outperforms SVM and KNN, although the latter are usually considered substantially supe-rior text classifiers. When no feature selection is per-formed, C4.5 constructs small decision trees that cap-ture the concept much better then either SVM or KNN. Surprisingly, even when feature selection is op-timized for each classifier, C4.5 formulates a powerful classification model, significantly superior to that of KNN and only marginally less capable than that of SVM. We also show the crucial importance of aggres-sive feature selection for this class of problems on a different document representation. In this experiment we extend the conventional bag of words with features constructed using the WordNet electronic dictionary by generalizing original words; again, SVM perfor-mance steadily increases as fewer features are selected. To account for this phenomenon, we developed a novel measure that predicts feature redundancy in datasets. This measure analyzes the distribution of features by their information gain, and reliably predicts whether feature selection will be beneficial or harmful for a given dataset. Notably, computation of this measure does not require to actually build a classifier, nor to invoke it on a validation set to determine an optimal feature selection level.
 The main contributions of this paper are threefold. First, we describe a class of text categorization prob-lems that have many redundant features, and for which aggressive feature selection is essential to achieve de-cent level of SVM performance. The existence of such class of problems is in contrast to most of prior re-search in text categorization, which found the major-ity of features (except the rarest ones) to be relevant, and specifically beneficial for SVM classification. Sec-ond, we use two different feature sets to show that without an aggressive feature selection, SVM classifi-cation is substantially inferior to that of C4.5, which was previously shown to be a less capable text clas-sifier. Finally, we develop a measure that, given a dataset, predicts whether feature selection would be beneficial for it. This measure performs outlier detec-tion in the distribution of features by information gain, without actually classifying the documents. We conducted a series of experiments to explore the utility of feature selection for datasets plagued with re-dundant features. In what follows, we first describe the construction of the datasets used in the experiments, and then proceed to developing a measure that pre-dicts the utility of feature selection for a given dataset. 2.1. Datasets Acquiring datasets for text categorization based on Web directories has been often performed in prior studies, which used Yahoo! (Mladenic &amp; Grobelnik, 1998), ODP (Chakrabarti et al., 2002; Cohen et al., 2002) and the Hoover X  X  Online company database (Yang et al., 2002). This approach allows to elimi-nate the huge manual effort required to actually label the documents, by first selecting a number of cate-gories (= directory nodes) to define the labels, and then collecting the documents from the subtrees rooted at these categories to populate the dataset. In our prior work (Davidov et al., 2004) we devel-oped a methodology for automatically acquiring la-beled datasets for text categorization from hierarchi-cal directories of documents, and implemented a sys-tem that performed such acquisition based on the Open Directory Project ( http://dmoz.org ). In the present paper we use a subset of 100 datasets acquired using this methodology. Each dataset consists of a pair of ODP categories with an average of 150 docu-ments, and corresponds to a binary classification task of telling these two categories apart (documents are single-labeled, that is, every document belongs to ex-actly one category). When generating datasets from Web directories, where each category contains links to actual Internet sites, we construct text documents representative of those sites. Following the scheme in-troduced by Yang et al. (2002), each link cataloged in the ODP is used to obtain a small representative sample of the target Web site. To this end, we crawl the target site in BFS order, starting from the URL listed in the directory. A predefined number of Web pages (5 in this work) are downloaded, and concate-nated into a synthetic document , which is then filtered to remove HTML markup; the average document size after filtering is 11.2 Kilobytes.
 The datasets vary significantly by their difficulty for text categorization, and baseline SVM accuracy obtained on them is nearly uniformly distributed between 0.6 and 0.92. To list a few examples, datasets in our collection range from easy ones containing .
 The full collection of 100 datasets, along with additional statistics and all the raw data used in our experiments is available at http://techtc.cs.technion.ac.il/techtc100 . 2.2. Predicting the utility of feature selection In Section 3 we show that the majority of datasets we used in this study benefit greatly from aggressive feature selection. We conjectured that these datasets have a small number of features that together allow to learn the underlying concept concisely, while the rest of the features do more harm than good. To under-stand this phenomenon, we examined the distribution of features in each dataset by their information gain. Figure 1 shows this distribution for several sample with feature distribution similar to Dataset 46 ben-efit from feature selection immensely (for this particu-lar dataset, aggressive feature selection improved SVM accuracy from 0.60 to 0.93). Such datasets have sev-eral features with high information gain, while the rest of their features have markedly lower IG scores. In contrast to these, datasets similar to Dataset 1 are characterized with smooth spectrum of IG values X  X n such cases feature selection will often eliminate fea-tures that carry essential information; indeed, for this dataset feature selection caused SVM accuracy to drop from 0.86 to 0.74. For comparison, we show a similarly looking graph for the 20 Newsgroups (20NG) dataset, which is often used for text categorization experiments and for which feature selection was found particularly harmful (Bekkerman, 2003).
 Interestingly, high IG values of best-scoring features do not necessarily imply that feature selection will substantially improve the accuracy. For instance, Dataset 31 has several features with very high infor-mation gain, but its IG graph declines gracefully over subsequent features, and does not fall as sharp as for Dataset 46. Consequently, feature selection only im-proves SVM accuracy from 0.92 to 0.95 X  X  much more modest gain than for Dataset 46. On the other hand, Dataset 61 has somewhat lower initial IG values, but its IG graph declines very sharply. Feature selection was shown to be of high utility for this dataset as well, boosting the accuracy from 0.64 to 0.84.
 The above results imply that the absolute values of in-formation gain are of less importance than the speed of decline of IG values across features. To quantify this phenomenon, we need to assess the number of out-liers  X  X eatures whose information gain is highly above that of all other features. Under this definition the desired measure becomes easy to formulate. We first compute the information gain for all features, and then count the number of features whose information gain is higher than 3 standard deviations above the aver-age. Although the underlying distribution cannot be assumed to be normal, this familiar statistical crite-rion works very reliably in practice. Formally, let D be a dataset and let F be a set of its features. We define the Outlier Count (OC) as
OC ( D , F ) = |{ f  X  X  : IG ( f ) &gt;  X  IG + 3  X   X  IG }| , where  X  IG and  X  IG are the average and standard de-viation of information gain of the features in F . In Section 3 we show that Outlier Count reliably predicts the utility of feature selection for a variety of datasets. 2.3. Extended feature set based on WordNet Several studies in text categorization performed fea-ture construction using the WordNet electronic dic-tionary (Fellbaum, 1998). In this work we show that aggressive feature selection can significantly improve categorization accuracy for document representation extended with constructed features.
 Scott and Matwin (1999), and later Wermter and Hung (2002), used WordNet to change document representa-tion from a bag of words to a bag of synsets (WordNet notion of concepts), by using the hypernymy relation to generalize word senses. Since many words are not found in WordNet (e.g., neologisms, narrow technical terms, and proper names), we opted for extending a bag of words with WordNet-based features rather than completely changing document representation to a bag of synsets. To this end, we first perform feature gener-ation by generalizing document words using WordNet, and then decimate the generated features through fea-ture selection. In Section 3.4 we demonstrate that fea-ture selection is as important for generated features as it is for regular features (plain language words). 2.4. Feature selection algorithms A variety of feature selection techniques have been tested for text categorization, while Information Gain, gati &amp; Yang, 2002), Bi-Normal Separation (Forman, 2003) and Odds Ratio (Mladenic, 1998) were reported to be the most effective. Adopting the probabilistic notation from Sebastiani (2002), we use P ( t k , c i ) to denote the joint probability that a random document contains term t k and belongs to category c i , and N to denote the number of training documents. The above feature selection techniques are then defined as follows: 1. Information Gain (IG): 3. Document Frequency (DF): N  X  P ( t k ) 4. Bi-Normal Separation (BNS): 6. Random (RND) Actual feature selection is performed by selecting the top scoring features, using either a predefined thresh-old on the feature score or a fixed percentage of all the features available. In addition to these  X  X rincipled X  se-lection schemes, we unconditionally remove stop words and words occurring in less than three documents. 2.5. Classification algorithms and measures We used the datasets described in Section 2.1 to compare the performance of Support Vector Machines (Vapnik, 1995), C4.5 (Quinlan, 1993), and K -Nearest Neighbor (Duda &amp; Hart, 1973). In this work we used the SV M light implementation (Joachims, 1999) with We used classification accuracy as a measure of text categorization performance. Studies in text catego-rization usually work with multi-labeled datasets in which each category has much fewer positive examples than negative ones. In order to adequately reflect cat-egorization performance in such cases, other measures of performance are conventionally used (Sebastiani, 2002), including precision, recall, F 1 , and precision-recall break-even point (BEP). However, for single-labeled datasets all these measures can be proved to be equal to accuracy, which is the measure of choice in the machine learning community. In this section we evaluate the role of feature selec-tion for several classification algorithms operating on datasets with many redundant features. We conducted the experiments using a text categorization platform of our own design and development called H ogwarts . All accuracy values reported below were obtained us-ing 4-fold cross-validation scheme.
 When working with support vector machines, it is es-sential to perform adequate parameter tuning. In the case of a linear kernel (and under the assumption of equal cost of errors on positive and negative exam-ples), the only relevant parameter is C , namely, the trade-off between training error and margin. To op-timize this parameter, we set aside one fold of the training data as a validation set, and for each feature selection level selected the best C value from among 3.1. Validation of H ogwarts performance In this section we demonstrate that the results of clas-sifying existing datasets with H ogwarts are consis-tent with those in other published studies. Figure 2 shows the results of using SVM in conjunction with IG feature selection to classify three datasets frequently used in text categorization studies: 10 largest cat-egories of Reuters-21578 (Reuters, 1997), 20 News-groups (Lang, 1995), and Movie Reviews (Pang et al., 2002). 3 Using all features, H ogwarts achieved BEP of 0.922 on Reuters, 0.854 on 20 Newsgroups and 0.818 on Movie Reviews. These results are very similar to the performance obtained by other researchers (all us-ing SVM). Dumais et al. (1998) achieved BEP of 0.92 for the 10 largest Reuters categories. Bekkerman (2003) obtained BEP of 0.856 on the 20 Newsgroups dataset. Pang et al. (2002) obtained accuracy of 0.829 on the Movie Reviews dataset.
 As can be seen in Figure 2, any level of feature selection harms the performance on all of these datasets. The graphs for  X  2 and BNS feature selection algorithms exhibit behavior very similar to IG, so we do not show them here owing to lack of space. Note that all the experiments reported in the rest of the paper use the 100 datasets we acquired as explained in Section 2.1. 3.2. Predicting the utility of feature selection We now show that the Outlier Count measure defined in Section 2.2 reliably predicts the utility of feature selection. Figure 3 shows the improvement in SVM accuracy at several feature selection levels versus the baseline accuracy obtained using 100% of features. As we can see, Outlier Count strongly correlates with the magnitude of improvement that can be obtained through feature selection. We observe that at lower values of Outlier Count aggressive feature selection is highly beneficial. Conversely, at higher OC values much more moderate (if any) feature selection should be performed, while aggressive selection causes degra-dation in accuracy. The next section examines the correlation of Outlier Count with the differences in performance between individual classifiers.
 The Outlier Count for the datasets we used is nearly uniformly distributed between 6 and 62, with a single outlier value (no pun intended!) of 112 for Dataset 1 (Figure 1), for which feature selection caused SVM ac-curacy to drop from 0.86 to 0.74. For other datasets frequently used for text categorization, Outlier Count for Reuters-21578 is 78, Movie Reviews X 154, and 20 Newsgroups X 391, which explains why feature se-lection does for them more harm than good.
 Based on these findings, we conclude that using Out-lier Count for ordering datasets reflects the degree to which a dataset can be concisely described by only a few features, while the rest of the features are pre-dominantly redundant and have detrimental effect on classification results. 3.3. Comparison of classifiers Figure 4 compares the performance of SVM, KNN and C4.5 on the 100 datasets ordered by Outlier Count. When no feature selection is employed, the perfor-mance of C4.5 mostly dominates that of SVM and KNN, and only declines in the rightmost part of the graph, which contains datasets where a few features are not sufficient for learning the concept. Table 1 shows classifier accuracy without feature se-lection and with the optimal feature selection level for each classifier. We used paired t-test to assess the sig-nificance of differences in classifier accuracy over the 100 datasets (see Table 2). Without any feature selec-tion, the differences between classifiers were found to be very significant at p &lt; 5  X  10  X  3 or lower. For individ-ual classifiers, the improvement in accuracy due to fea-3.4. The effect of using different feature sets Figure 5 compares the performance of classifiers at different feature selection levels (using Information Gain). As we can see, C4.5 performs better than SVM except for the most aggressive FS levels, where their accuracy becomes nearly equal. Interestingly, C4.5 stays high above KNN at most FS levels.
 Figure 6 presents a similar graph for the extended fea-ture set based on WordNet. Here we use all features of the conventional bag of words, and only apply fea-ture selection to the constructed features. C4.5 clearly manages the multitude of redundant features much better than both SVM and KNN. It is also noteworthy that the accuracy of SVM and KNN increases steadily as feature selection becomes more aggressive, while the improvement in their performance with 0.5% features When using the optimal FS level (0.5% for both reg-ular words and WordNet concepts), the addition of WordNet features is only responsible for a minor im-provement in SVM accuracy from 0.853 to 0.854. 3.5. The effect of using different FS algorithms Figures 7 and 8 show the effect of using different fea-ture selection algorithms (see Section 2.4) with SVM and C4.5. Consistently with prior studies (Forman, 2003; Rogati &amp; Yang, 2002), we observe that IG, CHI and BNS are the best performers, while the difference trast with prior studies, we observe that on the family of datasets we described, the best performance of SVM is obtained when only using a tiny fraction of features (0.5% for the three best FS techniques). 3.6. Testing the relevancy of features In previous sections we showed that text categorization can greatly benefit from aggressive feature selection. We now address the question whether the features dis-carded by selection are at all relevant for classification. Following Joachims (1998), we sorted all features by their information gain, and then removed progressively larger fractions (0.1%, 0.5%, 1%, . . . , 10%, 20%, . . . , 100%) of the most informative features. As can be seen in Figure 9, the performance of an SVM classifier trained on the remaining features is noticeably better than random up to very high levels of such harmful  X  X election X . These results corroborate earlier findings by Joachims (1998), and support our hypothesis that the features removed through selection are redundant , even though most of them may be considered relevant. Studies in text categorization usually represent docu-ments as a bag of words, and consequently have to manage feature spaces of very high dimensionality. Most previous works in the field found that these nu-merous features are relevant for classification, and that in particular the performance of SVM text categoriza-tion peaks when no feature selection is performed. We described a class of datasets plagued with redun-dant features, such that their elimination significantly boosts categorization accuracy of a host of classifiers. Specifically, we showed that when no feature selection is employed on such datasets, SVMs are significantly outperformed by C4.5. To explain this phenomenon, we analyzed the distribution of features by their in-formation gain, and observed that this effect occurs when a small number of features are sufficient for con-cisely learning the underlying concept. We defined a measure named Outlier Count that, for a given dataset and fixed representation scheme, estimates the amount of feature redundancy through outlier analysis. In a series of experiments, we demonstrated that Out-lier Count reliably predicts the amount of improve-ment that can be gained through feature selection. These findings are backed by empirical evidence both for the conventional bag of words, and for a represen-tation extended through feature generation based on WordNet. We further performed a controlled ablation study to verify that the redundant features are in fact relevant for classification. To this end, we removed progressively larger fractions of most informative fea-tures, and found the remaining ones to suffice for bet-ter than random performance. Finally, we analyzed several established benchmarks for text categorization with respect to Outlier Count, and explained why they do not benefit from feature selection.
 Following the established practice in text categoriza-tion, throughout this paper we used an SVM classifier with a linear kernel. In an ancillary experiment we sought to determine whether a non-linear SVM kernel may fare better than a linear one when dealing with numerous redundant features. Without feature selec-tion, switching from a linear kernel to an RBF one reduced the accuracy from 0.769 to 0.687. Even at the optimal feature selection level, the accuracy achieved with an RBF kernel was slightly below that of a linear one (0.849 vs. 0.853), contradicting our anticipation of better performance by a more sophisticated kernel. However, this experiment should be considered pre-liminary, and in our future work we plan to conduct a thorough investigation of the ability of non-linear SVM kernels to withstand high rates of redundant features. In a recent study, Forman (2003) proposed a novel fea-ture selection algorithm named Bi-Normal Separation, which improved the performance of SVM text cate-gorization on a range of datasets. Peak performance was obtained when using 500 X 1000 features (approx-imately 10% of all available features on the average). More aggressive feature selection led to sharp degrada-tion of the results X  X sing less than 100 features caused macro-F 1 to decrease by 5% X 10% depending on the se-lection algorithm used.
 Our work corroborates the findings that feature selec-tion can help text categorization with SVMs, and de-scribes a class of problems where the improvement due to feature selection is particularly large. We showed that for this class of problems the improvement in ac-curacy can be twice as high as found by Forman (2003) (namely, 8.4% vs. 4.2%), while optimal performance is achieved when using much fewer features (between 5 and 40, depending on the dataset). We also evaluated several feature selection algorithms on text categoriza-tion problems characterized with many redundant fea-tures. Our results support earlier findings that Infor-mation Gain, Bi-Normal Separation and  X  2 are the most powerful feature selection algorithms, while the differences between them are not significant. It should be noted that for all the datasets we used, the utility of feature selection could be established by setting aside part of the training data to serve as a validation set. Indeed, the high redundancy level was so pronounced, that the optimal selection level for the testing data could almost always be correctly deter-mined on the validation fold. However, we believe that the introduction of Outlier Count and the use of ablation experiments that systematically eliminate most informative features, allow deeper understanding of the issues of feature redundancy and relevancy. We thank Ran El-Yaniv for advice on SVM tuning. Bekkerman, R. (2003). Distributional clustering of words for text categorization. Master X  X  thesis, CS Department, Technion X  X srael Inst. of Technology. Brank, J., Grobelnik, M., Milic-Frayling, N., &amp;
Mladenic, D. (2002). Interaction of feature selection methods and linear classification models. Workshop on Text Learning held at ICML-2002 .
 Chakrabarti, S., Joshi, M. M., Punera, K., &amp; Pennock,
D. M. (2002). The structure of broad topics on the web. Proc. of the Int X  X  World Wide Web Conference . Cohen, D., Herscovici, M., Petruschka, Y., Maarek,
Y. S., Soffer, A., &amp; Newbold, D. (2002). Personal-ized pocket directories for mobile devices. Proc. of the Int X  X  World Wide Web Conference .
 Davidov, D., Gabrilovich, E., &amp; Markovitch, S. (2004).
Parameterized generation of labeled datasets for text categorization based on a hierarchical directory. To appear in SIGIR X 04 .
 Duda, R., &amp; Hart, P. (1973). Pattern classification and scene analysis . John Wiley and Sons.
 Dumais, S., Platt, J., Heckerman, D., &amp; Sahami, M. (1998). Inductive learning algorithms and represen-tations for text categorization. CIKM (pp. 148 X 155). Fellbaum, C. (Ed.). (1998). Wordnet: An electronic lexical database . MIT Press.
 Forman, G. (2003). An extensive empirical study of feature selection metrics for text classification. Jour-nal of Machine Learning Research , 3 , 1289 X 1305. Joachims, T. (1998). Text categorization with sup-port vector machines: Learning with many relevant features. ECML X 98 (pp. 137 X 142).
 Joachims, T. (1999). Making large-scale SVM learning practical. In B. Schoelkopf, C. Burges and A. Smola (Eds.), Advances in kernel methods  X  support vector learning . The MIT Press.
 Lang, K. (1995). Newsweeder: Learning to filter net-news. ICML X 95 (pp. 331 X 339).
 Leopold, E., &amp; Kindermann, J. (2002). Text catego-rization with support vector machines: How to rep-resent texts in input space. Machine Learning , 46 , 423 X 444.
 Lewis, D. D., Yang, Y., Rose, T., &amp; Li, F. (2004).
RCV1: A new benchmark collection for text cate-gorization research. JMLR , 5 , 361 X 397.
 Mladenic, D. (1998). Feature subset selection in text learning. ECML X 98 (pp. 95 X 100).
 Mladenic, D., &amp; Grobelnik, M. (1998). Word sequences as features in text-learning. Proc. of 7th Electrotech. and Comp. Sci. Conf. (pp. 145 X 148).
 Pang, B., Lee, L., &amp; Vaithyanathan, S. (2002).
Thumbs up? Sentiment classification using machine learning techniques. EMNLP X 02 (pp. 79 X 86).
 Quinlan, J. R. (1993). C4.5: Programs for machine learning . Morgan Kaufmann.
 Reuters (1997). Reuters-21578 text categoriza-tion test collection, Distribution 1.0 . Reuters. http://www.daviddlewis.com/resources/ testcollections/reuters21578 .
 Rogati, M., &amp; Yang, Y. (2002). High-performing fea-ture selection for text classification. CIKM X 02 (pp. 659 X 661).
 Scott, S., &amp; Matwin, S. (1999). Feature engineering for text classification. ICML X 99 (pp. 379 X 388). Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Comp. Surveys , 34 , 1 X 47. Vapnik, V. (1995). The nature of statistical learning theory . Springer-Verlag.
 Wermter, S., &amp; Hung, C. (2002). Selforganizing clas-sification on the reuters news corpus. COLING X 02 . Yang, Y., &amp; Liu, X. (1999). A re-examination of text categorization methods. SIGIR X 99 (pp. 42 X 49). Yang, Y., &amp; Pedersen, J. (1997). A comparative study on feature selection in text categorization. ICML X 97 (pp. 412 X 420).
 Yang, Y., Slattery, S., &amp; Ghani, R. (2002). A study of approaches to hypertext categorization. JIIS , 18 ,
