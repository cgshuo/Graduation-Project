 Sharon Wulff sharon.wulff@inf.ethz.ch Department of Computer Science, ETH Zurich, Switzerland Ruth Urner rurner@cs.uwaterloo.ca Shai Ben-David shai@cs.uwaterloo.ca Department of Computer Science, ETH Zurich, Switzerland For the sake of concreteness, we focus on the case where D = { 0 , 1 } . The proofs readily generalize to arbitrarily domain D .
 A.1. NP-hardness of K = L = 2 We construct a reduction from MaxCut . A cut in a graph G = ( V,E ) is a partition of the vertex set into V 1 and V 2 . The size of the cut is the number of edges in E that connect vertices from V 1 to vertices from V 2 . The decision version of MaxCut is defined as follows: Input A graph G = ( V,E ), an integer r .
 Question Is there a cut of G of size at least r ? The size of a cut can also be defined as the total num-ber of edges, | E | minus the edges within V 1 and the edges within V 2 . For a subset U  X  V of vertices and a vertex v  X  V let v [ U ] denote the number of neighbors of v in U . We can re-write the size of the cut as s ( V 1 ,V 2 ) := Thus maximizing the size of a cut is equivalent to min-imizing the cost of the cut c ( V 1 ,V 2 ) defined as The MaxCut question can be reformulated as: Is there a cut of G of cost at most r ? Given an instance G = ( V,E ), as well as a cost r , we construct an instance M , with a 2 , 2-MCBC cost | M | . The construction is shown in figure 3. We start by defining the  X  X eft half X  of the matrix M . For ev-ery vertex v  X  V we introduce n = | V | rows r v 1 ...r v and n columns c v 1 ...c v n . We set the entries of M cor-responding to rows and columns of the same vertex (the  X  X iagonal blocks X ), to 1, i.e.  X  v, 1  X  i,j  X  n : M [ r v i ,c v j ] = 1. Let V = { v 1 ...v n } be an ordering of the vertices of G , if vertices v i and v j of G are con-nected by an edge, we set the entry M [ r v i j ,c M [ r v j i ,c v i j ] to 0. All other entries of the left half are set to ? .
 The  X  X ight side X  of M is an n 2  X  n 2 matrix as well, where the diagonal n  X  n blocks are set to 0 and the rest to ? . More formally, we introduce another set of n columns 0 v 1 ,... 0 v n for each vertex v  X  V . We set  X  v, 1  X  i,j  X  n : M [ r v i , 0 v j ] = 0. The remaining right half of M is set to ? . We refer to these columns as the 0-columns of a vertex v , and use O v for the set 1 ,... 0 1 ...c (note that while the 0 columns contain only 0 and ? entries, the 1-columns consist mostly of 1 and ? , but also contain a few 0 entries, corresponding to edges of the graph). Finally we use R v to refer to the set of rows { r v 1 ...r v n } associated with vertex v . The NP-hardness of 2 , 2-MCBC now follows directly from the NP-hardness of MaxCut and the following lemma.
 Lemma A.1. G has a cut of cost at most r if and only if M has a 2 , 2 -bi-clustering of monochromatic cost at Proof. We first show that a cut of cost at most r in-duces a solution of the 2 , 2-bi-clustering of cost 2 r | M | Let V 1 ,V 2 be a cut of G of cost at most r . We define a partition P R = { R 1 ,R 2 } of the rows and P C = { C 1 ,C 2 } of the columns as follows: For all v  X  V 1 we put R v in R 1 , C v in C 1 and O v in C 2 . For all v  X  V 2 we do the opposite, put R v in R 2 , C v in C 2 and O v in C 1 . The partition is depicted in figure 4. This results in a 2  X  2 bi-clustering with a majority of 1 in the diagonal blocks (upper left and lower right) and a majority of 0 is the off-diagonal ones (upper right and lower left). The off-diagonal blocks consist of only 0 and ? entries, such that the monochromatic cost due to these blocks is 0. The diagonal blocks consist of: n 2 | V 1-entries, and remaining entries are ? . Clearly there is a majority of 1 in these blocks, such that their total monochromatic cost amounts to exactly 2 c ( V 1 ,V 2 ) = 2 r (see (4)). Nor-malizing by the size of the matrix yields the required cost.
 Next we show that, if there is a bi-clustering of M with cost at most r , then there is cut of G of size 2 . Note that if there is any bi-clustering solution of cost at most r , then the optimal one has cost at most r , thus we consider a bi-clustering of optimal cost. For the sake of the proof we start by considering an optimal n 2 , 2-bi-clustering, namely a bi-clustering where every row is a set of the row partition (thus, we are only interested in the resulting 2-partition of the columns). We will then argue, that this optimal n , 2-bi-clustering is actually a 2 , 2-bi-clustering and therefore also the optimal 2 , 2-bi-clustering. We can assume that in an optimal solution identical columns are in the same cluster. Similarly, we can assume that two columns that are  X  X nverse X  of each other (one can be obtained from the other by replacing each 0 with a 1 and each 1 with a 0) are in different clusters. Thus, for any v , all columns in O v are in one cluster. Consider a vertex v and its corresponding set of rows R v . Without loss of generality let us assume that the 0-columns of v are in C 2 . Each of the rows r ,...,r v n contains n entries 1 and at most one entry 0 in the left half. The row r v 1 contains no 0. Equally the column c v 1 contains no 0 entries. Therefore, we can assume that c v 1 is in C 1 (it is the inverse of the columns in O v ). By way of contradiction, assume that not all columns of C v are in C 1 , say c v i is in C 2 for some i  X  2. As at least c v 1 is in C 1 , all columns in O v are in C the rows r v 1 ,...,r v n contain at most one 0 entry in the left half, we can assume that all these rows have a 1-block for C 1 and a 0-block for C 2 . Having a column c i in C 2 incurs a cost of at least n by its 1 entries, however moving it into C 1 can incur a cost of at most 1 as the column has at most one 0 entry and all its 1 entries do not contribute to the cost anymore. Thus, all columns in C v are in C 1 .
 We showed that for every vertex, all its 0 columns are in one cluster and all its 1-columns are in the other cluster (which group is in which cluster may vary). Every row therefore has a block pattern 1 , 0 or 0 , 1, and the only cost that it incurs per row is that of a 0 entry of the left half, which ended up in the 1-block of the row. Now, grouping all the rows with pattern 1 , 0 into R 1 and all the rows with pattern 0 , 1 in R 2 , leads to a 2 , 2-bi-clustering of the same cost. As this cost is optimal for an n, 2-bi-clustering, it is also optimal for 2 , 2-bi-clustering (if there was a 2 , 2-solution of lower cost, separating the rows into singleton sets for an n, 2-bi-clustering would lead to a lower cost solution for this as well).
 If we set V 1 to be the vertices whose 1-columns are in C 1 and V 2 the vertices whose 1-columns are in C 2 we obtain a cut. The only matrix entries that contribute to the cost of the bi-clustering are the 0 coming from edges within one of these sets.
 A.2. NP-hardness for larger K,L -MCBC Proof outline: We prove the claim by reducing the 2 , 2-MCBC problem to the K,L -MCBC problem. Given K and L as in the theorem (without loss of generality, we assume K  X  2 L  X  1 here), and an input matrix M to the 2 , 2-MCBC problem, we construct another matrix N such that an optimal K,L -MCBC partitioning of N will induce an optimal 2 , 2-MCBC partitioning of M .
 Let m and n be the number of rows and columns of M , respectively. The matrix N will consist of ( K  X  1)  X  ( L  X  1) blocks, each of size m  X  n . The top left corner block of N will be the input matrix M . All other blocks will be either all-zero matrices or all-one matrices. All the blocks in the top row of blocks, except the left-most block, will be all-zero ma-trices. The blocks indexed ( i, 1), for 2  X  i  X  2 L  X  2  X  (and i  X  K  X  1) (blocks that reside below the top left corner block) will also be all-zero matrices. And the blocks indexed ( i, 1), for 2 L  X  2  X  i  X  K  X  1 (also re-siding below the top left corner block) will be all-one matrices. Finally, for every 2  X  i  X  2 L  X  2 let v i vectors in { 0 , 1 } L  X  2 such that for all i 6 = j , v and none is the all-zero vector (i.e. let the set of the v be all vectors in { 0 , 1 } L  X  2 except the all-zero). Now let the ( i,j ) block, for 2  X  i  X  2 L  X  2 (and i  X  K  X  1) and 2  X  j  X  L  X  1, be a homogeneous matrix all of whose entries equal the j  X  1 entry of the vector v Finally, set the entries of the (2 L  X  2 + i,j ) block equal to those of the ( i,j ) block for all 1  X  i  X  2 L  X  2  X  (and i  X  K  X  1) and 2  X  j  X  L  X  1.
 It is easy to see that the optimal K,L -MCBC partition of N induces an optimal 2 , 2-MCBC partition over M . A.3. NP-hardness for matrices with arbitrary Given an &gt; 0, and K and L satisfying the condition of theorem 4.1. We show that K + 1 ,L + 1-MCBC is NP-hard restricted to input matrices containing at most an fraction of ? -entries by a reduction from general K,L -MCBC. Given a matrix M , we construct matrix N as follows: We add | M | 1 rows and columns to M such that the upper left block of N is identical to M . We set the entries of the upper right and lower left blocks to 1 and the entries to the lower right block to 0. Now N has at most an -fraction of ? entries. Further, it is easy to see that the optimal K + 1 ,L + 1-MCBC partition of N induces an optimal K,L -MCBC solution to M .
 B.1. Proof of theorem 5.2 To prove theorem 5.2 we first prove the following Theorem B.1. On input M , A , K,L , , X  , with prob-ability at least 1 - X  the monochromatic approximation algorithm (given in Algorithm 1), lines (4)  X  (11) out-puts a partition P of M such that Mon A ( M ,P )  X  OPT A + 4 where OPT A is the minimal monochro-matic cost with respect to A .
 Proof. We start by analyzing the partition of the columns by algorithm 1. Let P ? R = { R ? 1 ,...R ? K } de-note the optimal partition of the rows of M (with respect to pattern A ). We say that a sample R S  X  R is good , if there exists a partition P S R = { R S 1 ,...R of R S such that, for all columns j  X  C , except for at most | C | columns, the following holds: Namely, that in every column block l , the difference in the number of errors between the placement of the column j in the l  X  X h block under the optimal partition of the rows, and the partition P S R of the sample of rows, is bounded by .
 Let R S be a good sample of the rows, and let P S R = { R S 1 ,...R S K } be the partition of the sample for which all but a fraction of of the columns in C satisfy 5. Consider a column j  X  C , 1. If j satisfies 5, then placing j in a greedy man-2. If j does not satisfy 5, we can still bound the Altogether the number of errors incurred by the parti-tion of the columns is bounded by 2 mn . We can carry out the same analysis only for the partition of the rows, assuming we have a good sample of the columns. The overall increase compared to the optimal solution is then bounded by 4 mn .
 We defined the monochromatic pattern cost (2) as a fraction of the number of mistakes made by the par-tition divided by the size of the matrix ( mn ), we can therefore conclude that the algorithm yields a solution which is at most OPT+4 .
 Now it suffices to show the following.
 Lemma B.2. With probability at least 1 - X  over the random sampling, the rows and columns samples picked by the algorithm are good w.r.t the the optimal solution.
 Proof. As before we use R S to denote a sample of the rows of size t , let P ?S R denote the restriction of the optimal partition P ? R to the sample R S , that is We can focus our analysis on this specific partition of the sample of rows since the approximation algorithm is going over all possible partitions, and is therefore guaranteed to consider this one.
 Let j  X  C be a column, we define an indicator random variable  X  l i for each row index i  X  R S and column block index l  X  [ L ] in the following way  X  Where for a row index i , P ?S R ( i ) denotes its row block assignment in the partition P ?S R . The variable  X  simply the error associated with the i entry in the column j , if we place it in the column block l , given that the partition of the rows is given by P ?S R and the target pattern is A .
 It is easy to see that the random variable correspond-ing to the sum over  X  l i , for all i in the sample R S , is simply the error function defined in (5.1). Using Chernoff additive bound we can guarantee that with a sample size of t = 1 2 2 log L  X  equation (5) holds for a certain column j and column block l with prob-ability at least 1  X   X  L :
Pr ( k Err ( j,l | A ,P ? R )  X  Err ( j,l | A ,P S R ) k &gt; ) We apply markov inequality to get that for all columns j  X  C except for not more than | C | , for a specific column block j equation (5) holds with probability at least 1  X   X  L . Finally we get a guarantee of (5) for all blocks l with probability at least 1  X   X  .
 The same analysis applies for the approximated par-tition of the rows determined by a sufficiently large sample of the columns.
 Proof of theorem 5.2 Proof. According to theorem B.1, the monochro-matic approximation algorithm for a given pattern A , computes a partition P such that Mon A ( M ,P )  X  OPT A +4 . This in turn translates into a bound on the agreement of 1  X  Mon A ( M ,P )  X  OPT A  X  4 . Since the algorithm goes over all possible patterns, and fi-nally picks the pattern and partition with the lowest overall cost, the optimal pattern will be considered as well and thus the returned partition is guaranteed to have a cost  X  OPT + 4 or agreement  X  OPT  X  4 .
 The run time increase due to the iteration over the pat-terns is exponential in K,L but is constant in | M | . B.2. Proof Of Corollary 5.3 Proof. There is always a trivial solution to the monochromatic bi-clustering problem with an agree-ment score of at least 1 2 . This is simply assigning all of the rows and all of the columns to the same cluster. Note that the presence of missing entires im-plies that the agreement score of this trivial solution is even strictly larger than 1 2 (see definition 1). Therefore an additive 4 approximation translates into a relative (1  X  ) OPT bound on the agreement of the solution, with a fixed increase of the sample size and therefore the running time. 1  X  Mon ( M ,P )  X  OPT  X  4 = OPT (1  X  4 OPT )
