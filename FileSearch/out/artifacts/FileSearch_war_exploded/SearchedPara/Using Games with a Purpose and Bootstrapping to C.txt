 Sentiment detection analyzes the positive or negative polar-ity of text. The field has received considerable attention in recent years, since it plays an important role in providing means to assess user opinions regarding an organization X  X  products, services, or actions.

Approaches towards sentiment detection include machine learning techniques as well as computationally less expen-sive methods. Both approaches rely on the use of language-specific sentiment lexicons, which are lists of sentiment terms with their corresponding sentiment value. The effort in-volved in creating, customizing, and extending sentiment lexicons is considerable, particularly if less common lan-guages and domains are targeted without access to appro-priate language resources.

This paper proposes a semi-automatic approach for the creation of sentiment lexicons which assigns sentiment val-ues to sentiment terms via crowd-sourcing. Furthermore, it introduces a bootstrapping process operating on unlabeled domain documents to extend the created lexicons, and to customize them according to the particular use case. This process considers sentiment terms as well as sentiment in-dicators occurring in the discourse surrounding a particular topic. Such indicators are associated with a positive or nega-tive context in a particular domain, but might have a neutral connotation in other domains.

A formal evaluation shows that bootstrapping consider-ably improves the method X  X  recall. Automatically created lexicons yield a performance comparable to professionally created language resources such as the General Inquirer. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing -Linguistic Processing; H.5.3 [ Group and Organization Interfaces ]: Collabora-tive Computing Algorithms, Languages, Performance Sentiment Detection, Bootstrapping, Language Resources, Sentiment Lexicon, Crowd-Sourcing
Sentiment detection has attracted a lot of research interest in recent years. With the emergence of freely available opin-ions on the Web the need for efficient methods to interpret these opinions has arisen. Automated sentiment detection is capable of accomplishing this task. It facilitates means of large-scale investigation previously unmanageable for hu-mans, such as tracking political campaigns on the Web or market research in forums or blogs. Reliable sentiment de-tection is heavily dependent on the comprehensiveness and accuracy of the underlying a-priory knowledge, in most cases a so-called sentiment lexicon. This lexicon contains opin-ionated terms and is usually manually compiled. The oc-currence of these terms in a document serves as indicator for  X  X ositiveness X  or  X  X egativeness X  of a document. Manu-ally compiling sentiment lexicons can be cumbersome and such lexicons may lack comprehensiveness, especially in the case of less-spoken languages. The presented method com-bines a crowd-sourcing technique, which is used for creat-ing an initial sentiment lexicon, with a bootstrapping ap-proach that automatically expands sentiment lexicons with additional terms. As input serves an unlabeled text corpus, from which a labeled corpus is iteratively extracted. Based on this labeled corpus, previously unknown sentiment terms are extracted and added to the initial lexicon.

The remainder of this paper is structured as follows: Sec-tion 2 gives an overview of related work, followed by a de-scription of the proposed method in Section 3. Section 4 performs a comprehensive evaluation of our approach, com-paring the semi-automatically created lexicons to lexicons assembled by language experts. Section 5 concludes the pa-per and outlines future work.
This paper introduces an approach to combine games with a purpose and a lexicon-based sentiment detection method to create domain-specific sentiment lexicons. The following two subsections discuss related work in the field of sentiment detection, and provide background material on the use of crowd-sourcing applications in the tradition of games with a purpose.
Sentiment detection heavily relies on so-called sentiment lexicons, i.e. collections of terms and an a-priori assessment of their polarity. Well-known English resources are the Gen-eral Inquirer [19], the Subjectivity Lexicon [29] and the Sub-jectivity Sense Annotations [27, 8]. GermanPolarityClues [26] or the lexicon presented by Clematide and Klenner [3] are good examples of equivalent German resources.

Sentiment lexicons are valuable resources, and much work focuses on the creation of such lexicons. This task usually involves a lot of handicraft, making it time-consuming and resource-intensive. This explains the strong interest in reli-able automatic approaches.

In an early approach, Hatzivassiloglou and McKeown [9] used syntactical relations to identify new sentiment terms. Turney and Littman [23] use Pointwise Mutual Informa-tion (PMI) and Latent Semantic Analysis (LSA) to iden-tify sentiment terms in a large Web corpus. Terms with sufficient co-occurrence frequency with one of 14 paradigm terms (i.e., a gold standard list of seven positive and neg-ative terms) are assigned the same sentiment value as the respective paradigm term. Evaluated on the General In-quirer [19], PMI shows results comparable with the algo-rithm of Hatzivassiloglou and McKeown [9]. Using three different extraction corpora, Turney and Littman show that PMI does not outperform Hatzivassiloglou X  X  and McKeown X  X  algorithm but is more scalable [24]. LSA provided better results, but was not as scalable as PMI. Turney [22] uses the same techniques to identify new sentiment terms from a paradigm list of only two terms ( excellent and poor ). This procedure performed well on the review corpus. Beineke et al. re-interpret the previously discussed mutual association as a Na  X   X ve Bayes approach [2]; they also expand this unsu-pervised approach and create a supervised approach using labeled data.

In Esuli et al. [5] a semi-supervised approach creates Sen-tiWordNet , a sentiment resource based on the well-known linguistics resource WordNet [6]. They first manually la-bel all synsets containing 14 seed terms, which results in an amount of 47 synsets with positive label and 58 with neg-ative. All synsets obtained from certain relations (e.g. di-rect antonymy , similarity and derived-from ) with these seed synsets are labeled accordingly. Synsets without connection to the seed sets are classified as objective, as long as they do not have a different sentiment value in the General Inquirer. The so gathered data is used to train eight ternary classi-fiers, which classify the rest of WordNet. Kim and Hove [10] specify subjects by means of a Named Entity Recog-nition and assign them the overall sentiment value of the sentence. A list of 44 verbs and 34 adjectives expanded by WordNet synonyms and antonyms serves as sentiment lex-icon. A straightforward solution to accomplish sentiment detection in a language without existing sentiment lexicon is to use translation software. Denecke [4] applies a machine learning approach to multi-lingual sentiment detection using movie reviews from six different languages. Google Trans-late (www.google.com/language tools) converts foreign-lan-guage documents into English. The feature selection proce-dure extracts a total of 77 features out of four super classes [4]: (i) the frequency of word classes (i.e. the number of verbs, nouns, etc.); (ii) polarity scores for the 20 most fre-quent words and the averages scores for all verbs, nouns and adjectives are based on SentiWordNet [5]; (iii) the frequency of positive and negative words according to the General In-quirer; and (iv) textual features such as the number of ques-tion marks.

The a priori polarity of sentiment terms might change in different contexts. This problem is tackled by Gindl et al. [7], proposing an approach that dynamically refines the polarity by invocation of context. The first step is the iden-tification of ambiguous terms in a sentiment lexicon. For each of these ambiguous terms, probabilities for their oc-currence in positive and negative contexts are calculated by analyzing their occurrence in a corpus of positive and neg-ative reviews. Based on this information, the a priori po-larity of an ambiguous term is modified by analyzing terms co-occurring with the term in an unknown lexicon. Wilson et al. [29] examine 28 syntactical and linguistic features in a machine learning approach. Several of those features are context-based, e.g. invoking the sentence preceding or suc-ceeding the current one or the document topic. The features are tested using BoosTexter X  X  AdaBoost.MH algorithm [15] on the Multi-perspective Question Answering Opinion Cor-pus [28]. The approach has two steps: the first step filters subjective sentences from objective ones, and the second as-signs sentiment values to the subjective sentences. In their successive work [30] Wilson et al. use four different ma-chine learning algorithms to test their feature selection and also use a larger version of the corpus. Agarwal et al. [1] use the corpus to test n-grams and provide syntactical label for relations as context characteristics. Polanyi and Zaenen propose context handling strategies from a linguistic per-spective [12]. They distinguish two main groups of context modifiers: Sentence Based Contextual Valence Shifters and Discourse Based Contextual Valence Shifters .

Please refer to the surveys by Liu [11] and Tang et al. [21] for a more exhaustive overview of sentiment detection.
Human language technologies such as information extrac-tion and sentiment detection depend on appropriate lan-guage resources. Such resources can be acquired through Games with a purpose [25, 14], a crowd-sourcing mechanism and a special type of serious games that invites communi-ties of users with different levels of expertise to participate in value-adding processes. Games with a purpose leverage collective intelligence, which is described as combining  X  X e-havior, preferences, or ideas of a group of people to create novel insights X  [17]. Collective intelligence from groups of people often produces better results than individual domain experts [20].

Games with a purpose have been used successfully to solve problems that computers cannot yet solve, such as tagging images [25] and annotating content [18]. The main chal-lenges of game design are motivating users to play the game while generating useful data, and ensuring that the process yields unbiased results. Given appropriate design and au-thentication mechanisms, such games can capture individual knowledge according to the scientific criteria of objectivity, reliability, validity and representativeness. In the context of this paper, we harness the wisdom of the crowds through games with a purpose to be delivered via large-scale social networking platforms such as Facebook for compiling mul-tilingual sentiment lexicons. Advantages of this approach include a large number of possible players, intrinsic moti-vation within a social context, and more effective mecha-nisms to detect and combat attempts of manipulating re-sults. When adopting an approach based on filtering and cross-validation, the intrinsic motivation of users partici-pating in games with a purpose promises superior results compared to crowd-sourcing marketplaces such as Amazon Mechanical Turk (www.mturk.com). Merging several types of games (e.g. sentiment lexicon creation, translation, con-flict resolution) further increases the game X  X  attractiveness, reduces the risk of cheating, allocates collective intelligence more efficiently by prioritizing tasks across game types, and helps avoid the situation that dedicated players run out of new challenges.
Sentiment detection techniques use text features such as sentiment terms and sentiment indicators to assess the polar-ity (positive, negative) of text fragments. Sentiment terms have a distinct polarity and are usually domain-independent. In contrast, sentiment indicators occur within the discussion of topics which are often used in a positive or negative con-text (e.g. democracy, public debt, etc.). Therefore, these terms do not contain a polarity by themselves but rather in-dicate that the topic is likely to contain a certain sentiment. This is particularly useful in situations where only rudimen-tary sentiment lexicons are available (e.g. for less spoken languages or unusual application domains), since sentiment indicators have the potential to considerably improve the accuracy of sentiment detection in such settings (Section 4). Nevertheless, since topics are usually domain-specific, senti-ment indicators still have the limitation of being specific to a particular domain and, therefore, cannot be used across domains.

The proposed method introduces an approach which au-tomatically extracts sentiment terms and sentiment indica-tors by applying a bootstrapping process to domain-specific documents. The retrieved indicators then complement sen-timent dictionaries and increase the sentiment detection X  X  recall.

The sentiment values of domain-specific sentiment terms are usually limited to a particular domain. Sentiment in-dicators such as  X  X emocracy X  or  X  X ax raise X  do not contain a sentiment value per se but are associated with a certain sentiment in the given domain. Therefore, they provide a good indication of how an article is going to be perceived by its readers.

One objective of our approach is to improve the recall of sentiment detection for languages where sentiment resources are limited or still under development.

The presented approach starts with the creation of an ini-tial sentiment lexicon as described in Subsection 3.1. Based on this lexicon a bootstrapping algorithm (see Subsection 3.2) extracts further sentiment terms and indicators used to expand the initial lexicon.
This paper builds upon the lessons learnt from the Senti-ment Quiz (Figure 2), a Web-based social verification game for sentiment detection. It was developed as part of the US Election 2008 Web Monitor (www.ecoresearch.net/ele-ction2008), a project to investigate information diffusion via interactive online media, and the interdependence of news media coverage and public opinion [16].

The game is available in seven different languages and presents the player with potential sentiment terms. The player X  X  task is to evaluate these terms on a five-point scale (very positive, positive, neutral, negative, very negative) and he receives points based on how well his answer corresponds to the other player X  X  assessment of a particular term. If no prior evaluations are available for a term, the game assigns the player a score which is based on his average game perfor-mance. The sentiment quiz attracted more than 4 300 play-ers who have created a sentiment lexicon comprising 1 000 high quality terms as a by-product of their activities. Figure 2: The Sentiment Quiz, a word polarity game (www.modul.ac.at/nmt/sentiment-quiz)
A crucial task when applying such games with a purpose is to make sure that the games yield unbiased results and that users are prevented from raising their score by cheating. On a social networking site, users can identify other players and might collaborate to manipulate the game; e.g. by agreeing in advance on the answers to a limited set of questions. A number of simple measures can be taken to ensure output of high quality: (i) hide the identity of the other player; (ii) an-alyze the temporal distribution of answers; (iii) assign trust values to each player, which in turn determine the impact of their answers  X  e.g. insert questions with known answers into the exercise queue and identify users who tend to score low on these questions; (iv) avoid exploitable patterns in the sequence of answers, since users who identify the pat-tern could quickly earn credits without actually solving the puzzle. We also only consider terms which have received at least seven assessments to ensure a good quality of the initial sentiment lexicon used for the bootstrapping process.
We apply a bootstrapping algorithm to extract potential sentiment terms and sentiment indicators for the given do-main. An unlabeled corpus of TripAdvisor reviews serves as input for this step.

Figure 1 proves an overview of the three-step bootstrap-ping process. Initially we apply sentiment detection to deter-mine the sentiment of unlabeled Web reviews (Section 3.2.1) based on an initial sentiment lexicon, which was created by crowd-sourcing the task of annotating vocabulary with sentiment values to a Facebook game with a purpose (Sec-tion 3.1). We then identify representative examples of re-views with a positive and negative sentiment and use them to create a corpus of such reviews (Section 3.2.2). Finally, we extract sentiment indicators and terms from this corpus (Section 3.2.3), merge these terms into the sentiment dictio-nary, and repeat the process as required.
Applying a simple lexicon-based sentiment detection ap-proach estimates the sentiment (  X  ) of the extracted reviews:
The algorithm uses a bag of words approach and considers negation by scanning for negation triggers such as  X  X ot X  and  X  X ithout X  which invert the sentiment value of the following term. We applied a simple lexicon-based approach, which only considers simple grammatical constructs such as nega-tion, for detecting the sentiment of unlabeled documents. For the evaluation we complemented this approach with a Na  X   X ve Bayes classifier and Support Vector Machines.
The next step creates and expands a corpus of positive and negative reviews to be used for the extraction of sen-timent terms and indicators. The output of the sentiment detection component helps to identify the k strongest posi-tive and negative reviews ( doc i ) and the corresponding sen-timent thresholds (  X  + k and  X   X  k ). Due to the strength of their sentiment values we consider these reviews as representative examples of positive and negative discussions and therefore assemble corresponding learning corpora containing positive C + and C  X  negative examples:
The input corpus is a collection of 1 600 unlabeled holiday reviews downloaded from the website www.tripadvisor.com. The corpus is balanced, containing an equal number of pos-itive and negative reviews. We assign a positive polarity when a review has more than three stars, and a negative if it has less than three stars.
The extraction of new sentiment terms follows each ex-pansion of the corpora ( C + and C  X  ). For each term in the knowledge base the system calculates its probability of occurring in positive and negative sentences based on the Naive Bayes algorithm.
Subsequently, the m terms with the highest absolute prob-ability values and the corresponding sentiment thresholds P + and P  X  , i.e. the strongest m positive and negative terms, are added to the sentiment lexicon. Terms already in-cluded in the lexicon are disregarded. We also ignore terms which occur less then n min times in the corpus.
Our current approach applies this bootstrapping process multiple times and divides the number of representative sen-tences to include in the corpus creation step ( k ) by half after every run. The terms yielded by this process include rele-vant sentiment indicators and sentiment terms which con-siderably improve the performance of subsequent sentiment detection steps (Section 4).
Figure 3 visualizes the described evaluation process. The evaluation design focuses on the following research ques-tions: (1) is the quality of the bootstrapped and newly in-cluded sentiment terms high enough to improve the overall quality of the system, and (2) how well does this lexicon compare to a manually compiled lexicon which was assem-bled by language experts.

To answer these two questions we performed a 10-fold cross-validation of the following three lexicons based on three different sentiment detection algorithms:
The corpus used for cross-validation is a collection of 1 600 reviews downloaded from the TripAdvisor website (www.trip-advisor.com). For each run of the cross-validation the sys-tem creates an expanded lexicon from the training data. The presented lexicons are used by three different algorithms:
We chose Na  X   X ve Bayes and SVM as classifiers since they are standard algorithms and especially SVMs are known to deliver excellent results on high-dimensional data such as textual data. The WEKA tool serves as framework for the evaluation with the Na  X   X ve Bayes and the SVM algorithm. For this purpose we first converted the textual reviews into ARFF files, the common file format for WEKA. The lexical algorithm processes the reviews in plain text format. In order to ensure equivalence of the training and test data for both the WEKA environment and the lexical approach we did not use WEKA X  X  built-in 10-fold cross-validation mode but created the corresponding files ourselves.

Tables 1 and 2 contain the results of our evaluation. Ta-ble 1 compares the Facebook lexicon with the expanded lex-icon. The table can be read as follows: each triple con-tains the average of either recall, precision, or F-measure achieved with one of the three algorithms using either the Facebook or the expanded lexicon. R f refers to the average recall achieved with the Facebook lexicon ( f ), R e refers to recall obtained with the expanded lexicon ( e ). The column Sig has a check mark ( X ) when the difference is statisti-cally significant and a dot (  X  ) when it is not. In case the expanded lexicon delivers significantly worse results the col-umn contains a dashed circle ( ). The R implementation of Wilcoxon X  X  rank sum test serves for calculation of signif-icance values [13]. We regard significance values below 5 % (i.e. p &lt; 0.05) as significant.
 Table 1: Results of the 10-fold cross-validation with the WEKA LibSVM classifier Table 2: Comparison of the expanded lexicons with the General Inquirer
Table 2 contains a comparison of results achieved with both the expanded lexicon and the General Inquirer lexi-con. The results show, that although the semi-automatically compiled sentiment lexicon has less than half the number of sentiment terms, it sill performs similarly to the expert lex-icon for two of the three evaluated sentiment detection ap-proaches. The General Inquirer lexicon is only significantly better for results achieved with the Na  X   X ve Bayes classifier. We did not observe significant differences for the SVM clas-sifiers, yet the different values still indicate better results of the General Inquirer lexicon. For the lexical approach the expanded lexicon was even able to significantly outperform the General Inquirer lexicon in two cases (precision for pos-itive reviews and recall for negative reviews).

The lexical approach profited the most from the boot-strapping process. We obtained significant improvements for recall, precision, and F-measure. The improvements achieved with the Na  X   X ve Bayes and SVM classifiers were all significant except for recall of negative reviews.

Table 3 shows three terms which were incorporated into the sentiment lexicon during the bootstrapping process and lists sentences that illustrate how these terms improve the method X  X  accuracy. Interestingly, the intuitively negative term stops was identified as a positive sentiment term. Af-ter the lookup of sentences in the databases that contained this sentence, the reason became apparent. The term stops referred to bus or subway stations. In general, it is desir-able to live close to a bus stop, and the system also iden-tified it correctly. Therefore, stops can be considered as one of the afore-mentioned sentiment indicators. Only in the domain of holiday reviews it gets an obvious positive connotation (although it might also be used positively in domains completely different to holiday reviews). The two other examples, dingy and stained are sentiment terms -one can easily imagine them to be used negatively in a dif-ferent domain. The significant improvement achieved with the bootstrapped lexicon shows that the proposed method is a valuable tool under circumstances where sentiment re-sources are sparse.
This paper proposed a semi-automated process which com-bines Games with a purpose and a bootstrapping approach to create sentiment lexicons and customize them to a par-ticular domain. Complementing crowd-sourcing with boot-strapping yields an extended sentiment lexicon (containing sentiment terms and sentiment indicators), which consider-ably outperforms the accuracy of the initial dictionary.
The main contributions of this paper are (i) the intro-duction of the concept of sentiment indicators, which sup-ports sentiment detection by complementing known senti-ment terms with domain knowledge, (ii) applying Games with a Purpose to the task of generating language resources which are essential for many natural language detection and knowledge management tasks, (iii) introducing a bootstrap-ping process which automatically extends these resources by adding sentiment indicators and sentiment terms based on unlabeled domain documents, and (iv) performing a compre-hensive evaluation which shows that bootstrapping consider-ably improves the performance of the created sentiment lex-icon, and that the lexicon yielded from the semi-automatic process performs -depending on the used sentiment detec-tion method -about as good or only slightly worse than widely used language resources such as the General Inquirer, which have been compiled by language experts. stops (pos) dingy (neg) stained (neg)
This result is remarkable for a semi-automatically created resource, especially when considering that the main benefit of the introduced method is its applicability to languages and domains for which such high quality resources are not yet available. In such cases the effort required to create language resources is reduced significantly.

The evaluation also demonstrates that the introduced boot-strapping process is very efficient in learning sentiment terms and indicators. Nevertheless, it currently has the disad-vantage of not being able to distinguish between domain-independent sentiment terms and topic-related sentiment indicators. This is not a problem for domain-specific sen-timent detection as such, but is highly relevant for the abil-ity of reusing sentiment lexicons across domain. Future re-search will address this shortcoming by applying corpus-based methods such as the one introduced in Gindl et al. [7] for identifying domain-specific sentiment indicators. We will also explore the applicability of Games with a Purpose to the creation of other language resources such as test collections and text annotations. [1] Apoorv Agarwal, Fadi Biadsy, and Kathleen R.
 [2] Philip Beineke, Trevor Hastie, and Christopher [3] Simon Clematide and Manfred Klenner. Evaluation [4] Kerstin Denecke. How to Assess Customer Opinions [5] Andrea Esuli, Fabrizio Sebastiani, and Via Giuseppe [6] Christiane Fellbaum. WordNet -An Electronic Lexical [7] Stefan Gindl, Albert Weichselbraun, and Arno Scharl. [8] Yaw Gyamfi, Janyce Wiebe, Rada Mihalcea, and Cem [9] Vasileios Hatzivassiloglou and Kathleen R McKeown. [10] Soo-Min Kim and Eduard Hovy. Determining the [11] Bing Liu. Sentiment analysis and subjectivity. In Nitin [12] Livia Polanyi and Annie Zaenen. Computing Attitude [13] R Development Core Team. R: A Language and [14] Walter Rafelsberger and Arno Scharl. Games with a [15] Robert E. Schapire and Yoram Singer. BoosTexter: A [16] Arno Scharl and Albert Weichselbraun. An Automated [17] Toby Segaran. Collective Intelligence -Building Smart [18] K. Siorpaes and M. Hepp. Games with a Purpose for [19] Philip J. Stone. The General Inquirer: A Computer [20] J. Surowiecki. The Wisdom of Crowds: Why the Many [21] Huifeng Tang, Songbo Tan, and Xueqi Cheng. A [22] Peter D. Turney. Thumbs Up or Thumbs Down? [23] Peter D. Turney and Michael L. Littman.
 [24] Peter D. Turney and Michael L. Littman. Measuring [25] L. Von Ahn. Games with a Purpose. Computer , [26] Ulli Waltinger. GERMANPOLARITYCLUES: A [27] Janyce Wiebe and Rada Mihalcea. Word Sense and [28] Janyce Wiebe, Theresa Wilson, and Claire Cardie. [29] Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. [30] Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
