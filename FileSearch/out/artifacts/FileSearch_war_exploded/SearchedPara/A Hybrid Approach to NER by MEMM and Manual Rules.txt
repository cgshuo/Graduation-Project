 This paper describes a framework for defining domain specific Feature Functions in a user friendly form to be used in a Maximum Entropy Markov Model (MEMM) for the Named Entity Recognition (NER) task. Our system called MERGE allows defining general Feature Function Templates, as well as Linguistic Rules incorporated into the classifier. The simple way of translating these rules into specific feature functions are shown. We show that MERGE can perform better from both purely machine learning based systems and purely-knowledge based approaches by some small expert interaction of rule-tuning. I.2.7 [ Natural Language Processing ]: Text analysis  X  Named Entity Recognition, Information Extraction.
 Named Entity, Information, Document Collection, Statistical Model, Empirical Model, Machine Learning, Maximum Entropy. Named Entity Recognition, Text Mining, Machine Learning, Information Extraction, Maximum Entropy Markov Model. There are two traditional approaches to Named-Entity recognition (NER): knowledge-based approach and machine learning approach. Knowledge-based systems usually achieve better accuracy, but require huge am ounts of skilled labor by linguists and domain experts. Because of this, the recent research in NER is concentrated on machine learning techniques, which only require a manually labeled training set of documents. The best published ML-based systems perform on the level of knowledge-based systems for many categories. In this paper we present MERGE (Maximum Entropy Rule Guided Extraction)  X  a hybrid NER system which combines machine learning techniques, namely ME and manually written simple rules. MERGE benefits from both approaches and can outperform both manually written rules and standard machine learning systems. The rule language of MERGE is quite simple and the amount of n ecessary rule-writing is relatively small, as most of the work is done by the ML part of the system. A Maximum Entropy approach models a random process by making the distribution satisfy a given set of constraints, and making as few other assumptions as possible. The constraints are specified as real-valued feature functions over the data points. The expected value of each feature function under the ME distribution must equal the empirical expected value of function as found in the training dataset. In all other respects, the target distribution should be as uniform as possible, which means it must have the highest entropy. For our purposes, we use ME to model the conditional probability distributions, which slightly differ in the way expected values are calculated ([4]). possible outcomes. We assume that there is a true joint distribution P ( x , y ), but we are interested only in modeling the conditional P ( y | x ). For this purpose we can use a training set {( x k , y k )} k =1.. N generated by the true distribution, and a set of specific conditions. It can be shown that the unique most uniform distribution that satisfies all feature constraints has the form: where  X  i  X  X  are the parameters chosen to maximize the likelihood of the training data, and Z ( x ) is a normalization constant, which ensures that for every x the sum of probabilities of all possible outcomes is 1. The most common procedure for parameter estimation is the Generalized Iterative Scaling algorithm ([5]). A MEMM ([1]) consists of | Y | conditional ME models p p ( y | x , y' ), one for each y' . The model p probability of appearance of the label y immediately after the label y' in the context x . The probability of a whole label sequence y = y 1 y 2 ... y m , given the sentence x = x 1 x The best tagging can be found using Dynamic Programming similar to Vitterbi algorithm. The model p 0 ( y | x ) used at the beginning of a sentence is separate. The preprocessing of any text is done according to external definitions. First, the text is divided into tokens. A token is defined by a regular expression in which for an English text domain it might be in the form "[A-Za-z]+|[0-9]+|\S". Then, each token is assigned its features according to the feature templates. A template consists of a context rule  X  a binary test upon the positions in the text. It can test the exact character value of the current token and its neighbors, capitalization information, membership in an external word list, arbitrary regular expressions, externally supplied features like pos tags, etc. A template can also define a generic context rule, such as  X  X oken value X . Each template defines a set of features  X  one feature for every combination of an instantiated context rule and a category. The features are always built in such a way that exactly one feature from a template tests true at any text position. This implies that the sum of all features is constant, satisfying the requirements of the GIS algorithm. For example, one Feature Template may have the following form: This template would generate a feature function for every combination of the value "Word" feature at the previous token, the value of "Capitalization" feature at the current token, and the current tag. One such feature function might be: For Tagging, the labels are assigned at the token level  X  each word in a sentence is labeled by an entity type label or by label None . For multi-token entities we label each token by the same category label, making no distinction between beginning, middle, or ending tokens of the entity. Defining specific rules is done via a simple pattern matching language, with patterns working at the token level. A pattern syntax is similar to the regular expressions syntax, but with tokens instead of characters. Quantifiers *, +, ? are allowed, as well as the grouping parentheses  X ( X   X ) X . The angular brackets  X &lt;&gt; X  delimit the target entity. Features are generated for each token in the delimited entity. The tokens are either specified directly, or represented by token-classes of the form  X  X Boolean expression] X .The expressions are simple token-attribute=value checks, combined with logical operators &amp;(and) |(or), and !(not). Here is an example set of rules for some tricky organizations: In our MUC-7 evaluation the list of published Word Classes are used as feature functions themselves. Besides that, some intuitive Word Classes are defined within the Rule-Development framework, in order to use them as "wc" feature for Rule Writing. As an example we had a Word Class wc=WeekDay, including the words Sunday, Monday, etc. and it is used in a Rule: The translation of Rules into a Feature Function is done by checking the rule pattern at each token position. The above rule for Date will be translated into feature function: For keeping the value M constant for each pair (x,y), we defined also this same feature function for all the other possible tags. We made an evaluation of our methodology on MUC-7 data set, by checking the system performance on the whole possible training data (350 documents). By adding some approximately 50 more rules the overall performance reached to 93.5% as shown in Table 1, while overlapping match results come up to 95.4%. (In overlapping-match, a entity is considered correct if at least one word of it is tagged correctly) The lower overall performance is due to other entity types which had slightly lower results. run an implementation of Nymble ([2]) and TEG ([3]) on that same corpus, which produced the results shown on Table.2. [1] McCallum, A., Freitag, D., Pereira, F.: Maximum Entropy [2] Bikel, D. M., Miller, S., Schwartz, R., Weischedel, R.: [3] Rosenfeld, B., Feldman, R., Fresko, M., Schler, J., Aumann, [4] Berger, A., della Pietra, S., della Pietra, V.: A maximum [5] Darroch, J. N., Ratcliff., D.: Generalized iterative scaling for 
