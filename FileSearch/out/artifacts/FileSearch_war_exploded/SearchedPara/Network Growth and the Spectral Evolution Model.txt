 We introduce and study the spectral evolution model, which characterizes the growth of large networks in terms of the eigenvalue decomposition of their adjacency matrices: In large networks, changes over time result in a change of a graph X  X  spectrum, leaving the eigenvectors unchanged. We validate this hypothesis for several large social, collabora-tion, authorship, rating, citation, communication and tag-ging networks, covering unipartite, bipartite, signed and un-signed graphs. Following these observations, we introduce a link prediction algorithm based on the extrapolation of a network X  X  spectral evolution. This new link prediction method generalizes several common graph kernels that can be expressed as spectral transformations. In contrast to these graph kernels, the spectral extrapolation algorithm does not make assumptions about specific growth patterns beyond the spectral evolution model. We thus show that it performs particularly well for networks with irregular, but spectral, growth patterns.
 Categories and Subject Descriptors: H.4 Information Systems Applications: Miscellaneous General Terms: Algorithms, Theory, Performance Keywords: Graph kernels, link prediction, spectral graph theory
Many learning problems on networks can be described as link prediction: finding movies a user might like, recom-mending friends, predicting communication events, or find-ing related topics in a collaboration graph. In each of these cases, a given network is assumed to grow over time and the task is to predict where new edges will appear and, in some cases, to also predict their weights.

Any given link prediction algorithm can be interpreted as a model of graph growth, by assuming that networks grow according to its predictions. In this paper, we are concerned with those link prediction algorithms that have an algebraic description. We show that they imply a network growth model that we call the spectral evolution model . The spec-tral evolution model states that in terms of the eigenvalue decomposition of a network X  X  adjacency matrix, growth can be described as a transformation of the eigenvalues, without significant change in the eigenvectors. Several common link prediction functions are of this form, such as rank reduc-tion and the matrix exponential. These functions are graph kernels, and each of these algorithms represents a specific as-sumption about network growth, leading to a different spec-tral transformation function. The goal of this study is thus to give a method of choosing graph kernels, and further-more to generalize them to a generic spectral link prediction algorithm that only assumes the spectral evolution model.
By observing the spectral evolution of large networks, we arrive at two results: In the examples we study, the majority of networks grow reg-ularly when we look at single latent dimensions, but when we look at the whole spectrum they grow irregularly. This observation leads to the spectral extrapolation algorithm that we describe in this paper. As we will show, this al-gorithm has the advantage of being data-driven and free of any kernel-induced parameter, making it generalizable to networks of many different types.

We study the spectral evolution model in unipartite and bipartite, weighted, unweighted and signed network datasets from different application areas. We begin by introducing a motivational example and then define the spectral evolution model in Section 2. The model is tested on real networks in Section 3 and on common graph growth models in Section 4. As an application, we introduce the spectral extrapolation algorithm for link prediction in Section 5. Section 6 reviews related work and extensions to our approach, and Section 7 concludes this paper. Figure 1: As edges appear in a network, the eigen-value decomposition of the network X  X  adjacency ma-trix evolves spectrally according to the spectral evo-lution model.
We introduce the spectral evolution model here using the example of a social network and the common  X  X riend of a friend X  graph growth model. Given a network of n users connected by friendship links, let its adjacency matrix A  X  { 0 , 1 } n  X  n be defined as A ij = 1 when users i and j are connected and A ij = 0 otherwise. We then consider the eigenvalue decomposition A = U X U T . It can be shown that several common link prediction algorithms can be expressed as U F (  X  ) U T , where F is a function that applies the same real function f to each diagonal element of  X  [16]. An example is given by the matrix exponential exp( A ) = P where exp(  X  ) can be computed by applying the exponential function to  X   X  X  diagonal elements. The matrix exponential can be interpreted as a link prediction function as follows: Since the matrix power A i gives the number of paths of length i between any two nodes, the matrix exponential gives the sum over all paths between any two nodes, weighted by the inverse factorial of path length. This characterization of the matrix exponential makes it suitable as a realization of the  X  X riend of a friend X  model: All predicted links close triangles or longer cycles, and parallel paths between two nodes reinforce their connection while short paths make the connection stronger.

At the same time, Equation (1) shows that the matrix exponential is a spectral transformation of the adjacency matrix A . As we will see later, several other link prediction functions can be expressed as spectral transformations too, and provide the basis of the spectral evolution model: that all network growth is of this form.
The spectral evolution model states that the evolution of a network can be described by a change in the network X  X  spectrum, while the network X  X  eigenvectors stay largely con-stant over time. If A ( t ) is the adjacency matrix of a network at the time t , the spectral evolution model states that there is an orthogonal matrix U and diagonal matrices  X  ( t ) such that A ( t ) = U X  ( t ) U T is a valid eigenvalue decomposition for all t . Figure 1 summarizes the proposed spectral evolu-tion model. To examine the validity of this model, we follow two approaches:
First (in Section 3), we examine the evolution of real-world networks over time. To make our analysis as gen-eral as possible, we look at several types of networks: social networks, collaboration networks, communication networks, authorship networks, rating networks, citation networks and folksonomies. Some networks, such as rating graphs, have edge weights, others have negative edges, such as friendship and enmity networks, and others are unweighted. The spec-tral evolution model also applies equally to unipartite and to bipartite networks.

The second test of the spectral evolution model (in Sec-tion 4) is theoretical: Several existing models of network growth predict spectral network evolution. We derive this for common graph kernels, including the matrix exponen-tial, the von Neumann kernel, rank reduction methods, path counting, and community models. We also show that spec-tral evolution is specifically not implied by a random per-mutation model, rendering the spectral evolution model non-trivial and its observation in so many network types signifi-cant.
In this section, we examine a collection of large network datasets in order to verify the spectral evolution model em-pirically for a large variety of network types. In total, we examined 119 large networks. A subset of these datasets is listed in Table 2. The networks under study are of the following types: In all cases, edge creation times are known, for instance the moment a friendship forms, or the publication date of a paper.

We will denote the symmetric adjacency matrix of a net-work by A . For unweighted networks, this is a 0/1 matrix. For networks in which multiple edges are allowed, the en-tries are nonnegative integers. For signed networks, A is Figure 2: The spectral evolution of large real-world networks. At each time, the graphs show the k dom-inant eigenvalues or singular values of the network at that time. (a) The bipartite Netflix rating graph, (b) The Facebook social network. a  X  1/0/+1 matrix. In rating networks, the matrix entries are the rating values, from which the mean rating has been subtracted. Bipartite networks have special structure which can be exploited by working with the biadjacency matrix, whose singular values correspond to the absolute eigenval-ues of the adjacency matrix. We will make all derivations for the eigenvalue decomposition, but understand that the singular value decomposition can be used analogously.
For each network, we split the set of edges into n = 71 bins by edge arrival time. For each bin, we take a snapshot of the network after the arrival of that bin X  X  edges, and compute the first k eigenvalues or singular values of the resulting adjacency matrix, for symmetric and asymmetric networks respectively. We denote A ( t ) the adjacency matrix of all edges present after time t . Thus A ( n ) is the full adjacency matrix.

Figure 2 shows the spectra of several large networks as functions of time. The number of computed eigenvalues k is chosen in function of network size to give reasonable run-times. A first inspection of these plots shows that the eigen-values and singular values grow over time. The observed growth is sometimes regular, as in the case of Netflix, and sometimes irregular, as in the case of Facebook. A few ob-servations can be made immediately: Eigenvalues can have growth patterns so different from each other that one may overtake another. In the Facebook network, one eigenvalue is even constant in the second half of the growth. These irregularities indicate that simple graph kernels such as the matrix exponential cannot work well, as they apply the same function to each eigenvalue, and thus cannot predict that a latent dimension is stagnant, even though this is easy to see by experiment.

For our analysis of spectral growth to be complete, spectra must not only grow, but eigenvectors must be stable. This is inspected in the next test.
We have seen how network spectra change over time. We will now verify whether eigenvectors are stable as predicted by the spectral evolution model.

We compare, for each time t , the eigenvectors A ( t ) with the eigenvectors of A ( n ) of the complete network in Fig-ure 3. The plots show one curve for each latent dimen-sion k , showing the number of edges added between times t and n on the x-axis and the absolute dot product of the k th eigenvector of A at times t and n . Eigenvectors cor-responding to large eigenvalues are shown in bright colors and those corresponding to smaller eigenvalues in transpar-ent colors. These plots suggest the following interpretation: The general trend leaves the eigenvector similarity as mea-sured by the dot product near one for the lifetime of the network, with similarity being higher for those eigenvectors with larger eigenvalues. In some networks such as YouTube, the similarity of eigenvectors suddenly drops to zero at spe-cific points in time. As we will see, this is due to eigenvectors exchanging places in the decomposition, or in other words, the eigenvalues passing each other. The next test will in-spect these permutations.
How stable are eigenvectors over time? To answer this question we compute the absolute eigenvalues of eigenvector pairs at two times t 1 and t 2 . At time t 1 , the networks contain 75% of all known edges, and at time t 2 they contain all known edges. Let be the eigenvalue decompositions at times t 1 and t 2 then compute the cosine similarities | U T (1)  X  i U (2)  X  j all pairs ( i,j ) of latent dimensions. We show the resulting matrices using white for zero and black for one, and con-tinuous shades in between in Figure 4. These plots give an indication as to what extent eigenvalues are preserved over time. If all eigenvalues are distinct and network evolution is purely spectral, then these matrices are permutation matri-ces. In addition, they are diagonal unit matrices when the latent dimensions do not overtake each other. The deriva-tion from a diagonal matrix then gives an indication as to the monotony of the underlying spectral transformation.
Testing the eigenvector stability in this way has one draw-back. If two eigenvalues are (almost) equal, an exchange Figure 3: The evolution of eigenvectors: The sim-ilarity between the eigenvectors of the full graph and the eigenvectors of partial graphs, by increas-ing number of added edges. Each latent dimension is represented by one curve, with brighter colors for dominant latent dimensions. (a) The Netflix rating network, (b) The YouTube social network. between their eigenvectors does not change the matrix by much. Therefore, we have to inspect more carefully the well-separated eigenvalues, which are larger. The next test is designed to be robust against such multiple eigenvalues.
We apply our algorithm previously described in [16] to learn spectral transformations by finding a function of the diagonal eigenvalue matrix that best fits a certain spectral link prediction function. If is the eigenvalue decomposition at time t 1 , then at time t it is expected to become where D is diagonal. Figure 4: The absolute dot product of all eigen-value pairs at two times in network evolution. These plots show permutation matrices (with 0 in white and 1 in black) when the network evolution is purely spectral and eigenvalues are simple. The derivation from a diagonal matrix gives an indication as to the monotony of the underlying spectral transformation. (a) The Facebook social network, (b) The Flickr so-cial network.

The best fit of D in a least-squares sense is then given by The diagonality test plots in Figure 5 show the matrices  X  for several networks. These plots give an indication to what extent growth is spectral. If the growth between times t and t 2 is purely spectral, then the matrix  X  D is diagonal. The plots show this matrix to be diagonally dominant, indicating that the spectral evolution model is correct to a large extent, but not perfect.

In other words, there is a small amount of mixing between latent dimensions. The plots also show that the small off-diagonal values are not clustered near the main diagonal, implying that the small amount of mixing does not happen between adjacent latent dimensions specifically. We there-fore interpret this mixing as noise rather than a feature of network evolution. Figure 5: The diagonality test of spectral network evolution in the Facebook social network, showing a plot of the matrix  X  D described in the text. If network evolution was perfectly spectral, the plot would show a clean diagonal.
In Figure 2 we observed some graph spectra to evolve in a regular fashion and others not. However, these plots do not reveal which link prediction functions they match. To find out, we apply the method described in [16]. This method takes a network at an intermediate timeslice (the endpoint of the training set), and compares the intermedi-ate and the final spectra and corresponding eigenvalues by reducing the comparison to a one-dimensional curve fitting problem. This is achieved by computing how the spectrum should look like in the final graph. The underlying idea is that if network growth corresponds to a certain spectral link prediction function, the new spectrum is a corresponding real function of the intermediate spectrum.

Let A (1) and A (2) be the adjacency matrices at times t 1 and t 2 , and A (1) = U (1)  X  (1) U T (1) the eigenvalue decomposi-tion of A (1) . Assuming that graph growth follows a spectral transformation F ( A (1) ) = A (2) leads to the optimization problem Since this problem is independent of the off-diagonal entries in U T (1) A (2) U (1) , it can be reduced to the following form: where f (  X  ) is the real function such that F (  X  (1) ) ii
As an example, estimating the parameters of an expo-nential graph kernel F ( A (1) ) =  X  exp(  X  A (1) ) amounts to solving This minimization problem is a one-dimensional nonlinear curve fitting problem that can be easily be solved by exist-ing methods 1 . Figure 6 shows the results of curve fitting for several common graph kernels. While some networks e.g., the function lsqnonlin in MATLAB Figure 6: Comparing actual spectral transforma-tions and several known graph kernels in the signed Wikipedia vote graph. In this plot, each curve rep-resents a specific spectral transformation. If curve fitting is tight for a particular curve, the correspond-ing link prediction function describes the network growth well. On the other hand, outliers represent latent dimensions whose growth is not well described by known link prediction functions. indeed display growth patterns that follow specific spectral link prediction functions, others do not. In particular, some networks seem to follow a regular spectral transformation except for single latent dimensions, which grow differently. This observation is the basis of the method described in Section 5: The spectral growth has to be learned indepen-dently of any graph kernel. First however, we test whether the spectral evolution model is justified by theoretical graph growth models.
We have seen in Section 2 that the matrix exponential is a spectral transformation . In fact, several other link pre-diction algorithms are also spectral transformations. These methods show that the spectral evolution model arises nat-urally in different graph growth models such as diffusion models, path closing models and rank reduction models. At the same time, it is important to verify that the spectral evolution model is not trivial, i.e. that it does not arise sim-ply by a random graph growth model. To do this, we first show how it follows from common link prediction models, and then show that it does not arise from a simple random graph growth model.
A certain number of link prediction algorithms are known as graph kernels [8]. In this context, a graph kernel is a positive-semidefinite function of two nodes of a given graph that denotes similarity or proximity 2 . A graph kernel can be understood as a positive-semidefinite function K ( A ) of a graph X  X  adjacency matrix. If A = U X U T is the eigenvalue decomposition of the adjacency matrix A , then a spectral graph kernel can be written as K ( A ) = U F (  X  ) U T for var-
In another context, a graph kernel is a positive-semidefinite function of two (usually small) graphs. ious functions F (  X  ) which apply a real function f (  X  ) to each eigenvalue in  X  . By construction, graph kernels can be computed from the eigenvalue decomposition of A . The following are common choices for graph kernels:
The matrix exponential of the adjacency matrix can be used as a link prediction function [15].
 As mentioned in Section 2, the exponential graph kernel denotes the sum over all paths between any two nodes, weighted by the inverse factorial of path length. The ex-ponential graph kernel corresponds to the spectral transfor-mation f (  X  ) = e  X  X  .

The von Neumann kernel arises when considering a dif-fusion process in which a certain amount of flow is lost on each edge [13].
 where  X  is chosen such that  X   X  1 &gt; |  X  1 | , where |  X  largest absolute eigenvalue of A . The resulting spectral transformation is f (  X  ) = 1 / (1  X   X  X  ). The von Neumann kernel is also called the Katz index when it is used for link prediction [22].

For large graphs, the eigenvalue and singular value de-compositions can only be computed up to a small rank k , in practice not greater than about 100. The fact that link prediction using a reduced rank achieves reasonable results is explained by rank reduction being a good link predic-tion method in itself. Thus, varying k results in varying link prediction accuracy [28]. Reduction to a rank k can be interpreted as the spectral transformation f (  X  ) =  X  when |  X  |  X  |  X  k | , and f (  X  ) = 0 otherwise, where |  X  largest absolute eigenvalue of A .

We discussed that the exponential graph kernel can be understood as giving each path an importance equal to the inverse factorial of its length. This can be generalized to any weight function, and results in matrix polynomials P ( A ) and matrix powers as special cases. In the simplest case, this is the square A 2 , which corresponds to the triangle closing model of link prediction [20], since it counts the number of common neighbors of all vertex pairs. The corresponding spectral transformation is simply the polynomial P applied to each eigenvalue separately.
If a symmetric matrix A is updated by a rank-one matrix uu
T , then that update is a spectral transformation when u is an eigenvector of A [6]. This spectral transformation updates the eigenvalue corresponding to u by | u | 2 . If a la-tent dimension is interpreted as a subcommunity of a net-work described by an eigenvector u , an update of uu T can be understood as a change within that community. In a user X  X ser email network for instance, an update of uu T cor-responds to new email events among the subcommunity of users represented by u . The relative growth of individual la-tent dimensions is not predicted by this community model, justifying the spectral network model but not any specific graph kernel.
In several real-world networks, the growth is a spectral transformation, but does not follow any specific spectral transformation function. For instance, a latent dimension may stop growing and then level out, as in Figure 2(b) for the Facebook social network. As this plot shows, a stagnat-ing latent dimension may also restart growing. There may be several possible explanations for this behavior. The Wiki authorship datasets are particularly useful in this regard, because the names of users and items are known, allowing us to give interpretations of network growth. For many Wiki authorship networks, these dimensions represent automatic bots or other exceptional editing. We made the following observations: In most cases however, the reason for irregular but spec-tral growth is unknown because no vertex labels are pro-vided with the datasets. In these cases, the irregularity can nonetheless be learned because it is spectral.
The graph kernels reviewed in this section all predict spec-tral growth, which leads to the question whether non-spectral growth is possible under other graph growth models. Let A ( t ) = U X U T be the adjacency matrix of a network, and A is small and E can be thought of as an infinitesimally small edge added to the network. A perturbation argument then shows that the eigenvalue decomposition of A ( t +1)  X  U  X 
 X   X  U T has the following bounds [30]: As a result, eigenvectors are expected to change faster than eigenvalues for random additions to the adjacency matrix. We were able to confirm this prediction in synthetic random tests, which shows that spectral growth is not explained by a random graph model and is thus a nontrivial emergent property of real-world networks.
We now derive, as an application of the spectral evolution model, a new algorithm for link prediction. According to the spectral evolution model established in the previous sec-tions, the spectrum of a network evolves over time, while the eigenvectors remain constant. We saw that this assumption is true to a good approximation in large real-world networks. In this section, we will exploit this fact to derive a new algo-rithm for link prediction. We assume that a given network grows spectrally, but do not make any other assumptions. In particular, we will not assume that growth follows any spectral transformation function implied by a specific graph kernel. For this reason, our new link prediction algorithm is a generalization of spectral graph kernels.
Graph kernels such as the matrix exponential assume there is a real function f (  X  ) which describes the growth of all eigenvalues. In the graph kernels of Section 4, this function is very regular; it is positive and convex. Learned spectral transformations such as the one in Figure 6 are however ir-regular, and no simple function solves the resulting curve fitting problem well. This is consistent with the observation that some eigenvalues cross each other, indicating that a non-monotonous spectral transformation function is needed.
For these reasons, we consider the growth of each latent dimension separately. If we knew that eigenvalues did not cross each other, we could compare the i th eigenvalue at two points in time and extrapolate a third value. But because eigenvalues pass each other, we must learn the correspon-dence between new and old eigenvalues.
We now describe our algorithm for predicting links by extrapolation of spectral growth. We describe our method for symmetric graphs and the eigenvalue decomposition. For bipartite graphs, the singular value decomposition can be used analogously. To minimize the amount of computation needed, we base the extrapolation on only two times: t 1 , an intermediate time, and t 2 , the final time where the training set is complete. Therefore, our method only requires the computation of two eigenvalue decompositions.

We denote by A (1) and A (2) the adjacency matrices at times t 1 and t 2 , and then consider the eigenvalue decompo-sition at each time t : To find out which latent dimension i at t 1 corresponds to latent dimension j at t 2 , we compute a mean of eigenvalues at t 1 , weighted by the similarity between the two latent di-mensions at t 1 and t 2 . As a similarity measure, we use the dot product between the corresponding normalized eigen-vectors 3 . Thus if  X  (2) j is an eigenvalue at t 2 , its estimated previous value at t 1 is In other words, the values  X   X  (1) j are an estimate of the di-agonalization of A (1) by U (2) . We can then perform linear extrapolation to predict an eigenvalue  X   X  (3) j in the future. Using the matrix  X   X  (3) = (  X   X  (3) j ), the predicted edge weights are then U (2)  X   X  (3) U T (2) . Note that the computed eigenvalues  X   X  (3) j are not necessarily ordered by absolute value, instead they retain the ordering of U (2) .
We also experimented with various powers of the dot prod-uct, but results were mostly identical.
 Table 1: Summary of link prediction methods.
 P Polynomial f (  X  ) = P i  X  i  X  i NNP Nonneg. polynomial f (  X  ) = P i  X  i  X  i , X  i  X  0 EXP Matrix exponential f (  X  ) = e  X  X 
RED Rank reduction f (  X  ) =  X  when |  X  | X |  X  k | , AA Adamic/Adar  X  NB Common neighbors  X 
EXT Spectral extrapolation Irregular
We compare the performance of the spectral extrapolation algorithm to other link prediction methods on the task of predicting links in the collection of network datasets shown in Table 2. We split each network into training, validation and test sets of edges. The splits are chosen by timestamps. Each test set contains 30% of the total number of edges, and each validation set contains 30% of the number edges in each combined training and validation set. We then prune all nodes outside the giant connected component in the training set, in order to avoid predicting edges between unconnected parts of the network.

As a performance measure, we use the mean average pre-cision (MAP) [23] in the following way: For each vertex adjacent to edges in the test set, we compute the average precision of predicted edges, and compute the mean over all these average precisions. For unweighted and positively weighted networks (such as communication networks), the task is to predict the location of edges, and the mean av-erage precision is computed in relation to that goal, i.e. we count an edge as correctly predicted if it is present at least once in the test set. For other edge weights, we count an edge as correctly predicted when it has positive weight for signed networks, or if it has weight greater than the average for rating networks.

We compare the spectral extrapolation method with graph kernels and non-spectral link prediction methods. The graph kernels are learned using the method of [16]. The non-spectral link prediction methods are those described in [22]. A summary of all evaluated methods is given in Table 1. Figure 7 shows the extrapolation method applied to the English Wikipedia hyperlink graph. The numerical results are in Table 2.

These results show that the spectral extrapolation method outperforms graph kernels in some cases, but not always. In several cases, graph kernels learned using the method of [16] predict links with higher accuracy than spectral extrapola-tion. This can be explained either by growth following graph kernels very closely, e.g. the Wikipedia votes in Figure 6, or by overfitting, as for the Swedish Wikipedia.

The Wikipedia edit graphs are available for all Wikipedia languages separately, and a cross comparison suggests that the spectral extrapolation method works better for large graphs. The English Wikipedia is not included as it was too large to process, with about 200 million edits.
We can conclude from the experiments that the spectral evolution model results in a single link prediction algorithm Figure 7: Applying spectral extrapolation to the Wikipedia hyperlink graph. Spectral extrapolation is computed using only the decompositions at the split points between the training and validation and validation and test sets. suitable for networks that grow spectrally, and that makes the choice of a specific graph kernel obsolete. As we showed in Section 3, networks from all application areas are observed to grow spectrally. Therefore, the spectral extrapolation model can be applied universally to link prediction problems of any kind, replacing individual graph kernels in all kinds of link prediction applications.
As observed in several examples, a latent dimension can overtake another. In these cases, our model predicts cross-ings in the spectral plot. Looking more closely at actual spectra, we however observe something different: The smaller, growing eigenvalue slows down growth before it reaches the larger eigenvalue, and the larger eigenvalue starts growing at about the same rate. We observe this behavior in several datasets, as shown in Figure 8.

This phenomenon is called an avoided crossing and can be explained as follows [18, 8.5]. Let A and B be two symmet-ric rank-1 matrices of the same size. Then the eigenvalues of A + t B for a real parameter t display the avoided crossing Figure 8: An avoided crossing as observed in the growth of the Wikipedia link network X  X  two domi-nant eigenvalues and eigenvectors in (a), and in a rank-1 model in (b). The red and green color com-ponents denote the cosine distance of eigenvectors to initial eigenvectors. behavior. This behavior is explained by the fact that the two eigenvectors of A and B are not orthogonal in the gen-eral case and by considering the dimension of matrices with multiple eigenvalues.

In the rank-1 case, let A =  X aa T and B =  X bb T , then  X  and t X  are only eigenvalues of A + t B if a and b are collinear or orthogonal. Otherwise, an avoided crossing is observed as in Figure 8, where the color and brightness of points encodes the dot product of the corresponding eigenvectors with the initial eigenvectors, using green for the first and red for the second eigenvector. The plots show the two dominant sin-gular values of the Wikipedia link network, and a rank-1 simulation.

Thus, an avoided crossing indicates that a decomposition into outer products of orthogonal vectors is not the natural representation for some networks. If the true decomposi-tion using a and b is used, the crossing would be unavoided . Alternatively, an avoided crossing may result from a pertur-bation of orthogonal latent dimensions in the same manner.
A related but different link prediction method consists of computing a joint diagonalization of a network X  X  adjacency matrices at different times [31]. By construction, the result-ing network evolution is spectral, and the approximation at each timepoint is less precise than the eigenvalue or singular value decomposition as described in this paper.

In fact, our spectral evolution model provides a justifica-tion for these advanced methods, since joint diagonalization implies constant or near-constant eigenvectors.
The spectral transformation of kernel matrices has been used before in semi-supervised learning settings [5, 34]. These previous works however do not observe which spectral trans-formation applies to the used datasets, and are restricted to basic spectral transformations such as rank reduction and linear kernels. These works also do not take into account the temporal evolution of the data.
Other graph kernels than those presented here can be de-fined as the spectral transformation of either the combina-torial or normalized Laplacian matrix D  X  A , where D is the diagonal degree matrix. In our experiments, we found the Laplacian eigenvalues to not grow in a regular fashion, but instead  X  X ump X  at unpredictable times. In the literature, these kernels are mostly used for spectral clustering rather than link prediction [27].
The spectral evolution model states that in many real-world networks, growth can be described by a change of the spectrum, while the corresponding eigenvectors remain constant. This assumption is true to a large extent in most networks we analysed in many different types of networks. A limitation however exist: The observed phenomenon of avoided crossings hints at non-orthogonal latent dimensions.
We then derived a link prediction algorithm based on the spectral evolution model. This method generalizes sev-eral spectral link prediction functions. In actual networks, this method provides more accurate link prediction in many cases, in particular in networks with irregular but still spec-tral growth. For networks with very regular growth, regular graph kernels perform better however. Across all datasets, the performance of our method was better than any single link prediction method. Our new extrapolation method is also parameter free: Not only are there no parameters, as in various graph kernels, but our method makes the choice of a specific spectral growth model unnecessary.

Comparing different types of networks, the spectral evo-lution model seems to be true for bipartite user X  X tem net-works and true to a lesser extent for user X  X ser networks. We conjecture an application of spectral evolution to non-orthogonal matrix decompositions, based on the observa-tions of avoided crossings. We conjecture that a step to-wards this goal may be to consider the non-diagonal entries
Finally, the spectral evolution model provides the justifi-cation for more complex link predictions methods such as those based on tensor decomposition, by interpreting the eigenvalue decomposition over time as a joint diagonaliza-tion problem.
Acknowledgment. This research has been co-funded by the EU in FP7 in the WeKnowIt project (215453). [1] R. Albert, H. Jeong, and A.-L. Barab  X asi. The diameter [2] J. Bennett and S. Lanning. The Netflix prize. In Proc. [3] K. Bollacker, S. Lawrence, and C. L. Giles. CiteSeer: [4] L. Bro X zovsk  X y and V. Pet X r  X  X  X cek. Recommender system [5] O. Chapelle, J. Weston, and B. Sch  X  olkopf. Cluster [6] J. Ding and A. Zhou. Eigenvalues of rank-one updated [7] K. Emamy and R. Cameron. CiteULike: A [8] F. Fouss, L. Yen, A. Pirotte, and M. Saerens. An [9] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. [10] GroupLens Research. MovieLens data sets. [11] B. H. Hall, A. B. Jaffe, and M. Trajtenberg. The [12] A. Hotho, R. J  X  aschke, C. Schmitz, and G. Stumme. [13] J. Kandola, J. Shawe-Taylor, and N. Cristianini. [14] B. Klimt and Y. Yang. The Enron corpus: A new [15] R. Kondor and J. Lafferty. Diffusion kernels on graphs [16] J. Kunegis and A. Lommatzsch. Learning spectral [17] J. Kunegis, A. Lommatzsch, and C. Bauckhage. The [18] P. D. Lax. Linear Algebra and Its Applications . John [19] J. Leskovec. Stanford network analysis project. [20] J. Leskovec, L. Backstrom, R. Kumar, and [21] M. Ley. The DBLP computer science bibliography: [22] D. Liben-Nowell and J. Kleinberg. The link prediction [23] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [24] P. Massa and P. Avesani. Controversial users demand [25] A. Mislove. Online Social Networks: Measurement, [26] A. Mislove, H. S. Koppula, K. P. Gummadi, [27] A. Radl, U. v. Luxburg, and M. Hein. The resistance [28] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [29] D. Stewart. Social status in an open-source [30] G. W. Stewart. Perturbation theory for the singular [31] J. Sun, D. Tao, and C. Faloutsos. Beyond streams and [32] B. Viswanath, A. Mislove, M. Cha, and K. P.
 [33] Wikimedia Foundation. Wikimedia downloads. [34] X. Zhu, J. Kandola, J. Lafferty, and Z. Ghahramani. [35] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
