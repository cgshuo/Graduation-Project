 One of the core tasks in Opinion Mining consists of esti-mating the polarity of the opinionated documents found. In some scenarios (e.g. blogs), this estimation is severely af-fected by sentences that are off-topic or that simply do not express any opinion. In fact, the key sentiments in a blog post often appear in specific locations of the text. In this paper we propose several effective and robust polarity de-tection methods based on different sentence features. We show that we can successfully determine the polarity of doc-uments guided by a sentence-level analysis that takes into account topicality and the location in the blog post of the subjective sentences. Our experimental results show that some of our proposed variants are both highly effective and computationally-lightweight.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Theory, Experimentation Opinion mining, blog retrieval, sentence retrieval, polarity estimation, efficiency
The rise of social networks and blogs has led to new oppor-tunities and challenges regarding how to effectively deal with the opinionated nature of these resources. Our research is fo-cused on one of the most important web sentiment-oriented resources, the blogosphere. People frequently read blogs to determine the viewpoints of others. In order to build an ef-fective information retrieval (IR) system that helps users to find out what other people think, we need to have an under-standing regarding whether or not an opinion is present in a text and, if so, whether that opinion is positive or negative [14].

In recent years, several studies have been conducted to determine opinions in blog posts. Most of these address this challenge as a two-stage process that involves a topic retrieval stage (i.e. retrieve on-topic documents given a user query), and a re-ranking stage that takes into account opinion-based features [12]. In blogs, this second Opinion Mining (OM) stage usually involves two further subtasks: an opinion-finding task, where the main aim is to find opin-ionated posts related to the query, and a subsequent polarity task to identify the orientation of a blog post with respect to the topic (e.g. positive, negative or neutral).
We are concerned here with polarity estimation, a chal-lenging area that is much more demanding than topicality estimation [14]. One problem in polarity estimation is that there may be conflicting opinions in a blog post. For exam-ple, a blog writer may summarise pros and cons of a particu-lar argument before settling on an overall recommendation. This mixed set of opinions severely affects the quality of automatic methods designed to estimate the overall orienta-tion of the post. This issue is illustrated particularly well in the following example (taken from a popular film reviews X  blog 1 ). Despite the start of the post being predominantly negative, with several negative comments being made, the overall recommendation is positive:
Observe that the location of the subjective sentences may offer important clues when attempting to establish the polar-ity of the blog post. In the example above the last sentence is the one that expresses the overall view. Nevertheless, ex-isting literature on blog polarity estimation has disregarded this valuable information. For instance, the most effective polarity systems participating in the TREC blog tracks [12, 7] do not incorporate any feature based on this flow of sen-timents, but rather focus on a document-level estimation http://blog.moviefone.com of polarity that combines relevance to the topic with some sort of global orientation of the sentiments in the document (e.g. counting positive/negative terms). We argue that this is a rather strong simplification and claim that more effec-tive polarity estimation methods can be designed using a sentence-level approach.

In more restricted scenarios, such as corpus of movie or product reviews, some authors have found that the location of the sentiments could be valuable [13, 15, 1]. However, in blogs, the presence of noise (off-topic information or on-topic information that is non-opinionated) makes it difficult to locate the key polar sentences. In this respect, our thorough study of polarity gives useful insights on how sentiments are expressed in blog posts.

In the literature, it has been shown that the noise intro-duced by off-topic content in documents is a major issue that needs to be addressed to facilitate progress in OM [3, 19]. Therefore, we propose a refined analysis of the documents that takes into account not only the location of the senti-ments, but also their relevance to the query. More specifi-cally, we propose effective algorithms that consider two main factors when determining the key sentences for polarity esti-mation: the relatedness of the sentence and the query topic, and the location of a sentence in the post. We argue that this information, combined with evidence of polarity (i.e. positive/negative terms in the sentence), is extremely valu-able when attempting to detect the overall orientation of a post.

The contributions of this paper are:
The rest of the paper is organized as follows. In sec-tion 2 we explain the blog subjectivity and polarity esti-mation methods, and the methodology followed to combine sentence-level information with document scores. Sections 3 and 4 report the experiments and analyze their outcomes. Section 5 presents related work. The paper ends with Sec-tion 6, where we present the conclusions and outline our ideas for future work.
As we argued above, the noise introduced by off-topic con-tent in documents may severely harm OM performance be-cause documents might have query terms in a wrong con-text. Moreover, finding robust OM techniques that can be applied effectively across different underlying topic retrieval baselines is a real challenge. In the past TREC Blog tracks most polarity approaches did not give any added value over the topic retrieval baseline (meaning that the baseline, with no polarity-oriented capabilities, is not beaten by these ap-proaches) [12]. Actually, it is interesting to note that only one participant in TREC 2008 had on average improved the polarity performance of the five topic retrieval baselines pro-vided by the task. This illustrates how difficult it is to design effective polarity estimation methods.
 To deal with this challenging problem, we define a general OM polarity approach that involves searching for on-topic polar sentences and location-aware estimation of the docu-ment polarity. Given an initial topic retrieval baseline, we work at sentence level to find positive and negative sentences related to the query. Next, our method builds a ranking of positive posts and a ranking of negative posts by aggregat-ing relevance scores and sentence-level polarity information. Within this process we study different location-aware strate-gies to represent the overall view of the post. An alternative approach with no polarity capabilities is also studied. This final method, which is explained in subsection 2.4, simply promotes subjective documents (regardless of their polar-ity) and serves as a reference comparison.

The resources utilized in this research to estimate the opinions expressed in texts, and the methods to compute polarity and subjectivity scores based on these resources are explained below. To estimate subjectivity and polarity we use Opinion-Finder (OF) [22] 2 . OF is a state of the art subjectivity classifier that works as follows. First, the text is processed using part-of-speech tagging, name entity recognition, tok-enization, stemming, and sentence splitting. Next, a pars-ing module builds dependency parse trees where subjective expressions are identified using a dictionary-based method. This is powered by Naive Bayes classifiers that are trained using subjective and objective sentences. These sentences are automatically generated from a large corpus of unanno-tated data by two high-precision, rule-based classifiers.
Sentences are classified by OF as subjective or objective (or unknown if it cannot determine the nature of the sen-tence). Two classifiers are implemented: accuracy classifier and precision classifier. The first one yields the highest over-all accuracy. It tags each sentence as either subjective or objective. The second classifier optimizes precision at the expense of recall. It classifies a sentence as subjective or objective only if it can do so with confidence. Furthermore, OpinionFinder marks various aspects of the subjectivity in the sentences, including the words that are estimated to ex-press positive or negative sentiments or the confidence of the decisions made for the accuracy classifier ( diff ).
In order to have a precise representation of the mixed set of opinions in a blog post, we compute polarity at sentence level.

With the polar terms tagged by OF [23] we can naturally define the positive or negative polarity score of a sentence. To promote polar sentences that are on-topic, we run a sen-tence retrieval process to determine the relatedness between the query and each polar sentence. More specifically, we use the Lemur [20] implementation of tf-idf, with BM25-like weights 3 .

The combination of relevance and polarity is done through www.cs.pitt.edu/mpqa/opinionfinderrelease We build a sentence-level index and apply the well-known BM25 suggested configuration ( k 1=1 . 2 ,b =0 . 75), which has proved to be very robust in many retrieval experiments [18]. linear interpolation: where rel norm ( S, Q ) is the Lemur X  X  tf-idf score after a query-based normalization into [0,1] and pol ( S ) represents the num-ber of positive (resp. negative) terms tagged in the sentence S divided by the total number of terms in S 4 .  X   X  [0 , 1] is a free parameter.
Our objective in this paper is to apply sentence-level fea-tures combined with location information to improve blog polarity estimation. To this end we score sentences using eq. 1, but only take into account those subjective sentences that have at least one term tagged as positive/negative (i.e. sentences with pol ( S ) equal to 0 are discarded). We refer to the these sentences as polar sentences. To aggregate the individual sentence polarity scores in a document-level po-larity measure we work with the following alternatives to define a document polarity score ( pol S ( D,Q )):
Finally, we combine relevance and polarity evidence as follows: pol ( D,Q )=  X   X  rel norm ( D,Q )+(1  X   X  )  X  pol S ( D,Q )(2) where rel norm is the document X  X  relevance score (obtained from the initial topic retrieval baseline) after a query-based normalization in [0,1], pol S ( D,Q ) is one of the aggregation alternatives sketched above and  X   X  [0 , 1] is a free param-eter. Note that some aggregation techniques have an extra parameter: the number of sentences ( n ). By studying the behavior of this parameter we might discover valuable pat-terns about the way in which bloggers express their views.
For positive document retrieval pol ( S ) is the percentage of positive terms in the sentence, and for negative document retrieval pol ( S ) is the ratio of negative terms in the sentence.
As an alternative approach focused only on subjectivity, we use the proportion of subjective sentences in each re-trieved post and the accumulated confidence about their subjectivity (OF X  X  confidence [17, 21]) as subjectivity in-dicators. This approach has been adopted successfully in other studies [5]: where # subj and # sent are the number of subjective sen-tences and the number of sentences in a document, respec-tively. sumdiff is the sum of the confidence values of the subjective sentences in the document.

Next, we combine the relevance and subjectivity scores to promote subjective documents that are on-topic. The function used to combine the topic and subjective score of a document is simply: where rel norm is the normalized relevance score (obtained from the initial topic retrieval baseline) and subj norm is a query-based normalization of eq. 3.  X   X  [0 , 1] is a free pa-rameter. This method, with no polarity capabilities, serves as a reference comparison for polarity-oriented approaches.
In our evaluation we aim at answering the following re-search questions: 1. Are sentence-level methods effective for polarity esti-2. Is sentence location valuable in blog polarity estima-3. How does performance change as we focus the estima-
To answer the first question, we apply the sentence-level methods defined in subsection 2.3 to a set of different base-lines provided by TREC. This helps to assess the impact of our polarity techniques across a range of different topic-relevance baselines. We also compare the performance of our strategies against the best polarity methods participat-ing in TREC and against the subjectivity method sketched in subsection 2.4. We address the second question by analyz-ing whether we can achieve competitive performance taking only a few sentences from each post (the initial/last polar sentences). We answer the third question by studying the relationship between the number of sentences used and the performance achieved by our location-aware methods. This study takes into account both effectiveness and efficiency.
The remainder of this section describes the experimental setup used to support these investigations.
Our experiments are based on the BLOGS06 collection [8], which is one of the most renowned blog test collections with relevance, subjectivity and polarity assessments. Some statistics of the collection are reported in Table 1. Table 1: The main statistics of the BLOGS06 collec-tion. This collection was utilized in the TREC 2006, TREC 2007 and TREC 2008 blog tracks
We worked with the TREC 2006, TREC 2007 and TREC 2008 blog track X  X  benchmarks. All of them utilize the BLO-GS06 document collection as the reference collection for the retrieval experiments. Every year a new set of topics was provided and new judgments were made according to the documents retrieved by the participants. Details about these topics are reported in Table 2.
 Table 2: Topics provided in the TREC Blog tracks
Each TREC topic contains three different fields (i.e. ti-tle, description and narrative) but we only utilized the title field, which is short and the best representation of real user web queries, as reflected in the official TREC Blog track lit-erature [11, 9, 12]. Documents and topics were preprocessed with Krovetz stemmer and 733 English stopwords were re-moved.

For the assessment, the content of a blog post was defined as the content of the post itself and the contents of all com-ments to the post (i.e. permalink document). Documents were judged in two different levels by TREC assessors:
Blog web pages have noisy information within their inter-nal structure (e.g. links and advertisements). To remove such noise we built a preprocessing unit which can identify the main permalink components (i.e. title, post and com-ments) and discard the rest of the documents X  content. This unit uses a common HTML parser [6] to process the struc-ture of permalinked documents and a set of heuristics to find thecorecomponents. Themainideaistodetectpiecesof text in different HTML blocks and then classify them ac-cording to their positional information and size. This type of heuristic has been used successfully in other contexts [16]. We only used the information from title and post, because the comments could be misleading. Deciding how to use comments to effectively guide the estimation of polarity of the document is an interesting challenge that is out of the scope of this paper.
In TREC 2008, to allow the study of the performance of a specific opinion-finding technique across a range of dif-ferent topic-relevance baseline systems, a set of five topic-relevance baselines was provided. These standard baselines use a variety of different retrieval approaches, and have vary-ing retrieval effectiveness 5 . Participants were encouraged to apply their opinion-finding techniques on as many standard baselines as possible. This aims at drawing a better un-derstanding of the most effective and stable opinion-finding techniques, by observing their performance on common stan-dard topic-relevance baselines. Here we adopt this eval-uation methodology and apply our methods on these five topic-relevance baselines to assess the robustness of our tech-niques.
The polarity task was introduced in TREC 2007 as a nat-ural extension to the opinion-finding task. This task was initially defined as a classification problem where the sys-tems had to identify the real polarity of a blog post (i.e. positive, negative or mixed). To draw a better simulation of a real user search scenario the task was redefined in TREC 2008 as a typical adhoc retrieval task where the systems had to return a ranking of positive opinionated documents and a ranking of negative opinionated documents. We follow this
The baselines were selected by TREC from the runs sub-mitted to the initial ad-hoc retrieval task in the TREC blog track. These baselines were introduced in 2008 but runs were provided not only for the TREC 2008 topics but also for earlier topics.  X  [0..1] 0.1 Doc. Subjectivity (Comb.) eq. 4  X  [0..1] 0.1 Sentence Polarity (Comb.) eq. 1  X  [0..1] 0.1 Doc. Polarity (Comb.) eq. 2 n [1..10] 1 Number of Sentences eq. 2 Table 4: Parameters to train: the interval, the step used to train, a description and the formula affected by the parameters. definition of the polarity task. The measures applied to eval-uate performance are mean average precision (MAP), and precision at 10 documents (P@10).
We built two realistic and chronologically organized query datasets with the topics provided by TREC. Details about these configurations are shown in Table 3. The training topics were used to set all the parameters of our methods. In Table 4 we report some details about the parameters and their characteristics.

Two different training-testing processes focused on max-imizing MAP were ran: one for positive polarity retrieval and another for negative polarity retrieval.
In this section, we analyze the results obtained with the datasets described above and address our three research questions. Subsection 4.1, which is related to our first ques-tion, reports the results of our polarity estimation methods. Subsection 4.2 addresses the second question, by investigat-ing how location-aware methods work in comparison with other approaches that do not take into account location-based information. Finally, in subsection 4.3 we cover the third research question by studying the impact of the num-ber of sentences used by our polarity approaches and the performance obtained.
Tables 5 and 6 show the results of the polarity retrieval approaches. Each run is evaluated in terms of its ability to rank positive (resp. negative) opinionated permalinks higher up in the ranking. In order to have an overall performance for each method, we compute the mean of the positive and negative MAPs and P10s of each run (denoted Mix MAP and Mix P10 respectively) 6 . The best value in each column for each baseline is underlined. Statistical significance was estimated using the paired t-test at the 95% level. The sym-bols and indicate a significant improvement or decrease
Do not confuse with mixed polarity documents, which refer to documents with mixed opinions. over the corresponding baseline. To specifically measure the benefits of our polarity methods we also compare their per-formance against the results obtained from the subjectivity method (eq. 4, results reported in the rows labeled as Subj). The symbols and indicate a significant improvement or decrease over the subjectivity method.

Sentence-level polarity methods. The technique that shows the best performance across all different baselines is PolMeanBestN . In TREC 2007, PolMeanBestN is the best method in 17 out of 30 cases, showing usually significant improvements in performance with respect to the baseline and with respect to the subjectivity method. PolMeanAllN performs the best in 6 cases and PolMeanLastN is the best approach in 4 cases. Although PolMeanFirstN was never the best option, their results are close to the best ones in most scenarios. We will go back to this issue in subsec-tion 4.2. Observe also that, on average (mix column), some methods yield to a statistically significant decrease in per-formance for one of the baselines in TREC 2007 (baseline5) but PolMeanBestN does not. In TREC 2008, the relative merits of the methods remain the same.

Subjectivity Method. Not surprisingly, subjectivity in-formation alone is not useful in polarity scenarios (the sub-jectivity method hardly shows any significant improvement in performance with respect to the baseline).

Positive vs Negative rankings. Another observation is that the performance of negative document rankings is quite poor. It is interesting to note that TREC systems (see Table 8) show similar trends. We argue that this is due to the dif-ficulty to retrieve negative posts. As a matter of fact, these collections have many more positive documents than nega-tive ones. The difference is greatest in TREC 2007, where the number of positive documents is 2960 and the number of negative documents is 1844. In TREC 2008, the difference between the number of positive and negative documents is not so marked (3338 against 2789).

Comparison against TREC Systems. To put things in perspective, we report in Table 8 how our methods com-pare with those proposed by teams participating in TREC [12]. Here, we show the mean of the relative improvements over the five standard baselines. Observe that this polarity task is quite challenging: most TREC polarity systems failed to retrieve more positive or negative documents than the baselines 7 . The methods proposed in this paper perform as well as the best TREC polarity approach (KLE, Pohang Uni-versity of Science and Technology) [7], showing better per-formance for some configurations. Observe that our meth-ods and these TREC systems were evaluated under the same testing conditions. Going back to our first research question, these results show that sentence-level methods are an effec-tive strategy for polarity estimation, performing comparably to state-of-the-art TREC systems.

Parameters Trained. Table 7 reports the parameter values trained for each method. Observe that, although the methods proposed have up to three parameters, their opti-mal values are quite stable across collections.
 The subjectivity approach gets a high value of  X  (0.9). This parameter controls the relative weight of relevance over subjectivity in eq. 3. The value of this parameter indicates
We can only report the 2008 results because the polarity task was not defined as a ranking process until TREC 2008. Therefore, there are not official results for systems with ear-lier topics. 9  X  =0 . 6 , X  =0 . 8  X  =0 . 3 , X  =0 . 7 8 ,n =1  X  =0 . 6 , X  =0 . 6 ,n =3  X  =0 . 2 , X  =0 . 5 ,n =1 9 ,n =5  X  =0 . 5 , X  =0 . 8 ,n =3  X  =0 . 2 , X  =0 . 9 ,n =9 9 ,n =1  X  =0 . 5 , X  =0 . 8 ,n =3  X  =0 . 2 , X  =0 . 8 ,n =1 that the relevance component is much more important than the subjectivity component. This seems to indicate that the subjectivity approach is extremely sensitive to off-topic material.

Regarding  X  , we observe different trends in positive and negative polarity rankings. Positive rankings have lower val-ues of  X  (the value of this parameter is around 0.2 for positive document retrieval and around 0.5 for negative document re-trieval). The  X  parameter controls the trade-off between rel-evance and polarity at sentence level (see eq. 1). This means that in positive rankings the polarity evidence is more im-portant than content-match evidence. This might be due to a more reliable estimation of polarity for positive sentences (i.e. OF might be more reliable for positive polarity estima-tion) or it might be due to the presence of more noisy text (off-topic sentiments) in negative documents. This will be subject to further research in our future work.

Another important trend found affects the number of sen-tences used by PolMeanFirstN and PolMeanLastN (i.e. the parameter n ). In general, PolMeanFirstN takes more sen-tences to estimate polarity than PolMeanLastN .Thisfact seems to indicate that bloggers briefly summarise their views in the last part of the post. In contrast, if we want to have a reliable summary of the overall opinion obtained from the initial part of the post we need to take a longer subset of sentences. From Table 7 it is also interesting to observe that the number of sentences used by PolMeanBestN was 1 in most of the cases. This indicates that we can use the highest pol -sentence as the best guidance to understand the overall sentiment of a blog. This finding can be also use-ful, for example, to build polarity-biased snippets in a blog retrieval scenario.
The results reported above suggest that the best way to estimate the overall polarity of a post is to take the highest-pol sentences as a representation of the sentiments of the author. However, the methods based on sentences taken from specific document locations work often quite well. Fig-ure 1 depicts the evolution of performance of PolMeanFirstN and PolMeanLastN against the number of polar sentences taken. For each point in the plot, a indicates a signifi-cant decrease in performance over PolMeanBestN , while a  X  indicates a non-significant difference in performance with respect to PolMeanBestN . With few polar sentences the per-formance is not statistically different to the performance achieved by the best method. Interestingly, the number of sentences needed to achieve similar performance with re-spect to the best method differs between PolMeanFirstN and PolMeanLastN .With PolMeanLastN , the last two polar sentences are enough to have a level of effectiveness that is not statistically different to PolMeanBestN (for both MAP and P@10). With PolMeanFirstN , the initial four polar sen-tences seem to be a good choice to estimate polarity (with fewer sentences we obtain statistically significant decreases for some measure in some of the collections).

We have therefore successfully addressed our second re-search question: the use of location information is valuable in blog post polarity estimation, because the first four or last two polar sentences of a blog are good indicators of the overall sentiment.
In the previous section we have compared the performance of the best blog polarity estimation method ( PolMeanBestN ) against the location-aware methods. The reader might won-der why we should bother with these location-based meth-ods if we can achieve state-of-the-art performance with Pol-MeanBestN . In this respect, we argue that there are im-portant implications in terms of efficiency. PolMeanBestN , PolMeanAllN and Subj need to classify all sentences in the post to compute the polarity score of a document. In con-trast, the location-based methods just need to classify a small set of sentences.

In the literature, many authors have expressed their con-cerns about efficiency when using tools such as OF for opi-nion-finding [5, 4]. We argue that by reducing the amount of data we can substantially decrease the computational cost associated to polarity estimation. To further explore this issue, we took a random sample of 100 documents from the BLOGS06 text collection that had a mean of 6.5 polar sentences according to OF 8 . For each document we created new files based on the first four or the last two polar sen-tences (appropriate configurations for the PolMeanFirstN and PolMeanLastN methods, respectively, as discussed in subsection 4.2). For example, for the first four polar sen-tences of a document, we built a new file that contains the text of all sentences until the fourth polar sentence is found. Finally, we applied OF on each file and on the original docu-ment and recorded the average time needed to process each file (preprocessing, tagging and classification). In Table 9 we report the results of this experiment. The use of location-aware methods to estimate the polarity of a blog has a very positive impact in terms of efficiency. PolMeanLastN and PolMeanFirstN reduce substantially the computation time
The mean of polar sentences per document in the collection is 6.45. The standard deviation is 27.03. This high deviation is likely because of the presence of smap documents, which tends to be large. respect to baselines are bolded. decrease in performance over the PolMeanBestN method, while a performance with respect to the PolMeanBestN method. Table 9: Average time taken to classify complete documents vs the time taken to classify narrower subsets containing the first/last polar sentences. with respect to the full document approach (time required is reduced by 51.5% and 30.4%, respectively).

Observe that this classification, which is required to com-pute pol ( S ), can be done offline (at indexing time) but, still, there are also benefits on-line. With PolMeanBestN it is necessary to process all sentences at query time (to compute pol S ( D,Q ) in eq. 2) while PolMeanFirstN and PolMean-LastN only need to score a small set of sentences. Observe also that the best TREC polarity system (KLE) also treats the full document to find opinionated terms [7]. We argue that location-aware methods are more convenient because they get to similar levels of effectiveness with little com-putational effort. Furthermore, our findings are potentially applicable not only to learning approaches such as those based on OF but also to other methods that currently pro-cess whole documents.

This study answers our third research question: we can substantially improve the efficiency of the OM processes by focusing on small sets of sentences (initial/last polar sen-tences), and this reduction in the representation of the post does not affect the effectiveness of the polarity estimations.
Many opinion detection approaches have been proposed in the literature. Among the most successful studies in this subject are those focused on finding document contents that are both opinionated and on-topic [19, 3]. To meet this aim, some papers consider term positional information to find opinionated information related to the query. Santos et al. [19] applied a novel OM approach that takes into account the proximity of query terms to subjective sentences in a docu-ment. Gerani et al. [3] proposed a proximity-based opinion propagation method to calculate the opinion density at the position of each query term in a document. These two stud-ies led to improvements over state of the art baselines for blog opinion retrieval. Although we focus on polarity esti-mation (rather than opinion finding), we also need to filter out off-topic material. In this respect, we worked here with simple sentence retrieval methods and showed that they are consistent to estimate which polar sentences are on-topic.
Pang and Lee [13] considered the impact of the location of the opinionated sentences on the accuracy of two state-of-the art polarity classifiers of film reviews. They built polarity classifiers based on sentences from different parts of a doc-ument (e.g. first sentences, last sentences), however these classifiers were not able to overcome local-unigram state-of-the-art systems. Nevertheless, the results obtained showed that the last sentences of a document might be a good indi-cator of the overall polarity of the review.

Pang et al. [15] considered the impact of term positions in polarity classifiers and argued that the position of a word in the text might make a difference (e.g. movie reviews nor-mally conclude by summarising the author X  X  overall view). Each word was tagged according to whether it appeared in the first quarter, last quarter, or middle half of the document and this information was incorporated in a state-of-the-art unigram classifier. The results did not differ greatly from those obtained using unigrams alone, but the authors argue that the study of more refined notions of positions could be useful in polarity retrieval scenarios.

Beineke et al. [1] proposed several sentiment summarisa-tion approaches based on the analysis of data from a popular film reviews website 9 . This study revealed that the first and the last sentences of the reviews are more important for sum-marising opinions. To show the importance of the sentence locations, an automatic classifier was built based on two kind of sentence-location features: location within paragraph (i.e. opening, ending, interior or complete paragraph) and lo-cation within document (as the fraction of the document that has been completed until the sentence appears). These features were utilized in film reviews to predict whether a particular span of text should be chosen as a summary sen-tence. The authors found that the use of location-based features alone were insufficient to create proper summaries, being the best results achieved by a classifier that incorpo-rated both term frequencies in the sentences and positional information of the sentences.

In [10], Mao and Lebanon predict the global sentiment of a document by analyzing the sentiment flow at sentence-level. The results indicate that the classification performance is better than a bag of words approach.

In our paper we revisited these issues and studied whether this location information is also a good guidance in blogs (where, typically, we have much more noise than in movie reviews). The techniques proposed here are more sophisti-cated than the methods applied in film or product reviews, which simply take into account the first or the last part of a document. We analyzed here the first/last sentences that are subjective with respect to the query . This allowed us to successfully incorporate location-based information into a large-scale multi-topic scenario, such as the blogosphere. Furthermore, we conducted experiments to understand the implications of location-aware methods in efficiency.
In this paper we have investigated the impact of sentence-level information in a challenging problem: polarity estima-tion in blogs. In particular, we have deeply studied different ways to aggregate sentence-level evidence into a document polarity measure. We have also assessed the impact of sen-tence locations in polarity estimation and evaluated the per-formance of these techniques in terms of effectiveness and efficiency.

From the results obtained, we found that location-aware polarity methods yield state-of-the-art performance, which is robust across different topic-relevance baselines. We were also able to detect some patterns related to the way in which people write in blogs. More specifically, the overall polarity of posts relies on a few specific sentences (taken from the beginning, from the end, or from the set of high polarity sentences related to the query). This result could be also valuable for creating polarity-biased snippets. We have also www.rottentomatoes.com demonstrated that we can improve efficiency with no impact on effectiveness.

Most of the methods proposed in this paper are based on simple combinations of polarity and topicality. We are aware that there might be better and more formal ways to approach this combination of evidence (e.g. subjectivity and relevance might be combined using formal methods to learn query-independent weights [2]). This will be explored in the near future. Another problem relates to the number of free parameters to train. Although the optimal parameter values seem to be stable across collections, we plan to study alternative ways to introduce location information in our models. Related to this, we are also interested in studying more refined ways of representation of the sentiment flow of the documents. We also expect to explore the benefits of the use of sentence location for creating opinion-biased summaries.
 The authors thank the financial support given by Minis-terio de Ciencia e Innovaci  X on through research project ref. TIN2010-18552-C03-03. We would also like to thank David Elsweiler,  X  Alvaro Barreiro and Leif Azzopardi for their use-ful comments and suggestions. [1] P. Beineke, T. Hastie, C. Manning, and [2] N. Craswell, S. E. Robertson, H. Zaragoza, and M. J. [3] S. Gerani, M. J. Carman, and F. Crestani.
 [4] B. He, C. Macdonald, J. He, and I. Ounis. An effective [5] B. He, C. Macdonald, and I. Ounis. Ranking [6] M. Jericho. Jericho HTML parser. [7] Y. Lee, S.-H. Na, J. Kim, S.-H. Nam, H.-Y. Jung, and [8] C. Macdonald and I. Ounis. The TREC Blogs 2006 [9] C. Macdonald, I. Ounis, and I. Soboroff. Overview of [10] Y. Mao and G. Lebanon. Sequential models for [11] I. Ounis, C. Macdonald, M. de Rijke, G. Mishne, and [12] I. Ounis, C. Macdonald, and I. Soboroff. Overview of [13] B. Pang and L. Lee. A sentimental education: [14] B. Pang and L. Lee. Opinion mining and sentiment [15] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [16] J. Parapar and A. Barreiro. An effective and efficient [17] E. Riloff and J. Wiebe. Learning extraction patterns [18] S. Robertson. How okapi came to TREC. E.M.
 [19] R. L. T. Santos, B. He, C. Macdonald, and I. Ounis. [20] The Lemur Project. http://www.lemurproject.org/ , [21] J. Wiebe and E. Riloff. Creating subjective and [22] T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler, [23] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing
