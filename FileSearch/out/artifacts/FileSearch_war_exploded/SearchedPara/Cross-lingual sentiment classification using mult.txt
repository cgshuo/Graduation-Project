 fi cation aims to utilize annotated sentiment resources in one language 1. Introduction
Along with the rapid increase of internet access in the world today, the volume of user-generated content on the web has also intensi fi ed. Due to the enormous amount of this content, the task of summarizing their information into a useful format is a very dif and challenging problem. This challenge has motivated the natural language processing (NLP) communities to design and develop computational methods by which to mine and analyze the informa-tion of these text documents. Opinion mining or sentiment analysis is one of the most interesting fi elds in this area; it analyzes people's opinions, attitudes and sentiments towards entities such as pro-ducts, individuals, events, etc. ( Liu, 2012 ). Text sentiment classi tion refers to the task of determining the sentiment polarity (e.g. positive or negative) of a given text document and has received considerable attention due to its many useful applications in product review classi fi cation ( Zhou et al., 2013 ) and opinion summarization ( Ku et al., 2006 ).

Up until now, different methods have been used in sentiment classi fi cation. These methods can be categorized into two main groups, namely: lexicon-based and corpus-based. The lexicon-based methods classify text documents based on the polarity of words and phrases contained in the text ( Taboada et al., 2011; Turney, 2002 ). This group of methods requires a sentiment lexicon to distinguish between positive and negative terms. In contrast, corpus-based methods train a sentiment classi fi er based on a labelled corpus using machine learning classi fi cation algorithms ( Moraes et al., 2013; Pang et al., 2002 ). The performance of these methods depends intensively on both the quantity and quality of labelled corpus items as the training set.

Sentiment lexicons and annotated sentiment corpora are the most important resources for sentiment classi fi cation. However, since most recent research studies in sentiment classi fi written in a limited number of languages (always English), this has led to a scarcity of labelled corpus and sentiment lexicons in other languages ( Mart X n-Valdivia et al., 2013; Wan, 2011 ). Further, manual construction of reliable sentiment resources is a very dif fi cult and time-consuming task. Therefore, the challenge is how to utilize labelled sentiment resources in one language (i.e. English) for sentiment classi fi cation into another language. This leads to an interesting research area called cross-lingual sentiment classi fi cation (CLSC). The most direct solution to this problem is the use of machine translation systems to directly project the information of data from one language into another ( Balahur et al., 2014; Banea et al., 2008; Mart X n-Valdivia et al., 2013; Prettenhofer and Stein, 2010; Wan, 2009, 2011 ). Most existing works in this area have used machine translation systems to translate labelled training data from the source language into the target language and perform sentiment classi fi cation into the target language ( Balahur and Turchi, 2014; Banea et al., 2010 ). Some other researchers have employed machine translation in the opposite direction to translate unlabelled test data from the target language into the source language and to perform the classi fi cation in the source language ( Hajmohammadi et al., 2014b; Mart X n-Valdivia et al., 2013; Prettenhofer and Stein, 2010 ). A limited number of research works have used both directions of translation to create two different views of the training and test data to compensate for some of the translation limitations ( Hajmohammadi et al., 2014a; Pan et al., 2011; Wan, 2009, 2011 ).

However, because the training set and the test set come from two different languages having differing linguistic terms and writing styles, as well as originating from different cultures, translated text documents cannot cover all the vocabularies contained in the original text documents. Therefore, these methods cannot attain the performance results of monolingual sentiment classi fi methods in which the training and test samples are from the same language. Using multiple resources from multiple languages can alleviate the problem of vocabulary coverage in CLSC. This occurs because some vocabularies which are not covered by the feature set extracted from translated documents of the one source language may be covered by the feature set of another source language. This means that feature sets extracted from the training data of multiple source languages can cover more vocabularies of test documents in the target language. For example, the translation of  X  awesome
French is  X  g X nial  X  but a word in German with the same meaning  X  fantastisch  X  is translated to  X  fantastique  X  in French. Both words  X  g X nial  X  and  X  fantastique  X  are used in the French reviews and each word is covered by a different source language. Therefore, using a multiple source language technique is expected to show better performance in CLSC when compared with models which use only one source language.

Different term distribution between the original and the translated text documents is another important factor that can reduce the performance of CLSC. It means that a term may be frequently used in one language to express an opinion while the translation of that term is rarely used in the other language.
To overcome this problem, making use of unlabelled data from the target language can be helpful, since this type of data is always easy to obtain and has the same term distribution as the test data.
Therefore, employing unlabelled data from the target language in the learning process is expected to result in better classi in CLSC.

In the light of dif fi culties for CLSC, we address the task of CLSC via a multi-view semi-supervised learning framework. Speci fi propose a new learning model that uses labelled data from multiple languages (in this paper, two languages) as multiple training data-sets. Both directions of translation are then used to create different views of data. These individual views are then employed in a multi-view semi-supervised learning process to incorporate unlabelled data from the target language in the learning process.
The contributions of our work are as follows: (1) utilizing training data and their translations from multiple source lan-guages in CLSC to cover more vocabularies of test documents in the target language; (2) employing a multi-view semi-supervised learning strategy in order to incorporate unlabelled examples from the target language in the learning process of CLSC. This is achieved by creating multiple views from the documents in both the source and the target languages through automatic machine translation and using the  X  majority teaching minority  X  strategy to select the most con fi dent pseudo-labelled examples from unla-belled documents and adding them to the training sets in each of the individual views.

The proposed model was applied to book review datasets in four different languages. Experiments showed that the use of this model obtained better performance in comparison with some baseline methods.

The reminder of this paper is organized as follows: the next section presents related works on CLSC. Section 3 describes multiple views data creation. The proposed model is described in Section 4 , while an evaluation is given in Section 5 . Finally, Section 6 concludes this paper and outlines ideas for future research. 2. Related works
Cross-lingual sentiment classi fi cation has been extensively studied in recent years. These research studies are based on the use of annotated data in the source language (always English) to compensate for the lack of labelled data in the target language.
Most approaches focus on resource adaptation from one language to another with few sentiment resources. For example, Mihalcea et al. (2007) generate subjectivity analysis resources into a new language from English sentiment resources by using a bilingual dictionary. In other works ( Banea et al., 2010; Banea et al., 2008 ), automatic machine translation engines were used to translate the
English resources for subjectivity analysis. In a further study ( Banea et al., 2008 ), the authors showed that automatic machine translation is a viable alternative to the construction of resources for subjectivity analysis in a new language. In two different experiments, they fi rst translated training data of subjectivity classi fi cation from the source language into the target language.
They then utilized this translated data to train a classi target language and applied this trained classi fi er to classify test data. Additionally, in another experiment, machine translation was used to translate test data from the target language into the source language and a classi fi er was then trained based on training data in the source language. After the training phase, the translated test data was presented to the classi fi er for sentiment polarity predic-tion. Wan (2008) used unsupervised sentiment polarity classi tion in Chinese product reviews. He translated Chinese reviews into different English reviews using a variety of machine transla-tion engines and then performed sentiment analysis for both the Chinese and English reviews using the lexicon-based technique.
Finally, he used ensemble methods by which to combine the analysis results. This method requires sentiment lexicon in the target language and cannot be applied to other languages with no lexicon resource. Pan et al. (2011) designed a bi-view non-negative matrix tri-factorization (BNMTF) model in an attempt to solve the problem of cross-lingual sentiment classi fi cation. They used the machine translation to achieve two representations of training and test data; one in the source language and another in the target language. This model was then used to combine the information from two views.

Another approach is that of feature translation, which involves translating the features extracted from labelled documents ( Moh and Zhang, 2012; Shi et al., 2010 ). The features, selected by a feature selection algorithm, are translated into different languages.
Subsequently, based on those translated features, a new model is trained for each language. This approach only needs a bilingual dictionary to translate the selected features. It can, however, suffer from the inaccuracies of dictionary translation, in that words may have different meanings in different contexts. Therefore, selecting the features to be translated can be an intricate process.
Prettenhofer and Stein (2010) investigated CLSC from the domain adaptation view by employing structural correspondence learning (SCL) ( Blitzer et al., 2006 ). They adapted SCL to use unlabelled data and a word translation oracle to induce correspondence among the words from both the source and target languages. They selected some word pairs called pivots and then identi fi correlations between pivots and other words in unlabelled docu-ments. After that, a map was extracted to associate the original representation of a document in the source and target languages with its cross-lingual representation. The classi fi cation was per-formed in the new mapped space.

In another work, Wan (2009, 2011) used the co-training algo-rithm to overcome the problem of CLSC. He fi rst investigated basic methods for CLSC by using machine translation services. He then exploited a bilingual co-training approach to leverage annotated
English resources to sentiment classi fi cation in Chinese reviews. In this work, fi rstly, machine translation services were used to translate
English labelled documents (training documents) into Chinese and similarly, Chinese unlabelled documents into English. The author used two different views (English and Chinese) in order to exploit the co-training approach into the classi fi cation problem. Co-training usually selects high con fi dence examples to add to the training data.
If, however, the initial classi fi ers in each view are not good enough, the probability of adding examples having incorrect labels to the training set will be increased. Therefore, adding  X  noisy not only cannot increase the accuracy of the learning model, but will also gradually decrease the performance of each classi fi 3. Multiple views creation
The fi rst step in the construction of a multi-view semi-super-vised learning model is the creation of different views of data. For this task, labelled training examples from each of the source languages are translated into the target language and combined to create training data in the target language. In another instance, unlabelled examples are translated from the target language into each of the source languages. Therefore, we have labelled and unlabelled examples in both the source and target languages.
In this paper, we used labelled data from two different languages as the source languages. Consequently, we have both labelled and unlabelled examples in three different views, speci fi cally: source language 1, source language 2 and target language. Fig. 1 shows the process of multi-view data creation. We sorted these views into, namely: view1, view2 and view3 as shown in Fig. 1 . In each view, there is an individual feature set that was extracted from the training data of the corresponding view. Therefore, both labelled and unlabelled examples are represented based on the corre-sponding feature set of each view. 4. Multiple source languages multi-view (MLMV) semi-supervised learning model
The aim of the approach proposed in this paper is to improve the performance of CLSC by incorporating unlabelled data from the target language into a multi-view semi-supervised learning model. This is achieved by using labelled data from multiple source languages. In this model, each unlabelled example is classi fi ed from different views and based on different feature sets.
Those unlabelled examples that are con fi dently classi fi majority views are selected to add to the training set of minor views in an iterative process. This means that if major classi different views are con fi dent and in agreement with the predicted label of an unlabelled example, this example can be added to the training set of other views which disagree with major views. Con fi dence of label prediction for major views is calculated by averaging the individual con fi dence of classi fi ers in agreeing views. This strategy is similar to the  X  majority teaching minority strategy introduced in a previous study ( Zhou and Li, 2010 ). In the case of three classi fi ers; if two classi fi ers agree on the predicted label of a set of unlabelled examples, the examples from the set having maximum average con fi dence will be selected as the most con fi dent examples. These will then be added to the training set of another classi fi er. Average con fi dence of an example is calculated by averaging the con fi dence of majority classi fi ers in predicting the label of that example. The framework of the proposed model is illustrated in Fig. 2 . The detailed procedure is shown as follows: Algorithm 1. Multiple source Languages Multi-View (MLMV) learning Given: L 1 : The initial labelled training set from source language 1
Initial parameters: p : number of most con fi dent positive
In this algorithm, after creating different views of data, three individual classi fi ers ( h 1 , h 2 and h 3 ) are trained based on the training set of each view. They are then applied to the unlabelled examples pool in the corresponding view. Following this, in each view, a set of unlabelled examples which classi fi ers in other views agree with the prediction of their labels are identi fi ed and the most con fi dent p positive and n negative examples from this set are then selected as pseudo-labelled examples and added to the corresponding training set. The prediction con fi dences for these examples are calculated based on averaging the con fi dence of agreeing classi fi ers. The process of training set enrichment is repeated for a prede fi ned number of iterations. After the full learning process, the independent test samples are presented to the trained classi fi ers based on corresponding views. The prediction label for each test example is then computed based on the majority voting approach. The process of creating different views for test examples is similar to the process for unlabelled examples as explained in Section 3 . 5. Evaluation
In this section, we evaluate our proposed model for cross-lingual sentiment classi fi cation in four different languages in the book review domain and compare it with some baseline methods. 5.1. Datasets
We have selected book review documents from two different cross-lingual sentiment datasets. The fi rst dataset was used by ( Prettenhofer and Stein, 2010 ), and contains Amazon product reviews for three different domains consisting of books, DVDs and music. These are in four different languages, speci fi
English, French, German and Japanese. Each review document is labelled as being either positive or negative based on its sentiment polarity. We only selected book reviews from this dataset. The book review dataset in the English language contains 2000 (1000 positive and 1000 negative) documents considered as being one of the source language data. A total of 6000 review documents (3000 positive and 3000 negative) were selected from each of the French,
German and Japanese languages respectively and considered as being target languages data. Another dataset used in this paper is the pan reviews dataset ( Pan et al., 2011 ). This collection consists of three review datasets in different domains (movie, book and music) in both English and Chinese. We selected only book reviews in Chinese from this collection. This dataset also contains 4000 book review documents (2000 positive and 2000 negative) and is considered as unlabelled data in the Chinese language.
By combining review documents from these two datasets, four different evaluation datasets for cross-lingual sentiment classi tion were consequently formed as follows: 1. EnGe-Fr: In this set, French is considered as the target language while English and German are used as two different source languages. 2. EnFr-Ge: In this set, German is considered as the target language while English and French are used as two different source languages. 3. EnFr-Jp: In this set, Japanese is considered as the target language while English and French are used as two different source languages. 4. EnJp-Ch: In this set, Chinese is considered as the target language while English and Japanese are used as two different source languages.

In all datasets, all reviews in the source languages are translated into target languages and similarly, all reviews in target languages are translated into the source languages using the Google Translate engine ( http://translate.google.com/ ). Table 1 shows the properties of these four evaluation datasets.

In the pre-processing step, all English, French and German reviews are converted into lowercase. Special symbols and other unnecessary characters are eliminated from each review document.
In the Japanese text document, we applied MeCab 1 segmenter software to segment the reviews; while Chinese documents were segmented by the Stanford Chinese word segmenter. 2 In the feature extraction step, unigram and bi-gram patterns were extracted as sentimental patterns. To reduce computational complexity, we performed feature selection using the information gain (IG) techni-que. We selected 5000 high score unigrams and bi-grams as features. Each document is represented by a feature vector. Each entry of a feature vector contains a feature weight. We used term presence as feature weights because this method has been con-fi rmed as the most effective feature-weighting method in sentiment classi fi cation ( Pang et al., 2002; Xia et al., 2011 ). 5.2. Baseline methods
The following baseline methods were implemented in order to compare the effectiveness of proposed models using the same system. Based on existing literature, the co-training ( Wan, 2009, 2011 ) and the structural correspondence learning (SCL) ( Prettenhofer and Stein, 2010 ) algorithms are two of the most well-known and best-performing methods that previously applied to the CLSC. 1. Multiple source Language Single View learning model (MLSV):
This model also uses labelled data from two different source languages as training data but only one direction of translation is used. In this model, unlabelled documents are translated from the target language into two source languages. A traditional co-training algorithm ( Blum and Mitchell, 1998 ) is used with different training datasets in each language (co-training in view1 and view2). This model is then compared to the proposed model in order to evaluate the effect of using bidirectional translation to create multiple views in semi-supervised learning. 2. Single source Language Multiple View learning model in the fi rst Source Language (SLMV-S1): This is the traditional co-training algorithm which was used in the study by ( Wan, 2009, 2011 ). It used labelled data from the fi rst language as the source language data and unlabelled data from the target language in two views (view1 and view3). 3. Single source Language Multiple View learning model in the second Source Language (SLMV-S2): This is the traditional co-training algorithm which was used in a paper by ( Wan, 2009, 2011 ). It used labelled data from the second language as source language data and unlabelled data from the target language in two views (view2 and view3). 4. Ensemble of SLMV-S1 and SLMV-S2 models (SLMV-S12): This is the combination of the SLMV-S1 and SLMV-S2 model. The output prediction of this model is calculated based on the average of each individual model. 5. Structural Correspondence Learning model (SCL): We imple-mented this model as introduced in ( Prettenhofer and Stein, 2010 ). We used the Google Translate service to map words in the source vocabulary to the corresponding translation in the target vocabulary. Other parameters are set as used in ( Prettenhofer and Stein, 2010 ). This method is implemented only in the case of using English as the source language. 5.3. Experiment setup
In all experiments, we used the support vector machine classi fi er (SVM) ( Joachims, 1999 ) as the base classi fi view in all semi-supervised methods. SVM light ( http://svmlight. joachims.org/ ) is used as the SVM classi fi er in the experiments with all parameters set to their default values. The output value of the SVM classi fi er for a review document indicates the con level of its label prediction. The sign of the prediction value indicates the sentiment polarity of a document. 5.3.1. Cross-validation in semi-supervised learning
To generate reliable results, we performed a 3-fold cross validation on semi-supervised learning. For this task, the unla-belled documents in the target language are randomly divided into three groups of equal size. In each step of the cross validation, two groups of documents are treated as the unlabelled pool and the evaluation of the performance is based on the remaining group as an independent test set. The fi nal results are averaged over three iterations. 5.3.2. Performance measure
Generally, the performance of sentiment classi fi cation is eval-uated by using four indexes, namely: Accuracy, Precision, Recall and F1-score. Accuracy is the proportion of all true predicted instances against all predicted instances. An accuracy of 100% means that the predicted instances are exactly the same as the actual instances. Precision refers to the portion of true predicted instances against all predicted instances for each class. Recall denotes the portion of true predicted instances against all actual instances for each class. F1 is a harmonic average of precision and recall.
 5.4. Results and discussion
In this section, our proposed method is compared with fi ve baseline methods. We used p  X  n  X  5 for our proposed model and all co-training algorithms as in ( Wan, 2011 ). The total number of iterations is set to 30 iterations for all iterative algorithms. Table 2 shows the comparison results after the full learning process. As we can see in this table, the proposed model outperforms all baseline methods, especially regarding accuracy in all datasets. These results show that the use of multiple source languages in a multi-view learning approach can improve the accuracy of CLSC.
By comparing the MLMV and MLSV models, we can conclude that using training and test documents in two different (original and translated) forms in both the source and target languages has a bene fi cial effect on classi fi cation performance. This can be attrib-uted to the fact that the learning model uses original documents in at least one learning component of the model and consequently alleviates the destructive effects of translation errors.
Compared to the SLMV-S1 and SLMV-S2 models, MLMV shows better overall accuracy in all datasets. Due to the use of training data from more languages, more vocabularies can be covered from documents in the target language. Consequently, the classi accuracy is improved in comparison to the single source language models. Therefore, it can be concluded that using multiple source languages has a bene fi cial effect on the performance of CLSC.
As shown in this table, SLMV-S2 indicates that the worst perfor-mance occurred in the EnJp-Ch dataset. This means that cross-lingual sentiment classi fi cation in the Chinese language using Japanese labelled documents cannot result in a reliable outcome. However, in spite of low performance in Japanese-Chinese cross-lingual sentiment classi fi cation, the MLMV model outperforms all baseline methods in this dataset, at least in terms of a ccuracy. These results support the idea that the combination of multi-views in a multiple source language framework can help to improve the performance of CLSC.
In order to assess whether there are any signi fi cant differences in terms of accuracy between the proposed model and semi-supervised baseline methods, we conducted a statistical test based on accuracy results obtained from 3-fold cross-validation. We used a paired t -test to evaluate whether differences between two methods are statistically signi fi cant. Table 3 shows the numerical results of the statistical test. With the exception of those between
MLMV and SLMV-S12 in the EnFr-Jp and EnJp-Ch datasets, all other comparisons showed statistically signi fi cant differences, for a signi fi cant level of  X   X  0.05.

Fig. 3 shows the learning curves of various methods on four evaluation datasets. Each fold of cross-validation generated a learning curve for the experiment of each model. The fi nal learning curve was determined using the average accuracies of each point from generated curves. In the fi rst two datasets, the proposed model shows the best accuracy from among all baseline methods during the learning process. In the EnFr-Jp and EnJp-Ch datasets, at the starting point of the learning process, the ensem-ble of co-training model (SLMV-S12) shows better performance in comparison to the proposed model. However, after some learning cycles, the accuracy of the proposed model overtakes all baseline methods.

Fig. 4 compares the accuracy of combined views and each of the individual views of the proposed model during the learning process in four datasets. This fi gure shows that all individual views improved during the learning process. This means that the strategy of  X  majority teaching minority  X  assists in the improvement of each of the views. This fi gure also demonstrates that the combination of three views in CLSC outperforms all individual views. This supports the idea that the information in multiple views can complement each other to cover more vocabularies in the target language. Each view can cover some limited terms of test examples and a combination of these views covers more terms in the test examples. 6. Conclusion and future work This paper has proposed an MLMV learning model for CLSC.
It creates multiple views of both labelled and unlabelled documents by incorporating multiple source languages and an automatic machine translation engine. A multi-view semi-supervised learning strategy with  X  majority teaching minority  X  focus has been used to incorporate unlabelled examples from the target language in the learning process to improve the performance of CLSC. We conducted experiments on different datasets from different languages. Our proposed model was evaluated by comparing its performance to the performance of some baseline me thods. The experimental results have shown that using multiple source languages in a multi-view framework can help to improve the performance of CLSC. In fact, different views and different source languages can complement each other to cover the sentimental terms of test data. As a result, better performance in sentiment classi fi cation is achieved.
However, the selection of an appropriate language to be used as the source language proved to be a challenging task in this model. The similarity of source language and target language in terms of linguistic expression and writing style can help the cross-lingual sentiment classi fi cation. On the other hand, our proposed method obviously depends on the availability of the automatic machine translation engines for the source and the target languages respectively. Although most of the commercial machine transla-tion engines have the capability of translating text documents from and into a large number of languages, they cannot be used to freely translate large amounts of data. This issue may limit the use of machine translation in our proposed method.

In a future work, we plan to exploit different learning models via a combination of different views and multiple source languages with regard to the learning process. As a result of this, each learning model (e.g. SVM or Na X ve Bayes) can examine the classi fi process from different aspects.
 Acknowledgments
The Universiti Teknologi Malaysia (UTM) under research grant 03H02 and Ministry of Science, Technology &amp; Innovations Malaysia, under research grant 4S062 are hereby acknowledged for some of the facilities utilized during the course of this research work. References
