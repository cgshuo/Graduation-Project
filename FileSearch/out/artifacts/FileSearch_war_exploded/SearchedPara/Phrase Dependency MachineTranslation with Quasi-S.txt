
Toyota Technological Institute at Chicago Carnegie Mellon University syntax for either the source or target language. However, when using syntax for both languages of useful rules (Ding and Palmer 2005). Smith and Eisner (2006) introduced quasi-synchronous translation system inspired by quasi-synchronous grammar. The core of our approach is a new model that combines phrases and dependency syntax, integrating the advantages of phrase-based and syntax-based translation. We report statistically significant improvements over a phrase-preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction
Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence be-tween languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; S X gaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into trans-lation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. 2007). The availability of these parsers, and gains in their accuracy, triggered research interest in syntax-based statistical machine translation (Yamada and Knight 2001). malisms and features. Some use a parse tree for the source sentence ( X  X ree-to-string X ), others produce a parse when generating the target sentence ( X  X tring-to-tree X ), and search showed that substantial performance gains can be achieved if hard constraints X  specifically, isomorphism between a source sentence X  X  parse and the parse of its translation X  X re relaxed (Liu, L  X  u, and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care.
 the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: used for both strings, which limits the divergence phenomena that can be captured.
As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a monolingual grammar generate a piece of the translation X  X  tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between the two trees. A weighted QG uses feature functions to softly penalize or encourage particular types of syntactic divergence.
 spired by quasi-synchronous grammar. We exploit the flexibility of QG to develop a new syntactic translation model that seeks to combine the benefits of both phrase-based and syntax-based translation. Our model organizes phrases into a tree structure inspired by dependency syntax (Tesni ` ere 1959). Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. The result captures phenomena like local reordering and idiomatic translations within phrases, as well as long-distance relationships among the phrases in a sentence. We use the term phrase dependency tree when referring to this type of dependency tree; phrase dependencies have also been used by Wu et al. (2009) for opinion mining and previously for machine translation by
Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model.

For two of the language pairs we consider (Chinese  X  English and German  X  English),
Marcus, Santorini, &amp; Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and
Xing 2009). We also want to apply our model to languages that do not have tree-350 banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing . The
NLP community has developed a range of statistical algorithms for building unsuper-vised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010;
Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation.
 modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take addi-tional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and
Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu  X  English and English  X  Malagasy translation, reporting statistically significant improvements over our baselines. These initial results offer promise for researchers to apply syntactic translation models to the thousands of languages for which we do not have manually annotated corpora, and naturally suggest future research directions.
 synchronous grammar and dependency syntax and motivate our modeling choices.
We present our translation model in Section 3, describe how we extract rules in Sec-
Section 6. We present experiments measuring our system X  X  performance on translation tasks involving four language pairs and several test sets in Section 7. We find statistically significant improvements over a strong phrase-based baseline on five out of seven test sets across four language pairs. We also perform a human evaluation to study how our system improves translation quality. This article is a significantly expanded version of
Gimpel and Smith (2011), containing additional features, a new decoding algorithm, and a more thorough experimental evaluation. It presents key material from Gimpel (2012), to which readers seeking further details are referred. 2. Background and Motivation We begin by laying groundwork for the rest of the article. We define notation in
Section 2.1. Section 2.2 discusses how synchronous and quasi-synchronous grammar handle syntactic divergence. In Section 2.3, we introduce dependency syntax and review prior work that has used it for machine translation. Section 2.4 presents two examples of syntactic divergence that motivate the model we develop in Section 3. 2.1 Notation
We use boldface for vectors and we denote individual elements in vectors using sub-scripts; for example, the source and target sentences are denoted x x x =  X  x perscripts; for example, the sequence from source word i to source word j (inclusive) integers as [ k ]. This notation is summarized in Table 1. 2.2 Synchronous and Quasi-Synchronous Grammars
To model syntactic transformations, researchers have developed powerful grammat-ical formalisms, many of which are variations of synchronous grammars . The most widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005;
Melamed 2003), an extension of context-free grammar to a bilingual setting where two strings are generated simultaneously with a single derivation. Synchronous context-free grammars are computationally attractive but researchers have shown that they cannot handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky, and Melamed 2006; S X gaard and Kuhn 2009). Figure 1 shows two such examples of word alignment patterns in German X  X nglish data. These patterns were called  X  X ross-serial discontinuous translation units X  (CDTUs) by S X gaard and Kuhn (2009). CDTUs cannot even be handled by the more sophisticated synchronous formalisms given by
Eisner (2003) and Ding and Palmer (2005). CDTUs can be handled by synchronous tree adjoining grammar (STAG; Shieber and Schabes 1990), but STAG comes with sub-stantially heftier computational requirements. Furthermore, S X gaard and Kuhn (2009) found examples in parallel data that even STAG cannot handle.
 result from an emphasis on generating the two strings. However, for many real-world applications, such as translation, one of the sentences is provided. The model only needs to score translations of the given source sentence, not provide a generative account for sentence pairs . Smith and Eisner proposed an alternative to synchronous grammar X  quasi-synchronous grammar (QG) X  X hat exploits this fact for increased flexibility in translation modeling. A QG assumes the source sentence and a parse are given and scores possible translations of the source sentence along with their parses. That is, a quasi-synchronous grammar is a monolingual grammar that derives strings in the target language. The strings X  derivations are scored using feature functions on an alignment from nodes in the target tree to nodes in the source tree. The quasi-synchronous depen-dency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007) and the phrase dependency model we present in Section 3. 352 stantiated for a wide range of formalisms. Dependency syntax (which we discuss in
Section 2.3) has been used in most previous applications of QG, including word align-ment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside from translation, QG has been used for a variety of applications involving relationships among sentences, including question answering (Wang, Smith, and Mitamura 2007), paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence sim-plification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith 2011), and supervised parsing from multiple treebanks with different annotation conventions (Li, Liu, and Che 2012). 2.3 Dependency Syntax and Machine Translation
Many syntactic theories have been applied to translation modeling, but we focus in this article on dependency syntax (Tesni ` ere 1959). Dependency syntax is a lightweight syntactic heads (also called  X  X arents X ). Examples of dependency trees are shown in
Figure 2. Each word has exactly one parent, and $ is a special  X  X all X  symbol that is parent in the sentence. Formally, a dependency tree on an m -word sentence y y y is a function  X  y y y : { 1, ... , m } X  X  0, ... , m } where  X  y y y y . If  X  y y y ( i ) = 0, we say word y i is a root of the tree. The function  X  to have cycles. We restrict our attention to projective dependency trees in this article.
Projective dependency trees are informally defined as having no crossing arcs when all dependencies are drawn on one side of the sentence. See K  X  ubler, McDonald, and Nivre (2009) for formal definitions of these terms.
 jecting across word alignments than phrase structure trees (Fox 2002). This makes dependency syntax appealing for translation modeling, but to date there are not many tree-to-tree translation models that use dependency syntax on both sides. One exception is the system of Ding and Palmer (2005), who used a synchronous tree substitution grammar designed for dependency syntax, capturing non-isomorphic structure within rules using elementary trees. Another is the system of Riezler and Maxwell III (2006), who used lexical-functional dependency trees on both sides and also include phrase translation rules. Relatedly, Quirk, Menezes, and Cherry (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and
Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrase-based or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao,
Koehn, and Birch 2011). 2.4 Motivating Examples
Although Fox (2002) found that dependencies are more often preserved across hand-aligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the noise in automatic word aligners and parsers. Second, not all dependencies are preserved in hand-aligned data, so we would need to be able to handle non-isomorphic structure even if we did have perfect tools. The model we present in Section 3 avoids isomor-phism constraints from synchronous grammar and encourages dependency preserva-tion across languages by using dependencies on phrases  X  X lat multi-word units X  X ather than words.
 pendency tree-to-tree divergence in German X  X nglish data. 1
English parallel corpus used in our experiments (and described in Appendix A). We parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art depen-dency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head rules (Yamada and Matsumoto 2003). We parsed the German side using the factored model in the Stanford parser (Rafferty and Manning 2008), which is trained from the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser X  X  source dependencies. 2 tionship, meaning that the source words aligned to the parent and child in the English sentence have the same parent on the German side. Many sibling configurations appear when the English dependency is DET  X  N within a PP . By convention, the NEGRA for DET N . When the parser converts this to a dependency tree, the DET and N are made children of the P . In English dependency parsing, due to the Penn Treebank conventions, the DET is made a child of the N , which is a child of the P . There are many other instances like this one that frequently lie within PP s, like the  X  us and recent  X  years . However, if we tokenized the us as a phrase and also den usa , then both would be children of the preposition, and the dependency would be preserved. 354  X  X randparent-grandchild X  relationship. In the English dependency until  X  recently , the aligned source words are in a grandparent relationship in the source sentence X  X  depen-dency tree. We note, however, that if vor kurzem is tokenized as a phrase, then we might let the entire phrase be the child of bis , preserving the dependency across languages. some of the syntactic divergence in real-world data. The model we develop in the next section is based on this idea. 3. Model
In the previous section we noted two examples in which flattening dependency tree structure into  X  X hrasal dependencies X  could improve dependency preservation be-tween German and English. This idea is compatible with the well-known principle that translation quality is improved when larger units are modeled within translation rules.
For example, improvements were found by moving from word-based models to so-called phrase-based translation models. Modern phrase-based translation systems are typified by the Moses system (Koehn et al. 2007), based on the approach presented by
Koehn, Och, and Marcu (2003). Phrase-based models excel at capturing local reordering phenomena and memorizing multi-word translations. 2001) or syntax-like representations (Chiang 2005) handle long-distance reordering better than phrase-based systems (Birch, Blunsom, and Osborne 2009), and therefore perform better for certain language pairs (Zollmann et al. 2008). In order to better handle syntactic divergence and obtain the benefits of these two types of models, we use rules that combine phrases and syntax. In particular, our rules use dependencies between phrases rather than words; we call them phrase dependencies . When adding in source syntax, we eschew the constraints of synchronous grammar in favor of the feature-based approach of quasi-synchronous grammar. So we call our model a quasi-synchronous phrase dependency (QPD) translation model.
 our model. We discuss rule extraction in Section 4 and define the feature functions in the model in Section 5. Decoding is discussed in Section 6 and an empirical evaluation is given in Section 7. Key definitions used throughout this section and the remaining sections are listed in Table 2. 3.1 Phrase Dependencies
In Section 2.3 we defined dependency trees. Now we provide an analogous definition for phrase dependency trees. We first define a segmentation of a sentence into phrases.
Given a sentence y y y , where m = | y y y | , we define a phrase  X  as a word sequence y y y  X   X  356 k such that 1  X  j  X  k  X  m . The number of words in phrase  X  is denoted |  X  | . We define a phrase segmentation of y y y as  X  =  X   X  1 , ... ,  X  n 0  X  such that for i  X  [ n and  X  1  X  ...  X   X  n 0 = y y y , where  X  denotes string concatenation.
  X   X  : [ n say phrase  X  i is the root of the phrase dependency tree; we require there to be exactly one root phrase. As with dependency trees,  X   X  cannot have cycles. phrase dependency trees from the ordinary dependency trees defined in Section 2.3, we will sometimes refer to the latter as  X  X exical dependency trees. X  for opinion mining and a similar formalism was used previously for machine translation by Hunter and Resnik (2010). Phrase dependencies allow us to capture phenomena like local reordering and idiomatic translations within each phrase as well as longer-distance relationships among the phrases in a sentence. 3.2 Quasi-Synchronous Phrase Dependency Translation denote the set of its possible translations (correct and incorrect) in the target language.
Given a sentence x x x and its lexical dependency tree  X  problem as finding the target sentence y y y  X  , the phrase segmentation  X  segmentation  X   X  of y y y  X  , the phrase dependency tree  X  one-to-one phrase alignment b b b  X  such that language dependency parse  X  x x x is optional and can be omitted if no source dependency from QG, which are described in Section 5.3. Hence we call the model defined in Equation (1) a quasi-synchronous phrase dependency (QPD) translation model. (2003). The phrase segmentation variables  X  and the one-to-one phrase alignment b b b : [ n ]  X  [ n 0 ] are taken directly from phrase-based translation. For all i  X  [ n then  X  j is a subvector of x x x and  X  i is a subvector of y y y . If  X  as in phrase-based models.
  X  X arts X  of the output structures in the model. The feature functions that look only at
Moses phrase-based system (Koehn et al. 2007), so they decompose in the same way as in Moses. 4 For clarity, we partition the features and weights into two parts, namely,  X   X   X  =  X   X   X   X  0 ,  X   X   X  00  X  and h h h =  X  h h h 0 , h h h and  X   X   X  00 are the weights for the QPD features h h h 00
Equation (1) as the following:
Furthermore, we assume an additive decomposition across individual phrase depen-dencies in the phrase dependency tree  X   X  , allowing us to rewrite Equation (2) as where we introduce new notation f f f to represent the feature vector that operates on a single phrase dependency at a time in the  X  X rc-factored X  decomposition of h h h feature in f f f can look at the entirety of x x x and  X  x x x look at a single target-side phrase dependency  X   X  i ,  X   X 
Example. Figure 5 shows an example. The inputs to the model are a segmented Chi-nese sentence and its lexical dependency tree. We used the Stanford Chinese word segmenter (Chang, Galley, and Manning 2008) to segment the Chinese data and the
Stanford parser (Levy and Manning 2003) to get Chinese dependency trees. The outputs 358 of the model include a segmentation of the Chinese sentence into phrases, the English translation, its segmentation into phrases, a projective dependency tree on the English phrases, and a one-to-one alignment between the English phrases and Chinese phrases.
Four reference translations are also shown. In this example, the model correctly moved phrasal dependent. 4. Rule Extraction
In typical statistical machine translation (SMT) models, the space of allowable trans-lations is constrained by a set of rules . Informally, a rule consumes part of the input text and emits text in the output language. Building an SMT system typically requires collecting a massive set of rules from parallel text, a process called rule extraction . is constrained by the phrase pairs in the phrase table. we have additional structure (i.e., the phrase dependency tree  X  translations is still constrained solely by a standard phrase table. We allow  X  any projective phrase dependency tree on  X  , so the structure of  X  translations are scored, not what translations are permitted. We made this decision because we did not want to reduce the coverage of phrase-based models, which is one of their strengths. Rather, we wanted to better score their translations. extraction in other systems and we define feature functions on our rules in a standard way. In particular, we use the extracted rule instances to compute relative frequency estimates for many of the features presented in Section 5.
 extract rules that only look at target-side words and syntactic structure. In Section 4.2 we extract rules that also look at the source sentence, but not its syntax . (Although our system uses unlexicalized features based on source-side syntax, they do not derive from rules; we turn to features in Section 5). This lets us avoid the computational expense of parsing the source side of the parallel training corpus. 4.1 Target-Tree Rules
We first extract rules that only consider the target side: y y y ,  X  , and  X  used as the basis for  X  X ependency language model X  features (Shen, Xu, and Weischedel 2008; Galley and Manning 2009; Zhang 2009), though unlike previous work, our features model both the phrase segmentation and dependency structure. Typically, these sorts
However, there do not currently exist treebanks with annotated phrase dependency trees. Our solution is to use a standard lexical dependency parser and extract phrase dependencies using bilingual information. 7 Essentially, we combine phrases from the standard phrase extraction pipeline with selected lexical dependencies from the output of a dependency parser.
 begin by obtaining word alignments and extracting phrase pairs using the standard heuristic approach of Koehn, Och, and Marcu (2003). We then parse the target sentence with a projective dependency parser to obtain a projective dependency tree  X  sentence y y y . Note that  X  y y y is a tree on words , not phrases (cf.  X  side phrases in the phrase pairs from phrase extraction, we extract a phrase dependency (along with its direction) if the phrases do not overlap and there is at least one lexical dependency between them. If there is only a dependency in one direction, we extract dependency, and in its direction. Because we use a projective dependency parser, the longest lexical dependency between two phrases is guaranteed to be unique. If a phrase contains a root word in  X  y y y , we extract a phrase dependency with the wall symbol as its head.
 we extract phrase pairs that are p-consistent with (i.e., do not violate) the word align-aligned to y j . We define new notation R here instead of using b b b because R allows many-to-many word alignments, which are typically used for phrase extraction. pipeline used in Moses.
 phrase dependencies . We say a phrase dependency  X  y y y j is d-consistent with  X  y y y and R if: 1.  X  x x x j 0 i 0 , x x x l 0 k 0 such that  X  x x x j 0 i 3. the longest lexical dependency from y y y j i to y y y
The final condition also implies that there is a lexical dependency from a word in y y y word in y y y l k :  X  u , i  X  u  X  j , such that k  X   X  y y y 360  X  y y y , $  X  is d-consistent with  X  y y y and R if: 1.  X  x x x j 0 i 0 such that  X  x x x j 0 i 0 , y y y j i  X  is p-consistent with R 2.  X  u , i  X  u  X  j , such that  X  y y y ( u ) = 0 and target-side lexical dependency trees. We note that while extracting phrase depen-dencies we never explicitly commit to any single phrase dependency tree for a target sentence. Rather, we extract phrase dependencies from all phrase dependency trees compatible with the word alignments and the lexical dependency tree. Thus we treat phrase dependency trees analogously to phrase segmentations in phrase extraction. from the sentence pairs in which we found them. Specifically, for d-consistent phrase where I [ P ] is the indicator function that returns 1 if P evaluates to true and 0 otherwise.
The index u  X  is chosen to make  X  y u  X  , y  X  phrase dependency:
This lexical dependency is recorded for use in back-off features, analogous to the lexical weighting in phrase-based models. The fifth field in Equation (4) holds the direction of the phrase dependency, which is also the direction of the longest lexical dependency.
Root phrase dependencies use k = l = 0 in the parent phrase and designate $ as y direction of root phrase dependencies is inconsequential and can remain as 4.1.1 Examples. What do typical phrase dependencies look like? Tables 3 and 4 show some of the most frequent examples of root phrases and parent-child phrase dependencies extracted by this technique on our German X  X nglish (DE  X  EN) corpus.
The English side of the parallel corpus was parsed using TurboParser (Martins et al. 2010). Naturally, there are many phrase dependencies with a single word in each phrase, but because these are very similar to lists of frequent lexical dependencies in a parsed corpus, we have only shown dependencies with phrases containing more than one word.
 like , etc.), though the lexical root is typically a verb or auxiliary. These are examples of how we can get syntactic information for phrases that typically would not correspond to constituents in phrase structure trees.
 corpus is mostly European Parliamentary proceedings, certain formulaic and domain-specific phrases appear with large counts. When phrases attach to each other, they typically behave like their heads. For example, in the phrase dependency of the  X  union , the word union is the child phrase because of the is behaving like of . There is likely also a dependency from the to union whenever the longer phrase dependency is extracted, but due to our practice of following the longest lexical dependency in deciding the direction, of  X  union is favored over the  X  union .
 target language (English), the presence and counts of the phrase dependencies will 362 depend on the source language through the word alignments. For example, when of the union is expressed in German, the preposition will often be dropped and the definite article chosen to express genitive case. In our corpus, the most common translation of the English union is the German noun union , which is feminine. The genitive feminine definite article is der and, indeed, we find in the phrase table that the translation of of the union with highest probability is der union . 9 Thus the dominance of the phrase depen-dency of the  X  union (6,897 occurrences) as compared with of  X  the union (142 occurrences) is caused by the German translation. 4.1.2 Word Clusters. When trying to compute feature functions for dependencies between long phrases, we expect to face problems of data sparseness. Long phrases do not occur very often, so pairs of long phrases will occur less often still. One way to address this is to also extract rules that use part-of-speech (POS) tags in place of words. However, since words can have multiple POS tags, we would then need to infer POS tags for the words in order to determine which rule is applicable. So we instead use hard word clusters, which provide a deterministic mapping from words to cluster identifiers. Furthermore, certain types of hard word clusters, such as Brown clusters (Brown et al. 1992), have been shown to correspond well to POS tag categories (Christodoulopoulos, Goldwater, and Steedman 2010). We chose Brown clusters for this reason.
 are hard cluster labels and observations are words. The emission distributions are constrained such that each word has a nonzero emission probability from at most one cluster label. Clusters can be obtained efficiently through a greedy algorithm that approximately maximizes the HMM X  X  log-likelihood by alternately proposing new clusters and merging existing ones. This procedure actually produces a hierarchical clustering, but we discard the hierarchy information and simply use unique IDs for each cluster. The number of clusters is specified as an input to the algorithm; we used 100 clusters for all experiments in this article. Additional details on cluster generation for our data sets are provided in Appendix B.
 word by its Brown cluster ID: where clust() is a function that takes a sequence of words and replaces each by quent Brown cluster phrase dependencies, including root dependencies, are shown in
Table 5. 4.2 String-to-Tree Rules
Our simplest probability features use the information in these tuples, but we also extract tuples with more information to support richer features. In particular, we record aligned 364 source phrases and details about reordering and the presence of gaps between phrases.
That is, for d-consistent phrase dependencies  X  y y y j i for all i 0 , j 0 , k 0 , and l 0 such that the phrase pairs  X  x x x with R , and such that x x x j 0 i 0 does not overlap with x x x function that returns 1 if P evaluates to true and 0 otherwise. That is, we include the two target phrases, their aligned source phrases, the direction of the target attachment, the orientation between the source and target phrases (whether the two target phrases are in the same order as their aligned source phrases or swapped), whether a gap is present between the two target phrases, and finally whether a gap is present between the two source phrases. When y y y l k = $, all of the additional fields are irrelevant except the aligned source phrase x x x j 0 i 0 .
 tuples. A common cause of reordering in German-to-English translation relates to verbs.
Figure 6 shows two examples of frequently extracted phrase dependencies that model verb movement. Figure 6(a) gives an example of how German reorders the finite verb extracted rule, shown below the sentence pair, only applies when intervening words appear on the German side and no intervening words appear on the English side. This is indicated by the presence (absence) of an ellipsis on the German (English) side of the rule.
 thank X ) to the end of an independent clause when a modal verb ( m  X ochte ,  X  X ould like X ) is present. The ellipses on both sides indicate that other words must be present between both the source and target phrase pairs. We note that this rule says nothing about what fills the gap. In particular, the gap-filling material does not have to be translationally hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and
Venugopal 2006, inter alia). (a) (b) dency features in our model. We extract each tuple with a count of 1 each time it is observed, aggregate the counts across all sentence pairs in the parallel corpus, and use the counts to compute the statistical features we present in the next section. We also have structural features that consider string-to-tree and tree-to-tree configurations, but these do not require any rule extraction. In the next section we describe the full set of features in our model. 5. Features
Our model extends the phrase-based translation model of Moses (Koehn et al. 2007), so we include all of its features in our model. These include four phrase table probability lexicalized reordering features, and a word penalty feature. These features are contained in h h h 0 in Equation (3), reproduced here: 366
We now describe in detail the additional features f f f that are used to score phrase de-pendency trees. Each operates on a single phrase dependency and takes the arguments  X  x x x ,  X  x x x , c , d ,  X  c ,  X  d , c 0 , d 0 ,  X  c 0 ( d ), the target child phrase (  X  c ), the target parent phrase (  X  target parent ( d 0 ), the child-aligned source phrase (  X  phrase (  X  d 0 ).
 conditional probabilities computed using relative frequency estimation given the full field  X  given field  X  is estimated as
We use the notation  X  p in the following to indicate that relative frequency estimates are being used. 11 5.1 Target-Tree Features
We first include features that only consider the target-side words and phrase depen-feature is the sum of the scaled log-probabilities of each phrase dependency attachment in  X   X  : where dir( c , d ) is defined and returns the direction of the attachment for head index d and child index c , that is, the direction in which the child resides; root indicates that phrase c is the root. chosen to ensure the feature value is never negative. The reasoning here is that when-ever we use a phrase dependency that we have observed in the training data, we want to boost the score of the translation. If we used log-probabilities, each observed dependency would incur a penalty. The max expression prevents unseen parent-child phrase dependencies from causing the score to be negative infinity. Our motivation is derivation completely if it merely happens to contain an unseen phrase dependency. duce some shorthand for simplicity of presentation. We first redefine this feature: where probability features, and assume that there is always a corresponding f that has the same subscript and takes the same inputs, as in Equation (12). Furthermore, when presenting the remaining features, we will suppress the arguments of each for clarity; all take the same arguments as f pdep and g pdep .
 probability for the feature. For example, that is, the minimum log-probability is found, negated, and a small positive value (0.01) is added to ensure the feature is greater than zero. This ensures that, if a phrase dependency has been seen, its contribution is at least 0.01.
 g pdep . First, we include a version of this feature with words replaced by Brown clusters:
We also include lexical weighting features similar to those used in phrase-based ma-chine translation (Koehn, Och, and Marcu 2003). These use the longest lexical depen-dencies extracted during rule extraction. First, for all  X  child, parent, direction  X  lexical conditional probabilities  X  p lex ( y | y 0 , r ) using relative frequency estimation. lexical dependency  X  y , y 0  X  for direction dir( c , d ), we include the feature:
We include an analogous feature with words replaced by Brown clusters. Different instances of a phrase dependency may have different lexical dependencies extracted choosing the lexical dependency that maximizes  X  p lex ( y | y Koehn, Och, and Marcu (2003).
 pendencies, one for lexical dependencies, and the same two features computed on a transformed version of the corpus in which each word is replaced by its Brown cluster ID. 368 5.2 String-to-Tree Features
We next discuss features that consider properties of the source sentence x x x , its phrase segmentation  X  , and the phrase alignment b b b , in addition to y y y ,  X  , and  X  features still do not depend on the source tree  X  x x x , so they can be included even when a parser for the source language is not available. We will discuss features that use  X  Section 5.3.
 tional pieces of structure. All features condition on direction. The first pair of features condition on the source phrase (  X  c 0 ) aligned to the child phrase (  X  dependency (  X   X  c ,  X  d  X  ):
In the second feature, we condition on word clusters for the parent phrase  X  syntactic clusters, even at times resembling part-of-speech tags (Christodoulopoulos,
Goldwater, and Steedman 2010), it did not seem logical to model translation probabili-ties between source-and target-language word clusters. This is why we did not include a feature like the above with word clusters for  X  c and  X  simple kind of backoff or smoothing that allows some sharing across specific phrases, since statistics on phrase pairs are expected to be sparse.
 aligned source phrases in a target phrase dependency attachment, namely, whether the aligned source phrases are in the same order as the target phrases ( X  X ame X ) or if they are in the opposite order ( X  X wap X ):
Given this definition of ori, we define the following features that condition on orienta-tion (in addition to other fields): where the last two features condition on the aligned child phrase  X  direction and orientation.
 parent target phrases and gaps between the aligned phrases on the source side. The gap( c , d ) function indicates whether there is a gap between the phrases indexed by c and d :
Given this gap function, we define the following features: conditioning bar. We now present features that have both the child and parent phrases on the left-hand side: table. Including direction, orientation, and gaps enables us to model longer-distance re-orderings; we showed some examples of such frequently extracted phrase dependencies in Section 4.2.
 the feature ablation experiments in Section 7, we will partition these features into two parts: We refer to the six features with subscript clust as C as W ORD . 5.2.1 String-to-Tree Configurations ( CFG ) . We now present features that count instances of local reordering configurations involving phrase dependencies. We refer to the features described in this section and the next section as C FG . These features consider the target segmentation  X  , the target phrase dependency tree  X  370 tation. The first feature value is incremented if the child is to the left and the aligned source-side phrases are in the same order:
Another feature fires if the aligned source phrases are in the opposite order:
Analogous features are used when the child is to the right of the parent:
Figure 7. They are agnostic as to the presence of gaps between the two target phrases and between the two source phrases. We include 16 features that add gap information to these four coarse configurations, as shown in the remainder of the table. Four gap configurations are possible, constructed from one binary variable indicating the pres-ence or absence of a source gap paired with a binary variable indicating the presence or absence of a target gap. We replicate the four coarse features for each gap configuration, giving us a total of 20 string-to-tree configuration features, all shown in Figure 7. coarse configurations and orientation) 5.2.2 Dependency Length Features. Related to the string-to-tree configurations are features that score source-and target-side lengths (i.e., number of words crossed) of target-side phrase dependencies. These lengths can also be useful for hard constraints to speed up inference; we return to this in Section 6. These features and constraints are similar to those used in vine grammar (Eisner and Smith 2005).
 aligned source phrases in each attachment in  X   X  . Letting  X 
Although this feature requires the segmentation of the source sentence in order to determine the number of source words crossed, the actual identities of those words are not needed, so the feature does not depend on x x x . We would expect this feature X  X  weight to be negative for most language pairs, encouraging closeness in the source sentence of phrases aligned to each phrase dependency in the target.
 example, where  X  c = y y y j i and  X  d = x x x l k :
However, such a feature could require looking at the entire phrase segmentation being generated to score a single phrase dependency (e.g., if  X  would prevent us from being able to use dynamic programming for decoding (we discuss our approach to decoding in Section 6). Instead, we use a feature that considers bounds on the number of target words crossed by each phrase dependency. In particular, the feature sums the maximum number of target words that could be crossed by a particular phrase dependency. We will discuss how this feature is computed when we discuss decoding in Section 6.
 and the 2 string-to-tree dependency length features. Adding these 22 features to the 15 from Sections 5.1 and 5.2 gives us 37 QPD features so far. 5.3 Tree-to-Tree Features (T REE T O T REE )
The last two sets of features consider the source-side dependency tree  X  simultaneously. We use T REE T O T REE to refer to these features. 5.3.1 Quasi-Synchronous Tree-to-Tree Configurations. We begin with features based on the quasi-synchronous configurations from Smith and Eisner (2006), shown for lexical dependency trees in Figure 8. For a child-parent dependency on the target side, these configurations consider the relationship between the aligned source words. For exam-ple, if the aligned source words form a child-parent dependency in the source tree, then we have a  X  X arent-child X  configuration. There is also an  X  X ther X  category for those that do not fit any of the named categories.
 dencies. That is, for a child-parent phrase dependency  X   X  relationship between  X  c 0 and  X  d 0 , the source-side phrases to which  X  372
There are several options for computing configuration features for our model, since we use a phrase dependency tree for the target sentence, a lexical dependency tree for the source sentence, and a phrase alignment.
 present between any word in one source phrase and any word in the other source and the other with index k in source phrase c 0 , we have a parent-child configura-grandparent-grandchild configuration to be present, the intervening parent word must outside both phrases. In lieu of standard (non-sibling) c-command relationships, we define a modified c-command category as follows. We first find the highest ancestors of words j and k that are still in their respective phrases. Of these two ancestors, if neither is an ancestor of the other and if they are not siblings, then the  X  X -command X  feature fires.
 the feature for the single configuration corresponding to the maximum distance | j  X  k | . If no configurations are present between any pair of words, the  X  X ther X  feature fires.
Therefore, only one configuration feature fires for each extracted phrase dependency attachment.
 instances of each configuration feature: one set includes direction (6  X  2 = 12 features), another set includes orientation (12 features), and the final set includes both source-and target-side gap information (24 features). There are therefore 49 features in this category (including the single  X  X oot-root X  feature). 5.3.2 Tree-to-Tree Dependency Path Length Features. Finally, we include features that con-sider the dependency path length between the source phrases aligned to the target phrases in each phrase dependency. The features in Section 5.2.2 considered distance along the source sentence (the number of words crossed). Now we add features that consider distance along the source tree (the number of lexical dependency arcs crossed).
We expect the learned weights for these features to encourage short dependency path lengths on the source side. minimum undirected path length between each word in  X   X  of dependency arcs that must be crossed to travel from one word to the other along the mum directed dependency path length, we also include the following feature: If there is no directed path from x j to x k , minDirPathLen returns  X  .
 phrase-based features there are a total of 102 features in our model. 6. Decoding
For our model, decoding consists of solving Equation (1) X  X hat is, finding the highest-scoring tuple  X  y y y ,  X  ,  X  ,  X   X  , b b b  X  for an input sentence x x x and its parse  X  search problem, because it is at least as hard as the search problem for phrase-based models, which is intractable (Koehn, Och, and Marcu 2003). Because of this we use a coarse-to-fine strategy for decoding (Charniak and Johnson 2005; Petrov 2009). Coarse-to-fine inference is a general term for procedures that make two (or more) passes over smaller search space.
 k -best list of derivations using a phrase-based decoder. This  X  X oarse model X  would account for all of the phrase-based features. Then we could parse each derivation to incorporate the QPD features and rerank the k -best list with the modified scores; this is the  X  X ine model. X  The advantage of this approach is its simplicity, but other research has shown that k -best lists for structured prediction tend to have very little diversity (Huang 2008), and we expect even less diversity in cases like machine translation where latent variables are almost always present. Instead, we generate a phrase lattice (Ueffing, Och, and Ney 2002) in a coarse pass and perform lattice dependency parsing as the fine pass. lattices in Section 6.1. In Section 6.2 we present our basic lattice dependency parsing algorithm. We give three ways to speed it up in Section 6.3; one enables a more judicious feature weights  X   X   X  , and we describe the structured support vector machine reranking formulation from Yadollahpour, Batra, and Shakhnarovich (2013) that we use. We close 374 in Section 6.5 with a brief discussion of how this decoder differs from earlier versions published in Gimpel and Smith (2009b, 2011). 6.1 Phrase Lattices
The most common decoding strategy for phrase-based models is to use beam search (Koehn, Och, and Marcu 2003). The search is performed by choosing phrase pairs from the phrase table and applying them to translate source phrases into the target language. Coverage vectors are maintained during decoding to track which words have been translated so far. They are used to enforce the constraint that each source word appear in exactly one phrase pair.
 explored during decoding. For phrase-based models, this representation takes the form of a phrase lattice (Ueffing, Och, and Ney 2002), a finite-state acceptor in which each path corresponds to a derivation. Figure 9 shows an example. The source sentence and a reference translation are shown at the top of the figure. Each path from the start node on the left to a final node corresponds to a complete output in the model X  X  output space.
Each lattice edge corresponds to a phrase pair used in the output. All paths leading to a given node in the lattice must agree in the set of source words that have been translated thus far. So, every node in the lattice is annotated with the coverage vector of all paths that end there. This is shown for three of the nodes in the figure.
 on individual lattice edges. To make n -gram language model features local, all paths leading to a given node must end in the same n  X  1 words. are two nodes with equivalent coverage vectors that are separated because they end in different words ( you vs. could ). Decoders like Moses can output phrase lattices like these; the lattice simply encodes the paths explored during the beam search. 6.2 Lattice Dependency Parsing maximize over  X   X  , we perform lattice dependency parsing , which allows us to search over the space of tuples  X  y y y ,  X  ,  X  , b b b ,  X   X  through a lattice and parse structures on those paths.
 dependency parsing algorithm we use is a straightforward generalization of the arc-factored dynamic programming algorithm from Eisner (1996). The algorithm is shown in Figure 10. It is shown as a set of recursive equations in which shapes are used in
The equations ground out in functions edgeScore and arcScore that score individual lattice edges and phrase dependency arcs, respectively. 13 used; for decoding, the semiring  X  X lus X  operator (  X  ) would be defined as max and the semiring  X  X imes X  operator (  X  ) would be defined as + . The entry point when executing the algorithm is to build G OAL , which in turn requires building the other structures. ifying dynamic programming algorithms is similar to weighted deduction, but ad-implementation. Top X  X own dynamic programming avoids the overhead of maintaining a priority queue that is required by bottom X  X p agenda algorithms (Nederhof 2003; Eisner, Goldlust, and Smith 2005).
 done; structures can be built that are never used in any full parse. This problem appears when parsing with context-free grammars, and so the CKY algorithm works bottom X  up, starting with the smallest constituents and incrementally building larger ones. This is because context-free grammars may contain rules with only non-terminals. Top X  down execution may consider the application of such rules in sequence, producing long derivations of non-terminals that never  X  X round out X  in any symbols in the string. A dependency model, on the other hand, always works directly on words when building items, so a top X  X own implementation can avoid wasted effort.
 a top X  X own lattice dependency parser to consider some dependencies that are never used in a full parse. We address this issue in the next section. 6.3 Computational Complexity and Speeding Up Decoding
The lattice parsing algorithm requires O ( E 2 V ) time and O ( E lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. So we use three techniques to speed up decoding: (1) avoiding construction of items that are inconsequential (i.e., 376 that could never be contained in a full parse), (2) pruning the lattices, and (3) limiting the maximum length of a phrase dependency. 6.3.1 Avoiding Construction of Inconsequential Items. By design, our phrase lattices impose several types of natural constraints on allowable dependency arcs. For example, each node in the phrase lattice is annotated with a coverage vector X  X  bit vector indicating which words in the source sentence have been translated X  X hich implies a topological unreachable from other nodes. For example, for a three-word source sentence, there cannot exist a directed path from a node with coverage vector  X  0, 1, 0  X  to a node with coverage vector  X  0, 0, 1  X  . However, there may or may not be a path from a node with vector  X  0, 1, 0  X  to one with  X  0, 1, 1  X  .
 wasting time figuring out the best way to build items that would end at the two nodes.
To discover this, we use an all-pairs shortest paths algorithm to find the score of the best path between each pair of nodes in the lattice. The algorithm also tells us whether each edge is reachable from each other edge, allowing us to avoid drawing dependencies that will never ground out in a lattice path. We use the Floyd-Warshall algorithm (Floyd 1962). This adds some initial overhead to decoding, but in preliminary experiments we found that it saves more time than it costs. We actually run a modified version of the algorithm that computes the length (in words) of the longest path between any two nodes. If the maximum length between two nodes is  X  , the nodes are unreachable from each other. Before we build an item in the algorithm in Figure 10, we check reachability of the item endpoints and only proceed if one can reach the other.
 imum lengths to compute the target-side vine grammar features and constraints, as mentioned in Section 5.2.2. In particular we use a feature analog to f src lengths. 6.3.2 Lattice Pruning. To reduce phrase lattice sizes, we prune lattice edges using forward X  X ackward pruning (Sixtus and Ortmanns 1999), which has also been used by
Tromble et al. (2008). This pruning method computes the max-marginal for each lattice edge, which is the score of the best full path that uses that edge, then prunes edges whose max-marginal is below a certain fraction of the best path score in the lattice. Max-marginals have been used for other coarse-to-fine learning frameworks (Weiss, Sapp, and Taskar 2010) and offer the advantage that the best path in the lattice is preserved during pruning.
 threshold that leaves fewer than 2,000 edges in the resulting lattice. As complexity is pruning, the lattices contain more than 10 16 paths on average and oracle BLEU scores are typically 10 X 15 points higher than the model-best paths. 6.3.3 Maximum Dependency Lengths. We can easily adapt our vine grammar features to function as hard constraints on allowable dependency trees, as originally done by
Eisner and Smith (2005) for monolingual dependency parsing. We use two simple constraints on the maximum length of a phrase dependency used during translation.
One constrains the number of source words that are crossed from one aligned source phrase to the other aligned source phrase by the phrase dependency. The other con-strains the maximum number of target-side words crossed by any path from one we never build items that would require using dependency arcs that violate these 378 and also compare translation quality and decoding speed for several values of these hyperparameters. 6.4 Interaction with Learning
The use of a coarse-to-fine decoding procedure affects how we learn the parameters of our model. We use two separate versions of the phrase-based feature weights: one for lattice generation and one for lattice dependency parsing. This is common with coarse-to-fine strategies X  X eparate instances of coarser parameters are required for each subsequent pass. We first learn parameters for the coarse phrase-based model used tion 6.3.2) and use a second round of tuning to learn parameters of the fine model, which includes all phrase-based and QPD feature weights. We initialized the phrase-based feature weights using the default Moses weights. For the QPD features, we initialized the phrase dependency probability feature weights to 0.002 and the weights for all other features to 0.
 algorithms are available. We use Algorithm 3 from Huang and Chiang (2005), which lazily finds the k best derivations efficiently. In preliminary testing, we found that the k -best lists tended to be dominated by repeated translations with different derivations, so we used the technique presented by Huang, Knight, and Joshi (2006), which finds a unique k -best list, returning the highest-scoring derivation for each of k unique transla-tions. This modification requires the maintenance of additional data structures to store all of the previously found string yields for each item built during parsing. This incurs additional overhead but allows us to obtain a far more diverse k -best list given a fixed time and memory budget.
 performs competitively with minimum error rate training (Och 2003) but is more stable.
For training the fine model, however, we found that R AMPION tial improvements over the output of the coarse phrase-based model alone. We found better performance by using a fine learner designed for the k -best reranking setting, in particular the structured support vector machine reranker described by Yadollahpour,
Batra, and Shakhnarovich (2013). Though we are doing lattice reranking rather than k -best reranking, the learning problem for our fine model is similar to that for k -best reranking in that the decoder is exact (i.e., there is no pruning that could lead to different patterns of search error as the parameters change). That is, phrase lattice generation and pruning (described in Section 6.3.2) only depend on the coarse phrase-based feature weights and the maximum dependency length constraints (described in Section 6.3.3); they do not depend on the fine model parameters.
 reranking. For simplicity, we will only write the source sentence x x x and its translation  X  ,  X  ,  X  ,  X   X  , and b b b , but they are always present and used for computing features. We assume a tuning set with N source sentences: { x x x translations for source sentence x x x i . Let Y i = { y y y translations (outputs of our lattice dependency parsing decoder) for x x x the highest-quality translation in the set, that is, y y y is the negated BLEU+1 score (Lin and Och 2004) of y y y evaluated against references Y ( y y y ) in the set.
 problem as an L 2 -regularized slack-rescaled structured support vector machine (SSVM; solving the following quadratic program: In Equation (39b), the violation in the margin  X  i is scaled by the cost of the translation. solutions will not be tightly enforced. On the other hand, the margin between y y y cutting-plane algorithm of Joachims, Finley, and Yu (2009). the cutting-plane algorithm, we compute the tuning set BLEU score with all param-eter vector values that are considered. At convergence we return the parameters that our use of sentence-level BLEU+1 in the loss function and corpus BLEU for final evaluation.

Equation (39) on the fixed lists, each time pooling all previous iterations X  lists. We repeat until the parameters do not change, up to a maximum of 15 iterations. We used k -best lists of size 150 and a fixed, untuned value of  X  = 0 . 1 for all experiments. 6.5 Comparison to Earlier Work
The decoder described above represents some advances over those presented in earlier papers. Our original decoder was designed for a lexical dependency model; we used lattice dependency parsing on lattices in which each edge contained a single source-target word pair (Gimpel and Smith 2009b). Inference was approximated using cube decoding (Gimpel and Smith 2009a), an algorithm that incorporates non-local features in a way similar to cube pruning (Chiang 2007). After developing our QPD model, we moved to phrase lattices but still approximated inference using an agenda algo-rithm (Nederhof 2003; Eisner, Goldlust, and Smith 2005) with pre-pruning of depen-dency edges in a coarse pass (Gimpel and Smith 2011). 380 algorithm once two simple approximations are made: the pruning of the lattice and the use of maximum dependency length constraints. Hyperparameters control the severity of these two approximations and the use of an exact parsing algorithm allows us to measure their effects on runtime and accuracy. 7. Experiments and Analysis
We now present experimental results using our QPD model. Because our model extends phrase-based translation models with features on source-and target-side syntactic structures, we can conduct experiments that simulate phrase-based, string-to-tree, and tree-to-tree translation, merely by specifying which feature sets to include. This suggests an additional benefit of using a quasi-synchronous approach for machine translation. By using features rather than constraints, we can simulate a range of translation systems in a single framework, allowing clean experimental comparisons among modeling strategies and combining strengths of diverse approaches.

Section 7.2. We measure the impact of using unsupervised parsing in Section 7.2.1 and include feature ablation experiments in Section 7.2.2. We present the results of a manual evaluation in Section 7.3 and give examples. We conclude in Section 7.4 with a runtime analysis of our decoder and show the impact of decoding constraints on speed and translation quality. 7.1 Experimental Setup about language pairs, data sets, and baseline systems are given in Appendix A and
Appendix B. We repeat important details here. We use case-insensitive IBM BLEU (Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap (Koehn 2004) with 100,000 samples (p  X  0 . 05). 7.1.1 Language Pairs. We consider German  X  English (DE  X  EN), Chinese  X  English (ZH  X  EN), Urdu  X  English (UR  X  EN), and English  X  Malagasy (EN  X  MG) translation.
These four languages exhibit a range of syntactic divergence from English. They also vary in the availability of resources like parallel data, monolingual target-language data, and treebanks. It is standard practice to evaluate unsupervised parsers on languages that do actually have treebanks, which are used for evaluation. We consider this case as well, comparing supervised parsers for English and Chinese to our unsupervised parsers, but we also want to evaluate our ability to exploit unsupervised parsing for languages that have small or nonexistent treebanks, hence our inclusion of Urdu and
Malagasy. 7.1.2 Baselines. We compare our model to several baselines: to generate phrase lattices for our system. Our model adds new syntactic structures and features to Moses, but because our decoder use Moses X  phrase lattices, our approach can be viewed as rescoring Moses X  search space. There are pros and cons to this choice.
It lets us build on a strong baseline rather than building a system from scratch. Also, by comparing the third baseline ( X  X oses, SSVM reranking X ) to our model, we are able to cleanly measure the contribution of our QPD features. However, Hiero has been shown to perform better than phrase-based systems for certain language pairs (Chiang 2007;
Zollmann et al. 2008; Birch, Blunsom, and Osborne 2009), and in these cases Hiero features could also be used to rescore Hiero X  X  search space to potentially yield further improvements, but we leave this to future work. 7.1.3 Parsers. Our full QPD model requires parsers for both source and target languages.
For each language pair, the target-language parser is only used to parse the target side of the parallel corpus and the source-language parser is only used to parse the source side of the tuning and test sets.
 used for our experiments. In particular, we used the Stanford parser (Levy and Manning 2003; Rafferty and Manning 2008) for Chinese and German and TurboParser (Martins et al. 2010) for English (see Appendix A for details). The Stanford parser is fundamen-tally a phrase-structure parser and generates dependency trees via head rules, but we chose it for our experiments for its ease of use and compatibility with the tokenization we used, particularly the Chinese segmentation which we obtained from the Stanford Chinese segmenter. 15 of using unsupervised parsers, we also performed experiments in which we replaced supervised parsers for Chinese and English with unsupervised counterparts. We now describe how we trained unsupervised parsers for these four languages. 382 tences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automatic POS tags for dependency grammar induction can work as well as or better than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011) showed that unsupervised tags could work as well as those from a supervised POS tagger.
 proach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the  X  X irect gradient X  version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we use the gold standard POS tags from their respective treebanks for training the parser, then use the Stanford POS tagger (Toutanova et al. 2003) to tag the parallel data, tuning, and test sets. As our dependency parsing model, we use the dependency model with valence (Klein and Manning 2004) initialized with a convex initializer (Gimpel and Smith 2012a). The training procedure is described in Gimpel (2012). Our Chinese and
English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets.
 parser. Well-known algorithms exist for sampling derivations under a context-free grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can be used to sample projective dependency trees by representing a projective dependency grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer et al. 2010) to sample projective dependency trees uniformly at random for each sentence. 16 the target language requires parsing the target side of the parallel corpus, rerunning rule extraction and feature computation with the new parses, and finally re-tuning to learn new feature weights. By contrast, changing the source-side parser only requires re-parsing the source side of the tuning and test sets and re-tuning. 7.2 Results
We now present our main results, shown in Tables 6 X 9. We see that enlarging the performs Moses with stack size 200. For DE  X  EN (Table 6), SSVM reranking im-proves performance even without adding any more features, pushing the numbers close to that of Hiero; and adding our QPD features does not provide any additional improvement.
 Moses baseline when using target syntactic features (T GT of 0.7 BLEU with the full QPD model (T GT T REE + T REE T still lag behind the Hiero results on average, but are statistically indistinguishable from performance gap between Moses and Hiero, suggesting that the Moses search space (and even our heavily pruned Moses phrase lattices) has the potential for significant improvements when using the right features.
 vised parser for English, so the T REE T O T REE features are incorporated using our unsu-pervised Urdu parser. All QPD results are significantly better than all Moses baseline results, but there is no significant difference between the two QPD feature sets. This may be due to our use of unsupervised parsing; perhaps the Urdu parses are too noisy for us to see any benefit from the T REE T O T REE features. In Section 7.2.1 we measure the impact of using unsupervised parsing for ZH  X  EN translation. Hiero still significantly outperforms the QPD model, although we have halfway closed the gap between Moses and Hiero. 384 Moses and Hiero when using the full QPD model. 17 to incorporate T GT T REE features but we only see a statistically significant improvement when we add T REE T O T REE features, which use a supervised English parser. 7.2.1 Impact of Unsupervised Parsing. Table 10 shows results when comparing parsers for ZH  X  EN translation. We pair supervised and unsupervised parsers for English and
Chinese. The final row shows the Moses BLEU scores for comparison. parsing with supervised and unsupervised Chinese parsing.  X  = significantly better than sup/sup,  X  = significantly worse than sup/sup.

Chinese parser in place of the Stanford parser leads to the same average test set BLEU score. When instead using random Chinese parses, we see a significant drop on two of the three test sets and an average decrease of 0.5 BLEU. When pairing unsupervised English parsing with supervised Chinese parsing, we see an average drop of just 0.2 BLEU compared to the fully supervised case. When both parsers are unsupervised, BLEU scores drop further but are still above the best Moses baseline on average. parser (unsupervised and supervised), then extract rules consistent with any of the parses. This might give us some of the benefits of forest-based rule extraction, which has frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and
Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language pairs, we could pool the rules extracted from all parallel corpora for computing target-syntactic features. For example, adding the English phrase dependency rules from the DE  X  EN corpus could improve performance of our ZH  X  EN and UR  X  EN systems.
Moving beyond translation, we could use the pool of extracted rules from all systems (and using all parsers) to build monolingual phrase dependency parsers for use in other applications (Wu et al. 2009). 7.2.2 Feature Ablation. We performed feature ablation experiments for UR  X  EN transla-tion, shown in Table 11. Starting with T GT T REE features, which consist of word (W cluster (C LUST ), and configuration (C FG ) feature sets, we alternately removed each of omitting word features, but a larger drop when omitting word cluster features. This may be due to the small size of our training data for UR  X  EN (approximately 1 million words of parallel text). With limited training data, it is not surprising that unlexicalized features like the cluster and configuration features would show a stronger effect than the lexicalized features. 7.3 Human Evaluation
We focused on UR  X  EN and ZH  X  EN translation for our manual evaluation, as these language pairs showed the largest gains in BLEU when using our QPD model. We 386 began by performing a human evaluation using Amazon Mechanical Turk (MTurk) in order to validate the BLEU differences against human preference judgments and to identify translations that were consistently judged better under each model for follow-up manual evaluation. 7.3.1 Procedure. We first removed sentences with unknown words, as we feared they would only confuse judges. 18 We then randomly selected 500 sentences from UR  X  EN test 2 and 500 from the concatenation of ZH  X  EN test 1 and test 2. For each of the 1,000 sentences, we chose a single reference translation from among the four references to show to judges. 19 All text was detokenized. Judges were shown the reference transla-tion, the translation from the Moses system with SSVM reranking, and the translation from our QPD system with the full feature set. We randomized the order in which the two machine translations were presented. Judges were asked to select which translation was closer in meaning to the reference; alternatively, they could indicate that they were of the same quality. We obtained judgments like these from three judges for each of the 1,000 sentences. sentence was judged to be translated better by one system more often than the other, sentences were preferred over Moses, but for 28 X 33% of the sentences, the reverse was true.

Moses, and also when Moses still performs better. For a follow-up manual evaluation, the QPD model; these should be the clearest examples of success for each system. In looking at these sentences, we attempted to categorize the primary reasons why all three judges would have preferred one system X  X  output over the other. We began with two broad categories of improvement: word choice and word order. We divided word choice improvements into two subcategories: those involving verbs and those involving words other than verbs. The reason we made this distinction is because some differences in non-verb translation are not as crucial for understanding a sentence as differences in verb translation or word order. Anecdotally, we observed that when one sentence has a better verb translation and the other has a better preposition translation, judges tend to prefer the translation with the better verb. We noted some sentences that fit multiple categories, but in our analysis we chose a single category that we deemed to be the most important factor in the judges X  decisions.
 with the consensus on 25 and found that 19 of these improved due to better word were determined to be preferred due to word order. The top section of Table 13 shows representative examples when Moses X  translations were unanimously preferred. Moses handles prepositions and other function words better than the QPD model in these examples. This may occur due to the reliance of phrase-based systems upon strong n -gram language models to ensure local fluency. The QPD model uses all of Moses X  features, including the same n -gram language model, but it adds many other features that score longer-distance word order and may be overwhelming the n -gram model in certain cases.
 with the judges on 42. Of these, we found that 15 had improved word order, 14 had improvements in verb word choice, and 13 had improved word choice for non-verbs.
So the QPD model X  X  improvements were due to word order on 36% of unanimous sentences, compared with Moses X  24%, suggesting that the QPD model X  X  strength is in improving word order. The lower section of Table 13 shows representative examples.
Consider the final example in the table. The Moses translation has better local fluency, but mixes words across clauses, confusing the meaning. The QPD translation has two local disfluencies ( X  X ecause law-abiding citizen hopes to X  and  X  X on X  X  need to fear will attack X ), but has no erroneous word reordering across clause boundaries. 7.4 Decoding Speed
So far we have reported BLEU scores for various feature sets and parsers, but we have not discussed decoding speed. BLEU improvements may not be worth substantial 388 reductions in translation speed. In this section we report decoding speeds and BLEU scores for UR  X  EN translation as pruning thresholds are varied. Our lattice dependency parsing decoding algorithm is exact, but two pruning stages precede lattice parsing, as discussed in Section 6.3: (1) pruning the phrase lattices based on the phrase-based model scores, and (2) pruning the search space deterministically based on source-and target-side limits on dependency lengths. In this section, we measure the impact of the latter type of pruning only. 20 speeds. We find that we can set these limits to be relatively strict and get similar BLEU scores in less time. In all previous experiments, we used a source-side limit  X  and a target-side limit  X  y y y of 20. That is, all target-side phrase dependencies may cover a maximum of 20 words in the target sentence, and the number of words between the aligned source phrases can be at most 15. We often use a larger value of  X  whereas  X  x x x constraints the exact number of source words crossed by a dependency (see Section 6.3.3 for details).
 server with two 2.6-GHz dual-core CPUs. Decoding during tuning is time-consuming, because we generate unique 150-best lists for each iteration, so we only use two max dependency length settings for tuning. But given trained models, finding the 1-best output on the test data is much faster. So we experimented with more pruning settings for decoding. Table 14 shows our results. The upper table reminds us of the baseline
BLEU scores. The lower table shows what happens when we train with two pruning settings: (  X  x x x = 10,  X  y y y = 15) and (  X  x x x = 15,  X  algorithm on the lattice and performing lattice dependency parsing. We use the Moses decoding, which is generally in the range of a couple seconds per sentence, depending on how the phrase table and language model are accessed. The average time required to run the Floyd-Warshall algorithm on the lattices is approximately 0.8 seconds per sentence, so it begins to dominate the total time as the pruning thresholds go below (5, 5). The earlier numbers in this section used (  X  x x x testing, which causes test-time decoding to take approximately 6 seconds per sentence, only losing 0.1 BLEU. The only severe drops in BLEU appear when using thresholds below (5, 5). 8. Conclusion and Future Work
We presented a new approach to machine translation that combines phrases, depen-dency syntax, and quasi-synchronous tree-to-tree relationships. We introduced several categories of features for dependency-based translation, including string-to-tree and tree-to-tree features. We proposed lattice dependency parsing to solve the decoding problem and presented ways to speed up the search and prune the search space. We presented experimental results on seven test sets across four language pairs, finding statistically significant improvements over strong phrase-based baselines on five of the seven. Manual inspection reveals improvement in the translation of verbs, an important component in preserving the meaning of the source text. We showed that unsupervised 390 dependency parsing can be used effectively within a tree-to-tree translation system, enabling the use of our system for low-resource languages like Urdu and Malagasy. This result offers promise for researchers to apply syntactic translation models to languages for which we do not have manually annotated corpora.
 improved if parallel text is available and we have a parser for one of the languages: The parallel text can be word-aligned and the annotations can be projected across the word alignments (Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentoswki 2001). The projected parses can be improved by applying manually written rules (Hwa et al. 2005) or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009). If we do not have parsers for either language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011).
 plications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based model of Chiang (2005) induces a synchronous grammar from parallel text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011) showed that using a supervised POS tagger to label these synchronous rules can im-prove performance up to the level of a model that uses a supervised full syntactic parser.
They further showed that unsupervised POS taggers could be effectively used in place of supervised taggers. These results suggest that it may be fruitful to explore the use of simpler annotation tools such as POS taggers, whether supervised or unsupervised, in order to apply syntax-based translation to new language pairs.
 Appendix A. Language Pairs
We consider four language pairs in this article, two for which large amounts of par-allel data are available and two involving low-resource languages. The large-data language pairs we consider are Chinese  X  English (ZH  X  EN) and German  X  English (DE  X  EN). The two low-resource language pairs are Urdu  X  English (UR  X  EN) and English  X  Malagasy (EN  X  MG).
 (Martins et al. 2010). We used a second-order model with sibling and grandparent features that was trained to maximize conditional log-likelihood.
 language pair. The line and token counts are summarized in Tables A.1 X  X .3.

Chinese  X  English. For ZH  X  EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese seg-menter (Chang, Galley, and Manning 2008) in  X  X TB X  mode, giving us 7.9M Chinese 392 tokens and 9.4M English tokens. For tuning and testing, we used MT03 ( X  X une X ), MT02 ( X  X est 1 X ), MT05 ( X  X est 2 X ), and MT06 ( X  X est 3 X ). The Chinese text was parsed using the Stanford parser (Levy and Manning 2003).

German  X  English. We started with the Europarl corpus provided for the WMT12 shared task. We tokenized both sides, filtered sentences with more than 50 words, and down-cased the text. We then discarded every other sentence, beginning with the second, leaving half of the corpus remaining. We did this to speed our experiment cycle. The corpus still has about 850k sentence pairs. We did the same processing with the news commentary corpus, but did not discard half of the sentences. There were about 150k news commentary sentences, giving us a total of about 1M lines of DE  X  EN parallel training data. For tuning, we used the first 1,300 sentences from the 2008 2,051-sentence test set ( X  X une X ). For testing, we used the 2009 test set ( X  X est 1 X ). The tuning/test sets are from the newswire domain. The German text was parsed using the factored model in the Stanford parser (Rafferty and Manning 2008).
 Urdu  X  English. For UR  X  EN, we used parallel data from the NIST MT08 evaluation.
Although there are 165,159 lines of parallel data, there are many dictionary and otherwise short entries, so it is close to an order of magnitude smaller than ZH  X  EN.
We used half of the documents (882 sentences) from the MT08 test set for tuning ( X  X une X ). We used the remaining half for one test set ( X  X est 1 X ) and MT09 as a second test set ( X  X est 2 X ). The Urdu text was parsed using an unsupervised dependency parser as described in Section 7.1.3.
 English  X  Malagasy. For EN  X  MG translation, we used data obtained from the Global Voices weblogging community ( http://globalvoicesonline.org ), prepared by Victor
Chahuneau. 21 We used release 12.06 along with its recommended training, development (tuning), and test set. Like Urdu, the Malagasy text was parsed using an unsupervised dependency parser as described in Section 7.1.3.
 Appendix B. Experimental Details
Appendix A contains details about the data sets used in our experiments. Other experi-mental details are given here.
Translation Models. For phrase-based models, we used the Moses machine translation toolkit (Koehn et al. 2007). We mostly used default settings and features, includ-ing the default lexicalized reordering model. Word alignment was performed using
GIZA++ (Och and Ney 2003) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction. The only exception to the defaults was setting the distortion limit to 10 in all experiments.

Language Models. Language models were trained using the target side of the parallel corpus in each case augmented with 24,760,743 lines (601,052,087 tokens) of randomly selected sentences from the Gigaword v4 corpus (excluding the New York Times and Los
Angeles Times ). The minimum count cutoff for unigrams, bigrams, and trigrams was one and the cutoff for fourgrams and fivegrams was three. Language models were estimated using the SRI Language Modeling toolkit (Stolcke 2002) with modified Kneser-Ney smoothing (Chen and Goodman 1998). Language model inference was performed using KenLM (Heafield 2011) within Moses.
 of the parallel corpus, which contained 89,107 lines with 2,031,814 tokens. We did not use any additional Malagasy data for estimating the EN  X  MG language models in obtain.

Word Clustering. Brown clusters (Brown et al. 1992) were generated using code provided by Liang (2005). For each language pair, 100 word clusters were generated for the target language. The implementation allows the use of a token count cutoff, which causes the algorithm to only cluster words appearing more times than the cutoff. When the clusters are used, all words with counts below the cutoff are assigned a special  X  X nknown word X  used when the clusters are applied.
 along with 412,000 lines of randomly selected Gigaword data comprising 10,001,839 words. This data was a subset of the Gigaword data used for language modeling. The count cutoff was 2. For EN  X  MG, only the target side of the parallel corpus was used.
The count cutoff was 1. In all cases, the data was tokenized and downcased prior to cluster generation.
 Acknowledgments 394 396 398 400
