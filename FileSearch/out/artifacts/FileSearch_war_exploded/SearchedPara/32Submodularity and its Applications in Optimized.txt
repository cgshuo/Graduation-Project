 High levels of pollutants, such as nitrates, in lakes and rivers can lead to the rapid growth of algae. These algal blooms can be a severe threat to our drinking water resources. For example, the algal bloom at Taihu Lake, Jiangsu province, China in June 2007 deprived four million people from drinking water and required an estimated USD 14.5 billion of cleaning costs [MSNBC 2007]. Many of the growth processes associated with such algal blooms are still not sufficiently well understood, and need to be studied in the lakes instead of the lab. A natural approach to obtain this understanding is to monitor environmental quantities such as temperature, nutrient distribution, and fluorescence that are associated with such algal blooms. A promising technology for this goal is to use sensors carried by robotic boats to obtain measurements over a large spatial extent. However, since the time required to make each measurement is large, and the robot needs a certain amount of time to travel between sensing locations, it is important to plan trajectories that allow us to obtain the most useful information about the phenomenon under study [Singh et al. 2009a, 2009b].

A similar problem arises in the context of monitoring municipal drinking water dis-tribution networks. Accidental or malicious contaminations of such networks can affect a large population and cause significant harm. Such contaminations can potentially be detected by deploying sensors in the water distribution network. However, these sensors are fairly expensive (for example, the Hach Water Distribution Monitoring system may be a suitable solution, but is currently priced above USD 13,000 per unit). We must thus optimize the locations where these costly sensors are placed in order to maximize their effectiveness. The need for addressing this task has received significant attention, and recently the Battle of the Water Sensor Networks [Ostfeld et al. 2008] was instituted as a benchmark challenge to fertilize research in this area.
More generally, in sensing optimization, our goal is to learn something about the state of the world (such as temperature at a given location, or whether there is a contamination), by optimally making a small set of expensive measurements. The fundamental question is: How can we get the most useful information, at minimum cost? This fundamental problem has been studied in several communities, including statistics (experimental design) [Lindley 1956; Howard 1966; Chaloner and Verdinelli 1995; Robbins 1952], machine learning (active learning) [MacKay 1992; Cohn et al. 1996; Castro et al. 2005], operations research (facility location) [Nemhauser et al. 1978; Berry et al. 2006], sensor networks [Zhao et al. 2002; Gupta et al. 2003; Gonzalez-Banos and Latombe 2001], and robotics [Sim and Roy 2005; Stachniss et al. 2005; Ji et al. 2007]. However, most of the existing approaches rely on heuristics without guarantees, which can potentially perform arbitrarily poorly. There are also algorithms that are designed to find the optimal solution, such as algorithms for solving Partially Observable Markov Decision Processes (POMDPs, Smallwood and Sondik [1973]) or Mixed Integer Programs (MIPs, Schrijver [1998]). However, these techniques are often very difficult to scale to large problems.

In this article, 1 we present an overview of a new class of algorithms for addressing this fundamental question which both have strong theoretical performance guarantees and scale to large real sensing problems. Our algorithms are based on the key insight that many sensing problems satisfy submodularity , an intuitive diminishing returns property: Adding an observation helps more if we have made few observations so far than if we already have made many observations. Whenever a problem satisfies this diminishing returns property we can develop very effective algorithms that are both guaranteed to perform well in theory and work very well in practice. In addition to identifying the most informative sensing locations, our algorithms can handle more challenging settings, where sensors need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data, or where solutions need to be robust against adversaries and sensor failures.
In addition to introducing algorithms for solving submodular sensing problems, we also present results applying our algorithms to several real-world sensing tasks, in-cluding environmental monitoring using robotic sensors, activity recognition using a built sensing chair, and a sensor placement challenge. However, submodularity and our algorithms are not restricted to physical sensing problems. To illustrate this versatility, we also present results on deciding which blogs to read on the Web in order to stay on top of the most important stories.

In Section 2, we define the sensing optimization problem, and introduce the notion of submodularity in Section 3. In Section 4, we discuss two applications of submodular sensing problems in detail. Many real-world problems involve robustness requirements and additional constraints, which we discuss in Sections 5 and 6. Lastly, in Section 7 we highlight how the results apply to information gathering problems on the Web (generalizing the notion of a sensor). The sensing optimization problem seeks to find a set of sensors that provides the best possible sensing quality at the minimum possible cost; let us start by defining the notions of sensing quality and cost. Intuitively, a sensing quality function F ( A ) provides a score for selecting the set of locations A out of the possible locations V .
Consider the problem of protecting our water distribution systems from contam-inations. Here, F ( A ) measures, for example, the number of people protected by placing sensors at locations A . To understand such a function better, consider the effect of introducing a contamination at some location i ; as this contaminant spreads through the network, thousands or millions of people may be affected. Due to the complex dynamics of the system, contaminations at some locations spread further than at others, as illustrated in Figure 1(a). Once we place a sensor, some of these contaminations are detected earlier (Figure 1(b)), and a number of people are protected from the contaminant. A good set of sensor placements A , for example, Figure 1(c), may detect many contaminations early, so F ( A ) will be high. Conversely, a poor placement A , for example, Figure 1(d), will have low F ( A ). In Section 4.1, we provide a more formal definition of F ( A ) for this application.

In general, the goal of sensing optimization is to find a set A  X  which provides the maximum possible value: A  X  = argmax A F ( A ) (or at least find a solution which is close to this optimal value). Intuitively, we can maximize F ( A ) by placing sensors at every possible location, but, in real applications, we will have resource constraints, for instance, sensors cost money and we have a limited budget. The simplest type of constraint is a cardinality constraint :wehave k sensors, and we want to ensure that | A | X  k . In this case, our optimization problem becomes More generally, sensors may have different costs, for example, drilling into a pipe may be more expensive than placing a sensor in an existing junction. Here, we denote the cost of a sensor s by C ( s ); the cost of a set of sensors A is the sum of their individual costs, C ( A ) = s  X  A C ( s ). If we have maximum budget B to spend, then our constraint becomes C ( A )  X  B . In this article, we provide very practical algorithms with theoretical guarantees for such sensing problems and for much more general ones. Even the simplest formulation of these sensing problems in Eq. (1) is already NP-hard [Krause et al. 2008a], and it is unlikely that there will be an efficient algorithm for solving this problem exactly. However, consider what perhaps is the simplest possible heuristic: the greedy algorithm . This procedure starts by picking the element s 1 that provides the most information: s 1 = argmax s F ( s ). Then, we iteratively pick elements s that provide the most additional information: s i = argmax s F ( { s } X  X  s 1 ,..., s i  X  1 } ).
This heuristic seems na  X   X ve, because, for example, when we pick the second sensor, the choice of first sensor is fixed and cannot be revised. This simple heuristic, however, performs surprisingly well in practice in many real-world applications (e.g., Figure 2 for water distribution systems). In fact, for many practical applications it is hard to find an algorithm that performs significantly better than the greedy approach for the optimization problem in Eq. (1). This empirical observation leads to a very interesting theoretical question:  X  X hy does the greedy algorithm perform so well on sensor placement problems? X 
Interestingly, this question can be answered by introducing a structural property called submodularity that is present in many practical sensing optimization problems. Intuitively, a problem is submodular if we observe diminishing returns . For example, if we have deployed 5 sensors, a 6 th one will provide much additional information, while after deploying 500 sensors, the 501 st will provide much less new information. Figure 3 illustrates this concept using the classic notion of set cover. Diminishing returns can be seen throughout our lives, for instance, when applied to business practices, the Pareto Principle says that  X 80% of your sales come from 20% of your clients. X  In water distribution systems, we naturally see diminishing returns: a few sensors may protect a large portion of the population, but to protect the last few people, we may need many more sensors.

More formally, a set function is said to be submodular [Nemhauser et al. 1978] if adding an element s to a set A provides more gain than adding it to a set B ,if A is a subset of B ,thatis,  X  A  X  B  X  V and  X  s  X  V \ B ,wehavethat F ( { s } X  A )  X  F ( A )  X  F ( { s } X  B )  X  F ( B ). Many real-world problems satisfy this prop-erty, for example, the function measuring the population protected by sensing in our water distribution systems [Krause et al. 2008a]. In a sense, submodularity is a dis-crete analog of convexity [Lovasz 1983], and proving that a problem is submodular can be accomplished using similar techniques as proving convexity. As an example, con-sider the Shannon entropy of sets of random variables X 1 ,..., X n [Cover and Thomas 1991]. It is well known [Fujishige 1978] that the function F ( A ) = H ( X A ) (where X
A = [ X i 1 ,..., X i k ] is the vector of random variables indexed by set A ={ i 1 ,..., i k } ) is submodular. This result can be shown as follows: Consider the marginal benefit ( A ) = F ( A  X  X  s } )  X  F ( A ) of adding variable X s to vector X A . Using properties of the entropy function, it can be seen that s ( A ) = H ( X s | X A ). Now, the  X  X nforma-tion never hurts X  inequality [Cover and Thomas 1991] states that whenever A  X  B , then H ( X s | X A )  X  H ( X s | X B ) (i.e., conditioning on more information lowers the en-tropy in expectation). This inequality immediately implies submodularity of F ,since F (
A  X  X  s } )  X  F ( A ) = can build more complex submodular functions from simpler submodular functions (for example, nonnegative linear combinations of submodular functions remain submodu-lar). We will see such an example in Section 4.2.

If a problem is submodular, we can exploit this property to develop very efficient al-gorithms that are guaranteed in theory to provide near-optimal solutions, and perform extremely well in practice. To understand the power of this insight, let us consider the class of submodular functions that are monotonically increasing as the set size increases:  X  A  X  B  X  V ,wehavethat F ( A )  X  F ( B ). Intuitively, population protected is monotonic: adding more sensors always protects more people, since some contami-nations can be detected earlier. Similarly, for discrete random variables, the entropy function is monotonic. For monotonic submodular functions, the following seminal re-sult of Nemhauser et al. [1978] justifies why the greedy algorithm performs so well in practice:
T HEOREM 1. [N EMHAUSER ET AL . 1978]. For monotonic submodular functions, the qual-ity of the set of elements A greedy obtained by the greedy algorithm is within a constant factor of the optimal solution A  X  to Eq. (1) .

This result says that the quality of the solution provided by the greedy algorithm is at least 63% of the optimal quality (1  X  1 / e  X  0 . 63). In practice, the performance of the greedy algorithm is much closer to optimal, for example, see Figure 2. Further-more, no other efficient algorithm can obtain better performance in general, unless P = NP [Feige 1998]. These two facts provide a notable theoretical explanation as to why this simple heuristic performed so well in practice. In the remainder of this article, we will provide a number of sensing optimization problems and applications. In many of these applications, the greedy algorithm is not near-optimal, but, by exploiting submodularity, we can design very efficient algorithms with near-optimal performance. We will now present two case studies demonstrating real-world examples of submod-ular sensing problems. These applications were chosen as they illustrate different aspects of sensing optimization and involve different submodular functions. As introduced in our running example, accidental and malicious contamination in mu-nicipal drinking water distribution networks can pose a severe threat to the population. Such contamination could potentially be detected by deploying sensors in the junctions and pipes of the network. Existing sensor solutions are fairly expensive (sensors such as the Hach Water Distribution Monitoring system are currently priced above USD 13,000 per unit), and thus only small numbers of these sensors can be deployed. Hence optimizing their placement becomes of crucial importance. The problem of deploying a small number of sensors to optimize performance criteria such as time to detection or minimizing the expected population affected by contamination has received significant attention. During the 8th Annual Water Distribution Systems Analysis Symposium, Cincinnati, Ohio, in August 2006, a challenge (the Battle of the Water Sensor Networks, BWSN) was organized, in which the 13 participating teams competed for optimal perfor-mance [Ostfeld et al. 2008]. The statement of this challenge included a realistic model of a real metropolitan area water distribution network (Figure 4(a)) with 12,527 nodes, as well as a description of 3.6 million realistic contamination scenarios, which varied in the choice of injection location and time of the contaminant into the network, as well as other parameters. The EPANET 2.0 simulator developed by the Environmental Protection Agency (EPA) was used to estimate the impact of possible contaminations [Rossman 1999]. The goal of the challenge was to optimize sensor deployments with respect to multiple objectives: minimizing the time to detection, the expected popu-lation affected by contamination, the total amount of contaminated water consumed, and maximizing the detection likelihood. As Krause et al. [2008a] show, all of these objectives F ( A ) can be written as where p ( i ) is the probability that contamination scenario i  X  I occurs, and M is  X  0is the amount of protection (for example, reduction in detection time, population affected, etc.) that placing a sensor s  X  V provides in case contamination scenario i  X  I occurs. Objective functions of this form and thus the ones used in the challenge are submodular. Our approach was to exploit this submodularity property to provide near-optimal sensor placements. However, evaluating the objective function F ( A ) requires the values M is , and thus the estimation of the impact of all 3.6 million contamination events i  X  I ,along with the benefit of placing each sensor at each possible location. Due to the magnitude of the problem instance considered in the challenge (the largest water distribution network studied by the community by that time), this task required massive amounts of distributed computation (processing approximately 47 Terabyte of simulation data in a cluster of 20 multicore machines [Krause et al. 2008a]). In addition, evaluating each function F ( A ) is thus very computationally expensive, and running the greedy algorithm to select 20 sensors takes approximately 30 hours in a highly optimized implementation. By exploiting submodularity, we were able to drastically reduce this computation time to approximately 1 hour using a technique called lazy evaluations , as shown in Figure 4(a) [Krause et al. 2008a]. We call our algorithm that implements these lazy evaluations the CELF algorithm.

In order to evaluate the entries to the challenge, different parameter settings were varied in the problem instances, including the amount of contaminant introduced and the delay before detection. Altogether, contributions were evaluated in 30 different settings. Since the goal was to solve a multicriterion optimization problem, for each of these settings, the set of nondominated entries was identified. 2 Each participating research team was evaluated based on the total number of nondominated solutions (with 30 being the maximum possible score).

The participants in the challenge used different algorithms for optimization, includ-ing genetic algorithms and other heuristics, domain knowledge and exact solvers (such as approaches based on Mixed Integer Programming), that could be only applied to smaller parts of the problem instance due to its size. According to the performance evaluation by the organizers of the challenge (Figure 4(b)) our solution based on sub-modular function optimization obtained 24% higher score than the runner-up [Ostfeld et al. 2008]. Many people spend a large part of their day sitting, be it in an office chair, couch, or car seat. In these contexts, sustained poor sitting posture can cause significant adverse effects. For example, back pain affects 60% to 80% of adults in the U.S. at some time in time in their lives, causing expensive absence from work [Lahad et al. 1994]. Elderly nursing home residents, who often spend long periods of time seated, are likely to develop ulcers [Smith 1995]. Improved seating comfort for these individuals augmented with technology interventions may have significant impact on health productivity and injury prevention.

Posture is a primary measure of sitting comfort (and discomfort) [de Looze et al. 2003]. Posture information could also help predict a sitter X  X  state. For instance, subtle changes in seating posture (e.g., when talking to other people in the car) can indicate attentiveness; unattentive driving can lead to an increased chance of accidents. Hence, posture information can potentially be used to reduce severe adverse effects (health problems, decreased productivity, or car accidents).

Previous work [Tan et al. 2001; Zhu et al. 2003] used a high-fidelity pressure sensor mat placed on the seat to acquire posture information. This sensor mat obtains pressure values on a 84  X  48 grid, as shown in Figure 6(c). Using this high-resolution sensor, previous work obtained 80% accuracy for classifying ten different postures (Figure 6(a)), such as leaning forward, leaning backward, slouching, etc. While these approaches demonstrated significant potential for using posture recognition as input modality, the sensor solution is very expensive, currently priced at above USD 3,000, precluding a widespread deployment of such sensing technology.
One approach to reduce the cost is to replace the expensive, high-resolution sensor with a small number of well-placed sensors. In order to optimize the sensor deployment, we learn a probabilistic model P ( Y , X V ) from training data collected by Tan et al. [2001] from 20 users. This model encodes the statistical dependencies between the assumed posture Y and the pressure values X s at all 4032 locations s  X  V of the sensor mat. Based on this probabilistic model, we want to select the sensor locations A that maximize the information gain about the posture (i.e., maximally reduces the entropy of the posture predictor). Recently, we have shown that this information gain criterion F (
A ) = H ( Y )  X  H ( Y | X A ) is monotonic and submodular for a large class of probabilistic models arising in many real-world applications [Krause and Guestrin 2005]. The proof is based on the submodularity of the entropy function as introduced in Section 3. Hence, we can use the greedy algorithm for selecting the sensor locations.

However, we do not only have the choice of where to place sensors, but also which type of sensors to deploy. More specifically, we face the decision of whether to deploy small, point sensors, or larger, flat sensors that average pressure over an extended area (refer to Figure 6(b)). In addition, each of these sensor types has a different cost, and our goal is to select a most cost-effective sensor solution. While the regular greedy algorithm introduced in Section 3 does not apply in the case of nonuniform cost functions, we can use the CELF algorithm [Leskovec et al. 2007], a variant of the greedy algorithm that we developed for cost-benefit optimization, to solve this more general optimization problem.

For various choices of the budget, we use the CELF algorithm to optimize the sensor placement, and study the estimated increase in classification accuracy. We choose a budget at a point where placing additional sensors does not significantly increase the the (cross-validated) classification accuracy, and thus arrive at a final design of 19 sensors.

In order to evaluate the performance of the proposed sensor design, we instrumented a new chair (Figure 6(c)). The cost of this proposed sensor design is approximately USD 100. We then perform a user study with 20 naive subjects. In this experiment, the optimized sensor design achieved 78%, as compared to 80% using the expensive sensor mat. More details on this case study are given by Mutlu et al. [2007]. In Section 4.1, we considered the problem of selecting sensor locations in a water distribution network which effectively detect random, accidental contamination events (that is, each scenario i  X  I has a certain probability p ( i ) of occurrence). However, if an adversary learns about the sensor locations, they can act strategically and maliciously contaminate the network based on that knowledge. In order to protect against such an adversary, it is important to decide where to place sensors in order to maximize the worst-case detection performance.

As we show in Section 5.3, this and many other problems can be formulated as optimizing a sensor deployment against an adversary who can, after seeing our de-ployment, select the minimum over a set of submodular functions. More formally, we pick A  X  to maximize where F 1 ,..., F m is a collection of monotonic submodular functions. In the water net-works example, F i is the detection performance that sensors A achieve if the adversary chooses contamination scenario i  X  I . In the following, we will describe an algorithm for solving such robust sensing problems.
 Given the provably near-optimal performance of the greedy algorithm for optimizing submodular functions, that is, for solving Problem (1), a natural approach to the ro-bust optimization problem (3) would be to modify the greedy algorithm, so that, after elements s 1 ,..., s j  X  1 have been selected, it adds the element for example, in the water network example, it adds the sensor that most decreases the worst-case detection time.

Unfortunately, for the robust optimization problem (3), this greedy algorithm can fail arbitrarily badly. To see this, consider the following example, illustrated in Figure 7. Suppose there is two contamination events, i 1 , i 2 , and three possible sensor locations s 1 , s at location s 3 can detect both contamination events, but only after a long time, that is, F If we run the greedy algorithm, it will place a sensor at location s 3 first, since that is the only location s such that min i F i ( { s } ) &gt; 0. But once the greedy algorithm commits to picking s 3 , it can only select s 1 or s 2 , but not both, leaving one of the contamination events essentially unprotected. Thus, the greedy algorithm obtains a worst-case score of  X  . On the other hand, the optimal solution is to place sensors at locations s obtaining a worst-case score of 1, which is arbitrarily better than  X  .

One might think that this poor performance is a particular property of the greedy algorithm, and other algorithms might do better. Unfortunately, as we prove in Krause et al. [2008b], unless P = NP , there cannot exist any efficient algorithm that finds a set A of size k achieving any nontrivial approximation guarantee! The preceding is a very strong negative result: as long as one insists on picking at most k sensor locations, one cannot hope to efficiently recoup not even an exponentially small fraction of the optimal value achievable using k sensors. This insight motivates the question: Can we achieve nontrivial guarantees if we allow ourselves to pick slightly more than k sensors?
To address this problem, we have developed the S ATURATE algorithm [Krause et al. 2008b], which is guaranteed to efficiently find a solution A S such that min i F i ( A S )  X  OPT k , where OPT k is the optimal score achievable using k sensors. Rather than using k sensors, however, S ATURATE will use | A S | X   X  k for some small value  X &gt; 1. Hence, S
ATURATE is guaranteed to find a solution which performs at least as well as the best solution of size k , but the returned solution is slightly larger than k .

The key idea of the S ATURATE algorithm is the following: Suppose we can find an algorithm that, given an input value c either finds a solution A of size at most  X  k with min i F i ( A )  X  c , or guarantees that no such solution exists. We call such an algorithm an approximate feasibility checker . Such an algorithm would be extremely useful: if we give it any guess c on the optimal value OPT k , it can either tell us that c &gt; OPT k ,that is, c is a strict upper bound to the optimal value, or it finds a solution that achieves this value c and is approximately feasible (i.e., of size at most  X  k ). We can thus search for the optimal value c that is still approximately feasible. This search procedure can be efficiently implemented using a binary search strategy.

Hence, the main problem is to devise an approximate feasibility checker. One ap-proach to develop such an algorithm is to attempt to solve the optimization problem that is, find the smallest possible set of sensors that achieves a worst-case performance of at least c . Unfortunately, this optimization has a nonsubmodular constraint in it, as it requires that min i F i ( A )  X  c . Our key idea is to replace this nonsubmodular constraint by a submodular constraint. This can be done in the following way: For each function F Interestingly, this truncation preserves monotonic submodularity. Now consider the average truncated score, F c ( A ) = 1 m i F i , c ( A ). Since nonnegative linear combinations of submodular functions are still submodular, F c ( A ) is a submodular function. Now, note that that is, the worst-case score is at least c if and only if the average truncated score is at least c . Thus, instead of solving the nonsubmodular problem in Eq. (4), we can solve the equivalent submodular problem Surprisingly, this problem can be approximately solved using a variant of the greedy algorithm, which, instead of stopping after k sensors have been selected, stops after the score achieves value c . This greedy algorithm returns a set A G such that | A G | X   X  | A  X  | , where A  X  is an optimal solution to Problem 5, and  X  depends logarithmically on the size of the problem instance.

This insight suggests a very simple approximate feasibility checker: Run the greedy algorithm on F , and check whether the resulting solution is of size greater than  X  k (in which case the optimal solution must have value less than c ), or is at most  X  k (in which case an approximately feasible solution was found). When combined with the binary search strategy illustrated before, this procedure defines our S ATURATE algorithm, which provides a near-optimal solution to the robust sensing problem.

T HEOREM 2. Let OPT k = max | A | X  k min i F i ( A ) be the optimal robust value for placing k sensors. Then S ATURATE finds a solution A S that achieves at least this value: using slightly more sensors, | A S | X   X  k, where  X  = 1 + log max s  X  V i F i ( { s } ) . In the example of Figure 7, when eventually considering the truncation c = 1, S ATURATE in fact recovers the optimal solution.

One may ask whether one can improve on the logarithmic dependence of  X  on the size of the problem instance. As we have shown recently [Krause et al. 2008b], this dependence is necessary under reasonable complexity-theoretic assumptions. Hence, in a strong sense, the S ATURATE algorithm is a best-possible efficient algorithm for robust submodular optimization. We have evaluated the S ATURATE algorithm on several real-world sensing problems. The first application is in environmental monitoring. Problems such as monitoring algal blooms in lakes, as introduced in Section 1, require estimating spatial phenomena such as temperature, fluorescence, and nutrient distribution over a large spatial extent. One approach to estimate phenomena such as temperature is to make measurements at a small number of locations (e.g., using sensor buoys or sensors carried by robotic boats), and then predict the temperature at the unobserved locations using statistical models. Gaussian processes (also known as Kriging models) have been found very effective for that purpose [Cressie 1991]. However, in order to obtain accurate predictions, we must select measurement locations that minimize the expected error in the predictions made given these measurements.

One approach to quantify the prediction error at some location s  X  V is the expected posterior variance Var( X s | X A ) given that we have made measurements at locations A . can be shown to be a submodular function [Das and Kempe 2008]. Optimizing the average prediction error throughout the environment (e.g., lake) is a natural (submod-ular) function we can optimize. Unfortunately, such an average-case optimization does not guarantee good predictions at every point in the lake, which could lead us to miss an important event. A good alternative is to optimize the worst-case prediction error over the entire lake [Burgess et al. 1981]. Thus, selecting a set of locations to measure in order to minimize the worst-case prediction error is an example of a robust sub-modular sensing problem. We use S ATURATE algorithm on this problem, and compare it against the state-of-the-art from geostatistics, a highly fine-tuned simulated annealing algorithm, which has seven parameters that need to be carefully fine-tuned for the particular application at hand [Wiens 2005]. Figure 9(a) compares the performance of the algorithms on a problem of predicting pH values in a lake near Merced, California. We can see that the greedy algorithm performs very poorly on this task. The simulated annealing algorithm performs much better, requiring approximately half as many sam-ples to achieve the same worst-case error. The S ATURATE algorithm is competitive with the state-of-the-art while being much simpler to implement, requiring no parameters to tune and running approximately 10 times faster in our experiment.

In some sense, the worst-case prediction problem for the river data is not very chal-lenging, since the greedy algorithm does not perform arbitrarily badly, as the theory would predict. We get a very different outcome when we compared the performance of the algorithms on the problem of deploying sensors in drinking water distribution networks in order to minimize the worst-case time to detection. Figure 9(b) shows the results of this experiment. Interestingly, in this domain, the greedy algorithm does not manage to decrease the worst-case detection time at all. The explanation of this poor performance is that unless all contamination events are detected, the worst-case de-tection time remains the same. However, no single sensor can detect all contamination events, thus the greedy algorithm has no indication of which first sensor location to pick. The simulated annealing algorithm randomizes, so eventually it gets lucky and obtains nontrivial performance. However, simulated annealing performs drastically worse than the S ATURATE algorithm, which obtains 60% lower worst-case detection time when placing 25 sensors. So far, we have seen that many real-world sensing problems require maximization of a submodular function. However, each application presents its own challenges. We summarize the algorithms introduced in this paper in Table I. When deploying networks of wireless sensors, the chosen sensor locations cannot be too far apart, otherwise the sensors will not be able to communicate through wireless links. Similarly, if we use robotic sensors to make observations, the chosen sensor lo-cations need to be connected by a path of bounded length, which is then traversed by the robot with limited battery power. These two problems can be modeled by using complex cost functions: We can consider all sensor locations as nodes in a graph, and the cost C ( A ) of any set of locations A as the cost of the least expensive routing tree (when deploying wireless networks) or the least expensive path (when using robotic sensors) that connects the chosen locations in the graph. In these settings, each initial decision has a large effect on the cost of subsequent observations. For this reason, the greedy algorithm, which performs near-optimally for the problem of selecting the best k locations (i.e., C ( A ) =| A | ), can perform arbitrarily badly in these more complex set-tings. To address this challenge, we developed the P SPIEL and E MIP algorithms, which provide solutions that perform near-optimally both in terms of utility F ( A ) and cost C (
A ) [Krause et al. 2006; Singh et al. 2009a]. In a sensor placement study, our P SPIEL algorithm for padded Sensor Placements at Informative and Cost-effective Locations found a sensor placement of measuring light in buildings which outperformed a man-ual (more or less uniformly placed) sensor network deployment, leading to 44% lower prediction error and 19% lower communication cost. We also used our E MIP algorithm for efficient Multi-robot Informative Path planning to plan informative paths for the NIMS-AQ robot [Stealey et al. 2008] (Figure 10) in order to monitor electrical conduc-tivity and temperature in the San Joaquin river and in a lake near the University of California, Merced. In these experiments, our informative path planning approach led to a drastic reduction in experimentation time for minimal loss in prediction accuracy. Another fundamental constraint associated with wireless sensor networks is limited battery power: Every measurement and every message sent over the network drains the battery, leading to reduced network lifetime. One possible approach to increasing the lifetime of a deployed sensor network is sensor scheduling , activating sensors only infrequently as needed. Hence, in order to optimize sensor network performance, one has to decide both where to place the sensors and when to activate them. The traditional approach to this problem is to first decide where to place the sensors, and then optimize the sensor schedule. However, when battery constraints require sensor scheduling, a possibly more effective approach would be to simultaneously optimize both the placement and the schedule. We recently developed the E SPASS algorithm for efficient Simultaneous Placement and Scheduling of Sensors, which provides near-optimal performance for this simultaneous optimization problem [Krause et al. 2009]. We evaluate our algorithm on several monitoring problems, and demonstrate significant advantages over the traditional, stagewise approach, such as a 30% improvement in sensor network lifetime for the same prediction accuracy in a traffic monitoring application. All the algorithms introduced earlier assume that a model of the world is available that allows us to predict the sensing quality obtained by making any set A of measure-ments. However, in many applications, such a model may not be available a priori. In such applications, we need to obtain measurements both to estimate the model, and to make accurate predictions using the estimated model. This problem shares similarities with the exploration X  X xploitation dilemma in reinforcement learning, where the goal is to choose actions to explore (refine the model of the world) and exploit (maximize the reward based on the estimated model). Analogously, we have developed an algo-rithm for addressing this exploration X  X xploitation trade-off in environmental monitor-ing [Krause and Guestrin 2007]. Our EEGP algorithm (for Exploration X  X xploitation in Gaussian processes) provides logarithmic sample complexity for estimating the pa-rameters of the underlying Gaussian process. We demonstrate the algorithm on several environmental monitoring problems, and show that it leads to drastic reduction in pre-diction error when compared to passive (exploitation-only) approaches (e.g., a 65% reduction in RMS when monitoring pH in Merced river).

We also recently developed a Distributed Online Greedy (DOG) algorithm that is able to learn and optimize the objective function over time in a distributed manner [Golovin et al. 2010]. We applied this algorithm to several sensor selection problems, and demonstrated both theoretically and empirically that the distributed solutions rapidly converge to the solution of the centralized greedy algorithm applied to the unknown objective function. The algorithms for optimizing submodular set functions introduced before are a pow-erful tool for solving nonadaptive information gathering problems. For example, in sensor placement, the greedy algorithm will select a near-optimal, fixed set of k sensor locations, based on a model F of the environment. In many applications however, such as sensor selection and active learning, we wish to adaptively select observations based on previous measurements. In such problems, we are no longer interested in obtaining a nonadaptive solution (a set of elements), but rather an adaptive policy (e.g., a decision tree or conditional plan). In such problems, often one is interested in using adaptive greedy algorithms, that select the next best element conditioned on previous observa-tions. Unfortunately, we cannot use classical results on submodular set functions to analyze such adaptive algorithms.

We recently developed the notion of adaptive submodularity , a natural generaliza-tion of submodularity to adaptive policies [Golovin and Krause 2010]. We show that many problems, such as adaptive sensor selection, viral marketing, and active learn-ing, satisfy this adaptive submodularity property. We furthermore prove that many results from classical submodular set functions (such as performance guarantees for the greedy algorithm, the ability to use lazy evaluations, etc.) carry over to the adaptive setting. We believe that this adaptive submodularity property is a promising direction for tackling and studying challenging large-scale active learning, experimental design, and adaptive optimization problems. Thus far, the case studies we have used have focused on optimization problems related to physical sensing; we will now address a very different problem: sensing on the Web . In 2008, Time magazine asked: How many blogs does the world need? [Kinsley 2008], claiming that there were already too many out there. The blogosphere has grown at a tremendous rate, with tens of millions of active blogs generating hundreds of millions of postings per year [Spinn3r 2007]. This huge activity leads to huge overload of information, opening up significant sensing optimization questions like: if you only have 10 minutes a day to spend on the blogosphere, which blogs should you read in order to keep track of the big stories?
To understand how to address this question, consider how information spreads through the blogosphere. As illustrated in Figure 11, a story posted in some blog may be picked up (and linked to) by other blogs; from there, these postings may be linked to by yet more blogs, and so on. Such spread of information in the blogosphere forms what we call an information cascade [Leskovec et al. 2007]. A good blog is one that captures big stories (i.e., large cascades) early on (i.e., as close to the first source as possible).
At first, the problem of capturing information cascades seems quite different from the other tasks we have discussed thus far. In reality, however, the spread of contaminants through water distribution systems is very similar to the spread of information through the blogosphere. And, most importantly, both of these tasks can be formulated as submodular optimization problems which can be tackled by the algorithms we have described thus far.

As an example, let us formalize one criterion for characterizing how well a blog captures a cascade i sparked by a certain story: Consider reading a particular blog b , which discussed this story at a certain time t .Ifblog b captures the story early on in cascade i , and the story becomes huge (i.e., the cascade is large), then there will be many other postings in cascade i appearing at a time later than t . In this case, blog b was very successful at capturing cascade i . On the other hand, if you have a story which does not become popular, its cascade i will be small. If blog b captures cascade i late, few blogs will report on this story after blog b , but you will not lose much by being late, since the story was small in the first place. More formally, if we recommend a set A of blogs, the resulting value F ( A ) will be where M ib is the set of postings on cascade i that are dated later than the time blog b first appears in this cascade. Note the similarity between this objective and the one for water distribution systems in Eq. (2); most interestingly, we note that both objectives are submodular functions.

To demonstrate this algorithm in practice, let us consider a blog dataset from 2006, with 45,000 blogs, 10.5 million posts, and 16.2 million links (30GB of data) [Leskovec et al. 2007]. In this dataset, we discovered 17,589 significant cascades, where each blog participates in 9 . 4 different cascades on average. As Figure 12(a) shows, the CELF algorithm optimizing Eq. (6) greatly outperforms a number of other heuristic selection techniques. The best runner-up, performing about 45% worse than CELF, picks blogs ranked by the number of in-or out-links from/to other blogs that occurred in the dataset. Figure 12(b) plots the running time of selecting k blogs. We see that exhaustively enumerating all possible subsets of k elements is infeasible (the line jumps outoftheplotfor k = 3). The simple greedy algorithm scales as ( k | V | ), since for every increment of k we need to consider selecting all remaining | V | X  k blogs. The performance of our CELF algorithm is significantly better, for example, for selecting 100 blogs, the greedy algorithm runs 4.5h, while CELF takes 23 seconds (700 times faster).
The results presented so far assume that every blog has the same cost. Under this unit cost model, the algorithm tends to pick large, influential blogs that have many posts. For example, instapundit.com is the best blog, but it has 4,593 posts. Interest-ingly, most of the blogs among the top 10 are politics blogs: instapundit.com, michelle-malkin.com, blogometer.nationaljournal.com, and sciencepolitics.blogspot.com. Some popular aggregators of interesting things and trends on the blogosphere are also se-lected: boingboing.net, themodulator.org, and bloggersblog.com. The top 10 blogs had more than 21,000 thousand posts in 2006. Under the unit cost model, large blogs are important, but reading a blog with many posts is time consuming. This motivates the Number of Posts (NP) cost model, where we set the cost of a blog to the number of posts it had in 2006.

Comparing the NP cost model with the unit cost in Figure 12(c), we note that un-der the unit cost model, CELF chooses expensive blogs with many posts. For example, to obtain the same objective value, one needs to read 10,710 posts under unit cost model. The NP cost model achieves the same score while reading just 1,500 posts. Thus, optimizing the benefit cost ratio leads to drastically improved performance. In-terestingly, the solutions obtained under the NP cost model are very different from the unit cost model. Rather than picking large blogs which capture many stories, under the NP model, the algorithm discovered the notion of summarizer blogs (e.g., themodu-lator.org, watcherofweasels.com, anglican.tk); these bloggers navigate the blogosphere and discover and talked about stories that eventually become big. Blogs selected under NP cost appear about 3 days later in the cascade as those selected under unit cost, which further suggests that that summarizer blogs tend to be chosen under NP model.
We can also reverse our information management goal: rather than selecting blogs, we can find a principled approach for picking a set of posts that best covers the im-portant stories in the blogosphere. This task can also be formalized as a submodular optimization problem which is amenable to the algorithms described herein [El-Arini et al. 2009]. As an example, here is the set of posts that our algorithm selects for an eight-hour period on January 18, 2009, if our budget k is set to five. (1) Israel unilaterally halts fire as rockets persist (2) Downed jet lifted from ice-laden Hudson River (3) Israeli-trained Gaza doctor loses three daughters and niece to IDF tank shell (4) EU wary as Russia and Ukraine reach gas deal (5) Obama X  X  first day as president: prayers, war council, economists, White House The selected five posts all cover important stories from this particular day.
Since people have varied interests, the ideal coverage algorithm should incorporate user preferences in order to tailor the selected posts to individual tastes. For this task, we must learn a personalized coverage function for the blogosphere through user interaction, for example, from the user labeling posts they would like to read or not read, or by what posts the user clicked on [El-Arini et al. 2009]. As an example, Figure 13(a) illustrates the important words being discussed in the blogosphere on January 21 st , 2009, while Figure 13(b) shows the important words for the articles the algorithm picks for a user who is very interested in sports. Such a personalization problem exemplifies problems where, rather than being given a submodular function as we have discussed thus far, we need to address the problem of simultaneously learning a submodular function while optimizing it [El-Arini et al. 2009; Streeter et al. 2009]. Our society is drowning in information: The Internet allows us to access virtually every bit of information available to humanity, at any given time, and at virtually no cost. The ubiquitous availability of sensors allows monitoring of the physical world in an unprecedented manner. This development presents exciting new opportunities for the advancement of science and society. However, this explosion in availability of information also creates the fundamental challenge of developing new techniques for extracting the most useful information. We believe that the algorithms described in this article present an interesting step in this direction.

