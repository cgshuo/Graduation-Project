 Clustering in data mining is essential for various business applications. Numer-ous data clustering schemes have been proposed in recent years, subsequently attracting strong attention [1]-[6]. Many existing clustering methods have high computational time, or may have pattern recognition problems when using large databases. Therefore, an effi cient and effective clustering algorithm is important. Clustering approaches can be categorized as partitioning, hierarchical, density-based, grid-based and mixed. Partitioning methods like K-means attempt to identify K partitions containing similar objects [3]. K-means algorithm is eas-ily and quickly implemented, but does not accurately recognizing the shapes of patterns that are non-spherical or not the same size. DBSCAN, density-based clustering method, measures the density of a region, and thus accurately recog-nizes arbitrary shapes and different size clusters, and filters noise [4]. However, DBSCAN needs to examine all objects, and thus has a high computational time. Grid-based clustering methods define clus ters using grid-cell structures. These methods consider the grid-cell as a point to improve the problem of time cost, and can therefore cluster all objects quick ly. However, they cannot smoothly detect the edges of clusters, or remove indentations in neighboring clusters. CLIQUE is a classical grid-based clustering method [5].

To fulfill data clustering requirements and solving limitations of the above clus-tering methods, this work presents a new algorithm named AN ew G rid-based clustering method with E liminating indention for L arge databases ( ANGEL ) by hybridizing hierarchical, density-based and grid-based clustering approaches. Simulation results show that the proposed ANGEL approach is a highly effective and efficient clustering technique. K-means , which was presented in 1967, was the first clustering algorithm [3]. It includes the following steps. (1) Select K partition centers randomly from data sets. (2) Assign each object to its closest center. (3) Recalculate K partition cen-ters and repeat step 2 until the centers convergence. K-means always converges to a local optimum. Moreover, K-means can not filter noise, and does not cluster non-spherical patterns correctly.
 CLIQUE integrates grid-based and density-based clustering techniques [5]. CLIQUE initially generates a grid map from feature space. For each dimension, the algorithm identifies the high-density units by using the priori method. Al-though CLIQUE has a fast clustering time, but its cluster boundaries are either horizontal or vertical, owing to the nature of the rectangular grid.
DBSCAN is a first-density-detecting method, which depends on two argu-ments, namely Eps and MinPts [4]. Eps denotes the radius of the search cir-cle, and MinPts represents a number of mi nimal neighbors in the search circle. These arguments are a dopted to examine the  X  -neighbors contained in each ob-ject. DBSCAN can accurately recognize any arbitrary pattern by applying this expansion. Since each expansion must examine all objects, the time complexity of DBSCAN is also high when the database size is large.

GDH is a hybrid grid-based, density-based and hierarchical clustering tech-nique, presented by Wang [6]. GDH refers the idea of density function and gra-dient decrease and concept of sliding wi ndow [6]. It can significantly reduce the limitation of edge indention of traditional grid-based algorithms. However, GDH may fail in the edge of indention if two clusters are the same time in the hypercube. This section introduces the new ANGEL cl ustering concept, algorithm and its implemented steps in the algorithm step by step as follows:
The basic concept of ANGEL clusteri ng can be described in terms of the following four parts. (1) Feature space slicing and objects assigning: This step reduces the number of searching spaces is the main idea. Like GDH, ANGEL inputs the argument of hypercube X  X  length, and splits the feature space into a hypercube set. Each object of the dataset is assigned to an appropriate hypercube. A hypercube is called populated cube if the number of objects in the hypercube is greater than the threshold Hd . Fig. 1 describes this concept.

Influence function [6] is defined as a mathematical description that has the influence of an object has within its neighborhood. The density function [6] is defined as the sum of influence function of all objects in the region, and can be any arbitrary function. For simplicity, this work applies the Euclidean density function and Gaussian representation. The Gaussian density function is given by [6]: where N represents the number of objects of the region; d ( x, x i ) denotes the distance between x and x i ,and  X  is the standard deviation. A populated cube is called a density-attractor if it has the highest Gaussian density function among all cubes [6]. The density-attractor is the initial point of search space. (2) Identifying the main structure: This investigation employs the discrete-degree as a measure of grid-density detecting preprocesses to identify the main structure of cluster excluding the cluster edge. All populated cubes are split up into nine sub-hypercubes. ANGEL computes the number of objects within each sub-hypercube according t o the location of the objects. The range of discrete-degree is derived and defined as follows:
UL and LL represent the upper and lower lim its of discrete-degree, respec-tively. n denotes the number of objects of populated cube, and PTV is the percentage of the tolerance value. If all of the density of sub-hypercubes in the hypercube is between UL and LL , then the density in the hypercube is equally distributed, and the hypercube is the main structure of the cluster, and can be assigned to cluster directly. Otherwi se, the edge detection method has to be utilized, as displayed on hypercube B, C, E and F of Fig. 2. (3) Edge detection: The aim of this step is to detect accurately the edge of a cluster. A populated cube that does not belong to the main structure of the cluster may contain objects belonging to two different clusters, as illustrated on hypercube B and C in Fig. 2. Core points and border points of the cluster and noise can be recognized by using DBSCA N to perform detection on hypercubes B, C, E and F on the diagram of Fig. 2. Border points are redefined as objects resulting from a DBSCAN run that are the closest to the hypercube border. This redefinition shortens the computational time in DBSCAN. The light color points (on the border) on hypercube B, C, E and F of Fig. 2 represent border points. (4) Merge stage: After the edge detection stage, the algorithm merges the edge of the cluster with the main structure of the cluster, depending on which border is closest to the main structure. ANGEL repeats the process to recognize all clusters.
 The ANGEL clustering algorithm can be described as follows:
DataSets is an entire database or a partial dataset. Cl represents the length of a hypercube; PTV denotes the percentage tolerance value, and Hd is the threshold of the hypercube X  X  density. Eps represents a search radius, and MinPts denotes the smallest number of objects in the region. The algorithm is presented step by step below.
 Step 1. Initialization of all arguments.

Step 2. CreatGridStructure() function generates the structure of the hyper-
Step 3. CacluateGridsInfo() function computes the range of discrete-degree Step 4. Repeat the process by while loop.

Step 5. SelectDensityAttractor() function obtains the density-attractor, Step 6. If cube C is null, then stop the algorithm.
 Step 7. If the discrete-degree of cube C is equally distributed, then assign cube
Step 8. Otherwise, ANGEL applies DBSCAN for the edge detecting and returns
Step 9. Assign a sub-cluster of Cs resulting from a DBSCAN run to a cluster Step 10. ANGEL then searches the neighbors of the cube C with the The neighbor searching process SearchNeighbors(Cube) is as follows:
The neighbor searching step SearchNeighbors(Cube) can be illustrated as follows: Step 1. The SelectNeighbors() function returns a set of neighbors Step 2. Continue the process until the neighbors of the cube Cube is empty. Step 3. HighDensity() function returns a search subject to cube CurrCube with Step 4. As stated above, if th e discrete-degree of cube CurrCube is Step 5. Otherwise, ANGEL applies DBSCAN for edge detection, and returns a Step 6. Each sub-cluster of NCs is assigned to a cluster if its border points are Step 7. ANGEL then searches the neighbors of the cube CurrCube by the
The process is repeated to merge the entire cluster. In this work, ANGEL was implemented in a Java-based program, and run on a desktop computer with 256MB RAM, an Intel 1.5GHz CPU on Microsoft Windows 2000 professional Operational System. Seven synthetic datasets were employed in the experiment. Fig. 3 presents the original datasets. The results of the proposed ANGEL algorithm were compared with DBSCAN, K-means, CLIQUE and GDH. Four datasets, with 11,500, 115,000, 230,000 and 575,000 objects in seven synthetic datasets, an d all with 15% noise, were utilized in this experiment. The computational time of DBSCAN increases significantly as the number of databases increases. Hence, Table 1 does not list all of the simulation results for DBSCAN (N/A means that the simulations were not performed). Table 1 indicates that the proposed ANGEL with the lowest time cost, and the best clustering correctness rate and noise filtering rate. Due to the limitation of length, not all experimental results are shown. Fig. 4 depicts the experimen-tal results of ANGEL. The experimental results demonstrate that ANGEL can handle arbitrary shapes for clustering. However, K-means cannot recognize ar-bitrary shapes. Although CLIQUE and GDH could handle the arbitrary shapes in Dataset 4 to 7, CLIQUE could not smoothly detect the edges of clusters, or remove the indentations of neighboring clusters, due to the nature of the rect-angular grid. Additionally, the gradient decrease function in GDH placed some clusters in the wrong position if the hyp ercubes were neighbors but the gradient decrease between the hypercubes was too h igh. Table 1 shows the clustering ex-perimental results with ANGEL, K-means, DBSCAN, CLIQUE and GDH using 230,000 datasets.

In complex datasets such as DataSets 4, 5, 6 and 7, GDH and CLIQUE require set small capacity of hypercube for se gmenting and detecting the edges of the clusters that are close to each other. Hence, the time cost of GDH and CLIQUE raises with increasing numbers of hyp ercubes to be searched and processed. ANGEL usually yields more accurate results and performs fast than K-means, DBSCAN, CLIQUE and GDH, as shown in Table 1.
 This paper presents a new clustering algorithm called ANGEL that integrates grid-based, density-based and hierarchical approaches. The proposed algorithm makes the following contributions. First, the algorithm improves the clustering performance of large databases. Second, unlike conventional clustering methods, the proposed ANGEL algorithm successfully eliminates edge indention. Finally, the proposed algorithm accurately identifies large patterns that are close to each other. Additionally, simulation results reveal that the proposed new clustering algorithm performs better than some existing well-known approaches such as the K-means, DBSCAN, CLIQUE and GDH methods.
 Acknowledgments. The author would like to thank the National Science Council of Republic of China for financially supporting this research under con-tract no. NSC 95-2221-E-020-036.

