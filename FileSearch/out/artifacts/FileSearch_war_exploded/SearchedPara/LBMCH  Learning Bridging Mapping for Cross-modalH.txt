 Hashing has gained considerable attention on large-scale sim-ilarity search, due to its enjoyable efficiency and low storage cost. In this paper, we study the problem of learning hash functions in the context of multi-modal data for cross-modal similarity search. Notwithstanding the progress achieved by existing methods, they essentially learn only one common hamming space, where data objects from all modalities are mapped to conduct similarity search. However, such method is unable to well characterize the flexible and discriminative local (neighborhood) structure in all modalities simultane-ously, hindering them to achieve better performance.
Bearing such stand-out limitation, we propose to learn heterogeneous hamming spaces with each preserving the lo-cal structure of data objects from an individual modality. Then, a novel method to l earning b ridging m apping for c ross-modal h ashing, named LBMCH, is proposed to char-acterize the cross-modal semantic correspondence by seam-lessly connecting these distinct hamming spaces. Mean-while, the local structure of each data object in a modality is preserved by constructing an anchor based representation, enabling LBMCH to characterize a linear complexity w.r.t the size of training set. The efficacy of LBMCH is experi-mentally validated against real-world cross-modal datasets. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Hashing; Multi-modal Data; Bridging Mapping
Hashing is dramatically efficient for approximate nearest neighbor search while enjoying the low storage cost of load-ing low dimensional hash codes. It, hence, inspired numer-c  X  ous hashing methods over single-modal data, which is de-scribed within only one feature space.
 Figure 1: (a) Conventional methods learn the hash-ing functions to map all modalities into one common hamming space (b) Our proposed technique learns individual hamming space to well preserve the local structure for each modality.

With the rapid growth of information technology, signif-icant efforts are shifted to learning hashing functions from single-modal hashing to Cross-modal hashing, which is more widely seen in real-life application over multiple data sources as multi-modalities [9, 10, 8, 11, 12, 7], while characterize the same semantics, with each individual data source corre-sponding to one modality, where a query from one modal is issued to search semantically relevant results from another modal [14]. Typical example is image search via a text query to return the images characterizing the similar semantics as text query. There have been a lot of approaches, where the basic idea of learning cross-modal hashing functions is relying on canonical correlation analysis (CCA) [4]. Specif-ically, [5] extended conventional spectral hashing [13] from single modality to multi-modal data, and learnt a common hamming space in which the local structure of all modalities can be well preserved. Motivated by anchor graph hash-ing [6], Zhu et al. [16] proposed a linear cross-modal hash-ing method against anchor based representation regarding each modality. To effectively preserve local structure, [15] proposed a novel parametric method to learn an individual projection matrix for each data object.

Despite the progress achieved by these approaches, one fundamental limitation is that they learn only one common hamming space, which is unable to simultaneously charac-terize the flexible and discriminative local structure of each individual modal, or even ignore the local structure preser-vation, hindering them to achieve better performance.
Bearing the limitation above, we propose a novel tech-nique aimed at L earning B ridging M apping for C ross-modal H ashing, named LBMCH, to learn an individual hamming space adaptive to local structure of each modal, while auto-matically learn a bridging mapping to preserve cross-modal semantic correspondence by seamlessly connecting distinct hamming spaces.

Our proposed method is orthogonal to recent state-of-the-arts with featured contributions summarized below
In this section, we focus on two modalities, which can be easily extended to multiple modalities.
Assume N q is the number of training data in the q modal, denoted as X ( q )  X  R N q  X  d q , where d q is the dimen-sions for q th modal. x ( q ) i is the i th training data object from the q th modal. N pq is the number of observed semantic cor-respondence between training data objects in the p th q th modalities. Without relying on semantic labels of train-ing data, we study a more practical case with training data unlabeled. Unless otherwise specified, given any matrix A , A ij represents the entry at the i th row and j th column of A . Given x ( q ) i , we map it to k -dimensional representation z i , with its j as z ( q ) ij . We make this fit Gaussian distribution, formulated as where p ( q ) ij represents the refined representation based on z ij ,  X  is the bandwidth parameter controlling the decay rate k anchors. We further represent x ( q ) i as  X  ( q ) i = [ p We only keep the value for the s nearest anchors ( s k ) with others being 0, then we can normalize s positive values Instead of learning only one common hamming space, LBMCH is to learn hashing functions characterized by W p and W q for the p th and q th modalities, which can map train-ing data objects into distinct hamming spaces with m p and m q dimensions i.e., code length, respectively, such that m and m q may be different. A bridging mapping matrix i.e., the p th modal to search semantically relevant result within the hamming space from q th modal.
We rewrite two hashing projection matrices W p and W q to be W p = [ w 1 p ,  X  X  X  ,w m p p ] T and W q = [ w 1 q ,  X  X  X  ,w each column of W p and W q generates one bit of hash code for the p th and q th modal. Then we attempt to learn a bridg-ing mapping matrix, M , to map the hash codes from m p -dimensional hamming space to m q -dimensional hamming space or vice versa, by utilizing the cross-modal semantic correlation as provided by training data objects. That is, the cross-modal semantically related data objects should have similar hash codes after mapping. Besides, we expect the bridging mapping matrix to have a good generalization ca-pacity simultaneously. Then we propose to minimize the following objective function X where M hl is the l th entry of the h th row of M . S pq encodes the semantic correlation between training data objects from two modalities. S pq ij = 1 indicates that x ( p ) i is semantically || M || 2 F is leveraged into Eq. (2) to achieve the good gener-alization power.

Now we are ready to propose the overall objective function to learn hash functions W p and W q , for both modalities and the bridging mapping M , simultaneously.
Given the p th and q th modality, we propose to learn W p and M by minimizing the objective function below.

F ( W p ,W q , M ) = X +  X  +  X  where  X  and  X  are trade-off parameters, m p and m q denote the hash code length for the p th and q th modal. I m p and I q represent the identity matrix with the size of m p  X  m p and m q  X  m q . Unlike existing methods that narrowly con-sider only one hamming space, we account for a generalized case adaptive to different modalities, while bridged by map-ping M . Next, we will discuss the optimization solution to Eq. (3). Minimizing Eq. (3) is not jointly convex to W p , W q and M . Hence, we turn to propose a gradient descent method [2] to minimize Eq. (3), seeking the local optimization for each variable while fixing others. Motivation . Unlike conventional methods [15] by con-ducting one-to-one bit of hash code learning fashion ( e.g., learning only the p th bit for two modalities at each iteration) alternatively for two modalities regarding one common ham-ming space, we consider a more generalized case of learning heterogeneous hamming spaces via many-to-many bit re-lationship, characterized by bridging mapping M , which is more practical yet challenging. As we observed, the second term in Eq. (3) indicates that one bit of hash code from the p th modal is related to all bits from the q th modal, and vice versa. Hence, without loss of generality, we first initialize all m q bits from q th by using spectral hashing [13], while initial-izing all M hl to be 1. Particularly, we conduct a sequential optimization strategy, which comprises of two stages below. Why learn all bits from q th modal and M hl at the second stage? To answer this question, we first derive the gradient descent for M hl by calculating  X  X   X  M Then we update M hl by one step gradient descent. Remark-the h th hash bit from the q th modal, implied by ( w h q Thus, we can learn each entry M hl upon the h th bit i.e., w q from the q th modal, and all m p bits of hash code at the second stage . 2.4.1 Sequentially learning each bit from the p th modal
Assume we have learned the previous l  X  1 bits, then we can learn the l th bit by calculating the derivative with re-spect to w l p , that is, where Remarkably, all the above variables are updated and deter-formed by removing the projections of Z ( p ) on the subspace spanned by w k p ( k = 1 ,  X  X  ,l  X  1), thus to encourage bits de-correlation. To learn the optimum w l p w.r.t. the l th hash code, we have 2.4.2 Sequentially learning each bit from the q th modal
Assume we have learned the previous h  X  1 bits, and we are ready to learn the h th bit by calculating the derivative with respect to w h q , then we have where The variables above are updated due to Z ( q ) h , and Z Z to encourage bits de-correlation. To learn the optimum w w.r.t. the h th bit of hash code, we have while, we learn M hl via one step gradient descent by Eq. (4) based on w l q and all m p hash bits from the p th modal.
The time complexity of learning each bit e.g., w l p , from the p th modal of running Eq. (6) comes from the inverse compu-tation, which costs O ( k 3 ). The same cost holds for learning each bit e.g., w h q , from q th modality by the inverse compu-tation from running Eq. (8). The cost for updating M hl is O ( m p N pq k ) by implementing Eq. (4). Therefore, the total complexity is O (( m p + m q ) k 3 + m p m q kN pq + kN p number of anchors. As k N p and k N q , while m p and m q are the length of hash codes for p th and q th modalities, such that m p k and m q k .

Therefore, we expect m 2 j ( j = p,q ) &lt; min { N p ,N k 2 &lt; min { N p ,N q ,N pq } , which leads to m p m q min { kN p ,kN pq ,kN q } &lt; max { kN p ,kN q } . Having k as a constant, the time complexity can be rewritten as O (max { N p ,N q } ), which is linear to the number of training samples on both modalities.
We use Wiki 1 and NUS-WIDE 2 for cross-modal hash-ing with two tasks: retrieving relevant texts by using images as queries and retrieving images by using texts as queries.
The Wiki dataset consists of 2,866 Wikipedia documents, each of which contains one text-image pair. All documents are labeled by one of 10 semantic categories. Each image is represented by a 128-dimensional bag-of-visual SIFT feature vector and each text is encoded by a 10-dimensional feature vector generated by Latent Dirichlet Allocation (LDA) [1]. Following [16], 2173 documents are partitioned as training pairs and remaining 693 documents form query set.

The NUS-WIDE database contains 269,648 labeled im-ages crawled from Flickr, and is manually annotated with 81 categories. We prune it via selecting 186,577 image-tag pairs that belong to the ten largest concepts. The images are rep-resented by 500-dimensional bag-of-visual words (BoVW) and the tags are represented by 1000-dimensional tag oc-currence feature vectors. We randomly select 300  X  300 = 90 , 000 image-tag pairs to be training pairs, with remaining to be query set for two tasks. Besides the training and query set, we also conduct out-of-sample experiments by randomly selecting the 2000 images from the remaining to be query set to search tag modal.

Following [16], we set 300 and 600 anchors i.e., k = 300 and k = 600 in Eq. (1), for both modalities on Wiki and NUS-WIDE datasets. Among k centroids, we set s = 3 nearest anchors to encode the local structure of each data object for both modalities. We set  X  = 0 . 01 in Eq. (3). We consider three state-of-the-art competitors: (1) Cross-Modal Similarity-Sensitive Hashing ( CMSSH ) [3] tackles cross-modal hashing by using Adaboost to sequentially con-struct hash function for each modality whilst only consid-ering cross-modal similarity in one common hamming space without preserving local structures. (2) Cross-View Hashing ( CVH ) [5] extends spectral hashing to cross-modal hashing via CCA (Canonical Correlation Analysis) in one common hamming space. (3) Linear Cross-Modal Hashing ( LCMH ) [16] propose a cross-modal hashing using different anchor graphs, while preserving both intra-and-inter modal simi-larity in one common hamming space. Figure 2: Precision-recall curve on Wiki dataset
Results on NUS-WIDE Dataset. We show the result of two tasks on NUS-WIDE in Fig.2 (c)-(d). Obviously, we can see the advantage of LBMCH by flexibly learning the bridging mapping for various hamming spaces instead of only one common space characterized by other methods.
Time complexity against training size. We evalu-ate the time complexity with varied training size on two databases, and report the results in Fig.3. It can be seen that the time cost of our method grows linearly with in-creasing training samples, which demonstrates the efficiency of LBMCH . Figure 3: Time complexity against training size over two databases.
In this paper, we propose a novel technique by learning distinct hamming space so as to well preserve the flexible and discriminative local structure of each modality. Based on that, a bridging mapping is learned to seamlessly con-nect these individual hamming spaces for cross-modal hash-ing. Extensive experimental results over real-world datasets demonstrate the effectiveness and efficiency of our method. Acknowledgements. Xuemin Lin is supported by NSFC61232006, ARC DP120104168, ARC DP140103578, and ARC DP150102728. Wenjie Zhang is supported by ARC DP150103071 and ARC DP150102728.
