 User interfaces of modern search engines have evolved rapidly in recent years. Modern web search engines do not only return a list of documents as a response to a user X  X  query but they also provide various interactive features that help users in quickly finding what they are looking for or assist them in browsing the information. Google, for example, provides a list of query suggestions while a user is typing in her queries in the search box. Beyond Web search we also ob-serve more interaction emerging as illustrated by the success of AquaBrowser 1 as a navigation tool in digital libraries. Such interfaces rely on a wealth of knowl-edge that characterise the domain and specify relations between the different concepts and entities. A number of approaches have been developed to extract knowledge structures that could be exploited to enrich these interfaces. One promising approach is to perform search log analysis which captures the com-munity knowledge about the domain. Query flow graphs extracted from query logs are an example of these approaches which have proven to be useful for providing query recommendations.

In this study, we extend the query flow graph model which relies on query flows as implicit source of feedback by incorporating the post-query user brows-ing behaviour in the form of clicks. We explore various settings of this model by running an automatic evaluation on actual search logs to understand the impact of various interpretations of click information on the quality of query recommendations.

The paper is structured as follows. We will give a short review of related work in Section 2. Section 3 will descri be how we extend the query flow graph model by adding click information using query logs. The experimental setup is explained in Section 4. Results are presented and discussed in Section 5. We will draw conclusions in Section 6 and outline future work in Section 7. Query recommendations have b ecome ubiquitous in modern search engines. This is true for Web search engines but also for more specialised search engines. The challenge is to identify the right suggestions for any given search request, and this may depend on a number of factors such as the actual user who is searching, the context, the time of the day etc. A promising route for deriving query recommendations appears to be the exploitation of past interactions with the search engines as recorded in the logs . Several approaches have been proposed in the literature to provide query modification suggestions. Studies have shown that users want to be assisted in this manner by proposing keywords [19], and despite the risk of offering wrong suggestions they would prefer having them rather than not [16].

With the increasing availability of search logs obtained from user interactions with search engines, new methods have been developed for mining search logs to capture  X  X ollective intelligence X  for providing query suggestions as it has been recognised that there is great potential in mining information from query log files in order to improve a search engine [9,15].

Given the reluctance of users to provide explicit feedback on the usefulness of results returned for a search query, the automatic extraction of implicit feedback has become the centre of attention of muc h research. Clickthrough data is one form of the implicit feedback left by users which can be used to learn the retrieval ranking function [10], [11], [1]. Queries and clicks can be interpreted as  X  X oft relevance judgements X  [6] to find out what the user X  X  actual intention is and what the user is really interested in. Query r ecommendations can then be derived, for example, by looking at the actual queries submitted and building query flow graphs [4], [5], query-click graphs [6], cover graphs [3] or association rules [8]. Jones et al. combined mining query logs with qu ery similarity measures to derive query modifications [12].

Mining post-query click behaviour has also been studied and applied in in-formation retrieval task s. For example, Cucerzan et. al. [7] used landing page information to derive query suggestions. White et. al. [18] mined user search trails for search result ranking, where the presence of a page on a trail increases its query relevance. Click graphs were used by White and Chandrasekar to derive labels to shortcut search trails to help users reach target pages efficiently [17].
Given the successful application of both the query flow graph model as well as post-query click information we explore the potential of extending the query flow graph with click information for deriving query recommendation suggestions. 3.1 The Query Flow Graph The query flow graph was introduced in Boldi et al. [4] and applied for query recommendations.

The query flow graph G qf is a directed graph G qf =( V,E,w )where:  X  V is a set of nodes containing all the distinct queries submitted to the search  X  E  X  V  X  V is the set of directed edges;  X  w : E  X  (0 .. 1] is a weighting function that assigns to every pair of queries The graph can be built from the search logs by creating an edge between two queries q,q if there is one session in the logs in which q and q are consecutive. A session is simply defined as a sequence of queries submitted by one particular user within a specific time limit.

The weighting function of the edges w depends on the application. Boldi et al. [4] developed a machine learning model that assigns to each edge on the graph a probability that the queries on both ends of the edge are part of the same chain. The chain is defined as a topically coherent sequence of queries of one user. This probability is then used to eliminate less probable edges by specifying some threshold. For the remaining edges the weight w ( q,q )iscalculatedas: Where:  X  freq ( q,q ) is the number of the times the query q is followed by the query  X  R q is the set of all reformulations of query q in the logs.
 Note that the weights are normalised so that the total weights of the outgoing edges of any node is equal to 1. 3.2 Enriching the Query Flow Graph In this section we explain how we extend the query flow graph model with click data. The intuition here is to use implicit feedback in the form of clickthrough data left by users when they modify their queries which has been shown to be powerful feedback, e.g. [6]. We consider the number of clicked documents by a user after submitting a query as an indication of how useful the results are. This is line with previous work on evaluating search engines with clickthrough data [14].
 of the reformulation ( q,q ), where  X  k ( q,q ) is the number of the times the query q is followed by the query q and the user has clicked k (and only k )documents on the result list presented to the user after submitting query q . We aggregate over all users here.

We modify the weighting function in equation 1 to incorporate the click in-formation as follows Where C is an array of co-efficient factors for each band of click counts. Choosing different values for C i allows us to differentiate between queries that resulted in more or fewer clicks. For example queries which result in a single click might be interpreted as more important than the ones which resulted in no clicks or more than one click as the single click may be an indication of quickly finding the document that the user is looking for.
 In our experiments we investigate how different values of the co-efficient C i affect the quality of the query recommendations. Note that the weight-ing function of the standard graph in Equation 1 is the special case where C 0 = C 1 = C 2 = .. =1. 3.3 Query Recommendations Query recommendation is the problem of finding for a given query q relevant query suggestions. If we want to recommend only a single query, then we try to identify the  X  X ost important X  query q . The query flow graph can be used for this purpose by ranking all the nodes in the graph according to some measure which indicates how reachable they are from the given node (query). Boldi et al. [4] proposed to use graph random walks for this purpose and reported the most promising results by using a measure which combines relative random walk scores and absolute scores. This measure is where:  X  s q ( q ) is the random walk score relative to q i.e. the one computed with a  X  r ( q ) is the absolute random walk score of q i.e. the one computed with a
In our experiments, we adopted this measure for query recommendation and used the random walk parameters reported by Boldi et al. The aim of the experiments is to investigate whether the query flow graph can be enhanced and how the performance of query recommendations can be affected by different values of the coefficient factors of click counts presented in Equation 2.
The experiments conducted try to answer these questions: 1. Using search logs of a local search engine 2 , can we achieve better query 2. Does the same observation hold true when we use the search of another In this section we first provide a descr iption of the search logs used in these experiments. Then we introduce our experimental design and illustrate the dif-ferent models being tested. 4.1 Search Logs The main search log data in our experiments are obtained from the search engine of the Web sites of the University of Essex (UOE). In this search log we can obtain the query that has been entered, a time stamp of the transaction and the session identifier. In addition to that the clicked documents from the result lists by users following each query can also be obtained. We used a period of 10 weeks of logs between February and May 2011. During this period a total number of 142,231 queries were submitted to the search engine in 90,684 user sessions and 99,733 clicks on the results were logged. Figure 1 illustrates a histogram of the frequency of queries corresponding to the resulting number of clicks following each query as recorded in the l ogs of that search engine.

To validate the findings of our experiments on those search logs we conducted further experiments on search logs of another academic institution, the Open University (OU), where the same sort of data can be obtained. Figure 2 shows the corresponding histogram for the logs of the OU search engine using exactly the same 10-week period. It has a similar shape with much higher values of counts. In both histograms, for most cases the users either click on one result or do not click at any. 4.2 Query Flow Graphs To assess the quality of query recommendations that can be achieved using our enriched query graph model we used an automatic evaluation approach based on the search logs to compare the quality of recommendations for various com-binations of co-efficient factors of click counts.

Based on the fact that less than 2% of all queries result in more than 2 clicks, we simplified Equation 2 for the experiments as follows: where C k is the co-efficient factor of all click counts which are larger than 1. i.e. no matter whether a query has resulted in 2 or more clicks on resulting documents we treat all cases the same.

Table 1 lists all the combinations we considered in running the automatic evaluation framework.

We adopted the frequency weighting used by Boldi et al. [4] without incorpo-rating the learning step as our goal is to show how we can enrich the query flow graph with click data. The learning step can always be added to the enriched version of the graph.

QFG standard is the standard query flow graph where no click information are incorporated. QFG no zero is an enriched query flow graph where reformulations which result with no clicks on the presented document list to the user are not boost queries with a single click on the presented list. QFG penalise many penalises queries which attract 2 clicks or more.
 4.3 The Evaluation Framework The automatic evaluation framework assesses the performance of query recom-mender systems over time based on actual query logs by comparing suggestions derived from a query recommender to query modifications actually observed in the log files. The validity of the framework has been confirmed with a user study [2].

The evaluation is performed on arbitrary intervals, e.g. on a weekly basis. For all Q query modifications in a given week, w e can calculate the system X  X  Mean Reciprocal Rank ( MRR )scoreas where r i is the rank of the actual query modifications in the list of modifica-tions recommended by the system. Note tha t in the special case where the actual query modification is not included in the list of recommended modifications then 1 /r is set to zero. The above evaluation process results in a score for each logged week. So overall, the process produces a s eries of scores for each query recom-mendation system being evaluated. These scores allow the comparison between different system. One query recommender s ystem can therefore be considered superior over another if a statistically significant improvement can be measured over the given period.

In our experiments we start with an empty query flow graph and we go through the search log data. At the end of each interval, we calculate the MRR score for that interval by producing a ranked list of query suggestions using the process described in Section 3.3 and then we use that interval data to update the graph adding necessary edges and adjusting the weights.

Producing query suggestions from the graph is computationally expensive as it requires performing a random walk on the nodes in the graph. Due to computing limitations, when calculating the MRR score we consider only a sample of the query modifications in the batch by taking every tenth query modification. The automatic evaluation framework has been run on the various enriched query flow graphs listed in Table 1. We used the log files collected on the UOE search engine for our first experiments. We ran the evaluation on the entire 10-week period and used weekly batches to calculate the MRR scores for each graph.
Using the MRR scores, we can assess the graph performance over time in generating query recommendations and compare the performance of different graphs.

Table 2 presents the average weekly MRR scores obtained (ordered by aver-age score). We observe that the enriched query flow graphs are outperforming the standard query flow graph. Apart from QFG no zero all enriched graphs are producing higher average MRR scores. To perform a statistical analysis on the differences between the enriched query flow graphs, in Table 3 we compare the query flow graphs using the average percent increase of MRR scores and the p value of a two-tailed t-test.

We observe that when boosting the co-efficient factor of single clicks, statistically significant improvements are obtained. Both QFG boost one and QFG boost one more are significantly better than the standard query flow graph QFG standard . However no further improvement can be observed when we further boost the co-efficient factor of single clicks. In fact QFG boost one more is slightly pact of reducing the co-efficient factor of more than one click counts. The results show that this does not have a positive impact on the quality of recommenda-
Only enriched graph QFG no zero failed to improve the MRR scores, and in fact it was significantly worse than the standard graph with a high average per-centage decrease. This appears to be coun ter-intuitive as we would assume that queries resulting in no clicks are not good candidates for query recommendation suggestions, and this finding warrants further analysis in future experiments.
In any case, this last finding suggests that completely eliminating reformula-tions with no user clicks affects the quer y recommendation quality negatively. formulations but we are also penalising them as they have a smaller co-efficient factor.

To validate the findings we obtained the log files of another academic search engine. To get a comparable number of interactions we decided to run this ex-periment in daily batches over 10 days of the April 2011 logs, i.e. we now use daily intervals to update the graph and calculate the MRR scores.

Table 4 presents the results obtained in this experiment. The corresponding t-test results can be found in Table 5.

Despite some minor differences we can see the same pattern. The ordering of the graphs according to their average MRR scores is similar. Only positions query flow graphs are outperforming the standard query flow graph but no sta-tistical significant was observed this time.

Like before, reducing the co-efficient factor of many click counts did not have a positive impact on this dataset either. In fact QFG penalise many is now signif-icantly worse than QFG boost one . Again, we find that eliminating queries that result in no clicks does not improve performance but instead results are signifi-cantly worse. Query flow graphs built from query logs are a common and efficient technique to learn useful structures that can be utilised in query recommendation. We presented a new approach for incorporating user post-query browsing behaviour in the query flow graph. This is done by taking into account the number of documents that have been clicked by the user after submitting a query.
In this paper we explored variations of interpreting the number of clicked documents by conducting controlled, deterministic and fully reproducible exper-iments. which are based on an automatic evaluation framework that uses real world data to assess the performance of different models. Our experiments al-lowed us to quantitatively answer our research question and to draw very useful conclusions.

Boosting queries which result in a single document click has a positive impact on query recommendation. A single click can be interpreted as quickly reaching a landing page and rewarding these queries significantly improved the automatic evaluation scores. This is line with previous findings on using landing pages to generate query recommendations [7].

Eliminating queries which result in no clicks negatively impacted query rec-ommendation. One possible explanation (but certainly only one single aspect) could be that some users found what they are looking for in the result snip-pets and as a result they would not continue clicking on the right document. Therefore, the graph will miss those useful suggestions. Penalising these refor-mulations without completely eliminating them though would have a positive effect as graphs QFG boost one , QFG boost one more have a smaller co-efficient fac-tors for zero clicks.

We also show the observation made on one dataset was similar on a differ-ent dataset. The performance of the experimented graphs was similar on both datasets. However no statistical significance was observed for the enriched query flow graph over the standard query flow graph on the OU search engine. This may be due to the higher sparsity of the OU search engine logs. There is much room for future work. One area we will investigate is to automat-ically optimise the parameters. An extension of that work will then also allow us to look at building a machine learning model which can be trained on actual search log data taking as features the post query browsing behaviour including the click information to optimise the graph weighting function. Other browsing behaviour features can be further explored.

The appeal of an automated evaluation framework is that we can re-run ex-periments and explore a large search spa ce without any user intervention. The shortcoming is that any automated evaluation makes some simplifying assump-tions, and end users will ultimately need to be involved to assess the real impact of the query recommendation suggestions being employed. We see our evalua-tion as a first step in assessing what methods are promising and select those that promise the highest impact. We are about to incorporate a number of these models in a live Web site where we interleave recommendations coming from different models in the spirit of the active exploration approach presented by Radlinksi et al. [13] Acknowledgments. This research is part of the AutoAdapt 3 research project. AutoAdapt is funded by EPSRC grants EP/F035357/1 and EP/F035705/1.

