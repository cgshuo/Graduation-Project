
School of Business and IT, University of Bor X s, Bor X s, Sweden Department of Computer and Systems Sciences, Stockholm University, Stockholm, Sweden 1. Introduction
In predictive classi fi cation, the main goal is to build models that obtain good predictive performance when applied to novel (production) data. In situations where predictive performance is the only criterion, techniques producing opaque models, such as arti fi cial neural networks , support vector machines or ensembles , are most often chosen. In some situations, however, models also need to be interpretable, making the use of techniques that produce transparent models, such as decision trees or ordered rule sets, necessary. Unfortunately, these techniques are known to produce models with worse predictive performance than opaque models, thus leading to a situation where some predictive performance has to be sacri fi ced to obtain comprehensibility.

In data mining research, performance on production data is estimated in different ways, typically using cross-validation or a hold-out test set. It is generally acknowledged that in order to get a fair estimate of predictive performance, the data on which the model is evaluated must never be used to build the model . Thus, some data is usually completely set aside when building the data mining model. However, in real-world applications, when a data mining model is built for a speci fi c prediction task, utilizing all available data in the best possible way is often essential. In these situations, any means of maximizing production accuracy should be employed. This study focuses on the common situation where the input vectors in the production set are already determined and available when building the model, i.e., the model is actually built speci fi cally for classifying these instances.

The purpose of this paper is to suggestand thoroughly evaluatea straightforward, and yet very effective, way of increasing production accuracy for transparent models. As noted above the proposed method requires that production input vectors are available when building the model. The main idea in the method is to use a powerful opaque model to produce target values for unlabeled production instances, yielding additional training instances which are then utilized when building the transparent model. This opaque model is called an oracle , since the target values it produces are treated as ground truth by the training regime of the transparent model. The technique of fi rst using a strong classi fi er as an oracle when labeling the production set, and then using this labeled set as input when generating a transparent model, is consequently referred to as oracle coaching . 2. Background
This section brie fl y presents the targeted problem and describes related work. 2.1. Terminology
A predictive classi fi cation model is a function f mapping each instance x to one label in a prede fi ned set of discrete classes { c 1 ,...,c n } . Normally, predictive models are obtained by applying some supervised learning technique on historical (training) data. Most supervised learning techniques require that values for the target variable are available for all training instances. A training instance, thus, consists of an input vector x i with a corresponding target value y i . The predictive model is a function y = f ( x ;  X  ) able to predict a value y , given an input vector of measured values x and a set of parameters  X  for the model f . The process of fi nding the best  X  values is the core of the data mining technique.

When performing data mining, the data set is normally split into several parts. Each part is used differently and for different purposes. Unfortunately, the names of the data sets are not fully standardized. A fi rst, important, division is to separate the parts of the data set used to build the model from the parts used to evaluate the model.  X  The training set is the part of the data set used to build one or more models.  X  The validation set is used for model selection. The purpose of the validation set is to enable selection  X  The test set is used to evaluate the fi tted model. The test set is in no way used to build the model, but  X  The production set is the data set on which the model is actually used.

Somewhat confusingly, researchers normally report results on a test set, and if one or more speci fi c hold-out sets are used to somehow rank and select models, they are referred to as validation set 1, normally assumed to be available when model building is performed. We argue, however, that this may be overly restrictive. In many situations, as described in the introduction, it is actually only the values of the target variable of the production set that are not known when building the predictive model. In real-world applications, where the predictive model is explicitly built for the prediction task at hand, the unlabeled production instances that later will be used for the actual prediction, could also be used for building the model.

A typical situation illustrating this is when using predictive classi fi cation to target recipients of a marketing campaign, and the training data consists of past response data. Here, the very same customer signatures that will later be used as production data input vectors are available already during model construction. In this example, the oracle coaching would produce a highly accurate and interpretable predictive model that could be used not only to explain individual predictions, but also as a basis for expert analysis.

Another example is when medicinal chemists use in silico modeling to predict biopharmaceutical properties (e.g., toxicity) of certain compounds, based on existing compound descriptions labeled as either toxic or non-toxic. In this situation, models would often be built explicitly for predicting the toxicity of a pre-de fi ned set of production instances (compounds), typically described using attributes in the form of chemical descriptors. Here too, models produced by oracle coaching could explain predictions (why a speci fi c compound is predicted as toxic or not) and be used by medicinal chemists to fi nd and analyze general relationships in the data set.

To summarize, the purpose of oracle coaching is to provide means for generating both accurate and comprehensible models, focusing on speci fi c instances. To achieve this, oracle coaching requires a suf fi ciently sized production set, and that these unlabeled production instances (the input vectors) are available when building the predictive model. 2.2. Related work
Several machine learning schemes employ ideas related to the oracle coaching concept, most notably semi-supervised , transductive and active learning . The generation of transparent models from opaque ones has earlier been studied within rule extraction .

In semi-supervised learning, both labeled and unlabeled data are used when building models. Under this umbrella term, there are several fundamentally different approaches, as well as numerous variations; a good survey of the fi eld is found in [1]. Arguably, the most straightforward approach is self training . Several standard approaches fi rst build a model using only labeled data, and then use this model to label some of the unlabeled instances, thus creating more labeled training instances. This process may be repeated over several iterations, and there are many variations, but the fi nal classi fi er is normally trained using a majority of initially unlabeled instan ces. Normally, in each itera tion, the model labels only those unlabeled instances which it is most con fi dent about. It should be noted that semi-supervised classi fi cation is still an instance of inductive learning, since a model is ultimately built that can be used for classifying instances in the entire input space.

In contrast to inductive learning, transductive learning focuses on accurately classifying the production instancesonly, rather than aiming for accurate classi fi cationof all possibleinstances. However, as pointed out in [2], there is no crisp border between transductive and semi-supervised methods, as the former often result in models that may be used for classifying the entire input space, while methods of the latter type can be used also in the transductive setting. With this in mind, the term transductive learning is often used to charact erize an algorithm utiliz ing both labeled an d unlabeled data to obtain high accuracy on speci fi c production instances; see e.g. [3,4].
In active learning, the learner obtains labels for selected instances by asking an oracle. The key ingredient in active learning is that the learner chooses which unlabeled instances to query the oracle about. In the general case, the learner may choose freely from the input space without being limited to a given set of unlabeled instances, although such methods could still be applied in the more restricted case, e.g., by querying the oracle about those instances in the set that it is least con fi dent about. Some attempts to combine active and semi-supervised learning have been put forward, see e.g. [5]. For a comprehensive survey of active learning, see [6].

Similar to active learning, our suggested approach also relies on an oracle. However there are some important differences. The main component of active learning methods is the strategy for choosing a set of instances to query the oracle about. Strategies typically aim at selecting a set expected to maximize predictive performance, once labels are provided by the oracle. In our approach, the choice of instances to query the oracle about is not part of the learning algorithm, but is completely determined by the production set. Furthermore, instead of considering any type of oracle, such as a human domain expert, the oracle is assumed to be a strong (typically opaque) classi fi er generated from the training set, while the fi nal model is a transparent (typically weaker) classi fi er.

Rule extraction is the process of generating a transparent model based on a corresponding opaque predictive model. Rule extraction has been thoroughly investigated for arti fi cial neural ne tworks (ANNs), and the techniques have been applied mainly to ANN models; for an introduction and a good survey of traditional methods, see [7]. Craven and Shavlik [8] used the expression extraction strategy to describe the process of transforming the opaque model into a new, comprehensible, representation. There are two fundamentally different extraction strategies, decompositional ( open-box or white-box )and pedagogical ( black-box ). Decompositional approaches focus on extracting rules at the level of individual units within a trained ANN, while pedagogical approaches treat the opaque model as a black box. Two classic decompositional rule extraction algorithms are RX [9] and Subset [10]. The core pedagogical idea is to view rule extraction as a learning task, where the target concept is the function originally learnt by the opaque model. Black-box rule extraction is, consequently, an instance of predictive modeling, where each input-output pattern consists of the original input vector x i and the corresponding prediction f ( x ;  X  ) from the opaque model. Two typical and well-known black-box rule extraction algorithms are TREPAN [11] and VIA [12]. Some more recent rule extraction studies include [13,14]. Finally, it could be noted that although most rule extraction focuses on classi fi cation, there are also rule extraction algorithms like REFANN [15] that provide transparent regression models.

Naturally, extracted models must be as similar as possible to the opaque models. This criterion, called fi delity, is therefore a key part of the optimization function in most rule extracting algorithms. Most, if not all, rule extraction algorithms targeting fi delity use 0/1 fi delity, i.e., maximize the number of identical classi fi cations. Most importantly, for this application, the opaque model is thus a very accurate model of the function between input and output, so it could be used to label novel instances with unknown target values, as they become available. Naturally, these newly labeled instances could then be used as learning examples. We have previously shown that it could be advantageous to use production input vectors together with predictions from the opaque model when performing rule extraction [16]. In that study, we showed that using oracle data together with our rule extraction algorithm G-REX [17] led to a signi fi cant increase in accuracy for the extracted models.

Recently, we extended the suggested methodology to the more general predictive classi fi cation task, using well-known and readily available classi fi ers; see [18]. In this paper, we introduce some important modi fi cations of the original idea, and conduct a detailed analysis of the entire oracle coaching concept. Naturally, the overall goal is to show how oracle co aching can be utilized when striving for accurate transparent models.
 3. Method
This section gives a detailed description of the s uggested technique and outlines three experiments for evaluating and analyzing its performance. 3.1. Algorithms
As mentioned in the introduction, the purpose of this study is to evaluate and analyze the use of a high-accuracy opaque model (serving as a coaching oracle) for creating transparent predictive models. More speci fi cally, decision trees and rule sets induced directly from training data only are compared to decision trees and rule sets built using different combinations of training data and production data with class labels given by the oracle. For simplicity, and to allow easy replication of the experiments, the Weka workbench [19] is used for all experiments.
 In this study, two kinds of ensemble models are used as oracles, a large random forest [20] and a set of RBF neural networks, trained and combined using standard bagging [21]. For the actual classi fi cation, J48 and JRip are used since they represent what probably are the most famous tree inducer C4.5 [22] and rule inducer RIPPER [23], respectively. J48 obviously builds decision trees, while JRip produces ordered rule sets. In the experimentation, all Weka settings are left at the default values for the different techniques, unless stated otherwise.
 The ensemble (a random forest or a set of bagged RBFs) is fi rst generated using training data only. This ensemble (the oracle) is then applied to the production instances, creating production predictions. This results in two different data sets:  X  The training data: this is the original training data set, i.e., original input vectors with corresponding  X  The oracle data: this is the production instances with corresponding ensemble predictions as target
In the experimentation, these data sets are evaluated both when used on their own and when used in combination as input for the techniques producing transparent models. In practice, this means that J48 and JRip will optimize different combinations of training accuracy and production fi delity. These combinations are:  X  Induction (I): Standard induc tion using original traini ng data only, i.e., maxim izes traini ng accuracy.  X  Explanation (X): Uses only oracle data, i.e., maximizes production fi delity.  X  Indanation 1 (IX): Uses training data and oracle data, 3i.e., maximizes training accuracy and produc-3.2. Experimental setup
This study contains three experiments. In the fi rst experiment, both accuracy and area under the ROC curve (AUC) are used for evaluating the predictive performance, but in the second and third experiment, only accuracy is used. While accuracy is based only on the classi fi cations, AUC measures the ability to rank instances according to how likely they are to belong to a certain class; see e.g. [24]. AUC can be interpreted as the probability of ranking a true positive instance ahead of a false positive; see [25]. In addition, since the motivation for using oracle coaching is that comprehensible models are needed, model sizes are also reported. For J48 trees, the model size is the tree size, i.e., the total number of nodes in the tree. For JRip rule sets, the model size is the number of rules.
 For the evaluation, 10x4-fold cross-validation is used, i.e., 4-fold cross-validation repeated 10 times. The reason for not using the more standard value of ten folds is the fact that the use of only four folds results in what we believe to be a more representative proportion between training and production data. The reported accuracies, AUCs and model sizes are therefore averaged over the 10x4 folds.

In Experiment 1, which aims to evaluate the overall performance and robustness of the suggested method, J48 trees and JRip rule sets are built using oracle ensembles consisting of either a random forest with 300 trees or a set of 30 bagged RBF neural networks. It should be noted that, in order to make this paper self-contained, Experiment 1 is actually an extended version of the main experiment conducted in [18]. Extensions include the use of several more data sets for the evaluation, and a substantially more detailed analysis.

Experiments 2 and 3 contain a more detailed evaluation with the aim of fi nding ways to boost performance by changing either the oracle, the transparent model or the way in which training data and oracle data is combined.
 In Experiment 2a, the effects of varying the oracle and transparent model construction are studied. For the oracle variation, random forests of sizes 10, 100, 300 and 1000 trees are used as oracles. Since the default settings for J48 include pruning, which is not necessarily bene fi cial for the task of predicting and explaining relatively few production instances, Experiment 2b evaluates using unpruned J48 trees as transparent models. Here, the chosen oracle model is a 300-tree random forest.

Experiment 3, fi nally, explores alternative ways of combining the training data and oracle data by assigning different weights to the examples from the two sets when inducing the transparent model. When using normal IX, training instances and production instances have equal weights. Due to the choice of employing 4-fold cross-validation, the proportion between training and production instances is, however, 3:1, so there might be a risk of prioritizing training instances when building the transparent model. With this in mind, Experiment 3 evaluates assigning weights of 2, 3 and 6 to the production instances. Naturally, setting the weights to 3 makes t he training and the productio n sets equally important. Technically, the re-weighting is done by resamplin g of the data, resulting in larger data sets. This experiment also uses random forests with 300 trees as oracles, and J48 as the method for producing a transparent classi fi er.

The 30 data sets used are all well-known and publicly available from the UCI Repository [26]. 4. Results
This section presents and analyses the results for the three experiments. 4.1. Experiment 1
Table 1 shows the results for J48 when coached by random forest oracles. The accuracy and AUC results for the oracle (O) are included for comparison. The relationship between oracle and corresponding transparent model performance is further analyzed later in the paper. Starting with accuracies, it is very obvious that the use of oracle coaching outperforms standard tree induction, since both IX and X obtain higher accuracies than I on most data sets. On some data sets, like Glass , Sonar , Liver and Waveform , the increase in accuracy is actually up to 10%, and the mean increase for IX and X is 4.7% and 3.4%, respectively. Most importantly, using oracle data together with the original training set (IX) never results in reduced accuracy. When using oracle data only (X), slightly worse accuracies than I are obtained in a few cases. Comparing IX to X, it can be observed that augmenting the oracle data with training data clearly results in the highest overall accuracy.

Analyzing AUCs, the picture is very similar, although the differences are smaller. Nevertheless, a direct pairwise comparison between IX and I shows that IX obtains 22 wins and fi ve ties on the 30 data sets. The corresponding numbers when comparing X to I are 17 wins for X and fi ve ties. Overall, the results clearly show that the use of production data is bene fi cial also when considering the ranking ability of the models.

Looking at model sizes, fi nally, the result is what could be expected, i.e., using more training instances results in larger J48 trees. Speci fi cally, building the tree based on production instances only always gives the smallest tree. On average, the trees that are produced by X are less than half the size of the trees that are induced by I. Although there is not a strict one-to-one relationship between size and comprehensibility, it is fair to say that a number of trees built using I and IX are too complex to be deemed comprehensible. Still, the overall picture is that a large majority of all J48 models built are compact enough to allow interpretation.
To determine if there are any statistically signi fi cant differences, we use the statistical tests recom-mendedbyDem  X  sar [27] for comparing several classi fi ers over a number of data sets, i.e., a Friedman test [28], followed by a Nemenyi post-hoc test [29]. With three classi fi ers and 30 data sets, the critical distance (for  X  = 0.05) is 0.60, so based on these tests, there is a clear ordering with regard to accuracy. The models that are produced using IX are signi fi cantly more accurate than the models produced using X, which in turn are signi fi cantly more accurate than the models induced using standard J48. Comparing AUCs, also using mean ranks, the performance of IX is again signi fi cantly better than both I and X. There is also a clear, but not statistically signi fi cant, difference in AUC between X and I. Regarding model sizes, all differences are statistically signi fi cant; i.e., the models generated by X are signi fi cantly smaller than the models generated by I, which in turn are signi fi cantly smaller than the models generated by IX.

Table 2 shows the results for JRip when using random forests as oracles. The overall picture is very similar to the J48 results. More speci fi cally, the rule sets induced using IX are signi fi cantly more accurate, and have signi fi cantly higher AUCs, than both rule sets induced using oracle data only (X) and the standard JRip rule sets obtained from training data (I). Furthermore, the rule sets generated by X contain signi fi cantly fewer rules than the rule sets produced by IX and I. As a matter of fact, the only notable difference from the J48 results is the fact that the rule sets produced using standard JRip are not outperformed by X with respect to AUC.

Table 3 shows the results for J48 classi fi ers coached by bagged RBFs. Analyzing the results, the overall picture is almost identical to when using random forests as oracles. Trees generated from IX are again signi fi cantly more accurate, and have signi fi cantly higher AUC, than the other two setups. A minor difference to the previous results is that the difference in accuracy between X and I is not signi fi cant. Generally, there are only small differences in performance when comparing J48 trees coached by random forests and J48 trees coached by bagged RBFs. Speci fi cally, accuracy and AUC levels, as well as tree sizes, are very similar. If anything, the trees coached by random forests seem to be marginally more accurate, but also slightly larger.

The results for JRip classi fi ers coached by bagged RBFs in Table 4 show that the IX rule sets are again signi fi cantly more accurate, and have signi fi cantly higher AUCs, than models produced using I and X. For this setup, however, the accur acies obtained by standard JRip not u tilizing oracle data are very close to X. Furthermore, standard JRip actually outperforms X with regard to AUC, although the difference is far from signi fi cant.
The results from the above statistical tests are summarized in Table 5, where a + (  X  )forapairof methods indicates a statistically signi fi cant difference in favor of the fi rst (second) method, based on the Nemenyi post-hoc test. The most important conclusion that can be drawn from the fi rst experiment is that the setup combining training data with oracle data (IX) signi fi cantly outperforms using both training data alone (I) and oracle data alone (X) for both types of oracles considered (RF and RBF) and for both types of transparent models (J48 and JRip). Hence, the experiment strongly suggests that in case a transparent model is to be generated for the classi fi cation of production data, one can expect a gain in both accuracy and AUC by fi rst using a strong model to classify the production data, and then merging this with training data, from which the fi nal transparent model is generated.

One important observation from Experiment 1 is that the accuracies obtained by the coached model, when using both training and oracle data (i.e., IX), on a large majority of the data sets, are very close to the oracle accuracies. This is also re fl ected in the mean accuracies, where the use of RBF ensemble oracles actually results in coached models that are, on average, more accurate than the oracle. For random forest oracles, the mean accuracy is about one percentage point lower for the coached models, compared to the oracle. Looking at AUCs, however, the more complex ensemble models signi fi cantly outperform the coached models, in all four setups. Table 6 show wins, ties and losses for the oracle against IX. A standard sign test (  X  = 0.05) requires 21 wins, so random forest oracles are signi fi cantly more accurate than the coached models, while there are no statistically signi fi cant differences for RBF ensemble oracles.

This analysis, especially when looking at actual values, indicate that there is not necessarily a cost in terms of reduced accuracy that follows from the use of a transparent model when having access to an oracle. In order to get a better understanding of oracle coaching, and how the predictive performance of the resulting transparent model may be even further improved, we take a more detailed look at the performance of one of the methods for generating transparent models (J48) in relation to one of the oracle methods (RF). For this analysis, we de fi ne the following measures:  X  Correct fi delity is the proportion of production instances correctly predicted by the oracle that was  X  Incorrect fi delity is the proportion of production instances that was initially wrongly predicted by  X  Correct in fi delity is the proportion of production instances that was initially wrongly predicted by  X  Incorrect in fi delity is the proportion of production instances th at was initially correctly predicted by Table 7 shows a detailed analysis of the setup where J48 is coached by random forests. The column RF acc. contains the production accuracies obtained by the oracle, while the column J48 acc. contains the production accuracies obtained by the coached J48 trees. These accuracies are taken from Table 1 and are repeated here for convenience. The column Prod. fi d. , shows the proportion of production instances predicted identically to the oracle by the coached J48 classi fi er (production fi delity). The last
As discussed above, the difference in mean accuracy, between the oracle and IX, is only 0.003. On a number of data sets, the coached J48 tree is actually more accurate than the oracle. The explanation for the overall high accuracy of the transparent model is, of course, the very high production fi delity. On most data sets, the production fi delity is over 90%, so the tree models are clearly capable of mimicking the more complex oracle models. In this context, it should be noted that this is despite the fact that J48 uses default settings; i.e., it is not tailored in any way to over fi t the training (here production) examples. Consequently, the resulting tree models are generally not overly complex, as seen by the reported sizes in Table 1.

The correct fi delity scores show that the vast majority of all predictions that are correctly classi fi ed by the oracle are also correctly classi fi ed by the transparent model. As a matter of fact, when averaging over all data sets, more than 95% of all instances that are predicted correctly by the oracle, are also predicted correctly by IX and X. In addition, and somewhat surprising, as many as 10% of the instances where the oracle is wrong, are still predic ted correctly by IX and X. As a matter of fact, the incorrect fi delity of both IX and X is almost fully compensated by the correct in fi delity, most often re sulting in accuracies very close to the oracle accuracies. This suggests that the best way of further improving the predictive performance is by increasing the performance of the oracle, if possible, rather than trying to increase fi delity. Similar conclusions can also be drawn for the other combinations of oracles and transparent models, as seen in Table 8, where the numbers tabulated are mean values aggregated over all folds and data sets. 4.2. Experiment 2
Table 9 shows the results from varying the number of trees in the random forest oracle. As expected, a weaker oracle (here a random forest with fewer trees) will result in a lower oracle accuracy, which transfers to less accurate transparent models. Comparing four classi fi ers over 30 data sets, the critical distance for a Nemenyi post-hoc test (  X  = 0.05) is 0.86, so using only 10 trees in the random forest here results in signi fi cantly less accurate oracle coached models (using J48 trees), compared to using larger random forests, for both IX and X. Once the oracles are suf fi ciently accurate, however, the extra gain in accuracy for the transparent models as a res ult of using more accurate oracles, decr eases . Still, as indicated by the mean ranks, the overall picture is that the better the oracles are, the more accurate the coached transparent models will be. Another interesting observation is that when using weaker oracle models, production fi delities actually go down. This, coupled with the fact that the trees sizes, if anything, are slightly larger for the weakest oracles, seem to indicate that it may actually be easier for a transparent model to approximate a more accurate oracle.

Table 10 compares using pruned J48 trees (default settings) and unpruned J48 trees with no subtree raising. The results with regard to tree sizes and production fi delity are exactly as expected; i.e., using unpruned trees will increase the production fi delity at the expense of larger trees. However, the increased production fi delity does not carry over t o production accuracy. The likely e xplanation is that with IX and X accuracies already very close to oracle accuracies, higher production fi delities will increase incorrect fi delities to the same extent as correct fi delities.
Summarizing Experiment 2, the most important result is probably that more accurate oracles will lead to more accurate coached transparent models. Speci fi cally, nothing in the results indicates that it would be harder for the transparent model to learn from a more accurate oracle; i.e., the fi delity levels were consistent when varying the oracle performance. Using unpruned trees, built to maximize training accuracy rather than to generalize well, does indeed lead to more complex trees with higher fi delities, but does not increase the corresponding accuracies. 4.3. Experiment 3
Table 11 shows the results for using weighted production instances, where these have been assigned weights of 2 (IX-2), 3 (IX-3) and 6 (IX-6), as described in Section 3.2. Although the fi rst impression may be that there are only subtle differences, the mean ranks indicate that using weighted instances will result in small improvements on most data sets. With four classi fi ers and 30 data sets, the critical distance for a Nemenyi post-hoc test (  X  = 0.05) is 0.86, so the only signi fi cant difference with regard to accuracy is that IX-3 is signi fi cantly more accurate than IX. Nevertheless, it could be noted that all methods using weighted production instances clearly outperform IX. As a matter of fact, the number of wins (distributing ties evenly) against IX is 23 for IX-2 and 21 for IX-6, which would be enough for statistical signi fi cance in a pairwise comparison, using a standard sign test.
 Looking at tree sizes, it is clear that the use of weighted instances results in signi fi cantly larger trees. Despite this, the increase in size from IX to the best performing option IX-3, would only rarely debilitate interpretability. One possible exception is the Breast cancer data set, where IX produces quite small trees (28 nodes on average), while the IX-3 trees are more than double that size (59 nodes on average). 5. Conclusions
A method, called oracle coaching , was proposed for improving predictive performance of transparent models in the common situation where the instances to be classi fi ed are known at the time of model building. The approach employs a strong classi fi er (the oracle) to guide the generation of a (weaker) transparent model. This is accomplished by using the oracle to predict class labels for the production data, and then applying the weaker method on this data, possibly in conjunction with the original training set.

Results from evaluating and analyzing the performance of the method on 30 data sets from the UCI repository were presented. More speci fi cally, three experiments were conducted: i) an investigation of the relative performance of oracle coaching compared to standard induction, ii) an investigation of the effects of improving the oracle X  X  performance and increasing the fi delity of the transparent model, and iii) an investigation of assigning different weights to the training and oracle data.

Two variants of oracle coaching were considered in the fi rst experiment: using oracle data only and using both oracle and training data. The most important conclusion from the experiment was that the latter form of oracle coaching signi fi cantly outperforms both standard induction, i.e., using training data only, and using oracle data only, both with respect to accuracy and the area under ROC curve. This turned out to be true for both types of oracles considered (random forests and bagged RBF networks) and for both types of transparent models (J48 and JRip). Hence, the fi rst experiment gave strong evidence in favor of oracle coaching, showing that in case a transparent model is to be generated for the classi fi cation of production data, one can expect a gain in both accuracy and AUC by fi rst using a strong model to classify the production data, and then merging this with training data, from which the fi nal transparent model is generated. An analysis of the fi delity of the transparent models to the oracles showed that performance gains should rather be expected from increasing the performance of the oracle than from increasing fi delity.

The second experiment con fi rmed this analysis, by demonstrating that improving the predictive perfor-mance of the oracle in general carries over to the performance of the transparent model. The experiment further showed that very little is gained from increasing the fi delity by over fi tting the transparent model to the oracle data.

Finally, the third experiment showed that oracle coaching can be further improved by increasing the weight of the oracle data relative to the training data.

The overall conclusion is thus that oracle coaching is an effective and robust method for improving the predictive performance of transparent models whenever production data is available at the time of model building. 6. Discussion and future work
First of all, it must be noted that the situation targeted in this paper is that, for some reason, a black-box prediction machine is not suf fi cient. If comprehensibility is not an issue, techniques like neural networks, support vector machines and ensembles will almost always outperform techniques producing decision trees or rule sets.

Using production instances to build models is of course a somewhat delicate matter. Speci fi cally, most data mining researchers will have an almost instinctive feeling against it. The main point is, however, that oracle coaching uses only data that would be available when building models. Speci fi cally, the true production target values are, of course, not used during the model construction. Oracle coaching requires, however, a suf fi ciently sized production set, i.e., the problem must be one where predictions are made for sets of instances rather than one instance at a time.

For problems matching the description above, oracle coaching is actually a fairly straightforward procedure. Nevertheless, there are of course no integrated tools allowing a seamless application of oracle coaching. With this in mind, one prioritized task is to make the methods evaluated in this paper publicly available, e.g., as a Weka plug-in.

Finally, it could be noted that although this is not primarily a rule extraction paper, the results are still very relevant for that domain. More speci fi cally, the fact that interpretable models trained using oracle coaching have accuracies very close to the corresponding opaque models is a strong result for rule extraction in general. Clearly, it is possible to obtain comprehensible models explaining most of the complex relationships concealed in the opaque models. Some preliminary experiments also indicate that the use of an oracle coached specialized rule extractor, explicitly targeting fi delity while penalizing larger trees, may be able to further imp rove accuracy and/or comprehe nsibility.

Other directions for future work include investigating combinations of other strong methods for generating oracles, as well as alternative ways of generating transparent models. Of particular interest are methods of the latter type that do not necessarily generate models for the entire input space, but rather focus on local models (rules) for the production instances.

As shown by the last experiment, performance gains can be obtained from utilizing different weights for training and oracle data, even when using a rather straightforward weighting scheme. One direction for future work is to investigate more elaborate schemes, speci fi cally algorithms where the weights are automatically adapted to the speci fi c data sets.
 Acknowledgment This work was supported by the INFUSIS project www.his.se/infusis at the University of Sk  X  ovde, Sweden, in partnership with the Swedish Knowledge Foundation under grant 2008/0502.
 References
