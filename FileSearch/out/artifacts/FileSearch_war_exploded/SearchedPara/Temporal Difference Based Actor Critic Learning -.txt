 Actor-critic (AC) algorithms [22] were probably among the first algorithmic approaches to reinforce-ment learning (RL). In recent years much work focused on state, or state-action, value functions as a basis for learning. These methods, while possessing desirable convergence attributes in the context of table lookup representation, led to convergence problems when function approximation was in-volved. A more recent line of research is based on directly (and usually parametrically) representing the policy, and performing stochastic gradient ascent on the expected reward, estimated through try-ing out various actions and sampling trajectories [3, 15, 23]. However, such direct policy methods often lead to very slow convergence due to large estimation variance. One approach suggested in recent years to remedy this problem is the utilization of AC approaches, where the value function is estimated by a critic, and passed to an actor which selects an appropriate action, based on the approximated value function. The first convergence result for a policy gradient AC algorithm based on function approximation was established in [13], and extended recently in [5, 6]. At this stage it seems that AC based algorithms provide a solid foundation for provably effective approaches to RL based on function approximation. Whether these methods will yield useful solutions to practical problems remains to be seen.
 RL has also been playing an increasingly important role in neuroscience, and experimentalists have directly recorded the activities of neurons while animals perform learning tasks [20], and used imag-ing techniques to characterize human brain activities [17, 24] during learning. It was suggested long ago that the basal ganglia, a set of ancient sub-cortical brain nuclei, are implicated in RL. Moreover, these nuclei are naturally divided into two components, based on the separation of the striatum (the main input channel to the basal ganglia) into the ventral and dorsal components. Several imaging studies [17, 24] have suggested that the ventral stream is associated with value estimation by a so learning by a so called actor. Two further experimental findings support the view taken in this work. First, it has been observed [20] that the short latency phasic response of the dopamine neurons in the midbrain strongly resembles the temporal difference (TD) signal introduced in theory of TD-learning [22], which can be used by AC algorithms for both the actor and the critic. Since mid-brain dopaminergic neurons project diffusively to both the ventral and dorsal components of the striatum, these results are consistent with a TD-based AC learning interpretation of the basal ganglia. Second, recent results suggest that synaptic plasticity occurring at the cortico-striatal synapses is strongly modulated by dopamine [18]. Based on these observations it has been suggested that the basal gan-glia take part in TD based RL, with the (global) phasic dopamine signal serving as the TD signal [16] modulating synaptic plasticity.
 Some recent work has been devoted to implementing RL in networks of spiking neurons (e.g., [1, 9, 12]). Such an approach may lead to specific and experimentally verifiable hypotheses regarding the interaction of known synaptic plasticity rules and RL. In fact, one tantalizing possibility is to test these derived rules in the context of ex-vivo cultured neural networks (e.g., [19]), which are connected to the environment through input (sensory) and output (motor) channels. We then envision dopamine serving as a biological substrate for implementing the TD signal in such a system. The work cited above is mostly based on direct policy gradient algorithms, (e.g., [3]), leading to non-AC approaches. Moreover, these algorithms were based directly on the reward, rather than on the biologically better motivated TD signal, which provides more information than the reward itself, and is expected to lead to improved convergence. The TD-based AC algorithm developed in this section is related to the one presented in [5, 6]. While the derivation of the present algorithm differs from the latter work (which also stressed the issue of without the restriction to two time scales which was used in [5, 6, 13]. This result is also important in a biological context, where, as far as we are aware, there is no evidence for such a time scale separation. 2.1 Problem Formulation We consider a finite Markov Decision Process (MDP) in discrete time with a finite state set X of size |X| and a finite action set U . The MDP models the environment in which the agent acts. Each the transition probability from a state x  X  X to a state y  X  X given the control u . A parameterized policy is described by a conditional probability function, denoted by  X  ( u | x,  X  ) , which maps obser-vation x  X  X into a control u  X  U given a parameter  X   X  R K . For each state x  X  X the agent receives a corresponding reward r ( x ) . The agent X  X  goal is to adjust the parameter  X  in order to attain maximum average reward over time.
 For each  X   X  R K , we have a Markov Chain (MC) induced by P ( y | x, u ) and  X  ( u | x,  X  ) . The state transitions of the MC are obtained by first generating an action u according to  X  ( u | x,  X  ) , and then [ P ( y | x,  X  )] x,y  X  X  which is given by P ( y | x,  X  ) = assumptions are required in the proofs below.
 Assumption 2.1. (i) Each MC P (  X  ) , P (  X  )  X   X  P , is aperiodic, recurrent, and contains a single constants B r and B  X  , such that for all x  X  X , u  X  U ,  X   X  R K and 1  X  k 1 , k 2  X  K , we have | r ( x ) | X  B r , |  X  X  ( u | x,  X  ) / X  X  k | X  B  X  , |  X  2  X  ( u | x,  X  ) / X  X  k As a result of assumption 2.1(i), we have the following lemma regarding the stationary distribution (Theorem 3.1 in [8]). Lemma 2.1. Under Assumption 2.1(i), each MC, P (  X  )  X   X  P , has a unique stationary distribution, denoted by  X  (  X  ) , satisfying  X  (  X  ) 0 P (  X  ) =  X  (  X  ) 0 , where x 0 is the transpose of vector x . Next, we define a measure for performance of an agent in an environment. The average reward per stage of a MC starting from an initial state x 0  X  X  is defined by n . The agent X  X  goal is to find  X   X  R K which maximizes J ( x |  X  ) . The following lemma shows that under Assumption 2.1, the average reward per stage does not depend on the initial states (see Theorem 4.7 in [10]).
 Lemma 2.2. Under Assumption 2.1 and Lemma 2.1, the average reward per stage, J ( x |  X  ) , is inde-pendent of the starting state, is denoted by  X  (  X  ) , and satisfies  X  (  X  ) =  X  (  X  ) 0 r . Based on Lemma 2.2, the agent X  X  goal is to find a parameter vector  X  , which maximizes the average reward per stage  X  (  X  ) . Performing the maximization directly on  X  (  X  ) is hard. In the sequel we show how this maximization can be performed by optimizing  X  (  X  ) , using  X   X  (  X  ) . A consequence of Assumption 2.1 and the definition of  X  (  X  ) is the following lemma (see Lemma 1 in [15]). are bounded, twice differentiable, and have bounded first and second derivatives.
 Next, we define the differential value function of state x  X  X which represents the average reward the agent receives upon starting from a state x 0 and reaching a recurrent state x  X  for the first time. Mathematically, Based on the differential value definition we define the temporal difference (TD) between the states x  X  X  and y  X  X  . Formally, The TD measures the difference between the differential value estimate following the receipt of reward r ( x ) and a move to a new state y , and the estimate of the current differential state value at state x . 2.2 Algorithmic details and single time scale convergence which we assume to be bounded.
 Assumption 2.2. For all x  X  X , u  X  U , and  X   X  R K , there exists a positive constant, B  X  , such that |  X  ( x, u |  X  ) | X  B  X  &lt;  X  .
 In order to improve the agent X  X  performance, we need to follow the gradient direction. The following theorem shows how the gradient of the average reward per stage can be calculated by the TD signal. Similar variants of the theorem were proved using the Q -value [23] or state value [15] instead of the TD-signal.
 Theorem 2.4. The gradient of the average reward per stage for  X   X  R K can be expressed by The theorem was proved using an advantage function argument in [6]. We provide a direct proof in section A of the supplementary material. The flexibility resulting from the function f ( x ) allows us to encode the TD signal using biologically realistic positive values only, without influencing the convergence proof. In this paper, for simplicity, we use f ( x ) = 0 .
 Based on Theorem 2.4, we suggest an TD-based AC algorithm. This algorithm is motivated by [15] where an actor only algorithm was proposed. In [15] the differential value function was re-estimated afresh for each regenerative cycle leading to a large estimation variance. Using the continuity of the actor X  X  policy function in  X  , the difference between the estimates between regenerative cycles is small. Thus, the critic has a good initial estimate at the beginning of each cycle, which is used here in order to reduce the variance. A related AC algorithm was proposed in [5, 6], where two time scales were assumed in order to use Borkar X  X  two time scales convergence theorem [7]. In our proposed algorithm, and associated convergence theorem, we do not assume different time scales for the actor and for the critic.
 occur only at these times (batch mode). We define a cycle of the algorithm by the time indices which  X  (  X  ) respectively.
 Algorithm 1 Temporal Difference Based Actor Critic Algorithm 1: Given 2: Initiate the critic X  X  variables: 3: Initiate the actor:  X  0 = 0 and choose f ( x ) (see (4)) 4: for each state x t m +1 visited do 5: Critic: For all x  X  X  , N m ( x ) , min { t m &lt; k &lt; t m +1 | x k = x } , ( min(  X  ) =  X  ) 7: Project each component of  X  h m +1 and  X  m +1 onto H (see Assumption B.1.). 8: end for In order to prove the convergence of Algorithm 1, we establish two basic results. The first shows that the algorithm converges to the set of ordinary differential equations (5), and the second establishes conditions under which the differential equations converge locally. Theorem 2.5. Under Assumptions 2.1 and B.1, Algorithm 1 converges to the following set of ODE X  X  with probability 1 , where and where T (  X  ) , C (  X  ) , and D ( x ) (  X  ) are continuous with respect to  X  .
 Theorem 2.5 is proved in section B of the supplementary material, based on the theory of stochas-technique is that it does not need to assume two time scales.
 The second theorem, proved in section C of the supplementary material, states the conditions for which  X  (  X  t ) converges to a ball around the local optimum.
 Theorem 2.6. If we choose  X   X   X  B 2  X   X  / X   X  and  X  h  X  B 2  X  defined in Section C of the supplementary material. In this section we apply the previously developed algorithm to the case of neural networks. We start with the classic binary valued McCulloch-Pitts neuron, and then consider a more realistic spiking neuron model. While the algorithm presented in Section 2 was derived and proved to converge in batch mode, we apply it here in an online fashion. The derivation of an online learning algorithm from the batch version is immediate (e.g., [15]), and a proof of convergence in this setting is currently underway.
 A McCulloch-Pitts actor network assumed to be based on stochastic discrete time parallel updates, given by Here  X  ( v ) = 1 / (1 + exp(  X  v )) , and the parameters  X  in Algorithm 1 are given by { w ij } , where w ij ( n ) is the j 7 X  i synaptic weight at time n . Each neuron X  X  stochastic output u i is viewed as an action.
 Applying the actor update from Algorithm 1 we obtain the following online learning rule where d ( x ( n ) , x ( n + 1)) is the TD signal.
 The update (6) can be interpreted as an error-driven Hebbian-like learning rule modulated by the TD signal. It resembles the direct policy update rule presented in [2], except that in this rule the reward signal is replaced by the TD signal (computed by the critic). Moreover, the eligibility trace formalism in [2] differs from our formulation.
 We describe a simulation experiment conducted using a one layered feed-forward artificial neu-ral network which functions as an actor, combined with a non biologically motivated critic. The purpose of the experiment is to examine a simple neuronal model, using different actor and critic architectures. The actor network consists of a single layered feed-forward network of McCulloch-Pitts neurons, and TD modulated synapses as described above, where the TD signal is calculated by a critic. The environment is a maze with barriers consisting of 36 states, see Figure 1(b), where a reward of value 1 is provided at the top right corner, and is zero elsewhere. Every time the agent receives a reward, it is transferred randomly to a different location in the maze. At each time step, the agent is given an input vector which represents the state. The output layer consists of 4 output neurons where each neuron represents an action from the action set U = { up , down , left , right } . We used two different input representations for the actor, consisting either of 12 or 36 neurons (note that the minimum number of input neurons to represent 36 states is 6 , and the maximum number is 36). The architecture with 36 input neurons represents each maze state with one exclusive neuron, thus, there is no overlap between input vectors. The architecture with 12 input neurons uses a representa-tion where each state is represented by two neurons, leading to overlaps between the input vectors. We tested two types of critic: a table based critic which performs iterates according to Algorithm 1, and an exact TD which provides the TD of the optimal policy. The results are shown in Figure 1(c), averaged over 25 runs, and demonstrate the importance of good input representations and precise value estimates. A spiking neuron actor Actual neurons function in continuous time producing action potentials. In extension of [1, 9], we developed an update rule which is based on the Spike Response Model (SRM) [11]. For each neuron given by  X  t , t  X  t f j ) is the response induced by neuron j at neuron i . The second summation in (7) is over all spike times of neuron j emitted prior to time t . The neuron model is assumed to have a noisy threshold, which we model by an escape noise model [11]. According to this model, the neuron fires threshold and  X  i (  X  ) is a monotonically increasing function. When the neuron reaches the threshold it is assumed to fire and the membrane potential is reset to v r .
 We consider a network of continuous time neurons and synapses. Based on Algorithm 1, using a small time step  X t , we find We define the output of the neuron (interpreted as an action) at time t by u i ( t ) . We note that the neuron X  X  output is discrete and that at each time t , a neuron can fire, u i ( t ) = 1 , or be quiescent, u ( t ) = 0 . Using the definition of  X  from Section 2.2, yields (similar to [9]) Taking the limit  X t  X  0 , yields the following continuous time update rule Similarly to [1, 9] we interpret the update rule (9) as a TD modulated spike time dependent plasticity rule. A detailed discussion and interpretation of this update in a more biological context will be left to the full paper.
 We applied the update rule (9) to an actor network consisting of spiking neurons based on (7). The network X  X  goal was to reach a circle at the center of a 2D plain =, where the agent can move, using Newtonian dynamics, in the four principle directions. The actor is composed of an input layer and a single layer of modifiable weights. The input layer consists of  X  X ensory X  neurons which fire ac-cording to the agent X  X  location in the environment. The synaptic dynamics of the actor is determined by (9). The critic receives the same inputs as the actor, but uses a linear function approximation architecture rather than the table lookup used in Algorithm 1. A standard parameter update rule output layer of the actor consists of four neuronal groups, representing the directions in which the agent can move, coded based on a firing rate model using Gaussian tuning curves. The TD signal is calculated according to (3). Whenever it reaches the centered circle, it receives a reward, and is transferred randomly to a new position in the environment.
 Results of such a simulation are presented in Figure 3. Figure 3-a displays the agent X  X  typical random walk like behavior prior to learning, . Figure 3-b depicts four typical trajectories representing the agent X  X  actions after a learning phase. Finally, Figure 3-c demonstrates the increase of the average reward per stage,  X  , vs. time. We have presented a temporal difference based actor critic learning algorithm for reinforcement learning. The algorithm was derived from first principles based on following a noisy gradient of the average reward, and a convergence proof was presented without relying on the widely used two time scale separation for the actor and the critic. The derived algorithm was applied to neural networks, demonstrating their effective operation in maze problems. The motivation for the proposed algo-rithm was biological, providing a coherent computational explanation for several recently observed phenomena: actor critic architectures in the basal ganglia, the relation of phasic dopaminergic neu-romodulators to the TD signal, and the modulation of the spike time dependent plasticity rules by dopamine. While a great deal of further work needs to be done on both the theoretical and biologi-cal components of the framework, we hope that these results provide a tentative step in the (noisy!) direction of explaining biological RL.

