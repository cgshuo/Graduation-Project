 applicable to the majority of data, outlier detection targets the finding of the rare data whose behavior is very exceptional when compared with rest large amount of data. Studying the extraordinary behavior of outliers can uncover valuable knowledge hidden behind them and aid the decision makers to make profit or improve the service quality. Thus, mining for outliers is an important data mining research with numerous applications, including credit card fraud detection, discovery of criminal activities in electronic commerce, weather prediction, and marketing. 
A well-quoted definition of outliers is firstly given by Hawkins [1]. This definition states: an outlier is an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism. With increasing outliers are defined for solving problems in specific domains [3-22]. 
However, conventional approaches do not handle categorical data in a satisfactory manner, and most existing techniques lack for a solid theoretical foundation or applications. To fulfill this void, the problem of outlier detection in categorical data is defined as an optimization problem as follows [22]: finding a subset of k objects such minimized. 
In the above optimization problem, an ex haustive search through all possible solutions with k outliers for the one with the minimum objective value is costly since LSA algorithm is still very time-consuming on very large datasets. 
In this paper, we present a very fast greedy algorithm for mining outliers under the same optimization model. Experimental results on real datasets and large synthetic datasets show that: (1) Our algorithm has comparable performance with respect to Our algorithm can be an order of magnitude faster than LSA algorithm. The organization of this paper is as follows. First, we present related work in introduced in Section 4. The empirical studie s are provided in Section 5 and a section of concluding remarks follows. Statistical model-based methods, such as distribution-based methods [1,5] and depth-distributions of data are assumed known a priori in these methods. However, such assumption is not appropriate in real data mining applications. Distance based mining outliers in large databases. However, they primarily focused on databases regarded small clusters as outliers [12, 14] or identified outliers by removing clusters effectively from high dimensional datasets [3,4]. Support vector based methods detection. Outlier ensemble based methods are investigated recently in [24,25]. 
The preceding methods may be considered as traditional in the sense that they define an outlier without regard to class membership. However, in the context of supervised learning (where data have class labels attached to them) it makes sense to detection is considered in [19-21]. Entropy is the measure of information and uncertainty of a random variable [2]. If X probability function of X, the entropy E (X) is defined as shown in Equation (1). 
The entropy of a multivariable vector } ,..., { in Equation (2). 
The problem we are trying to solve can be formulated as follows [22]. Given a dataset D of n points ^ categorical attributes, i.e., ) ,..., ( 1 ^ m  X  . That is, 
In this problem, we need to compute the entropy of a set of records using Equation computation of entropy of a set of records. We assume the independences among the attributes, transforming Equation (2) into Equation (4). That is, the joint probability of and hence the entropy can be computed as the sum of entropies of the attributes. In this section, we present a greedy algorithm, denoted by greedyAlg1, which is effective and efficient on identifying outliers. 4.1 Overview Our greedyAlg1 algorithm takes the number of desired outliers (supposed to be k ) as Then, we need k scans over the dataset to select k points as outliers. In each scan, for each point labeled as non-outlier, it is temporally removed from the dataset as outlier and the entropy objective is re-evaluated. A point that achieves maximal entropy the size of OS reaches k . 4.2 Data Structure Given a dataset D of n points attribute values as referred values. Thus, in O (1) expected time, we can determine the frequency of an attribute value in corresponding hash table. 4.3 The Algorithm Fig.1 shows the greedyAlg1 algorithm. The coll ection of records is stored in a file on the disk and we read each record t in sequence. 04). outliers, i.e., one outlier is identified in each pass. In each scan over dataset, we read changed entropy value is computed. A record that achieves maximal entropy impact is selected as outlier in current scan and added to the set of outliers (Step 05-13). following Theorem, we show that the decreased entropy value is only dependent on the attribute values of the record to be temporally removed. Theorem 1: Suppose the number of records remained in D is n l , the record the decreased entropy value is: current iteration. Hence, Theorem results by considering all attributes. frequency of an attribute value in corresponding hash table. Hence, we can determine dependent on the attribute values of the record to be temporally removed. 4.4 Time and Space Complexities Worst-case analysis: The time and space complexities of the greedyAlg1 algorithm depend on the size of dataset ( n ), the number of attributes ( m ), the size of every hash table and the number of outliers ( k ). 
To simplify the analysis, we will assume that every attribute has the same number of distinct attributes values, p . Then, in the worst case, in the initialization phase, the time complexity is O ( nmp ). In the greedy procedure, since the computation of value change on entropy requires at most O ( mp ) and hence this phase has time complexity O ( nkmp ). Totally, the algorithm has time complexity O ( nkmp ) in worst case. 
The algorithm only needs to store m hash tables and the dataset in main memory, so the space complexity of our algorithm is O (( p + n ) m ). of implication of the compactness of categorical domains is that the parameter, p , can value in O (1) expected time, So, in practice, th e time complexity of greedyAlg1can be expected to be O ( nkm ). size of dataset, the number of attributes and the number of outliers, which make this algorithm scalable. Previous LSA algorithm pr esented in [22] has the time complexity in LSA) is usually larger than 10. A comprehensive performance study has been conducted to evaluate our greedyAlg1 algorithm. In this section, we describe those experiments and their results. We ran our algorithm on real-life datasets obtained from the UCI Machine Learning Repository algorithm. 5.1 Experiment Design and Evaluation Method Following the experimental setup in [22], we also used two real life datasets ( lymphography and cancer ) to demonstrate the effectiveness of our algorithm against FindFPOF algorithm [4], FindCBLOF algorithm [14], KNN algorithm [8] and LSA didn X  X  implement the RNN based outlier detection algorithm. 
For all the experiments, the two parameters needed by FindCBLOF algorithm are were obtained using the 5-nearest-neighbour ; For FindFPOF algorithm [4], the parameter mini-support for mining frequent patterns is fixed to 10%, and the maximal number of items in an itemset is set to 5. Since the LSA algorithm and greedyAlg1 are parameters. 
As pointed out by Aggarwal and Yu [3], one way to test how well the outlier detection algorithm worked is to run the method on the dataset and test the percentage These kinds of classes are also interesting from a practical perspective. 
Since we know the true class of each object in the test dataset, we define objects in assessment basis for comparing our algorithm with other algorithms. 5.2 Results on Lymphography Data The first dataset used is the Lymphography data set, which has 148 instances with 18 attributes. The data set contains a total of 4 classes. Classes 2 and 3 have the largest are small in size. The corresponding class distribution is illustrated in Table 1. Case Class codes Percentage of instances Commonly Occurring Classes 2, 3 95.9% Rare Classes 1, 4 4.1% dataset. The coverage is ratio of the number of detected rare classes to that of the rare classes in the dataset. For example, we let LSA algorithm find the top 7 outliers with the top ratio of 5%. By examining these 7 points, we found that 6 of them belonged to the rare classes. 
In this experiment, both the greedyAlg1 algorithm and LSA algorithm performed 10%, which is almost the twice for that of our algorithm. 
From the above results, we can see that greedyAlg1 algorithm achieves at least the same level performance as that of LSA algorithm on Lymphography data set. (Number of Records) 5.3 Results on Wisconsin Breast Cancer Data The second dataset used is the Wisconsin breast cancer data set, which has 699 34.5%). We follow the experimental technique of Harkins, et al. [17,18] by removing from [17,18]. 
Table 4 shows the results produced by the different algorithms. Clearly, among all of these algorithms, RNN performed the worst in most cases. In comparison to other algorithms, greedyAlg1 preformed very well in average. Hence, this experiment also demonstrates the effectiveness of greedyAlg1 algorithm. Case Class codes Percentage of instances Commonly Occurring Classes 1 92% Rare Classes 2 8% 
Although the performance of greedyAlg1 algorithm on identifying true outliers on performance are almost identical. And as we will show in next Section, our algorithm is very fast for larger dataset, which is more important in data mining applications. 5.4 Scalability Tests The purpose of this experiment was to test the scalability of the greedyAlg1 algorithm against LSA algorithm when handling very large datasets. A synthesized categorical dataset created with the software developed by Dana Cristofor (The source codes are public available at: http://www.cs.umb.edu/~dana/GAClust/index.html) is used. The data size (i.e., number of rows), the number of attributes and the number of classes are the major parameters in the synthesized categorical data generation, which were set to be 100,000, 10 and 10 separately. Moreover, we set the random generator seed to 5. We will refer to this synthesized dataset with name of DS1. 
We tested two types of scalability of the greedyAlg1 algorithm and LSA algorithm outliers for a given number of objects. Both algorithms were implemented in Java. All experiments were conducted on a Pentium4-2.4G machine with 512 M of RAM and running Windows 2000. Fig. 2 shows the results of using greedyAlg1 and LSA to find algorithms to find different number of outliers on DS1 dataset. 
One important observation from these figures was that the run time of greedyAlg1 algorithm tends to increase linearly as bot h the number of records and the number of greedyAlg1 algorithm is always faster than LSA algorithm and can be at least an order of magnitude faster than LSA in most cases. 
Hence, we are confident to claim that greedyAlg1 algorithm is suitable for mining very large dataset, which is very important in real data mining applications. synthetic datasets demonstrate the superiority of our new algorithm. This work was supported by the High Technology Research and Development Program of China (No. 2004AA413010, No. 2004AA413030) and the IBM SUR Research Fund. 
