 To reduce the information overload and help users select potential appealing products, recommender systems play an important role by recommending rel-evant products to users. Till now, promising progresses have been made from both academia and industry. Generally, recommendation needs to compare a large number of items to determine the users X  most preferred ones and this pro-cess will be very time consuming in the context of large user and item space. Efficiency in recommendation is accordingly becoming a challenging problem.
Several works have been proposed dea ling with the efficiency problem, which can be roughly classified into data structure based methods and hashing based methods [22]. As for the data structure based methods, researchers try to shrink the search space to reduce the comparison [20] [18] [7]. Typical examples are the kd-tree methods and some oth er partition methods [6] [ 8] [7]. Recently, hashing based methods show more superior than the data structure based methods [11]. For example, they can obtain low time complexity that is irrespective of the size of datasets and they need little memory space for the storage of the binary hash codes. Few researchers have tried using the hashing technology for fast recommendation [13] [25] [2 4]. Readers can refer to S ection 2 for more details.
To the best of our knowledge, exiting hashing based recommendation methods only learn binary codes for users and items, and they mainly focus on matrix data. For example, they learn hash codes for users and items so that their simi-larity in Hamming space can approach the rating matrix. In practice, recommen-dation can be rather complex to be represented by matrix data, for example, recommending items to users under specific queries in collaborative retrieval [17] [4] and recommending tags to images by specific users in personalized tag recommendation [10]. All th ese scenarios results in three or higher order tensor data with each dimension indicating one type of entities. Taking personalized tag recommendation as an example, the resulting three order tensor has values 1 or 0 indicating whether a tag is related to an image by a specific user or not.
In summary, the above scenarios involve fast search for one type of entities (target entities) using multiple types of entities (source entities) and usually ten-sor data is given representing the implicit or explicit relationship between the source entities and target entities. Th en conventional hashing based recommen-dation methods may fail for their disability dealing with tensor data. Besides, traditional hashing methods developed fo r multiple types of entities, i.e., cross-model hashing methods [9] [15], usually model the relationship between each two types of entities, which is different from our scenario and cannot be applied directly. Readers can refer to Section 2 for more details. Hence, in this paper, a novel hashing method for tensor data is proposed. We learn hash codes for each dimension of the tensor data. To preserve the relationships between source entities and target entities, their similarity calculation in the Hamming space is properly designed by bringing in operator matrices. Finally, extensive experi-ments on two complex recommendation tasks validate our proposed method. Our Contributions: 1) To the best of our knowledge, this is the first time to learn hash codes for tensor data and meanwhile it is successfully applied to complex recommendation problems. 2) We d esign operator matrices to explore the relationship between different types of source entities and auxiliary codes are accordingly constructed to enhanc e the recommendation performance. 3) We test our model in terms of personalized tag recommendation and collaborative retrieval, and extensive experimental results validate our proposed method. In the context of large user and item space, it becomes urgent to improve the rec-ommendation efficiency when compares user interests and item characteristics. As described before, the methods can be r oughly classified into data structure based methods [20] [18] [19] [7] and hashing based methods [25] [24]. As for the former ones, some researches utilize simple partitioning based methods, which partition the items or users into groups [6] [8]. These methods can reduce the item space or user space, but they may inevitable harm the recommendation performance. Recently, some special da ta structures, such as kd-tree and its variants are utilized for fast search [7] [5]. However, these methods are still time consuming when dealing with high dimensional and large scale datasets [3].
Recently, hashing technology has been brought to recommendation field and shows superior than the data structure based methods. Generally, hashing is one kind of the most popular methods for large scale nearest neighbor search problems, which learns binary codes as the new representations of entities [11]. Initiated by Locality Sensitive Hashing (LSH) [1], which is a family of hash functions mapping similar points to the same hash codes with high probability, some machine learning methods are now employed to design more effective hash codes [16] [2] [23]. As in recommendation field, Zhou and Zha [25] gave the first try and learned binary codes for users and items for fast item ranking. Zhang et al. [24] learned hash codes given the implict or explicit feedback matrix to preserve the users X  preferences over items. Furthermore, Wang et al. [13] [12] proposed to learn compact hashing codes for efficient tag completion and recommendation. Generally, previous hashing based recommendation methods mainly focus on the matrix data, e.g., user-item rating matrix and image-tag tag matrix. However, recommendation scenarios are more complex and usually tensor data is given, making conventional hashing methods failed. This is the motivation that promotes us to propose new hashing methods for tensor data.
Since we are learning to hash for multiple modalities, the research of cross-model hashing proposed to meet the need of similarity search across different types of entities is one of the most related works [9] [15]. However, exiting cross-model hashing methods mainly focus on modeling the pairwise relationship, i.e., the relationship between two types of en tities. For example, when people uses texts, images and videos for cross-model retrieval, the pairwise relationships between texts and images, between texts and videos and between images and videos are developed. Instead we are concentrating on the scenarios that use multiple types of entities to retrieve one type of entities and typical examples are personalized tag recommendation and collaborative retrieval. Generally, different scenarios lead to distrinct hashing methods to adjust their data structures, which makes previous cross-model hashing methods cannot be applied directly to solve the problem we are focusing on. Thus, we develop new hashing methods for multiple modalities and meanwhile enable for fast recommendation. 3.1 Notations We are given n order tensor data R with each dimension representing a type of entities and their values indicating the relationships between the n  X  1typesof entities and the other one type of entities. We call the n  X  1 types of entities source entities and the other one type of entities target entities. Our goal is to learn hash codes for each type of entities to preserve their relationship and meanwhile enable for fast similarity search. For example, for personalized tag recommendation, we have three types of entities: users and images as source entities and tags as target entities. Acco rdingly, a three order tensor is given indicating whether a tag is annotated to an image under a specific user or not. We need to learn hash codes for each user, each image and each tag so that tags can be rapidly recommended to images given specific users.

Suppose the n  X  1 source entities and the target entities are represented as X ( k ) ( k =1 , ..., n  X  1) and X ( n ) respectively and m k ( k =1 , ..., n ) denotes the number of entities of the k -th type. Then we need to learn hash codes H ( k ) ( k =1 , ..., n ) for all of them. Apart from the binary constraint on H ( k ) ,the key issue is to keep the similarity between the source entities and the target entities in Hamming space so that their relationship is consistent with the n ordertensordata R . In the following, we will elaborate our strategy achieving this goal. 3.2 Similarity Preserving Since the tensor data R indicates the implict or explicit relationship between all the source entities and the target entities, a natural way to calculate similarity between them is to compute the similari ty between each type of source entities and the target entities and then sum them. By doing so, the relationship between the source entities and the target entities can be explored in a straight-forward manner. Then for the s 1 , ...s n -th value of the tensor data R , its predicted value R code for the k -th entity in the k -th type. sim is an operator calculating the similarity between two hash codes to be introduced later.

In the above formulation, we directly co nsider the relationship between each type of source entities and the target enti ties. However, the interaction between different types of source entities are ignored, which may be essential to explore the relationship between the source entities and the target entities. For example, for collaborative retrieval, users behave quite different under different queries and clearly queries can influence users. To model the influence, we propose operator matrix for each source entity. For example, T ( j ) s j , served as the operator matrix of the s j -th entity in the j -th type, represents the influence this entity will bring to other types of source entities . Furthermore, given an entity R s 1 ,...s n ,wemodel the influence the other source entities to the s k -th source entity of the k -th type in the k -th type and binary it, we can obtain a new hash codes as auxiliary codes where sign is an element-wise symbolic operat ion for a vector with each element returning 1 or  X  1 based on whether its value is bigger than 0 or not.
Taking the interaction between different types of source entities into consid-eration, we modify Equation 1 as: where  X  is a positive value balancing the effect of the auxiliary similarity term.
In Equation 3, a natural way to define the similarity between two hash codes is the fraction of common bits between them, which is widely used in many hashing similarity is defined as: where B is the number of bits of the hash codes and H ( k ) s k H k . I is an indicator function returning 1 if the two operator numbers are the same otherwise 0.

After the calculation of similarity between the source entities and the target entities, many kinds of loss functions can be applied to construct the objective. Here we just use the simple square loss and it is written as where O is the observed entities of the tensor data R .  X  is a scaler parameter to limit the predicted similarity to be in the interval [0,1]. The last term is a regular regularization on the operator matrices with a positive tradeoff parameter  X  .
As for the constraints imposed on the hash codes, apart from the constraint that the elements of all the hash codes are { -1, +1 } , it is necessary to re-strict the hash codes to be balanced, w hich makes sure that each bit carries as much information as possible. In summary, the constraints are defined as H where the constraint H ( k ) 1 = 0 is used to preserve the balance of each bit. 3.3 Final Objective and Optimization Bringing Equation 3, 4 and 6 into Equation 5, we can obtain the final cost function. Because of the binary constraint on the variables, the cost function in Equation 5 is not differentiable. Moreover, the balance constraint and the sign operator make the optimization of the objective a non-trivial problem. To solve these problems, we firstly relax the sign operator to real values as most hashing leaning methods have done [21] [14]. Furthermore, the two constraints are converted into soft penalty terms as in [9]. Specifically, we add the following two terms to the cost function. And the final cost is written as: Since the final cost is in Equation 8 is not jointly convex with respect to all the variables, we use the stochastic gra dient descent method to obtain a local optimal solution and the gradients are calculated as where the gradient components in the above equation are given as
After solving the relaxed optimization problem as in Equation 8, we can obtain real-valued representations for all the entities denoted as Finally, using the constraints of Equation 6, we can obtain the final hashing codes as: where H ( k ) ij is the j -th bit of the i -thentityintypeof k . The whole algorithm is summarized in Algorithm 1.
 Algorithm 1. Learning to Hash for Recommendation with Tensor Data 4.1 Datasets We report experimental results on two widely used recommendation datasets and the statistics of the databases are summarized in Table 1.
 Last fm: It contains music artist listening information and tagging information sampled from Last.fm online music system. Each user has tagged some music artists and data tuples [user, artist, tag] are given. We use this dataset to test the performance of our method in terms of personalized tag recommendation. Similar to [10], we use a p-core 1 for Last fm to filter the dataset and p is chosen as 20 here.
 MovieLens: It is published by GroupLens research group and contains person-alized ratings to movies. Besides, the user tagging information is also provided and accordingly data tuples [tag, user, movie] can be obtained. We use this dataset for the experiments of collaborative retrieval. Similar to [4], the most common 50 tags are selected as the genre of the movies and [genre, user, movie] triple are utilized to mimic [query, user, item] triples for collaborative retrieval. Besides, the data preprocessing is the same as in [4]. 4.2 Experimental Settings To evaluate the performance of the proposed model, we compare our method with the following state-of-the-art hashing based recommendation algorithms. CFCodeReg: Zhou and Zha [25] proposed to learn binary codes for collabora-tive filtering. BCE-FIT: Wang et al. [13] learned binary codes embedding for tag recommendation. It should be noted that the above two methods learn hash codes for matrix data, i.e., rating matrix and tag matrix respectively. However, for personalized tag r ecommendation and collaborative retrieval problems, three order tensor data is provided. So we may compress the tensor data into matrix form for a comparison. Like in traditional recommendation, the factors of user and query are ignored in personalized tag recommendation and collaborative retrieval respectively. LCR-B: Weston et al. [17] proposed the first latent col-laborative retrieval algorithm (LSR). However, the learned latent representations are real values, which makes the comparison with our method unfair. Usually, the learned latent representations are in the interval [-1,+1], so we binary the real values with the constraints in Equation 6 that we have used. The learned bi-nary codes is then compared with our method. LHTD: Our proposed L earning to H ash for recommendation with T ensor D ata with the similarity calculated in Equation 1. LHTDi: Our proposed L earning to H ash for recommendation with T ensor D ata with the similarity calculated considering the i nteraction between different types of source entities as in Equation 3.

In all the experiments, we use Recall as in [4] as the evaluation metric. For a i and sort the target entities in desce nding order. Then, we measure Recall@ k , which is 1 if the target entity in the top k and 0 otherwise. Finally, the mean Recall@ k over the whole test dataset is reported. In the parameter setting, we empirically set  X  =1and  X  1 =  X  2 =0 . 001 in all the datasets. As for  X  ,it controls the importance of the interaction term and is searched in the interval [0, 1]. For all the compared methods, we follow the suggests their authors have given to achieve their best performan ce. All the experiments are run 10 times with randomly choosing 80% of the observed entities as the training set and the remaining as the testing set. 4.3 Numerical Results and Analysis From Figure 1, it can be seen that our method outperforms all the compared methods in both databases in terms of personalized tag recommendation and collaborative retrieval. Compared with LHTD, LHTDi considers the interaction between different types of source entities and the resulting auxiliary codes can well explore the relationship between the source entities, thus its performance is better than that of LHTD.

Compared with our method, CFCodeReg and BCE-FIT cannot consider hash-ing for all types of entities and this may in evitable harm the relationship between them. So the hash codes learned by thei r methods can not properly approach the tensor data and their performance is relatively poor. As for LCR-B, it learns latent representations for all types of entities and considers one-side interaction between different types of source entities. Compared with it, our method takes all pairwise interactions between different types of source entities into consider-ation and can better reflect their relatio nship. Besides, the hash codes learning of LCR-B is a two-stage process, and more information will be lost compared with our method that considers learning the hash codes in one objective. And these may be the reasons that our metho d performs better than that of LCR-B. 4.4 Parameter Selection In our model,  X  is a regularization parameter used to avoid over-fitting and  X  1 and  X  2 are parameters controlling the soft penalty terms. All these variables are empirically set as in Section 4 . 2. There is also an important parameter  X  that balances the effect of the auxiliary similarity term. And in this section, we test how parameter u influences the performance of our method. We choose the number of hash codes to be 8 and 32 and vary u in the interval [0, 1] on both datasets. The results are shown in Figure 2. When the number of hash codes is small, the performance of LHTDi is b ecoming better with the increasing of u . However, once the number of hash codes is big enough, we can see that LHTDi reaches the best performance when u is relatively small. This is reasonable be-cause the basic similarity between source entities and target entities can not well approach the tensor data using very few bits and the interaction term is accordingly becoming important. When the number of hash codes is relatively big, the basic hash codes can well embed the information of different entities and the effect of auxiliary codes is accordingly inapparent. 4.5 Operation Ability of Different Types of Operator Matrices In our method, we propose operator mat rices to model the interaction between different types of source entities, resulting in auxiliary codes to further improve the recommendation performance. In this section, we quantize the effect of the operator matrices and observe their rel ationship. For an entity of one type, we firstly calculate the change between its operator matrix multiplying the hash codes of the other types of entities and the hash codes of other types of entities themselves. Then we average the changes obtained by all this type of entities as the final operating effect of this type of entities. The comparison between different types of source entities are listed in Table 2.

From Table 2, it can be seen that operator matrices of different types of source entities are not equally important. 1) In personalized tag recommendation, user operator matrices are more important than that of artist. Compared with con-ventional tagging system, personalized tag recommendation adds the factor of user, which influences the tagging for the same artist. This may be the reason that user operator matrices are the main factors for the interaction. 2) As in collaborative retrieval, we observe that query operator matrices are more impor-tant than that of user. Since collaborative retrieval has the entities of queries compared with traditional recommendation, it shares the similar reason as in personalized tag recommendation for the comparison. In this paper, we have proposed a novel hashing method for tensor data, which retrieve one type of entities (target entities) using the query of many other types of entities (source entities) and has b een successfully applied for complex rec-ommendation problems, i.e., personalized tag recommendation and collaborative retrieval. In our model, we learn hash codes for each dimension of entities and properly design the similarity between them to approach the tensor data. Fur-thermore, to explore the relationship bet ween different types of source entities, operator matrices are developed, resulting in auxiliary codes for all the source entities to further enhance the recommendation performance. Extensive experi-ments have demonstrated the effectivenes s of our method compared with several state-of-the-art hashing algorithms. Since different source entities of the same type may share common chara cteristics, it may be necessary to build an oper-ator tensor as a dictionary instead of a list of operator matrices to reduce the computation. We leave this as future work.
 Acknowledgments. This work is jointly supported by National Basic Research Program of China (2012CB316300), and Nati onal Natural Science Foundation of China (61175003, 61420106015, U1435221, 61403390).

