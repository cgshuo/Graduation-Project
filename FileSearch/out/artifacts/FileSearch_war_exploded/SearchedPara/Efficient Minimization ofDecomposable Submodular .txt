 Convex optimization has become a key tool in many machine learning algorithms. Many seemingly multimodal optimization problems such as nonlinear classification, clustering and dimensionality reduction can be cast as convex programs. When minimizing a convex loss function, we can rest assured to efficiently find an optimal solution, even for large problems. Convex optimization is a structural property of continuous optimization problems. However, many machine learning prob-lems, such as structure learning, variable selection, MAP inference in discrete graphical models, require solving discrete, combinatorial optimization problems.
 has emerged as very useful in many combinatorial optimization problems arising in machine learn-ing: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-)optimal solutions. In particular, the minimum of a submodular function can be found in strongly polynomial time [11]. Unfortunately, while polynomial-time solv-able, exact techniques for submodular minimization require a number of function evaluations on the order of n 5 [12], where n is the number of variables in the problem (e.g., number of random variables in the MAP inference task), rendering the algorithms impractical for many real-world problems. Fortunately, several submodular minimization problems arising in machine learning have structure solved in O ( n 3 ) evaluations using Queyranne X  X  algorithm [19], and functions that decompose into attractive, pairwise potentials, that can be solved using graph cutting techniques [7]. In this paper, we introduce a novel class of submodular minimization problems that can be solved efficiently. In particular, we develop an algorithm SLG, that can minimize a class of submodular functions that we call decomposable : These are functions that can be decomposed into sums of concave functions applied to modular (additive) functions. Our algorithm is based on recent techniques of smoothed our algorithm on a joint classification-and-segmentation task involving tens of thousands of variables, and show that it outperforms state-of-the-art algorithms for general submodular function minimization by several orders of magnitude. We are interested in minimizing set functions that map subsets of some base set E to real numbers. I.e., given f : 2 E ! R we wish to solve for A 2 arg min A f ( A ) . For simplicity of notation, we use the base set E = f 1 ;:::n g , but in an application the base set may consist of nodes of a graph, pixels of an image, etc. Without loss of generality, we assume f ( ; ) = 0 . If the function f has no structure, then there is no way solve the problem other than checking all 2 n subsets. In this paper, we consider functions that satisfy a key property that arises in many applications: submodularity (c.f., [16]). A set function f is called submodular iff, for all A;B 2 2 E , we have Submodular functions can alternatively, and perhaps more intuitively, be characterized in terms of of f with respect to k 2 E at A ; intuitively this is the change in f  X  X  value by adding the element k to the set A . Then, f is submodular iff: Note the analogy to concave functions; the discrete derivative is smaller for larger sets, in the same way that ( x + h ) ( x ) ( y + h ) ( y ) for all x y;h 0 if and only if is a concave function on R . Thus a simple example of a submodular function is f ( A ) = ( j A j ) where is any concave function. Yet despite this connection to concavity, it is in fact  X  X asier X  to minimize a submodular this is that submodular minimization can be reformulated as a convex minimization problem. To see this, consider taking a set function minimization problem, and reformulating it as a mini-mization problem over the unit cube [0 ; 1] n R n . Define e A 2 R n to be the indicator vector of the set A , i.e., We use the notation x [ k ] for the k th element of the vector x . Also we drop brackets and commas extension of a set function f is a function ~ f on the unit cube ~ f : [0 ; 1] n ! R with the property related to minima of the extension: only satisfies the above property, but is also convex and efficient to evaluate. We can define the Lov  X  asz extension in terms of the submodular polyhedron P f : The submodular polyhedron P f is defined by exponentially many inequalities, and evaluating ~ f can be very efficiently computed as follows. For a fixed x let : E ! E be a permutation such that x [ (1)] ::: x [ ( n )] , and then define the set S k = f (1) ;:::; ( k ) g . Then we have a formula for ~ f and a subgradient: Note that if two components of x are equal, the above formula for ~ f is independent of the permuta-tion chosen, but the subgradient is not unique. Equation (2) was used to show that submodular minimization can be achieved in polynomial time [16]. However, algorithms which directly minimize the Lovasz extension are regarded as imprac-tical. Despite being convex, the Lov  X  asz extension is non-smooth, and hence a simple subgradient descent algorithm would need O (1 = 2 ) steps to achieve O ( ) accuracy.
 Recently, Nesterov showed that if knowledge about the structure of a particular non-smooth convex function is available, it can be exploited to achieve a running time of O (1 = ) [18]. One way this is done is to construct a smooth approximation of the non-smooth function, and then use an accelerated gradient descent algorithm which is highly effective for smooth functions. Connections of this work [2], Bach shows that computing the smoothed Lov  X  asz gradient of a general submodular function is equivalent to solving a submodular minimization problem. In this paper, we do not treat general submodular functions, but rather a large class of submodular minimization functions that we call decomposable . (To apply the smoothing technique of [18], special structural knowledge about the convex function is required, so it is natural that we would need special structural knowledge about the submodular function to leverage those results.) We further show that we can exploit the discrete structure of submodular minimization in a way that allows terminating the algorithm early with a certificate of optimality, which leads to drastic performance improvements. In this paper, we consider the problem of minimizing functions of the following form: where c ; w j 2 R n and 0 w j 1 and j : [0 ; w j 1 ] ! R are arbitrary concave functions. It can be shown that functions of this form are submodular. We call this class of functions decomposable submodular functions , as they decompose into a sum of concave functions applied to nonnegative modular functions 2 . Below, we give examples of decomposable submodular functions arising in applications.
 d submodular functions w ;y ( A ) = min( y; w e A ) and call them threshold potentials . In Section 5, we will show in how to generalize our approach to arbitrary decomposable submodular functions. Examples. The simplest example is a 2-potential , which has the form ( j A \f k;l gj ) , where (1) (0) (1) (2) . It can be expressed as a sum of a modular function and a threshold potential: Why are such potential functions interesting? They arise, for example, when finding the Maximum a Posteriori configuration of a pairwise Markov Random Field model in image classification schemes such as in [20]. On a high level, such an algorithm computes a value c [ k ] that corresponds to the log-likelihood of pixel k being of one class vs. another, and for each pair of adjacent pixels, classifies pixels by minimizing a sum of 2-potentials: f ( A ) = c e A + If the value d kl is large, this encourages the pixels k and l to be classified similarly. More generally, consider a higher order potential function: a concave function of the number of elements in some activation set S , ( j A \ S j ) where is concave. It can be shown that this can be written as a sum of a modular function and a positive linear combination of j S j 1 threshold potentials. Recent work [14] has shown that classification performance can be improved by adding functions j are piecewise linear concave functions, and the regions R j of various sizes generated from a segmentation algorithm. Minimization of these particular potential functions can then be reformulated as a graph cut problem [13], but this is less general than our approach. Another canonical example of a submodular function is a set cover function. Such a function can be reformulated as a combination of concave cardinality functions (details omitted here). So all functions which are weighted combinations of set cover functions can be expressed as threshold potentials. However, threshold potentials with nonuniform weights are strictly more general than concave cardinality potentials. That is, there exists w and y such that w ;y ( A ) cannot be expressed as
P Another example of decomposable functions arises in multiclass queuing systems [10]. These are of the form f ( A ) = c e A + u e A ( v e A ) , where u ; v are nonnegative weight vectors and is a nonincreasing concave function. With the proper choice of j and w j (again details are omitted here), this can in fact be reformulated as sum of the type in Eq. 3 with n terms.
 In our own experiments, shown in Section 6, we use an implementation of TextonBoost [20] and augment it with quadratic higher order potentials. That is, we use TextonBoost to generate per-pixel scores c , and then minimize f ( A ) = c e A + of pixels that we expect to be of the same class (e.g., by running a cheap region-growing heuristic). largest penalty when exactly half of R j is contained in A . This encourages the classification scheme to classify most of the pixels in a region R j the same way. We generate regions with a basic region-growing algorithm with random seeds. See Figure 1(a) for an illustration of examples of regions that we use. In our experience, this simple idea of using higher-order potentials can dramatically We now present our algorithm for efficient minimization of a decomposable submodular function f based on smoothed convex minimization. We first show how we can efficiently smooth the Lov  X  asz extension of f . We then apply accelerated gradient descent to the gradient of the smoothed function. Lastly, we demonstrate how we can often obtain a certificate of optimality that allows us to stop early, drastically speeding up the algorithm in practice. 4.1 The Smoothed Extension of a Threshold Potential The key challenge in our algorithm is to efficiently smooth the Lov  X  asz extension of f , so that we can resort to algorithms for accelerated convex minimization. We now show how we can efficiently smooth the threshold potentials w ;y ( A ) = min( y; w e A ) of Section 3, which are simple enough to allow efficient smoothing, but rich enough when combined to express a large class of submodular functions. For x 0 , the Lov  X  asz extension of w ;y is Note that when x 0 , the arg max of the above linear program always contains a point v which satisfies v 1 = y , and v 0 . So we can restrict the domain of the dual variable v to those points which satisfy these two conditions, without changing the value of ~ ( x ) : Restricting the domain of v allows us to define a smoothed Lov  X  asz extension (with parameter ) that is easily computed: ~ To compute the value of this function we need to solve for the optimal vector v , which is also the gradient of this function, as we have the following characterization: To derive an expression for v , we begin by forming the Lagrangian and deriving the dual problem: If we fix t , we can solve for the optimal dual variables 1 and 2 componentwise. By strong duality, we know the optimal primal variable is given by v = 1 ( x t 1 + 1 2 ) . So we have: 1 = max( t 1 x ; 0 ) ; 2 = max( x t 1 w ; 0 ) ) v = min (max (( x t 1 ) =; 0 ) ; w ) : This expresses v as a function of the unknown optimal dual variable t . For the simple case of 2-potentials, we can solve for t explicitly and get a closed form expression: However, in general to find t we note that v must satisfy v 1 = y . So define x ; w ( t ) as: Then we note this function is a monotonic continuous piecewise linear function of t , so we can use a simple root-finding algorithm to solve x ; w ( t ) = y . This root finding procedure will take no more than O ( n ) steps in the worst case. 4.2 The SLG Algorithm for Minimizing Sums of Threshold Potentials Stepping beyond a single threshold potential, we now assume that the submodular function to be minimized can be written as a nonnegative linear combination of threshold potentials and a modular function, i.e., f ( A ) = c e Thus, we have the smoothed Lov  X  asz extension, and its gradient: We now wish to use the accelerated gradient descent algorithm of [18] to minimize this function. This algorithm requires that the smoothed objective has a Lipschitz continuous gradient. That is, for schitz gradient, a direct consequence of the characterization in Equation 4. Hence we have a loose upper bound for the Lipschitz constant of ~ f : L D , where D = thermore, the smoothed threshold extensions approximate the threshold extensions uniformly: j ~ One way to use the smoothed gradient is to specify an accuracy " , then minimize ~ f for sufficiently small to guarantee that the solution will also be an approximate minimizer of ~ f . Then we simply min(max( x ; 0 ) ; 1 ) . Algorithm 1 formalizes our Smoothed Lov  X  asz Gradient ( SLG ) algorithm: Algorithm 1: SLG: Smoothed Lov  X  asz Gradient Input : Accuracy " ; decomposable function f . begin The optimality gap of a smooth convex function at the iterate y t can be computed from its gradient: In summary, as a consequence of the results of [18], we have the following guarantee about SLG: Theorem 1 SLG is guaranteed to provide an " -optimal solution after running for O ( D " ) iterations. SLG is only guaranteed to provide an " -optimal solution to the continuous optimization problem. set which is " -optimal for the original submodular function using Alg. 2 (see [9] for more details). Algorithm 2: Set generation by rounding the continuous solution Input : Vector x 2 [0 ; 1] n ; submodular function f . begin Output : Collection of sets C , such that f ( A ) ~ f ( x ) for all A 2 C 4.3 Early Stopping based on Discrete Certificates of Optimality In general, if the minimum of f is not unique, the output of SLG may be in the interior of the unit cube. However, if f admits a unique minimum A , then the iterates will tend toward the corner e
A . One natural question one may ask, if a trend like this is observed, is it necessary to wait for the it is possible to use information about the current iterates to check optimality of a set and terminate the algorithm before the continuous problem has converged.
 we can compute an optimality gap: In particular if g [ k ] 0 for k 2 A and g [ k ] 0 for k 2 E n A , then A is optimal. But if we only have knowledge of candidate set A , then finding a subgradient g 2 @ ~ f ( e A ) which demonstrates optimality may be extremely difficult, as the set of subgradients is a polyhedron with exponentially many extreme points. But our algorithm naturally suggests the subgradient we could use; the gradi-ent of the smoothed extension is one such subgradient  X  provided a certain condition is satisfied, as described in the following Lemma.
 Lemma 1 Suppose f is a decomposable submodular function, with Lov  X  asz extension ~ f , and smoothed extension ~ f as in the previous section. Suppose x 2 R n and A 2 2 E satisfy the fol-lowing property: min Then r ~ f ( x ) 2 @ ~ f ( e A ) This is a consequence of our formula for r ~ , but see the appendix of the extended paper [21] for a detailed proof. Lemma 1 states that if the components of point x corresponding to elements of A ~ f at e A (which by Equation 5 allows us to compute an optimality gap). In practice, this separation of components naturally occurs as the iterates move in the direction of the point e A , long before they ever actually reach the point e A . But even if the components are not separated, we can easily optimality gap. In summary, we have the following algorithm to check the optimality of a candidate Algorithm 3: Set Optimality Check Input : Set A ; decomposable function f ; scale ; x 2 R n . begin Output : gap , which satisfies gap f ( A ) f optimal, we want the components of the gradient r ~ f ( A + e A )[ k ] to be negative for k 2 A and does not change the signs of the components of the gradient, then in fact we have found the optimal set. This stopping criterion is very effective in practice, and we use it in all of our experiments. Figure 1: (a) Example regions used for our higher-order potential functions (b-c) Comparision of running times of submodular minimization algorithms on synthetic problems from DIMACS [1]. To extend our algorithm to work on general concave functions, we note that an arbitrary concave function can be expressed as an integral of threshold potential functions. This is a simple conse-quence of integration by parts, which we state in the following lemma: Lemma 2 For 2 C 2 ([0 ;T ]) , This means that for a general sum of concave potentials as in Equation (3), we have: Then we can define ~ f and ~ f by replacing with ~ and ~ respectively. Our SLG algorithm is essentially unchanged, the conditions for optimality still hold, and so on. Conceptually, we just use a different smoothed gradient, but calculating it is more involved. We need to compute the integrals of the form which we can compute, we can evaluate the integral by parts so that we need only evaluate , but not its derivatives. We omit the resulting formulas for space limitations. Synthetic Data. We reproduce the experimental setup of [8] designed to compare submodular minimization algorithms. Our goal is to find the minimum cut of a randomly generated graph (which requires submodular minimization of a sum of 2-potentials) with the graph generated by the speci-fications in [1]. We compare against the state of the art combinatorial algorithms (LEX2, HYBRID, SFM3, PR [6]) that are guaranteed to find the exact solution in polynomial time, as well as the Minimum Norm algorithm of [8], a practical alternative with unknown running time. Figures 1(b) and 1(c) compare the running time of SLG against the running times reported in [8]. In some cases, SLG was 6 times faster than the MinNorm algorithm. However the comparison to the MinNorm algorithm is inconclusive in this experiment, since while we used a faster machine, we also used a simple MATLAB implementation. What is clear is that SLG scales at least as well as MinNorm on these problems, and is practical for problem sizes that the combinatorial algorithms cannot handle. Image Segmentation Experiments. We also tested our algorithm on the joint image segmentation-and-classification task introduced in Section 3. We used an implementation of TextonBoost [20], then trained on and tested subsampled images from [5]. As seen in Figures 2(e) and 2(g), using only the per-pixel score from our TextonBoost implementation gets the general area of the object, but does not do a good job of identifying the shape of a classified object. Compare to the ground truth in Figures 2(b) and 2(d). We then perform MAP inference in a Markov Random Field with 2-potentials (as done in [20]). While this regularization, as shown in Figures 2(f) and 2(h), leads to improved performance, it still performs poorly on classifying the boundary. Finally, we used SLG to regularize with higher order potentials. To generate regions for our poten-tials, we randomly picked seed pixels and grew the regions based on HSV channels of the image. We picked our seed pixels with a preference for pixels which were included in the least number of previously generated regions. Figure 1(a) shows what the regions typically looked like. For our ex-periments, we used 90 total regions. We used SLG to minimize f ( A ) = c e A + where c was the output from TextonBoost, scaled appropriately. Figures 2(i) and 2(k) show the clas-sification output. The continuous variables x at the end of each run are shown in Figures 2(j) and 2(l); while it has no formal meaning, in general one can interpret a very high or low value of x [ k ] to correspond to high confidence in the classification of the pixel k . To generate the result shown in Figure 2(k), a problem with 10 4 variables and 90 concave potentials, our MATLAB/mex im-plementation of SLG took 71.4 seconds. In comparison, the MinNorm implementation of the SFO toolbox [15] gave the same result, but took 6900 seconds. Similar problems on an image of twice the resolution ( 4 10 4 variables) were tested using SLG, resulting in runtimes of roughly 1600 seconds. We have developed a novel method for efficiently minimizing a large class of submodular functions of practical importance. We do so by decomposing the function into a sum of threshold potentials, whose Lov  X  asz extensions are convenient for using modern smoothing techniques of convex opti-mization. This allows us to solve submodular minimization problems with thousands of variables, that cannot be expressed using only pairwise potentials. Thus we have achieved a middle ground between graph-cut-based algorithms which are extremely fast but only able to handle very specific types of submodular minimization problems, and combinatorial algorithms which assume nothing but submodularity but are impractical for large-scale problems.
 Acknowledgements This research was partially supported by NSF grant IIS-0953413, a gift from Microsoft Corporation and an Okawa Foundation Research Grant. Thanks to Alex Gittens and Michael McCoy for use of their TextonBoost implementation. [1] Dimacs, The First international algorithm implementation challenge: The core experiments , [2] F. Bach, Structured sparsity-inducing norms through submodular functions , Advances in Neu-[3] S. Becker, J. Bobin, and E.J. Candes, Nesta: A fast and accurate first-order method for sparse [4] F.A. Chudak and K. Nagano, Efficient solutions to relaxations of combinatorial problems with [5] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, The [6] L. Fleischer and S. Iwata, A push-relabel framework for submodular function minimization [8] Satoru Fujishige, Takumi Hayashi, and Shigueo Isotani, The Minimum-Norm-Point Algorithm [9] E. Hazan and S. Kale, Beyond convexity: Online submodular minimization , Advances in [10] T. Itoko and S. Iwata, Computational geometric approach to submodular function minimiza-[11] S. Iwata, L. Fleischer, and S. Fujishige, A combinatorial strongly polynomial algorithm for [12] S. Iwata and J.B. Orlin, A simple combinatorial algorithm for submodular function minimiza-[13] P. Kohli, M.P. Kumar, and P.H.S. Torr, P3 &amp; Beyond: Solving Energies with Higher Order [14] P. Kohli, L. Ladick  X  y, and P.H.S. Torr, Robust Higher Order Potentials for Enforcing Label [15] A. Krause, SFO: A Toolbox for Submodular Function Optimization , The Journal of Machine [16] L. Lov  X  asz, Submodular functions and convexity , Mathematical programming: the state of the [17] G. Nemhauser, L. Wolsey, and M. Fisher, An analysis of the approximations for maximizing [18] Yu. Nesterov, Smooth minimization of non-smooth functions , Mathematical Programming 103 [19] M. Queyranne, Minimizing symmetric submodular functions , Mathematical Programming 82 [20] J. Shotton, J. Winn, C. Rother, and A. Criminisi, TextonBoost for Image Understanding: Multi-[21] P. Stobbe and A. Krause, Efficient minimization of decomposable submodular functions ,
