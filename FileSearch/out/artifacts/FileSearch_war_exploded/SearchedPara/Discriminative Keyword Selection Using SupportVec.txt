
A common problem in speech processing is to identify propert ies of a speech segment such as scoring detector on a speech segment would be the predicted l anguage.
 short-term spectral characteristics of the speech and mode ls these with Gaussian mixture models latter approach for our paper.
 SVMs have become a common method of extracting high-level pr operties of sequences of speech system dictionary may be a strong indicator of topic.
 n higher order n -grams.
 of SVMs to determining properties. Section 3.1 describes a f eature selection method for SVMs. to language and topic recognition problems. We show qualita tively that the method produces in-teresting keywords. Quantitatively, we show that the metho d produces keywords which are good discriminators between classes. 2.1 Phone Recognition University (BUT) design [7]. The basic architecture of this system is a monophone HMM system with a null grammar. Monophones are modeled by three states. This system uses two powerful We developed a phone recognizer for English units using the B UT architecture and automatically imately 10 hours of speech. ANN training was accomplished using the ICS I Quicknet package [9]. The resulting system has 49 monophones including silence.
 The BUT recognizer is used along with the HTK HMM toolkit [10] to produce lattices. Lattices tokens.
 we have a hypothesized string of tokens, W = w ing two tokens at a time to form, W are formed from longer juxtapositions of tokens. The count f unction for a given bigram, d L , we find the expected count over all all possible hypotheses i n the lattice, The expected counts can be computed efficiently by a forward-backward algorithm; more details can be found in Section 3.3 and [11]. lattice, L , the joint probability of an n -gram, d where the sum in (2) is performed over all unique n -grams in the utterance. 2.2 Discriminative Language Modeling: SVMs We focus on token-based language recognition with SVMs usin g the approach from [1, 4]. Similar the unique n -grams, d probabilities are mapped to a sparse vector with entries The selection of the weighting, D form where g ity p ( d ties. Typical choices for g function g effect of any one feature on the kernel inner product. If we se t C all j . For the experiments in this paper, we use g streams.
 see [1] for more details. For two lattices, L If zero. The normalization D matrices (the Mercer condition). We use the package SVMTorc h [12]. Training is performed with a these two classes. 3.1 SVM Feature Selection varying, the most discriminative sequences for determinin g a property of a speech segment. The have to consider alternate methods.
 Suppose that we have a set of candidate keywords. Since we are already using an SVM, a natural algorithm for discriminative feature selection in this cas e is to use a wrapper method [13]. Suppose that the optimized SVM solution is and where b ( X magnitude entries in w correspond to significant features.
 iterative wrapper method for feature selection for SVMs whi ch has these basic steps: The algorithm may be iterated multiple times.
 SVM and rank n -grams according to the magnitude of the entries in the SVM mo del vector, w . As an example, we have looked at this feature selection metho d for a language recognition task function (CDF) of the SVM model values, | w | w result provides motivation that a small subset of keywords a re significant to the task. 3.2 Keywords via an alternating wrapper/filter method The algorithm in Section 3.1 gives a method for n -gram selection for fixed n . Now, suppose we construction: Keyword Building Algorithm lapping n -grams. For instance, suppose the keyword is some _ people which has phone transcript s _ p _ iy _ p _ l and combine these to produce a new keyword. 3.3 Keyword Implementation (the  X  (  X  ) of the final node or  X  (  X  ) of the initial node of the lattice). equation (8) becomes: trie structure [15] to compute the posteriors of our keywords. 4.1 Language Recognition Experimental Setup task) coming from three collection sources including Callf riend, Mixer and OHSU. We evaluated our system for the 30 and 10 second task under the the NIST 2005 closed condition which limits coming only from the OHSU data source.
 The training and evaluation data was segmented using an auto matic speech activity detector and were removed and lattice expected counts smaller than 10  X  3 were ignored. The top and bottom large). 4.2 Language Recognition Results (Qualitative and Quantit ative) ranking keywords from the English model only (since our phon e recognizer is trained using the n -grams there were many variations or partial n -gram matches to the same keyword, as well as n -grams that didn X  X  correspond to any apparent keyword.
 The equal error rates for our system on the NIST 2005 language recognition evaluation are summa-performance actually degraded significantly compared to th e 3-gram and 4-gram systems. 4.3 Topic Recognition Experimental Setup Topic recognition was performed using a subset of the phase I Fisher corpus (English) from LDC. prompt is: for topic detection. 4.4 Topic Recognition Results can see that many keywords show a strong correspondence with the topic. Also, there are partial keywords which correspond to what appears to be longer keywo rds, e.g.  X  X h_t_s_ih_k X  corresponds to get sick .
 span keywords using phones as a fundamental unit. The proble m was cast as a feature selection able keywords and improved performance could be achieved us ing this methodology. Hypothetical: One Million Dollars to leave the US Y_UW_M_AY_Y you may improvements. For instance, we might want to consider more g eneral keyword models where skips constructing higher order n -grams is a good area for exploration.
 [1] W. M. Campbell, J. P. Campbell, D. A. Reynolds, D. A. Jones , and T. R. Leek,  X  X honetic [3] Bin Ma and Haizhou Li,  X  X  phonotactic-semantic paradigm for automatic spoken document [4] Lu-Feng Zhai, Man hung Siu, Xi Yang, and Herbert Gish,  X  X i scriminatively trained language [5] T. Joachims, Learning to Classify Text Using Support Vector Machines , Kluwer Academic [8] Linguistic Data Consortium,  X  X witchboard-2 corpora, X  http://www.ldc.upenn.edu. [9]  X  X CSI QuickNet, X  http://www.icsi.berkeley.edu/Spee ch/qn.html. [10] S. Young, Gunnar Evermann, Thomas Hain, D. Kershaw, Gar eth Moore, J. Odell, D. Ollason, [11] L. Rabiner and B.-H. Juang, Fundamentals of Speech Recognition , Prentice-Hall, 1993. [12] Ronan Collobert and Samy Bengio,  X  X VMTorch: Support ve ctor machines for large-scale [13] Avrim L. Blum and Pat Langley,  X  X election of relevant fe atures and examples in machine [15] Konrad Rieck and Pavel Laskov,  X  X anguage models for det ection of unknown attacks in net-
