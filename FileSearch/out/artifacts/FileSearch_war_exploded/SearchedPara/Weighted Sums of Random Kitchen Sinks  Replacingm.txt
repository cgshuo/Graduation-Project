 of randomly connected  X  X ssociator units X  that computed random binary functions of their inputs architectures that randomly transform their inputs have been resurfacing in the machine learning community [2, 3, 4, 5], largely motivated by the fact that randomization is computationally cheaper than optimization. With the help of concentration of measure inequalities on function spaces, we linearities. The main technical contributions of the paper are an approximation error bound (Lemma 1), and a synthesis of known techniques from learning theory to analyze random shallow networks. Consider the problem of fitting a function f : X  X  R to a training data set of m input-output pairs { x fitting problem consists of finding an f that minimizes the empirical risk and regularized least squares classification [10]. P i =1  X  ( w i )  X  ( x ; w i ) or f ( x ) = parameterized by some vector w  X   X  , are weighted by a function  X  :  X   X  R . In kernel machines, scalar weights  X  and parameter vectors w jointly:  X  and w , the following algorithm first draws the parameters of the nonlinearities randomly from a optimization: Algorithm 1 The Weighted Sum of Random Kitchen Sinks fitting procedure.
 K , a scalar C , and a probability distribution p ( w ) on the parameters of  X  .
 Output: A function  X  f ( x ) = P K k =1  X  ( x ; w k )  X  k .
 Draw w 1 ,...,w K iid from p .
 Featurize the input: z i  X  [  X  ( x i ; w 1 ) ,..., X  ( x i ; w K )] &gt; .

With w fixed, solve the empirical risk minimization problem In pratice, we let C be large enough that the constraint (4) remains inactive. The when c is the amounts of fitting a linear SVM to a dataset of m K -dimensional feature vectors. is simple: Algorithm 1 can be implemented in a few lines of MATLAB code even for complex feature functions  X  , whereas fitting nonlinearities with Adaboost requires much more care. This between one and three orders of magnitude speedup over Adaboost. On the down side, one might obtain accuracies that are competitive with Adaboost, the same sampling distribution can be used unit variance.
 Formally, we show that Algorithm 1 returns a function that has low true risk. The true risk of a function f is and measures the expected loss of f on as-yet-unseen test points, assuming these test points are generated from the same distribution that generated the training data. The following theorem states risk attainable by functions in the class F p defined below: Define the set { x i ,y i } i =1 ...m are drawn iid from some distribution P , Algorithm 1 returns a function fies w ,...,w K .
 C p ( w ) .
 We prove the theorem in the next section, and demonstrate the algorithm on some sample datasets in Algorithm 1 returns a function that lies in the random set The bound in the main theorem can be decomposed in a standard way into two bounds: The following Lemma is helpful in bounding the approximation error: so that The proof relies on Lemma 4 of the Appendix, which states that the average of bounded vectors in a Hilbert space concentrates towards its expectation in the Hilbert norm exponentially fast.  X   X  (  X  ; w k ) , k = 1 ...K , with  X  k  X   X  (  X  k ) p (  X  4 to f 1 ,...,f K under this inner product.
 probability at least 1  X   X  over w 1 ,...,w K , there exists a function  X  f  X   X  F w that satisfies Proof. For any two functions f and g , the Lipschitz condition on c followed by the concavity of square root gives The lemma then follows from Lemma 1.
 w ,...,w K the empirical risk of every function in  X  F w is close to its true risk. w probability at least 1  X   X  over the dataset, we have of  X 
F w can be shown to be bounded above by C/ results from [12] which are summarized in Theorem 2 of the Appendix.
 Proof of Theorem 1. Let f  X  be a minimizer of R over F p ,  X  f a minimizer of R emp over  X  F w (the output of the algorithm), and  X  f  X  a minimizer of R over  X  F w . Then m 4 LC + 2 | c (0) | + LC The second term in Equation (15) is the approximation error, and by Theorem 1, with probability at least 1  X   X  , it is bounded above by app = LC  X  Zhang analyzed greedy algorithms and a randomized algorithm similar to Algorithm 1 for fitting sparse Gaussian processes to data, a more narrow setting than we consider here. He obtained bounds on the expected error for this sparse approximation problem by viewing these methods as stochastic gradient descent.
 Approximation error bounds such as that of Maurey [11][Lemma 1], Girosi [13] and Gnecco and proximation bounds cannot be used to guarantee the performance of Algorithm 1 because Algorithm 1 is oblivious of the data when it generates the parameters. Lemma 2 differs from these bounds in generating the parameters by sampling them from p instead. Furthermore, because  X  F w is smaller than the classes considered by [11, 14], the approximation error rate in Lemma 1 matches those of existing approximation error bounds. Figure 1: Comparisons between Random Kitchen Sinks and Adaboosted decision stumps on adult (first row), activity (second row), and KDDCUP99 (third row). The first column plots test error of each classifier as a function of K . The accuracy of Random Kitchen Sinks catches up to that of Adaboost as K grows. The second column plots the total training and testing time as a function of K . For a given K , Random Kitchen Sinks is between two and three orders of magnitude faster than Adaboost. The third column combines the previous two columns. It plots testing+training time required to achieve a desired error rate. For a given error rate, Random Kitchen Sinks is between one and three orders of magnitude faster than Adaboost. we only a present a few illustrations in this section.
 We compared Random Kitchen Sinks with Adaboost on three classification problems: The adult 15,000 instances. KDDCUP99 is a network intrusion detection problem with roughly 5,000,000 127-stances. activity is a human activity recognition dataset with 20,0000 223-dimensional instance, datasets were preprocessed by zero-meaning and rescaling each dimension to unit variance. The ply determine whether the w d th dimension of x is smaller or greater than the threshold w t . The sampling distribution p for Random Kitchen Sinks drew the threshold parameter w t from a normal distribution and the coordinate w d from a uniform distribution over the coorindates. For some ex-periments, we could afford to run Random Kitchen Sinks for larger K than Adaboost, and these runs under the hinge loss (though there is degradation in speed by a factor of 2-10). We used MATLAB optimized versions of Adaboost and Random Kitchen Sinks, and report wall clock time in seconds. Figure 1 compares the results on these datasets. Adaboost expends considerable effort in choosing the decision stumps and achieves good test accuracy with a few of them. Random Kitchen Sinks Figure 2: The L  X  norm of  X  returned by RKS for 500 different runs of RKS with various settings of K on adult . k  X  k  X  decays with K , which justifies dropping the constraint (4) in practice. time. In these experiments, Random Kitchen Sinks is almost as accurate as Adaboost but faster by one to three orders of magnitude.
 we have experimented with conjugate gradient-descent based fitting procedures for (2), and find again that randomly generating the nonlinearities produces equally accurate classifiers using many more nonlinearities but in much less time. We obtain similar results as those of Figure 1 with the random features of [4], and random sigmoidal ridge functions  X  ( x ; w ) =  X  ( w 0 x ) , To simplify the implementation of Random Kitchen Sinks, we ignore the constraint (4) in practice. cannot grow faster than K . Figure 2 shows that the L  X  norm of the unconstrained optimum of (3) K for which the constraint is never tight, thereby justifying dropping the constraint. Various hardness of approximation lower bounds for fixed basis functions exist (see, for example [11]). The guarantee in Lemma 1 avoids running afoul of these lower bounds because it does not seek to approximate every function in F p simultaneously, but rather only the true risk minimizer with high probability.
 It may be surprising that Theorem 1 holds even when the feature functions  X  are nearly orthogo-result which we leave for future work.
 One may wonder whether Algorithm 1 has good theoretical guarantees on F p because F p is R F p is smaller than the set considered by Jones [9] for greedy matching pursuit, and for which he obtained an approximation rate of O (1 / for practical applications is to conduct experiments with real data. The experiment show that F p indeed contains good predictors.
 faster than 1 / direct comparison of the bounds difficult, though we have tried to provide empirical comparisons. Lemma 4. Let X = { x 1 ,  X  X  X  ,x K } be iid random variables in a ball H of radius M centered around the origin in a Hilbert space. Denote their average by X = 1 K P K k =1 x k . Then for any  X  &gt; 0 , with probability at least 1  X   X  , Proof. We use McDiarmid X  X  inequality to show that the scalar function f ( X ) = X  X  E X X is concentrated about its mean, which shrinks as O (1 / be a copy of X with the i th element replaced by an arbitrary element of H . Applying the triangle inequality twice gives To bound the expectation of f , use the familiar identity about the variance of the average of iid random variables in conjunction with Jensen X  X  inequality and the fact that k x k X  M to get This bound for the expectation of f and McDiarmid X  X  inequality give To get the final result, set  X  to the right hand side, solve for , and rearrange. One measure of the size of a class F of functions is its Rademacher complexity: The variables  X  1 ,  X  X  X  , X  m are iid Bernouli random variables that take on the value -1 or +1 with equal probability and are independent of x 1 ,...,x m .
 The Rademacher complexity of  X  F w can be bounded as follows. Define S  X   X   X  R K k  X  k  X   X  C K :
R m [  X  F w ] = E the boundedness of  X  .
 The following theorem is a summary of the results from [12]: Theorem 2. Let F be a class of bounded functions so that sup x | f ( x ) |  X  C for all f  X  F , and function in F satisfies [2] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural [3] F. Moosmann, B. Triggs, and F. Jurie. Randomized clustering forests for building fast and [4] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in [5] W. Maass and H. Markram. On the computational power of circuits of spiking neurons. Journal [6] E. Osuna, R. Freund, and F. Girosi. Training support vector machines: an application to face [7] R. E. Schapire. The boosting approach to machine learning: An overview. In D. D. Denison, [12] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and [13] F. Girosi. Approximation error bounds that use VC-bounds. In International Conference on [14] G. Gnecco and M. Sanguineti. Approximation error bounds via Rademacher X  X  complexity.
