 With 271 million monthly active users 1 that produce over 500 million tweets per day 2 , Twitter is the most popular and fastest-growing microblogging service. Microblogging is a form of social media that enables users to broadcast short messages, links, and audiovisual content. In the case of Twitter, these so-called tweets can contain 140 characters and are posted to a network of followers as well as to a user X  X  public timeline. The brevity of tweets make them an ideal mobile communication medium and Twitter is therefore increasingly used as an information source for current events as they unfold. For example, Twitter data has been used to detect earthquakes [ 17 ], to track epidemics [ 10 ], or to monitor elections [ 21 ].
 in a certain geographical location and over a certain time period [ 3 ]. For tradi-tional media such as newspaper archives and news websites, the problem of event detection has been addressed by research from the area of Topic Detection and Tracking (TDT). However, topic detection in Twitter data streams introduces new challenges. First, Twitter  X  X ocuments X  are much shorter than traditional news articles and therefore harder to classify. Second, tweets are not redacted and thus contain a substantial amount of spam, typos, slang, etc. Finally, the rate at which tweets are produced is very bursty and continually increases as more people adopt Twitter every day.
 Several techniques for event detection in Twitter have been proposed. How-ever, most of these approaches suffer from two major shortcomings. First, they tend to focus exclusively on the information extraction aspect and often ignore the streaming nature of the input. As a consequence, they make unrealistic assumptions, which limit their practical value. Examples of such assumptions include buffering entire months of Twitter data before processing it or fixing a complex set of parameters at design-time using sample data. Second, very few authors have evaluated their technique quantitatively or comparatively. While most provide some qualitative evidence demonstrating their task-based perfor-mance, very few consider run-time performance. Therefore, little or no research to date has measured the computing cost of the same result quality for different approaches. We argue that understanding this trade-off is particularly important in a streaming setting, where processing needs to happen in real-time. In this paper, we present a method to study the task-based and the run-time performance of current and future event detection techniques. In order to mea-sure comparable run-time performance numbers, we propose to  X  X tandardize X  event detection techniques by implementing them based on a single data stream management system. Additionally, we developed several scalable measures to assess the task-based performance of event detection techniques automatically, i.e., without painstakingly crafting a gold standard manually. The specific con-tributions of this paper are as follows. 1. Streaming implementations of state-of-the-art event detection techniques for
Twitter that are consistent with respect to each other. 2. Detailed study of the task-based and run-time performance of well-known event detection techniques. 3. Platform-based approach that will enable further systematic performance studies for novel event detection techniques in the future.
 The remainder of this paper is structured as follows. Section 2 provides the background of this work by summarizing the state of the art in event detection for Twitter data streams. In Sect. 3 , we give a brief overview of Niagarino, the data stream management system that we used as an implementation platform. Section 4 describes the selected event detection techniques and their streaming implementations using Niagarino. Section 5 presents the results of the evalua-tion that we performed in order to study the selected task-based and run-time performance of these event detection techniques. Finally, concluding remarks are given in Sect. 6 .
 Our work is situated in the research field of analysis and knowledge discovery for social media data. Bontcheva et al. [ 8 ] provides a good general overview of sense making of social media data by surveying state-of-the-art approaches for mining semantics from social media streams. Due to the fast propagation speed of information in social media networks, a large number of works focus on event or topic detection and tracking for various domains. In this setting, Farzindar and Khreich [ 11 ] surveyed techniques for event detection in Twitter. The work presented in this paper targets approaches that support the detection of general (unknown) events [ 3 ] and we will therefore focus the following discussion on approaches that share this goal.
 on locality sensitive hashing. The approach uses the number of tweets and hash-tags, but also introduces a novel measure of entropy for the analysis. The method was evaluated using six months of data containing 163.5 million tweets and an average precision score was calculated against a manually labeled result set. Becker et al. [ 6 ] present an approach for  X  X eal-world event detection on Twit-ter X  that uses an online clustering method in combination with a support vector machine classifier. They focus on hashtags with special capitalization and check for retweets, replies, and mentions. The method is evaluated against a manually labeled result set for a one-month data set with 2.6 million tweets. Long et al. [ 14 ] use divisive clustering, whereas Weng and Lee [ 21 ] use discrete wavelet analysis and graph partitioning. Both of these approaches use word frequencies of indi-vidual words for event detection. The latter approach was evaluated by using a self-built ground truth, which is prepared by using a latent dirichlet allocation (LDA) method [ 7 ]. Cordeiro [ 9 ] proposes the use of continuous wavelet analysis to detect event peaks in the signal of hashtags and summarizes the detected events by using LDA. For evaluation purposes, they used a visual illustration of their results obtained from an eight-day data set with 13.6 million tweets. Zim-mermann et al. [ 22 ] present a text stream clustering method that detects, tracks, and updates large and small bursts in a two-level (global and local) topic hier-archy by using collected news articles. The technique proposed by enBloque [ 4 ] to detect emergent events relies on statistics about tags and pairs of tags. These statistics are computed using a time-sliding window and monitored for shifts in order to capture unpredictable and thus interesting developments. It has been evaluated on a two-week Twitter data set by conducting experiments to measure run-time performance and a user study to assess task-based performance. it is important to note that all existing approaches are realized as custom ad-hoc implementations, which limits the reproducibility and comparative evaluation of their results. As a consequence, little to no comparative evaluations of different event detection methods exist. In particular, none of these approaches have been evaluated to relate their task-based (result quality) and run-time performance (tweets per second). Therefore, there is a comprehensive lack of evaluation meth-ods for event detection techniques for social media data. In order to realize streaming implementations of state-of-the-art event detection techniques for Twitter, we use Niagarino 3 , a data stream management system that is developed and maintained by our research group. The main purpose of Niagarino is to serve as an easy-to-use and extensible research platform for streaming applications such as the one presented in the paper. The concepts embodied by Niagarino can be traced back to a series of pioneering data stream management systems, such as Aurora [ 2 ], Borealis [ 1 ], and STREAM/CQL [ 5 ]. In particular, Niagarino is an offshoot of NiagaraST [ 13 ], with which it shares the most common ground. In this section, we briefly summarize the parts of Niagarino that are relevant for this paper.
 In Niagarino, a query is represented as a directed acyclic graph where O is the set of operators used in the query and S is the set of streams used to connect the operators. The Niagarino data model is based on relational tuples that follow the first normal form, i.e., have no nesting. Two types of tuples can be distinguished, data and metadata tuples. Data tuples are strongly typed and have a schema that defines the domains of all attributes. All data tuples in a stream share the same schema, which corresponds to the output schema of the operator that generates the tuples and must comply with the input schema of the operator that consumes the tuples. In contrast, metadata tuples, so-called messages, are untyped and typically self-describing. Therefore, different messages can travel in the same stream. Messages are primarily used to transmit data and operator statistics in order to coordinate the operators in a query. Each stream is bidirectional consisting of a forward and a backward direction. While data tuples can only travel forward, messages can travel in both directions. Based on its relational data model, Niagarino implements a series of oper-ators. The selection (  X  ) and projection (  X  ) operator work exactly the same as their counterparts in relational databases. Other tuple-based operators include the derive ( f ) and the unnest (  X  ) operator. The derive operator applies a func-tion to a single tuple and appends the result value to the tuple. The unnest operator splits a  X  X ested X  attribute value and emits a tuple for each new value. A typical use case for the unnest operator is to split a string and to produce a tuple for each term it contains. Apart from these general operators, Niagarino provides a number of stream-specific operators that can be used to segment the unbounded stream for processing. Apart from the well-known time and tuple-based window operators (  X  ) that can be tumbling or sliding [ 12 ], Niagarino also implements data-driven windows, so-called frames [ 15 ]. Stream segments form the input for join ( ) and aggregation ( X ) operators. As with derive operators, Niagarino also supports user-defined aggregation functions. Niagarino opera-tors can be partitioned into three groups. The operators described above are general operators, whereas source operators read input streams and sink oper-ators output results. Each query can have multiple source and sink operators. This classification is similar to the notion of spouts and bolts used in Twitter X  X  data stream management system Storm [ 19 ].
 features. In particular, anonymous functions (  X  -expressions) are used in several operators in order to support lightweight extensibility with user-defined func-tionality. The current implementation runs every operator in its own thread. Operator threads are scheduled implicitly using fixed-size input/output buffers and explicitly through backwards messages. We focus on techniques with the specific task of first story detection, i.e., the detection of general (unknown) events, which is defined as a subtask of TDT [ 3 ]. In this section, we briefly describe the five state-of-the-art techniques that we selected for our study in terms of their functionality and the parameters used. Figure 1 illustrates these techniques by means of Niagarino query plans that use the operators described in the previous section. As can be seen in the figure, all of these techniques use the same pre-processing steps before the streaming tuples enter the actual event detection phase. The pre-processing selects all tweets that are non-retweets and in English. Additionally, each tuple is enriched with the derived distinct terms of the tweet that are not contained in a standard English stop-word list or can be considered noise (e.g., less than three characters, unknown characters, repetition of the same pattern, or terms without vowels). inverse document frequency (IDF) [ 18 ] over an entire time window. All values are then sorted and the top n terms are reported as events together with their top m most frequently co-occurring terms, which are also obtained by using the IDF measure.
 that explains the variation in a set of documents in terms of a set of  X  X opics X , i.e., distributions over the vocabulary. Since LDA is normally used for topic modeling, we equate a topic to an event. For each time window, LDA extracts n events that are described by m terms. The parameter number of iterations performed in the modeling phase, where a higher value typ-ically increases the quality of the detected events. To perform the LDA, we use Mallet 4 , an existing Java library.
 shift of IDF values of single terms in pairs of successive sliding windows of a pre-defined size. First, the IDF value of each term in a single window (with size s input ) is continuously computed and compared to the average IDF value of all terms within that window. Terms with an IDF value above the average are filtered out. The next step builds a window with size s 1 r 1 in order to calculate the shift from one window to the next. In this step, the shift value is again checked against the average shift of all terms and only terms with a shift above the average are retained. In the last step, a new sliding window with size s 2 that slides with range r 2 is created. The total shift value is computed as the sum of all shift values of the sub-windows of this window. If this total shift value is greater than the pre-defined threshold  X , the term is detected as event and reported together with its top 4 co-occurring terms. The first step of the Event Detection with Clustering of Wavelet-based Sig-nals (EDCoW) [ 21 ] algorithm is to partition the stream into intervals of and to build DF-IDF signals for each distinct term in the interval. These signals are further analyzed using discrete wavelet analysis that builds a second signal for the individual terms. Each data point of this second signal summarizes a sequence of values from the first signal with length  X . The next step then filters out trivial terms by checking the corresponding signal auto-correlations against a threshold  X  . The remaining terms are then clustered to form events with a modularity-based graph partitioning technique. Insignificant events are filtered out using a threshold parameter . Since this approach detects events with a minimum of two terms, we introduced an additional enrichment step that adds the top co-occurring terms to obtain events with at least five terms. The Wavelet Analysis Topic Inference Summarization (WATIS) [ 9 ] algorithm also partitions the stream into intervals of s seconds and builds DF-IDF signals for each distinct term. Due to the noisy nature of the Twitter data stream, signals are then processed by applying the adaptive Kolmogorov-Zurbenko filter (KZA), a low-pass filter that smoothens the signal by calculating a moving average with i kz iterations over n intervals. It then uses continuous wavelet transformation to construct a time/frequency representation of the signal and two wavelet analyses, the tree map of the continuous wavelet extrema and the local maxima detection, to detect abrupt increases in the frequency of a term. To enrich events with more information, the previously mentioned LDA algorithm (with used to finally report events that consist of five terms each. The evaluation of event detection techniques is itself a challenging task. Deter-mining an F 1 score in terms of precision and recall would require a ground truth (gold standard) to which the detected events can be compared. Due to the lack of such a ground truth for the Twitter data stream, some existing approaches have been evaluated using a manually created ground truth or based on user studies, if at all. Since both of these methods are very time-consuming and do not scale, we have experimented with a number of measures that can be applied automatically. In this section, we discuss the motivation behind these measures and present detailed results that were obtained by using them. 5.1 Measures In order to evaluate different techniques automatically, we defined five main measures (some with sub-measures), which are used for the individual ratings. The measures are described in the following.
 Precision (Search Engine). This measure describes the percentage of events that can be verified with the use of a search engine ( www.google.com ). For each detected event, the search engine is queried using the five event terms and a specific date range. A rating between 1 and 10 ( GoogleN ) is computed by checking how many of the first ten result hits point to a news website. News websites are identified based on a whitelist of domain names containing sites such as CNN, CBS, Reuters, NYTimes, and the Guardian. Based on this measure, detected events can be rated with respect to their newsworthiness on or at least one day after the detection date.
 Precision (DBPedia). This measure is calculated using the DBPedia which contains the abstracts (long versions) from all Wikipedia articles. In order to query the roughly four million English abstract, the native XML database BaseX 6 is used. For each detected event, the number of matching abstracts in DBPedia is computed using XQuery Full Text. We have defined three sub-measures. DBPedia5 is the precision using all five event terms, DBPedia4S only uses the top four event terms, and DBPedia4A queries DBPedia with all subsets of cardinality four. For the first two measures, an abstract is considered a match to an event if it contains all terms that were used in the query. For the third measure, an abstract matches if it contains all terms of one of the combinations. Recall. In order to compute the recall, Bloomberg 7 was crawled as their archive maintains a list of the most important news articles for each day. Crawling individual days leads to an average of about 200 events per day. Each crawled news item is then tokenized and cleaned by the same processes as the tweets. As a consequence, the short description of each news item by a series of terms can be very similar to the one obtained from the tweets. In order to calculate the similarity between detected events and a news item, eventSim ( which is based on the Levenshtein distance. The motivation behind eventSim ( e 1 ,e 2 ) is to compensate for misspellings or alternate spellings of terms as well as for different term sets describing sim-ilar events. An event is represented as an alphabetically sorted list of terms e =[ t the levSim ( t 1 ,t 2 ), which is the Levenshtein distance normalized to the range [0 ... 1]. If the similarity of a term of e 1 toatermof e 2 is above the threshold minTermSim , this combination is marked as hit and the algorithm continues with the next term of e 1 . Finally, eventSim ( e 1 ,e 2 ) aggregates the number of hits and normalizes it with the number of terms.
 In an effort to obtain a reasonable amount of hits, the parameters of this formula are set rather low. The parameter minTermSim issetto0 overall limit for eventSim issetto0 . 2. Two sub-measures are defined for the recall. Bloom1D calculates the recall just for the given date, whereas Bloom2D also includes the following day.
 Duplicate Event Detection Rate (DEDR). This measure is also based on the event similarity defined above in order to calculate the similarity of the events for one single technique and data set. Two sub-measures have been defined. For ADEDR (almost duplicate event detection rate) the parameter minTermSim is set to 0 . 8 and the limit for eventSim is set to 0 . 5, whereas for FDEDR (full duplicate event detection rate) the minTermSim is the same but the limit for eventSim is set to 0 . 9.
 Run-time Performance. Run-time performance is measured as the number of tweets per second that a technique is able to process. 5.2 Data Sets The data sets used in the study presented in this paper consist of 10% of the public live stream of Twitter for four days. Using the Twitter Streaming API with the so-called  X  X ardenhose X  access level, which is a randomly sampled sub-stream, we collected data for the first day of June, August, September, and October. Figure 2 provides statistics of the initial data set as well as for the processing steps that are common to all techniques (cf. Fig. 1 ). Figure 2a presents the total number of tweets for the chosen days grouped by the hour (given in GMT+1). As can be seen, the rate of tweets follows a regular daily pattern. On average, the incoming stream contains 2.3 million tweets/hour and 35,000 tweets/minute. Figure 2b shows the hourly tweet volumes after filtering out retweets at an average of 1.6 million tweets/hour. After the next step, shown in Fig. 2c , the data sets are further reduced to an average of 500,000 tweets/hour by filtering out tweets that are not in English. Finally, Fig. 2d shows an average of 120,000 distinct terms/hour that have been derived from all English tweets. 5.3 Experimental Setup In order to be able to compare the results of the five chosen techniques in a fair way, they have to be aligned in terms of the rate and number of events detected. The rate can be controlled by setting the time window on which a technique is performed. Since we are interested in (near) real-time event detection, a window of one hour was used. Note, that Shifty is the only true streaming algorithm that reports results continuously, whereas all other techniques only produce results after each hour. The number of events that are detected can be controlled by setting the specific parameters of each technique. Given that our recall measure assumes an average of 200 events per day and compensating for events that are detected multiple times, we aim for about 350 events per day. The parameter settings used are described below, whereas the actual number of detected events perdayandtechniqueareshowninTab. 1 .
 TopN. Per hour, the top n = 15 events are reported together with co-occurring terms to obtain a total of 360 events per day LDA. LDA is set to perform i = 500 iterations and to report 15 events, described by m = 5 terms each, per hour, yielding again a total of 360 events per day.
 Shifty. The IDF value is calculated over 1-minute intervals. The size of the window used to compute the IDF shift is s 1 = 2 minutes. The size of the window that aggregates and filters the IDF shift is s 2 = 4 minutes. Both windows slide by range r 1 = r 2 = 1 minute. By setting the threshold  X  = 0 . 35, we obtain all terms with a minute by minute IDF value that increases more than 35% over four minutes.
 EDCoW. The size of the initial intervals is set to s = 10 seconds and the number of intervals that are combined by the wavelet analysis to  X  = 32, yielding a total window size per value of 320 seconds. The other parameters are set to the same values as in the original paper (  X  =1and =0
As the original paper fails to mention the wavelet type that was used, we experimented with several types. The results reported in this paper are based on the Discrete Meyer wavelet, which showed the best performance.
 WATIS. The length of initial intervals is set to s = 85 seconds. For the KZ/KZA analysis, n = 5 intervals and i kz = 5 iterations are used, yielding a total window size of 425 seconds. LDA is set to perform i lda = 500 iterations and report a description with five terms per detected event.
 5.4 Results In the following, we present the results of our evaluation of event detection tech-niques in terms of run-time and task-based performance. Rather than discussing all results that we have obtained, we focus on the most significant measures and outcomes. While we do not claim that our measures are absolute, it should be noted that these results support relative conclusions.
 Run-Time Performance. Run-time performance was measured using Ora-cle Java 1.8.0 25 (64 bit) on server-grade hardware with 2 Intel Xeon E5345s processors at 2.33 GHz with 4 cores each and 24 GB of main memory. The cor-responding results for all techniques in terms of throughput (tweets/second) are given in Fig. 3 . We note that the performance of all techniques is very stable across the four days for which experiments were run. Taking into account the average rate of 35,000 tweets/minute (583 tweets/second), we can derive that all techniques are able to process the 10% stream in real-time on the tested hard-ware. However, taking a 100% stream (  X  5 , 830 tweets/second) into account, both LDA500 and WATIS would be too slow to process the stream in real-time on the tested hardware. In both techniques, the number of LDA iterations could be reduced, i.e., trading off result quality for performance. Finally, we point out that our experimental setup is stacked against our own technique, Shifty . In contrast to the other approaches that can only process tweets at the end of each one-hour window, Shifty processes tweets continuously and can therefore amortize its processing cost over the one-hour window.
 Task-Based Performance. The first measure of task-based performance that we will examine is the duplicate event detection rate. Results obtained using both the ADEDR and FDEDR sub-measures are given in Fig. 4 . In comparison to the other three techniques, both Top15 and LDA500 detect a large number of duplicates. This result is explained by the fact that these techniques identify events based on the absolute frequency of terms, i.e., without considering changes in the relative frequency. The ADEDR of the remaining three techniques is relatively low in the range of 15 X 18%. Shifty  X  X  FDEDR stayed consistently below 10% in all our experiments, whereas EDCoW and WATIS do hardly detect any duplicates at all. Finally, the results also show that there is little deviation in the detected number of duplicates over the four days in our data set. Apart from the duplicate event detection rate, we have also studied the task-based performance of the selected techniques in terms of precision and recall. Figure 5 summarizes the precision results of all techniques obtained with the Google1 , Google2 , DBPedia5 ,and DBPedia4S measures. We omit results from the DBPedia4A as our experiments showed that they are not discriminating. Even though the measures we defined yield a wide range of precision values, their relative ratio is always the same. Since our goal is to comparatively evaluate event detection techniques, we conclude that our measures are sound with respect to this criterion. Again, Top15 and LDA500 stand out with higher precision values than the other three techniques. The reason for this result is that our precision measures are slightly biased towards approaches that report duplicates. Figure 6 shows the recall results for the Bloom1D measure. Bloom2D is omitted as the results are almost exactly the same. First of all, it can be seen from the figure that the recall of all techniques is relatively low at 10 X 20%. Note that our recall measure is based on the Bloomberg news website, which lists an average of 200 topics per day. Even though techniques were configured to report about 1 . 5  X  as many events, our recall measure is nevertheless ambitious. For example, it is difficult to imagine that enough people will tweet about a topic such as Heathrow X  X  cargo statistics in order to detect it as an event. However, since we are only interested in relative measures, these low recall figures are not a problem. Rather, we can observe that Top15 and LDA500 generally have a lower recall than the other three techniques. As this outcome is to be expected due to the high duplicate event detection rate of these techniques, we can again conclude that our measure for recall is sound.
 In order to summarize the most discriminating measures presented in this paper, we define three scoring functions that can be used to compare the run-time and task-based performance of event detection techniques. The three scoring functions are defined as follows.
 value of the Google1 and Bloom1D measures for precision and recall, respectively. Alternatively, using DBPedia5 leads to very similar results. The second score, PFScore , also factors in the performance rate of the technique. Performance values are normalized to the range [0 ... 1] by setting the maximum processing rate that we measured to 1. Finally, the last measure, DPFScore , also includes the duplicate event detection rate of the technique. In the following, we have used the value of the FDEDR measure to calculate DPFScore .
 shows the scores that were assigned to each of the five techniques as aver-ages over the four days in the eval-uation data set. Even though Top15 scores relatively high in terms of pre-cision, its FScore is low due to a poor recall because of duplicates. As Top15 is consistently the fastest technique in our experiments, its PFScore is equal to its FScore . The high DEDR of Top15 has a noticeable negative effect on its DPFScore . LDA500  X  X  FScore is relatively high, but comes at a high perfor-mance penalty, which negatively affects both its PFScore and DPFScore .Based on these results, we can conclude that neither Top15 nor LDA500 are suit-able event detection techniques. This result is not surprising as both of these techniques have originally not been developed for this task.
 In particular, none of these techniques suffer significantly from duplicate event detection. Shifty and WATIS have a similar FScore , but are both negatively affected by their performance score. However, since Shifty  X  X  streaming algorithm was forced to an hourly reporting scheme for the sake of comparability, this score is still a good result for our technique. EDCoW scores impressive results for all scoring functions, which confirms that its status as the most cited event detection technique is well-deserved. This work however is the first to provide comparative and quantitative evidence for EDCoW  X  X  quality.
 tracking re-occurring events or changes in event descriptions. The need to study event detection techniques in both settings, motivates our separate definitions of FScore , PFScore ,and DPFScore . Both LDA500 and Top15 could be extended to explicitly avoid the detection of duplicate events. However, since the other techniques do allow for duplicates, we have chosen not to do so in this study. In this paper, we addressed the problem of comparatively and quantitatively studying the task-based and run-time performance of state-of-the-art event detec-tion techniques for Twitter. In order to do so, we have presented a two-pronged approach. First, we ensure comparable run-time performance results by providing streaming implementations of all techniques based on a data stream management system. Second, we propose several new measures that can assess the relative task-based performance of event detection techniques. The detailed study described in this paper has shown that these measures are sound and which of them are most discriminating. Finally, we defined scoring functions based on selected measures that revealed how the different techniques relate to each other as well as where their strengths and weaknesses lie.
 As immediate future work, we plan to take advantage of our platform-based approach to study further techniques, e.g., enBloque [ 4 ] and the approach of Petrovi  X  c et al. [ 16 ]. At the same time, the currently implemented techniques could be improved to process data continuously. Furthermore, the influence of the pre-processing on run-time and task-based performance should be studied. In our platform-based approach, we can easily remove existing operators (e.g., retweet filtering) and replace them with new operators (e.g., part-of-speech tag-ging or named-entity recognition). Finally, a deeper evaluation of how the dif-ferent parameters of a technique influence the trade-off between run-time and task-based performance could give rise to adaptive event detection techniques.
