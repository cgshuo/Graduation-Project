 Mingyuan Zhou, Haojun Chen, John Paisley, Lu Ren, 1 Guillermo Sapiro and Lawrence Carin There has been significant recent interest in sparse signal expansions in several settings. For ex-ample, such algorithms as the support vector machine (SVM) [1], the relevance vector machine (RVM) [2], Lasso [3] and many others have been developed for sparse regression (and classifica-tion). A sparse representation has several advantages, including the fact that it encourages a simple model, and therefore over-training is often avoided. The inferred sparse coefficients also often have biological/physical meaning, of interest for model interpretation [4].
 Of relevance for the current paper, there has recently been significant interest in sparse representa-tions in the context of denoising, inpainting [5 X 10], compressive sensing (CS) [11,12], and classifi-cation [13]. All of these applications exploit the fact that most images may be sparsely represented in an appropriate dictionary. Most of the CS literature assumes  X  X ff-the-shelf X  wavelet and DCT bases/dictionaries [14], but recent denoising and inpainting research has demonstrated the signif-icant advantages of learning an often over-complete dictionary matched to the signals of interest ( e.g. , images) [5 X 10, 12, 15]. The purpose of this paper is to perform dictionary learning using new non-parametric Bayesian technology [16,17], that offers several advantages not found in earlier approaches, which have generally sought point estimates.
 This paper makes four main contributions:  X  The dictionary is learned using a beta process construction [16, 17], and therefore the number of dictionary elements and their relative importance may be inferred non-parametrically.  X  For the denoising and inpainting applications, we do not have to assume a priori knowledge of the noise variance (it is inferred within the inversion). The noise variance can also be non-stationary.  X  The spatial inter-relationships between different components in images are exploited by use of the Dirichlet process [18] and a probit stick-breaking process [19].  X  Using learned dictionaries, inferred off-line or in situ , the proposed approach yields CS perfor-mance that is markedly better than existing standard CS methods as applied to imagery. In traditional sparse coding tasks, one considers a signal x  X  &lt; n and a fixed dictionary D = ( d 1 , d 2 , . . . , d M ) where each d m  X  &lt; n . We wish to impose that any x  X  &lt; n may be represented approximately as  X  x = D  X  , where  X   X  &lt; M is sparse, and our objective is to also minimize the ` 2 error k  X  x  X  x k 2 . With a proper dictionary, a sparse  X  often manifests robustness to noise (the model doesn X  X  fit noise well), and the model also yields effective inference of  X  even when x is partially or indirectly observed via a small number of measurements (of interest for inpainting, interpolation and compressive sensing [5,7]). To the authors X  knowledge, all previous work in this direction has been performed in the following manner: ( i ) if D is given, the sparse vector  X  is estimated via a point estimate (without a posterior distribution), typically based on orthogonal matching pursuits (OMP), basis pursuits or related methods, for which the stopping criteria is defined by assuming knowledge (or off-line estimation) of the noise variance or the sparsity level of  X  ; and ( ii ) when the dictionary D is to be learned, the dictionary size M must be set a priori , and a point estimate is achieved for D (in practice one may infer M via cross-validation, with this step avoided in the proposed method). In many applications one may not know the noise variance or an appropriate sparsity level of  X  ; further, one may be interested in the confidence of the estimate ( e.g. ,  X  X rror bars X  on the estimate of  X  ). To address these goals, we propose development of a non-parametric Bayesian formulation to this problem, in terms of the beta process, this allowing one to infer the appropriate values of M and k  X  k 0 (sparsity level) jointly, also manifesting a full posterior density function on the learned D and the inferred  X  (for a particular x ), yielding a measure of confidence in the inversion. As discussed further below, the non-parametric Bayesian formulation also allows one to relax other assumptions that have been made in the field of learning D and  X  for denoising, inpainting and compressive sensing. Further, the addition of other goals are readily addressed within the non-parametric Bayesian paradigm, e.g. designing D for joint compression and classification. 2.1 Beta process formulation We desire the model x = D  X  + , where x  X &lt; n and D  X &lt; n  X  M , and we wish to learn D and in so doing infer M . Toward this end, we consider a dictionary D  X  &lt; n  X  K , with K  X   X  ; by inferring the number of columns of D that are required for accurate representation of x , the appropriate value of M is implicitly inferred (work has been considered in [20, 21] for the related but distinct application of factor analysis). We wish to also impose that  X   X  &lt; K is sparse, and therefore only a small fraction of the columns of D are used for representation of a given x . Specifically, assume N c  X  2 represents the number of classes from which the data arise; when learning the dictionary we ignore the class labels y i , and later discuss how they may be considered in the learning process. The two-parameter beta process (BP) was developed in [17], to which the reader is referred for further details; we here only provide those details of relevance for the current application. The BP H  X  BP ( a, b, H 0 ) may be represented as with this a valid measure as K  X   X  . The expression  X   X  otherwise. Therefore, H (  X  ) represents a vector of K probabilities, with each associated with a respective atom  X  k . In the limit K  X   X  , H (  X  ) corresponds to an infinite-dimensional vector of probabilities, and each probability has an associated atom  X  k drawn i.i.d. from H 0 .
 Using H (  X  ) , we may now draw N binary vectors, the i th of which is denoted z i  X  { 0 , 1 } K , and the k th component of z i is drawn z ik  X  Bernoulli (  X  k ) . These N binary column vectors are used to constitute a matrix Z  X  { 0 , 1 } K  X  N , with i th column corresponding to z i ; the k th row of Z is associated with atom  X  k , drawn as discussed above. For our problem the atoms  X  k  X  &lt; n will correspond to candidate members of our dictionary D , and the binary vector z i defines which members of the dictionary are used to represent sample x i  X  X  . Let  X  = (  X  1 ,  X  2 , . . . ,  X  K ) , and we may consider the limit K  X   X  . A naive form of our model, for representation of sample x i  X  D , is x i =  X  z i + i . However, this is highly restrictive, as it imposes that the coefficients of the dictionary expansion must be binary. To address this, we draw weights w i  X  N (0 ,  X   X  1 w I K ) , where  X  w is the precision or inverse variance; the dictionary weights are now  X  i = z i  X  w i , and x i =  X   X  i + i , where  X  represents the Hadamard (element-wise) multiplication of two vectors. Note that, by construction,  X  is sparse; this imposition of sparseness is distinct from the widely used Laplace shrinkage prior [3], which imposes that many coefficients are small but not necessarily exactly zero.
 For simplicity we assume that the dictionary elements, defined by the atoms  X  k , are drawn from a multivariate Gaussian base H 0 , and the components of the error vectors i are drawn i.i.d. from a zero-mean Gaussian. The hierarchical form of the model may now be expressed as Non-informative gamma hyper-priors are typically placed on  X  w and  X  . Consecutive elements in the above hierarchical model are in the conjugate exponential family, and therefore infer-ence may be implemented via a variational Bayesian [22] or Gibbs-sampling analysis, with analytic update equations (all inference update equations, and the software, can be found at http://people.ee.duke.edu/  X  lihan/cs/ ). After performing such inference, we retain those columns of  X  that are used in the representation of the data in D , thereby inferring D and hence M . To impose our desire that the vector of dictionary weights  X  is sparse, one may adjust the parameters a and b . Particularly, as discussed in [17], in the limit K  X   X  , the number of elements of z i that are non-zero is a random variable drawn from Poisson ( a/b ) . In Section 3.1 we discuss the fact that these parameters are in general non-informative and the sparsity is intrinsic to the data. 2.2 Accounting for a classification task There are problems for which it is desired that x is sparsely rendered in D , and the associated weight vector  X  may be employed for other purposes beyond representation. For example, one may perform a classification task based on  X  . If one is interested in joint compression and classification, both goals should be accounted for when designing D . For simplicity, we assume that the number of classes is N C = 2 (binary classification), with this readily extended [23] to N C &gt; 2 . Following [9], we may define a linear or bilinear classifier based on the sparse weights  X  and the associated data x (in the bilinear case), with this here implemented in the form of a probit classifier. We focus on the linear model, as it is simpler (has fewer parameters), and the results in [9] demon-strated that it was often as good or better than the bilinear classifier. To account for classification, the model in (2) remains unchanged, and the following may be added to the top of the hierarchy: y  X   X   X  &lt; K +1 is the same as  X   X  &lt; K with an appended one, to account for the classifier bias. Again, one typically places (non-informative) gamma hyper-priors on  X   X  and  X  0 . With the added layers for the classifier, the conjugate-exponential character of the model is retained, sustaining the ability to perform VB or MCMC inference with analytic update equations. Note that the model in (2) may be employed for unlabeled data, and the extension above may be employed for the available labeled data; consequently, all data (labeled and unlabeled) may be processed jointly to infer D . 2.3 Sequential dictionary learning for large training sets In the above discussion, we implicitly assumed all data D = { x i , y i } i =1 ,N are used together to infer the dictionary D . However, in some applications N may be large, and therefore such a  X  X atch X  approach is undesirable. To address this issue one may partition the data as D = D 1  X  D 2  X  . . . D J  X  1  X  D J , with the data processed sequentially. This issue has been considered for point estimates of D [8], in which considerations are required to assure algorithm convergence. It is of interest to briefly note that sequential inference is handled naturally via the proposed Bayesian analysis. Specifically, let p ( D |D ,  X  ) represent the posterior on the desired dictionary, with all other model parameters marginalized out ( e.g. , the sample-dependent coefficients  X  ); the vector  X  represents the model hyper-parameters. In a Bayesian analysis, rather than evaluating p ( D |D ,  X  ) directly, one may employ the same model (prior) to infer p ( D |D 1 ,  X  ) . This posterior may then serve as a prior for D when considering next D 2 , inferring p ( D |D 1  X  X  2 ,  X  ) . When doing variational Bayesian (VB) inference we have an analytic approximate representation for posteriors such as p ( D |D 1 ,  X  ) , while for Gibbs sampling we may use the inferred samples. When presenting results in Section 5, we discuss additional means of sequentially accelerating a Gibbs sampler. 3.1 Image Denoising and Inpainting a monochrome image for simplicity, but color images are also readily handled, as demonstrated when presenting results. As is done typically [6, 7], we partition the image into N B = ( N y  X  B + 1)  X  ( N x  X  B + 1) overlapping blocks { x i } i =1 ,N B , for each of which x i  X  &lt; B 2 ( B = 8 is typically used). If there is only additive noise but no missing pixels, then the model in (2) can be readily applied for simultaneous dictionary learning and image denoising. If there are both noise and missing pixels, instead of directly observing x i , we observe a subset of the pixels in each x i . Note that here  X  and {  X  i } i =1 ,N B , which are used to recover the original noise-free and complete image, are directly inferred from the data under test; one may also employ an appropriate training set D with which to learn a dictionary D offline, or for initialization of in situ learning. In denoising and inpainting studies of this type (see for example [6, 7] and references therein), it is often assumed that either the variance is known and used as a  X  X topping X  criteria, or that the sparsity level is pre-determined and fixed for all i  X  { 1 , N B } . While these may be practical in some applications, we feel it is more desirable to not make these assumptions. In (2) the noise precision (inverse variance),  X  , is assumed drawn from a non-informative gamma distribution, and a full posterior density function is inferred for  X  (and all other model parameters). In addition, the problems of addressing spatially nonuniform noise as well as nonuniform noise across color channels are of interest [7]; they are readily handled in the proposed model by drawing a separate precision  X  for each color channel in each B  X  B block, each of which is drawn from a shared gamma prior.
 The sparsity level of the representation in our model, i.e. , {k  X  i k 0 } i =1 ,N , is influenced by the parameters a and b in the beta prior in (2). Examining the posterior p (  X  k | X  )  X  Beta ( a/K + P i =1 z ik , b ( K  X  1) /K + N  X  P settings of a and b tend to be non-informative, especially in the case of sequential learning (dis-cussed further in Section 5). Therefore, the average sparsity level of the representation is inferred by the data itself and each sample x i has its own unique sparse representation based on the posterior, which renders much more flexibility than enforcing the same sparsity level for each sample. 3.2 Compressive sensing We consider CS in the manner employed in [12]. Assume our objective is to measure an image the x i directly, pixel-by-pixel, in CS we perform the projection measurement v i =  X  x i , where v i  X  &lt; N p , with N p representing the number of projections, and  X   X  &lt; N p  X  64 (assuming that x i is represented by a 64-dimensional vector). There are many (typically random) ways in which  X  may be constructed, with the reader referred to [24]. Our goal is to have N p 64 , thereby yielding compressive measurements. Based on the CS measurements { v i } i =1 ,N B , our objective is to recover { x Consider a potential dictionary  X  , as discussed in Section 2. It is assumed that for each of the { x i } i =1 ,N B from the image under test x i =  X   X  i + i , for sparse  X  i and relatively small error k i k 2 . The number of required projections N p needed for accurate estimation of  X  i is proportional to k  X  i k 0 [11], with this underscoring the desirability of learning a dictionary in which very sparse representations are manifested (as compared to using an  X  X ff-the-shelf X  wavelets or DCT basis). For CS inversion, the model in (2) is employed, and therefore the appropriate dictionary D is learned jointly while performing CS inversion, in situ on the image under test. When performing CS analy-function is therefore modified slightly).
 As discussed when presenting results, one may also learn the CS dictionary in advance, off-line, with appropriate training images (using the model in (2)). However, the unique opportunity for joint CS inversion and learning of an appropriate parsimonious dictionary is deemed to be a significant advantage, as it does not presuppose that one would know an appropriate training set in advance. The inpainting problem may be viewed as a special case of CS, in which each row of  X  corresponds to a delta function, locating a unique pixel on the image at which useful (unobscured) data are observed. Those pixels that are unobserved, or that are contaminated ( e.g. , by superposed text [7]) are not considered when inferring the  X  i and D . A CS camera designed around an inpainting construction has several advantages, from the standpoint of simplicity. As observed from the results in Section 5, an inpainting-based CS camera would simply observe a subset of the usual pixels, selected at random. For the applications discussed above, the { x i } i =1 ,N B come from the single image under test, and consequently there is underlying (spatial) structure that should ideally be exploited. Rather than re-writing the entire model in (2), we focus on the following equations in the hierarchy: z i  X  Q k =1 Bernoulli (  X  k ) , and  X   X  Q vectors, corresponding to different segments in the image. Since the number of mixture components is not known a priori , this mixture model is modeled via a Dirichlet process [18]. We may therefore employ, for i = 1 , . . . , N B ,  X   X  Q K k =1 Beta ( a/K, b ( K  X  1) /K ) , where the z i are drawn i.i.d. from G . In practice we imple-ment such DP constructions via a truncated stick-breaking representation [25], again retaining the conjugate-exponential structure of interest for analytic VB or Gibbs inference. In such an analysis we place a non-informative gamma prior on the precision  X  .
 The construction in (3) clusters the blocks, and therefore it imposes structure not constituted in the simpler model in (2). However, the DP still assumes that the members of { x i } i =1 ,N B are exchange-able. Space limitations preclude discussing this matter in detail here, but we have also considered replacement of the DP framework above with a probit stick-breaking process (PSBP) [19], which explicitly imposes that it is more likely for proximate blocks to be in the same cluster, relative to distant blocks. When presenting results, we show examples in which PSBP has been used, with its relative effectiveness compared to the simpler DP construction. The PSBP again retains full conjugate-exponential character within the hierarchy, of interest for efficient inference, as discussed above. For the denoising and inpainting results, we observed that the Gibbs sampler provided better perfor-mance than associated variational Bayesian inference. For denoising and inpainting we may exploit shifted versions of the data, which accelerates convergence substantially (discussed in detail be-low). Therefore, all denoising and inpainting results are based on efficient Gibbs sampling. For CS we cannot exploit shifted images, and therefore to achieve fast inversion variational Bayesian (VB) inference [22] is employed; for this application VB has proven to be quite effective, as discussed below. The same set of model hyper-parameters are used across all our denoising, inpainting and CS examples (no tuning was performed): all gamma priors are set as Gamma (10  X  6 , 10  X  6 ) , along the lines suggested in [2], and the beta distribution parameters are set with a = K and b = N/ 8 (many other settings of a and b yield similar results). 5.1 Denoising We consider denoising a 256  X  256 image, with comparison of the proposed approach to K-SVD [6] (for which the noise variance is assumed known and fixed); the true noise standard deviation is set at 15 , 25 and 50 in the examples below. We show results for three algorithms: ( i ) mismatched K-SVD (with noise standard deviation of 30), ( ii ) K-SVD when the standard deviation is properly matched, and ( iii ) the proposed BP approach. For ( iii ) a non-informative prior is placed on the noise precision, and the same BP model is run for all three noise levels (with the underlying noise levels inferred). The BP and K-SVD employed no a priori training data. In Figure 1 are shown the noisy images at the three different noise levels, as well as the reconstructions via BP and K-SVD. A preset large dictionary size K = 256 is used for both algorithms, and for the BP results we inferred that approximately M = 196, 128, and 34 dictionary elements were important for noise standard deviations 15, 25, and 50, respectively; the remaining elements of the dictionary were used less than 0.1% of the time. As seen within the bottom portion of the right part of Figure 1, the unused dictionary elements appear as random draws from the prior, since they are not used and hence influenced by the data.
 Note that K-SVD works well when the set noise variance is at or near truth, but the method is un-dermined by mismatch. The proposed BP approach is robust to changing noise levels. Quantitative performance is summarized in Table 1. The BP denoiser estimates a full posterior density func-tion on the noise standard deviation; for the examples considered here, the modes of the inferred standard-deviation posteriors were 15.57, 25.35, and 48.12, for true standard deviations 15, 25, and 50, respectively.
 To achieve these BP results, we employ a sequential implementation of the Gibbs sampler (a batch implementation converges to the same results but with higher computational cost); this is discussed in further detail below, when presenting inpainting results.
 Figure 1: Left: Representative denoising results, with the top through bottom rows corresponding to noise Table 1: Peak signal-to-reconstructed image measure (PSNR) for the data in Figure 1, for K-SVD [6] and the 5.2 Inpainting Our inpainting and denoising results were achieved by using the following sequential procedure. a new B  X  B block. Further, consider all B  X  B blocks with left-bottom pixels at { p + `B, j + Figure 2: Inpainting results. The curve shows the PSNR as a function of the B 2 = 64 Gibbs learning rounds. mB } X   X  ( p  X  1) { N y  X  B + 1 , j + mB } X   X  ( j  X  1) { p + `B, N x  X  B + 1 } for ` and m that satisfy p + `B  X  N y  X  B + 1 and j + mB  X  N x  X  B + 1 . This set of blocks is denoted data set D pj , and considering 1  X  p  X  B and 1  X  j  X  B , there are a total of B 2 such shifted data sets. In the first iteration of learning  X  , we employ the blocks in D 11 , and for this first round we initialize  X  and  X  i based on a singular value decomposition (SVD) of the blocks in D 11 (we achieved similar results when  X  was initialized randomly). We do several Gibbs iterations with D 11 and then stop the Gibbs algorithm, retaining the last sample of  X  and  X  i from the previous step. These  X  and  X  i are then used to initialize the Gibbs sampler in the second round, now applied to the B  X  B blocks in D 11  X  X  21 (for D 21 the neighboring  X  i is used for initialization). The Gibbs sampler is now run on this expanded data for several iterations, the last sample is retained, and the data set is augmented again. This is done B 2 = 64 times until at the end all shifted blocks are processed simultaneously. This sequential process may be viewed as a sequential Gibbs burn in, after which all of the shifted blocks are processed.
 Theoretically, one would expect to need thousands of Gibbs iterations to achieve convergence. How-ever, our experience is that even a single iteration in each of the above B 2 rounds yields good results. In Figure 2 we show the PSNR as a function of each of the B 2 = 64 rounds discussed above. For Gibbs rounds 16, 32 and 64 the corresponding PSNR values were 26.78 dB, 28.46 dB and 29.31 dB. For this example we used K = 256 . This example was considered in [7] (we obtained similar results for the  X  X ew Orleans X  image, also considered in [7]); the best results reported there were a PSNR of 29.65 dB. However, to achieve those results a training data set was employed for initialization [7]; the BP results are achieved with no a priori training data. Concerning computational costs, the in-painting and denoising algorithms scale linearly as a function of the block size, the dictionary size, the sparsity level, and the number of training samples; all results reported here were run efficiently in Matlab on PCs, with comparable costs as K-SVD. 5.3 Compressive sensing We consider a CS example, in which the image is divided into 8  X  8 patches, with these constituting the underlying data { x i } i =1 ,N B to be inferred. For each of the N B blocks, a vector of CS measure-ments v i =  X  x i is measured, where the number of projections per patch is N p , and the total number of CS projections is N p N B . In this example the elements of  X  were constructed randomly as draws from N (0 , 1) , but many other projection classes may be considered [11, 24]. Each x i is assumed represented in terms of a dictionary x i = D  X  i + i , and three constructions for D were considered: ( i ) a DCT expansion; ( ii ) learning of D using the beta process construction, using training images; ( iii ) using the beta process to perform joint CS inversion and learning of D . For ( ii ), the training data consisted of 4000 8  X  8 patches chosen at random from 100 images selected from the Microsoft database ( http://research.microsoft.com/en-us/projects/objectclassrecognition ). The dictionary was set to K = 256 , and the offline beta process inferred a dictionary of size M = 237 .
 Representative CS reconstruction results are shown in Figure 3, for a gray-scale version of the  X  X astle X  image. The inversion results at left are based on a learned dictionary; except for the  X  X nline BP X  results, all of these results employ the same dictionary D learned off-line as above, and the algorithms are distinguished by different ways of estimating {  X  i } i =1 ,N B . A range of CS-inversion algorithms are considered from the literature, and several BP-based constructions are considered as well for CS inversion. The online BP results are quite competitive with those inferred off-line. One also notes that the results based on a learned dictionary (left in Figure 3) are markedly better than those based on the DCT (right in Figure 3); similar results were achieved when the DCT was replaced by a wavelet representation. For the DCT-based results, note that the DP-and PSBP-based BP CS inversion results are significantly better than those of all other CS inversion algorithms. The results reported here are consistent with tests we performed using over 100 images from the aforementioned Microsoft database, not reported here in detail for brevity.
 Note that CS inversion using the DP-based BP algorithm (as discussed in Section 4) yield the best results, significantly better than BP results not based on the DP, and better than all competing CS inversion algorithms (for both learned dictionaries and the DCT). The DP-based results are very similar to those generated by the probit stick-breaking process (PSBP) [19], which enforces spatial information more explicitly; this suggests that the simpler DP-based results are adequate, at least for the wide class of examples considered. Note that we also considered the DP and PSBP for the denoising and inpaiting examples above (those results were omitted, for brevity). The DP and PSBP denoising and inpainting results were similar to BP results without DP/PSBP (those presented above); this is attributed to the fact that when performing denoising/inpainting we may consider many shifted versions of the same image (as discussed when presenting the inpainting results). Concerning computational costs, all CS inversions were run efficiently on PCs, with the specifics computational times dictated by the detailed Matlab implementation and the machine run on. A rough ranking of the computational speeds, from fastest to slowest, is as follows: StOMP-CFAR, Fast BCS, OMP, BP, LARS/Lasso, Online BP, DP BP, PSBP BP, VB BCS, Basis Pursuit; in this list, algorithms BP through Basis Pursuits have approximately the same computational costs. The DP-based BP CS inversion algorithm scales as O ( N B  X  N p  X  B 2 ) . The non-parametric beta process has been presented for dictionary learning with the goal of image denoising, inpainting and compressive sensing, with very encouraging results relative to the state of the art. The framework may also be applied to joint compression-classification tasks. In the context of noisy underlying data, the noise variance need not be known in advance, and it need not be spatially uniform. The proposed formulation also allows unique opportunities to leverage known structure in the data, such as relative spatial locations within an image; this framework was used to achieve marked improvements in CS-inversion quality.
 Acknowledgement The research reported here was supported in part by ARO, AFOSR, DOE, NGA and ONR. [1] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines . Cambridge [2] M. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine [3] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical [4] B.A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy [5] M. Aharon, M. Elad, and A. M. Bruckstein. K-SVD: An algorithm for designing overcomplete [6] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over [7] J. Mairal, M. Elad, and G. Sapiro. Sparse representation for color image restoration. IEEE [8] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. In [9] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Supervised dictionary learning. In [10] M. Ranzato, C. Poultney, S. Chopra, and Y. Lecun. Efficient learning of sparse representations [11] E. Cand ` es and T. Tao. Near-optimal signal recovery from random projections: universal en-[12] J.M. Duarte-Carvajalino and G. Sapiro. Learning to sense sparse signals: Simultaneous sensing [13] J. Wright, A.Y. Yang, A. Ganesh, S.S. Sastry, and Y. Ma. Robust face recognition via sparse [14] S. Ji, Y. Xue, and L. Carin. Bayesian compressive sensing. IEEE Trans. Signal Processing , [15] R. Raina, A. Battle, H. Lee, B. Packer, and A.Y. Ng. Self-taught learning: transfer learning [16] R. Thibaux and M.I. Jordan. Hierarchical beta processes and the indian buffet process. In Proc. [17] J. Paisley and L. Carin. Nonparametric factor analysis with beta process priors. In Proc. [18] T. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics , 1, [19] A. Rodriguez and D.B. Dunson. Nonparametric bayesian models through probit stickbreaking [20] D. Knowles and Z. Ghahramani. Infinite sparse factor analysis and infinite independent com-[21] P. Rai and H. Daum  X  e III. The infinite hierarchical factor regression model. In Proc. Neural [22] M.J. Beal. Variational Algorithms for Approximate Bayesian Inference . PhD thesis, Gatsby [23] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian [24] R.G. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine , 24, 2007. [25] J. Sethuraman. A constructive definition of Dirichlet priors. Statistica Sinica , 4, 1994.
