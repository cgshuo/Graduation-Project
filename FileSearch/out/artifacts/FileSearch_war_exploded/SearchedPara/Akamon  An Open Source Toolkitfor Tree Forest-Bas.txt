 Syntax-based statistical machine translation (SMT) systems have achieved promising improvements in recent years. Depending on the type of input, the systems are divided into two categories: string-based systems whose input is a string to be simul-taneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006; Shen et al., 2008), and tree/forest-based sys-tems whose input is already a parse tree or a packed forest to be directly converted into a target tree or string (Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009; Wu et al., 2010; Wu et al., 2011a).
Depending on whether or not parsers are explic-itly used for obtaining linguistically annotated data during training, the systems are also divided into two categories: formally syntax-based systems that do not use additional parsers (Wu, 1997; Chiang, 2005; Xiong et al., 2006), and linguistically syntax-based systems that use PCFG parsers (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; Mi et al., 2008; Mi and Huang, 2008; Zhang et al., 2009), HPSG parsers (Wu et al., 2010; Wu et al., 2011a), or dependency parsers (Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). A classification 1 of syntax-based SMT systems is shown in Table 1.
Translation rules can be extracted from aligned string-string (Chiang, 2005), tree-tree (Ding and Palmer, 2005) and tree/forest-string (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2011a) data structures. Leveraging structural and linguis-tic information from parse trees/forests, the latter two structures are believed to be better than their string-string counterparts in handling non-local re-ordering, and have achieved promising translation results. Moreover, the tree/forest-string structure is more widely used than the tree-tree structure, pre-sumably because using two parsers on the source and target languages is subject to more problems than making use of a parser on one language, such as the shortage of high precision/recall parsers for languages other than English, compound parse error rates, and inconsistency of errors. In Table 1, note that tree-to-string rules are generic and applicable to many syntax-based models such as tree/forest-to-string models and string-to-tree model.

However, few tree/forest-to-string systems have been made open source and this makes it diffi-cult and time-consuming to testify and follow exist-ing proposals involved in recently published papers. The Akamon system 2 , written in Java and follow-ing the tree/forest-to-string research direction, im-plements all of the algorithms for both tree-to-string translation rule extraction (Galley et al., 2004; Mi and Huang, 2008; Wu et al., 2010; Wu et al., 2011a) and tree/forest-based decoding (Liu et al., 2006; Mi et al., 2008). We hope this system will help re-lated researchers to catch up with the achievements of tree/forest-based translations in the past several years without re-implementing the systems or gen-eral algorithms from scratch. Limited by the successful parsing rate and coverage of linguistic phrases, Akamon currently achieves comparable translation accuracies compared with the most frequently used SMT baseline system, Moses (Koehn et al., 2007). Table 2 shows the auto-matic translation accuracies (case-sensitive) of Aka-mon and Moses. Besides BLEU and NIST score, we further list RIBES score 3 , , i.e., the software imple-mentation of Normalized Kendall X  X   X  as proposed by (Isozaki et al., 2010a) to automatically evaluate the translation between distant language pairs based on rank correlation coefficients and significantly penal-izes word order mistakes.
 In this table, Akamon-Forest differs from Akamon-Comb by using different configurations: Akamon-Forest used only 2/3 of the total training data (limited by the experiment environments and time). Akamon-Comb represents the system com-bination result by combining Akamon-Forest and other phrase-based SMT systems, which made use of pre-ordering methods of head finalization as de-scribed in (Isozaki et al., 2010b) and used the total 3 million training data. The detail of the pre-ordering approach and the combination method can be found in (Sudoh et al., 2011) and (Duh et al., 2011).
Also, Moses (hierarchical) stands for the hi-erarchical phrase-based SMT system and Moses (phrase) stands for the flat phrase-based SMT sys-tem. For intuitive comparison (note that the result achieved by Google is only for reference and not a comparison, since it uses a different and unknown training data) and following (Goto et al., 2011), the scores achieved by using the Google online transla-tion system 4 are also listed in this table.
Here is a brief description of Akamon X  X  main fea-tures:  X  multiple-thread forest-based decoding: Aka- X  language models: Akamon can make use of  X  pruning: traditional beam-pruning and cube- X  MERT: Akamon has its own MERT module  X  translation rule extraction: as former men-Figure 1 shows the training and tuning progress of the Akamon system. Given original bilingual par-allel corpora, we first tokenize and lowercase the source and target sentences (e.g., word segmentation of Chinese and Japanese, punctuation segmentation of English).

The pre-processed monolingual sentences will be used by SRILM (Stolcke, 2002) or BerkeleyLM (Pauls and Klein, 2011) to train a n-gram language model. In addition, we filter out too long sentences here, i.e., only relatively short sentence pairs will be used to train word alignments. Then, we can use GIZA++ (Och and Ney, 2003) and symmetric strate-gies, such as grow-diag-final (Koehn et al., 2007), on the tokenized parallel corpus to obtain a word-aligned parallel corpus.

The source sentence and its packed forest, the tar-get sentence, and the word alignment are used for tree-to-string translation rule extraction. Since a 1-best tree is a special case of a packed forest, we will focus on using the term  X  X orest X  in the continuing discussion. Then, taking the target language model, the rule set, and the preprocessed development set as inputs, we perform MERT on the decoder to tune the weights of the features.

The Akamon forest-to-string system includes the decoding algorithm and the rule extraction algorithm described in (Mi et al., 2008; Mi and Huang, 2008). In Akamon, we support the usage of deep syn-tactic structures for obtaining fine-grained transla-tion rules as described in our former work (Wu et al., 2010) 7 . Similarly, Enju 8 , a state-of-the-art and freely available HPSG parser for English, can be used to generate packed parse forests for source sentences 9 . Deep syntactic structures are included in the HPSG trees/forests, which includes a fine-grained description of the syntactic property and a semantic representation of the sentence. We extract fine-grained rules from aligned HPSG forest-string pairs and use them in the forest-to-string decoder. The detailed algorithms can be found in (Wu et al., 2010; Wu et al., 2011a). Note that, in Akamon, we also provide the codes for generating HPSG forests from Enju.

Head-driven phrase structure grammar (HPSG) is a lexicalist grammar framework. In HPSG, linguis-tic entities such as words and phrases are represented by a data structure called a sign . A sign gives a factored representation of the syntactic features of a word/phrase, as well as a representation of their semantic content. Phrases and words represented by signs are composed into larger phrases by applica-tions of schemata . The semantic representation of the new phrase is calculated at the same time. As such, an HPSG parse tree/forest can be considered as a tree/forest of signs (c.f. the HPSG forest in Fig-ure 2 in (Wu et al., 2010)).

An HPSG parse tree/forest has two attractive properties as a representation of a source sentence in syntax-based SMT. First, we can carefully control the condition of the application of a translation rule by exploiting the fine-grained syntactic description in the source parse tree/forest, as well as those in the translation rules. Second, we can identify sub-trees in a parse tree/forest that correspond to basic units of the semantics, namely sub-trees covering a pred-icate and its arguments, by using the semantic rep-resentation given in the signs. Extraction of trans-lation rules based on such semantically-connected sub-trees is expected to give a compact and effective set of translation rules.

A sign in the HPSG tree/forest is represented by a typed feature structure (TFS) (Carpenter, 1992). A TFS is a directed-acyclic graph (DAG) wherein the edges are labeled with feature names and the nodes (feature values) are typed. In the original HPSG for-malism, the types are defined in a hierarchy and the DAG can have arbitrary shape (e.g., it can be of any depth). We however use a simplified form of TFS, for simplicity of the algorithms. In the simplified form, a TFS is converted to a (flat) set of pairs of feature names and their values. Table 3 lists the fea-tures used in our system, which are a subset of those in the original output from Enju.

In the Enju English HPSG grammar (Miyao et al., 2003) used in our system, the semantic content of a sentence/phrase is represented by a predicate-argument structure (PAS). Figure 2 shows the PAS of a simple sentence,  X  John killed Mary  X , and a more complex PAS for another sentence,  X  She ignored the fact that I wanted to dispute  X , which is adopted from (Miyao et al., 2003). In an HPSG tree/forest, each leaf node generally introduces a predicate, which is represented by the pair of LEXENTRY (lexical entry) feature and PRED (predicate type) feature. The arguments of a predicate are designated by the pointers from the ARG  X  x  X  features in a leaf node to non-terminal nodes. Consequently, Akamon in-cludes the algorithm for extracting compact com-posed rules from these PASs which further lead to a significant fast tree-to-string decoder. This is be-cause it is not necessary to exhaustively generate the subtrees for all the tree nodes for rule matching any more. Limited by space, we suggest the readers to refer to our former work (Wu et al., 2010; Wu et al., 2011a) for the experimental results, including the training and decoding time using standard English-to-Japanese corpora, by using deep syntactic struc-tures. In the demonstration, we would like to provide a brief tutorial on:  X  describing the format of the packed forest for a  X  the training script on translation rule extraction,  X  the MERT script on feature weight tuning on a  X  the decoding script on a test set.

Based on Akamon, there are a lot of interesting directions left to be updated in a relatively fast way in the near future, such as:  X  integrate target dependency structures, espe- X  better pruning strategies for the input packed  X  derivation-based combination of using other  X  taking other evaluation metrics as the opti-We thank Yusuke Miyao and Naoaki Okazaki for their invaluable help and the anonymous reviewers for their comments and suggestions.

