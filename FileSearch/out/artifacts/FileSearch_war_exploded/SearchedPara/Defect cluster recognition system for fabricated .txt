 1. Introduction
Advances in semiconductor technology and design have been the driving forces behind the successful progress of the electro-nics industrial sector. Over the past few decades, the semicon-ductor industry has evolved into one of the critical foundations and contributors of the world economy, recording a staggering 100% growth from the year 1996 to 2011( Semiconductor Industry
Association (SIA), 2011) . Today, the semiconductor industry is generating revenues of over USD $25.5 billion and is expected to continue ascending by 10.5% annually between 2011 and 2013.
Such rapid growth is going on hand-in-hand with a rapid increase of design and manufacturing complexity pushing the physical limits of semiconductor production techniques ( International
Technology Roadmap for Semiconductors, 2009 ). Reaching the nanoscale half-pitch dimensions alters the type and distribution of defects. This causes defects that were previously benign for microscale technology to manifest as killer defects.

The International Technology Roadmap for Semiconductor (ITRS), which is the world X  X  authority on the semiconductor industry, has identified the detection of systematic failures as one of the top challenges in the next generation of semiconductor products ( International Technology Roadmap for Semiconductors, 2009 ).
Production data, in particular, was singled out as one of the crucial elements in aiding the process feedback loop.

With rapid developments in computer technology and with the availability of low-cost storage devices, data logging has become a commercially feasible task and is currently a standard procedure in almost all industries. The data logging ensures that virtually all industrial process-related data are always ready for extraction and analysis. Yet there is still much knowledge buried in the huge data collection, which is waiting to be discovered. Furthermore, the knowledge banks are continually enriched with new data being included and compiled each day. Thus, it is envisage that by applying new appropriate analysis tools further improvements in the semiconductor manufacturing could be achieved.

The production of modern microelectronic devices has an important feature that makes it significantly different from other manufacturing processes. The semiconductor wafer fabrication results in large number of Integrated Circuits (ICs) produced simul-taneously in a multitude of sequential fabrication steps on a single piece of a silicon substrate ( Fig. 1 ). Every fabricated wafer contains 100s to 1000s of devices, which are then tested, singulated (i.e., individually removed from the matrix of the products on the processed wafer) and packaged into individual protection casings thus making so-called IC chips . This unique characteristic whereby each individual device is produced together with other devices on a single piece of wafer allows production dataset to be physically and meaningfully interpreted as a two-dimensional  X  X  X mage X  X .
Devices that fail due to an identifiable root-cause during the wafer fabrication process have a tendency to form unique and systematic patterns on the fabr icated wafers ( International Technology Roadmap for Semiconductors, 2009 ; Zhao and Cui, 2008 ; Ooi et al., 2011 ). These are known as defect clusters. Fig. 2 shows two examples of raw production test data, whereby Fig. 2 (a) shows random device failure (no identifiable defect clusters) and Fig. 2 (b) shows a defect cluster (a group of failing devices clustered together) on the bottom-left edge of the wafer. Defect clusters are no rmally located around a specific location and are process-related. There are six local defect patterns identified and examined in this research, which are Bull X  X -Eye, Blob,
Edge, Ring, Line and Hat ( Fig. 3 ). Their formal description is provided in Ooi et al. (2011 ). Thus, the problem of detecting systematic failures can be simplified into detecting and identifying these defect clusters from the production test data. 2. Intelligent defect cluster recognition system for fabricated semiconductor wafers
Developing a robust and accurate defect cluster recognition system for semiconductor wafers based on production test data is a non-trivial task. Unfortunately standard pattern recognition techniques do not work well for the semiconductor wafer based  X  X  X mages X  X  due to several object-specific issues. These issues are discussed below Section 2.1 . Section 2.2 outlines the aim of this research and the main contributions of the paper. 2.1. Challenges in developing an intelligent system for defect cluster recognition
The main factors that must be overcome in order to develop a robust-yet-accurate defect cluster recognition system for semi-conductor wafer datasets are listed below (they will be further elaborated below in individual subsections):
Variations in defect cluster size, shape, location and orienta-tion on the wafer for the same class of defects Non-symmetrical geometry Low signal-to-noise ratio Insufficient quality historical data for training
A-priori unknown best feature sets and/or best classifiers 2.1.1. Variations of defect cluster size, shape, location and orientation for the same class of defects
It is difficult to select versatile feature vectors with strong distinguishing capabilities that describe the different classes of defects accurately. Fig. 4 shows two examples of defects, whereby Fig. 4 (a) X (d) are examples of Blob defects, while Fig. 4 (e) X (h) are
Line defects. The feature vector must be sufficiently flexible to account for the variations in defect sizes, shapes and locations on the wafer in the Blob defect examples, as well as the orientation in the Line defect examples. Thus, direct application of principle component analysis or template matching (e.g., techniques dis-cussed in Jain et al. (2001 )) without some form of transformation is clearly insufficient for this application. 2.1.2. Non-symmetrical geometry
Although the semiconductor wafer dataset can be viewed in a two-dimensional space, it is still important to remember that each data point is an actual device being produced. Thus tradi-tional normalisation that graphically rotates the wafer map does not give meaningful results. Additionally, the location of each die on the wafer does not form a square dataset, but rather a circular pattern ( Fig. 5 (a)). The wafer perimeter (valid die locations at the wafer edges) varies for different devices and different processes; the data points usually do not form a perfectly symmetrical pattern (see Fig. 5 (b)). Thus, many classical algorithms would fail if applied to this subject area. 2.1.3. Low signal-to-noise ratio
In most of the previous examples, only  X  X  X lean X  X  defect patterns were shown. In actuality, the real semiconductor production test dataset is more similar to that shown in Fig. 6 . Fabricated semi-conductor wafers have a relatively small number of points in the dataset (ranging from 100 to 10,000 dies/wafer). Additionally, a high proportion of these small datasets (10% X 50%) are random failures, which are interpreted as a  X  X  X oise X  X . The  X  X  X oise X  X  level differs depending on the manufacturing yield, which in turn depends on the die and wafer sizes, employed fabrication tech-nology and other factors. Thus the failure distribution varies between different types of devices, different implementation technologies, etc.

It is important to note that clustering algorithms in pattern recognition and image processing have been developed for large numbers of pixel count (or points). With a small data count and high noise level, it is difficult to fine-tune most classical algo-rithms such that the Type I / II errors (false alarm and false rejection rates) would both be within acceptable limits. Furthermore, selecting an optimum threshold to differentiate between cluster and non-cluster regions (normally required in most clustering algorithms) is a non-trivial task when each device type has different numbers of dies per wafer, different configurations on a two-dimensional space and completely different overall failure rates. A preliminary investigative study performed in Ooi et al. (2011 ) shows that many clustering algorithms such as k-means , k-medoid , mean-shift , Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) and Clustering Using Representatives (CURE) do not provide meaningful results when applied to the research problem under consideration.

Fig. 6 shows an example of the Ring defect type for the same device type with three different manufacturing yields (40%, 70% and 90%). It can be seen that the noise level at 40% yield is very high, yet any applied intelligent recognition system must be capable of recognising the obvious defect at the wafer edges.
Conversely, the noise level is low at 90% yield, but this brings a new problem: the data points that form the defect cluster are quite sparse. Yet, the defect recognition system must still be capable of recognising this defect pattern once it occurs. 2.1.4. Insufficient quality historical data
The accuracy of any classifier depends greatly on the com-pleteness of the training set. For the application under discus-sion, this translates to getting sufficient amount of high quality historical production data for training a classifier to account for the issues highlighted above such as variations in defect cluster size, shape, orientation and location with different manufactur-ing yield and noise levels. Unfortunately, even though semi-conductor devices are manufactured in high volumes, it was observed that there is normally a lack of sufficient number or appropriate selections of good quality training samples available in the historical production test data-logs to obtain a stable statistical inference for a chosen classifier ( Kameyama and Kosugi, 1999 ).
Semiconductor manufacturers produce thousands of different product types annually to cope up with the increasing market demands. At the same time new innovations in design and fabrication are routinely achieved and implemented. This results in process changes being made on a monthly, if not weekly, basis ( Hsieh et al., 1999 ). There are also devices that are being produced in small volumes for niche applications. Thus, production life-cycles could be rather short for some semiconductor products leading to new more advanced device types being put into production. Furthermore, some defect clusters may not have yet been encountered during production (thus there will be no historical log on such cases), though this does not mean that they would not occur in future! The combined effect of these factors complicates the developmental efforts for any intelligent system ( Hsieh et al., 1999 ).

In general, it can be observed that approaches attempting to recognise the cluster type directly from the raw semiconductor production data encounter huge problems during implementation due to the above mentioned factors ( Ooi et al., 2011 ). To over-come this problem, a defect cluster simulator can be used to simulate the training set for the classifier. However, there are two important decisions are to be considered when using such an approach: 1. Should the training data contain  X  X  X oise X  X  to emulate the actual raw data? And if so, how much noise? 2. Should the training data contain only pure clusters, with the raw data filtered to extract a  X  X  X oiseless X  X  defect cluster?
And if so, what filtering technique should be applied to the raw data? 2.1.5. Best feature set or most suitable classifier is not known a-priori
A good feature set is an optimised set of attributes that best distinguishes between different classes ( Kononenko and Kukar, 2007 ). A good classifier is one that uses the feature set to obtain the most accurate grouping of data. A good classifier is generally required to test the efficacy of the features. Yet a good feature set is required to generate a good classifier ( Kononenko and
Kukar, 2007 ). This is a  X  X  X hicken-and-egg X  X  problem for most classi-fication algorithms, since neither the best feature set nor the best classifier is known a-priori .

When the salient features are unknown, an extremely large set of features is used to train the classifier in a hope of obtaining accurate classification r esults. Yet, the reverse is often desirable, whereby efficient and optimal machine learning and recognition are achieved through careful selection of an optimal subset of salient featur es. This is because the memory requirements and computational time are expected to be kept low without compromising the accuracy of the classifier ( Kononenko and Kukar, 2007 ; Jain et al., 2001 ; Xu and Wunsch
II, 2009 ). Thus, the chosen features should ideally be robust with respect to noise, easy to obtain and simple to interpret ( Xu and Wunsch II, 2009 ).

There is still no known and accepted feature set or classifier that best suit this application. This leads to an open question in the developmental work on how to best begin developing the defect recognition system: feature first or classifier first? 2.2. Research aims and contributions
The ultimate aim of this research is to develop an optimal and accurate defect cluster recognition system for application on the semiconductor wafer production data. In order to achieve this aim, while basing on the earlier results in defect clustering ( Ooi et al., 2011 ), an experimental study must be performed to address the needs for defect classification. The goal of such a study is to determine the following: 1. The optimal feature set for chosen classifier (as discussed above in Section 2.1.1 ) 2. The best feature extraction method and type of classifier (as outlined in Sections 2.1.1, 2.1.2 and 2.1.5 ) 3. The classifier training method while taking into account the lack of historical data and high level of noise in the relatively small count (as presented in Sections 2.1.3 and 2.1.4 )
Within the reported research, the outlined experimental study was conducted on large-scale industrial data of several million units of ICs from five types of semiconductor products that are in the mainstream high-volume production of one of the world X  X  leading semiconductor device manufacturers. The experimental results are presented and discussed in Section 4 . In order to follow this study, Section 3 provides the necessary theoretical foundation on suitable feature extraction methods and classifiers employed in the research. 3. Introduction into feature extraction, selection and classification
Feature extraction , selection and classification are inter-depen-dent tasks. Feature selection refers to algorithms that choose a small and optimal subset from a larger input set of features. It is normally performed during training of the classifier. Feature extraction , on the other hand, refers to algorithms that generate new and hopefully more useful features from the original set by applying a transformation ( Xu and Wunsch II, 2009; Jain et al., 2001 ). Extraction of features may provide more discriminative ability compared to the best of the original features. However, transformation normally presents a more abstract represen-tation of the data, thus the new feature may no longer be physically interpretable. Feature extraction normally precedes the feature selection stage. Finally, classification is a process of identifying a group (sub-population) to which particular obser-vations belong to. 3.1. Feature extraction and selection
In the field of pattern recognition, dimension reduction tech-niques such as the principal component analysis , factor analysis , linear discriminant analysis and projection pursuit are highly popular feature extraction methods ( Jain et al., 2001 ). These techniques work well only if the defects in a same class are geometrically well aligned. Rotation of the wafer map to align defect cluster is not feasible due to non-symmetrical geometry and relatively large die size. Besides, such a rotation would cause significant number of dies to be misaligned.

Since semiconductor wafers are approximately circular in shape this characteristic can be considered as invariant in its rotational properties. Thus, potentially suitable methods to extract this particular characteristic are the Polar Fourier Trans-form (PFT) and Rotational Moment Invariant (RMI), which are discussed below in Sections 3.1.1 and 3.1.2 , respectively. In addition, Section 3.1.3 provides the list of geometrical features to account for the variations in shape, size and defect location. 3.1.1. Polar Fourier Transform The Polar Fourier Transform is similar to the traditional Fourier Transform , except that it considers the frequency spectrum in polar coordinates ( Averbuch et al., 2006 ). Thus, it is invariant towards rotational properties. The polar grid of frequencies inside the concentric circle is defined as x p , q  X f x w  X  p , q , x below describes the discrete PFT operation, which includes a set of the samples F  X  x p , q  X  with sample points governed by Eq. (2) ( Averbuch et al., 2006 ).

F  X  x  X  X  x p , q  X   X  p p N cos p q 2 N x p , q  X   X  p p N sin p q 2 N ()
Fig. 7 shows an example of Discrete Polar Fourier Transform (DPFT) application on a line defect pattern whereby (a) and (b) show the before and after effect, respectively. Comparing wafers
A X  X , it can be seen that DPFT maps any rotational variances as a shift along the x -axis. Thus to eliminate any rotational variances between different line defect patterns, the peak of the frequency is centred on the x -axis. In this manner, a similar signature is obtained for the same defect pattern, as shown in Fig. 7 (c). Fig. 8 shows the results of applying DPFT on the six different types of defect clusters afterthe peak centring.

Since each defect cluster type has a unique and identifiable frequency domain signature, DPFT can be considered as a possible feature extraction method for classification of the semiconductor defect clusters. However, it is important to note that the DPFT does not consider any translational property of the local defect cluster patterns. Therefore, additional geometric features should be considered for inclusion into the feature set to increase the classification accuracy. These are discussed below in Section 3.1.3 .
The frequency signature for each defect cluster is obtained by calculating the eigenvectors using Principle Component Analysis  X  PCA ( Jolliffe, 2002 ). It is important to note that the conventional
PCA calculation is a lengthy process because the DPFT application leads to a high number of feature dimensions after transfor-mation. Thus, a two-directional approximation called ( 2D ) ( Zhang and Zhou, 2005 ) is used instead, as it offers a higher computational speed without significant loss in a calculation accuracy. 3.1.2. Rotational moment invariants
Moment functions were first introduced in 1962 by Hu, who employed the results of the theory of algebraic and geometric moments to derive the seven famous invariants to the rotation of two-dimensional objects which he used for automatic char-acter recognition ( Flusser, 2000 ; Flusser and Suk, 2006 ). These are simple properties, which are found via image moments.
They include the area or total intensity , its centroid and orientation information. Since then, moment invariants have become a classical tool in pattern recognition and object classification ( Mukundan and Ramakrishnan, 1998 ).

A moment function F pq of the order ( p  X  q ) for a general two-dimensional function f ( x , y ) can be given as Eq. (3), where z denotes the image region of the x X  X  plane, which is the domain of the function f ( x , y )( Mukundan and Ramakrishnan, 1998 );
C ( x , y ) is the moment weighting kernel or the basis set and is a continuous function of ( x , y )in z , the indices p and q usually denote the degrees of the coordinates x and y , respectively. Hu X  X  invariants are listed in Flusser (2000 ).

F  X 
Moment invariants with respect to image rotation can be easily derived from the complex moments. In particular, Rotational
Moment Invariant (RMI) is a good tool for classifying defect clusters on semiconductor wafers because it extracts the same features for the same type of cluster regardless of the rotational differences.
However, availability of Hu X  X  invariant is very restrictive for a given order. Research paper ( Flusser, 2000 ) proposes to use complex moments to generate more general rotational invariants, which include Hu X  X  invariant as special cases. The complex moment c the order ( p  X  q ) is an image moment function f ( x , y )asshownin
Eq. (4) where i is an imaginary term. Each complex moment can be expressed in terms of geometric moments m pq in Cartesian or Polar coordinates ( Flusser, 2000 ): c  X  c  X  c
According to Flusser (2000 ), if n Z 1 and k i , p i and q are non-negative numbers such that R n i  X  1 k i  X  p i q i Eq. (7) is invariant to rotation.
 I  X 
Application of Eqs. (4) and (7) generates a complex number which is the RMI feature. There is a large number of possible invariance property. However not all combinations are equally useful. Discretisation of the integration operation in Eq. (4) negatively impacts the theoretical invariance property and causes perturbation in the resultant RMI feature. Certain combinations may be more sensitive to the discretisation errors. In this study p , p 2 , q 1 , q 2 A {0.67, 0.79, 0.93, 1.00, 1.10, 1.30, 1.54, 1.82, 2.00, 2.15, 2.54, 3.00} were used because they are almost equally spaced numbers between the range of 0.67 (lower limit) and 3 (upper limit). These limits have been empirically selected to limit the dynamic range of the RMI value to prevent numerical instability. If no limits are enforced, the RMI value may be too large for representation even when using double precision num-bers. Indeed, there are 20,736 possible combinations using the set of p 1 , p 2 , q 1 , q 2 given above. Further restrictions are placed on k 1 and k 2 such that k 1  X  X  p 2 q 2 = p 1 q 1 p 2  X  q k  X  X  p 1 q 1 = p 1 q 1 p 2  X  q 2  X  to prevent numerical singularity during calculation. Apart from discretisation error, the geome-trical characteristics of the wafer (imperfect circle) and target defect class also affect the robustness of RMI features. Table 1 shows several typical defect clusters and corresponding rotated version. It is clear that the cluster rotation is not exact due to the non-ideal wafer geometry (as mentioned earlier the semiconduc-tor wafer is almost never a perfect circle). Even with the restric-tion of the upper and lower limits, the RMI value can go up to 10 4 range. Four arbitrarily chosen RMI are plotted in Table 2 . The results show that despite various sources of error, the RMI of the rotated cluster still produces a similar complex number. In addition, cross validating various RMI values yields reliable classification of a given cluster. 3.1.3. Geometrical features
It is important to note that the PFT and RMI features discussed in Sections 3.1.1 and 3.1.2 do not consider any translational property of the defect clusters. These properties are necessary to distinguish between different defect classes. Thus, Table 3 provides a set of geometric-based rules that are normally used to distinguish between the types of defect clusters for inclusion into the feature set so to increase the classification accuracy ( Guise et al., 2002 ). 3.2. Classification algorithms
Classification is the most popular application of all the machine learning methods. Classifiers utilise a mapping function to link the features to the respective class space ( Kononenko and Kukar, 2007 ). The mapping function can be either given to the classifier or, more commonly, it is learnt from a given set of high quality training data. Different classifiers have different ways to repre-sent the learning and mapping algorithms. Table 4 summarises the common classification methods in machine learning.
The semiconductor wafer datasets used in the reported research are non-linear with multi-classes . Thus Nearest neighbours , Support vector machines and Discriminant functions are typically unsuitable forthisapplication. Neural network classifiers were initially con-sidered. However, the neural network classification rule sets are not transparent to the user. This poses a serious problem in the industrial application. If there are any inaccuracies in classification, there is no method to determine their exact root-cause which impedes process improvement efforts. Therefore the use of this tool in the manufacturing process would not satisfy the Six-Sigma quality audits (Pande, et al., 2000), which are performed regularly by both the company and their clients. Hybrid functions are very complex to implement, and should only be considered if all other classification methods fail to provide suitable results. It is a general rule-of-thumb to use the  X  X  Occam X  X  Razor  X  X , principle stating that given several classifiers with the same training error, the simpler classifier is more likely to generalise better.

Thus for this particular application, the suitable classifiers are the Bayesian classifier and variations of Decision trees which are: Top X  X own Induction Decision Trees (TDIDT), Bagging on TDIDT , and Boosting on the Alternating Decision Tree . Section 4 provides the experimental flow to determine the most suitable classifier for this application. To aid in reader comprehension, the following subsections provide some theory on the selected classifiers. 3.2.1. Bayesian classifier The Bayesian classifier is a simple statistical classifier based on Bayes X  theorem ( Bramer, 2007 ). Assuming that each class is C k  X  [1, y , number of classes ] and n is a number of features F , the Bayes rule is written as Eqs. (8) and (9). The probability that an instance belongs to a particular class C k given a combination of features ( F 1 , F 2 , y , F n ) is shown in Eq. (10). P  X  C k , F 1 , F 2 , ... , F n  X  X  P  X  C k  X  P  X  F 1 , F 2 P  X  C k , F 1 , F 2 , ... , F n  X  X  P  X  F 1 , F 2 , ... , F
P  X  C 9
F 1 , F 2 , ... , F n  X  X  P  X  C k  X  P  X  F
The prior probability of an instance belonging to class is P ( C
It is calculated even before any of its features are obtained ( Bramer, 2007 ). Another prior probability is P ( F 1 , F data. These two priors are normally estimated from historical datasets. The priors will normally help to adjust the prediction of class for better accuracy. For example, if historically 90% of the observed defect clusters belong to Edge defect pattern and 10% belongs to Bull X  X  Eye, the prior of Edge cluster will be 0.9 and the prior of Bull X  X  eye cluster 0.1. This will increase the likelihood that a cluster will be categorised as an Edge class. The Bayesian classifier can be readily used with discrete features. For contin-uous features, discretisation must first be performed, thus some discretisation error will inevitably be introduced into the system ( Bramer, 2007 ). 3.2.2. Top X  X own Induction Decision Trees
Decision tree is a flowchart-like tree structure that consists of internal nodes, branches and leaves (or terminal nodes)( Bramer, 2007 ). Each path starts at the topmost node (also known as the root node ). Conditions are evaluated at each internal node, which corresponds to a test on an attribute. The outcome of the test is represented by each branch, which ends in a tree leaf. Each leaf corresponds to a discrete class label or decision rule ( Kononenko and Kukar, 2007 ). A good choice of feature or attribute is absolutely vital for high accuracy in prediction. Some features are far more informative than others, and thus the best attribute must be determined by a measure of impurity ( Kononenko and Kukar, 2007 ). This research uses entropy ( Quinlan, 1986 )to measure the impurity. It is an information-theoretic measure of the uncertainty in the training set in the presence of one or more possible classification. Decision trees result in a symbolic and logical representation of the classification function, which nor-mally conforms to a physical knowledge of the problem. Decision trees generally do not make any pre-determined assumptions on the statistical distribution of the dataset. This allows the features to be selected and used in an independent manner. The biggest problem for decision trees is the empty or null leaf phenomena ( Kononenko and Kukar, 2007; Bramer, 2007 ), where there is a valid path with no corresponding learning examples. This results in an unclassified instance. 3.2.3. Bagging on TDIDT
The natural extension of TDIDT is to apply bagging paradigm to reduce the problem of overlearning or over-fitting of the data ( Polikar, 2006 ). Bagging attempts to improve classifier perfor-mance by generating many different classifiers. This is achieved by training them on slightly different training sets as illustrated in
Fig. 9 whereby n decision trees are produced. The class is chosen by a combined decision (or majority voting) of the different trees.
The bagged TDIDT classification accuracy is normally comparable with that of the traditional TDIDT while it displays less sensitivity to noise from the training set and generalises better. There are several drawbacks of using bagging on TDIDT.
The rules used to classify an instance into a particular class are obscured when many trees are used in this manner. Bagging does not prevent the generation of contradictory rules from different trees. This negates the main advantage of decision trees over black-box approaches such as neural network, transparency of decision rules.

This paper only considers bagging on decision trees, which is equivalent to the standard ID3 algorithm used for bagging in the machine learning community ( Quinlan, 1986 ). The later extension of this algorithm is C4.5 , which is widely considered as the best decision tree and includes tree pruning , continuous attribute values , missing values and explicit rule generation ( Quinlan, 1992 ). Tree pruning is a process to remove unreliable tree branches that lead to a poor generalisation performance. According to Bauer and Kohavi (1999 ), bagging on decision trees works well without pruning because bagging itself reduces variance through the majority vote of many specialised trees. Pruning these trees on the other hand will generate many similar trees that give similar level of trade-off between specialisation and generalisation performance. Thus prun-ing bagged decision trees actually negates the benefit of statistical voting. A similar conclusion has also been drawn from another empirical study ( Quinlan, 1996 ) whereby comparison between the C4.5 and its bagged version produces similar error rates. 3.2.4. Boosting on the alternating decision tree
Boosting was introduced in 1990 by ( Schapire, 1990 ). It uses a set of several weak classifiers rather than one strong high performance classifier. In general, the boosting process involves training the next set of weak classifiers on a set of data points, which are reweighted according to the mistakes of the preceding classifiers. Freund and Schapire introduced an improved boosting algorithm known as AdaBoost in 1997 ( Freund and Schapire, 1997 ), which is now among the top 10 data mining algorithms ( Wu et al., 2007 ). There are other popular boosting algorithms available such as LogitBoost ( Friedman et al., 2000 ) and Brown-
Boost ( Freund, 2001 ). The BrownBoost was developed to address the learning instability problem caused by outliers in the training set. It has a longer computation time since its termination condition requires solving a non-linear equation ( Freund, 2001 ).
LogitBoost is an alternative approach to AdaBoost with compar-able performance in most cases. However it can be far more com-plex to implement because it uses a regression model depending on the dataset ( Friedman et al., 2000 ).
 All three boosting methods have comparable performances. In some cases, AdaBoost may outperform LogitBoost and
BrownBoost, while in others  X  either LogitBoost or BrownBoost may provide the lowest error rate ( Friedman et al., 2000; Freund, 2001 ; Schapire, 1999 ). During implementation, parameters within the chosen boosting algorithm should be tweaked until the best classification performance is obtained. The fabricated wafer yield does not have a problem of outliers. Thus in order to lower the complexity of the classifier, AdaBoost is chosen over BrownBoost and LogitBoost for implementation in the reported research.
In general, boosting on the TDIDT generates better results than those of the bagging IDIDT ( Quinlan, 1996 ). However it has a similar problem with bagging on TDIDT, whereby it generates complex trees with every boosting iteration. Thus, the final classification rules may be difficult (if not impossible) to interpret ( Freund and Mason, 1999 ). Additionally, theoretical and empirical studies in Reyzin and Schapire (2006 ) and Schapire et al. (1998) show that if the base classifier is too complex for its intended application, the performance of the boosted learning process will be negatively affected. Therefore, the alternating decision tree (ADTree) was specifically developed for boosted learning.
ADTree controls the complexity of each base classifier by con-sidering only two conditions at any time. Thus every node has only two branches. Each node in the ADT reehasapredictionvalue,and the final prediction value is the sum of all predictions in the transverse path. A positive sum represents one class, while the negative sum represents the other class for a two-class problem. In this manner, ADTree is able to return a measure of confidence known as a classification margin apart from simply classifying an instance ( Comite et al., 2003 ). A higher classification margin indicates a higher probability of the cluster belonging to that class. Every training iteration adds a decision  X  X  stump  X  X , which is one node and two leafs, to the ADTree. A stump is the simpl est form of a decision tree. 4. Experimental study 4.1. Experimental setup
One of the world X  X  leading semiconductor manufacturers was involved in and supported the reporting study by providing the necessary resources for testing and analysis. Those included several million units of integrated circuits of five types that were in the mainstream high-volume production (see Table 5 ). These devices were selected for experimental study in consultation with the manufacturing company and their characteristics were obtained from their respective design documents. They were specifically chosen to be of different technologies (half-pitch sizes), different complexity, number of metallisation layers and dies/wafer counts. The true names and functional descriptions of the devices have intentionally been changed in this paper for confidentiality reasons.

Three experiment rounds were performed to address the challenges discussed in Section 2 . The first one ( Section 4.2 ) was on classifier training, whereby the following three classifier training and implementation methods were compared: 1. Using historical production data to train a classifier with implementation on raw production test data; 2. Using simulated data (with noise) to train a classifier with implementation on raw production test data; 3. Using simulated data (noiseless) to train a classifier with implementation on filtered production test data.

Section 4.3 below describes an experiment that was performed to determine the best combination of rotational feature vector (discussed in Section 3.1.1 and 3.1.2 ) and classifier (discussed in
Section 3.2 ). Finally, the defect cluster recognition system is concluded in Section 4.4 with an experiment on the optimal feature set. Section 4.5 shows the proposed system flow for implementation on the semiconductor production test floor. 4.2. Classifier training
In order to perform this experiment, the type of classifier must be held constant while changing the training method. The boost-ing on ADTree was selected for the experiment due to its super-iority over the other classifiers from the list. This is because the boosting on ADTree was originally developed to overcome some of the weaknesses of previous versions of decision trees. It is important to note that although the ADTree was used in this experiment, the best classifier for this application was still unknown at the time of experimentation. At the same time, it could be expected that the classifier training results would scale accordingly when applied to other decision trees, as they would face the same problems during the training and implementation cycle. 4.2.1. Experimental flow
Fig. 10 shows the experimental flow for classifier training methods. At this point, the optimal feature set was still unknown, hence only the RMI features discussed in Section 3.1 were used during the Feature Extraction stage. This is because using all the features would result in an extremely large feature set, which would result in an extremely lengthy computation time.
In the first flow (Method 1 shown in Fig. 10 ), raw production test data was used to train and test the ADTree classifier. Hence, the dataset was divided into a training set and a test set. This is the standard train-and-test method used in supervised learning algorithms ( Kononenko and Kukar, 2007 ). In order to reduce variability and to provide a better generalisation of the classifier performance, 10-fold cross-validation was performed, whereby the dataset was randomly divided into training and test sets in ten iterations and the validation results were averaged.
The second and third experimental flows (Methods 2 and 3 shown in Fig. 10 ) involved the use of a defect cluster simulator, which will be discussed in detail below. To train the classifier, Method 2 uses  X  X  X oisy X  X  simulated wafers, or wafers that are simulated to emulate real production dataset. The amount of  X  X  X oise X  X  or random failure depends on the production yield. Thus, the average wafer yield for each device must be known prior to simulation. This is done to ensure that sufficient numbers of samples of high quality training data are available for training.
Method 3 trains the classifier using pure (noiseless) defect clusters. Thus to test the classifier, raw production data must be  X  X  X iltered X  X  to remove the random failure and to extract the defect clusters. The accuracy of the classifier would thus depend not only on the training set, but also on the effectiveness of the noise filtering technique. While other filtering methods can be used such as ( Wang, 2007 ; White et al., 2008 ), this research applied the Segmentation , Detection and Cluster Extraction (SDC) algorithm in Ooi et al. (2011 ) as the filtering method of choice.

Any wafer can be simulated if the valid device locations ( x X  X  positions on the wafer) are known. Thus, each defect cluster shown in Fig. 3 was simulated using the defect simulator algo-rithm shown in Fig. 11 .

The simulator produces  X  X  X erfect X  X  defect cluster patterns using the set of knowledge rules provided in Ooi et al. (2011 ) in the first stage. In the second stage, cluster variations are included to obtain more realistic defect patterns. These variations are achieved using the rules shown in Table 6 . In the final stage, random failures are included into the wafer map based on a predetermined wafer yield.
The outputs of the second and third stages were used in Methods 3 and 2, respectively. 4.2.2. Experimental results and discussions on classifier training methods
In the reported research data equivalent to over 300,000 different wafers were generated using the defect cluster simulator for the classifier training threshold selection. This included 5000 wafers for each of the six defect types, with and without random failure and five device types. The reason for simulating 5000 wafers for each point was to ensure that the classification criteria were observed to converge to an asymptotical value. Ooi et al. (2011 ) provides some examples on verification of the simulation stability.

Table 7 shows the classification results for the three trialled methods. It must be noted that Devices A, B and D did not period. For Device A, only Edge, Blob and Hat defect types were encountered, while for Devices B and C only Edge and Blob defect types were found. Thus, although Method 1 appears to be fairly accurate for Devices A, B and C, it is likely to fail if the production lines encountered a different defect type in future. Overall, Method 3 performs fairly consistently across the different device types compared to Methods 1 and 2. Thus, it can be concluded that
Method 3 gives a better generalisation for application across many different devices being produced by the manufacturer. 4.3. Experiment on combinations of rotational feature vectors with different classification algorithms
Two feature extraction methods capable of extracting defect cluster information while accounting for different orientation on the wafer were discussed earlier in Section 3.1.1 and 3.1.2 . These are: Rotational Moment Invariants (RMI) and Discrete Polar Fourier
Transform (DPFT). Both feature sets are subsequently used to train the four shortlisted classifiers, which are the Bayesian classifier ,
TDIDT , bagging on TDIDT and boosting on the ADTree . 4.3.1. Experimental flow on feature X  X lassifier combination Two feature vector sets were obtained as shown in Fig. 12  X  the
RMI and the DPFT feature sets, respectively. The shortlisted classifiers were trained based on the respective feature sets ( Fig. 12 ). Thus after the training, eight different classifiers were generated and compared against each other. The classifiers were trained and implemented using the method determined in Section 4.2.2 , whereby data from the defect cluster simulator were employed to generate the training set.
Production test data from the devices shown in Table 5 were used to generate the classifier performan ce table (after applying the SDC algorithm). 4.3.2. Experimental results and discussions on feature X  X lassifier combination
Table 8 shows the performance for every feature X  X lassifier combination for each defect cluster type when applied on the production dataset. It can be observed that the TDIDT and bagging on decision trees have poor average hit rates. The poor perfor-mance of the TDIDT is mostly caused by the unresolved missing branch (or null hypothesis) problem. Although bagging was expected to increase the performance, it did not manage to achieve so in this application.

It is clear that the pairing of RMI with boosting on ADTree results in the overall performance, while PFT with boosting on
ADTree has comparable performance. However, PFT describes clusters in frequency domain, which makes any refinement or modification on its features more difficult to perform. Addition-ally, RMI has a smaller feature spaces dimension, which makes it less affected by the curse of dimensionality when geometrical features are included into the feature set ( Section 4.4 ). This leads to the conclusion that the feature X  X lassifier combination that best suits the wafer-detection and classification application for this research is RMI with ADTree classifier. Its overall accuracy is still below 70% because geometrical features have not been con-sidered. Thus, the description of some defect clusters (in particular, the Blob defect type) is poorly defined, leading to very low identifi-cation rates. To overcome this weakness, a third and final experi-ment on the optimal feature set is discussed in the next section to increase its suitability for the real-world implementation. 4.4. Experiment on optimal feature set for defect cluster recognition system Two different features sets were used to generate two different ADTree classifiers each for Devices A X  X  ( Table 5 ). The RMI Feature Set consisted of only RMI features, while the Extended Feature Set comprised RMI and geometrical features (refer to Section 3.1.3 ). This was done to compare the differences in accuracy with and without the geometrical features. The results are shown in Table 9 .
It is clear from the experimental results that the ADTree classifier generated by Extended Feature Set is far superior in terms of accuracy. It offers average percentage point improve-ment of approximately 30% while achieving a very good recogni-tion accuracy of up to 96% depending on the product type. Thus it can be well recommended as the Defect Cluster Classifier for semiconductor wafers. 4.5. Proposed defect cluster recognition system
The defect cluster recognition system for fabricated semicon-ductor wafers shown in Fig. 13 is derived using the results of the three experiments discussed above in Sections 4.2, 4.3 and 4.4 .
The offline process involves the generation of the features and training of the ADTree classifier, which can take up to a few minutes depending on the performance of the employed compu-ter system. For example in this research, the training time was approximately 10 min on a commodity computer powered by a
Intel s Pentium s T4400 (2.2 GHz) processor. By using a more powerful processor, the training time can be reduced dramati-cally. At the same time it is important to note that this process is performed one time only for each device in an offline mode, and thus it does not affect the manufacturing throughput. On the other hand, the classification time is practically negligible for all the data in the manufacturing dataset due to the built-in simplicity of ADTree. As a result, the proposed system is capable of running in an online mode of application without negatively impacting the manufacturing throughput.

It is important to note that since simulation is used to produce the training set for the classifier, types of defect clusters recog-nisable by the system are limited to the defect types that were used for training. In this research, the system was trained to recognise a limited set of defects including Bulls-Eye, Blob, Line,
Edge, Ring and Hat. However, the system could be trained to recognise new defect types by specifying their geometry and simulating it.

The defect cluster simulator overcomes the problem of insuffi-cient training samples, while complementary application of the
SDC algorithm ( Ooi et al., 2011 ) filters the noisy raw production dataset. The combination of these algorithms overcomes the problem of the low signal-to-noise ratio discussed in Section 2.1 . The application of RMI feature extraction method with extended geometrical features provide good distinguishing for defect clusters encountered in semiconductor wafer fabrication, while its combination with the ADTree classifier results in an overall performance of up to 95% depending on the device type. 5. Conclusions
This paper presents a defect cluster recognition system that automatically recognises several types of known defect clusters found on fabricated semiconductor wafers. The proposed system generates an ADTree classifier that achieves classification accu-racy of up to over 95% depending on the product type. Incorpora-tion of the classifier into an online production data analysis system allows any type of known defect cluster encountered during the manufacturing process to be quickly and automatically identified. This provides a very valuable information that can be used for fast fault diagnosis and rapid root cause identification, which in turn leads to a better process control.
 Acknowledgements
The authors would like to thank the industrial partner, Free-scale Semiconductor for the provision of data, resources and equipment for this research and Monash University for the scholarship support. This research was supported by Malaysian Ministry of Higher Education Fundamental Research Grant Scheme FRGS/1/2011/SG/MUSM/03/1.
 References
