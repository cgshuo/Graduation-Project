 In order to satisfy and positively surprise its users, a recom-mender system needs to recommend items the users will like and most probably would not have found on their own. This requires the recommender system to recommend a broader range of items including niche items. Such an approach also supports online-stores that often offer more items than tradi-tional stores and need recommender systems to enable users to find the not so popular items as well. However, popular items that hold a lot of usage data are more easy to rec-ommend and, thus, niche items are often excluded from the recommendations. In this paper, we propose a new collab-orative filtering approach that is based on the items X  usage contexts. That is to say, an item is described by the items it is significantly often used with rather than by its users or content attributes. The approach increases the rating pre-dictions for niche items with fewer usage data available and improves the aggregate diversity of the recommendations. H.3.3 [ Information Storage and Retrieval ]: Information Retrieval and Search X  information filtering Aggregate Diversity, Item-Item Similarity, Long Tail, Niche Items, Recommender Systems, Usage Context
Recommender systems are ste adily becoming more im-portant in an expanding number of domains (movies, music, books, etc.) to filter relevant items for users. Recommend-ing relevant items alone, though, is often not sufficient to satisfy user expectations, but other characteristics, such as diversity, novelty, serendipity and trust must be considered as well [4, 21, 27].

We focus on increasing the aggregate diversity, i.e. the number of distinct items recommended across all users [3], by improving the calculation of the expected ratings espe-cially for niche items. It has been shown that popular items are recommended disproportionately often because they pro-vide extensive usage data and, thus, can be recommended to more users [17, 32]. However, a system focussing on pro-viding a wider range of items and not mainly popular items more likely recommends novel and diverse items to its users which often favour recommendations that are for items they would not have thought of by themselves [4, 27]. A large re-cent study analysing rating data shows that, for instance in the case of movies, users regularly give high ratings to niche items, suggesting that users value speciality items [19].
A broader range of recommended items is not only impor-tant for the users X  satisfaction but plays an important role in online stores as well [8, 20]. Many markets have historically been dominated by a small number of bestselling products (Pareto Principle [8]). Internet markets, though, exhibit a significantly less concentrated sales distribution. Anderson describes the phenomenon that niche products can grow to become a large share of total sales as  X  X he Long Tail X  [5]. There are two explanations for this phenomenon. First, an online store can easily offer a larger amount of items than a traditional store. Second, by using tools such as recom-mender systems, users can be encouraged to buy items they would not have found by themselves [8]. For some online stores it might even be more beneficial to recommend niche items. For instance, Netflix 1 could encourage users to rent movies from the long tail, which are less costly to license than blockbusters [20]. However, a higher aggregate diver-sity often lowers the accuracy since it requires the recom-mendation of idiosyncratic items as well [3].

We address the task of recommending items from the long tail and increasing the aggregate diversity without lowering the accuracy by introducing a new collaborative filtering ap-proach -called usage context-based collaborative filtering (UC-BCF) -where an item is described by the items it sig-nificantly often co-occurs with rather than by its users or content attributes. The idea is taken from linguistics where relations between words can be inferred from the words X  con-texts, e.g. the words they co-occur with in sentences [12].
The task of finding correlations between items in a dataset is well-known as association mining [10]. Association mining started with the analysis of shopping basket data to better understand and target the consumers X  behaviour by discov-ering inferences and rules between items and item sets, e.g. a user who buys item A and item B also buys item C with a probability of 80%. Since then, association mining has http://netflix.com/ been applied to many different domains in which the rela-tionships between objects can provide useful knowledge, e.g. risk analysis and clinical medicine.

However, our approach differs from association mining in that we do not assume two items to be related if they co-occur with each other, but if they significantly often co-occur with the same items. Thus, two items can be highly related even if they were never used together. For example, items A, B, and C are often used together as well as items A, B, and D. Thus, we assume the items C and D to be highly related because they are described through the same co-occurrences, here items A and B. Using this approach, we are able to cre-ate characteristic vectors also for rarely used items and to improve the calculation of the niche items X  rating predic-tions. Thus, the aggregate diversity of recommendations can be improved without lowering the accuracy.

The rest of the paper is structured as follows: In chapter 2 we give an overview of recommendation approaches that try to increase the diversity of recommendations and com-pare them to the usage context-based collaborative filtering that is specified in chapter 3. In chapter 4 we describe the set-up of the experiments including the used data sets and evaluation metrics to discuss the results of the experiments in detail in chapter 5. Finally, in chapter 6, we summarize the presented work and give an outlook on future work.
Recently, several recommendation approaches have been invented that do not focus on accuracy alone but on new evaluation metrics such as diversity, novelty, unexpected-ness, and serendipity of the recommended items to increase the user satisfaction [4, 21, 27]. So far, most work has been conducted for increasing the diversity which is divided in the individual and the aggregate diversity. The individual diversity describes the diversity of the items in a user X  X  rec-ommendation list, thus, increasing the individual diversity means avoiding overspecialisation. Strategies developed so far for increasing the individual diversity mostly calculate the quality of an item based on its similarity to the user and its dissimilarity to the items that are already selected for this user X  X  recommendation list [7, 33, 34]. The aggregate diversity describes the total amount of items recommended to the users. Improving the individual diversity does not mean improving the aggregate diversity, e.g. if each user gets recommendations for the same 10 movies from 10 dif-ferent genres, the individual diversity is high, but the aggre-gate diversity is still low. In the following, we will describe approaches that increase the aggregate diversity of recom-mender systems and compare these to our approach.
There are two lines of research that try to improve the aggregate diversity. The first line calculates the rating pre-dictions using existing filtering approaches to then re-rank the items with the highest predicted ratings to push items from the long tail in the recommendation lists. The second line of research tries to improve the estimation process espe-cially for rarely used items. We first present two approaches from the first and then two approaches from the second line.
Adomavicius and Kwon [3] propose to re-rank the list of candidate items for a user to improve the aggregate diversity. First, an ordered list of recommendations is calculated using any filtering technique. Second, for all items having a better expected rating than a given threshold, additional features are calculated, for instance the absolute and relative like-ability of an item (how many users liked the item among all users or among all users who rated that item, respectively) and the item X  X  rating variance. According to these features, the candidate items are re-ranked and only the top-N items are recommended. This way, niche items are pushed to the recommendation lists and very popular items are rejected. While this re-ranking technique can improve the aggregate diversity, it comes at the expense of accuracy.

In [2], Adomavicius and Kwon propose a graph-theoretic approach to select the top-N items for each user while yield-ing the maximum aggregate diversity. At first, the sys-tem calculates all candidate items for each user by apply-ing any filtering technique. Then, a bipartite graph is cre-ated with the users and the selected items as vertices con-nected through edges that hold the predicted ratings as edge weights. By solving the  X  X aximum bipartite matching prob-lem X  using a suitable algorithm, the maximum aggregate diversity is reached as each item is restricted to only one user. Users that receive less than N items in that process also receive recommendations for items that scored best in the first step. The accuracy and diversity of this graph-theoretic approach depends on the selection criteria for the items included in the graph. The more items are selected as candidate items for each user, the more diverse and less accurate are the recommendations and vice versa.

Park and Thuzhilin [28] group items from the long tail into clusters based on their attributes (e.g. name, descrip-tion, price) to compensate the missing usage information. For each cluster, they build a model predicting ratings based on the known ratings and the items X  attributes. Items from the head are not clustered, but for each of these items an individual predictive model is built. The splitting of the head and the tail is based on features like the items X  average ratings or their popularity. This approach improves the es-timation of rating predictions, but also relies on additional semantic metadata which is often not available.

Levy and Bosteels [25] explicitly push items from the long tail (here: songs from not well known artists) in the recom-mendation lists. After analysing the last.fm data set 2 they define each artist with less than 10.000 listeners as long tail artist. For each artist in the last.fm data set they calculate the k most similar long tail artists based on their listening history and tags applied to each artist. In order to create rec-ommendations for a user, the similarities between the long tail artists and the artists in the user profile are calculated and the user gets recommendations for songs from those long tail artist that are most similar to her favourite artists. No evaluation has been published so far.

The UC-BCF approach we propose tries to improve rat-ing predictions especially for niche items and thus falls in the latter research line mentioned above. In contrast to the other approaches of this line, however, it does not re-quire any semantic metadata (which is often not available or imcomplete) but calculates the item vectors based on the items X  usage contexts. Thus, it is not a hybrid but a new collaborative filtering approach. The first research line re-ranks candidate items based on their usage to push niche items from the long tail with the predicted ratings being calculated using any filtering technique. When using the UC-BCF approach to calculate the predicted ratings, these approaches can benefit from it as we show in subsection 5.3. http://www.last.fm/ Similar to user-and item-based collaborative filtering, the UC-BCF takes the user-item matrix as input. However, be-fore calculating the item pair similarities, the user-item ma-trix is transformed into an item-item matrix. In this matrix, each item is described by the k items it significantly often co-occurs with in a user profile. After the matrix transfor-mation, the item pairs are compared using traditional infor-mation retrieval approaches, e.g. by calculating the cosine similarity of their vectors. This way, the UC-BCF combines features of the user-and the item-based approaches: 1) If two items share a significant amount of users that rated them similarly, they are described through similar item vectors because they often co-occur with the same items in auserprofile. 2) If two items are rated similarly by a significant amount of similar users (not necessarily the same users), they are also described by similar item vectors because similar users co-rated the same items similarly.

Additionally, there is a third feature not contained in the user-or item-based approach: 3) If two items are rated dissimilarly by a significant amount of dissimilar users, they hold similar item vectors, because dissimilar users co-rated the same items dissimilarly.
Let X  X  take the following examples to clarify the mecha-nisms of the UC-BCF. Table 1 shows the ratings of the two users u 1 and u 2 for the movies m 1 -m 5 . For this exam-ple, the user-based collaborative filtering (UBCF) approach recommends movie m 2 to user u 1 and movie m 1 to user u 2 The item-based collaborative filtering (IBCF) approach fails because of the items X  usage data sparsity.

The UC-BCF approach first transforms the user-item ma-trix into an item-item matrix by calculating the most signifi-cant co-occurences for each item. For simplicity in this small example, we consider movies to be significant co-occurrences if at least one user rated them similarly (here: with a dif-ference of 1 star or less). Additionally, we just use 0 and 1 for stating if movies are co-occurrences or not, later, we will exchange the binary attribute for the significance value of the co-occurrence. Table 2 shows the vectors describing the movies m 1 -m 5 calculated this way.

The movies m 1 and m 2 are described by the same item vectors, therefore they receive a high similarity score and users who like m 1 are assumed to like m 2 as well and vice versa (of course also depending on the user X  X  other ratings).
Table 3 shows exemplary rating data for the users u 3 , u 4 and u 5 on the movies m 6 -m 9 . Here, the UCBF suffers from the sparsity of user u 5  X  X  profile. The IBCF recommends movie m 6 to user u 5 .

The item vector calculation of the UC-BCF for the movies m 6 -m 9 results in the vectors given in table 4. Similar to the item-based approach, the movie m 6 gets recommended to user u 5 .

The last exemplary rating data is given in table 5. The users u 6 and u 7 are very dissimilar according to their ratings. Therefore, the ratings of u 6 will not be considered when cal-culating recommendations for user u 7 and vice versa when using a user-based approach. The item-based approach suf-fers from sparsity as in the first example.

The transformation of this matrix into the usage context-based approach X  X  item-item matrix results in the vectors given in table 6, which shows that the movies m 10 and m 11 are assumed to be similar. This is because the movies m 12 and m 13 were co-rated similarly (1 star difference or less) with the movies m 10 and m 11 by users u 6 and u 7 . Here, it does not matter that u 6 rated the movies low and user u 7 rated them high. Thus, the approach would assume user u 7 to like movie m 10 because he likes movie m 11 .

These simple examples show, how the UC-BCF can com-bine the strength of the two standard UBCF and IBCF ap-proaches and compensate their weaknesses. In the next sec-tions, we will discuss how to choose the most significant co-occurrences for an item and how to calculate the similarity of two items based on their co-occurrences.
We define two items to be co-occurrences if they are con-tained in at least one user profile in which they are rated similarly. We assume two items to be rated similarly if their rating difference is below a given threshold t or if both items are rated above or below the user X  X  average rating. Not ev-ery co-occurrence is significant, rather most co-occurrences are coincidental, thus, we need to calculate a significance score for each co-occurring item pair.

Basic association measures calculate a significance score by comparing the observed frequency O of a co-occurrence with its expected frequency E , e.g. MI (mutual information) and z-score, see [26]. These simple association measures of-ten give close approximation to the more sophisticated as-sociation measures (as described below) and are therefore sufficient for many applications. They also have some limi-tations as they, e.g. tend to fail when calculating the signif-icance value for a frequent and an infrequent object [16].
In statistical theory, association measures are always based on a cross-classification of a set of items, e.g. using contin-gency tables. Table 7 shows the contingency table for the items i and j which are co-rated O 11 times. Additionally, i was rated by O 12 users who did not rate j , j was rated by O 21 users who did not rate i and O 22 users who did not rate any of these items at all. The expected values for these observed values are E 11 , E 12 , E 21 ,and E 22 , respectively.
Commonly used association measures that are based on contingency tables are the  X  2 test and log-likelihood, see [26]. The  X  2 test adds up the squared z-scores for each cell in the contingency table and puts it in relation to the ex-pected frequencies. Since the normal approximation implicit in the z-scores becomes inaccurate if any of the expected fre-quencies are small [16], the Yates X  continuity correction [13] shown in equation 1 offers a better approximation (corrected -test). Equation 2 shows the log-likelihood measure [14].
The association measure shown in equation 3 is based on the Poisson distribution. [22] gives a formal proof that justi-fies the assumption of a Poisson distribution for co-occurring objects in a data set if the frequency of most objects is much smaller than the size of the data set. Using the Poisson dis-tribution requires the calculation of the faculty of the nat-ural logarithm of O 11 , which is numerically hard to handle for a large O 11 , thus, using an approximation such as the Stirling X  X  formula, it can be simplified [6]. poisson = O 11 ( lnO 11  X  ln X   X  1) + 1 with  X  = R 1 C 1
After the calculation of the co-occurrences X  significance values, the most significant ones must be selected for each item by choosing its k most significant co-occurrences or by using a threshold. Since there is no standard scale of measurement to draw a clear distinction between significant and non-significant co-occurrences [15], the calculation of a suitable k or a threshold is an exploratory investigation. In our experiments, we will vary the vector sizes but only consider items if they hold a significance value that is over-average for the described item to avoid noise.
We calculate the similarity for each item pair using the cosine similarity which measures the angle between their co-occurrence vectors, see equation 4. Each item i is described by its vector V i . The cosine similarity always takes a value between 0 and 1.
We compute the expected rating p ( u, i )onanitem i for auser u by averaging the ratings given by the user to the other items in her profile P ( u ) while each rating is weighted by the corresponding similarity sim ( i, j ), see equation 5.
The computational complexity of the UC-BCF approach depends on the amount of time required to build the model (i.e. calculating the item-vectors and the item pair simi-larities) and the amount of time required to compute the recommendations using this model.

During the model building phase, we first need to compute the co-occurrence vectors by calculating the significance val-ues of all co-occurrences and select the k most significant ones for each item. The upper bound on the complexity of this step is O ( n 2 m )with n being the amount of items and m being the amount of usage contexts, i.e. user profiles, as we potentially need to calculate n  X  ( n  X  1) 2 significance val-ues, each requiring up to m operations. However, the actual complexity is significantly smaller because we only need to calculate significance values for those item pairs that were actually used together and the user profiles are generally sparse and the rated items are clustered [23]. In the second step of the model building phase, we need to compute the similarity of all item pairs using their co-occurrence vectors. The upper bound on the complexity is O ( n 2 k )asweneed to compute n  X  ( n  X  1) 2 similarities, each potentially requiring operations, with k typically being small ( &lt; 100, see section 5), thus, it can be simplified to O ( n 2 ).

Finally, the upper bound of complexity to compute the top-N recommendations for a given user profile P ( u )isde-scribed by O ( n | P ( u ) | ) as for all items that have a similarity greater than 0 to at least one of the rated items (which is potentially n , but usually much smaller) a predicted rating is calculated by considering all rated items.
This section describes the data sets, the evaluation metrics and the experimental set-up used for the evaluation of the UC-BCF approach in comparison to baseline approaches, i.e. UBCF and IBCF as well as matrix factorization techniques. In the next section, we present the results and give a detailed discussion about the gathered insights.
The MovieLens 3 data set comprises 1 million ratings from 6,040 users for 3,952 movies (95,81% sparsity) on a rating scale from 1 to 5. We randomly split the data set in five sub-sets to perform a 5-fold cross validation. Following [3], all users that hold less than 20 high ratings in the test set are re-moved from it and the ratings are added to the training set. We do so because the recommendation lists we create have a length up to 20 and we want to be able to distinguish be-tween relevant (liked) and irrelevant (not liked) items (rather than between liked and not yet reviewed items). As com-monly done in recommender systems literature, we define an item as highly rated if the user rated it with at least 4 out of 5 stars [4]. This results in test sets containing on average 147,494 ratings (85,309 high and 62,185 low ratings) from 2,152 users on 3,423 movies, the remaining ratings form the training sets. The Netflix data set is the one used for the Netflix 4 prize. Because of the size of the data set and the number of exper-iments we conducted, we pre-processed the data set by ran-domly selecting 9,006 users who rated 17,208 movies. Over-all, the data set comprises 1,863,197 ratings which results in a sparsity of 98,81%. In the same manner as for the Movie-Lens data set we split the data set in five subsets to perform a 5-fold cross validation and removed all users from the cur-rent test set that hold less than 20 high ratings. The test sets comprise on average 304,932 ratings (177,551 high and 127,581 low ratings) from 3,439 users for 12,515 movies.
Following [3], we present the aggregate diversity of a rec-ommender system as the total n umber of distinct items rec-ommended among all users, see equation 6, with P H ( u )be-ing the set of items highly rated by user u from the user set U and Rec ( u ) being the set of items recommended to her.
Note that we only consider correctly recommended items, i.e. movies highly rated by the user in the test set. We do so, because we only allow the recommender to recommend items that were actually rated by the user in the test set (see subsection 4.1). Thus, we know if an item was highly rated by the user or not and we do not want to increase the aggregate diversity by recommending items not relevant for the users. http://grouplens.org http://www.netflixprize.com/
Novelty is defined differently in several publications de-pending on the context and its purpose, e.g. the item X  X  novelty in respect to the user (which is related to the in-dividual diversity) or the item X  X  novelty in respect to the total amount of rec ommended items (which is related to the aggregate diversity). We apply the population-based item novelty evaluation metric proposed in [31] which is called ex-pected popularity complement (EPC) to measure the ability of a recommender system to recommend items from the long tail, see equation 7.

Here, we calculate the average  X  X on-popularity X 1  X  pop ( i of each item i r (i.e. the item that is at ranking position the actual recommendation list with size N ). The popularity pop ( i ) is calculated based on the times the item has been rated so far, hence, the item X  X  popularity is the ratio between the number of its ratings Rat ( i ) and the number of ratings of the most rated item in the item set I , see equation 8.
Just as for the aggregate diversity calculation, only rele-vant items are considered, this means rel ( u, i r ) can take the values 0 or 1. Additionally, the items are weighted according to their position r in the recommendation list by using a log-arithmic discount. Thus, a recommender gets a higher EPC value when it not only recommends items from the long tail but also ranks them highly in the recommendation lists.
Classification metrics measure the amount of correct and incorrect decisions, i.e. whether an item is correctly or in-correctly recommended or not recommended, respectively. The most popular metrics in this category are precision and recall that state the percentage of correctly recommended items in respect to all recommended items (precision) and the percentage of correctly recommended items in respect to all items that are highly rated (recall), see [11].
However, in a considerable amount of recommender sys-tems a fixed number of recommendations is offered to the user. Additionally, we need a uniform number of recom-mended items to evaluate our approach in respect to the aggregate diversity described below. Therefore, we use the accuracy in top-N approach, i.e. the average precision of the recommended items for all users, see equation 9.
In order to create a baseline to evaluate our approach against, we use the standard collaborative filtering meth-ods IBCF (with adjusted cosine similarity) and UBCF (with Pearson correlation based similarity). Although these meth-ods do not explicitly support the notion of novelty, diversity or the long tail, they constitute fairly reasonable baselines for performance measures besides classical accuracy mea-sures as pointed out in [9] and supported by [1].
Additionally, we tested the matrix factorization methods (MF) offered by the PREA (Personalized Recommendation Algorithms) toolkit [24] (i.e. Single Value Decomposition (SVD), Non-negative MF, Probabilistic MF, and Bayesian Probabilistic MF) as well as the MF methods offered by the Java port of the MyMediaLite Recommender System Library [18] (i.e. a standard MF as well as a Biased and a Factorized MF). Based on the performances of the different methods on our test sets and to not overload the diagrams, we choose to present the SVD [29] method from the PREA toolkit and the Biased Matrix Factorization (BMF) [30] from the MyMediaLite library.
We start our evaluation with predicting the top-N movies for each user in the MovieLens and the Netflix data set. Then, we calculate the aggregate diversity, the EPC, and the classification accuracy of the recommendation lists. We run the experiments with the result list size N = 5, 10, 15, and 20. The UC-BCF approach is tested with the as-sociation measures  X  2 -corr (Chi), log-likelihood (Log), and Poisson-based (Poisson), see section 3.2. Altogether, we run 240 experiments combining the different features and vary-ing the co-occurrence vector size from 10 to 100 (with the specific sizes 10, 20, 25, 30, 40, 50, 75 and 100). Due to size constraints and because the algorithms perform similar for the different values of N , we only present the results for N =10.
Fig. 1 shows the aggregate diversity, i.e. the total amount of distinct recommended items for the different techniques. The baseline algorithms perform dissimilar for the two data sets, for instance the Biased Matrix Factorization method is the best performing baseline on the MovieLens data set while on the sparser Netflix data set, it is the worst per-forming one. Overall, the user-based collaborative filtering approach performs best in terms of aggregate diversity.
For both data sets, the UC-BCF approach is able to out-perform the baseline algorithms whereupon the UC-BCF ap-proach using the association measure  X  2 -corr in combination with vector size 10 is the most promising one. For the Movie-lens data set it increases the amount of recommended items on average by 38.10% (from 26.59% compared to BMF to 46.10% compared to IBCF). For the Netflix data set it in-creases the amount of recommended items on average by 68.31% compared to the baseline algorithms (from 52.25% (UBCF) to 88.77% (BMF)).

It can be seen in Fig. 1 that an increasing vector size re-sults in a decreasing amount of distinct recommended items. Thelessitemsareusedtodescribeanitemthemoredi-verse are the item vectors and the more distinct items get recommended to the users. It is important to state that only correctly recommended items are used to calculate the amount of distinct recommended items as we want to avoid incorrectly recommended items no matter if they increase the aggregate diversity or not. For the sparser Netflix data set, even with vector size 100, the UC-BCF approach rec-ommends more distinct items than the baseline algorithms. For the MovieLens data set, the vector size needs to be at maximum 10-30 depending on the used association measure, to perform better than the BMF approach.

Fig. 2 shows the comparison of the classification accu-racy in the top-10 recommendation lists for the different ap-proaches. Here, the baseline algorithms perform similarly on both data sets. As could be expected, the matrix factoriza-tion methods SVD and BMF outperform the standard col-laborative filtering techniques IBCF and UBCF with SVD being the best and UBCF being the worst performing ap-proach for both data sets.

The UC-BCF approach with association measure  X  2 -corr reaches the best classification accuracy values with vector size 30 for MovieLens and vector size 50 for Netflix. For the MovieLens data set, the UC-BCF approach performs better than the BMF (0.46%), IBCF (1.10%), and UBCF (1.96%) approaches whereas the SVD performs better than the UC-BCF approach (0.82%). For the sparser Netflix data set, the UC-BCF approach is able to increase the classification accuracy compared to all baseline approaches (from 0.32% (SVD) to 2.72% (UBCF)).

As can be seen in Fig. 2, the accuracy of the UC-BCF ap-proaches increases with an increasing vector size until a size of 30-50 depending on the underlying association measure; thereafter it slightly decreases again. This is due to the fact that the more co-occurring items are used to describe an item, the greater is the chance of including coincidental and thus non-significant items which can be described as noise.
Fig. 3 shows the expected popularity complement (EPC) which states how well a recommender system performs in recommending items from the long tail. In terms of EPC, SVD and UBCF are the best performing baseline algorithms.
All UC-BCF approaches are able to outperform the base-line algorithms in terms of EPC. Similar to the aggregate diversity, the UC-BCF approach using  X  2 -corr as associa-tion measure with vector size 10 is the best performing one for the MovieLens and the Netflix dataset. This UC-BCF approach results in an improvement from 4.18% (UBCF) to 11.39 (IBCF) for the MovieLens data set and an improve-ment of 8.33% (UBCF) to 17.78% (BMF) for the Netflix data set.
The goal of the UC-BCF approach is to increase the ag-gregate diversity by pushing items from the long tail into the users X  top-N recommendation lists without decreasing the classification accuracy of the recommendations. There-fore,weneedtochooseaco-occurrencevectorsizethatisas small as possible to increase the aggregate diversity and the expected popularity complement (EPC). Additionally, the size needs to be large enough to not decrease the classifica-tion accuracy. These considerations result in a vector size between 20 and 25 depending on the used association mea-sure and the data set. Overall,  X  2 -corr with vector size 25 is the most promising association measure to be used with UC-BCF.

For the MovieLens data set, the usage of the UC-BCF approach with  X  2 -corr and vector size 25 raises the amount of recommended items up to 21.7% compared to the UBCF approach (from 919 to 1118) and at least by 5.45% compared to the BMF approach (from 1060 to 1118). In terms of accu-racy, the SVD approach receives a 0.86% better value than the UC-BCF approach. Nonetheless, the UC-BCF approach outperforms all other baseline algorithms by 0.41%-1.91%. Additionally, the EPC value shows that more items from the long tail are recommended.

For the Netflix data set, the amount of recommended items is improved up to 76.52% compared to the BMF ap-proach (from 2101 to 3709 items) and at least by 42.37% compared to the UBCF approach (from 2605 to 3709 items) when using the UC-BCF approach with  X  2 -corr and vec-tor size 25. The accuracy is slightly increased as well (from 0.1% (SVD) to 2.5% (UBCF)) and the EPC value shows an improvement in recommending items from the long tail.
The improvement of the aggregate diversity and expected popularity complement is more significant for the Netflix data set than for the MovieLens data set, which can be ac-counted for by the fact that the Netflix data set holds more rarely rated objects (see section 4.1) that benefit from the new approach.
In chapter 2 we present several techniques that explicitly try to improve the aggregate diversity and that can be di-vided in two research lines. The first line tries to increase the aggregate diversity by re-ranking the most promising items. The second line tries to increase the aggregate di-versity by improving the rating predictions for niche items usually using semantic metadata. The UC-BCF approach falls in the latter research line as it tries to improve the rat-ing predictions as well but differs in that it does not require any additional semantic metadata which is an advantage as such data is often not available or incomplete.

However, the UC-BCF approach is not meant to be a mere concurrency for the presented approaches but can be used as an enhancement. Firstly, similar to other collaborative filtering approaches, it can be combined with approaches using semantic metadata if available, thus, the techniques from the second line can be applied to UC-BCF. Secondly, it can be used as an underlying algorithm for the re-ranking approaches from the first line.

In order to demonstrate such a combination, we select a simple popularity-based re-ranking approach as introduced in [3]. First, we calculate the missing ratings in the user-item matrix by applying the UC-BCF. Then, we choose all items having a higher rating prediction than a given threshold as candidates. These items are re-ranked according to their popularity (i.e. the number of ratings they hold) in reverse order to recommend the first ten items. Thus, a very pop-ular item gets rejected, even if its predicted rating is high. We perform a 5-fold cross validation on the MovieLens and Netflix data sets as before to evaluate the recommendations. Additionally, in order to enable a better comparison, the threshold is varied from 3.8-4.2 stars to receive a common classification accuracy of 0.85 for each technique.
Fig. 4 shows the aggregate diversity reached using this re-ranking method for the baseline algorithms and the UC-BCF approach with  X  2 -corr and vector size 25 for the MovieLens and the Netflix data set. Compared to the baseline algo-rithms, the UC-BCF approach is able to increase the ag-gregate diversity from 3.18% (BMF) to 24.42% (UBCF) for the MovieLens data set and from 18.37% (SVD) to 55.8% (UBCF) for the Netflix data set. Similar to the experiments conducted before, the Netflix dataset which holds more niche items than the MovieLens dataset benefits even more from the UC-BCF approach in terms of aggregate diversity. Figure 4: Aggregate Diversity after Re-ranking (for an accuracy of 0.85 in top-10)
In this paper we present a new collaborative filtering ap-proach  X  named usage context-based collaborative filtering (UC-BCF)  X  in which items are described by items they sig-nificantly often co-occur with. In this way, items are similar if they often occur in similar usage contexts, i.e. in sim-ilar user profiles, but not necessarily together in the same user profiles. This approach combines and extends strengths from the well-known user-based and item-based collabora-tive filtering approaches and compensate their weaknesses. Using this new usage context-based approach, even niche items that do not hold an extensive usage history are rep-resented by a characteristic co-occurrence vector and, thus, the calculation of their rating predictions is improved.
We show that the UC-BCF increases the aggregate diver-sity compared to the standard collaborative filtering and ma-trix factorization techniques with only one case of accuracy loss (in all other cases, the accuracy can even be improved). Additionally, the rating prediction of seldomly used items is improved and thus, more items from the long tail are rec-ommended to the users. Therefore, the UC-BCF approach is better suited than the standard approaches to provide a user with idiosyncratic items and to increase the range of products sold by an online shop. Additionally, the UC-BCF approach is well suited to form the basis for approaches that re-rank possible recommendations to push even more items from the long tail.

In this paper we use user profiles as usage contexts, how-ever, with more detailed usage data available usage contexts can be formed considering e.g. the time a movie was watched or the company it was watched with, which might result in even better results. Furthermore, when creating recommen-dations e.g. for songs or web pages, it might be more suitable to consider one session (e.g. every listening or click without a break of more than an hour) as one usage context. We will further continue on this track to answer these questions. [1] Adamopoulos, P., and Tuzhilin, A. On [2] Adomavicius, G., and Kwon, Y. Maximizing [3] Adomavicius, G., and Kwon, Y. Improving [4] Adomavicius, G., and Tuzhilin, A. Toward the [5] Anderson, C. The Long Tail: Why the Future of [6] Bordag, S. A comparison of co-occurrence and [7] Bradley, K., and Smyth, B. Improving [8] Brynjolfsson, E., Hu, Y. J., and Simester, D. [9] Burke, R. Hybrid recommender systems: Survey and [10] Ceglar, A., and Roddick, J. F. Association [11] Cleverdon, C., and Kean, M. Factors determining [12] de Saussure, F. Course in General Linguistics .Open [13] DeGroot M., H., and Schervish, M. J. Probability [14] Dunning, T. E. Accurate methods for the statistics [15] Evert, S. Corpora and collocations. In Corpus [16] Evert, S. The Statistics of Word Cooccurrences: [17] Fleder, D. M., and Hosanagar, K. Recommender [18] Gantner, Z., Rendle, S., Freudenthaler, C., [19] Goel,S.,Broder,A.Z.,Gabrilovich,E.,and [20] Goldstein, D. G., and Goldstein, D. C. Profiting [21] Herlocker, J. L., Konstan, J. A., Terveen, [22] Holtsberg, A., W. C. Statistics for sentential [23] Karypis, G. Evaluation of item-based top-n [24] Lee, J., Sun, M., and Lebanon, G. AComparative [25] Levy, M., and Bosteels, K. Music recommendation [26] Manning, C. D., Raghavan, P., and Sch  X  utze, H. [27] McNee, S. M., Riedl, J., and Konstan, J. A.
 [28] Park, Y.-J., and Tuzhilin, A. The long tail of [29] Pat e r e k , A . Improving regularized singular value [30] Rendle, S., and Schmidt-Thieme, L.
 [31] V argas, S., and Castells, P. Rank and relevance [32] Zhang, M. Enhancing diversity in top-n [33] Zhang, M., and Hurley, N. Avoiding monotony: [34] Ziegler, C.-N., McNee, S. M., Konstan, J. A.,
