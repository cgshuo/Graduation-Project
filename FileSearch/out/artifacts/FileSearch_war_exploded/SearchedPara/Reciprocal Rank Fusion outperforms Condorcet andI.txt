 Reciprocal Rank Fusion (RRF), a simple method for com-bining the document rankings from multiple IR systems, consistently yields better results than any individual sys-tem, and better results than the standard method Condorcet Fuse. This result is demonstrated by using RRF to combine the results of several TREC experiments, and to build a meta-learner that ranks the LETOR 3 dataset better than any previously reported method.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]:retrieval models General Terms: Experimentation, Measurement Keywords: fusion, aggregation, ranking
While supervised learning-to-rank methods have garnered much attention of late, unsupervised methods are attractive because they require no training examples. In the search for such a method we came up with Reciprocal Rank Fu-sion (RRF) to serve as a baseline. We found that RRF, when used to combine the results of IR methods (including learning to rank), almost invariably improved on the best of the combined results. We also found that RRF consis-tently equaled or bettered other methods we tried, includ-ing established metaranking standards Condorcet Fuse and CombMNZ (cf. [4]).

RRF simply sorts the documents according to a naive scoring formula. Given a set D of documents to be ranked and a set of rankings R ,eachapermutationon1 .. | D | ,we compute where k = 60 was fixed during a pilot investigation and not altered during subsequent validation. Our intuition in choosing this formula derived from fact that while highly-ranked documents are more important, the importance of lower-ranked documents does not vanish as it would were, say, an exponential function used. The constant k mitigates the impact of high rankings by outlier systems.
 Condorcet Fuse combines rankings by sorting the doc-uments according to the pairwise relation r ( d 1 ) &lt;r ( d which is determined for each ( d 1 ,d 2 ) by majority vote among the input rankings. CombMNZ requires for each r a corre-sponding scoring function s r : D  X  R and a cutoff rank c which all contribute to the CombMNZ score: CMNZscore ( d  X  D )= |{ r  X  R | r ( d )  X  c }| X 
We conducted four pilot experiments, each combining the results of 30 configurations of Wumpus Search applied to four different TREC collections. The results of the first, shown in table 1, indicated that k = 60 was near-optimal, but that the choice was not critical. The results also showed, somewhat unexpectedly, that RRF bested competing ap-proaches, as well as more sophisticated learning methods whose investigation was the original impetus for our work.
We repeated our experiment with four sets of submissions to TREC tasks; the particular sets were selected because they have been used in previous metaranking evaluation. It is worthy of note that, while our pilot runs used exactly the same set of Wumpus configurations to generate the in-dividual rankings on different datasets, the individual rank-ings in these experiments were exactly those submitted by TREC participants. Table 2 shows the RRF result, as well as the best individual, Condorcet and CombMNZ results. The MAP score for RRF exceeds that of Condorcet Fuse in all cases, and CombMNZ in all but one. RRF also outperforms the best ranking in each experiment, with the exception of TREC 9, where the best ranking was derived using a human-in-the-loop. RRF outperforms the next-best ranking, which was automated.

The pilot and TREC experiments indicate that RRF out-performs Condorcet, CombMNZ and the best system by 4% to 5% on average. We use a simple sign test to establish sig-nificance. Discounting the first pilot run, RRF outperformed Condorcet all 7 times ( p  X  0 . 008), outperformed CombMNZ 6of7times( p  X  . 04), and outperformed the best individual result either 6 or 7 times (0 . 008  X  p  X  0 . 04), depending on whether or not the manual result is considered. Thus all measured differences are significant.

Our final experiment used the sample learning results sup-plied with the LETOR 3 1 dataset, as well as a logistic gradi-ent descent method (LGD) which we are developing. For the purpose of analysis, we combined the seven sets of document-query pairs into one and computed an overall MAP score. research.microsoft.com/en-us/um/beijing/projects/letor for the same systems applied to three other test collections. Method TREC 2004 Robust track. We also computed the difference between RRF and indi-vidual MAP scores, 95% confidence intervals, and p-value (likelihood under the null hypothesis that the difference is 0). Table 3 shows these results. RRF betters all individual rankings ( p&lt;. 003), the best by a margin of 0 . 02 (4%); Con-dorcet is inferior to RRF ( p  X  . 004) while apparently bet-tering the individual rankings ( p  X  . 2). CombMNZ edges RRF by a small margin ( p  X  . 2). None of the measured differences among the baseline systems is significant.
For brevity, we report MAP as the measure of system per-formance. P @ k , R -precision, and NDCG yield comparable results.

RRF is simpler and more effective than Condorcet Fuse, while sharing the valuable property that it combines ranks without regard to the arbitrary scores returned by partic-ular ranking methods [4]. RRF requires no special voting algorithm or global information; ranks may be computed and summed one system at a time, avoiding the necessity of keeping all rankings in memory. We conjecture that RRF outperforms Condorcet because it is better able to harness diversity within individual rankings. One or two systems that rank a document highly can substantially improve its rank relative to the more popular documents. With Con-dorcet, a simple majority of weak preferences may overrule substantially stronger ones.

CombMNZ multiplies the sum of the uncalibrated scores of individual system by the sum of a binary quantization of each rank. It is perhaps not surprising that its results have higher variance, ranging from insubstantially better than RRF to substantially worse than Condorcet. We conjecture that this effect is due to the fact that, by happenstance, some scores are more amenable than others.

To our knowledge, no reported result matches or exceeds the performance of the meta-learner formed by applying fu-sion to the LETOR baseline rank learning methods. So the meta-learner constitutes the best known method, and the re-sult raises the lower bound of what is known to be learnable from the dataset. This latter question is a matter of some interest, as the MAP scores for LETOR 3 approach the 65% considered achievable with human-adjudicated relevance [5]. [1] Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., and Li, [2] Freund, Y., Iyer, R., Schapire, R. E., and Singer, [3] Joachims, T. Optimizing search engines using [4] Montague, M., and Aslam, J. A. Condorcet fusion [5] Voorhees, E. M., and Harman, D. K. ,Eds. TREC -[6] Xu,J.,andLi,H. Adarank: a boosting algorithm for
