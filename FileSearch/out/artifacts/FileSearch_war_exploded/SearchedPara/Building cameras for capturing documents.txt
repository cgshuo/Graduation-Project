 Stephen Pollard , Maurizio Pilu Abstract. This paper explores those aspects of docu-ment capture that are specific to cameras. Each of them must be addressed in order to close the gap between tak-ing a photograph of a document and capturing the docu-ment itself. We present results in five areas: (1) framing documents using structured light, (2) robustly dealing with ambient illumination when capturing glossy docu-ments, (3) improving text quality when using mosaiced color sensors, (4) robustly and passively recovering per-spective and image plane skew using text flow, and (5) measuring and undoing page curl using structured light and an applicable surface model. The ultimate success of subsequent document recognition will be heavily de-pendent on the successful completion of these tasks. 1 Introduction Since working with documents involves both electronic and paper media, the need to transfer data across the electronic divide is of increasing importance. While a variety of scanning technologies (e.g. flatbed, handheld, drum) are in common use, there is growing interest in the camera as a method for capturing documents. With digital technology developing at its present rate (i.e. the current crop of consumer cameras have five to eight mil-lion pixels) it is clear that image sensors will provide the resolution necessary to capture the content of full-page documents.
 to raw resolution, must be overcome in order for doc-uments to be captured effectively with digital cameras. We have examined a number of the most important lim-itations of digital photography for capturing document images, and in this paper we present an overview of this endeavor, with many of our methods and results pre-sented for the first time. and desk-mounted camera configurations. Clearly, the constraints, limitations, and advantages of each scenario are rather different, leading us to different solutions. For a desk-mounted camera the chief advantages of camera technology are the potential for a capture device with a very small footprint and the opportunity to capture documents face up. Furthermore such a device can offer multifunctionality, potentially doubling as a presentation device or a video conferencing camera (e.g. [1]). For a hand-held device the primary advantage can in part be attributed to portability [2].
 between the two modalities. In each case there are a number of fundamental issues that must be addressed in order to capture document images rather than just digital photographs of documents. The ultimate success of more specific document and text recognition tasks will depend heavily on how well the document is captured in the first place.
 itself it is essential that we can accurately and easily frame the document of interest, and preferably without perspective skew or image rotation. To this end we have explored the use of structured light to assist the fram-ing process, and in Sect. 2 we outline our approach and the results of user studies to compare it with traditional camera framing methods.
 are still a number of issues that must be overcome. In comparison to a flatbed scanner, ambient lighting is very unpredictable and is especially troublesome for glossy documents with specular reflections. In Sect. 3 we explore the thorny issue of document specularity and present a novel solution to the illumination problem that utilizes a dual flash/strobe.
 always important to get the highest effective resolution possible when capturing documents of text. At the same time, it is desirable to capture colour documents as well as monochrome. However, goals of good colour reproduc-tion and maximizing resolution are in direct conflict due to the way colour is captured using a single mosaiced sensor. In Sect. 4 we show that an alternative treatment of the raw colour data can lead to improved document resolution and improved OCR.
 results from the unconstrained relationship between the document camera and the document surface. This has two major components. In Sect. 5 we present a robust method for dealing with perspective skew and image ro-tation based on the structure of the lines of text within a document, while in Sect. 6 we present a way of dealing with page curl using structured light. 2 Document framing One major limitation of existing digital cameras with respect to capturing documents is the apparent difficulty of the task. Existing framing methods, such as optical viewfinders, were developed for scenic capture with the head held erect and a task to roughly point the camera at the centre of the scene. When it comes to capturing a document on a desktop this framing model is unsuitable and results, without extreme care, in poorly framed and perspectively distorted images. 2.1 Using a structured light pattern to aid framing In [3] we have proposed the use of structured light as an alternative to traditional forms of viewfinder. In our experiments the structured light patterns are generated using a collimated light source and a diffractive optical element (DOE). Figure 1a illustrates the arrangement; as the laser light is shone through the DOE, complex inter-ference generates a structured light pattern [4]. The de-tails of the pattern are dependent upon the microstruc-ture of the specific DOE used. Virtually any bi-level light pattern can be generated from a simple line or grid of dots to a complex shape such as text or a company logo. In practice, most commercial DOE pattern generators 1 tend to be rather small with total fan angles of less than 30  X  . However, we were able to obtain specially fabricated DOEs from the Optical Interconnect Group of the Solid State Application Department of Hewlett Packard Lab-oratories in Palo Alto, CA (now part of Agilent) with much larger fan angles (90  X  ), which we used in some of our experiments. Figure 1b,c also shows a number of de-vice examples that are able to frame a document using structured light. 2.2 User studies We have performed a number of user trials comparing different framing aids: an optical viewfinder, a tilting LCD and different configurations of structured light (2-corner, 4-corner, full rectangle and triangulation) [5]. framed, absolute offset in X and Y , scaling in X and Y , skew, rotation and handshake were measured for both horizontally and vertically orientated documents for a range of viewing distances. Figure 2b X  X  illustrates some of the experimental devices in use, and Fig. 2a shows how the image of a target document was processed to compute the experimental parameters.
 plane were found for reaction time and each of the accu-racy/error measures. A small but illustrative subset of the results is shown in Table 1. We found that all the laser-based devices significantly outperformed the opti-cal viewfinder and the LCD. In general, the triangulation device was the most accurate; however, despite this supe-rior performance, subjective data collected immediately following the experiments highlighted the requirement for additional cues to provide feedback to relay the suc-cess or non-success of the capture task.
 ments also showed that, as expected, motion blur tends to increase with viewing distance. Interestingly, however, the visual feedback afforded by the laser-based viewfind-ers served to reduce handshake in both X and Y di-rections. The graph also includes the case of using the elbow as a stabilizer, which resulted in large reduction in handshake with respect to freeholding the device at similar distances. 3 Specular reflections One of the most important technology hurdles for doc-ument cameras is controlling the illumination in such a way as to give the required image quality across all environments and for a sufficiently wide variety of doc-uments. A particular problem arises with glossy paper (especially in the presence of page curl, as is typically the case along the spine of a magazine page). The spec-ular reflection of an illumination source in the environ-ment from the glossy surface will typically be 500 times brighter than the ambient diffuse reflection of the con-tent of the document itself, and no amount of image processing and/or exposure correction can overcome its effects. 3.1 Modelling the specularity Most glossy surfaces are not perfect mirrors, and hence specular reflections, including those of point sources, tend to be extended. This is because the surface struc-ture of such material tends to be randomly perturbed either visibly or at a microscopic scale. The complete re-lationship between incident light at a specific angle and that reflected in a particular direction is contained in the bidirectional reflectance distribution function (BRDF) of a surface material [6]. In general, the BRDF must be measured experimentally. However, in computer graph-ics the specular part of BRDF is approximated using the Phong model [7], shown in Fig. 4, where the distribution of the specularity is given by a bell-shaped curve accord-ing to the function cos n  X  , where  X  is the angle between the line of sight and the principal (mirror-like) reflected ray. When n is very large ( &gt; 10000), the specular reflec-tion is mirror-like, while for smaller n it becomes more extended. We shall see later that the Phong model gives a useful approximation to the specular reflections of many glossy document surfaces. 3.2 Constraints on the imaging geometry Clearly there will be an angle  X  limit beyond which the contribution of the specular reflection term cos n  X  limit will become practically insignificant with regard to the imaging process. We shall call  X  this limiting angle in this paper.
 will appear to the camera.
 is located at infinity, the angle subtended by the specu-larity  X  will equal the deviation from specular reflection  X  , and hence the fall-off of the specularity in the image will also be predicted by the Phong model, and we can fit cos n  X  directly. Furthermore, the angular visual limit of the specular reflection within the image  X  max will give a direct measure of the limiting specular angle  X  . source is at approximately the same distance from the document as the camera itself, as illustrated in Fig. 5b. In this case,  X  no longer equals  X   X  it is approximately half  X  and the approximate Phong model at the image is cos n 2  X  (similarly  X  =2  X  max ). Notice that in both this and the previous case the angular extent of the specu-larity is independent of the viewing distance, and hence for larger object distances the size of the specularity will grow with respect to the underlying image structure. Fig. 5c, where the surface is cylindrical (a good model of local page curl). In this case, as the surface orienta-tion changes the overall change in reflected angle will be twice as large so that the total range of surface angles over which a specularity will continue to be visible will be plus and minus half the specular limiting angle  X  . Hence the physical extent of the specularity across a tight ra-dius cylinder will not tend to change significantly with viewing distance as it is tied more closely to the surface structure than was the case for specularities on planar surfaces. 3.3 Real specularities Figure 6 shows planar and cylindrical (60-mm diame-ter) specularities on HP Matte Photo Paper. Notice that specularity along the cylinder looks longer than that on the plane. This results from the additional intensity fall-off across the cylinder all along its length due to Lam-bert X  X  Law (that reflectance of a perfectly diffuse surface is proportional to the cosine of the angle between the incident ray and the surface normal), but the actual ex-tent of the specularity is in fact unchanged. If we fit the Phong model through vertical cross-sections of the image in Fig. 6 passing through the peak of each specularity, we get the results in Fig. 7, left and right, for the plane and cylinder respectively. In each case we must double the angle measured in the image  X  and fit cos n 2  X  because the illuminant is as far from the document as the camera (Fig. 5b). In each case the specularity is reasonably well fitted with an exponent n = 200 (only the offset and gain of the fit are changed between the specularities). Such a specularity falls to 5% of its peak value at 9 . 9 (half angle) and to 1% at 12 . 25  X  . Our studies [8] indi-cate that this represents a useful worst-case model for specular reflections.
 reasonably representative over the full range of flash lo-cations. Specifically, the angle subtended by the spec-ularity is largely independent of viewing distance, and hence for higher camera positions the size of the spec-ularity grows with respect to the document. Also, the shape of the specularity (as opposed to its location) is largely independent of the angle of the flash with respect to the document normal (although the Phong model is well known to break as the incident angle approaches the grazing angle).
 Fig. 8 for both a small illuminant aperture (12-mm di-ameter) and an extended aperture (the whole of the 42  X  55 mm aperture of a xenon flash). Note that, while the mirror-like reflections are very dependent on the size of the aperture, this is not the case for the more diffuse forms of specularity where the size is almost constant. This fact is very important as it shows that the worst-case performance cannot be improved by simply reducing the size of the illumination source, as may be expected naively. 3.4 Single flash issues As is well known in photography and industrial vision, the best solution for this kind of problem is to  X  X wamp X  the ambient light using a combination of fast strobe il-lumination (a flash) and a short exposure that tightly brackets it. If the exposure is short enough, then all the light from the flash can contribute to the image with-out allowing much build-up from the ambient specular reflection (or any other ambient light for that matter). However, for exposures much over 1 ms it may be pos-sible for the effects of the ambient illumination to be noticeable (but not dominant), in which case it may be possible to take a second snapshot with the same expo-sure (but no flash) and subtract this from the first to remove the ambient contribution.
 order to avoid specular reflections from planar surfaces) presents a number of issues. First, there is a considerable illumination profile across the document, though this can be ameliorated to some extent using a compensating re-flector. Second, small variations in the surface orienta-tion of the document lead to relatively larger changes in reflected intensity when compared to less oblique illumi-nation, and they also lead to shadows. Third and most importantly, for curled documents with a high gloss fin-ish it is still easy to produce specular reflections from the strobe; this is also true for 3D objects. 3.5 Dual-flash solution Given the near impossibility of removing glare, we have proposed a dual-flash approach [9]. In this scheme, rather than trying to eliminate the specularity from the docu-ment surface, we take two pictures with separately lo-cated glare spots such that a single document image that is free from glare can be constructed from them. This requires that the specularities produced by the two strobes be (practically) always spatially distinct on the document surface. Figure 9a shows that, given a first strobe location, in order to prevent specular reflections, it presents a constraint on the possible location of the second strobe. We have shown [8] that in fact the con-straint is remarkably simple and that anywhere in the field of view the angle subtended between the two strobes must be at least 2  X  (twice the limiting specular angle). Using a limiting specular angle value of 15  X  , which ac-counts for the vast majority of specular images we have measured plus a small safety margin, implies that the angle between the strobes must be at least 30  X  for ev-ery point in the field of view. Figure 9b shows a simple example of using this constraint for the situation of a one-third partial shift lens design (i.e. the field of view of the camera is asymmetric) where one flash is placed as close as possible to the camera. Note that, in practice, a similar analysis is done on the corners of the document, not just the top and bottom of the page. 3.6 Experiments with the dual-flash system We have built an experimental document capture rig (Fig. 10) based upon the DVC 1300C 1.3 megapixel colour camera 2 that uses the Sony ICX085AK sensor, which has 6.7- X  m square pixels. In one mode of oper-ation, it is equipped with a pair of controllable xenon flash units (Active Silicon, Uxbridge, UK) that deliver 1.0 Joule in 7  X  s. These are arranged symmetrically on either side of the camera, subtending an angle of in ex-cess of 30  X  over the whole field of view.
 rig are shown in Fig. 11a X  X . In each case the images with the left-and right-hand strobes are shown with the combined result in the foreground. Each image is com-pensated according to a calibration profile obtained with a clean white test card. This makes allowance for the il-lumination profile of the individual flash units and per-forms a white balance operation for the spectral proper-ties of the strobe illumination. As can be seen, all results are very promising, giving good even illumination and freedom from the glare present in the individual strobe images. Notice that in Fig. 11a the curl of the document is in the same direction as the separation between the strobe units, while in Fig. 11b the curl is in the orthog-onal direction. In either case the specularities are easily dealt with, but the distance between them is much larger (and hence robust) for the horizontal curl (Fig. 11b), as would be expected. The final example in Fig. 11c is more complex as it includes curl in every direction. Even in this case the method deals nicely with the problem. 4 Color mosaic imaging Digital colour cameras almost exclusively employ a mo-saiced image sensor in which a single CCD array uses three or more colour filters, with each pixel capturing a single colour. The most popular arrangement is the Bayer pattern, which has alternating rows of green-red and blue-green filtered pixels. Hence 50% of the pix-els are green, approximating luminance, and 25% each are red and blue, reflecting the lower spatial sensitivity to chrominance of the human visual system. Further-more, to overcome sampling errors in the undersampled colour channels, an optical anti-aliasing filter is usually employed immediately on top of the sensor.
 point of view of capturing high-resolution images of text. While a colour capture device is desirable and increas-ingly becoming essential, it is unfortunate that we must suffer such a penalty in terms of monochrome text reso-lution in order to achieve it. The first and most obvious thing to do is to dispense with the anti-aliasing filter, though this does tend to introduce some aliasing arte-facts. However, this still leaves the issue of full-colour-plane reconstruction, or demosaicing, which requires the interpolation of each colour plane (see [10] for a review of interpolation schemes). 4.1 HiPass demosaicing In order to maintain as high a text resolution as pos-sible and reduce aliasing artefacts, we have developed the HiPass demosaicing method [11] outlined in Fig. 12. The method attempts to decompose the mosaic into a high-frequency monochrome and low-frequency RGB data. These can then be combined to approximate the true full-colour image. While some chrominance edges are attenuated, the method provides full-resolution monochrome text and avoids most colour aliasing arte-facts for photographic images.
 saic (shown expanded into red, green and blue colour planes), the corresponding pixel in the HiPass mosaic is constructed by subtracting from it a corresponding low-pass red, green or blue pixel constructed from the appropriate colour plane of the mosaic. Unfortunately, the HiPass mosaic tends to suffer from checker-boarding artefacts along chromatic edges where alternating pixels are derived from the different primary coloured pixels. This problem is corrected using a variant of the stan-dard gradient-based interpolation schemes outlined in [10]. For example, each pixel of the HiPass mosaic that is derived from a red or blue pixel can be corrected using 1D quadratic interpolation of the five neighbouring pix-els in the vertical or horizontal direction depending upon which has the lower intensity gradient. The interpolation is of the form C 0 =(2 I 0  X  I  X  2  X  I 2 +4 I  X  1 +4 I 1 ) I and C are the intensities of the HiPass mosaic and its corrected version, respectively. The corrected HiPass mo-saic is then added to each of the low-pass colour images to generate the fully reconstructed image. 4.2 Results and OCR tests Figure 13 shows how HiPass demosaicing gives im-proved monochrome text when compared to a demosaic-ing scheme optimized for photographic reproduction in the presence of an anti-aliasing filter (in this case the de-mosaicing scheme used is the HP850 digital camera). In each case the same raw mosaic image data were used as input. This improved image quality feeds straight through to OCR errors, as shown in the block graph of Fig. 14b for test charts similar to the one in Fig. 14a. In each case text is captured at 150 dpi and interpolated to 300 dpi after demosaicing with the option of apply-ing sharpening (using the standard unsharp mask [12]) in between. It is clear that with or without sharpening HiPass demosaicing results in much improved OCR with almost half the error rate in each case.
 captured with a camera (DVC 1300C) and a scanner (HP ScanJet 6100C). Thanks to HiPass demosaicing the results from the camera are very close to those of the scanner despite the fact that the original image was in the form of a mosaic.
 5 Correcting perspective page skew One of the main disadvantages of capturing a document with a camera is the introduction of geometric distor-tions dependent upon the camera orientation, in partic-ular perspective skew, as typified in Fig. 16a. tified a robust and general solution to rectifying text or highly structured document images that can be seen as complementary to other approaches present in the liter-ature.
 well known [13], the problem of passively and robustly detecting the geometric features needed is still an open problem.
 to correcting for document rotation, including some re-cent work explicitly addressing camera-based document imaging such as [14]. The main methods used include many variations of projection profile approaches, Hough-inspired techniques [15] and nearest-neighbour clustering [16]. An interesting approach that employs perceptual organization principles is [17], although it makes the as-sumption of parallel cues. Techniques dealing with the recovery of vanishing points from images are numerous, some using edges (e.g. [18]) and others using texture and other soft cues [19]. However, both edges and regular tex-tures cannot be generally assumed in text documents. extract text from perspectively skewed documents, but it does so using the document quadrilateral, which we do not assume is always visible. 3 through the detection of generic linear cues that can be used as geometric primitives to determine the document plane orientation with respect to the camera and hence the rectifying homography [22, 24], which allows us to produce the deskewed document (Fig. 16b). To do so we first detect horizontal linear cues, which we use to rectify the document such that text lines become horizontal and parallel. We then extract linear vertical clues that are used to perform the final rectification. 5.1 Illusory cues Figure 16a shows several kinds of linear cues that may arise in practice. Cue A is a vertical illusory [25] cue; cue B is a vertical hard line , in this case the projection of the actual document edge; cues C are horizontal illu-sory lines, inferred from the arrangement of characters into text lines; cue D is a horizontal hard line; finally, cue E is a quadrilateral that can be either illusory or cor-responding to an actual rectangular outline in the docu-ment (e.g. a figure box or the four document boundaries). Hard edges of types B and D can be detected rather triv-ially in many ways (e.g. the Hough transform) and will not be addressed in this paper. Illusory edges such as C and A are difficult to find reliably in practice, and most of the literature on document analysis has focused only on the problem of recovering groups of parallel text lines to compensate for rotation for OCR and other scanning applications. 5.2 Extraction of horizontal linear cues The algorithm to extract the horizontal illusory lines, which is described in detail in [22], is summarized here. A preprocessing stage [26] binarizes the input image, turn-ing it into blobs representing either single characters or (portions of) words or lines, depending upon the font size and the resolution considered (Fig. 17a). These blobs are divided into elongated (major axis longer than thrice the minor axis) or compact types. A pairwise saliency mea-sure is computed for pairs of neighbouring blobs and represents how likely the blobs are to be part of a text line. A network (Fig. 17b) is then built using the blobs and their associations. The network is then traversed to extract salient linear groups of blobs, which constitute the illusory horizontal cues (Fig. 17c). Isolated elongated blobs are also considered as individual cues. Note that the saliency measure used is based on perceptual organi-zation rules and is hence font type and size independent. 5.3 Partial rectification Once we have a pool of horizontal cues (Fig. 17c), we can perform a partial rectification (or deskew) of the document.
 using linear regression. Figure 17d shows an example of the fitted lines now representing the linear cues (short lines). The second step is to find the vanishing point [27] v x ,v y in the image. There are several techniques in the literature, but most are impractical here due to the relatively imprecise nature of the linear cues. However, we do obtain excellent results using a RANSAC approach [28] in the linear cue feature space to explicitly fit a line bundle of the form y  X  v y = m ( x  X  v x ). Some bundle lines are shown in Fig. 17d.
 clearly not possible, and hence the second step of this partial rectification is to find a homography in the image plane that makes all the lines of the bundle horizontal. The homography we compute follows the suggestion of [29], which argues that amongst infinitely many plane homographies that would perform this transformation, good results are achieved with the homography closest to a Euclidean transformation. This homography is a concatenation of four transforms, shifting the image to the origin, rotating the bundle to position the vanishing point on the horizontal axis, sending the bundle to in-finity, and shifting the image back. More details can be found in [22]. Figure 18a X  X  shows four examples of the application of this partial deskewing. 5.4 Extraction and use of illusory vertical cues In the absence of true vertical edges (cues B in Fig. 16a), it is necessary to extract illusory cues arising from high-level text organization, such as the paragraph X  X  left jus-tification. zontal cues in that we use an association network of blobs (Fig. 19a) that is searched for linear groups. However, the way the associations are made is substantially dif-ferent. In fact, since the vertical associations are rather weak, we have noticed that it is best not to potentially further weaken associations based on saliency measures such as those used for the horizontal cues. Rather, we have decided to only reject associations based on near impossibility and let all the others propagate until fur-ther committal. The rejection rules are simple and based on perceptual organization; more details can be found in [22].
 cally simplifies the network while preserving most of the associations belonging to actual illusory vertical cues, albeit still amongst other meaningless ones. Finally, a greedy split and merge strategy is used to group all these associations into extended near-vertical linear groups as shown in Fig. 19b. The strongest groups are elected as vertical cues, but amongst these incorrect ones always crop up. Figure 20 shows six examples of vertical cues detected.
 vanishing point and a few correct vertical cues that can be used to determine a second vanishing point, we have all the geometric information [23] to both test the cor-rectness of vertical cues (if we know the focal length) and then perform full rectification, resulting in an undis-torted document like that shown earlier in Fig. 16b. 6 Undoing paper curl distortion When scanning a book or document with a flatbed scan-ner, the user can minimize page curl effects by pressing it against the platen. However, when capturing the same document with a camera it is often not possible to con-trol page curl, which results in the situation shown in Fig. 21.
 can be corrected using the 3D profile of the surface to generate an undistorted version of the document. lem, namely designing a viable, cheap method to collect 3D profile data and undoing the curl using it. In this section we present an overview of the techniques, which are discussed in more detail in [30, 31] and [32]. 6.1 Recovery of 3D information We use structured light to recover the 3D profile of an imaged document. Specifically, we use a 2D structured light pattern, as opposed to a single stripe or point, as it avoids the requirement for expensive moving parts (and electronics) and allows us to recover the page curl in a single shot and not by sweeping the 1D pattern over the page, capturing a single image for each pattern position (which requires several seconds with a standard camera). Pagewide structured light projector and modelling. Fig-ure 22a shows the prototype we have built to demon-strate the approach. We used a 2D light pattern in the form of a set of 15 stripes generated by a laser with a diffractive grid that covers an A4 document. The structured light projector was manufactured by Lasiris (now part of StockerYale, Inc., Salem, NH, USA; http://www.stockeryale.com ) and uses a patented diffractive grid technology (illustrated in Fig. 22b) to produce non-Gaussian stripes and various other pat-terns. For generating a set of stripes, the product clev-erly uses two diffractive grids. The first one transforms a laser beam into a line by effectively having many Gaus-sian light profiles overlapping, each centred around peaks of the diffraction. The second one diffracts this line into 15 lines. This particular laser was chosen because it had the right parameter configuration to cover a whole A4 page with its stripes at a distance of about 30 cm. sheet is subject to conical aberration and is hence a conic surface. From information provided by the manufacturer of the diffractive gratings we were able to use specific diffractive optics equations to model the peaks of each stripe. However, what we actually needed was a way of mathematically modelling the entirety of each light sheet to be able to express the triangulation in closed form. To do this we have pragmatically used the diffractive model to create another, more useful model consisting of the equation of each light conical sheet [30, 31]. Stripe identification and triangulation. In order to per-form the triangulation, it is necessary to determine where lines are and which one corresponds to which conical sheet [30, 31]. There are two distinct parts in this pro-cess, stripe detection and stripe labelling, which ensures that we know which conical light sheet (and hence its equation) generated which detected stripe we see on the document. some assumptions suitable to our context: the acquisi-tion of 3D data for a static document can be simply done by capturing two images, one including the laser pattern and one without. In this way we always have two per-fectly overlapping images, which allows us to use image differencing to extract the stripes. Even so, it was neces-sary to employ an enhancing operator to the image dif-ference. Given the prevalently horizontal lines (Fig. 23a), the operator we used was a 1D Laplacian kernel (second derivative) applied only to the direction orthogonal to the stripes.
 which is very robust in general situations and can smoothly cope with gaps. First, the binary stripes are thinned down to a single pixel and connected pixels joined together into strings using a standard connected-component module. Next, strings that are too short are removed from the classification and deemed noise. Then, for each string, a heuristic strength measure is computed and, for each image column, starting from the top of the image, we assign successive, increasing label numbers to, and only to, the 15 strongest stripe points. Finally, for each string, we assign a label equal to the most frequent label assigned to all the points of that string. angulation, a well-known method for laser-based range finders (see [33, 34] for a good introduction) that con-sists of finding the intersection between the known coni-cal sheet of light and the optic ray going through a given point of the stripe projection on the image. An example of the semi-sparse data we obtain is given in Fig. 23b. 6.2 Undoing curl This section is concerned with the reconstruction of the flat state of a curled document or book with the sup-port of sparse depth measurements of its surface with a method such as that of the previous section. We devel-oped a novel method based on a geometrical model of paper deformation and a relaxation algorithm that al-lows us to fit this model to the data and subsequently unroll it into a plane so as to produce an undistorted version of the document.
 The problem and related techniques. A number of tech-niques have attempted to correct page curl using some form of depth measurement (e.g. [35 X 37]), but each of them has been restricted to the recovery and correction of curl using a simplified cylindrical model of deforma-tion rather than the more general case of interest here. unroll it to a planar state. However, a general surface cannot be unrolled to a plane without stretching or tear-ing, and thus this approach would inevitably cause local and global distortions. Figure 24 illustrates this point. A cloud of imprecise surface measurements has been smoothed and a B-spline fitted, which exhibits little bumps in some regions due to the inevitable measure-ment errors and bias. If we want to undo the curl of the document and make it look flat, we have to texture-map patches from the original image onto patches of a plane. This mapping could be computed, for instance, by integration of finite differences in the meshed sur-face, as shown in Fig. 24. However, by definition, a non-applicable surface can only be unrolled into a plane by either tearing or stretching, which would cause unnat-ural distortions in the image of the unfolded document image. In addition, due to the integrative nature of un-rolling a surface, locally small errors tend to build up and lead to unsightly distortions at the edges, as shown in Fig. 25a.
 comes from cloth deformations modelling in computer graphics, as inextensible cloth has the same differential geometry properties of paper [38]. In particular, [39] ex-plicitly tries to map a flat texture onto an arbitrary sur-face and points out that the mapping is distortion free (isometric) only if the surface is applicable; this tech-nique, although addressing the opposite problem, resem-bles the technique we used.
 Modelling curled paper with a discrete applicable surface. Curled paper is mathematically represented by an appli-cable surface, which has the property of being isometric with a plane and thus easily unrolled. A surface con-tinuous in G 0 is called an applicable surface when its Gaussian curvature vanishes at every point [40]. For this very reason, applicable surfaces are isometric with the plane and can be flattened without stretching or tear-ing. The analytical definition of an applicable surface is impractical, and thus we use a triangular truss (mesh) as a finite element approximation to the surface. The mesh can deform, but if adjacent node interdistances are kept constant, the mesh is the best discrete approximation to a developable surface afforded by a particular mesh res-olution and topology. Making the mesh finer can make the approximation error smaller, allowing us to model, for instance, paper creases. However, in general it is not possible to split triangles and refine the mesh locally in an adaptive fashion to reduce the fitting error once the mesh has started deforming since the resulting mesh might not be unrolled into a plane.
 Fitting the discrete applicable surface to 3D data. First, a mesh of suitable dimension, with known interdistances between nodes, is initialized to the sparse 3D data points; at this stage the surface could not possibly be unrolled into a plane, as nodes have moved about. Next, an iter-ative optimization method adjusts the points such that the initial distances between them are restored, thereby producing isometry with the plane. In [32] it is shown that this process converges to a developable state and that, more importantly, in doing so it optimally approx-imates the noisy 3D data with an applicable surface. torted version of the document. One of the advantages of using the process illustrated here is that we do not need to unroll the fitted surface into a plane. In fact, as we have started with a plane in the first place, we al-ready have a one-to-one correspondence between tiles of the fitted mesh and the tiles of the plane.
 of the curled book shown in Fig. 20. The image of Fig. 25a has been reconstructed by fitting a B-spline sur-face to lightly smoothed 3D input data and unfolded by integration starting from the centre. Where there are no 3D data, the reconstruction goes astray (see e.g. the top-left side of the book), causing considerable distortion. On the other hand, the result in Fig. 25b is the output with the proposed method. Note that the reconstruction presents far less distortion (even in the areas not cov-ered by 3D data  X  see the gaps between the stripes of Fig. 23a ) and the page edges are straight. The slight indentation on the book spine is due to the coarse mesh representation used (20  X  15 nodes), which could not fol-low accurately the spine curvature. By using a denser mesh we can easily overcome this problem, but at the cost of a longer convergence time. The slight residual global distortion is due to a bias in the 3D data caused by imprecise calibration of the cheap 3D acquisition sys-tem used. 4 7 Conclusion In this paper we have explored those aspects of document capture that are specific to cameras. In particular, we have presented the following results and methods. of user trials that show that direct visual feedback using structured light provides a very effective form of framing for document capture with a camera-like device. Subjects were both faster and more accurate with such framing aids than with traditional solutions. Even very restricted forms of direct visual feedback better enable users to frame documents and tend to reduce the presence of handshake, which in turn allows for increased exposure times.
 based document capture devices is their tight control of illumination. In an attempt to mirror this for a desktop document camera we proposed to overwhelm ambient il-lumination using a flash in conjunction with a tightly bracketed exposure. This is particularly important for glossy media such as magazine pages where the specu-lar reflection of an ambient light source would otherwise overwhelm the situation. We have shown that for a dual-flash configuration in which the angle subtended between the two strobes is greater than twice the limiting angle, it is unlikely for specularities from the two flashes to overlap and a single image free from glare to be robustly generated.
 ment resolution can be obtained from a mosaiced colour sensor if we dispense with the anti-aliasing filter and use a novel demosaicing algorithm to interpolate the miss-ing colours. This in turn leads to improved OCR perfor-mance when compared with methods designed to opti-mize photographic quality.
 the perspective skew that is typical of hand-held cap-ture. We show that even in cases where the full page is not visible (as the page boundary is what is typically used to perform perspective deskew), we can use the lit-tle structure available in the document to compute the rectifying homography that can be used to perspectively correct the document.
 of camera-based document capture, page curl. We have shown that cheap diffractive optical elements can be used to generate structured light from which we can recover sparse 3D profile information. This information can be used to dewarp the document, and we have done so using a geometrical model of paper fitted onto the sparse and noisy 3D data.
 only if we address each of these issues will the gap be-tween taking a photograph of the document and captur-ing the document itself be closed. It is clear that each of them is of great importance if we are to build a real document capture device based on camera technology. References
