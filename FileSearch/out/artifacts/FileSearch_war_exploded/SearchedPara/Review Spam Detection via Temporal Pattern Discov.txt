 Online reviews play a crucial role in today X  X  electronic com-merce. It is desirable for a customer to read reviews of products or stores before making the decision of what or from where to buy. Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews. We ob-serve that, in reality, a great portion ( &gt; 90% in the data we study) of the reviewers write only one review (singleton re-view). These reviews are so enormous in number that they can almost determine a store X  X  rating and impression. How-ever, existing methods did not examine this larger part of the reviews. Are most of these singleton reviews truthful ones? If not, how to detect spam reviews in singleton reviews? We call this problem singleton review spam detection .
To address this problem, we observe that the normal re-viewers X  arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating. Thus, we propose to detect such attacks via unusually correlated temporal patterns. We identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlations. In this way, the singleton review spam detection problem is mapped to a abnormally correlated pattern detection prob-lem. We propose a hierarchical algorithm to robustly detect the time windows where such attacks are likely to have hap-pened. The algorithm also pinpoints such windows in dif-ferent time resolutions to facilitate faster human inspection. Experimental results show that the proposed method is ef-fective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores. H.2.8 [ Database Applications ]: Data Mining Algorithms, Experimentation Review spam, time series, adversarial data mining
Online reviews and ratings about products and stores are essential parts in today X  X  electronic commerce where they provide helpful information for potential customers. A prod-uct or store with a decent rating and a high proportion of positive reviews will attract more customers and larger amount of business, while a couple of negative reviews/ratings could substantially harm the reputation, leading to finan-cial losses. Since there is no rule governing online reviews and ratings, some product providers or retailers are leverag-ing such public media to defame competitors and promote themselves unfairly, or even to cover the truth disclosed by genuine reviews. For example, suppose a customer finds the delivery service of a certain store unacceptably slow, she then writes a review about the fact and gives it a low rat-ing on a review website. This review and rating present a unfavourable impression of the store to potential customers, who might choose other stores after reading that review. In order to avoid the drainage of business caused by this negative yet truthful review, the store could employ or en-tice a group of people to write undeserving positive reviews about the delivery service. Similarly, the store could also ask these people to write unfavorable reviews about its competi-tors, from which the store would like to distract customers. These hired reviewers are called spammers and the reviews they write are called spam reviews. In order to protect cus-tomers, honest online stores and the whole electronic com-merce environment, it is desirable to detect spam reviews and take proper measures.

Previous works propose to use features of review contents and reviewers X  behaviors [5, 10, 11, 1] or graph connecting reviewers, stores and reviews [6], to detect spam reviews. These methods work best in the situations where spammers write many reviews (see related work). In reality, however, most reviewers write only one review. For example, 68% of the reviewers write a single review in the Amazon re-view dataset studied in [10], and this percentage is 90% in the dataset we study here. If a review is the only review a reviewer has written, we call it a singleton review (SR for short). In fact, as we shall see later, the SR definition can be generalized to cover reviewers with a few reviews, not necessarily just one. One question is: are most of these SRs honest reviews? The answer is probably not, due to the na-ture of spam attacks. For a store to rapidly raise its fame and rating (resp. defame others) it is desirable to have spammers post plenty of favorable (resp. unfavorable) reviews about it (resp. its competitors) in a short time. Usually a spammer would not post many reviews with similar ratings for a store under the same name. Instead, he would rather write spam reviews under different names to avoid being caught. This spamming strategy brings a large number of SR. Most of the statistics adopted by previous works would not work on such singleton reviews. For example, the mean and standard deviation of ratings given by a reviewer [5] become meaning-less if this reviewer has written only one review; in the rule or frequent pattern based detection method [11], singleton reviews are also ignored due to their low significance.
In the present paper, we focus on singleton review (SR) spam attack detection, and  X  X pammer X  refers to  X  X R spam-mer X  if not otherwise specified. According to the above ob-servations, we propose a novel approach that maps the SR spam detection problem to a abnoramlly correlated tempo-ral pattern detection problem. The proposed algorithm is based on multi-scale multidimensional time series anomaly detection. In particular, we construct statistics whose joint anomaly could be a strong indicator of SR spam attacks. The identified statistics includes the average rating, the to-tal number of reviews, and the ratio of singleton reviews among all reviews. We collect these statistics from ratings and reviews for each store to build the multidimensional time series, base on which we develop an SR spam detec-tion model. We combine temporal curve fitting and LCS (Longest Common Sub-sequence) algorithms to find out ab-normal sections in each dimension of the time series. We then devise a ranking-based algorithm to consolidate the anomalies in all dimensions to find out temporally correlated abnormal sections. Furthermore, since short term fluctua-tions are common in the constructed time series, we start with a larger time window (e.g. 2 months) to smooth out such noisy changes of the time series, so that any significant abnormal pattern can be detected robustly, therefore reduc-ing false positive rate. After any singleton review attack is detected, we scale down the window size, so that the exact abnormal points become more obvious and one can quickly locate the suspicious reviews.

This paper makes the following contributions:
In the data studied here and in [10], we can observe that the reviewers who write only one review (a singleton review) dominate the body of reviewers. The sheer number of these reviews implies that the rating of a store or product, and po-tentially the customers X  choices can be greatly manipulated by these reviews. This property make SR particularly at-tractive to spammers. Despite the significance of detecting SR spams, no existing work has addressed this problem. In-deed, detecting SR spams can be challenging. If a reviewer has written only one review, simply looking at this singleton review reveals little information about the true intention be-hind it, so it is hard for machines or even human beings to draw a conclusion. For example, in the experiment, we find that one reviewer said X  X or the few times that I X  X e contacted customer service via phone, email, or chat, the person has always been helpful and gone out of his/her way. X  At a first glance, this is a normal review talking about customer ser-vice. Since it is the only review the reviewer has written, existing spam detection algorithms will simply ignore this review. Even for human beings, it is extremely difficult to tell if this is a spam review or not. However, if we look at the aggregate reviewers X  behaviors in a temporal way, we can find that this review was written in a period when there was a burst of SRs and the rating of the store went up dramat-ically. It is abnormal for the number of reviews, the ratio of SR and the store rating to be temporally correlated, this review is pretty suspicious.

In particular, spammers have to write many positive (or negative) reviews in a short time, otherwise, the spammers are not effective in promoting or defaming a store or prod-uct. However, if a spammer posts his reviews quickly under thesamename,hecanbeeasilydetectedbycheckingthe duration between two consecutive reviews with similar rat-ing from a single person. So writing spam reviews under different names is a safer way. Based on the above reason-ing, we make the following conjectures on SR spam attacks. When such an attack occurs in a certain period, there tends to be a sharp increase in the number reviews and the ra-tio of SRs, together with an increase (or decrease) in the average rating. Therefore, we can transform the SR spam detection problem to abnormally correlated temporal pat-tern detection in a multidimensional time series consisting of the above three indices. Note that spammers may want to evade the proposed method by writing more than one but not too many reviews. For these spammers, we can easily modify the algorithm to catch them (see Section 3.2.1) and we focus on the detecting SR spams. In the next section, we make several assumptions about reviewers X  arrival patterns, which define a necessary condition of SR spam attacks.
It is the difference in spammers X  and genuine reviewers X  behaviors that motivates the proposed algorithm. We show Figure 1: Relationship of Spammers, Reviewers and Customers the relationship of reviewers, customers, spammers and SR spammers in Figure 1. Note that the areas do not reflect the actual number of groups of people. On one hand, we define a reviewer as a name under which at least one re-view is written. A customer is a person who has shopping experience with at least one store, and becomes a reviewer if he/she contributes a review. On the other hand, there are two categories of spammers, namely, SR spammers and other spammers. We assume a customer cannot be a spam-mer (the customer set does not intersect the spammer set in Figure 1). Indeed, though it is possible for a store to entice its customers to write favorable reviews for it, the store has to take the risks of the disclosing of such dishonesty to the public by its customers and harming the store X  X  reputation. Instead, a store is more willing to hire dedicated spammers. Also, a spammer might later become a customer and write a genuine review, but our aim here is to detect any spam-ming activities, instead of telling spammers from customers. Therefore, we assume that a customer cannot be a spammer.
We make certain assumptions of reviewers X  behaviors, di-vided into two phases: the arrival and writing phase. In the arrival phase, a customer buys something from a store or a spammer is hired or enticed by a store to write fake reviews. The writing phase is when a reviewer writes a review. There are mainly three patterns of arrival phase behaviors: normal arrival, promotion/sale event arrival, and spam attack pat-tern. First, the normal arrival pattern can be modeled by a homogeneous Poisson process with a fixed rate  X  . A Poisson process is a set of random variables { N ( t ): t  X  0 } satisfying the following properties [9]: where N ( t ) is the number of arrivals up to time t .  X  is a constant controlling the intensity of arrivals, with a larger  X  indicating more arrivals in a unit of time. Second, it is pos-sible for a store to promote their products over a period and therefore increase the traffic of customers and reviews. We model this arrival pattern using a non-homogeneous Pois-son process, with the rate parameter being a function of time  X  ( t ). Third, the spam attack arrival pattern is pretty much like that in the promotion mode, since a large number of spammers would be hired or enticed by the store.
In the writing phase, we model the writing behaviors of normal reviewers and spammers. First, in order to get the rewards offered by the store that tries to commit SR spam attacks, spammers tend to write spam reviews in a hurry, and there is seldom a delay in posting spam reviews. There-fore, we assume that the time when a spammer writes a review is the same as the time she arrives, and the spam re-views X  arrival pattern is the same as spammers X  arrival pat-tern, a bursty one. Second, for genuine reviewers, we claim that there are some random factors associated with the de-lays in posting their reviews after their shopping experiences, by the following reasons. A genuine reviewer seldom writes a review right after she shops with a store. Instead, most of them would write their reviews after receiving and trying out the products for some time. Therefore, one random fac-tor is the time spent on delivery, this factor depends on how a customer and a store choose the way of delivery, the traffic and logistic conditions and so on. Another random factor is the time spent on tryouts, which depends on customers X  per-sonal behaviors. The randomness associated with the delay in a genuine reviewer X  X  posting a review smoothes out the arrival intensity of reviews, even in a promotion event. In orther words, their reviews are less likely to concentrate in a short period and causing bursty peaks.

According to the above analysis, a spam attack tends to create a burst on the review arrival process, which is dis-tinct from the normal and even promotion review arrivals. Nonetheless, as fluctuations in the volume of reviews do ex-ist, bursty patterns in review arrival do not necessarily imply SR spam attacks. Observe that spammers are brought to-gether to bring up or down the rating of a store, the ratings of spam reviews are more likely to correlate with these re-views X  arrivals. In contrast, because genuine reviewers X  opin-ions about a store vary wildly, depending on their satisfac-tion with speed of delivery, quality of products and customer services, etc. If we average the ratings of genuine reviews in a certain period of reasonable length, the positive and negative ratings will cancel out each other, therefore, the average ratings should be stable over time and independent of genuine reviews X  arrivals. In summary, we should look at the joint abnormal patterns in review arrival and rating to detect such attacks more robustly.
According to the above assumptions, we propose an SR spam detection approach based on correlated abnormal pat-terns discovery in multidimensional time series. To raise or lower the rating of a store safely and rapidly, spammers tend to post a large number of reviews with a high or low rating under different names. If there is a sharp increase in the vol-ume of (singleton) reviews with rating increases or decreases dramatically at the same time, it is highly likely that the rat-ing is manipulated by the newly arrived reviews. Therefore, detection by exploiting the correlation between ratings and volume of (singleton) reviews is more robust compared to us-ing any single time series. In the paper, we will focus on the case where the rating goes up dramatically. The case with rating going down dramatically can be similarly modelled.
In the remaining of this section, we first describe how to construct multidimensional time series capturing reviewers X  behaviors (Section 3.2.1). Then in Section 3.2.2 we describe algorithms to detect correlated anomalies in all three time series. Lastly, in Section 3.2.3, we describe the proposed hierarchical framework for robust SR spam detection.
The detection approach is based on time series of the num-ber of reviews, average ratings and the ratio of singleton re-views. The data we study here is a set of reviews with texts and ratings posted for different stores on a review website in a certain time period. To construct these time series, we dis-card text information and keep the posting time and ratings of the reviews. This is reasonable as there exist other spam detection algorithms utilizing text information, so they are complementary methods to the proposed algorithm. The resulting data can be seen in this way: each store s has a series of ratings sorted in ascending order of posting time. where n s is the number of reviews for store s ,and ts i is the time stamp when r i is written, ts i  X  ts j for all 1  X  i&lt; j  X  n s . After choosing the time windows size (denoted by  X  t ), the time interval under investigation (denoted by I = [ t ,t 0 + T ]) can be divided into N = T/  X  t consecutive time windows or sub-intervals. Each time window is of length  X  t and contains reviews posted during that time window. Let I n denote the n -th time window, so Given a time window I n , we compute the average rating f the number of reviews f 2 , and the ratio of singleton reviews f . Formally, f 3 ( I n )= |{ r j : ts j  X  I n ,r j comes from an SR }| where | A | denotes the cardinality of the set A . Given a store s ,timeinterval I =[ t 0 ,t 0 + T ] and time window size  X  t , these aggregate functions represent a three dimensional time series and can be collectively represented by where f i ( n ) is a shorthand for f i ( I n ), i =1 , 2 , 3. In the following, we drop the index on stores and let F ( I,  X  t )de-note the time series constructed for a certain store. The way we construct these time series can be generalized to handle spammers who write just a few reviews with similar ratings. We can simply treat all the reviews as SR by ignoring re-viewers X  ids, then the way we construct these time series still makes sense and the proposed algorithm can detect SR attacks (see next section).
Given the three time series of a store, we would like to find out correlated abnormal blocks on all three series. In other words, these blocks should simultaneously present sudden increases in rating, ratio of singleton reviews and the number of reviews. Here we focus on the singleton review detection methodology based on burst detection algorithms. Instead of inventing a novel burst detection algorithm, which is not the focus in this paper, we use a three-step approach for the detection. First, on each dimension, we employ a Bayesian change point detection algorithm [4] to fit curves using the time series (other curve fitting algorithms will do the job, too). As an example, we plot the time series along with the fitted curves in Figure 2. We then apply a simple template matching algorithm to the fitted curves to detect bursty pat-terns. Lastly, a sliding window finds out the blocks in time series corresponding to a joint burst in all dimensions of the time series. In the above example, a joint burst is high-lighted by the red box in Figure 2.

Assuming that we have obtained the fitted curves, we describe in what follows the last two steps in details, for the curve fitting algorithm, please refer to [4]. Let C = { c 1 , c 2 , c 3 } be the fitted curves of the three dimensions of a time series. All curves have the same length (number of samples), which is also defined as the length of C . First, we want to detect sudden increases in each of the three curves separately. Based on the description of the arrival process in Section 3.1, this can be transformed to the problem of template matching. We use a step function-like template to represent a sudden rise in values Note that one can use other values for v so long as it rep-resents a sharp increase temporally. If a block on a fitted curve c = { c 1 ,...,c n } X  C is found to X  X atch X  X his template well, then we find an anomaly of interest on the curve. One can obtain all blocks of c by sliding a window through c , and all points falling into the window form a block which is denoted by where 1  X  i k  X  n for k =1 ,..., 5and i k +1 = i k +1 for k =1 ,..., 4. Note that the length of a block is chosen to have the same length as the template. We use a modified longest common substring (LCS) for matching [13] between v and b . In general, suppose we want to find the degree of match between two sequences z 1 = { z 1 1 ,...,z 1 n } and z { z 1 ,...,z 2 n } . Without loss of generality, one can think of z 1 as v and z 2 as b . In the modified LCS, how well two sequences match each other is measured by the number of points in one sequence matching those in the other sequence. By a  X  X atch X  between two points, we mean the absolute difference between the values of two points is less than a given threshold . The modified LCS algorithm uses the following dynamic programming formula to find out how many matches occur between z 1 and z 2 ,for0  X  i, j  X  n and | i  X  j | X  1: where M ( i, j ) records the number of matches between sub-j | X  1 makes sure that, z 1 i  X  z 1 is not matched to a point z  X  z 2 far away from the position of z 1 i . Figure 2: Bursty Patterns Detected in Store 24779 Algorithm 1 Bursts Detection in Single Time Series ( BD-STS ) Input: fitted curve c ,template v
Output: top k ranked periods with bursty pattern m =lengthof c . n =lengthof v . for i =1  X  m  X  n +1 do end for return Periods corresponding to top k values in s .
Algorithm 1 describes bursty pattern detection in a single time series. For each block b on a fitted curve c , we first normalize it. Then the modified LCS procedure described above finds out the number of matches between the template and the normalized block b . By this step, we can find out, in the time series, the locations corresponding to bursty pat-terns. Taking the degree of burst into account, the number of matches in each block is multiplied by the range of val-ues in that block (the one before normalization), such that greater bursts will be ranked higher.
 Algorithm 2 Correlated Abnormal Patterns Detection in Multidimensional Time Series ( CAPD-MDTS ) Input :Multidimensional curves C
Output: Periods when correlated anomalies appear for each dimension c i do end for n =lengthof C , w = time frame length (set to 5)
S =  X  // set of periods to return for b =1  X  n  X  w +1 do end for return S
After we obtain a list time points corresponding to the top k bursts in each of the dimensions, we need to find out the time windows corresponding to joint increases in all three di-mensions. By the first step, we know in each dimension the time when the bursty patterns appear, along with their in-tensities of burst. In the experiments, we take the top 5 time points. Then we slide a window of a certain size over the time axis. At each point, we find out how many top ranked locations in all dimensions are in the time frame specified by the current time window. A time window is reported if all three dimensions have bursty patterns falling into the window. These steps are formally described in Algorithm 2.
A running example based on the review data is shown in Figure 2. The length of the time window in time series construction is chosen to be 60 days. This example is also discussed in more detail in the experiment section. Each dimension of the time series is plotted in dark points (up-per box -rating, middle box -number of reviews, lower box -ratio of singleton review). The solid lines are the fitted curves (to be discussed in the next subsection). We use red vertical dash lines to highlight one of the suspicious blocks detected in time series by the proposed approach. The sig-nificant joint bursty pattern locates in { 19  X  24 } (from Oct 13, 2005 to Sep 12, 2006), as enclosed by the pair of vertical lines. The three curves all go up in this interval.
Given the review records of a store, one can construct multiple time series using different time window sizes (reso-lutions). If the window size is set too small, the general trend of a time series would be buried in a large number of fluctua-tions, which might cause high false positive rate. Therefore, we propose a hierarchical framework, which incorporates Al-gorithm 2 to robustly detect SR spam attacks. We summa-rize this hierarchical SR spam detection algorithm in Algo-rithm 3. We first smooth out short-term fluctuations using a larger window (lower resolution). Then we fit curves using these time series and use Algorithm 2 ( CAPD-MDTS )to detect any suspicious periods with correlated abnormal pat-terns, which indicate the high likelihood of SR spam attacks. A smaller window size (higher resolution) can be used to re-veal more details (e.g. the exact time of the burst). This is accomplished by constructing new time series with a higher resolution on the detected periods, and detecting any finer suspicious period. This process continues until one reaches the desired resolution such that the time of SR spam attacks can be easily pinpointed.
 Algorithm 3 Multi-Scale Spam Detection Algorithm 1: Input : Reviews data of a store, initial window size  X  t , 2: Output : Detected time intervals of spam activities. 3: Initialize time interval set S 0 = { I } .Scale =0. 4: while  X  t not small enough do 5: = +1, S =  X  . 6: for Each time interval I  X  S  X  1 do 7: Construct time series F ( I,  X  t ). 8: Fit a curve for each dimension of F ( I,  X  t ). 9: Sample the curves to obtain clean time series C . 10: S = S  X  CAPD-MDTS ( C ). 11: end for 12: Decrease window size  X  t , 13: end while 14: return S
In this section, we first describe the dataset we use, then we give a couple of case studies to provide evidences of spam-ming activities caught by the proposed method.
The review data we use in the experiments is a snapshot of a review website ( www.resellerratings.com )onOct6th, 2010 1 . It contains 408,469 reviews written by 343,629 re-viewers (identified by their IDs on the website) for 25,034 stores. 310,499 reviewers ( &gt; 90%) wrote only one reviews and about 76% (310,499/408,469) of the reviews are SRs. The distribution of the number reviewers writing a number of reviews is plotted in logarithm scale in Figure 3. As we can see, the relation between these two quantities roughly follows the power distribution. This is also observed in [10]. The main body of the data consists of reviews, along with information about stores and reviewers. For each review we keep the following information: its rating (ranging from 1 to 5), the posting date and whether it is an SR.
In this section, we report the experimental results of hu-man evaluation of the detected suspicious stores and reviews. We employ three human evaluators in this experiment.
One way to use the algorithm is to run it against the re-views for a store to detect any singleton spam attack. We focus on stores with large number of SRs, so in the evalu-ation we select top 53 stores, each of which has more than 1,000 reviews. We ask human evaluators to read the reviews from all 53 stores and make decisions regarding the suspi-ciousness of these stores. If two or more evaluators vote a store as being likely to have committed an SR spam at-tack, we tag it to be a likely dishonest store. According to the human evaluation, there are a total of 29 stores having at least two votes. Out of the 53 stores, the proposed al-gorithm labels 36 ones as suspicious stores and the rest as normal ones. Out of the 36 detected ones, 22 stores have at least two votes for being suspicious. The proposed algorithm misses 7 suspicious ones. The recall is 75.86% (22/29), in-dicating that the proposed algorithm can catch most of the stores involved in SR spam attack. The precision is 61.11% (22/36). Though this precision looks a bit low, since our goal is to identify suspicious stores for human experts to in-Thanks to Keith Nowicki vestigate further, the proposed approach only enlarges the suspicious set moderately with a decent recall.

Table 1 shows the agreement between evaluators when evaluating the detected stores. The numbers on the diagonal show how many stores each evaluator considers as dishonest. For example, evaluator 1 regards 17 out of 53 stores as suspi-cious ones. The off-diagonal numbers give how many stores that both evaluators in that row and column identify as dis-honest stores. For example, the number on the intersection of Evaluator 1 and Evaluator 2 means that both evaluators 1 and 2 agree upon 14 stores that are suspicious stores. In any case, there are 26 stores at least one of the evaluators regarding it to be suspicious. Comparing the off-diagonal numbers with the diagonal numbers shows the limitation of the content-based approaches. Even human evaluators ex-amining the contents cannot reach agreement a lot of the times as these cases are often very subtle.
We also ask three human evaluators to examine 147 re-views contained in the detected time window of burst given in the first case study (see next section). Each review is given a score (0-negative, 0.5-possibly, 1-positive) indicat-ing the degree of being regarded as a spam review by each evaluator. Lastly, for each review, the scores from three evaluators are added up to get the final score. Among the 147 reviews, 43 reviews (38 are SR) have final score at least 2, and 12 reviews (11 are SR) have final score equal to 3. This indicates that lots of spam identified are indeed SR and the proposed algorithm can locate the period when SR are more likely to happen.

On the other hand, we can also use similar evaluation technique as in Table 1. Table 2 shows the results of human evaluation on spam reviews. Evaluator 1 tags 59 out of 147 reviews as spam reviews, while the other two regard 41 and 72 reviews as spam reviews, respectively. There are 98 reviews that at least one of the evaluators regard as spam. Similarly, the numbers off the diagonal show the agreement between evaluators. Again, this table shows that it is not easy for human beings to reach agreements on whether a review is an SR spam, and content-based methods will be less effective in the detection of this kind of spams.
In this section, we closely study the evidences of SR at-tacks committed by several stores.
The results of running the proposed multidimensional multi-scale detection algorithm on the reviews of a store (id=24811) Figure 4: Anomaly detection on multi-scale multi-dimensional time series are shown in Figure 4. The multidimensional time series in the first subfigure (Figure 4(a)) is produced using a larger time window (30 days) with review data from Apr 2002 and to Aug 2010. The format of this figure is the same as that of Figure 2. In this higher level of detection, we find a sharp in-crease in { 45  X  50 } , which corresponds to the time interval from Oct 30, 2005 to Mar 30, 2006. Notice the burst occurs in a two-month period. We construct another 3-dimensional time series from the review data in the detected time win-dow to find more details of the burst. With the window size set to 15 days, we run the detection algorithm again. The block { 14  X  19 } (from Dec 17, 2005 to Mar 3, 2006) with suspicious activities are found and highlighted in Fig-ure 4(b). Note that this detected time window is smaller than the previous one. In this time interval, the number of singleton reviews increases from 57 to 154, the rating goes up from 4.56 to 4.79, and the ratio of singleton reviews goes up from 61% to 83%. These all happen in a two-weeks pe-riod. It looks like the ratings were bolstered by the sudden increase of singleton reviews.

However, one might still not be convinced that there are probably spams activities in the detected time intervals. We provide further evidences by analyzing the review contents. Note that looking for these evidences is not part of the pro-posed algorithm, which only uses the reviewers X  behaviors for detection. This is only for the purpose of validation. We find out that around the time when the bursts in all three dimensions are detected, the phrases  X  X ustomer service X  and  X  X ustomer support X  are unusually frequent among the SRs. Such correlation indicates that there could be spammers giv-ing undeserving high ratings to  X  X ustomer service X  of that store. Next we study this correlation quantatively.
When constructing multidimensional time series, we di-vide the time from Apr 2002 to Aug 2010 into intervals of two weeks. For each interval, we calculate the  X  X otness X  of the topic  X  X ustomer service X . The  X  X otness X  of a topic is the ratio of reviews about that topic to all reviews in a certain period. If a review contains one of the phrases X  X ustomer ser-vice X  and  X  X ustomer support X , we consider it to be related to that topic. In Figure 5, we show the trend of the hotness of the topic in the blue curve with solid squares. One can see that there is a burst of topic hotness occurs at time 90 (Feb 06, 2006, indicated by the dashed line). Note that this burst occurs in a two-week long period with the hotness goes up from 35% to 46%. Also, note that the time of this burst coincides with that of the burst detected in the multidimen-sional time series by the proposed algorithm. This makes the detected time interval look suspicious. The black hori-zontal solid line shows the topic hotness calculated from all reviews except those from the store being investigated. We can see that, on average, less than 16% of the reviews men-tions the phrases. This number is calculated using 376,758 reviews out of the total 408,470 reviews, so it well represents reviewers X  general interests of this topic. By comparison, we can see that the hotness within this store is twice as high as the average level. This is unlikely in normal business, since it is quite hard to gain the recognition of  X  X ustomer service X  from real customers in two weeks. After that time, the topic hotness keeps going up and is far higher than the average level. In particular, one out of two reviews is talking about  X  X ustomer service X  on average.

Besides  X  X opic hotness X , we consider two other reviewer behaviors. The green curve with solid circles shows the ratio of singleton 5 star reviews to the topic-related reviews. We can see that from the time Feb 06, 2006 on, this ratio is rather high, namely, more than 80% of the singleton reviews are related to  X  X ustomer service X . We can conclude that the burst and hotness of the topic is supported by the burst and high volume of singleton reviews. Lastly, the red curve with stars gives the ratio of reviews which are written by  X  X urry reviewers X  (HR) to the singleton 5 star reviews. We define an HR to be a reviewer who writes a review on the same day she registers her id. From the figure, one can tell that, from Feb 06, 2006 on, a high percentage (over 90%) of 5 star singleton reviews about  X  X ustomer service X  are produced by Figure 6: Bursty Patterns Detected in Store 24938  X  X urry reviewers X . Since at least for those who registered in 2005 never write another review in the following 5 years, this is quite dubious. As a way of validation, we read reviews of the store in the period of topic hotness burst. We found a reviewer once disclosed the fact that the store emailed her for a favorable rating. The reviewer had an unpleasant experience with that store and got customer service only after she low-rated it on the review website. When we try to investigate a store meritline with high SR spams identified by the proposed algorithm, we find out it also operates under another name cdrdvdrmedia .Hence this case of spamming is quite interesting. The following facts support this observation: first, the addresses of the two companies are the same 23 . Second, on the review web-site we are studying, meritline is an alien of cdrdvdrmedia . Third, according to a domain analysis website, these two stores have the same Google analytics account 4 . Lastly, one reviewer says the package and receipt she received were from meritline though she shopped with cdrdvdrmedia 5 .Weper-form the proposed multidimensional times series analysis on the reviews for meritline. Figure 2 (Section 3.2) shows the time interval when an SR spam attack is likely to have hap-pened. cdrdvdrmedia sells the same set of products as mer-itline does, but with a much lower rating. There are only 48 reviews in near 8 years (from Aug-2002 to Jan-2010). The average rating of the store is only 3.06 and people are talking about credit card problems, low-quality products and cus-tomer service. Therefore, the high volume of reviews and good rating for meritline are quite suspicious.
We find another store ( supermediastore ) which is likely involved in singleton review spamming. The multidimen-sional time series for the store and the detected bursty pat-terns are shown in Figure 6. This store is probably owned bythesameowneras meritline and cdrdvdrmedia .Thisis www.cdrdvdrmedia.com/contact-us.html www.la.bbb.org/business-reviews/ General-Merchandise-Retail-By-Internet/ Meritline-in-City-of-Industry-CA-13135057 domaintraker.com/meritline.com www.resellerratings.com/store/view/CDRDVDRMEDIA_ 17/page/1 , see username  X  X ableman X  supported by at least two forum posts 6 . We also find an interesting review 7 telling that the reviewer was cheated by supermediastore when it tried to entice her into spamming. The reviewer once received an email from the store about writing a review for it. In return, the reviewer would receive a  X  X ift X , which she never receive. This is a direct evidence that this store is hiring/enticing people to write favorable reviews. This review is written during the time when there is a burst of singleton reviews.
There is few existing algorithm specifically designed for singleton review spams detection. In [12], the authors con-sider singleton review spams in their data collecting process. They X  X rtificially X  X onstruct singleton review spams for eval-uation, while in this paper, the singleton review spams oc-cur in a real-world dataset. Therefore, the singleton reviews they construct don X  X  exhibit the temporal features that real-world singleton review spams should have. In [15], the authors attempt to detect hotels which are more likely to be involved in spamming. They construct several features, based on which two ranking algorithms detect the most sus-picious hotels. Their method is not comparable with the one we proposed, because their method is supervised (they need to know if certain positive reviews are shill reviews reacting to negative reviews). As their method provides a ranking list of hotels, one will not be able to tell how many hotels need further inspection, while the proposed method gives a list of suspicious stores for further human evaluation.
Another work considers reviewers X  behaviors by introduc-ing a social graph connecting reviewers, their reviews and stores [6]. They discover the reinforcement relations of re-viewers X  trustiness, reviews X  honesty, and stores X  reliability. They use such relations to discover suspicious spammers. However, their method is unable to handle singleton reviews, because SRs have insufficient information in the constructed graph to infer their trustiness. Another work [5] uses re-viewers X  behaviors as indicators of spamming. The features include multiple similar rating on a single product, similar-ity between reviews written by a single reviewer, etc. One will not be able to compute these features for SRs.
The works [1, 2] study group spammers. They first use frequent pattern mining to find groups of reviewers who fre-quently write reviews together. Then in [1], they construct features to find the most likely groups of spammers, or in [2], construct a graph modeling the relations between groups of spammers, spammers and products for group spammer ranking. These methods are novel in detecting a new form of spam activity. Yet they still do not address the singleton review spam problem, because only groups of reviewers who write reviews together at least 3 times will be considered as candidate groups. Similarly, the rule-based algorithm for un-expected review activities detection proposed in [11] would also fail, because during the rule discovery step, SRs will be pruned due to the support threshold.

Earlier works on spam detection mainly rely on review similarities to construct features for spam reviews. For ex-forum.doom9.org/archive/index.php/t-36023. html and forum.videohelp.com/threads/ 143262-Meritline-Very-Disappointed www.resellerratings.com/store/view/ Supermediastore/page/895 , see the review from the ID  X  X efile X / ample, the work in [10] categorizes spam reviews into three types: untruthful opinions; reviews on brands only; non-reviews. To detect these different types of spams, they use simple duplicate detection, feature construction and logis-tic regression. There are some drawbacks. First, it needs to collect a considerable amount of spam reviews manually. This is time consuming, even if it only looks at those re-views which are obviously spams. Second, it only deals with a small portion of all the reviews (neither the duplicated nor the manually labeled reviews represent a large enough fraction of the reviews). Finally, it does not address the singleton reviews spam problem, for example, the review-ers X  centric features again rely heavily on the basis that a reviewer posts more than one review.

There are researches on multidimensional or multivariate time series anomaly detection [16, 7, 3]. However, we cannot directly apply these methods to the singleton spam detec-tion problem. The methods in [3, 7] are designed for general abnormal pattern mining in multidimension time series, and [16] proposes to search all subspaces of all attributes in a re-lation (in the database sense), where the subspaces reported contain anomaly. We focus on detecting correlated abnormal patterns in all three dimensions, while a general detection algorithm will produce many abnormal patterns irrelevant to the problem in this paper, and therefore increase false positive rate.

Regarding burst detection, there are some excellent re-lated works [8, 14, 4]. The work in this paper is not directly comparable to any general burst detection or curve fitting algorithm for the following reasons: first, the algorithm we propose is a general framework, which can incorporate any burst detection algorithm. Second, the problem we study in this paper is not merely a burst detection problem in time series, since we are not given the time series. Instead, we need to model the behaviors of reviewers and construct the time series. Also we need to identify the way in which abnormal patterns are uniquely correlated.
This paper studies the problem of singleton review spam detection, which is both difficult and important to solve. We transform this problem to a temporal pattern discovery problem. We identify three aggregate statistics which are indicative of this type of spam attack, then we construct a multidimensional time series using these statistics. We design a multi-scale anomaly detection algorithm on multi-dimensional time series based on curve fitting. Experimental results show that the proposed algorithm is effective in de-tecting singleton review spams. This work is supported in part by NSF through grants IIS-0905215, IIS-0914934, CNS-1115234, DBI-0960443 and OISE-1129076, US Department of Army through grant W911NF-12-1-0066, and Google Mobile 2014 Program. [1] Mukherjee A, Liu B, Wang J, Glance N, and Jindal N. [2] Mukherjee A, Liu B, and Glance N. Spotting fake [3] Vahdatpour A and Sarrafzadeh M. Unsupervised [4] Chandra E and John W. E. bcp: An R Package for [5] Lim E-P, Nguyen V-A, Jindal N, Liu B, and Lauw [6] Wang G, Xie S, Liu B, and S.Yu P. Identify online [7] Cheng H, Tan P, Potter C, and A. Klooster S.
 [8] Jon K. Bursty and hierarchical structure in streams. [9] S. Karlin and H. M. Taylor. A First Course in [10] Jindal N and Liu B. Opinion spam and analysis. [11] Jindal N, Liu B, and Lim E-P. Finding unusual review [12] M Ott, Y Choi, C Cardie, and J. T. Hancock. Finding [13] Michail V, Marios H, Dimitrios G, and Eamonn K. [14] Michail V, Christopher M, Zografoula V, and [15] G Wu, D Greene, and P Cunningham. Merging [16] Li X and Han J. Mining approximate top-k subspace
