 Carnegie Mellon University Learning probabilistic graphical models from data serves two primary purposes: (i) find-ing compact representations of probability distributions to make inference efficient and (ii) modeling unknown data generating mechanisms and predicting causal relationships. Until recently, most constraint-based and score-based algorithms for learning directed g raphical models from continuous data required assuming relationships between variables ar e linear with Gaussian noise. While this assumption may be appropriate in many con texts, there are well known contexts, such as fMRI images, where variables have nonlinear dependencies a nd data do not tend towards Gaussianity. A second major limitation of the tradi tional algo-rithms is they cannot identify a unique structure; they reduce the set of possible structures to an equivalence class which entail the same Markov properties. The recently pr oposed ad-ditive noise model [1] for structure learning addresses both limitations; by taking advantag e of observed nonlinearity and non-Gaussianity, a unique directed acyclic structure can be identified in many contexts. However, it too suffers from limitations: (i) for certain distri-butions, e.g. linear Gaussians, the model is invertible and not useful for structure learning; (ii) it was originally proposed for two variables with a multivaria te extension that requires enumerating all possible DAGs, which is super-exponential in the number of va riables. In this paper, we address the limitations of the additive noise model. We intr oduce weakly additive noise models , which have the advantages of additive noise models, but are still useful when the additive noise model is invertible and in most cases when additive noi se is not present. Weakly additive noise models allow us to express greater uncertainty a bout the data generating mechanism, but can still identify a unique structure or a smaller equi valence class in most cases. We also provide an algorithm for learning an equivalence class for such models from data that is more computationally efficient in the more than two va riables case. Section 2 reviews the appropriate background; section 3 introduces weakly additive no ise models; section 4 describes our learning algorithm; section 5 discusses some relat ed research; section 6 presents some experimental results; finally, section 7 offers conclusions .. Let G = hV , Ei be a directed acyclic graph (DAG), where V denotes the set of vertices and E ij  X  E denotes a directed edge V i  X  V j . V i is a parent of V j and V j is a child of V i . For V  X  X  , Pa V i G denotes the parents of V i and Ch V i G denotes the children of V i . The degree of V i is the number of edges with an endpoint at V i . A v-structure is a triple h V i ,V j ,V k i X  X  such A joint distribution P over variables corresponding to nodes in V is Markov with respect to G if P P ( V ) = Y in P is entailed by the above factorization. A partially directed acyclic graph (PDAG) H for G is a mixed graph , i.e. consisting of directed and undirected edges, representing all DAGs Markov equivalent to G , i.e. DAGs entailing exactly the same conditional independencies. If V i  X  V j is a directed edge in H , then all DAGs Markov equivalent to G have this directed edge; if V i  X  V j is an undirected edge in H , then some DAGs that are Markov equivalent to G have the directed edge V i  X  V j while others have the directed edge V i  X  V j . The PC algorithm is a well known constraint-based , or conditional independence based, structure learning algorithm. It is an improved greedy version of the SGS [2] a nd IC [3] algorithms, shown below. Instead of searching all subsets of V\{ V i ,V j } for an S such edges can be removed based on the results of conditional independence tests with these S sets, and (iii) iteratively increases the cardinality of S considered until  X  V k  X  X  with degree greater than | S | . S is only considered if it is a subset of nodes connected to V i or V j at the current iteration. PC learns the correct PDAG in the large sample limit when the Markov, faithfulness, and causal sufficiency (that there are no unmeasured common causes of two or more measured variables) assumptions hold [2]. The partial correlation based Fisher Z-transformation test, which assumes linear Gaussian distributions, is us ed for conditional independence testing with continuous variables. The statistical advantage of PC is it limits the number of tests performed, particularly those with large conditioning sets . This also yields a computational advantage since the number of possible tests is exponent ial in |V| . The recently proposed additive noise model approach to structure learning [1] ass umes only that each variable can be represented as a (possibly nonlinear) function f of its parents plus additive noise  X  with some arbitrary distribution, and that the noise components are mutually independent, i.e. P (  X  1 ,..., X  n ) = X  X  Y is the true DAG, X =  X  X , Y = sin(  X X ) +  X  Y ,  X  X  X  Unif (  X  1 , 1), and  X  Y  X  Unif (  X  1 , 1). If we regress Y on X (nonparametrically), the forward model , figure 1a, and Figure 1: Nonparametric regressions with data overlayed for (a) Y regressed on X , (b) X regressed on Y , (c) Z regressed on X , and (d) X regressed on Z regress X on Y , the backward model , figure 1b, we observe the residuals  X   X  Y  X  X  X  X and  X   X 
X /  X  X  X  Y . This provides a criterion for distinguishing X  X  Y from X  X  Y in many cases, but there are counterexamples such as the linear Gaussian case, where the forward mo del is invertible so we find  X   X  Y  X  X  X  X and  X   X  X  X  X  X  Y . [1, 5] show, however, that whenever f is nonlinear, the forward model is noninvertible, and when f is linear, the forward model is only invertible when  X  is Gaussian and a few other special cases. Another limitation of this approach is that it is not closed under marginalization of intermediary variables when f is nonlinear, e.g. for X  X  Y  X  Z with X =  X  X , Y = X 3 +  X  Y , Z = Y 3 +  X  Z ,  X 
X  X  Unif (  X  1 , 1),  X  Y  X  Unif (  X  1 , 1), and  X  Z  X  Unif (0 , 1), observing only X and Z , figures 1c and 1d, causes us to reject both the forward and backward models. [5] shows this method can be generalized to more variables. To test whether a DAG is compatible wi th the data, we regress each variable on its parents and test whether the resulting res iduals are mutually independent. This procedure is impractical even for a few variables, however, si nce the number of possible DAGs grows super-exponentially with the number of va riables, e.g. there are  X  4 . 2  X  10 18 DAGs with 10 nodes. Since we do not assume linearity or Gaussianity in this framework, a sufficiently powerful nonparametric independence test must be used. Typically, the Hilbert Schmidt Independence Criterion [6] is used, which we now define. Let X be a random variable with domain X . A Hilbert space H X of functions from X to R is a reproducing kernel Hilbert space (RKHS) if for some kernel k ( , ) (the reproducing kernel for H X ), for every f ( )  X  X  X and x  X  X  , the inner product h f ( ) ,k ( x, ) i H k ( x,x  X  ), so we can compute inner products efficiently in this high dimensional space. The Moore-Aronszajn theorem shows that all symmetric positive definite kernels (mo st popular kernels) are reproducing kernels that uniquely define corresponding RKHSs [7]. Let Y be a random variable with domain Y and l ( , ) the reproducing kernel for H Y . We define the mean map X and cross covariance C XY as follows, using  X  to denote the tensor product. If the kernels are characteristic , e.g. Gaussian and Laplace kernels, the mean map is injective [8, 9, 10] so distinct probability distributions have different mean maps. The Hilbert Schmidt Independence Criteria (HSIC) H XY = kC XY k 2 HS measures the dependence of X and Y , where kk HS denotes the Hilbert Schmidt norm. [9] shows H XY = 0 if and only if X  X  X  X  Y for characteristic kernels. For m paired i.i.d. samples, let K and L be Gram matrices for k ( , ) and l ( , ), i.e. k ij = k ( x i ,x j ). For H = I N  X  1 be centered Gram matrices .  X  H XY = estimator for H XY [6]. To determine the threshold of a level- X  statistical test, we can use the permutation approach (where we compute  X  H XY for multiple random assignments of the Y samples to X , and use the 1  X   X  quantile of the resulting empirical distribution over  X  H
XY ), or a Gamma approximation to the null distribution of m We now extend the additive noise model framework to account for cases where additive noise models are invertible and cases where additive noise may not be present. Definition 3.1.  X  = D V i , Pa V i G E is a local additive noise model for a distribution P over V that is Markov to a DAG G = hV , Ei if V i = f Pa V i G +  X  is an additive noise model. Definition 3.2. A weakly additive noise model M = hG ,  X  i for a distribution P over V is a DAG G = hV , Ei and set of local additive noise models  X , such that P is Markov to G ,  X   X   X  such that there exists some graph G  X  (not necessarily related to P ) such that V i  X  Pa V j G  X  and D
V j , Pa When we assume a data generating process has a weakly additive noise model representat ion, we assume only that there are no cases where X  X  Y can be written X = f ( Y ) +  X  X , but not Y = f ( X ) +  X  Y . In other words, the data cannot appear as though it admits an additive noise model representation, but only in the incorrect direction. This representa tion is still appropriate when additive noise models are invertible, and when additiv e noise is not present: such cases only lead to weakly additive noise models which express grea ter underdetermination of the true data generating process.
 We now define the notion of distribution-equivalence for weakly additive noise models. Definition 3.3. A weakly additive noise model M = hG ,  X  i is distribution-equivalent to N = hG  X  ,  X   X  i if and only if G and G  X  are Markov equivalent and  X   X   X  if and only if  X   X   X   X  . Distribution-equivalence defines what can be discovered about the true data generating mechanism using observational data. We now define a new structure to partition data generating processes which instantiate distribution-equivalent weakly additive noise models. Definition 3.4. A weakly additive noise partially directed acyclic graph (WAN-PDAG) for M = hG ,  X  i is a mixed graph H = hV , Ei such that for { V i ,V j } X  X  , We now get the following results.
 Lemma 3.1. Let M = hG ,  X  i be a weakly additive noise model, D V i , Pa V i G E  X   X , and Proof. Since M and N are distribution-equivalent, Pa V i G = Pa V i G  X  . Thus, Ch V i G = Ch V i G  X  Theorem 3.1. The WAN-PDAG for M = hG ,  X  i is constructed by (i) adding all directed and undirected edges in the PDAG instantiated by M , (ii)  X  D V i , Pa V i G E  X   X , directing all V j  X  Pa V i G as V j  X  V i and all V k  X  Ch V i G as V i  X  V k , and (iii) applying the extended Meek rules [4], treating orientions made using  X  as background knowledge.
 Proof. (i) This is correct because of Markov equivalence [2]. (ii) This is correct by lem ma 3.1. (iii) These rules are correct and complete [4].
 WAN-PDAGs can used to identify the same information about the data generating mech-anism as additive noise models, when additive noise models are identifiable, but prov ide a more powerful representation of uncertainty and can be used to discover more inf orma-tion when additive noise models are unidentifiable. The next section describes an efficient algorithm for learning WAN-PDAGs from data. constraint-based search using the PC algorithm with a nonparametric conditiona l inde-pendence test (the Fisher Z test is inappropriate since we want to allow nonlinearity and non-Gaussianity) to identify the Markov equivalence class and (ii) a  X  X C-styl e X  search for noninvertible additive noise models in submodels of the Markov equivalence class. In the first stage, we use a kernel-based conditional dependence measure similar to HSIC [9] (see also [11, Section 2.2] for a related quantity with a different norm alization). For a conditioning variable Z with centered Gram matrix  X  M for a reproducing kernel m ( , ),  X  Y = ( Y,Z ). Let H XY | Z = kC XY | Z k 2 HS . It follows from [9, Theorem 3] that H XY | Z = 0 if and only if X  X  X  X  Y | Z when kernels are characteristic. [9] provides the empirical estimator:  X  H The null distribution of  X  H XY | Z is unknown and difficult to derive so we must use the permutation approach described in section 2. This is not straightforward since permuting X or Y while leaving Z fixed changes the marginal distribution of X given Z or Y given Z . We thus (making analogy to the discrete case) must cluster Z and then permute elements only within clusters for the permutation test, as in [12].
 This first stage is not computational efficient, however, since each evaluation of  X  H XY | Z is naively O N 3 and we need to evaluate  X  H XY | Z approximately 1000 times for each per-mutation test. Fortunately, we see from [13, Appendix C] that the eigenspectra o f Gram matrices for Gaussian kernels decay very rapidly, so low rank approximations o f these ma-trices can be obtained even when using a very conservative threshold. We implemented the incomplete Cholesky factorization [14], which can be used to obtain an m  X  p matrix G , where p  X  m , and an m  X  m permutation matrix P such that K  X  PGG  X  P  X  , where K is an m  X  m Gram matrix. A clever implementation after replacing Gram matrices in  X  H XY | Z with their incomplete Cholesky factorizations and using an appropriate equivalence to invert G  X  G +  X I p for  X  M instead of GG  X  +  X I m results in a straightforward O mp 3 operation. Unfortunately, this is not numerically stable unless a relatively large regula rizer  X  is chosen or only a small number of columns are used in the incomplete Cholesky factorizati ons. A more stable (and faster) approach is to obtain incomplete Cholesky factor izations G X ,G Y , and G Z with permutation matrices P X ,P Y , and P Z , and then obtain the thin SVDs for HP X G X ,HP Y G Y , and HP Z G Z , e.g HPG = USV , where U is m  X  p , S is the p  X  p diagonal matrix of singular values, and V is p  X  p . Now define matrices  X  S X ,  X  S Y , and  X  S Z and  X  G X ,  X  G Y , and  X  G Z as follows: We can compute  X  H XY | Z = ciently in O mp 3 by choosing an appropriate associative ordering of matrix multiplicatio ns. Figure 2 shows that this method leads to a significant increase in speed when used with a permutation test for conditional independence without significantly affecting the empirica lly observed type I error rate for a level-.05 test.
 In the second stage, we look for additive noise models in submodels of the Marko v equiv-alence class because (i) it may be more efficient to do so and require fewer tests since orientations implied by an additive noise model may imply further orientati ons and (ii) we Figure 2: Runtime and Empirical Type I Error Rate. Results are over the gener ation of 20 3-node DAGs for which X  X  X  X  Y | Z and the generating distribution was Gaussian. may find more orientations by considering submodels, e.g. if all relations are l inear and only one variable has a non-Gaussian noise term. The basic strategy used is a X  X C-st yle X  greedy search where we look for undirected edges in the current mixed graph (starting with the PDAG resulting from the first stage) adjacent to the fewest other undirected edges. If thes e edges can be oriented using additive noise models, we make the implied orientati ons, apply the extended Meek rules, and then iterate until no more edges can be oriented. Algorithm 2 provides pseudocode. Let G = hV , Ei be the resulting PDAG and  X  V i  X  X  , let U V i G denote the nodes connected to V i in G by an undirected edge. We get the following results. Lemma 4.1. If an edge is oriented in the second stage of kPC, it is implied by a noninverti ble local additive noise model.
 noise model. All U l  X  U V i G \ S must be children of V i by lemma 3.1. Lemma 4.2. Suppose  X  = h V i , W i is a noninvertible local additive noise model. Then kPC will make all orientations implied by  X  .
 Proof. Let  X  S = W \ Pa G V additive noise model, line 8 is satisfied so all edges connected to V i are oriented. Theorem 4.1. Assume data is generated according to some weakly additive noise model M = hG ,  X  i . Then kPC will return the WAN-PDAG instantiated by M assuming perfect conditional independence information, Markov, faithfulness, and causal sufficiency. Proof. The PC algorithm is correct and complete with respect to conditional independence [2]. Orientations made with respect to additive noise models are correct by lem ma 4.1 and all such orientations that can be made are made by lemma 4.2. The Meek rules, w hich are correct and complete [4], are invoked after each orientation made with resp ect to additive noise models so they are invoked after all such orientations are made. kPC is similar in spirit to the PC-LiNGAM structure learning algorithm [ 15], which assumes dependencies are linear with either Gaussian or non-Gaussian noise. PC-LiNGAM combines the PC algorithm with LiNGAM to learn structures referred to as ngDAG s. KCL [11] is a heuristic search for a mixed graph that uses the same kernel-based dependence measures as kPC (while not determining significance threhsholds via a hypothesis test), but does not take advantage of additive noise models. [16] provides a more efficient al gorithm for learning additive noise models, by first finding a causal ordering after doing a series of high dimensional regressions and HSIC independence tests and then pruning the resulting DAG implied by this ordering. Finally, [17] proposes a two-stage procedure fo r learning additive noise models from data that is similar to kPC, but requires the additi ve noise model assumptions in the first stage where the Markov equivalence class is identified. To evaluate kPC, we generated 20 random 7-nodes DAGs using the MCMC algorithm in [18] and sampled 1000 data points from each DAG under three conditions: linear dependencies with Gaussian noise, linear dependencies with non-Gaussian noise, and nonlinear dependen-cies with non-Gaussian noise. We generated non-Gaussian noise using the same procedure as [19] and used polynomial and trigonometric functions for nonlinear dependencies. We compared kPC to PC, the score-based GES with the BIC-score [20], and the ICA -based LiNGAM [19], which assumes linear dependencies and non-Gaussian noise. We applied two metrics in measuring performance vs sample size: precision, i.e. proportion of directed edges in the resulting graph that are in the true DAG, and recall, i.e. proportion o f directed edges in the true DAG that are in the resulting graph. Figure 3 reports the results. I n the linear Gaussian case, we see PC shows slightly better performance than kPC in precis ion, which is unsurprising since PC assumes linear Gaussian distributions. Only LiNGAM shows b etter recall, but worse precision. LiNGAM performs significantly better than the other algorithms in the linear non-Gaussian case. kPC performs about the same as PC in precision and recall, which again is unsurprising since previous simulation results have shown that no nlinearity, but not non-Gaussianity can significantly affect the performance of PC. In the nonlinear non-Gaussian case, kPC performs slightly better than PC in precision. We note, however, We also ran kPC on data from an fMRI experiment that is analyzed in [21] w here nonlinear dependencies can be observed. Figure 4 shows the structure that kPC learned, where each of the nodes corresponds to a particular brain region. This structure is the same as the one learned by the (GES-style) iMAGES algorithm in [21] except for the absence of one edg e. However, iMAGES required background knowledge to direct the edges. kPC successfully found the same directed edges without using any background knowledge. Domain experts in neuroscience have confirmed the plausibility of the observed relationships. We introduced weakly additive noise models, which extend the additive noise model f rame-work to cases such as the linear Gaussian, where the additive noise model is inver tible and thus unidentifiable, as well as cases where additive noise is not present. The weakly addi tive noise framework allows us to identify a unique DAG when the additive noise model assump-tions hold, and a structure that is at least as specific as a PDAG (possibly sti ll a unique DAG) when some additive noise assumptions fail. We defined equivalence classes for s uch models and introduced the kPC algorithm for learning these equivalence classes from data. Finally, we found that the algorithm performed well on both synthetic and r eal data. We thank Dominik Janzing and Bernhard Sch  X olkopf for helpful comments. RET was funded by a grant from the James S. McDonnel Foundation. AG was funded by DARPA IPTO FA8750-09-1-0141, ONR MURI N000140710747, and ARO MURI W911NF08 10242. [1] P. O. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Sch  X olkopf. Nonli near causal [2] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search . 2nd [3] J. Pearl. Causality: Models, Reasoning, and Inference . 2000. [4] C. Meek. Causal inference and causal explanation with background knowledge. In [5] K. Zhang and A. Hyv  X arinen. On the identifiability of the post-nonlinear causal mo del. [6] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch  X olkopf, and A. J. Smola. A kernel [7] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American [8] A. Gretton, K. Borgwardt, M. Rasch, B. Sch  X olkopf, and A. Smola. A kernel method [9] K. Fukumizu, A. Gretton, X. Sun, and B. Sch  X olkopf. Kernel measures of conditional [10] B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Sch  X olkopf. Inj ective [11] X. Sun, D. Janzing, B. Scholk  X opf, and K. Fukumizu. A kernel-based causal learning [12] X. Sun. Causal inference from statistical data . PhD thesis, Max Plank Institute for [13] F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of [14] S. Fine and K. Scheinberg. Efficient SVM training using low-rank kernel representa-[15] P. O. Hoyer, A. Hyv  X arinen, R. Scheines, P. Spirtes, J. Ramsey, G. Lacerda, a nd [16] J. M. Mooij, D. Janzing, J. Peters, and B. Scholk  X opf. Regression by dep endence mini-[17] K. Zhang and A. Hyv  X arinen. Acyclic causality discovery with additive nois e: An [18] G. Melan  X con, I. Dutour, and M. Bousquet-M  X elou. Random generation of dags fo r [19] S. Shimizu, P. Hoyer, A. Hyv  X arinen, and A. Kerminen. A linear non-gaussian acycli c [20] D. M. Chickering. Optimal structure identification with greedy search. Journal of [21] J. D. Ramsey, S. J. Hanson, C. Hanson, Y. O. Halchenko, R. A. Poldrack, and C. Gly-
