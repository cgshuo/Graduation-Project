 Today, massive amounts of data are collected by many organisations in both the private and public sectors. A large proportion of this data is about people, and often personal identifying details are stored together with application specific information, for example employment or medical details. When such data is analysed within an organisation, then normally, depending upon the desired outcomes, only parts of the personal information is used for an analysis (like age, gender, or postcode). In these cases, privacy and confidentiality are generally not of great concern, as the results of the analysis are only used within an organisation, and no detailed private or confidential information is released.
However, when data is being shared between organisations, privacy and con-fidentiality become of paramount importance, because personal information is commonly required to match records fro m different databases [1]. The aim of such linkages is to match all records that r efer to the same entity. Because real-world data is commonly dirty [2] (contains errors and variations) and often no unique entity identifiers are available, sophisticated approximate matching algo-rithms are required that use the available personal identifiers [3,4].
The process of data linkage or matching has in the past decade been recognised as an important and challenging problem, and a variety of novel linkage algo-rithms have been developed [3,4]. They mainly address the technical challenges of matching accuracy and scalability to very large databases. Another challenge for data linkage research is the lack of publicly available real-world test data sets that allow evaluation of new algorithms. This lack is due to privacy concerns, because it is illegal in most countries to publish data that contains, for example, personal details of customers or patients. As a result, data linkage researchers have to use publicly available data sets, or use their own (confidential) data, which prevents others from repeating experimental studies [5].

An alternative is to use synthetically generated data. This approach has sev-eral advantages. First, a user can control the size (number of records) and qual-ity (error characteristics) of the gene rated data sets. Second, such data can be published, and thus allows other researchers to repeat experiments and better evaluate algorithms. Third, the generator itself can be published, allowing oth-ers to generate data that is specifica lly tailored to their use, for example to their country or application domain. Fourth, because it is known which of the generated records are matches, it is possible to calculate matching rates [3].
Besides data linkage research, any application area where data containing per-sonal information is required for research purposes can benefit from synthetically generated data, because such data removes privacy and confidentiality concerns. Examples include research into privacy-preserving data sharing [1], publishing and mining, or statistical micro-data confidentiality.

The challenges when generating synthetic data are that it is not easy to cre-ate data with characteristics that are similar to real-world data. The frequency and error distributions of values have to follow real-world distributions, and de-pendencies between attributes have to be modelled. This paper describes a data generator with such characteristics. It i s a significant improvement over earlier generators [2,5,6], which created data in less realistic ways. As illustrated in Fig. 1, the data generator works in two steps. First, a user specified number of original records is created based on real values and their frequencies and dependencies, or using specific attribute generation rules [5,7]. Second, randomly select ed original records are modified into duplicate records. Alternatively, family and household records can be generated. As can be seen in Fig. 4, each record is given a unique identifier (  X  X ec id X ) that will facilitate the calculation of matching rates [3]. 2.1 Original Record Generation For original records, the values in name and address attributes are created ran-domly using frequency tables. Such tables can, for example, be extracted from telephone directories. For date, telephone and social security number attributes, a user can specify generation rules that determine the range of dates (such as start and end birth dates), or the number of digits (for example for telephone numbers). In the following, we describe the two major novel features of our data generator [7]: family and household data, and attribute dependencies. Generating Family and Household Data. The records for a family are generated by first selecting an original record at random. According to its age and gender values, it is assigned one of the roles husband , wife , son or daughter . The next step is to randomly select how m any other records are to be generated for this family. These records are then created by keeping the surname of the first family record, but modifying given name, gender and age values. Address attribute values are generally kept the same for all members of a family. De-pending upon the age of son and daughter records, however, a new address will be created with a certain probability, assuming the child has left home.
Household records are generated similarl y, with the main difference being that all records in a household have differen t names but the same address, and that all age values are above 18 (one of many parameters that a user can set [7]). Attribute Dependencies. A dependency occurs if the values in an attribute depend upon the values in one or more other attributes. For example, given names depend on the gender and the cultural background of a person, while suburb/town names depend on the state/territory they are located in. These dependencies are based on frequency tables, such as the one shown in Fig. 2.
When generating the original records, the key attributes (the attributes that others depend on) are generated first, and according to a selected key attribute value, a value from the dependent attribute is randomly chosen according to the corresponding frequency distribution. For example, using the values from Fig. 2, if the state  X  X LD X  has been selected, the suburb name  X  X llansford X  would be chosen with likelihood 6 . 25%,  X  X llendale X  with likelihood 68 . 75%, and  X  X llestree X  with likelihood 25%. To introduce randomness into the data, with a certain likelihood, as set by the user, a dependency is not followed, but rather a value is randomly chosen from the overall frequency table of the dependent attribute. 2.2 Error Modelling and Duplicate Record Modification As illustrated in Fig. 3, data can be entered through a variety of channels, each having its own error characteristics. For example, handwritten forms that are processed using optical character recogn ition (OCR) software will likely include substitutions between similar looking characters. On the other hand, phonetic er-rors, like the variations  X  X ickson X  and  X  X ixon X  , are introduced when information is dictated using speech recognition, or typed manually. Typing itself introduces certain errors more likely than others. Depending upon keyboard layout, mistyp-ing neighbouring keyboard keys, such as  X  X  X  and  X  X  X  , can occur. Often, depending upon the data entry channel, a combination of error types is introduced.
Our data generator can model typographic, phonetic and OCR errors. For each error type, a user can set how likely they are introduced when the duplicate records are generated. Setting the likelihood of typographic and phonetic errors to 0, for example, will result in duplicate records that only contain OCR errors. Typographic Errors. These errors include insertion, deletion, and substitu-tion of a character; and transposition of t wo adjacent characters. They are imple-mented as functions that apply the corresponding modification to a given input string with a certain likelihood (as set by the user), and return the modified string. Following studies of error distributions [8], the position of a modification is randomly chosen such that it more likely occurs in the middle or towards the end of a string, because real errors are less likely at the beginning of names. Optical Character Recognition Errors. OCR modifications are based on rules that consider shape similarity among characters, such as  X 5 X  and  X  X  X  or  X  X  X  and  X  X v X  . Around fifty such rules are used, representing the most likely OCR variations that might occur. When duplicate records are generated, one or more possible OCR modifications will be randomly selected and applied to an input string, and the modified string will be inserted into the duplicate record. Like phonetic errors, OCR errors can be a single character modification or a combi-nation of modifications (like a substitute and delete, or a delete and insert). Phonetic Errors. These errors are usually more complex than typographic or OCR errors, as they often include change s of character groups and depend upon the position within a string. The idea behind our approach in modelling phonetic errors is to employ the rules that are used in phonetic encoding methods [9], such as Phonix and Double-Metaphone . In encoding methods, such rules are used to group similar sounding names together, while we employ them to modify a name in order to generate a similar sounding variation of it. Currently, around 350 phonetic rules are used, each made of the following seven components. 1. Position. The position within the input string where the original string 2. Original pattern. This is the string (made of one or more characters) that 3. Substitute pattern. This is the string (made of zero or more characters) 4. Precondition. A condition that must occur before the original string pat-5. Postcondition. Similarly, a condition that must occur after the original 6. Pattern existence condition. This condition requires that a certain given 7. Start existence condition. Similarly, this condition requires that the The last four components of a rule (its conditions) can be set to  X  X one X  if they are not required. In the following we give two illustrative examples.  X  ALL, h, @, None, None, None, None (mustaph a  X  mustapa)  X  END, le, ile, C, None, None, None (bramble  X  brambile ) Our data generator is im plemented as part of the Febrl (Freely Extensible Biomedical Record Linkage) open source data linkage system [10], 1 and is writ-teninthe Python programming language. Due to the availability of its source code, it can be modified and extended according to a user X  X  needs. A large num-ber of parameters can be set by the user, including the number of original and duplicate records to be generated, the frequency and dependency look-up tables to be used, the distributions used for household and family records, and the various error characteristics to be a pplied when duplicates are created [7].
We used a variety of data sets to create our look-up tables, including a data set containing 99,571 names and their culture of origin (37 different cultures) [11], a data set with Australian postcode, suburb and state values as available from the Australia Post Web site, 2 and the various look-up tables supplied with the Febrl data linkage system [10]. Error and modification probabilities were set according to real world studies on typographic and other errors [8,12,13,14].
 Some example data that was created using our generator is shown in Fig. 4. As can be seen, the record identifiers (  X  X ec id X ) designate if a record is an original or a duplicate, and the duplicate records a re numbered and refer back to their original record, in order to allow the calculation of matching rates [3].
We have conducted a large number of experiments to validate our data gener-ator, by comparing real data sets with synthetic data that was generated using frequency tables based on the real data. A detailed analysis and discussion is provided elsewhere [7]. A user can rep eat our experiments by downloading the Febrl system and run the generator program supplied with it (and possibly change parameter settings a ccording to her or his needs). A first data generator for personal information was developed in the 1990s [2]. It allowed the generation of data based on lists of names, cities, states and postcodes, however without using any fr equency distributions. A user could set the size of the data sets to be generated, and the types and amount of errors to be introduced. An improved generator wa s described more recently [6]. It allowed attribute values to become missing, and it improved the variability of the created values. It is however unclear if this generator is using frequency information, as not many details have been described.

A first simple version [5] of our genera tor has been freely available as part of the Febrl [10] data linkage system. It improved upon earlier generators by including frequency tables of attribute values, more flexible setting of individual error probabilities, as well as inclusion of look-up tables with name variations (to be used for example for nick-names, known phonetic variations, and common misspellings). This generator however does not include attribute dependencies, does not allow creating family or household record groups, and it does not model errors as accurately as the new ve rsion described in this paper.

The phonetic error model presented in Sect. 2.2 is based on rules that were originally developed for phonetic encoding methods [9]. A common feature of phonetic encodings is that they convert n ame strings into codes according to how a name is being pronounced. Names that sound similar are converted into the same code. This is obviously a language dependent process, and most phonetic encoding methods have been developed for the English language.

Our work is also based on various studies that have analysed spelling and data entry errors and their corrections [8,12,13,14]. These studies found that most errors are single character errors, and that the distribution of error types depends upon the mode of data entry. For example, OCR output contains almost exclusively substitution errors, while th is type of error accounts for less than 20% of errors with keyboard based manual data entry [8]. Typically, up to 95% of misspellings in keyboard entry only contain one error; with only around 8% of first letters incorrect, compared to almost 12% of second and nearly 20% of third letters. We have presented a data generator for personal information that allows the generation of realistic synthetic data based on frequency tables and attribute generation rules. There are various ways to improve our generator. First, allow-ing the generation of not just personal information, but also application specific attributes (like medical, employee, or customer details) will make our genera-tor applicable to the wider data mining community. Second, extending family records to include other roles (such as cousins, aunts, uncles, etc.) and allow culture specific parameter settings will enable the generation of the complex family connections that occur in real lif e. Third, enabling Unicode characters will make our generator more international and will allow the generation of data sets containing, for example, Thai, Chinese, or Arabic characters. Finally, adding a graphical user interface will facilitate the setting of the many possible param-eters. Another part for our future work will be to fully integrate our new data generator into our Febrl data linkage system [10].

