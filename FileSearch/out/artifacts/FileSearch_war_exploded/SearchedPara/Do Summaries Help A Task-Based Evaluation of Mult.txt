 We describe a task-based evaluation to determine whether multi-document summaries measurably improve user perfor-mance when using online news browsing systems for directed research. We evaluated the multi-document summaries gen-erated by Newsblaster, a robust news browsing system that clusters online news articles and summarizes multiple arti-cles on each event. Four groups of subjects were asked to perform the same time-restricted fact-gathering tasks, read-ing news under different conditions: no summaries at all, single sentence summaries drawn from one of the articles, Newsblaster multi-document summaries, and human sum-maries. Our results show that, in comparison to source doc-uments only, the quality of reports assembled using News-blaster summaries was significantly better and user satis-faction was higher with both Newsblaster and human sum-maries.
 H.4.3 [ Information Systems Applications ]: Communi-cation Applications X  Information browsers Measurement, Experimentation, Human Factors text summarization, evaluation, user study, news browsing
Research on multi-document summarization of news has seen a surge of activity in the past five years, with the Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00. development of many multi-document news summarization systems (e.g., [5, 11, 14, 16]) and several that run on-line on a daily basis [14, 16] generating hundreds of sum-maries per day. Summarization evaluation methodology has also been actively explored. Since 2001, DUC (Docu-ment Understanding Conference), a NIST-run annual eval-uation conference, has organized quantitative evaluations of multi-document summarization systems which compare sys-tem content against a reference set of model summaries. The DUC corpus of clustered summary/document pairs has spurred research in evaluation methodology on automation [12], metrics [16], and new methods of comparison of multi-ple models that factor in perceived salience of information [15, 7].

A significant question remains: will the summaries gener-ated by such systems actually help end-users to make bet-ter use of the news? Multi-document summaries should enable users to more efficiently find the information they need. To find out whether they do, we performed a task-based evaluation of summaries generated by Newsblaster, a system that provides an interface to browse the news, fea-turing multi-document summaries of clusters of articles on the same event. We hypothesized that multi-document sum-maries would enable end users to more effectively complete a fact-gathering task. To this end, we compared the utility of four parallel news browsing systems: one with source doc-uments but no summaries or clusters, one with one-sentence multi-document summaries where the sentence is extracted from one of the articles, one with Newsblaster generated multi-document summaries and one with human written summaries. Both Newsblaster and human summaries were multi-document summaries of the same length (about 200 words); where Newsblaster extracted all of its sentences, however, humans chose content and phrasing without typi-cally using sentence extraction.

Our results show that, in comparison to source docu-ments only, the quality of reports assembled using News-blaster summaries was significantly better and user satis-faction was higher with both Newsblaster and human sum-maries. Users of Newsblaster and human summaries drew on summaries significantly more often in assembling their Figure 1: The prompt to one of the four tasks used in the evaluation. report and were more satisfied, while providing reports of similar quality. More generally, our results demonstrate that full multi-document summarization is a more powerful tool than either documents alone or the one-sentence approach, an approach that is closely related to that used in systems such as Google News. They also provide a frame of reference as human summaries are presumably the best summary that could be provided.

In the following sections, we overview the relevant features of Newsblaster and then discuss the design, execution and results of our evaluation. Our study drew upon the following key components of Newsblaster: 1. Article clustering. Newsblaster clusters the articles 2. Event cluster summarization. Newsblaster gener-3. User interface. Finally, Newsblaster presents the
We did not evaluate other features of Newsblaster, includ-ing search,  X  X pdate X  summaries, and related image collec-tion.
There has been considerable recent work on multi-document summarization (see [6] for a sample of systems). Ours is distinguished by its use of multiple summariza-tion strategies dependent on input document type, fusion of phrases to form novel sentences, and editing of extracted sentences. Our task-based or extrinsic [17] evaluation con-trasts with most recent work on evaluation of summaries, which has focused on quantitative evaluation comparing generated summaries against a set of ideal reference models [6, 12, 15, 7]. There have also been earlier organized and individual task-based evaluations of single document sum-marization. TIPSTER-III [8] and others [13, 3] used an in-formation retrieval task. Time and accuracy were measured to determine how well a user can judge the relevance of a retrieved document to a query. However, other factors such as summary length, type of query (some make it easy to determine relevance), and document type (when key words accurately characterize the text, these measures don X  X  dis-criminate well between summaries) have a critical impact on task results [10].

As in our work, a recent evaluation [1] also asks subjects to write reports given a topic. However, they treat the resulting reports as focused summaries (reports are restricted to 50 sentences in length) and they evaluate how well different quantitative metrics compute similarity between the reports. Thus, their work evaluates evaluation metrics. We modeled our evaluation on an approach used by the DARPA TIDES program for an Integrated Feasibility Ex-periment (IFE) (see [4] for a description of the system archi-tecture for this experiment). In the IFE, users are asked to write a report using a news aggregator as a tool. This task also resembles those that intelligence analysts carry out on a day-to-day basis [2] [9].
In designing our user evaluation, we were interested in whether Newsblaster is an effective tool for assisting the processing of large volumes of news. We designed our eval-uation to answer the following questions:
Each subject was asked to perform four 30-minute fact gathering scenarios using a Web interface. Each scenario involved answering three related questions about an issue in the news. These questions were presented to the user as part of a prompt, one of which is shown in Figure 1. The four tasks were, respectively: the Geneva Accord in the Middle East; Hurricane Ivan X  X  effects; conflict in the Iraqi city of Najaf; and attacks by Chechen separatists in Russia. Subjects were given a space to compose their report and a Web page that we constructed as their sole resource. They were told to cut and paste facts from either the summaries or articles on the page, or to paraphrase to write a report. The page contained four document clusters, two of which were centrally related to the topic at hand, and two of which Figure 2: The evaluation interface screen showing the list of documents that a user sees in the no sum-mary condition. were peripherally related. 1 Hence there were sixteen clusters in the study overall. We selected the clusters by doing a manual search through Newsblaster clusters to find groups that were either peripherally or closely related. Each cluster contained, on average, ten articles. Subjects thus had to find relevant information within forty articles to answer in-depth analysis questions for each of four scenarios. While all of the articles were related to the scenario topic, only about half of the articles contained answers to the specific questions.
There were four summary condition levels in the experi-ment: Level 1: Subjects were given no summaries. The Web page Level 2: Subjects were given a one-sentence summary for Level 3: Subjects were given a Newsblaster multi-Level 4: Subjects were given a human multi-document
Subjects had access to source documents in addition to the summaries. Links to the documents were available on
With the exception of one of the scenarios, where one clus-ter was related and three were peripheral.
 Figure 3: The evaluation interface screen showing a typical page for the single sentence summary con-dition. The user sees the summary and a list of articles each with its own summary. the same page when Summary Level 1 and Summary Level 2 were used (Figure 3) or by clicking on the cluster name when Summary Levels 3 and 4 were used (Figure 4).
Each scenario was followed by a survey that asked subjects to rate different aspects of their experience (e.g., difficulty of the task) along a five point scale, as well as some multiple choice questions. At the end of the experiment, each subject answered additional questions about their overall experience and had the opportunity to give comments.
We recruited 45 subjects for three studies, where sub-jects wrote reports under the four different summary con-ditions noted above. Our subjects came from a variety of backgrounds: 73% were university students, of whom 32% were engineering students. The rest were undergraduate lib-eral arts students, journalism students, or law students. A pre-experiment questionnaire revealed that most used online newspapers as their primary news source, and read the news about an hour per day. All were native speakers of Ameri-can English. Subjects were paid for their participation. An additional monetary prize was promised for the five writers whose reports scored the highest.
 The subjects in the first study below alternated between Summary Level 3 and Level 4 (i.e., Newsblaster and hu-man summaries); we controlled for scenario order and level order. The subjects in the next two studies had a single summary condition, Summary Level 1 (no summaries) or Level 2 (single-sentence summaries), and we controlled for scenario order. 2 Altogether, a total of 138 reports were writ-ten. We aimed at 11 subjects per summary level for each scenario (note that in Study A, subjects wrote for only two scenarios and thus we needed to double the number) and more subjects than expected showed up for Study C.
The design included two order permutations. While this is not a complete crossed design, we found no effect of level or scenario order on report quality. Figure 4: The evaluation interface screen show-ing a multi-document summary generated by News-blaster. The user clicks on the cluster title to see the list of associated articles.

Study A: 21 subjects wrote reports for two scenarios each
Study B: 11 subjects wrote reports for all four scenarios,
Study C: 13 subjects wrote reports for all four scenarios,
As illustrated in Figure 1, subjects were asked to assemble lists of important facts that addressed a three-part prompt. We scored the quality of the resulting reports on the basis of how well subjects included appropriate content. To do this, we needed a gold standard and a metric for comparing re-port content against the gold standard. To score the reports, we used the Pyramid method for evaluation [15], which has been demonstrated to be a reliable method for summary evaluation. The method uses multiple models, thus making the report scores less sensitive to the specific model used. The Pyramid method allows an importance weight to be as-signed to different information units, or content units. This is important for a subjective task such as report writing, where different facts are more or less important.
As a gold standard, we constructed a pyramid [15] of facts, or content units, for each scenario question for each sum-mary level, using the reports written by the rest of the study participants for the same question. For example, to score a
These subjects also wrote four reports in a session; thus all subjects saw all four scenarios in one of two orders. How-ever, we were experimenting here with two additional con-trol conditions for summary level that are not relevant for comparison with the two other studies reported here. report from the human summary condition, we constructed the pyramid using reports created using all other conditions (i.e., no summaries, minimal summary, Newsblaster sum-mary), plus the reports written by different people also with human summaries. This yielded, on average, 34 reports per pyramid, far greater than the number of summaries (five) needed to yield stable results [15]. Using this method, any fact (whether expressed as a word, a modifier, or a clause) that appears in more than one report is included in the pyra-mid. Facts that appear in more reports appear higher in the pyramid and are associated with a weight that indicates the number of times they are mentioned. Thus, more important facts have higher weight. If there are n reports, then there will be n levels in the pyramid. The top level will contain those facts that appear in all n articles, the next level facts that appear in n  X  1 articles, and so forth. A report SCU that does not appear in a pyramid has weight 0. Repetitions of the same SCU also have weight 0 and thus, duplication in a report does not increase the score. An ideal report of length x facts will include all facts from the top level, the next level and so forth, until x facts are included.
We score a report using the Pyramid scoring metric, which computes a ratio of the sum of the weights of report facts to the sum of the weights in an optimal report with the same number of facts. More formally, let T j refer to the jth tier in a pyramid of facts. If the pyramid has n tiers, then T is the top-most tier and T 1 , the bottom-most. The optimal score for a report with X facts is: where j is equal to the index of the lowest tier an optimally informative report will draw from. Then the pyramid score W is the ratio of D, the sum of the fact weights in the report, to MAX, the optimal report score.

This method has the following desirable properties:
No specific instructions were given to the report writers about how long their reports should be. Consequently, some people wrote much longer answers to some report questions than did others. Figure 5 shows a histogram of lengths of answers to report questions, where length is measured in content units. 4 The wide variation in length of answers to any of the three questions from each of the four prompts was
Length also varied widely in number of characters, or words; as we score on the basis of content units, we com-pare lengths using this measure. See Section 3.4 or [15] for a description of content units. Figure 5: Distribution of the length of the reports in content units across all four conditions unexpected, as it did not show up in our pilot study with far fewer subjects, on one scenario. It has been observed that report length has a significant effect on evaluation results [10]. To avoid the distortion that would arise from treat-ing reports of such widely disparate length equivalently, we restricted the length of reports to be no longer than one standard deviation above the mean, removing outliers. To do this, we truncated all question answers to a length of eight content units, which was the third quartile of lengths of all answers.
We used analysis of variance (ANOVA) to study the im-pact of the type of summary level on report quality. The de-pendent variable was the score for each report and summary type was used as a factor with four levels: machine multi-document summary, human multi-document summary, no summary at all and minimal summary.

In addition to the main factor of interest (summary level), we included other factors in the model to estimate their contribution to the report quality. These factors were report writer, report topic, and question.
We measured effectiveness of the summaries in a fact-gathering task in three ways: 1. By scoring the reports and comparing scores across 2. By comparing user satisfaction per summary condi-3. By comparing whether subjects preferred to draw re-
The results of scoring reports for content are shown by summary condition in Table 1. The quality of reports tends to improve when subjects carry out the task with better quality summaries; the pyramid scores are lowest when the subjects use documents only and highest when the subjects use the human summaries. Differences between the scores are not significant (p=0.3760 from ANOVA analysis), but when we drop the scenario subjects had most difficulty with, differences are significant as noted below.

We suspected that we would see differences when we looked at different scenarios and, as we shall see, the ANOVA does show that scenario is a significant factor. When scoring reports and in informal discussion with sub-jects, we observed that some scenarios were more difficult than others; the documents in the clusters for these scenar-ios did not contain as much information for the answers. For example, in the Geneva Accord scenario, one subject wrote  X  X he [user study] page brought up a large amount of useless articles and information, and especially on the last article [Geneva], only a few of the articles had any relevance [sic] at all. X  The Hurricane Ivan scenario, on the other hand, seemed one for which subjects could provide responses to questions. In retrospect, this may have been due to the fact that the event clusters for Geneva contained more editorials with less  X  X ard X  news, while the clusters for Hurricane Ivan contained more breaking news reports.

Given the problematic feedback on scenario 1 and the dif-ference in types of documents in the clusters, we concluded that there was a design problem for this scenario. We re-moved the Geneva Accord scenario scores from the mix and recomputed averages of pyramid scores per summary con-dition as shown in Table 2. These results show that report quality is lowest with documents only, improves with mini-mal one-sentence summaries and improves again with News-blaster summaries. The full ANOVA tables for all three sce-narios apart from Geneva are shown in Table 3; the ANOVA table shows that summary level is a marginally significant factor in the results.

Our primary interest in the experiment was to measure the impact of the different multi-document summaries, de-termining exactly which summary levels made a difference. So, given the ANOVA model, we compared the report scores under each multi-document summary condition to those written under different summary conditions. 95% simultane-ous confidence intervals for the comparisons were computed by the Bonferroni method. The only difference that was significant at the 0.05 level was that between Newsblaster summaries and no summaries at all. Thus, we conclude that report quality with Newsblaster summaries is signifi-cantly better than reports produced with documents only. The differences between Newsblaster and minimal or human summaries are not significant, although results with human summaries are slightly below Newsblaster summaries.
The ANOVA shows that scenario, question and subject are also significant factors in the result. Furthermore, there are significant interactions between summary level and sce-nario, between summary level and question, and between scenario and question.
Six of the questions in our exit survey required responses along a quantitative continuum. Each of the responses was assigned a score from 1 to 5 and a natural-language equiv-alent, with low scores corresponding to deep dissatisfaction and high scores expressing full satisfaction. For each ques-tion, Figure 6 shows the questions and the responses for each summary level at the extremes of the possible responses. It also shows the averages of the subjects X  responses at the Table 1: Mean Pyramid Scores on Reports, all Sce-narios included.
 Table 2: Mean Pyramid Scores on Reports, Scenario 1 (Geneva Accords) excluded. bottom of the table. This numeric representation of user satisfaction increases monotonically from Level 1 to Level 4.
Subjects were asked to compare their experience in the study with the experience they would expect to have on the same task using a Web search. Subjects were more likely to think the system they used was more effective than a Web search when they used Newsblaster than when they used either documents only (p=0.0798 5 ) or single-sentence summaries (p=0.0101). Users were more likely to feel that they had read more than they needed to with documents only and with single-sentence summaries than with either Newsblaster summaries or human summaries. The differ-ence for this question is marginally significant between sub-jects with human summaries and subjects with no sum-maries (p=0.0924)
Questions 3 and 4 show that subjects found it easier to assemble their facts with summaries than with documents only to complete the task and that they were more likely to feel they had enough time with summaries than with documents only. A pairwise  X  2 test shows the difference is marginally significant for question 3 between human sum-maries and no summaries (p=0.0838), is significant between one sentence summaries and no summaries (p=0.0401), al-though not quite significant between Newsblaster summaries and no summaries (p=0.1682). The difference for question 4
For the user study questions, significance is determined us-ing a pairwise  X  2 test.

Question Level 2 Level 3 Level 4 1. Which was most help-ful? source articles helped most 64% 48% 29% equally helpful 32% 29% 29% summaries helped most 5% 24% 43% 2. How did you budget your time? Most searching, some writing 55% 48% 67% Half searching, half writing 39% 29% 19%
Mostly writing, some search-ing is significant, or marginally so, between each condition with a summary and no summaries (p=0.0636, Newsblaster/no summary; p=0.0126 human/no summary; p=0.0001 one sentence/no summary). There is no significant difference between different summary levels for either question 3 or 4. There were no significant differences between responses for the different summary levels for either question 5 or 6. Responses to the multiple choice questions are shown in Table 7. Responses to question 1 again show that users were more satisfied with human level summaries than News-blaster summaries and with Newsblaster summaries than with one sentence summaries. More than four times the pro-portion of subjects replied that summaries were more useful than source articles with Newsblaster summaries than with one sentence summaries. Responses to question 2 show that subjects spent the least time searching when given News-blaster summaries, but unintuitively, the most time when given human summaries.

In the space for open comments, many subjects com-mented on the need for a method of searching the interface for events about particular keywords. An efficient search-able interface over summaries is being developed as part of Newsblaster, but was not evaluated in this study.
The above results were echoed in the citation habits of subjects. When subjects wrote a report, they were asked to cite the location where they found a fact that they ex-tracted for their report. We compared the number of times a subject extracted facts from source articles with extrac-tions from summaries. The citations in Level 2 (one sen-tence summary) reports credited summaries 8% of the time. For Levels 3 (Newsblaster) and 4 (human), the proportion was 17% and 27% respectively. This means that report writers were much more likely to reuse text from News-blaster multi-document summaries than from the minimal summaries (p=0.0057 on one-sided t-test); there was a ten-dency to include more content when presented with human summaries than with Newsblaster summaries (p=0.1257 on one-sided t-test).
We began by asking whether multi-document summaries help users to find the information they need. Our user study shows that when a news browsing system contains such sum-maries, there is a significant increase over the no-summary condition in the quality of information that they include in their report. Users feel that they are able to find substan-tially more of the information that is relevant. This result demonstrates that summaries do help subjects do a better job of using news to assemble facts on given topics.
When we developed Newsblaster, we speculated that sum-maries could help users find necessary information in either of the following ways: 1. they may find the information they need in the sum-2. the summaries may link them directly to the relevant
Our user study confirms these beliefs and shows that as the quality of the summary increases (from Level 2 to Level 4), the greater the effect. The increase in citations shows that as quality of the summary increases, users significantly more often find the information they need in the summary without a significant decrease in report quality. At the same time, they report that they read fewer articles when they have either a Level 3 or a Level 4 summary. This confirms our belief that better multi-document summarization saves reading time and facilitates finding the relevant documents. This is further reinforced by the fact that almost five times the proportion of subjects using Level 3 summaries than those using Level 2 summaries reported that summaries were more helpful for the task than source articles. That number almost doubles again for subjects with Level 4 summaries.
There are some issues we need to address in future stud-ies. First, we expected to find a significant increase in re-port quality as summary quality increased. We only found a significant increase in quality between reports written with documents only and reports written with summaries. The lack of significant increase between Summary Level 2 and Summary Level 3 could be due to a number of factors. There were two problems in presentation of information for these two levels. First, the interface for Summary Level 2 identi-fied individual articles with a title and a one sentence sum-mary; we modeled this design after commercial online news providers. The interface for Summary Level 3 only had ti-tles for each article. In order to pinpoint the effect of dif-ferent quality summaries on report quality, we need to run a follow-on study which compares how subjects do with a single sentence multi-document summary paired with a list of article titles only and how they do with a Newsblaster summary paired with a list that contains both titles and one-sentence, single document summaries. Second, the in-terface for Summary Level 2 shows the list of individual articles on the same Web page as the multi-document sum-mary for the cluster. In contrast, the interface for Summary Level 3 shows the multi-document summary and cluster title on the same page and requires the subject to click on clus-ter title to see the list of individual articles. The different number of clicks required in the interface may have affected time-to-task completion as well as search strategy.
Another problem that we noted was that reports written by subjects were of widely varying length. Reports var-ied from a minimum of 102 words to a maximum of 1525 words. We adjusted for this in the current study by trun-cating reports. Lengthy reports not only had more material, but tended to have more duplication of facts, which clearly makes for less effective reports. The impact of truncating re-ports requires follow-up study. We plan to correct for these two problems with more specific directions about the length and nature of report required. We also will experiment with modifications of the task so that subjects will write coher-ent reports, rather than cut and paste sentences from docu-ments. We hypothesize that this will require more synthesis of material, and lead to more consistency in length.
In order to have a realistic task-based evaluation, we de-veloped complex prompts across a range of topics. As a con-sequence, we could simultaneously investigate a wide range of factors. Given that scenario and question had significant effects on report quality, we need to understand more clearly how the four scenarios contrast, and how question difficulty compares within and across prompts. It is also possible that variables we did not explicitly test for, such as cluster size, article length, semantic coherence within clusters, or seman-tic distance between clusters, influenced the outcome.
We have shown that it is feasible to conduct a task-based, or extrinsic, evaluation of summarization that yields signif-icant conclusions. Our answer to the question, Do Sum-maries Help? , is clearly yes. Our results show that subjects produce better quality reports using a news interface with Newsblaster summaries than with no summaries. Also, as summary quality increases from none at all to human, user satisfaction increases. In particular, full multi-document summaries, of which Newsblaster and human summaries are representative, help users perform better at fact-gathering than they do with no summaries. Users are also more sat-isfied with multi-document summaries than with minimal one-sentence summaries such as those used by commercial online news systems. These results affirm the benefit of re-search in multi-document summarization.

However, we have also demonstrated that many factors influence the degree to which summaries help. A complete answer to the question is clearly complex, and a single study can only give partial insight. A secondary contribution of our experiments is the identification of additional possible effects on task completion, such as specific interface design, report length, and scenario design, none of which were pre-dicted by our pilot. These insights provide a road-map for follow-on studies that can even more finely pinpoint the ef-fect of multi-document summaries on task performance.
We would like to thank Andrew Rosenberg for his help with the statistical analysis in an early stage of the eval-uation. This work was supported in part by the Defense Advanced Research Projects Agency under TIDES grant NUU01-00-1-8919. [1] E. Amigo, J. Gonzalo, V. Peinado, A. Penas, and [2] J. W. Bodnar. Warning Analysis for the Information [3] R. Brandow, K. Mitze, and L. Rau. Automatic [4] S. Colbath and F. Kubala. Tap-xl: An automated [5] H. Daume, A. Echihabi, D. Marcu, D. S. Munteanu, [6] Proceeding of the second, third and forth document [7] H. Halteren and S. Teufel. Examining the consensus [8] T. Hand. A proposal for task-based evaluation of text [9] F. J. Hughes and D. A. Schum. Evidence marshaling [10] H. Jing, R. Barzilay, K. McKeown, and M. Elhadad. [11] C.-Y. Lin and E. Hovy. From single to [12] C.-Y. Lin and E. Hovy. Automatic evaluation of [13] I. Mani and E. Bloedorn. Multi-document [14] K. R. McKeown, R. Barzilay, D. Evans, [15] A. Nenkova and R. Passonneau. Evaluating content [16] D. R. Radev, S. Blair-Goldensohn, Z. Zhang, and [17] K. Sparck-Jones and J. R. Galliers. Evaluating Natural
