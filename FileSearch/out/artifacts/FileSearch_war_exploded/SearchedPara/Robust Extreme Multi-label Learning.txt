 Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divide-and-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm.
  X  Computing methodologies  X  Supervised learning; Multi-label Learning; Low-rank Algorithm
In contrast to conventional single-label learning, in which each example is assigned only one label, multi-label learning evaluates examples with multiple labels. Many real-world applications use multi-label learning [31] including text cat-egorization and image/video annotation [7]. The rapid evo-lution of information techniques has fueled the emergence of large-scale multi-label applications with huge numbers of la-bels. For example, given over a million labels (categories) on Wikipedia, one might wish to build a classifier to annotate a new article or web page with a subset of the most relevant categories. In another example, taking billions of YouTube videos as distinct labels, a task might be to recommend a ranked list of labels to a single user. Hence, extreme multi-label learning with an extremely large number of labels has become an important research focus.

Binary relevance (BR) [24] seeks to independently train a classifier for each label. BR is a straightforward approach for multi-label learning, but due to prohibitive training and prediction costs arising from large numbers of labels, this method becomes less useful. A number of embedding-based approaches have been proposed to overcome this extreme multi-label learning problem that reduce the effective num-ber of labels. The approaches assume that the label matrix is low rank. Different techniques can be used to compress and decompress label vectors, including compressed sensing [15], Bloom filters [9], SVD [23], landmark labels [2, 5], and output codes [32].

The low-rank label matrix assumption in embedding meth-ods is violated in many real-world applications due to the presence of tail labels occurring in a handful of data points. Histograms of the label matrices on the wiki10 , Delicious-L and Amazon datasets are shown in Figure 1. There are more than 10 4 labels which occur in at most 2 examples on each dataset, such that they are not well approximated by any linear low-dimensional basis. This tail label issue is fre-quently and persistently neglected in multi-label learning. Recently, instead of projecting label vectors into a linear low-rank subspace, [4] addressed the tail label problem by learning embeddings that non-linearly captured label cor-relations by preserving the pairwise distances between la-bel vectors. Although this provides an alternative approach to handling tail labels, we must also ask whether low-rank based approaches really are now redundant.

Here we revisit the classical low-rank principle in multi-label learning and suppress the influence of tail labels. Tail labels can be regarded as label matrix outliers, inspiring us to decompose the label matrix into a low-rank part that de-picts label correlations and a sparse part that captures tail labels. Various effective embedding techniques are then ap-plicable. To improve scalability and leverage the growing availability of parallel computing architectures, the divide-and-conquer approach is adopted to optimize the resulting objective function, whose performance can be theoretically guaranteed with high probability. Our theoretical analy-sis shows that the proposed low-rank and sparse decompo-sition is useful for improving the generalizability of multi-label learning algorithms. Experiments on real-world data demonstrate the significance of studying tail labels in multi-label learning and the effectiveness of the proposed algo-rithm. number of associated examples.
The multi-label learning problem has been extensively studied in practice [31] and theory [13, 27]. Broadly speak-ing, existing multi-label learning algorithms can be catego-rized into two groups: algorithm adaptation and problem transformation methods. Algorithm adaptation methods adapt, extend and customize an existing machine learning algorithms for the task of multi-label learning, while problem transformation methods transform the multi-label learning problem into one or more single-label classification or regres-sion problems. Representative examples include boosting [20, 26, 16], decision trees [25, 17], neural networks [29, 10], support vector machines [12, 14] and k nearest neighbors classifier [30, 21].

The key in multi-label learning is modeling inter-label cor-relations and using them for label vector prediction. In many applications such as text categorization and functional ge-nomics, the labels are often organized in the form of a tree or directed acyclic graph, so that label relationship can be exploited from the knowledge resources as prior knowledge for multi-label learning [6, 14]. In most real-world tasks, however, prior knowledge of label relationship is often un-available. It is thus necessary to model the label relationship directly. For example, Sun et al. [22] proposed to employ hy-pergraphs to exploit the higher-order relations among multi-ple instances sharing the same label in multi-label learning. They constructed a hyperedge for each label, and included all instances annotated with a common label into one hyper-edge, thus capturing their joint similarity. Zhang and Zhang [28] used a Bayesian network to characterize the dependence structure between multiple labels, and learned a binary clas-sifier for each label by treating its parental labels in the de-pendence structure as additional input features. However, as the number of labels keeps growing, these algorithms are usually computationally infeasible.

Embedding-based approaches are significant for handling extreme multi-label learning problem by reducing the effec-tive number of labels. Generally, they assume that the label matrix is low-rank, and then project label vectors into a low-dimensional subspace. Hence, instead of directly learning to predict the original high-dimensional label vector of each ex-ample, the training complexity is largely decreased by first learning the predictor of embedded label vectors, and then a decompression operation is employed to lift the embedded label vectors back to the original label space.

Various compression and decompression techniques have been exploited by existing embedding methods. Hsu et al. [15] addressed classification problem with a large number of labels via a three-step approach. First, random transfor-mation is used to project the high-dimensonal label vector into a low-dimensional space; next, a regression model is trained to predict each dimension of the transformed label vector; finally, for a test example, its predicted label vector in the low-dimensional space is projected back to the origi-nal label space. Considering the drawback of random trans-formation in [15], Tai and Lin [23] proposed the principal label space transformation, which uses principal component analysis (PCA) to accomplish the compression operation on the high-dimensional label vector. Since PCA in the label space only focuses on minimizing the encoding error between high-dimensional feature vectors and their low-dimensional representations [23], Chen and Lin [8] proposed conditional principal label space transformation, which improves [23] by simultaneously considering the label encoding error and training error in the low-dimensional label space. Based on canonical correlation analysis (CCA), Zhang and Schnei-der [32] also took both feature matrix and label matrix into consideration. After that, a maximum margin formulation was developed to learn an output coding, which is predic-tive and discriminative so that the codings for different label vectors are easy to predict and significantly different from each other. Instead of using label transformation, Balasub-ramanian and Lebanon [2] proposed to train only a small subset of the labels, which come from the original labels, so that the difficulty of the learning problems can be decreased. Supposing that the non-selected labels are to be faithfully and easily constructed from the selected ones, a group-sparse learning problem is investigated to discover the optimal la-bel subset [2]. However, the structured sparsity optimiza-tion problem in [2] is computationally expensive, especially when there are a lot of labels to select from. Bi and Kwok [5] alleviated this problem by proposing an efficient label selec-tion method based on randomized sampling. Following the assumption in [2], Bi and Kwok [5] designed the sampling probability of each label using its leverage score in the best rank-k subspaces of the label matrix.

Recently, Yu et al. [27] modeled multi-label classifica-tion as a general empirical risk minimization (ERM) prob-lem with a low-rank constraint, which generalizes both la-bel and feature dimensionality reduction. Given squared-L 2 loss, LEML algorithm in [27] has a closed form solu-tion, and can be reduced to the conditional principal label space transformation algorithm in [8]. Various loss func-tions and regularizers are applicable in this ERM frame-work for preventing overfitting and increasing scalability. However, as suggested by [4], the low-rank assumption in embedding-based approaches is easily violated by tail labels in real-world datasets with a large number of labels. In-stead of globally projecting high-dimensional label vectors into a low-rank subspace, SLEEC algorithm in [4] learns low-dimensional embedding which non-linearly capture la-bel correlations by preserving the pairwise distances between only the closest label vectors. Regressors are then trained to predict the embedded label vector for each example. Dur-ing prediction, rather than using a decomposition matrix, SLEEC uses a k -nearest neighbor (kNN) classifier in the em-bedding space, which leverages the fact that nearest neigh-bors have been preserved during training.
Given a training data set { ( x 1 ,y 1 ) ,  X  X  X  , ( x n ,y x i  X  R d is the feature vector of the i -th example, and y i  X  { X  1 , 1 } L is the corresponding label vector, the fea-ture matrix is denoted as X = [ x 1 ;  X  X  X  ; x n ]  X  R n  X  d label matrix is Y = [ y 1 ;  X  X  X  ; y n ]  X  { X  1 , 1 } n  X  L example x i will have label-j ; otherwise, there is no label-i for example x j . Multi-label learning aims to learn a hypothesis f : R d  X  X  X  1 , 1 } L that accurately predicts the label vector for a given example.

There are a large number of labels in the extreme multi-label learning setting. Tail labels, which only occur with several examples, cannot be ignored, and a number of rows in the label matrix Y will have plenty of  X  1-valued entries while occasionally being dotted with several 1-valued entries. Due to the existence of these rows, the classical low-rank assumption on the label matrix is violated. We attempt to modernize the low-rank principle by carefully formulating the tail labels in the extreme multi-label learning problem.
One reasonable approach to interpret the label matrix in multi-label learning is to assume that the label matrix can be well approximated using a low-dimensional subspace. This assumption has been well justified in many practical situa-tions. A general model can be written as where b Y is the low-rank approximation of Y , and loss func-tion ` (  X  ) is employed to penalize the loss generated by a multi-label predictor parameterized by W .

Problem (1) has been studied for decades, and various techniques have been used to formulate the low-rank b Y , the predictor f (  X  ) and the loss function ` (  X  ). However, as men-tioned above, tail labels damage the low-rank assumption over Y and render the estimated b Y arbitrarily far from the true Y ; the learned multi-label predictor is, therefore, seri-ously influenced as a result. A method that can extract the low-rank components from Y even in the presence of tail labels would be desirable.

To achieve this, we treat tail labels as outliers and decom-pose the label matrix to where b Y L is of low rank and depict label correlations and b Y
S is the sparse component capturing the influence of tail labels. These two components can be obtained by solving the following objective: Given the low rank and sparse components b Y L and b Y S , we expect to learn regression models that predict them using the input features. That is, we require that b Y L  X  WX and b Y
S  X  HX , where W,H  X  R d  X  L . Hence, problem (3) can be reformulated as: Since the rank and card constraints tend to increase the opti-mization complexity, we employ two popular matrix factor-ization heuristics (to encourage low-rankness) and L 1-norm minimization (to encourage sparsity) to relax the constraints in problem (4), such that where {  X  1 , X  2 , X  3 } are positive constants, W is supposed to have W = UV given U  X  R d  X  k and V  X  R k  X  L , and an L 2-regularization has been included for H . It is expected that by solving problem (5), we can obtain a low-rank func-tion (i.e., W = UV ) and a sparse function (i.e., H ), which together are used for multi-label prediction.
In this section, we first present the basic optimization method for problem (5) and then adopt the divide-and-conquer strategy to develop an optimization method appli-cable to extreme multi-label learning.
Problem (5) can be solved by alternatively solving the following three subproblems until convergence: In the following, we illustrate the optimization of these three subproblems respectively.
In problem (6a), updating V is simple since each column v of V can be independently updated: which is easy to solve in a closed form as the dimension of v (i.e. k ) is generally small. Setting the gradient of Eq. (7) w.r.t. v j to zero, the optimal v  X  j corresponding to the j -th label can be obtained as v =
Problem (6b) is equivalent to where u  X  R dk denotes vec ( U ), and e X ij = vec ( x i v is a closed form solution for this problem, u However, it is inefficient to compute the closed form solu-tion for the above problem when d is large due to the huge computational cost of inverting a dk  X  dk matrix. Instead, it is more appropriate to employ efficient gradient descent methods (e.g. conjugate gradient descent) for optimization.
In problem (6c), given e Y = Y  X  XUV , each column H j can be independently solved, The L 1 norm in problem (9) involves both H j and X , which makes the problem non-smooth and disallows the standard proxy function-based optimization methods. One way to cir-cumvent this difficulty is by introducing an auxiliary variable Z j = XH j and transforming problem (9) into where  X  &gt; 0 is the penalty parameter, and  X  is the scaled dual variable.

Fixing H j and  X  , problem (10) is reduced to The optimal Z  X  j can be obtained via soft thresholding oper-ation [11], where Algorithm 1 Robust Extreme Multi-label Learning Input: X , Y , t  X  1 For i = 1 ,  X  X  X  ,t do In Parallel End
ColumnProjection ([( c W ) 1 ,  X  X  X  , ( c W ) t ] , ( c W ) Fixing Z j and  X  , H j can be solved from the following objec-tive function: min The gradient w.r.t. H j is calculated as By setting Eq. (15) to zero, H j can be easily solved out in a closed form. On the other hand, if the dimension of H j great, cheap gradient descent optimization method can be applied.  X  can be updated via Through alternatively updating H j , Z j and  X  , the optimal H j for problem (9) can be achieved.
The divide-and-conquer strategy can be employed to in-crease the algorithm X  X  capability for handling extremely large numbers of labels. The original optimization problem is first divided into cheaper sub-problems that can be efficiently solved in parallel. The solutions to these subproblems can then be combined to achieve the final solution. The whole optimization procedure is summarized in Algorithm 1.
Divide Step. Given the label matrix Y  X  { X  1 , 1 } n  X  L , we randomly partition the matrix Y into t m -column sub-matrices { ( Y ) i } t i =1 , where we suppose L = tm and each ( Y i )  X  { X  1 , 1 } n  X  m . Hence, the original problem is divided The basic optimization method described in Section 4.1 can be adopted to solve these sub-problems in parallel, which outputs the solutions ( c W ) 1 , ( b H ) 1 ,  X  X  X  , ( c
Conquer Step. This conquer step exploits column pro-jection to integrate the solutions of sub-problem solutions. The final approximation W to problem (5) can thus be space of ( c W ) 1 . After obtaining c W , we can then launch the optimization over each each column of b H in parallel to obtain the sparse component of the resulting multi-label predictor.
In this section, we prove the upper bounds on the estima-tion error of the basic and divide-and-conquer optimization methods, respectively. The generalizability of our learning model is also analyzed.
Given a feature matrix X  X  R n  X  d and the correspond-ing label matrix Y  X  { X  1 , 1 } n  X  L for n training points of L labels, we suppose that where W 0  X  R d  X  L is the ground truth weight, Y S is the sparse component to formulate the influence of tail labels, and is a data-independent noise term. Given this training data, we aim to estimate W 0 by performing empirical risk minimization: where e Y = Y  X  Y S . Note that although the method in Eq. (4) uses a regularized rank-constrained formulation, we analyze the trace norm-regularized version without the rank constraint for simplicity. Since the class of rank-constrained matrices is smaller than the class of trace norm-constrained matrices, we can in fact expect better theoretical results here.
 We generically denote the estimator for  X  XX by X T and the estimator for  X  X positive semidefinite. Thus, the estimator for c W naturally becomes, c W = arg min We use the following theorem to show that c W solved in Eq. (19) is an approximated estimation of the true W 0 .
Theorem 1. Suppose the smallest eigenvalue of  X  XX is bounded by  X  min ( X  XX )  X   X  &gt; 0 . The estimation error satisfies
Proof. Let  X  = c W  X  W 0 . Considering the optimality of c W for problem (19), we have  X  W 0 +  X  ,  X  XX ( W 0 +  X )  X  X  X  2  X   X  X  X  X  W 0 ,  X  XX W 0  X  X  X  2  X   X  X which can be rearranged to Since the smallest eigenvalue of  X  XX is bounded, we have Given k  X  k  X   X k  X  k F , the right hand side of Eq. (21) can be upper-bounded by Combining all the above results, we get The result then follows.
 According to Theorem 1, the estimation error bound de-pends on  X  X preted as the perturbations of  X  X of Y S is no greater than that of Y , the tail labels will not overwhelm the estimation over W .

We next analyze the estimation error in the divide-and-conquer optimization method. Suppose the compact singu-lar value decomposition (SVD) of W is U W  X  V T W , where  X  is diagonal and contains k non-zero singular values of W , and U
W  X  R d  X  k and V W  X  R L  X  k are the corresponding left and right singular vectors of W . We assume the true weight W is (  X ,k )-coherent, whose definition is given as below, Definition 1. Given  X  0 ( V W ) = d k max 1  X  i  X  d k ( V  X  ( W ) = q dL k max i,j | e T i U W V T W e j | , for any  X  &gt; 0 , if rank ( W ) = k , max(  X  0 ( V W ) , X  0 ( V W ))  X   X  and  X  we call W is (  X ,k ) -coherent.

We first invoke a lemma from [18] to show that column projection can produce an approximation that is nearly as good as a given rank-k target by sampling a number of columns proportional to the coherence.

Lemma 1. [18]. Given a matrix W  X  R d  X  L and a rank-k approximation A  X  R d  X  L , choose m  X  ck X  log( L ) log(1 / X  ) / where c is a fixed positive constant, and let f W  X  a matrix of m columns of W sampled uniformly without re-placement. Then, with probability at least 1  X   X  , where f W proj  X  R d  X  L is derived by projecting f W onto the space of W .

Recall that the true weight W 0 has been partitioned into mizing distinct sub-problems in parallel and correspond to t umn projection to derive the approximation of W 0 with the help of c W , and denote it as c W proj . The difference between W 0 and c W proj can be bounded by the following theorem.
Theorem 2. For m  X  ck X  log( L ) log(1 / X  ) / 2 , where c is a fixed positive constant. The original problem has been di-vided into t sub-problems. If a basic optimization method yields the estimation error satisfying Theorem 1 for each sub-problem, then with probability at least 1  X   X  , the esti-mation error in the divide-and-conquer method is bounded by
Proof. According to Lemma 1, with probability at least 1  X   X  , the following inequality holds: By adding k c W  X  W 0 k F to both sides of the above inequality, we get which implies that This ends the proof.
 Compared to the estimation error bound derived by apply-ing a basic optimization method to estimate W 0 in Theorem 1, Theorem 2 exhibits an approximate recovery error with appropriate probability. It is instructive to note that the divide-and-conquer approach provides a controlled increase in error and a controlled decrease in the probability of suc-cess. Users can, therefore, adjust the optimization speed and accuracy.
Given n multi-label points sampled i.i.d. from the dis-tribution Q = X  X  Y , the proposed model aims to learn ( c
W, b H )  X  X  = W  X H by performing ERM as follows: where b L ( W,H ) is the empirical risk of the predictor ( W,H ). Our goal would be to show that ( c W, b H ) has good general-ization properties, that is: where L ( W,H ) = E ( x,y ) [ ` ( y,f ( x,W,H ))] is the population risk of the predictor.

The Rademacher complexity is an effective way to mea-sure the richness (complexity) of the function class F , based on which the generalization error bound of the learning al-gorithm can easily be obtained using standard approaches [3].

Definition 2. Given a sample S = { x 1 ,  X  X  X  ,x n }  X  X and a real-valued function class F defined on a space X , the empirical Rademacher complexity of F is defined as where  X  = (  X  1 ,  X  X  X  , X  n ) are independent uniform { X  1 } -valued Rademacher random variables. The Rademacher complexity of F is
The Rademacher complexity of the proposed multi-label learning algorithm can thus be written as: where w j and h j correspond the j -th columns of W and H , respectively, and together determine the predictor for the j -th label. We denote e x as the weighted summarization P i =1  X  i x i . L copies of e x are then stacked into e X . Hence, the multi-label Rademacher complexity can be simply written as whose upper bound can be revealed by the following theo-rem.

Theorem 3. The proposed algorithm learns W and H over n training points with L labels i.i.d. sampled from distribution Q = X  X Y . For any data point, k x k 2  X   X  . The learning algorithm encourages that rank ( W )  X  k and k XH k 1  X   X  . k W k F is assumed to be upper bounded by  X  . Then, the Rademacher complexity of F is Proof. According to Eq. (29), we have Then the following bounds can be easily obtained, and This proves To make conclusions of Theorem 3, consider a typical algo-rithm that neglects tail labels and attempts to minimize the trace norm of W 0 = W + H for multi-label learning and with corresponding Rademacher complexity of O ( p rank ( W 0 ) /n ). As shown above, tail labels will significantly violate the low-rank assumption on W 0 , and thus the large rank ( W 0 ) will lead to a greater Rademacher complexity. In contrast, the bound revealed in Theorem 3 is composed of two compo-nents corresponding to the low rank W and sparse XH , respectively. By separating the influence of tail labels, the rank of W (i.e., k ) will be smaller. Although tail labels might influence multi-label learning, its significance on the bound can be dramatically decreased by constraining the sparsity of XH (i.e. k XH k 1  X   X ). Hence, decomposing the multi-label model into low-rank and sparse parts to handle tail labels is helpful for decreasing the Rademacher complexity of the function class, which in turn improves the generaliza-tion error bound of the algorithm.
In this section, we evaluate the proposed algorithm on six benchmark multi-label datasets: Bibtex , Delicious-S , Medi-amill , Wiki10 , Delicious-L and Amazon . All these datasets were provided by [4], and have already been pre-separated into training and test sets. A summary of the statistics of datasets is shown in Table 1. #training is the number of training examples; #test is the number of test examples; #features is the number of features; #labels is the number of labels; #card-label is the average number of positive la-bels per example; #card-feature is the average number of nonzero features per example. The first three datasets with less than 1,000 labels are regarded as small datasets, while the last three are large datasets. We set the embedding di-mension in REML algorithm as 0 . 8 L for the small datasets, and 200 for the large datasets. Since the Delicious-L and Amazon datasets are rather large, in optimizing REML we divided the original problems into 2 and 4 subproblems re-spectively using the divide-and-conquer strategy. For the other datasets, we directly solve REML using the basic op-timization method.

In experiments, we compared our proposed Robust Ex-treme Multi-label Learning (REML) algorithm with LEML [27], which is the state-of-the-art label embedding method based on the low-rank assumption over label matrix, and SLEEC [4], which is a recently developed method using the neighborhood embedding technique to handle tail la-bels. Other representative multi-label learning algorithms, such as CS [15], CPLST [8], ML-CSSP [5] and 1-vs-All [1], which are only applicable for small datasets, are included in comparison experiments as well.

We used two metrics to evaluate the multi-label classifi-cation performance in the experiments, both of which have been widely used in the fields of multi-label learning and ranking. Precision at k measures the fraction of true positive predictions in the top k scoring labels, given the predicted score vector b y  X  R L . nDCG at k measures the usefulness, or gain, of a label based on its position in the predicted label list. We refer readers to [19] for more detailed information.
We compare the classification performance of the pro-posed REML algorithm with those of leading methods on three large datasets: Wiki10 , Delicious-L and Amazon . The classification results measured in Precision@ k and nDCG@ k are presented in Figures 2-4. It can be seen that REML stably performs better than LEML on these large datasets. For example, REML improves over LEML by as much as 18% and 20% in terms of Precision@3 and nDCG@3 on the Wiki10 dataset. This is because that the success of LEML mainly depends on the low-rank assumption, which tends (see Figure 1). REML provides an appropriate approach to preserve the validity of low-rank assumption by elegantly handling the tail labels as outliers. Hence, REML is able to achieve satisfactory classification results when faced with a number of tail labels, which is evidenced by its comparable performance with respect to the SLEEC method.
We next conduct multi-label classification over three small datasets: Bibtex , Delicious-S , MediaMill , which can be han-dled by more leading multi-label learning algorithms, such as CPLST and CS. The classification results in Precision@ k are shown in Figures 5-7, while Table 2 presents the results in nDCG@ k . Since the tail label problem is not acute on these small datasets, LEML and other comparison algorithms can achieve fine classification results. However, LEML is inferior to SLEEC, which uses neighborhood embedding to get rid of the influence of tail labels. Hence, SLEEC improves LEML by 4 . 8% in terms of Precision@1 on the Bibtex dataset and 7 . 8% in terms of Precision@3 on the MediaMill dataset. We suggest that the performance gap between LEML and SLEEC can be bridged by the REML algorithm, which is able to suppress the influence of tail labels and activate the low-rank assumption on multiple labels. On the Delicious-S dataset, REML is the closest competitor of SLEEC, while on the Bibtex dataset, REML takes a lead ahead of SLEEC in terms of Precision@5. This demonstrates that the low-rank assumption is powerful for processing multiple labels, and its performance can be further strengthened by care-fully investigating the tail labels as outliers in the low-rank formulation.
We next perform experiments on the Wiki10 dataset, to explore the trade-off between computation and accuracy when using divide-and-conquer technique to optimize the proposed REML algorithm (denoted as  X  X C-REML X ). Table 3 presents the time required to solve REML with different numbers of subproblems (i.e., t ), and the corresponding classifica-tion results in Precision@ k and nDCG@ k . From this table, we find that DC-REML performs comparably to REML for smaller values of t , and the performance gradually degrades for larger subproblem number, which is consistent with the theoretical analysis in Section 5.1. Most importantly, DC-REML have significantly sped up REML by dividing the original problem into 5 subproblems, with an acceptable per-formance degradation of 4% relative to REML in terms of Precision@3. Hence, given the scalability provided by the divide-and-conquer optimization technique, we can flexibly manage the optimization accuracy and time cost in solving large-scale multi-label learning problems.
In this paper, we study the tail labels problem in multi-label learning, where the scarce labels associate with limited number of examples. To prevent the damage of tail labels on the low-rank assumption over multiple labels, we treat tail labels as outliers and develop a robust extreme multi-label learning algorithm. Divide-and-conquer approach applied to optimize the resulting objective function is beneficial for im-proving the scalability, and its advantages in balancing accu-racy and computation have been theoretically demonstrated. We analyze the generalization error of the proposed algo-rithm, and suggest that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world datasets demonstrate the significance of investigating tail labels and the promising performance of the proposed algorithm.
 We greatly thank anonymous reviewers for their positive support and constructive comments for improving the paper quality. The work was supported in part by Australian Re-search Council Projects FT-130101457, DP-140102164 and LE140100061, NSFC61375026 and 2015BAF15B00.
 [1] S. V. N. V. B. Hariharan and M. Varma. Efficient [2] K. Balasubramanian and G. Lebanon. The landmark [3] P. L. Bartlett and S. Mendelson. Rademacher and [4] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain. [5] W. Bi and J. Kwok. Efficient multi-label classification [6] W. Bi and J. T. Kwok. Multi-label classification on [7] G. Carneiro, A. B. Chan, P. J. Moreno, and [8] Y.-N. Chen and H.-T. Lin. Feature-aware label space [9] M. M. Cisse, N. Usunier, T. Artieres, and P. Gallinari. [10] K. Crammer and Y. Singer. A family of additive [11] D. L. Donoho. De-noising by soft-thresholding. [12] A. Elisseeff and J. Weston. A kernel method for [13] W. Gao and Z.-H. Zhou. On the consistency of [14] B. Hariharan, L. Zelnik-Manor, M. Varma, and [15] D. Hsu, S. Kakade, J. Langford, and T. Zhang. [16] S.-J. Huang, Y. Yu, and Z.-H. Zhou. Multi-label [17] C.-L. Li and H.-T. Lin. Condensed filter tree for [18] L. W. Mackey, M. I. Jordan, and A. Talwalkar. [19] C. D. Manning, P. Raghavan, H. Sch  X  utze, et al. [20] R. E. Schapire and Y. Singer. Boostexter: A [21] E. Spyromitros, G. Tsoumakas, and I. Vlahavas. An [22] L. Sun, S. Ji, and J. Ye. Hypergraph spectral learning [23] F. Tai and H.-T. Lin. Multilabel classification with [24] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining [25] C. Vens, J. Struyf, L. Schietgat, S. D X zeroski, and [26] R. Yan, J. Tesic, and J. R. Smith. Model-shared [27] H.-F. Yu, P. Jain, and I. S. Dhillon. Large-scale [28] M.-L. Zhang and K. Zhang. Multi-label learning by [29] M.-L. Zhang and Z.-H. Zhou. Multilabel neural [30] M.-L. Zhang and Z.-H. Zhou. Ml-knn: A lazy learning [31] M.-L. Zhang and Z.-H. Zhou. A review on multi-label [32] Y. Zhang and J. G. Schneider. Multi-label output
