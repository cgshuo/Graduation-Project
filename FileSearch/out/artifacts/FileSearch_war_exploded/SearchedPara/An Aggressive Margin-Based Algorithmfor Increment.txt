 Requests of analyzing collected peri od data have been emerged in recent prac-tical applications that includes network traffic analysis [1], anomaly detection [2], and intrusion detection [3]. Generally, those applications are implemented for adjusting classifiers/detectors periodically. Most of incremental learning ap-proaches have been proposed based on deci sion-tree [4], neural network [5,6], and Support Vector Machines (SVM) [3,7,8,9,10]. Typically they are designed to build the statistic classification model based on the previously seen samples and to correct its prediction mistakes on new labeled samples. While focus-ing on the sample space, SVM generalizes the separating hyperplane (classifier) based on the whole sample distribution, and maximizes the margins of labeled samples (support vectors). The margin of a sample is a distance between the sample and the separating hyperplane. And SVM is theoretically proven that the hyperplane is able to well separate samples with different labels. In [10], an incremental batch SVM approach was designed to update the classifier by solving a constrained optimization problem based on each set of collected sam-ples. An example is illustrated in Fig. 1 (a) where the classifier w i is adjusted as w i +1 depending on the set of samples, { x i complicated constrained optimization p roblem since those collected samples are adopted simultaneously. Other approaches [8,9] adjusted SVM classifiers incre-mentally by identifying each new sample as a support vector or not. Different with [10], in Fig. 1 (b) the classifier w i is adjusted as w i 1 using first sample x i 1 in the set, and then w i 1 is updated as w i 2 using x i 2 .Thus w i is incrementally ad-justed as w i +1 depending on each sample in the set. The advantage of [8,9] is to maintain useful samples that were previously seen as support vectors and to ob-tain efficient update steps without solving a constrained optimization problem. But in those SVM approaches, the hyperplanes might not be quickly adjusted when encountering diverse sample distribution. In other words, the diverse sam-ples have small chances to be support vectors because the distribution of those samples is significantly different with the distribution of samples in the set. Thus in this paper, our approach is to simplify the constrained optimization problem for update steps and to adapt the diverse sample distribution for classifiers.
Rather than training the SVM classifier based on each sample or each set of collected samples, our approach adjusts the current classifier incrementally according one sample in ea ch collected set. Thus for each potential update, we formulate an optimization problem with single constraint. Additionally our up-dated classifier shall correct prediction mistakes of the previous classifier as many as possible. Compared with [10], we divide a complicated constrained optimiza-tion problem into several simpler ones. In other words, the classifier is adjusted as several potential ones depending on different samples. An example is illus-is selected as the next w i +1 . In this paper, we are motivated by the simplicity of online Passive-Aggressive (PA) algorithm [11]. One sample X  X  margin is selected as the basis for classifier adjustment. Thus in our approach, while a sample is used for updating and its sign is incorrectly predicted, the classifier adjustment is aggressively achieved within the margin. Additionally the updated classifier shall correct prediction mistakes of the previous classifier as many as possible. In this paper, we formulate a simple constrained optimization problem for each sample and then the candidate updated classifier is the solution derived using Lagrange multipliers. It is noted that, we get a closed form solution for each potential updated classifier. Particularly the selected new classifier, updated by the suitable margin, shall obtain the best classification accuracy on the collected dataset. It is expected that, this selectio n strategy is able to avoid the new clas-sifier being extremely specific to the previous one. And the updated classifier could flexibly adapt the diverse sample distribution because there is no need for the proposed approach to maintain previously seen samples.

Basically PA has the ability to frequently update the classifiers, but its two straightforward approaches may not be able to achieve impressive results. Firstly, PA update steps are specific to each labeled sample whether it is inconsistent or not. The consequence is that updated classifiers would obtain the unstable prediction ability. Secondly, the other PA approach is to update the classifier re-spectively using each sample. Then the se lected classifier among all updated ones shall have the best classification accura cy on the collected dataset. Compared with our proposed approach, this approach does not actively correct prediction mistakes of the previous classifier. Thus these two approaches do not fully uti-lize the learning knowledge in each collected dataset. Moreover our approach is similar with re-sampling approaches, like bagging [12], to obtain improved classi-fication accuracy by depending on subsets of the sample set. The major difference is that, we focus on designing efficient update steps for online applications so that a closed form solution for the updated classifier could be obtained.
The rest of our paper is organized as follows. The online PA algorithm is reviewed in Section 2. In Section 3, we det ailedly describe the proposed approach and build the mathematic model. Experim ental results are presented in Section 4. Finally, we conclude the paper in Section 5. In online learning, each training sample is discarded after it is used to update the classifiers. Some research works like the Perceptron algorithm [13,14,15] and margin-based approaches [16,17] have been proven to be effective in a board range of applications. Additionally it is worth noting the Passive-Aggressive (PA) Algorithm [11] is a margin-based online learning approach that could be applied for various prediction tasks. PA uses linear predictors for label prediction of each incoming sample. And each upda te step of PA is executed depending on the margin of the labeled sample. The objective of PA update is to adjust the previous classifier as less as possible while the condition of classifier adjustment is satisfied. At the round t ,let w t be the vector of weights, x t be the sample, y  X  X  +1 ,  X  1 } be x t  X  X  true label, and the term y t ( w t  X  x t ) be the signed margin. The new classifier w t +1 is the solution to the following constrained optimization problem, where l ( w, ( x t ,y t )) is the hinge loss of w  X  X  prediction on x t . Typically whenever the loss is zero, PA is passive and w t +1 = w t means no clas-sifier adjustment. And while the loss is positive (less than 1), w t is aggressively updated by adjusting more than the margin, y t ( w t  X  x t ), and then the constrain l ( w t +1 , ( x t ,y t )) = 0 can be satisfied. Then the Lagrangian of the optimization problem in Eq. (1) is defined as Eq. (3).
 Let the partial derivation of l with respect to w be zero and then let the deviation of  X  with respect to  X  be zero, we have Ultimately the PA update is performed by solving the constrained optimization problem in Eq. (1). And it is theoretically shown that the aggressive update strategy of PA modifies the weight vector as less as possible. The effectiveness of PA in solving problems of classification and regression is formally analyzed in [11]. Based on this well-defined learning model of PA, several online algorithms [18,19] have been proposed for adding confidence information and handling non-separable data. While each set of labeled period samples c omes, the existing classifier shall be periodically updated for adapting the latest sample distribution. In this pa-per, we propose an incremental learning algorithm, named Incremental Passive-Aggressive (IPA). It adjusts the current classifier incrementally using one sample in each collected set. For each potential sample, there are two update steps in IPA: 1) to correct prediction mistakes of the current classifier, and 2) to ag-gressively update the current classifier by adjusting more than the margin. At last, the error minimization classifier o n the collected dataset is selected as the next classifier. Before formulating the model of the proposed approach, we de-fine some notations. Given the labeled dataset K t collected at the round t ,there round t , the vector of weights. When using each labeled sample x k  X  K t ,the updated classifier w t +1 shall correct mistakes of the previous classifier w t as many as possible and w t shall be adjusted as less as possible. Aggressively, if x k obtains the incorrect predicted sign from w t , then the adjustment for w t should be achieved within more than x k  X  X  margin. Thus these update steps to w t are formulated as the constrained optimization problem, where C 0 is a constant to control the tradeoff between the classifier deviation and the corrected prediction mistakes, and l ( w, ( x i ,y i )) is the hinge loss function.
Furthermore, after w t is updated using every sample x k  X  K t according to Eq. dates for the new classifier. In order to a void the new classifier being extremely specific to the current classifier, the sel ection strategy is to find the proper clas-sifier which has the most accurate classification performance on K t .Whenmore than one updated classifiers have the hig hest classification accuracy, we select the updated classifier which has the smallest difference with w t . Hence the new classifier w t +1 , selected among the candidate set of the updated classifiers, is the solution to the optimization problem, w where C is a large constant in order to select w strongly depending on the errors.
To solve the problem in Eq. (4), let C 0 =1and  X  t , the subset of | K t | ,be the set of samples of which predicte d labels are incorrectly decided by w t . While the loss of each sample in  X  t is positive (less than 1), the Lagrangian of the constrained optimization problem is defined as Eq. (6): Let the partial derivation of l with respect to w be zero, Then substituting Eq. (7) into Eq. (6), we have At last let the deviation of Eq. (8) with respect to  X  be zero, Ultimately, each update of the proposed incremental learning algorithm is per-formed by solving the constrained optimization in Eq. (4) and the updated clas-sifier is determined by solving Eq. (5). It is theoretically presented in Eq. (7) and (9) that the update to the current classifier w t is performed by correcting its prediction mistakes  X  t , and by adjusting it within the margin when the sample is incorrectly predicted. Overall the proposed algorithm is presented in Algorithm 1. At each round t , the dataset K t is collected to update the current classifier w . And the samples of which predicted labels are incorrectly assigned by w t are identified as  X  t , at line 4-5. Then for each sample x k  X  K t , the current classifier w t is individually updated as the candidate classifier w k according to Eq. (7) and (9), at line 7-8. At last, the classifier w k is selected as w t +1 if it gains the least prediction errors on K t , at line 10. Particularly at the first round, w 1 is initialized as (0 , ..., 0) and its prediction result is always positive. Thus the w 1 is adjusted as the first updated classifier w 2 depending on the false positive sample that could cause the minimum || w 2  X  w 1 || . Moreover in addition to minimizing the classifier deviation, we correct mistakes of the previous classifier. In terms of convergence, each classifier is adjuste d as small as possible. Also it is expected that, our approach is able to adaptively enhance the degree of adjusting classi-fiers when encountering diverse sample distribution that would cause significant prediction losses.
Algorithm 1. Incremental PA Learning Algorithm In this section, our experiments are des igned to present the performance of our approach in classification accuracy while the classifier is incrementally updated by several small training sets. To present the effectiveness of updating classifiers in our approach, we also implement the online PA and an incremental batch SVM [9]. Additionally in order to show the effectiveness of correcting mistakes of the previous classifier in eq. (4), the performance of our approach with C 0 =0 is also compared in following experiments. In terms of evaluating classification accuracy of a classifier, we would like to significantly present classification results of samples in two different classes. We use the measurement of micro-average ac-curacy to average the classification accura cies that are calculated in two classes, respectively. For consistence, the summa tions of loss errors in the eq. (4) and (5) are also revised as (1 -micro-average accuracy).

Table 1 presents 13 real-world data coll ections from 4 different sources used in our experiments. The multi-domain sentiment dataset 1 contains product reviews downloaded from Amazon.com from four product types (domains): Kitchen, Books, DVDs, and Electronics. Each domain has several thousand reviews, but the exact number varies by domain. In t his experiment, only Books, DVDs are used for evaluating performance of those learning approaches. From the second data source, the dataset at ECML/PKDD-2006 discovery challenge 2 is used to decide whether received emails are spam o r non-spam. Especially there are over 10,000 features in those three datasets, Books, DVDs, and Emails. But it is dif-ficult to analyze performance of the SVM classifiers implemented in Matlab [9] because the execution is time consuming on those high dimensional datasets. Thus we randomly select a part of documents, as presented in Tab. 1, in fol-lowing experiments. From the third data source, Spamming Bots [20] is the set of response codes of the sent emails , collected in National Chung Cheng University (CCU). It is used to analyze the behavior of each email sender and then to detect the spamming bots. At last the other datasets are the benchmarks in the UCI repository 3 . While we evaluate classification performance of learning approaches, we randomly divide each dataset into 10 subsets, and one of subsets is received at each round. In other words, one subset is used for initially training the classifier and deciding the value of C 0 in eq. (4) by obtaining the highest classification accuracy on the first subs et. Then others are received at each of 9 rounds. The classification accuracy at each round is measured by classifica-tion results of the classifier updated at previous rounds. To reduce variability in experimental results, we arrange 10 subset-round permutations on each dataset and average those 10 classification accuracies at each round.

At first these experiments, except on Diabetes in Fig. 2, are demonstrated that the proposed IPA has better performance than IPA with C 0 =0.That means, in addition to minimizing the classifier deviation, it is effective in eq. (4) to correct mistakes for updatin g the previous classifier. And on Diabetes , correction of mistakes to the classifier could not improve the classification ac-curacy on latter samples. It seems, on Diabetes previous learning knowledge is not useful for latter label prediction. Secondly on Australian , Ionosphere , Bots , and 10+4 in Fig. 3-4, it is presented that the online PA method can not obtain the remarkable classification performance since its update strategy is specific to each labeled sample. That means, the online PA method tends to be updated by inconsistent samples. Furtherm ore, except experimental results on Australian and Ionosphere in Fig. 3, it is shown that our approach obtains the best (or similar) classification accuracy in comparison with other approaches. We update the classifier by carefully analyzing classifier adjustment caused for the labeled dataset. Then the remarkable classific ation accuracy is obtained at each round after the classifier is incrementally updated on most of datasets. Also it is shown that our approach has the ability to adapt the diverse sample distribution for classifiers because we obtain better per formance in accuracy than the SVM ap-proach of which support vectors are maintained as informative samples. Mention to the performance on Australian and Ionosphere , it seems ambiguous or noise samples exist so that the approaches (PA and IPA) to incrementally update the classifier by one sample do not have impressive results. In this case, collected samples in the set might be simultaneously used for updating classifiers, like the incremental batch SVM, to filter out misleading or noise samples.

Interestingly on CYT+MEI , cp+im , German ,and Emails in Fig. 7-8, the incremental batch SVM approach has biased results. It is observed that, in es-timating performance of the classifier, it focuses on non-weighting estimated errors, instead of average weights for errors on two respective classes. Still on those datasets, proposed IPA has the practical ability to obtain the best clas-sification accuracy. Hence, our approach t o update classifiers is not affected by biased classification results. In this paper, we propose an efficient in cremental learning approach to deal with the practical requirement of frequently updating classifiers. Our approach is proposed to adjust the classifier incrementally using one sample in each collected set. That is, the classifier is aggressively updated by adjusting more than the margin of a sample, and its prediction mistakes are corrected as more as possible. For each potential update step, we get a closed form solution for the updated classifier through solving a simple constrained optimization problem. At last the selected classifier shall have the least pr ediction errors on the collected dataset. Our experimental results are presented that, when updating a classifier, it is effective to correct its prediction mistakes, in addition to minimizing the classifier deviation. And it is also shown that our approach has the ability to adapt the diverse sample distribution for classifiers. Except several datasets that consist of some misleading or noise samples, the classifier that is incrementally adjusted by our approach is able to gain remarkable classification accuracy. Therefore it is presented that the proposed approach is suitable to be applied for effectively adjusting the existing classifiers us ing periodically co llected datasets.
