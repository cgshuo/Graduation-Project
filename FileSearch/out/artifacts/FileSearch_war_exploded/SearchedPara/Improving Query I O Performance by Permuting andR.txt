 The I/O performance of query processing can be improved using two complementary approaches: improve the buffer and the file system management policies of the DB buffer manager and the OS file system manager (e.g. page re-placement), or improve the sequence of requests that are submitted to a file system manager and that lead to actual I/O X  X  ( block request sequences ). This paper takes the latter approach. Exploiting common file system practices as found in Linux, we propose four techniques for permuting and re-fining block request sequences: Block-Level I/O Grouping , File-Level I/O Grouping , I/O Ordering ,and Block Recy-cling . To manifest these techniques, we create two new plan operations, MMS and SHJ , each of which adopts some of the block request refinement techniques above. We im-plement the new plan operations on top of Postgres running on Linux, and show experimental results that demonstrate up to a factor of 4 performance benefit from the use of these techniques.
 Categories and Subject Descriptors: H.2.4 [Database Management]: Query processing General Terms: Algorithms, Performance Keywords: Block request sequence, I/O sequence
A Database Management System (DBMS) depends upon two components to manage how data is transferred to and from disk: the Database (DB) Buffer Manager ,andthe underlying Operating System X  X  File System Manager .The File System Manager manages the block requests concern-ing reads and writes of files . It controls the OS I/O buffer that is shared by all processes, and provides an interface for processes to manage persistent data in files. The DB Buffer Manager manages the dedicated DB buffer, whose content is determined by reads (queries) and writes (up-dates) of database objects (i.e., relations). Management here is exclusively of a buffer that is independent of the OS Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. I/O buffer, and is used only by the query processing engine of the DBMS.
 Acting in concert though typically independently, the DB Buffer Manager and File System Manager together comprise a policy for mapping a query-generated sequence of block read and write requests (a  X  block request sequence  X ) to a sequence of actual I/O X  X  that result in data transfer to and from disk (an  X  X /O Sequence X ). (Note that the I/O sequence may not be exactly the same as the block request sequence because some block requests in the sequence may be satisfied by the (DB or File System) buffer and therefore, never result in a disk I/O.) This combined policy can be characterized as a function, f , that accepts a block request sequence, as input and produces an I/O sequence, f ( s ) as output.
The effectiveness of f for a given block request sequence, s , can be measured in numerous ways, including: 1. I/O Count: the number of blocks transferred to and 2. Disk Seek Count: the number of disk seeks required to 3. I/O Time: theamountoftimerequiredtoperformthe
These measures are listed in i ncreasing order of the num-ber of I/O performance factors they consider. I/O Count measures buffer management effectiveness. Disk Seek Count measures buffer management effectiveness as well as clus-tering of consecutively requested blocks in the block request sequence. I/O Time more generally measures the effective-ness of physical disk layout by accounting for consecutively requested sets of blocks that are a short disk seek apart.
A traditional approach to improving I/O performance is to alter f , either at the DB Buffer or File System levels. A survey of buffer management strategies is presented in [3]. A novel, complementary approach proposed in this paper as-sumes that f is fixed, and instead permutes and refines the block request sequences input to f , to better exploit buffer and file system management policies to achieve better I/O performance. For the purposes of this paper, we assume a fixed f which is the combined buffer and disk management policies of the Postgres DB Buffer Manager and the Linux File System Manager. While the results we present are spe-cific to Linux and Postgres, we believe that the approach of manipulating block sequence requests to better exploit underlying buffer management policies has general applica-bility to any DBMS and File System combination.
To demonstrate our approach, we show two example block request sequences that both result in sorting a small file, consisting of four initial runs that are merged two at a time The first is that produced by a s tandard external merge sort. The second is a refinement of the first produced using our techniques.

Figure 1a illustrates the steps involved in sorting R using a standard external merge sort ( EMS ) as described in most introductory database texts [10]. The tree structured di-sorting individual runs, A , B , C and D into sorted runs B , C and D respectively; level 1 involves merging runs A and B into a new run E ,and C and D into F ;andlevel 2 involves merging E and F into the result run, G .Eachrun in the diagram is annotated with an integer (1-7) reflecting the order in which runs are constructed. Because EMS is a lazy merge algorithm [5], level 1 runs ( E and F )areonly constructed after all level 0 runs ( A , B , C ,and D )have been constructed.

Figure 1b shows how R is sorted using an alternative merge sort introduced in this paper ( Multilevel Merge Sort , or MMS ) that permutes the block request sequence produced by EMS . One of the major differences between MMS and EMS is that MMS performs an eager rather than lazy merge sort. As evidenced by the differing numbering scheme, this means that the level 1 run, E ,isconstructed immediately after the level 0 runs, A and B , are constructed. Similarly, F is con-structed immediately after C and D .

Figure 2 contrasts the block request sequences generated by EMS and MMS . A block request sequence shows data re-quests made by a process in the order these requests are is-sued. ( R X refers to a request to read block X .) For example, the block request sequence for EMS shows that every level 0 run (e.g., A ) is read, and its sorted version ( A ) is written the block request sequence for MMS shows that the writes of
A and B are immediately followed by the reads of the same blocks for the purpose of generating the two pages ( and E 2 )ofthelevel 1 run, E . These two block request se-
To keep the example as simple as possible, we assume that each of the initial runs occupies a single page, and that the internal memory used for sorting runs (1 page) is distinct from the internal memory used for merging (3 pages) as in Postgres [12].

Table 1: Process Block Request Sequences (LRU) quences result in different I/O sequences when processed by the Linux file system. We assume the file system practices applied by Linux are:  X 
LRU-Based Page Replacement,  X 
Delayed Buffer Flushing (A dirty page is written to disk only when it is evicted from the I/O buffer).
 Table 1a and 1b compare the I/O buffer contents and the I/O sequences of the two block request sequences in Figure 2 assuming a 3-page I/O buffer. The first column of each table ( X  X lock Requests X ) shows the block request sequence for each sort algorithm. The shaded block requests are the of each table ( X  X /O Buffer X ) show the contents of the buffer after each block request is satisfied, and the last column of each table ( X  X /O Sequence X ) shows what (if any) disk input ( I
X ) or output ( O X )operationsmustbeexecutedforsome block X to satisfy the corresponding block request.
The better I/O behavior exhibited by MMS attributes to how it permutes its block request sequence to better exploit the following file system practices:  X 
LRU-Based Page Replacement: Note that the 4 th -8 th re-quests of the MMS block request sequence involve reading runs A ( R run E ( W E 1 and W E 2 ). The reads of A and B occur im-mediately after these runs have been constructed, meaning that they are still contained in the I/O buffer when the read requests are satisfied. Thus, MMS does not need to execute actual I/O X  X  to satisfy these block requests. In contrast, EMS constructs run E with the 9 th -12 th requests of its block re-quest sequence, meaning that A and B are required long after they have been constructed, and therefore after they were evicted from the buffer. As a result, EMS must perform actual I/O X  X  ( I A and I B )tobring A and B back into the I/O buffer. The same phenomenon is exhibited in the con-struction of run F using C and D .Asaresult, MMS has no actual I/O X  X  corresponding to actual I/O X  X , I A , I B , and I D ,executedby EMS .Thisdemonstratesthecaching benefits of eager merging resulting from better locality in the block request sequence.  X  Delayed Buffer Flushing: Not only must EMS read runs A , B , C and D into the buffer to construct runs E and F , EMS must previously have output these runs to disk when they were originally evicted from the I/O buffer. On the other hand, MMS no longer has a need for A and B once run E has been constructed, and thus need not ever flush A or B to disk! Of course, there is no way for a file system to know that a dirty page is no longer needed by an application. So MMS exploits delayed buffer flushing to trick the file system into never writing the dirty page to disk. The technique in-volves block recycling : fooling the file system into thinking it is updating a page of one file when it is in fact, over-writing this page with a page from a different file. This is what MMS does in response to the block request, W E 2 : page E 2 overwrites page A without the file system X  X  knowledge, thereby circumventing the page replacement operation that would have flushed A to disk. Delayed buffer flushing en-ables this technique because A is not forced to disk when it is constructed but only when it is evicted from the I/O buffer. Because the construction of E is interpreted by the filesystemasawriteto A ,evictionof A never takes place. This optimization accounts for the remaining 4 actual I/O X  X  ( O not MMS .
This paper demonstrates the viability of permuting and refining block request sequences as a technique for improving the I/O performance of query processing. This technique has two benefits:  X 
It offers a complementary research direction to the work on developing new file system and DB buffer management strategies (e.g., new page replacement policies).  X 
It also provides a relatively uninvasive way to improve the performance of existing DB systems. Instead of altering the DB buffer manager or the file system manager of the underlying DB or file system, it assumes these are fixed, and exploits them at the operator level.

We will demonstrate these benefits in the context of Post-gres running on Linux, and show how the block request se-quences for temporary files constructed by Postgres X  block operations (sort, hash join) can be permuted and refined to better exploit the management strategies used by the file system manager 2 . Thus, we assume a single fixed f based on the file system management policies of Linux. We effectively permute and refine the block request sequence produced by Postgres sort and hash join operations by in-troducing alternative versions of these operations that pro-duce alternative block request sequences. The results shown in this paper lay the groundwork for more general study of how permuting and refining block request sequences can im-prove the I/O performance for the management policies used in any underlying DBMS and file system.
 The remainder of this paper is organized as follows. In Section 2, we discuss four common file system practices and present techniques for permuting and refining block request sequences in file systems to exploit these practices. In Sec-tion 3, we introduce two new sort and join operations, MMS and SHJ , which use the above techniques. In Section 4, we Postgres temporary file construction bypasses the DB Buffer manager and instead directly interacts with Linux file system manager to construct files. show experimental results and illustrate the I/O benefits of MMS and SHJ with file system tracing tools, and before dis-cussing related work in Section 5 and concluding with future research directions in Section 6. In this section, we describe the file system practices of Linux (Section 2.1) and introduce four techniques that can be used to manipulate block request sequences to achieve better I/O performance (Section 2.2).
Linux employs the following file system techniques which can be exploited during query processing:  X  LRU-Based Page Replacement: Linux chooses pages in its I/O buffer to replace based on their page reference counters. Because it decreases page counters exponentially, its page replacement behavior approximates LRU.  X 
I/O Merging: disk scheduler merges I/O requests over ad-jacent blocks into a single multi-block request to reduce the number of actual I/O X  X  required to respond to all I/O re-quests in the I/O request buffer.  X 
Delayed Buffer Flushing: When a page in the I/O buffer is changed ( X  X irtied X ), an Operating System can choose to flush its updated contents at a later time.  X 
Block Grouping: This file system practice involves pre-allocating a contiguous group of disk blocks to a file when a new block is needed. For example, Ext2FS pre-allocates a group consisting of up to 8 adjacent blocks by default when allocating a new block [2].
We introduce four techniques for permuting and refining block request sequences in ways that exploit the file system practices discussed in Section 2.1. To illustrate these tech-niques, we will refer to the example block request sequence in Figure 3a involving four disk blocks: A , B , C ,and D Figure 3: A Block Request Sequence and its Per-mutations
Block-level I/O grouping involves permuting a block re-quest sequence so that block requests involving the same block are in close proximity within the sequence. 3 We saw
Obviously, not all permutations of a block request sequence are possible. For example, a read request and a write request of the same block cannot be reordered without changing the semantics of the requesting application. However, the plan operations that we propose are constrained to permute block request sequences in a semantics-preserving way. one example application of this technique in Section 1, where MMS reordered block requests so that requests to read blocks ample, given the block request sequence of Figure 3a, block-level I/O grouping might result in the block request sequence shown in Figure 3b where all block requests involving pages A and B occur before all block requests involving pages C and D .

Block-level I/O grouping exploits two file system practices to achieve better I/O performance for the same set of block requests:  X 
LRU-Based Page Replacement: Block-level I/O grouping effectively reduces the block-reference working set given a fixed window size. Because LRU attempts to maintain the current working set in the file system buffer, this means that the resulting block request sequence is likely to result in more buffer hits and fewer actual I/O X  X  to satisfy a given set of block requests (as was the case in the example presented in Section 1).  X 
Delayed Buffer Flushing: If a page is written multiple times in a block request sequence, and is only flushed to disk when it is evicted from the file system buffer, block-level I/O grouping improves the chance that the dirty page is flushed to disk only once, even if it is written multiple times.
File-level I/O grouping involves permuting a block request sequence so that block requests involving blocks from the same file are in close proximity within the sequence. For ex-ample, if blocks A , B and D from the block request sequence of Figure 3a were from one file, R , and block C was from another file, S , then file-level I/O grouping of this sequence might result in the block request sequence shown in Fig-ure 3c where all block requests involving file R are grouped together.
 This technique exploits block grouping to achieve better I/O performance for the same set of block requests. To illustrate, suppose that an application generates the block request sequence, such that A , B and D belong to a new file, R and C belongs to a new file, S . Suppose further that block groups pre-allocated to a file can hold 2 pages. Then, the block request sequence above would result in a block group being allocated to file R and assigned pages A and B , followed by a block group being allocated to file S and being assigned page C (with the second block in the group unassigned), followed by a second block group being allocated to file R and assigned page D (with the second block in the group unassigned). This is illustrated below: Now suppose that file-level I/O grouping is applied to the above block request sequence, resulting in the block request sequence shown below: This block request sequence would produce the following block assignments: Observe that given the original block allocation, a file scan of
R requires two disk seeks, whereas the same file scan with the blocks allocated as above requires only one disk seek. This simple example illustrates that file-level grouping can help reduce the cost of performing a sequential scan of any file whose blocks are allocated from disk as a result of executing a given block request sequence.

An alternative strategy to make files more contiguous is to increase the sizes of block groups. However, this may result in increased internal fragmentation (by increasing the number of unused blocks at the end of a file) and therefore may increase the disk head distance between files, making algorithms that require alternating I/O X  X  between files (as in a MERGE operation) more expensive.

As with block-level I/O grouping, arbitrary reordering of a block request sequence is not possible without affecting the semantics of the requesting applications. However, our SHJ operator (Section 3) reorders independent page writes that construct partition files, thereby preserving the semantics of the join.
Suppose that blocks A , B , C and D from the block re-quest sequence of Figure 3a lie on tracks 100, 150, 151 and 101 respectively. Then, it may pay to order the block re-quest sequence so that block requests for blocks that lie on adjacent tracks are themselves adjacent, as demonstrated in the block request sequence of Figure 3d.

This reordering exploits I/O merging. Specifically, if ad-jacent I/O requests for adjacent blocks both result in active I/O X  X , then the adjacent requests will be merged into a sin-gle multi-block request. Take the block request sequence of Figure 3d for example. If the 3 rd and 4 th requests in this sequence (writing A and D ) both go to disk, they will do so with a single I/O. Similarly for the 9 th and 10 th requests (writing B and C ). As with block-level and file-level I/O grouping, our operator that performs I/O ordering ( SHJ ) does so in a semantics-preserving way.
Suppose, A in the block request sequence of Figure 3a be-longs to a temporary file and is no longer needed after it is last referenced (i.e. after it is read as a result of executing the 8 th request of the sequence). We can evict A from the I/O buffer without flushing it to disk even though it is dirty because it is no longer needed. There is no way for a file sys-tem to know that a dirty page is no longer needed. However, a query operator can  X  X rick X  the file system into throwing out a no longer needed page by storing a new page in it. For example, if D is a new page and the write of D follows the last read of A , D can overwrite the page allocated to A . This benefit was seen in the example application of MMS presented in Section 1, where pages A and B ( C and D ), used to construct run E ( F ) were overwritten by the pages of
E ( F ), and therefore never written to disk.
In this section, we present an alternative external sorting algorithm, Multi-Level Merge Sort ( MMS ),andanalternative hash join algorithm, Sort Hash Join ( SHJ ). These two algo-rithms both adopt some of the techniques we presented in Section 2.2 and differ from traditional sorting and hash join algorithms in the block request sequences they produce.
MMS is a variation of eager merge sort [6] that recycles blocks from runs that are no longer needed for use in new runs. An eager merge sort differs from the lazy merge sort of EMS by eagerly merging runs as soon as enough runs have been constructed, and prior to the construction of all runs of the same merge level. MMS performs an eager merge sort, but also records the blocks of runs that have been merged (which can be identified with file names and offsets) and uses them to store blocks of newly constructed runs. By merging runs  X  X agerly X , MMS reads runs shortly after writing them and achieves Block-Level I/O Grouping .Byreplacing the blocks of old runs with those of new runs, MMS achieves Block Recycling . We have illustrated MMS in Section 1.
Figure 4 shows the pseudocode of MMS ( N is the merge fan-out and M is he maximal merge level). C i counts the number of runs at level i ,and R j i refers to the j th run at level i . MMS consists of the following two phases:  X 
CreateInitialRuns: This phase reads tuples from the in-put relation, S , until the memory is full, or until there is no tuple left in S ( EOF ( S ) ). It then sorts tuples in memory and stores them as an initial run, and repeats this process until there are N initial runs ( C 0 = N ), or until there is no tuple left in S . R 0 0 , ..., R C 0  X  1 0 point to the newly created runs.  X 
MultiLevelMerge: This phase merges all runs of level i into new runs at level i + 1, until it reaches a level, L taining fewer than N runs ( C L &lt;N ). MergeWithRecycle records the blocks of runs that are no longer needed, and recycles them for blocks of new runs.
 MMS alternates between these two phases until the input is exhausted. Then it produces the final run by merging runs level by level.

In short, by merging runs eagerly and writing new runs to no longer needed runs, MMS achieves Block-Level I/O Group-ing and Block Recycling, and exploits LRU-Based Page Re-placement and Block Recycling.
SHJ is a variation of Hybrid Hash Join ( HHJ )thatperforms one pass replacement selection sort over tuples on their par-tition numbers prior to their placement in partitions. HHJ uses a separate output buffer to collect tuples that are stored in partition files. Whenever an output buffer gets full, it sends a block request to the file system to write the buffer to the correspondent partition file. Thus, it is the order in which the output buffers get full that determines the block request sequence of HHJ . SHJ reorders tuples before writ-ing them to the output buffers, which changes the order in which the output buffers fill, and as a consequence, changes the order of the block requests. By sorting by partition num-ber prior to partitioning, SHJ achieves both File-Level I/O Grouping and I/O Ordering .

SHJ determines the number of partitions it creates in the same way that HHJ does. Let R be the build relation (the smaller of the join relations), M be the available memory, | R | and | M | denote the size of R and M measured in memory pages. The number of partitions SHJ creates is ( F is a fudge factor, which in this paper, we assume to be 1.) N is also the number of partition files SHJ creates for each join relation. SHJ uses N memory pages to create output buffers and the remaining | M | X  N memory pages to reorder tuples.

Figure 5 shows the pseudocode of SHJ . R and S are the build and probe relations of the join. N is the partition number calculated with Equation 1. We use a heap H to reorder tuples based on replacement selection. SHJ consists of the following two phases:  X 
Build: This phase reads tuples from the input, and inserts them into a priority heap ordered by their partition num-bers. It then removes tuples from the top of the heap and stores them into the output buffers of their corresponding partition files. Function HeapInsert ( H , t i , p i ) inserts tuple t i into a position in heap H based on its partition number p i . HeapRemove ( H ) returns the tuple at the top of H and removes it from H .  X 
Probe: Same as in HHJ , this phase creates an in-memory hash-table, and populates the hash-table with all tuples from a build partition file R i . It then probes the hash-table using tuples read from the corresponding probe partition file S to produce join results. Function CreateHashtable ( R i creates a hash-table and inserts all tuples into R i to it.
We illustrate how SHJ achieves File-Level I/O Grouping and I/O Ordering with a simple example. Assume a rela-tion of 19 pages that is partitioned with 7 memory pages ( | R | =19 , | M | =7). AccordingtoEquation1, HHJ creates 2 partition files ( N = 3). We also assume for simplicity that disk blocks and memory pages are of the same size, the file system pre-allocates two adjacent disk blocks to a file, and each tuple occupies a memory page.

Table 2a shows all the tuples to be partitioned. (Tuples, block requests and disk pages are in white if they belong to partition 0, light gray if to partition 1, and dark gray if to partition 2.) HHJ keeps the tuples of partition 0 ( T 1 , in memory and writes the remaining 14 tuples to partition files. The first row in Table 2b ( X  X lock Requests X ) shows its block request sequence. The file system allocates block groups to partitions 1 and 2 according to the block requests. The result shows that both partition files 1 and 2 are stored in 4 separate block groups (row  X  X age X  and  X  X ile X ). Thus, accessing each of them sequentially requires 4 disk seeks. In total, accessing these two files requires 8 disk seeks.
To partition the same relation, SHJ creates 3 partition files. Thus, SHJ writes one more partition file to disk than HHJ does (partition 0). However, though more data are writ-ten to and read from disk by SHJ , the costs of the reads and writes are offset by the placement of blocks from SHJ result-ing in fewer disk seeks. Of the 7 memory pages, SHJ uses 3 as output buffers, and the remaining 4 to sort tuples. It writes tuples to their partition files with the block request sequence in Table 2c. We can see from Table 2c that ac-cessing each file requires 2 disk seeks. In total, it requires 6 disk seeks to access the partition files of SHJ ,whichis less. This demonstrates the benefit brought by File-Level I/O Grouping .

Table 2 also shows that SHJ achieves I/O Ordering .As discussed in Section 2, consecutive block requests that re-quire consecutive disk blocks are likely to be merged into one actual I/O by the I/O scheduler. The block request sequence of HHJ contains 7 block requests that are followed immediately by requests demanding disk blocks that are not adjacent to the blocks they demand (These are underlined in Table 2a: W T 2 , W T 5 , ...). Meanwhile, the block re-quest sequence of SHJ contains only 4 such block requests. Again, although the request sequence of SHJ contains more requests, it can be processed with fewer actual I/O X  X  and therefore faster because the I/O scheduler is able to merge more requests in it into one 4 . Thus, by sorting, SHJ also achieves I/O Ordering . This reduces the number of seeks required to write the partition files.

To summarize, SHJ  X  X  strategy of partially sorting its input during the build phase of the hash join achieves File-Level I/O Grouping and I/O Ordering , and exploits I/O Merging and Block Grouping .
To determine the benefits of performing block request re-finement in query processing, we implemented MMS and SHJ on top of Postgres 8.0 running on Linux. In Section 4.1, we compare the execution time of MMS against the Postgres sort, Polyphase Merge Sort ( PPMS ) [7] and several other ex-ternal sorting algorithms. In Section 4.2, we compare the performance of SHJ with that of Postgres X  built-in Hybrid Hash Join ( HHJ ). For each of these experiments, we also test our theories about how their benefits derive from better ex-ploitation of file system practices by tracing file system be-havior during query processing.
 We conducted all experiments on a system with a 2Ghz Pentium 4 processor, 1 GB RAM, and two IDE hard drives over Linux-2.4.31 compiled with modules that enable tracing
Note that a block request seq uence reflects the order in which blocks are written to the I/O buffer. If two blocks are written to the I/O buffer closely, they are likely to be flushed out together. To make this example simple, we leaved out the part how the I/O buffer flushes dirty pages. of disk I/O and file system activity. One 120 GB hard drive is used to store the Linux kernel, executables, and all user files. The other 300 GB hard drive is dedicated to database files. The 300 GB hard drive is formatted to the ext2 file system with a file system block size of 4 KB, and a block group pre-allocation size of 8 blocks.

The tracing code is executed only when certain tracing modules are loaded into the system. All experiments are run with and without loading these tracing modules. We use the default I/O buffer flushing configuration of Linux. Pages are flushed to disk asynchronously when dirty pages exceeds 30% of the whole memory or dirty pages are older than 3 seconds, and synchronously when dirty pages exceeds 60% of the whole memory.
 The plan execution times were generated using Postgres X  EXPLAIN ANALYZE tool. Our testing data is generated with DBGEN for TPC-H [13] with various scale factors. We categorize tables by their sizes: tables less than 512 MB are small, between 512 MB and 1.5 GB are medium, and greater than 1.5 GB are large. We choose two tables from each category in our experiments.

To make sure that the timing result of the algorithms were not influenced by external processes (i.e., I/O workloads or buffer page references from other processes), we reboot the system immediately before a query is processed. Only kernel processes and Postgres are running in the system during query processing. In this section, we compare the performance of MMS vs. Postgres PPMS (Section 4.1.1) and show the impact of Block-Level I/O grouping and Block Recycling on the performance of MMS (Section 4.1.2 and 4.1.3). We start by comparing the performance of the four sort algorithms in Table 3. These four algorithms cover the space of possibilities given the choices between eager ( MMS ) vs. lazy merging ( PPMS ), and block recycling or no block recycling ( -NR ). All sort algo-rithms were configured to use the same amount of sort mem-ory that is allotted by default to Postgres X  PPMS (i.e., 1 MB)
Figure 6b compares the execution time of the four sort algorithms described previously, as well as two additional variations of MMS : MMS(1) and MMS(2) . These sort algorithms were added to the experiments, because results showed that for medium to large files, algorithms that do block recy-cling ( PPMS and MMS ) performed worse t han corresponding
The sort memory specifies the amount of private memory each sort (or hash) operation in Postgres can have. If there are n simultaneous sorts (or hashes), the total sort memory will be n times the default. Therefore, common wisdom dictating that the number should be kept fairly small to avoid swapping. A detailed discussion about this issue can be found in [8]. algorithms that do not ( PPMS-NR and MMS-NR ). We spec-ulated that this had to do with the fact that block recy-cling results in discontiguous allocations of blocks to runs, and that for very large runs, this results in poor scan per-formance due to excessive seek movement. We tested this theory by looking at hybrid algorithms that perform block level 2 merges ( MMS(2) ). For all six tables, the MMS -based al-gorithms consistently performed better than the PPMS -based algorithms. For very large tables (Large-2), MMS(1) is over four times faster than PPMS .

MMS-NR consistently performs better than PPMS-NR over all size tables. The reason is that MMS-NR exploits the LRU-based page replacement policy of Linux, and thereby gets a higher I/O buffer hit ratio. For the same reason, MMS performs better than PPMS .

When the table being sorted is smaller than the I/O buffer (Small-1, Small-2 and Medium-1), PPMS , MMS and MMS(2) all perform much better than PPMS-NR and MMS-NR .Thisis because these algorithms perform block recycling. 6 MMS-NR and PPMS-NR do not recycle blocks from old runs and instead write new runs to new disk blocks. Every time they write a new disk block, an I/O buffer page needs to be allocated to buffer the disk block. Although the table size is less than the I/O buffer, the same amount of data is added to the I/O buffer at each merge level. Once the number of I/O buffer pages used to cache the output exceeds the buffer flushing threshold, the I/O buffer flushes pages to disk to obtain more clean buffer pages. By flushing pages they no longer need to disk, these algorithms perform more writes than needed. Also, after the I/O buffer is flushed, run blocks which will be needed again soon are replaced by newly created run blocks and need to be read back from disk, further increasing the number of disk I/O X  X  required.

As tables get large, the cost of PPMS increases dramati-cally. This is because block recycling results in intermedi-ate sort runs that are highly discontiguous. For large files, this results in poor I/O performance, as writing and read-ing those runs require excessive disk head (seek) movement. For the same reason, MMS does not do well for very large files. MMS-NR and PPMS-NR always write runs to new blocks, thereby storing intermediate runs contiguously. Therefore they take less time to finish, even though they read and write more data. We will compare the actual I/O X  X  executed by these algorithms in the following section.

As we speculated, MMS(1) and MMS(2) performed best when sorting large tables. Compared to PPMS and MMS ,these algorithms access data sequentially after the first few merge levels and thereby avoid excessive random disk I/O. Com-pared to MMS-NR , these algorithms recycle blocks in the first few merge levels and reduce the number of writes.
In this section, we compare the read requests and actual data read by each of the sorting algorithms to test the hy-pothesis that eager sort algorithms such as MMS benefit from exploiting LRU-based page replacement, and therefore needs to perform fewer actual I/O X  X  to read data from disk.
For these experiments, we used the Linux tools STRACE
The block recycling benefits of MMS(1) are negligible for small tables, given that block recycling is performed only for a single merge level. and IOSTAT to log all system calls and disk I/O X  X . The read requests comes from STRACE. We use STRACE to trace all read system calls of the process and sum up their request sizes. The actual I/O X  X  comes from IOSTAT. We use it to report the blocks read from disk as the query is processed. The actual I/O percentage is the actual I/O divided by the read requests. We compare the I/O requests and actual I/O percentages of the sorting algorithms in Table 4. Note that because these tools measure the amount of data requested, and actually transferred to/from disk, the  X  X ctual I/O per-centage X  results serve as an estimate of the I/O buffer miss ratio rather than an actual measure of such.

For small tables (Small-1 and Medium-1), the actual I/O percentage for PPMS-NR , MMS-NR and MMS(1) is higher than that for the other sort algorithms. This is because these al-gorithms do not perform disk block recycling ( PPMS-NR and MMS-NR ) or do not perform enough disk block recycling to make a difference to performance ( MMS(1) ). Merging adds new blocks to the I/O buffer, which causes blocks which will be used soon to be evicted from the I/O buffer. These run blocks are read back later in the merging phase, which results in more actual I/O X  X  and a higher actual I/O per-centage. PPMS reads less data because it performs disk block recycling. By doing so, PPMS is able to use a limited set of buffer pages to cache all the runs it needs when table is small. The number of buffer pages it requires to cache runs is the table size.

For larger table (Large-1), MMS-NR and MMS read much less data than PPMS and PPMS-NR do. This is because they both exploit the LRU-based page replacement policy of Linux, and access run blocks from the I/O buffer before they are evicted when runs are fairly small. While when a table is larger than the I/O buffer, the I/O buffer cannot cache all runs, therefore, runs of PPMS and PPMS-NR are flushed to disk before they are merged and need to be read back.

With large tables, PPMS runblockshavetobewritten to disk before they are used again. MMS reads the small-est amount of data in all cases because it maximizes the buffer hit ratio by both grouping block requests and recy-cling blocks. MMS-NR , MMS , MMS(1) and MMS(2) differ only by their block recycling level and their relative performance confirms that more block recycling results in smaller actual I/O percentages.

Note that when tables get large, the actual I/O percentage does not correspond to the execution time. For example, the actual I/O percentage of PPMS-NR is about the same as PPMS but its execution time is much smaller. This is the effect of the random disk I/O X  X  introduced by recycling blocks, which was discussed in Section 3.
MMS exploits delayed buffer flushing and prevents dirty pages which are no longer required from being written to disk by performing disk block recycling. This results in less data being actually written to disk. Table 5 compares the actual I/O percentage of the sorting algorithms. Again, we use STRACE and IOSTAT to get the write requests and the actual I/O, tracing the write system calls and actual block written to disk.

Compared to PPMS , PPMS-NR has a higher actual I/O per-centage for small tables. This is because PPMS is able to cache all of the runs it needs in the I/O buffer and perform disk block recycling effectively. When the table gets large, PPMS-NR writes the same amount of data as PPMS does. For MMS-NR , MMS(1) , MMS(2) ,and MMS , their actual I/O percent-ages reflect how much block recycling they do (more block recycling leads to lower actual I/O percentages).
In this section, we compare the execution times of HHJ and SHJ and show how SHJ benefits from performing I/O ordering (Section 4.2.2) and file-level block grouping (Sec-tion 4.2.3). Table 6 lists the sizes of tables joined in our experiments. We choose Partsupp and Supplier because Supplier ,which determines the number of partitions, is small compared to the Postgres sort memory. We have done experiments on larger tables, and for these, SHJ still outperforms HHJ but requires more memory.

Figure 7 compares the performance of SHJ with HHJ .For small tables, SHJ does slightly worse than HHJ .Thisisbe-cause when the tables are small, the I/O buffer is sufficiently large that neither of the two algorithms actually read par-tition files from disk. Therefore, SHJ is not able to get any benefit from I/O merging or block grouping, but incurs CPU overhead because it sorts tuples, thereby writing more data to and reading more data from the I/O buffer than HHJ .As tables get larger, partition files are actually written to disk and read back, and the savings SHJ gets from faster disk I/O outweighs the CPU overhead it incurs. Therefore, for large tables SHJ performs better than HHJ . But when the build relation gets too large (Large-2), more partition files must be created and in this case, SHJ is not able to benefit from block grouping and starts to perform worse than HHJ .
I/O merging produces fewer but larger I/O requests. We tested to see if SHJ benefits from I/O merging by comparing the average size of I/O requests for SHJ and HHJ .Iftheaver-age size of an I/O request differs for the two algorithms over the same input, this implies that I/O merging was more suc-cessful for the algorithm with the higher average I/O request size.
 For this experiment, we used a disk request tracing tool, DCM, to trace all I/O requests of a particular partition sent to the IDE disk controller. DCM reports the type (read or write), location and size of each I/O request. We average the size of all requests reported during query processing.
Table 7 compares the average size of I/O requests for SHJ and HHJ . We used one table from each category (Small-1, Medium-1, Large-1) in our test set. The average I/O size of SHJ is greater than that of HHJ in all three cases.
It is worth mentioning that for table, Large-1, the aver-age read size of SHJ and HHJ is about the same, but the average write size of SHJ is larger than that of HHJ .This benefit comes mostly from the ordered and better merged write requests.
File-level I/O grouping results in contiguous block groups being allocated to the same file. One or more contiguous block groups of the same file form a contiguous region .We have illustrated in Figure 2 that the file-level grouped block request sequence of SHJ leads to files consisting of large con-tiguous regions.
 Table 8: Contiguous Region Sizes of HHJ and SHJ
We assume for this experiment that the distance on disk between contiguous regions is independent of the size of con-tiguous regions, and therefore, a file broken up into fewer, larger contiguous regions will be faster to scan than a file of the same size that is broken up into more, smaller con-tiguous regions. Thus, our experiments test to see which of SHJ or HHJ creates hash partitions consisting of larger (on average) contiguous regions.

We trace the blocks allocated to each file with an ext2 inode tracing tool of our own. It logs the block number allocated to each inode. We then find out the number of contiguous regions of each file and calculate out the average contiguous region sizes.
 Table 8 shows the average block group size of HHJ and SHJ . The average block group size of HHJ is 8, equals to the block group size of the file system. For Small-1 and Medium-1, SHJ is able to perform file-level I/O grouping effectively and stores files in block groups six and twice the system default size respectively. As a consequence, the I/O requests to retrieve the files created by SHJ can be better merged, which has been verified in the previous section by the average I/O size.
The results above demonstrate that permuting and refin-ing block request sequences does reduce the I/O costs of plan operations. The results also show that refining block request sequences by modifying plan operations has certain limitations. For example, SHJ can only use its private mem-ory for sorting; it works only when the build relation is fairly small. This motivates us to think of other ways to permute and refine block request sequences.

Besides modifying plan operations, we can also permute and refine the block request sequences of plan operations by changing their inputs. Take hash join as an example, if its inputs are almost grouped by their partition numbers, it produces block requests that are file-level grouped without sorting. Permuting and refining block request sequences in this way breaks the limitation imposed by plan operations and has the potential to bring more benefits.
Database management systems are implemented on top of operating systems. Stonebraker studied the operating system supports for database systems basing on UNIX and Ingres [11]. He argued that many operating system sup-ports are slow or inappropriate for database systems and suggested a small efficient operating system be built for database systems. In contrast, we approach this issue from the opposite direction  X  changing the plan operations to ex-ploit the practices used by operating systems.

Our work resembles in spirit, the cache-aware query pro-cessing work done independently by Ailamaki [1], Ross [14] and others. Like cache-aware query processing, our work is about formulating query processing techniques that exploit the characteristics of the underlying hardware or software system over which the DBMS operates.

DB2 sorts block reads in its list prefetching [4] to minimize the cost of disk seeks. Our I/O ordering and grouping tech-niques go further in two respects. First, we sequence writes of new files so that logically consecutive blocks (blocks be-longing to the same file) are physically positioned in close proximity on disk (thus making future file scans cheaper). Secondly, we group writes of blocks across multiple files by disk location so as to minimize seek time during writing. Thus, DB2 reduces the cost of randomly accessing existing files, our I/O ordering and grouping techniques reduce the cost of writing new files as well as the cost of accessing them sequentially.

Graefe discussed  X  X ager merge X  and  X  X azy merge X  for ex-ternal merge sort in [6]. MMS chooses to perform  X  X ager merge X  for the purpose of grouping block requests instead of reducing the number of runs. Patel et al. [9] noticed the in-terference between the reads of join relations and the writes of partition files in Hybrid Hash Join and built a more ac-curate model to estimate the cost of Hybrid Hash Join. But they did not consider the interference between the writes of different partition files. SHJ reduces the interference be-tween the writes of different files in a semantics-preserving way.
In this paper, we propose improving query I/O perfor-mance by permuting and refining block request sequences of plan operations based on the combined management poli-cies of the DB buffer and the file system. We studied four common file system practices used in Linux: LRU-Based Page Replacement , I/O Merging , Delayed Buffer Flushing , and Block Grouping , and developed four block request re-finement techniques: Block-Level I/O Grouping , File-Level I/O Grouping, I/O Ordering, and Block Recycling. To man-ifest these techniques, we created two new plan operations, Multilevel Merge Sort ( MMS ), Sort Hash Join ( SHJ ). Both of these operations adopt some of the block request refinement techniques.

Experimental results show that the best sorting perfor-mance occurs with versions of MMS that perform block recy-cling during early merge levels, but not during later merge levels. This is shown to be due to the benefits of Block-Level I/O Grouping on I/O buffer performance, and the benefits of Block Recycling to exploit delayed buffer flushing when constructing short intermediate runs. We also show that SHJ outperforms HHJ in most cases, due to its use of I/O Order-ing to exploit I/O Merging, and File-Level I/O Grouping to exploit Block Grouping. The only exceptions are for joining extremely small tables (where the I/O buffer is sufficiently large to make it unnecessary to flush hash table partitions to disk) and for extremely large tables (where too many parti-tion files are created and block requests can not be grouped effectively).

Block request sequence refinement is a complementary ap-proach to techniques that improve the management of the DB buffer and the underlying file system. We have demon-strated its viability as a technique for improving query I/O performance. In our future work, we plan to refine block requests from other directions (e.g. changing the inputs to plan operations) so that we can achieve more effective block request sequence refinement and bring more benefits.
