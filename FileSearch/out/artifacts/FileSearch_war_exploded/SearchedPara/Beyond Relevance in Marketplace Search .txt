 In this paper we study diversity and its relations to search relevance in the context of an online marketplace. We conduct a large-scale log-based study using click-stream data from a leading eCommerce site. We introduce 3 main metrics  X  selection (diversity), trust, and value. In our analysis we also show how these interact with relevance in different ways. We study the benefits of diversity and also show why guaranteeing diversity is important. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  information filtering. Economics, Experimentation, Measurement. eCommerce, search, diversity, relevance, trust, value, selection Relevance in ranking for keyword search has been extensively studied. Since search queries are likely to have different interpretations for different users, showing diverse set of results is necessary. For instance, for a generic query like  X  X p3 players X  different kinds of mp3 players should be returned. Ideally, the result set should properly account for the interest of the overall user population. Diversity in information retrieval and web search has also been well studied [2]. [4] describes Maximal Marginal Relevance (MMR) criterion to reduce redundancy while maintaining query relevance in re-ranking retrieved documents. Each document in the ranked list is selected according to a combined criterion of query relevance and novelty of information. [1] shows efficient query processing techniques that guarantee diversity. Although some attributes of a narrow section of inventory (automobiles) that impact diversity are discussed in, it does not provide enough insight into what dimensions impact user satisfaction and diversity in eCommerce search. Our work focuses on bridging this gap. As little work has been done around diversity in eCommerce search, we study and describe the role of relevance and diversity in eCommerce search by using large-scale retrospective datasets from a leading eCommerce site which has at any point in time millions of items for sale in different categories by different sellers. In section 2 we describe various dimensions that affect search results. In section 3 we describe experiments to discover the interactions between these dimensions and also show why guaranteeing diversity in product search is essential. Finally, we note our conclusions and plan for future work in section 4. For online marketplaces supporting C2C and B2C transactions sellers pay to list their inventory, however, the engine has to sort items based upon what the buyer is likely to buy and also provide a good buying experience. Various factors which influence buyer satisfaction, marketplace reputation and sell-through of items for sale come into play. Some of these factors which need to be considered in building the item result set are discussed below. For online retailers like Wal-Mart and Target the trust or reputation associated with the products is that of the online retailer itself as they control and manage the entire inventory sold on their site. However, in a multi-buyer to multi-seller marketplace, as there are many sellers selling different items, the trust factor associated with every item is different. If the same product is being sold for the same price by two different sellers with different reputations, buyers would surely want to buy it from the seller with a better reputation. The trust associated with an item is based on the reputation of the seller who is selling the item. The reputation score for a seller could again be influenced by various factors like feedback from former buyers, return policy for items sold by the seller, shipping options offered by the seller and other factors. Total trust associated with a result set which we refer to as  X  would be dependent on the collection of trust values of every individual item in the result set. For our experiments we use seller data and feedback from former buyers to compute the trust value of every item.  X  determines the long term health of the marketplace. In today X  X  online world, buyers have a lot of choice. A compelling value proposition is something that many buyers look for, before completing their purchase. Given two exactly same items with the same trust score, buyers are more likely to buy the one with a lower price. Thus, value proposition provided by an price of an item is less than the typical selling prices of similar items then the value proposition for the item is high and vice versa. The total value aspect associated with a result set would be dependent on the average value provided by all the items that the result set comprises of and we refer to it as  X  . Diversity of an item set is an indicator of the variety of items found in the result set. On eCommerce sites, the different factors determining diversity of result set could be types of sellers, auctions vs. fixed price items, shipping options, payment options and also factors like what product types or categories the surfaced inventory belongs to (For e.g., if a user issues a vague or ambiguous query like  X  X eru X  the items in the corresponding result set could belong to different categories like  X  X tamps X ,  X  X oins and Paper Money X ,  X  X ostcards X ,  X  X rt X  and many others). If relevance is not impacted, then providing a wider selection to buyers is good in general. We calculate diversity of a result set based on 3 different factors; the diversity of sellers whose items are present in the set, the diversity of the items based on format of the item and the diversity measured in terms of the amount of people whose interests would be satisfied by the result set. We calculate the seller diversity using this Simpson X  X  index metric. d , where d S is the seller diversity, N is the total number of items in the result set, and n is the total number of items in the result set from one particular seller. Format diversity d F is calculated in a similar way. For our work, we have considered two formats: fixed price items and auction items. We use data from a leading eCommerce site which has individual items for sale, and where many of the long-tail items are not necessarily grouped by product ids, or UPC/EAN/ISBN numbers. As most users don X  X  look beyond the first couple of pages of the result, the goal is to be able to show all kinds of desirable items for sale on the first page of results. Although two individuals might issue the same query, their intents could be different. We look at past history of user activity to associate desirable items with a given query. As described in [3] we mine the user click-through, buying and bidding behavior to find out a mapping from queries to features. For example, if users in the past issued a search for  X  X oger federer X  and then ended up buying  X  X -shirts X  ,  X  X ollectibles X , and  X  X ackets X , then we will have a mapping from the query  X  X oger federer X  to the features ("t-shirts", "collectibles X ,  X  X ackets") with some weights ( ) 321 This query to feature mapping is based on a desirability metric. So if lot of  X  X oger federer posters X  were available on sale but none of them got bought on site after querying for  X  X oger federer X , then the vector for the query  X  X oger federer X  would have the feature  X  X oster X  with a negative weight. Given a result set R for a query Q , we look at the feature vector V for query Q and try to see how many of the positive features for Q are found in R , which gives a measure of desired item diversity The feature set of R is the collection of terms found in the titles of any of the items found in R . So if for the query  X  X oger federer X  t-shirts, collectibles and rackets were found in the result set, the item diversity score d I would be 1.0, else it would be some number between 0 and 1.0 depending on which features for the query  X  X oger federer X  were found in the result set R and the weights for those features. We combine these individual diversities to get a composite selection score, ( ) ddd IFSS ,,  X  = . Our approach to learning the function  X  is similar to the approach used in computing Maximal Marginal Relevance (MMR) [4]. We use a linear combination function to express total selection score S as a linear combination of individual diversities d S , d F and d I . We learn the weights  X  and  X  through human judgments and use a function  X  which linearly combines d S , d F and proportions 0.2, 0.4 and 0.4 respectively. As a simple measure of relevance, we see how every individual item in the set is relevant to the query. The total relevance of the item set for the query is then an average of the relevance of individual items for the query. The relevance of an item I for a query Q is again calculated using the feature vector V for the query Q which was described in section 2.3. The vector V for query Q is comprised of different features with positive and negative weights. An item I are the terms in the title of the item. The relevance value of item 
I for query Q is, term i t in the feature vector V for query Q . If a term present in the feature vector for query Q then 0 = i w . Thus 11  X   X   X  i r . The average of the relevance values the items in the result set gives the relevance  X  ( 11  X   X   X   X  ) for the result set. Most eCommerce sites provide an option for users to sort the retrieved item set using deterministic sort factors like  X  X rice: Low to High X  or  X  X est-selling X  or  X  X ime: Newly Listed X  or  X  X istance: nearest first X  etc. If the search engine is Boolean and ranks the items based on some factors like price of item or auction ending time, there is a higher probability that irrelevant items will show up in the result set. This is true in case of queries like  X  X pod nano X  where the intent of most buyers is to buy an mp3 player but a Boolean search with TimeRank sort might show only  X  X kins X ,  X  X hargers X  and  X  X atteries X  depending on the available inventory for each of these kinds of items. As discussed in sections 2.3 and 2.4, we use historical click-through and user activity information to map a query to features, and this information is used to calculate selection ( S ) and relevance (  X  ). Thus, it becomes important that in construction of the ideal result set, room be left for a perturbation factor P , so that new items (not indicated in the query to feature mapping), also have some non-zero probability of making it to the ideal result set. Some queries to feature mappings remain stable over time while others do not. For example, we extracted some historical data to see how the top feature related to the query  X  X oster X  changes with time. It changes rapidly and dynamically as new movies or celebrities get popular. Similar changes in features for the query  X  X arry potter X  were observed as new books and movies were released. Identical effects are observed when newer models of electronic products get released as well. Our experiments consisted of running queries against the search engine, obtaining the item set from the first page of search results, and calculating various measures for those item sets. For our experiments we used 3 different sets of queries. The first set consists of 50 most frequent queries on the site which cover close to 2% of total volume of traffic because of the power-law nature of search traffic and we refer to it as TopQueriesSet . We also looked at top 1000 queries and randomly picked 100 queries out of those 1000. This forms our second set which we refer to as the RandomQueriesSet . We mined user sessions to find the sessions in which users issue a query and then exit the site i.e. they don X  X  do any other viewing, bidding or buying activity after issuing the search. We collected such queries and these queries comprise our third set which we refer to as ExitQueriesSet . For our experiments we have used two different ranking options. The first option is ranking by time ( TimeRank) , where the items that are about to expire show up first. The second option is ranking by RelevanceMatch ( RelevanceMatchRank ) which is provided by the site to sort items by some multi-dimensional relevance function. We calculated the trust, value, selection and relevance scores for item sets returned by queries from TopQueriesSet and RandomQueriesSet . We performed the experiments using both TimeRank and RelevanceMatchRank and analyzed various dimensions. The trust scores are better for TopQueriesSet as compared to RandomQueriesSet irrespective of the ranking used. This shows that popular and in-demand items (head inventory) are usually available to be bought from reputable sellers. However, the not so popular items in the long tail may not always be available from reputed sellers. We also observed that with either set, the values of  X  (trust) and  X  (relevance) are higher with RelevanceMatchRank as compared to TimeRank as the conditional ranking algorithm in RelevanceMatchRank uses some of the features that we use to measure  X  and  X  of the retrieved item set. For TimeRank, values of  X  are low for many queries irrespective of the query set being TopQueriesSet or RandomQueriesSet. Selection S is not dependent on the query set ( TopQueriesSet or RandomQueriesSet ), neither is it dependent on the ranking. S is different for different queries (varied from 0.35 to 0.78) and it is observed that S is a function of how vague, ambiguous or generalized the query is.  X  is high and stable irrespective of the ranking algorithm applied or the query set used, implying that generally the result item sets do offer a high value proposition. In order to understand and observe the importance of diverse result sets, we experiment to observe the correlation between the diversity of the result set and the exit rate of users from site. There are many users who search for items and then exit without performing any other on-site activity. These search terms are available in ExitQueriesSet . We see that some users exit because no items are returned for their query or few items are returned for their query and those items don X  X  pique their interest. However, there is another section of users, who issue queries, and who also see a full page of item results, but they still quit the site, without even clicking on the items. This could happen if the items were irrelevant, so relevance does impact exit rates. However, there are user sessions where exits happened after search for which items were shown with a reasonably high value of the relevance factor. We try to observe if this has any correlation with diversity. An item set with reduced diversity although relevant, may not be able to account for the interests of the overall user population. Some queries for which S was low but  X  was high and led to users exiting from site are shown below in Table 1. 
Table 1 Values of S for some queries from ExitQueriesSet which had a high value of  X  . Note that only items that are available for sale, tagged as desirable based on historical query mappings but still were not found in the result set are It can be seen that many queries in Table 1 are generic. It is difficult to know which game board a user is looking for when a user searches for  X  X ame board X . However, for users who are specifically looking for something but using generic queries; not finding what they want is frustrating and those users might just walk away. As seen from column 2 of Table 1, items shown for these queries were relevant, but as there were lot of items that could be reasonably relevant (based on historical click-through data) to these vague queries, probably items that the user was looking for were missing from the result set and the user just exited the site after the search. As the queries get more specific usually the desirable diversity found in result set also goes up because more specific the query, less the number of different desired items matching the query and higher the chance that most of those themes get captured by the item set. For example, the query  X  X ame board X  has a low S of 0.40 but the query  X  X onopoly game board X  has a selection score S of 0.65. In order to see if diversity S and relevance  X  are correlated we calculated those scores for different queries. A high value of selection S did not necessarily mean that the result set had a good relevance  X  and vice versa. The queries with high values of S as well as  X  lead to a good buyer experience. The queries with low values of both S and  X  lead to a poor buyer experience. However, both values would be low with a RelevanceMatchRank sort only if appropriate inventory is not available on the site, and is more of an inventory availability problem than a search or ranking problem. Queries leading to result sets with high S and low  X  usually have good variety of items but are missing variety within popular items. To please most users with optimal satisfaction a good balance of S and  X  is needed. Queries leading to result sets with high  X  and low S are of interest to our study. As described in section 3 , such result sets cause some users to walk away as they don X  X  see any items matching their interest. The average selection score for queries that led to exit is 0.61. As a comparison the average selection score of top queries was 0.72. Table 2 shows values of S for some popular ipod related queries leading to high user satisfaction. The higher satisfaction was measured in terms of number of sales in user sessions in which those queries were issued. It can be seen that queries that lead to higher satisfaction have higher selection ( S ) scores for corresponding result sets. 
Table 2 Table indicating Selection ( S ) values for popular experience. The S values for these queries are much higher Query ( S ) Query ( S ) In order to precisely measure and quantify the impact of diversity on satisfaction of users, we looked at a large sample of click-stream logs (More than 1M queries conducted by users). We looked at all search impressions, for which the relevance  X  of item set shown was above a threshold 4.0 = r t empirically. We term these impressions as data points with high =  X  . We looked at all impressions which led to user satisfaction and user dissatisfaction. User satisfaction was defined as a click on one of the items shown to the user. User dissatisfaction was defined as exit from the site. For all these search impressions we also measured the selection score S . Selection score 3.0  X  S was referred to as lowS = and selection score 3.0 &gt; S was referred to as highS = . We observed that  X   X   X   X  significant based on the large number of samples from the data we used. This shows that on pages with relevant item sets, the probability of a user being dissatisfied is significantly increased if the diversity of the item sets on the page is lowered. Thus, it is essential to guarantee diversity and proper selection to be able to satisfy most users from the population. None of these factors are highly correlated and in order to satisfy majority of buyer population the result set needs to be optimized based on  X  (Trust),  X  (Value Proposition), S (Selection / Diversity),  X  (Relevance) and P (Perturbation). We described factors that need to be optimized in eCommerce search in order to minimize the risk of dissatisfaction of the average user. We conducted a large-scale empirical study to find out the interplay between these factors  X  trust, value, selection, relevance and perturbation. We also showed through log analysis that lack of diversity is correlated with metrics like exits of users from site and that a lack of diversity could have a detrimental impact on an online marketplace. An advanced search interface like the one we describe in [5] gives more control to the user but also requires more inputs from the user, due to which we believe its utility is limited to experts. As part of future work we would like to build a ranking function that can optimize search results based on the above mentioned factors to be able to help the non-expert average users. [1] Vee, E., Srivastava, U., Shanmugasundaram, J., Bhat, P., Yahia, S.A. [2] Agarwal, R., Gollapudi S., Halverson A., Ieong S. Diversifying Search [3] Parikh N., Sundaresan N. Inferring Semantic Query Relations from [4] Carbonell J., Goldstein J. The use of MMR, Diversity-Based Re-[5] Parikh N., Sundaresan N. A User-Tunable Approach to Marketplace 
