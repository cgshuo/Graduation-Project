 Transforming vast amounts of collected  X  possibly noisy  X  data into useful information, through such processes as clustering and classification, is a really interesting research topic. The machine learning and data mining communities have extensively studied the behavior of classifiers  X  which is the focus of this work  X  in different settings (e.g., [13,20,8,9]), however the effect of noise on the classification task is still an interesting and open problem. The importance of studying noisy data settings is augmented by the fact that noise is very common in a variety of large scale data sources , such as sensor networks and the Web. Thus, there rises a need for a unified framework studying the behavior of learning algorithms in the presence of noise, regardless of the specifics of each algorithm.
In this work, we study the effect of training set mislabeling noise 1 on a clas-sification task. This type of noise is common in cases of concept drift ,wherea target concept shifts over time, rendering previous training instances obsolete. Essentially, in the case of concept drift, feature noise causes the labels of previ-ous training instances to be obsolete and, thus, equivalent to mislabeling noise. Drifting concepts appear in a variety of settings in the real world, such as the state of a free market or the traits of t he most viewed movie. Giannakopou-los and Palpanas [10] have shown that the performance 2 of a classifier in the presence of noise can be effectively approximated by a sigmoid function, which relates the signal-to-noise ratio in the training set to the expected performance of the classifier. We term this approach the  X  X igmoid Rule X .

In our work, we examine how much added benefit we can get out of the sigmoid rule model, by studying and analyzing the parameters of the sigmoid in order to detect the influence of each parameter on the learner X  X  behavior. Based on the most prominent parameters, we define the dimensions characterizing the algorithm behavior, which can be used to construct criteria for the comparison of different learning algorithms. We term this set of dimensions the  X  X igmoid Rule X  Framework (SRF) . We also study, using SRF, how dataset attributes (i.e., the number of classes, features and instances and the fractal dimensionality [6]) correlate to the expected performance of classifiers in varying noise settings.
In summary, we make the following contributions. We define a set of intu-itive criteria based on the SRF that can be used to compare the behavior of learning algorithms in the presence of noise. This set of criteria provides both quantitative and qualitative support for learner selection in different settings. We demonstrate that there exists a connection between the SRF dimensions and the characteristics of the underlying dataset, using both a correlation study and re-gression modeling. In both cases we discovered statistically significant relations between SRF dimensions and dataset characteristics. Our results are based on an extensive experimental evaluation, using 10 synthetic and 14 real datasets originating from diverse domains. The heterogeneity of the dataset collection validates the general applicability of the SRF. Given the variety of existing learning algorithms, researchers are often inter-ested in obtaining the best algorithm for their particular tasks. This algorithm-selection is considered part of the meta-learning domain [11]. According to the No-Free-Lunch theorems (NFL) described in [22] and proven in [23], [21], there is no overall best classification algorit hm. Nevertheless, NFL theorems, which compare the learning algorithms over diverse datasets, do not limit us when we focus on a particular dataset. As mentioned in [1], the results of NFL theorems hint at comparing different classification algorithms on the basis of dataset char-acteristics. Concerning the measures of performance that help distinguish among learners, in [1] the authors compared algorithms on a large number of datasets (100), using measures of performance that take into consideration the distribu-tion of the classes within the dataset, thus using the characteristics of datasets. The Area Under the receiver operating Curve (AUC) is another measure used to assess machine learning algorithms and to divide them into groups of classi-fiers which have statistically significant difference in performance [2]. In all the above studies, the analysis of performance has been applied on datasets without noise, while we study the behavior of classification algorithms in noisy settings. Our present study is based on the work of G. Giannakopoulos and T. Palpanas [10] on concept drift, which illustrated that a sigmoid function can efficiently describe performance in the presence of va rying levels of training set mislabeling noise. In this work, we analytically study the sigmoid function to determine a set of parameters that can be used to supp ort learner selection in different noisy classification settings.

The behavior of machine learning classifiers in the presence of noise was also considered in [14]. The artificial datas ets used for classific ation were created on the basis of predefined linear and nonlinear regression models, and noise was injected in the features, instead of the cl ass labels as in our case. Noisy models of non-markovian processes using reinforcement learning algorithms and Temporal Difference methods are analyzed in [18]. In [4], the authors examine multiple-instance induction of rules for different noise models. There are also theoretical studies on regression algorithms for noisy data [19] and works on denoising, like [17], where a wavelet-based noise removal technique was shown to increase the efficiency of four considered machine learn ers. In both noise-re lated studies [19], [17] attribute noise was considered. Ho wever, we study class-related noise and do not consider specific noise models, which is a different problem. Class-related noise is mostly related to concept drift, as was also discussed in the introduction. In an early influential work, the problem of concept attainment in the presence of noise was indicated and studied in the STAGGER system [16]. To the best of our knowledge, there has been no work related to the selection of a classifier in a concept drift setting, based on the level of noise and other qualitative criteria, which will be reported below. In order to describe the performance of a classifier, the  X  X igmoid rule X  of [10] considers a function which relates signal-to-noise ratio of the training set to the expected performance. This function is called the characteristic transfer function (CTF) of a learning algorithm. In this work we will call it also the sigmoid function of an algorithm. The function is of the form where m  X  M ; b, c &gt; 0; Z = log (1+ S )  X  log (1+ N ); S is the amount of  X  X ignal X  or true data, while N is the amount of  X  X oisy X  or distorted data; hence, Z is the signal-to-noise ratio. As was shown in [10] the sigmoid function effectively approximates the performance of a classifier in noisy settings.

The behavior of different machine learning algorithms in the presence of noise can be compared on several axes of comparison , based on the sigmoid function parameters. Related to performance we can use (a) the minimal performance m ; (b) the maximal performance M ; (c) the width of the performance range r performance varies. Related to the sensitivity of performance to the change of the signal-to-noise ratio we want to know (a) within which range of noise levels there is a significant change in performance when changing the noise; (b) how we can tell apart algorithms that impro ve their performance even when the signal-to-noise levels are low over those which only improve in high ranges of signal-to-noise ratio; (c) how we can measure the stability of performance of an algorithm against varying noise; (d) at what noise level an algorithm reaches its average performance. To address the se requirements we perform an analytic study of the sigmoid CTF of an algorithm. This analysis helps devise measurable dimensions that can answer our questions.

The domain of the sigmoid is in the general case Z  X  (  X  X  X  , +  X  ). The range of values is ( m, M ). Based on the first three deriv atives, we determine the point Z inf = d + 1 c log b ,whichis the point of inflection (curvature sign change point). In the case of the sigmoid function, this point is also the centre of symmetry. Furthermore, Z inf indicates the shift of the sigmoid with respect to the origin of the axes. The zeros of the third order derivative are Z (3) 1 , 2 = d  X  1 c log 2  X  which can be used to estimate the slope of the sigmoid curve. Figure 1 illustrates the sigmoid curve and its points of interest.
In the following section, we formulate and discuss dimensions that describe the behavior of algorithms, based on our axes of comparison.
 3.1 Sigmoid Rule Framework (SRF) Dimensions We define several SRF dimensions based on the sigmoid properties, in addition to m, M, r alg defined in Section 3. We define as active noise range a range [ Z  X  ,Z  X  ] where the change of noise induces a measurable change in the performance. To calculate [ Z  X  ,Z  X  ], let us assume that there is a g ood-enough performance for a given task, approaching M for a given algorithm. We know that f ( Z )  X  ( m, M ) and we say that the performance is good enough if f ( Z )= M  X  ( M  X  m )  X  p, p =0 . 05 3 . We define the size of the signal-to-noise interval in which f ( Z )  X  [ m +( M  X  m )  X  p, M  X  ( M  X  m )  X  p ]tobethe learning improvement of the algorithm. Then, using the inverse function f  X  1 ( y ) we calculate the points Z  X  (corr. Z  X  ) which is the bottom (corr. top) point in Figure 1 for a given p .We term the distance d alg = Z  X   X  Z  X  as the width of the active area of the machine learning classifier (see Figure 1). Then, r alg d improvement over signal-to-noise ratio change; we term this measure the slope indicator , as it is indicative of the slope of the CTF.

In the following paragraphs we describe how the analysis of the CTF allows to compare learning algorithm perf ormance in the presence of noise. 3.2 Comparing Algorithms Given the performance dimensions described above, we can compare algorithms as follows. For performance we can use: m, M, r alg . Algorithms not affected by the presence or absence of noise will have a minimal r alg value. In a setting with a randomly changing level of noise this parameter is related to the possible variance in performance. Related to the sensitivity of performance to the change of the signal-to-noise ratio we can use: (a) the active noise range [ Z  X  ,Z  X  ]. The width of the active area of the algorithm d alg = Z  X   X  Z  X  , which is related to the speed of changing performance for a given r alg in the domain of noise. A high d alg value indicates that an algorithm varies its performance in a broad range of signal-to-noise ratios, implying less stability of performance in an environment with heavily varying degrees of noise. We say that the algorithm operates when the level of noise in the data is within the active noise range of the algorithm; (b) the lower bound Z  X  of the active noise range, which suggests which algo-rithm operates earlier in noisy environment and which can reach its maximal performance fast; (c) the point of inflection Z inf , that shows the signal-to-noise ratio for which an algorithm gives the average performance. Z inf can be used to choose the algorithm that reaches its average performance under more noise.
A parameter related to both performance and sensitivity is the slope indica-expected to have a significant impact on the performance. An algorithm with a more, using the same dimension one can choose more stable algorithms, when the variance of noise is known. In this case, one may choose the algorithm with the Based on the above discussion, we consider the algorithms with higher maximal performance M , larger width of performance range r alg , higher slope indicator we expect to get high performance from an algorithm if the level of noise in the dataset is very low, and low performance if the level of noise in the dataset is very high. Decision makers can easily formulate different criteria, based on the proposed dimensions and particular settings. In the following paragraphs, we describe the experimental setup, the datasets and the results of our experiments.

In our study, we used the following machine learning algorithms, implemented in Weka 3.6.3 [12]: (a) IBk  X  K-nearest neighbor classifier; (b) Naive Bayes clas-sifier; (c) SMO  X  support vector classifier ( cf. [15]); (d) NbTree  X  a decision tree with naive Bayes classifiers at the leaves; (e) JRip  X  a RIPPER [5] rule learner implementation. We have chosen representative algorithms from different fami-lies of classification approaches, covering very popular classification schemes [24].
We used a total of 24 datasets for our experiments. 4 Fourteen of them are real, and ten are synthetic. All the datasets w ere divided into groups according to the number of classes, attributes (features) and instances in the dataset as is shown on Figure 2. There are 12 possible groups that include all combinations of the parameters. Two datasets from each group were employed for the experiments.
We created artificial datasets in the cases were real datasets with a certain set of characteristics were not available. We produced datasets with known in-trinsic dimensionality. The distribution of dataset characteristics is illustrated in Figure 3. The traits of the datasets illustrated are the number of classes, the number of attributes, the number of instances and the estimated intrinsic (fractal) dimension.
 The ten artificial datasets we used were built using the following procedure. Having randomly sampled the number of classes, features and instances, we sample the parameters of each feature distribution. We assume that the fea-tures follow the Gaussian distribution with mean value (  X  ) from the interval [  X  100 , 100] and standard deviation(  X  ) from the interval [0 . 1 , 30]. The  X  and  X  intervals allow overlapping features across classes.

Noise was induced as follows. We created stratified training sets, equally sized to the stratified test sets. To induce noise, we created noisy versions of the training sets by mislabeling instances. Using different levels l n of noise, l n = that a training instance will be assigned a different label than their true one. Hence, we obtained 20 dataset ver sions with varying noise levels. 4.1 Using SRF We performed experiments of  X  X oisy X  classification using the generated datasets, performing 10-fold cross validation per algorithm, and calculated the average performance for varying noise levels. Given the 20 levels of signal-to-noise ratio and the corresponding algorithm performance, (i.e., classification accuracy) we estimated the parameters of the sigmoid. The search in the parameter space is performed by a genetic algorithm, estimating an approximate good set of parameters as was proposed in [10]. The quality of estimation is checked using the Kolmogorov-Smirnov test. The results obtained are statistically significant.
A sample of true and sigmoid-estimated performance graphs for varying levels of noise can be seen in Figure 4. In our experiments, the parameters of the sigmoid were estimated offline, but SRF can be applied in an online scenario, as well, using a training period.

Figure 5 illustrates the means of the SRF parameters per algorithm, over all 24 datasets. As an example of interpretation of the figure using SRF, the plots indicate that (for the studied range of datasets) SMO is expected to improve its performance faster than all other algorithms, when the signal-to-noise ratio increases. This conclusion is based on the slope indicator ( r alg d IBk has a smaller potential for improvement of performance (but also smaller potential for loss) than SMO when noise levels change, given that the width of the performance range r alg is higher for SMO. This difference can also be seen in Figure 4, where the distance betw een minimum and maximum performance values is bigger for the SMO case (see Figure 4(left)).
We stress that parameter estimation does not require previous knowledge of the noise levels, but it is dataset dependent. In the special case of a classifier selection process, having an estimate of the noise level in the dataset helps to reach a decision through the use of SRF. 4.2 Statistical Analysis We now study the connection between the d ataset characteristics and the sig-moid parameters (using the same 24 datasets), irrespective of the choice of the algorithm. We consider the results obtained from all the algorithms as different samples of SRF parameters for a particular dataset. We use regression analy-sis to observe the cumulative effect of the dataset characteristics on a single parameter, and we use correlation anal ysis to detect any connection between each (dataset characteristic , sigmoid parameter) pair. We examine the connec-tions between dataset chara cteristics and the sigmoid parameters both individ-ually, and all together, in order to draw the complete picture.
 Regression Analysis. We wanted to examine how the number of classes ( x 1 ), number of features ( x 2 ), number of instances ( x 3 ), and intrinsic dimensional-ity 6 (as fractal correlation dimension [3]) ( x 4 ) of a dataset influence the CTF parameters.

We applied a leave-one-out process, wh ere one dataset is left out from train-ing and used for testing on every run. We used in turn m , M , r alg , d alg ,and dimensions are reported in Table 1, where average errors between observed and predicted SRF dimensions are shown. For each SRF dimensions chosen, we have observed 5 values (since 5 machine learning algorithms were used), and having estimated them for 24 datasets, we end up with 120 predictions for a single SRF dimension. We calculated four types of errors: (1) MSE  X  mean square error; (2) MAE  X  mean absolute error; (3) RMSE  X  relative mean square error and (4) RMAE  X  relative mean absolute error. The last column of Table 1 shows the average of the adjusted R 2 statistic for models that where estimated for all the SRF dimensions (average on the 24 datasets). Figure 6 illustrates how our models fit the test data, showing that in most cases the true values of the sigmoid parameters for each dataset (illustrated by circles that correspond to 5 algorithms for each test dataset i , i =1 , 2 , ..., 24) are within the 95% confi-dence level zone around the estimated values. This finding further supports the connection between the dataset parame ters and SRF dimensions. According to the results, the chosen parameters of the datasets can be used to predict the parameters of the sigmoid of the algorithms.
 Correlation Analysis. We used three different correlation coefficients  X  Pear-son correlation for linear correlation, Spearman X  X  rho and Kendall X  X  tau for monotonic correlation  X  to analyze the c onnection between the parameters of the datasets and the CTF parameters (cf. Table 2). We qualitatively interpret the Correlation, [0 . 3; 0 . 5)  X  Medium Correlation, [0 . 5; 1]  X  Strong Correlation.
Summarizing the results from all the corre lation coefficients (refer to Table 2), some interesting conclusions can be drawn. First, the number of classes ( x 1 )isin-versely correlated to r alg d the lower the sensitivity to noise variation (check on r alg d of classes, the higher the impact of reducing noise on performance (check r alg and M ). These conclusions are also supporte d by the direct correlation between the number of classes and the width of the active area of the algorithm d alg . We also note the complete lack of significant correlation between the minimum perfor-mance m and all of the SRF dimensions: given enough noise an algorithm always performs badly. Thus, the number of classes significantly influences the behavior of an algorithm, regardless of the family of the algorithm. Second, the number of features ( x 2 ) provides a minor reduction of sensitivity to noise variation (re-sulting from low correlation to d alg ). This conclusion is also supported by the negative influence on r alg d the maximal performance M , which shows (rather contrary to intuition) that more features may negatively affect perfor mance in a noise-free scenario. This is most probably related to features that are not essentially related to the labeling process, thus inducing feature noise. Third, there is a correlation between the number of instances ( x 3 )and r alg d more instances) reduce sensitivity to noise variation. Last, fractal dimensional-ity ( x 4 ) of a dataset has low, but statistically significant negative influence on M and on r alg . Fractal dimensionality is indicative of the  X  X omplexity X  of the dataset. Thus, if the dataset is complex (high x 4 ) machine learning is difficult even at low noise levels. We note that low r alg may be preferable in cases where the algorithm should be stable even for low signal-to-noise ratios.

The correlation analysis demonstrates the connection between dataset char-acteristics and SRF dimensions. Consequently, the SRF can be used to reveal a-priori the properties of an algorithm with respect to a dataset of certain char-acteristics. This allows an expert to select a good algorithm for a given setting, based on the requirements of that settings. Such requirements may, e.g., relate to the stability of an algorithm in varying levels of noise and the expected maximum performance in non-noisy datasets. Machine learning algorithms are often used in noisy environments. Therefore, it is important to know a-priori the properties of an algorithm with respect to a dataset of certain characteristics. In this work, we investigate whether some simple dataset properties (namely, number of classes, number of features, number of instances and fractal dimensiona lity) can help in the above direction.
We propose the  X  X igmoid Rule X  Framework, which describes a set of dimen-sions that may be used by a decision maker to choose a good classifier, or to estimate SRF dimensions, based on a range of dataset characteristics. Our ap-proach is applicable to user modeling tasks, when the user changes behavior over time, and to any concept drift problems for data series mining. We showed that the parameters related to the behav ior of learners correlate with dataset characteristics, and the range of their variation may be predicted using regres-sion models. Therefore, SRF is a useful meta-learning framework, applicable to a wide range of settings that include noise. However, using these SRF mod-els for parameter prediction does not provide enough precision to be used for performance estimation.

As part of our ongoing work, we examine whether the  X  X igmoid Rule X  also stands in the case of sequential classification. Preliminary experimental results on the  X  X limate X  UCI dataset (taking into account its temporal aspect) indicate that, indeed, the  X  X igmoid Rule X  and therefore SRF are directly applicable, and can be used as a means to represent the behavior of an HMM-based classifier in the presence of noise. This finding may open the way to a broader use of the SRF, including sequential learners.
 Acknowledgements. This research was partially supported by FP7 EU IP project KAP (grant agreement no. 260111).

