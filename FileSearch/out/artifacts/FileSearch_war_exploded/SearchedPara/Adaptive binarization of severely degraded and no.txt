 ORIGINAL PAPER Brij Mohan Singh  X  Rahul Sharma  X  Debashis Ghosh  X  Ankush Mittal Abstract This paper presents a new adaptive binariza-tion method for the degraded document images. Variable background, non-uniform illumination, and blur caused by humidity are the addressed degradations. The proposed method has four steps: contrast analysis, which calculates the local contrast threshold; contrast stretching, threshold-ing by computing global threshold; and noise removal to improve the quality of binarized image. Evaluation of pro-posed method has been done using optical character recog-nition, visual criteria, and established measures: execution time, F-measure, peak signal-to-noise ratio, negative rate metric, and information to noise difference. Our method is tested on the four types of datasets including Docu-ment Image Binarization Contest (DIBCO) series datasets (DIBCO 2009, H-DIBCO 2010, and DIBCO 2011), which include a variety of degraded document images. On the basis of evaluation measures, the results of proposed method are promising and achieved good performance after extensive testing with eight techniques referred in the literature. Keywords OCR  X  Binarization  X  Degraded documents  X  Evaluation measures 1 Introduction Document image processing is a subject of research since more than three decades due to increased demand for archiv-ing and preserving documents in large quantities worldwide indigitalformat.Extractionoftextfromimagesanditsrecog-nition may be challenging due to the presence of noise and degradation in document images. Variable background, shad-ows, non-uniform illumination, ink bleed-through, and blur are few types of degradation, which appear frequently and may occur due to several reasons that range from the cap-turing source type to environmental conditions [ 1 , 2 ]. There-fore, it is important to preprocess or remove degradation from the document images, so that all subsequent stages such as segmentation, feature extraction and selection classification, and consequently the accuracy rate of the optical character recognition (OCR) systems are not affected [ 3 ].
Document image binarization is one important pre-processing step that is used in OCR systems. This converts gray-level images into binary images, thereby facilitating the separation of textual characters from the background in the document image. Many document image binarization meth-ods have been proposed in the literature [ 4 , 5 ]. However, selecting the most optimum method for binarization is a dif-ficult task due to the presence of variety of degradations in document images.

According to Sezgin et al. [ 4 ] and Chen et al. [ 5 ], bina-rization methods are classified into six categories: histogram-based,clustering-based,entropy-based,foregroundattribute-based, spatial binarization-based, and locally adaptive meth-ods. Further, Wen et al. [ 6 ] divided these methods into three major categories: clustering-based, threshold-based, and hybrid methods. The threshold-based methods include two sub-categories: global and local. Global binarization meth-ods [ 7  X  10 ] determine a single threshold value for the whole image, while local or adaptive binarization methods [ 2 , 11  X  16 ] determine threshold for every individual pixel using local information derived from the candidate pixel and is needed for the calculation of threshold value for each pixel. The variety of binarization methods indicate that there is a lacuna of an efficient and accurate framework for OCR, which can handle noisy, degraded, or poor-quality document images.
This paper is organized in following sections: Sect. 2 describes the review of well-known and recent binarization techniques. Section 3 describes the proposed binarization method. Section 4 presents the analysis of results. Section 5 gives the conclusion of results. 2 Review of the binarization methods The purpose of document image binarization is to produce an image with black (or white) text on white (or black) back-ground. Ideally, when texts are printed or written using same color ink on uniform background, pixels in the gray image will belong to only two distinct gray levels X  X ne gray level corresponding to the background and the other correspond-ing to the text pixels [ 2 , 17  X  19 ].

As discussed above, threshold-based techniques can be global and local. Global binarization is not useful in the cases where a document image contains degradations such as variable illumination and contrast. Some referred stud-ies on global binarization methods are due to Abutaleb [ 7 ], Kapur et al. [ 8 ], Kittler and Illingworth [ 9 ], and Otsu [ 10 ]. Otsu [ 10 ] approach to global binarization is the most success-ful approach for OCR systems because of its computational efficiency and effectiveness [ 4 , 20 , 21 ]. Otsu [ 10 ] proposed a global thresholding method based on statistical methods. Any of the existing global method cannot solve the problem of non-uniform illumination as the threshold value is same for whole of the document but illumination is not.
Local binarization techniques perform better in the pres-ence of non-uniform illumination and degradation in docu-ment images. A number of approaches to local thresholding are available in the literature [ 2 , 6 , 11  X  16 ].
Bernsen et al. [ 11 ] proposed a local thresholding method based on mean and contrast information for the calculation of threshold over a local region. First, it calculates the values for Z high and Z low , where Z high and Z low are the highest and lowest gray levels in the r  X  r region, around the pixel ( x The threshold is calculated using formula: T ( x , y ) = Z high It also measures local contrast in the local region as: C ( x , y ) = Z If local contrast is less than a predefined value X  t  X , then region represents a single class, either foreground or background. Large text areas may occur in the image document, in that case pixel is taken as background. Bernsen et al. used win-dow size of 15 and t = 15. According to Trier and Taxt [ 20 ] evaluation, Bernsen [ 11 ] approach almost completely recovers text but produces large background noise especially where the document regions do not contain text.

Niblack [ 12 ] proposed a method that calculates a pixel-wise threshold by shifting a rectangular window across the image. The threshold T for the center pixel of the window is computed using the mean m and the variance s of the gray values in the window T ( x , y ) = m ( x , y ) + k  X  s ( x , y ) (3) The value of  X  k  X  controls the amount of text region inside the local window. To conserve local details and handle local illumination level, one requires small window size but if we choose too small window size, then it will not cover object and eliminate noise in the gray image. Window size of 15  X  15 and k = X  0 . 2 is recommended by Trier and Jain [ 21 ]. According to Gatos et al. [ 2 ], Niblack X  X  [ 12 ] method fails when the background contains light texture.

Sauvola et al. [ 13 ] proposed a similar algorithm by making some assumptions based on the distribution of gray values associated with foreground and background pixels. Thresh-old  X  T  X  is computed as: T ( x , y ) = m ( x , y )  X  1 + k  X  where m ( x , y ) and s ( x , y ) are the same as in Niblack X  X  method.  X  R  X  is the dynamic range of standard deviation. Values of R = 128 and k = 0 . 5 are used. Sauvola X  X  [ 13 ] approach shows robust results in the processing of severely degraded document images. It can handle variable illumina-tion, noise, and resolution variation in the document image but fails in very light and very dark background [ 2 ].
Kim et al. [ 14 ] proposed a local adaptive approach based on water flow model, in which image surface is considered as three-dimensional (3D) terrain. In this model, water is poured onto the terrain surface and it fills valleys by flowing down to the lower regions of the terrain.  X  w  X  is the amount of water sprinkled on the terrain, and a mask of ( 2 s + 1 )  X  ( 2 s is used to simulate the flow of water drop. Values of w = and s = 3 are used. Results of Kim et al. [ 14 ] approach are compared with Otsu X  X  [ 10 ], Nakagawa and Rosenfeld X  X  [ 22 ], and Niblack X  X  [ 12 ] method with post-processing technique proposed by Liu and Srihari X  X  [ 23 ]. According to this study, Kim X  X  method is better than other compared methods but in many cases such as low illumination and very high contrast, it producesbackgroundnoiseandcharactersbecomeextremely thinned and broken.

Gatos et al. [ 2 ] developed a method, using a combination of existing techniques, for degraded documents with uneven background. First, it eliminates noisy area and increase con-trast between background and foreground objects by using low-pass Wiener filter. After that, it applies Sauvola X  X  [ 13 ] method to obtain an approximation on background and fore-ground objects. In the next step, it estimates the background of the image by interpolation. After that, it thresholds the image and finally it improves the quality of binarized image by using three preprocessing steps. Gatos et al. X  X  [ 2 ] method is an adaptive local binarization approach, which removes smear and strains, variable illumination, low-contrast, large signal-dependent noise, and shadow types of degradations from the document image. On the basis of comparison of results with existing binarization approaches discussed in of low illumination.

Pai et al. [ 15 ] proposed a novel thresholding approach that combines the advantages of local and global threshold-ing methods, which is based on the intelligent block size detection, and then perform thresholding based on each block of degraded image. Comparison of results with Otsu X  X  [ 10 ], Niblack X  X  [ 12 ], Sauvola [ 13 ], and Gatos X  X  [ 2 ] methods of binarization, Pai et al. X  X  [ 15 ] method achieved better results and removed degradations from the document images. Pai et al. X  X  [ 15 ] method increases the quality of thresholding and takes very less processing time. However, from the results, it is observed that it produces background noise in many cases and its thresholding quality is approximately same as Gatos et al. X  X  approach [ 2 ].

Valizadeh et al. [ 16 ] combines two existing algorithms that were complementary to each other. First phase is the application of modified Kim et al. method. The main dif-ference between Kim X  X  [ 14 ] method and its modification is that it pours water only onto the edge pixels and uses the stroke width and double edge feature to separate out texts. Valizadeh et al. [ 16 ] combined Niblack X  X  [ 12 ] method using morphological dilation operator on the binary image pro-duced by their contrast independent binarization algorithm. This method has shown better results in comparison with Sauvola X  X  [ 13 ], Otsu X  X  [ 10 ], Kim X  X  [ 14 ], and Niblack X  X  [ 12 ] method. Valizadeh X  X  approach [ 16 ] works well in most of the degraded document images but produces some noise in the area not containing text and takes more execution time among other compared methods.

On the basis of the literature review of local and global thresholding algorithms, we conclude that none of the approaches produce satisfactory results on severely degraded images that contain distinct type of degradations such as vari-able background and shadows, non-uniform illumination and contrast, ink bleed-through, and blur. In our experiments, our proposed method described in Sect. 3 performs the best for low-contrast images and above-discussed degradations, which is not handled by the earlier methods discussed in the literature. 3 Proposed algorithm In this paper, we propose a new method for document image binarization that is based on local contrast analysis. A pixel at the character boundary will generally have high contrast with respect to its neighboring background pixels. Thus, the presence of character pixels in a window may be detected by using local contrast information, and the threshold value to be used for binarization within that window is adapted accordingly. Some related approaches of transition are also discussedin[ 24  X  29 ].Thelocalcontrastinformationmayalso be used in removing noise and joining broken characters in the binarized document image as explained below. 3.1 Local contrast analysis Let f ( x , y ) be the gray image of an input document of size M  X  N , where M is the number of lines and N is the number of pixels per line in the image. The intensity or gray level of a pixel is given as f ( x , y )  X  X  0 , n  X  1 ] , where x and y denote the vertical and horizontal coordinates of the pixel, and n is the number of gray levels in the image. We calculate the gradient at a pixel position ( x , y )as g ( x , y ) = max [ d h ( x , y ), d v ( x , y ) ] (5) differences along the horizontal and vertical directions, given by d ( x , y ) = | f ( x , y )  X  f ( x , y  X  1 ) | (6) ( x , y ) = | f ( x , y )  X  f ( x  X  1 , y ) | (7)
The gradient g ( x , y ) is generally very high at points of transition from background to foreground (character seg-ments) or vice versa. Thus, g ( x , y ) gives a measure of the local contrast between a pixel at ( x , y ) and its immediate preceding neighbors, both along the horizontal and verti-cal directions. Ideally, a document image will have only two distinct gray levels corresponding to the dark character pixels ( f black ) on light uniform background ( f white ). Con-sequently, the gradient image will also have two distinct gray levels X  X ero at points where the candidate pixel and its immediate neighbors are part of the same character seg-ment or background, and some relatively high nonzero value ( f will not be true in a document corrupted with noise due to degradation, non-uniform illumination, or having non-uniform background. Accordingly, the gray level in the gra-dient image may range from 0 to n  X  1. The normalized histogram of this gradient image is calculated as H ( i ) = P i where P i is the number of pixel points in the image that have intensity gradient g ( x , y ) = i .

Subsequently, thecumulativehistogram is calculatedfrom the normalized histogram of the gradient image as H
The maximum contrast value C max over all pixels in the input document image is determined from the cumulative histogram, which is given as the smallest value of i in the range 0 to n  X  1forwhich H cum ( i ) = 1. Also, the contrast threshold value C th of the image is determined as C th = where i is the largest value in the set 0 , 1 , 2 ,..., n such that H cum ( i )&lt; P th . A pixel in the image is clas-sified as a character boundary pixel or noise if the gradient g ( x , y ) at that pixel location is above C th . The parameter P is a user-defined constant in the range [0, 1]. Under above-discussed experiment conditions, it has been experimentally observed by us that the percentage of pixels corresponding to character boundary and noise amounts to about 5% in a typical document image. Accordingly, the value of P th is set to 0.95 in our work. Some authors also suggested ways for finding this threshold in [ 24  X  26 ]. The choice for the value of P th is critical in separating text pixels from background in a document image X  X ower value of P th results in increased false alarm with non-text pixels detected as text pixels while higher value of P th results in increased misdetection. Fig-ure 1 shows the result obtained using P th = 0 . 95. Effect of using higher value of P th is also demonstrated in this figure. We observe that P th = 0 . 99 results in elimination of many character pixels, as shown in the first line in Fig. 1 .How-ever, P th = 0 . 99 may be good if the documents contain few text lines or non-dense text on the whole document. Nev-ertheless, in our work, we assume that input test document images contain dense text and so P th = 0 . 95 is selected.
Assuming that the input document contains sufficient amount of text pixels, the C th value will be generally high (for blank documents, the contrast measure g ( x , y ) will be small throughout the document resulting in a low C th ). However, a fixed contrast threshold C th for an entire image may not be always appropriate. Image may contain non-uniformity in local contrast due to improper illumination, natural degra-dation in old documents, textured background, ink bleed-through, watermark, and so on. Consequently, the contrast threshold value varies within the image. So, we divide the image into smaller blocks and calculate blockwise local con-trast threshold L th value by repeating the above process for each block separately. We segment the image into a dimen-sion of W  X  W sub images as shown in Fig. 2 where W is a window size and W = min 32 W , After that, we round down the value of W to the nearest power of 2. We limit W to half of the size of lowest dimen-sion because sometimes document images are in improper rectangular shapes.
 In the above figure, an image of 9  X  14pixels is shown. Let the value of W is 4. It is clear that W is always set to 4 by taking pixels right to left for windows of right boundary and bottom to top for windows of bottom boundary. In this example, a total of 12 segments will be formed by 4 horizon-tal divisions and 3 vertical divisions. Fixing segment size is necessary because segment must be large enough to accom-modate contrast analysis process with sufficient information that is needed to calculate contrast threshold.

It may be observed that for image blocks having textual characters, measure of local contrast g ( x , y ) will be signif-icantly high at some locations resulting in high L th .Onthe other hand, the measure of local contrast will be generally low throughout a block that does not contain any text result-inginlow L th . Accordingly, since some blocks may contain text pixels while some blocks may not, neither the global threshold alone nor the local threshold is appropriate. So, we propose to use a modified contrast threshold, which is a combination of the global contrast threshold C th and the local contrast threshold L th within a block. This modified threshold value is calculated as L where  X  is any value between 0 and 1. In our work, we have chosen  X  as  X  = 1 which is also shown in Fig. 3 . This particular choice of ensures that when the local threshold L th is equal to the global threshold C th ,thevalueof  X  is zero and hence the contrast threshold within a window is not modified. On the other hand, as the local threshold L th differs from the global threshold C th , the threshold value is modified as weighted sum of the two threshold values L th and C th .

Consequently, blocks that do not contain text pixels will use contrast threshold very near to the global threshold C while blocks containing text will use threshold value close to the local threshold L th . Improvement of binarized output using modified local contrast threshold over local contrast threshold and global contrast threshold is shown in Fig. 4 . 3.2 Contrast stretching Following the local contrast analysis discussed above, we now divide the image into overlapping blocks and apply his-togram stretching in every block. We implemented the his-togram stretching of the windows in zig X  X ag sliding window manner as shown in Fig. 5 . However, if the difference in the lowest intensity pixel and the highest intensity pixel is less than L th , then the window contains only background pix-els and hence no histogram stretching is applied. Instead, all pixels in such a block are set to white (background). On the other hand, blocks containing texts are enhanced to obtain a high-contrast image suitable for binarization. This is accom-plished by employing contrast stretching [ 30 ] in which the gray value f ( x , y ) in the original image is modified as f ( x , y ) = ( n  X  1 )  X  f where n is the number of gray levels considered, f max and f min denote the maximum and minimum gray levels in the original document image, respectively. Since the blocks are overlapping, every pixel in the image will be contained in more than one block and will assume different values due to histogram stretching applied blockwise. Accordingly, the final modified gray value is calculated as the average of all these different values.

We choose histogram stretching over histogram equaliza-tion because it provides a better distribution in order to make light pixels even lighter and dark pixels closer to black. Thus, the transformation function stretches the gray levels from their original range in the input document to the full range [0, n  X  1] making white background pixels even whiter and character pixels closer to black. Figure 6 shows the result of histogram stretching on a 606  X  441 image with 256 gray levels.

However, portions of background that are poorly illumi-nated compared to other parts of the image will become darker due to contrast stretching resulting in dark spots on the background when thresholded. Similarly, character seg-ments present in degraded and poorly illuminated parts of the document may be removed after binarization. In view of this, contrast stretching is applied locally over each of the blocks or windows formed in contrast analysis step.
Further, histogram stretching is applied on overlapping blocks; otherwise, the output may not remove degradation instead it may deteriorate the situation. This situation is shown in Fig. 7 (b). The images shown in Fig. 7 (a) are divided into 32  X  32 non-overlapping windows and then perform con-trast stretching in each window as shown in Fig. 7 (c).
On the other hand, when overlapping blocks are consid-ered, the contrast in the transformed output image is prop-erly distributed and it is free from any degradation, as shown Fig. 7 (d).
 3.3 Thresholding In this stage, we finally select the threshold for the binariza-tion of the document image. As said before, an ideal docu-ment image will have only two distinct gray levels f black f white . Accordingly, any value in between these two gray val-uesmaybeselectedforthresholding[ 31  X  33 ].But,undernon-ideal situations, gray values corresponding to black character pixels will spread about f black even after contrast stretching. Same holds for the background pixels also.

Therefore, the first step in threshold selection involves computing the histogram of the enhanced image f ( x , y ) obtained after contrast stretching in the previous step. Thus, first we compute the histogram of the transformed image f ( x , y ) and divide it into two equal halves. This is done on the presumption that the contrast stretching step above will set character pixels mostly in the range 0 to n / 2  X  1 and background pixels in the range n / 2to n  X  1. After that, we search for the highest frequency in each half. Let f black f white are the gray levels with highest frequency in the black half and white half, respectively. In case, we have more than one highest frequency gray level for a black half, we choose theright-most(i.e.,whichhashigherindex);similarly,incase of a tie in white half, we choose the left-most (i.e., which has lower index). The threshold T is computed as T Finally, the binarized image g ( x , y ) is obtained as: g ( x , y ) = where f ( x , y )&gt; T is used for the background pixels. Thus, in the final output image, foreground character pixels are assigned value 0 and background pixels are assigned 1. 3.4 Noise removal In the final step, we improve the quality of binarized image by removing noise introduced by the process of binariza-tion such as fix broken and thin text. Background pixels that are darker than the threshold T due to several reasons like improper illumination and image degradation will create unwanted dark spots on the binarized document. In similar manner, character pixels that are lighter than the threshold T will produce white spots on the character segments and may even result in broken character segments.

Preservation of foreground objects (character segments) is very important in case of very low-resolution document images. In order to retrieve lost character pixels, we check the following two conditions for every background pixel in the binarized image g ( x , y ) :
Vertical condition: g ( x  X  1 , y ), g ( x + 1 , y ) = 0given g ( x , y ) = 1 and d v ( x , y ), d v ( x + 1 , y )  X  L th
Horizontal condition: g ( x , y  X  1 ), g ( x , y + 1 ) = 0given g ( x , y ) = 1 and d h ( x , y ), d h ( x , y + 1 )  X  L th
If any of the above two conditions is satisfied, then the pixel is set to foreground, i.e., g ( x , y ) = 0. Figure 8 shows the result of this step on the image obtained after threshold-ing. It is clear from Fig. 8 that letter  X  X  X  and  X  X  X  were fixed. 4 Experimental results In this work, we considered documents containing machine-printed and handwritten text only. Documents containing non-text regions, such as pictures and graphics, are not con-sidered. Images of the documents are obtained using scan-ning, taking snapshot, and downloaded from the Internet. Document images are first converted into gray-scale image followed by binarization.

The proposed method was tested on four sets of doc-ument images. First, three datasets are collected from the books, newspapers, library, and Internet. Fourth dataset is collected from [ 34  X  36 ]. First dataset contains five computer-generated (artificial) images for which we have calculated ground-truth. Second set contains twenty degraded docu-ment images suitable for printed English OCR. Third set contains various documents including historical, ancient, and camera-captured document images. The images of first, second, and third datasets are of various dimensions with 8-bit gray levels. Fourth set contains benchmark datasets taken from Document Image Binarization Contest (DIBCO: DIBCO 2009, H-DIBCO 2010 and DIBCO 2011) series [ 34  X  36 ], which include a variety of degraded document images. Four computer-generated images out of five of dataset 1 show the effect of one of the following degradations: variable back-ground and shadows, non-uniform illumination, and blur caused by humidity. Rest one of the five computer-generated image of dataset 1 contains all of the above-discussed degra-dations, which is shown in Fig. 9 (a).

For dataset one and four, we performed ground-truth-based evaluation measures and OCR-based evaluation for dataset two. The evaluation of dataset 3 is based on the visual perception. Proposed approach of binarization is compared with eight recent and benchmark binarization methods that are Otsu, Bernsen, Niblack, Sauvola, Kim, Gatos, Pai, and Valizadeh. The experimental results of all the methods on all used datasets are shown in Figs. 9 , 10 , 11 , 12 , 13 , 14 and 15 . Figures 13 , 14 , and 15 show some of the best results of DIBCO 2009, 2010, and 2011 standard datasets.

All algorithms are implemented in C++ on GCC 3.4.2 (mingw-special) compiler and were run on Pentium Dual Core 1.60GHz processor with 1.5GB RAM and Windows XP Operating System. 4.1 Ground-truth-based evaluation measures The five standard evaluation measures such as execution time,F-measure[ 16 ],peaksignal-to-noiseratio(PSNR)[ 16 ], negative rate metric (NRM) [ 16 ], and information to noise difference (IND) [ 16 ] were used to compare the performance of all discussed methods. A higher-quality binarized image has higher F-measure and PSNR but lower NRM. Used terms related to measures are given in Table 1 .
 Execution Time: It is a time spent by the system executing the task (including the time spent executing run-time or system services on its behalf). However, this measure X  X  results may vary from machine to machine.
 F-measure: It is the weighted harmonic mean of precision and recall. The equation of F-measure is as follows: F As the alpha value increases, the weight of recall increases in the measure. When using precision and recall, the set of possible labels for a given instance is divided into two sub-sets, one of which is considered  X  X elevant X  for the purposes of the metric. Recall is then computed as the fraction of cor-rect instances among all instances that actually belong to the relevant subset, while precision is the fraction of correct instances among those that the algorithm believes to belong to the relevant subset.
 Peak Signal-to-Noise Ratio (PSNR): PSNR is the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its rep-resentation.
 PSNR = 10  X  log where MSE = image and I is binarized, MSE is mean square error. Negative Rate Metric (NRM): It is based on pixelwise mis-matches between the ground-truth and observations in a frame.
 NRM = where NR FN and NR FP denote the number of false-negative and false-positive pixels, respectively. TN and TP are the number of true negatives and true positives. Information to Noise Difference (IND): This method is used to test the quality of the binarized image based on the infor-mation and noise.
 the number of black pixels in ground-truth and in binarized image, respectively. Here, I value signifies the information pre-served in the binarized image, and N value represents the noise in the binarized image. The value of IND ranges between  X  to +1 where +1 means binarized image is the exact copy of ground-truth while  X  1 signifies that binarized image is the invert of ground-truth.

Figure 9 showstheresultscomparisonofproposedmethod over eight binarization methods of one computer-generated image of dataset 1. Table 2 shows the evaluation results of Fig. 9 (a) of proposed method over eight binarization meth-ods. The results of proposed method over other binarization methods on remaining four artificial images of dataset 1 are shown in Table 3 .

The observations of Fig. 9 (a) on five standard evaluation measures are  X  Execution Time: Otsu X  X  method takes lowest (0.006s),
Kim X  X  method takes highest (33.672s) and proposed method takes 1.294s time to execute.  X 
F-measure: The F-measure of proposed method is the highest (87.88%) and lowest (26.03%) is of Sauvola X  X  method.  X 
PSNR: The PSNR of proposed approach is the highest (14.6db) and lowest (3.59db) of Otsu X  X  method.  X 
NRM: The NRM of proposed method is 3.59 and Otsu X  X  is 40.36.  X 
IND: The IND of proposed method is highest (0.771) and lowest (  X  0 . 181) is of Otsu X  X  method.

Theexperimentalresultsofdataset2areshowninFigs. 10 , 16 , 17 , Tables 4 , and 8 . Results of dataset 3 are shown in Figs. 11 and 12 . Figures 13 , 14 , 15 and Tables 5 , 6 , 7 show the results of DIBCO 2009, 2010, and 2011, respectively. 4.2 OCR-based evaluation Document images of dataset 2 were evaluated using two OCRs: ABBYY Fine reader [ 37 ] and Free OCR [ 38 ]. We evaluate the quality of proposed method over other discussed algorithms on OCR results by calculating Levenshtein dis-tance [ 39 ] from ground-truth document image. Table 11 shows an example of OCRs results after the application of four binarized methods, which performed best on this image. Table 3 shows the evaluation results of Fig. 10 (a) of dataset 2 on both the OCRs. Figure 16 shows the results of ABYY Fine reader on non-binarized image of dataset 2. Figure 17 shows the results of non-binarized images of dataset 2 on ABYY Fine Reader and Free OCR. 4.3 Observations Important observations of tested binarization techniques can be summarized as follows:  X 
Otsu X  X  method does not give satisfactory results on any of the test input document images as it is a global thresholding algorithm and degradations generally have local variance noise.  X 
Niblack X  X  and Bernsen X  X  method produces great amount of background noise.  X 
Sauvola X  X  method overcomes the background noise prob-lems but produces thinned and broken characters. In vari-able contrast image, it often does not capture foreground pixels in low-contrast regions.  X 
Kim X  X methodhasgoodresultinmaximuminputdocument images but suffers from variable strength of characters, i.e., somewhere it is thick or joined and somewhere it is thin or broken, in some input cases. The running time of this method is extremely large as compared to proposed.  X 
Gatos X  method uses Sauvola X  X  method as an intermedi-ate step to extract information about foreground pixels; as a result, it inherits some of the problems of Sauvola X  X  method like: not capturing foreground pixels in low-contrast regions of variable contrast image. It has thick characters causing merging of characters in low-resolution document images.  X  Pai X  X  method is based on histogram to detect blocks.
It properly handles non-uniform illuminated images but failed in very low or very high illuminated document images.  X 
Valizadeh X  X  method is quite good and works well in most of the document images but leaves some noise in area not containing text. It is also the second worst running time among methods used in this literature.  X 
Proposed method has greater performances as compared to all other methods discussed in this paper in the cases of very severely degraded document images having multiple types of degradations. Basically, it recovers text from the images having variable illumination and variable contrast.
The main application of proposed approach is in the recog-nition of mobile-captured document images under variable lighting conditions. However, the proposed method cannot work well if the document does not contain dense text, for example documents having only one or two text lines in the whole page. The reason behind producing noise in the above-stated case is due to its recovery phase because it searches text in a document image. In the cases of very faint text, dark edges of stains of ink bleed-through, and dark water drops, it produces noise and broken charac-ters. 4.4 Discussion on DIBCO series datasets (2009 X 2011) DIBCO (2009 X 2011) is the International Document Image Binarization Contest organized in the context of ICDAR con-ference. The general objective of the contest is to identify current advances in document image binarization using eval-uation performance measures [ 40  X  42 ].

In our experiments, we have also evaluated all the datasets of DIBCO 2009 (Handwritten Images), 2010 (Handwrit-ten Images), and 2011(Machine-Printed and Handwritten Images). Figures 13 , 14 , and 15 show some of the best results of DIBCO series images: DIBCO 2009 H03, DIBCO 2010 H04, and DIBCO 2011 HW6 along with the evaluation results of corresponding images that are shown in Tables 5 , 6 , and 7 of all the discussed methods. Table 8 shows the average evaluation results of all the images of DIBCO 2009, 2010, and 2011, respectively. Ranking comparisons of all methods on all DIBCO datasets are shown in Table 9 . Table 10 shows the comparisons of F-measure of proposed method of all the images of DIBCO datasets (2009 X 11). From the experi-ments shown in Tables 9 and 10 , it is observed that proposed method produces better results on handwritten images and it produced broken characters in the case of very faint text edges. It also produced little noise in the case of ink bleed-through degradation, less text in a document, and dark spots of water drops.

From the Table 10 and DIBCO dataset X  X  characteristics, following are the detailed observations:
DIBCO 2009 All the images of DIBCO 2009 dataset con-tain handwritten text only. Image H01 contains aging effect, faint text, and approximately uniform background. Proposed method has achieved highest F-measure: 91.79 among other images of this dataset. Image H02 contains shadows of the text of next side and aging effect. Proposed method does not work well on very dark background shades of text. So, as a consequence, it produced noise at those areas. For this image, it achieved a lowest F-measure: 49.22 among all the images. H03 image is having light shadows of text of next side and approximately uniform background. Pro-posed method achieved second highest F-measure: 88.69. Image H04 contains ink bleed-through degradation and tran-sitions of gray levels in some places of background. Proposed method achieved third highest F-measure: 77.59. Image H05 contains less text, dark background, and faint text. Proposed method recovered text but produced little noise and achieved F-measure: 50.01.

DIBCO 2010 All the images of DIBCO 2010 dataset contain handwritten text only. Image H01 contains faint text and uniform illumination background. Proposed method achieved F-measure 81.82 and it recovered text but produced little noise where some background text or dark edges of stains appears. Image H02 contains less text and approx-imately uniform illumination. However, proposed method recovered text but produced noise where text is not present and it achieved lowest F-measure 34.04 for this dataset. Image H03 contains few text lines and having dark shad-ows of the text of next side of the document. Proposed method achieved sixth highest F-measure: 80.14 among all the images. H04 Image contains dark shadows of text of next side of the document, so in view of this proposed method pro-duced noise as it assumes shadows as a text of the document and achieved second highest F-measure: 88.64. Image H05 contains few faint text lines at the lower side of the image. Proposed method recovered text and removed background lines but produced broken characters, which was actually very faint and achieved F-measure: 82.45, fourth highest among all the images. H06 Image is having aging effect, stains, and faint characters. Proposed method recovered text but produced little noise, broken characters, and achieved F-measure: 76.13. Image H07 contains uniform degraded back-ground. Proposed method removed degradation and achieved F-measure: 90.717. Image H08 contains aging effect and having faint text. Proposed method recovered almost all the text but produced some broken characters and achieved F-measure: 86.41. Image H09 contains faint text, line shadow on background, and uniform illumination. Image H10 con-tains less and faint text. Proposed method recovered text but some characters are broken due to faint effect and it achieved F-measure: 68.42.

DIBCO 2011 (Handwritten) Image HW01 is having dark stains at right side of the background of the image. Pro-posed method recovered text from the area where very dark stains are not present but produced some noise at degraded area and achieved lowest F-measure: 74.84 among all the images. Image HW02 contains uniform background, aging effect, and light shadow of text of the next side. Pro-posed method removed all the degradations and achieved highest F-measure: 90.45 among all the images. Image HW03 contains faint text, water spots, and line shadows at background. Proposed method recovered text but pro-duced broken characters in some cases and achieved F-measure: 86.18. Image HW04 contains water spots and few characters are faint. Proposed method recovered text but produced some broken characters whose are faint and removed text where transition of gray level high on back-ground and achieved F-measure: 80.73. Image HW05 is hav-ing few dark stains. Proposed method removed spots but produced some noise due to very dark edges of stains and achieved F-measure: 89.26. Image HW06 contains verti-cal folding effect degradation in the middle of the image, almost removed degradation but produced noise where dark edges of degradation exist and achieved F-measure: 84.98. Image HW07 contains shadows of text and aging effect. Proposed method almost recovered text but produced noise in some parts of the images and achieved F-measure: 85.95. Image HW08 is having less and faint text. Proposed methodrecoveredalmostallthetextandachievedF-measure: 86.51.

DIBCO 2011 (Machine-Printed) Image PR1is having less text, so in view of this proposed method produced noise at that area of the document image. Proposed method recov-ered text boundaries and achieved F-measure: 64.29. Image PR2 is having text shadows of the text of back side. Proposed method recovered text almost but produced some noise and achieved F-measure: 66.17. Image PR3 contains less text. Proposed method recovered text almost but produced noise in some areas where text is not present and achieved highest F-measure: 82.49 among all the images. Image PR4 is having aging stains and shadows of the text of next side of the doc-ument. Proposed method recovered text but produced some noise due to dark edges of stains and achieved F-measure: 47.55. Image PR5 contains dark spots of water drops. Pro-posed method recovered text but produced noise where text is not present and achieved F-measure: 71.62. Image PR6 is having dark spots on the background and very less text, so in view of this, proposed method produced noise and achieved lowest F-measure: 26.06. Image PR7 is having very less text, so proposed method produced noise and achieved F-measure: 59.60. Image PR8 contains faint text and some degradation. Proposed method recovered text but produced some broken characters and also produced noise in no text areas and achieved F-measure: 79.68.

Following are the discussions of DIBCO images on the basis of resulted F-measure and PSNR shown in Tables 5 , 6 and 7 , which shows the evaluation results of Figs. 13 , 14 , and 15 .
 Proposed: Our proposed method has first rank for all the Figs. 13 , 14 , and 15 among all the methods on the basis of highest F-measure (88.69, 88.64, and 84.98, respectively) and PSNR (16.66, 17.62, and 16.96, respectively).
Gatos: It has second rank for all the Figs. 13 , 14 , and 15 among all methods on the basis of F-measure (87.23, 81.11, and 70.94, respectively) and PSNR (16.41, 15.64, and 14.39, respectively).

Kim: It achieved third rank on the basis of F-measure (86.28 and 70.24, respectively) and PSNR (15.33 and 13.30, rank for the Fig. 14 and Table 6 . F-measure and PSNR are 15.37 and 0.37, respectively.

Otsu: Otsu method achieved fourth rank and having 84.114 F-measure and 14.506 PSNR. It has ninth and fifth rank for the Fig. 14 (F-measure: 15.37, PSNR: 0.37) and Fig. 15 (F-measure: 65.17, PSNR: 12.19).

Valizadeh: It has fifth rank on the basis of F-measure (70.869) and PSNR (12.79). It has fifth and fourth rank for the Figs. 14 and 15 , respectively (F-measure: 37.40, 65.62, PSNR: 6.03, 12.17).

Pa i : Pai X  X  method has sixth rank, 60.081 F-measure, and 9.102 PSNR. It has seventh rank for the Figs. 14 and 15 , respectively (F-measure: 32.13, 33.17, PSNR: 4.24, 5.98). Bernsen: This method has seventh rank on the basis of F-measure (59.56) and PSNR (9.40). It has third and ninth rank for the Figs. 14 and 15 , respectively (F-measure: 73.58, 28.93, PSNR: 13.49, 5.32).

Sauvola: Sauvola X  X  method has eighth rank among all the methods on the basis of F-measure (52.44) and PSNR (12.03). It has fourth and sixth rank for the Figs. 14 and 15 , respectively (F-measure: 56.79, 40.32, PSNR: 20.29, 12.57). Niblack: It has lowest (ninth) rank, F-measure, and PSNR. It has sixth and eight rank for the Figs. 14 and 15 respectively (F-measure: 37.40, 40.32, PSNR: 5.27, 4.97). 4.4.1 Comparison with ICDAR document image All DIBCO datasets are evaluated on AMD Dual Core processor C60 with 2 GB RAM and Windows 7 Operating System. Most of the images of DIBCO 2009 X 11 datasets have uniform illumination contains degradations like aging, ink bleed-through, and smears.

Proposed method works well in the cases of non-uniform illumination and variable contrast as discussed in Sect. 4.3 . It is observed from the experiments with DIBCO images that proposed method has lower F-measure, PSNR, and NRM over DIBCO winning methods. Proposed method is specifi-cally designed to handle very severely degraded images with dense text as shown in Figs. 9 , 10 , 11 , and 12 . Since, we do not have any standard dataset including DIBCO, which contains very severely degraded images, so in view of this we col-lected our own dataset that contained very severely degraded images having dense text lines, non-uniform illumination, and variable contrast. Most of the images of DIBCO 2009 X 11 datasets have uniform illumination and contained degrada-tions like aging, ink bleed-through, and smears as discussed in Sect. 4.4 . In view of this, proposed approach may not be properly evaluated only on the basis of DIBCO datasets. The main application of proposed approach is to binarize low illuminated and variable contrast images. Experiments on DIBCO datasets show that the proposed method works well on handwritten images as shown in Table 9 : DIBCO 2011.
Following are the comparative discussions on the DIBCO winning methods and proposed approach: DIBCO 2009: This competition was won by Lu and Tan [ 40 ]. The winner method achieved average F-measure: 91.24, PSNR: 18.66, and NRM: 4.31. Our proposed method achieved an average F-measure: 71.46, PSNR: 15.43, NRM: 8.54 and IND: 0.53.

DIBCO 2010: The competition was won by Su et al. [ 41 ] achieved F-measure: 91.50, PSNR: 93.58, and NRM: 19.78. Our proposed method achieved average F-measure: 76.89, PSNR: 15.81, NRM: 13.27 and 0.60 for the DIBCO images for the DIBCO images H01-H10 (estGT).

DIBCO 2011: T. Lelore and F. Bouchara won the DIBCO 2011 competition and achieved average F-measure: 92.7 and PSNR: 19.92 for the handwritten dataset, and F-measure: 69.35 and PSNR: 12.33 for the printed dataset. Proposed method achieved average F-measure: 84.86 and PSNR: 17.19 for the handwritten dataset, and F-measure: 62.18 and PSNR: 11.16 for the printed dataset. 5 Conclusion An approach for adaptive binarization of severely degraded document images is presented, which can deal with different types of degradations such as variable background, shadows, non-uniform illumination, low contrast, ink bleed-through, and blur. Local contrast analysis, contrast stretching, thresh-olding, and noise removal steps are used to recover text from the document images containing non-uniform and degraded background. The experimental results and comparison with eight benchmark and recent algorithms show the effective-ness of our proposed method. However, proposed method produces noise in case of very less text, large gap between text lines, and very high contrast just above the text line or in the whole document image. The reason behind produc-ing noise is due to its recovery phase as it searches text in a document.

Most of the works reported in the field of OCR are on good-quality documents. But still it remains a highly chal-lenging task to implement an OCR that works under all possi-ble degradation and non-uniform illumination conditions and gives highly accurate results. Elaborate study and research on poor-quality documents need further exploration to make it a viable solution in the development of robust OCR sys-tem.
 References
