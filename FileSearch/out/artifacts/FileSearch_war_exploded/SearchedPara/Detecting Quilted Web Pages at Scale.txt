 Web-based advertising and electronic commerce, combined with the key role of search engines in driving visitors to ad-monetized and e-commerce web sites, has given rise to the phenomenon of web spam: web pages that are of lit-tle value to visitors, but that are created mainly to mislead search engines into driving traffic to target web sites. A large fraction of spam web pages is automatically generated, and some portion of these pages is generated by stitching to-gether parts (sentences or paragraphs) of other web pages. This paper presents a scalable algorithm for detecting such  X  X uilted X  web pages. Previous work by the author and his collaborators introduced a sampling-based algorithm that was capable of detecting some, but by far not all quilted web pages in a collection. By contrast, the algorithm presented in this work identifies all quilted web pages, and it is scal-able to very large corpora. We tested the algorithm on the half-billion page English-language subset of the ClueWeb09 collection, and evaluated its effectiveness in detecting web spam by manually inspecting small samples of the detected quilted pages. This manual inspection guided us in itera-tively refining the algorithm to be more efficient in detecting real-world spam.
 H.3.3 [ Information Search and Retrieval ]: Clustering, Information Filtering Algorithm, Experimentation, Measurement Web spam detection, phrase-level duplication, scalable algo-rithms
Over the past 20 years, the World Wide Web has evolved from a way for scientists to exchange data and ideas to be-come the general population X  X  medium of choice for informa-tion, communication, entertainment, and commercial trans-actions. Web-mediated electronic commerce in the United States is projected to reach close to $ 200 billion in 2011 [1]. Search engines such as Google and Bing form a key compo-nent of this web-based ecosystem, in that they enable users to locate content and services. The explosive rise in the popularity of the web and the volume of e-commerce, to-gether with the key role of search engines, has given rise to the phenomenon of web spam: web content that is of little value to users, but created mainly to drive search engine traffic to particular target sites. Web spam creates a neg-ative experience for search users, lowering the utility they derive from the search service. Consequently, web spam de-tection is an active area of research, both in search engines and in academia.

A substantial fraction of spam web pages is automatically generated, and some of that content is constructed out of all or parts of other web pages. In this paper, we introduce a technique for detecting all such web pages in a given col-lection, exhaustively and at large scale, using data-parallel infrastructure. We coin the term quilted web page to refer to a page that is  X  X titched X  together out of  X  X atches X  taken from multiple other web pages. Figure 1 shows an example of such a quilted page (though arguably not spammy in na-ture) together with three of the source web pages that each donated a patch of words. The notion of quilted web page is congruent with our an earlier notion of phrase-level repli-cation [9], but more constrained than the notion of local text reuse [15] introduced by Seo and Croft, which refers to por-tions of text shared between two documents, but does not insist on a large fraction of a document consisting of patches of multiple other documents.

The notion of textual similarity is at the heart of Informa-tion Retrieval, and there is a wide gamut of similarity-based problems, ranging from the retrieval problem (finding those documents in a collection that contain the  X  typically few  X  terms of a query) over local text reuse detection to the other extreme of near-duplicate detec tion (clustering a collection into classes of highly similar documents). The problem ad-dressed in this paper is situated in the middle of the gamut (along with local text reuse detection), but it borrows ideas from the extreme end of the gamut, namely near-duplicate detection, and in particular shingling . The core idea of the seminal shingling algorithm , due to Broder et al. , is to segment each document into overlap-ping k -grams called shingles (drawing on the image of the overlapping shingles of a roof), to compact the shingles by hashing them, and to draw a consistent sample from the set of hashed shingles. In an early version of the algo-rithm [6], this was achieved by retaining all hash values h of a document where h mod p =0(with p fixed in ad-vance), resulting in a variable-length vector of shingleprints . A refined version [5] employed randomly selected permu-tations and retained for each document those hash values that are minimal among all the document X  X  hashes under a permutation, resulting in a fixed-sized shingleprint. Ei-ther scheme greatly reduces the amount of data required to represent each document, but neither addresses the time-complexity issues of document clustering, naive pair-wise comparison having quadratic complexity. A subsequent re-finement (see [8] for a detailed description) combines groups of shingles into megashingles such that any two documents sharing a megashingle are wit h high probability near-dupli-cates of one another, thereby making it possible to clus-ter a set of documents into near-duplicate sets in linear time. Shingling is a purely syntactic technique; it does not presume any knowledge of the semantics of the document, and does not involve any language models. A competing approach to shingling, Charikar X  X  locality-sensitive hashing scheme [7], is similarly syntactic in nature: Documents are tokenized through some extraneous mechanism (e.g. simple word-breaking or segment ation into overlapping k -grams), resulting in feature vectors that are viewed as points in a very high-dimensional space. A set of k hyperplanes in that high-dimensional space is fixed ahead of time. The algo-rithm determines for each point (document) and each hy-perplane, what side of the hyperplane the point falls on, producing a bit per hyperplane and resulting in a k -bit hash value. Near-duplicate documents have, with high proba-bility, highly similar hash values. The interested reader is referred to Henzinger X  X  experimental comparison [10] of Broder X  X  and Charikar X  X  algorithms. Like these two algo-rithms, the algorithm described in this paper is purely syn-tactic. Moreover, it uses Broder X  X  idea of segmenting each document into a sequence of overlapping k -grams.
There are multiple works addressing the middle portion of the gamut, detecting phrase-level replication. Fetterly, Manasse and Najork [9] attacked the problem by using core ideas of shingling  X  segmenting each document into over-lapping k -grams and then performing a sequence of data reduction steps: hashing k -grams, collapsing near-duplicate documents (through shingling proper), eliminating popular k -grams, sampling the hashed k -grams of each document, and finally constructing sets of documents covering the sur-viving hashes of each document. Due to the sampling-based approach, their algorithm identifies some, but by no means all of the quilted pages in a collection, and some but not all of the  X  X atch sources X  of each quilted page. The algo-rithm described in this paper resembles our earlier attempt in many ways, but it is not probabilistic: given a collection, it will identify all quilted pages and all their source pages.
Baeza-Yates, Pereira and Ziviani [2] attacked the related problem of constructing a genealogical tree of web pages  X  deducing which web pages are derived from which other web pages  X  by segmenting pages into three-sentence shingles (as opposed to the more common work-level shingling), collaps-ing duplicate documents in a pre-processing stage, and fi-nally deducing parent-child relationships between pages us-ing a greedy algorithm. Their work is similar to ours in that it is not sampling-based, but differs in the choice of fingerprinting unit (three-sentence units vs. k -word units), the corpus size it was applied to (14 million vs. 503 million pages), and the underlying motivation.

Seo and Croft [15] describe an algorithm for detecting lo-cal text reuse , a document containing a patch of text taken from another document. Their algorithm (like both our ear-lier [9] and our current algorithm) is purely syntactic; docu-ments are viewed as simple sequences of terms. It segments each document into non-overlapping sequences of terms us-ing a technique called hash-breaking , and uses another tech-nique called DCT (discrete-cosine transform) fingerprinting to make the algorithm less sensitive to small modifications of text. The algorithm outputs pairs of documents shar-ing a patch of text; unlike our algorithm it is not aimed at identifying quilted documents incorporating portions from multiple sources. Seo and Croft evaluated their algorithm empirically, and found precision and recall of text reuse to be good. It is difficult to ascertain the scalability of their approach. Their largest experiment processed a corpus of about 3 million blog posts; by comparison, the algorithm proposed in this paper was tested on a collection of over half a billion web pages.

More recently, Bendersky and Croft took [3] took another stab at the problem of detecting text reuse. Their approach is aimed at finding reuse in web-scale collections, and it as-sumes the existence of a retrieval system for that collection (say, a search engine). The algorithm does not aim to detect all instances of text reuse; rather, it takes a set of topical statements as input and aims to find all instances of text reuse related to these topics (leveraging the search engine to retrieve candidate documents for each topic). Another im-portant difference to the previously described approaches is that theirs is not purely syntactic  X  it does involve language models, query likelihoods, and temporal information.
Another related line of work is that of Kolak and Schilit, who studied the problem of identifying quotations (short pieces of text excerpted from other documents) in a large collection of documents, namely the Google Books corpus. The underlying objective was to treat quotations as a form of  X  X ink X  from the quoting to the quoted document. Like our past and present work, their algorithm borrows key ideas from shingling: documents are segmented into overlapping k -grams which are then hashed (core ideas of shingling), and hashes are added to an index to build up a collection of repeated text sequences. Their algorithm has been im-plemented in the MapReduce framework and appears to be highly scalable. While Kolak and Schilit X  X  algorithm has many commonalities with the algorithm described in this paper, the two algorithms do compute different things: their algorithm identifies quotations (portions of text shared be-tween documents) while ours identifies quilted web pages (web pages that consist largely of portions of other web pages).

Finally, it is worth pointing out that the problem of iden-tifying quilted web pages is quite similar to that of identi-fying plagiarism. There exists a large body of work on pla-giarism detection; two exemplary systems include COPS [4] and SCAM [16]. The interested reader is further referred to a study by Hoad and Zobel [11] comparing the effectiveness of various algorithmic approaches to plagiarism detection.
The remainder of the paper is structured as follows: Sec-tion 2 introduces a new algorithm for detecting quilted web pages, first by providing an intuitive explanation, then by framing the algorithm in a set-theoretic setting, and finally by sketching its implementation in DryadLINQ [17], a plat-form for data-parallel cluster computing that leverages LINQ, Microsoft X  X   X  X anguage-integrated query X  extension to the C# programming language. LINQ is based on list comprehen-sions, which in turn are closely related to set comprehen-sions, making the leap from specification to implementation quite small. Section 3 describes the experimental validation of our algorithm. It provides details on the data set and the computational infrastructure used in this study, and describes the runs we performed. We measured the effec-tiveness of the quilt detection algorithm in identifying spam web pages by manually inspecting a small sample of detected quilted pages (together with the source pages that provided the  X  X atches X  for each  X  X uilt X ). While (per definition) ev-ery detected page was indeed a quilt, only a small fraction of these pages was considered spam. Analyzing the nature of these false-positives guided us in refining our algorithm with several heuristics, whic h greatly reduced the rate of false positives. Finally, section 4 offers some concluding re-marks and avenues for future research.
Our quilt detection algorithm takes a corpus of web pages as input, and outputs the set of all  X  X uilted X  web pages, together with the source web pages providing the textual  X  X atches X  that went into the quilt. Rather than consider-ing only proper phrases as potential patches, we segment each n -word document into n  X  k + 1 overlapping k -grams. Discarding common k -grams that occur in more than m doc-uments as well as unique ones that occur in only one doc-ument leaves us with a set of candidate patch grams .We consider a page to be quilted iff at least a  X  fraction of its k -grams are patch grams, and if the minimum cover set of documents from which the patch grams are drawn contains at least c documents. So, our definition of quilted page in-volves the four parameters k , m , c and  X  . The choice of these parameters impacts both the effectiveness (what fraction of quilted pages would be considered spam by human judges) and the efficiency (how long it takes to process the corpus) of the detection algorithm.

More formally, we consider a corpus D of documents (web pages). Each document d  X  D is a sequence of terms (words) t 1  X  X  X  t n , which can be grouped into n  X  k + 1 overlapping k -grams, with g 1 = t 1  X  X  X  t k , g 2 = t 2  X  X  X  t k +1 ,etc. We write grams k ( d )todenotethe k -gram-set of document d , and docs k ( g )todenote { d  X  D : g  X  grams k ( d ) } ,thesetof documents containing k -gram g . We define the patch gram set of a document d (those k -grams of d that co-occur in at least one but fewer than m other documents) as follows: pgrams k,m ( d )= { g  X  grams k ( d ):1 &lt; | docs k ( g ) Obviously, pgrams k,m ( d )  X  grams k ( d ) for all d , k and m . We define the patch-fraction of a document d as the fraction of patch grams in the document X  X  gram set: Moreover, we define sources k,m ( d ), the source documents contributing patch grams to a document d , as follows: In other words, sources k,m ( d ) is a (not necessarily unique) minimal set of documents whose combined patch grams cover all the patch grams of d . The set cover problem is NP-hard [12], but there is a well-known greedy approximation algorithm, detailed below. Using this notation, we define quilted web pages as follows: Definition: Adocument d is said to be ( k, m, c,  X  ) -quilted iff patchfrac k,m ( d )  X   X  and | sources k,m ( d ) | X  c . Before describing the approximation algorithm for comput-ing sources k,m ( d ), we first provide a few theorems concern-ing the role of the parameters in the definition of quiltedness: Theorem 1 (Monotonicity in  X  ): A document that is ( k, m, c,  X  )-quilted is also ( k, m, c,  X  )-quilted for any  X  Proof: Straightforward. For any document d , d is ( k, m, c,  X  )-quilted implies | sources k,m ( d ) | X  c and patchfrac k,m which implies patchfrac k,m ( d )  X   X  since  X   X   X  ,whichim-plies d is ( k, m, c,  X  )-quilted.
 Theorem 2 (Monotonicity in c ): A document that is ( k, m, c,  X  )-quilted is also ( k, m, c , X  )-quilted for any c Proof: Straightforward. For any document d , d is ( k, m, c,  X  )-quilted implies patchfrac k,m ( d )  X   X  and | sources k,m which implies | sources k,m ( d ) | X  c since c  X  c ,whichim-plies d is ( k, m, c , X  )-quilted.
 Theorem 3 (Monotonicity in m ): A document that is ( k, m, c,  X  )-quilted is also ( k, m ,c, X  )-quilted for any m Proof: Assume m  X  m , and consider a ( k, m, c,  X  )-quilted document d ,so patchfrac k,m ( d )  X   X  and | sources k,m ( d ) It is easy to see that pgrams k,m ( d )  X  pgrams k,m ( d ), which implies that patchfrac k,m ( d )  X  patchfrac k,m ( d )  X   X  .Fur-thermore, since pgrams k,m ( d )  X  pgrams k,m ( d ), and since sources k,m ( d )isthe minimal set of documents whose patch-grams cover pgrams k,m ( d ), the minimality property implies that | sources k,m ( d ) | X | sources k,m ( d ) | X  c . Taken together, this implies that d is ( k, m ,c, X  )-quilted.
 The attentive reader will notice that there is no Theorem 4  X ( k, m, c,  X  )-quiltedness is not monotonic in k .Itisfairly easy to see why: choosing a very small value of k may render most k -grams common, i.e. occurring in more than m doc-uments, while choosing a very large value of k may render most k -grams unique, i.e. occurring in only a single docu-ment. At either extreme, the average document will tend to have a smaller patch gram set, and hence a lower patch fraction, thereby reducing the number of documents d for which patchfrac k,m ( d )  X   X  .
 As mentioned above, the set cover problem is NP-hard. We compute sources k,m ( d ), the source documents contribut-ingpatch-gramstoadocument d , through the following greedy approximation algorithm: C  X  X  ( d ,g ): d  X  D \{ d } X  g  X  grams sources k,m ( d )  X  X  X  while C =  X  do It should be pointed out that Theorem 3 does not necessarily hold when sources k,m ( d ) is approximated rather than com-puted precisely. Nonetheless, even when using the above greedy approximation, we expect the set of quilted docu-ments in a large collection to increase  X  X ostly X  monotoni-cally as m increases.

We implemented the above set-theoretic definition of quilt-edness using DryadLINQ [17], a data-parallel cluster com-puting platform that leverages LINQ, Microsoft X  X   X  X anguage integrated query X  extension to the C# programming lan-guage. LINQ is based on list comprehensions , a concept that originated in the functional programming community and that in turn is related to mathematical set notation (some-times called set comprehensions). Because of the fairly close relationship between LINQ and set notations, it is reason-ably straightforward to cast the above definitions in LINQ and C#. The entire implementation, shown in Figure 5 is just 74 lines of C# and LINQ, not counting utility li-braries for hashing and mapping between textual and nu-meric ClueWeb09 document identifiers.

Functional programs in general and list comprehensions in particular can be efficiently evaluated in a data-parallel fashion, and DryadLINQ exploits that fact by executing any LINQ query on many machines in parallel. A DryadLINQ program is started on a single machine. Whenever the con-trol flow encounters a LINQ query, DryadLINQ ships that query to available machines in a compute cluster, which Figure 2: Data flow graph of the DryadLINQ imple-mentation of our quilt detection algorithm. also store the data (in our case, the corpus of documents) in a distributed fashion. Data is organized into streams segmented into multiple partitions , and the partitions of a stream are distributed across the cluster. A query is de-composed into multiple stages , each stage consisting of the maximal pipeline of operators (e.g. Select , Where , GroupBy , Join , Partition and Merge ) that can be executed locally by reading and writing items in a streaming fashion. Each stage is executed in parallel, with the degree of parallelism deter-mined by the partitioning of the input streams, and spread over the available machines in the cluster. A DryadLINQ computation can be characterized by a directed acyclic graph, with the nodes corresponding to stages and the edges cor-responding to data flowing from one stage to the next. Fig-ure 2 shows the data flow graph of the DryadLINQ imple-mentation of our quilt detection algorithm. It contains 14 stages (represented as ovals), and completing each stage in-volves runni ng 1000 processes distributed over the machines in the cluster. The processes in each stage are indepen-dent from each other and thus parallelizable; in practice, the DryadLINQ runtime runs ju st one such process per ma-chine at any given time.
The experiments described in this section were performed on a cluster of about 220 computers, each running the Win-dows Server 2003 64-bit operating system. Each machine had two dual-core AMD Opteron 2218 HE CPUs clocked at 2.6 GHz, 16 GB of DDR2 RAM, four 750 GB SATA drives, and a 1 Gbit/sec Ethernet NIC. Groups of about 25 com-puters were connected to a Blade RackSwitch G8052 48-port full-crossbar local switch; and the nine local switches were interconnected through a Blade RackSwitch G8264 switch, with 30 Gbit/sec full-duplex inter-switch bandwidth achieved via port aggregation.

For our experiments, we used the English-language subset of ClueWeb09 [14]. The collection was assembled through a web crawl conducte d in 2009. The full collection con-sists of over a billion web pages, while the English-language subset is comprised of 503,898,901 pages. We segmented each page into individual words by embedding the Bing HTML parser into DryadLINQ and performing the parsing and word-breaking on our compute cluster. We subsequently performed several runs of the DryadLINQ implementation of our quilt detection algorithm, exploring various choices of the parameters k , m , c and  X  . Typical runs took between 18 and 48 hours depending on parameter combinations as well as job competition. Since our compute cluster is shared among multiple users and a job X  X  individual processes are routinely aborted by the cluster scheduler to accommodate other users, the aforementioned running times should only be viewed as upper bounds. Table 1: Percentage of ( k, m, c,  X  ) -quilted pages in ClueWeb09 and average number of source pages, for various choices of k , m , c and  X  .

The output of each quilt detection run is a stream con-taining the ClueWeb09 document IDs of all detected quilted pages, together with a minimal 1 set of source documents. We picked parameter combination ( k, m, c,  X  )=(5 , 50 , 4 , 0 . 5) as a pivot point and varied the parameters around that pivot point, one dimension at a time. Table 1 shows the number of quilted pages in our corpus and the average number of source documents for each of the parameter combinations we explored. Figure 3 plots the number of quilted pages for each parameter sweep, giving credence to Theorems 1 X 3 stating the monotinicity in  X  , c and m , as well as illustrating the non-monotonicity in k . The pivot point is highlighted in each plot.

In order to evaluate the effectiveness of our quilt detec-tion algorithm at identifying spam web pages, we drew a small sample from the set of detected quilts, and extracted the words of each quilted page and their source documents from the ClueWeb09 corpus (again leveraging DryadLINQ). We then manually inspected each quilt and its sources and labeled them as positive (i.e. spam) or negative (not spam). Figure 4 shows a screen shot of the tool we used to perform this labeling.

Table 2 shows the results of labeling five of the runs de-scribed in Table 1,the first being the pivot point and the other four extreme choices of k , m , c ,and  X  . For each run, we sampled 100 ( k, m, c,  X  )-quilted pages and assessed them using the judging tool. Table 2: Percentage of sampled ( k, m, c,  X  ) -quilted pages that are labeled as spam, for five choices of k , m , c and  X  .

Among the five runs, the parameter combination we chose as the pivot point performed best. It is not clear how sig-nificant the observed differences in effectiveness are. The sample size is relatively small, but more importantly, our spam judgments are truly that: judgment calls. It was of-ten quite hard to determine whether a given quilted page was spam or not. For one, a substantial fraction of web pages in the ClueWeb09 collection no longer exists in the
Or rather, approximately minimal, due to the greedy set cover approximation algorithm. wild; and often enough, the domain of the page either no longer exists or has been taken over by a  X  X omain parking X  service. In such cases, we had to judge the page solely based on its textual content, without considering embedded im-ages or other pages in the same domain  X  two important cues in making spam judgments. Moreover, there is a fine line between low-quality content with commercial intent and outright spam. We might (somewhat arbitrarily) decide that an Amazon reseller engaging in keyword stuffing is pursuing a legitimate business, but a blog that is  X  X eviewing X  mira-cle patent medicines and monetizing this through a referral program is spam. It is hard to draw the line, and hard to do so consistently.

We were disappointed by the low percentage of quilted pages detected by our algorithm that would actually be con-sidered spam by a human observer. By examining the many false positives, it became quickly clear that our algorithm identified the home pages of many blogs as quilted, with the full articles of the blog posts being the source documents contributing fragments to the blog home page. This obser-vation guided us to add the following simple heuristic to our quilt detection algorithm: each source document should re-side on a different web server than the quilted document. More formally, the initialization of C in the algorithm given in Section 2 is replaced by: C  X  X  ( d ,g ): d  X  D \{ d } X  g  X  grams We refer to pages satisfying this refined definition of quilted-ness as foreign-sourced quilted pages. There is some ambigu-ity as to what constitutes a web server: it could be identified by a symbolic host name, a paid domain name, an IP ad-dress, or even a domain registrar record. In our experiments, we treated paid domain names (such as microsoft.com or cam.ac.uk ) as the identifiers of servers, and we insisted on source pages to originate from a different domain than the quilted page.

We ran this variant of our quilt detection algorithm on the same data set as before, with the parameters ( k, m, c,  X  )= (5 , 50 , 4 , 0 . 5) that we had chosen as a pivot point. This run detected 20,576,548 quilted pages (a proper subset of about 35% of the pages detected by the unmodified quilt detection algorithm), each one incorporating an average of 28.26 for-eign source pages. We labeled 100 pages drawn uniformly at random from the set of foreign-sourced quilted pages, and judged 34 of them as spam. This significantly increase in the fraction of quilted pages judged as spam provides some evidence that the heuristic of requiring each source page to originate from a different server than the quilted page is indeed effective at surfacing spam.

None of the pages in our sample were of the blatant machine-generated variety we observed during our 2005 study, sug-gesting that spammers have become more sophisticated. How-ever, our algorithm was very effective in surfacing web sites that appropriated semantically coherent parts of other web sites (say, a short essay) and used this content as  X  X ait X  to drive content to their own site, which was typically not top-ically related to the essay. Another pattern we observed repeatedly was  X  X ookie-cutter X  web sites, where apparently independent entities would offer essentially the same digital content, or sell the same products.

About two-thirds of the foreign-sourced quilted pages iden-tified by our technique were not judged as spam. Within this set, we observed two notable patterns: 1. Publicly available documents (such as technical docu-2. Some organizations (for example, Hartnell College) uti-
The second class of false-positives accounted for about one-fifth of all false-positives. Spot-checking a few of them showed that most of these domains resolve to the same canonical IP address. For example, the symbolic host names hartnell.edu and hartnell.cc.ca.us resolve to 198.189. 134.200 ; and the symbolic host names efinancialcareers. com , efinancialcareers.ie , efinancialcareers.be resolve to 74.115.248.70 . Using IP addresses to represent web servers fits well into the data-parallel implementation of our algorithm. The alternative approach of leveraging the ap-parent textual similarity between these domain names, while feasible, is harder to shoe-horn into the DryadLINQ im-plementation. We implemented the former approach and tested it on the same data set as before, with the parame-ters ( k, m, c,  X  )=(5 , 50 , 4 , 0 . 5) that we had chosen as a pivot point. This run detected 19,729,026 quilted pages, each one incorporating an average of 29.22 source pages hosted on web servers with different IP addresses than that of the quilted page. In other words, using this stricter notion of differing web servers has little impact on how many pages are detected as quilted. Again, we sampled 100 pages from this run and manually inspected them, finding 26 of them to be spam, which is noticeably lower than the 34% spam yield uncovered by the previous run. We attribute it to the imprecise nature of the assessment process.

In the course of assessing 100 samples each from 7 runs, we found that many quilted pages drew most of their patch-grams from just a few source pages, while most of the source pages contributed just one or a few patch grams. It would be interesting to modify the quilt detection algorithm (and the very definition of quiltedness) so as to ensure that each source page contributes at least a fraction of  X  of the patch-grams in the quilted page. We leave this as an avenue for future work.
This paper builds on our earlier work on detecting phrase-level duplication on the web [9]. We give a formal definition of what constitutes a quilted web page (a web page that is stitched together out of textual patches taken from other web pages) and prove a few properties of that definition; we provide an algorithm for exhaustively detecting all quilted web pages in a given collection; we describe a data-parallel implementation of the algorithm that scales to very large collections; we evaluate the effectiveness of the algorithm in detecting web spam by applying it (with various param-eterizations) to the half-billion page English-language sub-set of the ClueWeb09 collection [14] and judging samples of the detected quilted pages as to whether or not they indeed constitute spam; and we suggest a few heuristics to improve effectiveness.

A major area for future work will be to combine the syn-tactic approach of our algorithm with semantic (and domain-specific) techniques for spam detection. While the syntac-tic technique described in this paper (by virtue of its ex-haustiveness) is fully effective at detecting all quilted web pages, the task of effectively identifying spam must ulti-mately incorporate semantic information. Our algorithm (correctly) identities news aggregation web sites to contain quilted pages (pages that collate opening paragraphs of news articles from other sources); but such news aggregation pages are not spam  X  users derive a real value from them. A po-tential semantic signal that distinguishes news aggregation from spam is attribution  X  a link from each excerpt to the underlying article.

The core ideas described in this paper are applicable not only to spam suppression in search engines, but also (and probably more so) to detecting plagiarism in web-scale cor-pora. Here again, semantic information (such as proper at-tribution) will be needed in addition to the purely syntactic approach of our technique.
 This work grew out of a 2004 collaboration with Mark Man-asse and Dennis Fetterly, and benefited from our many dis-cussions. Dennis played a crucial role in the work described in this paper, by word-breaking the ClueWeb09 collection using the Bing HTML parser, supplying me with IP ad-dresses for the URLs in the ClueWeb09 collection, provid-ing me with a judging tool to adapt for this project, and pa-tiently answering my many questions related to DryadLINQ. Thanks to the entire DryadLINQ team for building a system to make cluster-scale computing easy and fun, and particu-larly to Yuan Yu for being super-responsive in fixing bugs. Finally, thanks to Jamie Callan and his team at CMU for compiling the ClueWeb09 collection, which has arguably be-come the reference data set for reproducible web corpus re-search. I can hardly wait for ClueWeb12! [1] Jacqueline Anderson, Reineke Reitsma, Patti F. [2] Ricardo Baeza-Yates,  X  Alvaro Pereira, and Nivio [3] Michael Bendersky and W. Bruce Croft. Finding Text [4] Sergey Brin, James Davis, and Hector Garcia-Molina. [5] Andrei Z. Broder. On the Resemblance and [6] Andrei Z. Broder, Steven C. Glassmann, Mark S. [7] Moses S. Charikar. Similarity Estimation Techniques [8] Dennis Fetterly, Mark Manasse and Marc Najork. On [9] Dennis Fetterly, Mark Manasse and Marc Najork. [10] Monika Henzinger. Finding Near-Duplicate Web [11] Timothy C. Hoad and Justin Zobel. Methods for [12] Richard M. Karp. Reducibility Among Combinatorial [13] Okan Kolak and Bill N. Schilit. Generating Links by [14] Lemur Project. The ClueWeb09 DataSet. Online at [15] Jangwon Seo and W. Bruce Croft. Local Text Reuse [16] Narayanan Shivakumar and Hector Garcia-Molina. [17] Yuan Yu, Michael Isard, Dennis Fetterly, Mihai Budiu,
