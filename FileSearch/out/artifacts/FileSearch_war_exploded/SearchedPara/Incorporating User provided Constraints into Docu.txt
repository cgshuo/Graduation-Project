
Document clustering without any prior knowledge or background information is a challenging problem. In this paper, we propose SS-NMF: a semi-supervised non-negative matrix factorization framework for document clus-tering. In SS-NMF, users are able to provide supervision for document clustering in terms of pairwise constraints on a few documents specifying whether they  X  X ust X  or  X  X an-not X  be clustered together. Through an iterative algorithm, we perform symmetric tri-factorization of the document-document similarity matrix to infer the document clusters. Theoretically, we show that SS-NMF provides a general framework for semi-supervised clustering and that existing approaches can be considered as special cases of SS-NMF. Through extensive experiments conducted on publicly avail-able data sets, we demonstrate the superior performance of SS-NMF for clustering documents.
Document clustering is the grouping of text documents into meaningful clusters in an unsupervised manner. It is one of the most important tasks in text mining and has re-ceived extensive attention in the data mining community re-cently [6,19,38].

Information retrieval (IR) needs range from a specific search at one end to an open ended browsing of the database at the other [8]. A keyword-based search, where the user is interested in retrieving all the documents that have an exact match with the query keyword, is an example of a specific search scenario. On the other hand in open-ended browsing, the user generally has a broader perspective of the informa-tion he/she is looking for and is interested in browsing and navigating through the database. While traditional IR tech-niques have been well developed for the specific search sce-nario, they are ill-suited for providing a browsing capability to the user. A good document clustering algorithm can pro-vide a holistic view of the text corpus and hence overcome the limitations of traditional IR techniques.

Document clustering methods in general can be catego-rized into document partitioning (flat clustering) and ag-glomerative (hierarchical) clustering. Partitioning methods typically divide the documents in a given number of clus-ters directly. Hierarchical clustering aims to obtain a hi-erarchy of clusters by building a tree structure, that shows how the clusters are related to each other. The clustering result of the documents can be obtained by cutting the tree at a desired level. One of the popular hierarchical docu-ment clustering methods is the hierarchical agglomerative clustering (HAC) that proceeds in a bottom-up fashion by iteratively merging small clusters into larger ones [7, 35]. This is continued until all the documents get merged into one single cluster at the root node of the tree. Variations of HAC algorithm have been proposed that differ based on the method adopted to compute the similarities between the clusters. Some of the common methods to measure cluster similarity are single-linkage, complete-linkage, and group-average linkage. While, the first two use the maximum and minimum distance between the clusters, respectively, group-average linkage uses the cluster center distance. [14] has studied the different types of similarity measures and their effect on clustering accuracy.

Some of the widely applied methods in document parti-tioning include k -means [12], probabilistic clustering using the Naive Bayes or Gaussian mixture model [1,28], etc. k -means produces clusters that minimizes the sum of squared distances between the data points and their corresponding cluster centers. On the other hand, both naive Bayes and Gaussian mixture model define a probabilistic cluster model and try to find the model by maximizing the likelihood of the data. The problems associated with these methods is that they make a strict assumption on the distribution of the document corpus. k -means assumes every document clus-ter has a compact shape, the Naive Bayes model assumes feature independence in the document corpus feature space, and the Gaussian mixture model assumes that the density of each cluster can be approximated by a Gaussian distribu-tion. Since, the actual underlying distribution of the docu-ment corpus can be different, these methods are susceptible to their a priori assumptions.

Recently, document clustering based on spectral cluster-ing has emerged as a popular approach [9,11]. These meth-ods model the documents as vertices of a weighted graph with edge weights representing the similarity between two documents. Clustering is then obtained by  X  X utting X  the graph vertices into different partitions. Partitioning of the graph is obtained by solving an eigenvalue problem where the clustering is inferred from the top eigenvectors. As can be seen from the above discussion, document cluster-ing has been extensively studied and various methods pro-posed. However, accurately clustering documents without domain-dependent background information, is still a chal-lenging task.

In this paper, we propose a non-negative matrix fac-torization (NMF) [23, 24] based framework to incorpo-rate prior knowledge into document clustering. Under the proposed semi-supervised NMF (SS-NMF) methodology, user is able to provide pairwise constraints on a few docu-ments specifying whether they  X  X ust X  or  X  X annot X  be clus-tered together. We derive an iterative algorithm to perform symmetric non-negative tri-factorization of the document-document similarity matrix. The correctness of the algo-rithm is proved by showing that the algorithm is guaran-teed to converge. We also prove that SS-NMF is a general and unified framework for semi-supervised clustering by es-tablishing the relationship between SS-NMF and other ex-isting semi-supervised clustering algorithms. Experiments performed on publicly available text data sets demonstrate the effectiveness of the proposed work.
There have been prior efforts on using user provided information to improve clustering. [17] proposed incorpo-rating background knowledge into document clustering by enriching the text features using WordNet 1 . In [21], some words per class and a class hierarchy were sought from the user in order to generate labels and build an initial text clas-sifier for the class. A similar technique was proposed in [27], where the user is made to select interesting words from automatically selected representative words for each class of documents. These user identified words were then used to re-train the text classifier. Active learning approaches have also found application in semi-supervised clustering. [13] has proposed to convert a user recommended feature into a mini-document which is then used to train an SVM classifier. This approach has been extended by [31] which adjusts SVM weights of the key features to a predefined value in binary classification tasks. Recently, [18] presented a probabilistic generative model to incorporate extended feedback that allows the user and the algorithm to jointly ar-rive at coherent clusters that capture the categories of inter-est to the user. [5,20,30] proposed methods where the user provided class labels a priori to some of the documents. These algorithms use the labeled data to generate seed clus-ters that initialize a clustering algorithm, and use constraints generated from the labeled data to guide the clustering pro-cess. Proper seeding biases clustering towards a good re-gion of the search space, while simultaneously producing a clustering similar to the specified labels.

However, in certain applications, supervision in the form of class labels may be unavailable. For example, complete class labels may be unknown in the context of clustering for speaker identification in a conversation [2], or cluster-ing GPS data for lane-finding [34]. In some domains, pair-wise constraints occur naturally, e.g., the Database of In-teracting Proteins (DIP) data set contains information about proteins co-occurring in processes, which can be viewed as must-link constraints during clustering. Similarly, for doc-ument clustering, user knowledge about which few docu-ments are related or unrelated can be incorporated to im-prove the clustering results. Moreover, it is easier for a user who is not a domain expert to provide feedback in the form of pairwise constraints than class labels, since pro-viding constraints does not require the user to have signif-icant prior knowledge about the categories in the data set. Amongst the various methods proposed for utilizing user provided constraints for semi-supervised clustering [3, 4], two of the well-known include the semi-supervised kernel k -means (SS-KK) [22] and semi-supervised spectral clus-tering with normalized cuts (SS-SNC) [19]. While, SS-KK transforms the clustering distance measure by weighted ker-nel k -means with reward and penalty constraints to perform semi-supervised clustering of data given either as vectors or as a graph, SS-SNC utilizes supervision to change the clus-tering distance measure with pairwise information by spec-tral methods. The SS-NMF framework presented in this paper, allows the user to provide pairwise constraints on a small percentage of the documents. Specifically, these con-straints specify whether two documents should belong to the same cluster or should strictly belong to different clus-ters.
The entire document collection is typically represented using the vector space model [32] by a word-document ma-trix X  X  R m  X  n where columns index the documents and rows denote the words appearing in them. The documents are treated as vectors with words as their features such that an entry x fi in the matrix signifies the relevance of word f for document d i , usually by the frequency of the word appearing in the document.

We propose a semi-supervised NMF (SS-NMF) model for document clustering. NMF has received much atten-tion recently and proved to be very useful for applications such as pattern recognition, text mining, multimedia, and DNA gene expressions. It was initially proposed for  X  X arts-of-whole X  decomposition [23, 24], and later extended to a general framework for data clustering [10]. It can model widely varying data distributions and accomplish both hard and soft clustering simultaneously. When applied to the word-document matrix X , NMF factorizes X into two non-negative matrices [36], where P  X  R m  X  k is cluster centroid, Q  X  R n  X  k is cluster indicator, and k is the number of clusters.

In the proposed model, we perform symmetric non-negative tri-factorization of the document-document simi-larity matrix A = X T X  X  R n  X  n as, where G  X  R n  X  k is the cluster indicator matrix. An entry g ih in G gives the degree of association of document d i with cluster h . The cluster membership of a document is given by finding the cluster with the maximum association value. S  X  R k  X  k is the cluster centroid matrix that gives a compact k  X  k representation of X .

Supervision is provided as two sets of pairwise con-straints on the documents: must-link constraints C ML and cannot-link constraints C CL . Every pair of documents, ( d same cluster. Similarly, all possible pairs ( d i , d j )  X  implies that the two documents should belong to differ-ent clusters. The constraints are accompanied by associ-ated violation cost matrix W . An entry w ij in this matrix denotes the cost of violating the constraint between docu-ments d i and d j , if such a constraint exists, that is, either ( d a distortion measure D : R m  X  R , to compute distance between documents. Assuming the text corpus consists of k semantic concepts, the goal is to partition the set of doc-uments into k disjoint clusters { X h } k h =1 , such that the to-tal distortion between the documents and the corresponding cluster representatives is (locally) minimized according to the given distortion measure D , while constraint violations are kept to a minimum. We define the objective function of SS-NMF as follows: where A = A  X  W reward + W penalty is affinity or similar-ity matrix A with constraints W reward = { w ij | ( d i , d C
ML , s.t.y i C
CL , s.t.y i a constraint between documents d i and d j , and y i is the cluster label of d i . S  X  R k  X  k is the cluster centroid, and G  X  R n  X  k is the cluster indicator. Equation (3) can be re-written as: J We propose an iterative procedure for the minimization of equation (3) where we update one factor while fixing the others. The updating rules are, Thus, the SS-NMF algorithm for document clustering can be illustrated in Algorithm 1.
 Algorithm 1 SS-NMF Algorithm
INPUT: Document-document similarity matrix A , num-ber of clusters k , constraint penalty matrix W penalty , and constraint reward matrix W reward
OUTPUT: Clusters { X h } k h =1 with Y h = { i | d i  X  X h
METHOD:
We now prove the theoretical correctness and conver-gence of SS-NMF. Motivated by [29], we render the proof based on optimization theory, auxiliary function and several matrix inequalities. 3.3.1 Correctness First, we prove the correctness of algorithm. 1. Following the standard theory of constrained optimiza-2. Based on the Kuhn-Tucker complementarity condi-3. Applying the Hadamard multiplication on both sides 4. Based on the above two equations, we derive the pro-3.3.2 Convergence Next, we prove the convergence. This can be done by mak-ing use of an auxiliary function similar to that used in [23]. Due to space constraints, we give an outline of the proof and omit the details. 1. Assuming L ( S , S ) is an auxiliary function of J ( S 2. Similarly, assuming L ( G , G ) is an auxiliary
We now show that SS-NMF is a general and uni-fied framework for semi-supervised clustering by es-tablishing the relationship between SS-NMF and other well-known semi-supervised clustering algorithms, i.e., semi-supervised kernel k -means (SS-KK) [22] and semi-supervised spectral clustering with normalized cuts (SS-SNC) [19]. In fact, both these algorithms can be considered to be special cases of SS-NMF.

Proposition 1 . Orthogonal SS-NMF clustering is equiv-alent to SS-KK clustering.
 Proof . The SS-NMF objective function is, The equation can be written as, J SS  X  NMF = A  X  GSG T 2 = A  X  G G T 2 = Tr ( A T A  X  2 G T AG + G T G ) if let S = Q T Q and G = GQ T . Since Tr ( A T A + G
T G ) is a constant, the minimization of J becomes a maximization problem as, The SS-KK objective function is [22], where  X  (  X  ) is the kernel function and  X  h the centroid. Let E be the matrix of pairwise squared Euclidean distances among the data points, W the constraint matrix and G the cluster indicator. Equation (18) becomes the minimization of the following function, We can convert the minimization of equation (19) to a max-imization of the problem, where K = A + W and A the similarity matrix.

It is clear that the objective function of SS-NMF (equa-tion (17)) is equivalent to that of SS-KK (equation (20)) if K = A . The G in equation (17) represents the same clus-tering as G of equation (20) does.
Proposition 2 . Orthogonal SS-NMF clustering is equiv-alent to SS-SNC clustering.
 Proof . The objective function of SS-SNC is [19], J where A = A  X  W reward  X  W penalty is the pairwise similarity matrix with constraints, D = diag ( d 1 , ..., d the diagonal matrix, g h is the cluster indictor, scaled clus-ter indicator vector z h = D D
It can be shown that the minimization of equation (21) becomes a maximization problem as,
Also, it can be seen that equation (17) is equivalent to equation (22) if A =  X  A . Moreover, the G in equation (17) represents the same clustering as Z of equation (22) does. From the above two proofs, we can see that the SS-NMF, SS-KK, and SS-SNC are mathematically equivalent. How-ever, notice that in SS-NMF, the matrix A might have some negative values, which is not permitted in traditional NMF [23, 24]. In this case, one possible solution is to perform some normalization techniques to guarantee non-negative values. Alternatively, we can simply relax the non-negative constraint to allow negative values as in Semi-NMF [26]. In either of the approaches, the clustering result will not get affected. In SS-NMF, the cluster indicator G is near-orthogonal and can produce soft clustering results. The cluster centroid S can provide good characterization of the quality of data clustering because the residue of the ma-trix approximation J = min A  X  GSG T is smaller than J = min A  X  GG T . On the other hand, for SS-KK and SS-SNC, if input matrix is added with constraint weight W , in order to ensure positive definiteness, certain additive con-straints need to be enforced. Moreover, these constraints are difficult to be relaxed. Also, the cluster indicator G or Z is required to be orthogonal, leading to only hard clustering results. Hence, both SS-KK and SS-SNC can be viewed as special cases of SS-NMF with orthogonal space constraints. Thus, SS-NMF essentially provides a general and flexible mathematical framework for semi-supervised data cluster-ing. In this Section, we further illustrate the advantages of SS-NMF using a toy data set shown in Figure 1a, which follows an extreme distribution consisting of 20 data points forming two natural clusters: two circular rings with 10 data points each. Traditional unsupervised clustering methods, such as (kernel) k -means, spectral normalized cut or NMF, are unable to produce satisfactory results on this data set. However, after incorporating knowledge from the user in the form of constraints, we are able to achieve much better results.

Unlike SS-SNC, SS-NMF maps the documents into a non-negative latent semantic space. Moreover, SS-NMF does not require the derived space to be orthogonal. Figures 1b and c show the data distributions in the two spaces for SS-NMF and SS-SNC, respectively. Data points belong-ing to the same cluster are depicted by the same symbol. For SS-NMF, we plot the data points in the space of two column vectors of G , while for SS-SNC the first two singu-lar vectors are used. Clearly, in the SS-NMF space, every data point takes non-negative values in both the directions. Furthermore, in SS-NMF space, each axis corresponds to a cluster, and all the data points belonging to the same cluster are nicely spread along the axis. The cluster label for a data point can be determined by finding the axis with which the data point has the largest projection value. However, in the SS-SNC space, there is no direct relationship between the axes (singular vectors) and the clusters.

Table 1 shows the difference of cluster indicator between the hard clustering of SS-KK and soft clustering of SS-NMF. An exact orthogonality in SS-KK means that each row of cluster indicator G has only one nonzero element, which implies that each data object belongs to only 1 clus-ter. The near-orthogonality of cluster indicator G in SS-NMF relaxes this a bit, i.e., each data object could be-long fractionally to more than 1 cluster. This can help in knowledge discovery in the cases where the data point is evenly projected along the different axes. For instance, g 16 = { 0 . 1220 , 0 . 1233 } indicates that this data point may belong to any one of the two clusters.

SS-NMF uses an efficient iterative algorithm instead of solving a computationally expensive constrained eigen de-composition problem as in SS-SNC. The time complexity of SS-NMF is O ( tkn 2 ) where k is the number of clusters, n is the number of documents, and t is the number of it-erations. In fact, the time complexity is similar to that of the classical SS-KK clustering algorithm. However, com-pared to SS-KK, SS-NMF algorithm is simple as it only in-volves some basic matrix operations and hence can be easily deployed over a distributed computing environment when dealing with large data sets. Another advantage in favor of SS-NMF is that a partial answer can be obtained at interme-diate stages of the solution by specifying a fixed number of iterations.
In this Section, we empirically demonstrate the perfor-mance of SS-NMF in clustering documents by comparing it with well-established unsupervised and semi-supervised clustering algorithms.
We primarily utilize the data set used in [15] 2 . Data sets oh 0 and oh 5 are from OHSUMED collection [16], a sub-set of MEDLINE database, which contains 233 , 445 docu-ments indexed using 14 , 321 unique categories. Data set re is from Reuters-21578 text categorization collection Distri-bution 1.0 [25]. Data set Fbis is from the Foreign Broadcast Information Service data of TREC-5 [33].
For the experiments, we mixed some of the data sets mentioned above. Table 2 shows the details. These data sets were created as follows: 1. Classes Graf t -Survival and P hospholipids from 2. Data set England -Heart was created by mix-3. Interest -T rade was formed by mixing Interest and 4. We randomly selected 2, 3, 4, and 5 classes from Fbis We performed feature selection on the words according to [37] by retaining the top 10% of the words based on mutual information in each of the data sets.

We evaluate the clustering results using confusion matrix and the accuracy metric AC. Each entry ( i, j ) in the confu-sion matrix represents the number of documents in cluster i that belong to true class j . The AC metric measures how accurately a learning method assigns labels  X  y i to the ground truth y i , and is defined as, where n denotes the total number of documents in the ex-periment, and  X  is the delta function that equals one if  X  y = y anteed to find the global minimum, it is beneficial to run the algorithm several times with different initial values and choose one trial with a minimal objective value. In real-ity, usually a few number of trials is sufficient. In the case of NMF and k -means, for a given k , we conducted 20 test runs. 3 trials are performed in each of the 20 test runs and final accuracy value is the average of all the test runs.
We compare the performance of SS-NMF model on all the 7 data sets with the following 6 clustering methods: (1) k -means, (2) kernel k -means, (3) spectral normalized cuts, (4) NMF, (5) SS-KK, (6) SS-SNC. The first four methods are the most popular unsupervised data clustering methods, whereas SS-KK and SS-SNC are the representative semi-supervised ones. Through these comparison studies, we demonstrate the relative position of SS-NMF with respect to unsupervised and semi-supervised approaches to docu-ment clustering.

We first perform comparison of the 4 unsupervised clustering approaches with SS-NMF having pairwise con-straints on only 3% pairs of all the possible document pairs, which is ( total docs 2 ) . Each of the constraints were generated by randomly selecting a pair of documents. If both the documents have the same class label ( must-link ) , then the constraint is assigned maximum weight in the document-document similarity matrix. On the other hand, if they belong to different classes ( cannot-link ), then the mini-mum weight in the similarity matrix is used for the con-straint. For kernel k -means, we used a Gaussian (exponen-tial) kernel K ( x , y ) = exp(  X  x  X  y 2 / 2  X  2 ) , with vari-ance  X  =0 . 00001 for 2 clusters and  X  =0 . 01 for more than 2 clusters. In Table 3, we compare the algorithms on all the data sets using AC values. The performance of the first three methods is similar with NMF proving to be the best amongst the unsupervised methods. However, the ac-curacy of NMF greatly deteriorates and is unable to pro-duce meaningful results on data sets having more than 2 clusters. On the other hand, the superior performance of SS-NMF is evident across all the data sets. We can see that in general a semi-supervised method can greatly enhance the document clustering results by benefitting from the user provided knowledge. Moreover, SS-NMF is able to gen-erate significantly better results by quickly learning from the few pairwise constraints provided. Table 4, demon-strates the performance of SS-NMF when varying amounts of pairwise constraints were available a priori . We report the results in terms of the confusion matrix C and the clus-ter centroid matrix S . As the available prior knowledge in-creases from 0% to 5% , we can make the following two key observations. Firstly, the confusion matrices tend to become perfectly diagonal indicating higher clustering ac-curacy. Second observation pertains to the cluster centroid matrix S which represents the similarity or distance between the clusters. Increasing values of the diagonal elements of S indicate higher inter-cluster similarities. As expected, when the amount of prior knowledge available is more, the per-formance of the algorithm clearly gets better.

In Figure 2a, the sparsity pattern of a typical document-document matrix A = X T X ( England-Heart in the figure) before clustering is shown. The SS-NMF algorithm is ap-plied to the modified similarity matrix  X  A . Document clus-tering leads to re-ordering of the rows and columns of the matrix. Figures 2b and c, show the  X  A matrices for England-Heart and Fbis5 data sets after clustering with 5% pairwise constraints. Document clusters are indicated by the dense sub-matrices in these matrices.
We now compare SS-NMF with the other two semi-supervised clustering approaches. As before, for SS-KK, a Gaussian kernel was used. In Figures 3 and 4, we plot the AC values against increasing percentage of pairwise con-straints available, for the algorithms on all the data sets. On the whole, all three algorithms perform better as the per-centage of pairwise constraints increases. While the perfor-mance of SS-KK is close to that of SS-SNC on the data sets in Figure 3, it is clearly left out of the race completely in Figure 4. This is mainly because of the fact that SS-KK is unable to maintain its accuracy when producing more than 2 clusters. While, the performance of SS-SNC is head-to-head with SS-NMF on Fbis2 and Fbis3 , it is consistently outperformed by SS-NMF on the rest of the data sets. An-other noticeable fact is that the curve for SS-KK and SS-SNC might take a slow rise in some cases indicating that they need more amount of prior knowledge to improve the performance. Comparatively, SS-NMF gets better accuracy than the other two algorithms even for minimum percentage of pairwise constraints.
We presented SS-NMF: a semi-supervised approach for document clustering based on non-negative matrix factor-ization. In the proposed framework, users are able to pro-vide supervision in terms of must-link and cannot-link pair-wise constraints on the documents. We derived an itera-tive algorithm to perform symmetric tri-factorization of the document-document similarity matrix. We have proved that SS-NMF provides a general framework for semi-supervised clustering and that existing approaches can be consid-ered as special cases of SS-NMF. Empirically, we showed that SS-NMF outperforms 6 well-established unsupervised and semi-supervised clustering methods in clustering docu-ments using publicly available text data sets.
 This research was partially funded by the 21 st Century Jobs Fund Award, State of Michigan, under grant: 06-1-P1-0193.
