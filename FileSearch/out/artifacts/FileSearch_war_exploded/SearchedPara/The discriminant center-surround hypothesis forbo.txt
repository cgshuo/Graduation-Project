 The psychophysics of visual saliency and attention have bee n extensively studied during the last decades. As a result of these studies, it is now well known tha t saliency mechanisms exist for a number of classes of visual stimuli, including color, orien tation, depth, and motion, among others. More recently, there has been an increasing effort to introd uce computational models for saliency. One approach that has become quite popular, both in the biolo gical and computer vision communi-ties, is to equate saliency with center-surround differenc ing. It was initially proposed in [12], and has since been applied to saliency detection in both static i magery and motion analysis, as well as to computer vision problems such as robotics, or video com pression. While difference-based modeling is successful at replicating many observations fr om psychophysics, it has three signifi-cant limitations. First, it does not explain those observat ions in terms of fundamental computational principles for neural organization. For example, it implie s that visual perception relies on a linear measure of similarity (difference between feature respons es in center and surround). This is at odds with well known properties of higher level human judgments o f similarity, which tend not to be symmetric or even compliant with Euclidean geometry [20]. S econd, the psychophysics of saliency offers strong evidence for the existence of both non-linear ities and asymmetries which are not eas-ily reconciled with this model. Third, although the center-surround hypothesis intrinsically poses saliency as a classification problem (of distinguishing cen ter from surround), there is little basis on which to justify difference-based measures as optimal in a c lassification sense. From an evolutionary perspective, this raises questions about the biological pl ausibility of the difference-based paradigm. An alternative hypothesis is that all saliency decisions ar e optimal in a decision-theoretic sense . This hypothesis has been denoted as discriminant saliency i n [6], where it was somewhat narrowly proposed as the justification for a top-down saliency algori thm. While this algorithm is of interest only for object recognition, the hypothesis of decision the oretic optimality is much more general, and applicable to any form of center-surround saliency. Thi s has motivated us to test its ability to explain the psychophysics of human saliency, which is bette r documented for the bottom-up neural pathway. We start from the combined hypothesis that 1) botto m-up saliency is based on center-surround processing, and 2) this processing is optimal in a d ecision theoretic sense. In particular, it is hypothesized that, in the absence of high-level goals, the most salient locations of the visual field are those that enable the discrimination between cente r and surround with smallest expected probability of error. This is referred to as the discriminant center-surround hypothesis and, by definition, produces saliency measures that are optimal in a classification sense. It is also clearly tied to a larger principle for neural organization: that all perceptual mechanisms are optimal in a decision-theoretic sense.
 In this work, we present the results of an experimental evalu ation of the plausibility of the discrim-inant center-surround hypothesis. Our study evaluates the ability of saliency algorithms, that are optimal under this hypothesis, to both We derive decision-theoretic optimal center-surround alg orithms for a number of saliency problems, ranging from static spatial saliency, to motion-based sali ency in the presence of egomotion or even complex dynamic backgrounds. Regarding the ability to repl icate psychophysics, the results of this study show that discriminant saliency not only replicates a ll anecdotal observations that can be ex-plained by linear models, such as that of [12], but can also ma ke (surprisingly accurate) quantitative predictions for non-linear aspects of human saliency, whic h are beyond the reach of the existing approaches. With respect to practical saliency algorithms , they show that discriminant saliency not only is more accurate than difference-based methods in pred icting human eye fixations, but actu-ally produces background subtraction algorithms that outp erform the state-of-the-art in computer vision. In particular, it is shown that, by simply modifying the probabilistic models employed in the (decision-theoretic optimal) saliency measure -from w ell known models of natural image statis-tics, to the statistics of simple optical-flow motion featur es, to more sophisticated dynamic texture models -it is possible to produce saliency detectors for eit her static or dynamic stimuli, which are insensitive to background image variability due to texture , egomotion, or scene dynamics. A common hypothesis for bottom-up saliency is that the salie ncy of each location is determined by how distinct the stimulus at the location is from the stimuli in its surround (e.g., [11]). This hypoth-esis is inspired by the ubiquity of  X  X enter-surround X  mecha nisms in the early stages of biological vision [10]. It can be combined with the hypothesis of decisi on-theoretic optimality, by defining a classification problem that equates between center and surround . Mathematically, the feature responses within the two wind ows are interpreted as observations drawn from a random process X ( l ) = ( X d , conditioned on the state of a hidden random variable Y ( l ) . The observed feature vector at any location j is denoted by x ( j ) = ( x The saliency of location l , S ( l ) , is quantified by the mutual information between features, X , and class label, Y , The l subscript emphasizes the fact that the mutual information i s defined locally, within W function S ( l ) is referred to as the saliency map . Since human saliency has been most thoroughly studied in the domain of static stimuli, we first derive the optimal solution for discriminant saliency in th is domain. We then study the ability of the discriminant center-surround saliency hypothesis to e xplain the fundamental properties of the psychophysics of pre-attentive vision. 3.1 Feature decomposition The building blocks of the static discriminant saliency det ector are shown in Figure 1. The first stage, feature decomposition, follows the proposal of [11] , which closely mimics the earliest stages of biological visual processing. The image to process is firs t subject to a feature decomposition into into two color opponent channels, R  X  G for red/green and B  X  Y for blue/yellow opponency. These and the intensity map are convolved with three Mexican hat wavelet filters, centered at spatial frequencies 0 . 02 , 0 . 04 and 0 . 08 cycle/pixel, to generate nine feature channels. The featur e space X consists of these channels, plus a Gabor decomposition of th e intensity map, implemented with a dictionary of zero-mean Gabor filters at 3 spatial scales (ce ntered at frequencies of 0 . 08 , 0 . 16 , and 0 . 32 cycle/pixel) and 4 directions (evenly spread from 0 to  X  ). 3.2 Leveraging natural image statistics In general, the computation of (1) is impractical, since it r equires density estimates on a potentially high-dimensional feature space. This complexity can, howe ver, be drastically reduced by exploiting a well known statistical property of band-pass natural imag e features, e.g. Gabor or wavelet coeffi-cients: that features of this type exhibit strongly consistent patterns of dependence (bow-tie shaped conditional distributions) across a very wide range of clas ses of natural imagery [2, 9, 21]. The consistency of these feature dependencies suggests that th ey are, in general, not greatly informative about the image class [21, 2] and, in the particular case of sa liency, about whether the observed feature vectors originate in the center or surround. Hence, (1) can usually be well approximated by Since (2) only requires estimates of marginal densities, it has significantly less complexity than (1). This complexity can, indeed, be further reduced by resortin g to the well known fact that the marginal densities are accurately modeled by a generalized Gaussian distribution (GGD) [13]. In this case, all computations have a simple closed form [4] and can be mapped i nto a neural network that replicates the standard architecture of V1: a cascade of linear filterin g, divisive normalization, quadratic non-linearity and spatial pooling [7]. Figure 1: Bottom-up discriminant saliency detector. 3.3 Consistency with psychophysics To evaluate the consistency of discriminant saliency with p sychophysics, we start by applying the discriminant saliency detector to a series of displays used in classical studies of visual attention [18, 19, 14] 2 . In [7], we have shown that discriminant saliency reproduce s the anecdotal properties of saliency -percept of pop-out for single feature search, dis regard of feature conjunctions, and search asymmetries for feature presence vs. absence -that have pre viously been shown possible to replicate with linear saliency models [11]. Here, we focus on quantitative predictions of human performance, and compare the output of discriminant saliency with both hu man data and that of the difference-The first experiment tests the ability of the saliency models to predict a well known nonlinearity entation contrast, by comparing the conspicuousness of ori entation defined targets and luminance defined ones, and using luminance as a reference for relative target salience. He showed that the saliency of a target increases with orientation contrast, b ut in a non-linear manner: 1) there exists a threshold below which the effect of pop-out vanishes, and 2) above this threshold saliency increases with contrast, saturating after some point. The results of t his experiment are illustrated in Figure 2, which presents plots of saliency strength vs orientation co ntrast for human subjects [14] (in (a)), for discriminant saliency (in (b)), and for the difference-based model of [11]. Note that discrim-inant saliency closely predicts the strong threshold and sa turation effects characteristic of subject performance, but the difference-based model shows no such c ompliance.
 The second experiment tests the ability of the models to make accurate quantitative predictions of search asymmetries. It replicates the experiment designed by Treisman [19] to show that the asym-metries of human saliency comply with Weber X  X  law. Figure 3 ( a) shows one example of the displays used in the experiment, where the central target (vertical b ar) differs from distractors (a set of iden-tical vertical bars) only in length. Figure 3 (b) shows a scat ter plot of the values of discriminant saliency obtained across the set of displays. Each point cor responds to the saliency at the target location in one display, and the dashed line shows that, like human perception, discriminant saliency follows Weber X  X  law: target saliency is approximately line ar in the ratio between the difference of target/distractor length (  X  x ) and distractor length ( x ). For comparison, Figure 3 (c) presents the cor-responding scatter plot for the model of [11], which clearly does not replicate human performance. We have, so far, presented quantitative evidence in support of the hypothesis that pre-attentive vi-sion implements decision-theoretical center-surround sa liency. This evidence is strengthened by the Figure 3: An example display (a) and perfor-mance of saliency detectors (discriminant saliency (b) and [11] (c)) on Weber X  X  law experiment. already mentioned one-to-one mapping between the discrimi nant saliency detector proposed above and the standard model for the neurophysiology of V1 [7]. Ano ther interesting property of discrim-inant saliency is that its optimality is independent of the s timulus dimension under consideration, or of specific feature sets. In fact, (1) can be applied to any typ e of stimuli, and any type of features, as long as it is possible to estimate the required probability d istributions from the center and surround neighborhoods. This encouraged us to derive discriminant s aliency detectors for various computer vision applications, ranging from the prediction of human e ye fixations, to the detection of salient moving objects, to background subtraction in the context of highly dynamic scenes. The outputs of these discriminant saliency detectors are next compared with either human performance, or the state-of-the-art in computer vision for each application. 4.1 Prediction of eye fixations on natural images We start by using the static discriminant saliency detector of the previous section to predict human eye fixations. For this, the saliency maps were compared to th e eye fixations of human subjects in an image viewing task. The experimental protocol was that of [1], using fixation data collected from 20 subjects and 120 natural images. Under this protocol, all saliency maps are first quantized into a binary mask that classifies each image location as either a fi xation or non-fixation [17]. Using the measured human fixations as ground truth, a receiver oper ator characteristic (ROC) curve is then generated by varying the quantization threshold. Perf ect prediction corresponds to an ROC area (area under the ROC curve) of 1, while chance performanc e occurs at an area of 0.5. The predictions of discriminant saliency are compared to those of the methods of [11] and [1]. Table 1 presents average ROC areas for all detectors, across the entire image set. It is clear that discriminant saliency achieves the best performance among the three detectors. For a more detailed analysis, we also plot (in Figure 4) the ROC areas of the three detectors as a function of the  X  X nter-subject X  ROC area (a measure of the consistency of eye moveme nts among human subjects [8]), for the first two fixations -which are more likely to be driven by bo ttom-up mechanisms than the later ones [17]. Again, discriminant saliency exhibits the stron gest correlation with human performance, this happens at all levels of inter-subject consistency, an d the difference is largest when the latter is strong. In this region, the performance of discriminant s aliency ( . 85 ) is close to 90% of that of humans ( . 95 ), while the other two detectors only achieve close to 85% ( . 81 ). 4.2 Discriminant saliency on motion fields Similarly to the static case, center-surround discriminan t saliency can produce motion-based saliency maps if combined with motion features. We have impl emented a simple motion-based de-tector by computing a dense motion vector map (optical flow) b etween pairs of consecutive images, and using the magnitude of the motion vector at each location as motion feature. The probability distributions of this feature, within center and surround, were estimated with histograms, and the motion saliency maps computed with (2). Despite the simplicity of our motion representation, the di scriminant saliency detector exhibits in-teresting performance. Figure 5 shows several frames (top r ow) from a video sequence, and their discriminant motion saliency maps (bottom row). The sequen ce depicts a leopard running in a grass-land, which is tracked by a moving camera. This results in sig nificant variability of the background, due to egomotion, making the detection of foreground motion (leopard), a non-trivial task. As shown in the saliency maps, discriminant saliency successfully d isregards the egomotion component of the optical flow, detecting the leopard as most salient. 4.3 Discriminant Saliency with dynamic background While the results of Figure 5 are probably within the reach of p reviously proposed saliency models, they illustrate the flexibility of discriminant saliency. I n this section we move to a domain where traditional saliency algorithms almost invariably fail. T his consists of videos of scenes with com-plex and dynamic backgrounds (e.g. water waves, or tree leav es). In order to capture the motion patterns characteristic of these backgrounds it is necessa ry to rely on reasonably sophisticated prob-abilistic models, such as the dynamic texture model [5]. Suc h models are very difficult to fit in the conventional, e.g. difference-based, saliency framework s but naturally compatible with the discrim-inant saliency hypothesis. We next combine discriminant ce nter-surround saliency with the dynamic texture model, to produce a background-subtraction algori thm for scenes with complex background dynamics. While background subtraction is a classic problem in computer vision, there has been relatively little progress for these type of scenes (e.g. se e [15] for a review).
 A dynamic texture (DT) [5, 3] is an autoregressive, generati ve model for video. It models the spatial component of the video and the underlying temporal dynamics as two stochastic processes. A video is represented as a time-evolving state process x a linear function of the current state vector with some obser vation noise. The system equations are observation noise are given by v initial condition is distributed as x dynamic texture can be learned for the center and surround re gions at each image location, enabling a probabilistic description of the video, with which the mut ual information of (2) can be evaluated. We applied the dynamic texture-based discriminant salienc y (DTDS) detector to three video se-quences containing objects moving in water. The first (Water -Bottle from [23]) depicts a bottle floating in water which is hit by rain drops, as shown in Figure 7(a). The second and third, Boat and Surfer, are composed of boats/surfers moving in water, and s hown in Figure 8(a) and 9(a). These sequences are more challenging, since the micro-texture of the water surface is superimposed on a lower frequency sweeping wave (Surfer) and interspersed wi th high frequency components due to turbulent wakes (created by the boat, surfer, and crest of th e sweeping wave). Figures 7(b), 8(b) and 9(b), show the saliency maps produced by discriminant sa liency for the three sequences. The DTDS detector performs surprisingly well, in all cases, at d etecting the foreground objects while ig-noring the movements of the background. In fact, the DTDS det ector is close to an ideal background-subtraction algorithm for these scenes.
Figure 6: Performance of background subtraction algorithms on: (a) Water-Bo ttle, (b) Boat, and (c) Surfer. Figure 7: Results on Bottle: (a) original; b) discriminant saliency with DT; and c) GMM m odel of [16, 24]. For comparison, we present the output of a state-of-the-art background subtraction algorithm, a Gaussian mixture model (GMM) [16, 24]. As can be seen in Figur es 7(c), 8(c) and 9(c), the resulting surface. Note, in particular, that the waves produced by boa t and surfer, as well as the sweeping wave crest, create serious difficulties for this algorithm. Unlike the saliency maps of DTDS, the resulting foreground maps would be difficult to analyze by su bsequent vision (e.g. object tracking) modules. To produce a quantitative comparison of the salien cy maps, these were thresholded at a large range of values. The results were compared with ground -truth foreground masks, and an ROC curve produced for each algorithm. The results are shown in F igure 6, where it is clear that while DTDS tends to do well on these videos, the GMM based backgroun d model does fairly poorly.
Figure 8: Results on Boats: (a) original; b) discriminant saliency with DT; and c) GMM model of [16, 24].
Figure 9: Results on Surfer: (a) original; b) discriminant saliency with DT; and c) G MM model of [16, 24].
