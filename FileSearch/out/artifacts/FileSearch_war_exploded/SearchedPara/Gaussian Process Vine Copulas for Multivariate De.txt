 David Lopez-Paz david.lopez@tue.mpg.de Max Planck Institute for Intelligent Systems Jose Miguel Hern  X andez-Lobato jmh233@eng.cam.ac.uk Zoubin Ghahramani zoubin@eng.cam.ac.uk University of Cambridge Copulas are becoming a popular approach in machine learning to describe multivariate data (Elidan, 2012; Kirshner, 2007; Elidan, 2010; Wilson &amp; Ghahramani, 2010). Estimating multivariate densities is difficult due to possibly complicated forms of the data dis-tribution and the curse of dimensionality. Copulas simplify this process by separating the learning of the marginal distributions from the learning of the mul-tivariate dependence structure, or copula , that links them together into a density model (Joe, 2005). Learn-ing the marginals is easy and can be done using stan-dard univariate methods. However, learning the cop-ula is more difficult and requires models that can rep-resent a broad range of dependence patterns. For the two-dimensional case, there exists a large collection of parametric copula models (Nelsen, 2006). However, in higher dimensions, the number and expressiveness of families of parametric copulas is more limited. A solu-tion to this problem is given by pair copula construc-tions, vine copulas or simply vines (Bedford &amp; Cooke, 2002; Kurowicka &amp; Cooke, 2006). These are graphical models that decompose any multivariate copula into a hierarchy of bivariate copulas, where some of them will be conditioned on a subset of the data variables. The deeper a bivariate copula is in the vine hierarchy, the more variables it will be conditioned on. If the conditional dependencies described above are ignored, vines are a straightforward approach to construct flex-ible high-dimensional dependence models using stan-dard parametric bivariate copulas as building blocks. The impact of ignoring conditional dependencies in the copula functions is likely to be problem specific. Hobaek et al. (2010) show thorough experiments with synthetic data that, in specific cases, ignoring condi-tional dependencies can lead to reasonably accurate approximations of the true copula. By contrast, Acar, Genest and Neslehova (2012) indicate that this sim-plifying assumption can be in other cases misleading, and develop a method to condition parametric bivari-ate copulas on a single scalar variable. In this paper, we extend the work of Acar et al. (2012) and propose a general technique to construct arbitrary vine mod-els with full conditional parametric bivariate copulas. Our results on several real-world datasets show that it is often important to take into account conditional dependencies when constructing a vine model.
 The proposed method is based on the fact that most parametric bivariate copulas can be specified in terms of Kendall X  X  rank correlation parameter  X   X  [  X  1 , 1] (Joe, 1997). The dependence of the copula on a vec-tor of conditioning variables u = ( u 1 ,...,u d ) T is then captured by specifying the relationship  X  =  X  ( f ( u )), where f : R d  X  R is a non-linear function and  X  : R  X  [  X  1 , 1] is a scaling operation. We follow a Bayesian approach to learn f from available data. In particular, we place a Gaussian process (GP) prior on f and use expectation propagation for approximate in-ference (Rasmussen &amp; Williams, 2006; Minka, 2001). To make our method scalable, we use sparse GPs based on the generalized FITC approximation (Snelson &amp; Ghahramani, 2006; Naish-Guzman &amp; Holden, 2007). When the components of a d -dimensional random vec-tor x = ( x 1 ,...,x d ) T are independent, their density function p ( x ) can be factorized as The previous equality does not hold when x 1 ,...,x d are not independent. Nevertheless, the differences can be corrected by multiplying the right hand side of (1) by a specific function that fully describes any possi-ble form of dependence between the random variables x ,...,x d . This function is called the copula of p ( x ) (Nelsen, 2006), and satisfies: where P ( x i ) is the marginal cumulative distribution function (cdf) of the random variable x i . The copula c is the joint multivariate density of P ( x 1 ) ,...,P ( x and it has uniform marginal distributions, since P ( x )  X  U [0 , 1] for any random variable x (Casella &amp; Berger, 2001). This non-linear transformation from x to P ( x ) is known as the Probability Integral Transform (PIT). The copula is the density of x after eliminat-ing all the marginal information by applying the PIT to each individual component of x . Therefore, c de-scribes any dependence patterns which do not depend on the marginal distributions. If every P ( x i ) is contin-uous, then c is unique for any p ( x ) (Sklar, 1959). How-ever, infinitely many multivariate distributions share the same underlying copula (Figure 1).
 The main advantage of copulas is that they separate the learning of univariate marginal distributions from the learning of the multivariate dependence struc-ture that describes how they are coupled (Joe, 2005). Learning the marginals is easy and can be done using standard univariate methods. However, learning the copula is more difficult and requires models that can represent a broad range of dependence patterns. For the two-dimensional case, a large collection of para-metric copula models is available (Nelsen, 2006). Some examples are the Gaussian, Student, Clayton, Inde-pendent, Gumbel or Frank copulas. Each of these families describes a different dependence structure be-tween two random variables. An intuitive example is the copula that describes independence, that is, the independent copula: it has density constant and equal to one, as one can infer from equations (1) and (2). The Appendix contains more on the bivariate Gaus-sian Copula, which is used extensively used through-out this paper.
 Although there exist many parametric models for two-dimensional copulas, for more than two dimensions the number and expressiveness of families of paramet-ric copulas is more limited. A solution to this prob-lem is given by pair copula constructions, vine copulas or simply vines (Joe, 1996; Bedford &amp; Cooke, 2002; Kurowicka &amp; Cooke, 2006). 2.1. Regular Vines Vine copulas are hierarchical graphical models that factorize a d -dimensional copula density into a product of d ( d  X  1) / 2 bivariate conditional copula densities. They offer great modeling flexibility, since each of the bivariate copulas in the factorization can belong to a different parametric family. Several types of vines have been proposed in the literature. Some examples are canonical vines (C-Vines), drawable vines (D-vines) or regular vines (R-Vines). In this paper we focus on regular vines, since they are a generalization of all the other types (Dissmann et al., 2012).
 An R-vine V specifies a factorization of a copula den-sity c ( u 1 ,...,u d ) into a product of bivariate condi-tional copulas. Such R-vine is constructed by forming a nested set of d  X  1 undirected trees, in which each of their edges corresponds to a conditional bivariate cop-ula density. A particular nested set of trees identifies a particular valid factorization of c ( u 1 ,...,u d ). These trees can be sequentially constructed as follows: 1. Let T 1 ,...,T d  X  1 be the trees in a R-Vine V , each 2. Every edge e  X  E i has associated three sets of 3. The first tree in the hierarchy has set of nodes 4. For any edge e  X  E 1 joining nodes j,k  X  V 1 , 5. The i -th tree has node set V i = E i  X  1 and edge set 6. Edges e = ( e 1 ,e 2 )  X  E i have conditioned, con-Each of the edges in the trees T 1 ,...,T d  X  1 forming the vine V is a different factor in the factorization of c ( u 1 ,...,u d ), i.e. a different conditional bivariate cop-ula density. Since there are a total of d ( d  X  1) / 2 edges, V factorizes c ( u 1 ,...,u d ) as the product of d ( d  X  1) / 2 factors. We now show how to obtain the form of each of these factors. For any edge e ( j,k )  X  T i with condi-tioned set C ( e ) = { j,k } and conditioning set D ( e ) we define c jk | D ( e ) to be the bivariate copula density for u j and u k given the value of the conditioning variables { u i : i  X  D ( e ) } , that is, cdf of u j given the value of the conditioning variables { u i : i  X  D ( e ) } . Then, the vine V formed by the hierarchy of trees T 1 ,...,T d  X  1 specifies the following factorization for the copula density: as shown by Kurowicka &amp; Cooke (2006).
 Figure 2 exemplifies how to construct a regular vine that factorizes the copula density c ( u 1 ,u 2 ,u 3 ,u 4 the product of 6 bivariate conditional copula densities. The first tree T 1 has node set V 1 = { 1 , 2 , 3 , 4 } . The edge set E 1 is obtained by selecting a spanning tree over G 1 , the complete graph for the nodes in V 1 . Our choice for E 1 is highlighted in bold in the left-most plot in Figure 2. Edges in E 1 = { e 1 ,e 2 ,e 3 } have con-ditioned and constraint sets C ( e 1 ) = N ( e 1 ) = { 1 , 3 } , conditioning sets D ( e 1 ) = D ( e 2 ) = D ( e 3 ) = { X  X  . The second tree in the hierarchy has node set V 2 = E 1 . In this case, we select a spanning tree over G 2 , a graph with node set V 2 and edge set formed by pairs of edges e ,e j  X  E 1 sharing some common node v k  X  V 1 . We select E 2 = { e 4 ,e 5 } with conditioned sets C ( e 4 N ( e 1 ) X  N ( e 2 ) = { 1 , 2 } and C ( e 5 ) = N ( e 2 ) X  N ( e { 1 , 4 } , conditioning sets D ( e 4 ) = N ( e 1 )  X  N ( e and D ( e 5 ) = N ( e 2 )  X  N ( e 3 ) = { 3 } , and constraint sets D ( e 4 ) = N ( e 1 )  X  N ( e 2 ) = { 1 , 2 , 3 } and D ( e graph G 3 with node set V 3 = E 2 and only one edge e 6 . This last edge is the only possible spanning tree and has node set V 3 and edge set E 3 = { e 6 } . The edge e conditioning set D ( e 6 ) = N ( e 4 )  X  N ( e 5 ) = { 1 , 3 } and The resulting factorization of c ( u 1 ,u 2 ,u 3 ,u 4 ) given by the tree hierarchy is shown at the bottom of Figure 2. There exist many factorizations of a copula density c ( u 1 ,...,u d ) in terms of bivariate copulas. Each fac-torization is determined by the specific choices of the spanning trees T 1 ,...,T d in the algorithm described above. In practice, the trees are selected by assigning a weight to each edge e ( j,k ) (copula c jk | D ( e ) ) in the graphs G 1 ,...,G d  X  1 and then selecting the maximum spanning tree at each iteration. A common practice is to directly relate the weight of the edge e ( j,k ) to the amount of dependence described by the correspond-ing copula c jk | D ( e ) . This amount of dependence can be measured as the absolute value of the empirical Kendall X  X   X  correlation coefficient between the sam-tree can then be selected efficiently using Prim X  X  algo-rithm (Prim, 1957; Dissmann et al., 2012).
 On the first tree of a vine, only pairwise dependencies are described, and the corresponding copulas are not conditioned. The following trees describe dependen-cies between 3, 4, ... and d  X  1 variables by means of increasingly deeper conditioning, until completing a full description of the joint d  X  dimensional copula density. Since the cost of constructing the full tree hi-erarchy is quadratic in d , one may choose to prune the vine and construct only the first d 0 &lt; ( d  X  1) trees, ignoring the remaining copula densities in the factor-ization. Since the independent copula has pdf constant and equal to one, this pruning assumes independence in the higher order interactions captured by the ig-nored copulas (Brechmann et al., 2012) 2.2. Conditional Dependencies in Vines As shown in equations (3) and (4), vine distributions require to calculate marginal conditional cdfs and con-ditional bivariate copula densities. The number of variables to condition on increases as we move deeper in the vine hierarchy. In general, to obtain the fac-tors corresponding to the i -th tree, we have to condi-tion both copula densities and marginal cdfs to i  X  1 variables. The computation of the conditional cdfs ap-pearing at tree T i can be done using the copula func-tions from the previous tree T i  X  1 . In particular, the following recursive relationship holds D ( e ) \{ k } ) is the cdf of the conditional copula den-sity c jk | D ( e ) \{ k } and D ( e ) \{ k } denotes the condition-ing set D ( e ) with the element k removed (Joe, 1996). This derivative has well-known, closed-forms for each parametric copula (refer to the Appendix for the Gaus-sian copula case). However, we still have to compute the conditional bivariate copula densities. A solution commonly found in the literature is to assume that the copulas c jk | D ( e ) in (4) are independent of their condi-tioning variables. This is known as the simplifying as-sumption for vine copulas (Hobaek et al., 2010). The main advantage is that we can construct vine mod-els using standard unconditional parametric copulas. The disadvantage is that we may fail to capture some of the dependencies present in the data. As an alterna-tive to the simplifying assumption, we now present a general technique to construct conditional parametric bivariate copulas. In this section we address the estimation of the condi-tional copula of two random variables X and Y given a vector of conditioning variables Z = ( Z 1 ,...,Z d ) T  X  R . Let P X | Z and P Y | Z be the conditional cdfs of X and Y given Z . Patton (2006) shows that the condi-tional copula of X and Y given Z is the conditional distribution of the random variables U = P X | Z ( X | Z ) and V = P Y | Z ( Y | Z ) given Z . We assume a paramet-ric bivariate copula for the joint distribution of U and V . This type of copulas can often be fully specified in terms of Kendall X  X   X  rank correlation coefficient (Joe, 1997). Table 1 shows, for some widely-used copula families, the domain of their parameter  X  and the cor-responding bijective expressions for  X  as a function of Kendall X  X   X  . To capture the dependence of the copula on Z we introduce a latent function g : R d  X  [  X  1 , 1] such that  X  = g ( Z ). The task of interest is then to estimate g given observations of X , Y and Z . When P X | Z and P Y | Z are known, we can transform any sample of X , Y and Z into a corresponding sample of U , V and Z . Let D = {D U,V = { ( u i ,v i ) } n i =1 , D { z i } n i =1 } be such a sample, where u i and v i and z are paired. To guarantee that g ( x )  X  [  X  1 , 1], we as-sume w.l.o.g. that g ( x ) = 2 X ( f ( x ))  X  1, where  X ( x ) is the standard Gaussian cdf and f : R d  X  R is a non-linear function that uniquely specifies g . We can infer g by placing a Gaussian process prior on f and then computing the posterior for f given D (Rasmussen &amp; Williams, 2006). For this, let f be the n -dimensional vector such that f = ( f ( z 1 ) ,...,f ( z n )) T . The prior for f given D Z is where m is a n -dimensional mean vector and K is an n  X  n covariance matrix generated by the covariance function or kernel k ij  X  Cov[ f ( z i ) ,f ( z j )] where  X  is a vector of lengthscales and  X  ,  X  0 are am-plitude and noise parameters. Then, the posterior dis-tribution for f given D U,V and D Z is where p ( D U,V | f ) = Q n i =1 c ( u i ,v i |  X  = 2 X ( f p ( D U,V |D Z ) is a normalization constant and c (  X  ,  X |  X  ) is the density of a parametric bivariate copula speci-fied in terms of Kendall X  X   X  . Given a particular value of Z such as z ? , we can make predictions about the conditional distribution of U and V given z ? using p ( f ? | f , z ? , D z ) = N ( f ? | k T K  X  1 f ,k  X  k T K (Cov( f ( z ? ) ,f ( z 1 )) ,..., Cov( f ( z ? ) ,f ( z n Cov( f ( z ? ) ,f ( z ? )). Unfortunately, (8) and (9) cannot be computed analytically, so we decide to approximate them using Expectation Propagation (EP) (Minka, 2001). This method approximates each of the n factors in p ( D U,V | f ) with an unnormalized Gaussian distribu-tion whose mean and variance parameters are updated iteratively by matching sufficient statistics. See Ras-mussen &amp; Williams (2006) for further details. To refine each of these univariate Gaussians, we have to compute three unidimensional integrals using quadrature meth-ods. For prediction at z ? , we sample f ( z ? ) from the Gaussian approximation found by EP and then aver-age over copula models with  X  = 2 X ( f ( z ? ))  X  1. The resulting conditional copula model is semi-parametric: The dependence between U and V given Z is paramet-ric but the effect of Z on the copula is non-parametric. 3.1. Sparse GPs to Speed up Computations The total cost of EP is O ( n 3 ), since it is dominated by the computation of the Cholesky decomposition of an n  X  n matrix. To reduce this cost, we use the FITC approximation for Gaussian Processes (Snelson &amp; Ghahramani, 2006; Naish-Guzman &amp; Holden, 2007). Under this approximation, the n  X  n covariance matrix K is approximated by K 0 = Q + diag( K  X  Q ), where Q = K nn matrix generated by evaluating (7) at all combinations of some n 0 n training points or pseudo-inputs , and K nn 0 is the n  X  n 0 matrix with the covariances between all possible combinations of original training points and pseudo-inputs. These approximations allow us to run the EP method with cost O ( nn 2 0 ). The kernel hyper-parameters  X  ,  X  and  X  0 and the pseudo-inputs are optimized by maximizing the EP approximation of the model evidence (Rasmussen &amp; Williams, 2006). Learning a vine with our method scales linearly with the number of samples n , but quadratically with the number of pseudo-inputs n 0 and the number of vari-ables d : thus, its complexity is O ( d 2 n 2 0 n ). By contrast, learning a simplified vine has complexity O ( d 2 ). 3.2. Related Work Acar, Genest and Neslehova (2012) first addressed the lack of conditional dependencies in the parametric cop-ulas that form a vine model. They use a method sim-ilar in spirit to the one described above, and model  X  as a non-linear function of a single conditioning vari-able Z (Acar et al., 2011). However, their method cannot handle multivariate conditional dependences and consequently, they only show results for trivari-ate vines. They use the Maximum Local Likelihood (MLL) method to infer a non-linear relationship be-tween  X  and Z . Acar et al. approximate linearly f at any point z where f needs to be evaluated. Then, they adjust the coefficients of the resulting linear form using the available observations in the neighborhood of z . In particular, given a sample D = {D U,V = solving the optimization problem where the neighborhood of z is determined by the Epanechnikov kernel k h ( x ) = 3 bandwidth h . An estimate of f ( z ) is then obtained as the intercept of the linear approximation at z , that is, f ( z )  X  b ? z, 0 . Acar et al. adjust h by running a leave-one-out cross validation search on the training data. Some disadvantages of the MLL method are: (i) it can only condition on a single scalar variable, (ii) we have to solve the optimization problem (10) for each predic-tion that we want to make and more importantly (iii) since it is a local-based method (similarly as nearest neighbours) it can lead to poor predictive performance when the available data is sparsely distributed. We evaluate the performance of the proposed method for the estimation of vine copula densities with full conditional dependencies. Because GPs are an impor-tant part in this method we call it GPVINE. We com-pare with two benchmark methods: (i) a vine model based on the simplifying assumption (SVINE), which ignores any conditional dependencies in the bivari-ate copulas, and (ii) a vine model based on the MLL method of Acar et al. (2012) (MLLVINE). MLLVINE can only handle conditional dependencies with respect on a single scalar variable. Therefore, we can only eval-uate its performance in the construction of vine models with two trees, since additional levels would require to account for multivariate conditional dependencies. In all the experiments, we use 20 pseudo-inputs in the generalized FITC approximation. The Gaussian pro-cesses use a kernel function given by (7), whose hyper-parameters and pseudo-inputs are tuned by maximiz-ing the EP estimate of the marginal likelihood. The mean of the GP prior (6) is chosen to be constant and equal to  X   X  1 (( X   X  MLE + 1) / 2), where  X   X  MLE is the maximum likelihood estimate of  X  for an uncondi-tional Gaussian copula given the training data. In MLLVINE, the bandwidth of the Epanechnikov kernel is selected by running a leave-one-out cross validation search using a 30-dimensional log-spaced grid ranging from 0.05 to 10. To simplify the experiments, we fo-cus on regular vines generated using bivariate Gaus-sian copulas (see Appendix A) as building blocks. The extension of the proposed approach to select among different parametric families of bivariate copulas is straightforward: the best family for a given pair of variables can be selected by Bayesian model selection, using the evidence approximation given by EP. All the data are preprocessed to have uniform marginal distri-butions: this is done by mapping each marginal obser-vation to its empirical cumulative probability. 4.1. Synthetic Data We first perform a series of experiments with synthetic three-dimensional data. In particular, we sample the scalar variables X , Y and Z according to the follow-ing generative process. First, Z is sampled uniformly from the interval [  X  6 , 6] and second, X and Y are sampled given Z from a bivariate Gaussian distribu-tion with zero mean and covariance matrix given by Data Trees SGVINE GPVINE
Cloud
Glass
Jura
Shuttle
Weather
Stocks
Housing Var( X ) = Var( Y ) = 1 and Cov( X,Y | Z ) = 3 / 4 sin( Z ). We sample a total of 1000 data points and then choose 50 subsamples of size 100 to infer a vine model for the data using SVINE, MLLVINE and GPVINE. Average test log-likelihoods on the remaining data points are shown in Table 3. GPVINE obtains the best results. Figure 4 displays the true value of the function g that maps u 3 to  X  in the conditional copula c ( P ( u 1 | u 3 ) ,P ( u 1 | u 3 ) | u 3 , X  ), where u the empirical cumulative probability levels of the sam-ples generated for X , Y and Z , respectively. We also show the approximations of g generated by GPVINE and MLLVINE. In this case, GPVINE is much better than MLLVINE at approximating the true g . 4.2. Real-world Datasets We evaluate the performance of SVINE, MLLVINE and GPVINE on several real-world datasets. For each dataset, we generate 50 random partitions of the data into training and test sets, each containing half of the available data. The different methods are run on each training set and their log-likelihood is then evaluated on the corresponding test set (higher is better). The analyzed datasets are described in Section 4.2.1. Table 2 and Figure 3 show the test log-likelihood for SVINE and GPVINE, when using up to 1 ,..., ( d  X  1) trees in the vine, where d is the number of variables in the data. In general, taking into account possible dependencies in the conditional bivariate copulas leads to superior predictive performance. Also, we often find that the gains obtained get larger as we increase the number of trees in the vines. However, in a few of the datasets the simplifying assumption seems valid (Stocks and Jura datasets). Table 3 shows results for all methods (in-cluding MLLVINE) when only two trees are used in the vines. In these experiments, MLLVINE is most of the times outperformed by GPVINE. To better mea-sure the percent improvement experienced when us-ing GPVINE, one can subtract the achieved likelihood when using only the first tree of the vine from the all results. We also show how GPVINE can be used to discover scientifically interesting features through learning spatially varying correlations (Figure 5). 4.2.1. Description of the Datasets Mineral Concentrations The jura dataset con-tains the concentration measurements of 7 chemical elements (Cd, Co, Cr, Cu, Ni, Pb, Cn) in 359 loca-tions of the Swiss Jura Mountains (Goovaerts, 1997). The uranium dataset contains log-concentrations of 7 chemical elements (U, Li, Co, K, Cs, Sc, Ti) in a total of 655 water samples collected near Grand Junction, CO (Cook &amp; Johnson, 1986). Acar et al. (2012) use the measurements for Co , Ti and Sc to evaluate the performance of MLLVINE. We replicated this task for the three analyzed methods (Table 3).
 Barcelona Weather OpenWeatherMap (Extreme Electronics Ltd., 2012) provides access to meteorologi-cal stations around the world. We downloaded data for the 300 weather stations nearest to Barcelona, Spain (41.3857N, 2.1699E) on 11/19/2012 at 8pm ( weather dataset). Each station returns values for longitude , lat-itude , distance to Barcelona , temperature , atmospheric pressure , humidity , wind speed , wind direction and cloud cover percentage . Figure 5 shows how the poste-rior mean of  X  for the copula linking the variables at-mospheric pressure and cloud cover percentage varies when conditioned on latitude and longitude .
 World Stock Indices We apply the probability in-tegral transform to the residuals of an ARMA(1,1)-GARCH(1,1) model with Student t innovations. The residuals are obtained after fitting this model to the daily log-returns of the major world stock indices in 2009 and 2010 ( stocks dataset, 396 points in total) (Brechmann &amp; Schepsmeier, 2013). The indices are the US American S&amp;P 500 , the Japanese Nikkei 225 , the Chinese SSE Composite Index , the German DAX , the French CAC 40 and the British FTSE 100 Index . UCI Datasets We also include experimental results for the Glass , Housing , Cloud and Shuttle datasets from the UCI Dataset Repository (Frank &amp; Asuncion, 2010). Vine copulas are increasingly popular models for mul-tivariate data. They specify a factorization of any high-dimensional copula density into a product of con-ditional bivariate copulas. However, some of the con-ditional dependencies in these bivariate copulas are usually ignored when constructing the vine. This can produce overly simplistic estimates when dealing with real-world data. To avoid this, we presented a method for the estimation of fully conditional vines using Gaussian processes (GPVINE). A series of ex-periments with synthetic and real-world data show that, often, GPVINE obtains better predictive perfor-mance than a baseline method that ignores conditional dependencies. Additionally, GPVINE performs favor-ably with respect to state-of-the-art alternatives based on maximum local-likelihood methods (MLLVINE). DLP and JMLH contributed equally to this work. We thank P. Hennig, A. W. Wilson, C. Czado, N. Kr  X amer and E. C. Brechmann for their helpful feedback. DLP was funded by Fundaci  X on Caja Madrid and the PAS-CAL2 Network of Excellence. JMHL was funded by Infosys Labs, Infosys Limited.
 The bivariate Gaussian copula with correlation param-eter  X  represents the dependence structure found in a bivariate Gaussian distribution of two random vari-ables with correlation  X  . The Gaussian copula has cdf where  X  2 (  X  ,  X |  X  ) is the cdf of a bivariate Gaussian with marginal variances equal to one and correlation  X  , and  X   X  1 is the quantile function of the standard Gaussian distribution. The corresponding pdf is where  X  2 is the derivative (pdf) of  X  2 . The conditional cdfs are given by where  X  is the standard Gaussian cdf.
 Acar, E. F., Craiu, R. V., and Yao, F. Dependence calibration in conditional copulas: A nonparametric approach. Biometrics , 67(2):445 X 453, 2011.
 Acar, E. F., Genest, C., and Neslehova, J. Beyond sim-plified pair-copula constructions. Journal of Multi-variate Analysis , 110:74 X 90, 2012.
 Bedford, T. and Cooke, R. M. Vines X  X  new graphical model for dependent random variables. The Annals of Statistics , 30(4):1031 X 1068, 2002.
 Brechmann, E.C. and Schepsmeier, U. Modeling de-pendence with C-and D-vine copulas: The R pack-age CDVine. Journal of Statistical Software , 52(3): 1 X 27, 2013.
 Brechmann, E.C., Czado, C., and Aas, K. Truncated regular vines in high dimensions with applications to financial data. Canadian Journal of Statistics , 40 (1):68 X 85, 2012.
 Casella, George and Berger, Roger. Statistical Infer-ence . Duxbury Resource Center, 2001.
 Cook, R. D. and Johnson, M. E. Generalized bur-rparetologistic distributions with applications to a uranium exploration data set. Technometrics , 28: 123 X 131, 1986.
 Dissmann, J., Brechmann, E. C., Czado, C., and
Kurowicka, D. Selecting and estimating regular vine copulae and application to nancial returns. arXiv preprint , 2012.
 Elidan, G. Copula Bayesian networks. In Advances in Neural Information Processing Systems 23 , pp. 559 X 567, 2010.
 Elidan, G. Copulas and machine learning. Invited survey to appear in the proceedings of the Copulae in
Mathematical and Quantitative Finance workshop , 2012.
 Extreme Electronics Ltd. OpenWeatherMap, 2012. URL http://openweathermap.org/ .
 Frank, A. and Asuncion, A. UCI machine learning repository, 2010. URL http://archive.ics.uci. edu/ml .
 Goovaerts, P. Geostatistics for natural resources eval-uation . Oxford University Press, 1st edition, 1997. Hobaek, I., Aas, K., and Frigessi, A. On the simplified pair-copula construction. simply useful or too sim-plistic? Journal of Multivariate Analysis , 101(5): 1296 X 1310, 2010.
 Joe, H. Families of m -variate distributions with given margins and m ( m  X  1) / 2 bivariate dependence pa-rameters. Distributions with Fixed Marginals and Related Topics , 1996.
 Joe, H. Multivariate Models and Dependence Con-cepts . CRC Press, 1997.
 Joe, H. Asymptotic efficiency of the two-stage esti-mation method for copula-based models. Journal of Multivariate Analysis , 94(2):401 X 419, 2005.
 Kirshner, S. Learning with tree-averaged densities and distributions. In Advances in Neural Information Processing Systems 20 , 2007.
 Kurowicka, D. and Cooke, R. Uncertainty Analysis with High Dimensional Dependence Modelling . Wi-ley Series in Probability and Statistics, 1st edition, 2006.
 Minka, T. P. Expectation Propagation for approxi-mate Bayesian inference. Proceedings of the 17th
Conference in Uncertainty in Artificial Intelligence , pp. 362 X 369, 2001.
 Naish-Guzman, Andrew and Holden, Sean B. The gen-eralized FITC approximation. In Advances in Neu-ral Information Processing Systems 20 , 2007.
 Nelsen, R. An Introduction to Copulas . Springer Series in Statistics, 2006.
 Patton, A. J. Modelling asymmetric exchange rate dependence. International Economic Review , 47(2): 527 X 556, 2006.
 Prim, R. C. Shortest connection networks and some generalizations. Bell System Technology Journal , 36: 1389 X 1401, 1957.
 Rasmussen, C. E. and Williams, C. K. I. Gaussian
Processes for Machine Learning . The MIT Press, 1st edition, 2006.
 Sklar, A. Fonctions de repartition `a n dimension set leurs marges. Publ. Inst. Statis. Univ. Paris , 8(1): 229 X 231, 1959.
 Snelson, E. and Ghahramani, Z. Sparse Gaussian pro-cesses using pseudo-inputs. Proceedings of the 20th
Conference in Advances in Neural Information Pro-cessing Systems , pp. 1257 X 1264, 2006.
 Wilson, A. G. and Ghahramani, Z. Copula processes.
In Advances in Neural Information Processing Sys-
