 Educational research has demonstrated the effec-tiveness of reflection prompts (Boud et al., 2013) to enhance interaction between instructors and stu-dents (Van den Boom et al., 2004; Menekse et al., 2011). However, summarizing student responses to these prompts for large courses (e.g., introduc-tory STEM, MOOCs) is an onerous task for hu-mans and poses challenges for existing summa-rization methods. First, the linguistic units of stu-dent inputs range from single words to multiple sentences. Second, we assume that the concepts (represented as phrases) mentioned by more stu-dents should get more attention from the instruc-tor. Based on this assumption, we introduce the notion of student coverage , defined as the number of students who semantically mention a particular phrase. The more student coverage a phrase has, Table 1: An example reflection prompt, 53 stu-dent responses and a gold-standard summary. The numbers in the square brackets indicate the num-ber of students who semantically mention each phrase (i.e., student coverage). the more important it is. To illustrate the new task, an example is shown in Table. 1.

In this work, we propose a phrase summariza-tion method that addresses the above challenges. First, our summaries are created from extracted phrases rather than from sentences. Phrases are easy to read and browse like keywords, and fit bet-ter on small devices when compared to sentences. For example, including phrases such as  X  I didn X  X  fully understand  X  (S5) and  X  were confusing to me  X  (S8) in the summary is a waste of space. Second, we adopt a metric clustering paradigm with a se-mantic distance to estimate the student coverage of each phrase in the summary; a semantic metric allows similar phrases to be grouped together even if they are in different textual forms. Experimental results demonstrate the utility of our approach.
Although not the focus of this paper, we have also built a mobile application called CourseMIR-gorithm (Luo et al., 2015). Fan et al. (2015) report a preliminary study about the usage of the applica-tion. While summarization systems that extract sen-tences are dominant, others have published in  X  X ummarization X  at other levels besides the sen-tence. For example, Ueda et al. (2000) de-veloped an  X  X t-a-glance X  summarization method with handcrafted rules. Recently, keyphrase ex-traction (Hasan and Ng, 2014; Liu et al., 2009; Medelyan et al., 2009; Wu et al., 2005) has re-ceived considerable attention, aiming to select im-portant phrases from input documents, which is similar to phrase summarization. In this paper, we propose a general framework to adapt sentence summarization to phrase summarization.

Clustering has been used to score sentences and has shown good improvement in text summariza-tion (Yang et al., 2012; Li and Li, 2014; Gung and Kalita, 2012). In this work, we are using a metric clustering with semantic similarity to esti-mate the student coverage at a phrase level. Sim-ilarly, both diversity-based summarization (Car-bonell and Goldstein, 1998; Zhang et al., 2005; Zhu et al., 2007) and our proposed method aim to estimate and maximize student coverage by mini-mizing redundancy in the output phrases. Differ-ently, our method performs the redundancy reduc-tion at a cluster level (a group of phrases) rather than penalize redundancy with a greedy iterative procedure sentence by sentence, and not only the information content is considered, but also the in-formation source. Our data consists of student responses collected from 53 undergraduates enrolled in an introduc-tion to materials science and engineering class. The students were asked to complete a survey at the end of each of 25 lectures during a semester, consisting of three carefully designed reflection Table 2: W ord C ount (WC) in student responses (Student-WC), WC per phrase in TA X  X  summary (TA-PWC), WC in TA X  X  summary (TA-WC) and phrase count in TA X  X  summary (TA-PC) prompts: 1)  X  X escribe what you found most in-teresting in today X  X  class. X  2)  X  X escribe what was confusing or needed more detail. X  3)  X  X escribe what you learned about how you learn. X 
In total, more than 900 responses were collected for each prompt. Currently, gold-standard sum-maries of 12 out of 25 lectures are created by the teaching assistant for that course for each reflec-tion prompt. The summaries include not only the important phrases, but also the number of students who mentioned them (i.e., student coverage). 4 lectures are randomly selected as a development set and the remaining data used as a test set, yield-ing 12 sets of development data and 24 sets of testing data, each with a prompt, the students X  re-The statistics of the student responses and the TA X  X  summary are shown in Table 2. The phrases summarized by the TA are significantly shorter than the student responses (7.1 vs. 9.2, p &lt; 0.001). We formulate our task as a standard extrac-tive summarization problem. Unlike standard sentence-level extraction where the input and out-put are sentences, the input of our task ranges from words or phrases to full sentences. The output is a list of important phrases and the summary length (either # of phrases or words) is no more than L .
The proposed algorithm involves three stages: candidate phrase extraction , phrase clustering , and phrase ranking . 4.1 Candidate phrase extraction We extract noun phrases (NPs) from the input us-ing a syntax parser from the Senna toolkit (Col-lobert, 2011), preserving the most important con-tent from the original responses without losing too much context information compared to keywords. For example,  X  X he concept of thermal expansion X  (S5) is extracted as a candidate phrase. Only NPs are considered because all reflection prompts used in the task are asking about  X  X hat X , and knowl-
Due to the noisy data, malformed phrases are excluded, including single stop words (e.g.  X  X t X ,  X  X  X ,  X  X here X ,  X  X othing X ) and phrases starting with a punctuation mark (e.g.  X  X  X  X ,  X + indexing X ). 4.2 Phrase clustering Although phrases are more meaningful and less ambiguous compared to keywords, they suffer from the sparsity problem, especially in our data set when 89.9% of the phrases appeared only once. The challenge is the fact that students use different words for the same meaning (e.g.,  X  X icycle parts X  and  X  X ike elements X ).

We use a clustering paradigm with a seman-tic distance metric to address this issue. Among different clustering algorithms, K-Medoids (Kauf-man and Rousseeuw, 1987) fits well for our prob-lem. First, it works with an arbitrary distance matrix between datapoints. It allows to use pair-wise semantic similarity-based distance between phrases, yielding metric clustering. Second, it is robust to noise and outliers because it minimizes a sum of pairwise dissimilarities instead of squared Euclidean distances. It shows better performance than an LDA-based approach to group students X  short answers for the purpose of semi-automated grading (Basu et al., 2013). Since K-Medoids picks a random set of seeds to initialize as the clus-ter centers (called medoids), the clustering algo-rithm runs 100 times and the cluster with the min-imal within-cluster sum of the distances is retained to reduce random effects.

Distance metric . The semantic similarity is implemented using SEMILAR (Rus et al., 2013), using the latent semantic analysis trained on the Touchstone Applied Science Associates corpus (S  X  tef  X  anescu et al., 2014). The distance matrix D is constructed from the similarity matrix S by ap-plying the following transformation: D = e  X  S , which is similar to the common heat kernel but
Number of clusters . For setting the number of clusters without tuning, we adopted a method from Wan and Yang (2008), by letting K = K is the number of clusters and V is the number of candidate phrases instead of the number of sen-tences. 4.3 Phrase ranking In order to estimate the student coverage , phrases are clustered with the algorithm introduced above. We assume the phrases in a cluster are semanti-cally similar to each other and any phrase in a clus-ter can represent it as a whole. Therefore the cov-erage of a phrase is assumed to be the same as the coverage of a cluster, which is a union of the stu-dents covered by each phrase in the cluster.
To select the most representative phrase in a cluster, LexRank (Erkan and Radev, 2004), a graph-based algorithm for computing relative im-portance of textual units (working for both sen-tences and phrases), is used to score the extracted candidate phrases. The top ranked phrase in the cluster is added to the output summary. This pro-cess starts from the cluster that has the most es-timated student coverage and repeats for the next cluster until the length limit is reached.

Note that when the student coverage is the same between two clusters, the score of the top-ranked phrases in the clusters according to LexRank is used to break the tie: the higher, the better. We use the ROUGE evaluation metric (Lin, 2004) and report R-1 (unigrams), R-2 (bigrams), and R-SU4 (bigrams with skip distance up to 4 words), including the recall (R), precision (P) and F-Measure (F). These scores measure the over-lap between human-generated summaries and a machine-generated summary.

We design and compare a number of other summarization methods to evaluate the proposed phrase summarization approach.

Keyphrase extraction . Maui (Medelyan et al., 2009) is selected as the baseline, which is one of the state-of-the-art keyphrase extraction methods.
Sentence to phrase summarization . Existing sentence summarization techniques can be used for phrase summarization by extracting candidate phrases and treating them as sentences. Within this framework, we adapt MEAD (Radev et al., 2004) and LexRank (Erkan and Radev, 2004) to comparison (named as OriMEAD).

Diversity-based summarization . We ap-plied the MMR (Carbonell and Goldstein, 1998), a popular diversity-based summa-rization method as a post-processing step to the MEAD ( MEAD+MMR ) and LexRank ( LexRank+MMR ) baselines. 6
Clustering+Medoid . To show the performance using the clustering alone, this baseline selects the medoid phrase instead of using LexRank to rank the phrases in a cluster to form the summary.
Results . The performance on the test set is shown in Table 3 with the length limit L as 4 phrases (the average phrase number in the TA X  X  summary). Similar results can be observed when the length limit is based on the number of words, but cannot be reported here due to page limit.
First, our proposed method (last row), which clusters the extracted phrases and uses LexRank to score them, can outperform all the baselines over all three ROUGE scores in terms of F-measure. In addition, the proposed model performs better than the clustering and LexRank alone. Through a paired t -test, our model outperforms LexRank statistically in terms of precision for all three ROUGE scores and significantly improves Clus-tering+Medoid on all R-2 scores (except the pre-cision with 0.06 p-value). We believe that the semantic similarity based clustering complements LexRank in two ways: 1) LexRank depends on the cosine similarity of TF-IDF vectors to build the graph while the clustering takes semantic sim-ilarity into account. 2) The clustering performed a global selection to form a summary by group-ing similar phrases and ranking them by the num-ber of covered students (similar to what the hu-man did). Compared to LexRank, our approach captures the student coverage explicitly. While modifying LexRank by using semantic similarity is possible, estimating the student coverage is not straightforward.

Second, OriMEAD tends to select long sen-tences, resulting in a high recall but a low preci-sion. The phrase version (MEAD) improves both the P and F scores by removing unnecessary parts in the original sentences.
 Lastly, the proposed method outperforms the MMR based baselines on the precision and F-measure of all three ROUGE scores. We observed that the MMR baselines suffer from the issue of di-verse expressions used the students (e.g.,  X  X raphs X  and  X  X harts X ). In this paper, we presented a novel application to summarize student feedback to reflection prompts by a combination of phrase extraction, phrase clustering and phrase ranking. It makes use of metric clustering to rank the phrases by their student coverage, taking the information source into account. Experimental results demonstrate the good effectiveness of the model. While the proposed method improved the performance against MMR, other summarization methods with-out an additional MMR component do exist, in-cluding SumBasic (Vanderwende et al., 2007), KLSUM and TopicSUM (Haghighi and Vander-wende, 2009). An initial experiment shows they do not yield better performance with default pa-rameters. However, we will revisit it since these methods are meant for full sentences and are not optimized within the phrase framework.

In the future, we plan to have additional an-notation to evaluate the relative importance us-ing the student coverage numbers. We also de-ployed CourseMIRROR in a statistics class in Spring 2015 and have created gold-standard sum-maries, which will allow us to both replicate the intrinsic evaluation of this paper with a new and larger dataset as well conduct an extrinsic evalua-tion beyond ROUGE scores. Finally, we are inter-ested in applying our summarization approach to other types of user-generated content from mobile-applications (e.g., review comments).
 This research is partially supported by an internal grant from the Learning Research and Develop-ment Center at the University of Pittsburgh. We thank Muhsin Menekse for providing the data set. We thank Jingtao Wang and Xiangmin Fan for de-veloping the CourseMIRROR application and for valuable suggestions about the proposed summa-rization algorithm. We also thank anonymous re-viewers for insightful comments and suggestions.
