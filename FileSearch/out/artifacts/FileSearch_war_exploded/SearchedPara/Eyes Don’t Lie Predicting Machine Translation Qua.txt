 Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy , on an absolute scale (White et al., 1994). However, dif-ferent evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter-and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift to-wards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment.

The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hansen and Ji, 2010).

The overall difficulty of a sentence and its syn-tactic complexity affects reading behavior (Coco and Keller, 2015). Ill-formed sentences take longer to process, and may cause the reader to jump back while reading. Hence, by looking into how evalu-ators read the translations and their accompanying references, we can learn about: ( i ) the complexity of a reference sentence, and ( ii ) the quality of a trans-lation sentence.

Using reading patterns from evaluators could be a useful tool for MT evaluation: ( i ) to shed light into the evaluation process: e.g. the general reading be-havior that evaluators follow to complete their task; ( ii ) to understand which parts of a translation are more difficult for the annotator; and ( iii ) to develop semi -automatic evaluation systems that use reading patterns to predict translation quality.

In this paper, we make a first step towards ( iii ): us-ing reading patterns as a method for distinguishing between good and bad translations. Our hypothesis is that bad translations are difficult to read, which may be reflected by the reading patterns of the eval-uators. Motivated by the notion of reading diffi-culty, we extracted novel features from the evalua-tor X  X  gaze data, and used them to model and predict the quality of translations as perceived by evaluators. A perfectly grammatical sentence can be difficult to read for several reasons: unfamiliar vocabulary, complex syntactic structure, syntactic or semantic ambiguity, etc. (Harley, 2013). Reading automatic translations is even more challenging due to untrans-lated words, incorrect word order, morphological disagreements, etc. Cognitively processing difficult sentences generally results in modified reading pat-terns (Garrod, 2006; Coco and Keller, 2015).
In this paper, we analyze the reading patterns of human judges in terms of the word transitions (jumps), and the time spent on each word (dwell time); and use them as features to predict the quality score of a specific translation. For the sake of sim-plicity, as recommended by Guzm  X  an et al. (2015), we only consider a monolingual evaluation scenario and ignore the source text . However, our features and experimental setup can be extended to include source-side features. 2.1 Features Jump features While reading text, the gaze of a person does not visit every single word, but it ad-vances in jumps called saccades . These jumps can go forwards ( progressions ) or backwards ( regres-sions ). The number of regressions correlates with the reading difficulty of a sentence (Garrod, 2006; Schotter et al., 2014; Metzner, 2015). In an evalu-ation scenario, a fluent reading would mean mono-tonic gaze movement. On the contrary, the reader may need to jump back multiple times while reading a poor translation. We classify the word-transitions according to the direction of the jump and distance between the start and end words. For subsequent words n , n + 1 , this would mean a forward jump of distance equal to 1 . All jumps with distance greater than 4 were sorted into a 5+ bucket. Additionally, we separate the features for reference and translation jumps. We also count the total number of jumps. Total jump distance We additionally aggregate while evaluating a sentence. We have reference dis-tance and translation distance features. Again, the idea is that for a well-formed sentence, gaze distance should be less, compared to a poorly-formed one. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of transitions between reference and translation. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quality of a transla-tion (Doherty et al., 2010). Our feature counts the time spent by the reader on each particular word. We separate reference and translation features. Lexicalized Features The features discussed above do not associate gaze movements with the words being read. We believe that this information can be critical to judge the overall difficulty of the reference sentence, and to evaluate which transla-tion fragments are problematic to the reader. To compute the lexicalized features, we extract streams of reference and translation lexical sequences based on the gaze jumps, and score them using a tri-gram language model. Let R i = r 1 ,r 2 ,...,r m be a sub-sequence of gaze movement over reference and there are R 1 ,R 2 ,...,R n sequences, the lex feature is computed as follows: The normalization factor | R i | is used to make the probabilities comparable. We also use un-normalized scores as additional feature. A similar set of features lex ( T ) is computed for the transla-tions. All features are normalized by the length of the sentence. 2.2 Model For predicting the quality scores given by an eval-uator, we use a linear regression model with ridge regularization. The ridge coefficient  X   X  is the value of  X  that minimizes the error: Here the parameter  X  controls the amount of shrink applied to regression coefficients. A high value of  X  shrinks the coefficients close to zero (Hastie et al., 2001). We used the implementation provided in the glmnet package of R (Friedman et al., 2010), which inherits a cross-validation mechanism that finds the best value of  X  on the training data. We used a subset of the Spanish-English portion of the WMT X 12 Evaluation task. We selected 60 medium-length sentences which have been evalu-ated previously by at least 2 different annotators. For each sentence we selected the best and worst translations according to a human evaluation score based on the expected wins (Callison-Burch et al., 2012). As a result, we had 60 references with two corresponding translations each, adding up to a total of 120 evaluation tasks. Each evaluation task was performed by 6 different evaluators, resulting in 720 evaluations.

The annotators were presented with a translation-reference pair at a time. The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between. This was done to prevent any possi-ble spurious effects that may arise from remember-ing the content of a first translation, when evaluating the second translation of the same sentence. During each evaluation task, the evaluators were asked to as-sess the quality of a translation by providing a score between 0 X 100 (Graham et al., 2013). The observed inter-annotator agreement (Cohen X  X  kappa) among our annotators was 0.321. This is slightly higher than the overall inter-annotator agreement of 0.284 reading patterns we use the EyeTribe eye-tracker at a sampling frequency of 30Hz. Please refer to Ab-delali et al. (2016) for our Eye-Tracking setup and to know about iAppraise , an evaluation environment that supports eye-tracking. 3.1 Evaluation In our evaluation, we used eye-tracking features to predict the quality of a translation in a pairwise sce-nario in a protocol similar to the one from WMT X 12. First, we obtained the predicted scores  X  y k translations A and B when evaluated by evaluator k . Then, we computed the agreements w.r.t. the same pair of translations. That is, we considered an agreement when rankings were in order, e.g.  X  y k it a disagreement. Finally, we computed Kendall X  X  tau correlation coefficient as follows:  X  = agg  X  dis We evaluated the performance using a 10-fold cross-validation. While the folds were selected randomly, we ensured that all translations corresponding to the same sentence were included in the same fold, to prevent any overlap between train and test. In this section, we first analyze the results of co-herent feature sets to measure their predictive power and to validate the intuitions about the information they capture. Later, we use combination of features and assess their suitability as evaluation metrics. 4.1 Gaze as a translation quality predictor In Table 1, we show the results for the predictive models trained on different feature sets. For simplic-ity, we divide the feature groups in: reference only features (I), translation only features (II), translation and reference features (III); and lexicalized features (IV). In the last group, we also add a tri-gram lan-guage model scores for comparison purposes.
 Reference only features In section I of the ta-ble, we observe the prediction results for the mod-els that only used features from the references. Un-surprisingly, most of these features lack the predic-tive power to determine whether translation A is bet-ter than translation B (  X  from 0 . 06 to 0 . 13 ). One would expect that important phenomena that can be observed only on the reference (e.g. the overall dif-ficulty of the sentence), are neutralized in a pairwise setting, because an evaluator would read both in-However, some features like the dwell time (  X  = 0 . 13 ) yield better results than others. This could be explained by the need to go back to the reference, when reading a confusing translation, thus spending more time reading the reference.
 Translation only features In section II, we ob-serve the results for the translation features. At a first glance, we realize that the correlation results are much higher than for the reference features (  X  from 0 . 17 to 0 . 23 ). This supports the hypothesis that reading patterns can help to distinguish good from bad translations. Furthermore, it also supports specific intuitions about these reading patterns. For example, the fluency of a sentence is important (for-ward jumps,  X  = 0 . 17 ), but the number of regres-sions are better predictors of the quality of a sen-tence (  X  = 0 . 22 ). Additionally, the time spent read-ing a translation (dwell time) is a good predictor of the quality (  X  = 0 . 22 ). All of the above validate the intuition that reading patterns capture informa-tion about the quality of a translation. In general, using translation eye-tracking features in a pairwise evaluation, can help to predict which translation is better.
 Translation and reference features Reference and translation features are not independent. Inter-region jumps capture the number of times that eval-uators go between translation and references before making judgment. In section III, we observe that these features can be useful to predict the quality of a translation (  X  = 0 . 18 ).
 Lexicalized features In the last rows of the ta-ble, we show that reading patterns help to evaluate more than just the fluency of a translation. A simple language model score ( B LM ), is a weaker quality predictor (  X  = 0 . 17 ) than most of the eye-tracking translation features. Using the lexicalized version of the jump features gives additional predictive power (  X  = 0 . 22 ). Furthermore, by adding the total num-Table 1: Results of individual eye-tracking features based on reference region, translation region, inter-region and lexicalized information ber of jumps and backward jumps to the LM fea-tures, we would obtain a considerable gain in corre-lation (  X  = 0 . 30 ). This suggests that the reading patterns capture information about more than just fluency. 4.2 Gaze to build an evaluation metric So far, we X  X e shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency. One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi -automatically. That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation. Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (Papineni et al., 2002), which uses lexical information and is designed to measure not only fluency but also adequacy . In Table 2, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model. In (II) we re-introduce the results of lex-icalized jumps feature. In (III) we present results of BLEU and the combination of eye-tracking fea-tures with it. Finally in (IV) we present the human-to-human agreement measured in average Kendall X  X  tau and in max human-to-human Kendall X  X  tau. Combinations of translation jumps In section I we present several combinations of features. All of them include the backward jumps feature. This fea-ture provides predictive power (  X  = 0 . 22 ), which is orthogonal to other features. This is in line with our initial hypothesis that for a bad translation, an evaluator needs to go back and forth several times to understand it. Combining the backward jumps with the total number of jumps (CTJ 1 ) slightly increases the correlation to  X  = 0 . 25 . Adding the jump dis-tance (CTJ 2 ) also increases its  X  to 0 . 27 . While this correlation is lower than BLEU (  X  = 0 . 34 ), it does showcase the predictive power of the reading pat-terns.
 Combinations with BLEU When we combined BLEU with the translation jumps, we observed an increment in the  X  to 0 . 37 . Combining BLEU with the lexicalized jumps, yields the best combination (  X  = 0 . 42 ). Although moderate, these increments suggest that the reading patterns could be capturing additional phenomenon besides adequacy and flu-ency, such as structural complexity. These phenom-ena remain to be explored in future work.
 Human performance On average, evaluators agreements with each other are fair (  X  = 0 . 33 ) and below the best combination (CB 3 ), while the maxi-mum agreement of any two evaluators is relatively higher (  X  = 0 . 53 ). This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human. However, there is still room for improvement with respect to the most-agreeing pair of evaluators. Eye-tracking devices have been used previously in the MT research. Stymne et al. (2012) used eye-tracking to identify and classify MT errors. Table 2: Result of combining several jump and lex-icalized features with BLEU. The column Feature Sets shows the name of the systems whose features are combined for that particular run. We also in-cluded the average and maximum observed tau be-tween any two evaluators, as a reference.
 Doherty et al. (2010) conducted a study using eye-tracking for MT evaluation and showed correlation between fixations and BLEU scores. Doherty and O X  X rien (2014) evaluated the quality of machine translation output in terms of its usability by an end user. Guzm  X  an et al. (2015) used eye-tracking to show that having monolingual environment improves the consistency of the evaluation.
 Our work is different, as we: i) proposed novel eye-tracking features and ii) model gaze movements to predict human judgment. We have shown that the reading patterns detected through eye-tracking can be used to predict human judgments of automatic translations. To this end, we extracted novel lexicalized and non-lexicalized fea-tures from the eye-tracking data motivated by no-tions of reading difficulty, and used them to predict the quality of a translation. We have shown that these features capture more than just the fluency of a translation, and provide complementary informa-tion to BLEU. In combination, these features can be used to produce semi-automatic metrics with im-proved the correlation with human judgments.
In the future, we plan to extend our experiments to a large set of users and different language pairs. Ad-ditionally we plan to improve the feature set to take into account phenomena such as early termination , i.e. when an evaluator makes a judgment before fin-ishing reading a translation. We plan to deepen our analysis to determine what kind of information is be-ing used beyond fluency and adequacy.

