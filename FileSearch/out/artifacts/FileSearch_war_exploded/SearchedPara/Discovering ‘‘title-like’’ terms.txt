 1.Introduction
The problem of generating headlines or titles has been addressed in both the computational linguistic and information retrieval communities. The generation of headlines may be considered the ultimate form of automatic abstraction of a document, which is condensed into a phrase or a clause. Extractive summa-rization techniques ( Hahn &amp; Mani, 2000 ) were typically not use dsince it was expecte dthe hea dline coul d not be found from any phrase or clause in a document. Instead, statistical language models ( Banko, Mittal, &amp; Witbrock, 2000 ; Silber &amp; McCoy, 2002 ) were use dto generate hea dlines for Reuters news articles, base d on word co-occurrence statistics in the document and on sequential constraints in headlines.

In information retrieval, terms in titles an dhea dlines are important clues to the topic(s) to which the document belongs. Search engines heavily weight content terms in titles and headlines ( Filman &amp; Pena-
Mora, 1998 ). Others ( Berger &amp; Mittal, 2000 ) considered that a set of terms, instead of a phrase or a clause, woul dbe more representative for the titles of web pages because a web page may not be a coherent or self-contained document. Terms in titles have also demonstrated their utility for improving ( Chan et al., 2002 ) the automatic construction of concept hierarchies ( Sanderson &amp; Croft, 1999 ). Jin, Hauptmann, an dZhai (2002) used titles as a means to bridge the gap between queries and documents to enhance the language modeling aspect of cross-lingual information retrieval, thereby improving the effectiveness of retrieval.
Shou an dSan derson (2002) showe dhow to improve the effectiveness of merging retrieval lists using hea d-lines. Ko, Park, an dSeo (2004) use dtitle terms to enhance the text categorization performance.
Given the importance of terms in titles and headlines, it would be useful to identify other terms that could potentially be used in titles but were not selected due to the linguistic constraints of titles/headlines tasks discussed previously, such as constructing concept hierarchies, modeling the language of titles, cate-gorizing texts, weighting terms to retrieve information, as a gist of the web page content, etc.
Apart from titles, there have been substantial interests in discovering keyphrases ( Frank, Paynter, Wit-ten, Gutwin, &amp; Nevill-Manning, 1999 ; Jones &amp; Paynter, 2002 ; Turney, 2000 ) to assist information retrieval.
Keyphrases are typically keywords that exist after the abstract of a technical paper. Unfortunately, not all documents contain keyphrases, for instance newswire articles. On the other hand, many articles do contain titles, implicating using titles have wider applicability and/or complementing the work on the automatic extraction of keyphrases. Linguistically, keyphrases are mostly nouns or compoun dnouns, while titles may have more variety, like short sentences (e.g. punch line) as well as (compound) nouns. Most terms in titles are expecte dto occur in documents (as verifie dhere), where as keyphrases or keywor ds describing more general genre may have never occurred in the document. Due to these differences in linguistic con-straints between titles an dkeyphrases, as well as differences in the scope of applicability, it is worthwhile to explore the potential to discover title-like terms, whether to complement automatically or manually ex-tracte dkeyphrases or not.

A relate dbut different problem is text categorization where the classifier assigns the class label of a cat-egory to the document. Although the label can be considered as finding terms that relate to the topic and therefore the title, the class label is the same for all documents assigned to the same category. In addition, training documents have to be assigned to a set of pre-defined categories, which is not necessary for the problem of discovering title-like terms. Apart from the categories, many title terms are mentioned in the documents and therefore we believe that  X  X  X itle-like X  X  terms should be derived from terms in the document but not in the title. Essentially, text categorization is a recognition problem which is relate dto but different from the induction problem of discovering title-like terms.
 To fin d X  X  X itle-like X  X  terms, we propose to use a classifier, similar to automatic keyphrase extraction.
Although classifiers have been use dfor keyphrase extraction, our novel metho dology is to place more atten-tion on finding characteristics or properties of title terms occurring in documents because the methodology assumes that terms similar to those in titles shoul dbehave similarly in running text. Summary statistics an d graphs are used to display characteristics of title term behavior in text as a preliminary indicator of title term occurrences. These characteristics although similar are different from those for keyphrase extraction.
The classifier makes a binary decision for each term in the document, to determine whether or not it is a title-like term. This decision is based on a set of features that measure salient characteristics to support the decision. There are a variety of classifiers that can make such a decision, for example multi-layer perceptron ( Rumelhart &amp; McClelland, 1986 ), Bayesian classifiers ( Aeberhard, Coomans, &amp; Devel, 1994 ), support vector machine ( Joachims, 1998 ), etc. We use da decision-tree classifier, C4.5 ( Quinlan, 1996 ), because of its simplicity, versatility, ease of training, efficient decision-making capability and acceptable performance in similar text-processing problems, such as the detection of language-model errors ( Hung, Luk, Yeung,
Chung, &amp; Shu, 2000 ). Our focus in this research is not on the design of the classifier or the comparison of classifier performance, which are the establishe dareas of research in pattern recognition, but on discov-ering goo dor interesting features to support the decision to accept or reject the incoming term as a title-like term. An examination of those features relate dto the behavior of terms in titles can be useful or revealing to other language-processing tasks, such as abstraction or information retrieval. For instance, since Luhn  X  s seminal work ( Luhn, 1958 ), it has been assume dthat topical terms shoul dappear many times in a docu-ment. Interestingly, many terms in titles appeare donce in some (short) Reuters news articles.

Fin ding goo dfeatures is an elusive goal. Features are goo dif they yiel dgoo dclassification performance, an dthis is unknown until the features are define dan duse d. In pattern recognition, the general approach is to identify as many relevant features as possible. Good features are then selected from among a set of features for classification. The classifier, C4.5, has this capability by selecting the feature that yields the largest infor-mation gain, measure din the Shanon  X  s information theoretic sense. The set of features that we will explore are those that reflect the behavior of terms in the documents. For example, according to Luhn (1958) , topical terms are those that occur many times in a document. Hence, the occurrence frequency of a term may be an indicator of how likely that term is a title-like term. It is thus included in the feature set.
The pattern-recognition approach to finding  X  X  X itle-like X  X  terms was adopted for several reasons. First, the identification does not involve any linguistic constraints or probabilistic co-occurrence constraints to form-ing phrases or clauses, as in the generating of headlines. Second, the identification is more flexible in that the classifier can use features, which are not use din statistical extractive summarization techniques. Thir d, the classifier can combine different feature values to make a decision, instead of using some (perhaps arbi-trary) threshold to decide which terms to filter. Finally, our problem is to find more  X  X  X itle-like X  X  terms that are not in the title, instead of generating a headline for the document.

The rest of this paper is organize das follows. In Section 2, a number of features are propose dan deval-uated. Some features are based on past research in extractive summarization and headline generation since none of them can fin d X  X  X itle-like X  X  terms by themselves, demonstrating the nee dfor a classifier. The next section is a series of experiments to evaluate the effectiveness of finding  X  X  X itle-like X  X  terms using a classifier an dto tune the classifier for better classification performance. Since the evaluation is base don comparing the human judgment of  X  X  X itle-like X  X  terms, agreement with the human judgment of  X  X  X itle-like X  X  terms is also investigated. The last section contains the conclusion. 2.Features
In this section, we use d25,000 news articles from Reuters ( Lewis, 1997 ) to extract summary information (e.g. proportion of occurrences) in order to examine title term behavior in documents. These terms were stemme dusing the Porter stemming algorithm ( Porter, 1980 ). Stop words were filtered because we assumed that only content-bearing title terms were of interest. The average number of content-bearing terms (words) in a title was 7.0. The document size varied between 1 and 7 Kbytes, including the markup. Each document was a self-containe dnews article. Although the observe dresult may not be generalizable to some web pages, it is reasonable to use Reuters news articles in an exploratory study, assuming that their titles are of a high quality and representative of documents from push-based information service providers.
This examination of title term behavior in a document makes the important assumption that title and title-like terms shoul dappear in the document as well as the title. Such an assumption appears to be vali d most of the time because the proportion of (content-bearing) title terms that occur in the document is 94%. This proportion represents the best recall performance of any classifier in finding terms in the titles.
Base don the observe dtitle term behavior, we formulate features for classification. These features can be groupe dinto two major classes: distribution-base dan dlinguistic-base dfeatures. Distribution-base dfea-tures are measure dby some occurrence-counting mechanism (e.g., the location of the information ( Lin &amp; Hovy, 1997 ) in the document and the frequency of the term), which relates to the information used for extractive summarization techniques. Linguistic-base dfeatures are measure dby detecting some linguis-tic properties. Here, only two general types of linguistic features were examined; i.e., (a) the part-of-speech of the term an d(b) features base don some measure of sentence type or length ( Table 1 ). 2.1. Location-based features (f 1  X  X  4 )
It is likely that the title term will be foun din the first paragraph of the document because we observe d that 74% of the title terms occurre din the first paragraph. Hence, an obvious feature is to provi de infor-mation about where the term appeared. Three location features are used: a binary feature, f whether the term has occurre din the first paragraph, another binary feature, f an dthe final binary feature, f 3 , for the rest of the document (i.e., the body).

Another measure base don the location of the title term is the number of different places in which the title term is found. The different places are counted on the basis of the number of paragraphs. The measurement, calle dcoverage, denote das f 4 , is normalize dinto a percentage (i.e., the coverage equals the number of par-agraphs in which the title term appears, divided by the total number of paragraphs in the document). The rationale of this feature is that if the title term is a topical term, then it shoul dappear in many different places in the document. 2.2. Frequency-based features (f 5  X  X  8 )
In extractive summarization, term frequencies play an important role in identifying sentences to include in abstracts. Therefore, it is likely that term frequencies are an important indication of title terms. Origi-nally, we considered that terms occurring once were unlikely to be title terms. Filtering these terms would improve efficiency, reduce noise and thereby improve precision. However, to our surprise, a large propor-tion of title terms occurre donly once in the document in Fig. 1 . Hence, filtering terms that occurre donce woul dsubstantially affect the recall performance of the classifier. Therefore, terms that occurre donce in the document were not filtered.

In information retrieval, the classic TF-IDF scores ( Salton &amp; Buckley, 1998 ) are use dto rate the signif-icance of a term. For discovering title-like terms, we are dealing with a single document. Hence, each term in the document does not have a document frequency defined and the corresponding inverse document fre-quency (IDF) is also undefined. Therefore, the TF-IDF scores were not used in our problem of discovering title-like terms. 2.3. Document-size feature (f 9 )
The document size, which is measured as the number of paragraphs in the document, may affect the dis-tribution of terms in titles in a document. For example, a title term is likely to occur once if the document contains a few paragraphs. On the other hand, the likelihood of a title term occurring once in a large doc-ument with many paragraphs will be low. Fig. 2 shows the likelihoo dof fin ding terms in an dnot in titles occurring only once in the document, against the number of paragraphs in a document. As expected, as the document size increases, the likelihood of a title term occurring only once decreases. However, the likeli-hood of non-title terms occurring once also decreases. Comparatively, in long documents it would be about twice as likely to fin da non-title term that has occurre donce than a title term that has occurre donce. It is not easy to determine what is the best document size to disregard terms that occurred once. Therefore, the classifier is provided with a document-size feature so that it can decide on the combination of features that woul dignore the terms with a single occurrence. 2.4. POS-based feature (f 10 )
Due to linguistic constraints, only certain terms occur in a title. These linguistic constraints may manifest themselves as restrictions of terms with certain combinations of parts-of-speech. This gives rise to an un-even distribution of parts-of-speech occurrences in titles. Table 2 shows the likelihoo dof certain parts-of-speech tags foun din 25,000 titles; i.e., the number of titles with the part-of-speech tag p divided by the total number of titles. An error-transformation tagger ( Brill, 1995 ) is use dto assign parts-of-speech tags to each term. Interestingly, the most likely occurring part-of-speech tag in a title is a singular noun followed by verbs. 2.5. Sentence-based features (f 11  X  X  13 )
Intuitively, long sentences are usually use dto elaborate certain concepts. Since titles can be consi dere das condensing the document, it is likely that title terms are being elaborated in the long sentences. In order to show the relation between the title an dthe details of the article, long sentences can be measure din terms of the number of words (i.e., f 13 ) in a sentence, as well as whether the sentence contains some subordinate clauses for elaboration. Since there is no deep grammatical analysis of the sentences, the presence of com-mas (i.e., f 11 ) or quotation marks (i.e., f 12 ) is use das in dication of some form of elaboration.
Fig. 3 shows the proportion of sentences with commas (Type A) an dsentences with quotation marks (Type B), an dthe proportion of Type A an dType B sentences that contain title terms. The latter propor-tion can be considered as the relative frequency estimate of the likelihood of a title term occurring in those two types of sentences. An interesting result is that significant proportions, 65% an d25%, of documents contain Type A an dType B sentences, respectively. Another interesting result is that the likelihoo dof fin d-ing a title term in these types of sentences is high (above 85%). 3.Evaluation
In this evaluation, to demonstrate the potential utility of the proposed features, 10,000 documents were randomly sampled to form the training data from the 25,000 documents used in the previous sec-tion. Another 5000 documents disjoined from the training data (i.e., the 10,000 randomly sampled doc-uments) were randomly sampled from the remaining 15,000 documents to form the test data. The remaining 10,000 documents were not used. The C4.5 classifier was trained using the content words in the 10,000 titles of the training data to make binary decisions; i.e. as to whether the current term is or is not a title term.

The performance is measure dbase don the recall an dprecision of discovering a title term. Both recall an dprecision can be average dat the micro-level (i.e., average dover all terms) or at the macro-level (i.e., average dover all titles or documents). To express mathematically the recall an dprecision values, let the be C i . The micro-average recall, r , the macro-average recall, R , the micro-average precision, p , an dthe macro-average precision, P , are defined as follows: where ^ is the conjunction operation, card(.) returns the cardinality of a set and there are N documents in the test data. In recognition, the error rate can be used as a measure of performance. However, since there are many terms that are non-title terms, the likelihoo dof correctly rejecting a non-title term is high, inflat-ing the overall accuracy rate or deflating the overall error rate. Therefore, error rate is not used as a per-formance measure here. In addition, precision is not as significant as recall because the aim of this research is to discover new title-like terms that are terms not directly found in the titles. Therefore, lower precision is an acceptable performance, but high recall is needed to ensure that most, if not all, title-like terms are identified. 3.1. Balanced training
Initially, we use dtitle terms as positive examples an dall non-title terms as negative examples. Unfortu-nately, the performance was not goo d(as shown in Table 3 ). We suspect that there were many more neg-ative examples than positive examples, causing the C4.5 classifier to favor rejecting a term as a title term. To balance the training of positive an dnegative examples, the negative examples (i.e., unique non-title terms) were randomly chosen from the document, such that the number of negative examples is equal to the num-ber of positive examples (i.e., content-bearing title terms) in the document. Table 3 shows that substantial improvements were foun din the micro-average an dthe macro-average recall. Since the recall is upper-bounded by the proportion of title terms that occur in the document (i.e., 94%), the difference between the upper boun dan dthe attaine drecall by the classifier is just 11%, which is consi dere da goo dresult. Since our aim is to discover title-like terms, the low precision values do not cause much alarm. 3.2. Discovering title-like terms
The recall an dthe precision measures reflect the fact that the classifier is fin ding the title terms an dthat additional non-title terms are also identified. Since the precision is only 32%, about twice the number of non-title terms have been identified compared with title terms. These mis-identified non-title terms behave similarly to title terms in the training data. They may be title-like terms but they were not chosen because of linguistic constraints or the author  X  s choice.

In order to examine whether these mis-identified non-title terms are title-like, a group of seven subjects (who were computer science postgraduates or final year undergraduates) were asked to identify title terms in the document. To select the title terms, 210 randomly sampled documents from the 5000 documents in the test set were grouped into seven document sets. Each document set, consisting of 30 documents, was assigne dto a subject, who selecte dterms that he/she consi dere dshoul dbe in the title. The recall an dpre-cision of the identified title terms by the subject were measured by comparing them with the title terms of the corresponding documents. The micro-average recall and precision were measured, since the macro-average an dmicro-average performances of the classifier were similar.

Table 4 shows the recall an dprecision of the manual i dentification of title terms by each subject for 10 document subsets. Interestingly, there are substantial variations in recall (i.e., between 71% and 94%) and precision (i.e., between 19% and 66%) across different subjects. The average recall across different document subsets was 85%, very similar to the recall (i.e., 83%) attaine dby the classifier. The precision of manually identified title terms was 48%, which is substantially higher than the precision (i.e., 32%) attained by the classifier.

To examine whether the non-title terms identified by the classifier are acceptable to the subjects as title terms, each subject was aske dto rate the terms i dentifie dby the classifier, as well as the terms in the original title. The rating is between 1 an d5, where 1 means unacceptable an d5 means very acceptable. A rating of 3 is interprete das a  X  X  X air term to be in the title X  X  an da rating of two is interprete das  X  X  X ot much relate dto the theme of the article. X  X  The subjects were briefe dabout the interpretations of these ratings before the experiment.

In the experiment, the title terms form a kin dof control group to examine how subject reacte dto them compare dwith machine i dentifie dtitle-like terms. We expecte dthat the title terms have a higher rating that the machine identified title-like terms. By placing the title terms and the title-like terms together to the sub-ject, the tendency of subjects to accept more terms as title terms is reduced because subjects tend to balance their rating to be more evenly distributed. Hence, the title terms will receive the higher ratings while the title-like terms will receive a lower rating.

Fig. 4 shows the cumulative percentage of ratings for non-title terms identified by the classifier and for terms in titles. Clearly, terms in titles have a higher rating than non-title terms identified by the classifier, as expected. Therefore, we consider that the subjects did not (unconsciously) provide favorable ratings for the non-title terms. Considering that terms rated at 3 are acceptable, a term with a rating of more than 2.5 on average suggeste dthat the term was rate dmore acceptable than unacceptable. Therefore, terms rate don average to be larger than 2.5 were considered to be acceptable to most users. For title terms, over 95% were acceptable. For terms identified by the classifier, 58% were considered acceptable by the subjects. Therefore, the number of identified as title-like but non-title terms is about the same as the number of identified title terms.

To determine whether a figure of 58% for the terms identified as title or title-like can be considered good, we carrie dout an experiment to determine the average agreement of terms that are manually selecte dby seven subjects. Each subject was aske dto rea dthe same 30 documents so that the precision of the manually identified terms could be compared. Thirty documents were used. This was because, in estimating the pro-portion, this number can be considered as a binomially distributed random variable, which approaches the normal distribution when the number of samples approaches 30. In this case, one can use the standard devi-ation to determine the confidence limit of these proportions if desired.

Table 5 shows the confusion matrix where the manually identified set of terms for each subject was used as a reference set to be compare dwith the manually i dentifie dset of terms for every other subject. The var-iation of the precision is quite substantial (i.e., between 35% an d85%) because of differences among sub-jects. For example, subject 4 may be more restrictive or prudent in identifying title terms, leading to higher precision. On other hand, subject 7 may be more comprehensive or relaxed in identifying title terms, leading to lower precision. When the average precision was compute dfor all possible comparisons of i dentifie d terms by two different subjects, it was found to be 58.6%, which is close to the 58% precision of the terms identified by the classifier as title or title-like terms. 4.Conclusionandfuturework
This research used a pattern-recognition approach to identify  X  X  X itle-like X  X  terms. The underlying premise of why the identified terms are  X  X  X itle-like X  X  was that the identified terms behave similarly to the title terms observed in documents. This behavioral similarity was measured both in terms of the observed distribu-tional patterns of the title terms an din terms of the observe dlinguistic patterns of the title terms, in a large set of documents (i.e., 25,000 cases).

By means of balance dtraining, the recall of the i dentifie dterms by the classifier was goo d(i.e., 83%) an d similar to that achieved by the manual identification of title terms (i.e., 85%). As expected for discovering title-like terms, the precision of the identified terms by the classifier was much lower than for the manually identified title terms. However, when the subjects were asked to rate whether the terms identified by the classifier are from the title, then the proportion of title an dtitle-like terms ju dge dby the subjects among the terms identified by the classifier was 58%. This proportion is similar to the average precision (i.e., 58.6%) of manually identified terms among different subjects. Therefore, the classifier found the same number of title and title-like terms as did the manual identification of title and title-like terms. We conclude that the iden-tification of title-like terms using the pattern-recognition approach is encouraging, as almost all title terms (i.e., 83%) can be foun dan dhalf of the discovere dnon-title terms are rate das  X  X  X itle-like X  X  terms by the sub-jects. Our future work examines how to improve the ratings of the terms identified by the classifier by fil-tering terms that are considered to be highly unlikely candidates as title-like terms. A possible approach is to use another classifier to identify these unlikely terms.
 Acknowledgment We thank the reviewers for their insightful comments that have improve dthe paper.
 References
