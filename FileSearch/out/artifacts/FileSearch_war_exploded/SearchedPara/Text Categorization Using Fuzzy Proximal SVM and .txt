 Text categorization is the task of assigning a given text document to one or more predefined categories. It is gaining popula rity due to the increased availability of documents in digital form and the following need to access them in flexible ways. A variety of statistical and machine learning techniques have been applied to automate the TC system in recent past [1]. Among the entire machine learning methods, linear SVM achieved impressive performance for TC tasks [2-3]. In its simplest form, a linear SVM is a hyperplane that separates a set of positive examples from negative examples with maximum margin. The approach is systematic and motivated by statistical learning theory (SLT) and Bayesian arguments. 
Numerous linear SVM formulations and decomposition algorithms have been reported in literature. Among them linear PSVM proposed by Fung and Mangasarian code and just requires only the solution of a single system of linear equations. PSVM is obtained by changing inequality constraints to equality constraints in conventional SVM. This change, though simple, changes the nature of the problem such that the two bounding hyperplanes of SVM become proximal planes around which the datapoints of corresponding class are clustered. The Lagrangian function for the optimization problem is constructed and solved with KKT conditions to get the solution of PSVM. Using Sherman-Morrison-Woodbury (SMW) formula, Fung and Mangasarian [4] showed that the solution of linear PSVM can be obtained by just finding an inverse of size equal to the dimension of the classification problem. 
While critics argue linear PSVM may not be suitable for TC because of inherent high dimensionality of text documents [5], we bring forward linear PSVM together with recently proposed distributional cluste ring (DC) of words [6] to realize its potential in TC realm. DC is a word clusteri ng technique that drastically brings down the dimensionality of text documents. Bekkerman et al. [7] showed improved performance on 20newsgroups corpus by just using 300 word clusters over using 15,000 word features selected using mutual information (MI) feature selection measure. They used word clustering in a more general Information Bottleneck (IB) framework [7]. Thus word clusters greatly reduce the dimension of text documents without any significant compromise in classification performance. Hence, linear PSVM which can take advantage of this reduced dimension, is a ready choice for TC among other linear SVM formulations. In this paper, we present experimental results of linear PSVM and its fuzzy extension FPSVM proposed by Jayadeva et al. [8] with word clusters based document representation. We present experimental results comparing PSVM/FPSVM with linear SVM light [9] and SVM lin [10] on popular WebKB [11] text corpus. Through numerous experiments on subsets of WebKB, we reveal the merits of PSVM/FPSVM over other linear SVMs, like fast training, independence of training time with respect to category for which training is done, and independence of training time with respect to penalty parameter of SVM. We study the classification performance of all these algorithms in terms of standard information retrieval metrics like F1 measure, break even point (BEP) and testing accuracy. 
The rest of the paper is organized as follows: In Section 2 we describe text categorization preliminaries and document representation issues. Section 3 gives a brief overview of linear SVM, PSVM and FPSVM. Experimental results are explained and discussed in Section 4. Finally Section 5 discusses the conclusion and future work. Notations used in this paper: All vectors will be column vectors unless transformed to a row vector by a prime ' . A column vector of ones in real space of arbitrary dimension will be denoted by . e A column vector of zeros in a real space of arbitrary which is a row vector in .  X  n For a vector x  X  X  X  n , * x denotes the vector in n  X  with components () * 1 result of applying step function component-wise to x. Automatic TC is supervised learning problem which involves training a classifier with some labeled documents and then using the classifier to predict the labels of unlabeled documents. Each document may belong to multiple labels or single label or no label at all. Hence a binary classifier is learned for each label or category to form a complete text categorization system. During testing phase, each binary classifier will classify whether an incoming document belongs to its category or not. Below we will discuss the document representation issues. 2.1 Document Representation Documents which typically are string of characters, have to be transformed into a representation suitable for the learning algorithm of the classifier. This transformation involves several steps like preprocessing, dimensionality reduction, feature selection/extraction and term weighting. We used  X  bag of words  X  representation in all our experiments. We removed stop words using a stop word dictionary [12] consisting of 319 words to reduce the dimension. In addition to stop words, we removed words that occurred in only one document of the text corpus. We also performed word stemming using Porter stemmer algorithm [13]. After stemming and stop word removal, we obtained document representation using IB based distributional clustering of words [7], while conventional way is to obtain significant word features using feature selection measures like document frequency (DF), MI etc... IB based DC of words has been used in [7], and is suggested as an effective alternative to traditional feature selection measures. We used publicly available distributional clustering implementation with default parameter setting [14]. Following [7], after generating clusters we used term weighting based on word cluster count to represent each document. All these document representation steps were performed using MATLAB 7.3.0 (R2006b) [15] software. SVMs represent novel learning techniques that have been introduced in the framework of Structural Risk Minimization (SRM) and in the theory of VC bounds. In the simplest binary pattern recognition tasks, SVMs use a linear separating hyperplane to create a classifier with a maximal margin. Consider the problem of n -dimensional space of features is represented by a matrix . ln A  X   X  X  X  The diagonal matrix ll D  X   X  X  X  with entries ii D as 1 + or 1  X  . Given the above problem, SVM X  X  linear soft margin problem is to solve the following primal Quadratic Programming Problem (QPP): where C is the penalty parameter and y is a vector of nonnegative slack variables. The optimal separating hyperplane can be expressed as where n x  X  X  X  is any (new) datapoint, n w  X  X  X  and b  X  X  X  are parameters. Since the boundary described as () 0 dx = lies midway between the two bounding hyperplanes given by: and separates the two classes from each other with a margin of classified as class 1 + or 1  X  according to whether the decision function () yields 1 or 0 respectively. An important characteristic of SVM is that it can be extended in a relatively straightforward manner to create nonlinear decision boundaries [16]. However, we will restrict our discussion to linear SVM as it has been shown that linear SVM performs better than nonlinear SVM in TC [3]. Conventional training algorithms for SVMs are complex and slow. Standard quadratic optimizers cannot easily solve the immense size of dual of the QPP (1) that arises from SVMs. Many decomposition methods and new formulations have been proposed to address this issue. As a representative from the family of such decomposition methods, we will use SVM light [9] in our experiments for comparison. Further, special SVM formulations and algorithms exclusively for applications like TC that work with linear kernel have been proposed; As a representative we will use SVM lin [10] algorithm that uses modified Newton method along with conjugate gradient solver for fast training of linear SVMs. 3.1 Linear Proximal SVM PSVM [4] does binary classification by assigning data depending on its proximity to the two parallel hyperplanes that are pushed apart as far as possible. PSVM uses the modified primal QPP given by: 
The difference between primal QPPs (1) and (4) is that, QPP (4) minimizes the square of 2-norm of slack variables as opposed to 1-norm in QPP (1) and measures constraints of QPP (1) are changed to equalit y constraints in QPP (4). Note that the constraint 0 ye  X  doesn X  X  appear in (4) as the inclusion of 2 This simple change completely changes the nature of the formulation so that the bounding planes (3) now become proximal planes around which the datapoints of each class are clustered. By constructing the Lagrangian function and solving for its KKT conditions, Fung and Mangasarian [4] showed the solution of (4) to be: 
They used the popular Sherman-Morrison-Woodbury (SMW) formula to obtain this result. Thus PSVM algorithm is extremely simple and it needs only the solution of a single system of linear equations. Even a classification problem with millions of datapoints can be solved by just inverting a matrix of the order of input space dimension (plus one). In general, machine learning problems have input space dimension typically of the order 100 or less. However text documents will have very high dimension of the order of 10,000 or more. Because of this reason, TC researchers argue that PSVM may not be suitable for TC as it will need inversion of a very large matrix [5]. It is very true when we work with text documents without any dimensionality reduction. But, dimensionality reduction has always been an inherent constituent in text categorization; in fact researchers argue that dimensionality reduction often improves the classification performance. Hence, we take advantage of the recently proposed DC to achieve drastic reduction in dimension of the text documents, which will also aid PSVM. We used publicly available PSVM implementation [17] in all our experiments. 3.2 Linear Fuzzy Proximal SVM Often in real-world classification problems, some datapoints are more important than other datapoints. That is, datapoints no more exactly belong to one or more classes belong. Therefore a fuzzy membership, 0 1, i s  X  X  X  can be associated with each datapoint , i A indicating the extent to which the datapoint may be said to belong to its target class. In linear FPSVM [8], thes e membership values are used to assign relatively different penalty parameter for slack variables, instead of fixed penalty parameter in PSVM. FPSVM is obtained by solving the following optimization problem. where Y=Sy and S denotes a diagonal matrix, () 12 , ,..., , l Sdiagss s = whose diagonal entries are membership values of datapoints. Using SMW formula the solution of FPSVM has been shown to be: 
Thus the solution of FPSVM can also be obtained by solving a system of simultaneous linear equations similar to PSVM. We modified the PSVM implementation available at [8] to FPSVM and used in all our experiments. We performed all our experiments on WebKB corpus. Also called as four universities dataset, WebKB is a collection of Web pages from large number of university computer science departments [11]. The corpus contains 8,282 Web pages, manually classified into seven categories. Following previous studies [18], we used top four categories: course, faculty, project, and student, in all our experiments. This led us to a corpus with 4,199 documents and 11,521 word features. We generated 200 word clusters for document representation. It is to be noted that, all document representation steps like, preprocessing, dimensionality reduction and word clustering were done on the entire dataset before splitting them into training/testing sets. We created training and testing splits only during classification phase. We created many subsets of WebKB to carry out our experiments. A training set of size X means it will contain randomly selected X documents per category in training set and the remaining documents will be used for testing. We compared four linear SVM algorithms SVM light , SVM lin , PSVM and FPSVM in terms of computing time and standard information retrieval metrics F1 measure/BEP/accuracy measured on testing set. We calculated these performance measures separately for each of the four categories and then averaged (micro/macro) them to ge t final performance measures. As far as computing time is concerned we report total computing time over all four categories. For both SVM light and SVM lin the tolerance of stopping criteria was set to default value of 0.001. We used publicly available implementations of both these algorithms for our experiments. Default values for all parameters of these implementations were used unless otherwise stated. We conducted a grid search for best penalty parameter C in the range following [8], we present experimental results for a simple case wherein datapoints belonging to class 1 + are assigned a membership value s + and datapoints belonging to class 1  X  are assigned a membership value . s  X  For each category, we maximized micro BEP of FPSVM using best values of s + and s  X  obtained through search in the range 0.1 to 1 in steps of 0.1. For all implementations, after training SVM for a category, we fit a two parameter sigmoid trained with regularized binomial maximum likelihood [19], so that SVM outputs posterior probabilities. Using these posterior probabilities, a threshold is learnt for each category to maximize micro BEP. All experiments were implemented using MATLAB 7.3.0 (R2006b) [15] software on a PC with Intel Core2Duo processor (2.13GHz), 1GB RAM. 
Table 1 shows the micro BEP performance of all four algorithms on numerous subsets of WebKB corpus. Table 2 shows testing accuracy of all four algorithms on the same subsets. All these experimental results are average of three random trials on each subset. These two tables show that, PSVM performs better than SVM light and is comparable with SVM lin . However, FPSVM emerges as the best performer among all algorithms in terms of both micro BEP and testing accuracy. These observations are also reflected in other performance measures like, micro F1, macro F1, and macro BEP SVM light /SVM lin . This is because, in PSVM and FPSVM, the training times are dominated by dimensionality of the corpus with which we are working, thus making them ideal choice to combine with powerful dimensionality reduction methods. Figure 2 shows how these algorithms scale with respect to penalty parameter C . These results whereas the training time of PSVM/FPSVM are constant and are independent of penalty parameter as dimension size remains fixed. In fact we observed on large unbalanced datasets that the training time of PSVM/FPSVM are independent of the category for which training is done as dimension size remains fixed. 
Though fuzzy extensions are possible for other versions of SVMs as well, in view variations/category, FPSVM seems to be more attractive. In this paper we have investigated PSVM/FPSVM algorithms with distributional clustering of words for text categorization. Distributional clustering of words, being a powerful dimensionality reduction technique, drastically brings down the dimension of text documents to work with. PSVM/FPSVM algorithms take advantage of this reduced dimension leading to fast training with competitive results. The training time of PSVM/FPSVM is independent of the penalty parameter or the category for which training is done. We have compared PSVM/FPSVM with popular SVM light and SVM lin algorithms on numerous subsets of WebKB corpus. While PSVM gives comparable results, FPSVM outperforms all other algorithms in terms of BEP, F1 measure and testing accuracy. Thus our experiments demonstrate that, FPSVM combined with distributional clustering of words is a better choice for text categorization. Our future work is to explore the use of rough sets based dimensionality reduction techniques along with PSVM/FPSVM in text categorization. 
