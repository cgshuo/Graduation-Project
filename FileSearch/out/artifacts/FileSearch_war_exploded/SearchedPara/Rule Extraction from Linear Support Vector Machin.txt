 We describe an algorithm for converting linear support vec-tor machines and any other arbitrary hyperplane-based lin-ear classifiers into a set of non-overlapping rules that, un-like the original classifier, can be easily interpreted by hu -mans. Each iteration of the rule extraction algorithm is formulated as a constrained optimization problem that is computationally inexpensive to solve. We discuss various properties of the algorithm and provide proof of convergenc e for two different optimization criteria We demonstrate the performance and the speed of the algorithm on linear clas-sifiers learned from real-world datasets, including a medic al dataset on detection of lung cancer from medical images. The ability to convert SVM X  X  and other  X  X lack-box X  classi-fiers into a set of human-understandable rules, is critical n ot only for physician acceptance, but also to reducing the reg-ulatory barrier for medical-decision support systems base d on such classifiers.
 I.5.m [ Pattern Recognition ]: Miscellaneous Algorithms Rule extraction, Linear classifiers, Mathematical program -ming, medical decision-support.
Support Vector Machines (SVMs) [22, 11] and other lin-ear classifiers are popular methods for building hyperplane -based classifiers from data sets, and have been shown to have excellent generalization performance in a variety of applications. These classifiers, however, are hard to inter -pret by humans. For instance, when an unlabeled example is classified by the linear classifier as positive or negative , the only explanation that can be provided is that some lin-ear weighted sum of the variables of the example are lower (higher) than some threshold; such an explanation is com-pletely non-intuitive to human experts. Humans are more comfortable dealing with rules that can be expressed as a hypercube with axis-parallel surfaces in the variable spac e.
Previous work [20, 17] and more recent work [10] included rule extraction for neural networks but very few work has been done to extract rules from SVMs or any other kind of hyperplane-based classifier. Recently Nunez et al [15] proposed a method to extract rules from an SVM classi-fier which involves applying a clustering algorithm first to identify groups that later define the rules to be obtained.
We propose a methodology for converting any linear clas-sifier into a set of such non-overlapping rules. This rule set is (asymptotically) equivalent to the original linear clas si-fier, covers most of the training examples in the hyperplane halfspace. Unlike [15] our method does not require compu-tationally expensive data preprocessing steps (as cluster ing) and the rule extraction is done in a very fast manner, typi-cally it takes less than a second to extract rules from SVM X  X  trained on thousands of samples. Our algorithm does not required anything more complicated that solving simple lin -ear programming problems in 2 n variables where n is the number of input features (after feature selection).
In the next section we briefly discuss the medical rele-vance of this research. The ability to provide explanations of decisions reached by  X  X lack-box X  classifiers is not only i m-portant for physician acceptance, but it is also a vital step in potentially reducing the regulatory requirements for in -troducing a medical decision-support system based on such a classifier into clinical practice. Section 3 then describe s the commonly used linear support vector machine classifier and gives a linear program for it. Section 4 provides our rule extraction algorithm; Each iteration of the rule extractio n algorithm is formulated as one of two possible optimization problems based on different  X  X ptimal X  rule criteria. The firs t formulation, which seeks to maximize the volume covered by each rule, is a constrained nonlinear optimization problem whose solution can be found by obtaining the closed form solution of a relaxed associated unconstrained problem. Th e second formulation, which maximizes the number of samples covered by each rule, requires us to solve a linear program-ming problem. In Section 5 we discuss finite termination and convergence conditions for our algorithm. Section 6 summarizes our results on 4 publicly available datasets, an d an additional medical dataset from our previous work [3] in building a CAD system to detect lung cancer from computed tomography volumes. We conclude in Section 7 with some thoughts on further extensions and applications.

We now describe the notation used in this paper. The notation A  X  R m  X  n will signify a real m  X  n matrix. For such a matrix, A  X  will denote the transpose of A and A i will denote the i -th row of A . All vectors will be column vectors. For x  X  R n , k x k p denotes the p -norm, p = 1 , 2 ,  X  . A vector of ones in a real space of arbitrary dimension will be denoted by e . Thus, for e  X  R m and y  X  R m , e  X  y is the sum of the components of y . A vector of zeros in a real space of arbitrary dimension will be denoted by 0. A separating hyperplane , with respect to two given point sets A and B , is a plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B . A bounding plane to the set A is a plane that places A in one of the two closed halfspaces that the plane generates. The symbol  X  will denote the logical  X  X nd X  and the symbol  X  will denote the logical  X  X r X . The abbreviation  X  X .t. X  stands for  X  X uch that X . For a vector x  X  R n , the sign function sign ( x ) is defined as sign ( x ) i = 1 if x i &gt; 0 else sign ( x ) x  X  0, for i = 1 , . . . , n . 2. MEDICAL RELEVANCE
From the earliest days of computing, physicians and sci-entists have explored the use of artificial intelligence sys -tems in medicine [18]. A long-standing area of research has been building computer-aided diagnosis (CAD) systems for the automated interpretation and analysis of medical im-ages [16]. Despite the demonstrated success of many such systems in research labs and clinical settings, these syste ms were not widely used, or even available, in clinical practic e. The primary barrier to entry in the United States is the re-luctance of the US Government to allow the use of  X  X lack box X  systems that could influence patient treatment.
Although the Food and Drug Administration (FDA) has recently granted approval for CAD systems based on  X  X lack-box X  classifiers [19], the barrier to entry remains very high . These systems may only be used as  X  X econd-readers X , to offer advice after the initial physician diagnosis. More sig -nificantly, these CAD systems must receive pre-market ap-proval (PMA). A PMA is equivalent to a complete clinical trial (similar to the ones used for new drugs), where the CAD system must demonstrate statistically significant im-provement in diagnostic performance when used by physi-cians on a large number of completely new cases. This is a obviously a key area of research in CAD, but not the fo-cus of this paper. The FDA has indicated that the barrier to entry for CAD systems that are able to explain their conclusions, could be significantly lowered. Note, this wil l not lower the barrier in terms of generalization performanc e on unseen cases, but the FDA is potentially willing to con-sider using performance on retrospective or previously see n cases and significantly reduce the number of cases needed for a prospective clinical trial. This is critical, because a full-blown clinical trial can add several years delay to the release of a CAD system into general clinical practice.
Much research in the field of artificial intelligence, and now knowledge discovery and data mining has focused on the endowing systems with the ability to explain their rea-soning, both to make the consultation more acceptable to the user, and to help the human expert more easily identify errors in the conclusion reached by the system [4]. On the other hand, when building classifiers from (medical) data sets, the best performance is often achieved by  X  X lack-box X  systems, such as, Support Vector Machines (SVMs). The research described in this paper will allow us to use the su-perior generalization performance of SVM X  X  and other linea r hyperplane-based classifiers in CAD system, and using the explanation features of the rule extraction algorithm to re -duce the regulatory requirements for market introduction o f such systems into daily clinical practice. 3. HYPERPLANE CLASSIFIERS: 1 -NORM
We consider the problem of classifying m points in the n -dimensional input space R n , represented by the m  X  n matrix A , according to membership of each point A i in the class A + or A  X  as specified by a given m  X  m diagonal matrix D with plus ones or minus ones along its diagonal. For this problem, depicted in Figure 1, the linear programming support vector machine [11, 5] with a linear kernel (this is a variant of the standard SVM [22, 6]) is given by the following linear program with parameter  X  &gt; 0: where kk 1 denotes the 1-norm as defined in the Intro-duction. That this problem is indeed a linear program, can be easily seen from the equivalent formulation: For economy of notation we shall use the first formulation (1) with the understanding that computational implementation is via (2).

If the classes are linearly inseparable, which is often the case in real-world datasets, then two planes bound the two classes with a  X  X oft margin X  (i.e. bound approximately with some error) determined by the nonnegative error variable y , that is:
The 1-norm of the error variable y is minimized paramet-rically with weight  X  in (1), resulting in an approximate separating plane. This plane classifies data as follows: where sign ( ) is the sign function defined in the Introduc-tion. Empirical evidence [5] indicates that the 1-norm for-mulation has the advantage of generating very sparse solu-tions. This results in the normal w to the separating plane x w =  X  having many zero components, which implies that many input space features do not play a role in determining the linear classifier. This makes this approach suitable for feature selection in classification problems. Since our rul e extraction algorithm depends directly on the features used Figure 1: The LP-SVM classifier in the w -space of R n . The plane of equation (3) approximately sepa-rating points in A + from points in A  X  . Figure 2: Two-dimensional example where the non-overlapping rules covering the halfspace ( { x s.t. w  X  x &lt;  X  } ) are represented as cyan rectangles by the hyperplane classifier, sparser normal vectors w will lead to rules depending on a fewer number of features. 4. RULE EXTRACTION FROM HYPERPLANE
In the previous section, we described a linear program-ming SVM formulation to generate hyperplane classifiers. We now present an algorithm to extract rules of the form: to approximate these classifiers. Note that every rule form defined above defines an hypercube in the n dimensional space with edges parallel to the axis. Rule of this form are very intuitive and can be easily interpreted by humans.
Our rule extraction approach can be applied to any linear classifier regardless of the algorithm or criteria used to co n-struct the classifier, including Linear Fisher Discriminan t (LFD) [13], Least squares SVM X  X  (LS-SVMs) [21] or Proxi-mal SVMs (PSVM) [7]. Denote by P  X  ( w,  X , I ) the problem of constructing rules for the classifier for the region: based on the classification hyperplane w  X  x =  X  obtained by solving problem (1). Note that the problem of rule extrac-tion P + ( w,  X , I  X  ) where is the same as P  X  (  X  w,  X   X , I ). We now establish that this is equivalent to solving the problem with positive hyperplane coefficients,  X  = 1 and the feature domain being the unit hypercube. Consider a diagonal matrix T constructed in the following way: and a vector b with components b = { u i if w i &lt; 0 , l We now define a transformation of coordinates such that y = T ( x  X  b ). Note that w i &gt; 0  X  0  X  y i = T ii ( x i  X  l i ) = w i &lt; 0  X  0  X  y i = T ii ( x i  X  u i ) = hence, I is transformed to [0 , 1] n . Furthermore x = T  X  1 and hence, the hyperplane of interest becomes which is equivalent to: Thus the problem becomes P  X  (  X  w, 1 , I 0 ) in the new domain, where I 0 = [0 , 1] n . 1 Note that the components of  X  w are positive as w  X  b &lt;  X  and w i T ii &gt; 0.

For the rest of this paper we will concentrate in finding rules with the following properties:
In mapping the original problem to the unit hypercube the measure of volume is merely a scaled version of the original problem, and thus the optimum remains the same.
Figure 2 illustrates an example in two dimensions where the halfspace w  X  x &lt;  X  is almost totally covered by rules represented by hypercubes with a vertex in the hyperplane w x  X   X  = 0.

Given a region I we can define the  X  X ptimal X  rule accord-ing to different criteria, Next we present two of them.
An optimal rule can be defined as the rule that covers the hypercube with axis-parallel faces with the largest pos -sible volume. Since the log function is strictly increasing , arg max f ( x ) = arg max log ( f ( x )), we can find the rule that maximizes the log of the volume of the region that it en-closes (instead of the volume). Assuming that the linear transformation T was already applied and that one corner of the region lies on the hyperplane, this rule can be found by solving the following problem: The Lagrangian function for this nonlinear constrained op-timization problem is: L ( x,  X ,  X  ) = log(
The KKT optimality conditions for problem 8 are given by:
In order to find a solution for problem (8) we will first consider solutions for the relaxed equality constrained pr ob-lem: The KKT optimality conditions for problem (11) (which are very similar to the KKT conditions of problem (8)) are given by:
From the KKT optimality conditions (12) we obtained the following closed form solutions for the relaxed optimizati on problem:
A solution x  X  of the original optimization problem (8) can be obtained from the solution (13). Let X  X  define x  X  as follows: x Where, where A = { i |  X  x i &gt; 1 } and n I is n  X  X  A | . with  X  above we have that: wx  X   X   X  = if 0  X  x  X  i  X  1 ,  X  i  X  { 1 , . . . , n } , then x  X  is the optimal solution for problem 8, otherwise define  X  x = x  X  and recal-procedure can be seen as a gradient projection method for which convergence is well established [2, 1].
Another optimal rule can be defined as the rule that covers the hypercube with axis-parallel faces with that contains t he largest possible number of training points in the halfspace . Given a transformed problem P  X  (  X  w, 1 , I 0 ), we want to find x  X  such that w  X  x  X   X   X  = 0 and | C | (cardinality of C )is maximal, where: The following Linear programming formulation is an approx-imation to this problem :
Note that the variable y  X  0 acts as a slack or error vari-able that is minimized in order in order for the rule to cover the largest possible amount of points.
 We can now use either one of the optimal rule definitions de-scribed in subsections 4.1 and 4.2 to propose an iterative pr o-cedure that extract as many rules as we require to describe adequately the region of interest. We first demonstrate that in a n -dimensional feature space, extracting one such a rule results in n new similar problems to solve. Let the first rule extracted for the transformed problem P  X  (  X  w, 1 , I  X  i =1 (0  X  x i &lt; x  X  i ). The remaining volume on this side of the hyperplane that is not covered, is the union of n nonin-tersecting regions similar to the original region, namely that is, the rule inequalities for the first i  X  1 components of x are satisfied, the inequality that relates to the i th component For each x  X  I j , we have 0  X  x i &lt; x  X  i and for each x  X  I we have x  X  i  X  x i &lt; 1. Hence, I i are nonintersecting, and the rules that we arrive at for each I i will be  X  X ndependent X . Now we extract the optimal rule for each of these regions that contains a training data point using a depth first search . use the same transformation as described in equations (5)-(7) to transform each of the n subproblems P  X  (  X  w, 1 , I problems equivalent to the original problem P  X  (  X  w, 1 , I
Next, we state our algorithm to obtain a set of rules R that cover all the training points belonging to A  X  such that w x &lt;  X  . Let R be the set containing all the extracted rules, and U be the set containing the indices of the points uncovered by the rules in R . R and U are initialized to  X  and A  X  respectively, d max (which bounds the maximum depth of the depth first search, typically less than 20) is assigned , and w,  X  are obtained by solving the LP-SVM (1) before ExtractRules is invoked for the first time.

Algorithm 4.1. ExtractRules ( w,  X , I, d ) : Algorithm for rule extraction from linear classifiers. 1. If d = d max , stop. 2. Transform problem P  X  ( w,  X , I ) into P  X  (  X  w, 1 , I 3. Obtain y  X  by solving problem P  X  (  X  w, 1 , I 0 ) using either 4. Calculate x  X  = T  X  1 y  X  + b , get new rules  X  R ( x 5. Let C = { x  X  U st.  X  R ( x  X  ) is true } = U  X   X  R ( x 6. Update U  X  U  X  C . If U =  X  , stop. Else d  X  d + 1 . 7. for k = 1 to n do 5. ALGORITHM CONVERGENCE PROPERTIES
We now derive the rate at which the volume covered by the rules extracted for P ( w, 1 , I 0 ) converges to the total volume of the region of interest.

Lemma: The volume of the region { x s . t . w  X  x &lt;  X , x 0 } is Proof: We show this by induction. For n = 2, this is the area of a right-angled triangle with sides  X /w 1 and  X /w 2 which is  X  2 / 2 w 1 w 2 . Now, assume that this is true for n = k . V k +1 ( w,  X  ) = where w  X  i contains all components of w except the i -th.
Lemma: For any S  X  { 1 , 2 , . . . , n } , the volume of a re-gion defined by w  X  x &lt; 1 and 0  X  x i &lt; 1 , 1  X  i  X  n is bounded by Proof: We can assume without loss of generality that S is { 1 , 2 , . . . , k } (if it is not, the coordinates may be permuted so that it is). The volume of interest, say V is given by V = Z where the first two inequalities are because the upper limit in the integral is replaced by an upper bound, and the last equality comes from the previous lemma with  X  = 1.
Lemma: At each  X  X tage X , the algorithm covers at least  X  = n ! n n of the volume yet to be covered. Hence,the volume remaining after k stages is at most (1  X   X  ) k V 0 . Proof: The volume covered by the rule is given by where A as before is the set of active constraints, and the inequality above comes from the fact that for i  X  A , 1 nw (the original solution to the relaxed problem violates the constraints). Using the result of the previous lemma, and setting S = { 1 , . . . , n }\ A , we have the last inequality arises because the bound is monotonical ly increasing in | A | with it being the smallest when | A | = 0.
Lemma: At each stage, the algorithm reduces the largest distance from an interior point yet to be covered to the sep-arating hyperplane by a factor of 1  X  1 /n .
 Proof: We establish the lemma for one stage of P ( w, 1 , I (a simple scaling argument would extend it to a general  X  and I , and hence to further stages of the problem as well). The largest distance from the plane in I 0 In region I i , as x i  X  x  X  i and w  X  x is monotonically increasing in each coordinate When i  X  A , then I i has no interior points. When i /  X  A ,  X  x = w i x  X  i = 1 /n . Hence,
Theorem: After extracting t rules, the remaining vol-ume is at most (1  X   X  ) log n t  X  1 of the original volume. More-over, the rule extraction algorithm covers in finite time any dataset that has all points in the interior of I . Proof: As described before, each rule extraction leads to n further  X  X ubproblems X . Hence, the number of rules to be extracted in stage k is n k  X  1 , and the number of rules ex-have been extracted and k stages are complete, Hence, at least log n t  X  1 stages are complete, and hence, by a previous lemma, at most (1  X   X  ) log n t  X  1 of the volume remains (which converges to 0 as t  X  X  X  ). Moreover, by the previous lemma we have that at the end of stage k , Hence, for a data point x , we have that x is covered when i.e. when Hence, the entire data set A  X  is covered when i.e., when We now use this to establish termination of the algorithm for a given data set in finite time. Let us assume the con-trary, i.e. that there is a point x # such that w  X  x # &lt;  X  and it is not covered in the rule extraction process. By the pre-vious lemma, we have that y = x # + (  X   X  w  X  x # ) w/ 2 || w || is not covered (as it is greater than x ). Moreover, any point in the hypercuboid x # i  X  x i &lt; y i is not covered by the rules. Hence the volume of the uncovered region is at least Q of the theorem. Hence, the point x # gets covered after a fi-nite number of iterations. 6. NUMERICAL TESTING
To show the effectiveness of our rule extraction algorithm, we performed experiments in five real-world datasets. Three of the datasets are publicly available datasets from the UCI Machine Learning Repository [14]: Wisconsin Diagnosis Bre ast Cancer (WDBC), Ionosphere, and Cleveland heart. The fourth dataset is a dataset related to the nontraditional au -thorship attribution problem related to the federalist pap ers [9] and the fifth dataset is a dataset used for training in a computer aided detection (CAD) lung nodule detection al-gorithm, we refer to this set as the Lung CAD dataset. Ex-periments for the five datasets were performed to test the capability of algorithm 4.1 to cover training points correc tly classified by the SVM hyperplane. For each experiment, we obtained a separating hyperplane using the 1  X  norm lin-ear programming SVM (LP-SVM) formulation as described in equation (1). The state of the art optimization software CPLEX was used to solve the corresponding linear program-ming problems. Ten-fold cross validation was used as a tuning procedure to determine the SVM parameter  X  . In All the experiments, the resulting hyperplane classifier wa s sparse, this means that the set { w i s.t. w i 6 = 0 , 1  X  i  X  n } was  X  X mall X , this was expected because of the effect of the 1  X  norm regularization term on the coefficients w i . Having a sparse hyperplane implies that the dimensionality of the training dataset can be reduced by discarding the features corresponding to w i = 0 since they do not play any role in the classification.
 Once the hyperplane was obtained we applied algorithm 4.1 using one of the two criteria for optimal rules described in subsections 4.1 and 4.2. The first criteria is based in find-ing rules that maximizes the volume of the region covered by the rule, we will refer to this variant of algorithm 4.1 as Volume Maximization ( VM ). The second criteria is to find rules that attempt to cover a many points of the training set as possible. We will call this variant of algorithm 4.1 Point Coverage Maximization ( PCM ).
Results for both VM and PCM are reported in Tables 1 and 2 including: total number of optimization problems solved, total execution time, total number of extracted rul es and percentage of correctly classify points by the hyperpla ne that were covered by the extracted rules.
 It is important to note that the results reported included only rules that covered more than one point. We consid-ered that rules that covered only one point did not have any generalization capability and therefore were discarded. I n general, the algorithm can be tuned to discard rules that do not cover enough points according to a number predefined by the user.

Empirical results on the five datasets as reported in Ta-bles 1 and 2 show the effectiveness of both the VM and PCM variants of our proposed algorithm. In most cases our algorithms covered more of 90% of the training points using only a few rules. As was expected, the VM variant seems to solve more  X  X asy X  optimization problems and generate more rules. On the other hand, the PCM variant solved fewer optimizations problems (linear programming problems) but that were slightly harder to solve, generating fewer rules.
Note that Tables 1 and 2 appear at the end of this paper (after the references). Next, we will discuses in more detai l the results obtained for the WDBC dataset and the lung-cad dataset since medical diagnosis applications is of spec ial interest to us.
The first experiment relates to the publicly available WDBC dataset that consists of 683 patient data. The classificatio n task associated with this dataset is to diagnose breast mass es based solely on a Fine Needle Aspiration (FNA). Doctors identified nine visually assessed characteristics or attri butes of an FNA sample which they considered relevant to diagno-sis (for more detail please refer to [12]). After applying th e LP-SVM algorithm and discarding the features correspond-ing to the w i = 0, we ended up with a hyperplane classifier in 5 dimensions that achieved 95 . 0% tenfold testing set correct-ness. After applying our ExtractRules-PCM algorithm to cover the 214 points in A  X  correctly classified by the hy-perplane we obtained a total of 7 non empty, non-singleton rules that cover 99.8% of the points. Similarly we obtained a total of 3 non empty, non-singleton rules that cover 98.1% of points in A + correctly classified by the hyperplane. For example after using the fact that all the features values are integers between 1 and 10 we obtained the following rule that covered 383 of the 435 positive training points:
The second experiment relates to a set of data used in a computer aided detection (Lung CAD) system for pul-monary nodule detection on thin slice multidetector CT scans. The Lung CAD algorithm performs the following processing steps: a) lung segmentation; b) candidate gener -ation; c) feature calculation at each candidate location; d ) classification and e) presenting CAD findings to a physician for review. The task of the candidate generation step, is to reduce the search space by quickly generating a list of sus-picious locations for different types of nodules at a high sen -sitivity without considering the specificity. For this, sha pe based characteristics are used to generate a candidate list . For each candidate in the list a set of features is calculated . Those features are based on the intensity, the shape, the cur -vature, and the location. The goal of the last processing ste p is to increase the specificity without decreasing the sensit iv-ity by pruning the list of candidates. For this, a classifier is used. Our dataset consists on 274 candidates represented by 34 numerical features. Each datapoint corresponds to a candidate labeled as a nodule or not a nodule. The LP-SVM algorithm generated a classifier in only 5 features with 82 . 5% tenfold testing set correctness. Our ExtractRules-PCM algorithm extracted a total of 10 nonempty, non-singleton rules that cover 94% of positive training points correctly classified for the hyperplane, similarly we obtained a total of only 5 nonempty, non-singleton rules that cover 98.4% of the points in A + correctly classified for the hyperplane.
We have described an efficient algorithm for converting any arbitrary linear classifier into a rule set that can be eas -ily interpreted by humans. We presented two variants of our algorithm based on different criteria for selecting  X  X ptima l rules X . One main advantage of our algorithm is that it only involves solving relatively simple optimization problems in a few variables. We also discussed various properties and pro -vided a detailed convergence analysis of the algorithm. Em-pirical results on several real-world data sets demonstrat e the efficacy and speed of our method.

We plan to extend our numerical results to include com-parisons to other rule-based classification methods. We are also considering other mathematical programming formula-tions where the rules can overlap since overlapping rules ma y have an advantage that may depend on the specific problem.
An interesting extension of this work would be to com-bine our rule extraction algorithm with a recently proposed Knowledge-based SVM [8] to design an incremental algo-rithm to handle massive amounts of data. The algorithm could  X  X ompress X  training data in the form of rules obtained form different  X  X hunks X  and then integrate all the obtained rules into a Knowledge-based SVM.

The incorporation of the feature selection into the rule extraction problem is also a possibility we are exploring at this moment. This approach would generate rules that de-pend on different features instead of depending on the same preselected subset of features.

So far we have focused on developing rule sets that are hu-man interpretable models that are equivalent to the origina l linear classifier. An equally important use of our method would be to provide an explanation of the classification for a new unlabeled (test) example. The most obvious way is to present the user with the specific rule that includes the test example. For instance, when working with physicians, we have found that an explanation of a classification label whic h is in terms of a bounding hypercube, is far more understand-able than  X  X xplaining X  a label because some weighted sum of the variables is less than some constant.
 The interesting case arises when no rule covers the test ex-ample. The obvious extension to execute ExtractRules on the region I which contains the test example, until a cover-ing rule is found. However, the resulting rule may cover a very small volume around the test example, rendering the explanation useless. An alternate approach is not to build a rule set that is equivalent to the entire classifier, but in-stead to revise the original problem defined in (8) to extract just one rule  X  the largest possible hypercube (rule) which contains the test example. Such a rule, however, may not have much explanatory value because in most cases the test example will lie on one of the surfaces of the hypercube.
A more satisfactory explanation for a test sample may be provided by a rule where the example lies well within the interior of the rule, far away from the bounding spaces. The rule that provides the  X  X ptimal X  explanation, can be created by drawing a normal from the test sample to the hyperplane, and the intersection of the normal with the hy-perplane defines the corner of a uniquely defined bounding hypercube (rule), which centrally contains the test sample . Additionally, we can provide a confidence associated with the explanation (rule); ideally the explanation rule shoul d cover all training examples in A + ( A  X  ), contain only all positive (negative) training samples, be as large as possib le (the volume ratio with respect to the rule created by Ex-tractRules ), and for the test sample to be as far from the hyperplane. All these factors may be used to adjust the con-fidence associated with the rule (for the specific test sample ) by weighting it using some scoring scheme. In general, these criteria may be applied to any explanatory rule, not just the  X  X ptimal X  explanatory rules created as defined above. [1] D. P. Bertsekas. Nonlinear Programming . Athena [2] Dimitri P. Bertsekas. Projected Newton methods for [3] F. Beyer, L. Zierott, J. Stoeckel, W. Heindel, and [4] E.H.Shortliffe B.G.Buchanan. Rule-Based Expert [5] P. S. Bradley and O. L. Mangasarian. Feature [6] V. Cherkassky and F. Mulier. Learning from Data -[7] G. Fung and O. L. Mangasarian. Proximal support [8] G. Fung, O. L. Mangasarian, and J. Shavlik.
 [9] Glenn Fung. The disputed federalist papers: Svm [10] F. J. Kurfes. Neural networks and structured [11] O. L. Mangasarian. Generalized support vector [12] O. L. Mangasarian, W. N. Street, and W. H. Wolberg. [13] S. Mika, G. R  X atsch, J. Weston, B. Sch  X olkopf, and [14] P. M. Murphy and D. W. Aha. UCI machine learning [15] Haydemar Nu  X  n ez, Cecilio Angulo, and Andreu Catal. [16] K. Preston. Computer processing of biomedical [17] A. Tickle R. Andrews, J. Diederich. A survey and [18] L. B. Lusted R. S. Ledley. Reasoning foundations of [19] J. Roehrig. The promise of cad in digital [20] G. Towell &amp; J. Shavlik. The extraction of refined rules [21] J. A. K. Suykens and J. Vandewalle. Least squares [22] V. N. Vapnik. The Nature of Statistical Learning
