 This paper studies the problem of question retrieval in com-munity question answering (CQA). To bridge lexical gaps in questions, which is regarded as the biggest challenge in retrieval, state-of-the-art methods learn translation models using answers under an assumption that they are parallel texts. In practice, however, questions and answers are far from  X  X arallel X . Indeed, they are heterogeneous for both the literal level and user behaviors. There are a particular-ly large number of low quality answers, to which the per-formance of translation models is vulnerable. To address these problems, we propose a supervised question-answer topic modeling approach. The approach assumes that ques-tions and answers share some common latent topics and are generated in a  X  X uestion language X  and  X  X nswer language X  respectively following the topics. The topics also determine an answer quality signal. Compared with translation mod-els, our approach not only comprehensively models user be-haviors on CQA portals, but also highlights the instinctive heterogeneity of questions and answers. More importantly, it takes answer quality into account and performs robustly a-gainst noise in answers. With the topic modeling approach, we propose a topic-based language model, which matches questions not only on a term level but also on a topic level. We conducted experiments on large scale data from Yahoo! Answers and Baidu Knows. Experimental results show that  X  The work is done when the author is an intern at Microsoft Research Asia.
 the proposed model can significantly outperform state-of-the-art retrieval models in CQA.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models comunity question answering; question retrieval; supervised topic model; answer quality
Community question answering (CQA) portals, like Ya-hoo! Answers, Baidu Knows and Quora, have emerged as hot platforms for people to share their knowledge and learn from each other. In the last decade, these web sites have attracted a great number of users, and have accumulated a large amount of content generated by these users. The content is usually organized as questions and lists of an-swers associated with metadata like user votes to answers and askers X  awards to the best answerers. This data has made CQA archives valuable repositories for various tasks like knowledge mining, question-answering and searching, etc.

One fundamental task for reusing content in CQA is find-ing similar questions for queries, as questions are the keys to accessing the knowledge in CQA. Many studies have been done along this line [6, 7, 9, 20]. One big challenge for ques-tion retrieval in CQA is that users are used to expressing similar meanings with different words, which creates lexical gaps when matching questions based on common terms. For example, we find that for a user query  X  X ow do I get knots out of my cats fur X , there are good answers under an ex-isting question  X  X ow can I remove a tangle in my cat X  X  fur X  in Yahoo! Answers. Although the two questions have very similar meanings, since they share few common words, it is hard for classic retrieval models like BM25 [15] and lan-guage models for information retrieval ( LMIR ) [21] to rec-ognize their similarity. The lexical gap has become a major barricade preventing traditional IR models from retrieving similar questions in CQA.
To overcome this issue, existing work considers leverag-ing answers of questions and learning translation models to improve traditional IR models. The basic assumption be-hind these models is that questions and answers are  X  X arallel texts X  and relationships of words can be established through word-to-word translation probabilities [20]. The translation models represent state-of-the-art methods for question re-trieval in CQA. However, we argue that in practice, ques-tions and answers are far from  X  X arallel X . Questions and answers are highly asymmetric on the information they con-tain. One question may have multiple answers. These an-swers may be diverse and contain much more information than the question. Moreover, the language of answers is usually more casual than that of questions. For example, answerers like using words such as  X  X ello X ,  X  X .O.L X , or  X  X ow X  in their answers to make them friendly or appealing. These words do not help much in retrieving similar questions, s-ince askers seldom use them. Additionally, on popular CQA sites like Yahoo! Answers, there is usually a large variance in quality of answers, from highly valuable knowledge to spam, and low quality answers take a relatively high proportion of all answers (more than 30% according to [10, 16]). Table 1 gives an example 1 . We can see that Answer 2 is a low qual-ity answer regarding to the question. It makes no sense as an answer and is useless for bridging lexical gaps when re-trieving the question. On the other hand, Answer 1, another answer in the answer list of the question, not only provides rich information for the question, but also has the poten-tial to help question retrieval, as the answerer used  X  X i-fi X  as a related word of  X  X nternet X  in the context. Intuitively, low quality answers will pollute training data and make the learnt translation probabilities unreliable. It is better to se-lect good answers for model learning. For translation-based methods, it is not trivial to do so.

In this paper, we study how to leverage answers to retrieve similar questions for queries. Specifically, we head for model-ing questions and answers in a more natural way. The model should not only highlight the instinct heterogeneity of ques-tions and answers, but also be flexible enough to take answer quality into account. To this end, we propose a supervised question-answer topic modeling approach for question re-trieval in CQA. The underlying assumption is that although questions and answers are heterogeneous in many aspects, they share some common latent factors. The latent factors represent topics in askers X  and answerers X  minds. Follow-ing the common topics, askers ask questions in a  X  X uestion language X , while answerers provide answers in an  X  X nswer
The full page is available at https://answers.yahoo.com/ question/index?qid=20080316101123AAItTLV language X . Moreover, there is a signal indicating the quality of an answer. The signal simulates how other users evaluate the answer with respect to the question in a question-answer pair. It is determined by how well askers and answerers fol-low common topics in the model. With the guide of quality signals, questions and answers are mapped into a common latent space (topic space) and question-question similarity can be measured with the help of information in answers compressed in that space. With the learnt topic space, we further propose a topic-based language model for question retrieval in CQA, which naturally integrates the matches in latent space and traditional retrieval models. We derive a collapsed Gibbs sampling method to estimate parameters in the model. The method is efficient in computation. Com-pared with existing methods, our learning approach compre-hensively models user behaviors in CQA portals and at the same time naturally takes answer quality into account.
Another advantage of our method is that it is capable of leveraging rich metadata associated with answers and au-tomatically learning answer quality signals. Specifically, we take  X  X est answer X  as the gold standard and extract a variety of features from both answer texts and associated metadata. With the training data and features, we learn a score func-tion to generate answer quality signals and let these signals supervise the learning of the topic space. By this means, our model can not only leverage the power of big data accu-mulated in CQA sites, but also save on the labor of human annotators.

We test the proposed model on large scale Yahoo! An-swers data and Baidu Knows data. Yahoo! Answers and Baidu Knows represent the largest and most popular CQA archives in English and Chinese, respectively. We conducted both quantitative and qualitative evaluations. The results show that our model can significantly outperform state-of-the-art translation-based approaches for question retrieval in CQA.

Our contributions in this paper are three-fold: 1) the pro-posal of considering answer quality when leveraging answers for question retrieval in CQA; 2) the proposal of a supervised question-answer topic model, which not only highlights the heterogeneity of questions and answers but also naturally takes answer quality into account; 3) an empirical verifi-cation of the efficacy of the proposed model on large scale English and Chinese CQA data.

The rest of the paper is organized as follows: Section 2 summarizes related work; Section 3 gives an overview of state-of-the-art methods for question retrieval in CQA; Sec-tion 4 elaborates our model, including intuition, formulation and algorithm; Section 5 reports experimental results; and finally Section 6 concludes the paper.
The problem of question retrieval arose from finding simi-lar questions from frequently asked questions (FAQs). In re-cent years, along with the flourishing of community question answering (CQA) archives, more attention is paid to ques-tion searches in CQA. Particularly, language model based methods are proven effective. Most CQA researchers focus on leveraging metadata in CQA to improve the performance of the traditional language models for information retrieval [21]. Basically, there are two groups of work. The first group considers leveraging categories of questions. For ex-ample, Cao et al. [7, 6] proposed a language model with leaf category smoothing in which they estimated a new smooth-ing item for language models from questions under the same category. The other group leverages answers and learns var-ious translation models to bridge lexical gaps in question-s. For example, Jeon el al. [9] proposed learning word-to-word translation probabilities from question-question pairs collected based on similar answers. Xue et al. [20] learned a translation model from question-answer pairs. In this paper, we also consider leveraging answers for retrieving similar questions in CQA. Our model is different from translation models. It unveils the common latent factors in question-answer pairs, and matches questions in the latent space. Particularly, the model takes answer quality into account, and performs robustly against noise in low quality answers.
We propose a topic modeling approach for question re-trieval in CQA. Topic model refers to a family of proba-bilistic generation models that simulate the process of peo-ple writing documents and project documents into latent spaces. Early topic models like pLSI [8] assume each doc-ument is a mixture of topics and each word in a document is generated from an independent topic. Latent Dirichlet allocation (LDA) [4], which represents the state-of-the-art topic modeling approach, further assumes the distribution of topics is generated from a Dirichlet distribution. Based on LDA, many extensions have been developed for various applications. For example, Zhao and Xing [22] proposed a bilingual topic model and applied it to machine translation. Recently, supervised topic models [12, 23] were proposed to leverage side information such as labels of documents and image tags in regression or classification tasks. In this pa-per, we model the generation of questions and answers in a way similar to bilingual topic model. The difference is that answer quality is considered in our model.

Applying topic modeling techniques to information re-trieval problems is not a new idea. Xing and Bruce [19] first applied LDA to IR and proposed an LDA-based lan-guage model for ad-hoc retrieval. In recent years, prob-abilistic latent aspect models have also been introduced to CQA. For example, Cai et al. [5] incorporated question cate-gory information into the traditional LDA and combined the topic model with a translation-based language model. Ji et al. [11] proposed a question-answer topic model for question retrieval in CQA. In this paper, we also attempt to apply probabilistic latent factor techniques to question retrieval in CQA. Our model is most related to the one proposed by Ji et al. The stark difference between our model and their work is that we observed the harm of low quality answers which are prevalent in popular CQA archives and incorporated answer quality signals into learning process. Therefore, our model performs more robustly against noise in answers, as will be demonstrated in Section 5.
State-of-the-art question retrieval models are effective when answers have a high level of quality. Unfortunately, in prac-tice, answer quality in popular CQA archives such as Yahoo! Answers is far from satisfying. Agichtein et al. [1] found that low quality answers take a relatively high proportion of all answers. The same phenomenon was also observed by Jeon el al. [10] and Sakai et al. [16] on popular Korean and Japanese CQA portals. To detect high quality answer-s, many methods have been proposed. For example, Jeon et al. [10] presented a framework to predict answer quali-ty from non-textual features. Agichtein et al. [1] extracted both content features and usage features to learn a classifier for recognizing high quality answers. Chirag and Jefferey [17] proposed learning a logistic regression model to predict answer quality in CQA. In this paper, we employ answer quality to supervise the learning of question-answer topic space. The quality signals are automatically learnt from large scale CQA data with content and usage features used in the existing work. The automatic learning of answer qual-ity makes our supervised topic model feasible in processing large scale CQA data.
Before presenting our model, we give a brief overview of state-of-the-art retrieval models in CQA.
In recent years, language models for information retrieval ( LMIR ) [21] and their extensions have been proven effec-tive for question retrieval in CQA. Formally, given a query question q and a candidate question Q , LMIR calculates the similarity between q and Q by where P ml ( w | Q ) represents the maximum likelihood of term w estimated from Q , and P ml ( w | C ) is a smoothing item which is calculated as the maximum likelihood in a large corpus C . The smoothing item avoids zero probability which stems from the terms appearing in the query but not in the candidate.  X   X  (0 , 1) is a parameter which acts as a trade-off between the likelihood and the smoothing item.

LMIR generally performs well when there is a large pro-portion of common terms between query q and candidate Q . In practice, however, people are used to expressing similar meanings with different words, which makes LMIR suffer from the term mismatch problem when measuring similari-ty of questions.
To improve LMIR when there are lexical gaps in question-s, state-of-the-art methods learn word-to-word or phrase-to-phrase translation probabilities from answers, and incorpo-rate the information into LMIR . Formally, given a query question q and a candidate question Q , the translation-based language model [3] is given by where Here  X  ,  X  , and  X  are parameters, satisfying  X  +  X  = 1. P tp ( w | v ) represents the translation probability from term v in candidate question Q to term w in query question q .
Xue, et al. [20] further improved the translation-based lan-guage model by introducing an extra answer likelihood term P ml ( w | a ) estimated from the answer a of question Q . The model is defined as
P trba ( q | Q,a ) = Y where
P mx ( w | Q,a ) =  X P ml ( w | Q ) +  X P tr ( w | Q ) +  X P Here  X  is an extra parameter satisfying  X  +  X  +  X  = 1.
From Equation (2) and Equation (3), we can see that translation-based language models solve the term mismatch problem by linking similar words or phrases through trans-lation probabilities. Translation probabilities are estimated from question-answer pairs under the assumption that ques-tions and answers are parallel texts. In practice, however, questions and answers are far from  X  X arallel X . Indeed, they are highly asymmetric on the information they contain and answers are more noisy than questions. Particularly, there is usually a large amount of low quality answers in popu-lar CQA portals. The low quality answers, like Answer 2 in Table 1, can pollute training data and make the learnt translation probabilities unreliable. Therefore, it would be better to model relationships between questions and answers in a more natural way and make the matching of question-s robust to noise in answers. In the next section, we will consider mining common latent factors in questions and an-swers and matching questions in a latent space learnt with the help of answers and their quality signals.
We present matching questions in a latent space. The la-tent space is learnt not only from questions, but also from answers. Particularly, the learning of the latent space is guided by the quality of answers. The better the quality of an answer is, the more contributions the answer will make to the learning of the latent space. We propose a supervised question-answer topic model to learn the latent space. The model assumes that askers and answerers follow the same latent topics and write questions and answers in a  X  X uestion language X  and an  X  X nswer language X , respectively. The la-tent topics further determine a signal which measures the quality of the answer with respect to the question. We in-terpolate the matching of questions in the latent space into the traditional LMIR and propose a topic-based language model. To estimate the parameters, we derive a collapsed Gibbs sampling method which is fast in computation. Com-pared with translation models, our model comprehensively simulates behaviors of users on CQA archives. It not only reflects the instinct heterogeneity of questions and answers, but also highlights the importance of good answers in con-trast to bad answers in learning.
We propose a generative model to describe how questions and answers are created in CQA archives. Before formaliz-ing the generative process, let us first imagine how askers, answerers, and other users interact on CQA archives. In a common case, askers have some topics on their minds and follow them to write their questions. Answerers see the ques-tions, understand the topics of askers and follow the topics to give their answers. Finally, other users review questions and answers, and show their evaluations based on how they feel the answers fulfill needs in the questions. The evalua-tions usually reflect answer quality.
 Figure 1: Supervised question-answer topic model We try to model the interactions of users on CQA archives. Suppose that the vocabulary size is M and the total number of questions is N . Given a question q and an answer a in the answer list of q , q and a can be represented as word where | q | and | a | are total number of words in q and a , respectively. Suppose that the total number of answers in the answer list of q is N q . For a fixed number of topics K , there are two M  X  K matrices  X  q = h  X  (1) q ,... X  ( K )  X  topics of questions and answers, respectively. In addition to these, there is a variable y q,a  X  R , indicating the quality of a with respect to q . The larger y q,a is, the better quality a has. y q,a can be obtained from human labelers. In this paper, we propose automatically learning y q,a from CQA data, as will be seen in Section 5.1.2.

The generative process can be formalized as: 1. Draw a topic distribution  X   X  Dirichlet (  X  ) 2. For word position i = 1 : | q | in question q 3. For each answer a in the answer list of q
Here, GLM (  X  ,  X  ,  X  ) represents a generalized linear model [13]. Dirichlet (  X  ) is a Dirichlet distribution with  X  as a hyperparameter.  X  ( z q i ) q and  X  ( z a j ) a are drawn from Dirichlet distributions Dirichlet (  X  q ) and Dirichlet (  X  a ), respectively, where  X  q and  X  a are hyperparameters. With a little ma-nipulation of notations, we suppose that z is both a integer ranging from 1 to K and a K -dimensional indicator vector with only one element is 1 indicating the topic and all other elements are 0. The meaning of z is clear in context. The graphical model for the generative process above is given by Figure 1.

We call our model a supervised question-answer topic mod-el. The model highlights the instinct heterogeneity of ques-tions and answers. It reflects the asymmetry of questions and answers in their generative processes. A question is first generated in a  X  X uestion language X  (i.e.,  X  q ), then answers associated with the question are generated with each in an  X  X nswer language X  (i.e.,  X  a ). At the same time, the mod-el also captures the relationships behind the heterogeneity of questions and answers, that is they are generated follow-ing the same topic distributions. Moreover, the model fur-ther considers influence of answer quality to latent topics, which simulates evaluations from other users to question-answer pairs. These characteristics differentiate our model from the existing bilingual topic model [22] and supervised topic models [12, 23].
With the supervised question-answer topic model, we map questions and answers into a low dimensional latent space with the supervision of answer quality signals. In the s-pace, semantically related words in questions and answers are grouped together and noise is filtered. With the space, we can measure similarity of questions on a topic level. For-mally, suppose we have obtained the posterior estimation of  X  ,  X  q ,  X  a , given a query question q and a candidate ques-tion Q with an answer a , we interpolate matching in latent space into the traditional LMIR , and propose a topic-based language model as follows:
P tblm ( q | Q,a ) = Y where
P mx ( w | Q,a ) =  X P ml ( w | Q ) +  X P sqatm ( w | Q ) +  X P
P sqatm ( w | Q ) = X Here,  X  ,  X  ,  X  , and  X  are parameters, satisfying  X   X  (0 , 1) and  X  +  X  +  X  = 1.  X   X  is defined as where  X   X  (0 , 1) is a combination parameter.

From Equation (4), we can see that question pairs are matched on both a term level and a topic level. The match-ing on the topic level is determined by a linear combination of question topic distribution  X  q and answer topic distribu-tion  X  a , as seen in Equation (5). The impact of  X  q and  X  in matching can be controlled through  X  .

The topic-based language model is similar to the translation-based language model plus answer likelihood proposed by Xue et al. [20] (cf. Equation (3)) on appearance. The dif-ference is that we solve the term mismatch problem through latent factors supervised by answer quality while Xue et al. rely on translation probabilities which could be unreliable when answers have low qulaity.
We employ a Gibbs sampling approach to estimate the parameters in the supervised question-answer topic model. Specifically, we follow the work of Zhu et al. [24], introduce max-margin regularization to Bayesian inference, and de-velop a collapsed Gibbs sampling method. The method is efficient in computation.

Let Z denote all topics in questions and answers,  X  de-note all topic distributions, and y denote all quality signals. Define where P 0 (  X ,  X  , Z , X  q , X  a ) is a prior probability and term Leibler divergence. We try to solve the following optimiza-tion problem: pected -insensitive loss, where N is the total number of questions, for question q i , N i is the total number of answers between the quality label y q i ,a j and the prediction  X  z ,a j is the average of topic vectors in q i and a j . P is the space of probability distributions, and c is a parameter act-ing as a trade-off between the Bayesian inference L ( Q ) and max-margin regularization R .

To solve problem (6), we define an unnormalized pseudo-likelihood of response variable:
Then, optimization problem (6) can be re-written as arg min where The solution of optimization problem (7) is given by where  X  ( y , W ) is a normalizer. Following the work of Polson and Scott [14],  X  ( y q,a |  X ,z q ,z a ) can be represented as
If we define
Q (  X ,  X  , Z , X  q , X  a ) can be represented as the marginal dis-tribution of a higher-dimensional distribution Q (  X  ,  X  ,  X  ,  X  , Z ,  X  q ,  X  a ), which is defined as:
Moreover, Q (  X ,  X  ,  X  ,  X  , Z , X  q , X  a ) is the solution of the following optimization problem: arg min
By integrating out (  X  , X  q , X  a ), we have the collapsed pos-terior distribution Q (  X ,  X  ,  X  , Z ) as:
Let n ( t ) q,k and n ( t ) a,k denote the number of times that term t is observed with topic k in all questions and all answers, respectively; and n q i ,k and n a j ,k denote the number of times that topic k is observed in question q i and answer a j , respec-tively. Then, the conditional distributions of Q (  X ,  X  ,  X  , Z ) can be expressed as follows:
For  X  : we assume its prior is an isotropic Gaussian dis-tribution P 0 (  X  ) = Q K k =1 N (  X  k ; 0 , X  2 ). Then, we have  X 
For z : we can derive the conditional distribution of a topic z given the other topics Z q as: For z in question q i : where w q i ,s represents the s -th word in question q means the topic of s -th word in question q i is k ; n ( t ) number of times topic k is observed on term t in all questions with w q i ,s excluded; n q i ,k, q s is the number of times topic k is observed in question q i with w q i ,s excluded.  X  = 1 For z in answer a j of question q i : where w a j ,s represents the s -th word in answer a j means the topic of s -th word in answer a j is k ; n the number of times topic k is observed on term t in al-l answers with w a j ,s excluded; n a j ,k, q s is the number of times topic k is observed in answer a j with w a j ,s
For  X , X  : We can prove that  X   X  1 q i ,a j and  X   X  1 q i inverse Gaussian distributions: b &gt; 0.

With the conditional distributions above, we can construc-t a Markov chain to learn our model. In training, we finish the burn-in stage in T iterations based on the Markov chain, as outlined in Algorithm 1. The per-iteration time complex-ity of this algorithm is O ( N total K + K 3 ), where N total total number of words appearing in questions and answers.
After training, we can get the estimations of b  X  , b  X  q b  X  a as: where b  X  ( k ) q i is the probability of topic k in question q is the probability of word w under topic k in questions, and b  X  a,w is the probability of word w under topic k in answers.
Algorithm 1: Learning supervised question-answer top-ic model 10: draw a topic from distribution (10) 11: end for 12: end for 13: for each answer a in the answer list of q do 14: draw  X   X  1 for a from distribution (11) 15: draw  X   X  1 for a from distribution (12) 16: end for 17: end for 18: end for
An interesting observation is that Equation (9) reveals the effect of answer quality on the learning of question topic dis-tribution. Specifically, the quality signal y q,a is encoded in  X  q,a , which is defined as y q,a an answer a has, the larger y q,a will be. A large y q,a lead to a large  X  q,a , which means the answer has more con-tributions to the learning of the question topic distribution (cf. Equation (9)).
We conducted experiments to test the performance of the proposed method on question retrieval.
We crawled a large number of questions (a.k.a. question titles) associated with descriptions (a.k.a. question bod-ies) and answers from Yahoo! Answers and Baidu Knows. Yahoo! Answers and Baidu Knows represent the largest and the most popular CQA archives in English and Chi-nese, respectively. The data covers all categories of Ya-hoo! Answers and Baidu Knows. For Yahoo! Answers, we crawled 1 , 186 , 542 questions with 950 , 127 descriptions and 8 , 430 , 784 answers. 80% of the questions have an ex-tra description, and on average each question is associated with 7 . 1 answers. For Baidu Knows, we crawled 773 , 194 questions with 425 , 257 descriptions and 2 , 904 , 596 answer-s. 55% of the questions have a description, and the average number of answers per question is 3 . 8. We used this data for training models. To prepare labeled data sets, we collected some extra questions that have been posted more recently than the training data, and randomly sampled 1 , 423 and 605 questions for Yahoo data and Baidu data, respectively. We took these questions as query questions. All question-s, descriptions, and answers were lowercased and stemmed. Stopwords were also removed.

We separately indexed the Yahoo data and the Baidu data using an open source Lucene.Net System 2 . For each query question from Yahoo! Answers and Baidu Knows, we retrieved several candidate questions from the correspond-ing indexed data based on the inline ranking algorithm in Lucene.Net. On average, each query question from Yahoo! Answers has 15 candidate questions and the average number of candidate questions for Baidu data is 8.

We recruited human judges to label the relevance of the candidate questions regarding to the query questions. Specif-ically, for each type of language, we hired three native judges. Each judge labeled a candidate question with 1 if it is  X  X el-evant X  to the query question, otherwise the judge labeled it with 0. Each candidate question got three labels and the majority of the labels was taken as the final decision for a query-candidate pair. We randomly split each of the two labeled data sets into a validation set and a test set with a ratio 1 : 3. The validation set was used for tuning pa-rameters of different models, while the test set was used for evaluating how well the models ranked relevant candidates in contrast to irrelevant candidates.

Please note that rather than evaluate both retrieval and ranking capability of different methods like many existing works [7, 6], we compared them in a ranking task. This may lose recall for some methods, but it can enable large scale evaluation and will facilitate others reproducing our experiments and comparing their methods on our data, as we published the labeled data at http://research.microsoft. com/en-us/people/wuwei/wuwei.aspx .

To evaluate the performances of different models, we em-ployed Mean Average Precision (MAP) [2], Mean Reciprocal Rank (MRR) [18], R-Precision (R-Prec) [7], and precision at position 1 (P@1) as evaluation measures. These measures are widely used in literature on question retrieval for CQA.
To implement our supervised question-answer topic mod-el, we have to collect quality signals for each answer. Ide-ally, these signals should be offered by human annotators. In practice, however, this would make our model infeasible on million-scale experiments. Intuitively, the performance of the proposed topic model will heavily depend on the quality of response signals. To balance the accuracy and the cost of learning, we propose automatically learning answer quality signals from CQA data.

Specifically, we extracted the following features: 1) An-swer length; 2) Fraction of best answers an answerer is award-ed in all answers he or she provided; 3) Unique number of http://lucenenet.apache.org/ words in an answer; 4) Word overlap between a question and an answer.

With these features, we took the best answer as the gold standard and trained logistic regression models [17] from the 8 , 430 , 784 answers of Yahoo! Answers and 2 , 904 , 596 answers of Baidu Knows, respectively. The advantage of logistic regression is that quality signals learnt are naturally scaled. With the automatic learning method, we not only leveraged metadata associated with answers, but also saved on the cost of human annotators.

Note that we only used four features to learn answer quali-ty signals. The four features are effective enough for predict-ing answer quality. We also found that metadata like user votes is sparse and biased in Yahoo! Answers and Baidu Knows. We implemented our model with both user votes and the learnt quality signal as supervision, and compared them with other retrieval models.

For CQA sites like Quora, there are no best answers. In this case, we have to make some adjustment on the learning of answer quality signals. For example, we can take the answer with the most votes as the gold standard and train a regression model.
The traditional language model for information retrieval ( LMIR ) given by Equation (1) was taken as a baseline. We also considered the following baseline methods:
Translation-based methods We learned different word-to-word translation probabilities with all the training da-ta of Yahoo! Answers and Baidu Knows. The translation probabilities were incorporated into the model of Xue et al. given by Equation (3). Specifically, we employed the pooled approach proposed by Xue et al. [20] and estimat-ed translation probabilities using GIZA++ 3 . We took each question-answer pair as a parallel text and denoted the mod-el as TAL Q  X  A ; we also concatenated all answers as a long document and trained a translation model from pairs of question and the long document. We denoted the model as TAL Q  X  AllA ; since best answers may have higher qulaity than other answers, we trained a model with question-best answer pairs, and denoted the model as TAL Q  X  BA . Be-sides these models, we also concatenated questions and de-scriptions, and trained translation models using the enriched questions and answers in the same way as we did above. The and TAL Q &amp; D  X  BA , respectively. Finally, to make a complete comparison, we trained a model from question-description pairs, and denoted it as TAL Q  X  D . In total, for each data set (Yahoo and Baidu data), we have 7 variants of transla-tion models for comparison.

Topic model based methods We concatenated a ques-tion, its description and all answers associated to form a document, and ran traditional LDA [4] on the document. The learnt latent factors and topic distributions were used to calculate question similarity in a way similar to Equa-tion (4). This method ignores the structures of question-s and answers. We took it as a baseline and denoted it as TBLM LDA . We also implemented the question-answer topic model proposed by Ji et al. [11] and considered two variants of the model: 1) the original QATM + TransLM , http://www.statmt.org/moses/giza/GIZA++.html which is the best performing model in [11] 4 ; 2) we calcu-lated P sqatm ( w | Q ) in Equation (4) with the latent factors and topic distributions learnt by QATM, and denoted the model as TBLM QATM . Parameters in all topic models were estimated with Gibbs sampling approaches.
 We denoted our model using the learnt quality signal as TBLM SQATM . We also implemented the model with user votes as supervision, and denoted it as TBLM SQATM  X  V We compared our model with all baseline methods on the test data sets.
There are several parameters we have to determine in our experiments. For LMIR and translation-based methods, we tuned the smoothing parameters {  X , X , X , X  } in { 0.1, 0.2,  X  X  X  , 0.9 } . For topic model based methods, following [24], we used the symmetric Dirichlet priors  X  = 1 K 1 ,  X  q =  X  a = 0 . 01  X  1 , where 1 is a vector with all entries 1. We tuned the number of topics (i.e., K ) in { 10, 20, 30, 40, 50 } and the number of it-erations of Gibbs sampling (i.e., T ) in { 100, 200,  X  X  X  , 1000 } . The best parameter combinations for LDA and QATM are (30, 1000) and (50, 1000) respectively on both data sets. After we incorporated LDA and QATM into Equation (4), the smoothing parameters {  X , X , X , X  } also need tuning. We tuned them in the same way as in translation-based meth-ods. In QATM + TransLM , there is an extra parameter  X  . We tuned it in { 0.1, 0.2,  X  X  X  , 0.9 } .

In our supervised question-answer topic model, besides number of topics and number of iterations, we used the s-tandard normal prior with  X  2 = 1 in P 0 (  X  ) and set = 1 e in the expected -insensitive loss R . c in optimization problem (6) is selected from { 0.001, 0.01, 0.1, 1, 10, 100, 1000 } .  X  in Equation (5) is tuned in { 0, 0.1, 0.2, ... , 0.9, 1 } . The best parameter combination for TBLA SQATM ( K = 40 ,T = 1000 ,c = 1 , X  = 0 . 8) and for TBLA SQATM  X  V is ( K = 40 ,T = 1000 ,c = 1 , X  = 0 . 7).
Table 2 shows the quantitative evaluation results of dif-ferent models on Yahoo data and Baidu data, respectively. From Table 2, we can see that TBLM SQATM outperform-s all baseline methods on all evaluation metrics. When we
We did not implement QATM with posterior regulariza-tion, as its performance is comparable with QATM in [11]. conducted a statistical test ( t -test), the results show that the improvements are statistically significant ( p value &lt; 0 . 01).
TBLM SQATM significantly outperforms LMIR , which demonstrates that matching questions with latent topics es-timated from question-answer pairs can effectively address the term mismatch problem. Its significant improvements over translation-based methods and other topic model based methods further verify that incorporating answer quality in-to learning can calibrate the learnt latent space and make the model robust to noise in answers.

TBLM SQATM  X  V performs badly on both data sets. We analyzed the reason and found that user votes are very s-parse and biased on Yahoo and Baidu data. Many answers with high quality only received a few votes, while some bad answers, due to their funny words, attracted much atten-tion. Moreover, most answers have 0 votes, because they are posted late. Therefore, in many cases, TBLM SQATM  X  V de-generated to TBLM QATM , and sometimes it is even worse, due to unreliable quality supervision.

On both data sets, topic model based methods perform better than most translation-based methods. The result-s demonstrate that the latent factor approach can model CQA data, especially relationships between questions and answers, in a better way than simply taking questions and answers as parallel pairs, and matching questions via latent factors can be a better alternative for bridging lexical gap-s in questions than matching questions through translation probabilities estimated from question-answer pairs.
Translation-based methods perform inconsistently on Ya-hoo data and Baidu data. On Yahoo data, the best perform-ing model is TAL Q &amp; D  X  AllA , while on Baidu data, the best model is TAL Q  X  D . This result may stem from the different average numbers of answers per question in the two data sets (7 . 1 in Yahoo data v.s. 3 . 8 in Baidu data). When there are only a few answers, the influence of noise in answers is enlarged and the translation from answers becomes more unreliable. Learning only from best answers can be a way to alleviate the hurt of low quality answers, however, this method will also lose some useful information. Therefore, on Baidu data, translation with question-description pairs performs even better than translation with question-answer pairs. The same phenomenon is observed on the comparison of TBLM LDA and TBLM QATM . In fact, when answers are insufficient, the question-answer topic model is more vulner-able to low quality answers than the traditional LDA. This further emphasized the importance of considering answer quality in learning.
We investigate why TBLM SQATM can outperform base-line methods on question retrieval.

Table 3(a) gives an example for comparing TBLM SQATM and LMIR . Candidate question  X  X hat is the eye dr. look-ing for when he/she dilates my eyes? X  is relevant to query question  X  X hat does a doctor look for when they dilate your eyes X , but they share a relatively small proportion of com-mon words. Particularly,  X  X octor X  in query mismatches with  X  X r. X  in candidate which is a abbreviation of  X  X octor X . On the other hand, another irrelevant candidate  X  X hen doctors dilate your eyes, what do they use? X  shares a large propor-tion of common words with the query question. LMIR mea-sures question similarity only based on the common terms they share. Therefore, it ranked the irrelevant one at a high-er position than the relevant one. Our method TBLM SQATM , on the other hand, can group semantically related words like  X  X octor X ,  X  X r. X ,  X  X urse X , X  X edical X , and  X  X ancer X , et al. to-gether in a latent topic space, and successfully recognize the similarity between the relevant candidate and the query.
Table 3(b) gives an example for comparing TBLM SQATM and TAL Q &amp; D  X  AllA (performed best on Yahoo data). Similar to Table 3(a), query question and relevant candidate ques-tion express similar meanings with different words. In the query,  X  X nfants X  was used, while in the relevant candidate the same information need was expressed by the word  X  X a-leverage answers to bridge the lexical gaps. However, we found that translation probabilities were polluted by noise from answers. In the translation table of TAL Q &amp; D  X  AllA  X  X nfant X  can only be translated from words like  X  X ld X ,  X  X ar-rier X ,  X  X ear X ,  X  X ive-in X ,  X  X pinion X , and  X  X ee X  with high proba-bilities. None of these words appear in the relevant candi-date, and TAL Q &amp; D  X  AllA degenerated to LMIR . Therefore, TAL Q &amp; D  X  AllA ranked the irrelevant question which shares more common terms with the query at a higher position. On the other hand, for TBLM SQATM , we checked the topic dis-tributions estimated for the relevant candidate, and found that in the space of the most significant topic, semantically related words like  X  X aby X ,  X  X ody X ,  X  X hild X ,  X  X omen X ,  X  X irth X  and  X  X nfant X  were grouped together and got high genera-tive probabilities. Therefore, although  X  X nfants X  and  X  X aby X  mismatch with each other, their semantic relationship was captured by TBLM SQATM . With answer quality signal-s, TBLM SQATM performed more robustly against the noise in answers, and was capable of leveraging useful information in good answers to solve the term mismatch problem.
Table 3(c) gives an example for comparing TBLM SQATM and TBLM QATM . In this case, the relevant candidate is long and contains some related sense like  X  X urned movie X  and  X  X vd rom X . Therefore, on the term level, the short ir-relevant question seems more likely a good candidate. To capture the semantic similarity between the candidates and the query question, both TBLM SQATM and TBLM QATM try to leverage the topic space. However, when we checked the topic distributions estimated for the relevant candidate, we found that the most significant topic of SQATM contains words like  X  X c X ,  X  X omputer X ,  X  X d X ,  X  X lay X ,  X  X atch X ,  X  X hone X ,  X  X pod X ,  X  X ecognize X ,  X  X indow X  and  X  X vd X  with high probabil-ities, while in the most significant topic of QATM , words Table 3: Examples for comparison between TBLM SQATM and baselines like  X  X ame X ,  X  X ar X ,  X  X ahoo X ,  X  X ome X  X  X c X ,  X  X omputer X ,  X  X ox X ,  X  X riend X ,  X  X un X  and  X  X vd X  dominated. It is clear that the top-ic space of QATM was polluted by the noise in low quality answers. With answer quality signals, SQATM can learn a better topic representation than QATM , and successfully recognize the similarity between the relevant candidate and the query question.
We studied how the number of topics (i.e., K ), the number of iterations in training (i.e., T ), and the trade-off between question topic and answer topic (i.e.,  X  in Equation (5)) influence the performance of TBLM SQATM . All analysis was done on Yahoo data, since it is larger than Baidu data.
For topic number K , intuitively, too few topics are not sufficient to capture the latent structures in questions and answers, while too many topics will introduce noise to the learnt latent space. We tried different numbers of K . Figure 2(a) gives the results. The results verified our intuitions and indicated that 40 is the best choice of topic number.
For iteration number T , on one hand, it is preferred to iterate until convergence; on the other hand, early stopping, as a kind of regularization, could lead to better performance of the model. We tried different numbers of iterations on TBLM SQATM . Figure 2(b) shows the result. From the figure, we did not see evident influence of early stopping and the sampling method quickly get converged after 600 iterations.

Finally, Figure 2(c) illustrates the influence of  X  to the performance of TBLM SQATM . From the figure, we can see that a large  X  will help improve the relevance and  X  = 0 . 8 is the best. Thus, we can conclude that answer topics con-tribute more in solving the term mismatch problem. This result is reasonable, because there is richer information in answers than in questions, and our model can effectively mine the useful information from answers and leverage the information to match questions in a better way.
We propose a supervised question-answer topic model for question retrieval in CQA. With the model, we match ques-tions in a topic space learned from question-answer pairs under the guide of answer quality signals. Experiments on million scale real world CQA data verify the efficacy of the proposed model.
This work was supported by NSFC (Grand Nos. 61170189, 61370126, 61202239), the Research Fund for the Doctoral Program of Higher Education (Grand No. 20111102130003), the Fund of the State Key Laboratory of Software Develop-ment Environment (Grand No. SKLSDE-2013ZX-19), and Microsoft Research Asia Fund (Grand No. FY14-RES-OPP-105).
