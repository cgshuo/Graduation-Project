 1. Introduction
Isaac Lipschits (1930 X 2008) was a Dutch historian and political scientist. One of his works is an annotated collection of election manifestos (party programmes) for the Dutch elections between 1977 X 1998 ( Lipschits, 1977 ). For each election year made available for purchase nationally before the election. To facilitate voters X  decision-making process when comparing parties on election issues, Lipschits manually labelled the manifestos with themes: he segmented the manifestos into coher-ent text fragments, gave them a unique identifier consisting of the party X  X  acronym and a number, and added an index of themes in the back of the book referring to these identifiers.

In the Political Mashup project ( Marx, 2009 ), Dutch political data from 1814 onwards is being digitized and indexed. The political promises and actions, in the news as well as in user X  X enerated content. The data are not only digitized and inte-sons and events. In traditional historical and political research, scientists who work with textual data have to open each  X  potentially relevant document or book separately, and search and browse through them using a static register. This requires a vast amount of manual analysis and time. The goal of Political Mashup is to automate part of this manual effort.
Election manifestos are traditionally considered to be a key source of information on the ideological stance of a political
Lipschits are part of the Political Mashup data. The aims of the work presented in the current paper are: (1) to digitize the starting points for our work are the Lipschits books, scanned as PDF files.

The challenges we face in digitizing the data and building the classifier are: the PDFs contain complicating OCR errors; the text fragments have been assigned multiple themes by Lipschits (more than 6 on average), which resulted in a large number of themes (more than 200) in total, for a relatively small corpus; there are inconsistencies in the labelling schemes over time (e.g.  X  X ishing industry X  vs.  X  X ishing industry policy X ) new themes have emerged over time (e.g.  X  X nformation technology X ) and the same themes may have changing content over time (concept drift); automatic text segmentation leads to shorter and much more segments than in the manually segmented training data.
We took the following approach: We first converted the scanned PDFs to XML data in which each text fragment has been fying election manifestos from 2002 onwards using the data from the 1980s and 1990s. We then evaluated the results by having a domain expert manually assess a sample of the classified data (Section 6 ).

We found that our automatic classifier obtains the same precision as a human classifier on unseen data. Its recall could be improved by extending the set of themes with newly emerged themes. In addition, we found that although a smaller theme set could improve classification scores even more, a more fine-grained classification is preferred by domain experts. 2. Background and related work
Automatic text classification (or document categorization) in predefined categories on the basis of pre-categorized exam-rank the classes according to their estimated relevance to the text. Like any supervised machine learning task, text catego-ally read as words , although many attempts have been made to extend words with n -grams or phrases ( Sebastiani, 2002 ).
Considering each word in the corpus as an independent feature makes text categorization a classification problem in a high-dimensional feature space. The most widely used algorithms for text classification are Support Vector Machines (SVMs) and Na X ve Bayes (NB), although NB is mostly considered as a baseline for more sophisticated techniques. Van Mun (1999) argues that in domains with large amounts of features it is better to use Balanced Winnow ( Dagan, Karov, &amp; Roth, 1997; we used for training the Lipschits classifier in more detail. 2.1. Political text classification
Automatic classification of political texts using supervised learning techniques has been applied to legislative texts, parliamentary documents, manifestos, and even speeches of the Dutch Queen ( Breeman et al., 2009; Hillard, Purpura, &amp; the European Union has trained a multi-label classifier on EU legislation labelled with the official EU EUROVOC thesaurus for each input document. Their JEX system reaches an R -precision of 0.56 for English documents. On average, documents are in the U.S. since 1947. The bills have been labelled according to a hierarchical classification scheme comprising 20 major mans (measured as inter-rater agreement: Cohen X  X  j  X  0 : 8 at the subtopic level), while reducing human labor effort by about 80%. An important difference with our work is that the topics in their classification scheme are mutually exclusive; approach.
 political texts depending on whether they support or oppose a given political issue under discussion ( Awadallah, Ramanath, political texts tend to contain few sentiment-bearing words. Hirst, Riabinin, and Graham (2010) aim to classify Canadian leg-sifier not distinguishes between the ideologies of the parties but the roles of the party (government or opposition). More recently, Diermeier, Godbout, Yu, and Kaufmann (2012) succeeded in predicting a senator X  X  ideological position with 92% accurracy based on terms associated with conservative and liberal ideologies. They conclude their paper with the wish to be able to investigate the content shift of ideologies over time. Yu et al. (2008) use U.S. senate speeches as documents and the speakers X  party affiliation as classification labels. They formulate a number of recommendations for text classifica-the relevant issues change over time and new themes can emerge in new data. Fourth, the data is not independent: in the case of political speeches, speakers respond to each other, adopting themes and vocabulary from each other. The fifth prob-classifier, we face the first three challenges.

Radev (2010) develop a statistical learning model that uses word choices to infer topical categories covered in a set of speeches and to identify the topic of specific speeches. Their method is unsupervised: they do not use a list of predefined topics, and they do not need a set of human-labelled examples. They use the topic model to examine the agenda in the
U.S. Senate from 1997 to 2004. From the topics that are identified by their algorithm, they manually infer category labels and they create a hierarchical topic scheme. 2.2. Dutch political data
Some work has focused on Dutch political data: Jijkoun, Marx, De Rijke, and van Waveren (2007) describe an electoral search engine aimed at helping the general public to make an informed decision whom to vote for in the 2006 elections. They indexed news articles, blogs and election manifestos, and built a search engine in which the users can search by theme or free text. In this work, 179 themes were predefined by the Instituut voor Publiek en Politiek (IPP). thematic queries by users of the website. In total, 28 thousand thematic searches were performed compared to 117 thousand matic labelling of political texts for application in a search system.

Gielissen and Marx (2009) describe the development of PoliDocs.nl, a Web Information System for the disclosure of Dutch parliamentary publications. They transformed long PDF documents containing the meeting notes of one day into a uniform
XML format and make them available in a new Web Information System. The authors present a list of 15 requirements for the Information System that were formulated by end-users through a collective Dutch weblog. Most requirements are tech-line where parliamentary papers that belong together are grouped X  X . This requirement indicates that for end users of a search system for political data, time aspects are important, and grouping of documents (by party or theme) is necessary. Gielissen year and political party.

Kaptein and Marx (2010) use transcripts of meetings of the Dutch parliament for experiments with focused retrieval and result aggregation . They evaluate the above-described PoliDocs.nl interface. They find that users performed thematic search tasks up to eight times faster using focused retrieval and faceted search compared to standard full document retrieval. They stress the importance of time-and party-specific information when searching for themes in political data: for users,  X  X  X t is interesting to see how that theme developed over time and which political parties claimed that theme X  X  (p. 425). 2.3. Temporal change in text classification
As we have mentioned above, an important aspect of political data is temporal change. Forman (2006) discusses the prob-lem of topic drift in classifying news data. The authors learn a new classifier for each day, and they augment the bag-of-words feature vector with additional binary features that are generated by the predictions previous daily classifiers would have made for today X  X  cases. Mour X o et al. (2008) show evidence that time should be taken into consideration in classifica-tion techniques and algorithms. According to Rocha, Mour X o, Pereira, Gon X alves, and Meira (2008) , the performance of clas-sification results) and the time effects (training data from mismatching time moments give worse classification results).
They evaluate their approach on scientific articles. The trade-off between training set size and temporal match is also rele-history (election years: 1977, 1981, 1986, etc.), while scientific literature and news can be dated and timed at finer granu-ical texts. 3. Converting the PDF books to enriched publications tion. The body of data are the manifesto texts, which have been manually segmented by Lipschits. Each segment has been labelled with a number. The numbering starts at 1 for each party. We call these number labels  X  X opic numbers X . At the back of manifestos per party (e.g. CD, CDA, D66, etc.) by topic number. See Fig. 1 a for an example of the register.
The PDFs were converted to XML using pdftohtml with the -xml
We used a Perl script for textual clean-up; the process is described in the next two subsections. 3.1. Body of data: numbered paragraphs
In the election manifesto texts, the start of each new topic is labelled with a topic number. In the source PDF, topic num-bers are preceded by a bullet, so the task of passage segmentation can be defined as: finding a bullet followed by a number, and then saving all text between this number and the following bullet as content belonging to the topic number. However, we had to take into account a number of problems. First, during the scanning process the edge of the right page sometimes became part of the scan of the left page, so for a number of pages additional noisy text fragments (sometimes containing a bullet) were part of the XML data. Another scanning problem was that a few pages were mistakenly not scanned, as a result of which some topic numbers were missing.

In addition, we came across numerous OCR problems. We included a list of frequently recurring OCR corrections (such as  X  X O X  for the party name  X  X VD X ) in our clean-up script. Some bullets were not recognized, or a bullet was recognized where there was none. Moreover, numbers behind bullets were not always recognized (or recognized as letters e.g.  X  X  X  for  X 1 X ). This was further complicated by inconsistencies in text formatting between party programmes: sometimes bullets were used for starts at the number). We solved these problems by writing a set of regular expression patterns that try to find and recon-struct all topic numbers for each party, using the presence of bullets and expectations on the following number based on the preceding number.

In order to assess the quality of the passage segmentation, we counted (1) the number of automatically identified pas-sages and (2) the number of automatically identified text passages that start with a number (indicating correct segmenta-tion). The results are in Table 1 .
 3.2. Register: per theme the listed paragraph numbers
When processing the theme register in the back of the book, we dealt with two challenges. First, we had to decide which subsequent lines should be concatenated and which not. For example, in Fig. 1 , the second line should be concatenated to the the OCR had sometimes incorrectly recognized column splits between party names and number sequences. For example, in trailing number lines (such as the line starting with  X  X 118, 121 X  X ) are intertwined with incorrect column splits.
Manual cleanup was not an option because the total number of index lines per year was around 5000. We solved the deci-sion problem of concatenating subsequent lines using a set of heuristics that takes into account punctuation, a set of likely topic names (extracted from the register itself), the set of party names, and a regular expression that matched sequences of numbers with possible OCR errors. 3 We solved incorrect column splits using two parallel stacks of party names and number sequences, pairing them up while keeping track of incomplete pairs. We checked the soundness of the converted theme regis-ters by assuring that there are no remaining party names that are not followed by a number sequence and no number sequences that are not preceded by a party name. 3.3. Results of digitizing the election manifestos
Using the procedure described above, we were able to convert all texts from the election manifestos of 1998, 1994 and 1986 to annotated XML files of acceptable quality. The result are three XML files in which each text fragment has been anno-tated with the themes from the register. For the 1981 manifestos, the quality of the scans was too poor. The 1977 and 1989 manifestos use another encoding of topic numbers to text fragments, leading to new and more severe OCR problems. There-fore, we did not convert these years to annotated XML files yet.

Having the manifestos in a uniform XML format with all paragraphs annotated with their themes has advantages for both electronic publishing of the data and diachronic comparative data analysis. We briefly discuss these two aspects here. Note that both training data and automatically classified manifestos are stored in the same XML format. With a simple XQuery we can create the input data (in e.g. SPSS or CSV format) for performing diachronic comparative saliency analysis of the parties
Laver (2001) : i.e., how much attention gives each party to each theme (election topic) and how does that change over the years?
Using XSLT and CSS stylesheets we can publish the manifestos as hyper documents with a number of additional features not present in printed or scanned format. First, next to creating an inverted (back of the book) theme-index with hyperlinks to the paragraphs, we can create such an inverted theme-index for each manisfesto, as an alternative to a table of contents for that manifesto. This index also provides a good impression on the saliency of the themes of that party in that election with the themes ordered by their dispersion value. An example is presented in Fig. 2 for the manifesto of the  X  X ensioners party X  in 1998. We see that dispersion is a good indicator of saliency of a theme: half of the themes in the top 20 directly relate to themes relevant for elderly people. Second, we can  X  X ag X  each paragraph with its themes, making it quickly clear verted index of the specific party. Third, we may allow users to create  X  X heme-views X  of a manifesto: only show the para-graphs labelled by that theme. This makes it easy to compare the standpoints of different parties on a specific theme, or diachronically compare one party on one theme.

We have made the following data available for further research: the transformed manifestos in XML format for the years 1986, 1994 and 1998; the automatically labelled manifestos in XML for the years 2006, 2010 and 2012; the RelaxNG schema which validates these XML files; the XSLT stylesheet which transforms the XML into hyperlinked HTML documents as de-scribed above; the HTML output for all XML files; the XQuery creating a saliency matrix in CSV, and the output for the year 1998. The data is available at http://data.politicalmashup.nl/lipschits . 4. Automatic classification
We aim at developing a Lipschits classifier that can assign themes to unseen Dutch election manifestos that were written after Lipschits X  work, i.e. from 2002 onwards. Since we do not have manually labelled data from these recent years, we have classifiers that we use, Support Vector Machines and Balanced Winnow. In Section 4.3 , we optimize their parameters, and in
Section 4.5 , we compare their performance on the classification task. 4.1. Classification data
As described in the previous section, we transformed the digitized Lipschits books to a collection of short texts with asso-ciated themes. See Table 2 for the statistics per year. The table shows that although the texts are only a few hundred words long (317 on average, with a standard deviation of 252), Lipschits assigned more than six themes per text on average.
The large number of themes assigned per text segment seems to suggest that Lipschits X  segmentation led to segments that are not coherent and may be too long. We looked into the possibility of splitting the texts in smaller segments, and classi-fying them separately, but we found that even within one sentence often more than one Lipschits theme occurs (see for example the text in Fig. 3 ). Furthermore, Lipschits intended his labeling to apply to entire paragraphs. The large number of themes assigned by Lipschits is not so much a result of long text segments (they are only 317 words on average) but of his very detailed classification system, and the political domain, where themes tend to be interrelated. 4.2. Classification method
The classification task we consider is a multi-label classification task, with each Lipschits theme being a class. We used quency to 2 and the minimum corpus frequency to 3. The LCS has one classifier built in: Balanced Winnow ( Littlestone, 1988;
Dagan et al., 1997 ), and it has an interface to the linear SVM
The Balanced Winnow algorithm learns two weights for each feature t and class c ; W
W and W is the effective weight of the feature. In the training phase, Balanced Winnow goes through the set of training examples in multiple iterations. The score of a document d for a class c is computed as where s  X  t ; d  X  is the strength of the term t in d (e.g. its tf-idf weight).
 In the original Winnow implementation ( Littlestone, 1988 ), a document d is considered to be part of the class c if when SCORE  X  c ; d  X  &gt; h  X  . All positive examples with a score below h , all negative examples with score above h
When a training document belonging to class c scores below h for c , the positive weight W plied by a (it is promoted) and the negative weight W t ; c not belong to class c but scores above h , the positive weight of the active feature is demoted and the negative weight is promoted.
 In SVM, a hyperplane is constructed that separates the positive from the negative training examples in the feature space.
The best hyperplane is the one that has the largest distance to the nearest training examples in both classes. The larger the realistic data, there is no hyperplane that can completely separate the positive from the negative training examples. Cortes and Vapnik (1995) introduced the soft margin approach, in which a hyperplane is constructed that separates the training set with a minimal number of errors. This approach requires a training parameter c that controls the trade-off between the width of the margin and the number of classification errors made during training.

In addition to the Winnow and SVM parameters, the LCS has three parameters that govern the selection of classes in the selected classes is determined per text using three parameters: the maximum number of returned classes ( maxranks ), the minimum number of returned classes ( minranks ), and a threshold on the classification score. 4.3. Optimizing the classification parameters
SVM X  X  linear kernal has one parameter c , the tradeoff between training error and margin. For Winnow, we optimized the values for the threshold parameters h  X  and h and its weight update parameters a and b . We also tuned the class selection parameters minranks , maxranks and threshold in the LCS. 5
For tuning the parameters, we randomly partitioned the 1998 data into 10 equally sized parts and took one of the parts as optimization criterion. For the SVM parameter c , we followed the suggestion by Hsu, Chang, and Lin (2003) to test exponen-tially growing values of c , from 2 13 to 2 13 . We obtained the optimal F1-score for c  X  2.

For the Winnow parameters a ; b ; h  X  and h , we used a range of parameter values comparable to Koster and Beney (2007) and we performed a full grid search. The resulting F1 landscape exhibits a lot of variation, with optimal values in divergent areas. The best scores appear to lie in the area of high a and h optimal parameter combination.

With the optimal choice for the SVM and Winnow parameters we tuned the class selection parameters. We set minranks to 1 so that each text gets assigned at least one theme. In the case of Winnow, we used the natural threshold of 1 (like in
Koster &amp; Beney (2007) ). For SVM, we searched the optimal threshold by varying its value from 2 to 2 in steps of 0.2. For maxranks, we searched over the values from 6 to 20 in steps of 2. For SVM, we observe large differences in precision scores when varying threshold and maxranks. Taking precision into account besides F1, the optimal parameter combination is a maxranks value of 14 and either a threshold of 0.6 (with a good F1 score but low precision) or a threshold of 1 (with lower
F1 but much higher precision). For Winnow, the differences in F1 for the best scoring values of maxranks are small; max-ranks values of 10, 12 and 14 all give reasonable results for precision and F1. 4.4. Evaluation measures As evaluation measures we use precision, recall and F1.

We will also report Mean Average Precision (MAP), which gives the quality of the generated ranking of the themes per text: where P  X  k  X  is the precision at rank k ; n is the total number of assigned themes, n mean of this score over all texts in the test set. 4.5. Comparing Winnow and SVM
We trained both SVM and Balanced Winnow with parameter settings as described in Section 4.3 on the data from 1986 and 1994 and evaluated the trained models on the data from 1998. The results are in Table 3 . For SVM, we show the results for two threshold values because the threshold that gave an optimal F1 score led to a very low precision. For Winnow, we on F-scores for individual texts  X  n  X  826  X  obtained by Winnow 1 and SVM 2 showed that Winnow gives significantly better obtained by Winnow (46.9% compared to 47.0%), but against at the expense of a much lower precision (49.3% compared to 70.5%). Considering the results in Table 3 , we decided to continue our experiments with Balanced Winnow as classifier. Because the differences between maxranks = 10 and maxranks = 14 are small, we opt for the somewhat higher precision and
MAP and set maxranks to 10. 8 5. Classification experiments
In all classification experiments, we used the texts from the 1998 manifestos as test data, and the 1986 and 1994 man-ifestos as training data. In all experiments, we used the optimized Winnow parameters that were found by tuning on a (held-out) subset of the 1998 data (Section 4.3 ). 5.1. Selection of training data
We first evaluated the use of the years 1986 and 1994 as training data, both separately and in combination. As a compar-ison, we performed 10-fold cross validation experiments on the 1998 data. First, it processes the training set and fills two arrays:
X  X  X  c 1 ; c 2 ; ... c N  X  , where N is the number of training examples and c theme t j is listed as often as it occurs in the training set.

For each example e k in the test set, the script randomly takes a number x from X , and it randomly takes x times a theme t from Y , while ensuring that there is no t j occurs more than once for e assigning the labels t j to the test example.

The results are in Table 4 . Because not all themes from the 1998 texts are present in the 1986 or 1994 data, a classifier trained on the earlier years and evaluated on 1998 data can never obtain 100% recall. We calculated the maximum recall for each training set as follows. Let T test be the set of themes in the test data, T recall is:
Table 4 shows that training on the data from 1994 gives much better results than training on the data from 1986. Adding the 1986 data to the 1994 data results in a slightly lower recall but an improvement in MAP. The lower recall is probably due to an increase of the number of themes in the training data, making the classification problem more complex. Because of changes in the theme set, adding a new year to the training data tends to add more confusion. For example, having both the themes  X  X ishing industry X  from 1986 and  X  X ishing industry policy X  from 1994, which cover the same topics but in different years, yields a more complex classification problem than having only one of them.

Table 4 also shows that the experiments with the (1986 and) 1994 training data do not reach the level of the cross val-idation experiment. This means that the data from earlier years are only partly representative for the 1998 data. We inves-tigated the effect of non-overlapping themes between the years: 42 themes from the 1998 data do not occur in the training not occur in the 1998 data (e.g.  X  X ids X ,  X  X roperty tax X ).

Of course, better results could be obtained if we would use the theme set from 1998 to train the classifier on. However, theme set is unknown. Since the theme set of the test year will be unknown, we not only aim at selecting the best training data but also the best theme set from the training data. We did an experiment in which we used the data from 1986 and 1994 to train on, but changed the theme set: all themes from 1986 + 1994 or only the 1994 themes. The results of this exper-iment are in Table 5 . We also included the results for using the 1998 theme set (third row) as reference.
The first row of Table 5 is the same as the fourth row in Table 4 . The second row of Table 5 shows that by keeping only the themes from the most recent year in the training data we obtain a higher precision. The higher recall in the third row of
Table 5 shows that non-overlapping themes are indeed a problem for classifying across years. We continue our experiments with the data from both 1986 and 1994 as training data, but using only the theme set from 1994. 5.2. Variants of the bag of words representation There is evidence that text classification can be improved by adding word bigrams ( Braga, Monard, &amp; Matsubara, 2009; corpus size ( Sebastiani, 2002 ). We extracted all within-sentence bigrams from the texts in our corpus and added them to the bag of unigrams for each text. We also lemmatized the texts using the output of the Dutch morpho-syntactic analyzer Frog ( Van den Bosch, Busser, Canisius, &amp; Daelemans, 2007 ) and experimented with lemmatized unigrams and bigrams. Finally, we experimented with removing stopwords from the feature set. The results of classification experiments with the variants of the data are in Table 6 .

Since none of the alternative text representations gives results above the unigram baseline, we decide to continue our exper-iments with unigrams only, wordforms rather than lemmas and without stopword removal. 5.3. Merge themes
In the experiments presented thus far we have trained on a relatively large number of themes in a relatively small num-ber of texts: 210 themes 10 occurring in 1718 texts (1986 + 1994 data). We investigated whether theme merging can be prof-(e.g.  X  X ruba X  and  X  X etherlands Antilles X ). For the purpose of reducing the number of themes, we clustered similar themes to-a term vector with relative term frequencies for all (lemmatized) terms occurring in the theme subcorpus. For each pair of clustered the themes using an agglomerative hierarchical clustering approach ( Day &amp; Edelsbrunner, 1984 ): 1. The clustering process is initiated by making each theme its own cluster. 2. Iteratively, the two clusters are selected that are the most similar following a complete-linkage strategy: The similarity a theme in cluster B. range of  X  1 : 0 ... 0 : 2  X  in steps of 0.05.

For each of the threshold values, we replaced the original themes by the merged themes in the example data (both the training and the test set). Then we performed a number of analyses on the resulting data in order to decide on the optimal similarity threshold for merging: (1) we evaluated the classification performance (in terms of F 1) with the merged data when classifying the 1998 data; (2) we calculated the entropy of the theme distribution: least one text); and (3) we counted the proportion of themes on average attached to a text: in which D is the number of texts, t d is the number of themes on a text and t is the total number of themes in the set.
The rationale behind (2) and (3) is to find the optimal level of granularity: On the one hand we would want a classification problem with low entropy, making the classification problem easier, and on the other hand we do not want to move too far Table 7 shows the calculated statistics.

Table 7 shows that the lowest entropy is obtained for a very low number of themes (4). Although this makes the classi-fication. If we multiply entropy with the relative number of themes on a text, then we find the lowest (optimal) H A at the similarity threshold of 0.85. At that point, the number of themes has been reduced from 210 to 120. Among these 120, there are 37 clusters of merged themes and 83 themes that were not merged. Examples of merged themes are:
We see that conceptually similar themes are clustered for a similarity threshold of 0.85 on the 1994 theme set. After opti-mization of the 1994 theme set, we applied the theme merging to the 1998 theme set. We asked a domain expert, a political journalist, to give her opinion on three versions of the 1998 theme set: 1. the set of 101 themes after merging similar themes with a threshold of 0.85 (optimum for the 1994 data); 2. the set of 140 themes after merging similar themes with a threshold of 0.90 (a more conservative merging strategy); 3. the set of 218 original Lipschits themes, without merging.

According to our expert, it would be very difficult to judge the merged themes as relevant or irrelevant because some parts may be relevant and others may not. In addition, in the theme set resulting from the 0.85 threshold, some of the merged themes were noisy, such as  X  X ports policy, crime prevention, law enforcement, police policy, addiction X . The theme canal shipping, sea ports and sea shipping X ) as irrelevant when the text was about airports. Therefore, we decided not to merge the themes from the 1998 set but keep the original, fine-grained Lipschits theme set. 5.4. Comparing the automatic classifier to Lipschits himself
As a reference for the quality of the automatic classifier, we asked a domain expert to manually evaluate the labels as-signed by Lipschits himself (for more details see Section 6 ). When presenting the domain expert with 50 randomly selected texts from the 1998 data, she thought they were automatically labelled. The quality of the manual classification can be seen as a reasonable upper bound for our classifier. The results are in Table 8 .

When we compare the two rows in Table 8 , we see that the Lipschits assigned more themes per text than our automatic classifier, leading to a higher recall and F 1 score, but that precision is almost equal. 6. Labelling unseen election manifestos 6.1. Annotation and evaluation setup
We built an automatic Lipschits classifier that can assign themes to unseen election manifestos. We trained the classifier on all available data from 1986, 1994 and 1998, but only kept the themes from the most recent year: 1998. We used a stan-dard bag-of-words representation without lemmatization and removal of stopwords. We configured the classification sys-tem to assign a minimum of 1 and a maximum of 10 labels to each text fragment, with a Winnow threshold of 1.
We obtained the unlabelled election manifestos from 2006, 2010 and 2012. lished as PDF files, and automatically turned into HTML while trying to preserve the paragraph structure. The HTML files were manually checked on paragraph breaks and bad breaks were removed. The paragraphs in the unlabelled data are considerably shorter than the text fragments in the training data: around 40 words on average (in the older Lipschits data, multiple para-graphs were often brought together under one topic number). Statistics on the data from 2006 onwards are in Table 9 .
We automatically labelled the texts using our Lipschits classifier. Then we asked a domain expert, a political journalist, to manually evaluate a sample of the automatic annotations. The assessment interface is shown in Fig. 3 .

We measured precision and recall by counting the number of assigned themes judged as relevant, the number of assigned themes judged as not relevant 13 and the number of themes that the expert typed in the field for missing themes: and 6.2. Evaluation
The expert assessed 200 text fragments picked randomly from the data. She judged seven of these as not assessable be-cause they were too short, so 193 texts remain. She was expected to actively add themes if she thought a theme was missing.
Although there was a list of themes available through auto-completion, the expert often chose to formulate new terms. results are in Table 10 .

The data from 2006 seem more difficult to classify than the data from 2010 and 2012, seen from the lower Precision, Re-call and F -score obtained for that year. However, this difference is not significant according to Welch X  X  t -test.
When we compare the results obtained with the automatic classification of the 2006 X 2012 data to the evaluation of the P -value for the difference between the means is 0.038.

One of the likely reasons for the difference is the effect of the  X  X heme gap X : for themes in the 2012 data that did not exist yet in 1998 we did not have any training data. In order to estimate this effect, we performed an error analysis: We manually went through the 243 false misses that the expert reported for the 2006 X 2012 data and categorized them in three groups: 1. The missed theme already existed in the 1998 data, possibly with different spelling or in the form of a synonym of the term formulated by the expert (e.g.  X  X are X  vs.  X  X ealth care X , and  X  X nvironment X  vs.  X  X nvironmental policy X ); 2. The missed theme is related to another theme in the 1998 data, possibly more general or more specific (e.g.  X  X ublic trans-port X  vs.  X  X ailway transport X ,  X  X mall and medium enterprises X  vs.  X  businesses X ,  X  X mployment X  vs.  X  X nemployment X ) but the missed theme is not new; 3. The missed theme is completely new compared to the 1998 theme set (e.g.  X  X utrition X ,  X  X uropean regulations X ,  X  X axi law X ,  X  X ames X ,  X  X ublic broadcasting X ,  X  X ocial coherence X ).

We counted the themes in each category over the years; the results are in Table 11 . 7. Conclusions
We digitized three years of Dutch election manifestos annotated by the Dutch political scientist Isaac Lipschits: 2574 short ( 300-words) texts that Lipschits labelled. There are more than six political themes per text on average. We used these data to train a classifier that can automatically label new, unseen election manifestos with themes.
 detail seems to be preferred by domain experts; (2) change of themes over the years affects recall of the learned classifier, but (3) precision is comparable to the precision obtained by a human expert labeller. Thus when using old political texts to classify new texts, work is needed to link and expand the set of themes to newer topics.

In future work, we will create a faceted search interface (using the themes as facets) for the digitized election manifestos, so that our work can be used by interested researchers, students and journalists. It may be interesting to investigate the work is to investigate approaches to expand a political theme set with recently emerged themes. This could be done using a topic identification method ( Quinn et al., 2010 ) on contemporary political texts.
 Acknowledgements
This research was supported by the Netherlands Organization for Scientific Research (NWO) under Project No. 380-52-005 (PoliticalMashup). We thank Marjolein Bax for her annotation work, and Lars Buitinck for his help with the OCR X  X d texts. References
