 The nDCG measure has proven to be a popular measure of retrieval effectiveness utilizing graded relevance judgments. However, a number of different instantiations of nDCG ex-ist, depending on the arbitrary definition of the gain and discount functions used (1) to dictate the relative value of documents of different relevance grades and (2) to weight the importance of gain values at different ranks, respectively.
In this work we discuss how to empirically derive a gain and discount function that optimizes the efficiency or sta-bility of nDCG. First, we describe a variance decomposition analysis framework and an optimization procedure utilized to find the efficiency-or stability-optimal gain and discount functions. Then we use TREC data sets to compare the op-timal gain and discount functions to the ones that have ap-peared in the IR literature with respect to (a) the efficiency of the evaluation, (b) the induced ranking of systems, and (c) the discriminative power of the resulting nDCG measure. Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 In-formation Search and Retrieval:Retrieval models General Terms: Experimentation, Measurement, Relia-bility Keywords: Evaluation, nDCG, Generalizability Theory
The evaluation of retrieval systems has been a significant area of research in IR. Evaluation measures play a critical role in the development of retrieval systems either as metrics in comparative evaluation experiments, or as objective func-tions to be optimized in a learning-to-rank fashion. Due to
We gratefully acknowledge the support provided by NSF grants IIS-0533625 and IIS-0534482 and by the European Commission who funded parts of this research within the Tripod project under contr act number I ST-FP6-045335. their importance, dozens of measures have appeared in IR literature, with average precision being the dominant one.
One of the main criticism traditional evaluation measures, such as average precision, have received is due to the as-sumption they make that retrieved documents can be con-sidered as either relevant or non-relevant to a user X  X  request. In other words, traditional measures treat documents of dif-ferent degrees of relevance as equally important. Naturally, however, some documents are more relevant to a user X  X  re-quest than others and therefore more valuable to a user than others.

The nDCG measure [10, 9] has proven to be one of the most popular measures of retrieval effectiveness that uti-lizes graded relevance judgments. The underline model of user search behavior on which nDCG is based makes two assumptions: (1) highly relevant documents are more valu-able to the user than marginally relevant documents, and (2) the greater the rank a relevant document appears the less valuable to the user that document is.

In the framework used to define nDCG, first relevance scores are mapped to relevance grades, e.g. a score of 3 is given to highly relevant documents, a score of 2 to fairly relevant documents and so on. Relevance scores are viewed as the gain returned to a user when examining the docu-ment. Thus, the relative value of relevance scores dictates how much more valuable for instance a highly relevant doc-ument is to a user than a marginally relevant. Even though, relevance scores were used directly as gains when nDCG was originally introduced, alternative gain functions that map gain values to relevance scores have appeared in the liter-ature. To account for late arrival of relevant documents, gains are then discounted by a function of the rank. The discount function is viewed as a measure of the patience of a user to step down the ranked list of documents. As in the case of gains, a number of different discount functions has appeared in the literature. The discounted gains are then summed progressively from rank 1 to k and this discounted cumulative gain is normalized to range from 0 to 1, resulting in the normalized discounted cumulative gain (nDCG).
Hence, nDCG can be considered as a functional of a gain and a discount function. By utilizing different gain and dis-count functions one is able to accommodate different user search behavior patterns on different retrieval scenarios.
Even though nDCG offers a flexible family of measures so far the selection of the gain and discount functions has been done rather arbitrarily, based on speculations of the search behavior of an average user and speculations of the correlation of the measure to user satisfaction. For instance, Burges et al. [7], introduced an exponential gain function (2 rel(r)  X  1,whererel(r)istherelevancescoreofthedocu-ment at rank r) to express the fact that the gain of a highly relevant document is not just twice the gain of a relevant one for an average user. Further, the logarithmic discount function (1 /log ( r + 1)) dominated the literature compared to the Zipfian one (1 /r ) based on the speculation that the gain a user obtains by moving down the ranked list of docu-ments does not drop as sharply as indicated by the Zipfian discount.

Despite these reasonable speculations, Al-Maskari et al. [1] exhibited that cumulative gain without discounting (CG) is more correlated to user satisfaction than discounted cumula-tive gain (DCG) and nDCG (at least when computed at rank 100). This result not only questions the overall utility of the discount function but most importantly underlines the need for a methodological selection of gain and discount functions. However, given the infinite number of possible gain and dis-count functions, the vast differences in user search behavior, the many possible retrieval tasks and the difficulty in mea-suring user satisfaction, a complete analysis of the different gain and discount functions with respect to user satisfaction is prohibitively expensive, if at all possible.

In order to methodologically reduce the space of possible gain and discount functions, a number of correlation studies has been conducted to examine whether different functions lead to equivalent nDCG measures regarding the induced ranking of systems. Voorhees [14] utilized nDCG in TREC Web Track evaluation weighting highly relevant documents by factors 1 to 1000 in relation to marginally relevant docu-ments and she concluded that varying the gain function leads to different ranking of systems. In a similar study, Kek  X  al  X  ai-nen [11] also examined how different weighting schemes of relevance scores affect the ranking of systems and similarly to Voorhees [14] concluded that the larger the relative dif-ference between relevance grades, the more the ranking of systems deviates from that in the binary case. Furthermore, by comparing the rankings of systems induced by the dis-counted cumulative gain (DCG) and cumulative gain with-out discounting (CG), she demonstrated that discounting the gain values also alters the induced ranking of systems.
Given the fact that nDCG variations result in different rankings of systems and thus they evaluate different aspects of retrieval effectiveness along with the difficulty in studying gain and discount functions with respect to user satisfaction, one can compare different variations of nDCG based on other desirable properties of the re sulting measure. For instance for different gain and discount functions, one can investigate how informative the resulting variations of nDCG are, i.e. how well do they summarize the relevance of the underline ranked list of documents [2], how discriminative they are, i.e. how well do they discriminate good from bad systems [12], or how stable they are, i.e. how different the rankings of systems are over different sets of queries [6]. Sakai [13] com-pared the effect of a number of different gain and discount functions on the discriminative power of nDCG.

In this paper, we adopt the variance component analy-sis framework proposed by Bodoff and Li [4] to measure the stability/efficiency of the resulting nDCG measure when dif-ferent gain and discount functions are utilized. Based on this framework, we define a stability-or efficiency-optimal gain function by treating gain values of relevance grades as un-known variables and optimizing for the aforementioned sta-bility/efficiency measure. We compare the resulting function to both the linear and the exponential variates that have ap-peared in the literature, both in terms of stability/efficiency and induced rankings of systems. Similarly, we also define a stability-or efficiency-optimal discount function and com-pare it against the Zipfian, the log and a linear function. Further, we also define a Pareto optimal combination of gain and discount function, i.e. the combination of gain and dis-count function that maximizes the minimum stability. Fi-nally, we explore whether the stability-(efficiency-) optimal gain and discount functions lead also to an nDCG measure with high discriminative power [12].

The rest of the paper is organized as follows: In Section 2 we describe the methodology used to obtain stability-or efficiency-optimal gain and discount functions, in Section 3 we present the results of our methodology and conclude in Section 4.
In this section, we describe a methodology to numerically derive stability-(efficiency-) optimal gain and discount func-tions. First, we adopt the methodology used by Bodoff and Li [4] to assess the reliability of an IR test collection.
Given an evaluation measure, a number of retrieval sys-tems, a set of queries and a document collection, Bodoff and Li considered two sources of variability in the observed system scores of the retrieval systems when they are run over the given queries, (a) the actual performance differ-ences between systems, and (b) differences in the nature of the queries themselves. This way, Bodoff and Li [4], quanti-fied the quality of the test collection as the proportion of the total variability observed in the scores of the retrieval sys-tems that is due to actual performance differences among these systems.

In a similar manner, different sources of variability can be considered and quantified. For instance, earlier than Bodoff and Li, Banks et al. [3] considered, as an additional to the systems and queries source of variability, the judges that assess the relevance of the documents to the queries.
In this work, we consider the evaluation measure itself as a source of variability. In particular, given a number of retrieval systems, a set of queries and a document cor-pus, we consider gain and discount function of nDCG as unknown variables and we select the ones that maximize the proportion of variability due to actual performance dif-ferences among systems. The proportion of variability re-flects the stability of the evaluation measure, and thus by maximizing this proportion we maximize the stability of the measure. Furthermore, the more stable a measure is the less queries it requires to reliably evaluate the retrieval systems. Thus, by maximizing stability we also maximize efficiency in terms of required queries.

We numerically computed the stability optimal gain and discount function by employing (a) variance decomposition analysis of the nDCG scores [3, 4, 5] and (b) optimization. In the following subsections, we describe both components of our methodology in details.
Assume an experimental design that involves n s systems run over a sample of n q queries resulting in a set of n s ranked lists of documents. Further assume that each list of documents is evaluated by nDCG and the overall quality of a system is captured by averaging the nDCG values over all topics. Systems, then, are ranked by their mean scores, i.e. mean nDCG.

Hypothetically, if a second set of topics was available, the systems could be run over this new set of topics and new mean nDCG scores (and consequently new ranking of the systems) would be produced. The question that naturally arises is, how many topics are necessary to guarantee that the mean nDCG scores do not change radically when two dif-ferent query sets are used, or alternatively how many topics are necessary to guarantee that the mean nDCG scores of systems reflect their actual performance?
Given different sets of topics one could decompose the amount of variability that occurs in mean nDCG scores (as measured by variance) across all sets of topics and all sys-tems into three components: (a) variance due to actual per-formance differences among systems X  system variance ,(b) variance due to the relative difficulty of a particular set of topics X  topic variance , and (c) variance due to the fact that different systems consider diff erent sets of topics hard (or easy) X  system-topics interaction variance . Note that among the three variance components, only the variance due to sys-tems and system-topics interactions affect the ranking of systems X  X t is these two components that can alter the rel-ative differences among mean nDCG scores, while the topic variance will affect all systems equally, reflecting the overall difficulty of the set of topics.

Ideally, one would like the total variance in mean nDCG scores to be due to the actual performance differences be-tween systems rather than the other two sources of variance. If this would be the case, running the systems over differ-ent topic sets would result in each system having identical mean nDCG scores regardless of the topics used, and thus mean nDCG scores over a single set of topics would be 100% reliable in evaluating the quality of the systems.
In practice, retrieval systems are run over a single given set of topics. The decomposition of the total variance into the aforementioned components in this case can be realized by fitting an ANOVA model into nDCG scores [3, 4, 8]. Given the variance components tools from Generalizability Theory [5] can be used to quantify the stability of the eval-uation.
There are two coefficients that predominate in Generaliz-ability Theory to quantify the stability of the evaluation, the generalizability coefficient and the dependability coefficient, with the former reflecting the stability of the system rank-ings and the latter the stability of the system effectiveness scores. They both lie in a zero to one range.

The former coefficient is the ratio of the system variance and the variance in relative nDCG scores (i.e. in system rankings), that is the summation of the system and system-topic interaction variance, and it can be interpreted as an approximation to the squared correlation between the relative mean nDCG scores observed over the given set of topics and the relative mean nDCG scores that would be observed if infinite number of topics was available.

The dependability coefficient,  X , is the ratio of the system variance and the total variance, and it can be interpreted as an approximation to the squared correlation between the mean nDCG scores observed over the given set of topics and the mean nDCG scores that would be observed if infinite number of topics was avail-able. Note that both  X  and E X  2 decrease with the topic set size. Further note that E X  2 is always larger than  X . In our experiments we employ only the latter coefficient since stable scores infer stable rankings.

Also note that the computation of the two coefficients is done independently of the estimation of the variance compo-nents. That is, first the variance components are estimated over a set of available topics (50 topics in our experiments). Then, the two aforementioned coefficients are using these es-timates to project reliability scores to topic sets of any size. The topic set size in the computation of the coefficients does not need to be the same as the topic set size used to estimate the variance components (it can even be larger).

As mentioned before, in this work we consider the gain values for different relevance grades and the discount fac-tors for different ranks used in nDCG as unknown variables. Given a fixed-size topic set we would like to obtain the gain values/discount factors that maximize the stability of the mean nDCG scores of the systems.
In the optimization process employed, we use  X  as the objective function to maximize with respect to the gain val-ues/discount factors employed in nDCG.

Note that nDCG is a scale-free measure with respect to both the gain values and the discount factors in the sense that multiplying either the gain or the discount with any number does not affect the nDCG score. For this reason, we enforced the gain values to be a probability distribution over relevance grades and the discount factors to be a prob-ability distribution over ranks. This way we limit the range of values both for the gain and the discount within the [0 , 1] range and reduce the unknown parameters by one. Further-more, it so happens that there maybe some fluctuation in the values of the optimal discount factors, e.g. the discount factor on a certain rank may happen to be larger than the one on a lower rank. This is not justifiable from an IR per-spective and thus, we also enforce that the discount factors are non-increasing with the rank. The same may be true for the gain values, hence we enforce them to be non-decreasing with the relevance grade. Further, we set the gain value for non-relevant documents equal to zero.

Moreover, note that the coefficient  X  in Equation 2 is a monotonically non-decreasing function of the number of queries. In other words, the gain or discount function that is optimal for n queries is also optimal for n +1 queries. Therefore, in the optimization process we set the number of queries equal to 1.

The optimization setup for the gain/discount function is mathematically expressed in Figure 1. When we optimize for the discount factors we consider the gain values as given, { gain ( grade j ) }
Subject to: 1 . 2 .gain ( grade j )  X  gain ( grade j +1 )  X  0  X  j :1  X  j  X  where k is the number of relevance grades. { disc ( rank r ) } Subject to: 1 . 2 .disc ( rank r )  X  disc ( rank r +1 )  X  0  X  j :1  X  j  X  N where N is the cut-off rank at which nDCG is calculated. while when we optimize for gain values we consider the dis-count factors as given. We also perform a multi-objective optimization to simultaneously optimize for the gain and the discount function. For the purpose of the optimiza-tion, we used the fmincon MATLAB function for the nor-mal optimization and the minimax MATLAB function for the multi-objective optimization. Both functions employ Se-quential Quadratic Optimization.
The afore-described optimization framework was applied to the TREC 9 and 10 Web track collections and the TREC 12 Robust track collection. The number of participating sys-tems for the three collections is 105, 97 and 78 respectively. All systems were run over 50 queries 1 . Documents returned as a respond to these queries were judged by a single asses-sor in 3 relevance grades scale: highly relevant, relevant and non-relevant. The task in all tracks was the usual ad-hoc retrieval task.

For each one of the three test collections, we calculated the optimal discount factors (given a linear gain function), the optimal gain values (given a logarithmic discount function) and the Pareto-optimal gain values and discount factors. We compared the optimal gain and discount functions with the a number of commonly used gain and discount functions both with respect to the stability/efficiency of the resulting nDCG measure and with respect to the induced by the resulting nDCG ranking of systems.
In this section we present the results of the optimization for the discount function. The gain values were set to 0, 1 and 2 for non-relevant, relevant and highly relevant docu-ments respectively and they were treated as constants dur-ing the optimization. We performed the optimization over the TREC 9, 10 and 12 data sets for nDCG computed at rank 10, 20 and 100 and we report the results in Fig-ure 2. We compare the optimal discount factors  X  blue solid curve with circles as markers in the figure  X  (a) with the Zipfian discount function (1/rank)  X  green solid curve with plus signs as markers, (b) with the log discount function (1/log 2 (1+rank))  X  dark blue solid curve with triangles as markers and (c) with the linear discount function ( (cut-
The TREC 12 Robust track collection includes 100 queries, however the first 50 of them were obtained from TREC 6, 7 and 8, where documents were judged as either relevant or non-relevant. For this reason, we did not use these 50 queries in our studies. off rank + 1 -rank) / cut-off rank)  X  magenta solid curve with crosses as markers. For comparison purposes, we trans-formed the linear, log and Zipfian discount factors to prob-ability distributions over the ranks.

As it can observed in Figure 2, the optimal discount func-tion is the least steep one among the discount function con-sidered. The log discount function is the one closest to the optimal, while the Zipfian drops much faster than the op-timal. The linear discount also appears to be close to the optimal one, at least when only the top ranks are consid-ered..

Looking at the right-most plots for each TREC, that is the plots corresponding to nDCG at rank 100, one can observe that the top ranks are the ones that mainly matter and thus they are given higher discount factor, while the rest of the ranks are given a rather small and constant discount factor. The number of the top-most ranks that really matter seems to be collection dependent, with the top 10 ranks being the important ones for TREC 9 and 12 and the top 20 ranks being the important ones for TREC 10. A further obser-vation one can make is that, even though the rest of the ranks are given a rather constant discount factor, this con-stant is far from zero (or at least farther than the discount factors the log and the linear discount function assigns to those ranks) suggesting that documents lower at the ranked list may also be useful in discriminating systems efficiently. This further suggests that computing nDCG at top ranks is sub-optimal since computing nDCG at some cut-off rank implicitly assigns zero discount factors to the ranks below that cut-off.

For the purpose of completeness, Figure 3 illustrates the results when we optimized the stability (efficiency) of nDCG without enforcing the non-increasing constraint for the dis-count factors. We only report results for nDCG at rank 20. One may observe that the optimal un-constraint discount factors are not strictly decreasing with the rank. Intuitively, these fluctuations are due to the fact that often times simi-lar systems return relevant do cuments at the top ranks and thus the only way to discriminate them is by looking deeper in the ranked list of documents. Thus, once again, this indi-cates that lower ranks may very well help in discriminating systems.

Figure 4 illustrates the stability of the nDCG measure (i.e. the fraction of the variance in the mean nDCG values due to actual performance differences between systems) when com-puted using (a) the optimal, (b) the log, (c) the Zipfian, and (d) the linear discount function. As expected, the optimal discount function eliminates all variance components other function (1/rank), (c) the log discount function (1/log 2 (1+rank)) and the linear discount function ((cut-off rank+1-rank)/(cut-off rank)). function is also included for comparison purposes. linear discount function. than the one due to systems, faster (in terms of queries) than the rest of the discount functions. The log discount function is the second most stable one while the Zipfian and the linear lead to the least stable nDCG measure.
Finally, in Table 1 we compare the efficiency of the nDCG when the optimal discount function is employed with the efficiency of nDCG when the log, the Zipfian and the linear discount functions are employed. To calculate the efficiency, we fit an ANOVA model into the resulting nDCG scores, for each one of the discount functions. Then, setting the value of X equalto0 . 9, that is 90% of the total variance in the nDCG scores being due to the actual performance differ-ences between systems, and using Equation 2 we computed the necessary number of queries to reach the given stability. As expected the log discount function is the closest to the optimal one.

To conclude, the stability-(efficiency-) optimal discount function is less steep than any of the commonly used dis-count functions. The widely used log discount function is the one closest to the optimal discount function, while the Zipfian and the linear ones are the least stable. Further-more, the optimal discount factors over low ranks are far from zero which suggests that looking further down at the ranked list of documents (regardless of the underline user search behavior and patience to step down the ranked list) can improve the reliability of system comparisons. Table 1: Number of queries required to achieve 0.95 stability in evaluation.

We also performed an optimization for the gain values as-signed to the different relevance grades of documents. In this case, the discount factors were treated as constants. The log discount function, the closest to the stability-(efficiency-) optimal discount function, was utilized. Further, we set the gain value for the non-relevant equal to zero and optimized for the gain values of the relevant and highly relevant doc-uments. We performed the optimization over TREC 9, 10 and 12 data sets for nDCG at ranks 3, 10, 20, 100 and 200. Instead of the gain values themselves, we report the ratio between the gain value assigned to the highly relevant documents and the gain value assigned to the relevant ones. The results can be viewed in Table 2. As in the case of the discount function, we performed both an un-constraint and a constraint optimization. In the constraint optimization we enforced the gain value of the highly relevant documents to be greater than or equal to the gain value of the relevant ones. The optimal gain value ratios for the un-constraint op-timization are reported in the first column of Table 2, while the ones for the constraint optimization are reported in the second column. The last two columns show the ratio of the gain values when the linear and exponential gain functions are utilized.

By comparing the first two with the last two columns of the table one can observe that the utility of relevant docu-ments in comparative evaluation of systems is underrated by the commonly employed gain functions. The optimal ratio of the gain values for highly relevant and relevant documents is in most of the cases much smaller than 2 or 3. Intuitively, this means that relevant documents are almost equally dis-criminative to the highly relevant ones. Good systems will retrieve both highly relevant and relevant documents while bad systems will have difficulties in retrieving either highly relevant or relevant documents. Thus, discriminating sys-tems regarding their performance can be similarly done with either relevant or highly relevant documents. Note that this is true for the particular TREC collections under study and the systems run over these collections and it may not be true in the general case.

In the un-constraint optimization column, highly relevant documents still appear more di scriminative than relevant documents for most of the cases. However, there are cases, e.g. in TREC 12 with nDCG computed at low ranks, that relevant documents appear to be more discriminative than highly relevant documents. An intuitive explanation of this behavior may be given by fact that the total number of highly relevant documents retrieved by systems in TREC 12 is quite small and highly relevant documents tend to appear at the very top of the ranked lists, while they are almost absent from the deeper ranks. Thus when deeper ranks are considered, highly relevant documents lose some of their discriminative power. The percentage of relevant and highly relevant documents on average (over all queries) at each rank for TREC 12 can be viewed in Figure 5. Percentage Figure 5: The percentage of documents that are relevant and the percentage of documents that are highly relevant on average (over all queries) at each rank for TREC 12.

Finally, for both TREC 9 and 10, one can observe a trend in the optimal ratio between the grades for relevant and highly relevant documents, with the ratios originally increas-ing by the rank nDCG is computed at and then dropping. This phenomenon needs to be further explored.

In Table 3 we compare the efficiency of the nDCG mea-sure calculated at rank 100 when the optimal gain function is employed with the efficiency when the linear or the ex-ponential gain functions are employed. As in the case of discount functions, to calculate the efficiency of each mea-sure, we fit the ANOVA model into the resulting nDCG scores, for each on of the discount functions. Then, setting the value of  X  equal to 0 . 95, that is 95% of the total variance in the nDCG scores is due to the actual performance differ-ences between systems, and using Equation 2 we compute the necessary number of queries to reach the given stability. Interestingly, the values in the table are almost identical for all gain functions for TREC 9 and 10, while only for TREC 12 the optimal gain is significantly better than the linear or the exponential ones in terms of efficiency.

Comparing Table 3 with Table 1 one can observe that the choice of the discount function affects much more the effi-ciency (stability) of the resulting nDCG measure than the choice of the gain function. As mentioned before, intuitively this means that at least in these particular collections when a system is good it retrieves both many highly relevant and many relevant documents, while when a system is bad it fails to retrieve either. Even thought, this is true for the given test collections, this may not be the case for other test collections and in particular for collections with more than three relevance grades, where for instance retrieving enough marginally relevant documents may not necessarily mean that the system can also retrieve enough excellent doc-uments (where excellent is more than 1 relevance grade away from marginally relevant). Unfortunately, currently we do not possess any such collection and thus we leave this as a future work.
 Table 3: Number of queries required to achieve 0.95 stability in evaluation.

Finally, we performed multi-objective optimization in or-der to optimize efficiency (stability) for both the gain and the discount functions simultaneously. To do so, we uti-lized the minimax MATLAB function, which produces the Pareto optimal discount and gain functions. That is, the discount and gain functions that maximize the worst case value of nDCG stability. We performed the optimization over TREC 9, 10 and 12, concluding that the Pareto opti-mal gain and discount functions are very close to the optimal gain and discount functions when the optimization is done independently for gains and discounts. The multi-objective optimal discount function for TREC 9 when nDCG is com-puted at rank 20 is shown in Figure 6. For comparison reasons, the optimal discount function when linear gain is used is also shown in the figure. As it can be observed in all cases the discount factors obtained from the multi-objective optimization are almost equal to the ones obtained with lin-ear gains used. The multi-objective optimal ratio between highly relevant and relevant documents is reported in the third column of Table 2. As it can be observed, except for the case of TREC 9, when nDCG is computed at very low ranks, the multi-objective optimal ratio is very close to the one obtained with the log discount function. This may be an indication that gain and discount functions indepen-dently affect the stability of the measure. Similar plots are obtained for all TREC X  X  and all ranks nDCG is computed at.
Different gain and discount functions employed in the cal-culation of nDCG may result in different mean nDCG values and therefore different rankin gs of the systems. To investi-gate how gain and discount functions affect the nDCG score and thus the induced ranking of systems, we calculated the mean nDCG at rank 100 for different gain and discount func-tions and computed the Kendall X  X   X  between the induced rankings.

The scatter plots in Figure 7 illustrate the mean nDCG and linear discount function. Figure 6: The multi-objective optimal discount function along with the optimal discount function when linear gains are used for TREC 9 and nDCG computed at rank 20. scores for the optimal discount function ( x  X  axes ) computed at rank 100 against the mean nDCG scores for the log, Zip-fian and linear discount functions respectively ( y  X  axes )for TREC 9, 10 and 12. The RMS Error and Kendall X  X   X  are reported in the plots. The Kendall X  X   X  between the rankings of systems induced by any two discount functions are also reported in Table 4.
 By inspecting both the scatter plots in Figure 7 and the Kendall X  X   X  values in Table 4 one can observe that both the rankings by the linear discount function and the rankings by the log discount function are very close to the rankings TREC 9 log 0.8960 0.9282 1.0000 0.9374 TREC 10 log 0.8625 0.9184 1.0000 0.9453 TREC 12 log 0.8149 0.8828 1.0000 0.8581 by the optimal discount function. As illustrated in Fig-ure 2, these two discount functions are the closest to the optimal one. The rankings by the Zipfian discount function are widely different than the ones by the optimal discount function, especially in TREC 12.

This wide difference between the induced rankings by the optimal discount function and the Zipfian one can be ex-plained by revisiting Figure 4. As it can be observed, for the Zipfian discount function, only 80% of the differences in the mean nDCG scores over a set of 50 queries (which is the case in all scatter plots here), is due to actual performance dif-ferences between the systems, while the corresponding per-centage for the optimal discount function is about 90%. The corresponding percentages for TREC 9 (where the ranking of systems for the two discount functions are closer to each other) are 90% and 95% respectively, while for TREC 10 (where the ranking of systems are almost identical) the per-centages are around 88% and 90%, respectively. Therefore, the ranking by the Zipfian discount in TREC 12 incorpo-rates a lot of noise which is reduced in the case of TREC 9 and 10.

The scatter plots in Figure 8 illustrate the mean nDCG scores computed at rank 100 for the optimal gain function ( x  X  axes ) against the mean nDCG scores for the exponential gain function ( y  X  axes ) for TREC 9, 10 and 12. The RMS Error and Kendall X  X   X  are also reported in the plots.
As it can be observed in Figure 8 the rankings by the op-timal discount function are almost identical with the rank-ings by the exponential gain function. This is one more indication that for the particular test collections with the three grades of relevance the ratio between the gain values for relevant and the gain values for highly relevant docu-mentsdoesnotaffecttherankingofsystems(atleastfor the ratio values examined in our studies, i.e. ratio values less than 3). What is particularly striking is that even for TREC 12, where the optimal gain function gives the exactly same gain value to both relevant and highly relevant, and thus essentially conflates the two relevance grades in one, the Kendall X  X   X  between the rankings is 0 . 94, with the top 6-7 systems ranked in the exact same order by both gain functions. This states that good systems do equally good in retrieving relevant and highly relevant documents, while bad systems do equally bad in retrieving either relevant or highly relevant documents.

The corresponding scatter plots for the linear gain func-tion look very similar to the ones in Figure 8 and for this reason they are not reported here.
As mentioned before, intuitively, efficiency and stability seem to correlate well with discriminative power, since the variability in a measure that discriminates systems well will most probably be due to actual performance differences be-tween systems. In this section we perform some basic ex-periments to test whether this hypothesis is correct.
Sakai [12] proposed a methodology to compare evaluation methods in terms of their ability to discriminate between systems based on Bootstrap Hypothesis Tests . According to his framework, all pairs of systems are considered and the hypothesis that their mean scores over a set of queries are the same is tested. To test this hypothesis Sakai [12] employs a bootstrap test, creating 1000 bootstrap samples. The achieved significance level (ASL), that is the significance level required to reject the zero hypothesis that two systems have the same mean score, is computed for each pair of sys-tems. Finally, evaluation measures are compared in terms of ASLs. The smaller the ASLs a metric achieves the more discriminative the metric is.

To optimize for discriminative power, one would need to minimize the obtained ASLs while treating gain and dis-count function as unknowns. This is not a trivial opti-mization and it seems at least computationally inefficient. However, if stability (efficiency) is well correlated with dis-criminative power, then the stability-optimal nDCG will also demonstrate high discriminative power.

To test out thesis, we adopted the bootstrap hypothesis testing methodology, and compared 4 variations of nDCG, (a) nDCG with optimal gain and optimal discount, (b) nDCG with linear gain and log discount, (c) nDCG with exponen-tial gain and log discount, and (d) nDCG with linear gain and linear discount. We followed the experimental setup in Sakai [12] and used only the top 30 systems from each data set (TREC 9, 10 and 12), since  X  X ear-zero X  runs are unlikely to be useful for discussing the discriminative power of the measures. We considered all the remaining pairs of systems and for each one of the pairs we created 1000 bootstrap sam-ples and calculated the achieved significance level (ASL) for all aforementioned nDCG measures. Figure 9 illustrates, for each one of the nDCG measures, the ASLs of systems pairs. The horizontal axis represents all system pairs sorted by ASL. The pairs of systems at the left of a given ASL level, are those that the measure cannot discriminate.
As it can be observed from the plots, when the stability-(efficiency-) optimal gain and discount functions are utilized nDCG outperforms all other variations with respect to dis-criminative power. The linear/exponential gain and log dis-count nDCG measures appear to be the next most discrimi-native ones, while the linear gain and linear discount nDCG appears to be the less discriminative one.
Despite the flexibility nDCG offers in the selection of the appropriate gain and discount function, so far this selection has been done rather arbitrarily, based on speculations of the search behavior of an average user and speculations of the correlation of the measure to user satisfaction. Recent work [1] has shown that the most commonly employed gain and discount functions are loosely related to user satisfac-tion which underlines the need for a more methodological selection of gain and discount function. However, given the infinite number of possible gain and discount functions, the vast differences in user search behavior, the many different possible retrieval tasks, a complete analysis of the different gain and discount functions with respect to the user satis-faction is prohibitively expensive, if at all possible.
In this work, we numerically computed a stability-or efficiency-optimal gain and discount function by treating gain values and discount factors as unknowns and optimizing for a stability/efficiency measure defined based on General-izability theory. We compared the resulting gain function to both the linear and the exponential functions and the re-sulting discount function to the log, Zipfian and linear ones.
According to our results, the optimal discount function is less steep than all commonly used discount functions, giv-ing reasonably high weights to lower ranks, while the relative difference between gain values is much smaller than the com-monly used ones, giving almost equal weights to both rele-vant and highly relevant documents. The latter was rather striking, since weighting relevant and highly relevant docu-ments equally did not seem to alter the ranking of systems. Note that this is true for the particular collections and sys-tems under study and it may not reflect the general case.
Finally, we demonstrated that the stability-(efficiency-) optimal nDCG measure outperforms the dominant in the lit-erature nDCG measure with respect to discriminative power as well. function. [1] A. Al-Maskari, M. Sanderson, and P. Clough. The [2] J.A.Aslam,E.Yilmaz,andV.Pavlu.Themaximum [3] D. Banks, P. Over, and N.-F. Zhang. Blind men and [4] D. Bodoff and P. Li. Test theory for assessing ir test [5] R. L. Brennan. Generalizability Theory .
 [6] C. Buckley and E. M. Voorhees. Evaluating evaluation [7] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, [8] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, [9] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [11] J. Kek  X  al  X  ainen. Binary and graded relevance in ir [12] T. Sakai. Evaluating evaluation metrics based on the [13] T. Sakai. On penalising late arrival of relevant [14] E. M. Voorhees. Evaluation by highly relevant
