 In this paper, we present our approach for geographic per-sonalization of a content recommendation system. More specifically, our work focuses on recommending query top-ics to users. We do this by mining the search query logs to detect trending local topics. For a set of queries we com-pute their counts and what we call buzz scores, which is a metric for detecting trending behavior. We also compute the entropy of the geographic distribution of the queries as means of detecting their location affinity. We cluster the queries into trending topics and assign the topics to their corresponding location. Human editors then select a subset of these local topics and enter them into a recommendation system. In turn the recommendation system optimizes a pool of trending local and global topics by exploiting user feedback. We present some editorial evaluation of the tech-nique and results of a live experiment. Inclusion of local topics in selected locations into the global pool of topics resulted in more than 6% relative increase in user engage-ment with the recommendation system compared to using the global topics exclusively.
 H.3 [ Information Storage and Retrieval ]: H.3.1 Con-tent Analysis and Indexing; H.3.3 Information Search and Retrieval; H.4 [ Information Systems Applications ]: H.4.2 Types of Systems; Algorithms, evaluation, live experimentation Trending search queries, clustering, location affinity, person-alization, content recommendation
Personalization of online content recommendation systems is the task of targeting users with topics tailored to their interests, personal preferences, location, or demography. In-ternet users engage more with topics that are personally relevant to them, and location is one of the most engaging features that is being exploited heavily by current desktop and mobile applications. For example, users like to know the latest news about their local sports teams and athletes, or like to know about local crimes, elections results, weather events in their area, community festivals or concerts, and local commute disruptions.

Internet portals that recommend content to their users like Yahoo! 1 ,MSN 2 ,orAOL 3 , supply their users with top-ics and corresponding relevant material to browse for enter-tainment or to stay up-to-date on current events. Many of these topics are either time-sensitive, hyper-local, or both. In order for personalization to scale for many different lo-cations and to recommend fresh topics in almost real-time, automatic event discovery techniques are highly desirable to power personalized experiences to users.

The search query stream is a valuable and rich source of information about user activity on the internet. It conveys the users X  demand for information at a particular time and geographic location. Some of these information needs are time-sensitive, pertaining to current events, or hyper-local, pertaining to local events, or both. Mining the search query stream is one of the most effective ways of finding interesting user demand , which recommendation systems can exploit to provide the appropriate supply using the content available.
The Trending Now Module on Yahoo! X  X  front-page, shown in Figure 1, is one of few online trending topic recommenda-tion systems deployed on the internet. The trending topics displayed pertain to different events, mainly globally trend-ing and of interest to a wide user base. Every click on the trending topics takes the user to a search results page where the topic is entered automatically as a query. These top-ics belong to a larger editorially created pool of topics that are further optimized by the online content recommendation system.

The goal of this paper is to describe the framework we developed to supply trending local topics into the pool of topics used by the Trending Now content recommendation system. This framework will expand the pool of topics and http://www.yahoo.com http://www.msn.com http://www.aol.com show users from different locations trending topics local to them, in addition to the globally trending topics.
In order to automatically supply local topics to the trend-ing now pool of queries four steps are required. First, we need to detect buzzing queries which are spiking in near real-time. These queries need be distinguished from navi-gational queries (such as  X  X acebook X  or  X  X ahoo X ). Queries with fresh news related stories should be given preference. Second, queries need to be clustered around topics ranked according to buzz scores and user views. That is, we need to extract from the buzzing queries the buzzing topics asso-ciated with clusters of queries. The third step required for geographic personalization is a mechanism to detect local queries and their affinity to the geographic locations using the geographic information available in the query logs. This is needed so that buzzing topics will be assigned to the lo-cations where their queries are mostly issued from. Finally, the fourth step is to have human editors oversee the selec-tion process. The trending local topics will continuously be refreshed and human editors will select the most interesting topics shown to them on an internal dashboard we created for this task.
 To evaluate our framework, we ran two separate tests. The first test was an editorial evaluation of the local topics for selected locations over a period of time. The second was a live test where we included the local topics selected by the editors to be part of the pool of topics for Trending Now. The local topics were added to the global topics, ranked, and presented to users in the respective locations, using the online learning and ranking algorithms described in [10]. A significant increase in user engagement was achieved with the local topics versus the global ones.

In Section 2 we describe related work and motivate the approach that we have taken. In Section 3 we describe the query buzz detection system and in Section 4 we describe the query location affinity detection technique. In Section 5 we describe our approach for query clustering into topics. In Section 6 we describe the editorial evaluation results and in Section 7 we describe the live test experiments and results. We conclude and briefly discuss future work in Section 8. Figure 1: Trending Now Module on Yahoo! front-page on 11/1/2011 at 3 PM PDT, user logged-in from Sunnyvale CA. Topics T1 and T10 are local topics and the rest are global.
The Content Optimization and Relevance Engine (CORE) [1, 2, 3] is the internal engine that drives content recommen-dation on many Yahoo! properties. It is applied on different content like news stories, today X  X  videos, and trending now topics. There are usually many items in the pool of items for each of these properties and CORE is what decides which of these items to display. CORE implements a partitioning approach to segment the users in real-time into two groups: an exploration and an exploitation group. The exploration group is made of a smaller number of users who provide click-through-rate (CTR) feedback on the content recommended to them according to a random policy. This constitutes the online learning process. The items are then continuously re-ranked according to most engaging and shown to the rest and majority of the users (exploitation). This happens in real-time with models continuously updating. Dong et al. [10] discuss applying the CORE technology on the Trending Now Module on Yahoo! front-page.

The CORE powered Trending Now module starts with a pool of topics (or stories) that human editors choose after consulting multiple web resources showing current trends, such as Yahoo! Buzz Index 4 and our internal engine for de-tection buzzing queries [11]. Using this bounded and fixed pool of topics, CORE optimizes the list by re-ranking it in real-time based on user X  X  engagement. Online personaliza-tion is applied where users are segmented into groups based on similarity features like age or gender and the optimal ranking for the topics happens within these segments [10]. Currently there is no consideration of any prior knowledge on the affinity between the topics and the segments. The same list of topics is optimized within the different segments.
The goal of our recommendation framework is to provide the optimal list of trending topics for each user segment, in addition to expanding the pool of topics. Human editors help oversee the selection process in a semi-supervised fash-ion. The framework we provide allows them to select trend-ing local topics in particular locations and to add them into the corresponding pool of topics. For example, the query  X  X hicago Cubs X , a trending local topic for Chicago, can be added to the pool of topics and be restricted for users from Chicago only. In our experiments, user segmentation isbasedongeography.

The geographic features derived from the offline search stream can be used as a prior for online optimization. This will help the current optimization approach which suffers from the cold start problem [10], especially for personaliza-tion. It starts with a flat prior or equal chance for each topic for each of the user segments. In our work, we tag the top-ics with the target location. In the example above, Chicago tagged topics will be restricted to users from Chicago ac-cording to our framework and hence the re-ranking happens in a fair way. Topics are not demoted due to no-click events from users in other locations who might not be interested in such topics. Similarly, more offline features are to be added like age, gender, and topic category (e.g. sports, fi-nance, shopping, etc.). Figure 2 shows our trending topics detection framework feeding the CORE pool of topics for Trending Now Module.

Trend detection in the search query stream is a technique used to visualize what users are searching for. For example, Google Trends 5 displays a small list of globally trending top-ics, up to 20 per day. Google Insights 6 provide more flexible analytics and capabilities like trends in different categories, http://buzzlog.yahoo.com/overall http://www.google.com/trends http://www.google.com/insights/search locations, or variable time intervals. Yet, the smallest in-terval over which to compute trends provided is seven days. This does not allow for breaking-news detection and might include some stale stories due to the coarse granularity of the selection window. Choi and Varian [7] describe the auto-regressive model used for detecting trends in Google Trends.
Trend detection has also been applied on the Twitter stream of tweets 7 . Like queries, tweets are user gener-ated, and are an effective way to propagate late breaking information. Trendsmap 8 is a tweet based trend visualiza-tion service that geographically shows the trending tokens in Twitter. The technique used seems to be token-based which may be noisy and generates tokens that are not news-related. Our buzz detection framework on the other hand makes use of a language modeling [16] approach for detect-ing buzzing queries [11] as described below. News-related queries buzz scores are boosted. In addition, the trending topics generated by our framework are generalized n-grams, usually more than one token long. This would generate more informative topics for the users to engage with.

Both Google Insights and Trendsmap seem to provide trends for a particular location that could also be globally trending. The work by Welch and Cho [15] uses the lexical information contained in the query itself to identify location. In our work, we use the entropy [8] of the geographic distri-bution of the topics over all locations and filter out globally trending topics. The local topics our framework provides are those exhibiting the greatest affinity to a particular lo-cation. We make use of user logs information that provides the physical location of the users to compute the affinity. This helps enforce location personalization and drive more user engagement. The work in [6, 14] applies query location information to improve web search results ranking. Bennett et al. [6] build a location distribution model based on gaus-sian mixtures for URLs using the users click information and for queries using query logs. They also use entropy as a feature to measure the location affinity of queries. They match the URL and query location distribution to improve the ranking of URLs given a query. http://twitter.com http://trendsmap.com Figure 2: Our proposed trend detection framework feeding the CORE pool of topics for Trending Now Module.
The first component in our framework shown in Figure 2 is responsible for finding buzzing or spiking search queries. It operates in two phases. In the first phase, candidate queries are generated by mining the search logs and selecting queries that integrated a news display in the web search results [9], the top most frequent queries, and all the queries in the last hour with a minimum count and buzz threshold. The buzz score for each query is then computed in the second phase by comparing the query language model scores at the current time window, same time window in the previous day, same time window of the same day one week ago, and the same time window of the same day one month ago. The window size varies between two to six hours depending on the time of the day. The exact window length and the number of top most frequent queries to include were tuned internally. The language models are trained from search queries, queries triggering news displays, and from news feeds. This process is repeated many times per day, and each time produces a  X  X ictionary X  which contains the above listed set of queries, their buzz scores, and their counts in the last time window. For more information about buzz score computation refer to [11].
Our goal is to find queries that have high affinity to specific geographic locations. In our work, we restrict ourselves with the top 50 US Designated Market Areas (DMAs). DMAs are geographic areas defined by Nielsen Media Research com-pany as a group of counties that make up a particular tele-vision market. There are 210 Nielsen DMAs in the United States. Examples of DMAs are metropolitan areas (e.g. DMA  X  X allas Fort-Worth X ) or groups of cities (e.g. DMA  X  X an Francisco-Oakland-San Jose X ) or individual cities (e.g. DMA  X  X os Angeles X , DMA  X  X an Diego X , etc.). DMAs are proven (by the television industry) to be effective in target-ing geographic audience with customized services.
We rely on the query location entropy [8, 12] to find local queries as described in Equation 1. entropy ( dma | q )=  X 
There are two ways to compute the probability of a DMA p ( dma i | q )givenaquery q . The first approach is simply the posterior probability of the DMA given the query as can be computed from the query logs. The internal Yahoo! search query logs provide the physical location (on the DMA level) of the user issuing a query with high accuracy. We aggregate the query DMA volume v ( dma i ,q )over24hourintervals. For a list of N DMAs, where N is 50 in this case, we compute these probabilities for all the queries in the buzz dictionary (described in Section 3) at the current hour. Queries not making it into the buzz dictionary are excluded.
The previous approach does not normalize within a DMA volume. Hence, larger DMAs will be favored. If we assume the DMAs to be equally probable, we arrive at the following normalized likelihood solution, which we adopt for the rest of this paper. This approach favors affinities to smaller DMAs where local topics are more evident relative to the size of the DMA. where
The entropy measure conveys location affinity. Queries with high entropy are global queries and those with low en-tropy are local. Using the DMAs X  normalized likelihoods, we can determine which DMAs the queries are mostly re-lated to. For example running the algorithm on data from December 10, 2010, we get the statistics in Table 1 for a selected list of queries. Since we use only 50 DMAs, the maximum entropy value, which assumes a uniform distri-bution over DMAs, is log 2 (50) = 5 . 64. Buzz scores range between [0  X  1] where the buzzing queries have scores closer to 1.

The first example in Table 1  X  X ingwood nj murder X  has a high buzz score, low entropy, and affinity to DMA  X  X ew York X , making it a buzzing local query. Similarly the second and third queries are names of a politician and a judge in the states of California and Florida respectively, buzzing around the states X  election times. The normalized likelihoods are computed for the 50 DMAs and sum to one. Figure 3 is a scatter plot of the distribution of the buzz and entropy scores of queries. In this figure, queries with count less than 50 for that day are excluded. The majority of the remaining queries are high entropy and low buzz queries (e.g.  X  X ma-zon X ,  X  X est buy X ). An example of low buzz and low entropy queries is  X  X raigs list san diego X . The buzz scores computed do not take location into consideration and are computed over the whole search traffic. Due to data sparsity, some locally trending topics might be missed.
The previous section described the geographic feature im-plementation that is added to the current queries dictionary. In addition to the buzz score, the new dictionary includes the query counts in the last 24 hours, the entropy of the query distribution over the 50 DMAs, and the normalized likelihood of the query in each of the DMAs. We use the counts in the last 24 hours to get a more accurate affinity of the queries to the DMAs, whenever these counts are avail-able. To get the topics, the queries need to be clustered and the most prominent DMA for each of the clusters, or topic, need to be identified.
Having identified queries that are buzzing at a particular time, we need a way to cluster those queries around  X  X opics X  that users are interested in. For example among the query set ( X  X alifornia Budget X ,  X  X alifornia Budget Crisis X ,  X  X al-ifornia Budget News X , and  X  X alifornia State Budget X ) we wish to single out  X  X alifornia Budget X  as  X  X he topic X  around which these queries cluster. To determine such topics algo-rithmically we define a topic score for each query and we choose as our topics those queries with the highest topic scores. The topic score for a query q is defined in terms not just of how many times users searched for exactly q (the volume of q over all DMAs, v ( q )) but how many times they searched for anything that included q (generalized volume of q over all DMAs, v ( q )). Thus a search for  X  X alifornia State Budget X  would contribute to the generalized volume of  X  X alifornia Budget X .

We define two flavors of topic scores for queries, one based on buzz and the other on locality as shown in Equation 5. The locality score locality ( dma | q ) is derived from entropy according to Equation 6. The queries from the new dictio-nary are sorted according to each of these topic scores sep-arately and the top-n queries (n is tuned internally) from each list form the candidate list of topics. The buzz score will result in a buzzing list of topics and the locality score will result in a local list of topics. Each topic is then asso-ciated with the set (or  X  X ariants X ) of queries Q that contain all the words of the topic. Other advanced query cluster-ing techniques have been proposed in the literature [4, 13, 5] which take the search results into consideration and are used for various applications like search assist. The simpler lexically-based solution we use has the advantage of being fast and robust, and proved to be sufficient for our task. Figure 3: Buzz versus entropy distribution for December 10, 2010 search queries with count more than 50. topic localityscore ( q )= locality ( dma | q ) log { 1+ where
Once two candidate list of topics are identified according to the two topic scores described in Equation 5, the DMA pertaining to each topic and its query variants set Q is de-termined. The effective DMA count for each set is computed from the count of each query in the set. The global query count v ( q ) is used and weighted by the normalized likelihood of the query in that DMA as shown in Equation 7. The ef-fective count count ( Q, dma i ) is computed for all 50 DMAs. The DMA with the largest count is chosen as the DMA for this cluster. Finally the two lists are merged and ranked as shown in Figure 4 which describes the process. We apply certain thresholds on the topics to make sure we retain relevant ones. Topics pass the filtering if their base query count, buzz score, and locality score exceed the thresholds that are tuned separately. The resulting topics and its variant sets are presented in the editorial dashboard we designed so that editors have a good picture about the reason each topic is trending, which is often evident from the query variants that contain it.

Before deploying the framework for a live test, we ran an exhaustive editorial test for the topics chosen for six test DMAs. These DMAs are listed in Table 3. Three editors were chosen for this task. They were asked to judge each topic taking into consideration the DMA assigned to it by the framework. Editors provided grades {  X  X xcellent X , X  X ood X ,  X  X air X , and  X  X ad X  } to the topics according to preset editorial guidelines. The judgments were influenced by two aspects: recency or buzziness, and local affinity to the correspond-ing DMA. We prepared an editorial tool which presented the topic sets to each editor and recorded their judgments. The tool allowed editors to view the topic, the queries (or variants) clustered around it, and also allowed them to click on the topics and inspect the web search results page or the news results page. The editors completed a live test from June 1 through June 3, 2011. Table 2 shows the grade distribution for the three editors for one day during the sec-ond test period. Each editor was presented with 45 topics from the six DMAs during that day. There were 20% over-lap in the topics assigned to each pair of editors so that we keep track on intra-editor differences.  X  X otal with overlap X  is sum of the results for the three editors without taking repeating topics into account. The  X  X ggregate X  results take into account the editorial differences. As shown in the re-sults, 25.2% of the topics shown to the editors have a grade of  X  X xcellent X  and are suitable to be included into the pool of topics for the Trending Now Module. Around 69.2% of the topics have a grade of  X  X ood X  or better (GOB), from which some topics may also be suitable for Trending Now. In addition, the results show a good degree of consistency in the judgments distribution between editors especially for the  X  X xcellent X  topics.  X  X ood X  topics are usually local and buzzing topics but tailored to a narrow audience or leading to search results with obscene content.  X  X air X  topics could be local and buzzing but also navigational or unrecognized by most users in that location.  X  X ad X  topics include those that are not local, not buzzing, misspelled, or completely generic.
We ran the editorial test for three days with a fresh list of topics presented to the editors every day containing trending topics that are local to the six DMAs shown in Table 3. The table lists the number of  X  X xcellent X  judgments the topics received for the three days in each of the six DMAs. It also shows the average number of  X  X xcellent X  topics per day per DMA. It also shows that a similar percentage of the  X  X xcellent X  topics is found every day of the test for all of the DMAs. Smaller DMAs like  X  X leveland-Akron X  and  X  X etroit X  have lower number of  X  X xcellent X  topics than larger DMAs like  X  X ew York X . They make 3% and 4% of the total number of  X  X xcellent X  topics found for the three days respectively, versus 39% for DMA  X  X ew York X .

Table 4 shows the aggregated judgments results for the three days. A similar number of  X  X xcellent X  topics is found as discussed above. The total judgments for the three days period was 286, out of which 257 are unique. It is expected that some topics repeat from one day to the other. The same judgment aggregation criteria were followed as the one de-scribed above. 69  X  X xcellent X  topics were found, out of which 53 are unique, contributing to 20.6% to the total number of unique topics judged during the three days period.
To evaluate the effectiveness of the local topics our frame-work provided, we ran a live test on the Trending Now Mod-ule on Yahoo! X  X  front-page shown in Figure 1. The live test covered the six DMAs presented in Table 3. During the test, real-time programming of local topics took place and impacted users in these six DMAs. The traffic for each DMA was split into 45% for the  X  X est X  bucket, 45% for the  X  X on-trol X  bucket, and the remaining 10% for the  X  X andom X  ex-ploration (learning) bucket as described in Table 5. The local topics were added in a semi-supervised fashion into the global pool of topics that are programmed daily. The size of the global pool varies daily and at any given time it would be competing with the local topics in the  X  X est X  buck-ets for each DMA. The editorial selection of the global topics continued to happen independently of this task. Using the dashboard tool we prepared, human editors selected a sub-set of the local topics for each of the six DMAs that they deemed suitable according to internal editorial guidelines. The editorial tool refreshes the set of local topics for each of the DMAs in near real-time. Due to time zones differences for the DMAs, one editor took charge of programming topics for DMAs  X  X leveland-Akron X  and  X  X ew York X . A second ed-itor programmed topics for DMAs  X  X hicago X  and  X  X etroit X , and a third editor programmed topics for DMAs  X  X allas-Ft. Worth X  and  X  X os Angeles X . Table 5 describes the 13 buckets we ran. The  X  X andom X  bucket contains a randomly shuffled list of global and local topics, out of which 10 topics are dis-played to 10% of the users who fall into this bucket from the six DMAs. The global topics are common to all users from the six DMAs, but location-based filtering is applied such that the local topics are shown only to the corresponding DMA users. The users X  engagement with the topics is used to re-rank the list of topics continuously. The higher the click through rate (CTR) on the topic the higher is its rank, regardless whether it is a local or a global topic. To assess the impact on the overall module, the top 10 ranked list of local and global topics is displayed to 45% of users who fall into the  X  X est X  bucket in each DMA. The overall module CTR for each of the six  X  X est X  buckets is then computed. In parallel, the local topics are filtered-out and the ranked list of top 10 global topics is displayed to 45% of users who fall into the  X  X ontrol X  bucket. The overall module CTR for each of the six  X  X ontrol X  buckets is then computed. The differ-ence in the CTR between the  X  X est X  and  X  X ontrol X  buckets in each DMA measures the impact of adding the local top-ics into the pool of global topics. The six DMAs included in the test make about 25% of the total US traffic contributing millions of views daily.

Table 6 shows the relative increase in CTR on the mod-ules in the  X  X est X  buckets over the modules in the  X  X ontrol X  buckets for each of the DMAs for a period of five days from August 8 through August 12, 2011. The CTR computation for the  X  X est X  buckets included the views and clicks on lo-cal and global topics together. The  X  X ontrol X  buckets did not include any local topic. Online learning and re-ranking is applied as described in this paper [10] and included age and gender personalization in the  X  X ontrol X  and  X  X est X  buck-ets too. Improvements in CTR are observed in the  X  X est X  buckets over the  X  X ontrol X  buckets for all DMAs. This im-provement is due to the added local topics in the DMAs  X  X est X  buckets. Overall, there is a 5.91% improvement in CTR for all DMAs running this test. The table also shows the DMA volume, which is the ratio of views in the  X  X on-trol X  and  X  X est X  buckets in each of the DMAs to the views in the  X  X ontrol X  and  X  X est X  buckets for all DMAs. In essence, this measures the volume of the DMA relative to the volume of the six DMAs. DMA  X  X os Angeles X  is the largest DMA with 32% DMA volume. A CTR improvement of 5.13% was achieved in DMA  X  X os Angeles X .

Table 6 also shows the total number of local topics pro-grammed during the five days period. There were 78 local topics and 201 global ones programmed during this time. There are some differences in the number of local topics programmed and the rate per day between this period and the period when the editorial test was performed for a few reasons. First, this was a live test in which there were more constraints on the search landing page experience. Second, this is a different time period and the traffic might be differ-ent. Third, the editors supervising the selection process were getting more experienced with time. The  X  X opics% X  column in the table presents the ratio between the number of topics programmed for the DMA over the total number of topics programmed for the six DMAs. In essence, this represents the volume of topics programmed for each DMA versus the total topics. The column before the last presents the ratio of this topic volume to the DMA volume. On average, this should be closer to 1.0 for DMAs like  X  X hicago X . This value is 2.51 for DMA  X  X etroit X  which means that more topics were programmed (relative to the other DMAs) in propor-tion to the volume of this DMA (relative to the volume of the total DMAs). Around 12.86% improvement in CTR oc-curred for DMA  X  X etroit X  which is correlated with this ob-servation. The trend is reversed for DMA  X  X ew York X  where this ratio is 0.71 and 3.41% improvement in CTR. In general, the more local queries are programmed in a DMA, relative to the DMA volume, the more improvement in CTR is ob-served as the results seem to corroborate. The last column shows the ratio of the number of local topics versus the total number of topics, including the global ones, in the pool for each DMA. The local topics compete with the global pool of topics in each DMA independently of other DMAs and form a small portion of the pool (less than 10%) as shown in the table. More analysis on the performance of local versus global queries is below.
 Figure 5 shows the relative increase in CTR for the overall Trending Now Module in the  X  X est X  versus  X  X ontrol X  buckets in each of the six test DMAs. We ran the live experiment for a period of more than five weeks. We achieved significant improvements across all DMAs. In DMA  X  X etroit X , one of the smaller DMAs tested, an increase of 12.71% has been achieved overall. The largest CTR increase of 14.85% oc-curred in the second week of the test (July 25 through 29). This proves the effectiveness of geographic personalization in such locations where users engage well with local events. In addition, an overall increase of 8.58% occurred in DMA  X  X os Angeles X  which is the largest DMA tested. This is also very significant in that it increases the engagement of the largest user segments in the test. On the other hand, the engagement in DMA  X  X ew York X  was not as strong due to a configuration issue that restricted local topics from appear-ing at position one in the module. This restriction was later removed but the engagement increase with local events in this DMA continued be be lower than other DMAs. This may indicate that users in DMA  X  X ew York X  are less inter-ested in local events than users in other DMAs. Yet, the engagement with local topics was still higher than that with the global topics. For all the DMAs, and throughout the five weeks period, an increase of 6.38% was achieved from July 15 through August 20, 2011 9 . Figure 5: Relative increase in CTR between the modules in the  X  X est X  and  X  X ontrol X  buckets for the six DMAs and five weeks live test period.

Figure 6 shows the number of local and global topics pro-grammed in the five DMAs over the five weeks period on a log 2 scale. The largest DMA to receive local topics was DMA  X  X os Angeles X  followed by  X  X hicago X  and  X  X etroit X . The number of local topics programmed was consistent over the five weeks period. The number of global topics pro-grammed was on the order of ten times more in each DMA.
In the previous section, we presented results showing im-provements in the overall module CTR when allowing local topics to appear in the  X  X est X  buckets and not in the  X  X on-trol X  buckets. The overall module CTR included clicks on local and global topics together in the  X  X est X  buckets. How-ever, this does not prove that the improvement is solely due * in Figure 5 denotes excluding weekends. to clicks on local topics. There is a very unlikely scenario where users click more on global topics when shown with less attractive local topics in the  X  X est X  buckets. To verify that this is not the case and to assess the performance of the local topics versus global ones we measure the improvement in CTR on the local topics versus the global ones separately. We measure the CTR when a local topic occupies each po-sition on the module shown in Figure 1. We also measure the CTR of the global topics in each position. This pro-vides a direct comparison between the local topics and the global topics. Table 7 reports the relative increase in CTR when local topics occupy each position on the Trending Now Module versus when global topics occupy that position.
For  X  X ll-Buckets X  we measure the CTR increase between the local topics in  X  X est X  and  X  X andom X  buckets versus the CTR of the global topics in all the buckets including the  X  X ontrol X  ones. This shows a relative increase of 61% at all positions, 16% increase at position one and so forth. For the  X  X est vs Control X  we measure the CTR increase between the local topics in  X  X est X  and the global topics in  X  X ontrol X  buckets for all six DMAs. This measures the difference be-tween the ranked list of local topics in  X  X est X  when shown with global topics, versus the ranked list of global topics in  X  X ontrol X  when shown alone with no bias from local topics. A relative improvement of 70% in achieved at all positions. For  X  X hicago X , we measure the CTR difference between the local topics in the  X  X est X  for  X  X hicago X  and the global topics Figure 6: Number of local and global topics programmed per DMA and week. in the  X  X ontrol X  bucket for  X  X hicago X . A relative improve-ment of 74% is achieved at all positions. There is roughly no difference in engagement at position one when a local or global topic occupies this position. Similarly for DMA  X  X os Angeles X , yet more engagement is achieved at other posi-tions. Notice that the increase is more dramatic at lower positions (on the module counting 1 down to 5 then 6 down to 10) due to the position bias phenomenon [10]: users are more likely to click on position one than on lower positions on the module especially when the ranking of topics is ap-plied. Hence the improvement is more apparent at lower positions. The improvement is more consistent across po-sitions in the  X  X andom X  bucket which proves that the local topics are more engaging irrespective of ranking bias. An increase of 39% is achieved in the  X  X andom X  bucket across all positions.

Figure 7 shows the distribution of the views and clicks of the local and global topics over the module positions in the  X  X est X  buckets. Recall that  X  X est X  buckets apply topic ranking learned from the  X  X andom X  bucket. Local topics are viewed more at top positions than on the lower ones due to the positive feedback they receive in the  X  X andom X  bucket and hence are ranked high. Around 18.19% of the views of local topics happened at position one. The views of global topics is flat across positions with almost equal likelihood of 10%. It is interesting to see that the views of global topics at position one is less than 10%, around 9.27% due to the com-petition with local topics to be displayed at this position. Around 42.15% of clicks on local topics happened on posi-tion one, 31.07% of clicks on global topics at this position. This shows the position bias phenomenon and the increased attractiveness of local topics on this position compared to the global topics. The click distribution tapers off at lower positions for both local and global topics.

Figure 8 shows the distribution of the views and clicks of local and global topics in the  X  X andom X  bucket. These topics are randomly shuffled and only 10 of them are shown to each user at a time. The figure shows the flat distribution of the views and the position-biased distribution of clicks. Similar distribution is achieved for local and global topics views and clicks since no ranking is applied here.
In this paper, we presented a framework that identifies trending local topics for the purpose of personalized Trend-ing Now programming. We described how we computed geographic features for the search queries and how to use these features to detect local queries. We also presented our clustering approach that finds topics from the queries and assigns it to geographic locations. An exhaustive editorial test was performed to assess the quality and quantity of the through August 12, 2011.
 Figure 7: Distribution of views and clicks of local and global topics over module positions in the  X  X est X  buckets from Au-gust 8 through August 12, 2011. Figure 8: Distribution of views and clicks of local and global topics over module positions in the  X  X andom X  buckets from August 8 through August 12, 2011. local topics provided by the framework. The live test showed improvements in CTR in the  X  X est X  buckets compared to the  X  X ontrol X  ones. Significant increase in user engagement was achieved with the local topics versus the global ones. Future work will focus on joint optimization of buzz and locality detection and on combating data sparsity using collabora-tive filtering techniques. This work has been deployed and powers the Trending Now module on Yahoo! front page as evident in Figure 1 and impacts millions of users in selected locations.
