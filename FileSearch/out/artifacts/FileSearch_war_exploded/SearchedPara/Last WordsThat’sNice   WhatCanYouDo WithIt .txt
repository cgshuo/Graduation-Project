 University of Brighton
A regular fixture on the mid 1990s international research seminar circuit was the  X  X illion-neuron artificial brain X  talk. The idea behind this project was simple: in order to create artificial intelligence, what was needed first of all was a very large artificial brain; if a big enough set of interconnected modules of neurons could be implemented, then it would be possible to evolve mammalian-level behavior with current computational-neuron technology. The talk included progress reports on the current size of the artificial brain, its structure,  X  X pdate rate, X  and power consumption, and explained how intelli-gent behavior was going to develop by mechanisms simulating biological evolution.
What the talk didn X  X  mention was what kind of functionality the team had so far managed to evolve, and so the first comment at the end of the talk was inevitably  X  X ice work, but have you actually done anything with the brain yet? X  evaluation scores that measure and assess various aspects of systems, in particular the similarity of their outputs to samples of human language or to human-produced gold-standard annotations, but are we leaving ourselves open to the same question as the billion-neuron artificial brain researchers? Shrinking Horizons
HLT evaluation has a long history. Sp  X  arck Jones X  X  Information Retrieval Experiment (1981) already had two decades of IR evaluation history to look back on. It provides a fairly comprehensive snapshot of HLT evaluation at the time, as much of HLT evaluation research was in the field of IR . One thing that is striking from today X  X  perspective is the rich diversity of evaluation paradigms X  X ser-oriented and developer-oriented, intrinsic and extrinsic 2  X  X hat were being investigated and discussed on an equal footing in the context of academic research. At the same time Sp  X  arck Jones described a lack of consolidation and collective progress, noting:  X  X here is so little control in individual tests and so much variation in method between tests that interpretations of the results of any one test or of their relationships with those of others must be uncertain X  (page 245). tial research effort to evaluation. We have far more established evaluation techniques and comparative evaluation is the norm. In fact, virtually all HLT subfields now have some form of competitive evaluation. 3 But it seems we have achieved comparability at the price of diversity. The range of evaluation methods we employ has shrunk dra-matically. Not only is virtually all evaluation in HLT research now developer-oriented and intrinsic, but, even more narrowly, most of it is a version of one of just three basic intrinsic techniques: (i) assessment by trained assessors of the quality of system outputs according to different quality criteria, typically using rating scales; (ii) automatic mea-surements of the degree of similarity between system outputs and reference outputs; and (iii) human assessment of the degree of similarity between system outputs and reference outputs. 4 purpose X  X f the embedded component or end-to-end system X  X s not part of task defi-nitions, and we do not test how well components or systems fulfill (some aspect of) the application purpose.
 Tasks in Need of an Application
Because application purpose does not figure in it, the intrinsic evaluation paradigm treats tasks as generic even though this may not always be appropriate. Kilgarriff warned against treating the word sense disambiguation ( WSD ) task as generic right at the start of the SENSEVAL evaluation competitions:
Navigli 2007), but the case of WSD points to a more general issue: in intrinsic evaluations, the output representation formalism (e.g., tag set, syntactic formalism) is fixed in the form of gold-standard reference annotations, and alternative representations are not subject to evaluation. There is evidence that it may be worth looking at how different representations perform. For example, Miyao et al. (2008) found significant differences between different parse representations given the same parser type when evaluating their effect on the performance of a biomedical IR tool. The intrinsic set-up makes it impossible to perform such evaluations of alternative representations, because this 112 requires an external X  X xtrinsic X  X oint of reference, as is provided by an embedding system like the IR tool in Miyao et al. X  X  work.
 not know which applications (if indeed any) systems are good for, we also don X  X  know whether the task definition (including output representations) is appropriate for the application purpose we have in mind.
 A Closed Circle
Whereas in analysis tasks evaluation typically measures the similarity between system output representations and gold-standard reference representations, in tasks where the output is language (e.g., MT , summarization, data-to-text generation), system outputs are compared to human-produced reference texts, or directly evaluated by assessors.
Methods for evaluating these evaluation methods, or  X  X eta-evaluation X  methods, look in particular at the reference outputs and similarity measures they involve. In analysis, where there are single target outputs, and similarity measures are a matter of count-ing matching brackets or tags, we can X  X  do much more than assess inter-annotator agreement and perform error analysis for reference annotations. In generation, it is the similarity measures that are scrutinized most. Metrics such as BLEU and ROUGE were conceived as surrogate measures 5 (theUin BLEU stands for  X  X nderstudy X ). Surrogate measures in science in general need to be tested in terms of their correlation with some reliable measure which is known to be a true indicator of the condition or property (e.g., karyotyping for chromosomal abnormalities) for which the surrogate measure (e.g., serum testing for specific protein types) is intended to be an approximate indicator.
In HLT , we test (surrogate) automatic metrics in terms of their correlation with human ratings of quality, using Pearson X  X  product-moment correlation coefficient, and some-times Spearman X  X  rank-order correlation coefficient (Lin and Och 2004). The stronger and more significant the correlation, the better metrics are deemed to be. The human ratings are not tested.
 ment can ever be shown to be wrong. If human judgment says a system is good, then if an automatic measure says the system is good, it simply confirms human judgment; if the automatic measure says the system is bad, then the measure is a bad one, its re-sults are disregarded, and the system is still a good system. This is a classic closed-circle set-up: It isn X  X  falsifiable, and it doesn X  X  include a scenario in which it would be concluded that the initial theory was wrong. The problem lies with treating what is but another surrogate measure X  X uman quality ratings X  X s a reliable, objective measure of quality. We may be justified in not accepting contradicting metric scores as evidence against human quality judgments and humanlikeness assessments, but perhaps we should pay attention if such intrinsic measures are contradicted by the results of user-performance and other extrinsic experiments. For example, in a comparison of graphical representations of medical data with textual descriptions of the same data, Law et al. (2005) found that, whereas in intrinsic assessments doctors rated the graphs more highly than the texts, in an extrinsic diagnostic performance test they performed better with the texts than the graphs. Engelhardt, Bailey, and Ferreira (2006) found that subjects rated over-descriptions as highly as concise descriptions, but performed worse at a visual identification task with over-descriptions than with concise descriptions. In a recent set of evaluation experiments involving 15 NLG systems, the eight intrinsic measures tested (although correlating strongly and positively with each other) either did not correlate significantly with the three extrinsic measures of task performance that were also tested, or were negatively correlated with them (Belz and Gatt 2008). In parsing, Miyao et al. (2008) performed an extrinsic evaluation of eight state-of-the-art parsers used as part of a biomedical IR tool. The effect parsers had on IR quality revealed a different system ranking than the WSJ -Corpus based F-scores reported for the same parsers elsewhere. Unreliable Evidence?
There is some indication that human quality judgments and measurements of similarity with human-produced reference material may not be able to live up to the role they are currently assigned. We know that agreement between annotators is notoriously difficult to achieve, particularly at the more subjective end of the spectrum (see, for example,
Reidsma and op den Akker 2008). Stable averages of human quality judgments, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian, Shen, (Belz and Reiter 2006). In fact, the large variance typical of human quality judgments can result in higher agreement between automatic metrics and human judges than among the human judges (Burstein and Wolska 2003; Belz and Reiter 2006).
 humans do it X  is an indicator of quality, that the more similar HLT system outputs are to human outputs the better they are. In fact, to some it is a matter of a priori fact that humans cannot be outperformed in HLT by machines:
Clearly, there do exist low-level HLT tasks that machines can perform faster and more accurately than humans (e.g., concordance construction, spell checking, anagram find-ing). But there is some evidence that there are more complex HLT tasks for which this is the case, too. In genre classification, even ad hoc systems have matched human performance (Jabbari et al. 2006). In NLG , domain experts have been shown to prefer system-generated language to alternatives produced by human experts (Reiter et al. 2005; Belz and Reiter 2006). In WSD , Ide and Wilks have pointed out that  X  X laimed and tested success rates in the 90%+ range are strikingly higher than the inter-annotator agreement level of 80%+, and to some this is a paradox X ; they conclude that the only explanation that seems to fit the data is that the average tested person is not as good as the tested systems at this task (Ide and Wilks 2006, page 52).
 Limited Conclusions
So, current HLT evaluation practices involve a limited number of basic evaluation techniques capable of testing for a limited range of system characteristics; because 114 they do not involve a system-external perspective we can X  X  test systems for suitability for application purpose and we can X  X  effectively meta-evaluate evaluation procedures; instead, we have to rely heavily on human judgments and annotations that we know to be unreliable in many cases. In addition, we tend to evaluate systems on a single corpus (failing to make use of one way in which an extrinsic perspective could be introduced into an intrinsic evaluation set-up). In this situation only very limited conclusions can be drawn from evaluations. When large companies with corporate lawyers are among the participants of an HLT competition, this fact must be made explicit in a prominently displayed formal disclaimer:
Prevailing evaluation practices guide the development of an entire field of research; flagship shared-task evaluation competitions such as MT -Eval, DUC , SEMEVAL ,and such evaluations to give some indication of  X  X ow well [...] systems would perform in applications X ? Towards a More Extrinsic Perspective
The explanation routinely given for not carrying out extrinsic evaluations is that they are too time-consuming and expensive. There clearly is a need for radical innovation in this area, and industry involvement and crowd-sourcing may provide ways to offset cost. But there are things we can do now, even with limited budgets. For example, automatic extrinsic evaluations are possible, and avoid the cost of human participants:
Kabadjov, Poesio, and Steinberger (2005) tested an anaphora resolver embedded in a system. Even evaluation experiments involving human subjects do not have to come with an exorbitant price-tag: REG  X 08, a competition in the field of referring expression generation which had very minimal funding, included a task-performance experiment in which the speed and accuracy with which subjects were able to identify intended referents was tested (Gatt, Belz, and Kow 2008).
 current HLT evaluation practices is to combine methods for extrinsic validation with current intrinsic techniques. What makes extrinsic evaluation infeasible in many cases is not the cost of a single experiment, but the fact that the experiment has to be repeated for every data set and for every set of systems, and that the cost of the experiment is the same every time it is run. In contrast, extrinsic validation involves one-off validation procedures for evaluation metrics, reference material, and training data. Because they are one-off experiments that form part of the development of evaluation methods and data resources, they can be achieved at a much lower cost than extrinsic evaluation methods that are directly applied to systems. Extrinsic validation can potentially take many different forms; the following are three examples: automatic metrics, are evaluated in terms of their correlation with user-performance and other application-specific evaluations. material is assessed by testing it directly in user-performance/application-specific evaluations. Intrinsic evaluation techniques using the reference material can then be weighted in favor of more highly scoring material. from extrinsically motivated experiments, for example, reading speed or eye-tracking information, or scores obtained in other user-performance/application-specific eval-uations. Training procedures can then be weighted in favor of more highly scoring material.
 Conclusions
Science and technology is littered with the remains of intrinsic measures discarded when extrinsic measures revealed them to be unreliable indicators. brain researchers pursued an intrinsic measure (size) without testing it against the cor-responding extrinsic measure (improved functionality), and ended up with an artificial brain that was very, very large, but was of little actual use (and certainly didn X  X  fulfill its declared application purpose of controlling a robot kitten).
 to get a paper into ACL or COLING that doesn X  X  have evaluation results; at the same time we consider tables of metric scores on single data sets a meaningful form of evaluation.
If we think that, say, the purpose of a parser is to place brackets in text where a human annotator thinks they should go, then we X  X e doing fine. If we think it is to facilitate high-quality IR , NER , and similar tasks, then we need to evaluate extrinsically, with reference to a range of application contexts (if the tool is intended to be a generic one) or a specific application context (if it is a specialized tool); then we need to stop picking the low-hanging fruit, and instead put our energies into solving the hard problem of how to situate evaluation in a context:
Putting greater emphasis on the extrinsic perspective in HLT research will result in improved checks and balances for the evaluation methods we apply; it will enable us to 116 make better predictions about how the methods we develop will perform when applied to the purpose we develop them for; and it will mean that we have a better answer when we are asked  X  ...but what can you do withit? X  References
