 Traditional pseudo-relevance feedback is document-level feedback which assumes that the entire content of a feedback document are relevant to the query [1-2]. How-ever, this assumption is unreasonable, since normal documents are topically diverse so that they may contain non-relevant parts as well as relevant parts to a given query, even though they are relevant documents. Hence, document-level feedback cannot prevent the feedback process from selecting non-relevant expansion terms from non-relevant parts, undermining the retrieval performance. To handle this problem, pas-expansion terms to a query-relevant passage, in order to minimize the risk of selecting non-relevant terms, and to increase the possibility of selecting relevant terms [3,4]. 
Previous works on passage-level feedback have explored window-type passages pre-fixed regardless of documents [3,4]. However, this fixed-length window passage we need to introduce the concept of variable-length passages, in which the length of a document. 
To this end, we propose a new type of passage, called a completely-arbitrary pas-sage , in which all possible word sequences of arbitrary-length are included to the set of candidate passages. The concept of a completely-arbitrary passage is different from the arbitrary passage proposed by Kaszkiel [6]. Kaszkiel X  X  arbitrary passage (either still has the restriction of fixing the length of passage. Based on this new type of pas-sage, we propose a two-stage passage-level feedback approach which consists of 1) small length can be selected as the best passage. Such a short passage may not contain a sufficient context to select expansion terms. Thus, we put the second step, the pas-sage extension, in which the boundary of the best relevant passage is extended in forward and backward directions until the best relevant passage contains a sufficient amount of useful expansion terms. We call the extended passage the maximally rele-vant passage (MRP). However, this second step raises a serious issue, the determina-tion of increment of length for constructing MRP, indicating  X  X ow many additional contexts should we extend from the best passage, to optimize the selection of expan-sion terms? X  We convert this issue to an optimization problem, by pre-defining a criterion and determining the maximally relevant passage as one that maximizes the criterion. 
Experimental results show that the proposed two-stage feedback using the com-pletely-arbitrary passage significantly improves the document-level feedback, as well as slightly increasing the performance of the passage-level feedback using window-passage. Considering that Liu X  work [4], which first applied the passage-level feed-successful report that passage-level feedback is better than document-level feedback in language modeling approaches. 2.1 Passage Retrieval: Finding the Best Passage Suppose that Q , D , and SP( D ) are a query, a document, and the set of all passages of document D , respectively. Let Score( Q , P ) be the similarity score between passage P (or document) and query Q , Then, the passage retrieval is summarized as follows: 
In the above Eq. (1), P best indicates the best passage. According to the definition of passages, there are several options for setting SP( D )  X  a semantic passage, a window passage, etc. We refer to the window passage as the fixed-length arbitrary passage in dow. Completely-arbitrary passage (notated by SP COMPLETE ( D )), which we propose as a new type of passage, is denoted as an arbitrary subsequence of adjacent words in document D [6]. The ranking of completely-arbitrary passages can be efficiently im-plemented through cover-set ranking by significantly reducing the number of com-pletely-arbitrary passages to be checked in a document [6]. 2.2 Passage Extension: Extending the Best Passage After the first stage, we find the best passage which is the most relevant snippet to a relevant passage (MRP). The proposed extension method is a center-oriented exten-MRP and then enlarges it towards forward and backward directions by the same length. In other words, let us suppose that l is the length of the best passage, and L is the increment of the length (the expansion length ). Our expansion strategy is given as follows: Center-oriented expansion : Put the best passage at center-position of final MRP. backward directions, respectively. 
The above expansion strategy has some trivial exceptional cases. When the best passage is near the boundaries of start or end position of a document, we cannot put the best passage as center of MRP. For all exceptional cases, we adopt the following principle  X  1) The expansion length should be L , and 2) the best passage should be at the center of MRP, as possible as we can. 2.3 Automatic Determination of L for Each Document For center-oriented expansion, we need to determine L (the expansion length). Fixing L for all feedback documents is unreasonable, since their relevant portions would be MRP, and then convert the determination of L to an optimization problem which finds the best one to maximize the criterion. 
As for such criterion, we employ Sim( P , F ), which means a similarity between a candidate relevant passage P and the set of feedback documents F . Then, our optimi-zation problem for determining L is formulated as follows: where P best is the best passage selected from the first passage retrieval stage, and P ext the expanded passage obtained after applying our center-oriented expansion strategy  X  L as follows: E( P best ) can be incrementally constructed by first extending the best passage by  X  L to expanded passage. Note that we additionally introduce W min which indicates the mini-mum length of MRP. This parameter is necessary for preventing an extremely short passage from having unreasonably high similarity value. By using W min , many non-Sim( P , F ) can be made. As a result, MRP of reasonable-length can be selected only by evaluating Sim( P , F ). A default value for W min is 100. The proposed framework has two model-dependent parts -Score( Q , P ) and Sim( P , F ). Score( Q , P ) is defined as query-likelihood from passage language models [7]. For estimating passage language model, we select Jelinek-Mercer smoothing method with smoothing parameter  X  . Let us suppose that  X  Q ,  X  P , and  X  C are query language model for a given query, passage language model for passage P , and collection language model, respectively. Then, Score( Q , P ) is defined as follows: where 
For Score( Q , P ), suppose that  X  F is feedback language models from feedback doc-uments. We propose the following log-likelihood ratio Sim Ratio ( P , F ) between genera-tion probabilities of passage P from  X  F and  X  C : 
We further smooth the feedback model P( w |  X  F ) with collection language model  X  C by using Jelinerk-Mercer smoothing: ) | ( )  X  | ( ) 1 ( smoothing parameter  X  F is close to the smoothing parameter of document model, not smoothing parameter  X  of passage model. A default value for  X  F is 0.25. For evaluation, we used four TREC test collections  X  TREC4-AP, TREC7, TREC8 and WT2G. TREC4-AP is the sub-collection of Associated Press in disk 1 and disk 2 for TREC4 (the number of documents is 158,240). Other test collections are the same standard method was applied to extract index terms. 
For all experiments, the baseline run used Dirichlet-prior for the document model due to its superiority over Jelinek-Mercer smoothing, using smoothing parameter  X  as follows. We use acronym DM for this baseline run. To determine the smoothing pa-selected the best performed one for each test collection. For passage language models, Jelinek-Mercer smoothing is applied. For smoothing passage language model,  X  is fixed to 0.9 for TREC4-AP, and 0.1 for other test collections. In a similarity metric of set to 0.25. We used MAP (Mean Average Precision) for evaluating all experiments. For feedback model, we adopted Zhai X  X  model-based feedback (i.e. the generation method using two-component mixture model [2]). For the best interpolation parame-ter  X  , we performed feedback runs using different interpolation parameter  X  between 0.05 and 0.9, and selected the best performed one. For most runs, the best  X  was less than 0.4. 
Table 1 shows results of the baseline (DM), document-level feedback (Doc), two passage-level feedbacks using window passage ( W as parameter) and completely-arbitrary passage (  X  L as parameter), across the different numbers of feedback docu-ments ( R ). To see whether or not a passage-level feedback improves Doc, we performed Wilcoxon signed rank test to examine whether the improvement was statis-tically significant or not, at 95% and 99% confidence level. We attached  X  and to the figure (performance number) of each cell in the table when the test passes at 95% and 99% confidence level, respectively. Both of passage-level feedback improve docu-ment-level feedback. However, the window-passage does not frequently show a sta-tistically significant improvement over the Doc for some test collections such as TREC4-AP and WT2G. At the best W (W=150) , among the total of 20 runs, only 9 runs show a significant improvement at 95% confidence level, and only 4 runs at 99% confidence level. On the other hand, the complete-arbitrary passage does show statis-tically significant improvement over Doc, for all test collections, including TREC4-AP and WT2G which are not statistically significant in case of window passage. At the best  X  L (  X  L=75) , among total 20 runs, 13 runs shows a significant improvement at 95% confidence level, and 8 runs at 99% confidence level. 
Summing up, the proposed two passage-level feedbacks clearly show significant improvement over the baseline (DM) at a high confidence level. Compared with doc-ument-level feedback (Doc), the proposed completely-arbitrary passage is much more effective than the window passage, by having many runs with significant improve-ments for all test collections. Especially, the proposed completely-arbitrary passage is almost close to a parameter-less approach due to much-less-sensitivity of  X  L . This work was supported by the Korea Science and Engineering Foundation (KOSEF) through the Advanced Information Technology Research Center (AITrc), also in part by the BK 21 Project and MIC &amp; IITA through IT Leading R&amp;D Support Project in 2007. 
