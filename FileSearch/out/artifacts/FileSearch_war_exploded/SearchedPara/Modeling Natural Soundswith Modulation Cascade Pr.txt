 Computational models for sensory processing are still in their infancy, but one promising approach has been to compare aspects of sensory processing with aspects of machine-learning algorithms tive modeling framework to derive these learning algorithms. For example, Independent Component Analysis (ICA) and Sparse Coding (SC), Slow Feature Analysis (SFA), and Gaussian Scale Mix-ture Models (GSMMs) are examples of algorithms corresponding to generative models that show (although see [2]). The purpose of this paper is to address this imbalance.
 This paper has three parts. In the first we review models for the short-time structure of sound and argue that a probabilistic time-frequency model has several distinct benefits over traditional time-frequency representations for auditory modeling. In the second we review a model for the long-time models are combined with the notion of auditory features to produce a full generative model for sounds called the Modulation Cascade Process (MCP). We then show how to carry out learning and inference in such a complex hierarchical model, and provide results on speech for complete and missing data tasks. a few sinusoids. Of course, the spectral content of natural sounds changes slowly over time. This is handled by time-frequency representations, such as the Short-Time Fourier Transform (STFT) and spectrogram, which indicate the spectral content of a local, windowed section of the sound. More specifically, the STFT ( x The (possibly frequency dependent) duration of the window ( r example, the window for speech is typically chosen to last for several pitch periods, so that both pitch and formant information is represented spectrally.
 The first stage of the auditory pathway derives a time-frequency-like representation mechanically at the basilar membrane. Subsequent stages extract progressively more complex auditory features, with structure extending over more time. Thus, computational models of auditory processing often begin with a time-frequency (or auditory-filter bank) decomposition, deriving new representations from the time-frequency coefficients [4]. Machine-learning algorithms also typically operate on the time-frequency coefficients, and not directly on the waveform. The potential advantage lies in the ease with which auditory features may be extracted from the STFT representation. There are, however, associated problems. For example, time-frequency representations tend to be over-complete (e.g. the number of STFT coefficients tends to be larger than the number of samples of the original sound the STFT this manifold is a hyper-plane). Algorithms that solve tasks like filling-in missing data an ad hoc manner by projecting time-frequency coefficients back onto the manifold according to an the waveform on the same heuristics that led to the original time-frequency representation. Not only the  X  X indow X , to be chosen automatically.
 The heuristic behind the STFT  X  that sound comprises sinusoids in slowly-varying linear superpo-sition  X  led Qi et al [6] to propose a probabilistic algorithm called Bayesian Spectrum Estimation (BSE), in which the sinusoid coefficients ( x trolled by the dynamical parameters  X  very slow, and as  X  the coefficients is given by  X  sinusoids, plus Gaussian noise. This model is essentially a Linear Gaussian State Space System with time varying weights defined by the sinusoids. Thus, inference is simple, proceeding via the Kalman Smoother recursions with time-varying weights. In effect, these recursions dynamically adjust the the short-time structure of sounds and it will essentially form the bottom level of the MCP. In the next section we turn our attention to a model of the longer-time structure. A salient property of the long-time statistics of sounds is the persistence of strong amplitude mod-ulation [7]. Speech, for example, contains power in isolated regions corresponding to phonemes. The phonemes themselves are localised into words, and then into sentences. Motivated by these observations, Anonymous Authors [8] have proposed a model for the long-time structures in sounds using a demodulation cascade. The basic idea of the demodulation cascade is to represent a sound as a product of processes drawn from a hierarchy, or cascade, of progressively longer time-scale mod-ulators. For speech this might involve three processes: representing sentences on top, phonemes in the middle, and pitch and formants at the bottom (e.g. fig. 1A and B). To construct such a repre-sentation, one might start with a traditional amplitude demodulation algorithm, which decomposes a signal into a quickly-varying carrier and more slowly-varying envelope. The cascade could then be built by applying the same algorithm to the (possibly transformed) envelope, and then to the en-and the envelope found by the demodulation algorithm are well-behaved. Unfortunately, traditional methods (like the Hilbert Transform, or low-pass filtering a non-linear transformation of the stimu-lus) return a suitable carrier or envelope, but not both. A new approach to amplitude demodulation is thus called for.
 In a nutshell, the new approach is to view amplitude demodulation as a task of probabilistic in-ference. This is natural, as demodulation is fundamentally ill-posed  X  there are infinitely many decompositions of a signal into a positive envelope and real valued carrier  X  and so prior infor-mation must always be leveraged to realise such a decomposition. The generative model approach makes this information explicit. Furthermore, it not necessary to use the recursive procedure (just described) to derive a modulation cascade: the whole hierarchy can be estimated at once using a single generative model. The generative model for Probabilistic Amplitude Demodulation (PAD) is A set of modulators ( X (
Z scales of these processes, inherited by the modulators, are ordered such that  X  the modulators are formed by passing these variables through a point-wise non-linearity to enforce positivity. A typical choice might be which is logarithmic for large negative values of z (m) transforms the Gaussian distribution over z (m) good match to the marginal distributions of natural envelopes. The parameter a (m) controls exactly positive signals modulate a Gaussian white-noise carrier, to yield observations y point-wise product. A typical draw from this generative model can be seen in Fig. 1C. This model surprising observation is that this very simple model is excellent at demodulation. Inference in this model typically proceeds by a zero-temperature EM-like procedure. Firstly the carrier ( x (1) Slower, more Bayesian algorithms that integrate over the modulators using MCMC indicate that this approximation is not too severe, and the results are compelling. We have reviewed two contrasting models: The first captures the local harmonic structure of sounds, but has no long-time structure; The second captures long-time amplitude modulations, but models model. We are guided by the observation that the auditory system might implement a similar syn-thesis. In the well-known psychophysical phenomenon of comodulation masking release (see [9] for a review), a tone masked by noise with a bandwidth greater than an auditory filter becomes audible x x x y Figure 1: An example of a modulation-cascade representation of speech (A and B) and typical sam-ples from generative models used to derive that representation (C). A) The spoken-speech waveform (black) is represented as the product of a carrier (blue), a phoneme modulator (red) and a sentence lope ( x (2) with a carrier (blue), a phoneme modulator (red) and a sentence modulator (magenta). if the noise masker is amplitude modulated. This suggests that long-time envelope information is processed and analysed across (short-time) frequency channels in the auditory system. A simple way to combine the two models would be to express each filter coefficient of the time-frequency model as a product of processes (e.g. x widely seperated channels of natural sounds can be strongly correlated [7]. Furthermore, comodu-lation masking release suggests that amplitude-modulation is processed across frequency channels and not independently in each channel. Presumably this reflects the collective modulation of wide-band (or harmonic) sounds, with features that span many frequencies. Thus, a synthesis of BSE and PAD should incorporate the notion of auditory features.
 The forward model. The Modulation Cascade Process (MCP) is given by Once again, latent variables are arranged in a hierarchy according to their time-scales (which de-pend on m ). At the top of the hierarchy is a long-time process which models slow structures, like the sentences of speech. The next level models more quickly varying structure (like phonemes). for instance). Unlike in PAD, the middle and lower levels now contain multiple process. So, for example if K two modulators in the middle level, and one slowly varying modulator at the top (see fig. 2A). The idea is that the modulators in the first level independently control the presence or absence of individual spectral features (given by P phoneme might be periodic, but this periodicity might change systematically as the speaker alters A. B.
 Figure 2: A. Schematic representation of the MCP forward model in the simple case when K K 2 = 2 top (magenta) to the fastest (blue) with an intermediate modulator between (red). The outer-product of the modulators multiplies the generative weights (black and white, only 4 of the 8 shown). In turn, these modulate sinusoids (top right) which are summed to produce the observations (bottom right). B. A draw from the forward model using parameters learned from a spoken-sentence (see region depicted in the bottom four panels. lowest layer of the hierarchy. The role of the modulators in the second level is to simultaneously turn on groups of similar features. For example, one modulator might control the presence of all the harmonic features and the other the broad-band features. Finally the top level modulator gates all the auditory features at once. Fig. 2B shows a draw from the forward model for a more complicated example. Promisingly the samples share many features of natural sounds.
 Relationship to other models. This model has an interesting relationship to previous statistical models and in particular to the GSMMs. It is well known that when ICA is applied to data from corresponding to similar filters sharing power. GSMMs model dependencies using a hierarchical framework, in which the distribution over the coefficients depends on a set of latent variables that introduce correlations between their powers. The MCP is similar, in that the higher level latent power of ICA coefficients are a sign that AM is prevalent in natural scenes. The MCP can be seen as a generalisation of the GSMMs to include time-varying latent variables, a deeper hierarchy and a probabilistic time-frequency representation.
 Inference and learning algorithms. Any type of learning in the MCP is computationally demand-ing. Motivated by speed, and encouraged by the results from PAD, the aim will therefore be to find a joint MAP estimate of the latent variables and the weights, that is of combined MAP and ML inference in such models: namely that the weights grow without bound, enabling the modal values of latent variables to shrink towards zero, increasing their density under the prior. The resulting cost function is, We would like to optimise this objective-function with respect to the latent variables ( x (m) generative tensor ( g large number of latent variables to estimate ( T  X  ( K is that the generative tensor contains a large number of elements D  X  K EM-like algorithm that iterates between updating the latents and the weights. First we outline the initialisation procedure.
 oped for the MCP will be explained in some detail. The main idea is to learn the model one layer at a time. This is achieved by clamping the upper layers of the hierarchy that are not being learned to are clamped and the mean of the emission distribution becomes where  X  the cost-function ( log p (X , Y , G |  X  ) ) with respect to the un-clamped latents ( x (1) generative weights (  X  latents and fewer parameters to estimate. When this process is complete, the second layer of latent stage. One choice is to set the individual weights to their averages g learn the lower level weights. These chunks are chosen to be relatively stationary segments that have a time-scale similar to the second-level modulators. This allows us to make the simplifying assumption that just one second-level modulator was active during the chunk. The generative tensor can be therefore be initialised using g stage of learning to converge faster, and to a similar solution.
 are updated simultaneously using gradient based optimisation of Eq. 11. In the M-Step, the gen-erative tensor is updated using co-ordinate ascent. That is to say that we sequentially update each g optimisation of the generative tensor and latent variables is possible, but the memory requirements using the usual linear regression solution which involves a prohibitive matrix inverse). The MCP was trained on a spoken sentence, lasting 2s and sampled at 8000Hz, using the algorithm 200 ms, 2 s } . The time-frequency representation had D/ 2 = 100 sines and D/ 2 = 100 cosines spaced logarithmically from 100  X  4000 Hz. The model was given K first level of the hierarchy, and K Opteron with 2Gb of memory. x x x Figure 3: Application of the MCP to speech. Left panels: The inferred latent variable hierarchy. At top is the sentence modulator (magenta). Next are the phoneme modulators, followed by the intra-phoneme modulators. These are coloured according to which of the phoneme modulators they interact most strongly with. The speech waveform is shown in the bottom panel. Right panels: The learned spectral features ( p g 2 ample, the top panel show the spectra from g modulator look similar and offer the features only differ in their phase.
 The results can be seen in Fig. 3. The MCP recovers a sentence modulator, phoneme modulators, and intra-phoneme modulators. Typically a pair of features are used to model a phoneme, and often monic (modelling voiced features) and those which are broad-band (modelling unvoiced features). One way of assessing which features of speech the model captures is to sample from the forward model using the learned parameters. This can be seen in Fig. 2B. The conclusion is that the model is capturing structure across a wide range of time-scales: formants and pitch structure, phoneme structure, and sentence structure.
 the generated data contain fewer transients and noise segments than natural speech, and more vowel-like components. The reason for this is that at the sampling rates used, many of the noisy segments are indistinguishable from white-noise and are explained using observation noise. These problems are alleviated by moving to higher sampling rates, but the algorithm is then markedly slower. The second difference concerns the inferred and generated latent variables in that the former are much sparser than the latter. The reason is that learned generative tensor contains many g particular pairs of phoneme and intra-phoneme modulators are active. So although many modulators are active at one time, only one or two make sizeable contributions. Conversely, in inference, we is not the case, the inference goes to the maximum of the prior which is zero. In effect there are large error-bars on the non-contributing components X  estimates.
 Finally, to indicate the improvement of the MCP over PAD and BSE, we compare the algorithms abilities to fill in missing sections of a spoken sentence. The average root-mean-squared (RMS) on a different spoken sentence from the same speaker, before inference was carried out on the test data. To make the comparison fair, BSE is given an identical set of sinusoidal basis functions as MCP, and the associated smoothness priors were learned on the same training data.
 Typical results can be seen in fig. 4. On average the RMS errors for MCP, BSE and PAD were: Figure 4: A selection of typical missing data results for three phonemes (columns). The top row shows the original speech segement with the missing regions shown in red. The middle row shows the predictions made by the MCP and the bottom row those made by BSE. and therefore it merely serves as a baseline in these experiments. Both MCP and BSE smoothly interpolate their latent variables over the missing region. However, whereas BSE smoothly inter-polates each sinusoidal component independently, MCP interpolates the set of learned auditory fea-it improves over BSE by such a large margin. We have introduced a neuroscience-inspired generative model for natural sounds that is capable of capturing structure spanning a wide range of temporal scales. The model is a marriage between a tic demodulation cascade (that captures the long-time structure). When the model is trained on a that capture structures like different voiced sections of speech. The upper levels comprise a tem-porally ordered set of modulators are used to represent sentence structure, phoneme structure and intra-phoneme variability. The superiority of the new model over its parents was demonstrated in a missing data experiment where it out-performed the Bayesian time-frequency analysis by a large margin.

