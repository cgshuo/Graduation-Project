 The n -point correlation functions (npcf) are powerful spatial statistics capable of fully characterizing any set of multidi-mensional points. These functions are critical in key data analyses in astronomy and materials science, among other fields, for example to test whether two point sets come from the same distribution and to validate physical models and theories. For example, the npcf has been used to study the phenomenon of dark energy, considered one of the ma-jor breakthroughs in recent scientific discoveries. Unfortu-nately, directly estimating the continuous npcf at a single value requires O ( N n )timefor N points, and n may be 2, 3, 4 or even higher, depending on the sensitivity required. In order to draw useful conclusions about real scientific prob-lems, we must repeat this expensive computation both for many different scales in order to derive a smooth estimate and over many different subsamples of our data in order to bound the variance.

We present the first comprehensive approach to the entire n -point correlation function estimation problem, including fast algorithms for the computation at multiple scales and for many subsamples. We extend the current state-of-the-art tree-based approach with these two algorithms. We show an order-of-magnitude speedup over the current best approach with each of our new algorithms and show that they can be used together to obtain over 500x spee dups over the state-of-the-art in order to enable much larger datasets and more accurate scientific analyses than were possible previously. J.2 [ Physical Sciences and Engineering ]: Astronomy; G.4 [ Mathematical Software ]: Algorithm design and anal-ysis N-point Correlation Functions, Jackknife Resampling
In this paper, we discuss a hierarchy of powerful statistics: the n -point correlation functions , which constitute a widely-used approach for detailed characterizations of multivariate point sets. These functions, which are analogous to the mo-ments of a univariate distribution, can completely describe any point process and are widely applicable.

Applications in astronomy. The n-point statistics have long constituted the state-of-the-art approach in many sci-entific areas, in particular for detailed characterization of the patterns in spatial data. They are a fundamental tool in astronomy for characterizing the large scale structure of the universe [20], fluctuations in the cosmic microwave back-ground [28], the formation of clusters of galaxies [32], and the characterization of the galaxy-mass bias [17]. They can be used to compare observations to theoretical models through perturbation theory [1, 6]. A high-profile example of this was a study showing large-scale evidence for dark energy [8]  X  this study was written up as the Top Scientific Break-through of 2003 in Science [26]. In this study, due to the massive potential implications to fundamental physics of the outcome, the accuracy of the n -point statistics used and the hypothesis test based on them were a considerable focus of the scientific scrutiny of the results  X  underscoring both the centrality of n -point correlations as a tool to some of the most significant modern scientific problems, as well as the importance of their accurate estimation.

Materials science and medical imaging. The ma-terials science community also makes extensive use of the n -point correlation functions. They are used to form three-dimensional models of microstructure [13] and to character-ize that microstructure and re late it to macroscopic proper-ties such as the diffusion coefficient, fluid permeability, and elastic modulus[31, 30]. The n -point correlations have also been used to create feature sets for medical image segmen-tation and classification [22, 19, 2].

Generality of npcf. In addition to these existing ap-plications, the n -point correlations are completely general. Thus, they are a powerful tool for any multivariate or spa-tial data analysis problem. Ripley [23] showed that any point process consisting of multidimensional data can be completely determined by the distribution of counts in cells. Thedistributionofcountsincellscaninturnbeshowntobe completely determined by the set of n -point correlation func-tions [20]. While ordinary statistical moments are defined in terms of the expectation of increasing powers of X ,the n -point functions are determin ed by the cross-correlations of counts in increasing numbers of nearby regions. Thus, we have a sequence of increasingly complex statistics, anal-ogous to the moments of ordinary distributions, with which to characterize any point process and which can be estimated from finite data. With this simple, rigorous characterization of our data and models, we can answer the key questions posed above in one statistical framework.

Computational challenge. Unfortunately, directly es-timating the n -point correlation functions is extremely com-putationally expensive. As we will show below, estimating the npcf potentially requires enumerating all n -tuples of data points. Since this scales as O ( N n )for N data points, this is prohibitively expensive for even modest-sized data sets and low-orders of correlation. H igher-order correlations are often necessary to fully understand and characterize data [32]. Furthermore, the npcf is a continuous quantity. In or-der to understand its behavior at all the scales of interest for a given problem, we must repeat this difficult computa-tion many times. We also need to estimate the variance of our estimated npcf. This in gen eral requires a resampling method in order to make the most use of our data. We must therefore repeat the O ( N n ) computation not only for many scales, but for many different subsamples of the data.
In the past, these computational difficulties have restricted the use of the n -point correlations, despite their power and generality. The largest 3-point correlation estimation thus far for the distribution of galaxies used only approximately 10 5 galaxies [16]. Higher-order correlations have been even more restricted by computational considerations.

Large data. Additionally, data sets are growing rapidly, and spatial data are no exception. The National Biodiversity Institute of Costa Rica has collected over 3 million observa-tions of tropical species along with geographical data [11]. In astronomy, the Sloan Digital Sky Survey [25] spent eight years imagining the northern sky and collected tens of ter-abytes of data. The Large Synoptic Survey Telescope [14], scheduled to come online later this decade, will collect as much as 20 terabytes per night for ten years. These mas-sive datasets will render n -point correlation estimation even more difficult without efficient algorithms.

Our contributions. These computational considerations have restricted the widespread use of the npcf in the past. We introduce two new algorithms, building on the previ-ous state-of-the-art algorithm [9, 18], to address this entire computational challenge. We present the first algorithms to efficiently overcome two important computational bottle-necks. For each of these problems, we present new algorithms ca-pable of sharing work between different parts of the compu-tation. We prove a theorem which allows us to eliminate a critical redundancy in the computation over multiple scales. We also cast the computation over multiple subsamples in a novel way, allowing a much more efficient algorithm. Each of these new ideas allows an order-of-magnitude speedup over the existing state of the art. These algorithms are there-fore able to render n -point correlation function estimation tractable for many large datasets for the first time by al-lowing N to increase and allow more sensitive and accurate scientific results for the first time by allowing multiple scales and resampling regions. Our work is the first to deal directly with the full computational task.

Overview. In Section 2, we define the n -point correla-tion functions and describe their estimators in detail. This leads to the O ( J  X  M  X  N n ) computational challenge men-tioned above. We then introduce our two new algorithms for the entire npcf estimation problem in Section 3. We show experimental results in Section 4. Finally, we conclude in Section 5 by highlighting future work and extensions of our method.
Due to the computational difficulty associated with esti-mating the full npcf, many alternatives to the full npcf have been developed, including those based on nearest-neighbor distances, quadrats, Dirichlet cells, and Ripley X  X  K function (and related functions) (See [24] and [3] for an overview and further references). Counts-in-cells [29] and Fourier space methods are commonly used for astronomical data. How-ever, these methods are generally less powerful than the full npcf. For instance, the counts-in-cells method cannot be corrected for errors due to the edges of the sample window. Fourier transform-based methods suffer from ringing effects and suboptimal variance. See [27] for more details.
Since we deal exclusively with estimating the exact npcf, we only compare against other methods for this task. The existing state-of-the-art methods for exact npcf estimation use multiple space-partitioning trees to overcome the O ( N scaling of the n-point point estimator. This approach was first introduced in [9, 18]. It has been parallelized using the Ntropy framework [7]. We present the serial version here and leave the description of our ongoing work on parallelizing our approach to future work.
We now define the n -point correlation functions. We pro-vide a high-level description; for a more thorough defintion, see [20, 27]. Once we have given a simple description of the npcf, we turn to the main problem of this paper: the com-putational task of estimating the npcf from real data. We give several common estimators for the npcf, and highlight the underlying counting problem in each. We also discuss the full computational task involved in a useful estimate of the npcf for real scientific problems.

Problem setting. Our data are drawn from a point pro-cess . The data consist of a set of points D in a subset of . Note that we are not assuming that the locations of in-dividual points are independent, just that our data set is a fair sample from the underlying ensemble. We assume that distant parts of our sample window are uncorrelated, so that by averaging over them, we can approximate averages over multiple samples from the point process.

Following standard practice in astronomy, we assume that the process is homogeneous and isotropic. Note that the n -point correlations can be defined both for more general point processes and for continuous random fields. The estimators for these cases are similar to the ones described below and can be improved by similar algorithmic techniques.
Defining the npcf. We now turn to an informal, in-tuitive description of the hierarchy of n -point correlations. Since we have assumed that properties of the point process are translation and rotation invariant, the expected number of points in a given volume is proportional to a global den-sity  X  . If we consider a small volume element dV , then the probability of finding a point in dV is given by: with dV suitably normalized. If the density  X  completely characterizes the process, we refer to it as a Poisson process.
Two-point correlation. The assumption of homogene-ity and isotropy does not require the process to lack struc-ture. The positions of points may still be correlated. The joint probability of finding objects in volume elements dV and dV 2 separated by a distance r is given by: where the dV i are again normalized. The two-point corre-lation  X  ( r ) captures the increased or decreased probability of points occurring at a separation r . Note that the 2-point correlation is characterized by a single scale r and is a con-tinuously varying function of this distance.

Three-point correlation. Higher-order correlations de-scribe the probabilities of more than two points in a given configuration. We first consider three small volume ele-ments, which form a triangle (See Fig. 1(b)). The joint probability of simultaneousl y finding points in volume ele-ments dV 1 ,dV 2 ,and dV 3 , separated by distances r 12 ,r r 23 ,isgivenby: The quantity in square brackets is sometimes called the com-plete (or full) 3-point correlation function and  X  is the re-duced 3-point correlation function . We will often refer to  X  as simply the 3-point correlation function, since it will be the quantity of computational interest to us. Note that unlike the 2-point correlation, the 3-point correlation depends both on distance and configuration. The function varies contin-uously both as we increase the lengths of the sides of the triangle and as we vary its shape, for example by fixing two legs of the triangle and varying the angle between them.
Higher-order correlations. Higher-order correlation functions (such as the 4-point correlation in Fig. 1(c)) are defined in the same fashion. The probab ility of finding n -points in a given configuration can be written as a summa-tion over the n -point correlation functions. For example, in addition to the reduced 4-point correlation function  X  , the complete 4-point correlation depends on the six 2-point terms (one for each pairwise distance), four 3-point terms (one for each triple of distances), and three products of two 2-point functions. The reduced four-point correlation is a function of all six pairwise distances. In general, we will denote the n -point correlation function as  X  ( n ) (  X  ), where the argument is understood to be refer to this set of pairwise distances as a configuration ,or in the computational context, as a matcher (see below).
We have shown that the n -point correlation function is a fundamental spatial statistic and have sketched the defi-nitions of the n -point correlation functions in terms of the underlying point process. We now turn to the central task of this paper: the problem of estimating the n -point corre-lation from real data. We describe several commonly used estimators and identify their common computational task. Figure 1: Visual interpretation of the n -point corre-lation functions.

We begin by considering the task of computing an esti-mate b  X  ( n ) ( r ) for a given configuration. For simplicity, we consider the 2-point function first. Recall that  X  ( r )captures the increased (or decreased) probability of finding a pair of points at a distance r over finding the pair in a Poisson distributed set. This observation suggests a simple Monte Carlo estimator for  X  ( r ). We generate a random set of points R from a Poisson distribution with the same (sample) den-sity as our data and filling the same volume. We then com-pare the frequency with which points appear at a distance close to r in our data versus in the random set.

Simple estimator. Let DD( r ) denote the number of pairs of points ( x i ,x j ) in our data, normalized by the total number of possible pairs, whose pairwise distance d ( x i is close to r (inawaytobemadeprecisebelow). LetRR( r ) be the number of points whose pairwise distances are in the same interval (again normalized) from the random sample (DD stands for data-data, RR for random-random). Then, a simple estimator for the two-point correlation is [20, 27]: This estimator captures the intuitive behavior we expect. If pairs of points at a distance near r are more common in our data than in a completely random (Poisson) distribution, we are likely to obtain a positive estimate for  X  .

This simple estimator suffers from suboptimal variance and sensitivity to noise. The Landy-Szalay estimator [12] overcomes these difficulties. Here the notation DR ( r )de-notes the number of pairs ( x i ,y j ) at a distance near r where x is from the data and y j is from the Poisson sample (DR  X  data-random pairs). Other widely used estimators use the same underlying quantities  X  pairs of points satisfying a given distance constraint [12, 10].

Three-point estimator. The 3-point correlation func-tion depends on the pairwise distances between three points, rather than a single distance as before. We will therefore need to specify three distance constraints, and estimate the function for that configuration. The Landy-Szalay estima-tor for the 2-point function can be generalized to any value of n , and retains its improved bias and variance [29]. We again generate points from a Poisson distribution, and the estimator is also a function of quantities of the form D ( or D ( i ) R ( n  X  i ) . These refer to the number of unique triples of points, all three from the data or two from the data and one from the Poisson set, with the property that their three pairwise distances lie close to the distances in the matcher.
All estimators count tuples of points. Any n -point correlation can be estimated using a sum of counts of n -tuples of points of the form D ( i ) R ( n  X  i ) ( r ), where i ranges from zero to n . The argument is a vector of distances of length the configuration. We count unique tuples of points whose pairwise distances are close to the distances in the matcher in some ordering.
Note that all the estimators described above depend on the same fundamental quantities: the number of tuples of points from the data/Poisson set that satisfy some set of dis-tance constraints. Thus, our task is to compute this number given the data and a suitably large Poisson set. Enumer-ating all n -tuples requires O ( N n ) work for N data points. Therefore, we must seek a more efficient approach. We first give some terminology for our algorithm description below. We then present the entire computational task.

Matchers. We sp ecify an n -tuple with one for each pairwise distance in the tuple. We mentioned above that we count tuples of points whose pairwise dis-tances are  X  X lose X  to the distance constraints. Each of the pairwise distance constraints consists of a lower and upper refer to this collection of distance constraints r as a matcher . We refer to the entries of the matcher as r ( l ) ij and r the indices i and j refer to the volume elements introduced above (Fig. 1). We sometimes refer to an entry as simply r , with the upper and lower bounds being understood.
Satisfying matchers. Given an n -tuple of points and amatcher r , we say that the tuple satisfies the matcher if there exists a permutation of the points such that each pair-wise distance does not violate the corresponding distance constraint in the matcher. More formally:
Definition 1. Given an n -tuple of points ( p 1 ,...,p n )and amatcher r , we say that the tuple satisfies the matcher if there exists (at least one) permutation  X  of [1 ,...,n ]such that for all indices i, j  X  [1 ,...n ] such that i&lt;j .
The computational task. We can now formally define our basic computational task:
Definition 2. Computational Task 1: Compute the counts of tuples D ( i ) R ( j ) ( r ) . Given a data set D , random set R , and matcher r ,and0  X  i  X  n , compute the number of unique n -tuples of points with i points from D and n  X  points from R , such that the tuple satisfies the matcher. Computing these quantities directly requires enumerating all unique n -tuples of points, which takes O ( N n )workand is prohibitively slow for even two-point correlations.
Multiple matchers. The estimators above give us a value b  X  ( n ) ( r ) at a single configuration. However, the n -point correlations are continuous quantities. In order to fully char-acterize them, we must compute estimates for a wide range of configurations. In order to do this, we must repeat the computation in Defn. 2 for many matchers, both of different scales and configurations.

Definition 3. Computational Task 2: Multiple match-ers. Given a data set D , random set R , and a collection of M matchers { r m } , compute D ( i ) R ( j ) ( r m )foreach1 This task requires us to repeat Task 1 M times, where M controls the smoothness of our overall estimate of the npcf and our quantitative picture of its overall behavior. Thus, it is generally necessary for M to be large.

Resampling. Simply computing point estimates of any statistic is generally insufficient for most scientific applica-tions. We also need to bound the variance of our estima-tor and compute error bars. In general, we must make the largest possible use of the available data, rather than with-holding much of it for variance estimation. Thus, a resam-pling method is necessary.

Jackknife resampling is a widely used variance estimation method and is popular with astronomical data [15]. It is also used to study large scale structure by identifying variations in the npcf across different parts of the sample window [16]. We divide the data set into subregions. We eliminate each region from the data in turn, then compute our estimate of the npcf. We repeat this for each subset, and use the resulting estimates to bound the variance. This leads to our third and final computational task.

Definition 4. Computational Task 3: Jackknife re-sampling. We are given a data set D , random set R ,aset of M matchers r m , and a partitioning of D into J subsets D k .Foreach1  X  k  X  J ,constructtheset D (  X  k ) = D/D k . This task requires us to repeat Task 1 J times on sets of size D  X  D/J .Notethat J controls the quality of our vari-ance estimation, with larger values necessary for a better estimate.

The complete computational task. We can now iden-tify the complete computational task for n -point correlation estimation. Given our data and random sets, a collection of M matchers, and a partitioning of the data into J subre-gions, we must perform Task 3. This in turn requires us to perform Task 2 J times. Each iteration of Task 2 requires M computations of Task 1. Therefore, the entire compu-tation requires O ( J  X  M  X  N n ) time if done in a brute-force fashion. In the next section, we describe our algorithmic approach to simultaneously computing all three parts of the computation, thus allowing significant savings in time.
We have identified the full computational task of n -point correlation estimation. We now turn to our new algorithm. We begin by addressing previous work on efficiently comput-ing the counts D ( i ) R ( j ) ( r ) described above (Computational Task 1.) We first describe the multi-tree algorithm for com-puting these counts. We then give our new algorithm for directly solving computations with multiple matchers (Com-putational Task 2) and our method for efficiently computing counts for resampling regions (Computational Task 3).
We build on previous, tree-based algorithms for the n -point correlation estimation problem [9, 18]. The key idea is to employ multiple kd -trees to improve on the O ( N scaling of the brute-force approach. (a) Comparing two nodes. (b) Comparing three Figure 2: Computing node-node bounds for prun-ing. kd -trees. The kd -tree [21, 5] is a binary space partition-ing tree which maintains a bounding box for all the points in each node. The root consists of the entire set. Children are formed recursively by splitting the parent X  X  bounding box along the midpoint of its largest dimension and partitioning the points on either side. We can build a kd -tree on both the data and random sets as a pre-processing step. This requires only O ( N log N )workand O ( N )space.

We employ the bounding boxes to speed up the naive com-putation by using them to identify opportunities for prun-ing . By computing the minimum and maximum distances between a pair of kd -tree nodes (Fig. 2), we can identify cases where it is impossible for any pair of points in the nodes to satisfy the matcher.

Dual-tree algorithm. For simplicity, we begin by con-sidering the two-point correlation estimation (Alg. 1). Recall that the task is to count the number of unique pairs of points that satisfy a given matcher. We consider two tree nodes at a time, one from each set to be correlated. We compute the upper and lower bounds on distances between points in these nodes using the bounding boxes. We can then com-pare this to the matcher X  X  lower and upper bounds. If the distance bounds prove that all pairs of points are either too far or too close to possibly satisfy the matcher, then we do not need to perform any more work on the nodes. We can thus prune all child nodes, and save O ( | T 1 | X | T 2 | )work. If we cannot prune, then we split one (or both) nodes, and re-cursively consider the two (or four) resoling pairs of nodes. If our recursion reaches leaf nodes, we compare all pairs of points exhaustively.

We begin by calling the algorithm on the root nodes of the tree. If we wish to perform a DR count, we call the algorithm on the root of each tree. Note also that we only want to count unique pairs of points. Therefore, we can prune if T 2 comes before T 1 in an in-order tree traversal. This ensures that we see each pair of points at most once.
Multi-tree algorithm. We can extend this algorithm to the general n case. Instead of considering pairs of tree nodes, we compare an n -tuple of nodes in each step of the algorithm. This multi-tree algorithm uses the same basic idea  X  use bounding information between pairs of tree nodes to identify sets of nodes whose points cannot satisfy the matcher. We need only make two extensions to Alg. 1. First, we must do more work to determine if a particular Algorithm 1 DualTree2pt (Tree node T 1 ,Treenode T 2 , matcher r ) 5: end if 10: DualTree2pt ( T 1 . left ,T 2 . left) Algorithm 2 MultiTreeNpt (Tree node T 1 ,..., Tree node T ,matcher r ) 5: end if 10: Let T i be the largest node tuple of points satisfies the matcher. We accomplish this in Alg. 3 by iterating over all permutations of the indices. Each permutation of indices corresponds to an assignment of pairwise distances to entries in the matcher. We can quickly check if this assignment is valid, and we only count tuples that have at least one valid assignment. The second exten-sion is a similar one for checking if a tuple of nodes can be pruned (Alg. 4). We again iterate through all permu-tations and check if the distance bounds obtained from the bounding boxes fall within the upper and lower bounds of the matcher entry. As before, for an D ( i ) R ( j ) count, we call the algorithm on i copies of the data tree root and j copies oftherandomtreeroot.
The algorithms presented above all focus on computing in-dividual counts of points  X  i.e. Computational Task 1, from Sec. 2.2. This approach improves the overall dependence on the number of data points  X  N  X  and the order of the cor-relation  X  n . However, this does nothing for the other two parts of the overall computational complexity. We now turn to our novel algorithm to count tuples for many matchers simultaneously, thus addressing Computational Task 2.
Intuitively, computing counts for multiple matchers will repeat many calculations. For simplicity, consider the two-point correlation case illustrated in Fig. 3(a). We must count the number of pairs that satisfy two matchers, r 1 and r 2 (assume that the upper and lower bounds for each are very Algorithm 3 TestPointTuple (points p 1 ,...,p n ,matcher r ) 10: if At least one permutation  X  is marked valid then Algorithm 4 TestNodeTuple (Tree node T 1 ,..., Tree node T n ,matcher r ) 10: if At least one permutation  X  is marked valid then close to the distances shown for simplicity). If we first per-form the computation for r 1 using the dual-tree algorithm (Alg. 1), at some point we will consider the pair of nodes in the figure. We compute the lower bound distance from their bounding boxes and see that we can prune. After this algorithm has finished, we again use Alg. 1, this time with matcher r 2 . We may again encounter the same pair of nodes, which requires us to compute the same distance bounds and make the same pruning decision. If we can consider the two matchers simultaneously, we can make the pruning decision for both and save the unnecessary work. We can also save work in the base case. We need only compute the distance between each pair of points once (if we have not been able to prune the pair completely). We can then immediately identify which matcher(s), if any, it satisfies.

Two-point case. In the two-point case, this improve-ment is straightforward, and has already been shown in [9]. We must only modify Alg. 3 and Alg. 4 to consider a collec-tion of matchers. If the matchers are regularly spaced, then we can identify which (if any) the node or point pair may satisfy in constant time. For a rbitrary matchers, we can sort them and find possible matching pairs in logarithmic time.
General case. The general n -point case requires more caution. The presence of permutations in Alg. 4 makes the straightforward approach mentioned above more difficult. Each pair of nodes in each permutation will possibly satisfy (a) Pruning for multiple different matchers. It becomes prohibitively expensive to determine which, if any, matchers may be satisfied. Instead, we can avoid searching all permutations entirely. We make use of the following observation, proven below: if the distance bounds violate a matcher in a particular order, they will violate it in all possible permutations.
Theorem 3.1. Given a matcher r ,let be the set of upper bounds r ( u ) ij , in the matcher, sorted in ascending order. Let bound distances d min ij obtained from the bounding boxes of n tree nodes, also sorted in ascending order. Then, if d l i for any i ,no n -tuple of points from the nodes can satisfy the matcher.

Proof. Any permutation  X  of the bounding boxes as-signs each lower-bound distance d l i to some upper bound distance in the matcher u  X  ( i ) . In order for this permutation to work, we need that d l i &lt;u  X  ( i ) for all i .Let i be the index such that d l i &gt;u i in the condition of the theorem. Then, any permutation must map i to an index j .If j&lt;i , then d l i &gt;u j = u  X  ( i ) .If j&gt;i ,then  X  ( j ) &lt;i .There-fore, d l j &gt;u  X  ( j ) . Therefore, no permutation can satisfy the matcher.
 We can easily arrive at a similar conclusion regarding upper bounds.

Multi-matcher pruning. Using these observations, we can improve the pruning rule in Alg. 4. With each n -tuple of nodes, we store the upper and lower bound distances, sorted in ascending order. We can compare these with the sorted distances in the matcher, using the observations above. If any lower bound distance is greater than a corresponding maximum matcher distance, we prune. When we recurse, we compute the n  X  1 new upper and lower bounds on node distances and update the sorted lists.

The total running time for pruning checks is therefore re-duced from something proportional to n !toasingleloopof size sorted lists of bounds. Unfortunately, for common values of n (2 and 3), this is not a significant advantage. How-ever, this approach makes a much greater difference in the multi-matcher version of the algorithm.

Specifying multiple matchers. We specify a set of matchers r m by giving a minimum and maximum range, r Algorithm 5 MultiTestNodeTuple (Tree node T 1 ,..., Tree node T n , matcher minima r min i , matcher maxima r 5: end if ber of equally-sized bins b ij for each dimension. Each pos-sible choice of bins, one from each dimension, then forms a single matcher. The total set of matchers consists of matchers, where the product runs over all dimensions of the matcher.

We can then provide a multi-matcher version of Alg. 4. We sort the minimum and maximum matcher ranges in ascend-ing order. We can then compare these against the bounds from the nodes X  bounding boxes.

We extend Alg. 3 by marking a permutation invalid if the point-point distance is greater than the maximum matcher distance or less than the minimum. If the distance falls in between, we can identify which bin contains it in constant time. We store a vector of possible bins for each permuta-tion, and if a permutation has a valid bin for each dimension, we can increment our count for that dimension. Note that we still only count each tuple at most once.

Alternative matcher specifications. Our specifica-tion of multiple matchers may seem somewhat restrictive. On the one hand, our framework allows for a wide range of possible configurations by allowing each dimension to be specified separately. However, we have assumed that no bins overlap and that the thickness of each bin is fixed. Neither of these restrictions are fundamental to our method. We are currently developing an extension which will allow overlap-ping bins and varying thicknesses, such as commonly occurs in three point computations. This modification requires only limited extensions to the pruning rule and base case.
We now turn to our second new algorithm  X  a method to efficiently compute counts for resampling regions (Compu-tational Task 3). Recall from Sec. 2.2 that we are given a partitioning of the data set into subsets D i .Wethencon-struct i data sets D (  X  i ) = { i  X  D | i  X  D i } . We now compute counts which satisfy the matcher for each set D (  X  i ) . Assum-ing that the subregions each contain roughly the same num-ber of points, we see that for J subregions and an n-point estimation algorithm which requires T ( N )timefor N data points, we require O ( JT ( N  X  N/J )) time for each matcher. Instead, we can rearrange this computation to share work between different subcomputations.

Redundant work. The intuition for our new approach is shown in Fig. 3(b). Assume we are computing a count of the form DDD ( r ) and that the three points shown in the figure satisfy the matcher. For all but the three regions in which the tuple X  X  points lie, we will need to do the work to find and count the tuple since it factors into the total DDD count for all the D (  X  i ) . Therefore, we will perform all the work needed to find this tuple J  X  3times.

New algorithmic strategy. We can avoid this extra work by working with the subsets D i directly. For instance, Algorithm 6 EfficientResampling (Data sets D i :1  X  i  X  J ,matcher r ) 5: results[j] += result if we compute the count D i D i D i ( r ), this result will appear in this count once and add its intermediate result into the J 1 final results. In general, we consider n distinct sets D compute the number of tuples from them that satisfy the matcher, and add that intermediate count into the result for each D (  X  j ) that does not appear in the computation. We show this approach in Alg. 6. We maintain an array of results of length J ,wherethe j th entry corresponds to the count D (  X  j )  X  X  X  D (  X  j ) ( r ). This method can be easily extended to include random sets by including the random set as one of the D i in the input to Alg. 6.

For J resampling regions, each containing roughly N/J points, this algorithm requires O ( T ( N ) is the time required for a single n -point correlation computation on N points. The naive version loops over all resampling regions, so it requires O ( J  X  T ( N )) time. While the combinatorial dependence on n may seem to be a dis-advantage, it is easily made up for by the reduced problem size for each n -point computation. As we will show in our experimental results, this method provides a considerable speedup.

We have presented two new algorithms which, along with the previous multi-tree algorithm [9, 18], address the entire computational problem of estimating n -point correlations. Our two algorithms can be used in conjunction for maximum speedups. We simply provide the set of matchers described in Sec. 3.2 to the efficient resampling algorithm (Alg. 6). We now turn to some empirical results for our new algorithms.
Algorithms. We have presented two new algorithms for n -point correlation estimation. We show results with our new efficient resampling algorithm (Alg. 6) and with a naive resampling ( O ( J  X  T ( N ))) approach, which simply loops over resampling regions. We also show results with our multi-matcher method and a naive-matcher O ( M  X  T ( N )) algo-rithm which simply loops over matchers and calls Alg. 2. We therefore present results for each new method alone, and in combination. We also show results for the original multi-tree algorithm [18], which we call the single-matcher, naive-resampling algorithm. We have also estimated runtimes for the brute-force O ( J  X  M  X  N n ) algorithm.

In our charts and tables, we use the following labels for these algorithms: Table 1: Mock catalog experimental results for 2-point correlations, 10 6 points. 30 resampling regions, 20 matchers.
 Table 2: Mock catalog experimental results for 3-point correlations, 10 6 points. 12 resampling regions, 18 matchers. Speedups are over the brute-force al-gorithm. The naive-naive algorithm was omitted be-cause of time constraints.

Implementation details. We implemented our algo-rithms in C++. We used the kd -tree implementation in the MLPACK scalable machine learning library [4]. All experi-ments were performed in serial on a 6 core, AMD Phenom 3.3 GHz machine.

Datasets. We provide experimental results on uniform (Poisson data) in three dimensions and a mock galaxy cat-alog generated from N -body simulations. Both these sets are representative of the data used in actual n -point corre-lation estimation. Galaxy catalogs reproduce the correlation statistics of observed data. Computations involving Poisson distributed sets, such as those in the estimators in Section 2.1, represent a significant fraction of the total computa-tional overhead.

Results. In Fig. 4, we show runtimes for all four algo-rithms on uniform data of different sizes. The numbers of matchersandresamplingsubsetsareshownwitheachfigure. Note that for all three figures, each of our new methods pro-vides a large speedup, shown in Table 3. We see that in our experiments, each of our new algorithms provides roughly an order-of-magnitude speedup. These effects stack when used together, so we see speedups of we ll over 100.
The efficient resampling algorithm provides a greater ad-vantage than the multi-matcher method in our experiments. However, this effect is less pronounced in our 4-point ex-periments (Fig. 4(c)). These experiments used more match-ers than the others, suggesting that the savings due to the multi-matcher algorithm will increase with more matchers.
We have described the first fast algorithms for the en-tire n -point correlation function estimation problem. Both of these new algorithms build on the current state-of-the-art tree-based approach by identifying computational bot-tlenecks in the entire npcf estimation problem. One of our Table 3: Speedups over the brute-force algorithm and current state-of-the-art. Timings are from the largest values of N in Fig. 4. methods computes the counts necessary for estimating the correlations at multiple scales in a single pass. The other ef-ficiently incorporates the jackknife resampling step required to estimate the variance. We have shown that each of these methods can provide an order-of-magnitude speedup over the current best algorithms and that they can be used in conjunction for speedups up to 500 or greater.

We are currently developing two extensions of this ap-proach. First, we are implementing another, more general multi-matcher algorithm which will be capable of handling overlapping bins. Second, we are working on a parallel im-plementation of our approach for the largest data sets. [1] J. Bardeen et al. The statistics of peaks of gaussian [2] L. Cooper et al. Two-point correlation as a feature for [3] N. Cressie. Statistics for Spatial Data . John Wiley &amp; [4] R.R.Curtinetal.MLPACK:AScalableC++ [5] J. H. Friedman et al. An Algorithm for Finding Best [6] J. Fry. The galaxy correlation hierarchy in [7] J. Gardner et al. A framework for analyzing massive [8] T. Giannantonio et al. High redshift detection of the [9] A. G. Gray and A. W. Moore.  X  N -Body X  problems in [10] A. Hamilton. Toward better ways to measure the [11] INBio. The National Biodiversity Institute of Costa [12] S. Landy and A. Szalay. Bias and variance of angular [13] D. Li et al. 3D reconstruction of carbon nanotube [14] LSST. The Large Synoptic Survey Telescope . [15] R. Lupton et al. The SDSS imaging pipelines. In [16] C. McBride. Our Non-Gaussian Universe: Higher [17] C. McBride et al. Three-point correlation functions of [18] A. Moore et al. Fast algorithms and efficient statistics: [19] K. Mosaliganti et al. Tensor classification of n -point [20] P. Peebles. The Large-Scale Structure of the Universe . [21] F. P. Preparata and M. I. Shamos. Computational [22] R. Ridgway et al. Image segmentation with [23] B. Ripley. Locally finite random sets: Foundations for [24] B. Ripley. Spatial statistics . Wiley-Blackwell, 2004. [25] SDSS. The Sloan Digital Sky Survey . www.sdss.org . [26] C. Seife. Breakthrough of the year: Illuminating the [27] I. Szapudi. Introduction to higher order spatial [28] I. Szapudi et al. Fast cosmic microwave background [29] I. Szapudi and A. Szalay. A new class of estimators for [30] S. Torquato. Random Heterogeneous Materials . [31] S. Torquato and G. Stell. Microstructure of two-phase [32] S. White. The hierarchy of correlation functions and Figure 4: n -point correlation estimation computa-tion times on log-log scale. All timings are in sec-onds.
