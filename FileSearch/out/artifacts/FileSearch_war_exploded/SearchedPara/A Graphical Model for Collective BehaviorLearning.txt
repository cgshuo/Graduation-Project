 Collective intelligence is a shared or group intelligence that emerges from the in-teractions (both collaborative and competitive) of many individuals and appears in consensus decision making of agents. Collective behaviors can be modeled by agent-based games where each individual agent follows its own local rules. Agent-based experimental games have attracte d much attention in different research areas, such as psychology [1], economics [2,3], financial market modeling [4,5] and market mechanism designs [6,7]. Agent-based models (ABM) of complex adaptive systems (CAS) provide invaluable insight into the highly non-trivial collective behavior of a population of com peting agents. These systems are uni-versal and researchers aim to model the systems where involving agents with similar capability are competing for a limited resource. Agents may share global information and learn from past exper ience. In such a complex system, if we assume that every agent in the market knows the history data, the key problem is how to decide to act based on this global information.

The minority game [8] is a simple ABM originating from the El-Farol Bar [9] problem. An odd number of agents compete with each other to be in the minority by making one of two choices; the players who end up in the minority side win the game. The collective recogniz able patterns generated by the minority game are due to the interaction of many individual agents. There are two main features for the minority game: first, the minority rule, which makes complete steady state in the population impossible and secondly, every agent has its own way of perceiving the available global information about the game and using it into its own strategy. Each agent is aware of the global information and can use this information to make decisions based on its own unique strategy, as it is unrealistic that all agents follow the same deterministic strategy [10]. Due to its simplicity and attractive properties, the minority game model has attracted muchattentionindifferentr esearch communities [11].

In [12] the authors purpose that coll ective behaviors of MG can be decom-posed into several micro-level behaviors from different group of agents and they use Genetic Algorithm (GA) to optimize the agent behavior parameters in order to get the best guess of the original system. Also, different game theory model can be used to model the individual beh avior. More accurate a model is for individual behavior, more accurate collective behav ior we can obtain [13]. Un-certainty is present in all real-world s cenarios, especially when it comes to the task of predicting the outcomes of aggre gated actions. Probabilistic graphical models (PGM) [14] have the capability to deal with uncertainty by incorporat-ing prior beliefs about the domain and updating these beliefs as new evidence is obtained. Using PGMs we can construct richly structured models to understand hidden relations.

In this paper, we propose a novel generative probabilistic graphical model for modeling the process of generation of the collective behavior from the individual behavior. We use the proposed PGM to infer the behavior of individual agents from the available global information and use the learned agent behavior to pre-dict the future collective behavior. The main contribution of our work is four fold: (1) We use a game theory model, the minority game, to model individual behavior. It also has the flexibility of using any appropriate game theories to model individual behavior; (2) A PGM is used to model generative process of collective data from a group of individual actions. We can infer the individual strategy from the observable collective data, and this can be used for predict-ing future collective data; (3) We use this novel framework to show how it can be applied to time-series data mining of financial market data; (4) Comprehen-sive experiments on real world stock market data and foreign exchange rates demonstrate the effectiveness of the new model. In a MG, there are an odd number of players and each player must independently choose one of two options at each round of the game, the players who end up at the minority side are winners and the choice of the minority players is referred to as the winning choice r . There is no prior communication among players; the only global information available is numbers of players corresponding to the two choices from the previous rounds. 2.1 Terminology We begin by first introducing the notation and the terminology used in this paper: Agent : A player of the game is referred to as an  X  X gent X  and it is the entity that makes decisions based on its  X  X trategy X . The number of agents that participate in the MG is N , which is an odd number. Agent is indexed by an integer A where A  X  X  1 , 2 ,...,N } .
 Choice : An action made by an agent. Choice C has two possible values: C  X  X  0 , 1 } . The total number of choices are N .

Game : Every run of the MG will be referred to as a  X  X ame X . In every game the choices are represented as a vector of N elements of binary value { 0 , 1 } .It can also be viewed as a sequence of choices C 1 ,C 2 ,...,C N where C n is the n th agent X  X  choice in the game. The total number of games is denoted by G .
Minority Choice : For every game the choice of the agents on the minority side is the winning outcome and is called the  X  X inority choice X . Formally, let t denote the current game, then the minority choice in game t is defined by:
Memory and History : In the minority game we assume that the agent X  X  actions are governed by its strategy and previous minority choices of the game. If the agents have an m -bit memory which means that the agent will take into the information (minority choices) in the previous m rounds. The minority choice for the last m round of games is defined as the  X  X istory X  H  X  X  1 , 2 , 3 ,...,K } and K =2 m ,where K is the maximum value of history. History is normally a binary string of the past m minority choices, but without loss of generality, for the representational convenience of our model we have defined history as a decimal number. For example if the agents have a 3-bit memory then the history belongs to the set H = { 1 , 2 , 3 ,..., 8 } .

Strategy : We assume that each agent X  X  action is governed by a strategy which can be regarded as a set of rules or functions taking the previous minority choices as inputs. Given a history, the agent makes its decision based on its own predefined rules named  X  X trategy X  S [8,15,16]. The strategy is a mapping from each possible m -bit memory(each possible history) to a corresponding choice of making choice-0 or choice-1. Therefore, there are 2 2 m possible strategies in the strategy space and we assume that at one time each agent has exactly one strategy. A strategy can be regarded as a particular set of decisions on all the permutations of previous history of minority choices. Fo r example, the first 3 rows of Table 1 show the 3-bit memory, history and a sample strategy.
Probabilistic Strategy : For each agent,  X  X robabilistic strategy (PS) X  is a strategy that maps the history to a probability distribution over the two choices Memory (000) 2 (001) 2 (010) 2 (011) 2 (100) 2 (101) 2 (110) 2 (111) 2 History 1 2 3 4 5 6 7 8 Strategy 1 0 1 1 0 0 0 1 instead of mapping directly to one choice only. We use a Bernoulli distribution to represent probabilistic strategy. The bottom row of Table 1 shows a sample PS where P B ( p ) represents a Bernoulli distribution, p is the probability of making choice-0 and q =1  X  p is the probability of making choice-1.The advantage of using such a strategy is that it is able to incorporate uncertainty in the learning process of the PS via PGMs, because for the next game this PS provide a prior to the PGM. 2.2 Probabilistic Graphical Model for Collective Behavior Learning Previous works show that collective b ehavior can be decomposed into the ag-gregation of individual agents X  actions [12,13]. In this paper we assume that the collective behavior is generated by agents with probabilistic strategies. Our task is two fold: first, to decompose the collective behavior by inferring individual agent behaviors, and the second, to use these learned individual behaviors to predict the future collective behavior. PGMs provide us the ability to deal with uncertainty and incorporate prior knowledge. Moreover, the proposed PGM will provide a unified framework for both the inference of individual behaviors and the prediction of the global behavior.

With this premise,the motivation behind our purposed PGM is to model the procedure of an agent making a choice. We first start by drawing a distribution over agents from a Dirichlet prior, then we randomly select an agent from this agent distribution to make a choice. After selecting this agent we observe the history for the present game. Then the agent X  X  choice, corresponding to the observed history, is sampled from its PS. This is repeated N times to generate all the choices of a game. Formally this generative process can be outlined as:  X  For each agent and each history:  X  Observe the history H for the current game.  X  For each choice where  X  and  X  are scalars that parameterize the symmetric prior Dirichlet dis-tributions, Dir 2 (  X  ) denotes a 2-dimensional Dirichlet with the scalar parameter  X  and Dir N (  X  ) denotes an N dimensional Dirichlet parameterized by  X  .Asym-metric Dirichlet is a Dirichlet distribution where every component of the param-eter is equal to the same scalar value. The Dirichlet distribution is a distribution over discrete distributions and it is conjugate to the Multinomial distribution; each component in the sampled random vector is the probability of drawing the item associated with that component. Mult ( . ) denotes a discrete Multinomial distribution.

The  X  X hoice distribution X   X  n,k is a 2-dimensional random vector that corre-sponds to the probability of making choice-0 or choice-1 for agent n and history k . For a single agent n , the set of these distributions for all values of history corresponds to that agent X  X  PS i.e. {  X  n,m =1 ,...,K } is the PS of agent n . The dis-tribution  X  n is the  X  X gent distribution X  and is a N -dimensional random vector where each component gives the probability of selecting the agent index associ-ated with that component. The correspo nding directed graphical model is shown in Figure 1. It is worth mentioning here that the choices that are observed are unordered choices: the N choices for every game are provided as a string of 0 X  X  followed by 1 X  X . If the actual choices made by the N =5 agents were [1,0,1,0,0] in one game and [0,1,1,0,0] in the other game then in both cases the observed choices would be [0,0,0,1,1]. These unordered choices are referred to as choices throughout the paper.

The joint distribution corresponding to the PGM in Figure 1 is: Notice that since H is observed and does not depend on another variable. It is a deterministic variable and p ( H ) can be omitted from the joint of Eq. (2). The model specifies a number of dependencies between random variables: the agent index A n depends on agent distribution  X  n , the choices C n depends on the agent index A n ,history H and all of the choice distributions  X  1: N, 1: K .Herethe of the agent denoted by A n we find the choice distribution corresponding to the observed history H . From Figure 1 we can also see that once choices are ob-served the agent distributions  X  n and choice distributions  X  n,k are conditionally dependent according to d -separation. In generative probabilistic modeling, we treat our data as arising from a genera-tive process that includes hidden and observed variables. This generative process defines a joint probability distribution over both the observed and latent random variables. We perform data analysis by using that joint distribution to compute the conditional distribution of the latent variables given the observed variables. The decomposition of collective behavior to individual behavior corresponds to observing the choices and history and inferring the posterior distribution of the hidden variables. The choices and the history are the global behavior and the posterior of the agent distributions p (  X  1: N | C 1: N ,H ) and choice distributions tion can be written as: where the numerator of Eq. (3) is defined in Eq. (2). In order to normalize this posterior distribution we need to marginalize over the latent variables to give the denominator as shown in Eq. (4):
Posterior inference of our model is done using approximate message passing algorithm [17], specifically we are using Variational Message Passing algorithm (VMP) [18] in the Infer.Net [19] package. VMP is deterministic approximate inference algorithm that is guaranteed to converge to some solutions and it works by using only local message passing operations. Our goal is to infer the individual behaviors, then use these individual behaviors to predict the choices for the next game and calculate our accuracy of prediction. This procedure is explained in the Algorithm 1. Here C n ( t + 1) denotes the actual c hoices of the next game. PC n ( t +1) denote predicted choices of the n ext game and the predicted minority choice of the next game  X  r ( t + 1) is defined as: Algorithm 1. Model Inference and Prediction Then the prediction accuracy Acc ( t ) after observing the game t is defined as: where the #( . ) is an incremental counter, initialized with 0 at t = 1, that incre-ments by 1 each time its argument ( . )istrue. 4.1 Test on Artificial Data To test the validity of our model we performed experiments on an artificial dataset generated according to the assumptions of the MG and used our model to learn the individual behavior and then make predictions. To make our ex-periment more realistic we assumed that some agents follow a random strategy because in the real-world there are always certain trends in the data that can-not be either modeled or captured. The random agent makes random choices between 0 and 1 following a uniform distribution. We further assume that some agents are adaptive agents and they might divert from their original strategy during the experiment. Every adaptive agent maintains its loosing probability for every history; if this loosing probability is greater than a threshold (0.6 in our experiments) then the agent changes its strategy (by random picking another strategy from the strategy space, e.g. [10]). Moreover, the change in strategy can only occur after every 200 games. We set  X  =0 . 25,  X  =0 . 25, m =3, N =31 and use our framework of Algorithm 1 to make predictions on this data. Figure 2 shows the prediction accuracy and error bars for 1000 games with 31 agents, 10% random and 20% adaptive agents, along with the prediction accuracy for the case of 10% random and no adaptive agents. To obtain averaged results, the experiment is repeated 10 times. Str ategies of adaptive agents change every 200 games resulting in a dip in prediction accuracy and after 1000 games the accuracy is around 67% with adaptive agents and 84% without adaptive agents. 4.2 Experiments on Real-World Market Data The minority game is related to many real-world complex scenarios [5,20,8] in-cluding financial markets. In the following experiments, stock market index data and the foreign exchange (FX) rate of U.S. Dollar (USD) against Renminbi (RMB) and Japanese Yen (JPY) against RMB are tested (Table 2). On the macro-level, the global behavior of these real-world market data appear ran-dom and unpredictable, but in our experiments we assume the global behavior of these markets as being generated a ccording to the minority game and then use the PGM to infer local behaviors to predict possible future trends of these markets. This framework provides a new way of understanding the relationship between macro-trends and micro-trends; the combination of these individual be-havior have the potential of generating very complex and apparently random global behaviors.

In our experiments we predict whether a stock market index or FX rate will rise or fall the next day. Thus, rise and fall are the two values corresponding to the two possible minority choices (choice-0 and choice-1) and one trading day corresponds to one game. Formally, for a stock market index (or a FX rate), let the opening price of that trading day be denoted by P o ( t ) and the closing price for that trading day be denoted by P c ( t ), then the stock market fluctuation for that day can be encoded to the minority choice by:
The proposed PGM has two observed variables for the history and choices, respectively. To obtain the choices from the market data, we first need to give an appropriate number of agents. Given the market data, the number of agents making choice-1, denoted by # C 1 , is obtained according to: Based on the training data, the value of # C 1 is scaled appropriately. The quan-tity P o ( t )  X  P c ( t ) is not always positive and is roughly centered around zero, so we shift it appropriately to make # C 1 non-negative. This shifting factor is greater than the magnitude of its minimum or maximum value and it becomes thenewmean.Wethenset N equal to twice the shifting factor. Then the choices are obtained by forming a string of [ N  X  # C 1 ] zeros followed by # C 1 ones. 4.3 Experimental Results The parameters for the PGM used in ou r experiments are the following: the hyper-parameters  X  and  X  governing the prior Dircihlet distribution of agent distributions  X  1: N and of choice distributions  X  1: N, 1: K , respectively, were both set to 0.25 as from our multiple experiments we found that these values work well for most real markets, moreover, the history was assumed to be generated by agents having a memory length of 3 i.e. m =3.
 In order to get stable and unbiased results we ran the algorithm (described in Algorithm 1) 10 times for each data set and then calculated the corresponding error bars. We tested the proposed PGM on 6 datasets, 4 are from real sock markets and 2 datasets are for RMB exchange rate (shown in Table 2). All the data was downloadable by searching for each company by its stock index 1 and the exchange rate data is also available online 2 . Figure 3 shows the prediction accuracy defined in Eq. (6) for each of the data sets for 1000 games. It also shows another dashed curve for each dataset that corresponds to random prediction, based on discrete uniform distribution over [0 , 1], for the future trends of the real datasets. This dashed curve represents the base line as it corresponds to no learning and just randomly predicting  X  r ( t +1)  X  U (0 , 1). Therefore as long as our prediction accuracy can be above this curve we can consider that our proposed inference and prediction technique has learned some local behaviors that can predict the future trends with more than random accuracy.

The accuracy of USD-RMB exchange rate after 1000 games is around 60.78% whichishighcomparedtotheaccuracyob tained for other real-world data sets. This may suggest that this exchange rate data has some prominent patterns and these findings are consistent with the findings of [12]. Conversely, for JPY the exchange rate against RMB for the same time period was analyzed and the result show that our algorithm does not perform much better than random prediction. In fact for the first 700 games the random prediction works better than our proposed algorithm, however after that our algorithm performs slightly better with an accuracy of 51.28% after 1000 games. For the other markets the accuracy is between 52% and 58%. T he prediction accuracy on Shandong Bohui Paper Industrial Co. and CITIC Securities Co. datasets is above random prediction but for China Minsheng Banking Co. random prediction performs better for the first 350 games, after that the proposed algorithm performs better on average although the the error bars overlap suggesting that on some occasions the performance of the proposed technique is comparable to random prediction. The prediction accuracy for Kweichow Moutai increases to around 60% in the first 130 games after which it drops indicating a change in the trend for this market, then after game 280 the accuracy begins to increase again indicating another major change in the trend, but the increase in accuracy suggests that the new trend is similar to the trend p reviously observed by the model.
Table 2 provides the details of the dat a sets we used in experiments and the number of agents that were set for each data set based on Eq. (8). For our simulations we used C# along with Matlab on a 32-bit computer with 3GB of RAM and two 2.93GHz processors. And for one iteration of Algorithm 1 it takes around 1.1 sec if the number of agents N is 31. Therefore the total duration for 10 iterations of Algorith 1 for 1000 games is around 3 hours. In this paper we modeled the process of gen eration of the collective behavior of the minority game with a PGM and showed that we can use the proposed PGM to decompose the collective behavior by inferring individual agent behavior and then use them to predict the future trends of real world market data. We first performed experiments on artificial data to validate our model and then tested it on the real-world market data. Although finding patterns of real-world market data has always been a controversial topi c as it violates the efficient-market hy-pothesis (EMH) [21], however, based on our empirical studies, we indeed found statistical significant patterns by training the new proposed model on history data. Especially for the USD-RMB exchange rate, we present quantitative ev-idence that there are some stronger patterns comparing to other FX rate and stock index. Our future work will focus on applying the framework of Bayesian learning to learn the hyper-parameters  X  and  X  instead of setting them by ex-perimental evaluation and also to test our framework on more real-world data but not limited to stock market indexes and FX rates.
 Acknowledgements. This research is funded by the NSFC under the Grant No. 61305047.

