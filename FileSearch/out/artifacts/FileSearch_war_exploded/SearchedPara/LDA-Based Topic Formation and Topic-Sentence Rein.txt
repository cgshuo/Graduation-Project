 Document summarization requires producing a short summary for one or more docu-ments that conveys the most important information in the original document(s). With the explosive increase of documents on the Internet, document summarization has proved to be an essential task for accessi ng the overloaded information. Generally, document summarization approaches can be divided into abstractive or extractive. Abstractive summarization approaches usually rewrite the original documents to generate a concise abstract that requires sophisticated nature language generation techniques. In contrast, extractive summari zation approaches just select a number of indicative text fragments from the original documents to form a summary and thus are much easier to implement. Up to the present, most summarization approaches are still built upon the extractive summarization framework in which sentence ranking is the issue of most concern. 
Various sentence ranking approaches are proposed for extractive summarization through the years. More recently, the application of graph-based ranking algorithms has attracted much concern. The basic assumption of graph-based sentence ranking approaches is that the importance scores of two textual units (such as words, sen-tences or documents) are related to the degree of their relevance. Many graph-based ranking algorithms are examined in the co ntext of document summarization, includ-ing the HITS-style mutual reinforcement algorithms, the PageRank-style random walk algorithms and their variations. 
As for graph-based sentence ranking approaches, two pivotal issues are the defini-tions of the nodes and edges of the text graph. In most existing approaches, nodes are usually defined by words, sentences, documents etc. Edges are then defined by the relationship between these textual units. Though various graph-based ranking algo-rithms based on different textual units are well studied, the definitions of the nodes in most studies are confined to these original textual units in the documents. To over-come this limitation, some recent studies focus on incorporating high-level informa-tion beyond the original textual units into the ranking algorithms. Typically, they adopt clustering algorithms to achieve the objective. The sentences in the given doc-uments are first clustered into sentence clusters that are regarded as meaningful top-ics. The sentence clusters are then integrated into the text graph as extra nodes. Based on this, cluster-based graph models are developed to rank the sentences through a mutual reinforcement process between clusters and sentences, using the cosine simi-larity metric to construct the edges between sentences and clusters. What we argue here is that it may not be very appropriate to define and manage the topics in this way. sentence is assumed to belong to one cluster only. However, the mutual reinforcement process needs to formulate the relevance between every sentence and every cluster, cluster. Therefore, a conflict exists here indeed and it makes the cluster-based rein-forcement algorithm less reasonable. 
In view of this, we try to define topics in another way so that its definition can con-form to the hypothesis of the reinforcement scheme. A powerful topic model, namely the Latent Dirichlet Allocation (LDA), is adopted in our study to form the topics with such characteristics. For the documents to be summarized, the LDA model is first estimated on them to discover a set of LDA topics. Then the estimated model is in-graph is constructed according to the obtained statistics and a reinforcement algorithm is developed to calculate the sentence ranking scores. We believe that LDA is more suitable for topic detection than clustering fo r the following reasons. First of all, each topic in LDA is defined as a distribution over the words and each sentence is viewed as a mixture of the topics. Therefore, the LDA-based topics can well satisfy the rein-forcement hypothesis that a sentence is supposed to be related to more than one topic. under the LDA model, which are well consistent with the definition of the nodes. 
The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 provides a brief introduction of LDA-based topic formation and Sec-tion 4 details the topic-sentence reinforcement ranking algorithm. Section 5 presents the experimental results. Sectio n 6 concludes the whole paper. Document summarization is to produce a short summary for one or more documents which can convey the most important information in the original documents. Techni-cally, document summarization can be divided into abstractive and extractive. The abstractive technique involves paraphrasing sections of the source documents; while the extractive technique just selects indicative text fragments from the original docu-ments to generate the summary. Up to now, most summarization approaches are still built on the extractive approach which is easy to implement, and we focus on the extractive method as well in this paper. 
The most essential issue in extractive method is the ranking of text fragment. The centroid-based MEAD, implemented by [15], ranks sentences by features like sen-tence length, cluster centroids, position in contexts. In NeATS [12], they add theme information in sentence ranking. Recently graph-based summarization approaches are mainly inspirited by the analysis of the link structure of the World Wide Web, such as Google X  X  PageRank [9] and Kleinberg X  X  HITs [10]. In [8], Zha considered the mutual reinforcement principle that a term which appears in many salient sentences should have a high saliency score and so does a sentence which contains many salient words. Based on this principle, a bipartite graph is built upon the pair-wise relevance be-tween each sentence and each word and then a HITS-style ranking algorithm is ap-plied to calculate the saliency scores of both sentences and words through a mutual reinforcement process. Later, other graph-based models are also investigated for the sentence ranking problem. Erkan and Radev [11] developed an undirected graph to represent the documents by regarding sentences as vertices and calculating the cosine similarity between sentences for establishing the links. The saliency scores of the sentences are then calculated by an algorithm called LexRank, which is derived from PageRank. TextRank [13, 14] is another representative approach motivated by Page-Rank that shares similar ideas. Later on, Otterbacher et al. [6] adopted a weighted undirected graphs to represent the given docum ent sets by the cosine similarity. Then, a PageRank-style random walk model with edge weights is proposed to calculate sentence ranking scores based on the weighted graph. Besides the above works that mainly focus on generic summarization, graph-based ranking algorithms are also employed in query-oriented summarization wh ich is the leading research topic in the area recently. The usage of clusters in graph-based ranking models was introduced by Wan and garded as the meaningful topics for the given document set. These sentence clusters are then used as the extra nodes in the document graph. It is reported that the cluster-level information is able to improve the sentence ranking result through a mutual reinforcement process between clusters and sentences. In order to better combine the clustering process and the ranking process, Ca i et al. [16] conducted a further research in which a reinforcement approach that tightly integrates the graph-based ranking model and the clustering scheme was proposed. Latent Dirichlet Allocation (LDA) [1] is a generative model that allows sets of obser-vations to be explained by unobserved groups which explain why some parts of the { w each topic z k is a distribution over the word vocabulary W . Two kinds of distributions, p ( W | z k ), are both modeled by multinomial distributions with Dirichlet priors. Different from the well-known bag-of-word model, words in LDA are assumed independent of P ( z k ). Compared to other topic models such as pLSA, the use of the Dirichlet priors in LDA results in more reasonable topic mixtures. The smoothed generative model of LDA is illustrated in Fig. 1, in which  X  and  X  denote the parameters of the Dirichlet multinomial per-document topic distribution and the per-topic word distributions respectively. 
The LDA model is too complex to be solved by exact inference. In the original work by Blei, the Variational Bayesian method is used to approximate the posterior distribution for inference. Griffiths and Steyvers [4] propose an inference method based on Gibbs sampling that calculates the topics without explicitly representing the parameters. This method is competitive in speed and performance. In the method,  X  and  X  are estimated by sampling from the posterior distribution P ( Z | D ,  X  ,  X  ) which is calculated by hand, P ( D |  X  ,  X  ) is approximated by a Gibbs Sampling process. tions of  X  and  X  can be derived from the number of times a word w i has been assigned assigned to a topic z k (denoted by n ( d j | z k ) in the samples. Here we can write the esti-mations as and illustrate them in Fig. 2 below. 
Given the estimated topics Z , we also view a sentence in the document set as a mixture of the topics. Using the similar Gibbs sampling processes, the estimated LDA these probabilities to obtain the relations between sentences and topics in the bipartite graph, as detailed next. For a document set, we build a bipartite graph by (1) using the estimated LDA topics and the sentences as nodes; (2) using the per-topic distributions of the sentences and below. Formally, the constructed graph is denoted as G = ( V , E ), where the nodes set V = V
S  X  V Z , V S consists of nodes of sentences in S and V Z consists of the nodes of top-importance scores passed between nodes. Therefore, it is natural to use the probability other hand, the weights of the edges from topics to sentences cannot be simply de-fined by this probability. These weights reflect how much importance a topic should give to a sentence, or how dominative the sentence is to the topic. A sentence that is much dominated by the topic may still not be dominative since it may only consist of some less indicative words of the topic. Therefore, here we need to re-define the weighted edges from sentences to topics, different from the ones from topics to sen-tences. To calculate the weights, we assume that a sentence is supposed to be more dominative to a given topic if the words it contains are dominative to this topic. and the inferred per-topic distribution of sentences P ( Z | S ) just acts as the weights for and we use the word distribution P ( T | Z ) to calculate the average word probability of the sentence as the weight, i.e., where t and | S l | denote a word in the sentence S l and the total word number of l respec-approaches, the edges are defined by the cosi ne similarity metric and thus are symme-tric. However, the weighted edges defined in our graph are asymmetric. In fact, in the original HITS model for hyperlink analysis, the edges indicating the linking informa-tion between web pages are also asymmetric because the two edges with opposite front and tail ends indeed indicate different relations. Note that the asymmetric weighted edges do not affect the correctness of the mutual reinforcement algorithm. 
In the mutual reinforcement algorithm, the authority scores (denoted as Authori-ty( calculated. The scores in the ( n +1) th iteration are calculated based on the scores in the nth iteration, i.e.,  X  , respectively, the matrix form of the reinforcement process can be given as The above process continues until the convergence is achieved [10]. It can be proved  X  the authority matrix  X   X  X  X  X   X   X  X  X  X  . 
For the numerical calculation, we set the initial scores of all the sentences and the topics to 1 . After each iteration of reinforcement, both the hub vector V Z and the au-thority vector V S are normalized, i.e. The termination condition is then set as when the maximum gap between the sentence scores in two successive iterations is smaller than a pre-given threshold (say 0.00001 here), i.e., We conduct the experiments on a generic multi-document summarization data set from DUC 2004 [3]. The data set contains 45 document sets, with each set consisting of 10 newswire documents. According to the task definition, a summary is required for each document set and the length of the summary is strictly limited to 665 bytes. The ROUGE 1 [5] toolkit is used for evaluation. ROUGE is a widely recognized au-tomatic summarization evaluation method which evaluates system-generated summa-ries by matching them to reference summaries. In this study we report two common ROUGE scores, ROUGE-1 and ROUGE-2, which are based on Uni-gram match and Bi-gram match respectively. 
In pre-processing, stop-word removal and word stemming are performed by using an NLP toolkit GATE 2 . After the topic-sentence reinforcement process, all the sen-tences in a document set are ranked by their authority scores and successively ex-tracted into the summary until the length limit exceeds. The MMR algorithm [2] is adopted to control redundancy in the constructed summaries. For each sentence to be mary. The sentence is extracted only when it is considered not significantly overlap-ping any previously selected sentence. 
To demonstrate the advantage of LDA topics in the topic-sentence reinforcement scheme, we also implement two other graph-based approaches for comparison. Both of them use the proposed reinforcement algorithm to rank sentences and construct summaries, but they use different topic definitions when constructing the bipartite graph. The first one simply deems each original document as a single approximate topic and the other one adopts the clustering-based method as introduced in [7], re-garding a sentence cluster as a topic. Two clustering algorithms, K-means and Spec-tral Clustering, are implemented and examined in this experiment. In these approach-es, the weight of an edge connecting a document/cluster node and a sentence node is calculated by the cosine similarity between the document/cluster and the sentence. For fair comparisons, the MMR algorithm is also implemented on these systems. 
Table 1 shows the average ROUGE-1, ROUGE-2 scores along with the 95% con-fidential intervals over the 45 document sets for all the three approaches (denoted as LDA, Spectral, K-means, and Document re spectively). For the approaches, the topic number and cluster number are both set to the square root of the total sentence num-ber, an experimental value that follows the same strategy in [7]. 
As demonstrated, the clustering-based approaches and the LDA-based approach indeed outperform the approach that simply uses the original documents as unpro-cessed natural topics, which shows the benefit of introducing extra nodes into the graph-based sentence ranking model. Moreover, the LDA-based approach can even significantly outperform both clustering-based approaches. This clearly shows that LDA is better at discovering meaningful topics than clustering methods. This result again demonstrates the importance of an appropriate topic definition in topic-sentence reinforcement. As discussed in the introduction section, the assumption implied in the relevance assumption of the reinforcement algorithm. This conflict makes clusters less effective in improving sentence ranking results. 
LDA requires a pre-determined topic number K. Obviously, the estimated topics are dependent on K, and so does the subsequent graph-based ranking results. In the next experiments, we attempt to investigate the influence of K on the performance of the LDA-based approach in turn. Fig. 4 and Fig. 5 illustrate the ROUGE-1 and ROUGE-2 scores of the LDA-based approach with different Ks. 
As illustrated in the above figures, the topic number K indeed influences the per-formance of the LDA-based summarization method. First of all, the performance drops for very small Ks. Moreover, the performance curve shows obvious fluctuations when numbers on different granularities. For example, while using 50 topics has achieved a causes deterioration of the result since some rational topics may be wrongly split. The achieved, which can be viewed as a proper topic set on a more detailed level. When we continuously increase the topic number, similar situations may occur to the fluctuations of the performance. On the other hand, compared to the results of fixed topic numbers, the experimental strategy that uses the square root of sentence number shows its advan-tage and reasonability because the result of the LDA-based method reported in Table 1 is indeed better than the best result in Fig. 4 and Fig. 5. In the DUC 2004 data set, a document set only contains about 10 documents. Since LDA is a probabilistic model developed for a large scale corpus, the efficiency of LDA in our study may be affected by the data insufficiency problem. Therefore, we consider another strategy that uses sentence-level input to train the LDA model. Ac-from the sentence segmentation process. The other processes of the graph-based summarization method are just kept the same. Table 2 below provides the ROUGE-1 and ROUGE-2 scores of the LDA-based model with different inputs, and the topic number is set to the square root of sentence number. 
From the experimental results, we observe that the document-level input is indeed better than the sentence-level input for the LDA-based ranking algorithm. As men-tioned above, the insufficiency of documents is the main problem of the document-level input. On the other hand, the main problem of the sentence-level input is that the information provided by a sentence is not as complete and self-contained as by a doc-ument. As a matter of fact, two words that are not in the same sentence but in adjacent sentences may also be highly related. Because both methods have their advantages and disadvantages, we would like to study ways to integrate the advantages of the two strategies in our further studies. In this paper, we study the application of the LDA model to graph-based document summarization. Compared to the existing clustering-based approaches, LDA can discover more meaningful topics and thus can better score and rank the sentences for document summarization. Experimental results on the DUC 2004 data set clearly demonstrate the effectiveness of the proposed approach. In the future work, we would like to explore the role of LDA in graph-based summarization more extensively, in-cluding better topic estimation scheme and alternative uses of the graph-based ranking models. Acknowledgements. The work presented in this paper is supported by a Hong Kong RGC project (No. PolyU 5230/08E). 
