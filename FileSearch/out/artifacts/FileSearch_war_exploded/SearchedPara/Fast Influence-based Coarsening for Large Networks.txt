 Given a social network, can we quickly  X  X oom-out X  of the graph? Is there a smaller equivalent representation of the graph that preserves its propagation characteristics? Can we group nodes together based on their influence properties? These are important problems with applications to influence analysis, epidemiology and viral marketing applications. In this paper, we first formulate a novel Graph Coarsening Problem to find a succinct representation of any graph while preserving key characteristics for di  X  usion processes on that graph. We then provide a fast and e  X  ective near-linear-time (in nodes and edges) algorithm coarseNet for the same. Using extensive experiments on multiple real datasets, we demonstrate the quality and scalability of coarseNet , en-abling us to reduce the graph by 90% in some cases without much loss of information. Finally we also show how our method can help in diverse applications like influence maxi-mization and detecting patterns of propagation at the level of automatically created groups on real cascade data. H.2.8 [ Database Management ]: Database Applications X  Data Mining Graph Mining; Propagation; Di  X  usion; Coarsening
The unprecedented popularity of online social networking websites, such as Facebook, Google+, Flickr, and YouTube, has made it possible to analyze real social networks. Word of mouth marketing and viral marketing strategies have evolved to take advantage of this network structure by utilizing net-work e  X  ects. Similarly, understanding large-scale epidemio-logical datasets is important for designing e  X  ective propaga-tion models and containment policies for public health. The sheer size of today X  X  large social networks makes it challeng-ing to perform sophisticated network analysis.

Given a propagation graph, possibly learnt from cascade analysis, is it possible to get a smaller nearly di  X  usion-equivalent representation for it? Getting a smaller equiv-alent graph will help multiple algorithmic and data min-ing tasks like influence maximization, immunization, under-standing cascade data and data compression. In this paper, we study a novel graph coarsening problem with the aim of approximating a large social network by a much smaller graph that approximately preserves the network structure. Our primary goal is to find a compact representation of a large graph such that di  X  usion and propagation processes on the large graph can be studied by analyzing the smaller rep-resentation. Intuitively, most of the edges in a real network are relatively unimportant; hence we propose characterizing and  X  X ontracting X  precisely such edges in a graph to obtain a coarse representation.

The main contributions of this paper are: (a) Problem Formulation: We carefully formulate a novel (b) E cient Algorithms: We develop coarseNet , an ef-(c) Extensive Experiments: We show that coarseNet is
The rest of the paper is organized as follows: Section 2 gives related work and Section 3 briefly gives the notation and explains some technical preliminaries. Section 4 pro-vides a formal definition of the Graph Coarsening Problem that we introduce, while Section 5 presents our approach and solution. In Section 6, we show how our coarsening frame-work can be applied to solve influence maximization on large networks. Finally, Section 7 gives experimental results while we conclude in Section 8.
The idea of coarsening a network for some task is not new, and has been used extensively in the popular commu-nity detection techniques (METIS [21] and GRACLUS [9]): nevertheless, they use di  X  erent metrics for coarsening like cut-based, flow-based or heavy-edge matching-based condi-tions. In contrast we study di  X  usion-based metrics, and do not aim to find communities.

The related problem of graph sparsification has also been well studied in the theory community under the notion of  X  X panners X  [10]. A spanner is a sparse subgraph that main-tains the pairwise distances between all nodes within a mul-tiplicative or additive factor. Fung et al. [12] study the cut-sparsifier problem which asks for a sparse weighted sub-graph such that the weight of all cuts is maintained within a small multiplicative factor. Graph sparsification for influ-ence analysis has emerged as a new tool for analyzing large networks. Mathioudakis et al. [28] propose an algorithm to find the sparse backbone of an influence network. The ma-jor di  X  erence is that graph sparsification removes edges (so the nodes stay the same), while we coarsen and contract node-pairs to reduce the graph. Another line of very recent work [30] tries to learn influence models at community-scale, using groups supplied by graph-partitioning algorithms like METIS. Our work is related in the sense that we also aim to  X  X roup X  nodes, but not based on link-based communities, instead automatically based on nodes X  di  X  usion characteris-tics. In that sense we believe our work provides a comple-mentary viewpoint: learn models directly at the node level, and then try to group them appropriately automatically.
The rest of the related work can be categorized into Epi-demic Thresholds, Influence Maximization, Other Optimiza-tion problems, and General Information Di  X  usion. Epidemic Thresholds. The classical texts on epidemic models and analysis are May and Anderson [1] and Heth-cote [20]. Much research in virus propagation focuses on the so-called epidemic threshold, i.e. determining the conditions under which an epidemic will not break out. Widely-studied epidemiological models include homogeneous models [2, 29, 1] which assume that every individual has equal contact with others in the population. While earlier works [23, 31] fo-cus on some specific types of graph structure (e.g., random graphs, power-law graphs, etc), Chakrabarti et al. [6] and Ganesh et al. [13] found that, for the flu-like SIS model, the epidemic threshold for any arbitrary graph depends on the leading eigenvalue of the adjacency matrix of the graph. Prakash et al. [32] further extended the result to a broad class of epidemic models.
 Influence Maximization: The influence maximization prob-lem was introduced by Domingos and Richardson [34]. Kempe et al. [22] formulated it as a combinatorial optimization problem under the Independent Cascade Model, proved it is NP-Hard and gave a simple 1 1 /e approximation based on the submodularity of expected spread of a set of starting seeds. Numerous follow-up papers have looked at speeding-up the algorithm (e.g., [27, 16, 8, 24, 7]).
 Other Optimization Problems. Another related prob-lem is immunization, i.e, the problem of finding the best vertices for removal to stop an epidemic, with e  X  ective im-munization strategies for static and dynamic graphs [19, 38, 4]. Other such problems where we wish to select a subset of  X  important  X  vertices on graphs, include  X  X utbreak detec-tion X  [27] and finding most-likely starting points ( X  X ulprits X ) of epidemics [26, 33].
 General Information Di  X  usion. There is a lot of re-search interest in studying dynamic processes on large graphs, (a) blogs and propagations [18, 25, 22], (b) information cas-cades [3, 14, 17] and (c) marketing and product penetra-tion [35]. These dynamic processes are all closely related to virus propagation. General algorithms for information di  X  usion based optimization include [36]. Table 1 gives some of the notation.
 Symbol Definition and Description
A , B ,... matrices (bold upper case) ~ a, ~ b, . . . column vectors a j or a ( j ) j th element of vector a n number of vertices in the graphs m number of edges in the graphs  X  the reduction factor ~ u G , ~ v G Right and left first eigenvectors (for G ) IC Model The Independent Cascade Model
GCP Graph Coarsening Problem (see Defini-coarseNet Our algorithm for GCP IC Model. A social network is a directed, weighted graph G =( V, E, w ). Usually each vertex v 2 V represents an indi-vidual of the network and edges represent influence relation-ships between these individuals. The Independent Cascade (IC) model is a popular di  X  usion model used to model the way influence propagates along the edges of a social network. In this setting, a vertex v 2 V is called active if it has been influenced and inactive otherwise. Once an inactive vertex becomes active, it always stays active, i.e. we focus only on progressive models. Given a seed set S  X  V of initially active vertices, the Independent Cascade model proceeds in discrete time steps as follows. At time step t ,let S t denote the set of vertices activated at time t . Every vertex u 2 S is given a single chance to activate each currently inactive neighbor v with probability of success w ( u, v ) independently of all other interactions. If u succeeds, then v becomes ac-tive at time t + 1. This di  X  usion process continues until no more activations are possible. The influence spread of seed set S , denoted by ( S ), is the expected number of activated vertices at the end of the process.
Motivated by the fact that in any real network, most edges and vertices are not important (due to the heavily skewed degree distributions), we propose a graph coarsening prob-lem which involves pruning away precisely such edges (and vertices). We aim to coarsen the graph to obtain a much smaller representation which retains the di  X  usive proper-ties. We coarsen a graph by successively merging adjacent node pairs. We attempt to quickly find  X  X ood X  edges which have little e  X  ect on the network X  X  di  X  usive properties. At first glance, this seems impossible as the di  X  usive proper-ties of a graph are highly dependent on the connectivity of the vertices and edge weights. Further, determining which node pairs to merge and analyzing the e  X  ect of merging two nodes on di  X  usion are non-trivial. Informally, we study the following problem in this paper: Definition 4.1 (Informal Problem).
 Input: Weighted graph G =( V, E, w ) and a target fraction 0 &lt;  X  &lt; 1 Goal: Coarsen G by repeatedly merging adjacent node pairs to obtain a weighted graph H =( V 0 ,E 0 ,w 0 ) such that Role of Eigenvalues. In order to address the informal problem described above, we need a tractable way to charac-terize the di  X  usive properties of a network. Recent work [32] shows that for almost any propagation model (including the IC model), important di  X  usion characteristics (in particular the so-called epidemic threshold) of a graph (after removing self loops) are captured by the spectrum of the graph, specif-ically, by the first eigenvalue of the adjacency matrix. Thus it is natural to believe that if the first eigenvalue of the coars-ened graph H (its adjacency matrix) is close to that of the original graph G , then H indeed approximates G well. Al-though the work of [32] deals with undirected graphs, their findings are also applicable to strongly connected directed graphs.
 Merging node pairs. To explicitly formulate the problem in Definition 4.1, we also need to define what happens when a node pair is merged (i.e. an edge is contracted) in a weighted graph. More precisely, after merging neighboring vertices a and b to form a new node c , we need to determine the new edge weights of all incoming and outgoing edges of c . In order to maintain the di  X  usive properties of the network, we need to reweight the new edges appropriately. x e d b a
To see why this is crucial, consider Figure 1. Assume that the IC model is being run. Suppose we need to pick the two best seeds (i.e. two nodes with the maximum influence spread as defined in the previous Section) from the top 5-vertex chain. Further assume that the graph is undirected and each edge has the same weight =0 . 5. Clearly, vertices b and e are the best. If we merge vertices { a, b } , we get the bottom 4-vertex chain. To still match the original solution, we correspondingly want { c , e } to be the best seed-set in the new chain X  X ut if edge { d, c } remains the same weight, any of the pair of vertices { e , c } or { x , d } are the best seed sets in the 4-vertex chain. This motivates the need to reweight suitably so that new coarsened graph still retains the original characteristics.

The main insight is that if we select c as a seed, we are in-e  X  ect intending to choose only one of vertices a and b to be seeded (influenced); which suggests that the likelihood of d being influenced from c is either 0 . 5or0 . 25 (corresponding to when a or b is chosen respectively). Hence the weight of edge ( c, d ) should be modified to reflect this fact.
We propose the following solution: Suppose e =( a, b )is contracted and a and b are merged together to form  X  X uper-vertex X  c (say). We reweight the edges adjacent to a and b while coarsening so that the edges now represent the aver-age of the transmission probabilities via a or b . So in our example of Figure 1, edge { c, d } would have weight 0 . 375 (average of 0 . 5 and 0 . 25). Further, we can verify that in this case { e, c } will be the best seed-set, as desired. Figure 2: Reweighting of edges after merging node pairs
Extending the same principle, Figure 2 shows the general situation for any candidate node pair ( a, b ) and how a merge and re-weight (= contract) operation will look like. More formally, our contract operation is as follows:
Definition 4.2 (Merging node pairs). Let Nb i ( v ) (re-spectively Nb o ( v ) )denotethesetofin-neighbors(resp.out-neighbors) of a vertex v .Let v i u = w ( u, v ) and v o u denote the weight of the corresponding edges. If the node pair ( a, b ) is now contracted to a new vertex c ,and w ( a, b )= 1 and w ( b, a )= 2 ,thenthenewedgesareweightedas-c c Graph Coarsening Problem. We are now ready to state our problem formally. Motivated by the connections be-tween the di  X  usive and spectral properties of a graph, we define the following Graph Coarsening Problem to find the set of node pairs which when merged (according to Defi-nition 4.2) lead to the least change in the first eigenvalue. Further, since a vertex cannot influence itself, we assume without loss of generality that the graph G has no self loops. Definition 4.3 (Graph Coarsening Problem).
 Input: Directed, strongly connected, weighted graph G = ( V, E, w ) without self loops and a target fraction 0 &lt;  X  &lt; 1 is obtained from G by merging all node pairs in E 0 .
A related problem is Edge Immunization [37] that asks for a set of edges whose removal leads to the greatest drop in the first eigenvalue. In contrast, GCP seeks to find a set of edges whose contraction (Definition 4.2) leads to the least change in the first eigenvalue. The Edge Immunization problem is known to be NP-hard [37].
As obvious algorithms to GCP are clearly exponential, we propose a greedy heuristic that repeatedly merges a node pair which minimizes the change in the first eigenvalue. Let G ( a,b ) denote the graph G after merging nodes a and b (and incorporating the re-weighting strategy), and G denote the first eigenvalue of the adjacency matrix of G . We define the score of a node pair( a, b ) as follows -
Definition 5.1 (Score). Given a weighted graph G = ( V, E, w ) and an adjacent node pair ( a, b ) , score ( a, b ) is de-fined by:
Intuitively, if score ( a, b )  X  0, it implies that edges ( a, b ) and ( b, a ) do not play a significant role in the di  X  usion through the graph and can thus be contracted. Figure 3 shows an example of our approach.
 Na  X   X ve Algorithm: The above intuition suggests the fol-lowing na  X   X ve algorithm for selecting node pairs for merging. At each stage, calculate the change in the eigenvalue due to merging each adjacent node pair, choose the node pair leading to the least change, merge the chosen nodes, and repeat until the graph is small enough. An implementation for this, even using the Lanczos algorithm for eigenvalue computation for sparse graphs, will be too expensive, tak-ing O ( m 2 ) time. Can we compute (maybe approximately) the scores of each node pair faster? Main Idea: We use a matrix perturbation argument to de-rive an expression for the change in eigenvalue due to merg-ing two adjacent nodes. Using further information about the specific perturbations occurring due to merging two ad-jacent nodes, we show that the change in the eigenvalue can be approximated well in constant time. Thus, we obtain a linear ( O ( m )) time scheme to estimate the score of every pair of adjacent nodes.
Let a and b denote the two neighboring vertices that we are trying to score. We assume that the first eigenvalue of the graph G and the corresponding right and left eigen-vectors ~ u, ~ v are precomputed. Further since the graph G is strongly connected, by the Perron-Frobenius theorem, the first eigenvalue G and the eigenvectors ~ u and ~ v are all real and have positive components. When it is clear from the context, we drop subscripts G and ( a, b ). In the proofs that follow = G and = ( a,b ) as there is no ambiguity. Let A denote the adjacency matrix of the graph. Further as ~ u denotes the eigenvector of A ,let u a = u ( a ) denote the component of ~ u corresponding to vertex a . Merging nodes changes the dimensions of the adjacency matrix A which we handle by viewing merging nodes a, b as adding b 0 s neighbors to a and isolating node b .

Approximation 5.1 provides an equation for the change in the eigenvalue by a matrix perturbation argument. Proposi-tion 5.2 and Proposition 5.3 show how our reweighting strat-egy helps us to approximate the score ( a, b ) in constant time.
Approximation 5.1. The change in eigenvalue can be approximated by = ~ v denotes an infinitesimally small change in the adjacency matrix A and ~ u denotes the corresponding change in the eigenvector ~ u .
 Justification. By the definition of an eigenvalue and eigen-vector of a matrix, we have Perturbing all values of (1) infinitesimally, we get Premultiplying by ~ v T and using (1) and (2),
Using expression (3) along with prior knowledge about the perturbations to the adjacency matrix A and the eigenvector ~ u , we obtain an expression for computing the score of the node pair.

Proposition 5.2 (Score Estimate). Under Approx-imation 5.1, the score of a node pair score ( a, b ) can be ap-proximated as (ignoring second order terms).

Proof. Approximation 5.1 provided an expression for eigenvector. Now A , i.e., change in the adjacency matrix edge weights in the coarsened graph for clarity. can be considered as occurring in three stages namely (i) Deletion of a , (ii) Deletion of b , (iii) Insertion of c . Assume that c is inserted in place of a . Thus we obtain, where ~ e v denotes a vector with a 1 in the v th row and 0 elsewhere. Further, as we modify only two rows and columns of the matrix, this change A is very small.

Also, deletion of vertices a and b cause a th and b th compo-nents of ~ u and ~ v to be zero. ~ u , i.e, change in the eigenvector ~ u can thus be considered as setting u a and u b to zero, fol-lowed by small changes to other components and to u a due to addition of c . Thus we obtain,
Although ~ u cannot be considered as small, we assume that the changes ~ after setting u a and u b components to zero are very small.

Substituting for A ,weget Since ~ v T ~ e a = v a and similarly, But ~ v T ~ a i = v a and ~ a o T ~ u = u a and similarly, Now using (4) and (5) consider, Since A and ~ are both very small, we ignore the second order term ~ v T A ~ . ) ~ v T A ~ u = ~ v T A ( u a ~ e a u b ~ e b ) Since self loops do not a  X  ect di  X  usion in any way, we can assume without loss of generality that G has no self loops. Further, simplifying using definitions of eigenvalue we get, Ignoring small terms, we also have, Substituting (6),(7) and (8) in Approximation 5.1, we get
Note that every term in this expression is a simple product of scalars, except for the ~ u T ~ c o term. We now show that even ~ u be computed in constant time.

Proposition 5.3. Using the re-weighting scheme as de-fined in Definition 4.2, if c denotes the new vertex created by merging nodes { a, b } and ~ c o denotes the out-adjacency vector of c , ~ u T ~ c o = (1 + 2 ) where 1 is the weight of edge ( a, b ) and 2 is the weight of the edge ( b, a ) .

Proof. Let X = Nb o ( a ) \ Nb o ( b ) ,Y = Nb o ( b ) \ Nb Nb o ( a ) \ Nb o ( b ). Since, c is adjacent only to neighbors of a and b , we have where W is the weight of a self loop added at c . Note that a self loop does not a  X  ect di  X  usion in any way (as a node can not influence itself). We use a self loop only in the analysis so as to compute the scores e ciently.
 As per our reweighting scheme (See Definition 4.2) But, by definition of eigenvalues, we know that where a o ( Z )= Similarly, we get Substituting Equations (10), (11), in (9), We now choose W =( (1 + 2 ) so that we get ~ u T ~ c o = (1 + 2 )
Corollary 5.1. Given the first eigenvalue and corre-sponding eigenvectors ~ u, ~ v ,thescoreofanodepair score ( a, b ) can be approximated in constant time.
 Proof. Substituting for ~ u t ~ c o in Proposition 5.2 using Proposition 5.3, we obtain an expression for score ( a, b ) that is composed entirely of scalar terms. Thus we can estimate the edge score in constant time.
Using the approximation described in the previous section, we assign a score to every pair of adjacent nodes of the graph. We then sort these node pairs in ascending order of the absolute value of their scores. Intuitively, we would like to merge a node pair if it has minimal score. Given an upper bound of  X  , the graph is then coarsened by contracting  X  n node pairs one by one in this order ignoring any pairs that have already been merged. We give the pseudo-code of our algorithm coarseNet in Algorithm 1.

Lemma 5.1 (Running Time). The worst case time com-plexity of our algorithm is O ( m ln( m )+  X  nn  X  ) where n notes the maximum degree of any vertex at any time in the coarsening process.

Proof. Computing the first eigenvalue and eigenvector of the adjacency matrix of the graph takes O ( m ) time (for example, using Lanczos iteration assuming that the spec-tral gap is large). As shown in Section 5.1, each node pair can be assigned a score in constant time. In order to score all m adjacent pairs of nodes of the graph, we re-quire linear i.e. ( O ( m )) time. The scored node pairs are sorted in O ( m ln( m )) time. Merging two nodes ( a, b ) re-quires O ( deg ( a )+ deg ( b )) = O ( n  X  ) time. Since we each Algorithm 1 Coarsening Algorithm -coarseNet ( G,  X  ) Input: A directed, weighted graph G =( V , E , w ), Output: Coarsened graph G  X  coarse =( V 0 , E 0 , w 0 ) 1: i =0 2: n = | V | 3: G 0 = G 4: for each adjacent pair of nodes a, b 2 V do 5: Compute score ( a, b ) using Section 5.1 6:  X  ordering of node pairs in increasing order of score 7: while i  X   X  n do 8: ( a, b )=  X  ( i ) 9: G 0 Contract G 0 ( a, b ) merge at most  X  n pairs of nodes, the merging itself has time complexity O (  X  nn  X  ).

Therefore, our worst-case time complexity is O ( m ln( m )+  X  nn  X  ).
The eigenvalue based coarsening method described above aims to obtain a small network that approximates the di  X  u-sive properties of the original large network. As an example application, we now show how to apply our graph coarsen-ing framework to the well studied influence maximization problem. Recall that given a di  X  usion model (IC model in our case) and a social network, the influence maximization problem is to find a small seed set of k nodes such that the expected number of influenced nodes is maximized.
Since we have designed our coarsening strategy such that nodes and edges important for di  X  usion remain untouched, we expect that solving influence maximization on the coars-ened graph is a good proxy for solving it on the much larger original network. The major challenge in this process is to determine how to map the solutions obtained from the coars-ened graph back onto the vertices of the original network. But due to the carefully designed coarsening strategy which tries to keep important, candidate vertices unmerged, we observe that a simple random pull back scheme works well in practice.

More formally, we propose the following multi-stage ap-proach to solve influence maximization: 1. Coarsen the social network graph G by using Algo-2. Solve the influence maximization problem on G coarse 3. Pull back the solutions on to the vertices of the orig-
Algorithm 2 describes our strategy to solve influence max-imization problems. Note that a similar strategy can be ap-plied to study other problems based on di  X  usion in networks. Algorithm 2 cspin : Influence Maximization Framework Input: A weighted graph G =( V , E , w ), the number of seeds Output: Aseedset S of k seeds 1: G  X  coarse ,  X  coarseNet ( G ,  X  ) (See Algorithm 1) 2: s 0 1 ,s 0 2 ,...,s 0 k InfluenceMaximization( G  X  coarse 3: for i =1 ,...,k do 4: s i random sample from  X  1 ( s 0 i ) 5: return S = { s 1 ,s 2 ,...,s k }
We performed several experiments to show the e  X  ective-ness of coarseNet algorithm and also the GCP framework for cascade analysis.
 Dataset #Vertices #Edges Mean Degree Flickr small 500,038 5,002,845 20.01 Flickr medium 1,000,001 14,506,356 29.01 Flickr large 2,022,530 21,050,542 20.82 DBLP 511,163 1,871,070 7.32 Amazon 334,863 1,851,744 11.06 Brightkite 58,228 214,078 7.35 Portland 1,588,212 31,204,286 39.29 Flixster 55,918 559,863 20.02 Datasets. All experiments were conducted on an Intel Xeon machine (2.40 GHz) with 24GB of main memory 1 . We used a diverse selection of datasets from di  X  erent domains to test our algorithm and framework (see Table 2). These datasets were chosen for their size as well as the applicability to the di  X  usion problem. coarseNet was tested on data from Flickr, DBLP, Amazon, Brightkite and Portland epidemiol-ogy data. In the Flickr data, vertices are users, and links rep-resent friendships [5]. In the DBLP data, vertices represent authors and edges represent co-authorship links. Brightkite is a friendship network from a former location-based so-cial networking service provider Brightkite. In the Amazon dataset, vertices are products and an edge represents that the two products are often purchased together. The Port-land dataset is a social contact graph of vertices representing people and edges representing interactions X  X t represents a synthetic population of the city of Portland, Oregon, and has been used in nation-wide smallpox studies [11]. Finally, we also used a real cascade dataset Flixster 2 , where cascades of movie ratings happen over a social network.
Code at: http://www.cs.vt.edu/~badityap/CODE/coarsenet.tgz http://www.cs.ubc.ca/~jamalim/datasets/
We want to measure the performance of coarseNet algo-rithm on the GCP problem. In short, we can coarsen up to 70% of node-pairs using coarseNet , and still retain almost the same eigenvalue.
As a baseline we used random , a random node-pair coars-ening algorithm (randomly choose a node-pair and contract), used in some community detection techniques. Figure 4 shows the values of as the reduction factor  X  increases when we ran coarseNet and random on three datasets (we set a weight of 0 . 02 for this experiment). We observed that in all datasets, as the reduction factor  X  increases, the values of barely change for coarseNet , showing that the di  X  usive properties are maintained even with almost 70% contraction; while random destroyed the eigenvalue very quickly with increasing  X  . This shows that (a) large graphs can in fact be coarsened to large percentages while maintain-ing di  X  usion; and (b) coarseNet e  X  ectively solves the GCP problem. As we show later, we apply the GCP problem and coarseNet on a detailed sample application of influence maximization.
Figure 5 shows the running times of coarseNet w.r.t.  X  and n . To analyze the runtime of coarseNet with re-spect to graph size ( n ), we extracted 6 connected compo-nents (with 500K to 1M vertices in steps of 100K) of the Flickr large dataset. As expected from Lemma 5.1, we ob-serve that in all datasets, as the reduction factor  X  increases, the running time increases linearly (figures also show the linear-fit, with R 2 values), and scale near-linearly as the size of the graph increases. This demonstrates that coarseNet is scalable for large datasets.
Here we demonstrate in detail a concrete application of our GCP problem and coarseNet algorithm to di  X  usion-related problems. We use the well-known Influence Maxi-mization problem. The idea as discussed before is to use the Coarsen-Solve-Project CSPIN framework (see Section 6). In short we find that we obtain 300  X  speed-up on large net-works, while maintaining the quality of solutions. Propagation probabilities: Since accurate propagation probabilities for these networks are not available, we gener-ate propagation probabilities according to two models fol-lowing the literature. Algorithms and setup: We can use any o  X  -the-shelf al-gorithm to solve Inf. Max. problem on the smaller coars-ened network. Here, we choose to use the fast and popular pmia [7] algorithm. We then compared the influence spreads . and running-times of the cspin framework with the plain pmia algorithm to demonstrate gains from using GCP. Quality of solution (Influence spread). In all experi-ments, the influence spread generated by our cspin approach is within 10% of the influence spread generated by pmia . In some cases, we even perform slightly better than pmia .Fig-ure 6(a) shows the expected spread obtained by selecting k = 1000 seeds on five datasets. For these experiments, the percentage of edges to merged is set at 90% and we use the uniform propagation model.
 Quality w.r.t  X  . We find that we can merge up to 95% of the edges while still retaining influence spread. As more edges are merged, the coarsened graph is smaller; so the superseeds in G  X  coarse can be found faster and thus we ex-pect our running time to decrease. We ran tests on the Flickr medium dataset for 1000 seeds and varied  X  from 80% to 95%. Figure 6(b) shows the ratio of the expected influ-ence spread obtained by cspin to that obtained by pmia is almost 1 with varying  X  .
 Quality of solution: E  X  ect of unbiased random pull-back. coarseNet groups nodes which have similar di  X  u-Table 3: Insensitivity of cspin to random pullback choices : Expected influence spread does not vary much. sion e  X  ects, hence choosing any one of the nodes randomly inside a group will lead to similar spreads (hence we do the random pullback in cspin ). Note we do not claim that these groups belong to link-based communities X  X nly that their di  X  usive e  X  ects are similar. To demonstrate this, we performed 100 trials of the random pullback phase for the Flickr small graph. For these trials, 1000 superseeds were found by coarsening 90% of the edges. In each trial, we use these same superseeds to find the 1000 seeds independently and uniformly at random. Table 3 shows that the coe cient of variation of the expected spread is only 5 . 061  X  10 5 Scalability w.r.t number of seeds ( k ). As the budget k increases, we see dramatic performance benefits of cspin over pmia . We run experiments on Flickr small, and Port-land by setting  X  = 90%, and k varied from 0 . 01% to 1% of | V | . Figure 7(a,b) shows the total running times (including the coarsening). Due to lack of space we show only the re-sults for the trivalency model (the uniform case was similar). In all datasets, as k increases, the running time of cspin in-creases very slowly. Note that we get orders of magnitude speed-ups: e.g. on Flickr pmia takes more than 10 days to find 200+ seeds, while cspin runs in 2 minutes.
 Scalability w.r.t  X  . We can see that the running time also drops with increased coarsening as seen in Figure 6(c). Scalability w.r.t n . We ran cspin on the components of increasing size of Flickr large with k = 1000 and  X  =90%. Figure 7(c) plots the running times: cspin obtains a speed up of around 250  X  over pmia consistently.
We now briefly describe how the GCP problem can help in understanding cascade datasets in an exploratory setting. Methodology: We used a Flixster dataset, where users can share ratings of movies with friends. There is a log-file which stores ratings actions by each user: and a cascade is supposed to happen when a person rates the same movie soon after one of her friends. We use the methodology of [15] to learn influence probabilities of a IC-model over the edges of the friendship network from the traces. We then coarsen (c) Running time vs  X  . the resulting directed graph using coarseNet to  X  =50%, and study the formed groups (supernodes). Note that this is in contrast to the approaches where the group information is supplied by a graph-partitioning algorithm (like METIS), and then a group-based IC model is learnt. The base net-work had 55 , 918 nodes and 559 , 863 edges. The trace-log contained about 7 million actions over 48 , 000 movies. We get 1891 groups after removing groups with only one node, with mean group size 16 . 6 with the largest group having 22061 nodes (roughly 40% of nodes).
 Distribution of movies over groups: Figure 8 shows the histogram of the # of groups reached by the movie propaga-tions (following [30], we assume that a movie reaches a group if at least 10% of its nodes rated that movie). We show only the first 100 points of the distribution. We observe that a very large fraction of movies propagate in a small number of groups. Interestingly we observe a multi-modal distribution, suggesting movies have multiple scales of spread. Figure 8: Distribution of # groups entered by movie traces. Groups through the lens of surrogates: An important point to note is that our groups may not be link-based com-munities: we just ensure that nodes in a group have the same di  X  usive properties. We validated this observation in the previous section (Table 3). Hence a natural question is if groups found in Flixster have any other natural structure (e.g. demographics) X  X f they do, we can get a non-network external surrogate for similar di  X  usive characteristics. For-tunately, the Flixster does contain a couple of auxiliary fea-tures for its users (like ID, Last Login, Age). We calcu-lated the Mean Absolute Error (MAE) for  X  X ge X  inside each group, and compared it with the MAE across groups. We found that the average MAE inside the group is very small (within 2 years) compared to a MAE of almost 8 outside, which implies that ages are concentrated within groups and can act as surrogates for di  X  usive characteristics.
We propose influence-based coarsening as a fundamental operation in the analysis of di  X  usive processes in large net-works. Based on the connections between influence spread and spectral properties of the graph, we propose a novel Graph Coarsening Problem and provide an e  X  ective and ef-ficient heuristic called coarseNet . By carefully reweight-ing the edges after each coarsening step, coarseNet at-tempts to find a succinct representation of the original net-work which preserves important di  X  usive properties.
We then describe the cspin framework to solve influence maximization problems on large networks using our coarsen-ing strategy. Experimental results show that cspin indeed outperforms traditional approaches by providing high qual-ity solutions in a fraction of the time.

Finally we show that our coarseNet framework can also be used for examining cascade datasets in an exploratory set-ting. We observe that in our case study the nodes merged together form meaningful communities in the sense of hav-ing similar di  X  usive properties which can serve as surrogates using external demographic information.
 Future work can consist of resolving the complexity of GCP and investigating more applications of our framework to tasks where spectral properties may need to be preserved. Acknowledgements. The authors would like to thank Chris-tos Faloutsos for discussions. This material is based upon work supported by the US Army Research O ce under Grant No. W911NF0910206, by the NSF under Grant No. IIS-1353346, by the NSA (under a  X  X cience of Security X  lablet) and by the VT College of Engineering.
