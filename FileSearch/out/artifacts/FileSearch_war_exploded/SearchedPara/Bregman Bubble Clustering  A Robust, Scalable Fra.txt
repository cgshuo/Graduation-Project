
In traditional clustering, every data point is assigned to at least one cluster. On the other extreme, One Class Clus-tering algorithms proposed recently identify a single dens e cluster and consider the rest of the data as irrelevant. How-ever, in many problems, the relevant data forms multiple natural clusters. In this paper, we introduce the notion of Bregman bubbles and propose Bregman Bubble Clustering (BBC) that seeks k dense Bregman bubbles in the data. We also present a corresponding generative model, Soft BBC, and show several connections with Bregman Cluster-ing, and with a One Class Clustering algorithm. Empiri-cal results on various datasets show the effectiveness of ou r method.
Many unsupervised learning problems involve summa-rizing the data using a small number of parameters. Algo-rithms such as K-Means partition the data into k clusters directly for a given k , while other methods give a hierarchy of clusters. However, in many real-world problems, only a subset can be summarized well, while the rest of the data shows little or no clustering tendencies. Typically in such cases only a portion of the data, containing multiple natu-ral groupings, is relevant. These include: (1) Market-bask et data, where only a subset of customers show coherent be-havior. (2) Many web-mining applications where recover-ing the most relevant items of key categories is more im-portant than obtaining an exhaustive list. (3) Many types of bioinformatics datasets. For example, gene-expression datasets measure expression level of genes compared to a control across a few thousand genes. The experiments typi-cally cover only a specific  X  X heme X  such as stress-response, and therefore only a few genes related to the conditions show good clustering. Biologists are interested in recov-ering small, multiple clusters formed from a small subset of Other types of biological data that share similar propertie s include protein mass spectroscopy and phylogenetic profile data.

For such situations, we would like to design clustering algorithms that are (1) scalable, (2) can cluster only a vari -able fraction of the whole dataset, (3) find multiple cluster s, and (4) can work with a wide variety of distance measures. Existing density-based methods for finding dense clusters such as DBSCAN [5] are not suitable for many such situ-ations because of implicit metric assumptions, and are not scalable to very large problems since they either require an in-memory O ( n 2 ) distance matrix, or an efficient index that usually does not exist for high-dimensional datasets. Re-cently introduced One Class Clustering methods such as OC-IB [3] and BBOCC [8] use local search that are much cluster.
This paper substantially generalizes the single-cluster approach of BBOCC, and consists of three major ex-tensions/enhancements that lead to a robust and scalable framework for finding multiple dense clusters. Our main contributions are as follows: 1. We present a generalization of BBOCC called Breg-2. We present an extension to BBC called Pressuriza-3. We develop a generative (soft) model consisting of a 4. We performed evaluations on a variety of datasets 5. An appropriate model selection strategy is discussed.
Notation : Bold faced variables, e.g. x represent vec-tors. Sets are represented by calligraphic upper-case alph a-bets such as X and are enumerated as { x the individual elements. |X| represents the size of set X Capital letters such as X are random variables. R and R d represent the domain of real numbers and a d -dimensional vector space respectively.
A variety of density-based methods are based on the idea These approaches also have the ability to find arbitrary shaped clusters. DBSCAN [5] is a popular method in the database community for clustering and indexing 2-D and 3-D datasets. Jiang et al. [11] applied density based clusteri ng to gene-expression data.
 proposed [3] that uses the notion of a Bregmanian ball to find a single dense region using an iterative relocation al-gorithm. Gupta and Ghosh [8] described an improved local search called BBOCC, and provide performance guarantee using an enumeration-based seeding. Earlier approaches to One Class Clustering [18, 17] used convex cost functions spondingly, for finding a small number of outliers. Cram-mer and Chechik [3] explain why these approaches are not suitable when the goal is to find locally dense regions. Our approach is similar to that of [8] and [3], with the addi-tional property that we can find multiple dense regions.
In the context of clustering microarray data, discovering overlapping gene clusters is popular since many genes par-ticipate in multiple biological processes. Gene Shaving [9 ] uses PCA to find a small subset of genes that show strong expression change compared to the control sample, and al-lows them to be in multiple clusters.
Bregman divergences form a family of distance mea-sures, defined as follows: Let  X  : S 7 X  R be a strictly convex function defined on a convex set S  X  R d , such that  X  is differentiable on int ( S ) , the interior of Bregman divergence D fined as D  X   X  is the gradient of  X  . For example, for  X  ( x ) = k x k D Distance. Similarly, other forms of  X  lead to other Breg-man divergences such as Logistic Loss, Itakura-Saito Dis-tance, Hinge Loss, Mahalanobis Distance and KL Diver-gence [15, 2].
Let X = { x } n Let G  X  X  represent a non-exhaustive clustering consisting of k clusters {C j } k j =1 with X\G points that are  X  X on X  X  care X , i.e., they do not belong to any cluster. For a given Bregman Divergence D resentatives { c G = {C j } k j =1 , we define the cost Q as the average distance of all points in G from their assigned cluster representative:
Given s , k and D from X are to be clustered into a clustering G  X  X consisting of k clusters, where 1  X  k &lt; n and k  X  s  X  n we define the clustering problem as:
Definition 1 : Find the smallest cost G consisting of k clusters in X , such that |G| = s .
A Bregmanian ball [3] B troid c defines a volume in R d such that all points x where D  X  ( x , c )  X  r { x the average D
Given a set of k cluster representatives, and a fixed s , it can be shown that the clustering that minimizes Q consists of: (1) the assignment phase, where each point is assigned to the nearest cluster representative, and (2) picking poin ts closest to their representatives first until s points are picked. Let r from its cluster representative.

These clusters can be viewed as k Bregman bubbles such that: (1) they are either pure Bregmanian balls of radius r  X  r max or are (2) touching bubbles that form when two or more Bregmanian balls, each of radius r Bregmanian balls B overlap when  X  x : ( D At the point of contact, the touching bubbles form linear cluster representative. For the part of its boundary where a bubble does not touch any other bubble, it traces the contour of a Bregmanian ball of radius r arise naturally as the optimum solution for Q for a given k and D Figure 1. An illustration showing (a) three
Bregman bubbles, and (b) a Bregmanian ball (solid line), and two other possible balls (dot-ted lines). The union of the points enclosed by the three possible balls in (b) is the same as the set of points enclosed by the three bubbles.

Figure 1 illustrates a 2-D example of Bregman bubbles vs. balls. Unlike Bregmanian balls, the boundary of the Bregman bubbles can only be defined in the context of other bubbles touching it. It is important to note that the volume of the convex hull of points in one bubble could be smaller than that of the adjacent touching bubble, and the bubbles could also have different number of points assigned to them. Algorithm 1 BBC-S
For most real life problems, even for a small s , find-ing the globally optimal solution for problem definition 1 would be too slow. However, a fast iterative relocation al-gorithm that guarantees a local minima exists. Bregman Bubble Clustering-S (BBC-S, Algorithm 1) starts with centers and a size s as input. Conceptually, it consists of three stages: (1) the assignment phase, where each point is assigned to the nearest cluster representative, (2) pick ing points closest to their representatives first until s points are picked, and (3) updating the centers. It is interesting to no te that stages 1 and 3 of BBC-S are identical to the Assignment Step and the Re-estimation step of the Bregman Hard Clus-tering [2], properties that lead to the unification describe d in Section 8. Stages 1, 2 and 3 are repeated until there is no change in assignment between two iterations -i.e. the al-gorithm converges. Algorithm 1 describes a more detailed implementation of BBC-S where line number 10 represents Stage 1, lines 14 to 18 map to Stage 2, while lines 22-24 represent Stage 3. We randomly pick k data points from X as the starting cluster representatives, but alternative i nitial-ization schemes could be implemented.
 Theorem 4.1. [2]: Let X be a random variable taking values in X = { x a Bregman divergence D problem has a unique minimizer given by c  X  = = E Proposition 4.2. Algorithm 1 is guaranteed to converge to a local minima for all Bregman divergences.

This follows from the observation that at each iteration the cost Q either declines or stays the same. It is easy to show that for a given set of cluster representatives, the clu s-ter assignment stages 1 and 2 give the lowest possible cost. Therefore, in stages 1 and 2, the cost cannot increase but can decrease. If no point X  X  cluster assignment changes in stage s 1 and 2, the cost stays the same and the algorithm converges. Similarly the cost Q at stage 3 can either decline or stay the same because of Theorem 4.1. By using a heap sort at stage 2, each iteration of BBC-S takes O ( nkd + s log n ) time making it really fast.
An alternative formulation of the BBC algorithm is pos-sible where a threshold cost q the size s : Definition 2 : Find the largest G consisting of k clusters in X with cost no more than q
We can show that this definition also results in Bregman bubbles as the optimal solution for a given set of k cluster representatives. Definitions 1 and 2 are equivalent, since f or a given q the same s , the same solution has the same smallest possible cost q q the cost is more than q modification results in two very different algorithms. For a fixed s as input, for iterations in sparse regions the bubbles expand until s points are covered. As the bubbles move into denser regions, their radii shrink. BBC-Q does not have this property and generally gives worse performance when the bubbles are small. This observation led us to the idea of Pressurization discussed in Section 6. Furthermore, in many problems there is no intuitive way to determine q while users often have an idea of what fraction of their data might cluster well. This makes Definition 1 a more natural choice.
Banerjee et al. [2] proposed a soft clustering algorithm called Bregman Soft Clustering as a mixture model consist-ing of k distributions, taken from the family of regular expo-nential distributions (that include well known distributions such as Gaussians, Multinomials, Poisson, etc.). They went on to prove the following important result: Theorem 5.1. There is a bijection between regular expo-nential families and regular Bregman divergences (equatio n 2). where  X  is a convex function, and the conjugate function of  X  , D  X  is the corresponding Bregman divergence, p (  X , X  ) is the corresponding regular exponential distribution wit h cumulant  X  , f tion that depends on the choice of  X  ,  X  is a scaling factor, is the expectation parameter,  X  are the natural parameters of , and x x . For the sake of notational simplicity, for the rest of the paper, unless stated explicitly otherwise, when we mention x we implicitly refer to the sufficient statistics of x , i.e.
Examples of Bregman divergences and the correspond-ing exponential distribution that have been popular for bot h hard and soft clustering models include squared Euclidean Distance (Gaussian distribution), KL-divergence (multin o-mial distribution) and Itakura-Saito distance.
BBC can be thought of as a non-exhaustive hard clus-tering where points can belong to either one of the k clus-ters or to a  X  X on X  X  care X  group. Correspondingly, Soft BBC can be formulated as modeling the data as a mixture of k distributions from the exponential family and an additiona l  X  X ackground X  distribution that corresponding to the  X  X on X  t care X  points. Since we are trying to find k dense clusters, for a good solution the  X  X on X  X  care X  group should be the least dense. One way to model this low density background is with a uniform distribution. The goal of building such a Soft BBC model is to give us deeper insights into the im-plicit modeling assumptions behind BBC.
The Soft BBC model is defined as follows: Let X = { x be the desired number of clusters. Let Y = { Y hidden random variables taking values from 0 to k corre-sponding to k + 1 mixture components associated with the data points, where 0 corresponds to a uniform background distribution, and 1 to k corresponds to k exponential mix-tures. The likelihood of the data points is given by: Algorithm 2 Soft BBC where {  X  { p k clusters, and p uniform distribution. Assuming the points are sampled i.i.d., the log-likelihood of the observed data is given by: where  X  denotes the priors and mixture component pa-rameters. It is non-trivial to directly optimize the likeli hood function due to the presence of mixture components.
Since p fines the volume of its domain. This domain should include the convex hull of X , which yields an upper bound for p In equation 4, keeping all other parameters constant, a lowe r value of p now, we only consider the case where p value. Therefore, the only parameters we can optimize over are the priors {  X  eters {  X  (A) where  X  a fixed value  X  1 . To maximize the log-likelihood func-tion, we adopt a standard EM-based approach [14] and first construct the negative free energy function: where  X  P = {{  X  p ( Y timates of Y . It can be shown that the EM procedure with the E and M steps alternately optimizing F (  X  P,  X ) over  X  and  X  is guaranteed to converge to a local maxima  X  P  X  and  X  F (  X 
P,  X ) leads to a local maxima on the original likelihood given by equation 4. Hence we will now focus on obtaining the updates involved in the E and M steps for the two cases. Case (A):  X 
E-Step : In this step we optimize F (  X  P,  X ) (equation 5) over  X  P under the constraints that the tipliers {  X  derivatives w.r.t.  X  p ( Y tion for re-estimating the probability of each point coming from any of the 0 to k components, given the current model parameters: log p ( x i ,Y i = j |  X )  X  1  X  log  X  p ( Y i = j | x i )  X   X  where p ( x and  X  ers, we obtain:  X  p ( Y i = j | x i )  X  =
M-Step : In this step we optimize F (  X  P,  X ) over  X  under constraints that the inequality constraints are not binding. Using La-grange multiplier  X  for the constraint and taking derivatives w.r.t.  X  and on eliminating  X  , we obtain:
Note that the update equation for the background distri-bution prior,  X  exponential mixture distributions  X  mixture component parameter estimation can be obtained by setting derivatives over {  X 
This results in the update equation for the exponential distribution mixtures {  X  } k x [2]:
Case (B):  X 
E-Step : Since keeping  X  additional constraints, this step is identical to that of ca se A.

M-Step : Keeping  X  on the priors so that we now require and  X  not binding and by using a Lagrange multiplier and taking derivatives, we arrive at:
The optimal mixture component parameters are obtained exactly as in case A.

Choosing an appropriate p BBC algorithm, one can show that the parameter  X  sentially a function of p step):
Using this relation, for a given  X  component parameters, it is possible to solve for p one cannot do this in the EM framework since the best value for p tionship allows us to calculate the value of p seed parameters. A fast approximation of p mated by (1) performing the first E step (equations 7 and 8), then (2) computing the p i for each x in p i max [ i ] n 1 where s =  X   X  0 n  X  .
BBC-S is able to find locally dense regions because of its ability to explicitly ignore large amounts of data by con -sidering only points close to the cluster representatives f or cluster membership. During each iteration, the bubble rep-resentatives move to a lower cost nearby location. But when the dense bubbles are naturally small, i.e. threshold small, only a few close neighbors get assigned, thereby de-creasing the mobility of the representatives at each itera-tion. This makes it difficult for BBC-S to find small, dense regions far from initial seed locations. On the other hand, starting with a large s would be contrary to the goal of find-ing small dense regions. This problem is even more severe with BBC-Q, since the bubbles cannot expand automati-cally in sparser regions.

Is there a way to improve upon the ability of BBC-S to  X  X xpand X  in a sparse region, while still optimizing cluster -ing over small, dense regions? We start by defining a con-cept called Bregman bubble pressure that is analogous to the pressure around air bubbles in a body of water on Earth. When air-bubbles rise in a column of water, the outside pressure drops, and the bubbles expand. In the case of BBC-S, we can imagine this external pressure as being inversely proportional to the input threshold s ; a larger threshold cor-responds to a smaller external pressure, leading to larger bubbles.

BBC-Press : We propose an algorithmic enhancement to BBC called Pressurization that is designed to improve the quality of the local minima discovered. We start the first iteration of BBC-S with a small enough pressure to cause all points to be assigned to some cluster, and slowly increase the pressure after each iteration. An additional parameter  X   X  [0 , 1) that controls the rate of pressure increase is used as an exponential decay parameter, and s = s +  X  ( n  X  s )  X  j  X  1  X  is used instead of s for the j th ation. Convergence is tested only after ( n  X  s )  X  j  X  1 somewhat slower but more robust alternative involves run-ning BBC-S to full convergence after each recomputation of s , and yields slightly better empirical results. Pressuriza -tion can also be implemented for BBC-Q by varying q instead of s .
 Soft BBC-Press : Pressurization can also be extended to Soft BBC for Case B when  X  and p  X  X xplained X  by the k exponential mixtures. This may lead to bad local minima problems similar to (although less severe than) the one faced in BBC. Therefore, we propose a soft version of Pressurization that takes a decay parameter  X   X  [0 , 1) and runs Soft BBC (Case B) multiple times as follows: (1) start with some initial model parameters {  X  1 run Soft BBC to convergence. (2) at trial r set  X   X  (1  X   X  r  X  1 ) , and for r &gt; 1 set current model parameters to the output of last trial: {  X  r step (2) until  X  final run with  X 
Pearson Correlation ( P ) is a popular similarity measure for clustering gene-expression and other biological datas ets. Pearson Distance ( D defined as 1  X  P ( x , y ) , and is also equal to the Squared When D as Average Pearson Distance (APD). The following directly follows from a proof given by Dhillon and Modha [4]: Proposition 7.1. For any cluster C representative c points in C c  X  = argmin
Therefore, when D (Algorithm 1), the optimal representative for each cluster is computed by averaging the z-scored points rather than the original points, and then again z-scoring the resultant mean. This minor modification makes BBC-S applicable to D We are now ready to look at how the generative model Soft BBC relates to the BBC problem, specifically the for-mulation where the number of points classified into the k real clusters (excluding the  X  X on X  X -care X  cluster) is fixed ( Definition 1 , Section 4.3), and show the following: Proposition 8.1. BBC optimizes a lower bound on the log-likelihood objective function of Soft BBC.
 Proof. Let us consider the cost function: where p  X  ( Y j |  X  j ) and 0 otherwise, which is essentially equivalent to the posterior class probabilities based on the hard assignment s used in BBC. It can be shown [12] that for a fixed set of mixture parameters  X  = {  X  } k log-likelihood objective of Soft BBC (Equation 4):
This result is independent of the choice of priors {  X  while L nents, based on Equations 2 and 16, one can readily obtain the following form for L
If the number of points assigned to the uniform distribu-tion is fixed to n  X  s , s points are assigned to the k exponen-tial distributions, and p Equation 17 that: Proposition 8.2. Maximizing L imizing the BBC objective function Q (Equation 1).
From Proposition 8.2 and Equation 16 we have the proof for Proposition 8.1.
 Proposition 8.3. BBC with a fixed s as input (Definition 1, Section 4.3) is a special case of Soft BBC with fixed  X  Proof. Let us consider an extreme case when  X   X   X  for Soft BBC (see Equation 4 and 2). Then the class poste-rior probabilities in Soft BBC converge to hard assignment (BBC) ensuring that L ( X  |X ) = L Since BBC is equivalent to optimizing L tion 8.2), we can also view BBC with fixed s as input as a special case of Soft BBC with fixed  X 
The following other interesting unifications can also be shown easily for our framework: 1. BBC is a special case of BBC-Press when  X  = 0 . 2. Bregman Bubble Clustering becomes BBOCC when 4. Bregman Bubble Clustering reduces to Bregman Hard
Figure 2 summarizes the hierarchy of algorithms de-scending from BBC-Press and Soft BBC. We could think of BBC as a search under  X  X onstant pressure X , and for Bregman Hard Clustering this pressure is zero. Note that for k = 1 , Bregman Clustering is not very meaningful 9 ,  X   X   X  , (ii) whether  X  0 is 0 (equation 3), and (iii) whether fixed s or q whereas BBC gives rise to BBOCC. In the context of find-ing dense regions in the data, BBC can be thought of as a conceptual bridge between the problem of one class cluster-ing and exhaustive k class clustering. However, the defin-ing characteristic of BBC is its ability to find small, dense regions by modeling a small subset of the data. BBC com-bines the salient characteristics of both Bregman Hard Clus -tering and BBOCC resulting in an algorithm more powerful than either, and that works across all Bregman divergences. BBC-S is a natural extension of BBOCC-S following di-rectly from a common underlying generative model, and is not just a heuristic; the difference in the generative model is only in having a single vs. multiple exponential distribu -tions mixed with a uniform background. Table 1. A summary of the datasets used.

Mic. stands for gene-expression data from microarray experiments, Sim. for artifi-cial/simulated data, Sq. E. stands for
Squared Euclidean, and D is the distance function used for clustering. |C| is the num-ber of classes in the data.

Table 1 describes the essential attributes of the datasets that we report results on. The Gauss-2 dataset was gener-ated using five 2-D Gaussians of different variances (Fig-ure 3) and a uniform distribution. Similar datasets were generated with five Gaussians in 10-D and 40-D to pro-duce Gauss-10 and Gauss-40 datasets. These datasets are useful for verifying algorithms since the true labels are known exactly. Both Gasch Array [6] and Lee [13] are yeast microarray datasets. The Gasch Array dataset contains labels for experiments, and is therefore use-ful for evaluating clustering of experiments in a very high-dimensional (6,151) space. The Lee dataset con-sists of 591 gene-expression experiments on 5,612 yeast genes obtained from the Stanford Microarray database [7] (http://genome-www5.stanford.edu/) and also contains a Gold standard based on Gene Ontology (GO) annotations (http://www.geneontology.org). The Gold standard con-tains 121,406 pairwise links (out of a total of 15,744,466 gene pairs) between 5,612 genes in the Lee data that are known to be functionally related.
Evaluation Criteria : Evaluating clustering is a chal-lenging problem since the clustering itself is unsupervise d and there is no direct way of identifying correspondence between class labels and clusters. Besides using the in-ternal cost measure Q , we also performed three different types of evaluations based upon the type of labeled data: (1) Adjusted Rand Index (ARI) [10], which returns 1 for a perfect agreement between clusters and class labels and 0 when the clustering is as bad as random assignments. (2) p-value : We obtained p-values for individual clusters of Yeast genes using Funspec (http://funspec.med.utoronto.ca/) [16]. ances are updated for s =750, for k = 7 (left), and k = 5 (right). k was set to 7 in the left figure to (3) Overlap Lift : It is not possible to use ARI to evaluate against the links in the Lee Gold standard. Instead, we com-pute statistical significance as follows: k clusters of size is the number of correct links observed then Overlap Lift correct links are observed as compared to random chance, where f standard.

For all evaluations, the points in the background or the  X  X on X  X  care X  cluster are excluded from the evaluation. Note that the clustering is performed in a completely unsuper-vised setting and the class labels were only used for evalua-tion.

Evaluating Soft BBC : We tested Soft BBC with Gaus-sians as the exponential mixture components. If the Gaus-sian variances are treated as a part of the mixture parameter s {  X  diameters (Figure 3, right) that fit natural cluster diamete rs. There are eight possible variations of Soft BBC depending upon whether (i)  X  (ii) variances are updated or not, and (iii) all cluster vari -ances are forced to be equal or not. We present results on the Soft BBC implementation that gives the best results: updat-able, unequal variances with a fixed  X  Soft BBC for Gaussians with an alternative model that we call Mixture-6 where the uniform background distribution is replaced by a large, fixed variance Gaussian while the other k
Gaussian variances are updated, and  X 
Hard Assignments for Soft BBC : On convergence, the points are assigned to the mixture with the largest proba-bility. A post-processing was performed that recomputes p such that exactly ( n  X  s ) /n points are assigned to the  X  X on X  X  care X  set. A similar conversion was required for evaluating the soft model Mixture-6.

Comparison with other methods : We also compared our method with Bregman Hard Clustering, Single Link Ag-glomerative clustering and DBSCAN. Bregman Hard Clus-tering assigns every data point into a cluster. To be able to compare it meaningfully with BBC, we picked s points closest to their respective cluster representatives. This pro-cedure was also used for Single Link Agglomerative clus-tering. For the two DBSCAN parameters, we set MinPts to 4 as recommended by Ester et al. [5], while we searched for Eps that resulted in s points in clusters. k is automati-cally estimated by DBSCAN while for all the other methods and datasets k was set to |C| (Table 1), except for the Lee dataset (where |C| is not known) where we set k to 10. All five methods use the same (and the appropriate) distance measure; Sq. Euclidean for the Gaussian and Pearson Dis-tance for the gene-expression datasets respectively.
Pressurization with Soft BBC : For the lower dimen-sional datasets, Soft BBC-Press, does extremely well, giv-ing near-perfect results (ARI  X  1) for up to 40% coverage on Gauss-10 data and an ARI between 0.8 and 0.9 for up to 40% coverage on Gauss-2 data. We only tested the Soft BBC and the Mixture-6 models on Gauss-2 and Gauss-10 datasets, mainly to validate Soft BBC and Soft BBC-Press. This is because, exponential mixture models in general, in-cluding Bregman Soft Clustering, Mixture-6 and Soft BBC all suffer from an inherent flaw that makes them imprac-tical for high dimensional datasets; there are rounding er-rors while estimating the mixture membership probabilitie s (equation 7), and these rounding errors worsen exponen-tially with the dimensionality of the data d , so much so that the models generally do not work well beyond d = 10 However, the main purpose of designing Soft BBC was initialization. to show that a fundamental generative model lies behind BBC (Section 8). Furthermore, figure 4(a) and (b) show that Mixture-6 has no clear performance advantage over Soft BBC. Also, Mixture-6 does not conform to the form required to incorporate Pressurization and does not cor-respond to any known  X  X ard X  model for Bregman diver-gences, and a hard model is essential to scale to higher di-mensional datasets.
 Pressurization with (Hard) BBC : As predicted, both BBC and Soft BBC without Pressurization tend to be a lot more sensitive to initialization, and BBC-Press performs a l-most as well as Soft BBC-Press on the Gauss-2 and Gauss-10 datasets giving ARI  X  1 for coverages of up to 40%. On the Gauss-40 dataset, BBC-Press continues to give an ARI  X  1 for up to 40% coverage. In contrast, we were unable to run Soft BBC-Press for Gauss-40 dataset because of severe rounding errors. These results are impressive given that th e ARI was obtained as averages of multiple runs with random seeding. In Figure 5(f), for lower coverages, BBC-Press gives significantly lower cost ( APD ) as compared to both BBC and Bregman Hard, which also used a similar cost function. The improvement against labeled data using BBC Press as compared to BBC is also dramatic for both Gasch Array and Lee, showing that Pressurization also works well for clustering high dimensional gene experiments (Figure 5(a)) or genes (5 (d)). Note that the error bars were plotted on all the local search algorithms, but are often too small to be visible.
 Comparison with other types of Algorithms : On the Gaussian datasets (Figure 4(d) to (f)), and on the two gene-expression datasets (Figure 5(b) and (e)), DBSCAN, Sin-gle Link Agglomerative and Bregman Hard Clustering all perform much worse than BBC-Press in general, and espe-cially when clustering a part of the data. These results are based on evaluation using labels not used for clustering; us -ing ARI on Gaussians (Figure 4(d) to (f)) and Gasch Array (Figure 5(b)) and using Overlap Lift on Lee (Figure 5(e)), and are therefore independent of the clustering methodol-ogy. Figure 5(e) shows that (1) BBC-Press not only beats other methods by a wide margin but also shows high en-richments of links for low coverages (over 6 times for 5 % coverage), and (2) Single Link Agglomerative clustering does not work well for clustering genes and gives results not much better than random. On all datasets, Single Link tends to perform the worst; one explanation might be its inability to handle noisy data. In fact, for some situations (Figure 4(d) to (f)), DBSCAN and Single Link Agglomera-tive give slightly worse than random performance resulting in ARI values that are slightly below 0. The performance difference between our method (BBC-Press) and the other three methods is quite significant on all the five datasets, given the small error bars. Additionally, if we were to pick the minimum-cost solution out of multiple trials for the lo-cal search methods, the differences in the performance be-tween BBC-Press vs. DBSCAN and Single Link becomes even more substantial, e.g. Figure 5, (b) vs. (c) for Gasch Array.
 Selecting size and number of dense clusters : In BBC-Press, s controls the number of data points in dense clus-ters. The dense clusters were invariably very pure when using BBC-Press, with near-perfect clusters on the Gaus-sian data for s of up to 40% of n , while on the Gasch Array dataset the performance peaks at a coverage of around 0.3 but shows a general decline after that. The rapid increase in cluster quality with decreasing s is more pronounced in BBC-Press than in the other methods, and shows that on these datasets, dense regions are indeed highly correlated with the class labels; the confirmation of this phenomena is tantalizing considering the fact that the clustering proce ss was completely unsupervised. In practice, selecting dense clusters with BBC-Press requires choosing an appropriate s and k . If small amounts of labeled data is available, the best k can be estimated for a fixed s using an approach such as PAC-MDL [1], while a reasonable s can be picked by ap-plying BBC-Press on a range of s and picking the  X  X nee X  (e.g. Figures 4(a),(b),(c) and 5(c) show a sudden decline in ARI near s = 0 . 4  X  n ). Alternatively, in many problems can be an input, while s simply has to be a small threshold (e.g. for finding a small number of relevant web documents, or a small number of relevant genes (Figure 5(e)).
Visual Verification : Although the results based on per-formance measures show the effectiveness of our method, visual verification serves as another independent validati on that the clusters are not only statistically significant but also useful in practice. For the Gauss-2 dataset, it is easy to ver -ify the quality of the clusters visually (Figure 3). For the Gasch Array clustering, most clusters were generally very pure using BBC-Press for lower coverages. For example, when only 70 out of 173 experiments are clustered by re-peating BBC-Press 20 times and picking the lowest cost so-lution, the average ARI is around 0.6 over 12 classes. Some clusters are even purer, for example, one of the clusters con -tained 12 out of 13 points belonging to the class  X  X PD X , while there are 22 experiments of type YPD. This gives us an accuracy of 92.31% for a coverage of 0.591 when 40 % of the data was clustered into 12 clusters. Similarly, for the Lee dataset, we verified a high purity cluster using Fun-Spec; 10 out of 14 genes in one of the clusters belonged to the functional category  X  X ytoplasmic and nuclear degra-cluster belonging into to the category by random chance. Many other gene clusters on the Lee dataset also had low p-values for some of the categories recovered by FunSpec.
Empirical results show that BBC-Press outperforms other potential alternatives by a large margin and gives goo d results on a variety of problems involving low to very high-dimensional feature spaces. BBC-Press can be seen as a powerful extension of One Class Clustering to a multi-class setting where the goal is to find dense regions in the data. Our method extends the notion of  X  X ensity-based cluster-ing X  to a large class of divergence measures, and is per-haps the first that uses a local search/parametric approach. The low time and space complexity of the local search ap-proach, coupled with the robustness provided by Pressuriza -tion, makes it possible to find multiple dense regions on ex-tremely large and high-dimensional datasets, thus opening density-based clustering to much larger problems. Breg-man Bubble Clustering can also be thought of as a con-ceptual bridge between partitional clustering algorithms and the problem of One Class Clustering. The Soft BBC model shows that BBC arises out of a more fundamental model involving a mixture of exponentials and a uniform back-ground, and explains why BBC performs better than Breg-man Clustering by incorporating a model for the  X  X oisy X  background. The extension of BBC to Pearson Correlation (Pearson Distance) makes it applicable to a variety of bio-logical datasets where finding small, dense clusters is crit i-cal.
 Acknowledgments : This research was supported by NSF grants IIS-0325116 and IIS-0307792. We are also grateful to Srujana Merugu for some useful discussions.
