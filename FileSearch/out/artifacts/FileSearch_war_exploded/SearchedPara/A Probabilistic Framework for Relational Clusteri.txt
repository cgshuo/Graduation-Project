 Relational clustering has attracted more and more attention due to its phenomenal impact in various important appli-cations which involve multi-type interrelated data objects, such as Web mining , search marketing, bioinformatics, cita-tion analysis, and epidemiology. In this paper, we propose a probabilistic model for relational clustering, which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clus-tering, semi-supervised clustering, co-clustering and graph clustering. The proposed model seeks to identify cluster structures for each type of data objects and interaction pat-terns between different types of objects. Under this model, we propose parametric hard and soft relational clustering algorithms under a large number of exponential family dis-tributions. The algorithms are applicable to relational data of various structures and at the same time unifies a number of stat-of-the-art clustering algorithms: co-clustering algo-rithms, the k-partite graph clustering, and semi-supervised clustering based on hidden Markov random fields.
 Categories and Subject Descriptions: E.4 [ Coding and Information Theory] :Data compaction and compres-sion; H.3.3[ Information search and Retrieval] :Clustering; I.5.3[ Pattern Recognition] :Clustering.
 General Terms: Algorithms.
 Keywords: Clustering, Relational data, Relational clus-tering, Semi-supervised clustering, EM-algorithm, Bregman divergences, Exponential families.
Most clustering approaches in the literature focus on  X  X lat X  data in which each data object is represented as a fixed-length attribute vector [38]. However, many real-world data sets are much richer in structure, involving objects of multi-ple types that are related to each other, such as documents and words in a text corpus, Web pages, search queries and Web users in a Web search system, and shops, customers, suppliers, shareholders and advertisement media in a mar-keting system.
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
In general, relational data contain three types of infor-mation, attributes for individual objects, homogeneous re-lations between objects of the same type, heterogeneous re-lations between objects of different types. For example, for a scientific publication relational data set of papers and au-thors, the personal information such as affiliation for authors are attributes; the citation relations among papers are ho-mogeneous relations; the authorship relations between pa-pers and authors are heterogeneous relations. Such data violate the classic IID assumption in machine learning and statistics and present huge challenges to traditional cluster-ing approaches. An intuitive solution is that we transform relational data into flat data and then cluster each type of objects independently. However, this may not work well due to the following reasons.

First, the transformation causes the loss of relation and structure information [14]. Second, traditional clustering approaches are unable to tackle influence propagation in clustering relational data, i.e., the hidden patterns of differ-ent types of objects could affect each other both directly and indirectly (pass along relation chains). Third, in some data mining applications, users are not only interested in the hid-den structure for each type of objects, but also interaction patterns involving multi-types of objects. For example, in document clustering, in addition to document clusters and word clusters, the relationship between document clusters and word clusters is also useful information. It is difficult to discover such interaction patterns by clustering each type of objects individually.

Moreover, a number of important clustering problems, which have been of intensive interest in the literature, can be viewed as special cases of relational clustering. For example, graph clustering (partitioning) [7, 42, 13, 6, 20, 28] can be viewed as clustering on singly-type relational data consisting of only homogeneous relations (represented as a graph affin-ity matrix); co-clustering [12, 2] which arises in important applications such as document clustering and micro-array data clustering, can be formulated as clustering on bi-type relational data consisting of only heterogeneous relations. Recently, semi-supervised clustering [46, 4] has attracted significant attention, which is a special type of clustering us-ing both labeled and unlabeled data. In section 5, we show that semi-supervised clustering can be formulated as clus-tering on singly-type relational data consisting of attributes and homogeneous relations.

Therefore, relational data present not only huge challenges to traditional unsupervised clustering approaches, but also great need for theoretical unification of various clustering tasks. In this paper, we propose a probabilistic model for relational clustering, which also provides a principal frame-work to unify various important clustering tasks includ-ing traditional attributes-based clustering, semi-supervised clustering, co-clustering and graph clustering. The pro-posed model seeks to identify cluster structures for each type of data objects and interaction patterns between dif-ferent types of objects. It is applicable to relational data of various structures. Under this model, we propose para-metric hard and soft relational clustering algorithms under a large number of exponential family distributions. The algorithms are applicable to various relational data from various applications and at the same time unify a number of stat-of-the-art clustering algorithms: co-clustering algo-rithms, the k-partite graph clustering, Bregman k-means, and semi-supervised clustering based on hidden Markov ran-dom fields.
Clustering on a special case of relational data, bi-type rela-tional data consisting of only heterogeneous relations, such as the word-document data, is called co-clustering or bi-clustering. Several previous efforts related to co-clustering are model based [22, 23]. Spectral graph partitioning has also been applied to bi-type relational data [11, 25]. These algorithms formulate the data matrix as a bipartite graph and seek to find the optimal normalized cut for the graph. Due to the nature of a bipartite graph, these algorithms have the restriction that the clusters from different types of objects must have one-to-one associations. Information-theory based co-clustering has also attracted attention in the literature. [12] proposes a co-clustering algorithm to maximize the mutual information between the clustered ran-dom variables subject to the constraints on the number of row and column clusters. A more generalized co-clustering framework is presented by [2] wherein any Bregman diver-gence can be used in the objective function. Recently, co-clustering has been addressed based on matrix factorization. [35] proposes an EM-like algorithm based on multiplicative updating rules.

Graph clustering (partitioning) clusters homogeneous data objects based on pairwise similarities, which can be viewed as homogeneous relations. Graph partitioning has been stud-ied for decades and a number of different approaches, such as spectral approaches [7, 42, 13] and multilevel approaches [6, 20, 28], have been proposed. Some efforts [17, 43, 21, 21, 1] based on stochastic block modeling also focus on homo-geneous relations.

Compared with co-clustering and homogeneous-relation-based clustering, clustering on general relational data, which may consist of more than two types of data objects with various structures, has not been well studied in the liter-ature. Several noticeable efforts are discussed as follows. [45, 19] extend the the probabilistic relational model to the clustering scenario by introducing latent variables into the model; these models focus on using attribute information for clustering. [18] formulates star-structured relational data as a star-structured m -partite graph and develops an al-gorithm based on semi-definite programming to partition the graph. [34] formulates multi-type relational data as K-partite graphs and proposes a family of algorithms to iden-tify the hidden structures of a k-partite graph by construct-ing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures. The above graph-based algorithms do not consider attribute information.

Some efforts on relational clustering are based on induc-tive logic programming [37, 24, 31]. Base on the idea of mutual reinforcement clustering, [51] proposes a framework Figure 1: Examples of the structures of relational data. for clustering heterogeneous Web objects and [47] presents an approach to improve the cluster quality of interrelated data objects through an iterative reinforcement clustering process. There are no sound objective function and theoret-ical proof on the effectiveness and correctness (convergence) of the mutual reinforcement clustering. Some efforts [26, 50, 49, 5] in the literature focus on how to measure the similar-ities or choosing cross-relational attributes.

To summarize, the research on relational data clustering has attracted substantial attention, especially in the special cases of relational data. However, there is still limited and preliminary work on general relational data clustering.
With different compositions of three types of information, attributes, homogeneous relations and heterogeneous rela-tions, relational data could have very different structures. Figure 1 shows three examples of the structures of relational data. Figure 1(a) refers to a simple bi-type of relational data with only heterogeneous relations such as word-document data. Figure 1(b) represents a bi-type data with all types of information, such as actor-movie data, in which actors (type 1) have attributes such as gender; actors are related to each other by collaboration in movies (homogeneous rela-tions); actors are related to movies (type 2) by taking roles in movies (heterogeneous relations). Figure 1(c) represents the data consisting of companies, customers, suppliers, share-holders and advertisement media, in which customers (type 5) have attributes.

In this paper, we represent a relational data set as a set of matrices. Assume that a relational data set has m dif-ferent types of data objects, X (1) = { x (1) i } n 1 i =1 { x j th type and x ( j ) p denotes the name of the p th object of the j th type. We represent the observations of the relational data as three sets of matrices, attribute matrices { F ( j ) R j  X  n j } m j =1 , where d j denotes the dimension of attributes for the j th type objects and F ( j )  X  p denotes the attribute vec-tor for object x ( j ) p ; homogeneous relation matrices { S R and x ( j ) q ; heterogeneous relation matrices { R ( ij ) where R ( ij ) pq denotes the relation between x ( i ) p and x above representation is a general formulation. In real ap-plications, not every type of objects has attributes, homo-geneous relations and heterogeneous relations. For exam-ple, the relational data set in Figure 1(a) is represented by only one heterogeneous matrix R (12) , and the one in Figure 1(b) is represented by three matrices, F (1) , S (1) and R Moreover, for a specific clustering task, we may not use all available attributes and relations after feature or relation selection pre-processing.

Mixed membership models, which assume that each ob-ject has mixed membership denoting its association with classes, have been widely used in the applications involving soft classification [16], such as matching words and pictures [39], race genetic structures [39, 48], and classifying scientific publications [15].

In this paper, we propose a relational mixed membership model to cluster relational data (we refer to the model as mixed membership relational clustering or MMRC through-out the rest of the paper).
 Assume that each type of objects X ( j ) has k j latent classes. We represent the membership vectors for all the objects in X sum of elements of each column  X  ( j )  X  p is 1 and  X  ( j ) the membership vector for object x ( j ) p , i.e.,  X  ( j ) probability that object x ( j ) p associates with the g th latent class. We also write the parameters of distributions to gen-erate attributes, homogeneous relations and heterogeneous distribution parameter matrix for generating attributes F such that  X  ( j )  X  g denotes the parameter vector associated with rameter matrix for generating homogeneous relations S ( j )  X  heterogeneous relations R ( ij ) . In summary, the parameters of MMRC model are In general, the meanings of the parameters,  X ,  X , and  X , de-pend on the specific distribution assumptions. However, in Section 4.1, we show that for a large number of exponential family distributions, these parameters can be formulated as expectations with intuitive interpretations.
 Next, we introduce the latent variables into the model. For each object x j p , a latent cluster indicator vector is gen-erated based on its membership parameter  X  ( j )  X  p , which is matrix for all the j th type objects in X ( j ) .
Finally, we present the generative process of observations, { F 1. For each object x ( j ) p 2. For each object x ( j ) p 3. For each pair of objects x ( j ) p and x ( j ) q 4. For each pair of objects x ( i ) p and x ( j ) q In the above generative process, a latent indicator vector for each object is generated based on multinomial distribution with the membership vector as parameters. Observations are generated independently conditioning on latent indica-tor variables. The parameters of condition distributions are formulated as products of the parameter matrices and latent der this formulation, an observation is sampled from the distributions of its associated latent classes. For example, C  X  q indicates that x ( j ) q is with the h th latent class, then ( C implying that the relation between x ( i ) p and x ( j ) q by using the parameter  X  ( ij ) gh .

With matrix representation, the joint probability distrib-ution over the observations and the latent variables can be formulated as follows,
Pr ( X  |  X ) = and similarly for R ( ij ) .
In this section, based on the MMRC model we derive para-metric soft and hard relational clustering algorithms under a large number of exponential family distributions.
To avoid clutter, instead of general relational data, we use relational data similar to the one in Figure 1(b), which is a representative relational data set containing all three types of information for relational data, attributes, homogeneous relations and heterogeneous relations. However, the deriva-tion and algorithms are applicable to general relational data.
For the relational data set in Figure 1(b), we have two types of objects, one attribute matrix F , one homogeneous relation matrix S and one heterogeneous relation matrix R . Based on Eq.(1), we have the following likelihood function,
L ( X  |  X ) = Pr ( C (1) |  X  (1) ) Pr ( C (2) |  X  (2) ) Pr ( F |  X  C Our goal is to maximize the likelihood function in Eq. (2) to estimate unknown parameters.

For the likelihood function in Eq.(2), the specific forms of condition distributions for attributes and relations depend on specific applications. Presumably, for a specific likelihood function, we need to derive a specific algorithm. However, a large number of useful distributions, such as normal dis-tribution, Poisson distribution, and Bernoulli distributions, belong to exponential families and the distribution functions of exponential families can be formulated as a general form. This nice property facilitates us to derive a general EM al-gorithm for the MMRC model.

It is shown in the literature [3, 9] that there exists bijection between exponential families and Bregman divergences [40]. For example, the normal distribution, Bernoulli distribution, multinomial distribution and exponential distribution cor-respond to Euclidean distance, logistic loss, KL-divergence and Itakura-Satio distance, respectively. Based on the bi-jection, an exponential family density Pr ( x ) can always be formulated as the following expression with a Bregman di-vergence D  X  , where f  X  ( x ) is a uniquely determined function for each ex-ponential probability density, and  X  is the expectation para-meter. Therefore, for the MMRC model under exponential family distributions, we have the following,
In the above equations, a Bregman divergence of two matri-ces is defined as the sum of the Bregeman divergence of each pair of elements from the two matrices. Another advantage of the above formulation is that under this formulation, the parameters,  X ,  X , and  X , are expectations of intuitive in-terpretations.  X  consists of center vectors of attributes;  X  provides an intuitive summary of cluster structure within the same type objects, since  X  (1) gh implies expectation rela-tions between the g th cluster and the h th cluster of type 1 objects; similarly,  X  provides an intuitive summary for cluster structures between the different type objects. In the above formulation, we use different Bregman divergences, D  X  1 , D  X  2 , and D  X  3 , for the attributes, homogeneous re-lations and heterogeneous relations, since they could have different distributions in real applications. For example, suppose we have  X  (1) = tion,  X  (1) =  X  structures of the data are very intuitive. First, the center attribute vectors for the two clusters of type 1 are and from different clusters are barely related and cluster 1 is denser that cluster 2; third, by  X  (12) we know that cluster 1 of type 1 nodes are related to cluster 2 of type 2 nodes more strongly than to cluster 1 of type 2, and so on so forth.
Since the distributions of C (1) and C (2) are modeled as multinomial distributions, we have the following Substituting Eqs. (4), (5), (6), (7), and (8) into Eq, (2) and taking some algebraic manipulations, we obtain the fol-lowing log-likelihood function for MMRC under exponential families, log( L ( X  |  X )) = constant in the log-likelihood function.

Expectation Maximization (EM) is a general approach to find the maximum-likelihood estimate of the parameters when the model has latent variables. EM does maximum likelihood estimation by iteratively maximizing the expec-tation of the complete (log-)likelihood, which is the following under the MMRC model, where  X   X  denotes the current estimation of the parameters and  X  is the new parameters that we optimize to increase Q . Two steps, E-step and M-step, are alternatively performed to maximize the objective function in Eq. (10).
In the E-step, based on Bayes X  X  rule, the posterior proba-bility of the latent variables, is updated using the current estimation of the parameters. However, conditioning on observations, the latent variables are not independent, i.e., there exist dependencies between the posterior probabilities of C (1) and C (2) , and between those of C (1)  X  p and C (1)  X  q . Hence, directly computing the pos-terior based on Eq. (11) is prohibitively expensive.
There exist several techniques for computing intractable posterior, such as Monte Carlo approaches, belief propa-gation, and variational methods. We follow a Monte Carlo approach, Gibbs sampler, which is a method of constructing a Markov chain whose stationary distribution is the distri-bution to be estimated.

It is easy to compute the posterior of a latent indicator vector while fixing all other latent indicator vectors, i.e., where C (1)  X  X  p denotes all the latent indicator vectors except for C (1)  X  p . Therefore, we present the following Markov chain to estimate the posterior in Eq. (11). Note that at each sampling step in the above procedure, we use the latent indicator variables sampled from previous steps. The above procedure iterates until the stop crite-rion is satisfied. It can be shown that the above procedure is a Markov chain converging to Pr ( C (1) , C (2) | F , S , R , Assume that we keep l samples for estimation; then the posterior can be obtained simply by the empirical joint dis-tribution of C (1) and C (2) in the l samples.
After the E-step, we have the posterior probability of la-tent variables to evaluate the expectation of the complete log-likelihood, Q ( X  ,  X   X ) = In the M-step, we optimize the unknown parameters by
First, we derive the update rules for membership parame-ters  X  (1) and  X  (2) . To derive the expression for each  X  we introduce the Lagrange multiplier  X  with the constraint P Substituting Eqs. (9) and (13) into Eq. (15), after some algebraic manipulations, we have Summing both sides over h , we obtain  X  = 1 resulting in the following update rule, i.e.,  X  (1) hp is updated as the posterior probability that the p th object is associated with the h th cluster. Similarly, we have the following update rule for  X  (2) hp
Second, we derive the update rule for  X . Based on Eqs. (9) and (13), optimizing  X  is equivalent to the following optimization, arg min We reformulated the above expression as, arg min
To solve the above optimization, we make use of an im-portant property of Bregman divergence presented in the following theorem.

Theorem 1. Let X be a random variable taking values in X = { x i } n i =1  X  S  X  R d following v . Given a Bregman divergence D  X  : S  X  int ( S ) 7 X  [0 ,  X  ) , the problem has a unique minimizer given by s  X  = E v [ X ] . The proof of Theorem 1 is omitted (please refer [3, 40]). Theorem 1 states that the Bregman representative of a ran-dom variable is always the expectation of the variable. Based on Theorem 1 and the objective function in (20), we update  X   X  g as follows,
Third, we derive the update rule for  X . Based on Eqs. (9) and (13), we formulate optimizing  X  as the following optimization, where  X  p denotes Pr ( C (1) gp = 1 , C (1) hq = 1 | F , S , R , p, q  X  n 1 . Based on Theorem 1, we update each  X  gh as follows,
Fourth, we derive the update rule for  X . Based on Eqs. (9) and (13), we formulate optimizing  X  as the following optimization, where  X  p denotes Pr ( C (1) gp = 1 , C (2) hq = 1 | F , S , R , n 1 and 1  X  q  X  n 2 . Based on Theorem 1, we update each  X  gh as follows,
 X 
Combining the E-step and M-step, we have a general re-lational clustering algorithm, Exponential Family MMRC (EF-MMRC) algorithm, which is summarized in Algorithm 1. Since it is straightforward to apply our algorithm deriva-tion to a relational data set of any structure, Algorithm 1 is proposed based on the input of a general relational data set. Despite that the input relational data could have var-ious structures, EF-MMRC works simply as follows: in the E-step, EF-MMRC iteratively updates the posterior prob-abilities that an object is associated with the clusters (the Markov chain in Section 4.2); in the M-step, based on the current cluster association (posterior probabilities), the clus-ter representatives of attributes and relations are updated as the weighted mean of the observations no matter which exponential distributions are assumed.

Therefore, with the simplicity of the traditional centroid-based clustering algorithms, EF-MMRC is capable of mak-ing use of all attribute information and homogeneous and heterogenous relation information to learn hidden structures from various relational data. Since EF-MMRC simultane-ously clusters multi-type interrelated objects, the cluster structures of different types of objects may interact with each other directly or indirectly during the clustering process to automatically deal with the influence propagation. Be-sides the local cluster structures for each type of objects, Algorithm 1 Exponential Family MMRC Algorithm Input: A relational data set tial family distributions (Bregman divergences) assumed for the data set.
 Output: Membership Matrices {  X  ( j ) } m j =1 , attribute expec-tation matrices {  X  ( j ) } m j =1 , homogeneous relation expecta-tion matrices {  X  ( j ) } m j =1 , and heterogeneous relation expec-Method: 1: Initialize the parameters as  X   X  = 2: repeat 3: { E-step } 4: Compute the posterior 5: { M-step } 6: for j = 1 to m do 7: Compute  X  ( j ) using update rule (17). 8: Compute  X  ( j ) using update rule (22). 9: Compute  X  ( j ) using update rule (24). 10: for i = 1 to m do 11: Compute  X  ( ij ) using update rule (26). 12: end for 13: end for 14:  X   X  =  X  15: until convergence the output of EF-MMRC also provides the summary of the global hidden structure for the data, i.e., based on  X  and  X , we know how the clusters of the same type and differ-ent types are related to each other. Furthermore, relational data from different applications may have different proba-bilistic distributions on the attributes and relations; it is easy for EF-MMRC to adapt to this situation by simply us-ing different Bregman divergences corresponding to different exponential family distributions.

If we assume O ( m ) types of heterogeneous relations among m types of objects, which is typical in real applications, and let n =  X ( n i ) and k =  X ( k i ), the computational complexity of EF-MMRC can be shown to be O ( tmn 2 k ) for t iterations. If we apply the k-means algorithm to each type of nodes in-dividually by transforming the relations into attributes for each type of nodes, the total computational complexity is also O ( tmn 2 k ).
Due to its simplicity, scalability, and broad applicability, k-means algorithm has become one of the most popular clus-tering algorithms. Hence, it is desirable to extend k-means to relational data. Some efforts [47, 2, 12, 33] in the lit-erature work in this direction. However, these approaches apply to only some special and simple cases of relational data, such as bi-type heterogeneous relational data.
As traditional k-means can be formulated as a hard ver-sion of Gaussian mixture model EM algorithm [29], we pro-pose the hard version of MMRC algorithm as a general rela-tional k-means algorithm (from now on, we call Algorithm 1 as soft EF-MMRC), which applies to various relational data.
To derive the hard version MMRC algorithm, we omit soft membership parameters  X  ( j ) in the MMRC model ( C ( j ) the model provides the hard membership for each object). Next, we change the computation of the posterior proba-bilities in the E-step to reassignment procedure, i.e., in the E-step, based on the estimation of the current parameters, we re-assign cluster labels, { C ( j ) } m j =1 , to maximize the ob-jective function in (9). In particular, for each object, while fixing the cluster assignments of all other objects, we assign it to each cluster to find the optimal cluster assignment max-imizing the objective function in (9), which is equivalent to minimizing the Bregman distances between the observations and the corresponding expectation parameters. After all objects are assigned, the re-assignment process is repeated until no object changes its cluster assignment between two successive iterations.

In the M-step, we estimate the parameters based on the cluster assignments from the E-step. A simple way to derive the update rules is to follow the derivation in Section 4.3 but replace the posterior probabilities by its hard versions. For example, after the E-step, if the object x ( j ) p is assigned to the g th cluster, i.e., C ( j ) gp = 1, then the posterior Pr ( C 1 | F , S , R ,  X   X ) = 1 and Pr ( C (1) hp = 1 | F , S , R ,
Using the hard versions of the posterior probabilities, we derive the following update rule for  X  ( j ) , In the above update rule, since g th cluster,  X  ( j )  X  g is actually updated as the mean of the attribute vectors of the objects assigned to the g th cluster. Similarly, we have the following update rule for  X  ( j ) i.e.,  X  ( j ) gh is updated as the mean of the relations between the objects of the j th type from the g th cluster and from the h th cluster.

Each heterogeneous relation expectation parameter  X  ( ij ) is updated as the mean of the objects of the i th type from the g th cluster and of the j th type from the h th cluster,
The hard version of EF-MMRC algorithm is summarized in Algorithm 2. It works simply as the classic k-means. However, it is applicable to various relational data under various Bregman distance functions corresponding to vari-ous assumptions of probability distributions. Based on the EM framework, its convergence is guaranteed. When ap-plied to some special cases of relational data, it provides simple and new algorithms for some important data mining problems. For example, when applied to the data of one homogeneous relation matrix representing a graph affinity matrix, it provides a simple and new graph partitioning al-gorithm.
 Based on Algorithms 1 and 2, there is another version of EF-MMRC, i.e., we may combine soft and hard EF-MMRC together to have mixed EF-MMRC. For example, we first run hard EF-MMRC several times as initialization, then run soft EF-MMRC. Algorithm 2 Hard MMRC Algorithm Input: A relational data set tial family distributions (Bregman divergences) assumed for the data set.
 Output: Cluster indicator matrices { C ( j ) } m j =1 , attribute expectation matrices {  X  ( j ) } m j =1 , homogeneous relation ex-Method: 1: Initialize the parameters as  X   X  = 2: repeat 3: { E-step } 4: Based on the current parameters, reassign cluster la-5: { M-step } 6: for j = 1 to m do 7: Compute  X  ( j ) using update rule (27). 8: Compute  X  ( j ) using update rule (28). 9: for i = 1 to m do 10: Compute  X  ( ij ) using update rule (29). 11: end for 12: end for 13:  X   X  =  X  14: until convergence
In this section we discuss the connections between exist-ing clustering approaches and the MMRF model and EF-MMRF algorithms. By considering them as special cases or variations of the MMRF model, we show that MMRF pro-vides a unified view to the existing clustering approaches from various important data mining applications.
Recently, semi-supervised clustering has become a topic of significant interest [4, 46], which seeks to cluster a set of data points with a set of pairwise constraints.

Semi-supervised clustering can be formulated as a special case of relational clustering, clustering on the single-type re-lational data set consisting of attributes F and homogeneous relations S . For semi-supervised clustering, S pq denotes the pairwise constraint on the p th object and the q th object. [4] provides a general model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs). We show that it can be formulated as a special case of MMRC model. As in [4], we define the homogeneous relation matrix S as follows, where M denotes a set of must-link constraints; C denotes a set of cannot-link constraints; f M ( x p , x q ) is a function that penalizes the violation of must-link constraint; f C ( x p is a penalty function for cannot-links. If we assume Gibbs distribution [41] for S , where z 1 is the normalization constant. Since [4] focuses on only hard clustering, we omit the soft member parameters in the MMRC model to consider hard clustering. Based on Eq.(30) and Eq.(4), the likelihood function of hard semi-supervised clustering under MMRC model is Since C is an indicator matrix, Eq. (31) can be formulated as L ( X  | F ) = 1 The above likelihood function is equivalent to the objec-tive function of semi-supervised clustering based on HMRFs [4]. Furthermore, when applied to optimizing the objective function in Eq.(32), hard MMRC provides a family of semi-supervised clustering algorithms similar to HMRF-KMeans in [4]; on the other hand, soft EF-MMRC provides new and soft version semi-supervised clustering algorithms.
Co-clustering or bi-clustering arise in many important ap-plications, such as document clustering, micro-array data clustering.A number of approaches [12, 8, 33, 2] have been proposed for co-clustering. These efforts can be generalized as solving the following matrix approximation problem [34], is the relation representative matrix, and D is a distance function. For example, [12] uses KL-divergences as the dis-tance function; [8, 33] use Euclidean distances.

Co-clustering is equivalent to clustering on relational data of one heterogeneous relation matrix R . Based on Eq.(9), by omitting the soft membership parameters, maximizing log-likelihood function of hard clustering on a heterogeneous relation matrix under the MMRC model is equivalent to the minimization in (33). The algorithms proposed in [12, 8, 33, 2] can be viewed as special cases of hard EF-MMRC. At the same time, soft EF-MMRC provides another family of new algorithms for co-clustering.

Our previous work [34] proposes the relation summary network model for clustering k-partite graphs, which can be shown to be equivalent on clustering on relational data of multiple heterogeneous relation matrices. The proposed algorithms in [34] can also be viewed as special cases of the hard EF-MMRC algorithm.
Graph clustering (partitioning) is an important problem in many domains, such as circuit partitioning, VLSI design, task scheduling. Existing graph partitioning approaches are mainly based on edge cut objectives, such as Kernighan-Lin objective [30], normalized cut [42], ratio cut [7], ratio association[42], and min-max cut [13].

Graph clustering is equivalent to clustering on single-type relational data of one homogeneous relation matrix S . The log-likelihood function of the hard clustering under MMRC model is  X  D  X  ( S , ( C ) T  X  C ). We propose the following theo-rem to show that the edge cut objectives are mathematically equivalent to a special case of the MMRC model. Since most graph partitioning objective functions use weighted indica-tor matrix such that CC T = I k , where I k is an identity matrix, we follow this formulation in the following theorem.
Theorem 2. With restricting  X  to be the form of r I k for r &gt; 0 , maximizing the log-likelihood of hard MMRC cluster-ing on S under normal distribution, i.e., is equivalent to the trace maximization where tr denots the trace of a matrix.
 Proof. Let L denote the objective function in Eq. (34). The above deduction uses the property of trace tr( XY ) = tr( YX ). Since tr( S T S ), r and k are constants, the maxi-mization of L is equivalent to the maximization of tr( CSC The proof is completed.
 Since it is shown in the literature [10] that the edge cut ob-jectives can be formulated as the trace maximization, The-orem 2 states that edge-cut based graph clustering is equiv-alent to MMRC model under normal distribution with the diagonal constraint on the parameter matrix  X . This con-nection provides not only a new understanding for graph partitioning but also a family of new algorithms (soft and hard MMRC algorithms) for graph clustering.

Finally, we point out that MMRC model does not ex-clude traditional attribute-based clustering. When applied to an attribute data matrix under Euclidean distances, hard MMRC algorithm is actually reduced to the classic k-means; soft MMRC algorithm is very close to the traditional mix-ture model EM clustering except that it does not involve mixing proportions in the computation.

In summary, MMRC model provides a principal frame-work to unify various important clustering tasks includ-ing traditional attributes-based clustering, semi-supervised clustering, co-clustering and graph clustering; soft and hard EF-MMRC algorithms unify a number of stat-of-the-art clus-tering algorithms and at the same time provide new solu-tions to various clustering tasks.
This section provides empirical evidence to show the ef-fectiveness of the MMRC model and algorithms. Since a number of stat-of-the-art clustering algorithms [12, 8, 33, 2, 3, 4] can be viewed as special cases of EF-MMRC model and algorithms, the experimental results in these efforts also illustrate the effectiveness of the MMRC model and algo-rithms. In this paper, we apply MMRC algorithms to tasks of graph clustering, bi-clustering, tri-clusering, and cluster-ing on a general relational data set of all three types of infor-mation. In the experiments, we use mixed version MMRC, i.e., hard MMRC initialization followed by soft MMRC. Al-though MMRC can adopt various distribution assumptions, due to space limit, we use MMRC under normal or Poisson distribution assumption in the experiments. However, this Table 1: Summary of relational data for Graph Clus-tering.
 Figure 2: NMI comparison of SGP, METIS and MMRC algorithms. does not imply that they are optimal distribution assump-tions for the data. How to decide the optimal distribution assumption is beyond the scope of this paper.
 For performance measure, we elect to use the Normalized Mutual Information (NMI) [44] between the resulting cluster labels and the true cluster labels, which is a standard way to measure the cluster quality. The final performance score is the average of ten runs.
In this section, we present experiments on the MMRC algorithm under normal distribution in comparison with two representative graph partitioning algorithms, the spectral graph partitioning (SGP) from [36] that is generalized to work with both normalized cut and ratio association, and the classic multilevel algorithm, METIS [28].

The graphs based on the text data have been widely used to test graph partitioning algorithms [13, 11, 25]. In this study, we use various data sets from the 20-newsgroups [32], WebACE and TREC [27], which cover data sets of different sizes, different balances and different levels of difficulties. The data are pre-processed by removing the stop words and each document is represented by a term-frequency vector using TF-IDF weights. Then we construct relational data for each text data set such that objects (documents) are related to each other with cosine similarities between the term-frequency vectors. A summary of all the data sets to construct relational data used in this paper is shown in Table 1, in which n denotes the number of objects in the relational data, k denotes the number of true clusters, and balance denotes the size ratio of the smallest clusters to the largest clusters.

For the number of clusters k , we simply use the number of the true clusters. Note that how to choose the optimal number of clusters is a nontrivial model selection problem and beyond the scope of this paper.

Figure 2 shows the NMI comparison of the three algo-rithms. We observe that although there is no single winner on all the graphs, overall the MMRC algorithm performs better than SGP and METIS. Especially on the difficult data set tr23, MMRC increases performance about 30%. Hence, MMRC under normal distribution provides a new graph partitioning algorithm which is viable and competi-Table 3: Taxonomy structures of two data sets for constructing tri-partite relational data Figure 3: NMI comparison of BSGP, RSN and MMRC algorithms for bi-type data. tive compared with the two existing state-of-the-art graph partitioning algorithms. Note that although the normal dis-tribution is most popular, MMRC under other distribution assumptions may be more desirable in specific graph clus-tering applications depends on the statistical properties of the graphs. In this section, we apply the MMRC algorithm under Poisson distribution to clustering bi-type relational data, word-document data, and tri-type relational data, word-document-category data. Two algorithms, Bi-partite Spec-tral Graph partitioning (BSGP) [11] and Relation Summary Network under Generalized I-divergence (RSN-GI) [34], are used as comparison in bi-clustering. For tri-clustering, Con-sistent Bipartite Graph Co-partitioning (CBGC) [18] and RSN-GI are used as comparison.

The bi-type relational data, word-document data, are con-structed based on various subsets of the 20-Newsgroup data. We pre-process the data by selecting the top 2000 words by the mutual information. The document-word matrix is based on tf.idf weighting scheme and each document vector is normalized to a unit L 2 norm vector. Specific details of the data sets are listed in Table 2. For example, for the data set BT-NG3 we randomly and evenly sample 200 documents from the corresponding newsgroups; then we formulate a bi-type relational data set of 1600 document and 2000 word.
The tri-type relational data are built based on the 20-newsgroups data for hierarchical taxonomy mining. In the field of text categorization, hierarchical taxonomy classifica-Figure 4: NMI comparison of CBGC, RSN and MMRC algorithms for tri-type data. tion is widely used to obtain a better trade-off between effec-tiveness and efficiency than flat taxonomy classification. To take advantage of hierarchical classification, one must mine a hierarchical taxonomy from the data set. We see that words, documents, and categories formulate a sandwich structure tri-type relational data set, in which documents are central type nodes. The links between documents and categories are constructed such that if a document belongs to k cate-gories, the weights of links between this document and these k category nodes are 1 /k (please refer [18] for details). The true taxonomy structures for two data sets, TP-TM1 and TP-TM2 , are documented in Table 3.

Figure 3 and Figure 4 show the NMI comparison of the three algorithms on bi-type and tri-type relational data, re-spectively. We observe that the MMRC algorithm performs significantly better than BSGP and CBGC. MMRC per-forms slightly better than RSN on some data sets. Since RSN is a special case of hard MMRC, this shows that mixed MMRC improves hard MMRC X  X  performance on the data sets. Therefore, compared with the existing stated-of-the-art algorithms, the MMRC algorithm performs more effec-tively on these bi-clustering or tri-clustering tasks and on the other hand, it is flexible for different types of multi-clustering tasks which may be more complicated than tri-type clustering.
We also run the MMRC algorithm on the actor-movie re-lational data based on IMDB movie data set for a case study. In the data, actors are related to each other by collabora-tion (homogeneous relations); actors are related to movies by taking roles in movies (heterogeneous relations); movies have attributes such as release time and rating (note that there is no links between movies). Hence the data have all the three types of information. We formulate a data set of 20000 actors and 4000 movies. We run experiments with k = 200. Although there is no ground truth for the data X  X  cluster structure, we observe that most resulting clusters that are actors or movies of the similar style such as action, or tight groups from specific movie serials. For example, Ta-ble 4 shows cluster 23 of actors and cluster 118 of movies; the parameter  X  23 , 118 shows that these two clusters are strongly related to each other. In fact, the actor cluster contains the actors in the movie series  X  X he Lord of the Rings X . Note that if we only have one type of actor objects, we only get the actor clusters, but with two types of nodes, although there is no links between the movies, we also get the related movie clusters to explain how the actors are related.
In this paper, we propose a probabilistic model for re-lational clustering, which provides a principal framework to unify various important clustering tasks including tradi-tional attributes-based clustering, semi-supervised cluster-ing, co-clustering and graph clustering. Under this model, we propose parametric hard and soft relational clustering algorithms under a large number of exponential family dis-tributions. The algorithms are applicable to relational data of various structures and at the same time unify a number of stat-of-the-art clustering algorithms. The theoretic analy-sis and experimental evaluation show the effectiveness and great potential of the proposed model and algorithms.
