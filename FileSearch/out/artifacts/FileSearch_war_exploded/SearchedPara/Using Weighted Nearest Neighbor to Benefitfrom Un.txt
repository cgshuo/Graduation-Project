 The combination of supervised and unsupervised learning [1] is a growing sub-field of Machine Learning. Applications such as text-or image-mining and molec-ular profiling have revealed application areas that yield very little (and often expensive) labeled data but often plenty of unlabeled data. As traditional ma-chine learning algorithms are not able to use and benefit from the information available in the unlabeled data, custom built algorithms should be able to outper-form them. Current research in semi-supervised learning using algorithms such as Co-Training [2] or more recent approache s based on graph representations [3] confirms that this is indeed possible.

Most of the semi-supervised learning approaches use the labeled and unlabeled data simultaneously or at least in close collaboration. Roughly speaking, the unlabeled data provides information about the structure of the domain, i.e. it helps to capture the underlying distribution of the data. The challenge for the algorithms can be viewed as realizing a k ind of trade-off between robustness and information gain [1]. To make use of unlabeled data, one must make assumptions, either implicitly or explicitly. As reported in [3], the key to semi-supervised learning is the prior assumption of consistency, that allows for exploiting the geometric structure of the data distribution. Close data points should belong to the same class and decision boundaries should lie in regions of low data density; this is also called the  X  X luster assumption X .

In this paper, we introduce a very simple two-stage approach that uses the available unlabeled data to improve on the predictions made when learning only from the labeled examples. In a first stage , it uses an off-the-shelf classifier to build a model based on the small amount of available training data, and in the second stage it uses that model to transform the available unlabeled data into a weighted  X  X re-labeled X  data-set that together with the original data is used in a nearest neighbor classifier. We will show that the proposed algorithm improves on the classifier built in stage 1, especially in cases where much more unlabeled data is available compared to the labeled data.

The rest of the paper is structured as follows: in section 2 we describe a few related semi-supervised learning techniques. Section 3 introduces the proposed algorithm in detail. In section 4 we show experimental results using an array of different classifiers used in the first stage. Section 5 concludes and presents some directions for future work. Early methods in semi-supervised learning were using mixture models (in which each mixture component represents exac tly one class) and extensions of the EM algorithm [4]. More recent approaches bel ong to one of the following categories: self-training, co-training, transductive SVMs, split learning, and graph-based methods. In the self-training approach, a classifier is trained on the labeled data and then used to classify the unlabeled ones. The most confident (now labeled) unlabeled points are added to the training set, together with their predictive labels, and the process is repeated unt il convergence [5]. Approaches based on co-training [2] assume that the features d escribing the objects can be divided in two subsets such that each of them is sufficient to train a good classifier, and that the two sets are conditionally independent given the class attribute. Two classifiers are iteratively trained, each on one set, and they teach each other with a respective subset of unlabeled data and their highest confidence predictions. The transductive SVMs [6] are a  X  X atural X  extension of SVMs to the semi-supervised learning scheme. They aim at finding a labeling of the unlabeled data so that the decision boundary has a maximum margin on the original labeled data and on the (newly labeled) unlabeled data.

Graph-based methods attempt to capture the underlying structure of the data with a graph whose vertices are the available data (both labeled and un-labeled) and whose (possibly weighted) edges encode the pairwise relationships among this data. Ex amples of recent work in that d irection include Markov ran-dom walks [7], cluster kernels [8], and regularization on graphs [3]. The learning problem on graphs can gener ally be viewed as an estimation problem of a classi-fying function f which should be close to a given function y on the labeled data and smooth on the whole graph. Different graph-based methods mainly vary by their choice of the loss function and the regularizer [9]. For example, the work on graph cuts [10] minimizes the cost of a cut in the graph for a two-class problem, while [11] minimizes the normalized cut cost and [12, 3] minimize a quadratic cost. As noticed in [9], these differences are not actually crucial. What is far more important is the construction and quality of the graph, which should re-flect domain knowledge through the similarity function used to assign edges and their weights.

Collective classification [13] is an ILP approach that uses the relational struc-ture of the combined labeled and unlabeled data-set to enhance classification accuracy. With relational approaches, the predicted label of an example will often be influenced by the labels of related examples. The idea behind collec-tive classification is that the predicted labels of a test-example should also be influenced by the predictions made for related test-examples. The algorithm pre-sented in this paper is closely related to this, but works on non-relational data by using a distance and the nearest neighbor relation that results from it.
Also related to our approach, although originally not used in a transductive setting, is the work by [14]. Also using two stages, in the first stage an ensemble of neural networks is trained on the available data and the resulting model is used to generate random, extra training examples for a decision tree algorithm in the second stage. This approach could be easily adapted to the transductive setting by using the test set instead of randomly generated examples. The Yatsi algorithm 1 that we present in this paper will incorporate ideas from different algorithms that were discussed in the previous section. Since we really like the idea of giving the user the option to choose from a number of machine learning algorithms (like it is possible in co-training), we will develop a technique that builds on top of any standard machine learning algorithm. To incorporate the general idea behind collective classi fication, we use a nearest neighbor ap-proach and the distance between as a way of relating them to each other.
The Yatsi classifier (See Algorithm 1 for high-level pseudo-code) uses both labeled and unlabeled data in a two-stage set-up 2 . In the first stage a standard, off-the-self, classifier (or regression-algorithm) is trained on the available training data. Since this kind of data is limited in the specific application areas we are looking at, it is best to choose an algorithm that can learn a model well using only a small amount of learning data.

In the second stage, the model generat ed from the learning data is used to  X  X re-label X  all the examples in the test se t. These pre-labeled examples are then Algorithm 1 High level pseudo code for the two-stage Yatsi algorithm. used together with the original training data in a weighted nearest neighbor algorithm. The weights used by the nearest neighbor classifier are meant to limit the amount of trust the algorithm puts into the labels generated by the model from the first step. As a default value, we set the weights of the training data to 1 . 0 and the weights of the pre-labeled test-data to N/M with N the number of training examples and M the number of test-examples. Conceptually, this gives equal weights to the whole train-and the whole test-set. By adding a parameter F to the algorithm that will cause the weight of the test-examples to be set to F  X  ( N/M ), it becomes possible to vary the influence one wants to give to the unlabeled data and the classifier built in step 1. Values of F between 0 . 0 and 1 . 0 will lower the influence on the test-data and the learned model from the first step, values larger than 1 . 0 will increase their influence. In the experiments, we will test values ranging from 0 . 01 to 10. An F -value of 10.0 will adjust the weights of the individual examples such as to give the total test-set 10 times the weight of the total training set. 3.1 Weighted Nearest Neighbor In the previous section we stated the wa y we add a label and a weight to every example in the dataset that will be used for nearest neighbor classification. There are different ways in which to use weights fo r nearest neighbor classification. One way is to make the distance dependent on the weight of the examples. An obvious way would be to divide the standard distance by the weight of the example [15]. This would make it harder for examples with a small weight to influence the prediction. However, when using k-nearest-neighbor prediction, this approach will change the identity of the k selected examples and in a set-up like the one provided by Yatsi , where only 2 different weights are available, it could prevent the examples with the lower weight to ever be part of the k closest examples.
Another way of incorporating weights in nearest neighbor predictions is that once the k nearest neighbors are select ed, we choose to use the weights of the examples as a measure for their influence on the total vote. Instead of counting the number of neighbors that belong to each class, we sum their weight and predict the class with the largest weight. By normalizing the sums of the weights, so that they all add up to 1, we get an indication of the probability for each of the available classes. Note though, that the distance to an example does not influence its contribution in the vote. Once an example makes it into the set of the k closest examples, its contribution is only influenced by its weight.
For continuous class-values, where predictions are made using the sum over all examples in the dataset with t j being the target value of example j and dist ij being the distance between examples i and j , both ways of incorporating the weights of examples are equivalent. As such, although we have not yet im-plemented this and do not have any experimental results, Yatsi can be used for predicting continuous target values as well without major changes. 3.2 Other Nearest Neighbor Issues For our experiments, we fixed the number of nearest neighbor to 10. This is not a requirement for the Yatsi algorithm. Cross-validation on the labeled training examples could be used to adapt the number of nearest neighbors. However, the resulting values of k might be misleading because of the large amount of extra examples that will be available in the second step of the Yatsi algorithm.
Since the algorithm is designed to work in applications where the amount of labeled training data is limited, one can get away with less efficient algorithms in the first step. As we expect the amount of test data to greatly exceed that of the training data, most of the computational complexity will lie in the search for nearest neighbors, as this search spans the combined sets of examples.
Yatsi will therefore greatly benefit from using efficient nearest neighbor search algorithms. Currently, we use KD-trees [16] to speed up the nearest neighbor search. However, recently a lo t of research effort has gone into the development of more efficient search strategies for nearest neighbors, which can be directly applied to the Yatsi algorithm. Examples of such search strategies are cover trees [17] and ball trees [18]. We evaluated Yatsi using a number of datasets from the UCI-repository. We created labeled and unlabeled sets by sp litting the available data into randomly chosen subsets. We ran experiments with 1%, 5%, 10% and 20% of the available data labeled (the training set) and the rest available as the test-set. In general, we collected results from 29 different dat a set, except for the 1%-99% case split, where the 8 smallest data-set were removed because a 1% sub-set was not large enough to train a classifier on.

The design of Yatsi does not specify any specific algorithm to be used in the first step. We ran experiments with an array of algorithms that are all available in WEKA consisting of: AdaBoostM1 : This is a straightforward implementation of the AdaBoostM1 J48 : This is Weka X  X  reimplementation of the original C4.5 algorithm. Default Logistic : A straightforward implementation of logistic regression run with RandomForest : An implementation of Breiman X  X  RandomForest algorithm, SMO : Weka X  X  implementation of the SMO algorithm for training support vector IB1 : A standard nearest-neighbor algorithm using Euclidean distance with all
We also collected results f or different values of the weighting parameter F ranging from 0 . 1, i.e., giving 10 times as much weight to the training set as to the test-set, to 10 . 0 which does the exact opposite. We also ran some experiments that used no weights at all. These values used for the weighting parameter are a bit extreme but will give a good illustration of the behavior of the Yatsi algo-rithm. These experiments tr eat all the  X  X re-labeled X  te st-set examples exactly like training examples. Therefore, in the 1%-99% split case, the total weight of the test-set would be almost 100 times as big as that of the training-set.
We expect the performance of Yatsi to go down with the performance of the classifier trained on the labeled data in stage 1 as the amount of available training data decreases, but we expect (and will show) that the performance degrades slower, i.e., that Yatsi is able to improve on the results obtained by only learning from the labeled data. To get statistically sound results, we repeated every experiment 20 times.

Table 1 shows the number of statistically significant wins, draws and losses of Yatsi versus the classifier trained on the training-set in stage 1. For J48, we show the results for the experiment with the confidence set to 0 . 75. This is higher than normal so this setting generates slightly larger trees, which seems to be appropriate for the very small training sets that we use. Higher levels of pruning could even lead to empty trees in extreme cases. Overall, all the J48 experiments showed the same trend. The results shown for the RandomForest experiments are those with an ensemble size of 100. The ones with ensemble size 10 were similar with a slightly bigger advantage for Yatsi . On the SMO experiments, we show the results for the lin ear kernel experiments. For quadratic and cubic kernels, Yatsi produces less of an advantage, mostly due to the fact that the SMO predictions get better and Yatsi is not able to improve on them, but performs equal to the SMO algorithm more often. For AdaBoost, the shown results are obtained with the standard settings for J48; a range of different parameter values for AdaBoost produced almost identical results.

Overall, the results show that Yatsi often improves on the results of the base classifier. Especially when very little of the data is labeled, Yatsi gains a lot from having the unlabeled data availabl e. When the percentage of labeled data increases, Yatsi loses some of its advantage, but for the most part performs comparable if not better than the base classifier. The exception seems to be when one uses Random Forests. The weighted nearest neighbor approach of Yatsi loses some of the accuracy obtained by voting over the ensemble of trees.
To give more of an indication of the actual improvements reached by Yatsi in terms of predictive accuracy, Table 2 s hows the actual predictive accuracies from the experiments with 5%-95% splits when one uses J48 as the classifier in stage 1. To gain additional insights into the results, we compared error rates for J48 and Yatsi (J48) using different values for the weighting parameter F and with the percentage of labeled examples varying between 1% and 20% 3 . General trends are obvious, like the fact that more labels usually lead to globally better results, or that with a very small number of labels J48 usually performs worse than Yatsi but that J48 can outperform Yatsi when given more labeled data. With regard to the weighting parameter F we see that values of 0 . 1and1 . 0 consistently perform better than a value of 10 or without using weights, which indicates the advantage of taking a cautious approach that puts more trust into the originally supplied labels over the labels generated by the first stage classifier.
As already stated, all previous experiments were run with the number of near-est neighbors for the second stage fixed to 10. Because of the use of weights and the large difference in weights between training and test examples, we thought it might make sense to use a larger number of nearest neighbors, so we also per-formed experiments with 20 and 50 neares t neighbors in the 1% labeled training data case. Overall, these e xperiments showed very little difference with the 10 nearest neighbor ones. When there was a d ifference, there was a little improve-ment for low values of F (0.1 or 1.0) and a small loss for the cases where a high weight was given to the test-examples ( F =10 . 0 or no weights used at all). We have presented a simple two-stage idea that benefits from the availability of unlabeled data to improve on predictiv e accuracies of standard classifiers. Yatsi uses an off-the-shelf classification or regression algorithm in a first step and uses weighted nearest neighbor on the combined set of training data and  X  X re-labeled X  test data for actual predictions. Experim ental results obtained from both a large array of different classifiers used in the first step, different amounts of available unlabeled data and a relatively large selection of data-sets show that Yatsi will usually improve on or match the predictive performance of the base classifier used generated in the first stage. These improvements are largest in cases where there is a lot more unlabeled data available than there is labeled data.
The Yatsi algorithm in its current form is quite simple and therefore a num-ber of further improvements are possi ble. Some ideas hav e already been pre-sented in section 3 such as the inclusion of a more efficient nearest neighbor search algorithm or the use of cross validation to determine the best number of nearest neighbors to use. Also, the curre nt weighting scheme does not allow the user to stress the relative importance o f different classes. Appropriate weight-ing schemes for cost-sensitive settings could be easily integrated into the Yatsi algorithm. More elaborate extensions could include some sort of EM-algorithm that tries to match the  X  X re-labels X  of test-examples with the eventually pre-dicted values. Distance functions different to simple Euclidean distance could encode specialized domain knowledge and thus help improving classification per-formance. These directions would relate Yatsi more closely to both graph-based and kernel-based methods of semi-supervised learning.

