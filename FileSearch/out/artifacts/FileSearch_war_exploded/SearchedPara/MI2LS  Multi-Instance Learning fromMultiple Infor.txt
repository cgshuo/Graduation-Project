 In Multiple Instance Learning (MIL), each entity is normally ex-pressed as a set of instances. Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are of-ten described from several different information sources/views. For example, when applying MIL to image categorization, the charac-teristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in tradi-tional learning methods, leveraging the consistencies between dif-ferent information sources could improve the classification perfor-mance drastically.

Out of a similar motivation, to incorporate the consistencies be-tween different information sources into MIL, we propose a novel research framework  X  Multi-Instance Learning from Multiple In-formation Sources (MI 2 LS). Based on this framework, an algo-rithm  X  Fast MI 2 LS (FMI 2 LS) is designed, which combines Con-straint Concave-Convex Programming (CCCP) method and an adap-ted Stoachastic Gradient Descent (SGD) method. Some theoreti-cal analysis on the optimality of the adapted SGD method and the generalized error bound of the formulation are given based on the proposed method. Experimental results on document classification and a novel application  X  Insider Threat Detection (ITD), clearly demonstrate the superior performance of the proposed method over state-of-the-art MIL methods.
 I.2.6 [ Artificial Intelligence ]: Learning  X  Knowledge acquisition Algorithms, Performance, Experimentation Multi-View Learning, Multi-Instance Learning, Stoachastic Gradi-ent Descent
Traditional learning methods normally treat each example as a non-separable entity, and represent the example by one feature vec-tor. However, the semantic meanings of each individual example could vary among its constituent parts, rather than being consistent throughout the whole content. As one variation of traditional learn-ing methods, Multiple Instance Learning (MIL) [11] has been pro-posed to solve the label ambiguity problem. In particular, in MIL, each example/bag is divided into several different parts/instances. The labels are assigned to the bags, rather than individual instances. In this way, the features for the desired local object in each exam-ple will be less likely affected by its irrelevant parts, and therefore the learned model can be more accurate. A lot of work has been done for MIL classification [2, 11, 14, 23, 32, 46] and its variants, such as outlier detection [44], online learning [3], and ranking [19]. These methods have been widely employed in applications such as text mining [2], drug design [11], localized content based image retrieval (LCBIR) [32], human action recognition [1] and market targeting [46].

Most of the current MIL methods focus merely on solving prob-lems where examples are described by only one set of features. However, in many real-world applications, examples are often de-rived from several different information sources/views, and there-fore are represented by multiple sets of features. For example, in webpage classification, each webpage has disparate descriptions such as in-bound, out-bound links and textual content. In image retrieval, each image can be described by different kinds of fea-tures, such as RGB features, SIFT features [27], and texture fea-tures. Different sets of features normally have different statistical properties. As shown in previous studies in multi-view learning work [5, 12, 22, 25, 35, 39, 50], by leveraging the consistencies between different views, the classification performance can be im-proved. Therefore, designing a MIL algorithm that incorporates information from multiple sources is also expected to bring in per-formance improvements.

The existing research in this direction is rare. In [31], the au-thors did some experiments by using MIL on different views sep-arately and then combined them with equal weights. This method is straightforward. However, it does not consider the consisten-cies between different views. On the contrary, in this paper, to integrate the consistencies into MIL, a novel framework  X  Multi-Instance Learning from Multiple Information Sources (MI 2 proposed. From the MIL perspective, MI 2 LS integrates the na-ture of the multi-view setting into the MIL framework and impose the consistencies among multiple views. From multi-view learn-ing perspective, the new formulation explicitly handles the prob-lem of label ambiguity through modeling different segments of examples. More precisely, the new framework aims at designing classifiers for MIL on individual views and constraining the con-sistencies between these classifiers simultaneously. Based on the proposed framework, a concrete optimization formulation is sug-gested. However, the proposed formulation is non-convex and con-tains too many constraints derived on both the bag and the instance levels. Therefore, to solve the resulting optimization problem, we propose a novel method  X  Fast MI 2 LS (FMI 2 LS), which is a com-bination of Constrained Concave-Convex Procedure (CCCP) and Stochastic Gradient Descent (SGD). We prove that the proposed method is guaranteed to converge with some derived convergence bounds. Furthermore, the generalized error bound of the proposed method is analyzed. To show the effectiveness and efficiency of the proposed method, in the experiment part, a series of experiments are conducted on two benchmark text datasets, Reuters21578, We-bKB, as well as a newly introduced application of MIL  X  Insider Threat Detection (ITD). In this new application, MIL is employed to find the potential harmful insiders through analyzing their online behaviors, where the features during each time period is modeled as a bag and each bag contains instances derived from daily fea-tures. The different views in ITD indicate different types of online behaviors. Experimental results on this application and the two text datasets clearly demonstrate the advantages of our proposed method over state-of-the-art techniques.

The rest of this paper is organized as follows. Section 2 intro-duces the related work. Section 3 proposes the research problem and presents the proposed algorithm. Some theoretical analysis are given in Section 4. Section 5 presents the experimental results. Section 6 concludes the whole paper.
The concept of MIL was first introduced by Dietterich et al. [11] for predicting musk molecular. Since then, numerous research work has been done in MIL. Roughly speaking, MIL methods can be separated into three groups, (1) the group that is specifically de-signed to solve MIL [11, 29]; (2) the group that converts MIL to traditional single-instance problems and solve the resulting prob-lem through traditional learning methods [8, 9]. (3) the group that revises traditional single-instance learning methods by imposing MIL constraints [2, 7, 15, 16, 23, 24].

For the first group, APR [11], which encloses positive instances by an axis-parallel rectangle in the feature space, is the first method to solve MIL problems. Later, Maron and Lozano-P X rez proposed Diverse Density (DD) [29, 30], which tries to identify the con-cept point that resembles positive instance most, and classify un-labeled bags according to the distances between the instances in these bags and this concept point. In [33, 49], the authors acceler-ated DD method by applying Expectation-Maximization (EM), and proposed EM-DD.

In the second group, DD-SVM [9] picks a set of prototypes among the local solutions from DD method returned by different initializations and then design a large margin classifier based on the bag level features extracted from these selected prototypes. In [8], the authors embedded bags into a feature space spanned by instances, and apply 1-norm SVM to build the bag level classifiers.
Most of the MIL methods fall into the third group. Andrew et al.[2] proposed two different MIL formulations based on SVM [6], i.e., misvm for the instance level classification and MISVM for the bag level classification. Since the MIL formulations are non-convex, Gehler and Chapelle tried to use deterministic annealing and achieved better local solutions [16]. G X rtner et al. [15] put forward a kernel function directly based on bags. Later, Kwok and Cheung [24] advanced their work through proposing a marginal-ized MIL kernel and converting the MIL from an incomplete data problem to a complete data problem. In [7], the authors revised the loss functions of single-instance SVM and focus more on the pos-itive bags with smaller sizes. To improve the efficiency of misvm and MISVM, bundle method is adapted to solve the non-convex optimization problem [4]. Furthermore, some research work incor-porates the MIL constraints into gaussian process [23] and condi-tional random fields [10]. In [20, 26, 48, 51], the multi-instance multi-label problem has also attracted a lot of attentions, in which the labels are not restricted to be binary, but can be a vector. More-over, some other variants of MIL are also proposed, such as multi-instance outlier detection [44], multi-instance online learning [3] and multi-instance ranking [19].
 The previous research work is reasonable, and solves emerging MIL problems from different perspectives. However, few of them considered the case when examples are derived from multiple in-formation sources, while the previous work on traditional single instance learning methods has demonstrated superior performances of methods that consider the consistencies between different infor-mation sources over the ones that do not. Out of this motivation, the proposed framework MI 2 LS integrates the consistencies between different sources into a unified framework for MIL, and Fast MI is proposed to solve the suggested formulation in an efficient and effective way.
In a lot of real-world applications, examples are usually extracted from multiple information sources/views. It has been shown ex-tensively in prior research that utilizing the consistency between the multiple sources/views could achieve better performance [5, 12, 22, 25, 35, 39, 45, 47, 50]. In particular, one of the earliest work in multi-view learning is [5], in which the authors propose the co-training method to solve problems where the examples are described by two distinct views. In [12], the authors build clas-sifiers on different views and constrain the consistencies between different classifiers on each individual view. Moreover, they show that the Rademacher complexity of the function class can also be greatly reduced by regulating the consistencies.

This idea is further exploited in [25], in which the consistency term is incorporated into multi-view semi-supervised learning prob-lems, and it has shown a substantial improvement on the classifica-tion performance. Likewise, in [47], the authors introduce the con-sistency into local learning [43] and design a novel way to define the graph Laplacian. When applied to transfer learning [17, 45], imposing the consistencies between different views also shows su-perior performances in transferring the knowledge between differ-ent domains. Most existing multi-view learning methods are for the single instance settings, while MIL problem naturally exists in real world applications. So, different from the prior work, in this paper, the view consistency constraint is further applied to MIL problems, such that the label ambiguity problem in multi-view learning can be handled in a more principled way.
Suppose a set of n labeled bags: D = { ( B i , Y i ) ,i = 1 ,...,n } are available for training, where B i represents the i -th bag and Y i  X  { 1 ,  X  1 } is its binary label. The bag B i consists of a set of instances, and each instance is described by different views. In particular, the p -th view of instances in the i -th bag B the dimensionality of the p -th view. n i is the number of instances in the i -th bag and M is the total number of views. The objec-tive of Multi-Instance Learning from Multiple Information Sources (MI 2 LS) is to design a function f : B  X  { 1 ,  X  1 } by integrat-ing the consistencies between different views into MIL, such that classification on the unlabeled bags could be accurate.
We aim to leverage the instances derived from different informa-tion sources (views) and their labels simultaneously. The general framework of MI 2 LS is as follows: where  X ( w (1) ,..., w ( M ) ) is regularizer that depicts the capacity of the classifiers on different views, L c ( D , w (1) ,..., w resents the classification loss on the different views given by the classifiers, L a ( D , w (1) ,..., w ( M ) ) measures the consistencies of the classifiers on different views based on the corresponding classi-fication outputs. Since in MIL the outputs can be measured on both the instance level and the bag level, L a ( D , w (1) ,..., w also be defined on the bag level, the instance level or on both of the two levels. Through incorporating these three components, the pro-posed framework ensures that the classification on each individual view should be accurate enough and the output of each individual instance or bag given by the classifiers on different views should be consistent.

Following this framework and considering the case when fea-tures are derived from two views without the loss of generality ( M = 2 ), there are multiple ways of formulating the three dif-ferent terms. For the first part, one of the possible options to de-plied to L c ( D , w (1) ,..., w ( M ) ) similar to most large marge meth-ods. The -insensitive loss is used to define L a ( D , w (1) which requires the inconsistency between different views of each instance be within error bound and penalizes the discrepancy be-
In MI 2 LS, the instances on different views could be derived from different partition ways and the numbers of instances in the same bag could be different on different views. Here, we do not con-sider this case out of simplicity. As we shall see later, the proposed framework could handle this situation by imposing consistencies on the bag level.
In the proposed method, for simplicity, we only consider the case when the consistency is defined on the instance level. If the consis-tency is defined on the bag level, then the last constraint of problem (1) can be re-written to restrict the differences between the outputs of each bag on different views in a similar way. The resulting opti-mization problem can be solved using a similar method as the one proposed in this paper. If the consistency is defined on both of the two levels, the constraint can be considered as a combination of the bag and instance level consistencies. yond this bound. Then, a concrete formulation can be given as: min where N = P n i =1 n i , C (1) , C (2) and C are trade-off parameters tuning the importances on the classification losses on the corre-sponding views as well as the penalty term that measures the con-sistencies between different views.

The proposed optimization formulation imposed the view con-sistency assumption into the framework of MIL in a reasonable way. However, this is a non-convex optimization problem. So, it cannot be solved directly. Moreover, in many real world problems, the numbers of bags and instances are huge, which would result in a large number of constraints and therefore could drastically in-crease the computational complexity for solving this problem. To deal with this optimization problem efficiently and effectively, a concrete method  X  Fast MI 2 LS (FMI 2 LS) is therefore proposed in the following sections.
For the convenience of computation, without loss of generality, we introduce three concatenated vectors as: where 0 d p is a 1  X  d p zero vector. After this transformation, it converted to the following form:
Compared with problem (1), although this form is simplified, it is still non-convex and contains too many constraints. There are mul-tiple ways of handling the non-convex optimization problems, such as Constrained Concave-Convex Procedure (CCCP) [41], adapted bundle method [13] and deterministic annealing [16]. Due to the popularity of CCCP, we use this method to decompose this non-convex problem into a series of convex sub-problems and focus on the resulting convex subproblems. Furthermore, to reduce the time complexity on solving these subproblems, Stochastic Gradient De-scent (SGD) [37] method is adapted, such that the algorithm can find a local optimal solution in linear scale.
Given a starting point e w (0) 3 , CCCP iteratively computes with their first order Taylor expansions at e w ( t  X  1) . More precisely, for the t -th iteration of CCCP, the derived subproblem for solving problem (2) is: tive instance for the i -th bag on p -th view. Through solving a series of subproblems derived from CCCP, the method is guaranteed to converge to a local optimal solution of problem (2).

The resulting subproblem is convex. However, the cost of di-rectly solving this problem is non-trivial, especially when the num-bers of bags, instances, as well as the resulting constraints for the optimization problem are large. A lot of research work, such as bundle method [21, 40] and SGD method, has been proposed to improve the efficiency of similar optimization problems. In this paper, due to the superior performance, SGD is employed. Differ-ent from the traditional SGD method, in problem (3), we have two different sets of constraints, i.e., the ones on the bags and the ones on instances. The algorithm receives several parameters, i.e., S -the number of SGD iterations to perform; k 1 and k 2 ( k 1 k 2 &lt;&lt; N ) -the number of bags and instances to use for approx-imating the sub-gradients. At the beginning of SGD algorithm for the t -th CCCP iteration, we set e w ( t 0 ) to be e w ( t  X  1) is at most output of SGD for the t -th CCCP iteration is an averaged result of the last corresponding  X S SGD iterations. The averaged result is adopted here because of the superior performance as shown in [34]. For the s -th iteration of the SGD algorithm, we randomly pick a set of bags A s  X  X  1 ,...,n } , and another set of instances B of the instances (as indicated by A s ) in selected bags. By doing so, the computational cost can be reduced on both the bag level and the instance level. More precisely, during each SGD iteration, we replace problem (3) with an approximated convex sub-problem as follows: min = 1 + C , in the corresponding bag given by the classifier e w t 0 , i.e., the in-e w ( t ) represents the result from the t -th CCCP iteration. Here, the superscript t s means the s -th SGD iteration for the t -th CCCP iteration. stances indicated by j  X  p in Eq.(3), and its corresponding label from the selected bags in A s 5 . z i  X  B s represents the instances sampled from all of the instances in selected bags, i.e., A s . It is clear that the subgradient of f ( e w ,A s ,B s ) can be calculated as: where I ( p ) i 1 , I i 2 , I i 3 are indicator functions. I y e w T x ( p ) i &lt; 1 , and otherwise 0 ; I i 2 equals 1 if and otherwise 0 ; I i 3 equals 1 if e w T z (2) i  X  e w T erwise 0 . By setting the step length to be  X  s = 1 s , the updating scheme can be written as e w t s +1 = e w t s  X   X  s  X  X  ( e e w Here, of (4) should fall into, as shown in the later section. The final output for some constant  X   X  (0 , 1) . Based on the above derivation, the whole algorithm can be summarized in Table 1.
In this section, some important properties of the proposed method, such as the optimality and generalized error rate, will be analyzed.
It has already been shown in previous work that the CCCP [41] will converge asymptotically. During each CCCP iteration, SGD is used for solving the resulting convex sub-problem. In this section, we will investigate some important properties of the adapted SGD method, such as the bound of the optimal solution and the differ-ence between the objective function values of e w t S  X  and that of where e w t  X  refers to the optimal value for the t -th iteration.
Theorem 1: Suppose G ( e w ,A s ,B s ) = 1 k max { 0 , 1  X  y i e w T x ( p ) i } + C k , e 2 C U Proof: Please check the Appendix.

Theorem 2: The optimal solution of problem (3) should fall within the ball of Proof: Please check the Appendix.

Theorem 2 justifies the reason why during each SGD iteration, ball falling within this ball.

Theorem 3: For the s -th SGD iteration, the following inequality holds 6 : 1 S
X
To avoid confusion, please note that y i indicates the label for the i -th selected bag from A s , while Y i in the previous formulations refers to the label for the i -th bag in the whole dataset. f s ( e w t s ) here refers to f ( e w t s ,A s ,B s ) in problem (4). z i  X  , ( e w . plugging this result to Corollary 1 of [36], we can get: 1 S X Theorem 4: With probability over the choices of ( A 1 ,...,A and ( B 1 ,...,B S ) , we have that: E [ F ( e w t S  X  )  X  F ( e w t  X  )]  X  where F (  X  ) is the objective function in problem (3).
 plugging this into Theorem 5 of [34], we can get this conclusion.
In this section, we consider the class of functions F C (1) + C (2) ,D { g | g : e B  X  7 X  X  X  1 2 (max j k e w k 2  X  C (1) + C (2) , and with probability of at least 1  X   X  ,
Theorem 5: The empirical Rademacher complexity of the func-(max  X  + max  X  Proof: Please check the Appendix.

Theorem 6: Fix  X   X  (0 , 1) . Then, with probability at least 1  X   X  , +  X 
Proof: This result can be got by applying Theorem 5 to Theorem 4.9 in [38].
In this section, an extensive set of experiments on document clas-sification and a novel application  X  insider threat detection is pre-sented to demonstrate the effectiveness and efficiencies of the pro-posed method.
Reuters21578 7 is a benchmark dataset from Reuters newswire in 1987. It has 135 categories, with 21578 documents. We pick documents from 2 sub-categories as the positive examples. The same amount of documents from the remaining dataset are ran-domly picked as negative ones. In document classification, if a document belongs to a specific category, it is highly possible that not every passage of this document is related to this category. So, it could be better modeled as a MIL problem. More specifically, sim-ilar to [2], we treat each document as a bag and use the different http://daviddlewis.com/resources/textcollections/reuters21578/. complexity issue as stated in Experiment section. complexity issue as stated in Experiment section. fixed-length passages as instances. For each of the sub-dataset, af-ter removing the stop words and stemming, tf-idf [28] features are extracted and processed by PCA for one information source, and we use the hidden topics information obtained from Probabilistic Latent Semantic Analysis (PLSA) 8 of the binary word features as another one. For a detailed description of these two datasets, please refer to Table 2.
WebKB 9 is also a benchmark dataset for document classification, which contains webpages from computer science departments in around four different universities. There are seven categories in this other , with 8280 webpages in this dataset. The two most frequently appeared categories, i.e., course, and faculty, are used for classifi-cation, where each sub-dataset contains all of the webpages/bags from one of the two categories, and the same number of the neg-ative bags randomly sampled from the remaining six categories in WebKB. We use the same way as we do for Reusters21578 to ex-tract features from different views and model bags and instances. The detailed description of the two sub-datasets is summarized in Table 2.

Actually, PLSA[18] can be considered as a dimensionality reduc-tion method, which maps the documents into some fixed number of hidden topics. The topic distribution for each document can be used as low dimensional features. http://www.cs.cmu.edu/  X  webkb/
We obtained this real dataset from a big IT company. ITD is a project that is devoted to identify the potential harmful insiders through analyzing their online activities, such as sending emails, login, logout, downloaded files, etc. In this project, some experts are hired to decide whether during each period (around 30 days), each person in the database did malicious things or not. Based on these labelings, each online activity is quantified as a feature value. However, it is highly possible that a person may not do malicious things on each single day during the period in which he is marked as guilty. Out of this motivation, the features for the online behaviors within one day is considered as an instance and the instances during each period is treated as a bag. If a person is known to have done some malicious things in a specific period, then the corresponding collection of instances (days) is considered as a positive bag. Oth-erwise, this collection of instances will be considered as negative. The different activities are quantified into numeric features. These features are further divided into two groups according to the nature of the corresponding behaviors i.e, the group that describes his so-cial behaviors such as sending emails and interacting with friends on social media websites, and the group that depicts things he did by himself, such as logging in and out of a computer system. The whole dataset contains 1000 negative bags and 166 positive bags, where each instance is represented by two different views derived from the two feature groups as described above. Please refer to Table 2 for details on the size of the dataset.
In Reuters21578 and WebKB, since the positive and negative classes are relatively balanced, we use the classification accuracy as the measurement criteria. But for ITD dataset, the number of positive bags is far less than that of the negative ones. So, F1 score for the top 20 returned results is used here for measurement. In particular, F1 score is defined as F 1@20 = 2  X  Precision  X  Recall where, Precision and Recall are measured for the top 20 results.
We compare the proposed method with several state-of-the-art methods. MISVM and misvm [2] are MIL methods based on SVM. The difference between MISVM and midvm is that during each it-eration, to update the classifiers, MISVM tries to find a witness for each bag, while misvm assigns pseudo labels to all of the instances. MILES [8] tries to use a single vector to represent each bag through mapping these bags on a learned space. Citation KNN [42] adapts KNN to multi-instance by considering two different kinds of neigh-borhood relationships. These baseline methods could not be used to solve the multiple view problem directly. So, we concatenate the features in different views together and treat them as from one in-formation source. To demonstrate the benefits of ensuring the con-sistencies between different views without concatenating the fea-tures, we also conduct experiments by setting C to be 0 (FMI 0). It is clear that the formulation of the experiments proposed in [31] can be considered as a special case of FMI 2 LS-0. For the pro-posed method, k 1 is chosen as 10% of the number of bags, while k is 50% of the instances in sampled bags.  X  is set to be 0 . 2 . The number of SGD iterations is set to be 30 . By using 5 fold cross val-idation, C (1) and C (2) are searched through the grid 2 [  X  5:1:7] searched though 2 [  X  3:1:5] . The parameters of the baseline methods are also tuned similarly.
The experiments are conducted by specifying a specific ratio of each dataset for training and keeping the rest for testing. The av-erage results of 20 independent experiments on the three datasets with different training rations are shown in Fig.1, Fig.2 and Fig.3.
From these experimental results, we can see that the proposed method performs better than the other baseline methods in most cases. It is clear that considering the consistencies of examples on different views in MIL could significantly improve the classifica-tion performance. The time complexity of the proposed method is also very low, compared with the baseline methods. This is due to the fact that SGD could significantly reduce the time complexity. When compared with FMI 2 LS-0, it can be concluded that the time complexity of FMI 2 LS-0 is similar to that of the proposed method. But the performance of FMI 2 LS-0 is inferior. It further demon-strates the advantages of the proposed method by introducing the consistencies between different views.

For MISVM and misvm, both of these two methods are tradi-tional MIL methods. Their performances are good in terms of both the classification performance and time complexity. However, since these two methods do not consider the different character-istics from multiple information sources, and merely concatenate the different features by using only one feature vector, their perfor-mances are inferior to that of the proposed one.

Citation KNN is an adaption of nearest neighbor method. More specifically, it defines two different types of neighbors when mea-suring the similarities between two bags. It can be seen from the experiments that one of the major drawbacks for this method is that distances between test bags and training bags each time. Since it does not consider the consistencies between different views either, contenting the features on different views cannot bring in much ad-ditional benefits.

In MILES, during the training phase, the instances in training bags are mapped to a space spanned by the instances in positive bags, and then the most relevant examples are selected through one norm SVM. The method could capture the most important in-stances in an optimized way. However, the major issue is that its time complexity could be extremely high when the number of in-Figure 3: Performances Comparisons on ITD. F1 score for the top 20 returned results is used here due to the imbalance of this dataset. Some of the experiment results of MILES cannot be reported due to the time complexity issue as stated in Experi-ment section. stances in positive bags is large. This drawback could potentially hinder its uses in practical applications. In our experiments, this time complexity issue is also very evident. Some experimental results for MILES cannot be acquired due to the extremely large amount of training time. The performance of MILES is very com-petitive, compared with the other baseline methods. However, it is clear that, from these experiments, its performance cannot exceed the proposed method either.
In this paper, we investigate an interesting but rarely studied problem  X  Multi-Instance Learning from Multiple Information Sou-rces (MI 2 LS). To solve this problem, a general framework is pro-posed to incorporate the consistencies between different informa-tion sources/views into Multi-Instance Learning (MIL). Based on the proposed framework, a concrete method, FMI 2 LS (Fast MI LS) is designed. In particular, the proposed method integrates Con-strained Concave-Convex Programming (CCCP) method with an adapted Stoachastic Gradient Descent (SGD) method to solve the non-convex optimization problem in an efficient way. Some im-portant properties of the proposed method are analyzed thereafter. Experimental results on different applications, i.e., document clas-sification and the newly proposed application  X  Insider Threat De-tection (ITD), clearly demonstrate the superior performance of the proposed method against several other state-of-the-art MIL tech-niques on both efficiency and effectiveness. Based on the proposed method, in the future, we plan to extend the current work in the following ways: (1) In this paper, we didn X  X  tune the weights of dif-ferent views in the final classifier for simplicity. However, it is often the case that the data quality on different views could be different. We plan to design a method to adaptively tune the weights of dif-ferent views under the current framework. (2) Due to the nature of MIL, we can define different kinds of consistencies between views, i.e., on the instance level, the bag level, and the mixture of bag and instance level. It is an interesting topic to further investigate which one works better.
 [1] S. Ali and M. Shah. Human action recognition in videos us-[2] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support vec-[3] B. Babenko, M.-H. Yang, and S. Belongie. Robust object [4] C. Bergeron, G. M. Moore, J. Zaretzki, C. M. Breneman, [5] A. Blum and T. M. Mitchell. Combining labeled and unla-[6] B.Scholkopf and A.Smola. Learning with Kernels . MITPress, [7] R. C. Bunescu and R. J. Mooney. Multiple instance learning [8] Y. Chen, J. Bi, and J. Z. Wang. Miles: Multiple-instance [9] Y. Chen and J. Z. Wang. Image categorization by learning [10] T. Deselaers and V. Ferrari. A conditional random field for [11] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. Solving [12] J. Farquhar, D. Hardoon, H. Meng, J. Shawe-Taylor, and [13] A. Fuduli, M. Gaudioso, and G. Giallombardo. Minimizing [14] T. G X rtner, P. Flach, A. Kowalczyk, and A. Smola. Multi X  [15] T. G X rtner, P. A. Flach, A. Kowalczyk, and A. J. Smola. Multi-[16] P. V. Gehler and O. Chapelle. Deterministic annealing for [17] J. He and R. Lawrence. A graphbased framework for multi-[18] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR , [19] Y. Hu, M. Li, and N. Yu. Multiple-instance ranking: Learning [20] R. Jin, S. Wang, and Z.-H. Zhou. Learning a distance metric [21] T. Joachims. Training linear svms in linear time. In KDD , [22] T. Joachims and N. Cristianini. Composite kernels for hyper-[23] M. Kim and F. D. la Torre. Gaussian processes multiple in-[24] J. T. Kwok and P.-M. Cheung. Marginalized multi-instance [25] G. Li, S. C. H. Hoi, and K. Chang. Two-view transductive [26] Y.-X. Li, S. Ji, S. Kumar, J. Ye, and Z.-H. Zhou. Drosophila [27] D. G. Lowe. Object recognition from local scale-invariant [28] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to [29] O. Maron and T. Lozano-P X rez. A framework for multiple-[30] O. Maron and A. L. Ratan. Multiple-instance learning for [31] M. Mayo and E. Frank. Experiments with multi-view multi-[32] R. Rahmani and S. Goldman. MISSL: Multiple-instance [33] R. Rahmani, S. A. Goldman, H. Zhang, S. R. Cholleti, and [34] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient [35] D. Rosenberg, V. Sindhwani, P. Bartlett, and P. Niyogi. A [36] S. Shalev-shwartz and Y. Singer. Logarithmic regret algo-[37] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pe-[38] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pat-[39] V. Sindhwani and P. Niyogi. A co-regularized approach to [40] C. H. Teo, S. V. N. Vishwanathan, A. J. Smola, and Q. V. Le. [41] A. S. Vishwanathan, A. J. Smola, and S. V. N. Vishwanathan. [42] J. Wang, et Jean-Daniel Zucker, and J. daniel Zucker. Solving [43] M. Wu and B. Sch X lkopf. A local learning approach for clus-[44] O. Wu, J. Gao, W. Hu, B. Li, and M. Zhu. Indentifying multi-[45] D. Zhang, J. He, Y. Liu, L. Si, and R. D. Lawrence. Multi-[46] D. Zhang, Y. Liu, L. Si, J. Zhang, and R. D. Lawrence. Multi-[47] D. Zhang, F. Wang, C. Zhang, and T. Li. Multi-view local [48] M.-L. Zhang and Z.-H. Zhou. M3miml: A maximum mar-[49] Q. Zhang and S. A. Goldman. Em-dd: An improved multiple-[50] T. Zhang, A. Popescul, and B. Dom. Linear prediction mod-[51] Z.-H. Zhou and M.-L. Zhang. Multi-instance multi-label Proof of Theorem 1: To prove this theorem, we suppose  X  i  X  lowing inequality holds: k  X  X  ( w ,A s ,B s )
Proof of Theorem 2: Through calculating the dual of problem (4), it can be concluded that: s.t. 0  X   X  (1) i  X  C to the four sets of constraints in problem (3) respectively.
It is clear that,
So, the optimal solution of e w falls within the ball whose radius is  X 
Proof of Theorem 5: The Rademacher complexity of the func-tional space F C (1) + C (2) ,D can be upper bounded as follows:
