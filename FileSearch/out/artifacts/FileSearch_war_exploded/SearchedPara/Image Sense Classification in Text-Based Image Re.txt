 In web image search, users submit text queries to express their image needs and ambiguous query. If one sense of a query term dominates the other senses, most of the will be relatively few and even hard to be found. Meanwhile, images of different have to browse the list to find the requested images. Mixing up the two issues makes text-based image retrieval more challenging. Figure 1 shows an example of web im-result list, and the top 10 images belong to these two senses. In contrast, game system become hard to be found. 
The basic issues to disambiguate the images in text-based image retrieval are how many image senses there may be, and what the image senses they are. Here, an image term(s). For similar word sense disambiguation (WSD) problem, we can look up a most appropriate sense from the context. Due to the lack of word-image ontology, we the image result list of a text query, there may be many unknown senses. For exam-ple, the word tiger has two senses in WordNet  X  say, animal and person . In contrast, the result list for query term tiger may contain images related to tiger tank , which is a German heavy tank used in World War II, tiger shark , which is one of the largestpre-term tiger . 
The web provides rich images for different senses to increase coverage. The problem is how to collect enough noiseless sample images for each sense without too much hu-some degree. This idea can be employed to collect sample images. If the context related to an image sense can be found and added to a query term, then the ambiguity degree of narrowed down to person , when the words like wood and golf are added to query term sense and how many words are needed to collect the sample images. 
The set of sample images along with their text form a sense description. Several is-sues have to be considered, when a collec tion of sense descriptions are employed for image sense disambiguation. Firstly, the contextual information for each returned image is very few. Secondly, the words to be disambiguated in WSD problem often belong to one of the given senses. In contrast, the images returned by a search engine may not belong to any senses in the preco mpiled collection because irrelevant images may be retrieved. That makes image sense disambiguation harder. 
This paper is organized as follows. Section 2 introduces the related works. Section 3 Section 4 applies the results to disambiguate the image senses. Section 5 concludes the remarks. Cai et al. [1] used image, text features, and link features to cluster web image search results for browsing. Loeff, Alm and Forsyth [2] touched on image sense disam-biguation. They first manually annotated web pages for ambiguous query terms, with three kinds of label core sense (C), related sense (R), and unrelated sense (U). Then, they employed spectral clustering method to disambiguate images returned by search engines in three levels of sense granularity. They did not know the number of senses in the result list, so that deciding the number of clusters would be a problem. Besides, they did not know what senses there were in the result list, thus finding information to represent each cluster is a problem. Furthermore, dividing senses into core sense, related sense, and unrelated sense is very coarse. 
Zinger, et al. [3] considered WordNet lexical resources as basis of ontology and created large-scale image ontology. The im age collections were acquired through web-based image mining. Fluhr, et al. [4] constructed general purpose ontology, where each node is decorated with multilingual and multimodal data. Chang and Chen [5] extended WordNet to word-image ontology. The previous work submitted text queries to web image search engines to acquire images. Lexical ambiguity makes the alignment of lexical terms in WordNet and the retrieved images challenging. discusses how to collect text terms first and then sample images. An example tiger is considered as a seed word to demonstrate the algorithms. 3.1 Finding Text Terms to Label Senses 3.1.1 Finding Context Terms A seed word of a specific sense appears in the similar contexts. If we regard a context co-occur with it very often. The terms that co-occur with a given seed word with high frequency are considered as context terms for this word. Algorithm 1 extracts context terms for a given seed word. Algorithm 1. FindingContextTerms ( sw , CT ) Input: A seed word sw Output: A set CT of context terms for sw
Method: 1. Submit sw to a web page search engine, and retrieve the snippets. 3. Count the frequency of the remaining terms and choose the terms passing a 
At step 1, we use a web page search engine instead of an image search engine. At web pages. 3.1.2 Finding Related Terms of Context Terms Because a seed word is ambiguous, its context terms co-occur with different senses of seed word. For each context term, we can collect the highly co-occurring terms from the web in the similar way as that done on seed word in Section 3.1.1., use the infor-mation to group context terms and filter out context terms not related to any sense. A context term may relate to more than one sense of the seed word. That may confuse most common sense only. For example, the context term sport of the seed word tiger relates to a famous golf player (i.e., Tiger Woods) and a football team (i.e., LSU Ti-ger). The most common assignment will be the former instead of the latter. Algorithm 2 shows how to find the related terms of context terms. need less number of checks. That ensures the terms added to RT i are related to only we cannot add football into RT i later because the collocation strength between golf Table 1 lists some context terms of the seed word tiger , and their related terms. For each row, the first (an underlined term) and the rest denote a context term and its related terms. 1. Initialize RT i to { ct i } for i =1 to n . 2. Construct query q i with two query terms sw and ct i for i =1 to n . 3. Submit q i (for i =1 to n ) to a web page search engine, and retrieve the snippets. 4. Preprocess the snippets in the similar way as Step 2 of Algorithm 1. 6. Construct a collocation matrix G=[ g ij ] n  X  n as follows. 3.1.3 Grouping and Sense Labeling Algorithm 3 deals with the grouping of context terms based on their related terms. At double checking satisfies, we merge the related groups into a larger one. At step 3, we remove the groups composed of only one context term and its related terms. The con-text terms (along with related terms) in the same group will denote a sense. At step 4, corresponding group. Table 2 shows some of 13 groups generated using the relation-ships found by Algorithm 2. Each entry of a group contains a context term (under-lined) and its related terms. The sense label of each group is also shown in bold after group id. 3.2 Finding Sample Pages and Sample Images for Image Senses and image search engine to collect sample pages and sample images. Table 3 demon-strates top 5 sample images for senses of tiger . 1. Initialize TS to be an empty set. 3. Remove those singleton sets from TS . 4. For each remaining TS i , those terms appear in more than half of the 3.3 Experiments and Discussion In the experiments, we evaluate the results from the two measures: coverage and precision . The first metric concerns if all the image senses in the image result list are covered by the senses we found. The second metric concerns the quality of the sample jaguar , junk , star , plane , bat , and pick are adopted. 
We submit each word to Google image search engine, and retrieve the top 300 images. Total 3,000 images are annotated by human. There are four possible annota-tions shown as follows. Table 4 shows the evaluation of sense coverage. The number enclosed in parentheses is total senses found by our method. On the average, 97.12% of the image senses are covered. Most of the missing image senses are not very common. When an image sense is not popular, the frequency of terms related to this sense may not be high. 
For each word, we submit all the senses found by our method to Google image search engine, and retrieve the top 100 images for each sense. Assessors check if the retrieved images belong to the corresponding sense. If assessors think the query is too fuzzy to make decision, the group of images retrieved is considered as noise. Table 5 shows the precision of sample images. The number ( n / m ) enclosed in parentheses denotes n of m senses found are undecidable by assessors. In the experiments, 10% of senses are undecidable and the average precision is 68.26%. term. We will use them to classify the images returned by image search engine. Search engine provides three kinds of information for each returned image, including a thumbnail, a source web site, and a short snippet. Text features, URL features, and image features are extracted from sample pages, sample images, and the returned images for classification. Five types of classifiers are presented. 4.1 Feature Extraction and Classification Algorithms website features are extracted from sample pages (snippets) for each sense. The sam-ple pages are preprocessed in the similar way as the step (2) of Algorithm 1. Let T i be a set of m i terms features. of the highest score if the score is also larger than a threshold. If all the scores are less than the threshold, we regard G as an irrelevant image. An alternative procedure consid ers inverse sense frequency ( ISF ) defined as follows. containing t in the sample pages. The revised procedure is shown as follows. The second classifier called image classifier employs image features. Image features are extracted from each sample image. We first segment a sample image into 32  X  32 blocks, and compute an average RGB value for each block. An image is represented as a vector of 3,072 (32  X  32  X  3) average values. We represent a thumbnail as a vector of size 3,072 similarly. Assume there are n i sample images image sense s i . The following procedure determines the image sense which G belongs the distances of all the sample images and G are larger than a threshold, then G will be assigned to irrelevant category. The third classifier called URL classifier employs website features. We postulate that one sense per discourse in WSD [6]. The discourse here is the website in which im-image G , count how many sample images of each sense have the same URL as G , and select the sense of the highest count. This procedure is defined as follows, where U i is a set of top-level URLs of the sample pages of sense s i , and u G denotes top-level URL of G . Besides the fundamental URL classifier, we also consider the rank of each sample image returned by search engine. The revised procedure is shown as follows. The function rank ( u ) returns the rank of sample image u . The larger the reciprocal Rank of u is, the higher score it contributes. if The fourth classifier called expanded text classifier uses top-level URL u G to expand the short snippet of a returned image G for a target term v . We submit  X  v u G  X  to text search engine, collect the returned snippets, regard them as expanded text, and adopt the same procedure as the first classifi er to determine the suitable sense. classifiers. A sense s will get a vote 1 for an image G when a classifier labels G with Finally, the label (a sense or an irrelevant mark) of the highest vote will be proposed. 4.2 Experiments and Discussion In the experiments, we adopt the following 7 metrics to evaluate the performance of image sense classifiers. 
Recall, precision, and F-measure are common metrics used in information re-classifier will deal with as many senses as possible. Purity is similar to precision, but higher weight in precision. AvgNum is the average size for each class that a classifier proposes. Here, a useful classifier should provide enough images for each class. Fmeasure 0.5 is adopted because precision is more important than recall in this task. 
Table 6 shows the performance of different classifiers. Here, C# and Avg# abbre-viate average ClassNum and average AvgNum , respectively. Two baselines are experimented for comparisons. Baseline 1 neglects the unpopular senses, and classi-fies images to the most common sense. That is, C#=1 in baseline 1. The recall of the ever, only the major sense is considered in baseline 1. That is impractical. Compara-tively, baseline 2 deals with large number of possible senses (C#=10). The text classifier with ISF is better than that without ISF in most of the measures. query term. In contrast, the text classifier without ISF can only disambiguate 65% of can deal with 10 times more senses with competitive performance. Image classifier is worse than text classifier, but is still better than baseline 2. 
The recall and Avg# of URL classifier is the worst of all models since most of im-ages come from the unseen web sites. In contrast, its precision and Purity is the best of all models. That confirms the postulation of one sense per discourse: images in the same web sites share the same sense. The performance of the URL classifier with and without ranks is quite similar. That shows the ranks of snippets may not be a powerful factor. 
Comparing the pure text classifier and the expanded text classifier, we can find the expense of 13.50% precision decrease. The expanded text helps reduce the short snip-comparison. Given a set of image senses and a set of images, a user is asked to assign each image to a sense cluster. In this experiment, F1, F0.5 and Purity of human are 0.7622, 0.7700, and 0.5942, respectively. The merge classifier achieves 54.27%, 49.31%, 50.64%, and 51.61% of human performance in precision, recall, Fmeasure1, and Fmeasure0.5, respectively. 
We analyze the performance of various models under different query terms to find the influence factors. Baseline 1 depends on the dominating degree of most common irrelevant images. The quality of training sa mples is affected by the number of irrele-vant images. The performance of classifiers is sensitive to the quality of training sam-ples, the number of image senses, number of irrelevant images and the classification strategies. This paper investigates the image sense disambiguation in web image retrieval. A method to find unknown image senses is proposed. Total 97.12% of image senses returned by a search engine are covered. We also collect sample pages and sample images of each sense without human annotation. The average precision of sample images is 68.26%. We propose four kinds of classifiers using text, image, URL, and these classifiers. For classifying unseen images, the merge classifier achieves 0.3974 human performance. There are still spaces for further improvement. The irrelevant images influence performance very much. How to filter out irrelevant images is im-scriptions to an existing ontology like WordNet will be investigated. 
