 A multivariate time series (MTS) is a sequential collection of high dimensional observations generated by a dynamical system. The high dimensionality of MTS creates challenges for machine learning and data mining algorithms. To tackle this, feature extraction techniques are required to obtain computationally effi-cient and compact representations.

Gaussian Process Latent Variable Model [5] (GPLVM) is one of the most powerful nonlinear feature extraction algorithms. GPLVM emerged in 2004 and instantly made a breakthrough in dimensionality reduction research. The novelty of this approach is that, in addition to the optimisation of low dimensional coordinates during the dimensionality reduction process as other methods did, it marginalises parameters of a smooth and nonlinear mapping function from low to high dimensional space. As a consequence, GPLVM defines a continuous low dimensional representation of high dimensional data, which is called latent space. Since GPLVM is a very flexible approach, it has been successfully applied in a range of application domains including pose recovery [2], human tracking [20], computer animation [21], data visualization [5] and classification [19].
However, extensive study of the GPLVM framework has revealed some essen-tial limitations of the basic algorithm [5, 6, 8, 12, 19, 21, 22]. First, since GPLVM aims at retaining data global structure in the latent space, there is no guaran-tee that local features are preserved. As a result, the natural topology of the data manifold may not be maintained. This is particularly problematic when data, such as MTS, have a strong and meaningful intrinsic structure. In addi-tion, when data are captured from different sources, even after normalisation, GPLVM tends to produce latent spaces which fail to represent common local fea-tures [8, 21]. This prevents successful utilisation of the GPLVM framework for feature extraction of MTS. In particular, GPLVM cannot be applied in many classification applications such as speech and action recognition, where latent spaces should be inferred from time ser ies generated by different subjects and used to classify data produced by unknown individuals. Another drawback of GPLVM is its computationally expensive learning process [5, 6, 21] which may converge towards a local minima if the initialization of the model is poor [21].
Although recent extensions of GPLVM , i.e. back constrained GPLVM [12] (BC-GPLVM) and Gaussian Process Dynamical Model [22] (GPDM), allow sat-isfactorily representation of time series , the creation of gene ralised latent spaces from data issued from several sources is still a unsolved problem which has never been addressed by the research community. In this paper, we define  X  X tyle X  as the data variations between two or more datasets representing a similar phe-nomenon. They can be produced by different sources and/or repetitions from a single source. Here, we propose an extension of the GPLVM framework, i.e. Spatio-Temporal GPLVM (ST-GPLVM), which produces generalised and prob-abilistic representation of MTS in the presence of stylistic variations. Our main contribution is the integration of a spatio-temporal  X  X onstraining X  prior distribu-tion over the latent space within the optimisation process.
 After a brief review of the state of art, we introduce the proposed methodology. Then, we validate qualitatively our method on a real dataset of human behavioral time series. Afterwards, we apply our method to a challenging view independent action recognition task. Finally, conclusions are presented. Feature extraction methods can be divided into two general categories, i.e., deter-ministic and probabilistic frameworks. The deterministic methods can be further classified into two main classes: linear a nd non linear methods. Linear methods like PCA cannot model the curvature and nonlinear structures embedded in ob-served spaces. As a consequence, nonlin ear methods, such as Isomap [17], locally linear embedding [13] (LLE), Laplacian Eigenmaps [1] (LE) and kernel PCA [14], were proposed to address this issue. Isomap, LLE and LE aim at preserving a specific geometrical property of the underlying manifold by constructing graphs which encapsulate nonlinear relationships between points. However, they do not provide any mapping function between spaces. In contrast, kernel PCA obtains embedded space through nonlinear kernel based mapping from a high to low dimensional space. In order to deal with MTS, extensions of Isomap [3], LE [8] and the kernel based approach [15] were proposed.

Since previously described methods do n ot model uncertainty, another class of feature extraction methods evolved, the s o-called latent variable models (LVM). They define a probability distribution over the observed space that is conditioned on a latent variable and mapping parameters. Consequently, it produces a prob-abilistic generative mapping from the latent space to the data space. In order to address intrinsic limitations of probabilistic linear methods, such as probabilis-tic principal component analysis [18] (PPCA), Lawrence [5] reformulated the PPCA model to the nonlinear GPLVM by establishing nonlinear mappings from the latent variable space to the obser ved space. From a Bayesian perspective, the Gaussian process prior is placed over these mappings rather than the latent variables with a nonlinear covariance function. As a result, GPLVM produces a complete joint probability distribution over latent and observed variables.
Recently, many researchers have exp loited GPLVM in a variety of appli-cations [2, 5, 19, 20, 21], thus designing a number of GPLVM-based extensions which address some of the limitations of standard GPLVM. Lawrence [5, 6] pro-posed to use sparse approximation of the full Gaussian process which allows de-creasing the complexity of the learning pr ocess. Preservation of observed space topology was supported by imposing high dimensional constraints on the latent space [12, 21]. BC-GPLVM [12] enforces local distance preservation through the form of a kernel-based regression mapping from the observed space to the latent space. Locally linear GPLVM [21] (LL-GPLVM) extends this concept by defining explicitly a cylindrical topology to maintain. This is achieved by constructing advanced similarity measures for the back constrained mapping function and incorporation of the LLE objective function [13] into the GPLVM framework to reflect a domain specific prior knowledge about observed data. Another line of work, i.e. GPDM [22], augments GPLVM with a nonlinear dynamical mapping on the latent space based on the auto-regressive model to take advantage of temporal information provided with time series data.

Current GPLVM based approaches have proven very effective when modelling of MTS variability is desired in the latent space. However, these methods are inappropriate in a context of recognition based applications where the discovery of a common content pattern is more valuable than modelling stylistic variations. In this work, we tackle this fundamental problem by introducing the idea of a spatio-temporal interpretation of GPLVM. This concept is formulated by incor-porating a constraining prior distribution over the latent space in the GPLVM framework. In contrast to LL-GPLVM and BC-GPLVM, we aim at implicitly preserving a spatio-temporal structure of the observed time series data in order to discard style variability and discover a unique low dimensional representation for a given set of MTS. The proposed extension is easily adaptable to any variant of GPLVM, for instance BC-GPLVM or GPDM. Letasetofmultivariatetimeseries Y consists of multiple repetitions (or cycles) of the same phenomenon from the same or different sources and all data points { y ( y { x functions from the latent variable space X to the observed space Y under a constraint L to preserve the spatio-temporal patterns of the underlying manifold. The entire process is summarized in figure 1.

Initially the spatio-temporal constraints L are constructed. These constraints are exploited twofold. First they are used to better initialise the model by discovering a low dimensional embedded s pace which is close to the expected rep-resentation. Secondly, they constrain the GPLVM optimisation process so that it converges faster and maintains the spatio-temporal topology of the data. The learning process is performed using the standard two stage maximum a posteriori (MAP) estimation used in GPLVM. Latent positions X , and the hyperparameters  X  are optimised iteratively until the optimal solution is reached under the intro-duced constraining prior p ( X | L ). The key novelty of the proposed methodology is its style generalisation potential. ST-GPLVM discovers a coherent and continuous low dimensional representation by identifying common spatio-temporal patterns which result in discarding style variability among all conceptually similar time series. 3.1 Gaussian Process Latent Variable Model GPLVM [5] was derived from the observation that a particular probabilistic in-terpretation of PCA is a product of Gaussian Process (GP) models, where each of them is associated with a linear covariance function (i.e. kernel function). Con-sequently, the design of a non-linear probabilistic model could be achieved by re-placing the linear kernel function with a non-linear covariance function. From a Bayesian perspective, by marginalising over the mapping function [5], the com-plete joint likelihood of all observed data dimensions given the latent positions is: where  X  denotes the kernel hyperparameters and K is the kernel matrix of the GP model which is assembled with a combination of covariance functions: nonlinear radial basis function (RBF): where the kernel hyperparameters  X  = {  X ,  X ,  X  } respectively determine the out-put variance, the variance of the additive noise and the RBF width.  X  x i x j is the Kronecker delta function.

Learning is performed using two stage MAP estimation. First, latent variables are initialized, usually using PPCA. Secondly, latent positions and the hyperpa-rameters are optimised iteratively until the optimal solution is reached. This can be achieved by maximising the likelihood (1 ) with respect to the latent positions, X , and the hyperparameters,  X  using the following posterior: where priors of the unknowns are: p ( X )= N (0 ,I ), p (  X  i )  X  i  X   X  1 i .Themax-imisation of the above posterior is equivalent to minimising the negative log posterior of the model:  X  ln p ( X,  X  | Y )=0 . 5(( DN +1) ln 2  X  + D ln | K | + tr ( K  X  1 YY T )+ This optimization process can be achiev ed numerically using the scaled conju-gate gradient [11] (SCG) method with respect to  X  and X . However, the learning process is computationally very expensive, since O ( N 3 ) operations are required in each gradient step to inverse the kernel matrix K [5]. Therefore, in practice, a sparse approximation to the full Gaussian process, such as  X  X ully independent training conditional X  (FITC) approximation [6] or active set selection [5], is ex-ploited to reduce the computational complexity to a more manageable O ( k 2 N ) where k is the number of points involved in the lower rank sparse approximation of the covariance [6]. 3.2 Spatio-Temporal Gaussian Process Latent Variable Model The proposed ST-GPLVM relies on the no vel concept of a spatio-temporal con-straining prior which is introduced into the standard GPLVM framework in order to maintain temporal coherence and marginalise style variability. This is achieved by designing an objective function, where the prior p ( X )inEq.3is replaced by the proposed conditioned prior p ( X | L ): where L denotes the spatio-temporal constraints imposed on the latent space. Although p ( X | L ) is not a proper prior, conceptually it can be seen as an equiv-alent of a prior for a given set of weights L [21]. These constraints are derived from graph theory, since neighbourhood graphs have been powerful in design-ing nonlinear geometrical constraints for dimensionality reduction using spectral based approaches [1,13,17]. In particular, the Laplacian graph allows preserving approximated distances between all data points in the low dimensional space [1]. This formulation is extensively exploited in our approach by constructing cost matrix, L , which emphasizes spatio -temporal dependencies between similar time series. This is achieved by designing two types of neighbourhood for each high dimensional data point P i (figure 2):  X  Temporal neighbours (T): the 2m closest points in the sequential order of  X  Spatial neighbours (S): let X  X  associate to each point, P i , 2s temporal neigh-Neighbourhood connections defined in the Laplacian graphs implicitly impose points closeness in the late nt space. Consequently, the temporal neighbours allow to model a temporal continuity of MTS, whereas spatial neighbours remove style variability by aligning MTS in the latent space.

The constraint matrix, L , is obtained, first, by assigning weights, W ,tothe edges of each graph, G  X  X  T,S } , using the standard LE heat kernel [1]: Then, information from both graphs are combined L = L T + L S where L G = D
G  X  W G is the Laplacian matrix. D G = diag { D G diagonal matrix with entries: D G ii = N j =1 W G ij The prior probability of the latent variables, which forces each latent point to preserve the spatio-temporal topology of the observed data, is expressed by: where  X  represents a global scaling of the prior and controls the  X  X trength X  of the constraining prior. Note that although distance between neighbours (especially spatial ones) may be large in L , it is infinite between unconnected points.
The maximisation of the new objective function (5) is equivalent to minimising the negative log posterior of the model:  X  ln p ( X,  X  | Y, L )=0 . 5( D ln | K | + tr ( K  X  1 YY T )+  X   X  2 tr ( XLX T )+ C )+ where C is a constant: ( DN +1)ln2  X  +ln  X  2 . Following the standard GPLVM approach, the learning process involves minimising Eq. 8 with respect to  X  and X iteratively using SCG method [11] until convergence.

ST-GPLVM is initialised using a nonlinear feature extraction method, i.e. temporal LE [8] which is able to preserve the constraints L in the produced em-bedded space. Consequently, compared to the standard usage of linear PPCA, initialisation is more likely to be closer to the global optimum. In addition, the enhancement of the objective function (3) with the prior (7) constrains the op-timisation process and therefore furth er mitigates the problem of local minima. The topological structure in terms of spatio-temporal dependencies is implic-itly preserved in the latent space without enforcing any domain specific prior knowledge.

The proposed methodology can be applied to other GPLVM based approaches, such as BC-GPLVM [12] and GPDM [22] by integrating the prior (7) in their cost function. The extension of BC-GPLVM res ults in a spatio-temporal model which provides bidirectional mapping between latent and high dimensional spaces. Al-ternatively, ST-GPDM produces a spatio-temporal model with an associated nonlinear dynamical process in the latent space. Finally, the proposed extension is compatible with a sparse approximation of the full Gaussian process [5, 6] which allows reducing further processing complexity. Our new approach is evaluated qualitatively through a comparative analysis of latent spaces discovered by standard non-linear probabilistic latent variable models, i.e. GPLVM, BC-GPLVM and GP DM and their extensions, i.e., ST-GPLVM, ST-BC-GPLVM and ST-GPDM, where the proposed spatio-temporal constraints have been included.
Our evaluation is conducted using time series of MoCap data, i.e. repeated ac-tions provided by the HumanEva dataset [16]. The MoCap time series are firstly converted into normalized sequences of po ses, i.e. invariant to subject X  X  rotation and translation. Then each pose is represented as a set of quaternions, i.e. a 52-dimension feature vector. In this experim ent, we consider three different subjects performing a walking action comprising of 500 frames each. The dimensionality of walking action space is reduced to 3 dimensions [20, 22]. During the learn-ing process, the computational complexity is reduced using FITC [6] where the number of inducing variables is set to 10% of the data. The global scaling of the constraining prior,  X  , and the width of the back constrained kernel [12] were set empirically to 10 4 and 0.1 respectively. Values of all the other parameters of the models were estimated automatically using maximum likelihood optimisation. The back constrained models used a RBF kernel [12].

The learned latent spaces for the walk ing sequences with the corresponding first two dimensions and processing times are presented in figure 3. Qualitative analysis confirms the generalisation property of the proposed extension. Stan-dard GPLVM based approaches discrimin ate between subjects in the spatially distinct latent space regions. Moreover, action repetitions by a given subject are represented separately. In contrast, the introduction of our spatio-temporal constraint in objective functions allows producing consistent and smooth repre-sentation by discarding style variability in all considered models. In addition, the extended algorithms converge significantly faster than standard versions. Here, we achieve a speed-up of a factor 4 to 6. We demonstrate the effectiveness of the novel methodology in a realistic com-puter vision application by integrating ST-GPLVM within the view independent human action recognition framework proposed in [7]. Here, the training data comprises of time series of action im ages obtained from evenly spaced views located around the vertical axis of a su bject. In order to deal with this compli-cated scenario, the introduced methodology is extended by a new initialisation procedure and a new advanced constraining prior. The learning process of action recognition framework is summarised in figure 4.

First, for each view ( z=1..Z ), silhouettes are extracted from videos, normal-ized and represented as 3364-dimensional vectors of local space-time saliency features [7]. Then, spatio-temporal constraints L z are calculated. During initial-isation, style invariant one-dimensional action manifolds X z are obtained using temporal LE [8] and subsequently all these view-dependent models are combined to generate a coherent view invariant representation of the action [7]. The out-come of this procedure reveals a torus-like structure which is used to initialise GPLVM and encapsulates both style and view. Finally, the latent space and parameters of model are optimised jointly under a new combined prior p ( X | L ). This prior is derived by taking into acco unt constraints associated with each view: where L is a block diagonal matrix formed by all L z . Action recognition is performed by maximum likelihood estimation. Performance of the system is evaluated using the multi-view IXMAS dataset [23] which is considered as the benchmark for view independent action recognition. This dataset is comprised of 12 actions which are performed 3 times by 12 different actors. In this dataset, actors X  positions and orientations in videos are arbitrary since no specific instruc-tion was given during acquisition. As a co nsequence, the action viewpoints are arbitrary and unknown. Here, we use one action repetition of each subject for training, whereas testing is performed wi th all action repetitions. Experiments are conducted using the popular leave-one-out schema [4,23,24]. Two recognition tasks were evaluated using either a single view or multiple views. In line with other experiments made on this dataset [9,10,24], the top view was discarded for testing. The global scaling of the constraining prior and the number of inducing variables in FITC [6] were set to 10 4 and 25% of the data respectively. Values of all the other parameters of the models were estimated automatically using maximum likelihood optimisation.
Action recognition results are compared with the state of the art in table 1 (top view excluded). Examples of learned view and style invariant action descrip-tors using ST-GPLVM are shown in figure 5. Although different approaches may use slightly different experimental settings, table 1 shows that our framework produces the best performances. In parti cular, it improves the accuracy of the standard framework [7]. The confusion matrix of recognition for the  X  X ll-view X  experiment reveals that our framework performed better when dealing with mo-tions involving the whole body, i.e.  X  X alk X ,  X  X it down X ,  X  X et up X ,  X  X urn around X  and  X  X ick up X . As expected, the best recognition rates 78.7%, 80.7% are ob-tained for camera 2 and 4 respectively, since those views are similar to those used for training, i.e. side views. Moreover, when dealing with either different, i.e. camera 1, or even significantly different views, i.e. camera 3, our framework still achieves good recognition rate, i.e. 75.2% and 69.9% respectively. This paper introduces a novel probabilistic approach for nonlinear feature extrac-tion called Spatio-Temporal GPLVM. Its main contribution is the inclusion of spatio-temporal constraints in the form of a conditioned prior into the standard GPLVM framework in order to discover g eneralised latent spaces of MTS. All conducted experiments confirm the generalisation power of the proposed concept in the context of classification applications where marginalising style variabil-ity is crucial. We applied the proposed extension on different GPLVM variants and demonstrated that their Spatio-Temporal versions produce smoother, co-herent and visually more convincing descriptors at a lower computational cost. In addition, the methodology has been validated in a view independent action recognition framework and produced st ate of the art accuracy. Consequently, the concept of consistent r epresentation of time series should benefit to many other applications beyond action recognition such as gesture, sign-language and speech recognition.

