 Natural language processing systems are imper-fect. Decades of research have yielded analyzers that mis-identify named entities, mis-attach syn-tactic relations, and mis-recognize noun phrase coreference anywhere from 10-40% of the time. But these systems are accurate enough so that their outputs can be used as soft, if noisy, indicators of language meaning for use in downstream analysis, such as systems that perform question answering, machine translation, event extraction, and narra-tive analysis (McCord et al., 2012; Gimpel and Smith, 2008; Miwa et al., 2010; Bamman et al., 2013).

To understand the performance of an ana-lyzer, researchers and practitioners typically mea-sure the accuracy of individual labels or edges among a single predicted output structure y , such as a most-probable tagging or entity clustering arg max y P ( y | x ) (conditional on text data x ).
But a probabilistic model gives a probability distribution over many other output structures that have smaller predicted probabilities; a line of work has sought to control cascading pipeline errors by passing on multiple structures from earlier stages of analysis, by propagating prediction uncertainty through multiple samples (Finkel et al., 2006), K -best lists (Venugopal et al., 2008; Toutanova et al., 2008), or explicitly diverse lists (Gimpel et al., 2013); often the goal is to marginalize over structures to calculate and minimize an expected loss function, as in minimum Bayes risk decod-ing (Goodman, 1996; Kumar and Byrne, 2004), or to perform joint inference between early and later stages of NLP analysis (e.g. Singh et al., 2013; Durrett and Klein, 2014).

These approaches should work better when the posterior probabilities of the predicted linguistic structures reflect actual probabilities of the struc-tures or aspects of the structures. For example, say a model is overconfident: it places too much prob-ability mass in the top prediction, and not enough in the rest. Then there will be little benefit to us-ing the lower probability structures, since in the training or inference objectives they will be incor-rectly outweighed by the top prediction (or in a sampling approach, they will be systematically un-dersampled and thus have too-low frequencies). If we only evaluate models based on their top pre-dictions or on downstream tasks, it is difficult to diagnose this issue.

Instead, we propose to directly evaluate the cal-ibration of a model X  X  posterior prediction distri-bution. A perfectly calibrated model knows how often it X  X  right or wrong; when it predicts an event with 80% confidence, the event empirically turns out to be true 80% of the time. While perfect accuracy for NLP models remains an unsolved challenge, perfect calibration is a more achievable goal, since a model that has imperfect accuracy could, in principle, be perfectly calibrated. In this paper, we develop a method to empirically analyze calibration that is appropriate for NLP models (  X  3) and use it to analyze common generative and dis-criminative models for tagging and classification (  X  4).

Furthermore, if a model X  X  probabilities are meaningful, that would justify using its proba-bility distributions for any downstream purpose, including exploratory analysis on unlabeled data. In  X  6 we introduce a representative corpus explo-ration problem, identifying temporal event trends in international politics, with a method that is de-pendent on coreference resolution. We develop a coreference sampling algorithm (  X  5.2) which projects uncertainty into the event extraction, in-ducing a posterior distribution over event frequen-cies. Sometimes the event trends have very high reflecting when the NLP system genuinely does not know the correct semantic extraction. This highlights an important use of a calibrated model: being able to tell a user when the model X  X  predic-tions are likely to be incorrect, or at least, not giv-ing a user a false sense of certainty from an erro-neous NLP analysis. Consider a binary probabilistic prediction prob-lem, which consists of binary labels and proba-bilistic predictions for them. Each instance has a ground-truth label y  X  { 0 , 1 } , which is used for evaluation. The prediction problem is to gener-ate a predicted probability or prediction strength q  X  [0 , 1] . Typically, we use some form of a prob-abilistic model to accomplish this task, where q instance having a positive label ( y = 1 ).
Let S = { ( q 1 ,y 1 ) , ( q 2 ,y 2 ) ,  X  X  X  ( q N ,y N ) } be the set of prediction-label pairs produced by the model. Many metrics assess the overall quality of how well the predicted probabilities match the data, such as the familiar cross entropy (negative average log-likelihood), L ` ( ~y,~q ) = or mean squared error, also known as the Brier score when y is binary (Brier, 1950), Both tend to attain better (lower) values when q is near 1 when y = 1 , and near 0 when y = 0 ; and they achieve a perfect value of 0 when all q i = y i . 4
Let P ( y,q ) be the joint empirical distribution over labels and predictions. Under this notation, L 2 = E q,y [ y  X  q ] 2 . Consider the factorization where P ( y | q ) denotes the label empirical fre-quency, conditional on a prediction strength (Mur-ization to the Brier score leads to the calibration-refinement decomposition (DeGroot and Fienberg, 1983), in terms of expectations with respect to the prediction strength distribution P ( q ) : where we denote p q  X  P ( y = 1 | q ) for brevity.
Here, calibration measures to what extent a model X  X  probabilistic predictions match their cor-responding empirical frequencies. Perfect calibra-tion is achieved when P ( y = 1 | q ) = q for all q ; intuitively, if you aggregate all instances where a model predicted q , they should have y = 1 at q percent of the time. We define the magnitude of miscalibration using root mean squared error: Definition 1 (RMS calibration error) .
 The second term of Eq 1 refers to refinement , which reflects to what extent the model is able to separate different labels (in terms of the con-ditional Gini entropy p q (1  X  p q ) ). If the predic-tion strengths tend to cluster around 0 or 1, the re-finement score tends to be lower. The calibration-refinement breakdown offers a useful perspective on the accuracy of a model posterior. This paper focuses on calibration.

There are several other ways to break down squared error, log-likelihood, and other probabilis-tion error in this work, since unlike cross-entropy Algorithm 1 Estimate calibration error using adaptive binning.
 it does not tend toward infinity when near prob-ability 0; we hypothesize this could be an issue since both p and q are subject to estimation error. From a test set of labeled data, we can analyze model calibration both in terms of the calibration error, as well as visualizing the calibration curve of label frequency versus predicted strength. How-ever, computing the label frequencies P ( y = 1 | q ) requires an infinite amount of data. Thus approx-imation methods are required to perform calibra-tion analysis. 3.1 Adaptive binning procedure Previous studies that assess calibration in super-vised machine learning models (Niculescu-Mizil and Caruana, 2005; Bennett, 2000) calculate la-bel frequencies by dividing the prediction space into deciles or other evenly spaced bins X  X .g. q  X  [0 , 0 . 1) , q  X  [0 . 1 , 0 . 2) , etc. X  X nd then calculat-ing the empirical label frequency in each bin. This procedure may be thought of as using a form of nonparametric regression (specifically, a regres-sogram; Tukey 1961) to estimate the function f ( q ) = P ( y = 1 | q ) from observed data points. But models in natural language processing give very skewed distributions of confidence scores q (many are near 0 or 1), so this procedure performs poorly, having much more variable estimates near Algorithm 2 Estimate calibration error X  X  confi-dence interval by sampling.
 the middle of the q distribution (Figure 1).
We propose adaptive binning as an alterna-tive. Instead of dividing the interval [0 , 1] into fixed-width bins, adaptive binning defines the bins such that there are an equal number of points in each, after which the same averaging proce-dure is used. This method naturally gives wider bins to area with fewer data points (areas that re-quire more smoothing), and ensures that these ar-eas have roughly similar standard errors as those near the boundaries, since for a bin with  X  num-ber of points and empirical frequency p , the stan-dard error is estimated by bounded above by 0 . 5 / the procedure for estimating calibration error us-ing adaptive binning, which can be applied to any probabilistic model that predicts posterior proba-bilities. 3.2 Confidence interval estimation Especially when the test set is small, estimating calibration error may be subject to error, due to uncertainty in the label frequency estimates. Since how to estimate confidence bands for nonparamet-ric regression is an unsolved problem (Wasserman, 2006), we resort to a simple method based on the binning. We construct a binomial normal approx-imation for the label frequency estimate in each bin, and simulate from it; every simulation across all bins is used to construct a calibration error; these simulated calibration errors are collected to construct a normal approximation for the calibra-tion error estimate. Since we use bin sizes of at least  X   X  200 in our experiments, the central limit theorem justifies these approximations. We report all calibration errors along with their 95% confi-3.3 Visualizing calibration In order to better understand a model X  X  calibration properties, we plot the pairs ( X  p the adaptive binning procedure to visualize the calibration curve of the model X  X his visualization is known as a calibration or reliability plot . It provides finer grained insight into the calibra-tion behavior in different prediction ranges. A perfectly calibrated curve would coincide with the y = x diagonal line. When the curve lies above the diagonal, the model is underconfident ( q &lt; p q ); and when it is below the diagonal, the model is overconfident ( q &gt; p q ).

An advantage of plotting a curve estimated from fixed-size bins, instead of fixed-width bins, is that the distribution of the points hints at the refinement aspect of the model X  X  performance. If the points X  positions tend to cluster in the bottom-left and top-right corners, that implies the model is making more refined predictions. Using the method described in  X  3, we assess the quality of posterior predictions of several classi-fication and tagging models. In all of our exper-iments, we set the target bin size in Algorithm 1 to be 5,000 and the number of samples in Algo-rithm 2 to be 10,000. 4.1 Naive Bayes and logistic regression 4.1.1 Introduction Previous work on Naive Bayes has found its prob-abilities to have calibration issues, in part due to its incorrect conditional independence assump-tions (Niculescu-Mizil and Caruana, 2005; Ben-nett, 2000; Domingos and Pazzani, 1997). Since logistic regression has the same log-linear repre-sentational capacity (Ng and Jordan, 2002) but does not suffer from the independence assump-tions, we select it for comparison, hypothesizing it may have better calibration.

We analyze a binary classification task of Twit-ter sentiment analysis from emoticons. We col-lect a dataset consisting of tweets identified by the Twitter API as English, collected from 2014 to 2015, with the  X  X moticon trick X  (Read, 2005; Lin and Kolcz, 2012) to label tweets that contain at least one occurrence of the smiley emoticon  X :) X  as  X  X appy X  ( y = 1 ) and others as y = 0 . The smiley emoticons are deleted in positive examples. We sampled three sets of tweets (subsampled from the Decahose/Gardenhose stream of public tweets) with Jan-Apr 2014 for training, May-Dec 2014 for development, and Jan-Apr 2015 for testing. Each set contains 10 5 tweets, split between an equal number of positive and negative instances. We use binary features based on unigrams extracted from the twokenize.py 8 tokenization. We use the scikit-learn (Pedregosa et al., 2011) implementa-tions of Bernoulli Naive Bayes and L2-regularized logistic regression. The models X  hyperparameters (Naive Bayes X  smoothing paramter and logistic re-gression X  X  regularization strength) are chosen to maximize the F-1 score on the development set. 4.1.2 Results Naive Bayes attains a slightly higher F-1 score (NB 73.8% vs. LR 72.9%), but logistic regression has much lower calibration error: less than half as much RMSE (NB 0.105 vs. LR 0.041; Figure 2). Both models have a tendency to be undercon-fident in the lower prediction range and overconfi-dent in the higher range, but the tendency is more pronounced for Naive Bayes. 4.2 Hidden Markov models and conditional 4.2.1 Introduction Hidden Markov models (HMM) and linear chain conditional random fields (CRF) are another com-monly used pair of analogous generative and dis-criminative models. They both define a posterior over tag sequences P ( y | x ) , which we apply to part-of-speech tagging.

We can analyze these models in the binary cal-ibration framework (  X  2-3) by looking at marginal distribution of binary-valued outcomes of parts of the predicted structures. Specifically, we examine calibration of predicted probabilities of individual tokens X  tags (  X  4.2.2), and of pairs of consecutive tags (  X  4.2.3). These quantities are calculated with the forward-backward algorithm.

To prepare a POS tagging dataset, we ex-tract Wall Street Journal articles from the En-glish CoNLL-2011 coreference shared task dataset from Ontonotes (Pradhan et al., 2011), using the CoNLL-2011 splits for training, development and testing. This results in 11,772 sentences for train-ing, 1,632 for development, and 1,382 for testing, over a set of 47 possible tags.

We train an HMM with Dirichlet MAP us-ing one pseudocount for every transition and word emission. For the CRF, we use the L 2 -regularized L-BFGS algorithm implemented in CRFsuite (Okazaki, 2007). We compare an HMM to a CRF that only uses basic transition (tag-tag) and emission (tag-word) features, so that it does not have an advantage due to more features. In order to compare models with similar task perfor-mance, we train the CRF with only 3000 sentences from the training set, which yields the same accu-racy as the HMM (about 88.7% on the test set). In each case, the model X  X  hyperparameters (the CRF X  X  L 2 regularizer, the HMM X  X  pseudocount) are selected by maximizing accuracy on the devel-opment set. 4.2.2 Predicting single-word tags In this experiment, we measure miscalibration of the two models on predicting tags of single words. First, for each tag type, we produce a set of 33,306 prediction-label pairs (for every token); we then concatenate them across the tags for calibration analysis. Figure 3 shows that the two models exhibit distinct calibration patterns. The HMM tends to be very underconfident whereas the CRF is overconfident, and the CRF has a lower (better) overall calibration error.

We also examine the calibration errors of the individual POS tags (Figure 4(a)). We find that CRF is significantly better calibrated than HMM in most but not all categories (39 out of 47). For example, they are about equally calibrated on pre-dicting the NN tag. The calibration gap between the two models also differs among the tags. 4.2.3 Predicting two-consecutive-word tags There is no reason to restrict ourselves to model predictions of single words; these models define marginal distributions over larger textual units. Next we examine the calibration of posterior pre-dictions of tag pairs on two consecutive words in the test set. The same analysis may be impor-tant for, say, phrase extraction or other chunk-ing/parsing tasks.
We report results for the top 5 and 100 most fre-quent tag pairs (Figure 4(b)). We observe a simi-lar pattern as seen from the experiment on single tags: the CRF is generally better calibrated than the HMM, but the HMM does achieve better cali-bration errors in 29 out of 100 categories.
These tagging experiments illustrate that, de-pending on the application, different models can exhibit different levels of calibration. We examine a third model, a probabilistic model for within-document noun phrase coreference, which has an efficient sampling-based inference procedure. In this section we introduce it and ana-lyze its calibration, in preparation for the next sec-tion where we use it for exploratory data analysis. 5.1 Antecedent selection model We use the Berkeley coreference resolution sys-tem (Durrett and Klein, 2013), which was origi-nally presented as a CRF; we give it an equivalent a series of independent logistic regressions (see appendix for details). The primary component of this model is a locally-normalized log-linear dis-tribution over clusterings of noun phrases, each cluster denoting an entity. The model takes a fixed input of N mentions (noun phrases), indexed by i in their positional order in the document. It posits that every mention i has a latent antecedent selec-tion decision, a i  X  { 1 ,...,i  X  1 , NEW } , denoting which previous mention it attaches to, or NEW if it is starting a new entity that has not yet been seen at a previous position in the text. Such a mention-mention attachment indicates coreference, while the final entity clustering includes more links im-plied through transitivity. The model X  X  generative process is: Definition 2 (Antencedent coreference model and sampling algorithm) . Here x denotes all information in the document that is conditioned on for log-linear features f . e = { e 1 ,...e M } denotes the entity clusters, where each element is a set of mentions. There are M en-tity clusters corresponding to the number of con-nected components in a . The model defines a joint distribution over antecedent decisions P ( a | x ) = Q entity clusterings P ( e | x ) , where the probability of an e is the sum of the probabilities of all a vectors that could give rise to it. In a manner similar to a distance-dependent Chinese restaurant process (Blei and Frazier, 2011), it is non-parametric in the sense that the number of clusters M is not fixed in advance. 5.2 Sampling-based inference For both calibration analysis and exploratory ap-plications, we need to analyze the posterior distri-bution over entity clusterings. This distribution is a complex mathematical object; an attractive ap-proach to analyze it is to draw samples from this distribution, then analyze the samples.

This antecedent-based model admits a very straightforward procedure to draw independent e samples, by stepping through Def. 2: indepen-dently sample each a i then calculate the connected components of the resulting antecedent graph. By construction, this procedure samples from the joint distribution of e (even though we never com-pute the probability of any single clustering e ).
Unlike approximate sampling approaches, such as Markov chain Monte Carlo methods used in other coreference work to sample e (Haghighi and Klein, 2007), here there are no questions about burn-in or autocorrelation (Kass et al., 1998). Every sample is independent and very fast to compute X  X nly slightly slower than calculating the MAP assignment (due to the exp and normal-ization for each a i ). We implement this algorithm by modifying the publicly available implementa-5.3 Calibration analysis We consider the following inference query: for a randomly chosen pair of mentions, are they coref-erent? Even if the model X  X  accuracy is compara-tively low, it may be the case that it is correctly calibrated X  X f it thinks there should be great vari-ability in entity clusterings, it may be uncertain whether a pair of mentions should belong together.
Let ` ij be 1 if the mentions i and j are predicted to be coreferent, and 0 otherwise. Annotated data defines a gold-standard ` ( g ) i,j . Any probability distribution over e defines a marginal Bernoulli distribution for every proposi-tion ` ij , marginalizing out e : P ( ` ij = 1 | x ) = where ( i,j )  X  e is true iff there is an entity in e that contains both i and j .

In a traditional coreference evaluation of the best-prediction entity clustering, the model as-signs 1 or 0 to every ` ij and the pairwise precision and recall can be computed by comparing them to the corresponding ` ( g ) the q ij  X  P ( ` ij = 1 | x, e ) prediction strengths wise calibration, with the same binary calibration analysis tools developed in  X  3 by aggregating pairs with similar q ij values. Each q ij is computed by averaging over 1,000 samples, simply taking the fraction of samples where the pair ( i,j ) is coref-erent.
We perform this analysis on the develop-ment section of the English CoNLL-2011 data (404 documents). Using the sampling inference method discussed in  X  5 . 2 , we compute 4.3 mil-lions prediction-label pairs and measure their cali-bration error. Our result shows that the model pro-duces very well-calibrated predictions with less than 1% CalibErr (Figure 5), though slightly overconfident on middle to high-valued predic-tions. The calibration error indicates that it is the most calibrated model we examine within this pa-per. This result suggests we might be able to trust its level of uncertainty. 6.1 Entity-syntactic event aggregation We demonstrate one important use of calibration analysis: to ensure the usefulness of propagating uncertainty from coreference resolution into a sys-tem for exploring unannotated text. Accuracy can-not be calculated since there are no labels; but if the system is calibrated, we postulate that un-certainty information can help users understand the underlying reliability of aggregated extractions and isolate predictions that are more likely to con-tain errors.

We illustrate with an event analysis application to count the number of  X  X ountry attack events X : for a particular country of the world, how many news articles describe an entity affiliated with that country as the agent of an attack, and how does this number change over time? This is a simpli-fied version of a problem where such systems have been built and used for political science analysis (Schrodt et al., 1994; Schrodt, 2012; Leetaru and Schrodt, 2013; Boschee et al., 2013; O X  X onnor et al., 2013). A coreference component can im-prove extraction coverage in cases such as  X  Rus-sian troops were sighted . . . and they attacked . . .  X 
We use the coreference system examined in  X  5 for this analysis. To propagate coreference un-certainty, we re-run event extraction on multiple coreference samples generated from the algorithm described in  X  5.2, inducing a posterior distribution over the event counts. To isolate the effects of coreference, we use a very simple syntactic depen-dency system to identify affiliations and events. Assume the availability of dependency parses for a document d , a coreference resolution e , and a lexicon of country names, which contains a small set of words w ( c ) for each country c ; for example, w ( FRA ) = { france , french } . The binary function f ( c,e ; x d ) assesses whether an entity e is affiliated with country c and is described as the agent of an attack, based on document text and parses x d ; f For a given c , we first calculate a binary variable for whether there is at least one entity fulfilling f in a particular document, and second, the number of such documents in d ( t ) , the set of New York Times articles published in a given time period t , These quantities are both random variables, since they depend on e ; thus we are interested in the posterior distribution of n , marginalizing out e , If our coreference model was highly certain (only one structure, or a small number of similar struc-tures, had most of the probability mass in the space of all possible structures), each document would have an a posterior near either 0 or 1, and their sum in Eq. 5 would have a narrow distribution. But if the model is uncertain, the distribution will be wider. Because of the transitive closure, the prob-ability of a is potentially more complex than the single antecedent linking probability between two mentions X  X he affiliation and attack information can propagate through a long coreference chain. 6.2 Results We tag and parse a 193,403 article subset of the Annotated New York Times LDC corpus (Sand-haus, 2008), which includes articles about world news from the years 1987 to 2007 (details in ap-pendix). For each article, we run the coreference system to predict 100 samples, and evaluate f on interest is the number of articles mentioning at-tacks in a 3-month period (quarter), for a given country. Figure 6 illustrates the mean and 95% posterior credible intervals for each quarter. The posterior mean m is calculated as the mean of the samples, and the interval is the normal approxima-tion m  X  1 . 96 s , where s is the standard deviation among samples for that country and time period.
Uncertainty information helps us understand whether a difference between data points is real. In the plots of Figure 6, if we had used a 1-best coreference resolution, only a single line would be shown, with no assessment of uncertainty. This is problematic in cases when the model genuinely does not know the correct answer. For example, the 1993-1996 period of the USA plot (Figure 6, top) shows the posterior mean fluctuating from 1 to 5 documents; but when credible intervals are taken into consideration, we see that model does not know whether the differences are real, or were caused by coreference noise.

A similar case is highlighted at the bottom plot of Figure 6. Here we compare the event counts for Yugoslavia and NATO, which were engaged in a conflict in 1999. Did the New York Times de-vote more attention to the attacks by one particu-lar side? To a 1-best system, the answer would be yes. But the posterior intervals for the two coun-tries X  event counts in mid-1999 heavily overlap, indicating that the coreference system introduces too much uncertainty to obtain a conclusive an-swer for this question. Note that calibration of the coreference model is important for the credible in-tervals to be useful; for example, if the model was badly calibrated by being overconfident (too much probability over a small set of similar structures), these intervals would be too narrow, leading to in-correct interpretations of the event dynamics.
Visualizing this uncertainty gives richer infor-mation for a potential user of an NLP-based sys-tem, compared to simply drawing a line based on a single 1-best prediction. It preserves the gen-uine uncertainty due to ambiguities the system was unable to resolve. This highlights an alternative use of Finkel et al. (2006) X  X  approach of sampling multiple NLP pipeline components, which in that work was used to perform joint inference. Instead of focusing on improving an NLP pipeline, we can pass uncertainty on to exploratory purposes, and try to highlight to a user where the NLP system may be wrong, or where it can only imprecisely specify a quantity of interest.

Finally, calibration can help error analysis. For a calibrated model, the more uncertain a predic-tion is, the more likely it is to be erroneous. While coreference errors comprise only one part of event extraction errors (alongside issues in parse qual-ity, factivity, semantic roles, etc.), we can look at highly uncertain event predictions to understand the nature of coreference errors relative to our task. We manually analyzed documents with a 50% probability to contain an  X  X ttack X  X ng country-affiliated entity, and found difficult coreference cases.

In one article from late 1990, an  X  X ttack X  event for IRQ is extracted from the sentence  X  X ut some political leaders said that they feared that Mr. Hus-sein might attack Saudi Arabia X . The mention  X  X r. Hussein X  is classified as IRQ only when it is coreferent with a previous mention  X  X resident Saddam Hussein of Iraq X ; this occurs only 50% of the time, since in some posterior samples the coreference system split apart these two  X  X ussein X  mentions. This particular document is addition-ally difficult, since it includes the names of more than 10 countries (e.g. United States, Saudi Ara-bia, Egypt), and some of the Hussein mentions are even clustered with presidents of other countries (such as  X  X resident Bush X ), presumably because they share the  X  X resident X  title. These types of er-rors are a major issue for a political analysis task; further analysis could assess their prevalence and how to address them in future work. In this work, we argue that the calibration of pos-terior predictions is a desirable property of prob-abilistic NLP models, and that it can be directly evaluated. We also demonstrate a use case of having calibrated uncertainty: its propagation into downstream exploratory analysis.

Our posterior simulation approach for ex-ploratory and error analysis relates to posterior predictive checking (Gelman et al., 2013), which analyzes a posterior to test model assumptions; Mimno and Blei (2011) apply it to a topic model.
One avenue of future work is to investigate more effective nonparametric regression methods to better estimate and visualize calibration error, such as Gaussian processes or bootstrapped kernel density estimation.

Another important question is: what types of in-ferences are facilitated by correct calibration? In-tuitively, we think that overconfidence will lead to overly narrow confidence intervals; but in what sense are confidence intervals  X  X ood X  when cal-ibration is perfect? Also, does calibration help joint inference in NLP pipelines? It may also assist calculations that rely on expectations, such as in-ference methods like minimum Bayes risk decod-ing, or learning methods like EM, since calibrated predictions imply that calculated expectations are statistically unbiased (though the implications of this fact may be subtle). Finally, it may be in-teresting to pursue recalibration methods, which readjust a non-calibrated model X  X  predictions to be calibrated; recalibration methods have been de-veloped for binary (Platt, 1999; Niculescu-Mizil and Caruana, 2005) and multiclass (Zadrozny and Elkan, 2002) classification settings, but we are unaware of methods appropriate for the highly structured outputs typical in linguistic analysis. Another approach might be to directly constrain CalibErr = 0 during training, or try to reduce it as a training-time risk minimization or cost objec-tive (Smith and Eisner, 2006; Gimpel and Smith, 2010; Stoyanov et al., 2011; Br  X  ummer and Dod-dington, 2013).

Calibration is an interesting and important prop-erty of NLP models. Further work is necessary to address these and many other questions. David Bamman, Brendan O X  X onnor, and Noah A.
Smith. Learning latent personas of film charac-ters. In Proceedings of ACL , 2013.
 Paul N. Bennett. Assessing the calibration of naive Bayes X  posterior estimates. Technical report, Carnegie Mellon University, 2000.
 David M. Blei and Peter I. Frazier. Distance dependent Chinese restaurant processes. The
Journal of Machine Learning Research , 12: 2461 X 2488, 2011.
 Elizabeth Boschee, Premkumar Natarajan, and
Ralph Weischedel. Automatic extraction of events from open source text for predictive forecasting. Handbook of Computational Ap-proaches to Counterterrorism , page 51, 2013. Glenn W. Brier. Verification of forecasts expressed in terms of probability. Monthly weather review , 78(1):1 X 3, 1950.
 Jochen Br  X  ocker. Reliability, sufficiency, and the decomposition of proper scores. Quarterly
Journal of the Royal Meteorological Society , 135(643):1512 X 1519, 2009.
 Niko Br  X  ummer and George Doddington.

Likelihood-ratio calibration using prior-weighted proper scoring rules. arXiv preprint arXiv:1307.7981 , 2013. Interspeech 2013.
 Marie-Catherine de Marneffe, Timothy Dozat,
Natalia Silveira, Katri Haverinen, Filip Gin-ter, Joakim Nivre, and Christopher D. Man-ning. Universal Stanford dependencies: A cross-linguistic typology. In Proceedings of LREC , 2014.
 Morris H. DeGroot and Stephen E. Fienberg. The comparison and evaluation of forecasters. The statistician , pages 12 X 22, 1983.
 Pedro Domingos and Michael Pazzani. On the op-timality of the simple Bayesian classifier under zero-one loss. Machine learning , 29(2-3):103 X  130, 1997.
 Greg Durrett and Dan Klein. Easy victories and uphill battles in coreference resolution. In EMNLP , pages 1971 X 1982, 2013.
 Greg Durrett and Dan Klein. A joint model for entity analysis: Coreference, typing, and link-ing. Transactions of the Association for Com-putational Linguistics , 2:477 X 490, 2014.
 Jenny Rose Finkel, Christopher D. Manning, and
Andrew Y. Ng. Solving the problem of cascad-ing errors: Approximate Bayesian inference for linguistic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing , pages 618 X  626. Association for Computational Linguis-tics, 2006.
 Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. Bayesian data analysis . Chapman and Hall/CRC, 3rd edition, 2013.
 Kevin Gimpel and Noah A. Smith. Rich source-side context for statistical machine translation.
In Proceedings of the Third Workshop on Sta-tistical Machine Translation , pages 9 X 17, 2008. Kevin Gimpel and Noah A. Smith. Softmax-margin CRFs: Training log-linear models with cost functions. In Human Language Technolo-gies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-putational Linguistics , pages 733 X 736. Associ-ation for Computational Linguistics, 2010.
 Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gre-gory Shakhnarovich. A systematic exploration of diversity in machine translation. In Pro-ceedings of the 2013 Conference on Empiri-cal Methods in Natural Language Processing , pages 1100 X 1111, Seattle, Washington, USA, October 2013. Association for Computational
Linguistics. URL http://www.aclweb. org/anthology/D13-1111 .
 Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association , 102(477):359 X 378, 2007.
 Joshua Goodman. Parsing algorithms and met-rics. In Proceedings of the 34th Annual Meet-ing of the Association for Computational Lin-guistics , pages 177 X 183, Santa Cruz, Califor-nia, USA, June 1996. Association for Com-putational Linguistics. doi: 10.3115/981863. 981887. URL http://www.aclweb.org/ anthology/P96-1024 .
 Aria Haghighi and Dan Klein. Unsuper-vised coreference resolution in a nonparametric
Bayesian model. In Annual Meeting, Associa-tion for Computational Linguistics , volume 45, page 848, 2007.
 Robert E. Kass, Bradley P. Carlin, Andrew Gel-man, and Radford M. Neal. Markov chain
Monte Carlo in practice: a roundtable discus-sion. The American Statistician , 52(2):93 X 100, 1998. Shankar Kumar and William Byrne. Minimum
Bayes-risk decoding for statistical machine translation. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings , pages 169 X 176, Boston,
Massachusetts, USA, May 2 -May 7 2004. As-sociation for Computational Linguistics.
 Kalev Leetaru and Philip A. Schrodt. GDELT:
Global data on events, location, and tone, 1979 X  2012. In ISA Annual Convention , volume 2, page 4, 2013.
 Jimmy Lin and Alek Kolcz. Large-scale ma-chine learning at Twitter. In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data , pages 793 X 804. ACM, 2012.
 Michael C. McCord, J. William Murdock, and Branimir K. Boguraev. Deep parsing in Watson.
IBM Journal of Research and Development , 56 (3.4):3 X 1, 2012.
 David Mimno and David Blei. Bayesian check-ing for topic models. In Proceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing , pages 227 X 237, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguis-tics. URL http://www.aclweb.org/ anthology/D11-1021 .
 Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun X  X chi Tsujii. Evaluating dependency representations for event extraction. In Pro-ceedings of the 23rd International Confer-ence on Computational Linguistics (Coling 2010) , pages 779 X 787, Beijing, China, Au-gust 2010. Coling 2010 Organizing Commit-tee. URL http://www.aclweb.org/ anthology/C10-1088 .
 Allan H. Murphy and Robert L. Winkler. A general framework for forecast verification.
Monthly Weather Review , 115(7):1330 X 1338, 1987.
 Andrew Ng and Michael Jordan. On discrimina-tive vs. generative classifiers: A comparison of logistic regression and naive Bayes. Advances in neural information processing systems , 14: 841, 2002.
 Alexandru Niculescu-Mizil and Rich Caruana.
Predicting good probabilities with supervised learning. In Proceedings of the 22nd Interna-tional Conference on Machine Learning , pages 625 X 632, 2005.
 Brendan O X  X onnor, Brandon Stewart, and
Noah A. Smith. Learning to extract inter-national relations from political context. In Proceedings of ACL , 2013.
 Naoaki Okazaki. Crfsuite: a fast implemen-tation of conditional random fields (CRFs), 2007. URL http://www.chokkan.org/ software/crfsuite/ .
 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Van-derplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Ma-chine learning in Python. Journal of Machine Learning Research , 12:2825 X 2830, 2011.
 John Platt. Probabilistic outputs for support vector machines and comparisons to regularized like-lihood methods. In Advances in large margin classifiers . MIT Press (2000), 1999. URL http://research.microsoft.com/ pubs/69187/svmprob.ps.gz .
 Sameer Pradhan, Lance Ramshaw, Mitchell Mar-cus, Martha Palmer, Ralph Weischedel, and Ni-anwen Xue. CoNLL-2011 shared task: Mod-eling unrestricted coreference in Ontonotes.
In Proceedings of the Fifteenth Conference on Computational Natural Language Learning:
Shared Task , pages 1 X 27. Association for Com-putational Linguistics, 2011.
 Jonathon Read. Using emoticons to reduce depen-dency in machine learning techniques for senti-ment classification. In Proceedings of the ACL
Student Research Workshop , pages 43 X 48. As-sociation for Computational Linguistics, 2005. Evan Sandhaus. The New York Times Anno-tated Corpus. Linguistic Data Consortium , LDC2008T19, 2008.
 Philip A. Schrodt. Precedents, progress, and prospects in political event data. International Interactions , 38(4):546 X 569, 2012.
 Philip A. Schrodt, Shannon G. Davis, and Ju-dith L. Weddle. KEDS  X  a program for the machine coding of event data. Social Science
Computer Review , 12(4):561  X 587, December 1994. doi: 10.1177/089443939401200408.

URL http://ssc.sagepub.com/ content/12/4/561.abstract .
 Sameer Singh, Sebastian Riedel, Brian Martin, Ji-aping Zheng, and Andrew McCallum. Joint in-ference of entities, relations, and coreference.
In Proceedings of the 2013 Workshop on Auto-mated Knowledge Base Construction , pages 1 X  6. ACM, 2013.
 David A. Smith and Jason Eisner. Minimum risk annealing for training log-linear models. In Pro-ceedings of the COLING/ACL 2006 Main Con-ference Poster Sessions , pages 787 X 794, Syd-ney, Australia, July 2006. Association for Com-putational Linguistics. URL http://www. aclweb.org/anthology/P06-2101 .
 Veselin Stoyanov, Alexander Ropson, and Jason
Eisner. Empirical risk minimization of graphi-cal model parameters given approximate infer-ence, decoding, and model structure. In Interna-tional Conference on Artificial Intelligence and Statistics , pages 725 X 733, 2011.
 Kristina Toutanova, Aria Haghighi, and Christo-pher D. Manning. A global joint model for se-mantic role labeling. Computational Linguis-tics , 34(2):161 X 191, 2008.
 John W. Tukey. Curves as parameters, and touch estimation. In Proceedings of the Fourth
Berkeley Symposium on Mathematical Statis-tics and Probability, Volume 1: Contributions to the Theory of Statistics , pages 681 X 694,
Berkeley, Calif., 1961. University of Califor-nia Press. URL http://projecteuclid. org/euclid.bsmsp/1200512189 .
 Ashish Venugopal, Andreas Zollmann, Noah A.
Smith, and Stephan Vogel. Wider pipelines: N-best alignments and parses in MT training. In Proceedings of AMTA , 2008.
 Larry Wasserman. All of nonparametric statistics . Springer Science &amp; Business Media, 2006. Bianca Zadrozny and Charles Elkan. Transform-ing classifier scores into accurate multiclass probability estimates. In Proceedings of KDD , pages 694 X 699. ACM, 2002.
