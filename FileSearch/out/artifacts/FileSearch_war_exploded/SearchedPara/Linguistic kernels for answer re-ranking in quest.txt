 Alessandro Moschitti, Silvia Quarteroni * 1. Introduction
Automatic question answering (QA) systems return concise answers  X  i.e. sentences or phrases  X  to questions in natural information needs; on the other, the high linguistic complexity of QA systems suggests a need for more advanced natural Thompson, Callan, Terra, &amp; Clarke, 2004).

In question processing, useful information is gathered from the question to create a query; the latter is submitted to the document retrieval module that provides the set of the most relevant documents. The latter are used by the answer in terms of predicate argument structures (PASs) ( Shen &amp; Lapata, 2007 ).

In contrast, supervised machine learning methods that learn to rank answers from examples of question and answer pairs 2007) and partial tree kernel (PTK) ( Moschitti &amp; Quarteroni, 2008 ), that exploit PASs in PropBank question word.
 els and gradually introduce more and more advanced language technology. In more detail, we: (i) model sequence kernels encode deeper syntactic information and more structured shallow semantics and (iii) analyze the proposed shallow seman-swer re-ranking.

It is worth noting that, since finding a suitable question answering corpus for our study was difficult, tion questions from TREC 2001 ( Voorhees, 2001 ), whose answers, retrieved from Web and TREC data, respectively, were manually annotated by our group.
 pairs and use the latter to extract an effective feature vector representation.
Our main findings are that (i) kernels based on PAS, POS-tag sequences and syntactic parse trees improve on the BOW approach on both datasets: on TREC-QA, the improvement is high (about 63% in F1 score), making its application worth-QA basic system by 13%, confirming its promising applicability. Such improvement is much larger on WEB-QA.
In the remainder of this paper, Section 2 presents our use of kernel functions for structural information and Section 3 conclusions are drawn in Section 6 . 2. Kernel methods for structured data
Kernel methods refer to a large class of learning algorithms based on inner product vector spaces, among which support vector machines (SVMs) are well-known algorithms. The main idea behind SVMs is to learn a hyperplane / : O ! R n , where O is the set of objects; o is categorized in the target class only if H  X  ~ x  X  P 0. By exploiting the  X  X  X ernel trick X , the decision hyperplane can be rewritten as: where y i is equal to 1 for positive examples and to 1 for negative examples, a training instances and the product K ( o i , o )= h / ( o
Note that instead of applying the mapping / , we can directly use K ( o implicitly evaluated.

Taylor and Cristianini (2004) to evaluate the number of subsequences between two sequences, the syntactic tree kernel a yet more general representation of trees in terms of tree fragments. 2.1. String kernels
The string kernels that we work with count the number of substrings shared by two sequences containing gaps, i.e. some the following. Let R be a finite alphabet: R  X  S 1 n  X  0 R length of r , that can be written as s 1 , ... , s j s j with s character. Now, u is a subsequence of r if there is a sequence of indices u  X  s i 1 ... s i j u j or u  X  r  X  ~ I in short. Moreover, d  X  i.e. d  X  ~ I  X  X  i j u j i 1  X  1. Finally, given r 1 , r 2 use the following functions: for some k 6 1. These functions count the number of occurrences of u in the string r and assign them a weight k tional to their length. Hence, the inner product of the feature vectors for two strings r subsequences weighted according to their length and occurrence frequency: whole words, as in the word sequence kernel ( Cancedda, Gaussier, Goutte, &amp; Renders, 2003 ). 2.2. Tree kernels
The main idea underlying tree kernels is to compute the number of common substructures between two trees T without explicitly considering the whole fragment space. Let F  X f f and T 2 is defined as where N T 1 and N T 2 are the sets of nodes in T 1 and T 2
The D function is equal to the number of common fragments rooted in nodes n semantic tree fragments (SSTFs) ( Moschitti et al., 2007 ), and partial tree fragments (PTFs) ( Moschitti, 2006 ). 2.2.1. Syntactic tree kernel (STK) the production rules contained in the STF cannot be partially applied.

To compute the number of common STFs rooted in n 1 and n 2 ( Collins &amp; Duffy, 2002 ): 1. if the productions at n 1 and n 2 are different then D ( n 2. if the productions at n 1 and n 2 are the same, and n 1
D ( n 1 , n 2 )= k ; 3. if the productions at n 1 and n 2 are the same, and n k
The computational complexity of STK is O  X j N T 1 jj N T 2 average running time is linear in the number of tree nodes. 2.2.2. Shallow semantic tree kernel (SSTK) the algorithm: 0. if n 1 (or n 2 ) is a pre-terminal node and its child label is null , D ( n The above steps do not change the computational complexity of the original algorithm, which is therefore O  X j N 2.2.3. Partial tree kernel (PTK) for the partial tree kernel (PTK) is the following. Given two nodes n substrings. More formally: 1. if the node labels of n 1 and n 2 are different then D ( n 2. else where ~ I 1  X h h 1 ; h 2 ; h 3 ; ... i and ~ I 2  X h k 1 ; k the child subsequences with respect to the original sequence, to account for gaps. It follows that labels ( Moschitti, 2006 ). 2.3. Kernel engineering structures produce new kernels. Indeed, let K( t 1 , t 2 )= / ( t and t 2 into two new structures s 1 and s 2 with a mapping / that is a noticeably different kernel induced by the mapping / PAS
PTK and POS SK , obtained by applying PTK and SK to predicate argument structures and sequences of Part of Speech Tags, respectively. 3. Relational representations for question and answer pairs Chua, 2003; Hovy et al., 2001; Wu, Zhang, Hu, &amp; Kashioka, 2007 ).
 tween pairs clearly depends on syntactic and semantic properties; thus, in addition to the usual bag-of-word approach (BOW), we study methods to capture Q/A structures using string kernels over word and POS-tag sequences and tree kernels our approach and the choice of such features. 3.1. Classification of paired texts operates by comparing the question and answer contents in a separate fashion rather than just comparing a question with its corresponding candidate answers. In a learning framework where kernel functions are deployed, given two pairs p = h q 1 , a 1 i and p 2 = h q 2 , a 2 i , a kernel function is defined as tween kernels, e.g. sum or multiplication.
 questions and answers, respectively. In the following sections we describe several of such linguistically motivated representations. 3.2. Representation via word and POS-tag sequences and trees nel on the sequence of POS-tags of a question or answer. For example, given the sentence s POS sequence is WP AUX NN ? and possible subsequences extracted by POS
STK accepts the following syntactic parse tree for s 0 : 3.3. Shallow semantic representation would imply knowing the correct definition and comparing the current candidate to the former. When such information is unavailable (as in open domain QA) the learning algorithm must mimic the behavior of a human who does not know ment structures ( Bilotti, Ogilvie, Callan, &amp; Nyberg, 2007), described hereafter. 3.3.1. Predicate argument structures the subject of the second, it is the  X  X heme X  in both sentences.

To represent PASs in the learning algorithm, we work with two types of trees: shallow semantic trees for SSTK and shal-low semantic trees for PTK, both following PropBank definition, denoted by PAS matically generated by our system using the Semantic Role Labeling system described in Moschitti, Coppola, Giuglea, and
Basili (2005). As an example, let us consider sentence s 1 tation a 1 :[ A 1 Autism] is [ rel characterized] [ A 0 by a broad spectrum of behavior] [ inattention to surroundings and hypersensitivity to sound and other stimuli].

Such an annotation can be used to design a shallow semantic representation to be matched against other semantically similar sentences, e.g. s 2 :  X  X anic disorder is characterized by unrealistic or excessive anxiety X , resulting in a order] is [ rel characterized] [ A 0 by unrealistic or excessive anxiety].
 nitions and the latent semantics they contain (inherent to behavior, disorder, anxiety) are similar. Indeed, s as a definition even to one who only knows what the definition of autism looks like. The above annotation can be compactly represented by predicate argument structure (PAS) trees such as those in Fig. 3 . by PTK from their respective PASs, as illustrated in Fig. 3 c. An equivalent PAS representation (PAS 3.3.2. PTK vs. SSTK applied to PAS A comparison between SSTK and PTK suggests the following remarks: first, while PAS two fragments of Fig. 3 c and their equivalent in Fig. 4 b.

PASs is performed, since only nodes that are actually useful are represented.
Third, although the computational complexity of PTK is greater than the one of SSTK, the structures to which PTK is ap-plied are much smaller than those input to the SSTK. This makes PTK more efficient than SSTK. We show in the experiment section that the running time of PTK is much lower than that of SSTK (for both training and testing). the same argument order in PAS SSTK .

A1 i (cf. Section 2.1 ). This is another important property for modeling shallow semantic similarity. 3.4. YourQA, a baseline QA system
As mentioned earlier, our research focus is on non-factoid question answering, where the expected answer type mainly factoids.
 2001 edition of the major QA evaluation campaign, remains to our knowledge the first and one of the few events where a
In order to experiment with classifiers and re-rankers, an ordered list of candidate answers to each question is needed signed to address both factoid and non-factoid questions and return answers alternatively from the Web or from a closed corpus.

YourQA is organized according to three phases: question processing, document retrieval and answer extraction. During gine rank of the answer source document is used as a tie-breaking criterion.

In particular, based on the outcome of the question classifier, the answer extraction module determines whether the ex-tained in each candidate answer sentence is pinpointed down to the phrase or word level using relevant factoid QA phrase-prepositional phrase (NP-VP-PP) groups ( hd ): matches between the question keywords q i , with i &lt; j q j , and the candidate answer keywords a dividing by the number of question keywords, j q j : bow  X  q ; a  X  X  an answer, we resort to n -gram similarity, defining ng  X  q ; a  X  X  grams between q and a and ngrams ( q ) is the set of question n -grams. In the current version of YourQA, n =2.
Furthermore, chunk similarity chk ( q , a ) is a function of the number of common sentence chunks tween maxVP q and maxVP a ; HNP ( q , a ) is the number of common tokens between the head NPs associated with maxVP
VP a , respectively, and PP ( q , a ) is the number of common tokens between the PPs associated with maxVP a = 0.6, b = 0.2, c = d = 0.1.
 h q , a 1 i and h q 2 , a 2 i , and compare q 1 with q 2 and a will be processed so that rather than  X  X  X uessing X  correctness based on words or structures shared by q will be compared to their correspondent components q 1 and a structures.

To exemplify this, if q 1 is  X  X  X hat is autism? X  and the candidate answers are a ease X  vs a 0 1  X  X  X utism affects many people X , comparison with the correct pair formed by q a  X  X  X  golden parachute may be defined as a manager X  X  privilege X  will induce the kernel method to prefer a has a similar wording and structure to a 2 , hence h q 1 , a trast, this would not be the case using a similarity score matching q  X  X  X utism X .

This intuitively explains why even a bag-of-words kernel adjusting its weights on question/answer pairs has a better chance to produce better results than a bag-of-words question/answer similarity (or a variation thereof as implemented in Section 4.4 . 3.5. The YourQA corpora: WEB-QA and TREC-QA
In order to obtain our answer corpora, during the Document Retrieval phase, YourQA worked alternatively with two IR engines: Google, 6 to retrieve Web documents, and Lucene, the TREC competition, AQUAINT 6. 8 The two corpora are henceforth named WEB-QA and TREC-QA, respectively. not. However, the TREC-QA corpus was necessary to align with the methodology followed by traditional QA system evalu-ation drawn from IR on a closed corpus.

The answers returned by YourQA are in the form of sentences with relevant words or phrases highlighted and surrounded dhar, 2009 ). Each sentence of the top 20 paragraphs returned by YourQA was manually evaluated by two annotators based on whether or not it contained a correct answer to the corresponding question. The inter-annotator agreement was judged substantial (Cohen j = 0.635).
 as a positive instance if it answered the question, negative otherwise. without backbones X  was labeled +1. The resulting WEB-QA corpus contains 1309 sentences, 416 of which are positive; the
TREC-QA corpus contains 2256 sentences, 261 of which are positive. 4. Experiments
The aim of our experiments is twofold: on one hand, we demonstrate that our supervised approach applying kernels to quence, syntactic and shallow semantic tree kernels provide important linguistic information to describe the above-men-such as YourQA.
 In more detail, we test the kernel functions elaborated for question and answer representation against the WEB-QA and combinations for complex QA (Section 4.3 ). Our results show that our POS 4.1. Experimental setup
To run our experiments, we implement the following functions:
Finally, we implement combinations of the above kernels in the SVM-light-TK toolkit, functions in SVM-light ( Joachims, 1999 ).

Since answers often contain more than one PAS (see Fig. 3 a), we sum PTK (or SSTK) applied to all pairs P and P 2 are the set of PASs of the first two answers. More formally, let P ments t and t 0 by the PTK; the resulting kernel is questions and answers; the only exception are PAS PTK and PAS each reported value in our figures refers to the average over 5 different samples using five-fold cross-validation. 4.2. PTK vs. SSTK: performance and efficiency We compute the classification accuracy of SVMs by using either the PAS more correct answers (balanced classification problems are generally easier to solve). average running time. 4.3. Results for question X  X nswer classification
In these experiments, we test different kernels and some of their most promising combinations. Since the nature of the resentation instead of the more appropriate name combination (representation and kernel). In other words, we use BOW, to parse tree (PT). In the other notations, i.e. POS SK , PAS indicating the Word Sequence Kernel, i.e. a string kernel applied to word sequences.
To produce kernel combinations, we use the sum between kernels ual kernels ( Shawe-Taylor &amp; Cristianini, 2004 ).
 tween Precision and Recall; this is in order to verify whether any difference between models is systematically observed validation splits (Section 4.3.3 ). 4.3.1. F1 curves
First, we note that BOW achieves very high accuracy, comparable to the accuracy of PT; this is surprising when consid-
However, error analysis reveals a number of common patterns in the answers due to typical Web page phrasings that indi-cate if a retrieved passage is an incorrect answer, e.g. Learn more about X
This further motivates our experiments with the TREC-QA dataset, which is cleaner from a linguistic viewpoint and also more complex from a QA perspective as it contains fewer positive instances.
 Fig. 7 also shows that the BOW+PT combination improves on both individual models; however, POS+PT produces a lower Finally, both PAS SSTK and PAS PTK improve on previous models, yielding the highest result (PT+WSK+ PAS and PAS PTK (or even PAS SSTK ) considerably improves on BOW.

In summary, our results for WEB-QA strengthen the expectation that BOW may be outperformed by structural informa-than for the WEB-QA dataset. Indeed, BOW shows the lowest accuracy and also the accuracy of its combination with PT is lower than the one of PT alone. Interestingly, this time POS seems helpful since its combination with PT improves on PT tion as POS SK +PT improves on POS+PT, yet PAS adds further useful information as the best models are POS POS compare the accuracy of different models at a fixed cost-factor parameter. 4.3.2. Pointwise estimation and significance of results
The plots representing F1 vs. the cost-factor parameter suggest that the value of such parameter maximizing F1 can be obtained via cross-validation using the paired t -test.

Table 1 reports the average F1  X  the standard deviation over five folds achieved by the different kernels on the WEB-QA corpus. When examining our results, we note that: Despite the observed improvement on BOW in terms of F1 averaged over five folds, none of the results achieved on the most performing combinations of syntactic and shallow semantic information, exhibiting an improvement up to 3 points obtained on the TREC-QA corpus, reported in Table 2 . A comparative analysis with respect to Table 1 suggests that: tion with its answer set originates training pairs with a large word overlap, BOW tends to overfit.
Secondly, the results show that PT is important to detect typical description patterns, however POS sequences provide by about 4%. This is a relevant result considering that in standard text classification bigrams or trigrams are usually ineffective.
 generated by STK, PAS PTK is still capable to improve on the POS low semantics can be very useful to detect whether an answer is well formed and related to a question. Furthermore, error analysis reveals that PAS can provide patterns like: where X and Y need not necessarily be matched. Finally, the best model, POS and using effective feature engineering techniques such as kernel methods. 4.3.3. Precision/Recall curves curves of some interesting kernels for one of the five folds of the WEB-QA dataset. As expected, BOW shows the lowest curves; moreover, WSK, able to exploit n -grams (with gaps), produces very high curves when summed to PT. In general, of BOW, which prevents other models from clearly emerging.
 ing all the others.
 role of structural data representations in complex question answering. 4.4. Answer re-ranking their binary output to rearrange the answers returned by the YourQA system. Our re-ranking algorithm starts from the ary classifier; otherwise, the rank is pushed down, until a lower ranked incorrect answer is found. of YourQA and its underlying IR engine by examining the F1 and MRR of the answers corresponding to the top five docu-of the above systems is computed by labeling each of the five retrieved answers as correct. accuracy. Indeed, in the WEB-QA dataset, the IR engine (Google) is outperformed by YourQA since its ranks are based on for such documents, the emphasis on Precision provided by the YourQA answer extractor yields an increase in answer relevance.
 outperformed by YourQA and the re-ranker produces a further improvement by about 13%. Such a smaller difference com-ity of the TREC dataset.

It may be noted that having proven that re-ranking is effective in improving QA systems does not imply that structural complex questions. 4.4.1. Discussion out a study on about twenty-five different kernels, we preferred to keep our models simpler.
A second option to improve our methods while keeping the model complexity low would be the use of the classifier score for another question a difference of 0.005 may indicate very high reliability for the higher-scored classification. threshold. Thus, we argue that our heuristic approach of pushing answers down in the ranking when they are labeled as incorrect is more conservative and has a higher chance to improve the basic QA.

A possible alternative would be the conversion of SVM scores into actual probabilities, however once again these would the target answer. The above approaches are interesting research directions, albeit beyond the aim of this paper. 5. Related work
Early work on the use of syntax and semantics in Information Retrieval was carried out in Voorhees (1993), Voorhees question answering work shows that semantics and syntax are essential to retrieve punctual answers, e.g ( Hickl et al., 2006; Voorhees, 2004; Small et al., 2004 ). However, successful approaches in TREC-style systems were based on several depended on manual setting, which was often not disclosed.

In our study, we avoid this problem by focusing on a single phase of question answering, namely answer extraction. The et al., 2007; Surdeanu, Ciaramita, &amp; Zaragoza, 2008 ).

In Chen et al. (2006) , answer ranks were computed based on the probabilities of bigram language models generating can-
Our approach is different from the above in that we attempt to capture structural information, which has proven to be and TREC documents and we could show that the potential improvement reachable by our approach is much higher (about 63% over BOW). Moreover, we have designed a faster kernel for the processing of semantic information.
In summary, the main property of our approach with respect to previous work adopting syntactic and semantic structures ing algorithm robust to many irrelevant features (often produced by NLP errors). 6. Conclusions
We have approached answer selection, the most complex phase of a QA system. To solve this task, typical approaches use unsupervised methods that involve computing the similarity between query and answer in terms of lexical, syntactic, of back-off model in a discriminative setting, that we have proved to be effective.
In particular, we use POS-tag sequences, syntactic parse trees and predicate argument structures (PASs) along with se-quence kernels and syntactic and shallow semantic tree kernels. Extensive experiments on two different corpora that we its promising applicability.
 Regarding PAS, deeper analysis reveals that PTK can learn definition patterns such as: A1(Y) (e.g.  X  X erman measles, that result in red marks on the skin, are a common disease X ) and: A1(X) rel(characterize) A0(Y) (e.g.  X  X utism is characterized by the inability to relate to other people X ). ing natural language tasks.

In the future, we would like to experiment with our model on larger and different datasets and compare with (or better re-rank) more advanced QA systems. Moreover, an interesting open problem is how to jointly exploit the set of PASs of a sentence/paragraph in a more effective and compositional semantics-driven approach. References
