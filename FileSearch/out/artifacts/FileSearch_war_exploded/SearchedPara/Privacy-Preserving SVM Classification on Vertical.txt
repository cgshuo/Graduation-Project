 The goal of data mining is to efficiently analyze large quantities of data to find inter-esting patterns and/or summarize the data in novel ways. Classification is one of the most common applications found in the real world. The goal of classification is to build a model which can predict the value of one variable, based on the values of the other variables. For example, based on financial , criminal and travel data, one may want to classify passengers as security risks. In the fi nancial sector, categorizing the credit risk of customers, as well as detecting fraudulent transactions are both classification prob-lems. Numerous such problems abound.

There is considerable research on differen t classification algorithms. Indeed, several different solutions are commonly used in the real world. A basic assumption is that complete access to data is available, either in centralized or federated form. However, privacy and security concerns restrict acces s to data. Sharing of data may not be possible due to either legal or commercial reasons. For example, due to HIPAA laws [1], medical data cannot be released for any purpose without appropriate anonymization. Similar constraints arise in many applications. European Community legal restrictions apply to disclosure of any individual data.Customer da ta, process data, etc., is often a valuable business asset for corporations. For example, complete manufacturing processes are trade secrets (although individual techniques may be commonly known). All of these cases require distributed knowledge discovery, without the disclosure of data. (Section 5 discusses related work in this area of Privacy-Preserving Data Mining.)
We assume vertically partitioned data with at least three participating parties, i.e. , three or more parties that collect differ ent information about th e same set of entities. For instance, a bank, health insurance company and auto insurance company collect different information about the same people. A bank has customer information like av-erage monthly deposit, account balance. The health insurance company has access to medical information and other policy information. The car insurance company has ac-cess to information such as car type, accident claims, etc. Together, they might evaluate if the person is a credit risk for life insurance.

Support Vector Machine (SVM) classification is one of the most actively developed methodologies in data mining. SVM has proven to be effective in many real-world applications [2]. Like other classifiers, t he accuracy of an SVM classifier crucially de-pends on having access to the correct set of data. Data collected from different sites is useful in most cases, since it provides a better estimation of the population than the data collected at a single site.

In this paper, we propose a privacy-preserving SVM (support vector machine) clas-sification method on vertically partitioned data, PP-SVMV for short, such that each party (e.g., bank, insurance company) need not disclose its data or general information to other parties while still acquiring the same SVM classification accuracy as when the data is centralized. Our algorithm is e fficient and secure. We first overview SVM (Section 2) and develop our PP-SVM technique (Section 3). We empirically show the practicality of our method in Section 4. Finally, related work is discussed in Section 5. We first describe the notation to overview SVM. All vectors are column vectors unless transposed to a row vector by a prime superscript . The scalar (inner) product of two vectors x and y in the n -dimensional real space R n is denoted by x y and the 2-norm of x is denoted by || x || .An m  X  n matrix A represents m data points in a n -dimensional input space. An m  X  m diagonal matrix D contains the corresponding labels ( i.e. ,+1 or -1) of the data points in A . (A class label D ii ,or d i for short, corresponds to the i -th data point x i in A .) A column vector of ones of arbitrary dimension is denoted by e . The identity matrix of arbitrary dimension is denoted by I .

First, consider a linear binary classifica tion task, as depicted in Figure 1. For this problem, SVM finds the separating hyperplane ( w  X  x =  X  ) that maximizes the mar-gin , denoting the distance between the hyperplane and closest data points ( i.e. , support vectors). In practice, we use the  X  X oft X  margin to deal with noise, in which the distance from the boundary to each support vector could be d ifferent. The  X  X ard X  margin is for-mulated as 1 || w || , as illustrated in Figure 1. To maximize the margin while minimizing the error, the standard SVM solution is formulated into the following primal program [2, 3]: which minimizes the reciprocal of the margin ( i.e. , w w ) and the error ( i.e. , e y ). By having the slack variable y in the constraint (2), SVM allows error or the soft margin. The slack or error is minimized in the objective function (1) and it will be larger than zero when the point is on the wrong side or within the margin area. The soft margin parameter  X  (a user parameter) is tuned to balance the margin size and the error. The weight vector w and the bias  X  will be computed by this optimization problem. Once w and  X  are computed, we can determine the class of a new data object x by f ( x )= w x  X   X  , where the class is positive if f ( x ) &gt; 0 ,orelse negative .

In order to reduce the number of variables in the objective function and also be able to apply the kernel trick, we transform the primal problem to the following dual problem by applying the Largrange multipliers: vector x i . The coefficients  X  are to be computed from this dual problem. An m  X  m where K ( x i ,x j )= x i  X  x j for linear SVM. The support vectors are the data vectors { x i } such that the corresponding coefficients  X  i &gt; 0 . The weight vector w =  X  i d i x i and thus the classification function f ( x )=  X  i d i x i  X  x  X   X  for linear SVM. For nonlinear SVMs, f ( x )=  X  i d i K ( x i ,x )  X   X  , where we can apply a nonlinear kernel for polynomial kernel, ). [2] provides further details on SVM. To generate the global SVM model ( i.e. , the SVM model constructed from the data from multiple parties) without sharing any data among the parties, (1) the framework must be able to generate the global model only from models locally constructed by parties on their own data, without seeing others X  data. We call this requirement data privacy . To prevent disclosing the general classifi cation information on each party, (2) the local model must not be disclosed when jointly generating the global model. We call this requirement model privacy .

These two requirements lead to design our PP-SVM framework illustrated in Fig-ure 2. (Figure 2 involves only three parties but can be generalized to more.) Each party builds a local model L from its own data A ( 1 ), and each party securely merges its model with others ( 2 ), in order to generate the global model G ( 3 ). The global model G will be the same for every party, which will be used for classifying new data ob-jects. Assuming that the merge of the local models is done securely, this framework A 1 , A 2 , A 3 ). This section presents techniques that implement the framework. First, we discuss the choice for the local model L . Then, we present a method to securely merge the local models. 3.1 Local Model As we see from the last paragraph of Section 2, an SVM model is represented by the bias  X  , and a list of support vectors, their labels, and coefficients { ( x i ,d i , X  i ) } such computed from the dual problem in Section 2.

Given vertically partitioned data over multiple parties, we cannot use a local SVM model ( i.e. , computed only over local data) for our local model L in the framework (Fig-ure 2), because the global SVM model G cannot be built only from local SVM models; The globally optimal coefficients (computed by the dual problem) will be different from the locally optimal coefficie nts computed on local data. Since each party has the data of an attribute subset, the dual problem on the attribute subset will not generate the globally optimal coefficients. Thus, in our framework, the local model L needs to go beyond the standard SVM model.

To solve the dual problem globally, we need the m  X  m matrix Q = K ( x i ,x j ) d i d j in Eq.(3) which is computed over the data of all the attributes. The diagonal matrix D for d i is given as class labels, thus we only need to compute the global kernel matrix K = K ( x i ,x j ) . For linear kernel where K ( x i ,x j )= x i  X  x j , the global matrix K can be directly computed fro m local matrices because K is a gram matrix and a gram matrix can be merged from gram matrices of vertically partitioned data, as Lemma 1 proves. Lemma 1. Suppose the m  X  n data matrix A is vertically partitioned into A 1 and A 2 as Figure 3 illustrates. Let K 1 and K 2 be the m  X  m gram matrices of matrices A 1 and A 2 respectively. That is, K 1 = A 1 A 1 and K 2 = A 2 A 2 . Then, K , the gram matrix of A , can be computed as follows: Proof. An ( i, j ) th element of K is x i  X  x j ,where x i and x j are i th and j th data vectors in
A .Let x 1 and A 2 respectively. Then, From Eq.(6), each element in K is equal to the sum of the elements in K 1 and K 2 . Thus K Lemma 1 proves that local gram matrices are sufficient to build the global gram matrix which is the kernel matrix K for linear kernel. Some popular nonlinear kernel matri-ces can also be computed from the gram matrix: The polynomial kernel is represented by a dot product of data vectors (i.e., K ( x i ,x j )=( x i  X  x j +1) p ). The RBF ker-nel can also be represented by dot products (i.e., K ( x i ,x j )=exp(  X  || x i  X  x j || 2 g )= to construct the global kernel matrix K for nonlinear kernels such as polynomial and RBF which can be represented by dot products.

Thus, we use the local gram matrix as the local model L in our framework. Sec-tion 3.2 discusses how to merge L securely from each party to securely build the global gram matrix. Once the global gram matrix is built, each party can run a quadratic pro-gramming solver to compute the global SVM model G , which will be the same for every party. 3.2 Secure Merge of Local Models To keep both data and model privacy, it is necessary to securely merge the local models which are the m  X  m local gram matrices. A secure addition mechanism for m  X  m matrices is required. For k  X  3 parties, we developed such a method based on simple secure addition of scalars.

We first describe a simple method to securely calculate the sum of integers from individual sites under the assumption that there are at least three parties and the parties do not collude. We then extend the method so as to seamlessly merge the local models with high efficiency and privacy.
 Secure Sum of Integers: Formally, we assume k  X  3 parties, P 0 ,...,P k  X  1 , with party P holding value v i . Together they want to compute the sum v = k  X  1 i =0 v i . Assume that the sum v is known to lie in a field F .

The parties also randomly order themselves into a ring. The ordering can be selected by one of the parties, or by a third party. If the parties cannot decide on a suitable order and no third party can be found, then a protocol developed by Sweeney and Shamos can be used [4] to fix upon a random ordering. The protocol developed by Sweeney and Shamos is quite efficient and requires only O ( k ) communication. For this paper, to simplify the presentation, without loss of generality, we assume that this order is the canonical order P 0 ,...,P k  X  1 . In general, any order can be decided on. The protocol proceeds as follows:
P 0 randomly chooses a number R , from a uniform distribution over F . P 0 adds this to its local value v 0 , and sends the sum R + v 0 mod |F| to site P 1 . For the remaining sites P i ,i =1 ,...,k  X  1 , the algorithm proceeds as follows: P i receives P i then computes and passes it to site P i +1 (mod k ) . Finally, P 0 , subtracts R from the final message it gets (i.e., adds  X  R (mod |F| ) ) to compute the actual result.

Clearly, the above protocol correctly calcu lates the required sum. In order to eval-uate the security of the protocol, it is necessary to have a definition of what is meant by security. The area of Secure Multi-Party Computation (SMC) provides a theoretical framework for defining and evaluating secure computation. This protocol can be proven to be completely secure under our assumptions in the SMC framework. A complete proof of security is presented in our technical report [5].
 Secure Sum of Matrices: We can extend the secure addition of scalars to securely adding matrices. The key idea is as follows. Suppose a master party wants to merge ( i.e. , add) its local matrix with those in other slave parties. We assume that the parties have arranged themselves in some sequence and the master initiates the protocol. 1. The master party creates a random matrix of the same size as its local matrix. (The 2. The master party merges (adds) the random matrix with its local matrix, sends the 3. Each slave party, receives the perturbed matrix, merges it with its local matrix and 4. The master subtracts the random matrix from the received matrix, which results in
All addition is done in a closed field, and s ubtraction refers to addition of the com-plement. This secure addition mechanism is proven to be secure and efficient [5]. The extra computation required by the first part y is the generation of the random matrix and the final subtraction. In terms of communication overhead, k rounds are required for ev-ery party to acquire the summed matrix, where k is the number of participating parties. One problem with the matrix summation method is that it is vulnerable to collusion. The parties preceding and following a party , can collude to recover its local matrix. However, the technique can easily be made collusion resistant to q parties by splitting up the local matrices into q random parts and carrying out the addition protocol q times. The sum of the final matrices from all q rounds gives the real global matrix. As long as the parties are ordered differently in each run, recovery of a local matrix is only possible if collusion occurs in all q rounds. Further details can be found in [5]. 3.3 Security Our method preserves  X  X ata privacy X , since only the original party gets to exactly see the data; The local model is directly comput ed from the local data. However, to ensure  X  X odel privacy, X  we need at least three par ticipating parties; Each party gets the final global model, which is simply the sum of local models. Thus, with only two parties participating, the other party X  X  local matrix could be found simply by subtracting the local model from the global model. What is revealed, is the sum of the local models of the other parties. Since the SVM requires knowing the global matrix, this is always possible from the final result as well, and so is unavoidable.

We still need to analyze the effects of knowing the sum of gram matrices computed over the attributes of other parties. In general, the number and type of attributes of the other parties are still assumed to be unkno wn. As such, the summed matrix does not disclose any attribute values. If the exact number and types of attributes of the other parties are known, a number of quadratic equations will be revealed in the attribute val-ues; Every cell of the gram matrix corresponds to a dot product  X  thus the quadratic equation. Since the matrix is symmetric, there are a total of m ( m +1) / 2 distinct equa-tions (where m is the number of data objects). If the number of total variables (i.e., the sum of all the attributes of other parties) is larger than m ( m +1) / 2 , it is impossible to recover the exact attribute values. Knowing that the matrix is symmetric and positive semidefinite does not disclose further information. While this does reveal more infor-mation than strictly necessary, this is a trade off in the favor of efficiency. If complete security is required, the summed matrix could be kept randomly split between two of the parties, and an oblivious protocol run to compute the global model using the generic circuit evaluation technique developed for secure multiparty computation [6, 7]. The goal of the experiments is simply to demonstrate the scalability of our PP-SVMV. The accuracy will be exactly the same as tha t of SVM when the data is centralized. We revised the sequential minimal optimization (SMO) source 1 to implement the PP-SVMV. We used the Tic-Tac-Toe data set included in the SMO package for our exper-iment. We sampled around 958 data objects ( m ) and extracted around 27 features ( n ). PP-SVMV generates above 99% with an RBF kernel which is the same as that of the original SVM when the data is centralized.
To check the scalability of the PP-SVMV on an increasing number of participating parties, we vary the number of parties from three to ten. We divide the 27 features about equally between the participating parties. For instance, when ten parties participate, three parties have two features, and the other seven parties have three features. Figure 4 shows results of our experiments: The total training time (including the parallel local computations) hardly changes; SVM is sensitive to the number of data objects more than the features, and the change on the number of features are not visibly influential to the total training time. The difference of the communication time is also not visible due to the dominant computation time. The results are averaged over ten runs. Recently, there has been significant interest i n the area of Privacy-Preserving Data Min-ing. We briefly cover some of the relevant work. Several solution approaches have been suggested. One approach is to perturb the local data (by adding  X  X oise X ) before the data mining process, and mitigate the impact of the noise from the data mining results by using reconstruction techniques [8]. However, there is some debate about the security properties of such algorithms [9, 10]. The alternative approach of using cryptographic techniques to protect privacy was first utilized for the construction of decision trees [11]. Our work follows the same approach. A good overview of prior work in this area can be found in [12]. Recently, some alternat ive techniques such a s condensation[13] and transformation [14] have also been proposed.

In terms of data mining problems, work addressed includes association rule mining [15], clustering [16, 17], classification [18], and regression [19, 20]. All of the crypto-graphic work falls under the theoretical fra mework of Secure Multiparty Computation. Yao first postulated the two-party comparison problem (Yao X  X  Millionaire Protocol) and developed a provably secure solution [6]. This was extended to multiparty computations by Goldreich et al. [7]. The key result in this field is that any function can be computed securely. Thus, the generic circuit evaluation technique can be used to solve our current problem. However, the key issue in privacy-preserving data mining is one of efficiency. The generic technique is simply not efficie nt enough for large qua ntities of data. This paper proposes an efficient technique to solve the problem.

Yu and Vaidya [21] developed a privacy-preserving SVM classification on horizon-tally partitioned data. Since their method is based on the trick of the proximal SVM [3], it is limited to linear classification. Our PP-SVMV is the first one proposing a se-cure SVM classification on vertically partitioned data, which uses the techniques of the secure matrix addition [5] and distributed SVM [22]. We propose a scalable solution for privacy-preserving SVM classification on vertically partitioned data (PP-SVMV). With three or more participating parties, our method PP-SVMV securely computes the global SVM model, without disclosing the data or classi-fication information of each party to the others ( i.e. , keeping the model privacy as well as the data privacy ). Future work may address the idea of efficiently achieving complete security by keeping the global model split between parties as well.

