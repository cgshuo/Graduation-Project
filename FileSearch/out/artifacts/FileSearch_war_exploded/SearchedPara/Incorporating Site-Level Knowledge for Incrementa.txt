 We study in this paper the problem of incremental crawling of web forums, which is a very fundamental yet challeng-ing step in many web applications. Traditional approaches mainly focus on scheduling the revisiting strategy of each in-dividual page. However, simply assigning different weights for different individual pages is usually inefficient in crawling forum sites because of the different characteristics between forum sites and general websites. Instead of treating each in-dividual page independently, we propose a list-wise strategy by taking into account the site-level knowledge. Such site-level knowledge is mined through reconstructing the link-ing structure, called sitemap, for a given forum site. With the sitemap, posts from the same thread but distributed on various pages can be concatenated according to their times-tamps. After that, for each thread, we employ a regression model to predict the time when the next post arrives. Based on this model, we develop an efficient crawler which is 260% faster than some state-of-the-art methods in terms of fetch-ing new generated content; and meanwhile our crawler also ensure a high coverage ratio. Experimental results show promising performance of Coverage , Bandwidth utilization , and Timeliness of our crawler on 18 various forums. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -clustering, information filtering Algorithms, Performance, Experimentation Web Forum, Sitemap, Incremental Crawling
Due to the explosive growth of Web 2.0, web forum (also named bulletin or discussion board) is becoming an impor-tant data resource on the Web 1 , and many research efforts and commercial systems, such as Google, Yahoo!, and Live,  X 
This work was performed when the 3 rd and the 4 th authors were interns in Microsoft Research Asia. http://en.wikipedia.org/wiki/Internet forum have begun to leverage information extracted from forum data to build various web applications. This includes, for example, understanding user opinions from digital product reviews 2 for both product recommendation and business in-telligence, integrating travel notes for travel recommenda-tions 3 , and extracting question-answer pairs for QnA ser-vice [8], etc.

Fetching forum data (mainly user created posts) from var-ious forum sites is the fundamental step of most related web applications. To satisfy the application requirements, an ideal forum crawler should make a tradeoff between updat-ing existing discussion threads ( i.e., completeness )andget-ting new discussion threads in time ( i.e., timeliness ). Com-pleteness refers to identify updated discussion threads and download the newly published content. This is important to applications like online QnA services. As pagination is widely used in web forums, the posts of a question and its best answers are most likely to be distributed in different pages. In this case, a forum crawler should fetch all the pages from a discussion thread to ensure not missing any best-answers. Timeliness refers to efficiently discover and download newly published discussion threads. This is also important because many web applications need fresh infor-mation to response quickly to news events and new product releases. For example, users may be interested in knowing the fair and square feedbacks of iPhone shortly after it was released; and this creates the need to rapidly gather con-sumer comments of iPhone from various web forums.
However, most existing crawling strategies designed for general websites may not be suitable for forum crawling, and cannot well satisfy the above requirements. This is be-cause web forums have some unique characteristics. (I) Con-tent information in forum sites is organized in form of list . Because forum content, such as a long discussion thread, is usually too huge to be displaced in one single page, most web forums adopt pagination to divide long content into multiple pages, as shown in Fig. 1. (II) In web forums, pages can be categorized into different types according to their functions. The most two common page types in web forums are index-of-thread and post-of-thread . As shown in Fig. 1(a), a list of index-of-thread pages (called index list ) shows the basic in-formation (such as title, author, and creation-time) of all the discussion threads in a discussion board or sub-boards; and as shown in Fig. 1(b), a list of post-of-thread pages (called post list ) consists of the detailed content of all the posts from the corresponding thread. Unfortunately, these two charac-teristics are ignored by most e xisting crawlers. They simply www.dpreview.com www.tripadvisor.com (a) An exemplary index list and its index-of-thread pages (b) An exemplary post list and its post-of-thread pages Figure 1: An illustration of (a) Index-of-thread pages 3 in sub-board  X  X etting Started X , (b) Post-of-thread pages 4 in thread  X  X OW TO: Using sort-ing / paging on GridView w/o a DataSourceControl DataSource X , and (c) Data growth of an index list . treat each individual page in a forum as a single object, and optimize the crawling schedule for each page independently. This leads to the following two problems in forum crawling:
Towards a crawler optimized for forum crawling, in this paper, a list-wise incremental crawling strategy is proposed based on the mining of the site-level knowledge from target forums. First, unlike general crawlers which optimize crawl-ing schedule for every individual page, in our strategy we take a list (either index list or post list ) as the basic unit for crawling schedule optimization. Second, to understand the list-structures and the types-of-list of a forum, we propose a set of offline mining algorithms which try to first discover the site-level knowledge from the target sites and then in-corporate the knowledge to optimize the crawling schedules. The contributions of this paper are briefly highlighted as follows: In the experiments, the proposed solution has been evalu-ated on a mirrored data set containing 990,476 pages and 5,407,854 individua l posts, published from March 1999 to June 2008. It showed the crawler with our list-wise strategy is 260% faster than some state-of-the-art methods in terms of fetching the new generated content, and meanwhile our crawler still ensure a higher coverage ratio.

This paper is organized as follows. Related research ef-forts are reviewed in Section 2. In Section 3, we first briefly describe the framework of our approach and then explain each components in detail. Experimental evaluations are reported in Section 4, and we draw some conclusions in the last section.
To the best of our knowledge, little existing work in liter-atures has systematically investigated the problem of forum incremental crawling. However, there are still some previ-ous work that should be reviewed, as our approaches were motivated by them.

Some early work in [2,3] first investigated the dynamic web and treated the information as a depreciating commodity. They first introduced the concepts of lifetime and age of a page which is important for measuring the performance of an incremental crawler. However, they treated every page equally and ignored the importance and change frequency which are also important to an incremental crawler.
Later work improved the above work and optimized the crawling process by prioritizing the pages. Whether to min-imize the age or to maximize the freshness leads to a variety of analytical models by investigating different features. We classify them into three categories: (I) How often the content of a page is updated by its owner. Coffman et. al. [7] analyzed the crawling problem theoreti-cally. Cho et. al. [6] proposed several methods based on the page update frequency. However, most of these methods are based on the assumption that most web pages change as a Poisson or memoryless process. But the experimental re-sult in [3] showed that most web pages are modified during the span of US working hours (between 8 AM and 8 PM, Eastern time, Monday to Friday). This phenomenon is even more evident in forum websites because the content are all generated by forum users. Edwards et. al. [10] tried to use an adaptive approach to maintaining data on actual change rates for the optimization without the above assumption. However, it still treated each page independently and may waste bandwidth. Furthermore, we argue that some other factors, such as time intervals of the latest posts, are more important in web forums. We will show their importance in the experiment part. (II) The importance of each web page. Baeza-Yates et. al. [1] tried to determine the weight of each page based on some strategies similar to PageRank. Wolf et. al. [16] as-signed the weight of each page based on the embarrassment metric of users X  search results. In the user-centric crawl-ing strategy [13], the target s are mined from user queries to guide the refreshing schedule of a generic search engine; First of all, some pages may have equal importance weight but different update frequency, thus only measuring the im-portance of each web page is insufficient. Second, both static rank and content importance are useless in web fo-rums. Most pages in web forums are dynamic pages which are generated using some pre-defined templates. It is very hard to compute their PageRank-like static rank since there are medial links among these pages. Furthermore, the con-tent importance measurement is also useless. Once a post is generated, this post always exists unless the author deletes it manually. Before we get these post information, it is very hard to predict their importance. However, once we have their content importance information we usually do not need to revisit it anymore. Some work named focused crawling at-tempts to only retrieve web pages that are relevant to some pre-defined topics [5, 9, 11] or some labeled examples [14] by assigning pages similar to the target page a higher weight. The target descriptions in focused crawling are quite differ-ent in various applications. (III) The information longevity of each web page. Olston et. al. [12] introduced the longevity to determine revisit-ing frequency of each web page. However, the information longevity in forums is useless since once a post never dis-appears unless being deleted. This is one of the major dif-ferences between general web pages and forum web pages. Moreover, its three generative models are still based on the poisson distribution and some modified forms.

Realizing the importance of forum data and the challenges in forum crawling, Cai et al. [4, 15] studied how to recon-struct the sitemap of a target web forum, and how to choose an optimal traversal path to crawl informative pages only. However, this work only addressed the problem of fetching (b) Prediction Model Figure 2: The flowchart of the proposed solution for incremental forum crawling. as much as possible valuable data, but left the problem of refreshing previously downloaded pages [12] untouched.
All the existing methods ignore the tradeoff between dis-covering new threads and refreshing existing threads. Intu-itively, index-of-thread pages should get higher update fre-quency than post-of-thread pages because any users X  update activities will change index-of-thread pages. However, be-cause the number of post-of-thread pages is much larger than index-of-thread pages, index-of-thread pages may be overwhelmed by a mass of post-of-thread pages even if only a small percentage of post-of-thread pages are very active. These post-of-thread pages will occupy most bandwidth, and as a result new discussion threads cannot be downloaded ef-ficiently. Unfortunately, few of existing methods has taken this issue into account. We will show its importance in the experiment part.
In this section, we first describe the system overview of the proposed solution, and then introduce the details of each component.
The proposed solution consists of two parts, offline mining and online crawling , as shown in Fig. 2.

The offline mining is to learn the sitemap of a given fo-rum site with a few pre-sampled pages from the target site. Through analyzing these sampled pages, we can find out how many kinds of pages are there in that site, and how these pages are linked with each others [4]. This site-level knowledge is then employed in the online crawling process.
The online crawling consists of three steps: (a) identify-ing index lists and post lists ; (b) predicting the update fre-quency of a list using a regression model; and (c) balancing bandwidth by adjusting the numbers of various lists in the crawling queue, as shown in the left part of Fig. 2. Given a new crawled page, we first identify if it is an index-of-thread page or a post-of-thread page according to its layout infor-mation. Since an index list or post list consists of multiple pages, to reconstruct a list, we concatenate corresponding pages following the detected pagination links [15]. Then, for each reconstructed list, we extract the timestamps of the records (either posts or thread entities) containing in that list. Several statistics are then proposed based on the times-tamps to characterize the growth of a list; and a regression model is trained to predict the arrival time of the next record of that list. Finally, given a fixed bandwidth, we balance the numbers of index / post lists, to satisfy the requirements of both completeness and timeliness .
The sitemap knowledge is an organization graph of a give forum site, which is the fundamental component of the pro-posed crawling method in this paper. The details of its generation algorithm can be found in [4, 15], here we just briefly describe it as follows. To estimate the sitemap ,we first sample a few pages from the target site. Because the sampling quality is crucial to the whole mining process, to diversity the sampled pages in terms of page layout and to retrieve pages at deep levels, we adopt a combined strategy of breadth-first and depth-first using a double-ended queue. In the implementation, we try to push as many as possible unseen URLs from each crawled page to the queue, and then randomly pop a URL from the front or the end of the queue for a new sampling. In practice, it was found that sampling a few thousands pages is sufficient to reconstruct the sitemap for most forum sites. After that, pages with similar layout structures are further clustered into groups using the sin-gle linkage algorithm, as marked with blue nodes in Fig. 3. In our approach, we utilize the repetitive regions to charac-terize the layout of each page. Repetitive regions are very popular in forum pages to present data records stored in a database. Considering that two similar pages usually have different numbers of advertisements, images, and even some complex sub-structure embedded in user posts, the repet-itive region-based representation is more robust than the whole DOM tree [17, 18]. Finally, all possible links among various page groups are established, if in the source group there is a page having an out-link pointing to another page in the target group.

After that, a classifier is introduced to identify the node of index-of-thread pages or post-of-thread pages in a sitemap . The classifier is built by some simple yet robust features. For example, the node of post-of-thread pages always contains the largest number of pages in sitemap ,andthe index-of-thread pages should be the parents of post-of-thread pages, as marked with the red rectangle or green rectangle in Fig.3. The classifier is site-independent and can be used in differ-ent forum sites. We then map each individual page into one of the nodes by evaluate its layout similarity such as sim ( p new ,N index )and sim ( p new ,N post ); and  X  is the thresh-old for decision. We concatenate all the individual pages belonging to the same index list or post list together. The details of this process is described in Algorithm 1. The above process is fully automatic. With the concatenated index lists or post lists , our crawling method becomes a list-wise strat-Algorithm 1 The algorithm for constructing index lists and post lists . Suppose sitemap ( N,E ) is the sitemap knowledge with corresponding nodes N and edges E , N index is the node of index-of-thread pages, N post is the node of post-of-thread pages. 1: Suppose there is a new page p new . 2: Parse the page p new and get the pagination out-links list 3: if sim ( p new ,N index ) &lt; X  then 4: for all page p i in N index ,and OutLinks i is the pagi-5: If (1)  X  link j  X  OutLinks i and p new = link j ,or(2) 6: Concatenate p new into the index list of p i . 7: end for 8: else if sim ( p new ,N post ) &lt; X  then 9: for all page p i in N post ,and OutLinks i is the pagi-10: If (1)  X  link j  X  OutLinks i and p new = link j ,or(2) 11: Concatenate p new into the post list of p i . 12: end for 13: end if egy rather than page-level strategies used by most existing general crawlers.
To predict the update frequency of a list more accurately, we need to first extract the time information of each record (thread creation time in index lists or post time in post lists ). It is difficult to extract the co rrect time information due to the existence of noisy time records in forum pages, such as users X  registration time, last login time and so on. In Fig.4, the time contents in orange rectangle are all noisy content while only the purple one is the correct time information. There are three steps to extract the time information. We first get the timestamp candidates whose content is a short one and contains digit string such as mm-dd-yyyy or dd/mm/yyyy, or some specific words such as Monday, Tues-day, January, February, etc. Second, we align the html ele-ments containing time information into several groups based Algorithm 2 The algorithm of extracting timestamp DOM path from list L . Suppose tsSet is a set of all timestamp candidates for the pages in L and there are N unique DOM paths for these candidates in tsSet . 1: for all DOM path path i ,1  X  i  X  N do 2: Set SeqOrd =0and RevOrd =0. 3: for all page p j in L do 4: Get all cadidates tsList j in p j whose DOM path is 5: for all time candidate tc k in tsList j ,1  X  k  X  M 6: if tc k is earlier than tc k +1 then 7: SeqOrd ++ 8: else if tc k is later than tc k +1 then 9: RevOrd ++ 10: end if 11: end for 12: end for 14: end for 15: Let c = argmax i Order i 16: Return path c on their DOM path since the timestamp in each record should have the same DOM path in HTML document. Fi-nally, since the records are generated in sequence, the times-tamps should satisfy a sequential order. This helps filter noisy time records and extract the correct information. The details of this process is described in Algorithm 2.
The latest replies in each thread can be easily found by revisiting post lists . Similarly, the new discussion threads can be discovered by revisiting index lists . The problem of incremental crawling of web forums is converted to how to estimate the update frequency and predict when the next new record of a list ( post list or index list ) arrives. Based on the timestamp of each record, w e propose several features to help predict the update frequency and describe them in Table.1.

Furthermore, for index lists , we analyze the average thread activity in forum sites. We pro cess all the threads and cal-culate their active time by checking the time of the first post and the last post in each thread. The result is shown in Fig. 5. The figure represents the percentage of threads with different active time. From the figure we can see the percetage of active thread drops significantly when the ac-tive time becomes longer. More than 40% threads keep active no longer than 24 hours and 70% threads are no longer than 3 days. This is one of the major reasons why forum incremental crawling str ategy is different from tra-ditional incremental crawling strategies. Once a thread is created, it usually becomes static after a few days when there is no discussion activity. Thus we introduce a state indicator to avoid bandwidth waste. Suppose there is no discussion activity for  X  t na time since the last post. We compute the standard deviation of time interval  X  t sd by  X  t sd = 1 / ( N  X  1)  X  N number of post records. If  X  t na  X   X  t avg &gt; X   X   X  t sd set ds =1,otherwise, ds =0. Thisfactorisfor index lists only.

To combine these factors together, we leverage a linear regression model which is a lightweight computation model and is efficient for online processing.
 For each forum site, we train two models F list ( x )and F post ( x )for index lists and post lists separately. The two models are kept updated during the crawling process for the new crawled pages. In practice, we update the two mod-els every two months. By setting x 0 = 1, we can get the corresponding W by Equation 2.

In the crawling process, we predict the new coming records of each list by  X  I =( CT  X  LT ) /F ( x ), where CT is the current time and LT is the last revisit time (by crawler). We use the predicted  X  I to schedule the crawling process. Though a list may contain multiple pages, we do not need to revisit all of them. We sort the pages in each list based on the timestamps of the records in each page. For an index list ,if there are Num list records in each index-of-thread page, and we only need to revisit the top  X  I list /N um list pages, where  X  I list =( CT  X  LT ) /F list ( x ). For a post list , we need to revisit the last page or the new discovered pages.
As we have discussed in previous sections, we need to make a tradeoff between discovering new pages and refreshing ex-isting pages. The number of post-of-thread pages is much larger than index-of-thread . Even if only a small percentage of post-of-thread pages are very active, index-of-thread pages may be overwhelmed by a mass of post-of-thread pages, which will occupy most bandwidth. In this case, we need to allocate a dedicated bandwidth for post-of-thread pages since we can only get new discussion threads from them. We introduce a hyper bandwidth control strategy to balance the bandwidth between two kinds of list. The ratio between in-dex lists and post lists can be defined as: where given a time period  X  t , N index is the number of new discussion threads within time  X  t and N Post is the number of new posts arriving within time  X  t . We use the average ratio in history to balance the bandwidth between two lists.
Different from previous work which only considered re-visiting existing pages, in this paper, the scenario we have considered is a real case. The crawler is required to crawl a target forum site starting from its portal page. To have a thorough evaluation, a crawling task needs to last for one year or even longer so that we can measure the crawling performance using different metrics at different periods, for example, the warming up stage and the stable stage. This creates the need to build a simulation platform to compare different algorithms because the real-time crawling cannot be repeated for multiple crawling approaches. Before we describe the experimental details, we first introduce the ex-perimental setup and the evaluation metrics.
To evaluate the performance of our system on various sit-uations, 18 forums were selected in diverse categories (in-cluding bicycle, photography, travel, computer technique, and some general forums) in the experiments, as listed in Table 2. The average length of service of 18 sites is about 4.08 years.

As we wish to evaluate different methods in a long time period (about one year) under several different bandwidth situations while we still want the evaluation to be repeated under the same environment to fairly compare different ap-proaches. Because it is impractical to repeat a long lasting crawling process for many times on real sites, we built a simulation platform to facilitate our evaluation. Typically, aforumhostingserverorganizesforumdatausingaback-end database, and dynamically generates forum pages using page templates. To build the simulation platform for a fo-
Id Forum Site Description 1 www.avsforum.com Audio and video 2 boards.cruisecritic.com Cruise travel message 3 www.cybertechhelp.com Computer help community 4 www.disboards.com Disney trip planning 5 drupal.org Official website of Drupal 6 www.esportbike.com Sportbike forum 7 web2.flyertalk.com Frequent flyer community 8 www.gpsreview.net GPS devices review 9 www.kawiforums.com Motorcycle forum 10 www.pctools.com Personal computer tools 11 www.photo.net Photography community 12 photography-on-the.net Digital photography 13 forums.photographyreview.com Photography review 14 www.phpbuilder.com PHP programming 15 www.pocketgpsworld.com GPS help, news and review 16 www.railpage.com.au The Australian rail server 17 www.storm2k.org Weather discussion forum 18 forum.virtualtourist.com Travel and vacation advice rum, we need to first mirror the forum site by downloading all the forum pages, then parse the forum pages in a re-verse engineering manner and store the parsed forum posts in a database. Thereafter, we can simulate the response to a downloading request by regenerating a forum page according to the requested URL address and the crawling time.
More precisely, we first mirro red the 18 forum sites using a customized commercial searc h engine crawler. The crawler was configured to be domain-limited and depth-unlimited. For each site, the crawler started from its portal page and followed any links within that domain, and a unique URL address was followed only once. Consequently, the crawler mirrored all the pages up to June 2008 from 18 forum sites. The mirrored dataset contains 990,476 pages and 5,407,854 individual posts, from March 1999 to June 2008. Using manually wrote data extraction wrappers, we parsed all the pages and stored all the data records in a database.
The basic behavior of this simulation platform is to re-sponse for a URL request associated with a timestamp. We wrote 18 page generators for 18 forum sites to simulate the responses to requests. For any requested URL, since all the corresponding records can be accessed from the database, we can generate a HTML page with the same contents, layout, and related links as the one in the real site. Here we make an assumption that a post will never be deleted after it is generated. Based on this simple yet reasonable assumption, we can easily figure out at any given time whether a record exists based on its post time and how records are organized basedontheforum X  X layout,an d therefore be able to restore a snapshot of a forum site to the given time.

This simulation platform was used in all the following crawling experiments, and provided a fair experimental envi-ronment to each crawling approach. Assuming a fixed band-width, each crawler was required to crawl the given forum site starting its portal page and from the dummy time period 2006-01-01 to 2007-01-01.
To differentiate the advantage of list-wise strategy and the benefit of bandwidth control, we implemented two variants of our method: (1) list-wise strategy (LWS); (2) list-wise strategy + bandwidth control (LWS+BC).

Since the Curve-Fitting policy and Bound-Based policy in [12] are the state-of-the-art approaches and are more rel-evant to our work, we also included them in the experiments. The original Bound-Based policy only crawl existing pages. We have tried our best to adapt the structure-driven-based approach to forum crawling by: 1) giving a new discov-ered URL the highest weight; and 2) relaxing the interval condition for adjusting refresh period and reference time to accommodate the high frequent update situation in forum sites.

We also introduced an oracle strategy for comparison. In the oracle strategy, every update activity of each page in the target site is supposed to be known exactly. Given the fixed bandwidth, the oracle policy can choose the pages with more new valuable information to visit. The oracle strategy is an ideal policy and an upper bound for other crawlers.
Following pervious work, we assume that the costs of vis-iting different pages are equal, and we measure the band-width cost as the total number of pages which are required to crawl in a given time period [12]. To evaluate the overall crawling performance for each approach, we measure from the following three aspects:
In the forum crawling task, different from general web sites, once a post is submitted, the post will always exist until it is manually deleted by the creator or administrator. In this paper, we use the number of unique posts to measure the valuable information in forum sites. In the definition of coverage , I all should be the number of unique posts existing in the target forum site and I crawl should be the number of unique posts having been downloaded. In the definition of timeliness , X  t i is the time period from a post X  X  creating time to its downloading time. If we download a post one day after the post was created,  X  t i is one day.
To make a fair evaluation for all crawlers, we require all the crawlers to start from the portal page of each site with a fixed bandwidth 3000 pages per day. The crawlers begin to crawl pages from the dummy time 2007-01-01 and last about one year. To illustrate the performance changes of all crawlers in different time periods, we calculate the aver-age performance in everyday in terms of the aforementioned three measurements and present the results in Fig. 6.
From the figure we can see the performance changes ap-parently in the first 100 days and become stable after about 120 days. This is due to the so called warming up stage dur-ing which a crawler needs to first mirror the existing pages and after that it can download new pages. Suppose there are P old posts existing before the crawler starts,  X  P new posts will be generated every day and the bandwidth allows the crawler to crawl B posts per day. At the d th day, there are about P old + X  P  X  d posts existing in the target site. At the beginning, since P old  X  P  X  d , the crawler is required to download almost all posts belonging to P old .Theseareall new valuable information compared to the indexed reposi-tory. This is why in the first 100 days the bandwidth utiliza-tion was approximate to 1, the Coverage increases quickly and the Bandwidth Utilization decreases quickly. We call this stage the warming up stage forthecrawler. Afterabout P old / ( B  X   X  P ) days, the crawler may have finished down-loading all old posts and begins to only focus on the posts belonging to  X  P every day and the performance becomes stable.

Whatever refresh strategy a crawler chooses, if it only assigns new valuable information with the highest weight, Figure 6: One year performance of different crawlers in (a) Bandwidth Utilization, (b) Coverage, and (c) Timeliness. the length of the warming up stage will only depend on the number of old posts P old , new post generation speed  X  P and the bandwidth B . Furthermore, if the bandwidth B&lt;  X  P , it means the bandwidth is too small to cover daily generated new posts. In this case, the crawler may not be able to mirror the old posts unless the forum site stops generating new posts.

In general, the oracle method always performs the best in all measurements and acts as an ideal method. Beside the oracle method, the LWS+BC performs significantly better than other methods in terms of timeliness . The average coverage per day for all methods becomes to 100% after 100 days. This is because the bandwidth is enough and these methods can archive most historical records. But the per-formance of these methods on fetching daily new records are different as shown in timeliness results. For the given bandwidth, the timeliness of LWS+BC will decrease after 100 days. This is because LWS+BC can save enough buffer to catch up the update progress for new posts after it fin-ishes crawling all existing posts. The LWS can just keep the timeliness stable because it does not have additional band-width to catch up the update progress. The bound-based and curve-fitting policies get very similar performance. The timeliness of them all increases (note that the smaller the better for the timeliness measure) since they cannot fetch new posts timely and thus delay the downloading of new posts. We will analyze them in next section. Figure 7: The performance of different crawlers for different bandwidth in (a) Bandwidth Utilization, (b) Coverage, and (c) Timeliness.
A crawler for a commercial search engine is usually not allowed to frequently restart. The performance after its warming-up stage is thus more meaningful. We evaluated all methods under various bandwidth conditions and all crawlers were required to start from the portal page of the given site. The crawlers start from the dummy time 2006-01-01 and last about one year. We only calculate the average performance of different methods for the last 90 days from 2006-10-01 to 2006-12-31 and present the results in Fig. 7.

The curve-fitting policy and bound-based policy perform similarly on timeliness and bandwidth utilization while curve-fitting policy performs slightly better on coverage. This is consistent with their original results in [12].

LWS is better than curve-fitting policy and bound-based policy on all measurements, because we can estimate the up-date frequency more accurately with list-wise information. Furthermore, we can also avoid visiting all the pages in a list and thus save considerable bandwidth.

LWS+BC is the best policy which further improves the performance apparently compared to LWS. Though the av-erage coverage of different methods seems very close (from 0.98 to 1.0), the actual gap is relatively large when it mul-tiplies 1 million pages or 5 million posts. And the gap may become even larger when we continue to crawl more sites or the bandwidth becomes more limited, as shown in the Table 3: The performance differences between index lists and post lists . We use  X  X U X  as  X  X andwidth Utilization X  here for short.
 Methods Index lists Post lists BB 0.0013 0.9925 171 0.0173 0.9917 170 CF 0.0012 0.9936 167 0.0174 0.9931 166 LW S 0.0025 0.9971 73 0.0276 0.9943 81 LW S + B C 0.0030 0.9989 28 0.0329 0.9949 65
Oracle 0.0061 1 2 0.0415 0.9990 16 trends of Fig. 7(b). Although LWS can estimate the update frequency for index lists and post lists relatively more accu-rately, it is still very hard to balance these two kinds of pages. In contrast, the bandwidth control policy is more effective to balance between fetching valuable data and refreshing downloaded pages. Such a policy only slightly affects post-of-thread pages but benefits the index-of-thread pages a lot. When the bandwidth is set to 3000 pages per day, the aver-age timeliness for LWS+BC is about 65 minutes while the average timeliness for bound-based policy or curve-fitting policy is about 170 minutes and 165 minutes respectively. This shows that LWS+BC is 260% faster th an the bound-based policy or curve-fitting policy and thus is capable of downloading new posts timely. Meanwhile, it also achieve a high coverage ratio compared to other methods. To get more insights to this problem, we evaluate these two kinds of pages separately in the next section.
Given a fixed bandwidth of crawling 3000 pages per day, we evaluated the performance for index lists and post lists separately and showed the results in Table 3.

From the table, we can see that LWS improves the perfor-mance for both two kinds of pages. The timeliness of both index-of-thread pages and post-of-thread pages decreases sig-nificantly in Table 3 compared to the curve-fitting policy and bound-based policy. This is because LWS leverages more in-formation and can estimate the update frequency for both two kinds of pages more accurately.

LWS+BC further improves the performance for index-of-thread pages compared to LWS. The timeliness of index-of-thread pages decreases significantly in Table 3 while the timeliness of post-of-thread pages remains the same. It con-firms our previous assumption that bandwidth control can assign the right ratio accordin g to the real update numbers for different kinds of pages.
Realizing the importance of forum data and the challenges in forum crawling, in this paper, we proposed a list-wise strategy for incremental crawling of web forums. Instead of treating each individual page independently, as did in most existing methods, we have ma de two improvements. First, we analyze each index list or post list as a whole by concate-nating multiple pages belong to this list together. And then we take into account user behavior-related statistics, for ex-ample, the number of records and the timestamp of each record. Such information is of great help for developing an efficient recrawl strategy. Second, we balance discovering new pages and refreshing existing pages by introducing a bandwidth control policy for index lists and post lists .To evaluate the proposed crawling strategy, we conducted ex-tensive experiments on 18 forums, compared it with several state-of-the-art methods, and evaluated it under various sit-uations, including different stages through one year X  X  crawl-ing simulation, different bandwidths, and different kinds of pages. Experimental results show that our method outper-forms state-of-the-art methods in terms of bandwidth uti-lization, coverage, and timeliness in all situations. The new strategy is 260% faster than existing methods and mean-while it also achieves a high coverage ratio. [1] R. Baeza-Yates, C. Castillo, M. Marin, and [2] B. E. Brewington and G. Cybenko. How dynamic is [3] B. E. Brewington and G. Cybenko. Keeping up with [4] R. Cai, J.-M. Yang, W. Lai, Y. Wang, and L. Zhang. [5] S. Chakrabarti, M. van den Berg, and B. Dom. [6] J. Cho and H. Garcia-Molina. Effective page refresh [7] E. Coffman, Z. Liu, and R. R. Weber. Optimal robot [8] G. Cong, L. Wang, C.-Y. Lin, Y.-I. Song, and Y. Sun. [9] M. Diligenti, F. M. Coetzee, S. Lawrence, C. L. Giles, [10] J. Edwards, K. S. McCurley, and J. A. Tomlin. An [11] F. Menczer, G. Pant, P. Srinivasan, and M. E. Ruiz. [12] C. Olston and S. Pandey. Recrawl scheduling based on [13] S. Pandey and C. Olston. User-centric web crawling. [14] M. L. A. Vidal, A. S. da Silva, E. S. de Moura, and [15] Y. Wang, J.-M. Yang, W. Lai, R. Cai, L. Zhang, and [16] J. Wolf, M. Squillante, P. Yu, J. Sethuraman, and [17] Y. Zhai and B. Liu. Structured data extraction from [18] S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint
