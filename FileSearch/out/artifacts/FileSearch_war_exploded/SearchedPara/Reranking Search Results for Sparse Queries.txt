 It is well known that clickthrough data can be used to im-prove the effectiveness of search results: broadly speaking, a query X  X  past clicks are a predictor of future clicks on doc-uments. However, when a new or unusual query appears, or when a system is not as widely used as a mainstream web search system, there may be little to no click data available to improve the results. Existing methods to boost query performance for sparse queries extend the query-document click relationship to more documents or queries, but require substantial clickthrough data from other queries.

In this work we describe a way to boost rarely-clicked queries in a system where limited clickthrough data is avail-able for all queries. We describe a probabilistic approach for carrying out that estimation and use it to rerank retrieved documents. We utilize information from co-click queries, subset queries, and synonym queries to estimate the click-through for a sparse query. Our experiments on a query log from a medical informatics company demonstrate that when overall clickthrough data is sparse, reranking search results using clickthrough information from related queries signifi-cantly outperforms reranking that employs clickthrough in-formation from the query alone.
 Categories and Subject Descriptors: H.3.3 [ Information Search and Retrieval ]: Search process General Terms: Experimentation, Algorithms Keywords: Sparse Queries, Reranking, Query Log Mining, Sparse Clickthrough Data, Query Selection
Clickthrough data from query logs is widely used to im-prove document ranking [1, 8, 9, 10, 14, 15, 25]. But how does it work for new or unusual queries in a search sys-tem? Such (sparse) queries are problematic because little or no clickthrough information is available for them in query logs. This situation can be attributed to three causes: (1) either the query is unpopular or new and therefore very few searches have been issued with this query; (2) the query is not unpopular but the displayed ranking of the search results does not fulfill users X  information needs to the extent that results are rarely clicked; or (3) the query is not unpopular but the retrieved list of documents is of poor quality so that reranking with the same set would not improve the results. Mostly, such queries can be boosted with their own sparse clickthrough data if available [23], or information from other more popular queries such as co-click queries may be used to enrich the results [8, 25]. Existing approaches however require substantial clickthrough data from somewhere for solid improvements due to noise that is introduced in gath-ering the additional data [10, 23]. In this context, our work aims at answering the following question: how can we rerank search results for rare or sparse queries in a system for which limited clickthrough data is available?
Our aim is in particular to achieve a better ranking of doc-uments for underrepresented or sparse queries of types (1) and (2) described above. We do not deal with case (3) since this situation must be approached with a better retrieval algorithm and not with reranking.

We deal with issues (1) and (2) indirectly by estimating how likely a document is to be clicked given a query by using a language modeling framework [19]. Our hypothesis is that using clickthrough data from related queries for sparse queries aids in achieving a better ranking than solely using the query X  X  own sparse clickthrough data. The difficulty of the task arises from the fact that little or no clickthrough data is available not only for the query in question, but also for all other queries in the query log. In this situation, we cannot afford to use an approach that introduces too much noise.

For our task, we employ queries related to the query whose results are to be reranked. They come from one of the fol-lowing three sources: 1. Similar queries: these share at least one co-click with the original query, which is a well-known feature from previous work [8, 10, 13, 23]; 2. Subset queries: these are contained in the original query as an n-gram se-quence; 3. Synonym queries: these are lexically, semanti-cally, or syntactically related to the original query. We use the clickthrough data of these related queries in our models.
We test our models on queries from a medical query log that was obtained from a medical informatics company, Up-ToDate. We also use their Lucene-based search system that yields the initially ranked search results for a query. The users of this system are doctors and nurses in practices and hospitals  X  i.e., specialists in their domain. Therefore, the medical queries are very specific to the domain. In such a system, sparsity of clickthrough data is experienced more strongly than in general search engines that quickly acquire vast amounts of queries per day.

Our results show that all reranking models outperform the baseline for which clickthrough-boosting is employed with information from the original query only. Further, we ob-serve some interesting trends among the reranking models when reducing clicks in the dataset. Reranking with subset queries and a merged version that uses all related queries perform best, which is expected.

Our results are specific to a particular medium-scale search system, though we believe they are general enough to apply to any similar setting. We hypothesize that the same ideas will extend to general search engines with much greater lev-els of activity, but do not have access to the resources needed to test that belief.

This paper is organized as follows: the next Section 2 re-views relevant related work in the area. In Section 3 we detail the empirical setting that motivated us for this re-search and that constrains our possible solutions. Then, we describe the baselines and reranking models that use clickthrough data from related queries in Section 4. This is followed by the experiments and the results in Section 5. Finally, we conclude our work in Section 6.
It is well known that by means of clickthrough data search results can be reranked to significantly improve the results [1, 8, 9, 10, 14, 15, 25]. However, clickthrough data sparsity poses a problem [6, 8, 10, 13], since many queries often have a small number of clicks, which makes them incomplete. In-deed, queries often have no clicks at all. Gao et al. [10] ap-ply two smoothing methods to overcome the data sparsity problem, one of which is based on Good Turing smooth-ing. This is achieved by smoothing the features of 0-click queries with features of 1-click queries. The other  X  X mooth-ing X  method is based on query clustering on the clickthrough graph [8]. Here, the query-document click graph is extended with more queries and documents by following links between them. However, in their experiments on a dataset with less clickthrough data available, Gao et al. [10] show that the query clustering approach on the clickthrough graph does not help. This method only works well when substantial clickthrough data is available, since the introduction of ad-ditional documents in the click graph is likely to be noisy for queries originally not having a click with such documents.
One similarity between our work and Craswell and Szum-mer X  X  random walk on the click graph [8] is that we use information from co-click queries to estimate parts of the reranking models  X  this is equivalent to following a link be-tween a document and a co-click query. However, one major difference is that we do not include new documents in the document list to be reranked, since this is prone to be noisy when clickthrough data is sparse. This means that we do not go beyond the scope of 1-hop co-click queries. Another ma-jor difference is that in Craswell et al. X  X  work the query con-tent is not considered in the reranking process, whereas we also attempt to use information from other related queries for which the subqueries and synonyms of a query have to be analyzed.

There has been a lot of previous research that uses infor-mation from co-click queries for reranking [8, 10, 13, 23]. These queries are what we refer to as  X  X imilar queries X , and they are one possible source of related queries in our rerank-ing models. Zhao et al. [25] use clickthrough frequencies of subqueries in their reranking approach while calculating the likelihood of a document being retrieved, given a query. Synonym queries on the other hand have been explored in the literature in the form of query reformulation, query ex-pansion and query rewriting [11, 17, 22].

There are several learning to rank approaches that incor-porate clickthrough data [1, 9, 20]. Although we employ a simple regression approach to estimate the similarity of some queries (Section 4.3.2), we do not otherwise use learning ap-proaches. For reasons explained in the following section, our empirical evaluation is constrained to use a black-box retrieval system, for which we are combining very few fea-tures. A straightforward parameter sweep is as powerful as any learning method in this case.

To the best of our knowledge, although previous approaches have tested their retrieval and reranking algorithms on sparse queries, they have not employed them on sparse queries in a sparse system where the overall clickthrough data available is limited for all queries . In our work, we aim at addressing this aspect in particular. We note that such a setting seems unusual, but is common in the early days of any new search system. Indeed, major search engines have the same prob-lem  X  i.e., they have queries which have rarely been seen before. Further, their total count of such queries is almost certainly vastly greater. However, since these queries are a more noticeable part of the search experience for smaller search engines like that of UpToDate, this work is more valu-able in such a setting. Our results will show that  X  X parse X  means up until queries have close to 500 clicks.
The motivation for this research arose during discussions with the medical informatics company UpToDate. UpTo-Date hosts medical information that is searched on a daily basis by a large number of physicians. Although it receives a substantial number of queries each day, the volume is mi-nuscule compared to major search engines: our collection contains around 10 million queries per month compared to an estimated more than 10 billion queries handled by Google this past March [7].

The search engine used by UpToDate is based on ver-sion 1.4 of the open source Apache Lucene system 1 . It has been extensively tuned over several years, using parameter sweeps, evaluations on subsets of users, careful editing of the hosted information, and human intuition. A user X  X  query is converted into a complex weighted combination of the original query words, synonyms from a controlled vocabu-lary, and field references. In this work, we incorporate click-through information in the Lucene-based ranking, improv-ing retrieval effectiveness to the point that it is exceptionally good. As we show in Section 5.2, the system is extremely accurate, achieving NDCG closing in on 90% on average.
Not surprisingly, there are a number of queries that per-form much worse, the  X  X parse queries X  in particular. The em-pirical parts of this study are carried out in this environment. Whereas the baseline search system is a carefully crafted Lucene implementation that incorporates clickthrough in-formation for all queries with enough click history, our ex-http://lucene.apache.org/java/docs/index.html periments investigate methods for applying clickthrough in-formation for sparse queries in addition.

The goal of our empirical work is to explore the impact of several approaches for using sparse information to improve the ranking of the UpToDate-based system. That means we are constrained to combining a carefully crafted Lucene score with other information  X  here, clickthrough informa-tion from the query and its related queries. We leave for future work approaches that expose the inner workings of the Lucene black-box system and integrate clickthrough in-formation more elaborately.
In this section we first detail two baseline methods be-fore describing the reranking models that incorporate click-through data from related queries.
As mentioned before, the query log and search system we have for this work are based on the Apache Lucene system used by UpToDate. Lucene by default employs a combina-tion of the vector space model and the boolean model for re-trieval. The system has further been extended and tuned for this work as described in Section 3. We convert this retrieval system X  X  raw output score, score Luc ( Q,D ), to probabilities for our reranking models (Section 4.3) as follows: where the denominator of this equation sums over all docu-ments D i retrieved for the query Q .
The Clickthrough-Boosted Baseline (BoosLuc) is an ex-tension of the Lucene system in that every query is boosted with its own clickthrough information from the training data. A Lucene retrieved document D for query Q is reranked as follows: P where c CT ( Q,D ) is the clickthrough count for query Q with document D , c CT ( Q ) is the total clickthrough count for Q used for normalization, and P Luc ( D | Q ) is from Equation 1.  X  is defined as follows: which is Dirichlet-like smoothing, ensuring heavier weight on the clickthrough part in Equation 2 when more clickthrough data is available for Q , and less otherwise. With less click-through data, P Luc ( D | Q ), which estimates how likely D is to be relevant by content, has a larger impact. We determine  X  with a parameter sweep on the training data.
In this section we describe the reranking models for sparse queries that use clickthrough data from related queries. All reranking models have the same base model, but they differ in their source of related queries from which the informa-tion is drawn. The base model estimates the relevance of a document D given a query Q as follows:
P RelQ ( D | Q ) =  X   X  P CT ( D | Q ) + (  X   X  1)  X  P Luc ( D | Q ) (4) where  X  is Jelinek-Mercer smoothing tuned on training data. The first component P CT ( D | Q ) indicates how likely D is to be clicked based on the clickthrough information obtained from related queries: P CT ( D | Q ) =  X   X  X where Q is a set of related queries that varies depending on the particular model used.  X  is again determined by means of Dirichlet-like smoothing as in Equation 3, with the parameter  X  replaced with  X  . P ( D | Q 0 ) indicates the likelihood of D to be retrieved by the related query Q 0 : which is a normalized probability based on Q 0 s clickthrough data so that P D P ( D | Q 0 ) = 1.

P ( Q 0 | Q ) in Equation 5 stands for the quality of the re-lation between Q 0 and Q . Intuitively, Q and Q 0 are most related if Q 0 can predict the way Q  X  X  Lucene ranked doc-uments are going to be clicked. In other words, given the Lucene ranked list Q Luc of Q and the clickthrough ranked list Q
CT of Q 0 , the two queries are most related if Q Luc ranks documents in the order they are clicked in Q 0 CT . The list Q
CT is sorted in decreasing order of frequencies c CT ( Q 0 for Q 0 . This means that ideally exactly those documents in Q
Luc should be ranked lowest that were not clicked for Q 0 and are thus missing in Q 0 CT . Hence, we compare Q Luc Q CT as follows: where the relevance judgments for NDCG come from the ranking of Q 0 CT . That is, we map the clicks given by Q to relevance grades, with which Q Luc is evaluated (see Sec-tion 5.1.) In particular, here we use NDCG@n, where n  X  10  X  the NDCG value at the highest rank available, not exceed-ing 10. The NDCG values are normalized to probabilities so that P Q 0  X  X  P ( Q 0 | Q ) = 1.

Note that with this approach, documents in Q Luc that are ranked lowest and which were not clicked for Q 0 (absent in Q 0 CT ) will get a relevance grade of 0 in Q Luc documents are perfectly ranked at the bottom of the list, this will not result in a lower NDCG score.

To summarize, Equation 5 has the following consequences in extreme cases: 1. if there is no clickthrough data available at all for Q 2. if there is no clickthrough data available for any of the
In the usual case where some sparse clickthrough data is available for both Q and related queries, the model is a combination of all these components. Each related query contributes to the likelihood of D being relevant for Q . If there is no evidence for c CT ( Q 0 ,D ) given a particular Q then this results in D not being boosted by Q 0 . Neither do we apply a special form of smoothing here, nor do we use clicks with related queries on other documents to boost D , which would introduce noise into the estimation. The idea is that by means of different ways of locating related queries, we can find enough evidence to boost the right documents.
In this model a related query is a similar query Q 0 that shares at least one co-click on a document with the original query Q . The domain of similar queries therefore consists of all the training queries in the entire  X  X lickthrough corpus X  that have at least one click in common with Q  X  X  ranked list. We constrain the clickthrough corpus here to have a limited and meaningful set of similar queries that excludes junk queries. We locate similar queries efficiently via the reverted indexing approach [18]:
This is a preparation step for which we obtain the clicked document ids for every training query Q from the click-through data from which the index is built. The entries look as follows: docid ( D ) = { queries that retrieve D having c CT ( Q,D ) } This way we accumulate the queries that were clicked with every docid for all the training queries. The clickthrough counts are normalized so that P i c CT norm ( Q,D i ) = 1, where each D i has at least one click with Q .

This is how similar query lookup is achieved. Given an initial query Q , we retrieve similar queries Q 0  X  X  by query-ing the reverted index with each of the clicked docids for Q . This yields a combined list of similar queries with their nor-malized clickthrough counts. We conflate multiple entries of a similar query Q 0 in this list to obtain the set Q by averag-ing the normalized clickthrough count for each Q 0 . This is then used to estimate P ( D | Q 0 ) (Equation 6).

The similar queries are ranked in decreasing order of P ( Q as described in Section 4.3 (Equation 7). By construction, every similar query has a non-zero P ( Q 0 | Q ).
Under this model a related query Q 0 is defined as a sub-query of Q . The domain of related queries is therefore the power set of all non-empty n-gram subqueries of Q . Con-sidering subqueries is useful when the original query is too specific, such as in pediatric migraine headache , since sub-queries such as migraine headache or headache often capture more general content. We analyzed in our training data how frequently subqueries of sparse queries are clicked. For this, we took all training queries with fewer than 100 clicks, of which there are about 1 million queries, and obtained their subqueries. Table 1 contrasts the number of these training subqueries having clicks in a certain range versus the total number of training queries having click counts in the same range. The numbers show that subqueries of sparse queries are not as sparse as their corresponding super queries (bold entries in 2 nd column), with 10,368 of them having at least 100 clicks. But the subqueries are not more popular either, since the numbers in the higher click count bins are compa-rable (bold entries in 2 nd versus 3 rd columns).
 Table 1: Number of training subqueries having clicks in a certain range whose super queries have less than 100 clicks, versus the number of all train-ing queries having clicks in that range. In the subset query ranking model (Sub), we estimate P ( D | Q 0 ) as described in Equation 6, but for P ( Q train a linear regression function with several features on a portion of the training data, since this form of estimation yields better performance with subset queries. As the truth label during training we use NDCG ( Q CT ) (with relevance judgments from Q 0 CT ) from a different training split. The features are the following: overlap ( Q,Q 0 ) number of overlapping terms between the kcc ( Q ) ,kcc ( Q 0 ) Key Concept Classifier (KCC) score [4] sim ( Q,Q 0 ) weighted overlapping terms, where weight w ( t ) score Luc ( Q ) , score Luc ( Q 0 ) total Lucene retrieval score for query popweight ( Q,Q 0 ) popularity weight of Q 0 with Q : popscore ( Q,Q 0 ) popularity score of Q 0 with Q :
We ensure all features are between 0 and 1 through nor-malization. For the subset queries model, P ( Q 0 | Q ) is then a linear combination of these features with the trained weights. Note that most of these features just described are applica-ble to subset queries only. We tried training a function for the other reranking models, too, by including NDCG ( Q Luc as an additional feature. This feature proved to be the strongest one for them, which is what we use to estimate P ( Q 0 | Q ) for those approaches.
In Synonym Query Reranking (Syn), a synonym query is syntactically, semantically, or lexically related to the origi-nal query. So variations of a query such as singular/plural or paraphrases such as in hypernatremia and high plasma sodium level are valid synonym queries. As a synonym re-source we use a carefully constructed medical vocabulary by deputy editors from UpToDate. It has 24,415 canonical keyword entries with 150,383 synonym entries in total. Given a query, we construct the set of synonym queries Q as follows: first, if a canonical keyword entry exists for the query, we use the synonym entries associated with it. If there is no canonical entry for a query (which often happens with very general medical terms), we take the n-gram subset queries of the query and again look for canonical keyword entries for those subset queries. If still no synonym queries could be found after this step, we try to match the original query and its subqueries in the synonym entries, to then include the associated canonical entry and the remaining synonym entries for that query. The found set of synonyms is filtered to exclude 0-click queries. We use at most 10 synonyms from this set.

P ( Q 0 | Q ) and P ( D | Q 0 ) of these synonym queries are then estimated by means of the training clickthrough data as de-scribed in Equations 6 and 7, Section 4.3.
For this model, we merge the different kinds of related queries to one set, i.e., Q = similar queries  X  subqueries  X  synonyms For each type of related query we estimate P ( Q 0 P ( D | Q 0 ) the way it was described for the query in Section 4.3 and its subsections. We do not reduce the number of related queries in this process. Therefore, the effect on Equation 5 is that the sum is performed over a larger set of related queries.
The reason for having this merged reranking model is to compensate for a bad choice of related queries for any one of the methods  X  such as poor synonyms, no similar queries (for 0-click queries), or no subset queries (for original unigram queries).
Our query log is from UpToDate. The query log has 4 months of data. Tables 2 and 3 contain some statistics about the entire data set. We split the query log into three folds: two training folds for parameter tuning and one test fold on which the results are reported in Section 5.2. The folds are roughly equal in size.

For the experiments, we use 1407 queries together with the training portion of the query log and 972 queries with the test set. To avoid biasing parameters toward certain queries, the training queries used in parameter tuning are distinct Table 3: Distribution of clicks for all queries in the training and test data sets.
 from the test queries for which the results in this paper are reported. All training and test queries are popular (i.e., each query has at least 500 clicks). We perform the experiments both on these popular queries and we also simulate them as sparse queries by reducing the amount of training click data available to the models. This is done to understand the performance of sparse queries in a sparse environment .
Each query comes with clicks in the training folds as well as clicks in the test fold. For some of our experiments we use all of the clicks available. In others we reduce the num-ber of clicks available to the algorithms in order to simulate a sparse situation. The reduction of clicks for sparsity is achieved as follows: to have K clicks for a query, we evenly reduce the click count for each document so that the sum of all the clicks for the query roughly equals K (with some small error, due to multiplication with a small fraction). If the total click count for a query is less than K , the clicks are not altered. Therefore, this reduction of clicks preserves the distribution of the click counts. We choose this form of click reduction to preserve the trend in the data, i.e., the most frequently clicked document will still be the same after reduction. Note that clicks can be reduced within a single fold, but can also be reduced across two or three.

Our aim in reranking the Lucene results is to better ap-proximate the ranking as given by the click frequencies from users, similar to Zhao et al X  X  work [25]. Hence we use the clicks in the test fold as a representation of  X  X ruth. X  For the experiments reported in this paper, clickthrough truth comes from the test fold and model estimation is done with the training folds. Note that while the training clicks are reduced in some of the experiments, the clickthrough truth data from the test split is never reduced.

Given a query Q , in order to evaluate its ranked list of n documents R , we use two different measures. One of them uses relevance judgments from clickthrough truth, whereas the other compares the rank correlation to the clickthrough truth list. For the former measure, we use the truth clicks for Q from the test fold as relevance judgments. We do this by mapping the clicks to relevance grades for the well-known NDCG measure (Normalized Discounted Cumulative Gain) [12]: where i is the i th document in R and rel i is the relevance grade of document i . This measure is normalized with Z Q that NDCG=1.0 when ranking is perfect. We would like to be able to uniformly map the clicks to a reasonable range of relevance grades to be used with NDCG for any given query . We experimented with various schemes of mapping the clicks for all the queries in the training query log portion to find the most suitable approach. We discovered that we can achieve a reasonable range of real-valued relevance grades roughly between 1 and 4 if we use log 10 ( clicks ) for the mapping. Figure 1: Mapping clickthrough data to NDCG rel-evance grades for all queries in the training query log.
 Table 4: Relevance judgments from the entire test set rounded to discrete values.

Figure 1 shows the effect of choosing log 10 ( clicks ) with a cumulative distribution function of the clicks on the doc-uments of all the queries in the training data. Note that by this construction relevance grades are real-valued rather than discrete.

Table 4 shows the corresponding relevance judgments for the entire test set. For this, we rounded the real-valued rel-evance grades to discrete values. The numbers indicate that across the dataset, there are only 30  X  X erfect X  documents, 1115  X  X xcellent X  documents, 2772  X  X air X  documents, and 7121  X  X ad X  documents, which should not be ranked highly.
Therefore, given a query Q and a ranked list R obtained through a reranking method as detailed in Section 4, we evaluate the ranking of each D  X  R by means of the truth clicks of ( Q,D ) from the test fold mapped to NDCG rel-evance grades as follows: log 10 ( c CT ( Q,D )). Similarly, in Equation 7 where we estimate the quality of the relation P ( Q 0 | Q ) by means of NDCG, the frequencies c CT ( Q 0 Q
CT are also mapped to NDCG relevance grades in the same manner.

Since NDCG has been criticized in past research for not correlating well with user satisfaction [2, 21, 24], we also use a measure that directly evaluates the rank correlation between two lists. Unlike NDCG, this measure does not re-quire relevance judgments; the ranking of the lists are com-pared directly. We still show our results with NDCG, since it is a well-known measure widely used in the information retrieval community. As the rank correlation metric, we use the M Measure [3] which penalizes differences in rankings de-pending on the rank position, similarly to the weighted gen-eralized version of Kendall X  X  Tau and Spearman X  X  footrule presented in recent work by Kumar and Vassilvitskii [16]. The M Measure is defined as follows: where t and r are two ranked lists for which M yields 1 if t and r are identical at rank k , and 0 if they are completely disjoint at rank k . M 0 is defined as follows: where i  X  Z is the set of overlapping documents between t and r , j  X  S is the set of documents occurring in t but not in r , and l  X  T is the set of documents occurring in r but not in t . Note that a document being  X  X bsent X  in a list is assumed to have rank k + 1, which is denoted by the penalizing denominator k + 1 in S and T . The notation rank x ( y ) denotes the rank of document y in list x . The M Measure gives a higher weight to documents whose rankings are closer in high ranks of r and t , whereas the weight is lower with greater disagreements and lower rankings in the lists. The normalization constant in Equation 12 depends on k , referring to the case when Z is empty and all documents in r and t fall into the sets S and T .

In our evaluation, we observe M ( t,r )@ k for t being the truth list  X  the clickthrough ranked list in decreasing order of click frequencies  X  and for r  X  the reranked list by one of the reranking methods.
We first conduct the experiments on the unreduced 972 test queries to measure the performance of the baselines and all reranking models. For all models, the parameters  X  and  X  (for the reranking models) and  X  (for BoosLuc) are set using the training data. For estimating the components in these runs, the models have full access to the training clickthrough data. Table 5 shows the results for the popular queries.
We can see that the baseline system Luc is already very good with NDCG@1 = 0.894 and M@1 = 0.682, increasing at lower ranks up until 0.925 for NDCG@20 and 0.705 for M@20. However, we can significantly improve on this by just boosting with clickthrough data, which is what the sec-ond baseline BoosLuc represents. We observe NDCG@1 = 1.0, and at the other ranks we have almost perfect ranking, which is also reflected in the M Measure rank correlation. As for the reranking models Sim, Sub, Syn and Merged  X  they also perform very similarly to BoosLuc with Syn (the synonym queries reranking model) showing the largest gain. Syn beats all other methods significantly with p-value &lt; 0.04 using the paired two-sided t-test for both measures. This experiment shows that with popular queries in a popular system when a substantial amount of clickthrough data is available, we can successfully boost a well-performing base-line such as the Lucene system with clickthrough data from the query only. For even better performance, the synonym queries reranking model can be used.

We now consider what happens if far fewer clicks are avail-able on documents. We vary the number of clicks available for all queries , N , between 0 and 20,000 to simulate a sparse system that gradually progresses to a system with larger amounts of clickthrough data. Tables 6 and 7 and Figure 2 with the graphs show the simulated sparse query runs in a sparse system with little clickthrough data available for all queries . With higher N , the performance eventually be-comes identical to what is reported in Table 5, since this corresponds to running the models with maximum N .

In Tables 6 and 7 we present all the reranking models in comparison to BoosLuc with only at most 1-50 clicks avail-able for every query in the system. We notice a big, signifi-cant difference between BoosLuc and the reranking models at all ranks for these clicks, with BoosLuc catching up as the number of clicks increases. At NDCG@1 and M@1, all reranking models are almost perfect, whereas at lower ranks Sub and Merged perform best. This is also the case for 10 clicks, although Merged is not statistically significant over all methods at NDCG@5, but it is significantly better ac-cording to the M Measure. Sub clearly dominates for both 1 and 10 clicks for both measures. At 20 clicks, Sub is not the strongest method any more. The merged reranking model shows the best performance together with Sim at ranks 1 and 2. At 50 clicks we can also see that Merged dominates the results. This is not surprising, since Merged contains related queries from all models and it therefore has an ad-vantage over the other methods in extreme cases, such as when no subset queries are available, or no co-click queries or synonyms could be found. It is surprising that Merged does not perform best in Table 5, where more clickthrough data is available. One reason might be that the scores of many documents are heavily boosted due to the large num-ber of related queries  X  and that unfortunately many of those documents are non-relevant.

The parameters were fixed on training data individually for every different click setting for each of the methods. Note that the fixed parameter  X  is consistently higher in Tables 6 and 7 (  X  = 0 . 8 , 0 . 9) for most reranking models than it was in Table 5 (0.5, 0.6), which is the weight on the related queries reranking part in Equation 4. This is required for mak-ing up for the sparsity of clicks, whereas for more popular queries such a high emphasis on the information from the related queries is not required.  X  is very sensitive for differ-ent reranking models and depends on the number of clicks available in a sparse system, although lower values are pre-ferred. In the runs with the popular queries in Table 5 it is more stable with  X  = 20000.
 Figure 2 shows all the runs for the reranking models and BoosLuc (always the solid line) with NDCG and M Measure performance at the measured ranks 1, 10, and 20 for varying click counts. Each graph shows the results at a fixed NDCG or M Measure rank on the y axis with varying clicks on the x axis for the methods. In all the graphs, the BoosLuc curve starts off low and improves the more clicks (on the query itself for this baseline) are available. However, after 500 clicks the effectiveness has plateaued and no further gains occur. In fact, there is only modest value to clicks after the first 300. Therefore, the presented click range is between 0 and 800 or at most 2,000 for the graphs; the numbers do not change much afterwards and gradually progress into the results in Table 5.

As for the first two graphs with NDCG@1 and M@1 on the y axis, there is little noticeable difference between the reranking methods, except for the fact that they all outper-form BoosLuc. A slight dominance of Sub is visible in the M@1 graph. At NDCG@10 and M@10, this becomes much clearer: the lines for Syn and Sim are noticeably below the lines for Sub and Merged. This fact becomes even more ev-ident at NDCG@20 and M@20, with the gap between the lines growing larger. Another point is that between 0 and 100 clicks, in all the graphs the performance between the related query reranking models is very similar. With more than 100 clicks, the changes become noticeable. 0.004 using the paired two-sided t-test. using the paired two-sided t-test.
The results show very interesting trends: subset queries seem to be a more reliable clickthrough source for rerank-ing than synonyms or co-click queries at lower clicks, which is an interesting observation. For best performance over a wider range of low clicks, the merged model together with the subset query model are most helpful, as we learned from Tables 6 and 7. When substantial clickthrough data is avail-able, however, Syn outperforms all other methods as we saw in Table 5. Further, the results at rank 1 are near perfect for almost all the reranking models, whereas maintaining per-fect reranking quality at lower ranks until 20 seems slightly more difficult for some of the methods. These results show the sensitivity of the methods to the number of clicks avail-able and high-rank versus low-rank performance. What is evident in all the results however is that in a sparse system with sparse queries  X  even queries with up to 400 clicks  X  clickthrough boosting solely with the information from the original query is a poor choice, as is done with BoosLuc. Fur-thermore, using synonym queries can be helpful even when a system has large numbers of clicks available.

Finally, it is evident in Tables 6 and 7 and in Figure 2 that the two measures NDCG and M Measure mostly agree with one another with small exceptions, although the absolute improvement between BoosLuc and the reranking methods is greater with the M Measure.
In this paper, we tackled improving search results for sparse queries when the overall availability of clickthrough data is sparse. The aim was to approximate the ranking as defined by users X  clicks. For this, we presented several reranking models that use clickthrough counts from related queries to better estimate the relevance of a document, given a query. The general reranking model estimates the rel-evance of a document by considering the quality of the re-lated query and the likelihood that the related query will re-trieve the given document. The models differ in their source of related queries, which may be similar queries sharing at least one co-click with the original query, subqueries, or syn-onym queries. Finally, a merged model combines the various sources of related queries into one as evidence for reranking documents.

The experimental results showed that with popular queries the synonym queries reranking model significantly outper-forms basic clickthrough boosting of the original query, al-though all methods were able to greatly improve over the basic Lucene baseline often achieving nearly perfect results. For sparse queries in a simulated sparse system we observed that the related query reranking models are able to main-tain the strong performance and significantly improve over the clickthrough boosted baseline with large gains. In a very sparse system with only 1-50 clicks available for every query, the subset query and merged reranking models performed best.

As future work, it would be interesting to conduct fur-ther experiments on other sparse web data sets or on other domain-specific query logs from small-or medium-sized sys-tems to understand the generalizability and limitations of our models.
This work was supported in part by the Center for In-telligent Information Retrieval, in part by NSF grant #IIS-0910884, and in part by UpToDate. We wish to thank Up-ToDate for access to its query log and retrieval system. We are particularly grateful to Jerry Greene X  X  help in the time-consuming process of tagging data for our experiments. Any opinions, findings and conclusions or recommendations ex-pressed in this material are the authors X  and do not neces-sarily reflect those of the sponsors.
