 The work presented in this paper is motivated by the practi-cal need for content extraction, and the available data source and evaluation benchmark from the ACE program. The Chinese Entity Detection and Recognition (EDR) task is of particular interest to us. This task presents us several language-independent and language-dependent challenges, e.g. rising from the complication of extraction targets and the problem of word segmentation, etc. In this paper, we propose a novel solution to alleviate the problems special in the task. Mention detection takes advantages of machine learning approaches and character-based models. It manip-ulates different types of entities being mentioned and differ-ent constitution units (i.e. extents and heads) separately. Mentions referring to the same entity are linked together by integrating most-specific-first and closest-first rule based pairwise clustering algorithms. Types of mentions and enti-ties are determined by head-driven classification approaches. The implemented system achieves ACE value of 66.1 when evaluated on the EDR 2005 Chinese corpus, which has been one of the top-tier results. Alternative approaches to men-tion detection and clustering are also discussed and ana-lyzed.
 I.7 [ DOCUMENT AND TEXT PROCESSING ]: Mis-cellaneous Algorithms, Languages entity mentions in Chinese, mention detection, mention cat-egorization, and mention clustering
Today X  X  global web of electronic information provides a re-source of unbounded information-bearing potential. But to fully exploit this potential requires the ability to extract con-tent information from human language automatically. The Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. task of Entity Detection and Recognition (EDR) is one of the most important tasks in information extraction suggested by the Automatic Content Extraction (ACE) program, whose objective is to develop extraction technologies to support au-tomatic processing of human language data. The ACE EDR 2005 task requires detecting the seven types of ACE-defined entities being mentioned in the source languages (Arabic, Chinese or English), linking together the mentions that re-fer to the same entities, and recognizing the types and roles of the mentions and entities identified. It is a task funda-mental to many applications, such as text summarization, question answering and machine translation.

The ACE EDR task is motivated by and addresses the same issues as the MUC program that precedes it. However, it defines the targets of extraction in terms of the objects (i.e. logic objects in the world) rather than in terms of the words. To simulate a real world application, EDR presents us three new challenges. First, the named entities task, as defined in MUC, is to identify those words that are names of entities. In ACE, on the other hand, the corresponding task is to identify all mentions of an entity, whether named (e.g. George Bush), nominal (e.g. our president), or pronominal (e.g. he). The complicated constitution of nominal mentions makes the more difficult to be detected correctly. Second, according to the ACE guideline, not only the extent but also the head of each mention must be identified. The nested rather than liner structure problem then has to be addressed in mention detection. Third, as the target of extraction is the entities, the mentions corresponding to the same entity must be linked together into the entity (i.e. the set of core-ferring mentions). Practical coreference resolution is a must. However, not all entities are named entities. For example, half entities in the EDR 2005 Chinese corpus do not contain any name mentions. This presents the challenge to the tra-ditional coreference resolution methods. Furthermore, there also exits some language specific challenges rising from word segmentation problem when working on Chinese language.
In this paper, we propose a novel solution to alleviate the aforesaid language dependent and independent difficulties and investigate the use of several classification and clustering approaches, including Conditional Random Fields (CRF) and Support Vector Machine (SVM). The major differences of our work from the others are in three aspects. First, named, nominal and pronominal mentions are detected sep-arately in order to avoid the noise introduced in learning pro-cess and caused by different linguistic characteristics they exhibit. Second, heads and extents are detected separately. Normally heads are detected more accurately. They will play an important role in the subsequent categorization and clustering processes. Third, we do not treat the three types of mentions equally when linking them together into the entities. A pairwise clustering algorithm integrating most-specific-first and closest-first rules is suggested. The final system is developed as a sequence of three separated pro-cesses: (1) mention detection, which finds all mentions and their nested heads, if available; (2) mention categorization, which classifies the entity types and the roles of the detected mentions; and (3) mention clustering, which merge the men-tions into the clusters corresponding to the entities present in the text.

The remainder of the paper is organized as follows. Sec-tion 2 briefly reviews the related work on named entity recognition and coreference resolution. Sections 3 to 5 de-tails the approaches applied in the three parts of our Chinese EDR system. Section 6 then presents experiment set-up and evaluation results. When evaluated on the EDR 2005 Chi-nese corpus, the system achieves top-tier results. Section 7 discuss and analyze the alternative classification-based ap-proaches to mention detection and clustering. Finally, Sec-tion 8 concludes the paper.
Entity detection and recognition has close ties to named entity recognition and coreference resolution, which have been the focus of attentions in the recent past due to the increasing demand of content extraction.

Named entity recognition has been studied extensively in English. Recent research mainly focused on machine learning approaches, including Hidden Markov Models [13], Maximum Entropy [5], Robust Risk Minimization [3], Sup-port Vector Machine [4], and Transformation-Based Learn-ing [14]. With these approaches, the recognition task was transformed into a sequential tagging or a multi-class classi-fication problem. The lexical, syntactic and semantic fea-tures of the words were used as input to various learn-ing algorithms. Most state-of-the-art systems also incorpo-rated learning models with external word knowledge, such as Gazetteer and various types of dictionaries.

The same task in Chinese, however, is more difficult than in English due to word segmentation problem. Previous study has showed that most Chinese segmentation errors (about 90%) stemmed from unknown words, which are mostly names. A word-based Chinese named entity recognition model might take advantages of the boundary information added from word segmentation, but it would also unavoid-ably suffer from the segmentation errors which can hardly be recovered. A straightforward solution is to adapt a general use segmentation system into a specific task [14]. An al-ternative solution is to develop the models based on charac-ters. It was observed in [6] that pure character-based models could outperform word-based models. Even so, word infor-mation was still considered useful. For example, the tags indicating the relative position of a character to or in a seg-mented word [1] or the possible words surrounding the cur-rent focused character within a window size [3] were used as the features integrated into the character-based models.
Recent research in coreference resolution has exhibited a shift from knowledge-based approaches to data-driven ap-proaches [11, 9, 7]. The central idea was to recast corefer-ence resolution as a binary classification task. The classi-fiers were trained to determine whether two mentions are coreferring to each other. The clustering algorithms then partitioned the mentions into the entity clusters based on the pairwise coreference decisions or taking one step ahead to consider the relationships between the mentions under concern and the entity clusters already constructed [12, 2]. The clustering algorithms were mostly the greedy clustering, also known as best-first clustering algorithms, though the closest-first or aggressive-merge clustering algorithms have also been analyzed. The mention pairs under the consid-eration were normally a mention and its possible preceding antecedents. In [5] however, they included all the symmetric pairs. The features used in the classification models could be the content match of the two mentions (e.g. their edit-ing distance, the character/word matching percentage etc.), their relative positions (e.g. the distance, the order), the gender match which is particular for pronouns, or the com-binations of them.
According to the ACE guidelines, entity mentions are cat-egorized into three common types, namely named mentions (NAM), nominal mentions (NOM) and pronominal mentions (PRO). A NAM is the mention headed by a proper name. It is the most specific mention. The NOM or PRO in con-trast is headed by a common noun or refers to the person or the thing that is preciously specified or understood from the context. They are non-specific mentions though in different degrees.

Amongst three types of mentions, the number of PRO is quite limited. Thus, the best results could be expected provided with the sufficient dictionary entries. NAM and NOM, however, are normally longer in length and can be expressed more flexibly. In terms of the different character-istics they present, we believe it is appropriate to detect the boundaries of three types of mentions separately in order to avoid the noise introduced by one another. This idea has been verified experimentally in [10].

For each mention, both extent (i.e. the whole mention) and its head must be identified. It is nature X  X  way of find-ing out extents first, and then identifying the nested heads by analyzing phrase structures. The problem with it is if extent detection is less or even much less accurate than head detection, the errors it propagates will certainly bring negative impact on head detection in sequential processing mode. Our solution is to detect extends and heads indepen-dently in parallel mode. At least, the high accuracy from head detection could be expected. Then, extent and head boundaries are checked for consistency and the heads are combined together with the extents they corresponding to. During the combination, head and extent boundaries con-strain each other. This provides the second chance for those wrongly detected boundaries to be corrected.
We transform mention detection into the sequential tag-ging task. Six character-based CRF models are developed with FlexCRFs 1 for extents and heads of three types of men-tions. We regard each single character as a token and encode Free downloadable from http://www.jasist.ac.jp/ hieux-character related information (such as surface N -gram pat-terns), and word related information (such as dictionaries and word segmentation outputs) as the associated features. The sequential tagging is encoded with the typical  X  X -I-O X  scheme. Tag  X  X  X  is for the token that begins an extent or a head,  X  X  X  for the tokens that are inside and  X  X  X  for the tokens that are outside.

Given observation sequence o = { o 1 ,  X  X  X  , o T } and label se-quence s = { s 1 ,  X  X  X  , s T } , CRFs define following conditional probability where Z ( o ) is the normalization factor applied to all label sequences; f k denotes one of the K edge and state feature functions;  X  k is the feature weight associated with f k . are trained to maximize the log-likelihood where the second item is used as the smoothing function, and  X  2 denotes the variance of all feature weights.
For an input observation sequence o , the optimal tag-ging sequence is obtained by the following formula using the Viterbi algorithm
Table 1 presents the features we select and investigate for both extents and heads, where index 0 indicates the cur-rent focused character, indices  X  n/n indicate the charac-ters n positions to the left or right of the current character ( n  X  X  1 , 2 } ). In Table 1, the seven character-based N -gram features are applied to the three types of mentions and are deemed as the fundamental features. The dictionary-based and/or segmentation-based features are used in combination with the character-based features when they are necessary (see Section 6.1).

We manually construct three dictionaries, particularly for detecting person names, location names and pronouns. They are the lists of Chinese surnames, countries and capital cities in the world, China provinces and cities, and Chinese pro-nouns.

Word information associated to a character can be en-coded in two ways, by either checking whether word bound-aries are inserted around the focused character by a segmen-tation system (we call this segmentation-based features) or checking whether the character together with the surround-ing characters can form some words in given dictionaries (we call this word-based features). The latter requires larger searching space, involves more uncertainty, and results in unsatisfactory performance in our experiments. Taking  X   X  I &lt;  X  (Chinese people) X  as an example, the character  X  &lt; (person) X  can be a part of three potential words,  X  I &lt; , &lt;  X  and &lt; . The segmented boundary, if it is correct, can more accurately indicate where this character is located (e.g. at the beginning or the end) in a particular word.
The segmentation-based features we use consider the left or the right boundaries. The features exam whether the uan/flexcrfs/flexcrfs.html boundaries are inserted by the segmentation system preced-ing or following the characters of concern. To obtain these features, a dictionary-based bi-directional maximum match-ing segmentation system is implemented.
When mention heads and extents are detected indepen-dently, their boundaries are sometimes inconsistent. In other words, the extent and the head of the same mention might be overlapped due to incorrect boundary identification. There are also some cases where the heads of the extents are missed or vice versa. We apply heuristic rules to correct the incon-sistent or the missing cases, and combine the heads with the extents they belonging to. The rules we use include re-moving surplus extents if no heads are found to associate with them, adding extents (same as heads) if heads find no extents associated, and expanding extents to allow entire heads to be included if heads overlap extents. Heads have high priority in rectifying extents during combination.
Entity types are additional information used to character-ize the entities mentioned. In ACE EDR 2005, the entities are categorized into the following seven types: person, orga-nization, Geo-political entity, location, facility, vehicle and weapon. They are further divided into 45 subtypes. Since the mentions are the instances of the entities, they natu-rally inherit the type attributes from the entities they refer to. We therefore identify the types according to the men-tions instead of entities in the first place and then use them as the clues to mention clustering in the subsequent process.
Type identification is a typical classification task. The classifier we choose is the Support Vector Machine (SVM) which has been successfully used for classification and re-gression in natural language processing and text catego-rization. Given training set { ( x 1 , y 1 ) ,  X  X  X  , ( x x i  X  R n , y i  X  { 1 ,  X  1 } ( i = 1 , 2 ,  X  X  X  , l ), SVM algorithm solves the following primal problem subject to Its dual is subject to The decision function is where C denotes the extra cost for errors in non-separable cases. We use following radial basis kernel function
As classical SVM classifiers generate only binary results, we use one-against-the-rest approach to produce multi-class classification outputs. We implement a set of SVM classifiers to identify mention type first and then the subtypes under each type they belonging to 2 .

Based on our previous experiments, it is reasonable to be-lieve that head information is more related to the types (or subtypes) of a mention than the information provided by a diversified extent. All classifiers are trained on the charac-ters appearing in mention heads or head context ([-2, +2]). Considering the same character plays different roles when appearing at different positions (e.g. inside the head and next to the boundaries, inside the head but away from the boundaries, outside the head but within the context win-dow), different weights are assigned to it according to where it is located (see Table 2). Head boundary characters are more informative.
 Beside Head Boundary c 0 , c n 1 Note that this time the classifiers are not built for NAM, NOM and PRO separately, as they are manipulated in men-tion detection. Previous attempts to build separate clas-sifiers fail to generate better results. We wonder whether performance improvement could be achieved when a larger training corpus is available.
An entity is a set of mentions which refer to the same object. The mentions coreferring to one another are ob-served having the following common characteristics. First, from the contents they convey, NAM is the most specific one in expressing an entity. NOM takes second place. PRO is most non-specific in terms to its flexibility in coreferring to any other specific mentions. On the other hand, from discourse point of view, most pronouns corefer to the enti-ties mentioned in preceding the first or the second sentence. Coreference resolution of PRO is preferred in local context. NAM and NOM on the other hand are allowed to corefer freely within a document, which is considered as global con-text. Four simple yet effective clustering rules are specially designed to accommodate to the above-mentioned character-istics. The rules mainly use types (and/or subtypes), con-
In fact, ACE EDR 2005 also requires categorizing the entity classes and roles. Although in this paper, we introduce the approaches and the experiments with entity categorization, the same approaches also apply to class and role categoriza-tion. tent matches of heads, and relative positions, particularly for PRO, to decide whether to link two mentions together or not.

The process of clustering is like a snow ball rolling pro-cess. Rules are applied starting from the most specific men-tion (i.e. NAM) pairs. NAM-NAM pairs are examined first. Both the linked and the dangling NAM mentions are rec-ognized as the named entities. Then, NOM mentions are examined to see if they can be added to any existing named entity, i.e. NOM-NAM pairs are examined. If the rule al-lows, they are a part of the named entities. The next rule goes forward to check NOM-NOM pairs. Similarly, both the linked and dangling NOM mentions are recognized as the nominal entities, if they are not directly or indirectly connected with any NAM mention. Finally, the most non-specific PRO mentions are examined and added to the ex-isting named or nominal entities if they can be linked to any preceding NOM or NAM mention. When there is more than one link allowed, the closest one is picked up. If not, PRO-PRO pairs are examined and the pronominal entities are constructed at the last.

Figure 1 illustrates two named entities and one nominal entity. Note that, two nominal mentions are not allowed to link together if they have been included into two different named entities (e.g. NOM1 and NOM2 are not linked). This constraint is to avoid merging two different named entities together (such as  X   X  o  X  (President Clinton) and  X   X  o  X  (President Bush)). The advantage from using rule-based approach is to ensure a high precision.

Corpora used for training and evaluating learning models are provided by Linguistic Data Consortium (LDC) for ACE 2005 Chinese EDR task. There are 3,907 entities and 9,198 mentions in evaluation corpus. Among them, 46.6%, 50.8% and 2.5% entities are named, nominal and pronominal en-tities respectively. Pie graphs in figure 2 show the mention distributions.
The purposes of conducting the following sets of experi-ments are to reveal how the proposed features contribute to mention head and extent detection, and to how effective the suggested sequential tagging approach is.
 Figure 2: Mention Distribution in Evaluation Cor-pus The results are evaluated according to the standard Precision-Recall-F-Measure criteria. They are summarized in Table 3. As expected, head detection is more accurate than extent extraction with regard to all types of mentions, and PRO detection is more accurate than NAM and NOM detection. But the differences between head and extent de-tection results are not significant with NAM and PRO, al-though we do see the 25% difference with NOM. This can be easily explained in terms of data distribution. Many men-tions may share the same head and extent boundaries. The percentage of such cases is about 90% with NAM or PRO, but only 35% with NOM.

It is interesting to find out that if we regard character N -grams as the basic set of features for all types of mentions, the value added from word segmentation features is more remarkable than it from dictionary-based features in both head and extent detection. It looks like segmentation-based features could discover additional patterns which are not covered by dictionary-based features, even for PRO men-tions. This is unanticipated. The reasons may be two folds. The incomplete name lists is obviously one of the rea-sons. However, it is impossible to make it complete in any way. When taking a closer look at the results, we find that segmentation-based features could indirectly recover some errors made in mention detection by correctly grouping non-mention words together. Take  X  7  X   X   X   X   X   X  (returning native Jin Men people) X  as an example.  X  7  X   X  is a loca-tion name. Without using segmentation-based features,  X  7  X   X   X  is detected as the name but it is wrong. After adding segmentation-based features,  X  7  X   X  is correctly detected, simply because  X   X   X   X  is grouped together as a common general noun. What surprises us is that N -gram features alone don X  X  work at all in detecting PRO extents and even heads.

Table 4 presents the evaluations on mentions after comb-ing their extents and heads. The overall F-measure of 70.95% is promising although there is still room for improvement. One may ask what about if mentions of different types of mentions are detected together with one unified classifier for either heads or extents. The previous experiments have shown that the unified classifiers perform worse than the separate ones [10].
 Table 4: Mention Detection Results after Head and Extent Combination
We use accuracy criterion to evaluate the performance of classification, and set up the experiments on the true men-tions extracted from the ACE evaluation corpus instead of on the mentions detected by our proposed methods. Ta-bles 5 and 6 present the accuracies of types and subtypes separately.
 Table 5: Type Classification Accuracy based on TRUE Mentions The overall type accuracy of 90.38% is quite competitive. But the accuracy of LOC is much lower than the accuracies of the other subtypes. We check the errors made related to LOC mentions. The reasons can be two folds, i.e. there are fewer numbers of training samples and possibly its subtype categories are not quite well defined. The mention clustering algorithm is evaluated by Precision-Recall-F-measure criteria, and tested on true men-tions (true boundaries and true types/subtypes). As men-tion clustering is the last step in the EDR task, when it is done, the final results are available. Therefore, we evaluate using the provided ACE EDR evaluation tool. The result is shown in Figure 3 3 . The top F-measures come from LOC, GPE and ORG. In overall, F-measure of 87.4% is quite a good performance.
 Figure 3: ACE Evaluation Results based on TRUE Mentions and Types
Compared with the learning models, handcrafted rules are more accurate and drive to high precision. In fact, we have implemented in both ways. The rule-based approach ends up with better F-measure and saves above 2 days of train-ing time. It has the advantages of being more flexible to integrate constraints and handling special cases. Yet, it is still weak in handling the coreference resolution of PRO, especially in handling those PRO which have no coreferred NAM or NOM in the text at all (i.e. they belong to the PRO entities). This is the most serious problem the other researchers may also encounter.
Finally, the whole system is evaluated. Features used in mention detection are those asterisked in Figure 3. All the
FA: spurious entities output; Miss: entities missed; Err: entities detected but misrecognized; Tot: reference entities. Pre = ( Tot  X  Miss  X  Err ) / ( Tot + FA  X  Miss ) Rec = ( Tot  X  Miss  X  Err ) /Tot Mention Type = NOM ACE participating systems are evaluated and ranked accord-ing to EDR values (under  X  X ost X  columns in Figure 4 in the next page), though the standard Precision-Recall-F-measure criteria are given as well. The EDR values in Figure 4 are normalized values with different cost parameters assigned to different types of mentions. Among all types, GPE has the highest value. It benefits from the use of external loca-tion lists in mention detection. The FAC and WEA values are comparatively low due to the small number of train-ing samples. The PER value is also low. We simply use a Chinese surname list in PER detection. The list might be insufficient compared to the characters appearing in Chinese names and translated foreign names. We consider introduc-ing additional word lists as the complement in the future.
Table 7 compares the ACE values of our system with the official results of the best and worst ACE 2005 Chinese EDR participating systems. Our system is in the leading group. Table 7: Comparisons with Other ACE CEDR Sys-tems
Training CRF models in sequential tagging based mention detection is quite time consuming. Under the limited hard-ware and software resources, naturally the following ques-tion comes to our mind. Can we make use of the same set of features, but simply assume that the characters are in-dependent and focus on one single character once a time to avoid long distance dependency in the sequence? If this as-sumption holds, the mention detection would become more effective.

We implement this idea by considering boundary detec-tion as a classification problem and adopting a weighed Bayes classification algorithm. For any character, it can always be followed (or preceded) by a left, a right bound-ary or none of them. We train two binary Bayes classifiers, one for left boundaries and one for right boundaries. The left and right boundaries are identified independently. The equation is given below, where n is the number of the fea-tures used, f i denotes the i th of n features,  X  i is the weight of the feature f i .  X  i is tuned experimentally. The features used in B L or B R classifiers are listed in Table 8. Note that even the features considered in Bayes clas-sification are mostly the same as those used in sequential tagging, bigram pronoun features are not necessary and IN, IS features are replaced with START and END features.
Different from sequential tagging where boundaries guar-antee to be paired well, the left/right boundaries identified by Bayes classifiers are independent. Paring is thus required. We have experimented with minimum distance and maxi-mum distance paring. The former performs better. The experiment results are illustrated in Table 9.

Comparing Tables 3 and 9, sequential tagging detection significantly outperforms the classification detection with all three types of mentions. The increments range from 3% to 18%. The precisions of the two approaches are distributed quite similarly among the three mention types. But the recall of sequential tagging approach, especially to NAM, is significantly higher than the recall of classification approach. Sequential tagging approach itself does not require addi-tional boundary matching process. Thus, the approach itself avoids the additional errors introduced by paring boundaries in classification approach. Taking NAM head detection as an example, F-measures of left and right boundaries alone are 86.34% and 78.78% respectively before paring. How-ever, F-measure drops significantly to 69.6% after paring. We have tried many other ways to combine the boundaries. Unfortunately, no one gives the result approaching to 79% in sequential tagging approach. The independent assumption definitely does not hold.
Recent researches in coreference resolution have yielded data-driven systems that rival their hand-crafted counter-parts in performance [8]. In our study, the learning-based mention clustering is also implemented in attempt to solv-ing the resolution problem under a more generalized frame-work. A SVM classifier encodes the linking rules mentioned in Section 5 into the features and decides whether any two mentions detected corefer or not. The features include one real value feature, i.e. the degree of head similarity, and four binary features, i.e. whether two mentions occur in the same sentence, or one before another, whether they are ap-position, whether they are of the same entity subtype and whether one is the abbreviation of the other. The coreferred mentioned are then linked together as an entity. Algorithms similar to this one are called aggressive-merge-clustering in [8]. SVM-based coreference resolution actually encodes ad-ditional information by considering the content similarity and the position information in a more flexible way. Thus, it achieves quite a high recall. However, it suffers from low precision.

Figure 5 shows entity (after mention clustering) precision and recall which are based on the best SVM parameters tuned experimentally 4 . Compared with the evaluation re-
Tunable SVM parameters are C (extra cost for errors) and sults shown in Figure 3, learning-based algorithm detects as 4 times of spurious entities as rule-based algorithm does, and 1/6.5 times of missing entities. The lower precision leads to the overall lower F-measure.
 Figure 5: Learning-based Coreference Resolution Results based on TRUE Mentions and Types
Features used in both algorithms are almost the same ex-cept for those adapted for a particular algorithm. But the difference between their F-measures is more than 10%. Af-ter taking a closer look, we find the main difference lies in the way of applying these features. Note that the mentions are merged in SVM-based algorithm depends on the links identified among all the mentions. A wrongly identified link may merge two true entities as an incorrect one. However, the merging in rule-based algorithm starts from the most specific mentions, forms and completes the entity sets in-crementally. In such a way, the mentions which have been included in an entity set are not allowed to link to any men-tions already included in another entity.
This paper presents our recent work on Chinese entity detection and tracking. A novel solution is proposed to al-leviate the language-independent and language-dependent problems special in this task. The system is implemented in the sequence of three separated processes, namely mention detection, mention categorization and mention clustering. Detection takes advantages of machine learning approaches and character-based models. It manipulates different types of entities being mentioned and different constitution units (i.e. extents and heads) separately. Mentions referring to the same entity are linked together by integrating most-specific-first and closed-first rule based pairwise clustering algorithms. Types are identified by head-driven classifica-tion approaches. The system achieves ACE value of 66.1, which has been one of the top-tier results. The preliminary success encourages us to further explore how can effectively integrate three processes together in the future.
The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong  X  (parameter in the kernel function).
 Mention Type = NAM Mention Type = NOM Mention Type = PRO Kong (project number: CERG PolyU5211/05E) and par-tially supported by a grant from the National Natural Sci-ence Foundation of China (project number: 60573186). [1] W. Chen, Y. Zhang, and H. Isahra. Chinese named [2] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, [3] H. Guo, J. Jiang, G. Hu, and T. Zhang. Chinese [4] H. Isozaki and H. Kazawa. Efficient support vector [5] A. Ittycheriah, L. Lita, N. Kambhatla, N. Nicolov, [6] H. Jing, R. Florian, X. Luo, T. Zhang, and [7] X. Luo, A. Ittycheriah, H. Jing, N.Kambhatla, and [8] V. Ng. Machine learning for coreference resolution: [9] V. Ng and C. Cardie. Improving machine learning [10] D. Qian, W. Li, C. Yuan, Q. Lu, and M. Wu.
 [11] W. Soon, H. Ng, and C. Lim. Machine learning [12] X. Yang, J. Su, G. Zhou, and C. Tan. An np-cluster [13] G. Zhou and J. Su. Named entity recognition using an [14] Y. Zhou, C. Huang, J. Gao, and L. Wu.

