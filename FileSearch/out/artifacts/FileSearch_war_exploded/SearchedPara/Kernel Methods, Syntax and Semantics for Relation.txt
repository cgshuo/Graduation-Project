 Previous work on Natural Language Processing for Information Retrieval has shown the inadequateness of semantic and syntac-tic structures for both document retrieval and categorization. The main reason is the high reliability and effectiveness of language models, which are sufficient to accurately solve such retrieval tasks. However, when the latter involve the computation of relational se-mantics between text fragments simple statistical models may re-sult ineffective. In this paper, we show that syntactic and semantic structures can be used to greatly improve complex categorization tasks such as determining if an answer correctly responds to a ques-tion. Given the high complexity of representing semantic/syntactic structures in learning algorithms, we applied kernel methods along with Support Vector Machines to better exploit the needed rela-tional information. Our experiments on answer classification on Web and TREC data show that our models greatly improve on bag-of-words.
 H. [ Information Systems ]: Information Storage and Retrieval X  Content Analysis and Indexing, Linguistic processing Algorithms, Experimentation, Performance Support Vector Machines, Kernel Methods, Text Categorization, Question Answering, Natural Language Processing
Previous work on Natural Language Processing (NLP) for Infor-mation Retrieval (IR) has shown that the increase of the compu-tational complexity for the processing of advanced linguistic rep-resentations is not justified by the small gain in accuracy, which often turns out to be a decrease. For example, apparently promis-ing linguistic structures like subject-verb-object have been shown in TREC to be inadequate for typical retrieval tasks [35]. Similar findings have been derived for Text Categorization using advanced linguistic processing, e.g. [22, 12, 1, 26]. Despite such failure, work on Question Answering (QA) suggests that syntactic and lin-guistic structures help in solving the task [41, 13].

From the above studies, it emerges that when the retrieval task is linguistically complex , syntax and semantics may play a relevant role. In this perspective, one of the most complex Text Catego-rization task relates to the detection of the relationships between two text fragments. One typical example of relation, useful for de-signing retrieval systems, is the one holding between question and answer, i.e. if the latter text fragment correctly responds to the for-mer.

In QA, this task is mostly tackled by using different heuristics and classifiers, which aim at extracting the best answers [6, 8]. However, if the question is a definition, a more effective approach would be to test if there is a correct relationship between the answer and the query. This depends on the structures of the two text frag-ments. Designing language models to capture such a relation could be too complex since it requires expensive probabilistic models (in terms of design and computational resources) for the representa-tion of structural information. More specifically, such models suf-fer from (i) computational complexity issues, e.g. the processing of large bayesian networks, (ii) a high complexity to estimate and smooth probabilities and (iii) high sensitiveness to irrelevant fea-tures and processing errors. These aspects make the use of linguis-tic processing very difficult since it inevitably introduces structures which contain noise and errors.

In contrast, discriminative models such as Support Vector Ma-chines (SVMs) [38] have been proven to be robust to noise and irrelevant features. Thus, partially correct linguistic structures may still provide a relevant contribution since only the relevant informa-tion will be taken into account. Moreover, such a learning approach supports the use of kernel methods which allow for an efficient and effective representation of structured data.

SVMs and Kernel Methods have recently been applied to natural language tasks with promising results, e.g. [7, 20, 11, 32, 10, 21, 37, 17, 44]. More specifically, in question classification, tree ker-nels [43, 25] have shown accuracy comparable to the best models, e.g. [23].

Moreover, [31, 28, 24] have shown that shallow semantic infor-mation in the form of predicate argument structures (PASs) [14, 16] improves the automatic detection of correct answers to a target question. In particular, in [28], we proposed kernels for processing PASs (in PropBank 1 format [19]) extracted from question/answer pairs. However, the relatively high kernel computational complex-ity and the limited improvement on the bag-of-words (BOW) do www.cis.upenn.edu/~ace not make the use of such technique practical for real world re trieval applications.

In this paper, we carry out a thorough study on the use of synta c-tic/semantic structures for relational learning from ques tions and answers. We designed sequence kernels for words and Part of Speech Tags which capture basic lexical semantics and basic syn-tactic information. Then, we design a novel shallow semanti c ker-nel which is far more efficient and also more accurate than the one proposed in [28].

The extensive experiments carried out on two different corp ora of questions and answers, which have been derived from Web do c-uments and the TREC corpus show that:
In the remainder, Section 2 presents our use of kernel functi ons for structural information and Section 3 introduces our dat a repre-sentations. Section 4 reports on our experiments with the ab ove models whereas Section 5 illustrates the related work. Fina lly, con-clusions are drawn in Section 6.
Kernel Methods refer to a large class of learning algorithms based on inner product vector spaces, among which Support Vector M a-chines (SVMs) are one of the most well-known algorithms. The main idea behind SVMs is to learn a hyperplane H ( ~x ) = ~w ~x + b = 0 , where ~x is the feature vector representation of a classifying ob-ject o whereas ~w  X   X  n (a vector space) and b  X   X  are parameters learnt from training examples by applying the Structural Risk Min-imization principle [38]. The object o is mapped in ~x with a feature function  X  : O  X   X  n , where O is the set of the objects. o is categorized in the target class only if H ( ~x )  X  0 .
 The kernel trick allows us to rewrite the decision hyperplan e as: where y i is equal to 1 for positive and -1 for negative examples,  X  i  X   X  with  X  i  X  0 , o i  X  i  X  { 1 , .., l } are the training instances and the product K ( o i , o ) = h  X  ( o i )  X  ( o ) i is the kernel function associated with the mapping  X  .

Note that it is not necessary to apply the mapping  X  , we can use K ( o i , o ) directly. This allows, under the Mercer X  X  conditions [30], for defining abstract kernel functions which generate implicit feature spaces. In turn, this alleviates the feature extrac tion/design step and allows for the use of huge feature space (possibly in finite) since the scalar product (i.e. K ( , ) ) is implicitly evaluated.
In the remainder of this section, we present the String Kerne l (SK) proposed in [30] to evaluate the number of subsequences be-tween two sequences, the Syntactic Tree Kernel (STK) [7], wh ich computes the number of syntactic tree fragments, the Shallo w Se-mantic Tree Kernel (SSTK) [28], which considers fragments f rom PASs, and the Partial Tree Kernel (PTK) [25], which provides a very general representation of trees in terms of tree fragme nts.
The String Kernels that we consider count the number of sub-strings containing gaps shared by two sequences, i.e. some o f the characters of the original string are skipped. Gaps modi fy the weight associated with the target substrings as shown in the follow-ing.

Let  X  be a finite alphabet,  X   X  = S  X  n =0  X  n is the set of all strings. Given a string  X   X   X   X  , |  X  | denotes the length of the string,  X  can be written as s 1 ..s | s | with s i  X   X  and  X  [ i : j ] selects the subsequence of  X  if there is a sequence of indices ~ I = ( i with 1  X  i 1 &lt; ... &lt; i | u |  X  |  X  | , such that u = s u =  X  [ ~ I ] for short. d ( ~ I ) is the distance between the first and last character of the subsequence u in  X  , i.e. d ( ~ I ) = i | u | Finally, given  X  1 ,  X  2  X   X   X  ,  X  1  X  2 indicates their concatenation.
The set of all substrings of a text corpus forms a feature spac e denoted by F  X   X   X  . To map a string  X  into R  X  space, we can  X   X  1 . These functions count the number of occurrences of u in the string  X  and assign them a weight  X  d ( ~ I ) proportional to their lengths. Hence, the inner product of the feature vectors for two strings  X  1 and  X  2 returns the sum of all common subsequences weighted according to their frequency of occurrences and le ngths, i.e.

It is worth to note that: (a) longer subsequences receive low er weights; (b) valid substrings are sequences of the original string with some characters omitted, i.e. gaps; (c) gaps determine the weighting function since d ( . ) counts the number of characters in the substrings as well as the gaps that were skipped in the ori ginal string, and (d) symbols of a string can also be whole words, i. e. the Word Sequence Kernel [4].
The main underlying idea of tree kernels is to compute the num -ber of common substructures between two trees T 1 and T 2 out explicitly considering the whole fragment space. Let F = { f 1 , f 2 , . . . , f |F| } be the set of tree fragments and  X  dicator function equal to 1 if the target f i is rooted at node n and equal to 0 otherwise. A tree kernel function over T 1 and T fined as T K ( T 1 , T 2 ) = P n N 1 and N T 2 are the sets of nodes in T 1 and T 2 , respectively, and
The  X  function is equal to the number of common fragments rooted in nodes n 1 and n 2 , and thus, depends on the fragment type. We report its algorithm for the evaluation of the number of sy ntac-tic tree fragments (STFs) [7], the number of shallow semanti c tree fragments (SSTFs) [28], and the number of partial tree fragm ent (PTFs) [25].
A syntactic tree fragment (STF) is a set of nodes and edges fro m the original tree which is still a tree and with the constrain t that Figure 1: A tree for the sentence "Autism is a disease" with some of Figure 2: A tree for the sentence "Autism is a disease" with some of any node must have all or none of its children. This is equival ent to stating that the production rules contained in the STF can not be partial.

To compute the number of common STFs rooted in n 1 and n 2 the STK uses the following  X  function [7]: 1. if the productions at n 1 and n 2 are different then  X ( n 2. if the productions at n 1 and n 2 are the same, and n 1 3. if the productions at n 1 and n 2 are the same, and n 1 of node n and  X  is a decay factor penalizing larger structures. Figure 1 shows 10 STFs (out of 17) of the subtree on the left. STFs satisfy the constraint that grammatical rules cannot b e bro-ken. For example, [VP [VBZ NP]] is a STF which has two non-terminal symbols, VBZ and NP , as leaves whereas [VP [VBZ]] is not a STF.
A shallow semantic tree fragment (SSTF) is almost identical to a STF, the difference being that the contribution of special nodes labeled with null should be zero. This is necessary as the SSTK is applied to special trees containing SLOT nodes, which, wh en empty, have children labeled with null . Two steps are modified in the algorithm: 0. if n 1 (or n 2 ) is a pre-terminal node and its child label is null ,
If we relax the production rule constraint over the STFs, we o b-tain a more general substructure type called partial tree fr agment (PTF), generated by the application of partial production r ules, e.g. [VP [VBZ [is]]] in Figure 2. The  X  function for PTK is relatively simple. Given two nodes n 1 and n 2 , STK is applied to all possi-ble child subsequences of the two nodes, i.e. the String Kern el is applied to determine the subsequences and the STK is applied on each of such child substrings. More formally: 1. if the node labels of n 1 and n 2 are different then  X ( n 2. else  X ( n 1 , n 2 ) = where ~ I 1 = h h 1 , h 2 , h 3 , .. i and ~ I 2 = h k 1 , k sequences associated with the ordered child sequences c n the corresponding sequence, and, again, l ( ) returns the sequence length, i.e. the number of children.

Furthermore, we add two decay factors: for the depth of the tree and  X  for the length of the child subsequences with respect to the original sequence, i.e. we account for gaps. It follows t hat  X ( n 1 , n 2 )=  X   X  2 + X where d ( ~ I 1 ) = ~ I 1 l ( ~ I we penalize both larger trees and child subsequences with ga ps
Kernel engineering can be carried out by combining basic ker -nels with additive or multiplicative operators or by design ing spe-cific data objects (vectors, sequences and tree structures) for the target tasks.

It is worth noting that kernels applied to new structures pro duce new kernels as shown hereafter. Let K ( t 1 , t 2 ) =  X  ( t a basic kernel, where t 1 and t 2 are two trees. If we map t t into two new structures s 1 and s 2 with a mapping  X  M ( ) , we obtain: K ( s 1 , s 2 ) =  X  ( s 1 )  X  ( s 2 ) =  X  (  X  M ( t  X  induced by the mapping  X   X  =  X   X   X  M .
 For instance, in the next section, we will define the novel PAS and POS SK kernels by applying PTK and SK to innovative struc-tures, i.e. predicate argument structures and sequences of Part of Speech Tags, respectively.
Capturing the relationships between two text fragments is a com-plex task. Some work regarding the relation between questio n and answer has been carried out in [29, 13, 42]. In such work, the a n-swer extraction step is implemented by means of unsupervise d ap-proaches which measure the relevance between questions and an-swers. However, learning to classify if an answer is correct for a question (the problem will be formally defined in the next sec tion) is conceptually different from extraction in that not only t he relat-edness between the target question and answer is taken into a ccount but also the other Q/A training pairs are used. The similarit y be-tween pairs clearly depends on syntactic and semantic prope rties; thus, in addition to the usual bag-of-words (BOW), we study m eth-ods to capture Q/A structures using String Kernels over word and POS-tag sequences and tree kernels over full syntactic pars e trees (PTs) and shallow semantic trees.
An efficient algorithm for its computation is given in [25]. (b) Figure 3: Compact PAS P T K structures of s 1 (a) and s 2
A Q/A classifier receives pairs h q, a i as input and judges if the answer a correctly responds to the query q . For its design, a set of examples of correct and incorrect pairs is needed. The lea rning algorithm operates by comparing the content of questions an d the content of answers in a separate fashion rather than just com paring a question with its corresponding candidate answers. In a le arning framework where kernel functions are deployed, given two pa irs p 1 = h q 1 , a 1 i and p 2 = h q 2 , a 2 i , a kernel function is defined as K ( p 1 , p 2 ) = K  X  ( q 1 , q 2 )  X  K  X  ( a 1 , a 2 ) , where K kernel functions defined over questions and over answers, re spec-tively, and  X  is a valid operation between kernels, e.g. sum or multiplication.

In Section 2, we described sequence and tree kernels, which c an be applied to the sequential and tree representations of que stions and answers, respectively. In the following section we desc ribe several of such linguistically motivated representations .
For a basic syntactic and semantic representation of both qu es-tions and answers, we propose two different kernels: the Par t of Speech Sequence Kernel (POS SK ) and Word Sequence Kernel (WSK). The former is obtained by applying the String Kernel o n the sequence of POS-tags of a question or answer. For example , given the sentence s 0 : What is autism? , the associated POS se-quence is WP AUX NN ? and some of subsequences extracted by POS SK are WP NN or WP AUX . WSK is applied to word sequences of both questions and answers; given s 0 , sample substrings are: What is autism , What is , What autism , is autism , etc.
A more complete structure is the full parse tree (PT) of the se n-tence, that constitutes the input of the STK. For instance, t he STK accepts the syntactic parse, (SBARQ (WHNP (WP What))(SQ (VP (AUX is)(NP (NN autism))))(. ?)) , and extracts from it all possible tree fragments (STFs).
Our semantic representation takes into account that definit ions are characterized by a latent semantic structure, thanks to which similar concepts result in structurally similar formulations. Pre-cisely, understanding whether a candidate answer is correc t for a definition question would imply knowing the correct definiti on and comparing the current candidate to the former. When such inf orma-tion is unavailable (as in open domain QA) the learning algor ithm must mimic the behavior of a human (who does not know the exact definition) and check whether such answer is formulated as a  X  typ-ical X  definition and whether answers defining similar concep ts are expressed in a similar way. A method to capture sentence stru cture [2] is the use of predicate argument structures described he reafter.
Shallow approaches to semantic processing are making large strides in the direction of efficiently and effectively deriving tac it semantic information from text. Large data resources annotated with levels of semantic information as in the FrameNet [16] and ProbBank [19] projects, make it possible to design systems for the automat ic ex-traction of predicate argument structures (PASs)[5].

Such systems identify predicates (e.g. verbs) and their arg u-ments in a sentence. For example, in the English sentence,  X  X  ohn likes apples. X , the predicate is  X  X ikes X  whereas  X  X ohn X  and  X  X pples X , bear the semantic role labels agent (ARG0) and theme (ARG1). The crucial fact about semantic roles is that regardless of t he overt syntactic structure variation, the underlying predicates remain the same. Hence, for the sentence  X  X ohn opened the door X  and  X  X he door opened X , although  X  X he door X  is the object of the first senten ce and the subject of the second, it is the  X  X heme X  in both sentences . Same idea applies to passive constructions.

To represent PASs in the learning algorithm, we consider two trees: Shallow Semantic Trees for SSTK and Shallow Semantic Trees for PTK, both according to PropBank definition, indica ted as PAS SST K and PAS P T K , respectively. These are automatically generated by our system. As an example, let us consider the se n-tence (from our QA TREC corpus) s : Autism is characterized by a broad spectrum of behavior that includes stimuli. , which results in the PB annotation: Such annotation can be used to design a shallow semantic repr e-sentation that can be matched against other semantically si milar sentences, e.g. s : Panic disorder is characterized by unrealistic or excessiv e anxiety. This results in the PB annotation: It can be observed here that, although autism is a different d isease from panic disorder, the structure of both definitions and th e latent semantics they contain (inherent to behavior, disorder, an xiety) are similar. So for instance, s 2 appears as a definition even to someone who only knows what the definition of autism looks like.
The above annotation can be compactly represented by predic ate argument structure trees (PASs) such as those in Figure 3. He re, we notice that the semantic similarity between sentences is ex plicitly visible in terms of common fragments extracted by PTK from th eir respective PASs.
 An equivalent PAS representation (PAS SST K ) compatible with SSTK (see Section 2.2.2) was introduced in [28] (see Figure 4 ). Here, arguments follow a fixed ordering (i.e. rel , A 0 , A 1 , A 2 , . . . ) and a layer of SLOT nodes  X  X rtificially X  allows SSTK to genera te structures containing subsets of arguments. PAS P T K is semanti-cally equivalent to PAS SST K but PTK is able to extract a richer set of features which take gaps into account, (compare the fir st two fragments of Figures 3.(c) and 4). Moreover, PAS P T K does not need SLOT nodes to extract fragments containing argument su b-sets. This results in a visibly more compact representation (com-pare Figures 3.(b) and 4). Moreover, the accuracy of computi ng the Figure 4: PAS SST K of s 2 and some of its fragments produced by the SSTK. matches between two PASs can only improve as only nodes that a re actually useful are represented.
Our experiments aim at studying the impact of kernel methods applied to linguistic representations for the detection of QA rela-tions. In particular, we first show that our new shallow seman tic tree kernel is far more efficient and effective than previous ly pro-posed kernels for shallow semantic processing. Then, we stu dy the impact of the new Part of Speech Tagging sequence kernel, the Word Sequence Kernel and novel kernel combinations for QA cl as-sification. Finally, since the obvious use of the above class ifier is to improve the answer extraction phase, we experimented with i t and a basic QA system.
We implemented the String Kernel (SK), the Syntactic Tree Ke r-nel (STK), the Shallow Semantic Tree Kernel (SSTK) and the Pa r-tial Tree Kernel (PTK) described in Section 2 in our SVM-Ligh t-TK toolkit, available at disi.unitn.it/moschitti (which is based on SVM-Light [15] software). Each kernel is associate d with diverse input objects:
Kernel combinations are obtained by simply summing the targ et kernels. In particular, since answers often contain more th an one PAS (see Figure 3), we sum PTK (or SSTK) applied to all pairs P  X  P 2 , where P 1 and P 2 are the set of PASs of the first and second answer 3 . Although different kernels can be used for questions and answers, we used (and summed together) the same kernels exce pt for those based on PASs, which are only used on answers.
To train and test our text QA classifiers, we designed two data sets containing answers for definitional questions only (availa ble at disi .unitn.it/~silviaq/resources.html ). These are among the most complex and interesting in the literature [18, 9] as the y require deeper linguistic processing than factoid answers. We chos e them since the models for the solution of factoid questions may be too
More formally, let P t and P t  X  be the sets of PASs extracted from text fragments t and t  X  ; the resulting kernel will be K easy, e.g. simple language model approaches along with the u se of a named entity recognizer may be sufficient.

The datasets were created by first collecting the 138 TREC 200 1 test questions labeled as  X  X escription X  in [23] (one of the l argest available corpus of description questions) and, for each qu estion, by gathering the top 20 answer paragraphs extracted by our ba sic QA system (BQAS).
 BQAS was run on two sources: Web documents by utilizing Google ( code.google.com/apis/ ) and the AQUAINT data used for TREC X 07 ( trec.nist.gov/data/qa ) by exploiting Lucene ( lucene.apache.org ), yielding two QA classification corpora, namely WEB and TREC. Each paragraph was manually evaluated based on whether it contained an answer to the corresponding ques-tion. To simplify the classification problem, for each parag raph, we isolated the sentence with the maximal judgment 4 and labeled it as positive if it answered the question either concisely or wit h noise or as negative otherwise. The WEB corpus contains 1309 sentenc es, 416 of which are positive 5 answers and the TREC-QA corpus con-tains 2256 sentences, 261 of which are positive.
The accuracy of the classifiers is provided by F1 whereas the Q A system performance is measured in terms of the Mean Reciproc al Rank (MRR) 6 . All values reported on tables refer to the average of 5 different samples using 5-fold cross-validation whereas typically each plot refers to a single fold.

We carried out some preliminary experiments of the basic ker -nels on a validation set and we noted that the F1 was maximized by using the default cost parameters (option -c of SVM-light ),  X  = 0 . 04 and = 0 . 4 (see Section 2). The trade-off parameter varied according to different kernels on WEB data (so it needed an ad -hoc estimation) whereas a value of 15 was optimal for any kernel o n TREC corpus.
To make the use of kernels for the processing of semantic in-formation practical, we designed PAS P T K , which is specifically designed for PTK. This is more compact than PAS SST K and can be efficiently processed by PTK. In contrast, SSTK runs on lar ge
In case more than one sentence in the paragraph had the same judgment, we chose the first one.
For instance, given the question  X  X hat are invertebrates? X  , the sentence  X  X t least 99% of all animal species are invertebrat es, com-prising . . .  X  was labeled  X -1 X  , while  X  X nvertebrates are ani mals without backbones. X  was labeled  X +1 X . rk i is the rank of the first correct answer to question i . structures containing as many slots as the number of possibl e pred-icate argument types. This impacts on the memory occupancy a s well as on the kernel computation speed. PTK is able to proces s the same information with much smaller structures.

To test the above mentioned characteristics, we divided the train-ing (TREC) data in 9 bins of increasing size (200 instances be tween two contiguous bins) and we measured the learning and testin g time 7 for each bin. Figure 5 shows that in both the classification and learning phases, PTK is much faster than SSTK. With all tr ain-ing data, SSTK employs 487.15 seconds whereas PTK only uses 12.46 seconds, i.e. it is about 40 times faster, making the ex peri-mentation of SVMs with large datasets feasible.
 To completely assess the benefit of PAS P T K with respect to PAS SST K , we also need to compare them in terms of classification accuracy. In the next section, we extensively test several k ernels and their combinations.
In these experiments, we tested different kernels and some o f their most promising combinations. Since the nature of the t ype of the applied kernels strongly depends on the data they oper ate on, we simplify our notation by using only the name of the repr e-sentation instead of using the more appropriate name combin ation (representation and kernel). In other words, we used BOW, PO S and PT to indicate that a linear kernel is applied to bag-of-w ords and POS vectors and the syntactic tree kernel is applied to pa rse tree (PT).

The other kernel names show a subscription indicating the ap -plied kernel, i.e. POS SK , PAS SST K and PAS P T K . This suggests that SK is applied to POS sequences and that SSTK and PTK are applied to the PAS structures. The only exception is WSK indi cat-ing the Word Sequence Kernel, i.e. a string kernel applied to word sequences.

As kernel combinations, we used the sum between kernels 8 it yields the joint feature space of the individual kernels [ 30].
Table 1 shows the average F1  X  the standard deviation over 5-folds on Web (and TREC) data of SVMs using different kernels. We note that:
Processing time in seconds of a Mac-Book Pro 2.4 Ghz.
All additive kernels are normalized to have a similarity sco re be-tween 0 and 1, i.e. K  X  ( X 1 , X 2 ) = K ( X 1 ,X 2 )  X 
The above findings are interesting as the syntactic informat ion provided by STK and the semantic information brought by WSK and PAS P T K improve on BOW. The high accuracy of BOW is sur-prising if we consider that at classification time, instance s of the training models (e.g. support vectors) are compared with di fferent test examples since questions cannot be shared between trai ning and test set 9 . Therefore the answer words should be different and useless to generalize rules for answer classification. Howe ver, error analysis revealed that although questions are not shared be tween training and test set, there are common patterns in the answe rs due to typical Web page patterns which indicate if a retrieved pa ssage is an incorrect answer, e.g. Learn more about X .

Although the ability to detect these patterns is beneficial f or a QA system as it improves its overall accuracy, it is slightly mi sleading for our study. Thus, we experimented with the TREC corpus whi ch does not contain Web extra-linguistic texts and it is more co mplex from a QA task viewpoint (it is more difficult to find a correct a n-swer).
 Table 1 also shows the classification results on the TREC data set. A comparative analysis suggests that: Finally, PAS adds further information as the best model is PO S PT+PAS P T K , which improves BOW from 24.2 to 39.1, i.e. 61%.
To better study the benefit of the proposed linguistic struct ures, we also plotted the Precision/Recall curves. Figure 6 shows the curves of some interesting kernels for four (out of five) fold s of the Web dataset. As expected, BOW almost always shows the lowest curves (on 3 folds). Anyway, its relevant contribution is ev ident: when summed to PT, it produces the highest curves on folds 1 an d 2. Moreover, WSK, which is able to exploits n-grams (with gap s), summed to PT produces very high curves 10 (see folds 3 and 4). In summary, all the kernel combinations tend to achieve slig htly higher result than BOW. Again, the cause is the high contribu tion of BOW, which prevents the other models to clearly emerge.
Sharing questions between test and training sets would be an er-ror from a machine learning perspective as we cannot expect n ew questions to be identical to those in the training set.
Some of the kernels have been removed from the figures so that the plots result more visible.
The results on TREC in Figure 7 are more interesting. The con-tribution of BOW is always very low and thus the difference in accuracy with the other linguistic models is more evident. P OS +PT+PAS P T K , which encodes the most advanced syntactic and se-mantic information, shows a very high curve outperforming a ll the others (see folds 1, 3 and 4). Only, in Fold 2, the plots are clo se and POS SK +PT shows a competitive curve.
 The analysis of the above results suggests that: first as expe cted, BOW does not prove very relevant to learn re-ranking functio ns from examples; while it is useful to establish the initial ra nking by measuring the similarity between question and answer, it is almost irrelevant to capture typical rules that suggest if a descri ption is valid or not. Indeed, since test questions are not in the trai ning set, their words as well as those of candidate answers will be diff erent, penalizing BOW models. In these conditions, we need to rely o n syntactic structures which at least allow for detecting wel l formed descriptions.

Second, the results show that PT is important to detect typic al description patterns but POS sequences also provide additi onal in-formation since they are less sparse than tree fragments. Su ch pat-terns improve on the bag of POS-tags by about 4% (see POS vs POS SK on TREC data). This is a relevant result considering that in standard text classification bigrams or trigrams are usua lly inef-fective.

Third, although POS SK +PT generates a very rich feature set, i.e. POS patterns provided by SK and tree fragments generate d by STK, PAS P T K is still able to significantly improve the classifica-tion F1 by about 3%, suggesting that shallow semantics can be very useful to detect if an answer is well formed and is related to a ques-tion. Error analysis revealed that PAS can provide patterns like: -A1(X) R-A1(that) rel(result) A1(Y) -A1(X) rel(characterize) A0(Y) , where X and Y need not necessarily be matched.

Finally, the best model, POS SK +PT+PAS P T K , improves on BOW by 61%. This is strong evidence that complex natural languag e tasks require advanced linguistic information that should be ex-ploited by powerful algorithms such as SVMs and by effective fea-ture engineering techniques such as kernel methods.
In these experiments, we assess the impact of the binary QA classifiers learned during the previous experiments when us ed as re-rankers. Our classifier-based re-ranking algorithm pro ceeds as follows. Starting from the top answer in the ranked list elab orated by BQAS, if the answer is classified as correct by the learned c las-sifier, its rank is unchanged; otherwise the answer is pushed down, until the next answer labeled as incorrect according to the c lassifier is found.
Tables 2 and 3 report the classification F 1 of the answer clas-sifier (Column Class. F 1 ) and the MRR of the QA systems. In case of IR and BQAS, Class. F 1 is equal to the accuracy, i.e. the number of retrieved correct answers divided by the number of re-trieved answers, whereas in case of the Re-ranker rows, Class. F 1 refers to the question/answer classifier F1 (using BOW and th e best model, respectively). Moreover, for the re-rankers, the MR R col-umn refers to BQAS to which the BOW or the best re-rankers are applied.

From the Class. F 1 column, it is visible that BQAS improves on the baseline IR engines, but the learned Q/A classifiers re ach an almost double F1, i.e. 68.2 vs 36.8 on the WEB dataset (Table 2 ) and 39.1 vs 22.9 on the TREC dataset (Table 3). This suggests t hat re-rankers based on the classifiers draw more information fr om the available data with respect to BQAS. However, the real test o f the usefulness of the binary classifiers learned in the previous sections must be measured in terms of the improvement of MRR of BQAS after answer re-ranking.

In the MRR column of Table 2, we first report our baseline rank-ing performance for the WEB dataset. We see that the IR engine (Google) ranking is outperformed by BQAS since Google ranks is based on the entire documents rather than the more accurate s ingle passages. In the last 2 rows, we show the MRR of the re-rankers obtained by using the BOW and the best models (see Section 4.3 ) for re-ranking, respectively. With the BOW model, the re-ra nker achieves 77.4%, i.e. an improvement of 21% over the ranking o f BQAS; however, when the best re-ranking algorithm is applie d, we achieve 81.1%, i.e. an impressive 25% of improvement over th e baseline (Table 2). This shows that the additional structur al infor-mation provided by the best model improves the performance o f the re-ranker, which, in turn, improves the QA system.

In the TREC dataset (Table 3), the IR engine (Lucene) is also remarkably outperformed by BQAS; however, the BOW re-ranke r yields a further improvement despite the fact that the corre sponding F1 measure is not extraordinary. Furthermore, the best re-r anker model produces an improvement of about 4% over BQAS, i.e. a further 2% improvement over the BOW re-ranker (Table 3). The lower improvement on MRR of the best TREC re-ranker can be explained by its lower absolute classification accuracy tha n the Web re-ranker. This is due to the higher complexity of the TREC da taset.
Early work on using syntax and semantics in Information Re-trieval was carried out in [39, 40, 34] and in [36, 35]. The res ults showed that the use of advanced linguistic information is no t ef-fective for document retrieval. In contrast, Question Answ ering work shows that semantic and syntax are essential to retriev e punc-tual answers, e.g [13, 41, 33]. However, successful approac hes in Table 2: Classifier F1 and MRR (  X  Std. Dev.) of IR engine, BQAS, Table 3: Classifier F1 and MRR (  X  Std. Dev.) of IR engine, BQAS TREC are based on many interconnected modules exploiting co m-plex heuristics and fine tuning. The effective combination o f such modules strongly depends on a manual setting, which is not of ten discussed or published. This prevents us from deriving gene ral and useful findings on the use of natural language processors to m odel syntactic and semantic structures for retrieval tasks.

In our study, we avoid this problem by focusing on only one module of Question Answering, which can actually be seen as a typical Text Categorization task, i.e. the classification o f pairs of text fragments constituted by question and answer. Since so me kinds of questions can be solved with relatively simple repr esenta-tions, i.e. without the use of syntactic and semantic struct ures, we focus on the more complex task of processing definitional que stions [3, 6, 31, 2, 28, 24]. In particular, the last four articles us e predi-cate argument structures for re-ranking the answer lists, r eporting significant improvement.

To our knowledge, our work in [28] is the only one using kernel methods for answer re-ranking. We used a syntactic tree kern el and a shallow semantic tree kernel based on predicate argument s truc-tures for re-ranking design. However, we only experimented with a Question Answering corpus derived from Web documents and the reported improvement, although significant, did not jus tify the adoption of relatively computationally expensive approac hes like SVMs and kernel methods. In this paper, we have experimented with many more kernel types and with both Web and TREC docu-ments and we could show that the potential improvement reach able by our approach is much higher (about 61% over BOW). Moreover , we have designed a faster kernel for the processing of semant ic in-formation.

In summary, the main property of our approach with respect to previous work on the use of syntactic and semantic structure s is that we can define them without requiring a thorough linguist ic analysis. We do not carry out feature engineering since we si m-ply let kernel functions generate a large feature set (tree f ragments or substrings) which effectively represent the semantic/s yntactic in-formation. The feasibility of this approach is due to the SVM the-ory which makes the learning algorithm robust to many irrele vant features (often produced by the NLP errors).
In this paper, we study several types of syntactic/semantic in-formation: bag-of-words (BOW), bag-of-POS tags, syntacti c parse trees and predicate argument structures (PASs), for the des ign of automatic question/answer pair classifiers. In addition, w e investi-gate the role that syntax and semantics can play in modern Inf or-mation Retrieval systems. These binary classifiers can sele ct the correct answers from those provided by a basic QA system, thu s improving the final system accuracy.

Our learning framework is constituted by Support Vector Ma-chines (SVMs) and kernel methods applied to automatically g en-erated syntactic and semantic structures. On the one hand, S VMs are robust to irrelevant/noisy features, which are inevita bly con-tained in automatically generated linguistic structures. On the other hand, kernel methods allow for the extraction of many featur es from structured data, thus alleviating the manual design. I n par-ticular, we designed (i) a new shallow semantic kernel which is faster and more accurate than those previously proposed; (i i) a new sequence kernel over POS tags to encode shallow syntactic in for-mation; (iii) many kernel combinations (to our knowledge no previ-ous work uses so many different kernels) which allow for the s tudy of the role of several linguistic levels in a well defined stat istical framework. In addition, we tested the above models on two dif -ferent question classification corpora derived from Web and TREC documents, respectively, by also measuring their impact on a basic QA system.

The results suggest that:  X  the new kernel for processing PASs is more efficient and ef-fective than previous models so that it can efficiently be use d in answer re-ranking systems.  X  Kernels based on PAS, POS-tag sequences and syntactic parse trees improve on BOW on both datasets. On the TREC data the improvement is interestingly high, e.g. about 61%, making t heir application in retrieval systems worthwhile. Note that thi s goes far beyond our previous findings since in them we only observe d a small accuracy increase over Web documents.  X  Error analysis revealed that Web documents are slightly mis -leading in determining the role of linguistic structures si nce they contain many extra-linguistic patterns, e.g. "Learn more a bout X" which are easily captured by BOW. Thus, although the correct ness of a definition does not heavily depend on words but rather on struc-tures, BOW model is needed to carry out an important first leve l of answer filtering.  X  Our best question/answer classifier, used as re-ranker, sig nifi-cantly improves the QA system accuracy, demonstrating its p romis-ing applicability.

Finally, the mathematical elegance of kernel methods allow for the separation of data, e.g. our presented linguistic struc tures, which are easily understood, from the feature space, which is auto mat-ically generated. The latter may be complex to study since it is implicitly generated but at the same time it is easy to use tha nks to the availability of basic kernel functions for structured d ata. I would like to thank the anonymous reviewers for their profe s-sional and competent reviews. Particular thanks are due to S ilvia Quarteroni who carried out the data extraction and the evalu ation of the QA systems. Many thanks to my friends, Sindy and Hatim, to have ruined their Friday in correcting the syntax of an early draft of this paper. This work has been partially supported by the Eur opean Commission -LUNA project, contract n. 33549. [1] J. Allan. Natural language processing for information [2] M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. Structu red [3] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer . [4] N. Cancedda, E. Gaussier, C. Goutte, and J. M. Renders. [5] X. Carreras and L. M X rquez. Introduction to the [6] Y. Chen, M. Zhou, and S. Wang. Reranking answers from [7] M. Collins and N. Duffy. New ranking algorithms for parsi ng [8] K. Collins-Thompson, J. Callan, E. Terra, and C. L. Clark e. [9] H. Cui, M. Kan, and T. Chua. Generic soft pattern models fo r [10] A. Culotta and J. Sorensen. Dependency Tree Kernels for [11] C. Cumby and D. Roth. Kernel Methods for Relational [12] J. Furnkranz, T. Mitchell, and E. Rilof. A case study in u sing [13] A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, an d [14] R. Jackendoff. Semantic Structures . MIT Press, 1990. [15] T. Joachims. Making large-scale SVM learning practica l. In [16] C. R. Johnson and C. J. Fillmore. The framenet tagset for [17] J. Kazama and K. Torisawa. Speeding up Training with Tre e [18] H. Kazawa, H. Isozaki, and E. Maeda. NTT Question [19] P. Kingsbury and M. Palmer. From Treebank to PropBank. I n [20] T. Kudo and Y. Matsumoto. Fast Methods for Kernel-Based [21] T. Kudo, J. Suzuki, and H. Isozaki. Boosting-based pars e [22] D. D. Lewis. An evaluation of phrasal and clustered [23] X. Li and D. Roth. Learning question classifiers: the rol e of [24] M. C. M. Surdeanu and H. Zaragoza. Learning to rank [25] A. Moschitti. Efficient convolution kernels for depend ency [26] A. Moschitti and R. Basili. Complex linguistic feature s for [27] A. Moschitti, B. Coppola, A. Giuglea, and R. Basili. [28] A. Moschitti, S. Quarteroni, R. Basili, and S. Manandha r. [29] S. Narayanan and S. Harabagiu. Question Answering base d [30] J. Shawe-Taylor and N. Cristianini. Kernel Methods for [31] D. Shen and M. Lapata. Using semantic roles to improve [32] L. Shen, A. Sarkar, and A. k. Joshi. Using LTAG Based [33] S. Small, T. Strzalkowski, T. Liu, S. Ryan, R. Salkin, [34] A. F. Smeaton. Using NLP or NLP resources for informatio n [35] T. Strzalkowski, J. P. Carballo, J. Karlgren, A. H. P. [36] T. Strzalkowski, G. C. Stein, G. B. Wise, J. P. Carballo, [37] K. Toutanova, P. Markova, and C. Manning. The Leaf Path [38] V. Vapnik. The Nature of Statistical Learning Theory . [39] E. M. Voorhees. Using wordnet to disambiguate word sens es [40] E. M. Voorhees. Query expansion using lexical-semanti c [41] E. M. Voorhees. Overview of the TREC 2001 Question [42] Y. Wu, R. Zhang, X. Hu, and H. Kashioka. Learning [43] D. Zhang and W. Lee. Question classification using suppo rt [44] M. Zhang, J. Zhang, and J. Su. Exploring Syntactic Featu res
