
Chinese Academy of Sciences as Ranking SVMs [12, 13], RankBoost [11], and RankNet [5] hav e been widely applied. associated with them.
 to assume a random sampling process for the generation of lab eled documents per query. derive meaningful bounds for query layer error and document layer error respectively. 2.1 Pairwise Learning to Rank Assume there are n queries { q documents { d i represented by a set of features x i and query q process. Then the training set can be denoted as S = { S is the document sample for query q and pairwise surrogate loss l where I exponential, and logistic functions respectively. 2.2 Document-level Generalization Analysis be defined as below, where P 2 ( z,z 0 ) is the product probability of P ( z ) on the product space Z 2 . 1  X   X  , where (  X , F ,m )  X  0 when document number m  X  X  X  .
 bility at least 1  X   X  , R l other, and pairs are constructed only by documents associat ed with the same query. 2.3 Query-level Generalization Analysis In existing query-level generalization analysis [14], it i s assumed that each query q a deterministic document set S 1  X   X  , where (  X , F ,n )  X  0 as query number n  X  X  X  . ranking algorithms derived in [14] get even looser. behaviors of learning to rank algorithms more accurately th an previous work. 3.1 Two-Layer Sampling in IR manner. First, queries Q = { q distribution P ( q ) . Second, for each query q { ( d i 1 ,y i 1 ) ,  X  X  X  , ( d i m distribution P ( d | q resented by a set of matching features, i.e., x i the notation rules in Section 2.1, we use z i denote the training data for query q i.i.d. samples, random variables { z i same query q in this way as ( Q,S ) , where Q is the query sample and S = { S The two-layer sampling process can be illustrated using Fig ure 1. documents for each query.
 3.2 Two-Layer Generalization Ability ranking as follows, generalization ability, if with probability at least 1  X   X  , where  X  R l 0 iff query number n and document number per query m typical pairwise ranking algorithms. recall the concept of conventional Rademacher averages (RA )[3]. Definition 2. For sample { x E ables independent of data sample.
 two-layer generalization bounds of pairwise ranking algor ithms converge to zero. 2) E [ R m ( l  X F )]  X  D ( l  X F ,m ) , then with probability at least 1  X   X  , for  X  f  X  X  Remark: The condition of the existence of upper bounds for E [ R  X  has been proved that D ( l where c 4.1 Proof of Theorem 1 expected risk as follows: where  X  R l  X  R n ( f )  X  a concept called two-layer RA to describe the complexity of sample ( Q,S ) . where {  X  i document-layer reduced two-layer RA. If ( q,S ) = { q ; z two-layer RA, i.e., E 4.1.1 Query-Layer Error Bounds As for the query-layer error bound, we have the following the orem. Theorem 2. Assume l  X F is bounded by M , then with probability at least 1  X   X  , Proof. We define a function L are i.i.d. sampled, L Since l  X F is bounded by M , by the McDiarmid X  X  inequality, we have G q
Further assuming that there are virtual document samples { z query samples Q and  X  Q , we have L Substitute L 4.1.2 Document-layer Error Bound following theorem by concentration inequality and symmetr ization. Theorem 3. Denote G ( S ) , sup M , then we have: obtained by replacing document z i 0 Then by the McDiarmid X  X  inequality, with probability at lea st 1  X   X  , we have a sum-of-i.i.d. form. Assume S which permutes the m query follow the identical distribution, we have, where p = means identity in distribution. Define a function  X  G ( S
We can see that  X  G ( S using Eqn.(6), we can decompose E Third, we give a bound for E ument sample  X  S  X  Jointly considering (5), (7), and (8), we can prove the theor em. 4.1.3 Combining the Bounds bility at least 1  X   X  , two-layer RA can be bounded by D ( l  X F , b m 4.2 Discussions According to Theorem 1, we can have the following discussion s. obtained in [15]. (2) Only if n  X  X  X  and m query have been used for training. off by solving the following optimization problem: V , for the pairwise 0-1 loss, we have n  X  = c 1 can be used to guide the construction of training set for lear ning to rank. construction of training data.

