 empirical risk  X  L ( h ) = 1 n P n i =1  X  ( h ( x i ) ,y i ) of an i.i.d. sample ( x 1 ,y 1 ) ,..., ( x n ,y n ) . More formally, these are hypothesis classes with finite VC-subgraph dimension [23] (aka pseudo-dimension). For measure of complexity is the VC-subgraph dimension.
 excess risk can then be bounded as (in expectation over the sample): R predictors, R = B 2 sup k X k 2 2 .
 smooth loss functions such as the squared loss with a bounded second, rather then first, derivative. The second deficiency of (1) is the dependence on 1 / rate is possible. In Section 2 we obtain the following bound on the excess risk (up to logarithmic factors):  X  even for the squared-loss, the bound (3) is tight and the non-separable rate of 1 / non-smooth, smooth and strongly convex loss functions are discussed in Section 4 and summarized in Table 1. case of a convex loss function and norm-bounded linear predictors, but Section 3 capture a more general setting of the norm, rather then a measure of concentration like the Radamacher complexity as in (3) and Section 2. However, be convex (in Section 2 we make no assumption about the convexity of hypothesis class H nor the loss function  X  ). discussed above for ERM. Unlike the bound (3) for the ERM, the convex optimization bound avoids polylogarithmic optimization problems beyond classification, including problems for which ERM is not applicable (see, e.g., [24]). to the paper. Recall that the Rademacher complexity of H for any n  X  N given by [2]: Throughout we shall consider the  X  X orst case X  Rademacher complexity.
 (we always take derivatives w.r.t. the first argument). What type of bound can we obtain if we instead bound the derivative can be bounded in terms of the function value: Lemma 2.1. For an H -smooth non-negative function f : R 7 X  R , we have: | f 0 ( t ) | X  p 4 Hf ( t ) derivative. Looking at the dependence of (1) on the derivative bound D , we are guided by the following heuristic  X  (  X  w ,x ) at the ERM  X  w . Applying Lemma 2.1 to L (  X  h ) , we can bound | E [  X  0 (  X  w ,X )] |  X  is where the non-negativity of the loss plays an important role. Ignoring this important issue for the moment and plugging this instead of D into (1) yields L (  X  h )  X  L  X  + 4 (3).
 This rough intuition is captured by the following Theorem: probability at least 1  X   X  over a random sample of size n , for any h  X  X  , and so: where K &lt; 10 5 is a numeric constant derived from [20] and [6].
 Note that only the  X  X onfidence X  terms depended on b = sup |  X  | , and this is typically not the dominant term X  X e and that avoids a direct dependence on sup |  X  | .
 To prove Theorem 1 we use the notion of Local Rademacher Complexity [3], which allows us to focus on the behavior close to the ERM. To this end, consider the following empirically restricted loss class plexity of L  X  ( r ) scales with we use the second, rather then first, derivative, and obtain a bound that depends on the empirical restriction: Lemma 2.2. For a non-negative H -smooth loss  X  bounded by b and any function class H bounded by B : Applying Lemma 2.2, Theorem 1 follows using standard Local Rademacher argument [3]. 2.1 Related Results Rates faster than 1 / The Finite Dimensional Case : Lee et al [16] showed faster rates for squared loss, exploiting the strong convexity how in this non-parametric case, smoothness is necessary and plays an important role (see also Table 1). Aggregation : Tsybakov [29] studied learning rates for aggregation, where a predictor is chosen from the convex very low complexity. Specifically to get learning rates better than 1 / linear predictors with bounded norm, have covering numbers that scale as 1 / 2 and so these methods do not imply covering numbers that do not increase with at all, and so actually finite VC-subgraph-dimension. Chesneau et al caveats hold: even when L  X  = 0 , rates faster when 1 / Rademacher complexity (equivalently covering numbers) can be controlled. Although it uses some similar techniques, Local Rademacher Complexities : Bartlett et al [3] developed a general machinery for proving possible fast rates We also use Local Rademacher Complexities in order to obtain fast rates, but do so for general hypothesis classes, loss function and the magnitude of L  X  , but without any further assumptions on the hypothesis classes itself. Non-Lipschitz Loss : Beyond the strong connections between smoothness and fast rates which we highlight, we are (such as the squared loss) solely in terms of the Rademacher complexity. We now turn to online and stochastic convex optimization. In these settings a learner chooses w  X  W , where W is a all w , w 0  X  W scalar smoothness: Lemma 3.1. For an H -smooth non-negative f : W  X  R , for all w  X  W : k X  f ( w ) k  X   X  p 4 Hf ( w ) In order to consider general norms, we will also need to rely on a non-negative regularizer F : W 7 X  R that is a use the squared Euclidean norm regularizer: F ( w ) = 1 2 k w k 2 . 3.1 Online Optimization Setting In the online convex optimization setting we consider an n round game played between a learner and an adversary choice w i may only depend on the adversary X  X  choices in previous rounds. The goal of the player is to have low average objective value 1 n P n i =1 ` ( w i ,z i ) compared to the best single choice in hind sight [9]. according to z i and a stepsize  X  (to be discussed later) as follows: For the Euclidean norm with F ( w ) = 1 2 k w k 2 , the update (5) becomes projected online gradient descent [32]: w i +1  X   X  W ( w i  X   X   X  ` ( w i ,z i )) where  X  W ( w ) = arg min w 0  X  W k w  X  w 0 k is the projection onto W . Theorem 2. For any B  X  R and L  X  if we use stepsize  X  = 1 Note that the stepsize depends on the bound L  X  on the loss in hindsight. The above theorem can be proved using Lemma 3.1 and Theorem 1 of [26]. 3.2 Stochastic Optimization learning risk discussed in the Introduction and analyzed in Section 2. But instead of focusing on the ERM, we run regret bound of Theorem 2 to a bound on the excess risk: Corollary 3. For any B  X  R and L  X  , if we run Mirror Descent on the sample with  X  = 1 for any w  X   X  W with F ( w  X  )  X  B 2 and L ( w  X  )  X  L  X  , with expectation over the sample: slow O (  X / is small because of the self bounding property (Lemma 3.1).
 The following theorem provides a bound on excess risk similar to Corollary 3: Theorem 4. For any B  X  R and L  X  if we set  X  = 128 H n + and L ( w ? )  X  L  X  , we have that in expectation over sample of size n : here it is necessary to look at stability in expectation to get the faster rates. between the parametric and scale-sensitive cases and between the smooth and non-smooth cases, and argue that these learning guarantees for ERM in the stochastic setting, similar arguments can also be made for online learning. Table 1 summarizes the bounds on the excess risk of the ERM implied by Theorem 1 as well previous bounds for Lips-1 /  X  ` cific distributions over X X Y , where X = x  X  R d : k x k X  1 and Y = [0 , 1] . For the non-parametric lower-bounds, we will allow the dimensionality d to grow with the sample size n .
 Infinite dimensional, Lipschitz (non-smooth), separable is uniformly distributed over the d standard basis vectors e i and if X = e i , then Y = 1  X  an arbitrary sequence of signs unknown to the learner. Taking w ? = 1  X  Infinite dimensional, smooth, non-separable, even if strongly convex d = Y | ( X = e i )  X  X  ( r i of the expected risk is w ? = P d i =1 r i If the norm constraint becomes tight, i.e. k  X  w k = 1 , then L (  X  w )  X  L ( w ? )  X  1 / (4 d ) =  X / (4 Finite dimensional, smooth, not strongly convex, non-separable: Take d = 1 , with X = 1 with probability q and X = 0 with probability 1  X  q . Conditioned on X = 0 let Y = 0 deterministically and while conditioned on X = 1 let Y = +1 with probability p = 1 2 + 0 . 2  X  qn and Y =  X  1 with probability 1  X  p . Consider the following 1 -smooth loss : 5.1 Improved Margin Bounds  X  X argin bounds X  provide a bound on the expected zero-one loss of a classifiers based on the margin 0 / 1 error on the training sample. Koltchinskii and Panchenko [13] provides margin bounds for a generic class H based on the Rademacher complexity of the class. This is done by using a non-smooth Lipschitz  X  X amp X  loss that upper bounds the zero-one loss and is upper-bounded by the margin zero-one loss. However, such an analysis unavoidably leads to a 1 /  X  ( x 1 ,y 1 ) ,..., ( x n ,y n )  X  X   X { X  1 } define the  X  -margin empirical zero one loss as c err  X  ( h ) := 1 n P for all margins  X  &gt; 0 and all h  X  X  : where K is a numeric constant from Theorem 1.
 In particular, for appropriate numeric constant K : space based on the PAC Bayes theorem [19, 15]. However PAC-Bayes based results are specific to certain linear 5.2 Interaction of Norm and Dimension regularization. What determines the sample complexity? How does the error decrease as the sample size increases? error, should depend on the norm B , especially if d B 2 . However, for any fixed d and B , even if d B 2 , The asymptotic dependence on the dimensionality alone can be understood through Table 1. In this non-separable situation, parametric complexity controls can lead to a 1 /n rate, ultimately dominating the 1 / L  X  &gt; 0 when considering the scale-sensitive, non-parametric complexity control B . Combining Theorem 4 with the overall picture on the expected excess risk of ridge regression with an optimally chosen  X  : regime, where the excess risk is controlled by the norm and the approximation error and behaves as B X / d/n . This sheds further light on recent work by Liang and Srebro [18] based on exact asymptotics. 5.3 Sparse Prediction The use of the ` 1 norm has become popular for learning sparse predictors in high dimensions, as in the LASSO. The Thus, Theorem 1 along with Rademacher complexity bounds from [11] gives us, w small number of fairly uncorrelated features.
 Bounds similar to (7) have been derived using specialized arguments [12, 30, 5] X  X ere we demonstrate that bounds of note that the methods and results of Section 3 can also be applied to this setting. We use the entropy regularizer each feature X  X  negation). Recalling that w 0 1  X  2 Theorem 4 instead of Theorem 1 avoids the extra logarithmic factors.
