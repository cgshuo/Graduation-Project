 The traditional named entity task can provide semantic information of key en-tities, and is taken as a first step by any information extraction and question answering system. However, classifying entity names only into persons, loca-tions and organizations (PLO) is not sufficient to answer a question  X  X hat is the oldest city in the United States? X  in the TREC-10 QA task [1]. Although the answer type of the question can be determined as CITY, if location entities are not classified into their specific sub-types, such as CITY, STATE, COUNTRY, etc., it is impossible to filter out  X  X lorida X  , a state, from the text  X ... Florida is the oldest one in the United States ... X  By sub-categorizing location entities we can effectively reduce the candidate answer space of the question and help to point out the exact answer.
 graphic named entities can be classified into many sub-types that are critical for applications such as information extraction and question answering. As a first step, we define their seven sub-types: COUNTRY, STATE, COUNTY, CITY, MOUNTAIN, RIVER, and ISLAND. We attempt to identify and classify all instances of the seven types in plain text.
 named entities are frequently shared with person names as well as between their sub-types. For example,  X  X ashington X  may indicate a person in one context but may also mean a city or state in another context. Even country names cannot be exceptions. For some Americans,  X  X hina X  and  X  X anada X  may be cities where they live. Geographic named entities such as  X  X an X  and  X  X enter X  can also be shared with common nouns. Contextual similarity among geographic named entities is much higher than between PLO entities since they are much closer semantically. These make geographic named entity annotation task more difficult than the traditional named entity task.
 area and some supervised learning approaches have achieved comparable or better performance than handcrafted rule-based approaches, especially in the named entity task [2]. However, Manual corpus annotation is tedious and time-consuming work and requires considerable human effort. Domain shift may ren-der the existing annotated corpus useless even if it is available. On the other hand, there are many raw text corpora available for training and geographic entities for a gazetteer are also available in several web sites. These lead us to apply a bootstrapping approach using gazetteer entries as seeds. Gazetteer en-tries can be found in several web sites, such as the U.S. Census Bureau, 1 the Consortium for Lexical Research (CRL), 2 TimeAndDate, 3 Probert Encyclopae-dia -Gazetteer, 4 The World of Islands 5 and Island Directory, 6 and we gathered entities with their types to use both as seeds and for confirming types of candi-dates to be annotated in the bootstrapping process.
 annotate a raw corpus using seeds. From the initial annotation, boundary patterns are learned and applied to the corpus again to annotate new candidates of each type. Type verification is adopted to reduce over-generation, and one sense per discourse principle [3] expands the annotated instances in each document. As the bootstrapping loop continues, the annotated instances are increased gradually and also the learned boundary patterns become richer little by little.
 related works to our approach. Our bootstrapping approach is described in Sec-tion 3 and Section 4 gives the experimental results. Section 5 contains our re-marks and future works. Research on analysis of geographic references recently started to appear and focused on only classifying geographic entity instances in text [4, 5]. Li et al. [4] suggested a hybrid approach to classify geographic entities already identified as location by an existing named entity tagger. They first matched local context patterns and then used a maximum spanning tree search for discourse analysis. They also applied a default sense heuristic as well as one sense per discourse principle. According to their experiments, the default sense heuristic showed the highest contribution.
 named entity recognition. These approaches relate to bootstrapping learning al-gorithms using natural redundancy in the data, with the help of the initial seeds. Collins and Singer [6] applied the co-training algorithm to named entity classifi-cation. They used 7 simple seed rules to extract lexical and contextual features of named entities as two competing evidences, and learned the capitalized names from the fully parsed corpus. They only focused on named entity classification by assuming consecutive proper nouns as named entity candidates. Yangarber et al. [7] presented a bootstrapping technique to learn generalized names such as disease names that do not provide capitalization any longer. This makes the identification of such names more difficult. They used manually selected seeds, and boundary patterns and entity names acted as competing evidences in the bootstrapping process. The patterns for beginning and ending boundaries are independently learned from currently annotated names and applied to the raw corpus in order to obtain new candidate names. This bootstrapping process is quite similar to our method, but annotation targets differ between each ap-proach. We focus on the fine-grained sub-types of geographic entities whereas they deal with generalized names as well as locations. We also represent bound-arypatternswithcharacterpatternsandsemanticinformationaswellaslexical words, while they only use literal patterns with wildcard generalization. Their boundary patterns are used for identifying only one (left or right) boundary and the other is identified by a simple noun group matching [ADJ* N+] .Butthere are many instances that go beyond the boundary of the simple noun group. For example,  X  X ntigua and Barbuda X  and  X  X osnia and Herzegovina X  have a con-junction but indicate unique country names. Therefore, we do not restrict the entity boundary to a simple noun group. Our bootstrapping approach is based on two competing evidences: entity instances and their boundary patterns, as in other bootstrapping approaches [6, 7, 8]. in Figure 1. In the initial stage, we annotate a raw corpus with the seeds auto-matically obtained from the gazetteer. Starting and ending boundary patterns are learned from the annotation and applied to the corpus again to obtain new candidates of each type. Then we perform type verification of each candidate entity using the gazetteer and scores of boundary patterns applied. Finally, one sense per discourse principle [3] propagates the annotated instances to the en-tire document, which makes the boundary patterns much richer in the next loop. The principle also helps to remove erroneous annotated instances. As the boot-strapping loop proceeds, the annotated instances are increased gradually and the learned boundary patterns also become richer little by little. These processes are explained in detail in the following subsections. 3.1 Initial Stage Seed entities of each type can be obtained from the gazetteer. The gazetteer includes many ambiguous entities that should not be used as seeds. Ambiguous seeds can act as noise in the initial annotation and such noise can propagate to the next iterations. As a result, they diminish the final performance. Therefore, we automatically select only unambiguous entities by consulting the gazetteer as well as a part-of-speech dictionary. 7 Seed annotation also requires careful at-tention. Not all occurrences of unambiguous seeds in a corpus can be annotated as their types. For example,  X  X .S. X  in  X  X .S. Navy X  should not be annotated as COUNTRY since it is part of an organization name. For this reason, we anno-tate only occurrences having no proper nouns in their immediate left and right contexts. These efforts make it possible to start bootstrapping iteration with high precision (99%) and moderate recall (28%) on our test data. 3.2 Boundary Patterns Definition. For each annotated instance, we define two boundary patterns: starting ( bp s ) and ending ( bp e ) independently, i.e., where w  X  0 are the first and the last words of an annotated instance, respec-tively, and w  X  1 and w  X  2 are the left and the right contexts of an annotated instance. The context is determined by a rule-based noun phrase chunking we implemented and can be grouped into two: intra-phrase and inter-phrase con-texts. Intra-phrase context indicates a modifier or modifiee of the entity, within a base noun phrase containing the entity, and inter-phrase context means a mod-ifier or modifiee of the entity, outside the base noun phrase. These can make boundary patterns richer than simple adjacent word contexts do because they reflect syntactic constraints. f ( w ) means one of following four features: character pattern (marked with &amp; ), such as capitalization and digits, word forms (marked with % ), semantic category (marked with @ ) based on the WordNet hierarchy, 8 and a gazetteer category (marked with $ ) based on the gazetteer. These features are similar to internal sub-features of Zhou and Su [2]. Figure 2 shows some possible starting and ending boundary patterns of entities,  X  X os Angeles X  and  X  X aine X  . [%northern %maine] shows intra-phrase context and others is inter-phrase contexts. The left and the right contexts have windows of maximum 2 but it is not a strict word count in the case of semantic and gazetteer category features. Multi-words like  X  X an Diego X  and  X  X ice president X  count only one and can be abstracted with their gazetteer or semantic categories, i.e., $ city gaz and @person1 , respectively.
 Learning. All possible boundary patterns are generated from the corpus par-tially annotated in the initial stage or in previous iterations. Then, we score the generated patterns by checking if each pattern correctly bounds the previously annotated instances. We count the followings that are similar to Yangarber et al. [7]:  X  pos ( bp ): the number of matched instances that are already annotated as the  X  n eg ( bp ): the number of matched instances that are already annotated as a  X  u n k ( bp ): the number of matched instances that are not annotated yet. Then, the score of a boundary pattern, Score ( bp ), is computed as follows: Application. To obtain new entity candidates, we apply the learned patterns to the entire corpus. Since each pattern determines only one  X  i.e., starting or ending  X  boundary, a candidate is identified by a pair of starting and ending patterns of the same entity type. We limit the length of each candidate to six words since a geographic entity is usually composed of a fairly small number of words. However, the entity boundary is not restricted to a simple noun chunk to allow entities such as  X  X osnia and Herzegovina X  and  X  X he United States of America X  .
 boundary patterns used for identifying it. Generally, both the left and the right contexts can provide clues for the entity type, but it is not common for both contexts to provide reliable clues at the same time and usually only one of them supports the clue. This leads us to use a pair of thresholds, top threshold (  X  t ) and bottom threshold (  X  b ), to select only promising candidates in each iteration. At least one of the starting and ending scores of each candidate should be higher than  X  t and both should be higher than  X  b . 3.3 Type Verification We do not limit the entity boundary to the base noun phrase chunk in order to cover geographic entities bounded by complex noun phrases such as  X  X osnia and Herzegovina X  and  X  X he United States of America X  . However, this can also cause erroneous candidates such as  X  X oston and Chicago X  to be generated if the left context of  X  X oston X  provides a reliable starting boundary score and the right context of  X  X hicago X  provides a reliable ending boundary score. Even if we have such a restriction, erroneous candidates can also be extracted by less reliable boundary patterns. They act as noise in the next iterations and less reliable boundary patterns are increased gradually as the iteration proceeds. As a result, this may cause what is called garbage in and garbage out . To prevent erroneous candidates from being annotated, we employ type verification of each candidate if it was identified by less reliable patterns (i.e., Score ( bp ) &lt; X  v ). If we know that the identified candidate is really of its classified entity type, we can be convinced that at least it can be of its entity type in some contexts although it was identified and classified by less reliable boundary patterns. To verify the type of an entity candidate, we first analyze its prefix and suffix and then consult the gazetteer. We add or remove prefixes or suffixes such as Republic, State, Province, County,City,Cityof,River,Mt.,Island , etc., according to its entity type before searching the gazetteer, since the gazetteer does not have all possible forms of each entity. As a result, candidates are removed if they are less reliable and also not verified by the gazetteer. 3.4 Expanding and Correcting the Annotation It is common to use the heuristic such as one sense per discourse in the word sense disambiguation research [3]. In our research, we set a document as a discourse because we use newspaper articles as a training and test corpus which is described in Section 4. Each article is short enough to be regarded as one discourse. This heuristic is very helpful to expand the current annotation to the entire discourse to obtain a new annotation or to correct a mistaken annotation [9, 10]. That is, it increases positive instances as well as decreases spurious instances effectively. Type Consistency. Error correction is performed when the same entity can-didates from a document have different entity types. By the heuristic, they are all annotated with the type of entity having the most reliable boundary score if the difference between the two boundary scores is larger than a threshold. Otherwise, they are all ignored and their annotation is delayed until the next iterations.
 Location vs. Person. Location names, i.e., geographic named entities, are highly shared with a person X  X  names. This is because it is common to name a location after a famous person X  X  name, such as  X  X ashington X  .Thismakesit important to disambiguate locations from a person X  X  names. According to the heuristic, a location candidate can be filtered out if we know that one of its instances within a document indicates a person X  X  name. This is another error correction.
 name, 9 we first check if it has a title word, such as Mr. , Sen. and President . Second, we check typical patterns, such as  X   X  PERSON says  X ,  X  , PERSON says  X ,  X   X  says PERSON  X ,  X  , says PERSON  X ,  X  PERSON who  X , etc. Finally, we consult the gazetteer to see if it and its adjacent proper noun are all registered as a person name. For example,  X  X eorge Washington X  , an occurrence of a candidate  X  X ashington X  , filters out the candidate, since both  X  X eorge X  and  X  X ashington X  are used as a person name, based on the gazetteer.
 Location vs. part of Organization. Another issue is how to distinguish the location from part of the organization name. This is required when propagating the annotation of an entity to other occurrences within the document by the heuristic. For example, when  X  X .S. X  is annotated as COUNTRY in a document, it should not propagate to  X  X .S. Navy X  in the same document since it is a part of the organization name, whereas it should propagate to  X  X .S. President X  .One possible solution is to check the existence of an acronym for the suspected phrase, since it is common to use an acronym to represent a unique entity.  X  X .S. Navy X  is represented as the acronym  X  X SN X  but  X  X .S. President X  is not. To check the existence of their acronyms, we consult Web search engines by querying  X  X .S. Navy (USN) X  and  X  X .S. President (USP) X  , respectively, and check if there ex-ists more than one result retrieved. The following shows some acronym exam-ples that can be found by Web search engines and therefore whose typewritten sub-phrases should not be annotated:  X  United States -China Policy Founda-tion (USCPF) , X   X  New York Times (NYT)  X  X nd X  University of California , Los Angeles (UCLA) . X  a candidate can be modified by a prepositional phrase which is derived by in or comma (,) plus the candidate. For example, we can decide that  X  X eijing X  in  X  X eijing University X  is a part of the organization name, since the phrase  X  X eijing University in Beijing X  is found by Web search engines. If  X  X eijing X  in  X  X eijing University X  denotes CITY,  X  X eijing University X  indicates any university in Bei-jing and is not modified by the prepositional phrase duplicately. The following shows some other examples whose typewritten sub-phrases should not be anno-tated:  X  Shanghai No. 3 Girls X  School, Shanghai , X   X  Boston English High School in Boston  X  X nd X  Atlanta Brewing Co. in Atlanta . X  3.5 Iteration In each iteration, we control two thresholds,  X  t and  X  b , described in Subsec-tion 3.2, of boundary scores to extract new entity candidates. We set both thresholds high to maintain high precision in the early stages and decrease them slightly to increase recall as the iteration proceeds. We decrease  X  b at first and  X  t at next, i.e., (  X  t , X  b )=(0 . 90 , 0 . 90) (0 . 87 , 0 . 15)  X   X  X  X   X  (0 . 51 , 0 . 15). This reflects our observation that the most reliable one  X  left or right  X  context is preferred to moderately reliable two  X  X eft and right  X  contexts. The bootstrapping stops when the two thresholds arrive at (0.51,0.15) and there are no more new boundary patterns learned. The algorithm was developed and tested using part of New York Times arti-cles (June and July, 1998; 116MB; 21,000 articles) from the AQUAINT corpus. We manually annotated 107 articles among them for test and the counts of annotated instances were listed in Table 1. Others were used for training. We collected 78,000 gazetteer entries from several Web sites mentioned in Section 1. This includes non-target entities (e.g., CONTINENT, OCEAN, PERSON and ORGANIZATION) as well as various aliases of entity names. As a baseline, we could achieve high recall (94%) but poor precision (60%) by applying only the gazetteer without bootstrapping. Seeds were automatically selected from the gazetteer and their counts are also listed in Table 1.
 tigated the change in performance as the iteration proceeds. In each iteration, we applied the learned boundary patterns to the test corpus and measured re-call and precision using the standard MUC named entity scoring scheme. The learning curve is shown in Figure 3, marked with NYT06+NYT07 . The figure says that recall increases (28% to 84%) while precision decreases slightly (99% to 87%). However, after approaching to the maximum recall, precision dropped steeply. The maximum value of F1 was 86.27% when recall and precision were 83% and 90%, respectively. This performance is comparable to that of Lin et al. [11] although we distinguished fine-grained types of locations whereas they did not. It is also comparable to similar approaches in the traditional named entity task, e.g. [6], 10 considering that they only attacked named entity classifi-cation and that geographic named entity annotation is much more difficult than PLO-based annotation.
 formed bootstrapping on the half of the June corpus ( NYT06*1/2 ) and on the June corpus ( NYT06 ). The learning curves are also shown in Figure 3. The com-parison says that the large training corpus can maintain higher precision than the small training corpus until recall approaches the maximum. This is because the large training corpus has more instances to be learned and provides much richer boundary patterns than the small training corpus.
 to the performance. We performed bootstrapping on the training corpus us-ing boundary patterns represented with only lexical and character patterns. As shown in Figure 4, semantic and gazetteer features could maintain precision a little higher although any techniques for word sense disambiguation were not em-ployed. The effect of pattern generalization seemed to suppress noise caused by word sense ambiguity. Figure 5 explains how well the intra-/ inter-phrase context works, compared to just adjacent word context. the effect of the verification. The result is shown in Figure 6. The figure says that if we do not adopt the type verification, the erroneous candidates are not filtered out and drop precision drastically even in early iterations. In this paper, we present a bootstrapping algorithm for the task of geographic named entity annotation including both identification and classification. The algorithm bootstraps from a raw corpus and seed entities, learns starting and ending boundary patterns independently, and applies them to the corpus again to obtain new candidates. To reduce over-generation, each candidate is verified based on its boundary scores and the gazetteer. One sense per discourse princi-ple [3] expands the annotated instances and also corrects mistaken annotations. As the bootstrapping loop continues, the annotated instances are increased grad-ually and the learned boundary patterns become gradually richer. When training and testing on NYT newspaper articles, our approach achieved 86.27% F1 with 83% recall and 90% precision.
 into a statistical model and include other geographic entity types as well as PERSON and ORGANIZATION. Competing entity types may help to improve overall performance, as described in Lin et al. [11]. Co-occurrence information with the default sense heuristic suggested by Li et al. [4] will also be tried for disambiguating geographic named entities effectively.
 This work was supported by 21C Frontier Project on Human-Robot Interface (by MOST).

