 Dimensionality reduction is an ubiquitous and impor-tant form of data analysis. Recovering the inherent manifold structure of data X  X .e. the local directions of large versus minimal variation X  X nables useful rep-resentations based on encoding highly varying direc-tions. Not only can this reveal important structure in data, and hence support visualization, it also pro-vides an automated form of noise removal and data normalization that can aide subsequent data analysis. Although linear dimensionality reduction is a well studied topic, recent progress continues to be made with the investigation of convex regularizers, such as the trace norm or 2,1-norm, that enable application to general losses beyond squared error (Candes et al., 2011; Xu et al., 2010; Srebro &amp; Shraibman, 2005). The literature on nonlinear dimensionality reduction, by comparison, has grown more rapidly yet devoted rel-atively less attention to developing appropriate regu-larizers. This provides one of our main motivations. The focus of this paper is on unsupervised dimension-ality reduction; that is, we will not directly address su-pervised variants, e.g. (Weinberger &amp; Saul, 2009). We will also focus on non-parametric formulations that do not require an explicit map connecting the low and high dimensional representations; e.g. as in re-stricted Boltzmann machines (Larochelle &amp; Bengio, 2008), auto-encoders (Rifai et al., 2011), or parame-terized kernel reductions (Wang et al., 2010). The primary benefit of focusing on unsupervised, non-parametric formulations is that it allows a simple yet comprehensive overview of current methods. In partic-ular, we observe that nearly all current non-parametric methods can be expressed as regularized loss mini-mization of a reconstruction matrix followed by sin-gular value truncation. This perspective allows us to distinguish the role of the loss from that of the reg-ularizer : the loss relates the learned reconstruction to the data, whereas the regularizer relates the recon-struction to the desired topology independent of the data. Such a separation allows a simpler organization of current methods than current overviews (Burges, 2010; Lee &amp; Verleysen, 2010a). More importantly, it reveals new research directions. A brief overview of current losses, for example, reveals a useful alter-native that remains uninvestigated. Similarly, an as-sessment of current regularizers reveals that very few have been explored: in fact, only one family of con-vex regularizers has been widely used in the nonlinear case (distance maximization), which has known weak-nesses. Although non-convex regularizers have been proposed to mitigate these weaknesses, these introduce intractability. Our main contribution is to derive effi-cient new convex regularizers that are able to combine distance maximization with rank reduction. Below we will need to manipulate and relate data , ker-nel , and Euclidean distance matrices respectively. As-sume one is given t observations, either expressed as a t  X  n data matrix X ; a t  X  t kernel matrix K where K = K 0 and K &lt; 0; or a t  X  t squared Euclidean dis-tance matrix D where D = D 0 , D  X  0,  X  ( D ) = 0 and HDH 4 0, such that  X  denotes diagonal and H = I  X  1 t 11 0 denotes the centering matrix. Then we can map between these various matrices via (where the function intended is determined by the ar-gument). Note that it is easy to map a data matrix to its corresponding kernel or Euclidean distance matrix, but such a map is neither linear nor invertible: kernel matrices drop orientation, while Euclidean distance matrices drop orientation and translation. However, by centering the data or kernel matrix, thus removing translation information, the relationship between ker-nel and Euclidean distance matrices becomes simple. Proposition 1 A linear bijection exists between cen-tered kernel and squared Euclidean distance matrices. It is easy to verify that D ( K ( D )) = D and K ( D ( K )) = HKH for a valid Euclidean distance and kernel ma-trix respectively. Therefore if f ( D ) is convex in D then f ( D ( K )) must be convex in K . Proposition 1 thus al-lows one to equivalently re-express problems in terms of centered kernel matrices or Euclidean distance ma-trices without affecting expressiveness or convexity. In this paper we assume the target dimensionality d is fixed beforehand. That is, we are not addressing the problem of estimating the intrinsic dimensionality; for a survey see (Lee &amp; Verleysen, 2010a, Ch.3). We will also need to make use of the indicator function First briefly consider linear dimensionality reduction, which illustrates some basic points. Here one is given a data matrix X and seeks a reduced rank representation  X  X . It turns out that a simple, generic strategy covers almost all methods that have been proposed: First, solve the regularized loss minimization problem for a given loss L and regularizer R , obtaining the reconstruction  X  X . Then recover the low rank repre-sentation  X  X by truncating all but the top d singular  X  U  X 
 X   X  V 0 =  X  X is the singular value decomposition. Note that the loss relates  X  X to the data, X , whereas the reg-ularizer enforces assumptions on the reconstruction  X  X that are independent of the data. Interestingly, it is the regularizer , not the loss, that typically determines the computational difficulty of this problem.
 Regularizers: The role of the regularizer is to en-courage the desired topology. To illustrate, consider the common regularizers proposed for the linear case where  X   X  0 is a regularization parameter. All of these clearly encode a desire for reduced rank.
 In particular, the rank indicator (7) is the stan-dard regularizer for spectral dimensionality reduction, which eliminates the need for truncation. Unfortu-nately rank is not convex, and enforcing (7) is only known to be tractable for squared loss (10) (Jolliffe, 1986). For other losses, such as absolute error (11) or Bregman divergence (12), rank is normally enforced by means of alternating descent in a factored representa-tion: min AB L ( AB ; X ) where A and B are t  X  d and d  X  n respectively (Collins et al., 2001; Gordon, 2002). Unfortunately, this cannot guarantee optimality. 1 The difficulty of working with rank explains the emer-gence of convex, rank-reducing regularizers such as the trace norm (8) (Candes et al., 2011; Srebro &amp; Shraib-man, 2005) and block norm (9) (Xu et al., 2010). In fact, the trace norm is known to be the tightest con-vex approximation to rank. 2 These regularizers allow a tractable formulation for general convex losses, and also allow a desired rank to be enforced by appropri-ately choosing  X  (Cai et al., 2008).
 Loss Functions: Despite the importance of regular-ization, it is interesting to observe that until recently much of the research effort in linear dimensionality re-duction investigated alternative losses, including
L
L
L B Here k X k F denotes Frobenius norm, k X k 1 , 1 denotes 1,1 block norm, and (13) is a Bregman divergence as-sociated with strictly convex potential F . Beyond the squared error (10) used for PCA (Jolliffe, 1986), the absolute loss (11) has been recently proposed for ro-bust PCA (Candes et al., 2011; Xu et al., 2010), and Bregman divergences (12) have been implicitly pro-posed in exponential family PCA (Collins et al., 2001; Gordon, 2002). 3 Interestingly, all these standard loss functions are convex in  X  X .
 The conclusion we draw from the linear case that the loss functions considered are standard, convex, and not the source of computational difficulty. Instead, it is regularization that has posed the greatest difficulty, particularly the desire to reduce rank. Interestingly, for the non-parametric case we find the same holds: almost every method follows regularized loss minimiza-tion plus truncation, almost every loss adopted is stan-dard (if expressed between Euclidean distance matri-ces), and the main difficulty lies in devising regular-izers that encourage desired topology. Such a simple perspective is surprisingly not widely appreciated. A general non-parametric approach to dimensionality reduction can be obtained by expressing the problem in terms of kernel matrices. (Recall that data or Eu-clidean distance matrices can always be converted to kernel matrices.) Here we assume one is given a t  X  t kernel matrix K determined by the data and seeks a reconstruction  X  K that allows a reduced rank repre-sentation. Here too it turns out that a simple, generic strategy covers almost all methods that have been pro-posed: First, solve the regularized loss minimization for a given loss L and regularizer R , obtaining the re-construction  X  K . Then recover the low rank representa-tion  X  X by truncating all but the top d eigenvalues and is the eigenvalue decomposition of  X  K . (The reason for stating the loss in (14) in terms of D (  X 
K ) will be explained below. Note that if the loss function L is convex in its first argument it must also be convex in  X  K by Proposition 1.) Although the difference between the linear and non-parametric formulations does not appear large, (14) offers far greater flexibility in recovering alternative topological structures, such as nonlinear manifolds in the original data. In fact, a key intuition behind most non-parametric dimensionality reduction methods is unfolding , where one supposes the data lay on a low di-mensional curved submanifold that is to be  X  X nfolded X  into a linear subspace. Unfortunately, current propos-als conflate the role of the loss and regularizer in such a process, and propose only specific combinations of the two. We find it revealing to consider them separately. 4.1. Regularizers The natural role for regularization in dimensionality reduction is to relate the reconstruction  X  K to a desired topology, expressing prior assumptions about the na-ture of the target representation independent of the data. For example, one might seek a coordinate repre-sentation in a linear subspace, in which case it is desir-able to encourage  X  X lattening X  by spreading distances. However, regularization can express other topologies (see below). As before, the computational challenges appear to be primarily dictated by the regularizer. The most common target topology is a linear subspace, for which the most common regularizers considered are where  X   X  0 and  X   X  0 are regularization parameters. For example, the rank indicator (15) is the most com-monly used regularizer, often associated with classical spectral dimensionality reduction (kernel PCA) using squared error (22) (Schoelkopf et al., 1999; Ham et al., 2004). Unfortunately, rank is not convex, and efficient training procedures are not known for other losses. In-stead, for other losses, rank is typically enforced by resorting to local minimization in a factored represen-sentation matrix. Unfortunately, no current loss pro-vides a convex formulation in  X  X , and optimal solutions usually cannot be guaranteed.
 Consequently, convex regularizers have also played a prominent role in non-parametric dimensionality re-duction. For example, applying the trace norm here yields (16). Unfortunately, tr(  X  K ) = 1 0 D (  X  for centered  X  K , thus minimizing trace is equivalent to shrinking distances; in opposition to the desire to un-fold manifolds. Consequently, the negated regularizer (17) has proved more effective, forming one of the key components of maximum variance unfolding (MVU) (28) (Weinberger et al., 2004; 2007). Neither of these regularizers is completely satisfactory however. Partitioned regularizer: It is obvious what one would desire: to spread distances in the top d dimen-sions and shrink distances in the remaining dimen-sions, for the target dimensionality d . Such a regu-larizer has been proposed by (Shaw &amp; Jebara, 2007): R Unfortunately, (19) is not convex; (Shaw &amp; Jebara, 2007) resort to alternating minimization between  X  K and P . Below we provide new convex regularizers that approximate (19) , providing our main contribution. Topographic Methods: As an aside, it is interest-ing to note that alternative topologies can be encour-aged via regularization. For example, given a graph expressed as an adjacency matrix G  X  X  0 , 1 } t  X  t , a reg-ularizer can encourage  X  K to adopt G  X  X  structure
R which provides a generalized approach to topographic embedding (Quadrianto et al., 2010; Bishop et al., 1998). Unfortunately, (21) is concave in  X  K and the inner optimization over M is an NP-hard quadratic assignment problem, so we do not pursue this further. 4.2. Loss Functions The role of the loss function is to relate the recon-struction to the data . In the formulation (14) we have chosen to express losses as between Euclidean distance matrices . We will see that such a viewpoint, although nonstandard, provides clarity. For example, unfolding is naturally enabled by loss locality : errors in recon-structing small target distances should be punished more harshly than errors in larger distances. Loss locality allows larger distances in the reconstruction more leeway to adapt to the desired target topology. Expressing current losses in terms of distances reveals that they are almost all standard, convex, and obvious. We briefly survey standard losses to demonstrate how broadly the perspective applies to current methods, and to highlight useful alternatives.
 Classical Losses: The oldest losses used for dimen-sionality reduction do not express locality, and there-fore tend to recover linear subspace representations: Here all convex in  X  D (Dattorro, 2012, Ch.7). The dou-bly centered squared loss (22) is used in kernel PCA (Schoelkopf et al., 1999) (recall HDH =  X  2 K ( D )). Although (23) is frequently mentioned in the multidi-mensional scaling (MDS) literature (Cox &amp; Cox, 2001), its use is rare since it cannot be tractably combined with rank (15) (Dattorro, 2012). The absolute loss (25) in an important alternative that has been used in robust MDS (Cayton &amp; Dasgupta, 2006).
 Local Losses: Unlike the classical losses, however, local losses encourage unfolding by de-emphasizing er-rors on large distances. Let N ( D )  X  { 0 , 1 } t  X  t denote an adjacency function, such that N ( D ) ij = 1 indicates that i and j are neighbors in D (i.e. either within a distance of or among the k nearest neighbors). The best known examples of local losses are L (  X  D ; D ) = P ij ( D ij  X   X  D ij ) 2 /D ij (26) L (  X  D ; D ) = P ij ( D ij  X   X  D ij ) 2 w (  X  D ij ) (27) L (  X  D ; D ) = P ij [[ N ( D ) ij = 1 and  X  D ij 6 = D ij L (  X  D ; D ) = P ij N ( D ) ij ( D ij  X   X  D ij ) 2 . (29) These are the Sammon loss (26) (Sun et al., 2011; Lee &amp; Verleysen, 2010a); the curvilinear components loss (27) (Sun et al., 2010); the neighborhood indica-tor used in MVU and Isomap (28) (Weinberger et al., 2004; Tenenbaum et al., 2000); and the relaxed loss in-troduced in regularized MVU (29) (Weinberger et al., 2007), respectively. All such losses emphasize errors on small target distances (or predictions) over errors on large target distances. Moreover, they are all con-vex in  X  D . 4 Although the convexity of these losses with respect to  X  D has not always received significant notice in the literature, this is an important fact for (14). Bregman Divergences: Bregman divergences pro-vide another loss specification that emphasizes local-ity. Recall that a Bregman divergence is defined by B a strictly convex differentiable potential F , which by construction must be convex in  X  D . A number of such divergences have proved to be important in the dimen-sionality reduction literature, including B B B
B F 12 (  X  D k D ) = tr( p ( D ) 0 (log p ( D )  X  log p ( where  X  &gt; 0 is a scale parameter, log and exp are applied component-wise, and it is assumed the diver-gences are summed over all ij or all i where necessary. The unnormalized entropy (30) was proposed in (Sun et al., 2011) to approximate the Sammon loss (26), whereas the reciprocal exponential Bregman diver-gence (31) was proposed in (Sun et al., 2010) to ap-proximate the curvilinear components loss (27) under w (  X  d ) = exp(  X  d/ X  ). However, the latter approximation was achieve by placing  X  D in the second position, using ` (  X 
D ij ; D ij ) = B F ( D ij k  X  D ij ), which is no longer convex (see below). The Bregman divergence (32) matches the loss used in SNE (van der Maaten &amp; Hinton, 2008) up to a minor variation: the transfer p in SNE does not normalize over the entire vector, but only over the vector minus the current entry. The later, symmet-ric SNE error can also be recovered (almost) from the matrix-wise Bregman divergence (34) up to the same minor variation, plus a second exception: even though p (  X 
D ) is computed as in (35), p ( D ) is computed by av-eraging the column and row probabilities through ij using (33).
 Surprisingly, the exponential divergence (31) has not previously been used with  X  D in the first, convex po-sition. This yields a highly localized loss that is well suited to manifold unfolding, demonstrating even stronger locality than the other divergences. Therefore, we investigate its behavior further below.
 Large Margin Losses: Large margin losses for non-linear dimensionality reduction have also been pro-posed in (Shaw &amp; Jebara, 2009): L L where ` is a local margin loss function. The intuition behind the loss (36) is simple: for each node i , one would like the distances to all disconnected nodes j (such that N ( D ) ij = 0) to be greater than the dis-tance to the furthest connected node k (i.e. such that N ( D ) ik = 1). The second loss (37) is defined with respect to a class of alternative adjacency matrices N producible by running an efficient dynamic program on candidate distance matrices  X  D . This loss, termed  X  X tructure preserving X  in (Shaw &amp; Jebara, 2009), be-haves like a structured output SVM loss that tries to make sure that the sum of estimated distances on N ( D ) are less than on any alternative adjacency ma-trix in N , plus a margin dictated by how far N  X  N is from N ( D ). Both losses are convex.
 Non-convex Losses: Finally, even though non-convex losses are computationally problematic, a few important methods are expressible in this manner: L L where  X   X  0 is a weighting parameter. The first loss corresponds to tSNE (38), which is a modification of the symmetric SNE loss (34), using the same transfer p ( D ) on the target distances D , but using a  X  X eavier tailed X  transfer function on  X  D (van der Maaten &amp; Hin-ton, 2008). The second loss (40) can be shown to be equivalent to local linear embedding (LLE) (Roweis &amp; Saul, 2000; Saul &amp; Roweis, 2003; Ham et al., 2004) if one tracks the solution in the limit as  X  &amp; 0. Clearly, this formulation shows that the LLE loss is highly non-convex in  X  D .
 Note that other non-parametric dimensionality reduc-tion methods can also be expressed in terms of regular-ized minimization of a loss between distance matrices, but the above suffices to illustrate how comprehen-sively current methods can be covered. Interestingly, this loss-based framework generalizes probabilistic for-mulations (Lawrence, 2011), since e.g. large margin losses cannot be naturally expressed as log-likelihoods. Our main contribution is to propose two new convex regularizers for non-parametric dimensionality reduc-tion. In particular, we formulate convex relaxations of the partitioned regularizer (19), which simultenously seeks to spread distances on the top d dimensions while shrinking distances in the remaining directions. The consequences for both rank reduction and manifold un-folding are clear. We first introduce a slight modifica-tion of (19) by adding a small quadratic smoother R where  X  &gt; 0, and P is the same as defined previously in (20). Although this modified regularizer is still not convex in  X  K , it enables two useful relaxations. 5.1. Completed Square The first relaxation we propose is extremely simple: (41) can be made jointly convex in  X  K and P simply by completing the square, yielding R 6 (  X  K ) = min This is guaranteed to be an upper bound on (41) since we are merely adding a nonnegative term  X  2 k  X  +  X   X  P k A lower bound on (41) can be recovered by subtracting The main benefit of this relaxation is that it is ex-tremely simple and computationally attractive: a sim-ple modification of the alternating minimization strat-egy of (Shaw &amp; Jebara, 2007) now yields a global solu-tion. This does not, however, yield the tightest convex approximation of (41), as we now demonstrate. 5.2. Bi-conjugation Recall that the conjugate of a function f is defined by f ( y ) = sup x  X  dom( f )  X  x,y  X  X  X  f ( x ) (Borwein &amp; Lewis, 2006). Importantly, any function is lower bounded by its bi-conjugate; i.e. f  X  f  X  X  X  (Borwein &amp; Lewis, 2006,  X  4.2). Therefore, a general strategy for deriving maximal convex lower bounds on objective functions can be based on Fenchel bi-conjugation (Jojic et al., 2011). Here, we obtain a new regularizer by formulat-ing the tightest convex relaxation of (41) based on its bi-conjugate and define By construction, this must be a convex function in  X  K . Theorem 1 R  X  X  X  5 ( U ) = max where [  X  ] + = max(0 ,  X  ) is applied to the eigenvalues. Proof: We compute the Fenchel bi-conjugate of First, the conjugate of g ( K ) is easily derived as Note that the domain of g  X  is actually all t  X  t matrices, but (47) implies that g  X  ( Z ) = g  X  ( Z 0 ) = g  X  (( Z + Z hence for the purpose of computing g  X  X  X  no generality is lost if we restrict the domain of g  X  to symmetric matrices. Now for any U &lt; 0 we obtain g  X  X  X  ( U ) = max The last equality is due to von Neumann X  X  trace in-equality (Borwein &amp; Lewis, 2006) and the elementary Putting  X  = 0, Theorem 1 implies that the Fenchel biconjugate of (19) is exactly (17) (for Z =  X   X I ) if d &gt; 0 and is (16) (for Z =  X I ) if d = 0, which might explain the success of MVU (Weinberger et al., 2004). Although (44) appears to be a complex regularizer, it is not computationally much harder to optimize than (42). To evaluate R  X  X  X  5 ( U ), an optimal Z in (44) must be solved, but this is a convex problem and admits efficient optimization. First note that the inner max-imization in (44) necessarily attains its maximum at some extreme point of the set P (for we are maximizing a convex function). Next inspecting (49) and invoking von Neumann X  X  trace inequality one more time we re-duce (44) to its vector counterpart. Let { z i } and { u (both arranged in decreasing order) be the eigenvalues of Z and U , respectively, then we just need to solve: min Temporarily ignoring the constraints, the optimal z is obvious since all elements of z are separated in (50): Now observe that  X  z i automatically satisfies the order constraints in the two blocks, thus the only possibility of violating the order constraint is between the blocks, i.e.  X  z d &lt;  X  z d +1 . But if we knew  X  z d =  X  , we would be able to fix the order by setting Therefore we only need to find  X  that minimizes (50), which is a univariate convex piecewise quadratic func-tion in  X  , hence can be done very quickly.
 To summarize, for evaluating R  X  X  X  5 ( U ) (and obtaining a subgradient), we need to perform SVD on U and then solve (50). The former costs O ( t 3 ) while the latter costs O ( t ), where t is the number of points. Most evaluations of dimensionality reduction methods resort to subjective assessments of specific case studies. However, many proposals exist for quantitatively eval-uating the quality of different methods in a somewhat objective manner (Sun et al., 2011; Lee &amp; Verleysen, 2010b; van der Maaten, 2009). Evaluation is clearer when the regularizer and loss components are consid-ered separately. In particular, to compare regulariz-ers , an objective assessment can be based on the loss values achieved by the low-rank reconstruction. For a given loss function L , we measure the reconstruc-tion loss L ( D (  X  X  X  X 0 ); D ) achieved by the recovered low rank representation  X  X . Another objective assessment can be based on the run time of the corresponding methods. 5 Finally, we can assess the quality of the convex relaxations by measuring the gap between the final objective values achieved using the relaxed regu-larizers versus their source regularizer R 5 (41). In our experiments, we compare different regularizers on a representative set of loss functions. The losses we considered are: regularized MVU loss (29), SNE (34), and the reciprocal exponential Bregman diver-gence (31). The results are given in Table 1 for six reg-ularizers and three losses on four different data sets. 6 The regularizers used are R 2 (16), R 3 (17), R 4 (19), R 5 (41), R 6 (42), and R 7 (44). Additionally, we report the objective values for R 5 and its relaxations R 6 and R 7 , to assess the approximation gap.
 In terms of reconstruction error, the new convex reg-ularizers obtain the best reconstruction errors overall, almost always outperforming the other competitors. A particular disadvantage of the partitioned regulariz-ers is that they are not convex, hence sensitive to ini-tialization: the objective values (shown in the square brackets in the rightmost three columns) demonstrate that inferior local minima do exist, since the completed square regularizer ( R 6 ) provides an upper bound on the optimal partition(  X  ) objective. The run times for the new regularizers are somewhat slower. We have presented simple view of non-parametric di-mensionality reduction by separating the roles of the regularizer and loss function. The result is a com-pact survey of a large fraction of the literature, which reveals a natural loss function that has not been thor-oughly explored. More importantly, we developed two new convex relaxations of a useful, but non-convex reg-ularizer. We investigated the behavior of the new reg-ularizers across a representative sample of loss func-tions. Important directions for future research include extending the convex regularization framework to con-sider other target topologies, sparsity, and mixtures.
