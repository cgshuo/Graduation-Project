  X  Due to the prevalence of personalization and information fil-tering applications, modeling users X  interests on the Web has become increasingly important during the past few years. In this paper, aiming at providing accurate personalized Web site recommendations for Web users, we propose a novel probabilistic factor model based on dimensionality reduc-tion techniques. We also extend the proposed method to collective probabilistic factor modeling, which further im-proves model performance by incorporating heterogeneous data sources. The proposed method is general, and can be applied to not only Web site recommendations, but also a wide range of Web applications, including behavioral tar-geting, sponsored search, etc. The experimental analysis on Web site recommendation shows that our method outper-forms other traditional recommendation approaches. More-over, the complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations.
 H.3.3 [ Information Search and Retrieval ]: Search Pro-cess, Information Filtering Algorithm, Experimentation Probabilistic Factor Modeling, Web Site Recommendation, Matrix Factorization, Recommender Systems  X 
This work was partially done when the first author was on summer internship at Microsoft Research.  X  Irwin King is currently on leave from CUHK to be with AT&amp;T Labs.

How to find the most relevant search results for Web users has been extensively studied in both academia and industry during the past decade. Traditionally, commercial search engines try to satisfy online users X  information needs based on the queries issued by users. By taking advantages of the advanced learning and ranking algorithms, commercial search engine services have already become indispensable in solving Web users X  various information needs.
 However, due to the rapid growth of information on the Web, especially for the social Web, the traditional search paradigm can no longer meet Web users X  ever-growing in-formation needs. The Web is now shifting online users from  X  X earch X  to  X  X iscovery X . The current paradigm is that a user who has information needs or questions will search on com-mercial search engines for related Web sites or answers. Dif-ferent from  X  X earch X , the concept of  X  X iscovery X  refers to the ability to push relevant content to users either based on the meta data that have already been collected about the users, or by exposing their activities that their friends and social networks are engaged in.

From the above definitions, we can see that these two con-cepts are different but closely related since essentially, the idea of  X  X iscovery X  is similar to  X  X utonomous search X , which helps users search for things when they do not even yet know what they want. Due to its personalized nature,  X  X iscovery X  or  X  X utonomous search X , which combines a person X  X  context with others X  past opinions and actions, can therefore create tremendous value and relevance.

In this paper, in order to enhance online users X  experi-ence, we investigate a novel application, Web site recom-mendation, to help users proactively discover relevant Web sites by conducting autonomous search based on their past online behaviors.

Many methods [7, 13, 17, 19, 22, 23, 24, 32] have been proposed to tackle recommendation or collaborative filtering problems in the literature. Typically, in traditional recom-mender systems, modeling users X  interests needs to utilize a user-item rating matrix, which explicitly specifies users X  preferences. However, in the scenario of Web site recom-mendation, it is infeasible to ask Web users to explicitly rate Web sites they like or dislike. Instead, we can only take advantages of implicit user behavior data in the past to make recommendations. Hence, most of the traditional collaborative filtering approaches cannot be directly applied to the Web site recommendation task.

In this work, aiming at solving the Web site recommenda-tion problem effectively and efficiently, we propose a proba-bilistic factor modeling framework by utilizing implicit user-site frequency data based on the following motivations: 1. A Web user X  X  preference on different Web sites can be 2. Higher visiting frequency on a site from a user means 3. User-query issuing frequency data can be incorporated The first two motivations lead to a Probabilistic Factor Model (PFM) based on dimensionality reduction techniques. More specifically, the user-site frequency matrix is factor-ized into two low-dimensional factor matrices: user latent matrix and site latent matrix. The value of each dimension in a user factor vector follows a gamma distribution, which indicates a user X  X  preference in a latent topic. A site fac-tor vector also has similar definition. Then, the observed user-site frequency matrix is assumed to follow Poisson dis-tribution with the mean generated by the inner product of the user latent matrix and the item latent matrix. Finally, an efficient multiplicative updating algorithm is proposed to learn the latent factor matrices.

By considering the third motivation, we design a Collec-tive Probabilistic Factor Model (CPFM) which simultane-ously factorizes user-site matrix and user-query matrix by sharing the same user latent space. The experimental anal-ysis shows that CPFM model can further improve the rec-ommendation quality.
 The remainder of this paper is structured as follows. In Section 2, we provide an overview of several major approaches for recommender systems and some related work. Section 3 details the concept of low-rank matrix factorization. The proposed probabilistic framework and the learning algorithm are presented in Section 4. The results of an empirical anal-ysis is presented in Section 5, followed by the conclusion and future work in Section 6.
In this section, we review two research topics which are relevant to our work: Web user interests prediction and rec-ommender systems.
In this subsection, we review some closely related tasks in the literature to predict Web user interests.

Personalized search engine is a natural application to Web user interests modeling, which improves the ranking system (ranking problem can also be considered as recommendation problem) by incorporating an individual X  X  historical activi-ties. In [27], Sun et al. conducted sophisticated analysis on the correlation between users, their queries and search results clicked to model user preferences. The experimen-tal results indicate that the proposed CubeSVD approach can significantly improve Web search performance. In [20], Qiu et al. studied how a search engine can learn a user X  X  preference automatically based on her past click history and how it can use the user preference to personalize search re-sults. The experiments show that personalized search based
Figure 1: IE8 Site Suggestion for  X  X oogle.com X  on user preference yields significant improvements over the best existing ranking mechanism in the literature. Teevan et al. in [28] also utilized implicit information about users X  interests to re-rank Web search results within a relevance feedback framework.

Applications of implicit feedback to predict user future visits are also studied in [6, 11]. These systems typically establish historical click trails of a user or a community of users, and assess the accuracy of statistical machine learning models which predict future page visits [29].

Recently, White et al. evaluated how to effectively predict user interests based on five contextual information sources: social, historic, task, collection and user interaction [29]. The user X  X  Web page visits are automatically assigned to Open Directory Project (ODP) categories and then used to predict future visits of sites within a category. Different from this approach, in the work presented in this paper, we focus on predicting a lower granularity of actual frequencies of the visits, as opposed to a category level.

Starting from the release of Internet Explorer 8 (IE8), an important feature was introduced - X  X uggested Site X . As shown in Figure 1, IE8 lists Top 5 Web sites that are similar to the site a user types in the address bar. So far, this feature makes Web site recommendations based on a single site, which does not include any personalized results. In this paper, we will study how to personalize the suggestion results based on users X  past online behaviors.
Our work is closely related to the research of recommender systems. In this subsection, we review several major ap-proaches for recommender systems, especially for collabora-tive filtering.

A number of algorithms have been proposed to improve both the recommendation quality and the scalability prob-lems. As discussed in [9], one of the most commonly-used and successfully-deployed recommendation approaches is col-laborative filtering. These collaborative filtering algorithms can be divided into two main categories: neighborhood-based and model-based approaches. Neighborhood-based methods, or memory-based methods, mainly concentrate on finding the similar users [2, 10] or items [5, 17, 24] for generating recommendations. The neighborhood-based rec-ommendation algorithms are based on the assumption that those who agreed in the past tend to agree again in the future. They usually fall into two classes: user-based ap-proaches and item-based approaches. User-based approaches predict the ratings of active users based on the ratings of similar users found, while item-based approaches predict the ratings of active users based on the computed information of items similar to those chosen by the active user.
In addition to the neighborhood-based approaches, the model-based approaches to collaborative filtering train a pre-defined model by employing the observed user-item ratings. Algorithms in this category include the clustering model [12], the aspect models [7, 8, 25], the latent factor model [3], the Bayesian hierarchical model [31] and the ranking model [18]. Recently, due to its efficiency in dealing with large datasets, several low-dimensional matrix approximation methods [1, 13, 21, 22, 23, 26] have been proposed. These methods all focus on fitting the user-item rating matrix using low-rank approximations, and employ the matrix to make further pre-dictions. The low-rank matrix factorization methods are very efficient in training since they assume that in the user-item rating matrix, only a small number of factors influence preferences, and that a user X  X  preference vector is determined by how each factor applies to that user. Low-rank matrix approximations based on minimizing the sum-squared er-rors can be easily solved using Singular Value Decomposi-tion (SVD). In [22], Salakhutdinov et al. proposed a prob-abilistic graphic model by assuming some Gaussian obser-vation noises on observed user-item ratings. The proposed model achieved promising prediction results. In their follow-ing work proposed in [23], the Gaussian-Wishart priors are placed on the user and item hyper-parameters. Since exact inference is intractable in the new model, a Gibbs sampling method is proposed to iteratively learn the user and item latent matrices.

All of the low-dimensional approximation methods men-tioned above are based on factorizing the user-item rating matrix. Normally, in a recommender system, users can as-sign 5-scale integer ratings (from 1 to 5, and 5 indicates case of Web site recommendation, the range of the user-site frequency data is much wider than the rating data. A user can visit a site only once, but probably will also visit another site thousands of times or even more. The wide range of the frequency data poses new challenges in modeling the Web site recommendation problem. Some state-of-the-art algo-rithms based on the rating data may not perform well on the frequency data. For example, in the method proposed in [22], the rating data are modeled as Gaussian distribu-tions with some noises. This assumption is not suitable for modeling the frequency data since the underlying distribu-tion is probably not Gaussian. Hence, most of previous low-dimensional approximation methods cannot generate good recommendation results when they are applied to the Web site recommendation problems.
As mentioned in Section 2, a popular method in recom-mender systems is to factorize the user-item rating matrix, and to utilize the factorized user-specific and item-specific matrices for further missing data prediction [22, 23, 30]. The premise behind a low-dimensional factor model is that there are only a small number of factors influencing the prefer-scales. Some systems may use 10-scale integer ratings. ences, and that a user X  X  preference vector is determined by how each factor applies to that user [21].

In recommender systems, considering an m  X  n user-item rating matrix R describing m users X  numerical ratings on n items, a low-rank matrix factorization approach seeks to approximate the rating matrix R by a multiplication of d -rank factors R  X  UV T , where U  X  R m  X  d and V  X  R n  X  d with d &lt;&lt; min( m, n ). The matrix R in the real-world is usually very sparse since most of the users only rates few items.

The Singular Value Decomposition (SVD) method is em-ployed to estimate a matrix R by minimizing where U i and V j are row vectors with d values, I ij is the indicator function that is equal to 1 if user i rated item j and equal to 0 otherwise.
 Another popular method in recommender systems is the Probabilistic Matrix Factorization (PMF) method proposed in [22]. The conditional distribution over the observed rat-ings is defined as: where N ( x |  X ,  X  2 ) is the probability density function of the Gaussian distribution with mean  X  and variance  X  2 . The zero-mean spherical Gaussian priors are also placed on user and item feature vectors: p ( U |  X  2 U )= Hence, through a Bayesian inference, we have the following objective function: min where  X  1 ,  X  2 &gt; 0. The optimization problem is to minimize the sum-of-squared-errors objective function with quadratic regularization terms. Gradient based approaches can be applied to find a local minimum. The above algorithm is perhaps one of the most popular methods in collaborative filtering.
 Actually, when modeling the observed rating data, both SVD and PMF have the underlying assumption about Gaus-sian distribution. This assumption may not be appropriate when working on the frequency data. Hence, these models may encounter problems in generating good recommenda-tion results. The experimental analysis conducted in Sec-tion 5 demonstrates our concerns. In next section, we pro-pose two probabilistic models which can potentially gener-ate better Web site recommendation results than SVD and PMF.
The major task studied in this paper is to predict how frequently a user will visit a Web site. In commercial search engines or browser toolbars, query logs record the activities Table 1: Samples of Search Engine Query Logs of Web users, which reflect their interests and the relation-ships between users and queries, as well as users and clicked Web sites. As shown in Table 1, each line of the data we need includes the following information: a user ID, a query issued by the user, and a Web site on which the user clicked.
By aggregating the data in Table 1, we can construct two matrices, as shown in Figure 2. Figure 2(a) is the user-site frequency matrix, which contains how many times a user visited a Web site. Figure 2(b) gives the user-query fre-quency matrix, which indicates how frequently a user issued a query.

The problem under investigation is essentially how to ef-fectively and efficiently predict the missing values of the user-site frequency matrix by employing these data sources. To attack this problem, in Section 4.2, we propose a novel probabilistic factor model by using only the user-site fre-quency matrix, while in Section 4.3, we introduce a collec-tive probabilistic factor model by utilizing both user-site and user-query frequency matrices, which can further improve the model performance.
Our proposed Probabilistic Factor Model (PFM) is a gen-erative probabilistic model, which can be represented by the graphical model in Figure 3. Let F be an m  X  n data ma-trix whose element f ij is the observed count of event (or feature, or item) j by user i . Y is a matrix of expected counts with the same dimensions as F , and y ij denotes an element in matrix Y . Every observed element f ij in matrix F is assumed to follow Poisson distribution with the mean y ij in matrix Y , respectively. The matrix Y is factorized into two matrices U and V , where U is an m  X  d matrix, V is an n  X  d matrix and d is the dimensionality. Each element u ik ( k = 1 , ..., d ) in U encodes the preference of user i to latent topic k , and each v jk in V can be interpreted as the affinity of site j to the latent topic k . Finally, u ik and v are given the Gamma distributions as the empirical priors.
There are two reasons we use Gamma distributions to model u ik and v jk instead of Gaussian or other distribu-tions: (1) Gamma distribution is suitable for modeling non-negative values, while Gaussian distribution can model both Figure 3: Graphical Model for Probabilistic Factor Modeling negative and non-negative values. If we allow negative values in u ik and v jk , potentially, the model will generate negative frequency values, which is unreasonable in the real world. (2) The Gamma distribution is already proved to be effec-tive in modeling document latent vectors in text analysis [4], where the document-word relation is also represented as a frequency matrix.

Therefore, the generative process of an observed user-site count f ij in our model follows: 1. Generate u ik  X  Gamma(  X  k ,  X  k ) ,  X  k . 2. Generate v jk  X  Gamma(  X  k ,  X  k ) ,  X  k . 3. Generate y ij occurrences of item or event j from user 4. Generate f ij  X  Poisson( y ij ).

The gamma distributions of U and V follow the proba-bilistic functions where  X  = {  X  1 , ...,  X  d } ,  X  = {  X  1 , ...,  X  d } , u  X  k &gt; 0 and  X  k &gt; 0,  X (  X  ) is the Gamma function.
The Poisson distribution of F given Y can then be defined as where y ij =
Since Y = UV T , the posterior distribution of U and V given F can be modeled as
Hence, we can infer the log of the posterior distribution p ( U, V | F,  X  ,  X  ) as follows Figure 4: Graphical Model for Collective Probabilis-tic Factor Modeling
L ( U, V ; F ) =
Taking derivatives on L with respect to u ik and v jk , we have  X  L  X  L
Using similar techniques proposed by Lee and Seung in [16], by setting the learning rates to respectively, we can obtain the following multiplicative up-dating rules:
In many Web applications, we have multiple data sources instead of a single data source. In the case of Web site rec-ommendation, the Web users not only browsed many Web sites, but also issued several queries. The queries issued by users can also represent the interests of the users. If we can incorporate this information, we can potentially improve the model performance.

Actually, our proposed model in Section 4.2 can be easily extended to incorporate heterogeneous data sources. When we are observing the example matrices in Figure 2(a) and Figure 2(b), we notice that these two data sources share the same user space. Hence, we propose a Collective Prob-abilistic Factor Model (CPFM) by sharing the same user latent space. Intuitively, CPFM method should outper-form PFM method since the additional user-query matrix can help model users X  preferences more accurately. Figure 4 shows the graphical model for mining these two data matri-ces.

In this graphical model, F x is an m  X  p data matrix whose element f x il represents how many times user i issued query l . X is a matrix of expected counts with the same dimen-sions as F x , and x il denotes an element in matrix X . Ev-ery observed element f x il in matrix F x is assumed to follow Poisson distribution with the mean x il in matrix X , respec-tively. The matrix X is factorized into two matrices U and Z , where Z is a p  X  d matrix. Each element z lk in Z encodes the affinity of query l to the latent topic k . The definitions of F y , Y , U , V , u ik and v jk are the same as the definitions in Section 4.2.
 Similar to Eq. (8), we have the posterior distribution of U , V and Z , given F x and F y : p ( U, V, Z | F x , F y ,  X  ,  X  )
In this equation, p ( F x | X ) and p ( F y | Y ) are defined as: where x il =
Moreover, since every element in Z also follows a Gamma can be defined as:
Hence, by inferring Eq. (12), we have the log of the pos-terior distribution for CPFM model:
Similar to PFM, we can easily obtain the multiplicative updating rules for learning U , V and Z : u v z
In Eq. (17), we treat the information from the user-site matrix and the user-query matrix equally. However, some-times we may want to control how much information to use in each side, and to find a balance between these two data sources. Hence, we add a smoothing parameter  X  in order to tune the importance of these two data sources. The up-dating rules for u ik in Eq. (17) is then changed to u
Our proposed method is general, and can be easily ex-tended to incorporate other contextual information for other research problems. In this section, we utilize the user-query information to improve the PFM method. Actually, in the query logs, every Web site is also associated with some queries. Hence, a natural extension is to further incorporate the site-query frequency matrix by adding another matrix factorization in Figure 4. We hope this can help learn the site latent vectors more accurately. Besides Web site recom-mendation, many other research problems also have similar frequency data, like sponsored search, behavioral targeting, text mining, etc. Our model can also be easily applied to these problems. We do not discuss the details in this paper since our focus of this paper is to illustrate how to make Web site recommendations.
The main computation of gradient methods is evaluating the object function L and its gradients against variables. In PFM model, because of the sparsity of matrix F , the computational complexity of evaluating the object function L is O ( N F d ), where N F is the number of nonzero entries in matrix F . The computational complexities for gradients  X  L and  X  L  X  X  in Eq. (10) are also O ( N F d ). Therefore, the total computational complexity in one iteration is O ( N F d ), which indicates that the computational time of our PFM method is linear with respect to the number of observations in the user-site frequency matrix. The total complexity of PFM is then O ( N F dr ), where r is the number of iterations. Similarly, for CPFM, the complexity is O ( N F x dr + N F y dr ), where N and N F y are the number of nonzero entries in user-query and user-site matrices, respectively. Since our algorithm will converge after 10 to 20 iterations, this complexity analysis shows that our proposed approach is very efficient and can scale up with respect to very large datasets.
 Table 2: Statistics of User-Site and User-Query Fre-quency Matrices Statistics User-Site Frequency User-Query Frequency Min. Num. 4 10 Max. Num. 9,969 4,693 Avg. Num. 20.33 23.05
In this section, we conduct several experiments to com-pare the recommendation quality of our PFM and CPFM approaches with other baseline and state-of-the-art collabo-rative filtering or recommendation methods.

Our experiments are intended to address the following questions: 1. How does our approach compare with the baseline and 2. How do the model parameters  X  k and  X  k affect the 3. What is the performance difference when we use dif-4. How does the smooth parameter  X  affect the prediction 5. Is our algorithm efficient for large datasets?
We use two metrics, the Normalized Mean Absolute Er-ror (NMAE) and the Normalized Root Mean Square Error (NRMSE), to measure the prediction accuracy.
 The metric NMAE is defined as: where f i,j denotes the frequency user i visited Web site j , b f i,j denotes the frequency user i visited Web site j as pre-dicted by a method, and N denotes the number of tested data. The metric NRMSE is defined as: From the definitions, we can see that a smaller NMAE or NRMSE value means a better performance.
The primary source of data for this work is the anonymized logs of Web sites visited by users who opted-in to provide data through a widely-distributed browser toolbar. Each line of the data source may contain a lot of information. We only keep those entries which include a unique identifier for the user, query issued by the user, and the Web sites visited by the user. Similar to the work in [29], in order to remove variability caused by geographic and linguistic variations in search behaviors, we only include those entries generated in the English speaking United States locale.

The experiments conducted in this paper are based on one month browser toolbar data, which represents billions of Web site visits. In order to remove some outliers and clean up the data, we require that every user should at least visited 10 Web sites, and each Web site should at least be visited by 10 users. Moreover, we also constrain that a user should visit each Web site at least four times. URLs of all the Web sites are truncated to the site level. After pruning, we have 165,403 unique users, 265,367 unique URLs and 442,598 unique queries. Totally, the pruned dataset records 53,089,262 Web site visits. In the constructed user-site fre-quency matrix, 2,612,016 entries are observed, while in the user-query frequency matrix, the number of observed en-tries is 833,581. Some other statistics are shown in Table 2. From this table, we can see that a user visited a site for 9,969 times in a month, which shows that the range of user-site frequency matrix is very wide.
In this section, in order to show the effectiveness of our proposed recommendation approaches, we compare the rec-ommendation results of the following methods: 1. UserMean : this is a baseline method, which uses the 2. SiteMean : this is also a baseline method, which uti-3. SVD : this is a well-known method in matrix factor-4. PMF : this method is proposed by Salakhutdinov and 5. NMF : this method is originally proposed in [15, 16] 6. GaP : this approach is proposed in [4], and it is orig-
We use different amounts of training data (90%, 80%) to test the algorithms. Training data 90%, for example, means we randomly select 90% of the observed data as the training data to predict the remaining 10%. The random selection was carried out 5 times independently, and we report the average results. The experimental results using 10 and 20 dimensions to represent the latent features are shown in Ta-ble 3 and Table 4, respectively. The standard deviations of the results generated by our methods are all around 0.005. In Table 3, we set all  X  k = 20,  X  k = 0 . 2 in our PFM and CPFM models, while in Table 4, we change the settings to  X  k = 20,  X  k = 0 . 05 for both PFM and CPFM. In Section 5.4, we will explain why we need to choose different  X  k and  X  values when the dimensions are different. In addition, in all the experiments, we set the parameter  X  = 0 . 1 for our CPFM method.

From the results, we can observe that our methods consis-tently outperform other approaches in all the settings, which shows the promising future of our methods. The percent-ages in Table 3 and Table 4 are the improvements of our CPFM method over the corresponding approaches.
 We observe that the two baseline methods, UserMean and SiteMean, have the worst performance. Among these two methods, SiteMean performs much better than UserMean. This may indicate that users generally have diverse visit-ing patterns than sites. We also notice that two Gaussian based methods SVD and PMF are not suitable for factoriz-ing frequency data, which coincides with our discussion in Section 2.
From Table 3 and Table 4, we can see that our methods perform better when we choose a larger dimensionality. This is reasonable since more dimensions give us more flexibility to represent both user and site latent vectors. However, this is not always true since in the case we choose a very large dimensionality, we may experience severe overfitting problems.

As we discussed above, we set different  X  k and  X  k val-ues when choosing different dimensions. This is because we model Since the frequencies are fixed, if we use a larger dimension-ality, we need to employ some smaller values of u ik and v That is why in the case of d = 10, the optimal parameter settings are  X  k = 20 and  X  k = 0 . 2, while in the case where d = 20, we set  X  k = 20 and  X  k = 0 . 05. We also plot the Gamma probability density functions for these two settings in Figure 5. From this figure, we can see that we will have a very high probability to generate smaller values if we choose the setting  X  k = 0 . 05. This observation can help guide us to efficiently select appropriate parameters once the dimen-sionality is decided.
Parameters  X  k and  X  k are two important parameters since they define the shapes and scales of the Gamma distribu-tions. In Figure 6 and Figure 7, we independently study how these two parameters affect the model performance in PFM. Figure 6 shows the impact of parameter  X  k , given  X  k = 0 . 2. We can see that the model achieves the best per-formance when  X  k = 20. A larger or smaller  X  k value will lead to hurt the model performance.

Figure 6(c) shows some related Gamma distributions with  X  k fixed. From this figure, we can easily interpret why dif-ferent  X  k and  X  k values can significantly influence the per-formance. When  X  k = 20 and  X  k = 0 . 2, the model has the best performance. At the same time, this Gamma distribu-tion will generate values from 3 to 5 with high probabilities, which also means that these values can best represent user and site latent vectors. However, other parameter settings generate either much smaller or much larger values, which will potentially hurt the model performance.

The impact of parameter  X  k is shown in Figure 7, and it shares similar trends as shown in Figure 6.
In our CPFM method proposed in this paper, the parame-ter  X  balances the information from the user-site matrix and Table 5: Efficiency Analysis (90% as Training Data) the user-query matrix. It controls how much our method should use the information in each matrix. If  X  = 1, we only factorize the user-site frequency matrix, and simply employ users X  site visiting pattern in learning. If  X  equals to a very small value, we will count more on information from the user-query frequency matrix to learn users X  interests. For other cases in-between, we fuse information from the user-site matrix and the user-query matrix for collective proba-bilistic factor modeling and, furthermore, to make Web site recommendation for the users.
 The impacts of  X  on NMAE and NRMSE are shown in Figure 8. We observe that the value of  X  impacts the recom-mendation results a lot, which illustrates that fusing these two data sources can greatly improve the recommendation quality. As  X  increases, the NMAE and NRMSE decrease at first, but when  X  surpasses a certain threshold, the NMAE and NRMSE increase with further increase of the value of  X  . This phenomenon confirms with our initial intuition that fusing these two data sources together can generate better performance than purely using the user-site frequency ma-trix for recommendations.

In Figure 8(a) and Figure 8(b), when using 90% as train-ing data, we observe that, our CPFM method achieves the best performance when  X  is around 0.1, while smaller values like  X  = 0 . 01 or larger values like  X  = 0 . 3 can potentially hurt the model performance. Also a smaller  X  is preferred to a larger  X  indicates that we need to use more about user-query information than user-site information. The reason is perhaps the training data of user-site matrix is very sparse, which is less effective in learning the accurate interests of users.
The complexity analysis indicates that our method is lin-ear with the observations. We also conduct some exper-iments on measuring the efficiency of our method. The Seconds-per-Iteration evaluation results are shown in Ta-ble 5 for both PFM and CPFM methods. We find that our methods are very efficient since our method will converge af-ter 10 to 20 iterations. Hence, in 10-dimension case, we only need around 2 minutes to learn our PFM and CPFM mod-els. CPFM is generally a little bit slower than PFM since CPFM needs to deal with one extra user-query matrix.
All the experiments are conducted on a workstation con-taining an Intel Xeon CPU (2.5 GHz ) with 8G memory.
In this paper, aiming at providing personalized, accurate and efficient Web site recommendations for Web users, we propose two novel probabilistic factor models: PFM and CPFM. The experimental analysis on the large browser tool-bar data shows the effectiveness and efficiency of our meth-ods. The complexity analysis indicates our methods can be applied to very large datesets since they are linear with the observations in the user-site and user-query matrices. In order to reduce the model complexity, in our PFM and CPFM methods, we use the same set of  X  k and  X  k values for both user, site and query latent features. Actually, since users, sites and queries are totally different objects, they may belong to completely different Gamma distributions. Hence, in order to further improve the recommendation per-formance, in the future, we can slightly modify the model, and allow setting different  X  k and  X  k values for these three objects.

In all the experiments conducted in this paper, we need to set the values for  X  k and  X  k . In the future, we will investi-gate a more intelligent algorithm to automatically determine these values. We may run into some challenges since the data is very sparse, hence, learning  X  k and  X  k automatically may experience overfitting problems. Another solution is that we can further assume a distribution on top of these two pa-rameters. However, this is also difficult to implement since Gamma distribution does not have a very good conjugate prior distribution.

In our experiment, we only measure how accurate we can predict the user-site visiting frequency. Sometimes, only be-ing accurate cannot meet the application requirements. We also plan to design a prototype for users to use, and ask users to measure the effectiveness of the proposed methods.
For the PFM model itself, we can make further assump-tion that every user or site is generated from a mixture of Gamma distributions. We can also assume that the mixture coefficients follow a Dirichlet distribution. Since we proba-bly cannot infer the exact form of this model, we need to use sampling or variational approximation methods to solve the problem. This may however increase the complexity of the model.

Our proposed frame work is general; hence, in the future, we plan to apply our methods to other research problems, such as behavioral targeting and sponsored search. We can use the proposed methods to predict how many times a user will click an advertisement. Finally, we can also apply our methods to many text mining tasks, including clustering, classification, retrieval tasks, etc.
 The work described in this paper was partially supported by two grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK 415210 and Project No. CUHK 415410), and par-tially supported by a Google Focused Grant Project under  X  X obile 2014 X . The authors also would like to thank the reviewers for their helpful comments. [1] R. Bell, Y. Koren, and C. Volinsky. Modeling [2] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [3] J. Canny. Collaborative filtering with privacy via [4] J. Canny. GaP: a factor model for discrete data. In [5] M. Deshpande and G. Karypis. Item-based top-n [6] S. G  X  und  X  uz and M. T.  X  Ozsu. Recommendation models [7] T. Hofmann. Collaborative filtering via gaussian [8] T. Hofmann. Latent semantic models for collaborative [9] Z. Huang, H. Chen, and D. Zeng. Applying associative [10] R. Jin, J. Y. Chai, and L. Si. An automatic weighting [11] F. Khalil, J. Li, and H. Wang. Integrating [12] A. Kohrs and B. Merialdo. Clustering for collaborative [13] Y. Koren. Collaborative filtering with temporal [14] M. Kurucz, A. A. Benczur, and K. Csalogany.
 [15] D. D. Lee and H. S. Seung. Learning the parts of [16] D. D. Lee and H. S. Seung. Algorithms for [17] G. Linden, B. Smith, and J. York. Amazon.com [18] N. N. Liu and Q. Yang. Eigenrank: a ranking-oriented [19] H. Ma, I. King, and M. R. Lyu. Effective missing data [20] F. Qiu and J. Cho. Automatic identification of user [21] J. D. M. Rennie and N. Srebro. Fast maximum margin [22] R. Salakhutdinov and A. Mnih. Probabilistic matrix [23] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [24] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [25] L. Si and R. Jin. Flexible mixture model for [26] N. Srebro and T. Jaakkola. Weighted low-rank [27] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. [28] J. Teevan, S. T. Dumais, and E. Horvitz.
 [29] R. W. White, P. Bailey, and L. Chen. Predicting user [30] K. Yu, S. Zhu, J. Lafferty, and Y. Gong. Fast [31] Y. Zhang and J. Koren. Efficient bayesian hierarchical [32] D. Zhou, S. Zhu, K. Yu, X. Song, B. L. Tseng, H. Zha,
