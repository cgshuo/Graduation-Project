 The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infec-tion times as epidemics spread through the network. This prob-lem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed pat-terns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or com-putational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot. We define the trace complexity as the number of distinct traces required to achieve high fidelity in reconstructing the topology of the unobserved network or, more generally, some of its properties. We give algorithms that are competitive with, while being simpler and more efficient than, existing network inference approaches. Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the number of traces that an optimal infer-ence algorithm requires for performing this task in the general case. Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution with-out inferring the network. We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.  X 
S upported by AFOSR grant FA9550-09-1-0100, by a Cornell Uni-versity Graduate School Travel Fellowship, and by a Google Award granted to Alessandro Panconesi.  X  Supported by two Google Faculty Research Awards and by the MULTIPLEX project (EU-FET-317532).  X  Supported by AFOSR grant FA9550-09-1-0100.
 I.5.1 [ Computing Methodology ]: Pattern Recognition  X  Design Methodology Network inference, Independent Cascade Model, Network Epidemics, Sampling Complexity
Many technological, social, and biological phenomena are natu-rally modeled as the propagation of a contagion through a network. For instance, in the blogosphere,  X  X emes X  spread through an un-derlying social network of bloggers [1], and, in biology, a virus spreads over a population through a network of contacts [2]. In many such cases, an observer may not directly probe the under-lying network structure, but may have access to the sequence of times at which the nodes are infected. Given one or more such records, or traces , and a probabilistic model of the epidemic pro-cess, we can hope to deduce the underlying graph structure or at least estimate some of its properties. This is the network infer-ence problem, which researchers have studied extensively in recent years [1,7,12,13,20].

In this paper we focus on the number of traces that network infer-ence tasks require, which we define as the trace complexity of the problem. Our work provides inference algorithms with rigorous upper bounds on their trace complexity, along with information-theoretic lower bounds. We consider network inference tasks under a diffusion model presented in [13], whose suitability for represent-ing real-world cascading phenomena in networks is supported by empirical evidence. In short, the model consists of a random cas-cade process that starts at a single node of a network, and each edge { u, v } independently propagates the epidemic, once u is infected, with probability p after a random incubation time .

Overview of results. In the first part of this paper, we focus on determining the number of traces that are necessary and/or suf-ficient to perfectly recover the edge set of the whole graph with high probability. We present algorithms and (almost) matching lower bounds for exact inference by showing that in the worst case,  X ( n  X  1  X   X  ) traces are necessary and O ( n  X  log n ) traces are suffi-cient, where n is the number of nodes in the network and  X  is its maximum degree. In the second part, we consider a natural line of investigation, given the preceding strong lower bounds, where we ask whether exact inference is possible using a smaller num-b er of traces for special classes of networks that frequently arise in the analysis of social and information networks. Accordingly, we present improved algorithms and trace complexity bounds for two such cases. We give a very simple and natural algorithm for exact inferences of trees that uses only O (log n ) traces. 1 To further pur-sue this point, we give an algorithm that exactly reconstructs graphs of degree bounded by  X  using only O (poly( X ) log n ) traces, un-der the assumption that epidemics always spread throughout the whole graph. Finally, given that recovering the topology of a hid-den network in the worst case requires an impractical number of traces, a natural question is whether some non-trivial property of the network can be accurately determined using a moderate num-ber of traces. Accordingly, we present a highly efficient algorithm that, using vastly fewer traces than are necessary for reconstruct-ing the entire edge set, reconstructs the degree distribution of the network with high fidelity by using O ( n ) traces.

The information contained in a trace. Our asymptotic results also provide some insight into the usefulness of information con-tained in a trace. Notice that the first two nodes of a trace unam-biguously reveal one edge  X  the one that connects them. As we keep scanning a trace the signal becomes more and more blurred: the third node could be a neighbor of the first or of the second node, or both. The fourth node could be the neighbor of any nonempty subset of the first three nodes, and so on. The main technical chal-lenge in our context is whether we can extract any useful informa-tion from the tail of a trace, i.e., the suffix consisting of all nodes from the second to the last. As it turns out, our lower bounds show that, for perfect inference on general connected graphs, the answer is  X  X o X : we show that the First-Edge algorithm , which just returns the edges corresponding to the first two nodes in each trace and ignores the rest, is essentially optimal. This limitation precludes optimal algorithms with practical trace complexity 2 . This result motivates further exploration of trace complexity for special-case graphs. Accordingly, for trees and bounded degree graphs, we il-lustrate how the tail of traces can be extremely useful for network inference tasks.

Our aforementioned algorithms for special-case graphs make use of maximum likelihood estimation (MLE) but in different ways. Previous approaches, with which we compare our results, have also employed MLE for network inference. For instance, N ET I NF is an algorithm that attempts to reconstruct the network from a set of independent traces by exploring a submodular property of its MLE formulation. Another example, and closest to ours, is the work by Netrapalli and Sangahvi [20], whose results include quali-tatively similar bounds on trace complexity in a quite different epi-demic model.

Turning our attention back to our algorithms, our tree reconstruc-tion algorithm performs global likelihood maximization over the entire graph, like the N ET I NF algorithm [13], whereas our bounded-degree reconstruction algorithm, like the algorithm in [20], per-forms MLE at each individual vertex. Our algorithms and analysis techniques, however, differ markedly from those of [13] and [20], and may be of independent interest.

In the literature on this rapidly expanding topic, researchers have validated their findings using small or stylized graphs and a rel-atively large number of traces. In this work, we aim to provide,
A ll inference results in this paper hold with high probability.
On the other hand, the use of short traces may not be only a the-oretical limitation, given the real world traces that we observe in modern social networks. For example, Bakshy et al. [3] report that most cascades in Twitter ( twitter.com ) are short, involving one or two hops. in the same spirit as [20], a formal and rigorous understanding of the potentialities and limitations of algorithms that aim to solve the network inference problem.

This paper is organized as follows. Section 2 presents an overview of previous approaches to network learning. Section 3 presents the cascade model we consider throughout the paper. Section 4 deals with the head of the trace : it presents the First-Edge algorithm for network inference, shows that it is essentially optimal in the worst case, and shows how the first edges X  timestamps can be used to guess the degree distribution of the network. Section 5, instead, deals with the tail of the trace : it presents efficient algorithms for perfect reconstruction of the topology of trees and of bounded de-gree networks. Section 6 presents an experimental analysis that compares ours and existing results through the lens of trace com-plexity. Finally, Section 7 offers our conclusions.
Network inference has been a highly active area of investigation in data mining and machine learning [1,7,12,13,20]. It is usually assumed that an event initially activates one or more nodes in a net-work, triggering a cascading process, e.g., bloggers acquire a piece of information that interests other bloggers [15], a group of people are the first infected by a contagious virus [2], or a small group of consumers are the early adopters of a new piece of technology that subsequently becomes popular [22]. In general, the process spreads like an epidemic over a network (i.e., the network formed by blog readers, the friendship network, the coworkers network). Researchers derive observations from each cascade in the form of traces  X  the identities of the people that are activated in the pro-cess and the timestamps of their activation. However, while we do see traces, we do not directly observe the network over which the cascade spreads. The network inference problem consists of recov-ering the underlying network using the epidemic data.

In this paper we study the cascade model that Gomez-Rodrigues et al. [13] introduced, which consists of a variation of the indepen-dent cascade model [16]. Gomez-Rodrigues et al. propose N I NF , a maximum likelihood algorithm, for network reconstruction. Their method is evaluated under the exponential and power-law dis-tributed incubation times. In our work, we restrict our analysis to the case where the incubation times are exponentially distributed as this makes for a rich arena of study.

Gomez-Rodrigues et al. have further generalized the model to in-clude different transmission rates for different edges and a broader collection of waiting times distributions [12, 19]. Later on, Du et al. [7] proposed a kernel-based method that is able to recover the network without prior assumptions on the waiting time distribu-tions. These methods have significantly higher computational costs than N ET I NF , and, therefore, than ours. Nevertheless, experiments on real and synthetic data show a marked improvement in accu-racy, in addition to gains in flexibility. Using a more combinatorial approach, Gripon and Rabbat [14] consider the problem of recon-structing a graph from traces defined as sets of unordered nodes, in which the nodes that appear in the same trace are connected by a path containing exactly the nodes in the trace. In this work, traces of size three are considered, and the authors identify necessary and sufficient conditions to reconstruct graphs in this setting.
The performance of network inference algorithms is dependent on the amount of information available for the reconstruction, i.e., the number and length of traces. The dependency on the number of traces have been illustrated in [7], [12], and [13] by plotting the performance of the algorithms against the number of available traces. Nevertheless, we find little research on a rigorous analysis of this dependency, with the exception of one paper [20] that we n ow discuss.

Similarly to our work, Netrapalli and Sangahvi [20] present quan-titative bounds on trace complexity in a quite different epidemic model. The model studied in [20] is another variation of the in-dependent cascade model. It differs from the model we study in a number of key aspects, which make that model a simplification of the model we consider here. For instance, (i) [20] assumes a cas-cading process over discrete time steps, while we assume continu-ous time (which has been shown to be a realistic model of several real-world processes [13]), (ii) the complexity analyzed in [20] ap-plies to a model where nodes are active for a single time step  X  once a node is infected, it has a single time step to infect its neigh-bors, after which it becomes permanently inactive. The model we consider does not bound the time that a node can wait before infect-ing a neighbor. Finally, (iii) [20] rely crucially on the  X  X orrelation decay X  assumption, which implies  X  for instance  X  that each node can be infected during the course of the epidemics by less than 1 neighbor in expectation. The simplifications in the model presented by [20] make it less realistic  X  and, also, make the inference task significantly easier than the one we consider here.

We believe that our analysis introduces a rigorous foundation to assess the performance of existing and new algorithms for network inference. In addition, to the best of our knowledge, our paper is the first to study how different parts of the trace can be useful for different network inference tasks. Also, it is the first to study the trace complexity of special case graphs, such as bounded degree graphs, and for reconstructing non-trivial properties of the network (without reconstructing the network itself), such as the node degree distribution.
The cascade model we consider is defined as follows. It starts with one activated node, henceforth called the source of the epi-demic, which is considered to be activated, without loss of gener-ality, at time t = 0 .

As soon as a node u gets activated, for each neighbor v i an independent coin: with probability p it will start a countdown on the edge { u, v i } . The length of the countdown will be a ran-dom variable distributed according to Exp(  X  ) (exponential parameter  X  ). When the countdown reaches 0 , that edge is tra-versed  X  that is, that epidemic reaches v i via u .

The  X  X race X  produced by the model will be a sequence of tuples (node v , t ( v ) ) where t ( v ) is the first time at which the epidemics reaches v .

In [13], the source of the epidemics is chosen uniformly at ran-dom from the nodes of the network. In general, though, the source can be chosen arbitrarily 4 .

The cascade process considered here admits a number of equiv-alent descriptions. The following happens to be quite handy: in-dependently for each edge of G , remove the edge with probability 1  X  p and otherwise assign a random edge length sampled from Exp(  X  ) . Run Dijkstra X  X  single-source shortest path algorithm on the subgraph formed by the edges that remain, using source s and the sampled edge lengths. Output vertices in the order they are [ 7, 12, 13] consider other random timer distributions; we will mainly be interested in exponential variables as this setting is al-ready rich enough to make for an interesting and extensive analy-sis.
Choosing sources in a realistic way is an open problem  X  the data that could offer a solution to this problem seems to be extremely scarce at this time. discovered, accompanied by a timestamp representing the shortest path length.
In this section we will deal with the head of a trace  X  that is, with the edge connecting the first and the second nodes of a trace. We show that, for general graphs, that edge is the only useful infor-mation that can be extracted from traces. Moreover, and perhaps surprisingly, this information is enough to achieve close-to-optimal trace complexity, i.e., no network inference algorithm can achieve better performance than a simple algorithm that only extracts the head of the trace and ignores the rest. We analyze this algorithm in the next section.
The First-Edge algorithm is simple to state. For each trace in the input, it extracts the edge connecting the first two nodes, and adds this edge the guessed edge set, ignoring the rest of the trace. This procedure is not only optimal in trace complexity, but, as it turns out, it is also computationally efficient.

We start by showing that First-Edge is able to reconstruct the full graph with maximum degree  X  using  X ( n  X  log n ) traces, under the cascade model we consider.

T HEOREM 4.1. Suppose that the source s  X  V is chosen uni-formly at random. Let G = ( V, E ) be a graph with maximum de-gree  X   X  n  X  1 . With  X  n  X  p l og n traces, First-Edge correctly returns the graph G with probability at least 1  X  1 poly( n )
P R OOF . Let e = { u, v } be any edge in E . The probability that a trace starts with u , and continues with v can be lower bounded by n  X  , that is, by the product of the probabilities that u is selected as the source, that the edge { u, v } is not removed from the graph, and that v is the first neighbor of u that gets infected. Therefore, if we run c n  X  p l n n traces, the probability that none of them starts with the ordered couple of neighboring nodes u, v is at most: Therefore, the assertion is proved for any constant c &gt; 2 . We notice that a more careful analysis leads to a proof that traces are enough to reconstruct the whole graph with high proba-bility. To prove this stronger assertion, it is sufficient to show the probability that a specific edge will be the first one to be traversed is at least 2 n  X  1  X  e  X  1  X  m in  X   X  1 , p . In fact one can even show that, for each d  X   X  , if the First-Edge algorithm has access to O d + p  X  1 n log n traces, then it will recover all the edges having at least one endpoint of degree O ( d ) . As we will see in our experimental section, this allows us to reconstruct a large fraction of the edges using a number of traces that is significantly smaller than the maximum degree times the number of nodes.

Finally, we note that the above proof also entails that First-Edge performs as stated for any waiting time distribution (that is, not just for the exponential one). In fact, the only property that we need for the above bounds to hold, is that the first node, and the first neighbor of the first node, are chosen independently and uniformly at random by the process.
H ere, we discuss a number of lower bounds for network infer-ence. Due to limited space, we omit proofs in this section we discuss the main ideas underlying these results.

We start by observing that if the source node is chosen adversar-ially  X  and, say if the graph is disconnected  X  X o algorithm can reconstruct the graph (traces are trapped in one connected compo-nent and, therefore, do not contain any information about the rest of the graph.) Moreover, even if the graph is forced to be connected, by choosing p = 1 2 ( that is, edges are traversed with probability a n algorithm will require at least 2  X ( n ) traces even if the graph is known to be a path. Indeed, if we select one endpoint as the source, it will take 2  X ( n ) trials for a trace to reach the other end of the path, since at each node, the trace flips an unbiased coin and dies out with probability 1 2 .

T his is the reason why we need the assumption that the epidemic selects s  X  V uniformly at random  X  we recall that this is also an assumption in [13]. Whenever possible, we will consider more re-alistic assumptions, and determine how this changes the trace com-plexity of the reconstruction problem.

We now turn our attention to our main lower bound result. Namely, for p = 1 , and assuming that the source is chosen uniformly at ran-dom, we need  X  ( n  X  1  X   X  ) traces to reconstruct the graph.
First, let G 0 be the clique on the node set V = { 1 , . . . , n } , and let G 1 be the clique on V minus the edge { 1 , 2 } .

Suppose that Nature selects the unknown graph uniformly at ran-dom in the set { G 0 , G 1 } . We will show that with O ( n probability that we are able to guess the unknown graph is  X  that is, flipping a coin is close to being the best one can do for guessing the existence of the edge { 1 , 2 } .

Before embarking on this task, though, we show that this re-sult directly entails that O ( n  X  1  X   X  ) traces are not enough for re-construction even if the graph has maximum degree  X  , for each 1  X   X   X  n  X  1 . Indeed, let the graph G  X  0 be composed of a clique on  X  + 1 nodes, and of n  X   X   X  1 disconnected nodes. Let G be composed of a clique on  X  + 1 nodes, minus an edge, and of n  X   X   X  1 disconnected nodes. Then, due to our yet-unproven lower bound, we need at least  X  ( X  2  X   X  ) traces to start in the large connected component for the reconstruction to succeed. The proba-bility that a trace starts in the large connected component is O H ence, we need at least  X  ( n  X  1  X   X  ) traces.

We now highlight some of the ideas we used to prove the  X ( n lower bound. First, we show a technical lemma that proves that the random ordering of nodes produced by a trace in G 0 is uniform at random, and that the random ordering produced by a trace in G  X  X lose X  to being uniform at random.

L EMMA 4.2. Let  X  0 be the random ordering of nodes produced by the random process on G 0 , and  X  1 be the random ordering of nodes produced by the random process on G 1 . Then, (i)  X  uniform at random permutation over [ n ] ; (ii) for each 1  X  a &lt; b  X  n , the permutation  X  1 conditioned on the vertices 1 , 2 ap-pearing (in an arbitrary order) in the positions a, b , has its other elements chosen uniformly at random; (iii) the probability p  X  1 has the vertices 1 , 2 appearing (in an arbitrary order) in the positions a &lt; b is equal to p a,b = 1+ d a,b ( n a = 1 , b = 2 , and | d a,b |  X  O ln n n + 1 b o therwise; moreover, P
T he proofs we omitted here will appear in an extended version of this paper (in preparation).
 The preceding Lemma can be used to prove the following result: L EMMA 4.3. If we disregard timestamps, we cannot distinguish G 0 and G 1 with probability more than 1 2 + o ( 1) using only m = To prove this Lemma, we bound the KL-divergences of the two dis-tributions generated by G 0 and G 1 from all their weighted geomet-ric means, thus obtaining a bound on the Chernoff information [6]. Then we can use the latter bound to show that  X  n 2 log 3 n are necessary.

The KL-divergence bounds are obtained by leveraging on the strong approximation bounds for the likelihoods of (most) traces given by Lemma 4.2.

We now complement the above lower bound (that holds only if we disregard the timestamps), with a full-fledged lower bound. To do so, we show that under a conditioning having high probability, the probability that a set of traces has higher likelihood in G in G 1 is 1 2  X  o ( 1) .

L EMMA 4.4. Let a set of m = n 2  X   X  traces be given. Let W be the waiting times in the traces. There exists an event E such that, (i) for both the unknown graph G 0 and G 1 , the probability of E is 1  X  o (1) , moreover, (ii) conditioning on E , the probability that L ( W ) &gt; L 1 ( W ) is equal to 1 2  X  o ( 1) .

Finally, Lemma 4.3 and Lemma 4.4 together allow us to obtain the following Theorem:
T HEOREM 4.5. Suppose that at most m = n 2  X   X  traces are given. Then, no algorithm can correctly guess whether the un-known graph is G 0 or G 1 with probability more than 1 2 + o ( 1) . As already noted, the lower bound in the above Theorem can be easily transformed in a  X ( n  X  1  X   X  ) lower bound, for any 1  X   X   X  n  X  1 .
In this section we study the problem of recovering the degree distribution of a hidden network and show that this can be done with  X ( n ) traces while achieving high accuracy, using, again, only the first edge of a trace.

The degree distribution of a network is a characteristic structural property of networks, which influences their dynamics, function, and evolution [21]. Accordingly, many networks, including the Internet and the world wide web exhibit distinct degree distribu-tions [10]. Thus, recovering this property allows us to make in-ferences about the behavior of processes that take place in these networks, without knowledge of their actual link structure.
Let  X  traces starting from the same node v be given. For trace i , let t i be the differences between the time of exposure of v , and the the time of exposure of the second node in the trace.

Recall that in the cascade model, the waiting times are distributed according to an exponential random variable with a known param-eter  X  . If we have  X  traces starting at a node v , we aim to estimate the degree of v the time gaps t 1 , . . . , t  X  between the first node and the second node of each trace.

If v has degree d in the graph, then t i ( 1  X  i  X   X  ) will be dis-tributed as an exponential random variable with parameter d X  [8]. Furthermore, the sum T of the t i  X  X , T = P  X  i =1 t i , is distributed as an Erlang random variable with parameters (  X , d X  ) [8].
In general, if X is an Erlang variable with parameters ( n,  X  ) , and Y is a Poisson variable with parameter z  X   X  , we have that Pr [ X &lt; z ] = Pr [ Y  X  n ] . Then, by using the tail bound for the Poisson distribution [5, 17], we have that the probability that T is at most (1 +  X  )  X   X  d X  is
Similarly, the probability that T is at least (1  X   X  )  X   X  We then have:
L et our degree inference algorithm return  X  d =  X  T  X  a s the degree of v . Also, let d be the actual degree of v . We have: We have then proved the following theorem:
T HEOREM 4.6. Provided that  X  ln  X   X  1  X  2 tr aces start from v , the degree algorithm returns a 1  X   X  multiplicative approximation to the degree of v with probability at least 1  X   X  .
A na X ve interpretation of the lower bound for perfect reconstruc-tion, Theorem 4.5, would conclude that the information in the  X  X ail X  of the trace  X  the list of nodes infected after the first two nodes, and their timestamps  X  is of negligible use in achieving the task of perfect reconstruction. In this section we will see that the opposite conclusion holds for important classes of graphs. We specialize to two such classes, trees and bounded-degree graphs, in both cases designing algorithms that rely heavily on information in the tails of traces to achieve perfect reconstruction with trace complexity O (log n ) , an exponential improvement from the worst-case lower bound in Theorem 4.5. The algorithms are quite different: for trees we essentially perform maximum likelihood estimation (MLE) of the entire edge set all at once, while for bounded-degree graphs we run MLE separately for each vertex to attempt to find its set of neighbors, then we combine those sets while resolving inconsisten-cies.

In Section 6 we provide one more example of an algorithm, which we denote by First-Edge + , that makes use of information in the tail of the trace. Unlike the algorithms in this section, we do not know of a theoretical performance guarantee for First-Edge + so we have instead analyzed it experimentally.
 It is natural to compare the algorithms in this section with the N ET I NF algorithm [13], since both are based on MLE. While N I
NF is a general-purpose algorithm, and the algorithms developed here are limited to special classes of graphs, we believe our ap-proach offers several advantages. First, and most importantly, we offer provable trace complexity guarantees:  X (log n ) complete traces suffice for perfect reconstruction of a tree with high probability, and  X (poly( X ) log n ) traces suffice for perfect reconstruction of a graph with maximum degree  X  . Previous work has not provided rigorous guarantees on the number of traces required to ensure that algorithms achieve specified reconstruction tasks. Second, our tree reconstruction algorithm is simple (an easy preprocessing step fol-lowed by computing a minimum spanning tree) and has worst-case running time O ( n 2  X  ) , where n is the number of nodes and  X  =  X (log n ) is the number of traces, which compares favorably with the running time of N ET I NF .
In this section we consider the special case in which the under-lying graph G is a tree, and we provide a simple algorithm that requires  X (log n ) complete traces and succeeds in perfect recon-struction with high probability. Intuitively, reconstructing trees is much simpler than reconstructing general graphs for the following reason. As noted in [13], the probability that an arbitrary graph G generates trace T is a sum, over all spanning trees F of G , of the probability that T was generated by an epidemic propagating along the edges of F . When G itself is a tree, this sum degener-ates to a single term and this greatly simplifies the process of doing maximum likelihood estimation. In practical applications of the network inference problem, it is unlikely that the latent network will be a tree; nevertheless we believe the results in this section are of theoretical interest and that they may provide a roadmap for ana-lyzing the trace complexity of other algorithms based on maximum likelihood estimation.
 Algorithm 1 T he tree reconstruction algorithm.
 Input: A collection T 1 , . . . , T  X  of complete traces generated by Output: An estimate,  X  G , of the tree. 1: for all pairs of nodes u, v do 2: Let c ( u, v ) be the median of the set {| t i ( u )  X  t 3: if  X  a node p and a pair of traces T i , T j such that t 4: Set c ( u, v ) =  X  . 5: Output  X  G = minimum spanning tree with respect to cost ma-
The tree reconstruction algorithm is very simple. It defines a cost f or each edge { u, v } as shown in Figure 1, and then it outputs the minimum spanning tree with respect to those edge costs. The most time-consuming step is the test in step 3, which checks whether there is a node p whose infection time precedes the infection times of both u and v in two distinct traces T i , T j such that the infection times of u and v are oppositely ordered in T i and T j . (If so, then G contains a path from p to u that does not include v , and a path from p to v that does not include u , and consequently { u, v } cannot be an edge of the tree G . This justifies setting c ( u, v ) =  X  in step 4.) To save time, one can use lazy evaluation and perform this test only for pairs u, v that are about to be inserted into the tree.
The analysis of the algorithm is based on the following outline: first, we show that if { u, v } is any edge of G , then c ( u, v ) &lt;  X  with high probability (Lemma 5.1). Second, we show that if { u, v } is any edge not in G , then c ( u, v ) &gt;  X   X  1 with high probability (Lemma 5.2). The edge pruning in steps 3 and 4 of the algorithm is vital for attaining the latter high-probability guarantee. When both of these high-probability events occur, it is trivial to see that the minimum spanning tree coincides with G .

L EMMA 5.1. If { u, v } is an edge of the tree G , then Algo-rithm 1 sets c ( u, v ) &lt;  X   X  1 with probability at least 1  X  c some absolute constant c 1 &lt; 1 .
 P ROOF . First, note that the algorithm never sets c ( u, v ) =  X  . This is because if one were to delete edge { u, v } from G , it would disconnect the graph into two connected components G u , G taining u and v , respectively. The infection process cannot spread from G u to G v or vice-versa without traversing edge { u, v } . Con-sequently, for every node p  X  G u , the infection time t i strictly between t i ( p ) a nd t i ( v ) in all traces. Similarly, if p  X  G then the infection time t i ( v ) occurs strictly between t in all traces.

Therefore, the value of c ( u, v ) is equal to the median of | t t ( v ) | over all the traces T 1 , . . . , T  X  . In any execution of the infec-tion process, if the first endpoint of edge { u, v } becomes infected at time t , then the opposite endpoint receives a timestamp t + X where X  X  Exp(  X  ) . Consequently the random variable | t i ( u )  X  t is an independent sample from Exp(  X  ) in each trace. The lemma now follows by an application of Chernoff X  X  bound.

The proof of the following lemma, while similar to that of the p receding one, is somewhat more involved. It is omitted for space reasons.

L EMMA 5.2. If { u, v } is not an edge of G , then Algorithm 1 sets c ( u, v ) &gt;  X   X  1 with probability at least 1  X  c absolute constants c 2 &lt;  X  and c 3 &lt; 1 .

Combining Lemmas 5.1 and 5.2, and using the union bound, we find that with probability at least 1  X  ( n  X  1) c  X  1  X  n  X  1 set of pairs ( u, v ) such that c ( u, v ) &lt;  X   X  1 coincides with the set of edges of the tree G . Whenever the n  X  1 cheapest edges in a graph form a spanning tree, it is always the minimum spanning tree of the graph. Thus, we have proven the following theorem.

T HEOREM 5.3. If G is a tree, then Algorithm 1 perfectly recon-structs G with probability at least 1  X  ( n  X  1) c  X  1  X  n  X  1 some absolute constants c 1 , c 3 &lt; 1 and c 2 &lt;  X  . This probabil-ity can be made greater than 1  X  1 /n c , for any specified c &gt; 0 , by using  X   X  c 4  X  c  X  log n traces, where c 4 &lt;  X  is an absolute constant.
In this section, we show that O (poly( X ) log n ) complete traces suffice for perfect reconstruction (with high probability) when the graph G has maximum degree  X  . In fact, our proof shows a some-what stronger result: it shows that for any pair of nodes u, v , there is an algorithm that predicts whether { u, v } is an edge of G with failure probability at most 1  X  1 /n c , for any specified constant c &gt; 0 , and the algorithm requires only  X (poly( X ) log n ) indepen-dent partial traces in which u and v are both infected. However, for simplicity we will assume complete traces throughout this section.
The basic intuition behind our algorithm can be summarized as follows. To determine if { u, v } is an edge of G , we try to recon-struct the entire set of neighbors of u and then test if v belongs to this set. We use the following insight to test whether a candidate set S is equal to the set N ( u ) of all neighbors of u . Any such set de-fines a  X  X orecasting model X  that specifies a probability distribution for the infection time t ( u ) . To test the validity of the forecast we use a strictly proper scoring rule [11], specifically the logarithmic scoring rule, which is defined formally in the paragraph following Equation (1). Let us say that a set S differs significantly from the set of neighbors of u (henceforth denoted N ( u ) ) if the symmet-ric difference S  X  N ( u ) contains a vertex that is infected before u with constant probability. We prove that the expected score as-signed to N ( u ) by the logarithmic scoring rule is at least  X ( X  greater than the score assigned to any set that differs significantly from N ( u ) . Averaging over  X ( X  4 log  X  log n ) trials is then suf-ficient to ensure that all sets differing significantly from N ( u ) re-ceive strictly smaller average scores.

The scoring rule algorithm thus succeeds (with high probabil-ity) in reconstructing a set R ( u ) whose difference from N ( u ) is insignificant, meaning that the elements of R ( u )  X  N ( u ) are usu-ally infected after u . To test if edge { u, v } belongs to G , we can Algorithm 2 B ounded-degree reconstruction algorithm.
 Input: A n infection rate parameter,  X  .
 Output: An estimate,  X  G , of G . 1: for all nodes u do 2: for all sets S  X  V \{ u } of at most  X  vertices do 3: for all traces T i do 4: Let S u i = { v  X  S | t i ( v ) &lt; t i ( u ) } . 5: if S u i =  X  then 6: Let score i ( S, u ) = 0 if u is the source of T i , other-7: else 8: score i ( S, u ) = log | S u i | X   X  P v  X  S u 9: Let score( S, u ) =  X   X  1  X  P i score i ( S, u ) . 10: Let R ( u ) = argmax { score( S, u ) } . 11: for all ordered pairs of vertices u, v do 12: if t i ( v ) &lt; t i ( u ) in at least  X / 3 traces and v  X  R ( u ) then 13: Insert edge { u, v } into  X  G . 14: Output  X  G . now use the following procedure: if the event t ( v ) &lt; t ( u ) occurs in a constant fraction of the traces containing both u and v , then we predict that edge { u, v } is present if v  X  R ( u ) ; this predic-tion must be correct with high probability, as otherwise the element v  X  R ( u )  X  N ( u ) would constitute a significant difference. Sym-metrically, if t ( u ) &lt; t ( v ) occurs in a constant fraction of the traces containing both u and v , then we predict that edge { u, v } is present if u  X  R ( v ) .

KL-divergence. For distributions p, q on R having density func-tions f and g , respectively, their KL-divergence is defined by One interpretation of the KL-divergence is that it is the expected difference between log( f ( x )) and log( g ( x )) when x is randomly sampled using distribution p . If one thinks of p and q as two fore-casts of the distribution of x , and one samples x using p and applies the logarithmic scoring rule , which outputs a score equal to the log-density of the forecast distribution at the sampled point, then D ( p k q ) is the difference in the expected scores of the correct and the incorrect forecast. A useful lower bound on this difference is supplied by Pinsker X  X  Inequality: where k X k TV denotes the total variation distance.

Quasi-timestamps and conditional distributions From now on in this section, we assume  X  = 1 . This assumption is without loss of generality, since the algorithm X  X  behavior in unchanged if we modify its input by setting  X  = 1 and multiplying the timestamps in all traces by  X  ; after modifying the input in this way, the input distribution is the same as if the traces had originally been sampled using the infection process with parameter  X  = 1 .

Our analysis of Algorithm 2 hinges on understanding the con-ditional distribution of the infection time t ( u ) , given the infection times of its neighbors. Directly analyzing this conditional distri-bution is surprisingly tricky, however. The reason is that u itself may infect some of its neighbors, so conditioning on the event that a neighbor of u was infected at time t 0 influences the probability density of t ( u ) in a straightforward way at times t &gt; t much less straightforward way at times t &lt; t 0 . We can avoid this  X  X ackward conditioning X  by applying the following artifice.
Recall the description of the infection process in terms of Dijk-stra X  X  algorithm in Section 3: edges sample i.i.d. edge lengths and the timestamps t ( v ) are equal to the distance labels assigned by Dijkstra X  X  algorithm when computing single-source shortest paths from source s . Now consider the sample space defined by the tu-ple of independent random edge lengths y ( v, w ) . For any vertices u 6 = v , define a random variable  X  t ( v ) to be the distance label as-signed to v when we delete u and its incident edges from G to obtain a subgraph G  X  u , and then we run Dijkstra X  X  algorithm on this subgraph. One can think of  X  t ( v ) as the time when v would have been infected if u did not exist. We will call  X  t ( v ) the quasi-timestamp of v (with respect to u ). If N ( u ) = { v 1 , . . . , v the set of neighbors of u , and if we sample a trace originating at a source s 6 = u , then the executions of Dijkstra X  X  algorithm in G and G  X  u will coincide until the step in which u is discovered and is as-signed the distance label t ( u ) = min j {  X  t ( v j )+ y ( v equation, it is easy to deduce a formula for the conditional distribu-tion of t ( u ) given the k -tuple of quasi-timestamps  X  Using the standard notation z + to denote max { z, 0 } for any real number z , we have The conditional probability density is easy to calculate by differen-tiating the right side of (3) with respect to t . For any vertex set S not containing u , let S h t i denote the set of vertices v  X  S such that  X  ity density function of t ( u ) satisfies It is worth pausing here to note an important and subtle point. The information contained in a trace T is insufficient to determine the vector of quasi-timestamps  X  t , since quasi-timestamps are defined by running the infection process in the graph G  X  u , whereas the trace represents the outcome of running the same process in G . Consequently, our algorithm does not have sufficient information to evaluate log f ( t ) at arbitrary values of t . Luckily, the equation holds for all v 6 = u , since  X  t ( v ) differs from t ( v ) only when both quantities are greater than t ( u ) . Thus, our algorithm has suffi-cient information to evaluate log f ( t ( u )) , and in fact the value score i ( S, u ) defined in Algorithm 2, coincides with the formula for log f ( t ( u )) on the right side of (5), when S = N ( u ) and  X  = 1 .
Analysis of the reconstruction algorithm. The foregoing dis-cussion prompts the following definitions. Fix a vector of quasi-taining u , let p S be the probability distribution on R with density function One can think of p S as the distribution of the infection time t ( u ) that would be predicted by a forecaster who knows the values for v  X  S and who believes that S is the set of neighbors of u . Letting N = N ( u ) , each timestamp t i ( u ) is a random sample from the distribution p N , and score i ( S, u ) is the result of applying the logarithmic scoring rule to the distribution f S ( t ) and the random sample t ( u ) . Therefore The key to analyzing Algorithm 2 lies in proving a lower bound on the expected total variation distance between p N and p following lemma supplies the lower bound. Its proof is omitted for space reasons.

L EMMA 5.4. Fix a vertex u , let N = N ( u ) be its neighbor set, and fix some S  X  V \{ u } distinct from N . Letting  X  ( S  X  N, u ) denote the probability that at least one element of the set S  X  N is infected before u , we have
Combining Pinsker X  X  Inequality with Lemma 5.4 we immedi-ately obtain the following corollary.

C OROLLARY 5.5. If N = N ( u ) and S is any set such that  X  ( S  X  N, u ) &gt; 1 / 4 , then for each trace T i the expected value of score i ( N )  X  score i ( S ) is  X ( X   X  4 ) .
 Using this corollary, we are ready to prove our main theorem. T HEOREM 5.6. For any constant c &gt; 0 , the probability that Algorithm 2 fails to perfectly reconstruct G , when given complete traces, is at most 1 /n c .

P ROOF . Let us say that a set S differs significantly from N ( u ) if  X  ( S  X  N ( u ) , u ) &gt; 1 / 4 . Let us say that an ordered pair of vertices ( u, v ) violates the empirical frequency property if the empirical fre-quency of the event t i ( v ) &lt; t i ( u ) among the traces T differs by more than 1 12 f rom the probability that t ( v ) &lt; t ( u ) in a random trace. Exponential tail inequalities for sums of i.i.d. ran-dom variables establish that when  X  is as specified in the theorem statement, with probability at least 1  X  1 /n c , there is no vertex u such that R ( u ) differs significantly from N ( u ) and no ordered pair ( u, v ) that violates the empirical frequency property. The proofs of these high-probability guarantees are omitted for space reasons.
Assuming that no set R ( u ) differs significantly from N ( u ) and that no pair ( u, v ) violates the empirical frequency property, we now prove that the algorithm X  X  output,  X  G , is equal to G . If { u, v } is an edge of G , assume without loss of generality that the event t ( v ) &lt; t ( u ) has probability at least 1/2. By the empirical frequency property, at least  X / 3 traces satisfy t i ( v ) &lt; t i v must belong to R ( u ) , since if it belonged to R ( u )  X  N ( u ) it violating our assumption that R ( u ) doesn X  X  differ significantly from N ( u ) . Therefore v  X  R ( u ) and the algorithm adds { u, v } to Now suppose { u, v } is an edge of  X  G , and assume without loss of generality that this edge was inserted when processing the ordered pair ( u, v ) . Thus, at least  X / 3 traces satisfy t i ( v ) &lt; t R ( u ) . By the empirical frequency property, we know that a random if v belonged to R ( u )  X  N ( u ) this would violate our assumption that R ( u ) does not differ significantly from N ( u ) . Hence v  X  N ( u ) , which means that { u, v } is an edge of G as well.
In the preceding sections we have established trace complexity results for various network inference tasks. In this section, our goal is to assess our predictions on real and synthetic social and informa-tion networks whose type, number of nodes, and maximum degree (  X  ) we now describe.

We use two real social networks, namely two Facebook subnet-works comprising 503 (  X  = 48 ) graduate and 1220 (  X  = 287 ) undergraduate students, respectively [18]. We also generate three synthetic networks, each possessing 1024 vertices, whose genera-tive models frequently arise in practice in the analysis of networks. We generated a Barabasi-Albert Network [4] (  X  = 174 ), which is a preferential attachment model, a G ( n,p ) Network [9] (  X  = 253 ) with p = 0 . 2 , and a Power-Law Tree , whose node degree distribu-tion follows a power-law distribution with exponent 3 (  X  = 94 ).
First, we evaluate the performance of the algorithm to recon-struct the degree distribution of networks without inferring the net-work itself (Section 4.3). Figure 1 shows the reconstruction of the degree distribution using  X ( n ) traces of the Barabasi-Albert Net-work and the two Facebook subnetworks. We used 10 n traces, and the plots show that the CCDF curves for the real degrees and for the reconstructed distribution have almost perfect overlap.
Turning our attention back to network inference, the  X ( n  X  lower-bound established in Section 3 tells us that the First-Edge algorithm is nearly optimal for perfect network inference in the general case. Thus, we assess the performance of our algorithms against this limit. The performance of First-Edge is notoriously predictable: if we use  X  traces where  X  is less than the total number of edges in the network, then it returns nearly  X  edges which are all true positives, and it never returns false positives.
 If we allow false positives, we can use heuristics to improve the First-Edge X  X  recall. To this end, we propose the following heuris-tic that uses the degree distribution reconstruction algorithm (Sec-tion 4.3) in a pre-processing phase, and places an edge in the in-ferred network provided the edge has probability at least p of being in the graph. We call this heuristic First-Edge + .

In First-Edge + , we use the memoryless property of the expo-nential distribution to establish the probability p of an edge per-taining to a network G . The algorithm works as follows. Consider a node u that appears as the root of a trace at time t 0 = 0 . When u spreads the epidemic, some node v is going to be the next in-fected at time t 1 , which was sampled from an exponential distri-bution with parameter  X  . At time t 1 , notice that there are exactly d  X  1 nodes waiting to be infected by u , and exactly d v  X  1 wait-ing to be infected by v , where d u and d v are the degrees of u and v respectively. At time t 1 any of these nodes is equally likely to Figure 2: F1 score of the First-Edge, First-Edge + , and N ET -I
NF algorithms applied to different real and synthetic networks against a varying number of traces. (best viewed in color) be infected, due to the memoryless property. Moreover, the next node w that appears in a trace after time t 1 is going to be infected larger prefixes of the trace: given a sequence u 1 ,  X  X  X  , u fected nodes starting at the source of the epidemic, the probability T herefore, for every segment of a trace that starts at the source, we infer an edge ( u, v ) if p ( u,v ) &gt; p , computed using the recon-structed degrees, where p is a tunable parameter. In our experi-ments we arbitrarily chose p = 0 . 5 .

Note that First-Edge+ may not terminate as soon as we have in-ferred enough edges, even in the event that all true positives have been found, an effect that degrades its precision performance. To prevent this, we keep a variable T , which can be thought of as the temperature of the inference process. Let M be a counter of the edges inferred at any given time during the inference process, and
T he exact probability depends on the number of edges between each of the nodes u 1 , . . . , u k and the rest of the graph.  X  E b e an estimate of the total number of edges, computed using the degree reconstruction algorithm in the pre-processing phase. We define T = M  X  tion, whenever we infer a new edge, we flip a coin and remove, with probability T , a previously inferred edge with the lowest estimated probability of existence. Thus, while the network is  X  X old X , i.e., many undiscovered edges, edges are rapidly added and a few are re-moved, which boosts the recall. When the network is  X  X arm X , i.e., the number of inferred edges approaches | E | , we carefully select edges by exchanging previously inferred ones with better choices, thereby contributing to the precision.

Figure 2 contrasts the performance of First-Edge, First-Edge + and an existing network algorithm, N ET I NF [13], with respect to the F1 measure. N ET I NF requires the number of edges in the net-work as input, and thus we give it an advantage, by setting the number of edges to the true cardinality of edges for each network. In Figures 2(a) and 2(b), we observe that, as First-Edge + and N
ET I NF are less conservative, their F1 performances have an ad-vantage over First-Edge for small numbers of traces, with First-Edge+ approaching the performance to N ET I NF . Interestingly, in Figure 2(c), we see that First-Edge and First-Edge+ achieve per-fect tree inference with roughly 5 , 000 traces, which reflects a trace complexity in  X ( n ) rather than in  X (log n ) , which is the trace complexity of Algorithm 1. 7 This result illustrates the relevance of the algorithms for special cases we developed in Section 5. Last, in proving lower-bounds for trace complexity, we frequently use random graphs as the worst-case examples. This is shown in Fig-ure 2(d), where neither our algorithms nor N ET I NF can achieve high inference performance, even for large numbers of traces.
In accordance with our discussion in Section 4.1, we confirm that, in practice, we need significantly fewer than n  X   X  traces for in-ferring most of the edges. It is perhaps surprising that First-Edge+, which is extremely simple, achieves comparable performance to the more elaborate counterpart, N ET I NF . In addition, while N I
NF reaches a plateau that limits its performance, First-Edge+ ap-proaches perfect inference as the number of traces goes to  X ( n  X ) . In the cases in which N ET I NF achieves higher performance than First-Edge+, the latter is never much worse than the former. This presents a practitioner with a trade-off between the two algorithms. For large networks, while First-Edge+ is extremely easy to imple-ment and makes network inferences (in a preemptive fashion) in a matter of seconds, N ET I NF takes a couple of hours to run to com-pletion and requires the implementation of an elaborate algorithm.
Our goal is to provide the building blocks for a rigorous foun-dation to the rapidly-expanding network inference topic. Previ-ous works have validated claims through experiments on relatively small graphs as compared to the large number of traces utilized, whereas the relation that binds these two quantities remains insuf-ficiently understood. Accordingly, we believe that a solid foun-dation for the network inference problem remains a fundamental open question, and that works like [20], as well as ours, provide the initial contributions toward that goal.

Our results have direct applicability in the design of network in-ference algorithms. More specifically, we rigorously study how much useful information can be extracted from a trace for network inference, or more generally, the inference of network properties without reconstructing the network, such as the node degree distri-
I n our experiments Algorithm 1 consistently returned the true edge set without false positives with O (log n ) traces for various networks of various sizes. Therefore, in the interest of space we omit the data from these experiments. bution. We first show that, to perfectly reconstruct general graphs, nothing better than looking at the first pair of infected nodes in a trace can really be done. We additionally show that the remainder of a trace contains rich information that can reduce the trace com-plexity of the task for special case graphs. Finally, we build on the previous results to develop extremely simple and efficient recon-struction algorithms that exhibit competitive inference performance with the more elaborate and computationally costly ones.
