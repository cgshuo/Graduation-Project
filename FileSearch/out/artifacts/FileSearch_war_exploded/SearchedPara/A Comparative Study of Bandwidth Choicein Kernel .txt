 A critical task in Bayesian learning is estimation of the probability distributions of attributes in datasets, especially when the attributes are numeric. Tradition-ally, the numeric attributes are handled by discretization [1]. These methods are usually simple and computationally efficient. However, they suffer from some basic limitations [2, 3]. An alternative to calculating probability estimates for numeric attributes using discretized intervals is to estimate the probabilities di-rectly, using an estimate of the point-wise density distribution. Both parametric and nonparametric density estimation methods have been developed.

Parametric density estimation imposes a parametric model on the observa-tions. For example, the parameters for a Gaussian model are its sufficient statis-tics, the mean and variance. Normally simple parametric models do not work very well with Bayesian classification [4], as the real distributions do not exactly fit specific parametric models.

Some estimation methods, including Gaussian mixture models, use subsets of the data to obtain local models, then mix these models to obtain the density estimate for all observations. In contrast, Kernel Density Estimation estimates the probability density function by imposing a model function on every data point and then adding them together. The function applied to each data point is called a kernel function. For example, a Gaussian function can be imposed on every single data point, making the center of each Gaussian kernel function the data point that it is based on. The standard deviation of the Gaussian kernel function adjusts the dispersion of the function and is called a bandwidth of the function.

Given sufficiently large sample data, K DE can converge to a reasonable esti-mate of the probability density. As there are no specific finite parameters imposed on the observations, KDE is a nonparametric method.

The univariate KDE [5, 6] can be expressed as: where K ( . ) is the density kernel; x is a test instance point; X i is a training instance point, which controls the position of the kernel function; h is the bandwidth of the kernel, which controls the dispersion of each kernel; and n is the number of data points in the data. For a univariate Gaussian kernel K (  X  )= 1  X 
Naive Bayes is a widely employed effective and efficient approach for classi-fication learning, in which the predicted class label y ( x ) of a test instance x is evaluated by y ( x )= argmax c P ( c )  X  d i =1 P ( x i | c ) , where P ( c )istheesti-mated class probability, d is the number of attributes, x i is the i X  X h attribute of instance x ,and P ( x i | c ) is the estimated probability (or probability density) of x i given the class. KDE (Equation (1)) can be used to estimate the class condi-tional probabilities for numeric attributes. Because the Naive Bayesian classifier considers each attribute independently, we use only univariate kernel density estimation in this paper.

It is known that the specific choice of kernel function K is not critical [7]. The key challenge is the choice of the bandwidth. A bandwidth value which is too small will give a too detailed curve and hence leads to an estimation with small bias and large variance. Large bandwidth leads to low variance at the expense of increased bias.

Many bandwidth selection schemes in kernel density estimation have been studied mainly for optimizing the mean squared error loss of the estimation which supports good density curve fitting. However, bandwidth selection schemes are still not extensively studied in the classification context applying 0-1 loss criteria.
We look at the seven most commonly used b andwidth selection schemes in the statistical community plus two very simple schemes, using 52 datasets. It is shown that the choice of bandwidth dramatically affects the accuracy results of classifi-cation. An appropriate bandwidth selection scheme can achieve statistically sig-nificantly better classification performance than a commonly used discretization method. Surprisingly, the two simple bandwidth selection schemes both achieved good performance, whereas the more sophisticated and computationally expensive schemes delivered no improvemen t in classification performance. Background. Intuitively, it is assumed that there is a positive correlation be-tween the accuracy of the probability estimates and the accuracy of classification. Friedman [8] challenged this assumption and states that more accurate proba-bility estimates do not necessarily lead to better classification performance and can often make it worse.

Unfortunately, most bandwidth selection research considers the assumption to be true and attempts to achieve the highest possible probability estimation accuracy. These schemes are often based on a mean squared error (MSE) criteria, instead of a 0-1 loss criteria.

To the best of our knowledge, there is no p ractical bandwidth selection scheme that focuses on improving the classificat ion accuracy, rathe r than the accuracy of the probability estimates. A recent paper [9] explores the theory of bandwidth choice in classification under limited conditions. It states that the optimal size of the bandwidth for 0-1 loss based estimation is generally the same as that which is appropriate for squared error based estimation.

Generally speaking, KDE bandwidth choice in the context of classification under 0-1 loss is more difficult than bandwidth choice under MSE loss. For example, we consider using Cross-Validation to chose optimal bandwidths in Naive Bayes, using class labels as the supervised information. Every evaluation under 0-1 loss (according to the class label) should use all attributes in the dataset. This is a global optimization problem in which the optimal bandwidth for one attribute may interact with those for other attributes. It is different to the MSE criteria which only uses the attribute under consideration.

In this section we give some theoretical descriptions of the mean squared error criteria and describe 7 bandwidth selection schemes that are based on this criteria. We also discuss two schemes which are not theoretically related to MSE. Mean Squared Error Criteria. In probability density estimation, the Mean Squared Error (MSE) or Mean Integrated Squared Error (MISE) are the most used density estimation error criteria, The integral is in the range of x , to measure how well the entire estimated curve  X  f approximates the real curve f . The expectation operation averages over all pos-sible samplings. From this equation, we can get MISE (  X  f )= Bias 2 [  X  f ( x )] dx + E 2 [  X  f ( x )]. This equation is the starting point of the bandwidth selection scheme UCV we discuss below.
 We process E [  X  f ( x )] first by using Equation (1). This leads to E [  X  f ( x )] = test point x , we regard each X i as an independent and identically distributed random variable with distribution f . Making a simple variable substitution equation. The first term of f ( x  X  ht ) is canceled out by the negative f ( x ). The sec-ond term is also canceled out because the K ( t ) in the integral is a symmetric func-where R ( g )= g 2 ( x ) dx and  X  2 ( g )= x 2 g ( x ) dx .

In a similar way, we can get, Var [  X  f ( x )] = 1 nh R ( K ) . The elementary Equa-tion (2) becomes an asymptotic form, as the error term in Taylor expansion is the higher-order term of h , which monotonously decreases when samples grow. The asymptotic mean integ rated squared error is, This equation is the starting point for the bandwidth selection schemes BCV, STE and DPI, which are discussed below.
 Unbiased Cross-Validation (UCV) Scheme. The method of Unbiased Cross-Validation [10] is based on the elementary Equation (2). It is also called least squares cross-validation. UCV obtains a score function to estimate the per-formance of candidate bandwidth. In pra ctice, UCV minimizes the integrated square error, the Equation (4), which uses one realization of samples from the underlaying distribution f .
 where R ( g ) is similar to Equation (3).

Notice the first term in Equation (4) is only related to the estimated  X  f ( x ), so it is easy to process giv en a specific bandwidth  X  h . The third term is independent of the estimated  X  h and remains constant for all estimations, so it can be ignored. mean of  X  f ( x ) with respect to x .
 If we get n samples of x , for the sake of obtaining a stable estimation of E [  X  f ( x )], we can use a Leave-One-Out method to get an n-points estimation value of the  X  f ( x ). The Leave-One-Out method estimates the value of  X  f ( x i )by leaving the x i out and using the other n-1 points of x . This is why this method is called a Cross-Validation. We use  X  f  X  i ( x i ) to express this Leave-One-Out esti-Substituting this to Equation (4) we construct a score function in the sense of ISE. Now for some specific candidate bandwidth  X  h , we can give a unbiased cross validation score for the candidate bandwidth  X  h as, We can use a start bandwidth as a referen ce estimation, and make a brute-force search near this reference bandwidth with respect to the minima of UCV score function.
 Normal Reference Density (NRD-I, NRD and NRD0) Schemes. Nor-mal Reference Density [5] scheme is also called the Rule of Thumb scheme. It is based on Equation (3). To minimize AMISE, a simple first order differential can be used on Equation (3) towards the bandwidth h and setting the differential to zero. The optimal bandwidth is: This result still depends on the unknown density derivative function f ( x ), which will depend on h recursively again. Normal Reference Density scheme simplifies this problem by using a parametric model, say, a Gaussian to estimate f ( x ). Compared with the Cross-validation selection, this is a straightforward method and can lead to an analytical expression of bandwidth  X  h =1 . 06  X   X n  X  1 / 5 , where n is the number of samples and  X   X  is the estimated normal distribution standard deviation of the samples.

This bandwidth selection scheme is a classic one. We use this bandwidth as a standard bandwidth in our experiments. We call this scheme NRD-I .

A more robust approach [5] can be applied by considering the interquar-tile range (IQR). The bandwidth is calculated from the minimum of standard deviation and standard IQR:  X  h =1 . 06 min ( X   X , IQR/ 1 . 34) n  X  1 / 5 . This proce-dure [5] helps to lessen the risk of oversmoothing. We call the bandwidth the NRD bandwidth in this paper. A smaller version of NRD suggested in R [11] is  X  h =0 . 9 min ( X   X , IQR/ 1 . 34) n  X  1 / 5 . We call this bandwidth the NRD0. Biased Cross-Validation (BCV) Scheme. Biased cross-validation uses Equation (3) as the basis of the score function. Scott and Terrel [12] develop an estimation of R ( f ) in Equation (3), using  X  R ( f )= R (  X  f )  X  1 nh 5 R ( K ) , where f ,  X  f and K are second-order derivatives of distribution and kernel respectively. The right hand side of th is estimation can be evaluated given a specific bandwidth  X  h . Substituting the  X  R ( f ) to Equation (3), we can get a new score function,
A exhaustive search procedure similar to UCV scheme can be applied to find the optimal bandwidth.
 Direct-Plug-In (DPI) Scheme and Solve-The-Equation (STE) Scheme.
 The Direct-Plug-In scheme [13] is a more complicated version of the Normal Reference Density scheme. It seeks R ( f ) by estimation of R ( f ). This problem continues because the R ( f ( s ) ) will depend on R ( f ( s +2) ). Normally, for a specific s , R ( f ( s +2) ) is estimated by a simple parametric method, to obtain R ( f ( s ) )and so on. We call Direct-Plug-In Scheme the DPI in our experiments.
 Notice that Equation (5) is a fixed point equation h = F ( h ), where F ( h )= [6, 13] is applied by solving the fixed point of F ( h ). We call Solve-The-Equation scheme the STE in our experiments.
 Two Very Simple (WEKA and SP) Schemes. We use two very simple bandwidth selection schemes. These tw o schemes are both based on the range of data divided by a measure of the size of the samples. There is less theoretical consideration [4, 14, 15] of these met hods compared with the other methods discussed above. They merely conform to the basic requirement in KDE that when the number of samples approaches infinity, the bandwidth approaches zero.
Oneschemeuses zero as n increases, where n is the number of samples, range ( x ) is the range of values of x in training data. This scheme is used in WEKA [14], with some calibration that  X  h should be no less than 1 6 of the average data interval, which avoids  X  h becoming too small compared with the average data interval. We call this scheme WEKA.

The other scheme is a very old scheme[16]. The basic principle of this equation does not have very strong theoretical basis [15]. However it was widely used in the old version of S-PLUS statistic package (up to version 5.0) [17, page 135]. We call it the SP scheme. 3.1 Data and Design In addition to the nine bandwidth sel ection schemes described in Section 2, the widely used MDL discretization method [1] was also used as a performance reference. The Naive Bayesian classifier was the classifier used for all schemes being evaluated. Every statistic sample (every dataset, every experiment trial and fold) and every piece of classifier code is the same for all schemes. The only difference between each scheme in the classifier algorithm is the bandwidth of the kernel.
The fifty-two datasets used in the experiments were drawn from the UCI machine learning repository [18] and the web site of WEKA [14]. We use all the datasets that we could identify from these places, given the dataset has at least one numeric attribute and has at least 100 instances. Table 1 describes these datasets. Any missing values occurring in the data for numeric attributes were replaced with the mean average value for that attribute.

Each scheme was tested on each dataset using a 30-trial 2-fold cross validation bias-variance decomposition. A large number of trials was chosen because bias-variance decomposition has greater accu racy when a sufficiently large number of trials are conducted [19]. Selecting two folds for the cross-validation maximizes the variation in the training data from trial to trial.

Thirty trials and two folds yields sixty Naive Bayesian classification evalua-tions for each dataset. For these evalua tions we recorded the mean training time, mean error rate, mean bias and mean variance. Kohavi and Wolpert X  X  method [20] of bias and variance decomposition was employed to determine the bias and variance based on the obtained error rate.

Since there are nine alternative KDE classifiers and one discretization classi-fier, we get ten comparators of the performance measure for each dataset.
After the classification performance comparison, we also produce a statistic for the bandwidth distribution for alternative bandwidth selection schemes. The fifty-two datasets contain 1294 numeric attributes collectively. Every numeric attribute has at least two and at most 26 class labels. Since we evaluate the KDE for every class conditional probab ility, there are 10967 class conditional probability evaluation objects. Each of these evaluation objects produces 60 different realization samples by the 30 trails 2 fold cross-validation. Every band-width selection scheme is applied to each realization of the conditional probabil-ity evaluation objects, and produces an estimated bandwidth for that realization.
These bandwidths are transformed to a ratio to a standard bandwidth. We use the NRD-I bandwidth as the standard. By using these bandwidth ratios, we get a statistical distribution of the bandwidth size for each scheme. 3.2 Observations and Analysis Classification Error, Bias and Variance. We use Friedman X  X  method [21] to rank classification error, bias and variance. The scheme that performs the best is ranked 1, the second best is ranked 2 and so forth. The mean rank of classification accuracy an d time measure (real time) are summarized in Figure 1 as the shaded bars. Since the bandwidth calculations are carried out during training, the computational time for the test stage is essentially the same for all schemes and therefore is not reported.
 A win/tie/lose record (w/t/l) is calculated for each pair of competitors A and B with regard to a performance measure M. The record represents the number of datasets in which A wins loses or ties with B on M. The win/tie/loss records are summarized in Table 2.

We also apply statistical comparison methods of multiple classifiers over mul-tiple data sets recommended by Demsar [22].

The null hypothesis was rejected for all Friedman tests (using the 0.05 critical level) conducted on error, bias and variance, so we can infer that there exists significant difference among all ten schemes.

Having determined that a significant diff erence exists, the post-hoc Nemenyi test was used to identify which pairs of schemes differ significantly. The results of this test(using the 0.05 critical level) are illustrated by the line segments accompanying each bar in the graph in Figure 1. The length of these lines indicate the critical difference, and the performa nce of two methods are considered to be significantly different if the difference between their mean rank is greater than the critical difference (i.e. their vertical line segments do not overlap).
Figure 1 and Table 2 show that the more sophisticated bandwidth selection schemes investigated do not yield improved performance over simpler schemes, although they are far more computationally expensive. The poorest performer was BCV, which was statistically significantly worse than the more simplistic SP scheme (with w/t/l record 15/1/36 ) and WEKA scheme (with w/t/l record 11/0/41). UCV was also determined to be statistically significantly worse than the SP scheme (with /w/t/l record 18/0/36). The computational time costs of the four sophisticated schemes are far more than the others.
 The UCV scheme achieved low bias, but high variance, as stated by its name. Conversely, BCV achieved low variance, but high bias. Neither the SP scheme X  X  bias nor its variance was particularly high or low, and it was found to be sta-tistically significantly better than the discretization method and the two worst sophisticated bandwidth selection schemes, UCV and BCV. This analysis shows that the choice of bandwidth dramatically affects the accuracy results of classi-fication. The more sophisticated schemes can not guarantee good classification performance. Trade-off between bias and variance performance is essential to improve upon classification accuracy.

This analysis also shows that only one bandwidth selection scheme (the SP scheme) gives statistically better performance than a classical discretization method. It suggests that KDE can achieve statistically significantly better performance in classification, but the bandwidth selection schemes in classifica-tion behave different with traditional sophisticated bandwidth selection schemes. More theoretical researches are need ed for kernel density estimation in classification.
 Distribution of the Bandwidth. The distribution of the bandwidth size for each scheme is illustrated in Figure 2. By comparing Figure 1 and Figure 2 we can see that the bandwidth of BCV and WEKA is statistically larger than others. This gives them a small variance and large bias in classification. By contrast, NRD0, SP, STE and DPI tend to have smaller bandwidths. This gives them a relatively small bias and large variance in classification. We can see that there is a transition range (from approximately 0.5 to 1.5 times of NRD-I bandwidth) that indicates a change in tendency of bias-variance trade off, from a low-bias high-variance to a high-bias low-variance profile. This transition range is narrow. This relatively narrow distribution range shows that classification performance is more sensitive to the size of the bandwidth than was first thought. The more simplistic and less computationally intensive bandwidth selection schemes performed significantly better compared to some of the more sophis-ticated schemes in Naive B ayesian Classification. A k ernel density estimation method can significantly outperform a classical discretization method, but only when appropriate bandwidth selection schemes are applied.

Our experiments and analysis also show that an unsuitable bandwidth value can easily give poor classification performance. In a relatively narrow distribu-tion range, we find that the bias-variance trade off changes, from low-bias and high-variance to high-bias and low-variance. Comparison of the bandwidth dis-tribution patterns with error performance suggests that bandwidths within the range of 0.5 to 1.5 times NRD-I standard bandwidth are preferable.
 The authors thank Dr. Eibe Frank and Dr.Leonard Trigg for the helpful discus-sion and Mr. Colin Enticott for the support of cluster computing. This work is supported by Australian Research Council grant DP0770741.

