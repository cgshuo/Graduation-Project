 1. Introduction become as crucial as ever. We motivate the importance of privacy protection with two distinct applications in the growing section will offer a more technical review in the context of the state of the art.
 1.1. Statistical disclosure control
A microdata set is a database table whose records carry information concerning individual respondents, either people or companies. This set commonly contains key attributes or quasi-identi address, age, gender, height and weight. Additionally, the data set contains con respondent, such as salary, religion, political af fi liation or health condition. The classi may ultimately rely on the speci fi c application and the privacy requirements the microdata set is intended for. make it a widely popular criterion in the SDC literature. 1.2. Privacy in location-based services means of clustering of user coordinates, for a large integer k .
 location-based information that does not require perfectly accurate coordinates, say weather reports, traf accuratelocationinformationcouldbeexploitedbytheproviderstoinferuseridentities, possiblywiththehelp ofanaddressdirectory privacy may be applied to all devices, devices with a home location in more densely populated areas should belong to smaller service based on the perturbation of their home locations. 1.3. Contribution
In this work, we develop a multidisciplinary solution to the important problems of k -anonymous microaggregation and based information.

Our main contribution is two variations of a k -anonymous aggregation algorithm, one of which is particularly suited to the respondents indistinguishable from each other in the published database. Our algorithm in its second variation also exhibits excellent performance in the problem of clustering or macroaggregation, where k may take on arbitrarily large values. endowed with a numerical method to solve nonlinear systems of equations based on the Levenberg utility, inherent in any data perturbative method for privacy protection. Precisely, while maintaining the same k -anonymity constraints, our algorithm outperforms the state-of-the-art microaggregation MDAV by a signi margin, of up to 14 % in some cases, at no anonymity cost whatsoever.

Under a more general perspective, the algorithm presented here is a distortion-optimized, probability-constrained quantizer design method, applicable to a wide range of engineering problems, among which the most widely known match, within the privacy literature, would be microdata anonymization. Indeed, we choose microaggregation as the main motivating example, and consequently one of the experimental cases analyzed. Less common in the privacy literature is the problem of k -anonymous information. Essentially, we consider location-aware devices, commonly operative near a for it hints at the possibility of its applicability well beyond SDC with small k . 1.4. Contents general problem of k-anonymous aggregation as a quantizer design problem, and develops a novel modi on pseudonymization and microaggregation. Examples and empirical results are reported in Section 5 for various scenarios and detailed in Appendix A . 2. Background and state of the art
SDC and privacy in LBSs. Secondly, it offers a few mathematical preliminaries and a brief summary of the fundamentals of traditional quantizer design, in preparation for the formulation of our own work. 2.1. State of the art on statistical disclosure control
We now proceed to sketch a brief overview of the well-established over the next subsection.
 the help of external information. Microaggregation attains k-anonymity by selecting groups of at least k quasi-identi replacing them with a common representative value for each group. The ultimate goal of microaggregation algorithms is to among all possible k -anonymous aggregations, that with maximum data utility; in other words, that introducing the minimum introduction that the concept of k -anonymity, originally proposed by the SDC community [5,6] , is a widely popular privacy linking identities with quasi-identi fi ers, and another linking quasi-identi the unperturbed and the perturbed data, possibly in rather different alphabets.

Distortion-optimal multivariate microaggregation was proved to be NP-hard [12] , and its combinatorial complexity, even with a small number of samples, precludes brute-force search. 2 categorized into fi xed-size and variable-size methods, according to whether all groups but one have exactly k elements with common perturbed quasi-identi fi ers. The maximum distance (MD) algorithm [9] and its less computationally demanding variation, the maximum distance to average vector (MDAV) algorithm [10,13 the  X  -Approx algorithm [11] , the minimum spanning tree (MST) algorithm [16] , the variable MDAV (VMDAV) algorithm [17] and exploit projections onto one dimension but are reported to yield a much higher disclosure risk [19] . de fi nition of this privacy criterion establishes that complete reidenti same tuple of perturbed key attribute values. However, if the records in the group also share a common value of a con attribute, the association between an individual linkable to the group of perturbed key attributes and the corresponding con against the exploitation of the difference between the prior distribution of con posterior conditional distribution of a group given the observed, perturbed key attributes. For example, imagine that the
This vulnerability motivated the proposal of enhanced privacy criteria, some of which we proceed to sketch, along with the k -anonymity requirement, it is required that there be at least p different values for each con slight generalization called l -diversity [22,23] was de respect to p -sensitivity is that group of records must contain at least l skewness attacks. Furthermore, both are still susceptible to similarity attacks , in the sense that while con t -closeness if for each group sharing a common tuple of perturbed key attribute values, the distance between the posterior threshold t . As argued in Ref. [25] , to the extent to which the within-group distribution of con occur within a group that does not occur in the entire dataset.
 de message given an observed cryptogram. Conceptually, the privacy risk de t -closeness requirement, over all aggregated groups. The main advantage of this information leads to a mathematical formulation of the privacy  X  utility trade-off that generalizes a well-known, extensively studied
In conclusion, we would like to emphasize that despite the shortcomings of k -anonymity and its enhancements as a measure of k -anonymity in its numerous varieties, is the be-all and end-all of database anonymization [33] . 2.2. State of the art on privacy in location-based services
Because our contribution is essentially a novel algorithm for k -anonymous microaggregation and clustering, along with an refers. An example would be the query  X  Where is the nearest bank from my home address?
LBS provider to recognize the user ID, there exists a patent privacy risk. Namely, the provider could pro locations, the contents of their queries and their activity.
 provider cannot know the user ID, but merely the identity ID
TTP may act as a pseudonymizer by supplying a pseudonym ID between the pseudonym ID  X  and the actual user ID. A convenient twist to this approach is the use of digital credentials [34 granted by a TTP, namely digital content proving that a user has suf allow users to access a service with a certain degree of anonymity.

Unfortunately, this approach does not prevent the LBS from attempting to infer the real identity of a user by linking their location to, say, a public address directory, for instance by using restricted space identi capable of collecting queries for diverse services, which unfortunately might facilitate user pro requirement in certain ad hoc networks.
 sometimes referred to as obfuscation, presents the inherent trade-off between data utility and privacy common to any perturbative privacy method. A wide variety of fairly sophisticated perturbation methods for LBSs has been proposed [38
There exist TTP-free methods relying on the collaboration between multiple users to achieve group anonymity. Many positions.
 query-response functions in the form of a fi nite lookup table of precomputed answers, and are burdened with a signi computational overhead.

An approach to preserve user privacy to a certain extent, at the cost of traf proposed and implemented, with various degrees of sophistication [48 of technical considerations regarding bogus traf fi c generation for privacy [54] .
 contact the LBS provider directly from then on, as often as needed, using that centroid. also combine some of the ideas presented. While they do not assume that users necessarily trust each other, they do require provider. The fi rst example [61] envisions an architecture where two entities have access to user identities and location information separately. Accordingly, it is assumed that no information that could compromise location anonymity is exchanged not trust each other, nor require a TTP or location anonymizer. 2.3. Mathematical preliminaries and background on traditional quantization particular values they take on. Probability density functions (PDF) and probability mass functions PMF) are denoted by p and over a fi nite set of data points { x 1 , ... , x n }, simply by de by a value  X  x of a discrete r.v.  X  X . The quantizer map mapping the index Q into a value  X  X that approximates the original data, so that with an example where the r.v. X takes on values in  X  2 . Depending on the distinction is made between clustering and microaggregation, to emphasize the large and small size of the resulting cells, respectively.
 the set of fi nite bit strings, suitable for storage and transmission in computer systems. We re application requirement. Clearly, quantization comes at the price of introducing a certain amount of distortion between the original data X and its reconstructed version  X  X . In mathematical terms, we de measure , and consider the expected distortion D =E dX ;  X  that is, D = E  X  X  X   X  X  X  2 , popular due to its mathematical tractability.
 quantizers must satisfy the following conditions:  X 
Nearest-neighbor condition. Given a reconstruction function that is, each value x of the data is assigned to the index corresponding to the nearest reconstruction value.  X 
Centroid condition. In the special case when MSE is used as a distortion measure, given a quantizer q(x) , the optimal reconstruction function  X  x q  X  X  is given by that is, each reconstruction value is the centroid of a quantization cell.
 algorithm [2,3] , a quantizer design algorithm based on the alternating optimization of q(x) given such as the k -means method. 3 ( [66] ,  X II.E,  X III).

Lastly, we would like to remark that the literature abounds with sophisticated modi is undoubtedly that of automated document classi fi cation [74,75] . Conceivably, these modi next section, consists in a signi fi cant modi fi cation of a traditional quantization algorithm. 3. A modi fi cation of the Lloyd algorithm for k -anonymous aggregation 3.1. Formal statement of the k-anonymous aggregation problem as a quantizer design problem
In this section, we proceed to formalize the problem of k -anonymous aggregation in a generic fashion, as a probability-constrained, distortion-optimized quantizer design problem. Microaggregation, in the context of SDC, is a well-established problem with extensive literature, and consequently suf fi applicability of our work beyond that, in Section 4 , we shall provide a second example of k -anonymous aggregation, or more precisely, clustering, where the need for larger values of k arises naturally, this time in the context of LBSs.
In either case we are concerned with the general problem of constructing groups of k , possibly multivariate, data samples, and then fi nding a common representative value for each group minim izing a distortion measure between the original data and such k -anonymous representation. In the SDC application, the data samples are simply the quasi-identi respondents, and the common representative value replacing them in the published version of the data is the means to attain
We may now proceed to formally consider the design of minimum-distortion quantizers satisfying cell probability constraints, possibly discrete or continuous. The quantizer q(x) assigns X to a quantization index Q in a predetermined size. The reconstruction function  X  xq  X  X  maps Q into the aggregated key attribute value an approximation to the original data, de fi ned in an arbitrary alphabet alphabet X .

For any nonnegative (measurable) function dx ;  X  x  X  X  , called distortion measure, de
D =E dX ;  X  X , that is, a measure of the discrepancy between the original data values and their reconstructed values, which re fl ects the loss in data accuracy. An important example of distortion measure is dx
Alternatively, dx ;  X  x  X  X  may represent a semantic distance in an ontological hierarchy, and clustering problem is formalized, from a more general perspective, by means of probability constraints p q , for any given PMF p 0 ( q ).

In the important example of microdata k -anonymization, let n be the total number of records to be microaggregated. The k -anonymity constraint could be translated into probability constraints by setting j ensures that np 0 ( q )  X  k . More generally, for a given probability p suited to continuous probability models of user locations.
 Given a distortion measure dx ;  X  x  X  X  and probability constraints p quantization cells j Q j X  , we wish to design an optimal quantizer q *( x ) and an optimal reconstruction function that they jointly minimize the distortion D while satisfying the probability constraints.
We would like to stress that our formulation of the probability-constrained quantization problem may also semantic distances.

The remainder of this section investigates the problem of distortion-optimized, probability-constrained quantization formulated here, and motivated as the functionality implemented by the microaggregation process in the case of SDC, and by a nonlinear systems of equations inspired by the Levenberg  X  3.2. Optimization steps
Next, we propose heuristic optimization steps for probability-constrained quantizers and reconstruction functions, analogous modify the conventional Lloyd algorithm by applying its underlying alternating optimization principle to these steps. quantization: In the special case when MSE is used as distortion measure, this is the centroid step (2).
On the other hand, we may not apply the nearest-neighbor condition in conventional quantization directly, if we wish to guarantee the probability constraints p Q ( q )= p 0 ( q ). We introduce a cell cost functionc : cell boundaries appropriately to satisfy the probability constraints. Speci function c(q) , we propose the following cost-sensitive nearest-neighbor step :
Voronoi cells [65,66] determined by the traditional nearest-neighbor condition, it is routine to check that our modi de fi nes an additively weighted Voronoi partition composed of convex polytopes.

The step just proposed naturally leads to the question of how to p ( q )= p 0 ( q ) are satis fi ed, given a reconstruction function very successful in all of our experiments, including those reported in Section 5 . Speci c(q) at a time. To do so more ef fi ciently, we exploit the fact that only the coordinates of p may be changed, and compute the negative semide fi nite approximation in Frobenius norm to the Jacobian. 3.3. A modi fi cation of the Lloyd algorithm for probability-constrained quantization nearest-neighbor (1) and the centroid optimality (2) conditions. These are necessary but not suf algorithm can only hope to approximate a jointly optimal pair q *( x ), nonincreasing. We also mentioned that experimentally, however, the Lloyd algorithm very often shows excellent performance ( [66] ,  X II.E,  X III).

Bear in mind that our modi fi cation of the nearest-neighbor condition (Eq. 4 ) for the probability-constrained problem is a optimization principle, albeit with a more sophisticated nearest-neighbor condition (Eq. 4 ), and de
Lloyd algorithm for probability-constrained quantization. In recognition of the celebrated quantization design method which inspired this work, we shall call our algorithm the probability-constrained Lloyd (PCL) algorithm : 1. Choose an initial reconstruction function  X  x (q) and initial cost function c(q). 2. Update c ( q ) to satisfy the probability constraints p the end of Section 3.2 , setting the initial cost function as the cost function at the beginning of this step. 3. Find the next quantizer q (x) corresponding to the current 4. Find the optimal  X  x ( q ) corresponding to the current q (x), according to Eq. (3) . 5. Go back to Step 2, until an appropriate convergence condition is satis The initial reconstruction values may simply be chosen as j quantizer cells cannot have zero volume. Note that the numerical computation of c(q) in Step 2 should bene maximum number of iterations is exceeded.
 It is important to remark that the numerical computation of the costs in the PCL algorithm just described, by means of the function of these costs, as these numerical methods estimate partial derivatives through values of k , and, more generally, to any sort of probability constraint.
 property implying that the PCL algorithm can be used to improve the performance of any other aggregation algorithm, just by or small k . 3.4. A noisy variation for the special case of microaggregation or small k
We argued that the PCL algorithm described in Section 3.3 was only suitable for probability constraints leading to a large number of points per cell, or continuous models of the data, because the derivative-based numerical methods involved in the adjustment of the costs c(q) assumed a smooth dependence between them and the cell probabilities p samples around the original data samples, with decreasing variance. As the number of total samples is large, the numerical methods at the end of Section 3.2 are appropriate, and as the noise variance decreases with the iterations, the algorithm , is as follows: 1. Choose an initial reconstruction function  X  x ( q ) and initial cost function c ( q ). 2. For each original data sample, introduce new noisy samples with a given covariance matrix samples and the newly created noisy samples as data samples for the remainder of the algorithm (any previous noisy samples are replaced). 3. Update c ( q ) to satisfy the probability constraints p the end of Section 3.2 , setting the initial cost function as the cost function at the beginning of this step. 4. Find the next quantizer q (x) corresponding to the current 5. Find the optimal  X  x ( q ) corresponding to the current q (x), according to Eq. (3) . 6. Reduce the magnitude of the entries of the covariance  X  7. Go back to Step 2, until an appropriate convergence condition is satis
The number of noisy samples per data sample must be chosen so that the probability-constraints lead to a large number of for the resulting clouds of points to overlap, forming a fairly contiguous distribution, and intersect, allowing an approximate distinction of the original samples. The reduction of the variance follows a geometric progression. The appropriate maximum number of iterations is highly dependent on the data. behind it is detailed in Appendix A , and illustrated with a few examples. 3.5. On convergence, complexity and hierarchical partitioning
Shortly, we shall provide experimental evidence that PCL may, and often does, outperform the state-of-the-art MDAV in terms
MDAV, both in terms of sophistication and running time, as one may gather from the description above. Fortunately, the two scenarios of application that we contemplate in this paper will typically come with fairly lenient time constraints. concern for running time, we would not like to fi nish this section without a quick note on the convergence and complexity properties of PCL. Because PCL combines and extends two already sophisticated algorithms, namely the Lloyd and the Levenberg
As far as the conventional Lloyd algorithm is concerned, suf sequence of quantizers tends towards a stable, jointly optimal con more concretely, the scaling of number of iterations required with the size of the dataset, again, common practice imposes a analysis of PCL to be at least as intricate as that of Lloyd, being an extension.

There exist a number of mathematically involved convergence and complexity results on the Levenberg of PCL, where the costs are adjusted to satisfy the probability constraints.

In conclusion, aside from marking clear paths for future research, we constraint ourselves to a preliminary, merely unsurprisingly, the complexity of the algorithm is notoriously nonlinear with the number of cells, which is the number of
In our experiments of Section 3 , we shall see that the time required for prepartitioning was almost negligible compared to fl 4. A functional architecture for k -anonymous location-based information retrieval will remain at a functional level, in the sense that one of the building blocks, namely that carrying out k -anonymous microaggregation of location data, could in principle be implemented by any of the microaggregation algorithms cited in Section 2.1 , for example MDAV.
 and because its applicability to microaggregation suf fi ces to motivate it, we do not beyond what is strictly necessary to hint at the possibility that PCL may be applicable for large k . Section 5 will provide experimental evidence that PCL typically outperforms the state-of-the-art algorithm MDAV, for both microaggregation and key building block of the architecture described next. 4.1. Privacy attack against location-based querying
Recall from Section 1.2 that we consider devices frequently operative near a coordinates.
 phones used from the same workplace, we may contemplate others such as desktop computers at a particular Internet caf X  frequently visited at similar times, Internet-enabled PDAs used on a daily train to work with WiFi capabilities, and laptops commonly operated from a hotel one may regularly stay in. We assume that users submit their reference location to an their home, the branch of their bank in the area they work, local news and events, traf
Submission of a perfectly accurate location to an untrusted information provider poses a serious privacy risk. We provide a more speci fi c illustration of such privacy compromise by means of an example. A user accesses a location-based information provider from a home computer to enquire about an elegant, local Italian restaurant, to take her date to. The user speci service for online ticket purchases.
 yellow pages, infer her full name and home number. In this way, the attacker knows with signi theft.
This example illustrates that the solutions for preserving user privacy relying on anonymizers and pseudonymizers [34 corresponding to a fi xed reference location, as assumed here, plays the role of a quasi-identi
Finally, note that location anonymization must be used in conjunction with anonymization and pseudonymization techniques, such as those mentioned in Section 2.2 , that prevent attackers from obtaining any and all quasi-identi inherent in the communication system such as an IP address. In the event that the privacy requirement of k -anonymization of locations, viewed as quasi-identi fi ers, is constrained further to provide k -anonymization of all quasi-identi attacker, one must consider a joint solution, in which aggregation should be performed on the entire tuple of quasi-identi 4.2. Informal description of the architecture
We now present an informal description of an architecture for k -anonymous retrieval of location-based information. A more formal speci fi cation is the object of Section 4.3 .
 solutions take a step further from the idea of anonymizers and pseudonymizers and propose location anonymizers that perturb location data [55] , many of them based on k -anonymity and cloaking [37,57,44,58
The discussion of Section 4.1 and our assumption that reference locations are rather algorithms cited in Section 2.1 , for example MDAV. However, we shall introduce and recommend an algorithm of our own, which respectively, as we shall prove experimentally in Section 5 .

Intuitively, while the same measure of privacy may be applied to all devices, devices with a home location in more densely pseudonym, in order to access LBS providers. In the same way that we are lead to regard accurate location data as a quasi-identi fi er, we may regard the perturbed location data as a ( k -anonymous) quasi-pseudonym . of their home locations. 4.3. Formal description of the architecture
We formalize the architecture described in Section 4.2 , under the general formulation of Section 3.1 . Speci with a location anonymizer. Summarizing Section 4.2 , users are assumed to frequently operate near a location anonymizer collects accurate home location information, either from the users, or from publicly available address their exact home location whenever they access LBS providers. The centroid is represented by the r.v. digital credentials, as explained in Section 2.2 .

Suppose for the sake of argument that the population of users remained static. Under that idyllic hypothesis, the clustering and costs already computed by PCL to cluster the new users according to the modi afterwards, once the newer portion of the population acquires suf
Evidently, the actual k -anonymity attained by such slowly updated location anonymizer would be lower than intended when run only once on a population model represented by a probability distribution capturing the relative density of a growing attacker would still pose a privacy problem. As one may expect after this preliminary digression, the anonymization of time-varying datasets, data streams and trajectories [83,56,42] is still a topic of extensive research. of Fig. 5 (a) in the special case at hand. 5. Examples and experimental results proposed in Sections 3.3 and 3.4 , with experimental results for a few intuitive, synthetic datasets in small k-anonymity, respectively. Finally, in Section 5.3 , we analyze real, standardized datasets to con performance of PCL over MDAV in terms of the privacy  X  utility trade-off.

In all cases, MSE normalized per dimension is used as distortion measure, thus D = expectation is of course taken according to the empirical distribution of the n points x though our code has not been optimized for speed, we provide preliminary remarks regarding computational costs for the most challenging experiments carried out, in Section 5.3 . 5.1. Simple, illustrative examples
The following examples are meant to show that even for simple sample sets, the performance of PCL over MDAV may be dots depict the samples x 1 , ... , x n , and the large ones, the reconstruction values x illustrate cell assignments. The additively weighted Voronoi cells determined by the modi The probability constraints used for the case  X  x q , in which k is not a divisor of n , were arbitrarily set to p p
Q 2  X  X  = p Q 3  X  X  = 3 = 10 , because a common, fractional probability p
Interestingly, PCL yields a distortion roughly a 40% lower than that of MDAV, while respecting the very same k -anonymity fi necessarily depend on the data samples and the initialization of PCL. In fact, it is straightforward to leading to a lower distortion improvement, or even, albeit exceptionally, a poorer distortion, but also a higher distortion improvement. What these simple examples reveal, however, is the potential for a considerable improvement given by PCL over which shall also prove quite signi fi cant. 5.2. Experimental results for uniform and Gaussian statistics distributions of points in  X  2 . Namely, we fi rst assume that X is uniformly distributed on the square [0, 1] consider the case when X is composed of independent, zero-mean, unit-variance Gaussian entries. dif example, the height and (the logarithm of) the weight of adult men approximately follow a Gaussian model with correlation perspective of the motivating application for LBSs described in Section 1.2 , the set of home locations of users in a square, uniformly populated sector. The second distribution is intended to evaluate the circular urban area with a denser, centric downtown and more spread out suburbs.

Further, we contemplate both the cases of clustering and microaggregation, that is, both the cases of large and small k -and the PCL algorithm without noisy samples is applied for a common probability constraint p p
MDAV or enlarging slightly the fi rst few constraints. Even though one could conceivably optimize the choice of probability randomly chosen from the initial samples.
 hexagonal lattice, known to minimize the distortion among all two-dimensional lattices [85] . and Fig. 17 and 18 for the Gaussian case. The distortion improvements were approximately of up to 29 % over MDAV for the surprising was the fact that initializations based on MDAV did not lead in general to a better performance, despite being an of the inner iterations running the Levenberg  X  Marquardt optimization. In the uniform case, we used 4000 noisy samples per original sample, and 5000 in the Gaussian case. As our proof-of-concept implementation used a common variance value for all presumably due to the sparse samples forming the exterior of the cloud, and a very small samples at the center.

In this and other experiments carried out we observed that each optimization step decreased the distortion while approximately respecting the probability constraints. This experimental same low-distortion solution is found regardless of the initialization, in a small number of iterations. 5.3. Experimental results for a real, standardized dataset
We conclude our experimental results with the particularly challenging problem of microaggregation of a real, standardized
The dataset in question,  X  Census  X  , was used in the computational aspects of statistical con and has since then been served as a widely spread comparison test in the SDC literature. It contains 1080 records with 13 for equality constraints, just like MDAV,  X  Census  X  provides an adequate test for PCL, and given the fact that MDAV yields column of the data set for zero mean and unit variance.

We explored a reasonably wide range of target anonymity constraints, k the values in the microaggregation literature. Observe that since every column of the dataset underwent unit-variance number of dimensions, as we mentioned at the beginning of Section 5 , the numbers reported turn out to be equivalent to the popular SDC measure of sum of squared errors (SSE) divided by sum of squared total (SST).
Our implementation of MDAV followed the usual fi xed-size strategy that starts isolating groups of the target size k centroid. Hence, by speci fi cation, the anonymity attained by the algorithm was k
Matlab implementation of both MDAV and PCL, and that neither the code nor the numerical methods implemented were particularly optimized for speed, but originally designed merely to assess the privacy As we observed that the computational complexity of the cost adjustment, implemented by the numerical method based on the to limit the total computation cost, the strategy introduced in Section 3.5 . Speci computational complexity to a linear function of the complexity of each macrocell, but to implement the postpartitionings as over MDAV are against the most challenging case, namely when MDAV is not prepartitioned, but directly applied to the entire dataset for the best possible performance against PCL. In all cases we veri so that larger prepartitions could be used, the performance of PCL would improve.

The experiments reported employed the noisy version of PCL, with probability constraints corresponding to the same based on the results of MDAV, and the remaining 4 randomly chosen from the initial samples. Even though, as we mentioned, distortion comparisons are against unpartitioned MDAV, the initialization of PCL based on MDAV reconstructions did of course require to additionally compute MDAV on each prepartition. We also mentioned that resistance against variable cell size strategies. In any case, whenever k imposed on PCL roughly shared remaining points equally, in order to have a small margin for cost adjustment, rather than attained by PCL k min was occasionally slightly larger than the required target size k attained by the algorithm was actually slightly better than the one intended.
 to prepartitioning, when required.

We used 3 prepartitions of size approximately n PP =360 for k k gain in running time due to prepartitioning. For the rest of values of k 1000 points per cell. The number of iterations i ranged from 150 for the smallest k time, including prepartitioning when applied, ranged from approximately 16 min to 16 s, for the smallest and largest k respectively. Running times were hardly dependent on the initialization. The experimental results with PCL are summarized in k defective cell out of 43, with hardly any impact on the distortion (
ThedistortionsofMDAVandPCLareplottedin Fig. 19 ,andtherunningtimesofPCL,in Fig. 20 .Thedistortionimprovementsranged approximatelybetween9%and17%overMDAV,lesspronouncedthanforthemicroaggregationexperimentsonuniformandGaussian statistics in Section 5.2 . This is likely to be linked to two facts. First, noticeable price in running time, and that the distortion improvement depends on the distribution of data points. favorable outcome, the overall process would gracefully trade-off performance at the cost of complexity, evidently with a saturation effect.
 the same anonymity requirement, against the state-of-the-art MDAV.
Even though our proof-of-concept implementation was not meant to investigate computational issues, as PCL was initially need for a complete recomputation. 6. Conclusion respectively, SDC and privacy for LBSs, but potentially extensible to numerous other scenarios.
Moreprecisely,ourmaincontributionconsistsintwovariationsofa k -anonymousaggregationalgorithm,whichwecallPCL,oneof whichisparticularlysuitedtotheimportantproblemof k -anonymousmicroaggregationofdatabases.Theothervariationalsoexhibits excellent performance in the problem of clustering or macroaggregation, applicable to privacy in LBSs. This newly developed with a numerical method to solve nonlinear systems of equations based on the Levenberg
We illustrate the somewhat less known application of macroaggregation with a simple architecture for k -anonymous retrieval of location-based information. Essentially, we consider location-aware devices, commonly operative near a location. We then regard accurate, fi xed location data as a quasi-identi pseudonymization, location anonymization and the privacy criterion used in microdata k -anonymization. Precisely, accurate devices share a common centroid location.
 We report experimental results regarding k -anonymous clustering and microaggregation, that is, large and small k , for
Gaussian and uniform statistics, with MSE as distortion measure. The resulting quantization cells are observed to be convex polytopes, and just as in the conventional Lloyd algorithm, the sequence of distortions is nonincreasing and the clustering con fi gurations seem to rapidly converge to a low-distortion solution. While maintaining exactly the same k -anonymity constraints, our algorithm outperforms the state-of-the-art microaggregation algorithm MDAV by a signi distortion. The approximate distortion reduction was typically 20% and 15% for clustering of uniform and Gaussian data, dataset over which MDAV exhibited unparalleled performance, until now, called better privacy  X  utility trade-off than MDAV for a variety of data distributions, and that improvement depended on the data.
Despite the experimental evidence that PCL outperforms MDAV in all cases considered, in terms of data utility for the same order to gracefully trade-off distortion for running time, we propose a hierarchical application of PCL, where the may be initialized a number of times, in different ways, and the best outcome picked.
 to dynamic sets of samples distributed according to the original model.
 price of an increased computation time, which may preclude its use in the event that microaggregation over thousands of data applications, our proposed algorithm should become the preferred choice for microaggregation and clustering. Under stricter computational constraints, PCL may still prove helpful in quantifying the loss in privacy use of faster algorithms such as MDAV, and aid in assessing whether that loss is acceptable. minimum-distortion, probability-constrained quantization, which also addresses applications of similarity-based, workload-measure, possibly over categorical alphabets, for example semantic distances.
 Acknowledgment comments, which motivated major improvements on this manuscript. This work was partly supported by the Spanish
Government through projects CONSOLIDER INGENIO 2010 CSD2007-00004 and TEC-2008-06663-C03-01  X  P2PSec  X  , and by the Government of Catalonia under grant 2009 SGR 1362. Appendix A. Rationale behind the noisy PCL algorithm
This section is devoted to overview the rationale behind the noisy variation of the PCL algorithm proposed in Section 3.4 , empirically proven to be effective for small k in Section 5 . We emphasized that the key problem of the noiseless version of
PCL was the use of derivative-based numerical methods for the computation of the costs c(q) to constrain the probabilities p ( q ).
 given speci fi c data samples x 1 , ... , x n and reconstruction values in order to satisfy certain equality constraints on p q . Because adding a common constant to all costs does not change the c =0.
 Consider fi rsttheexampledepictedin Fig.21 ,inwhich n =10samplesaretobeaggregatedin j thus a common probability constraint p 0 = k n = 1 = 2 is enforced. De asa function of c 2 . Observe that minimization of this objective, that is, equivalent to satisfying the k -anonymity constraint p 1 = p
Consequently, estimates of local derivatives may not provide an ef An analogous example is represented in Figs. 23 and 24 , this time for n =15 samples to be aggregated in j samples each, with p 0 = k n = 1 = 3 .
 methods only based on local derivatives could not possibly
Let us look now into the effect of introducing noisy samples into the cost adjustment problem. We follow the noisy PCL algorithm described in Section 3.4 , but leave the reconstruction values samples and k =1, thus p 0 = k n = 1 = 2 , and once more in Figs. 27 and 28 , for n =15 samples and k =5, thus p reconstruction values, we could have just as well repeated the variance cooling as an inner iteration for each update of the reconstruction values, with similar results.

References
