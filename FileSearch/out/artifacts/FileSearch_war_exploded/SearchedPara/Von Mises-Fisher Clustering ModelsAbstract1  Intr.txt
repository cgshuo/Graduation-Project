 Siddharth Gopal SGOPAL 1@ CS . CMU . EDU Carnegie Mellon University, Pittsburgh, PA 15213 USA Carnegie Mellon University, Pittsburgh, PA 15213 USA With the advent of large amounts of unlabeled data, cluster-ing has emerged as an important tool for the end user to ob-tain a structured view of the data. Probabilistic clustering algorithms such as K-means (Gaussian mixtures), Multi-nomial Mixtures, Latent Dirichlet allocation (Blei et al., 2003) have emerged as the defacto standard for discovering the latent structures and relations in the data. Such proba-bilistic models define a generative model for the data by as-suming some rigid instance representation, for e.g. Multi-nomial Mixtures assumes that each instance is represented as discrete feature-counts and is drawn from one of many Multinomial distributions.
 However, it is questionable whether such representation of data is appropriate for all domains. For example, in text-mining (classification, retrieval, collaborative filtering etc) documents have typically been represented using a term frequency-Inverse Document frequency Normalized form (Tf-Idf normalization) (Salton &amp; McGill, 1986), where each document is represented as a point on a unit-sphere using a combination of both within-document frequencies and inverse corpus-frequences. Tf-Idf normalization has always shown better performance than feature-counts rep-resentation based on several supervised tasks such as clas-sification (Joachims, 2002), retrieval (Robertson, 2004) etc. Similarly, in Image-modeling, unit normalized spatial pyra-mid vectors is a common representation (Yang et al., 2009). Normalization is often an important step in data analysis because it removes the  X  X agnitude X  of the instances from the picture and places more importance on the directional distribution; in other words, we do not want unduly long documents or big images to distort our inferences, hence we represent the instances as relative contributions of indi-vidual features.
 Despite the practical successes of normalized data repre-sentation, they have not been well studied by Bayesian Graphical models. Since the data lies on a unit-sphere man-ifolds, popular clustering assumptions such as Gaussian (O X  X agan et al., 2004) or Multinomial (Blei et al., 2003; 2004; Blei &amp; Lafferty, 2006a;b) are not appropriate. On one hand we have empirical success of such normalized representation and on the other hand we have a wide vari-ety of graphical models that model data using a different representation. Can we get the best of both worlds? Can we develop models that are particularly suited to such unit-sphere manifolds but at the same time maintain flexibility like other graphical models? In this paper, we propose a suite of models using von Mises-Fisher (vMF) distributions which have been primarily used to model such directional data. vMF models have been long studied in the directional statistics community (Fisher, 1953; Jupp &amp; Mardia, 1989) and are naturally suited in such scenarios as they model distances between instances using the angle of separation i.e. cosine similarity. Works in vMF have typically fo-cused on low-dimensional problems (2D or 3D spaces) to maintain tractability and relying on Gibbs sampling for in-ference which is generally difficult to scale in higher di-mensions (Hasnat et al., 2013) (Mardia &amp; El-Atoum, 1976) (Guttorp &amp; Lockhart, 1988) (Bangert et al., 2010). More popular works include text clustering work by (Banerjee et al., 2006) (Banerjee et al., 2003) where an EM-based algorithm without Bayesian inference was used, and spher-ical topic models by (Reisinger et al., 2010) which mimic LDA with a Bayesian inference for learning the mean pa-rameters in vMF, but leave the crucial concentration param-eters (the variance parts) of the models to be set manually. In this paper, using the vMF distribution as the building block, we propose three increasingly powerful models for cluster analysis 1. The Bayesian vMF Mixture Model ( B-vMFmix ): A fully Bayesian formulation of mixture of vMF distribu-tions where each instance represented as a point on a unit-sphere is assumed to be drawn from one of many vMF dis-tributions. The parameters of the clusters are themselves drawn from a common prior which helps the clusters to share information among each other. 2. The Hierarchical vMF Mixture Model ( H-vMFmix ):
When the data we want to analyze is huge, one of the effi-cient means of browsing is by means of a hierarchy (Cut-ting et al., 1992). We extend B-vMFmix to H-vMFmix, to enable partitioning the data into increasing levels of speci-ficity as defined by a given input hierarchy. To our knowl-edge, this is the first hierarchical Bayesian model for vMF-based clustering. 3. The Temporal vMF Mixture Model ( T-vMFmix ): For temporal data streams, analyzing how the latent clusters in the data evolve over time is naturally desirable. We aug-ment B-vMFmix to the first temporal vMF-based model that accommodates changes in cluster parameters between adjacent time-points. For example, in a corpus of docu-ments, this could reflect the changing vocabulary within a cluster of documents.
 We develop fast variational inference schemes for all the methods and equally fast collapsed Gibbs sampling tech-niques for the simpler models. Our empirical comparison on several datasets conclusively establishes that vMF distri-butions are better alternatives to standard Gaussian, Multi-nomial Mixtures and can be successfully used for cluster analysis. To our knowledge, this is the first work that pro-vides a thorough treatment of vMF distribution in the con-text of a wide variety of graphical models for data analysis. The von Mises-Fisher (vMF) distribution defines a proba-bility density over points on a unit-sphere. It is parameter-ized by mean parameter  X  and concentration parameter  X  -the former defines the direction of the mean and the latter determines the spread of the probability mass around the mean. The density function for x  X  R D , k x k = 1 , k  X  k = 1 , X  &gt; 0 is given by, where I  X  ( a ) is the modified bessel function of first kind with order  X  and argument a . Note that  X  &gt; x is the cosine similarity between x and mean  X  and that  X  plays the role of the inverse of variance.
 The simplest vMF mixture model (Banerjee et al., 2006) assumes that each instance is drawn from one of the K vMF distributions with a mixing distribution  X  , where K is a known constant, and the parameters of the vMF distri-butions correspond to the underlying themes (clusters) in the data. The cluster assignment variable for instance x i denoted by z i  X  { 1 , 2 ,..K } in the probabilistic generative model given below, The k  X  X h cluster is defined by mean parameter  X  k and con-centration parameter  X  . The parameters P = {  X  ,  X  , X  } are treated as a latent variables. To train the model, we can use the familiar EM algorithm, to efficiently iterate between calculating the E [ Z ] in the E-step and optimizing P to max-imize the likelihood in the M-step. The update equations We improve upon the basic vMF mixture model in the above section by adding bayesian components in three sig-nificant ways a) the fully Bayesian vMF mixture model (B-vMFmix) b) A hierarchical extension of B-vMFmix and c) A temporal extension of B-vMFmix. 3.1. Bayesian vMF mixtures (B-vMFmix) The bayesian vMF mixture model is the fundamental build-ing block for the hierarchical and temporal vMF mixtures. This model enables sharing of information between the clusters by shrinking the cluster parameters towards each other using a prior. The generative model for given data D = { x i } N i =1 and a fixed number of clusters K is given by, The prior parameters are {  X , X  0 ,C 0 ,m, X  2 } . The cluster mean parameters  X  = {  X  k } K k =1 are commonly drawn from a prior vMF distribution with parameters {  X  0 ,C 0 } . The clus-drawn from a log-normal prior with mean m and variance . The mixing distribution  X  is drawn from a symmetric dirichlet with parameter  X  . This bayesian model improves over the simple vMF mixture in multiple ways; firstly we share statistical strength by shrinking the cluster mean pa-rameters towards a common  X  0 , secondly there is flexibility to learn cluster-specific concentration parameters  X  k with-out the risk of overfitting if the priors are appropriately set, thirdly the posterior distribution over the parameters gives a measure of uncertainty of the parameters unlike point es-timates in simple vMF mixtures. These advantages are ev-ident in our experimental section (in section 4). The likelihood of the data and the posterior of the parame-ter is given by, P ( Z ,  X  ,  X  , X  | . )  X  P ( X | Z ,  X  ,  X  , X  ) P ( Z ,  X  ,  X  , X  | m, X  Since the posterior distribution of the parameters cannot be calculated in closed form, we need to resort to approxi-mate inference using variational methods or sampling tech-niques. Using variational inference, we try to find a distribution that is closest in KL-divergence to the true posterior. We assume the following factored form for the approximate posterior. For the concentration parameters, the inference is not straight-forward because of the presence of log-bessel function. We present two different ways of estimating the posterior of the concentration parameters -(a) Sampling scheme and (b) Bounding scheme. Each scheme assumes a different form for the posterior distribution of  X  k  X  X . q (  X  k )  X  No-specific form k = 1 , 2 ..K [Sampling] We develop a variational algorithm where the posterior pa-rameters - X ,  X  ,  X  ,  X  are iteratively optimized to maximize The closed form updates for posterior parameters for  X  and z ik are given by, Next we present the posterior estimation of the concentra-tion parameters.
 Sampling: In the sampling scheme, we rely on estimating  X  X  (and related quantities such as log C D (  X  k ) ) by draw-ing samples from the posterior distribution. Sampling re-quires the computation of the conditional distribution for . However, variational inference (for the other parame-ters) does not maintain samples but instead maintains only posterior distributions. To overcome this issue, we rewrite the conditional distribution for  X  k in terms of the expecta-tion of posterior parameters rather than samples. Using the VLB and Jensen X  X  inequality on the conditional of  X  k , Having identified the proportionality of the conditional dis-tribution, we can use MCMC sampling with a suitable pro-posal distribution to draw samples. We used a log-normal distribution around the current iterate as the proposal dis-tribution.
 The MCMC sampling step for the posterior of  X  introduces flexibility into the model as the samples are now from the true posterior (given the rest of the parameters). The down-side is that the variational bound is no longer guaranteed to be a valid lower-bound since E q [log C D (  X  k )] and E are estimated through the samples. However, we observed that the posterior distribution of the  X  k  X  X  was highly con-centrated around the mode and the estimates from samples gave good empirical performance in terms of predictive power.
 This partial MCMC sampling does not increase computa-tional costs much since there are only K variables to be es-timated and the major computational bottleneck is only in updating  X  . Another alternative to repeated sampling is to use grid search, where we break down the continuous pos-terior distribution of  X  k across a finite set of points, and use this discrete distribution to estimate the expectation of vari-ous quantities.Refer the last part of supplementary material for a thorough discussion of the computational issues. Note that the same model with an unnormalized prior for  X  was used in . However since  X  is not a natural parameter of the vMF distribution, it is not clear whether the unnormal-ized prior results in a valid posterior distribution. Bounding: The core problem in doing full varia-tional inference for the model is the computation of the [log C D (  X  k )] in the VLB. We are not aware of any dis-tribution q (  X  k ) for which is there is a closed form expres-sion for E q [log C D (  X  k )] (due to the presence of the log bessel function in C D (  X  k ) ). To overcome this issue, we first upper-bound the log bessel function using a Turan type inequality (Baricz et al., 2011), followed by an approxima-tion using the delta method. More specifically, the growth of the modified bessel function of first kind with order v and argument u i.e. I v ( u ) can be lower-bounded by (Baricz et al., 2011),
Integrating over u &gt; 0 , Since all the expectations on the RHS of eq (2) are twice differentiable, we can use the delta approximation method. The expectation of a function g over a distribution q is given by Applying the equations (2), (3) to the VLB, we can estimate the posterior parameters a k ,b k by optimizing VLB using ply the delta method to calculate E q [log I v ( u )] , this leads to expressions that are not computable directly as well as numerically unstable. We can develop efficient sampling techniques for the model by using the fact that vMF distributions are conjugate w.r.t each other. This enables us to completely integrate out k } K k =1 and  X  and update the model only by maintaining the cluster assignment variables { z i } N i =1 and the concentra-are given by, The conditional distribution of  X  k is again not of a stan-dard form and as before, we use a step of MCMC sampling (with log-normal proposal distribution around the current iterate) to sample  X  k . The advantage of sampling is that the distribution of samples eventually converge to the true posterior, however, the downside is that it is not clear how many samples must be drawn. When the user does not have enough information to set the prior parameters, it is useful to be able to learn them directly from the data. The prior parameters {  X , X  0 ,C 0 ,m, X  2 } are estimated by maximizing the vari-ational lower-bound (which acts as a proxy to the true marginal likelihood). The details are discussed in the sup-plementary material. 3.2. Hierarchical vMF Mixtures (H-vMFmix) Often the data that the user wants to analyze is large and manually inspecting a flat layer of several clusters is harder than hierarchically browsing the data (Cutting et al., 1992). For such cases, we develop a hierarchical vMF mixture model that enables a hierarchically nested organization of the data.
 We assume that the user wants to organize the data into a given fixed hierarchy of nodes N . The hierarchy is defined by the parent function pa ( x ) : N  X  N which denotes the parent of the node x in the hierarchy. The generative model for the H-vMFmix is given by,  X   X  Dirichlet ( . |  X  ) The user specified parameters are {  X  0 ,C 0 } -parameter of the root-node and { m, X  2 , X  } . Each node n is equipped with a vMF distribution with parameters  X  n , X  n . The mean pa-rameters of the siblings nodes are drawn from a common prior defined by their parent vMF distribution. The concen-tration parameters for all the nodes are commonly drawn from a log-normal distribution with mean m and variance . The instance x i is drawn from the one of the leaf-nodes (Note that the data X resides on the leaf-nodes). One can also formulate slightly different models for e.g. by letting all the sibling nodes share the same concentration parame-ter; or by forming a hierarchy of concentration parameters etc -we leave such models for future research. By drawing the parameters of siblings from a common parent node, we are enforcing the constraint that nodes which are closer to each other in the hierarchy share similar parameters. Our hope is that this would enable data to be organized into the appropriate levels of granularity as defined by the hierar-chy. For inference, we can develop a similar variational inference methods with Empirical Bayes step to estimate the prior parameters. The details are presented in the sup-plementary material. 3.3. Temporal vMF mixtures (T-vMFmix) Sometimes, a data collection can evolve over time. In such cases, it will be useful to develop models that can capture the evolution of clusters over time. We present Temporal vMF mixture (T-vMFmix), a state-space model based on the vMF distribution where the parameters of a cluster at a given time point have evolved from the previous time point. fixed number of clusters K , the generative model is given by, specific concentration parameters  X  k  X  X  are commonly drawn from a log-Normal distribution with mean m and variance  X  2 . The mean parameters of the clusters at time t are drawn from a vMF distribution centered around the previous time t  X  1 with a concentration C 0 . This time-evolution of the cluster parameters introduces a layer of flexibility and enables T-vMFmix to accommodate changes in the mean parameter within a given cluster. The C parameter controls the sharpness of time-evolution; hav-ing a large value of C 0 ensures that cluster parameters are more or less the same over time whereas a low value of 0 enables the cluster parameter to fluctuate wildly be-tween adjacent time points. Note that it is also possible to incorporate time-evolution of the mixing distribution i.e. Dirichlet ( . |  X  ) (similar to (Blei &amp; Lafferty, 2006a)) For inference, we develop a mean-field variational ap-proach with Empirical Bayes step for estimating the prior as a regularization term forcing the parameters of the next time-step to be similar to the previous one. We recom-mend setting the parameter manually than directly learning Throughout our experiments we used several popular benchmark datasets (Table 1) -TDT-{ 4,5 } (Allan et al., son et al., 2007). All datasets (except NIPS) have associ-ated class-labels and are single-labeled i.e. each instance is assigned to exactly one class-label. For TDT-{ 4,5 } , we used only those subset of documents for which relevance judgements were available. 4.1. Metrics and Baselines First and the most natural question that we would like to answer is  X  X re vMF mixtures any better than standard Gaussian or Multinomial mixtures? X . Generally, compar-ing two clustering models is in itself a hard problem. Conclusive comparisons often involve detailed user-studies (Boyd-Graber et al., 2009) which are time-consuming and not always feasible. Therefore likelihood based compar-isons have been commonly used as an alternative (Blei &amp; Lafferty, 2006a),(Blei &amp; Lafferty, 2006b),(Teh et al., 2006). Likelihood measures the predictive power of the model on unseen data i.e. the generalization ability of the model -a metric widely used for model selection. However, since the support of the models are different -vMF models are defined on unit-spheres, Multinomial models are defined for non-negative integers etc, we can-not use likelihood on held-out test set to compare vMF and other non-vMF models (numerically, the likelihood of the vMF models is around 5 orders of magnitude larger than Multinomial or Gaussian mixtures). To address this issue, we compare vMF and non-vMF clustering models based on how well they are able to recover the ground-truth clusters -the human assigned class-labels are assumed to be the true ground truth clusters. We use six widely used eval-uation metrics for this comparison -Normalized Mutual Information (NMI) , Mutual Information (MI) , Rand In-dex (RI) , Adjusted Rand Index (ARI) , Purity and Macro-F1 (ma-F1). The definitions of the metrics can be found in (Banerjee et al., 2006), (Steinley, 2004) and (Manning et al., 2008). We compare the following 5 clustering meth-ods,  X  B-vMFmix : Our proposed Bayesian vMF mixture model that extracts flat clusters.  X  vMFmix : A mixture of vMF distributions described in
Section 2. Note that this is similar to the model developed in (Banerjee et al., 2006), except that all clusters share the same  X  . Using the same  X  for all clusters performed sig-nificantly better than allowing cluster-specific  X  k  X  X  -this may be due to the absence of Bayesian prior.  X  K-means (KM) : The standard k-means with euclidean distance and hard-assignments.  X  Gaussian Mixtures (GM) : A mixture of Gaussian dis-tributions with the means commonly drawn from a Nor-mal prior and a single variance parameter for all clusters -using cluster specific variances performed worse (even with an Inverse gamma prior).  X  Multinomial Mixture (MM) : A graphical model where each instance is represented as a vector of feature counts, drawn from a mixture of K Multinomial distribu-tions.
 We used the Tf-Idf normalized data representation for vMFmix, KM and GM, and feature counts representation (without normalization) for MM. For all the methods, ev-ery instance is assigned to the cluster with largest probabil-ity before evaluation. Also, for the sake of a fair compar-ison, we set the same random initial values of the cluster-assignment variables in all the algorithms; the results of each method is averaged over 10 different starting values. 4.2. Results Table 2 summarizes the main results of B-vMFmix, vMFmix, KM, GM and MM in the ground-truth based eval-uation on five data sets. Due to the lack space we only include the results for 30 clusters and two metrics NMI type of evaluation. The parameters of all the probabilistic models are estimated using MAP Inference . Among the six methods, B-vMFmix achieves the best performance on all the datasets. In fact, both B-vMFmix as well as vMFmix show a consistently strong performance against other meth-ods, suggesting that the clusters generated by vMF models are indeed better aligned with the ground truth than the non-vMF methods. To further validate our findings, we conducted two-sided significance tests using paired t-test between every method and B-vMFmix for both the metrics. The results of the 10 different runs are considered as ob-served samples. The null hypothesis is that there is no sig-nificant difference between the methods. Our experiments almost all the observed results are statistically significant. Table 3 compares the performance of the basic vMF mix-ture (vMFmix) and the fully Bayesian vMF mixture model (B-vMFmix) on six data sets. Since the support of both models are the same, i.e. points on a unit-sphere, we can directly compare them by looking at the predictive power on a held out test-set. The B-vMFmix is trained through variational inference (using Sampling method for the con-centration parameters) and vMFmix is trained through the EM algorithm. In our experiments, the sampling method performed better than the bounding method with an in-significant difference in running time, we therefore report all results using the sampling based method. Due to lack space we only include the results for K = 20 , 30 clusters (refer supplementary material for full results). The results show that B-vMFmix is able to assign a higher likelihood for unseen data for all six datasets.
 Figure 2 compares the performance of the hierarchical vMF mixture model (H-vMFmix) with vMFmix and B-vMFmix. We test H-vMFmix on 5 different hierarchies with vary-ing depth of hierarchy and branch factor (branch factor is the #children under each node). For example (height h=3,branching factor b=4) is a hierarchy 3 levels deep where each internal node in the hierarchy has 4 children each. We also plot the performance of the corresponding vMFmix and B-vMFmix; the number of clusters was set equal to the number of leaf nodes in the hierarchy. Due to the lack of space we plot the results for only two datasets -NEWS20 and CNAE (refer supplementary material for full results). The superior performance of H-vMFmix in terms of predictive power strongly supports the rationale for hier-archically shrinking the model parameters.
 Finally, we compare the predictive power of T-vMFmix with vMFmix and B-vMFmix on the following datasets which are temporal in nature, 1. NIPS : This dataset outlined in table 1 has 2483 re-2. NYTC-elections : This is a collection of New York We test the models by predicting the likelihood of data in the next time-point given all the articles from the beginning to the current time-point, similar to the setup used in (Blei &amp; Lafferty, 2006a). For simplicity, we fix the number of clusters to 30. For ease of visualization, we plot the rela-tive improvement in log-likelihood of the B-vMFmix and T-vMFmix models over the simple vMFmix model (this is because the log-likelihood between adjacent time-points are not comparable and fluctuate wildly). The results are plotted in Fig 1 (the supplementary material contains the results for 2 additional datasets). The results suggest that T-vMFmix by taking the temporal nature into account, is able to always assign a higher likelihood to the next time-point than B-vMFmix and vMFmix. To visualize the the cluster evolutionion over time, Fig 3 shows the progress of the mean parameter for one the the clusters in NIPS. The six words with largest weights in the mean parameter are shown over time. In order to fully understand the benefits of data representa-tion using Tf-Idf normalization, we performed controlled experiments on the two largest datasets -NEWS20 and TDT5. We compared the performance of representing the data using plain Tf normalization against Tf-Idf normaliza-iments using vMFmix and K-means (as reported in Table 4) show that on both the datasets, representing the data us-ing Tf-Idf normalization gives significant performance ben-efits. 4.3. Experimental Settings All our experiments were run on 48 core AMD opteron 6168 @ 1.92Ghz with 66GB RAM with full parallelization wherever possible. The main computational bottleneck in all our variational inference algorithm is the computation of  X  and  X  . The updates for both these parameters can be rewritten in terms of matrix products which can be ef-ficiently computed using state-of-art parallel matrix multi-plication tools. Refer section 10 of the supplementary ma-terial for a thorough discussion of the computational issues. In this paper we proposed a suite of powerful Bayesian vMF models on unit-sphere manifolds as an alternative to the popular approaches based on multinomial or Gaus-sian distributions. Our models enable full Bayesian in-ference with vMF models in flat, hierarchical and tem-poral clustering. Our fast variational/sampling algorithms make the methods scalable to reasonable data volumes with high-dimensional feature spaces. The experiments provide strong empirical support for the effectiveness of our ap-proaches -all our models outperformed strong baselines (k-means, Multinomial Mixtures and Latent Dirichlet Al-location) by a large margin on most data sets. For future work we would develop non-parametric versions of our vMF models as well as handle multi-field information. This work is supported in part by the National Science Foundation (NSF) under grant IIS 1216282. We thank the reviewers for their excellent feedback and Guy Blelloch for sharing his computational resources.
 Allan, James, Carbonell, Jaime G, Doddington, George,
Yamron, Jonathan, and Yang, Yiming. Topic detection and tracking pilot study final report. 1998.
 Banerjee, Arindam, Dhillon, Inderjit, Ghosh, Joydeep, and
Sra, Suvrit. Generative model-based clustering of direc-tional data. In SIGKDD , pp. 19 X 28. ACM, 2003.
 Banerjee, Arindam, Dhillon, Inderjit S, Ghosh, Joydeep, and Sra, Suvrit. Clustering on the unit hypersphere using von mises-fisher distributions. JMLR , 6(2):1345, 2006. Bangert, Mark, Hennig, Philipp, and Oelfke, Uwe. Us-ing an infinite von mises -fisher mixture model to cluster treatment beam directions in external radiation therapy. In ICMLA , 2010.
 Baricz,  X  Arp  X  ad, Ponnusamy, Saminathan, and Vuorinen,
Matti. Functional inequalities for modified bessel functions. Expositiones Mathematicae , 29(4):399 X 414, 2011.
 Blei, David M and Lafferty, John D. Dynamic topic mod-els. In ICML , 2006a.
 Blei, David M, Ng, Andrew Y, and Jordan, Michael I. La-tent dirichlet allocation. JMLR , 2003.
 Blei, David M, Griffiths, T, Jordan, M, and Tenenbaum, J.
Hierarchical topic models and the nested chinese restau-rant process. NIPS , 16:106 X 114, 2004.
 Blei, MD and Lafferty, JD. Correlated topic models. In NIPS , pp. 147 X 155. Citeseer, 2006b.
 Boyd-Graber, Jonathan, Chang, Jordan, Gerrish, Sean,
Wang, Chong, and Blei, David. Reading tea leaves: How humans interpret topic models. In NIPS , 2009.
 Cutting, Douglass R, Karger, David R, Pedersen, Jan O, and Tukey, John W. Scatter/gather: A cluster-based ap-proach to browsing large document collections. In SI-GIR , 1992.
 Fisher, Ronald. Dispersion on a sphere. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences , 1953.
 Globerson, A., Chechik, G., Pereira, F., and Tishby, N. Eu-clidean Embedding of Co-occurrence Data. JMLR , 8: 2265 X 2295, 2007.
 Guttorp, Peter and Lockhart, Richard A. Finding the lo-cation of a signal: A bayesian analysis. Journal of the American Statistical Association , 83(402):322 X 330, 1988.
 Hasnat, Md Abul, Alata, Olivier, and Tr  X  emeau, Alain. Hier-archical 3-d von mises-fisher mixture model. Workshop on Divergences and Divergence Learning, ICML , 2013. Joachims, Thorsten. Learning to classify text using sup-port vector machines: Methods, theory and algorithms . Kluwer Academic Publishers, 2002.
 Jupp, PE and Mardia, KV. A unified view of the theory of directional statistics, 1975-1988. International Statisti-cal Review/Revue Internationale de Statistique , 1989. Manning, Christopher D, Raghavan, Prabhakar, and
Sch  X  utze, Hinrich. Introduction to information retrieval , volume 1. Cambridge University Press Cambridge, 2008.
 Mardia, KV and El-Atoum, SAM. Bayesian inference for the von mises-fisher distribution. Biometrika , 63(1): 203 X 206, 1976.
 O X  X agan, Anthony, Forster, Jonathan, and Kendall, Mau-rice George. Bayesian inference . Arnold London, 2004. Reisinger, Joseph, Waters, Austin, Silverthorn, Bryan, and
Mooney, Raymond. Spherical topic models. In ICML , 2010.
 Robertson, Stephen. Understanding inverse document fre-quency: on theoretical arguments for idf. Journal of doc-umentation , 60(5):503 X 520, 2004.
 Salton, Gerard and McGill, Michael J. Introduction to modern information retrieval. 1986.
 Steinley, Douglas. Properties of the hubert-arable adjusted rand index. Psychological methods , 2004.
 Teh, Yee Whye, Jordan, Michael I, Beal, Matthew J, and
Blei, David M. Hierarchical dirichlet processes. JASA , 101(476), 2006.
 Yang, Jianchao, Yu, Kai, Gong, Yihong, and Huang,
Thomas. Linear spatial pyramid matching using sparse coding for image classification. In CVPR , pp. 1794 X 
