
Letting one's thoughts wander is not simply an arbitrary or rambling process. It can better be described as "associative thinking", where a complex chain of associative thoughts and ideas are linked. It is our contention that this seemingly chaotic process can be modeled by a random walk in a weighted directed graph. Furthermore, is it possible to predict mathematically the "steady state" of such a process, to determine where such wandering is leading. 
The random walk process uses rules of association, defined by the Local Confidence Gain (LCG) interestingness measure. Extracted concepts are used as nodes of a directed graph. The associative "forces" between any two concepts (measured by LCG) are used to weigh the edges connecting the nodes that create a graph of associations. 
It is common, yet not trivial, for people to look for data about a subject without knowing its exact nomenclature (for example, finding the name of a disease just by knowing its symptoms). Random walk in association graphs can discover highly informative phrases that can be used for query expansion in a way that better expresses the user's initial search goals. A different usage is to create a user profile representing his current interests. 
We used a modified version of the Turing Test to show that the random walk process discovers association rules that conform to a human associations generating process. By constructing the user associations we were able to build a profile representing the user's "line of thoughts". The suggested algorithm can be used in any database and can implement the ranking measures of other association rules. 
This chapter presents the keystones of the random walk based algorithm. Section 1.1 discusses the role of associations in the cognitive processes that deal with information retrieval from memory. Section 1.2 and 1.3 discuss the random walk basics and its convergence criteria. In section 1.4 we present the Confidence Gain (CG), an association interestingness measure and in section 1.5 we present the Local Confidence Gain (LCG) measure, which is a low complexity version of the Confidence Gain. Both CG and LCG represent the edges of the directed graph used for the random walk process. The nodes of such a graph are concepts (phrases) extracted from Internet pages. Section 1.6 presents the contribution of this paper. 
Since this work links association rules interestingness to human cognitive association processes, some of the terms differ from common terminology. We shall sometimes use terms such as " associative forces " or " generating associations " to describe the process of discovering and evaluating the interestingness of association rules between pairs of concepts. When describing an asymmetric association we shall use the term " stimulus " as the input and " response " as the output of the process. Furthermore, the phrase  X  user profile  X  represents the most significant concepts (phrases) that can describe user interests. A user profile is extracted by tracking user behavior while surfing the Internet, and extracting the most significant phrases used to specify the words that hold the information and are extracted during the calculations  X   X  terms  X ,  X  concepts  X , " phrases " and  X  associations  X .
Associations are cognitive links between data units in human memory. The discussion on imitating human associations goes back to Alan Turing, in his original article about an imitation game test of intelligence [3]. Turing made two basic claims. The first is that if a machine could pass the Turing Test, it would necessarily be intelligent. The second is that in the not too distant future it could in fact be possible to actually build such a machine. We shall not discuss here the first claim but rather ask what Researchers such as French [24] claimed that only things that have experienced the world as we have experienced it could pass the Turing Test. On the other hand, others [20] claimed that subcognitive questions, which are designed to probe a network of cultural and perceptual associations, could be answered using statistics over large corpuses. In this article we agree with the latter opinion. We claim that to some extent, the human associative world, representing the culture and understanding of the environment, can be reconstructed just by looking at the occurrences of certain terms with respect to each other. We claim that if a machine produces outputs that make sense and furthermore whether the machine is aware of what it is doing or not [4],[11],[15] 
Let G=(V,E) be an undirected connected graph with n nodes and m edges. A random walk in G starts at node 0 v At each step, the next node in the walk is selected randomly from the neighbors of the last node in the walk. The probability to move from node t v to any of its neighbors is 1 where () t v d is the degree of node sequence of visited nodes is a Markov Chain, where the matrix of transition probabilities is [1]
We can similarly formulate the transition probability matrix for a weighed directed graph as [2] 
Here, j i w  X  is the weight of the edge from i to j and w is the sum of all weights for outgoing edges of node i. Eq. [1] is, obviously, a private case of eq. [2]. 
An n n  X  matrix M with real entries ij m , is called a stochastic matrix provided that (i) for all ij m 1 0  X   X  (or primitive) provided there is an integer 0 0 &gt;
M has all positive entries. Let ) 0 ( location j of a vector x at time 0.  X  value of the element at location j at time 1. In matrix distribution of being at any node at time q+1. A regular stochastic matrix M has some interesting properties: (i) 1 is always an eigenvalue; (ii) the eigenvector v 1 can be chosen so that all entries are positive and . 1 1  X  = distribution p 1 v p M q  X  as q goes to infinity. These characteristics enable us to calculate the final probability representation of the associations could be formulated into a regular stochastic matrix, the "steady state" associations' distribution could then be calculated. A comprehensive discussion on Perron-Frobenious theorem can be found in [5].
CG [23] represents the gain of the current association over average confidence of similar associations. Formally, let I={I j , j=1,2...m} be a set of items. Let X  X  I, Y  X  different items. Let D be a set of transactions, where each form X  X  Y is defined as follows. Support measure Supp(X) is the fraction of transcriptions that contains item X in database D. The degree of support and confidence (Conf) for the rule (X  X  Y) is defined by: [3] the number of transactions containing X, and ||X&amp;Y|| is the number of transactions containing both X and Y. For the current discussion, X represents a stimulus term, and Y represents a possible response. A transaction is a web page containing a set of items. For a fixed stimulus, Conf is a function of all possible responses Y. The average confidence of response Y ) ( Conf Y is calculated by eq [2]. where n is the number of valid instances. A valid instance is a rule (I i  X  Y) having support and confidence above certain thresholds. CG, is a function of both X and Y: 
The averaged confidence is the essence of the new measure. It enables computing the average (expected) confidence of a term, while preserving a sense of locality, by performing the averaging process over other terms from connected to the stimulus term X. The process is based on parsing a set of the first 250 pages retrieved by Google search engine for a stimulus query phrase. For each phrase, the number of pages in which the phrase appears (1 to 250) is counted. The CG algorithm includes the following stages: a. Perform a search over stimulus term and retrieve 250 sites using Google-api [7]. b. Scan sites text and select 100 words having the highest frequency of appearance. c. Form a list of all possible couples (stimulus-response) of two words (10,000 couples). d. By using Google-API get the number of appearances over the Internet for each couple and its stimulus (first term). e. Calculate confidence of each couple by formula [3]. f. Calculate averaged confidence for each of the 100 terms by formula [4]. g. Calculate CG of each of the couples by formula [5]. [5] h. Return top ranked words sorted by descending CG value. 
Note that the numbers 250, 100 used in the algorithm were chosen empirically since they are large enough to represent the query results space, while still enabling a reasonable execution time of the algorithm. The algorithm was found to be not sensitive to the exact numbers. 
The main idea behind the Local Confidence Gain ("LCG") is to make better use of the retrieved documents. While in the basic CG measure, the retrieved sites are used merely for building the most frequent terms list, the local CG uses the term distribution for calculating the average confidence and CG. In order to compensate for the "locality" of the data (using a small number of web pages frequency of each response in the local dataset is compared compensation factor LC ("lingual confidence") is defined as: [6] N where N is the number of pages in the local dataset (typically 250), n is the total number of web pages available through a given search engine, N(R) is the number of sites number of web pages in the Internet that contain the term R. LCG is formulated as: [7] ) ( ) ( ) ( Y LC Y X CG Y X LCG  X   X  =  X  Note that if the local dataset is the entire Internet, LC=1 and LCG=CG. The modification can be justified in many ways such as looking at LC as a measure of the frequency of a specific term compared to its frequency in the English language. The advantage of LCG is its low complexity. Since stage e in the LCG algorithm is performed over the local dataset containing the retrieved sites, it is significantly faster. The LC calculation demands only 100 queries of Google-api, instead of 10,000 needed for calculating CG. A typical execution time for calculating LCG (for 250 pages and 100 most frequent terms) is smaller than one minute. 
This article extends the concept of random walk in directed graphs to the associative world. The LCG measure enables the creation of a directed graph representation of the links between concepts. The links are both directed and weighted. Using the random walk process on graphs enables us to simulate the associative flow between concepts (the nodes of the graph) and reach a steady state, revealing the probable outcome of a process. It is possible to simulate the importance of any link in the graph to the final outcome and possibly suggest a change in specific links so that the wanted outcome will be reached. The algorithm is called "Associations Rank" (AR). AR turns the PageRank algorithm to being query dependant. This enables the result to be the most relevant, whereas the simple PageRank algorithm only retrieves the most important documents. A large scale Turing Test was performed to show that concepts chosen by the AR algorithm are similar to associations generated by human beings for the same stimuli (inputs). Then, the random walk process was used to extract concepts representing the user contemporary profile. By analyzing proxy logs of the surfer's history the essence of the user interest was extracted. Finally, AR was used to "answer" user questions asked in natural language. 
Two of the most important characteristics of human associations are their asymmetry (e.g. the association of "Shampoo"  X  "Hair" is much stronger than "Hair"  X  "Shampoo") and the extensive usage on contextual (or background) knowledge. As mentioned before, terms such as "Shampoo" and "Hair" are the nodes of the graph while the edges exist where there is a high CG connection between any two nodes (terms). 
As in the concept of the PageRank algorithm, let us begin with terms represented as unconnected nodes randomly located on a plane. Suppose we begin with a random node (concept) and then perform a random walk to another node. The edges that connect the nodes are the associations. The following measure can then be formulated (named  X  X ssociation Rank X  or  X  X R X ): 
Here ) ( k i AR is the Association Rank of site i in the k iteration. AR represents the probability that concept i will basic concept behind the AR formula is that the probability that a concept will be reached is the sum of probabilities of reaching it from any related concept. For each concept j the probability to move to concept i is normalized by dividing the strength value (LCG) of the association between them (in direction from j to i) divided by the total association define matrix LCG as () () we get the matrix formulation: process can be solved in two ways. The first is by iterating formula [9] and the second is by finding the eigenvector that matches the largest eigenvalue, as discussed in the following section. In order for the process to converge, let us recall the Frobenious theorem. For a regular stochastic matrix M, if p is any probability distribution then 1 v p M q  X  as q goes to infinity. AR can easily become a probability distribution if entries for the LCG matrix to become regular, each entry must be made positive by replacing each null entry (i.e. any entry which represents a case where there is no associative link) with a small positive value  X  . Replacing null LCG values by  X  is equivalent to saying that there is a very small probability to move from one concept to any other concept. This intuitively does not contradict any of the pervious assumptions. 
Finally, the process of setting each entry ij to assume the form: 
Since LCG is regular and stochastic the process must converge to the eigenvector that corresponds to eigenvalue one. The following figure gives an example of a directed graph containing four nodes (concepts), Figure 1. Example of a Directed Graph Containing Four Concepts where L i  X  j is the measure LCG(i  X  j). L i  X  i represents the small probability for staying on the same node, and is inserted to ensure algorithmic convergence. Thus, the probability of moving from node 1 to node 3 at a stage n is given by the expression: 
The actual pages the user visits can be used as the retrieved by a search engine. Since the visited sites are temporarily stored on the user X  X  local machine, the process Internet. The algorithm can be performed in background at a preset frequency (typically every five minutes). a. Retrieve new pages visited since last execution. b. Create a list of all phrases of consecutive words up to n terms (n is a parameter set in advance  X  typically 2). c. Select the top 100 phrases that occur in as many of these documents as possible. d. Form the LCG matrix. Set the diagonal values and every null entry to 1e-8. Form LCG matrix by normalization. e. Perform 100 iterations (see eq. [10]). Compute the profile. 
The diagonal values are set to a small value (step d) since they represent an unwanted static association. All other nulls are set to small values for the matrix to be solve eq. [10] iteratively rather than directly computing the eigenvector that corresponds to the eigenvalue one. 
This process enables the user to get useful data as he surfs the Web, as we shall demonstrate later. 
One of the weaknesses of the Google's PageRank algorithm is that it retrieves the most important pages rather than the most relevant ones. This phenomenon occurs since the basic ranks are determined prior to the query (even though some adjustments are performed after the query phrase is presented). Our approach goes a step farther. After retrieving an initial set of pages, the AR algorithm is applied to find the most informative phrases within these pages. These phrases are then used to narrow the initial query. The "narrowed" phrase gives more focused results. The narrowing process can be unsupervised or supervised where the user selects which of the phrases best reflects his intentions. Phrase extraction is also needed where the user wishes to retrieve phrases that are more accurate, such as an exact disease name or a technical expression (e.g. "sunk cost", "solar eclipse"). The justification for such a process emerges form the fact that in many cases people can identify and categorize phrases as relevant even if they didn X  X  think of them in the first place. For example, if one is looking for information on the moon as it looks at the beginning of the month, then a good query phrase would be "lunar phases". To most people the term "lunar phases" would not "jump into thought" as appropriate, but when presented with it they will agree that it is a good addition to the query terms. 
The Turing Test is based on his claim that for a machine to succeed it must "fool" a human being. In the main experiment that tests this premise, the machine and a human are located in separate rooms. They must respond to identical questions posed by a human interrogator. If the interrogator cannot differentiate between the machine and a human, the machine is passes the Turing Test. In the current research 900, people were given questionnaires containing two sets of stimuli and responses. These people played the part of the interrogators in the Turing Test. Several researches explored association rules measures (see [19], [2]). In order to compare the AR algorithm with other association rules measures and human association, we selected seven asymmetrical association measures from added Lift measure (as a representative of the symmetrical association rules measures) LCG and AR measures. We focused on asymmetrical measures since, as indicated before, human associations are strongly asymmetrical. 
We used two large psychological databases as sources of human associations: 1. "The MRC Psycholinguistic Database: Machine Readable Dictionary" [13] (denoted EAT database); 2. "The University of South Florida Word Association, Rhyme, and Word Fragment Norms" [18] (denoted FAN database). The following method was employed in order to select valid stimuli set: a. FAN and EAT databases were compared and the top 100 stimuli terms having the largest overlap (identical responses) were selected (the stimuli set denoted as "S"); b. For each stimulus, the first 250 web pages (retrieved by Google search engine) were scanned and the 100 terms that appeared with the highest frequency in the scanned pages were stored (denoted as "top 100" set). The S set was used as a base dataset for evaluating the performance of the association measures. The asymmetric measures presented in formula [28] were used to rank each "top 100" set. For each measure, the response with the highest associative strength to the stimulus was given the value 1, and the response having the weakest associative strength to the stimulus was given the rank 100. The criterion for comparison was the claim that a better measure corresponds with a smaller sum. The following table shows the sum of overlapping terms ranks for each measure. Table 1. Sum of Overlapping Response Ranks for the "Top We selected the most promising five association rules, having the smallest sum of overlapping terms ranks, for further evaluation (LCG, AR, Lift, Klosgen and J-Measure). We then randomly selected 50 of the 100 top overlapping terms and built 70 questionnaires, with each containing two lists of stimuli (input terms) and responses (output terms). One of the sets was extracted from the EAT database while the other was a ranked set of responses of one of the examined association measures. Ten questionnaires containing only human associations were used as a validity check for the test. The following table contains the total number of correct and incorrect answers for each measure. A correct answer was where the interrogator succeeded to identify the human associations. Table 2. Distribution of Correct and Incorrect Answers. 
The best measure was the one who scored the highest ratio of number of "wrong" answers, since it was capable of "fooling" more people. Logically, the upper boundary of the wrong-to-total ratio was 50%, since the best an associations measure can do is to perform as well as a human being. In this case, the interrogator was not able to decide who is the human and the answers would distribute evenly giving about 50% of the votes to each candidate. Thus, by defining the score of each measure as %Incorrect Answers x 100/50, the AR was ranked highest by 90 (out of 100). 
A test group was then given a questionnaire containing human associations in both candidates. While being asked to decide which of the two subjects is human the results were 48% of the votes for candidate 1 and 52% of the votes for candidate 2. This result conformed to the expected values (50% for both candidates). 
The current test had one significant difference from the original Turing Test. While in the original Turing Test, there is an unlimited set of questions from which the questions and answers are given to the interrogator. This might have biased the decision of interrogators who might not have understood some of the terms. In order to overcome this problem, each interrogator was asked to grade his/her confidence level (0  X  not confident  X  5  X  very confident). By selecting only those who rated a high confidence level (4 or 5), we were able to eliminate cases where the interrogator did not understand the stimuli or responses. The measures performances are shown in the following table. Table 3. Distribution of Correct and Incorrect Answers for 
Although the AR score is somewhat smaller in this case  X  it is still better than other measures. 
The following two sections demonstrate the ability of the AR measure to extract valuable phrases from text. While section 3.2 focuses on the ability to track the user actions and predict user interests while surfing, section 3.3 starts with a seed query phrase and uses the AR algorithm over the query results to extract highly informative terms. These terms can be used to expand the query and extract more focused results. 
The IRCache Internet pages caching project [9] founded in 1995, has two goals: provide operational hierarchical caching services and provide large amounts of trace log file data to researchers and other organizations. The cache contains logs of proxy request collected from ten servers in the USA. We analyzed data taken on 12 September 2004 (containing more than five million entries). The data in the contained the sorted list of pages visited by the many users. We selected four logs and focused on the html pages. The convenience only the first four pages are presented), the results of AR algorithm (the ten highest ranked terms) and the speculated interest topic of each session. Table 4. Using the AR Algorithm to Analyze User Surfing 1.http://afiwcweb.lackland. af.mil/battlelab/index.htm 2.http://fmso.leavenworth. army.mil/fmsopubs/fmsop ubs.htm 3.http://images.kazaa.com/ generic_report/272.html 4.http://images.kazaa.com/ generic_report/res_1024.ht ml 1.http://mailing.worldsexci nema.com/galleries/animal fuckers/galleryaf09_y1.ht ml 2.http://publish.aps.org/ST ATUS/wvman1.html 3.http://www.aip.org/pubs ervs/style.html 4.http://www.aip.org/pubs ervs/style/4thed/toc.html 1.http://mail.gnome.org/arc hives/gtk-list/2004-September/msg00049.html 2.http://mail.gnome.org/arc hives/gtk-list/2004-September/msg00044.html 3.http://mail.gnome.org/arc hives/gtk-list/2004-September/msg00005.html 4.http://207.242.75.40/derb tech/windtunl.htm 1.http://www.jhuapl.edu/st ates/fl_0.html 2.http://www.chubby-pussy.com/gal010/thehun. html 3.http://www.cnn.com/200 4/SHOWBIZ/09/09/showb uzz/index.html 4.http://www.cnn.com/200 4/SHOWBIZ/Movies/09/0 9/neve.campbell.reut/index .html 
As can be seen, the terms extracted by the AR algorithm are good descriptors of user interest. These terms can function as the user contemporary profile and be used for further extraction of relevant web pages. Note that the fourth column (Speculated Topic) is only presented for the reader's convenience. In actual cases the top ranked terms are selected as topic descriptors. 
In our third experiment we began with just a simple question or a phrase. This phrase was fed into the Google search engine. The query results were analyzed by the AR algorithm, and the highest ranked terms were collected for further use. We then used the highest ranked terms to expand the original query and refine the search. The following table holds the query phrases, the AR algorithm highest ranked terms, the revised search phrase and it's first retrieved web page. 
Informative Terms (The extracted terms are used for query Original Query Phrase 
Sleep and appet-ite 
What is the origin of the human race
What is the furthest star in the solar system 
Hitchc-ock movies 
Once again the AR algorithm extracted terms that served as descriptors and could be used for farther query expansion. We chose to expand the query phrase by simply adding the first four terms extracted by the AR algorithm. A different approach would have been to give the user the list of extracted terms. The user would then be able to the original query phrase. 
Note that the question "what is the furthest star in the solar system?" was not stated properly, since the users meant to find the furthest Planet and not Star. The AR algorithm was able to find the term, "Pluto" (the fourth of the ranked terms) despite this "mistake". Furthermore, very common terms such as "who", "what", "is" are ignored by the Google search engine. Thus, for example, the query "what is the furthest star in the solar system", translated to only four terms "furthest, star, solar, and system". 
This chapter presented three different approaches for validation of the AR algorithm. First -the "Free Associations Turing Test" confronts the question whether the AR algorithm can "imitate" human associations. The second -analyzing proxy logs deals with the question whether the AR algorithm can extract human-interest topics from the user's surfing history. And finally, the third part deals with the question whether the AR algorithm can help the user to better express his thoughts and reach valuable information more efficiently. 
While the answer to the first question is based on a solid measures, 100 seed terms, 70 different questionnaires and 900 people who answered the different questionnaires), the latter two were based on much smaller samples. Thus the conclusions were qualitative rather than quantitative. Still, using the AR algorithm to extract highly informative phrases has been shown. 
The AR algorithm that extracts highly informative phrases was shown capable of both "imitating" humans by creating human-like associations and giving highly informative descriptors of Internet pages. The extracted descriptors were used to reveal the user current interest topics and to supply query expansion terms. The algorithm complexity is low, and can be performed in near real-time. The success of the algorithm raises the question whether actual human thinking processes are performed similarly. The algorithm is rather robust and can be employed for various usages, or to even use other asymmetrical association measures as links between concepts. One of the most promising usages we forecast for this algorithm is user conceptual modeling and prediction. By using the AR algorithm to extract concepts and associations from the pages the user visits, an analysis of "flow" in a conceptual graph can show trends and determine the probability for the user to reach certain concepts within his current surfing session. In future work we intend to identify common properties for similar users, and define a profiling technique based on the random walk algorithm. Many other interesting challenges are still waiting to be addressed in the web search algorithms area. For a good review of the most promising is the partitioning of the web by eigenvectors see [14],[10],[8]. [2] A.A. Freitas "On rule interestingness measures". [4] A.P. Saygin , I. Cicekli , and V. Akman, "Turing [5] C.R. MacCluer, "The many proofs and applications [7] Google API. http://www.google.com/apis/ [8] Haveliwala T.H., and S.D. Kamvar, " The second [9] IRCache Internet caching project. [10] J. Shi and J. Malik, "Normalized Cuts and Image [14] M.R. Henzinger "Algorithmic Challenges in Web [16] N. Deo, and P. Gupta, "Sampling the Web With [17] N. Deo, and P. Gupta, "Graph-Theoretic Web [18] Nelson D.L., C.L. McEvoy, and Schreiber T.A. [19] P. Tan, V. Kumar, and J. Srivastava, "Selecting the [21] R. Lempel and S. Morran , "SALSA: The stochastic [26] S. Brin, and L. Page, "The anatomy of a large-scale [28] T.J. Palmeri, "An exemplar-based random walk 
