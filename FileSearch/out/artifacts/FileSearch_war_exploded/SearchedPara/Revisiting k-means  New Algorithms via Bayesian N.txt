 Department of CSE, Ohio State University, Columbus, OH There is now little debate that Bayesian statistics have had tremendous impact on the field of machine learning. For the problem of clustering, the topic of this paper, the Bayesian approach allows for flexible models in a variety of settings. For instance, Latent Dirichlet Allocation (Blei et al., 2003), a hierarchical mixture of multinomials, re-shaped the topic modeling community and has become a standard tool in document analysis. Bayesian nonparamet-ric models, such as the Dirichlet process mixture (Hjort et al., 2010), result in infinite mixture models which do not fix the number of clusters in the data upfront; these meth-ods continue to gain popularity in the learning community. Yet despite the success and flexibility of the Bayesian framework, simpler methods such as k-means remain the preferred choice in many large-scale applications. For in-stance, in visual bag-of-words models (Fei-Fei &amp; Perona, 2005), large collections of image patches are quantized, and k-means is universally employed for this task. A major motivation for using k-means is its simplicity and scalabil-ity: whereas Bayesian models require sampling algorithms or variational inference techniques which can be difficult to implement and are often not scalable, k-means is straight-forward to implement and works well for a variety of ap-plications.
 In this paper, we attempt to achieve the best of both worlds by designing scalable hard clustering algorithms from a Bayesian nonparametric viewpoint. Our approach is in-spired by the connection between k-means and mixtures of Gaussians, namely that the k-means algorithm may be viewed as a limit of the expectation-maximization (EM) algorithm X  X f all of the covariance matrices correspond-ing to the clusters in a Gaussian mixture model are equal to  X I and we let  X  go to zero, the EM steps approach the k-means steps in the limit. As we will show, in the case of a Dirichlet process (DP) mixture model X  X he standard Bayesian nonparametric mixture model X  X e can perform a similar limiting argument in the context of a simple Gibbs sampler. This leads to an algorithm with hard cluster as-signments which is very similar to the classical k-means algorithm except that a new cluster is formed whenever a point is sufficiently far away from all existing cluster cen-troids. Further, we show that this algorithm monotonically converges to a local optimum of a k-means-like objective which includes a penalty for the number of clusters. We then take a step further into the realm of hierarchical Bayesian models, and extend our analysis to the hierarchi-cal Dirichlet process (HDP) (Teh et al., 2006). The HDP is a model for shared clusters across multiple data sets; when we take an analogous asymptotic limit for the HDP mix-ture, we obtain a novel k-means-like algorithm that clus-ters multiple data sets with shared cluster structure. The resulting algorithm clusters each data set into local clus-ters, but local clusters are shared across data sets to form global clusters. The underlying objective function in this case turns out to be the k-means objective with additional penalties for the number of local clusters and the number of global clusters.
 To further demonstrate the practical value of our approach, we present two additional extensions. First, we show that there is a spectral relaxation for the k-means-like objective arising from the DP mixture. Unlike the standard relaxation for k-means, which computes the top-k eigenvectors, our relaxation involves computing eigenvectors corresponding to eigenvalues above a threshold, and highlights an inter-esting connection between spectral methods and Bayesian nonparametrics. Second, given existing connections be-tween k-means and graph clustering, we propose a penal-ized normalized cut objective for graph clustering, and uti-lize our earlier results to design an algorithm for monotonic optimization. Unlike the standard normalized cut formula-tion (Shi &amp; Malik, 2000; Yu &amp; Shi, 2003), our formula-tion does not fix the number of clusters in the graph. We conclude with some results highlighting that our approach retains the flexibility of the Bayesian models while featur-ing the scalability of the classical techniques. Ultimately, we hope that this line of work will inspire additional re-search on the integration of Bayesian nonparametrics and hard clustering methods. We begin with a short discussion of the relevant models and algorithms considered in this work: mixtures of Gaussians, k-means, and DP mixtures. 2.1. Gaussian Mixture Models and k-means In a (finite) Gaussian mixture model, we assume that data arises from the following distribution: where k is the fixed number of components,  X  c are the mixing coefficients, and  X  c and  X  c are the means and co-variances, respectively, of the k Gaussian distributions. In the non-Bayesian setting, we can use the EM algorithm to perform maximum likelihood given a set of observations x ,..., x n .
 A related model for clustering is provided by the k-means objective function, an objective for discovering a hard clus-tering of the data. Given a set of data points x 1 ,..., x the k-means objective function attempts to find clusters ` ,...,` k to minimize the following objective function: The most popular method for minimizing this objective function is simply called the k-means algorithm. One ini-tializes the algorithm with a hard clustering of the data along with the cluster means of these clusters. Then the algorithm alternates between reassigning points to clusters and recomputing the means. For the reassignment step one computes the squared Euclidean distance from each point to each cluster mean, and finds the minimum, by computing ` ( i ) = argmin c k x i  X   X  c k 2 2 . Each point is then reassigned to the cluster indexed by `  X  ( i ) . The centroid update step of the algorithm then recomputes the mean of each cluster, updating  X  c for all c .
 The EM algorithm for mixtures of Gaussians is quite simi-lar to the k-means algorithm. Indeed, one can show a pre-cise connection between the two algorithms. Suppose in the mixture of Gaussians model that all Gaussians have the same fixed covariance equal to  X I . Because they are fixed, the covariances need not be re-estimated during the M-step. In this case, the E-step takes the following form: where  X  ( z ic ) is the probability of assigning point i to clus-ter c . It is straightforward to show that, in the limit as  X   X  0 , the value of  X  ( z ic ) approaches zero for all c ex-cept for the one corresponding to the smallest distance k x i  X   X  c k 2 2 . In this case, the E-step is equivalent to the reassignment step of k-means, and one can further easily show that the M-step exactly recomputes the means of the new clusters, establishing the equivalence of the updates. We also note that further interesting connections between k-means and probabilistic clustering models were explored in Kurihara &amp; Welling (2008). Though they approach the problem differently (i.e., not from an asymptotic view), the authors also ultimately obtain k-means-like algorithms that can be applied in the nonparametric setting. 2.2. Dirichlet Process Mixture Models We briefly review DP mixture models (Hjort et al., 2010). We can equivalently write the standard Gaussian mixture as a generative model where one chooses a cluster with prob-ability  X  c and then generates an observation from the Gaus-sian corresponding to the chosen cluster. The distribution over the cluster indicators follows a discrete distribution, so a Bayesian extension to the mixture model arises by first placing a Dirichlet prior of dimension k , Dir ( k,  X  0 ) , on the mixing coefficients, for some  X  0 . If we further assume that the covariances of the Gaussians are fixed to  X I and that the means are drawn from some prior distribution G 0 , we obtain the following Bayesian model: letting  X  = (  X  1 ,..., X  k ) . One way to view the DP mixture model is to take a limit of the above model as k  X  X  X  when choosing  X  0 = (  X /k ) e , where e is the vector of all ones. One of the simplest algorithms for inference in a DP mix-ture is based on Gibbs sampling; this approach was utilized by West et al. (1994) and further discussed by Neal (2000), Algorithm 2. The state of the underlying Markov chain consists of the set of all cluster indicators and the set of all cluster means. The algorithm proceeds by first looping repeatedly through each of the data points and performing Gibbs moves on the cluster indicators for each point. For i = 1 ,...,n , we reassign x i to existing cluster c with prob-ability n  X  i,c  X N ( x i |  X  c , X I ) /Z, where n  X  i,c is the number of data points (excluding x i ) that are assigned to cluster c . With probability we start a new cluster. Z is an appropriate normalizing con-stant. If we end up choosing to start a new cluster, we select its mean from the posterior distribution obtained from the prior G 0 and the single sample x i . After resampling all clusters, we perform Gibbs moves on the means: we sam-ple  X  c given all points currently assigned to cluster c,  X  c . In the following sections, we derive hard clustering algo-rithms based on DP mixture models. We will analyze prop-erties of the resulting algorithms and show connections to existing hard clustering algorithms, particularly k-means. 3.1. Asymptotics of the DP Gibbs Sampler Using the DP mixture model introduced in the previous section, let us first define G 0 . Since we are fixing the covariances, G 0 is the prior distribution over the means, which we will take to be a zero-mean Gaussian with  X I co-variance, i.e.,  X   X  N ( 0 , X I ) . Given this prior, the Gibbs probabilities can be computed in closed form. A straight-forward calculation reveals that the probability of starting a new cluster is equal to: Similarly, the probability of being assigned to cluster c is equal to Z normalizes these probabilities to sum to 1. We now would like to see what happens to these probabilities as  X   X  0 . However, in order to obtain non-trivial assign-ments, we must additionally let  X  be a function of  X  and  X  . In particular, we will write  X  = (1+  X / X  ) d/ 2  X  exp (  X  some  X  . Now, let  X   X  ( z ic ) correspond to the posterior proba-bility of point i being assigned to cluster c and let  X   X  ( z be the posterior probability of starting a new cluster. Af-ter simplifying, we obtain the following probabilities to be used during Gibbs sampling:  X   X  ( z ic ) = for existing clusters and  X   X  ( z i,new ) = for generating a new cluster. Now we consider the asymp-totic behavior of the above probabilities. The numerator for  X   X  ( z i,new ) can be written as It is straightforward to see that, as  X   X  0 with a fixed  X  , the  X  term dominates this numerator. Furthermore, all of the above probabilities will become binary; in particu-dominated by the smallest value of {k x i  X   X  1 k 2 ,..., k x  X  k 2 , X  } . In the limit, only the smallest of these values will receive a non-zero  X   X  value. The resulting update, therefore, takes a simple form that is analogous to the k-means cluster reassignment step. We reassign a point to the cluster corre-sponding to the closest mean, unless the closest cluster has squared Euclidean distance greater than  X  . In this case, we start a new cluster.
 If we choose to start a new cluster, the final step is to sam-ple a new mean from the posterior based on the prior G 0 and the single observation x i . Similarly, once we have per-formed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on G 0 and all observa-tions in a cluster. Since the prior and likelihood are Gaus-sian, the posterior will be Gaussian as well. If we let  X  x the mean of the points currently assigned to cluster c and n c be the number of points assigned to cluster c , then the Algorithm 1 DP-means posterior is a Gaussian with mean  X   X  c and covariance  X  where As before, we consider the asymptotic behavior of the above Gaussian distribution as  X   X  0 . The mean of the Gaussian approaches  X  x c and the covariance goes to zero, meaning that the mass of the distribution becomes concen-trated at  X  x c . Thus, in the limit we choose  X  x c as the mean. Putting everything together, we obtain a hard clustering algorithm that behaves similarly to k-means with the ex-ception that a new cluster is formed whenever a point is farther than  X  away from every existing cluster centroid. We choose to initialize the algorithm with a single cluster whose mean is simply the global centroid; the resulting al-gorithm is specified as Algorithm 1, which we denote as the DP-means algorithm. Note that, unlike standard k-means, which depends on the initial clustering of the data, the DP-means algorithm depends on the order in which data points are processed. One area of future work would consider adaptive methods for choosing an ordering. 3.2. Underlying Objective and the AIC With the procedure from the previous section in hand, we can now analyze its properties. A natural question to ask is whether there exists an underlying objective function cor-responding to this k-means-like algorithm. In this section, we show that the algorithm monotonically decreases the following objective at each iteration, where an iteration is defined as a complete loop through all data points to update all cluster assignments and means: This objective is simply the k-means objective function with an additional penalty based on the number of clus-ters. The threshold  X  controls the tradeoff between the tra-ditional k-means term and the cluster penalty term. We can prove the following: Theorem 3.1. Algorithm 1 monotonically decreases the objective given in (1) until local convergence.
 Proof. The proof follows a similar argument as the proof for standard k-means. The reassignment step results in a non-increasing objective since the distance between a point and its newly assigned cluster mean never increases; for distances greater than  X  , we can generate a new cluster and pay a penalty of  X  while still decreasing the objective. Sim-ilarly, the mean update step results in a non-increasing ob-jective since the mean is the best representative of a cluster in terms of the squared Euclidean distance. The fact that the algorithm will converge locally follows from the fact that the objective function cannot increase, and that there are only a finite number of possible clusterings of the data. Perhaps unsurprisingly, this objective has been studied in the past in conjunction with the Akaike Information Cri-terion (AIC). For instance, Manning et al. (2008) describe the above penalized k-means objective function with a mo-tivation arising from the AIC. Interestingly, it does not ap-pear that algorithms have been derived from this particu-lar objective function, so our analysis seemingly provides the first constructive algorithm for monotonic local conver-gence as well as highlighting the connections to the DP mixture model. Finally, in the case of k-means, one can show that the complete-data log likelihood approaches the k-means objective in the limit as  X   X  0 . We conjecture that a similar result holds for the DP mixture model, which would indicate that our result is not specific to the particu-lar choice of the Gibbs sampler. One of the most useful extensions to the standard DP mix-ture model arises when we introduce another DP layer on top of the base measure. Briefly, assume we have a set of data sets, each of which is modeled as a DP mixture. How-ever, instead of defining the base measure of each DP mix-ture using G 0 , the prior over the means, we instead let G itself be a Dirichlet process whose base measure is a prior over the means. The result is that, given a collection of data sets, we can cluster each data set while ensuring that the clusters across the data sets are shared appropriately. Due to space restrictions, we cannot describe the resulting hierarchical Dirichlet process (HDP) nor its corresponding sampling algorithms in detail, but we refer the reader to Teh et al. (2006) for a detailed introduction to the HDP model and a description of inference techniques. We will see that the limiting process described earlier for the standard DP can be straightforwardly extended to the HDP; we will out-line the algorithm below, and Figure 1 gives an overview of the approach.
 To set the stage, let us assume that we have D data sets, 1 ,...,j,...,D . Denote x ij to be data point i from data set j , and let there be n j data points from each data set j . The basic idea is that we will locally cluster the data points from each data set, but that some cluster means will be shared across data sets. Each data set j has a set of local cluster indicators given by z ij such that z ij = c if data point i in data set j is assigned to local cluster S jc . Each local cluster S jc is associated to a global cluster mean  X  p . 4.1. The Hard Gaussian HDP We can now extend the asymptotic argument that we em-ployed for the hard DP algorithm to the HDP. We will sum-marize the resulting algorithm; the derivation is analogous to the derivation for the single DP mixture case. As with the hard DP algorithm, we will have a threshold that deter-mines when to introduce a new cluster. For the hard HDP, we will require two parameters: let  X  ` be the  X  X ocal X  thresh-old parameter, and  X  g be the  X  X lobal X  threshold parameter. The algorithm works as follows: for each data point x ij we compute the distance to every global cluster  X  p . For any global cluster p for which there is no current associa-tion in data set j , we add a penalty of  X  ` to the distance (intuitively, this penalty captures the fact that if we end up assigning x ij to a global cluster that is not currently in use by data set j , we will incur a penalty of  X  ` to create a new local cluster, which we only want to do if the cluster if suf-ficiently close to x ij ). We reassign each data point x its nearest cluster, unless the closest distance is greater than  X  +  X  g , in which case we start a new global cluster (in this case we are starting a new local cluster and a new global cluster, hence the sum of the two penalties). Then, for each local cluster, we consider whether to reassign it to a dif-ferent global mean: for each local cluster S jc , we compute the sum of distances of the points to every  X  p . We reassign the association of S jc to the corresponding closest  X  p ; if the closest is farther than  X  g plus the sum of distances to the local cluster mean, then we start a new global cluster whose mean is the local mean. Finally, we recompute all means  X  p by computing the mean of all points (over all data sets) associated to each  X  p . See Algorithm 2 for the full specification of the procedure; the algorithm is derived directly as an asymptotic hard clustering algorithm based on the Gibbs sampler for the HDP.
 As with the DP-means algorithm, we can determine the un-derlying objective function, and use it to determine conver-gence. Let k = P D j =1 k j be the total number of local clus-ters, and g be the total number of global clusters. Then we can show that the objective optimized is the following: min This objective is pleasantly simple and intuitive: we mini-mize the global k-means objective function, but we incor-porate a penalty whenever either a new local cluster or a new global cluster is created. With appropriately chosen  X  and  X  g , the result is that we obtain sharing of cluster struc-ture across data sets. We can prove that the hard Gaus-sian HDP algorithm monotonically minimizes this objec-tive (the proof is similar to Theorem 3.1).
 Theorem 4.1. Algorithm 2 monotonically minimizes the objective (2) until local convergence. We now discuss two additional extensions of the proposed objective: a spectral relaxation for the proposed hard clus-tering method and a normalized cut algorithm that does not fix the number of clusters in the graph. 5.1. Spectral Meets Nonparametric Recall that spectral clustering algorithms for k-means are based on the observation that the k-means objective can be relaxed to a problem where the globally optimal solu-tion may be computed via eigenvectors. In particular, for the k-means objective, one computes the eigenvectors cor-responding to the k largest eigenvalues of the kernel ma-trix K over the data; these eigenvectors form the glob-ally optimal  X  X elaxed X  cluster indicator matrix (Zha et al., 2001). A clustering of the data is obtained by suitably post-processing the eigenvectors, e.g., clustering via k-means. Algorithm 2 Hard Gaussian HDP In a similar manner, in this section we will show that the globally optimal solution to a relaxed DP-means objective function is obtained by computing the eigenvectors of the kernel matrix corresponding to all eigenvalues greater than  X  , and stacking these into a matrix. To prove the correct-ness of this relaxation, let us denote Z as the n  X  k cluster indicator matrix whose rows correspond to the cluster indi-cator variables z ic . Let Y = Z ( Z T Z )  X  1 / 2 be a normalized indicator matrix, and notice that Y T Y = I . We can prove the following lemma.
 Lemma 5.1. The DP-means objective function can equiv-alently be written as max Y tr( Y T ( K  X   X I ) Y ) , where the optimization is performed over the space of all normalized indicator matrices Y .
 Proof. It was shown in Zha et al. (2001) that the standard k-means function can be expressed as a trace maximiza-tion of tr( Y T KY ) , over the space of normalized indicator matrices Y . Noting that tr( Y T (  X I ) Y ) =  X k as Y is an orthogonal n  X  k matrix, the lemma follows.
 Now we perform a standard spectral relaxation by relaxing the optimization to be over all orthonormal matrices Y : Using standard arguments, one can show the following re-sult (proof omitted due to lack of space): Theorem 5.2. By relaxing the cluster indicator matrix Y to be any orthonormal matrix, the optimal Y in the relaxed clustering objective (3) is obtained by forming a matrix of all eigenvectors of K whose corresponding eigenvalues are greater than  X  .
 Using this result, one can design a simple spectral algo-rithm that computes the relaxed cluster indicator matrix Y , and then clusters the rows of Y , as is common for spectral clustering methods. Thus, the main difference between a standard spectral relaxation for k-means and the DP-means is that, for the former, we take the top-k eigenvectors, while for the latter, we take all eigenvectors corresponding to eigenvalues greater than  X  . 5.2. Graph Clustering It is also possible to develop extensions to the DP-means algorithm for graph cut problems such as normalized and ratio cut. We state a result proven in Dhillon et al. (2007) for standard k-way normalized cut.
 Theorem 5.3. Let J ( K,W ) be the weighted kernel k-means objective with kernel matrix K and (diagonal) weight matrix W , and let Cut ( A ) be the k-way normalized cut objective with adjacency matrix A . Let D be the diag-onal degree matrix corresponding to A ( D = diag ( A e ) ). Then the following relationship holds: J ( K,W ) =  X n +tr( D  X  1 / 2 AD  X  1 / 2 )  X  (  X  +1) k + Cut ( A ) , when we define K =  X D  X  1 + D  X  1 AD  X  1 , W = D , and  X  is large enough that K is positive semi-definite.
 Let the DP-means objective X  X asily extended to kernel space and to use weights X  X e given by J ( K,W ) +  X k , and let the analogous penalized normalized cut objective be given by Cut ( A )+  X  0 k . Letting  X n +tr( D  X  1 / 2 AD  X  1 / 2 C , a constant, we have that J ( K,W ) +  X k =
C + Cut ( A )  X  (  X  + 1) k +  X k = C + Cut ( A ) +  X  0 k, where  X  0 =  X   X   X   X  1 . Thus, optimizing the hard DP weighted kernel k-means objective with model parameter  X  is equivalent to optimizing the penalized normalized cut objective with model parameter  X  0 =  X   X   X   X  1 , and with the construction of K and W as in the above theorem. Uti-lizing the results of Dhillon et al. (2007), one can show that the distance between a node and a cluster mean can be performed in O ( | E | ) time. A straightforward extension of Algorithm 1 can then be adapted for the above penalized normalized cut objective. We conclude with a brief set of experiments to demonstrate the utility of our approach. The goal is to demonstrate that hard clustering via Bayesian nonparametrics enjoys many properties of Bayesian techniques (unlike k-means) but fea-tures the speed and scalability of k-means.
 Setup. Throughout the experiments, we utilize normalized mutual information (NMI) between ground truth clusters and algorithm outputs for evaluation, as is common for clustering applications (it also allows us to compare results when the number of outputted clusters does not match the number of clusters in the ground truth). Regarding param-eter selection, there are various potential ways of choosing  X  ; for clarity in making comparisons to k-means we fix k (and g ) and then find a suitable  X  . In particular, we found that a simple farthest-first heuristic is effective, and we uti-lize this approach in all experiments. Given an (approxi-mate) number of desired clusters k , we first initialize a set T with the global mean. We iteratively add to T by finding the point in the data set which has the maximum distance to T (the distance to T is the smallest distance among points in T ). We repeat this k times and return the value of the maximum distance to T in round k as  X  . We utilize a simi-lar procedure for the hard HDP, except that for  X  ` we aver-age the values of the above procedure over all data sets, and for  X  g we replace distances of points to elements of T with sums of distances of points in a data set to elements of T . For Gibbs sampling, we consider the model where the co-variances are fixed to  X I , there is a zero-mean  X I Gaussian prior on the means, and an inverse-Gamma prior on  X  . (For the benchmark data, we considered selection of  X  based on cross-validation, as it yielded better results, though this is against the Bayesian spirit.) We set  X  = 100 throughout our experiments. We also consider two strategies for de-termining  X  : one where we place a gamma prior on  X  , as is standard for DP mixtures (Escobar &amp; West, 1995), and another where we choose  X  via a validation set.
 DP-means Results. We begin with a simple illustration of some of the key properties of our approach on a synthetic data set of three Gaussians, shown in Figure 2a. When we run DP-means on this data, the algorithm terminates within 8 iterations with an average NMI score of .89 (based on 100 runs). In contrast, Figure 2b shows the NMI scores of the clusterings produced by two Gibbs runs (no burn-in) over the first 5000 iterations, one that learns  X  via a gamma prior, and another that uses a validation set to tune  X  . The learning approach does well around 1500 iterations, but eventually more than three clusters are produced, lead-ing to poor results on this data set. The validation approach yields three clusters, but it takes approximately 3000 iter-ations before Gibbs sampling is able to converge to three clusters (in contrast, it typically requires only three itera-tions before DP-means has reached an NMI score above .8). Additionally, we plot the number of clusters produced by DP-means as a function of  X  in Figure 2c; here we see that there is a large interval of  X  values where the algorithm returns three clusters. Note that all methods are initialized with all points in a single cluster; we fully expect that better initialization would benefit these algorithms.
 Next we consider a benchmarking comparison among k-means, DP-means, and Gibbs sampling to demonstrate comparable accuracies among the methods. We selected 8 common UCI data sets, and used class labels as the ground-truth for clusters. Each data set was randomly split 30/70 for validation/clustering (we stress that validation is used only for Gibbs sampling). On the validation set, we val-idated both  X  and  X  , which yielded the best results. We ran the Gibbs sampler for 1000 burn-in iterations, and then ran for 5000 iterations, selecting every 10 samples. The NMI is computed between the ground-truth and the com-puted clusters, and results are averaged over 10 runs. The results are shown in Table 1. We see that, as expected, the results are comparable among the three algorithms: DP-means achieves higher NMI on 5 of 8 data sets in compar-ison to k-means, and 4 of 8 in comparison to Gibbs sam-pling. To demonstrate scalability, we additionally ran our algorithms over the 312,320 image patches of the Photo Tourism data set (Snavely et al., 2006), a common vision data set. Each patch is 128-dimensional. Per iteration, the DP-means algorithm and the Gibbs sampler require simi-lar computational time (37.9 seconds versus 29.4 seconds per iteration). However, DP-means converges fully in 63 iterations, whereas obtaining full convergence of the Gibbs sampler is infeasible on this data set.
 Hard Gaussian HDP Results. As with DP-means, we demonstrate results on synthetic data to highlight the ad-vantages of our approach as compared to the baselines. We generate parameters for 15 ground-truth Gaussian distribu-tions (means are chosen uniformly in [0 , 1] 2 and covari-ances are . 01  X  I ). Then we generate 50 data sets as follows: for each data set, we choose 5 of the 15 Gaussians at ran-dom, and then generate 25 total points from these chosen Gaussians (5 points per Gaussian). An example of one of the 50 data sets is shown in Figure 2d; in many cases, it is difficult to cluster the data sets individually, as shown in the figure.
 Our goal is to find shared clusters in this data. To eval-uate the quality of results, we compute the NMI between the ground-truth and the outputted clusters, for each data set, and average the NMI scores across the data sets. As a baseline, we run k-means and DP-means on the whole data set all at once (i.e., we treat all twenty data sets as one large data set) as well as k-means and DP-means on the in-dividual data sets. k-means on the whole data set obtains an average NMI score of .77 while DP-means yields .73. When we run the hard Gaussian HDP, we obtain 17 global clusters, and each data set forms on average 4.4 local clus-ters per data set. The average NMI for this method is .81, significantly higher than the non-hierarchical approaches. When we run k-means or DP-means individually on each data set and compute the average NMI, we obtain scores of .79 for both; note that there is no automatic cluster sharing via this approach. The hard Gaussian HDP takes 28.8s on this data set, versus 2.7s for k-means on the full data. Conclusions and Open Problems. This paper outlines connections arising between DP mixture models and hard clustering algorithms, and develops scalable algorithms for hard clustering that retain some of the benefits of Bayesian nonparametric and hierarchical modeling. Our analysis is only a first step, and we note that there are several av-enues of future work, including i) improvements to the basic algorithms using ideas from k-means, such as local search (Dhillon et al., 2002), ii) spectral or semidefinite re-laxations for the hard Gaussian HDP, iii) generalizations to exponential family mixture models (Banerjee et al., 2005), and iv) additional comparisons to sampling-based and vari-ational inference methods.
 Acknowledgements. We thank Trevor Darrell and the anonymous reviewers for helpful suggestions.

