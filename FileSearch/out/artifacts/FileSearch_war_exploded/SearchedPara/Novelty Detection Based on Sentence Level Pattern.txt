 The detection of new informati on in a document stream is an important component of many potentia l applications. In this paper, a new novelty detection approach based on the identification of sentence level patterns is proposed. Given a user X  X  information need, some patterns in sentences such as combinations of query words, named entities and phrases, may contain more important and relevant information than single words. Therefore, the proposed novelty detection approach focuses on the identification of previously unseen query-related patterns in sentences. Specifically, a query is preprocessed and represented with patterns that include both query words and required answ er types. These patterns are used to retrieve sentences, which ar e then determined to be novel if it is likely that a new answer is pr esent. An analysis of patterns in sentences was performed with data from the TREC 2002 novelty track and experiments on novelty det ection were carried out on data from the TREC 2003 and 2004 novelty tracks. The experimental results show that the proposed pa ttern-based approach significantly outperforms all three baselines in terms of precision at top ranks. H.3.3 [ Information Search and Retrieval ]: Query formulation and retrieval models Novelty detection, information patterns, named entities The goal of research on novelty detec tion is to provide a user with a list of materials that are relevant and contain new information with respect to a user X  X  information need. The goal is for the user to quickly get useful information without going through a lot of redundant information, which is a tedious and time-consuming task. A variety of novelty measures have been described in the literature [6, 7, 22]. These definitions of novelty, however, are quite vague and seem only indirectly related to the intuitive notions of novelty. Usually new words appearing in an incoming sentence/story/document contribute to the novelty scores in various novelty measures though in different ways. We believe that patterns such as combinations of query words, named entities, phrases and etc, which indicate the presence of possible answers, may contain more important and relevant information than single words given a user X  X  request or information need. For example, query 306 from the TREC novelty track 2002 is about  X  X frican Civilian Deaths  X  . The user is asking for the number of civilian non-combatants that have been killed in the various civil wars in Africa. Therefore a number should appear in sentences that relevant to the query. Let us cons ider the following four sentences given below. Sentence 1 (Relevant):  X  It could not verify Somali claims of more than 100 civilian deaths X  . Sentence 2 (Relevant):  X  Natal's death toll includes another massacre of 11 ANC [ African National Congress] supporters X . Sentence 3 (Non-relevant):  X  Once the slaughter began, following the death of President Juvenal H abyarimana in an air crash on April 6, hand grenades were thrown into schools and churches that had given refuge to Tutsi civilians . X  Sentence 4 (Non-relevant):  X  A Ghana News Agency correspondent with the West African force said that rebels loyal to Charles Taylor began attacking the civilians shortly after the peace force arrived in Monrovia last Saturday to try to end the eight-month-old civil war . X  Each of the four sentences has two terms (in bold) that match the key words from the query. However, only the first two sentences are relevant sentences. Besides the tw o matching words, the first two sentences also have a number 100 a nd 11 (underlined), respectively. Hence, the first two sentences are both topically and typically relevant to the query. The third sentence and the forth sentence are not relevant to the query mainly because they do not contain a number which is required by the user.
 For the example query given above, it is very difficult for traditional word-based approaches to separa te the two non-relevant sentences (sentence 3 and sentence 4) from the two relevant sentences (sentence 1 and sentence 2). Furthermore, the two non-relevant sentences are very likely to be id entified as novel sentences simply because they contain many new words that do not appear in previous sentences. Therefore, a new approach that can identify query-related information patterns beyond single words is desired. This motivates our work in this paper. The idea of identifying query-related patterns in sentences is inspired by question answering tec hniques and is similar to passage retrieval for factoid questions. Each query could be treated as multiple questions; each question is represented by a few query words, and it requires a certain type of named entities as answers. Instead of extracting exact answers as in typical question answering systems [14,19,20], we propose to fi rst extract interesting sentences with certain patterns that incl ude both query words and required answer types, indicating the presence of potential answers to the questions, and then identify novel se ntences that are more likely to have new answers to the questions. The rest of the paper is organized as follows. Section 2 gives a brief overview of related work on novelty detection. Section 3 introduces our understanding of novelty, and elaborates an analysis of sentence level patterns, focusing on named entities, with the data from the TREC 2002 novelty track data. Section 4 describes the proposed pattern-based approach to novelty detection. The experimental design and results are shown in S ection 5. Section 6 summarizes the paper with conclusions and future work. Novelty detection has been done at three different levels: event level, sentence level and document level. Work on novelty detection at the event level arises from the Topic Detection and Tracking (TDT) research, which is concerned with online new event detection/first story detection [1 ,2,3,4,5,16,18]. Current techniques on new event detection are usually based on clustering algorithms. Some model (vector space model, language model, lexical chain, etc.) is used to represent each incoming news story/document. Each story is then grouped into clusters . An incoming story will either be grouped into the closest cluster if the similarity score between them is above the preset similarity thres hold or start a new cluster. A story which started a new cluster will be marked as the first story about a new topic, or it will be marked as  X  X ld X  (about an old event) if there exists a novelty threshold and the si milarity score between the story and its closest cluster is great er than the novelty score. Research on novelty detection at the sentence level is related to the TREC novelty track for finding relevant and novel sentences given a queryand an ordered list of relevant documents [7, 8, 9, 10, 11, 12, 13, 22]. Novelty detection could be also performed at the document level, for example, in Zhang et al X  X  work [13] on novelty and redundancy detection in adaptive filtering, and in Zhai et al X  X  work [17] on subtopic retrieval. In current techniques developed for novelty detection at the sentence level or document level, new words appearing in sentences/docum ents usually contribute to the scores that are used to rank sentences/documents. Many similarity functions used in information re trieval are also tried in novelty detection. Usually a high similarity score between a sentence and a given query will increase the relevance rank of the sentence while a high similarity score between the sentence and all previously seen sentences will decrease the novelty rank of the sentence, for example, the Maximal Marginal Relevance model (MMR) introduced by Carbonell and Goldstein [23]. There are two main differences be tween our proposed approach and the approaches in the literature. First, none of the work described above treats new information as new answers to questions that represented users X  information requests, which we believe is essential in novelty detection. Second, in the aforementioned systems related to the TREC novelty track, either the title query or all the three sections of a querywere used merely as a bag of words, while we try to form answer patterns from the query. We argue that the definition of novelty or  X  X ew X  information is crucial for the performance of a novelty detection system. Unfortunately, novelty is usually not clearly defined in the literature. Generally, new words in the text of a sentence, story or document are used to calculate nove lty scores by various  X  X ovelty X  measures. However, new words are not equivalent to novelty (new information). For example, rephrasing a sentence with a different vocabulary does not mean that this revised sentence contains new information that is not covered by the original sentence. We give our definition of novelty as follows: There are two important aspects in this definition. First, a user X  X  query will be transformed into one or more potential questions for identifying corresponding query-related information patterns that include both query words and requi red answer types. Second, new information is obtained by detec ting those sentences that include previously unseen  X  X nswers X  corresponding to the query-related patterns. Although a user X  X  informati on need is typically represented as a query consisting of a few key words, our observation is that a user X  X  information need may be better captured by one or more questions that lead to correspondi ng information patterns. As shown in the example given in the introduc tion, the answer type in that query-related information pattern is NUMBER and the potential answer is those numbers in sent ences, i.e., 100 and 11. Therefore, the query-related pattern is a combination of query words and a number for the example query. Our novelty definition can be applied to novelty detection at different levels  X  event level, se ntence level and document level. In this paper we will study novelty de tection via information pattern identification at the sentence level . Throughout the paper, sentences that contain query-related patterns are called relevant sentences . Sentences that contain new patterns are called novel sentences . Novelty detection includes two c onsecutive steps: first retrieving relevant sentences and then detecting novel sentences. Our novelty definition is also a general one that works for novelty detection with any query that can be turned into questions. In this paper we focus on one type of question whose answers are named entities (NEs), including persons, loca tions, dates, time, numbers, and etc.[21]. We call these questions NE-questions . The information patterns that will be discussed in th is paper are NE patterns, which is a combination of both query words (of potential questions) and answer types (which requires named entities as potential answers). Since answers and new answers to NE-questions are named entities, understanding the distribution of named entity patterns could be very helpful both in finding releva nt sentences and in detecting novel sentences. We also want to understand certain NE combinations (patterns) for sepa rating relevant sentences from non-relevant sentences, and novel se ntences from non-novel sentences. These NE patterns in consideration are the number of named entities and the number of different types of named entities in a sentence. We analyzed two kinds of NE pattern distributions on the four classes of sentences: relevant , non-relevant, novel and non-novel. First we define two kinds of distributions on relevant and non-relevant sentences respectively. Assume that the total number of relevant sentences in a dataset is M r , and the total number of non-relevant sentences is M nr . Let us denote the number of named entities in a sentence as N, and the number of different types of named entities in a sentence as ND. If the occurrence of relevant sentences with N named entities is represented as O  X  X robability X  of the relevant sentences with N named entities can be represented as Similarly the occurrence and probability of the non-relevant sentences with N named entities can be represented as O P (N), where We can also define the occurrence and probability of the relevant sentences with ND types of named entities as O r (ND) and P where The occurrences and probability of the non-relevant sentences with ND types of named entities are On r (ND) and P nr (ND), where The occurrences and probabilities of the novel and non-novel sentences with N named entities or ND types of named entities can be defined in the same way. Note that here  X  X ovel X  means  X  X elevant and containing new information X  , while  X  X on-novel X  means  X  X on-relevant X  or  X  X elevant but contai ning no new information X . Let us assume that the total number of nove l sentences in the dataset is M and the total number of non-novel sentences is M nn occurrence and probability of the novel sentences with N named entities can be represented as O n (N) and P n (N), and of the non-novel sentences as O nn (N) and P nn (N), respectively , where The occurrence and probability of the novel sentences with ND different types of named entities can be represented as O P (ND), and of the non-novel sentences as O nn (ND) and P respectively , where In the following, we show and explain the results from our novelty data investigation. We use 101 queries where 53 queries are from the TREC 2002 novelty track and 48 queries are from the dataset collected by UMass. For each query there is a set of sentences that have been pre-marked as re levant/non-relevant, and novel/non-novel. The total number of senten ces for all 101 queries is 146,319, in which the total number of relevant sentences M r is 4,947, and the total number of non-relevant sentences M nr is 141,372. The total number of novel sentences M n is 4,170, and the number of non-novel sentences M nn is 142,149. In our experiments, named entities include the followings: person, location, organization, money, date, time, number, percentage, temper ature, ordered number, mass, height, length, period, energy , power, area, space, distance and object . Most of the named entities are identified by BBN X  X  IdentiFinder [21] and the rest by our own heuristic extractor. In this subsection, we perform two se ts of data analyses. In the first set, we compare the distributions of named entities in relevant and non-relevant sentences to the give n queries. In the second set, we further compare the distributions of named entities in novel and non-novel sentences. We have performed the t-test for significance on the data analysis, and the distributions of named entities in relevant/novel and non-relevant/non-novel sentences are significantly different from each other at the 95% confidence level except those two that are marked w ith an asterisk (one in Table 1 and one in Table 3). Tables 1 and 2 show the results of the first set of statistical analyses. In Table 1, the second and third columns show the distributions of relevant sentences and non-relevant sentences with different types of named entities, indicated in the first row (ND), whereas the fourth and fifth columns show the distri butions of relevant/non-relevant sentences with certain numbers of named entities, indicated by the number in the first row (N). Tabl e 2 gives statistical results on the number of relevant/non-relevant sentences that have some combinations of named entity types (patterns) that might be more important in novelty detection: pe rson and location, person and date, location and date, and person, lo cation and date. The results in Tables 1 and 2 indicate the following conclusions for NE patterns: (1). Relevant sentences contain more named entities than the non-relevant sentences (as a percentage). (2). The number of different types of named entities is more significant than the number of entities in discriminating relevant form non-relevant sentences, particularly when ND or N  X  2. (3). The particular NE combinati onal patterns we select (in Table 2) have more impact on relevant sentence retrieval. For general combinations of two types of named entities (ND = 2 in Table 1), the ratios of named entity occurrence percentiles P between relevant and non-relevant sentences is only 22.4%/19.4% =1.16. But the average ratio for th ree types of combinations of two different named entities (in Table 2) is 2.41. The ratios for the combinations of three types of named entities (ND=3) are 1.85 in the general cases (Table 1) and 3.21 in the particular person-location-date combination (in Table 2). 
Table 1. Named Entities(NE) distributions in relevant/non-Table 2. NE combinations in relevant / non-relevant sentences In the second set of analysis, we further study the distributions of named entities in novel and non-novel sentences. Tables 3 and 4 show the results. The design of the  X  X ovelty distribution X  experimental analysis in Tables 3 and 4 is the same as the design in Tables 1 and 2, except that in novelty distribution analysis, we measure the distributions of named entities with respect to novel and non-novel sentences respectively. We found similar results to those in relevant and non-relevant sent ences. The most important findings are: (1) there are relatively more novel sentences (as a percentage) than non-novel sentences that contain at least 2 different types of named entities (Table 3); and (2) there are relatively more novel sentences (in percentiles) than non-novel sentences that contain the four particular NE combinational patterns of interest (Table 4). Table 4. NE combinations in novel and non-novel sentences NE Combination # of Novel The second step of our investigation is to study the relationship of new named entities and novelty/redundancy, which is probably more important in novelty detecti on. For NE questions, relevant sentences should contain named entities as potential answers to given questions, and novel sentences should contain new answers or previously unseen named entities. Thus a relevant sentence with no new answers/named entities is said to be redundant. Table 5 shows that 67.2% of nove l sentences do have new named entities while only 45.7% of redunda nt sentences have new named entities. There are two interesting que stions based on these statistics. First, there are 32.8% novel sentences that don X  X  have any new named entities. Why are these sentences marked novel if they do not contain previously unseen named entities? Second, there are 45.7% redundant sentences that do cont ain new named entities. Why are these sentences redundant if they have previously unseen named entities? To answer these two questions, we did a further investigation on the novel/redundant sentences and its corresponding queries. We have found that most of the novel sentences without new named entities are related to particular queries. These queries can be transformed into general questions but not NE que stions that ask for certain type of named entities/patterns as answers. For example, query 420 from TREC novelty track data is concerned about the symptoms, causes and prevention of carbon monoxide poi soning. A relevant sentence to this query doesn X  X  have to have any named entities to be relevant, let alone new named entities. In fact, most of the relevant sentences for this query don X  X  contain any named entities at all. There are about 18 such queries out of the 101 queries investigated. For the second question, all types of new named entities that could be identified by our algorithms and appear in a sentence are considered in the statistics. However, for each NE question, only a particular type of named entity appeared in a relevant sentence is of interest. For example, query 306:  X  How many civilian non-combatants have been killed in the various civil wars in Africa ? X  For this query, a number appearing in a relevant sentence could be an answer, while a person name or other named entities may not be of interest. Therefore, a relevant sentence with a previously unseen person name could be redundant. Th is indicates that only certain types of named entities may contain important information for a query. In our definition, novelty means new answers to the potential questions representing a user X  X  info rmation need. Given this definition of novelty, it is possible to detect new information patterns by monitoring how the potential answers to a question change. Consequently, we propose a new novelty detection approach based on the identification of query-related patterns at the sentence level. There are two impor tant steps in the pattern-based novelty detection approach: query analysis and new pattern detection. At the first step, an information request from users will be (implicitly) transformed into one or more potential questions that determine corresponding query-relate d information patterns, which are represented by combinations of query words and required answer types to the query. At th e second step, sentences with the query-related patterns are retrieved as answer sentences. Then sentences that indicate potential new answers to the questions are marked novel. The first step of the proposed pattern -based approach is to analysis the user X  X  query and determine the possible query-related patterns that correspond to one or more potential specific questions or one general question, transforme d from the query. A question formulation algorithm first tries to automatically formulate multiple specific questions for a queryif possible. If this is not successful, a general question will be generate d. Each potential question is represented by a query-related pattern, which is a combination of a few query words and the expected answer type. In this paper, we deal with NE-questions that expect some type of named entities for answers. Therefore, a specific question would require a particular type of named entities for answers. Five types of specific questions are considered in the current system: PERSON, ORGANIZATION, LOCATION, NUMBER and DATE . For a question like  X  How many civilian non-combatants have been killed in the various civil wars in Africa  X , the query analysis compone nt formulates a query-related information pattern with both query words and an answer type. It first determines that the type of answer is NUMBER. Then it extracts civilian, non, combatant, kill, various, civil, war, Africa as query words of the question after question words (how many) and stopwords (have, been, in, the) are removed and stemming. General questions do not require a particular type of named entities for answers. Any types of named entities could be answers as long as the answer context is related to the questions. The types of named entities include the following: person, location, organization, money, date, time, number, per centage, temperature, ordered number, mass, height, length, period, energy, power, area, space, distance and object . Named entities are identified with an algorithm based on BBN X  X  IdentiFinder [21]. Each query from the TREC novelty tracks has three fields: title, description and narrative. Even though not explicitly provided in the format of a question, a significant number of the queries can be transforme d into multiple specific questions. There are many approaches that can be used for question formulation and pattern dete rmination. In our current implementation, we used a simple word-pattern matching algorithm to formulate questions and corre sponding information patterns from queries. For each type of the five NE-questions, a number of word patterns were constructed for ques tion type identification. Some word patterns were extracted from the TREC 2002 novelty track queries manually and some patterns were selected from Li &amp; Croft X  X  question answering system [20]. Some patterns are listed in Table 6. For a given query, the algorithm will go through the text in both the description and the narrative fields to identify terms that matches some word-patterns in the list. Th e query analysis component first tries to formulate at least two specific questions for each query if possible, because a single specific question probably only covers a small part of a query. If a query only has terms that match with patterns belonging to one type of question, or it does not have any matched terms at all, then a gene ral question is generated for the query. There are 50 queries in the TREC 2003 novelty track and 50 queries in the TREC 2004 novelty track. The word-pattern matching algorithm formulated multiple specific questions for 15 queries from the TREC 2003 novelty track and for 11 queries from the TREC 2004 novelty track, respectively. The remaining queries were transformed into general questions because of the lack of matched word patterns in their description and narrative fields. The new pattern detection step has two main modules: relevant sentence detection and then novel sent ence detection. First, a search engine takes the query words of the query-relate pattern generated from a potential question of a query and searches in its data collection to retrieve sentences that are likely to have correct answers. Our relevant sentence de tection module filters out those sentences that do not satisfy the query-related patterns. In another words, it first takes the results in finding relevant sentences with the TFIDF model implemented in LEMUR [24], and then removes the sentences that do not contain any  X  X nswers X  to the potential question. For a specific question, only a specific type of named entities that the question expects would be considered for potential answers. Thus a sentence without an expected type of named entities will be removed from the list. This is the main difference between our pattern-based approach and other word-based approaches. For general questions, all types of named entities could be potential answers. Therefore only sentences without any named entities are removed from the list. In both cases, a list of presumed answer sentences (which contain expected named entities to the question) is generated. To im prove the performance of finding relevant sentences and increase the rank for sentences with more named entities, the sentence retrieval module will further re-rank the sentences by a revised score S r , which is calculated according to one of the following equations: S r = S o +  X  *ND (9) where S o is the original score from the retrieval system we use, ND is the number of different types of named entities a sentence contains, N is the number of named entities and  X  is a parameter. with measurements of different types of named entities is more effective than Equation (10) with merely measurements of number of named entities in finding relevant sentences and identifying novel sentences. This observation is cons istent to our findings in named entity pattern distribution analysis shown in Section 3. Therefore, we use Equation (9) in the sentence retrieval module for the experiments reported in this pa per. This is the second main difference between our pattern-bas ed approach and the previous word-based approaches. Then, the new sentence detection module extracts all query-related named entities (as possible answers) from each answer sentence and detects previously unseen  X  X nswers X . For a query that is transformed into a general question, all named entities identified in an answer sentence will be extracted as poten tial answers. For a query with multiple specific questions formul ated, an answer sentence may have answers to one or more speci fic questions formulated from the query. So named entities related to any one of the specific questions in the answer sentences should be extracted. There is an answer pool associated with each question, which is initially empty. As sentences come in, new answers will be added to the answer pool when the novel sentence detecti on module determines that the incoming answers are previously unseen. A sentence will be marked novel if it contains new answers. In this section, we present a nd discuss the main experimental results. The data used in our experiments and the comparison of our approach and several baseline appr oaches are also described. Currently, there are three sets of data officially available for novelty detection at the sentence level. The TREC 2002 novelty track generated 54 queries. Each of the TREC 2003 novelty track and 2004 novelty tracks collected 50 queries. For each query from the 2002 and 2003 novelty tracks, there are up to 25 relevant documents that were broken into sentences. For each query from the 2004 novelty track, there are zero or more non-relevant documents in addition to 25 relevant documents as well. A set of sentences was marked relevant, and further a subs et of those sentences was marked novel. The main difference between th e three sets is that the TREC 2003 and 2004 novelty track collections exhibited greater redundancy and thus has less novel sentences [22]. Only 41.4% and 65.7% of the total relevant sentences were marked novel for the TREC 2004 novelty track and the TREC 2003 novelty track, respectively, while 90.9% of the total relevant sentences in the 2002 track are novel sentences. Our pattern-based approach was trained with the data from the TREC 2002 novelty track and tested on the data from the TREC 2003 and 2004 novelty tracks. We compared our pattern-based novelty detection (PBND) approach to three baselines: B-NN: baseline with initial retrieval ranking (without novelty detection), B-NW: baseline with new word detection, and B-MMR: baseline with maximal marginal relevance (MMR). For comparison, in our e xperiments, the same retrieval system based on the TFIDF tec hnique implemented in LEMUR toolkit [24] is used to obtain the retrieval results of relevant sentences in both the baselines and our approach. The evaluation measure used for performance comparison is precision at rank N. It shows the fraction of novel sentences in the top N sentences (N =5, 10, 15 ... in Tables 7-11.) delivered to a user. The precision at top ranks is more meaningful in real applications where uses only want to go through a small number of sentences. The first baseline does not perform any novelty detection but only uses the initial sentence ranking scores by the retrieval system directly as the novelty scores. One purpose of using this baseline is to see how much novelty detection processes may help in removing redundancies. Another purpose is to see how many novel sentences in the initial retrieval ranking list that our approach does not detect. Because of the  X  X ard X  decision (relevant or non-relevant, novel or non-novel) in the new pattern detection process, our novelty detection approach may produce a shorter list of sentences. The second baseline in our comparison is simply applying new word detection. Starting from the initial retrieval ranking, it keeps sentences with new words that do not appear in previous sentences as novel sentences, and removes t hose sentences without new words from the list. All words in the collection were stemmed and stopwords were removed. New words appearing in sentences usually contribute to the novelty scores used to rank sentences by various approaches, but new words do not necessarily contain new information. Our proposed approaches considered new named entities as possible answers to potential NE-questions of queries. Co mparing our approaches to this baseline helps us to understand which is more important in containing new information: new words (this baseline), new named entities (for general questions) or new answers (for specific questions). Many approaches to novelty detec tion, such as maximal marginal relevance (MMR), simple new word count measure, set difference measure, cosine dist ance measure, language model measures, etc. [6-13,24], were reported in the literature. MMR was introduced by Carbonell and Goldstein [23] in 1998, which was used for reducing redundancy while maintaining que ry relevance in document reranking and text summarization. MMR starts with the same initial sentences ranking used in other baselines and our approaches. In MMR, the first sentence is always novel and ranked top in novelty ranking. All other sentences are selected according their MMR scores. One sentence is selected and put into the ranking list of novelty sentences at a time. MMR scores are recalculated for all unselected sentences once a sentence is selected. The process stops until all sentences in the initial ranking list are selected. MMR is calculated by Eq. (11) where S i , and S j are the i th and j th sentences in the initial sentence ranking. Q represents the query, N is the set of sentences that have been currently selected by MMR and R/N is the set of sentences have not yet selected. Sim 1 is the similarity metric between sentence and query used in sentence retrieval and Sim 2 can be the same as Sim 1 or a different similarity metric between sentences. We use MMR as our third and main baseline because MMR was reported to work well in non-re dundant text summarization [23], novelty detection at document filtering [13] and subtopic retrieval [17]. Also, MMR may incorporate various novelty measures by using different similarity matrix between sentences and choosing different value of  X . For instance, if cosine similarity metric is used for Sim 2 and  X  is set to 0, then MMR would become the cosine distance measure reported in [7]. We tested the pattern-based novelty detection (PBND) approach on the data from the TREC 2003 and 2004 novelty tracks and compared it to the aforementioned three baselines. Three sets of experimental results are shown he re, which are (1) performance of identifying novel sentences for queries that were transformed into multiple specific questions (with query words and specific NE answer types); (2) performance of identifying novel sentences for queries that were transformed into general questions (with any NEs as answers); and (3) performance of finding relevant sentences for all queries . From Table 7 to Table 11, Chg% denotes the percent change compared to the first baseline and starts indicate statistically significant difference at a 95% confidence level by the Wilcoxon test. The purpose of the first set of results , shown in Tables 7 and 8, is to compare the performance of our patte rn-based approach to the three baselines for queries with specifi c question formulation. Our query analysis algorithm formulated multiple specific questions for 15 out of the 50 queries from the TREC 2003 novelty track and 11 out of the 50 queries from the TREC 2004 novelty track, respectively. We have the following observations and interpretations on the experimental results. (1). The proposed approach outperfo rms all baselines at top ranks. The performance of our approach with specific questions beats the first baseline by more than 20% at rank 30 on both the data from both the TREC 2003 novelty track and the 2004 novelty track. Within the top 30 sentences, our approach obtains more novel sentences than the baselines. For many users who only want to go through a small number of sentences for answers, novel sentences in the top 10, 20 or 30 ranks are more meaningful in real applications. Note that MMR performs slightly better than both the new word detection baseline and the first base line which solely uses the results from IR at low recall. (2) The precision of our approach at rank 1000 is significantly lower than the three baselines. Alt hough retrieving this number of sentences would be impractical fo r novelty-based applications, this result does indicate that are very precision-oriented. For example, in the top 1000 sentences (the last row of Table 7), the first baseline indicates that there are 218 novel sentences on average for each query; however our approach only detected 111 sentences. The first three rows in Table 7 show a summary of the 15 queries reported. Of the 3,990 novel sentences in total for the 15 queries with specific question formulation, our appro ach detected 1,079 correct novel sentences, whereas the number is 3268 for the first baseline B-NN. Table 7. Performance comparison in identifying novel sentences formulation (#TNS: # of Total Novel Sentences; #NSR: Total # of Novel Sentences Retrieved; #ASR: Average # of Sentences Table 8. Performance comparison in identifying novel sentences for 11 queries from TREC 2004 w/ specific questions The second set of experimental re sults compares the performance of our PBND approach to the three baselines for remaining queries that were transformed into general ques tions. The results of this set of experiments are given in Tables 9 and 10. The results show that the performance of our approach on th ese queries are slightly better than the baselines but the performance difference for these queries with general question formulations was not as significant as that for those queries with specific question formulations reported in Tables 7 and 8. This indicates that simply identifying new named entities in sentences does not produce a significant performance gain for novelty detection for general queries . Other types of questions that do not require named entities for answers also need to be considered in order to get better performance, especially for the queries that could not be transformed into multiple specific questions in the current implementation of the patte rn-based approach (reported in Tables 9 and 10). This is a major focus of our future work. The third set of experiments is designed to investigate the performance gain of finding releva nt sentences with the sentence re-ranking step in our approaches. Rememb er that, in our approach, the relevant sentence retrieval module re-ranks the sentences by the revised scores that incorporate the number of different types of named entities appeared in a sentence. Our hypothesis is that this re-ranking process would improve the performance of finding relevant sentences. We compare the performance of finding relevant sentences with and without re-ra nking. The comparison results are shown in Table 11, which verify our hypothesis at low recall, even if the difference is not significant. But the results in Tables 7 and 8 have shown that the pattern-based approach significantly outperforms all three baselines at low recall for identifying novel sentences. This indicates that our pattern-based approach makes a larger difference at the step of detecting novel sentences than at the step of finding relevant sentences. Table 9. Performance comparison in identifying novel sentences sentences for 39 queries from TREC 2004 w/ general questions Table 11. Performance comparison in finding relevant sentences The motivation of this work is to explore new methods for novelty detection, an important task to reduce the amount of redundant as well as non-relevant material presente d to a user. In this paper, we introduce a new definition of novelty (or new information) as new answers to the potential questions representing a user X  X  request or information need. Based on this definition, we have proposed a pattern-based approach to identif y novel sentences, i.e. sentences with certain patterns that indicate the presence of potential new answers to the questions related to a query. The proposed pattern-based approach was trained with the data from the TREC 2002 novelty track and tested on 100 queries from the TREC 2003 and 2004 novelty tracks. The experimental results show that the pattern-based approach significantly outperfo rms all three baselines in terms of precision at low recall, but only for queries where specific answer-related patterns can be form ulated. For general queries, there is small but not significant improvement. We have also investigated the distributions of named entities and patterns in relevant/novel and non-relevant/non-novel sentences. The important observation is that there are relatively more novel/relevant sentences than non-novel/non-relevant sentences that contain multiple types of named entities. This observation has been partially incorporated in the pattern-based approach. An important step in the proposed pattern-based approach is to determine information patterns that correspond to multiple specific questions (implicitly) transformed from a query. Currently, only NE-questions and NE-patterns are considered. In future work, we will improve the pattern-based approach to explore general patterns for the improvement of performan ce of general questions. Other future work will extend the pattern-based approach to novelty detection in other applications, such as new event detection and document filtering, etc. This work was supported in part by the Center for Intelligent Information Retrieval, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903. Any opinions , findings and conclusions or recommendations expressed in th is material are the authors and do not necessarily reflect those of the sponsor. [1] J. Allan, R. Paka, and V. Lavrenko,  X  X n-line New Event [2] Y. Yang, J. Zhang, J. Carbone ll and C. Jin,  X  X opic-conditioned [3] N. Stokes and J. Carthy,  X  X ir st Story Detection using a [4] M. Franz, A. Ittycheriah, J. S. McCarley and T. Ward,  X  X irst [5] J. Allan, V. Lavrenko and H. Ji n,  X  X irst Story Detection in [6] D. Harman,  X  X verview of the TREC 2002 Novelty Track X , [7] J. Allan, A. Bolivar and C. Wade,  X  X etrieval and Novelty [8] H. Kazawa, T. Hirao, H. Isozaki and E. Maeda,  X  X  machine [9] H. Qi, J. Otterbacher, A. Wi nkel and D. T. Radev,  X  X he [10] D. Eichmann and P. Srinivas an.  X  X ovel Results and Some [11] M. Zhang, R. Song, C. Lin, S. Ma, Z. Jiang, Y. Jin and L. [12] K.L. Kwok, P. Deng, N. Di nstl and M. Chan,  X  X REC2002, [13] Y. Zhang, J. Callan and T. Minka,  X  X ovelty and Reduncancy [14] E. M. Voorhees,  X  X verview of the TREC 2002 Question [15] S. E. Robertson,  X  X he Probability Ranking Principle in IR X , [16] Y. Yang, T. Pierce and J. Carbonell,  X  X  Study on Retro-[17] C. Zhai, W. W. Cohen and J. Lafferty,  X  X eyond Independent [18] T. Brants, F. Chen and A. Farahat,  X  X  System for New Event [19] X. Li and W. B. Croft,  X  X  valuating Question Answering [20] X. Li,  X  X yntactic Features in Question Answering X , Proc. [21] Daniel M. Bikel and Richard L. Schwartz and Ralph M. [22] I. Soboroff and D. Harman,  X  X verview of the TREC 2003 [23] J. Carbonell and J. Goldstei n,  X  X he Use of MMR, Diversity-[24]  X  X emur Toolkit for Langua ge Modeling and Information 
