 Applying algorithms to find patterns in the data is one way of understanding it, but  X  X  picture is worth a thousand words X . How can we visualize big graphs with billions of nodes and edges? And, more importantly, how can we spot and plot outliers in such graphs quickly? Big graphs are everywhere: the World Wide Web, social network, bi-ological network, phone call network, and many more. Visual mining on big graphs is a crucial tool for data miners to communicate with people outside: e.g., executives, government officials, domain experts, etc. In the case of outlier detection, visualization helps non-data miners understand the n ature and seriousness of outliers.

In this paper, we propose N ET -R AY , an open source package (available in http://kdd.kaist.ac.kr/netray ) implemented on top of M AP R EDUCE for visual mining on big graphs. N ET -R AY provides the following plots: 1. Spy plot (=adjacency matrix showing the nonzero patterns) of graphs, as shown in 2. Distribution plot of graph features including in/out-degrees and triangles. For ex-3. Correlation plot of graph features including in-degree vs. out-degree, degree vs.
N ET -R AY uses those plots for various graph mining tasks including finding com-munities, discovering correlations, detecting anomalies, and visualization, as shown in Fig. 1. N ET -R AY tackles two challenges. First, r eal world Web-scale graphs contain too much data, spanning Terabytes, for a standard single-machine plotting tools (e.g. gnuplot) to process. Moreover, the amount of information is too much to show on a standard screen with limited resolution, and thus careful reorganization and scaling of data are required. Second, even after presenting the data on the screen, finding rep-resentative outliers is difficult. N ET -R AY solves the problem by distributed projection, careful ordering and scaling, and efficient summarization of representative outliers. The main contributions of this paper are the following: 1. Method. We propose N ET -R AY , an open-source package for visualizing and 2. Scalability. N ET -R AY scales linearly with the numbe r of machines and the edges 3. Discovery. We e m p l o y N ET -R AY to analyze large, real world data including the
The rest of the paper is organized typically: proposed methods in Sections 2 and 3, discovery results in Section 4, related works in Section 5, and conclusion in Section 6. Table 1 lists the symbols and their definitions used in this paper. Visualization of the adjacency matrix of a g raph provides rich information about the connectivity patterns between the nodes, and leads to the discovery of community struc-tures. For small graphs, visualizing the adj acency matrix is tractable. However, visual-izing the adjacency matrix of very large graphs poses several challenges. First, the size of the adjacency matrix can easily go beyond th e resolution of a typical screen. For example, the adjacency matrix size of a 1 billion node graph becomes 1 billion by 1 billion; exactly visualizing the matrix requires 1 billion  X  1 billion pixels which are too many to be shown on a typical screen. We address the challenge by projecting the orig-inal matrix into a small matrix which can be shown on a typical screen. For example, the 1 billion by 1 billion matrix can be proj ected into a 1000 by 1000 matrix, where an element of the small matrix is set to the number of nonzeros in the corresponding submatrix of the big matrix. However, this projection poses the second challenge: the small matrix will be almost full in most cases, as shown in Fig. 2 (a).

In this section, we describe our proposed N ET -R AY -S PY method to address the two challenges of mining the adjacency matrix ; the first challenge of resolution is handled in Section 2.1, and the second  X  X ull matrix X  challenge is handled in Sections 2.2 and 2.3. 2.1 Projection To handle the problem that the adjacency ma trix is much larger than the screen, we project the original adjacency matrix into a small matrix which can be easily shown in a screen. In the following we assume a graph G with n nodes and m edges, and the target matrix of size s by s (e.g. s = 1000 ).

Let ( x,y ) be an edge in the graph, where 1  X  x,y  X  n . We project each edge by the mapped values ( + x  X  s n , and + y  X  s n , ) are in the range of [1 ,s ] . 2.2 Node Reordering Projection (Section 2.1) su ccessfully decreases the data s ize, but it has a drawback: the target matrix becomes full in most cases, thereby giving a false impression that the graph is a clique or a near-clique, as in Fig. 2 (a). Identifying the communities in the graph becomes difficult in this case. To overcome this problem we propose to cluster the nonzero elements in the adjacen cy matrix of the original graph.

Any edge clustering method (e.g. Metis [16], CrossAssociation [6], etc.) can be plugged into N ET -R AY -S PY . By default, N ET -R AY -S PY uses SlashBurn [13] for clus-tering the nonzeros in the adjacency matrix, s ince it provides the b est performance in terms of compression. SlashBurn reorders the nodes and assigns small node ids to high degree nodes, and clusters the nonzero eleme nts of the adjacency ma trix into the left, bottom, and diagonal area of the spy plot, thereby making huge empty areas in the spy plot. For example, see Fig. 2 (b) where SlashBurn clusters the nonzero elements of the adjacency matrix to show the connect ivity patterns between the nodes. 2.3 3-LOG Scaling In addition to the node reordering, we handle t he challenge of  X  X ul l matrix X  by scal-ing the x and y axes, as well as the numerical value of each element into log scale. We describe our proposed methods:  X 1-LOG X  (value) and  X 3-LOG X  (value, x, and y) scaling. 1-LOG: Value Scaling. In many real world adjacency matr ices, the distribution of the nonzero elements is skewed: i.e., some ar eas of the adjacency matrix are very dense, while others are very sparse. For this reas on, the simple linear scaling of the values loses the information on the subtle differences of the small values. For example, in Fig. 2 (a,b), most of the elements are colored blue, since few elements have the largest values (  X  70000) and most others values are smaller than 10% of the largest value. The red dots denoting the elements with the highest values are located near (0,0), but they are hardly visible. To resolve the problem, we use the log-scaling on the values, which we call the 1-LOG method. 1-LOG method shows the skewed distributions more effectively; e.g., in Fig. 2(c) we see that the area near (0,0) is very dense (red dots). Note the numbers along the color palette denote values raised to the power of 10 (e.g. 6 means 10 6 ). 3-LOG: Value and Axis Scaling. The nonzero pattern of th e reordered adjacency matrix is also skewed after node reordering: most of the elements are clustered around the origin (0,0), two axes, and the diagona l line, thereby leaving many empty spaces as shown in Fig. 2(c). To better utilize the empty space, we additionally use log-scale for the two axes, and thus create the 3-LOG (value, x, and y) plot. For example, Fig. 2 (d) shows the 3-LOG plot of the Weibo-KDD graph (described in Section 4). Note the active interactions between nodes in the range of 10 4  X  10 5 (source) and nodes in the range of 10  X  10 4 (destination) are clearly visible.

Using log scale for the values requires carefully defining the bounding rectangle in the log-space, so that the values are properly mapped into the screen. Let x min and x max denote the minimum and the maximum values resp. in the x axis, after the log scaling. The values y min and y max are defined similarly. Our idea is to map: (a) the lower, left boundary point ( x min , y min ) to the center of the lower, left boundary pixel, and (b) the upper, right boundary point ( x max , y max ) to the center of the upper, right boundary pixel. Then remaining points ( x,y ) are mapped naturally to
In very large graphs with billions of nodes a nd edges, the number of points easily exceeds billions; thus, it takes long to compute the mapping. In Section 3.1 we describe a distributed algorithm to compute the mapping. In this section we describe N ET -R AY -S CATTER , our proposed method for visualizing and mining scatter plots including distribution and correlation plots. We first describe the distributed projection method for visualizing very large scatter plots, and then the summarization/outlier detection method. 3.1 Distributed Projection Algorithm. For very large graphs with billions of nodes and edges, using a single machine for projection using Equation (1) takes very long. To speed up the task, our M
AP R EDUCE , a popular distributed data processing platform. We designed and imple-mented a two-stage M AP R EDUCE algorithm for the task. Given a set { ( x,y ) } of data points, the first stage finds the minim um and the maximum of each dimension: x min , x max , y min ,and y max . The second stage uses the values x min , x max , y min ,and y max , computed from the first stage, to find the mapping given by Eq. (1). Note that the same distributed algorithm can be used for digitizing the spy plot described in Sec. 2; in that case, the first M AP R EDUCE stage is omitted since the minimum and maximum values are known a priori ( log 1 and log n ,resp.). (a) Running time vs. edges (b) Running time vs. machines Scalability. Fig. 3 shows the scalability of N ET -R AY -S CATTER on the YahooWeb graph listed in Table 2. The Hadoop-based implementation was run on the OCC-Y Hadoop cluster described in Section 4, while the single-machine implementation on a machine with two dual-core Intel Xeon 3GHz CPUs and 4GB memory. Fig. 3(a) shows the running time vs. file size, where the number of reducer machines is fixed to 5. We see that the running time scales linearly on the edges size. Fig. 3(b) presents the running time comparison between a single-machine implementation and N ET -R AY -S CATTER on Hadoop. Note that N ET -R AY -S CATTER with 5 and 20 machines is 137  X  and 246  X  faster than the single-machine counterpart, resp. The running time with 20 machines is 1 . 8  X  (not 4  X  ) faster than with 5 machines, due to overhead of running Hadoop jobs. 3.2 Summarization and Mining The projected scatter plot still has many points: for example, the 1000 by 1000 scatter plot has at maximum 1 million points. Among these points, how can we automatically determine the representative points and outliers? We formally define the problem. Problem 1. Given N points x 1 ,..., x N ,find k representative points including top out-liers (outlier score of a point x j is the distance from x j to its nearest neighbor).
The choice of the representative points depends heavily on how we want to summa-rize the data. We list the desired properties of our summarization.  X  P1: Pick from Input. The output of the summarization should be a subset of the  X  P2: Outlier Detection. The output of the summariza tion should contain outliers or  X  P3: Scalability. The method should be fast and scalable.

Our proposed N ET -R AY -S CATTER method uses the following main ideas: 1) use a clustering algorithm to compute k clusters where k is carefully chosen (details in Section 3.3), 2) use the center points of the k clusters as the summaries, and 3) use singleton clusters (having 1 point in their clusters) as the outliers. The main question is, which clustering algorithm should we use to satisfy the three desired properties?
Our answer is to use the k -center [10] algorithm. Given a set X = { x 1 , x 2 ,..., x N } of N points, the k -center chooses a set of k points from X as cluster centers, c 1 ,..., c k to minimize the objective function max j max x  X  C j || x  X  c j || ,where || X || denotes L 1 or L 2 norm. The effect of the max () term is that an outlier, far from the rest of the points, is better to form a singleton cluster; otherwise the maximum distance between the points in the same cluster increases dramatically. Using the k -center for the summarization satisfies all the desired properties P1 , P2 ,and P3 . P1 is satisfied since only the subset of the input data points are chosen. P2 is satisfied since both singleton clusters (from outliers) and non-singleton clusters (from nor mal points) are spotted. Furthermore, P3 is satisfied since the greedy version of the k -center algorithm [10] runs in O ( kN ) time. Note that other algorithms like k -means and k -medoids do not satisfy all the properties: it can be shown that k -means violates P1 and P2 ,and k -medoids violates P2 .
N ET -R AY -S CATTER is shown in Algorithm 1. N ET -R AY -S CATTER first projects the input data using Equation (1). Then it applies the k -center algorithm with k = where N is the number of data points, as we will describe in Sec. 3.3. Finally, it picks the singleton clusters as outliers, and non-si ngleton cluster centr oids as normal points. Algorithm 1 :N ET -R AY -S CATTER for summarization and outlier detection
As an example of the outlier detection capability of N ET -R AY -S CATTER ,seeFig.4 (d) whose red circles denote singleton clusters. Note that the red circles include the two outliers pointed by red arrows. Moreover, the 5 red circled points are exactly the points with top 5 outlier scores (distance from a point to its nearest neighbor), showing the effectiveness of N ET -R AY -S CATTER . The effect of outliers in k -center was previously discussed in Charikar et al. [7]; however, the focus on the work is to perform  X  X obust X  clustering by ignoring outliers when building normal clusters for small k . In contrast, N
ET -R AY -S CATTER uses sufficiently large k (details in Section 3.3) so that the outliers, which form their own clusters, are detected automatically. 3.3 Discussion Parameter Choice. How to choose the parameter k for N ET -R AY -S CATTER ?Obvi-ously, k should be greater or equal to the numbe r of outliers that we want to find. The question is, how many outliers do we want to find? Our main target for using N ET -R AY -S
CATTER is to detect anomalous spikes in distribution or correlation plots. To detect the spike, we fix a value v in an axis (e.g., fix x coordinate), and investigate the set Y of points having the value v in the axis. If a point y in Y deviates significantly from the rest of the points in Y ,then y is treated as an outlier. Based on this motivation, we choose k as the number of distinct coordinates in either of the axes. Assuming uniform distribution of the points, the parameter is given by k = Complexity. Our method is scalable. Specifically: Lemma 1. N ET -R AY -S CATTER runs in O ( N 1 . 5 ) time.
 Proof. (Sketch) The summarization step takes O ( kN ) time with k = We present discovery results to answer the following questions.
 Q1 What connectivity patterns and communities does N ET -R AY -S PY reveal on real Q2 What patterns and anomalies does N ET -R AY -S CATTER detect in the distribution Q3 What patterns and outliers does N ET -R AY -S CATTER find in the correlation plots
We use the graph data listed in Table 2, and for each graph we extract and analyze the following information:  X  Spy plot (original, 1-LOG and 3-LOG)  X  Distribution plot (in-degree, out-degree, and triangle)  X  Correlation plot (in vs. out-degree, degree vs. triangle, and degree vs. PageRank)
The features (degree, PageRank, and triangl es [14]) of the graphs are extracted using the Pegasus graph mining package [15]. N ET -R AY is run on the OCC-Y Hadoop cluster, run by Open Cloud Consortium [1], with total 928 cores and 1 Petabyte disk. 4.1 Spy Plots Spy plots generated from N ET -R AY -S PY provide rich information on the connectivity patterns and communities in graphs. Figs. 2 and 5 show spy plots of real world graphs. We have the following observations. Connectivity Patterns. The spy plot enables us to easily identify the connectivity pat-terns in the graph. The high activity regions (yellow and red colors) of Figs. 2 (c,d), and 5 (a,b,c) show the heavy interactions between the nodes. Especially, in Fig. 5 (a) we observe that patents usually cite other patents which are neither too new nor too old: for a fixed source id, the corresponding vertical line has a single mode distribution with the maximum around the center.
 Community Identification. Spy plots enable the identification of communities; espe-cially we present sparse or dense subgraphs loosely connected to the rest of the graph. Observation 1. (Near-Spokes) Reordering nodes by N ET -R AY -S PY reveals  X  X ear-spokes X  ( sparse sub-graph loosely connected to the rest of the graph).

For example, see Fig. 5 (b,e) for the spy plots of US Patent and WWW-Barabasi. The three squares A 1 , A 2 ,and A 3 show the adjacency matrices of the induced subgraphs of the three near spokes.
 Observation 2. (Peripheral Near-Cliques) 1-LOG and 3-LOG visualization of the WWW-Barabasi graph by N ET -R AY -S PY reveal  X  X eripheral near-cliques X  ( clique-like subgraphs loosely connected to the rest of the graph), marked as  X  X N X  in Fig. 5 (e,f). 4.2 Distribution Plots Distribution plots generated from N ET -R AY -S CATTER provide abundant information on the regularities that graphs nodes follow, as well as deviating patterns. Fig. 6 shows the distributions of features in real world graphs. On the regularities, notice the power law-like slopes in the distributions of degree and triangles of real world graphs. It im-plies the formation of links and triangles ar e governed mostly by  X  X ich-get-richer X  pro-cess [17]. Furthermore, the distribution plots depict some  X  X pikes X  that deviate signifi-cantly from the fitting line of the majority of the nodes. We elaborate on the two types of spikes: one in the degree and the other in the triangle distributions.
 Spikes In Degree Distribution. The spikes in the degree distribution often come from anomalous or special behaviors requiring attentions. The first observation is on the spike of the degree distributions in WWW-Barabasi (Fig. 6 (d,e)).
 Observation 3. (Anomalous Spike in WWW-Barabasi) Spikes in the in/out-degree distributions of WWW-Barabasi comes from cliques.

The spikes are observed at A X  (in-degree 152, count 1192), B (in-degree 153, count 155), and A (out-degree 156, count 1353) of Fig. 6(d,e). It turns out that they form cliques, as we see in Fig. 7. Also, A is a subset of A X  . Finally, we note that the out-degree distribution of the YahooWeb graph, shown in Fig. 6 (b), has a spike coming from a link farm [12]. (a) Spy plot of A X  in (b) Spy plot of B in Spikes in Triangle Distribution. We also observe spikes in the triangle distribution plots: A* in Fig. 6 (f) corresponds to a spike where all 45 nodes participate in 7239 triangles. By investigating the data, we found that they form a clique, and they are subset of A in Fig. 6(d).
 Another spike occurs in the triangle distribution of YahooWeb graph in Fig. 6 (c). The rightmost red circle contains 402 nodes having 12,420,590 triangles. Among the 402 nodes, 389 nodes have out-degrees 0 and in-degrees 81316; moreover, the incoming edges for all these 389 nodes come from the same 81316 nodes. It turns out the 389 nodes are mostly adult sites, and the 81316 sites pointing to them are other adult sites that aim at boosting the ranking of 389 nodes.
 Observation 4. (Anomalous Spike in YahooWeb) The spike in the triangle distribution of YahooWeb (Fig. 6 (c)) is due to a set of adult sites pointed by other adult sites. 4.3 Correlation Plots Correlation plots generated from N ET -R AY -S CATTER provide opulent information on the communities, anomalous nodes, and corre lation between features of nodes. Fig. 8 shows the correlation plots using the node degrees, PageRank scores, and participating triangles from real world graphs.
 In and out-degrees. In Fig. 8 (a,d), popular nodes in graphs, like celebrities or portal sites, tend to have high in-degrees and small out-degrees.
 Degree and Triangles. In Fig. 8(b,e) star-like nodes, which have very sparsely con-nected neighbors, are easily identified in the lower, right corner of the degree vs. trian-gle plots. Spammers are identified in this scheme since, by their nature, they often have random neighbors with very few triangles. Als o near-clique communities are spotted in the upper left corner since a node with degree d can have d 2  X  d 2 triangles at most. Degree and PageRank. In Fig. 8 (c,f), higher number of in-degree, rather than total degree, is correlated with higher PageRank in general. However, it is possible to boost PageRank with small number of in degrees by having an in-neighbor with a very high PageRank. E.g, in Fig. 8 (c) a page in  X  X areerxroad.com X  has degree 3 with high PageR-ank since one of the in-neighbor has a very high PageRank. Although big graphs are ubiquitous, the existing visualization tools cannot handle effi-ciently billions of nodes. We review works on g raph visualization and outlier detection.
Graph Visualization. Apolo [8] is a graph tool for attention routing, that interactively expands the vicinities of a few seed nodes. OPAvion [2], an anomaly detection system for large graphs consists of Pegasus (feature aggregation [15]), OddBall (outlier detec-tion [3]) and Apolo. Here, in an attempt to understand the underlying patterns and detect outliers, we are interested in efficiently generating the spy and distribution/correlation plots of a graph, instead of plotting the grap h itself. In [21], Shneiderman proposes sim-ply scaled density plots to visualize scatter plots, and [4] presents sampling-based tech-niques for datasets with several thousands of points. [9] proposes an interactive graph visualization tool, but the focus is on only the adjacency matrix and the scalability is limited.

Clusters and Outliers in Spaces. For outliers, see LOF [5], LOCI [19], and angle-based methods [20]. For clustering, see methods like k -means in [11], k -harmonic means [22], k -medoids [18], and k -centers [10].

In general, there is not much work on visualization of the spy and scatter plots of features distributions an d correlations for graphs w ith billions of nodes and edges. In this paper, we tackle the problem of efficiently and effectively visualizing and mining billion-scale graphs. Our major contributions include: 1. Method. We propose N ET -R AY , a carefully designed algorithm for visualizing and 2. Scalability. N ET -R AY is linear on the number of machines and edges. 3. Discovery. We u s e N ET -R AY to visualize large, real-world graphs and report inter-
Interesting future research directions include visual mining of dynamic graphs and complex high-dimensional data, like tensors. Acknowledgments. Funding was provided by KAIST under project number G0413002. Funding was also provided by the U.S. ARO and DARPA under Contract Number W911NF-11-C-0088, by DTRA under contract No. HDTRA1-10-1-0120, by ARL un-der Cooperative Agreement Number W911NF-09-2-0053, and by the National Science Foundation under Grants No. IIS-1217559. The views and conclusions are those of the authors and should not be interpreted as representing the official policies, of the U.S. Government, or other funding parties, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.

