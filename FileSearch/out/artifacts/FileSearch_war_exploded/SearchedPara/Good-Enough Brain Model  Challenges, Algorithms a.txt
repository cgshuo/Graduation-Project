 Given a simple noun such as apple , and a question such as is it ed-ible? , what processes take place in the human brain? More specifi-cally, given the stimulus, what are the interactions between (groups of) neurons (also known as functional connectivity ) and how can we automatically infer those interactions, given measurements of the brain activity? Furthermore, how does this connectivity differ across different human subjects?
In this work we present a simple, novel good-enough brain model, or G E BM in short, and a novel algorithm S PARSE -S YS I are able to effectively model the dynamics of the neuron interac-tions and infer the functional connectivity. Moreover, G able to simulate basic psychological phenomena such as habitua-tion and priming (whose definition we provide in the main text). We evaluate G E BM by using both synthetic and real brain data. Using the real data, G E BM produces brain activity patterns that are strikingly similar to the real ones, and the inferred functional con-nectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other, as well as detect regularities and outliers in multi-subject brain activ-ity measurements.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing ; G.3 [ Mathematics of Computing ]: Probability and Statis-tics X  Time series analysis ; J.3 [ Computer Applications ]: Life and Medical Sciences X  Biology and genetics ; J.4 [ Computer Applica-tions ]: Social and Behavioral Sciences X  Psychology Brain Activity Analysis; System Identification; Brain Functional Connectivity; Control Theory; Neuroscience
MEG !  X  Y S  X   X   X 
AB
ODEL 0 turns out to be is not able to meet the re-Figure 1: Big picture: our G E BM estimates the hidden functional connectivity (top right, weighted arrows indicating number of in-ferred connections), when given multiple human subjects (left) that respond to yes/no questions (e.g., edible? ) for typed words (e.g., apple ). Bottom right: G E BM also produces brain activity (in solid-red), that matches reality (in dashed-blue).
Can we infer the brain activity for subject  X  X lice X , when she is shown the typed noun apple and has to answer a yes/no question, like is it edible ? Can we infer the connectivity of brain regions, given numerous brain activity data of subjects in such experiments? These are the first two goals of this work: single-subject, and multi-subject analysis of brain activity.

The third and final goal is to develop a brain connectivity model, that can also generate activity that agrees with psychological phe-
Here we tackle all these challenges. We are given Magnetoen-cephalography (MEG) brain scans for nine subjects, shown several typed nouns ( apple , hammer , etc), and being requested to answer a yes/no question ( is it edible? , is it dangerous? , and so on), by pressing one of two buttons.
 Our approach : Discovering the multi-billion connections among word iPod , and then apple , will think of Apple-inc , as opposed to the fruit apple word all the time, will eventually stop paying attention. the tens of billions [23, 2] of neurons would be the holy grail, and clearly outside the current technological capabilities. How close can we approach this ideal? We propose to use a good-enough approach, and try to explain as much as we can, by assuming a small, manageable count of neuron-regions and their interconnec-tions, and trying to guess the connectivity from the available MEG data. In more detail, we propose to formulate the problem as  X  X ys-tem identification X  from control theory, and we develop novel algo-rithms to find sparse solutions.

We show that our good-enough approach is a very good first step, leading to a tractable, yet effective model (G E BM), that can answer the above questions. Figure 1 gives the high-level overview: at the bottom-right, the blue, dashed-line time sequences correspond to measured brain activity; the red lines correspond to the guess of our G E BM model. Notice the qualitative goodness of fit. At the top-right, the arrows indicate interaction between brain regions that our analysis learned, with the weight being the strength of interac-tion. Thus we see that the vision cortex ( X  X ccipital lobe X ) is well connected to the language-processing part ( X  X emporal lobe X ), which agrees with neuroscience, since all our experiments involved typed words.
 Our contributions are as follows: Additionally, our G E BM highlights connections between multiple, mostly disparate areas: 1) Neuroscience, 2) Control Theory &amp; Sys-tem Identification, and 3) Psychology.

Reproducibility : Our implementation is open sourced and pub-the MEG data, however, in the online version of the code we in-clude the synthetic benchmarks, as well as the simulation of psy-chological phenomena using G E BM.
As mentioned earlier, our goal is to infer the brain connectivity, given measurements of brain activity on multiple yes/no tasks , of multiple subjects. We define as yes/no task the experiment where alive? X  ), and a typed English word (like, apple , chair ), and has to decide the answer.

Throughout the entire process, we attach m sensors that record brain activity of a human subject. Here we are using Magnetoen-cephalography (MEG) data, although our G E BM model could be applied to any type of measurement (fMRI, etc). In Section 5.4 we provide a more formal definition of the measurement technique.
Thus, in a given experiment, at every time-tick t we have http://www.cs.cmu.edu/~epapalex/src/GeBM. zip measurements, which we arrange in an m  X  1 vector y ( t ) tionally, we represent the stimulus (e.g. apple ) and the task (e.g. is it edible? ) in a time-dependent vector s ( t ) , by using feature repre-sentation of the stimuli; a detailed description of how the stimulus vector is formed can be found in Section 5.4. For the rest of the paper, we shall use interchangeably the terms sensor , voxel and neuron-region.

We are interested in two problems: the first is to understand how the brain works, given a single subject. The second problem is to do cross-subject analysis, to find commonalities (and deviations) in a group of several human subjects. Informally, we have: For the second problem, informally we have:
For the particular experimental setting, prior work [15] has only considered transformations from the space of noun features to the voxel space and vice versa, as well as word-concept specific pre-diction based on estimating the covariance between the voxels [7].
Next we formalize the problems, we show some straightforward (but unsuccessful) solutions, and finally we give the proposed model G
BM, and the estimation algorithm.
There are two over-arching assumptions: Non-linear/sigmoid models is a natural direction for future work; and so is the study of neuroplasticity, where the connectivity changes. However, as we show later, linear, static, models are  X  good-enough  X  to answer the problems we listed, and thus we stay with them.
However, we have to be careful. Next we list some natural, but unsuccessful models, to illustrate that we did do  X  X ue dilligence X , and to highlight the need for our slightly more complicated, G model. The conclusion is that the hasty, Model 0 , below, leads to poor behavior, as we show in Figure 2 (red, and black, lines), com-pletely missing all the trends and oscillations of the real signal (in dotted-blue line). In fact, the next subsection may be skipped, at a first reading.
Given the linearity and static-connectivity assumptions above, a natural additional assumption is to postulate that the m  X  1 activity vector y ( t + 1) depends linearly , on the activities of the previous time-tick y ( t ) , and, of course, the input stimulus, that is, the s  X  1 vector s ( t ) .
Formally, in the absence of input stimulus, we expect that where A is the m  X  m connectivity matrix of the m brain regions. Including the (linear) influence of the input stimulus s ( t )
The B [ m  X  s ] matrix shows how the s input signals affect the brain-regions.

To solve for A , B , notice that: y ( t + 1) = A B y ( t ) which eventually becomes Y 0 = A B Y
In the above equation, we arranged all the measurement vectors y ( t ) in matrices: Y = y (1)  X  X  X  y ( T  X  1) , Y 0 = y (2)  X  X  X  y ( T ) , and S = s (1)  X  X  X  s ( T  X  1)
This is a well-known, least squares problem. We can solve it  X  X s is X ; we can ask for a low-rank solution; or for a sparse solution -none yields a good result, but we briefly describe each, next.
None of the above worked. Fig. 2 shows the real brain activ-ity (dotted-blue line) and predicted activity, using LS (pink) and CCA (black), for a particular voxel. The solutions completely fail to match the trends and oscillations. The results for the ization, and for several other voxels, are similar to the one shown, and omitted for brevity.
 Figure 2: M ODEL 0 fails : True brain activity (dotted blue) and the model estimate (pink, and black, resp., for the least squares, and for the CCA variation).

The conclusion of this subsection is that we need a more compli-cated model, which leads us to G E BM, next.
Before we introduce our proposed model, we should introduce our notation, which is succinctly shown in Table 1.

Formulating the problem as M ODEL 0 is not able to meet the re-quirements for our desired solution. However, we have not ex-hausted the space of possible formulations that live within our set
Symbol Definition n number of hidden neuron-regions m number of MEG sensors/voxels we observe (306) s number of input signals (40 questions)
T time-ticks of each experiment (340 ticks, of 5msec each) x ( t ) vector of neuron activities at time t y ( t ) vector of voxel activities at time t s ( t ) vector of input-sensor activities at time t A [ n  X  n ] connectivity matrix between neurons (or neuron regions) C [ m  X  n ] summarization matrix (neurons to voxels) B [ n  X  s ] perception matrix (sensors to neurons) A v connectivity matrix between voxels R EAL real part of a complex number I MAG imaginary part of a complex number of simplifying assumptions. In this section, we describe G our proposed approach which, under the assumptions that we have already made in Section 2, is able to meet our requirements remark-ably well.

In order to come up with a more accurate model, it is useful to look more carefully at the actual system that we are attempting to model. In particular, the brain activity vector y that we observe is simply the collection of values recorded by the m sensors, placed on a person X  X  scalp. In M ODEL 0 , we attempt to model the dynamics of the sensor measurements directly. However, by doing so, we are directing our attention to an observable proxy of the process that we are trying to estimate (i.e. the functional connectivity). Instead, it is more beneficial to model the direct outcome of that process. Ideally, we would like to capture the dynamics of the internal state of the person X  X  brain, which, in turn, cause the effect that we are measuring with our MEG sensors.

Let us assume that there are n hidden (hyper-)regions of the brain, which interact with each other, causing the activity that we observe in y . We denote the vector of the hidden brain activity as x of size n  X  1 . Then, by using the same idea as in M ODEL may formulate the temporal evolution of the hidden brain activity as:
A subtle issue that we have yet to address is the fact that observed and we have no means of measuring it. We propose to resolve this issue by modelling the measurement procedure itself, i.e. model the transformation of a hidden brain activity vector to its observed counterpart. We assume that this transformation is linear, thus we are able to write
Putting everything together, we end up with the following set of equations, which constitute our proposed model G E BM:
The key concepts behind G E BM are:
Our solution is inspired by control theory, and more specifically by a sub-field of control theory, called system identification . In the appendix, we provide an overview of how this can be accom-plished. However, the matrices we obtain through this process are usually dense, counter to G E BM X  X  specifications. We, thus, need to refine the solution until we obtain the desired level of sparsity. In the next few lines, we show why this sparsification has to be done carefully, and we present our approach.

Crucial to G E BM X  X  behavior is the spectrum of its matrices; in other words, any operation that we apply on any of G E BM X  X  ma-trices needs to preserve the eigevnalue profile (for matrix the singular values (for matrices B , C ). Alterations thereof may lead G E BM to instabilities. From a control theoretic and stability perspective, we are mostly interested in the eigenvalues of they drive the behavior of the system. Thus, in our experiments, we heavily rely on assessing how well we estimate these eigenvalues.
Sparsifying a matrix while preserving its spectrum can be seen as a similarity transformation of the matrix to a sparse subspace. The following lemma sheds more light towards this direction. L EMMA 1. System identification is able to recover matrices B , C of G E BM up to rotational/similarity transformations. P ROOF . See the appendix.
 An important corollary of the above lemma (also proved in the ap-pendix) is the fact that pursuing sparsity only on, say, matrix not well defined. Therefore, since all three matrices share the same similarity transformation freedom, we have to sparsify all three.
In S PARSE -S YS I D , we propose a fast, greedy sparsification scheme which can be seen as approximately applying the aforementioned similarity transformation to A , B , C , without calculating or apply-ing the transformation itself. Iteratively, for all three matrices, we delete small values, while maintaining ther spectrum within from the one obtained through system identification. Additionally, for A , we also do not allow eigenvalues to switch from complex to real and vice versa. This scheme works very well in practice, pro-viding very sparse matrices, while respecting their spectrum. In Algorithm 1, we provide an outline of the algorithm.

So far, G E BM as we have described it, is able to give us the hid-den functional connectivity and the measurement matrix, but does not directly offer the voxel-to-voxel connectivity, unlike M which models it explicitly. However, this is by no means a weak-ness of G E BM, since there is a simple way to obtain the voxel-to-voxel connectivity (henceforth referred to as A v ) from G matrices.

L EMMA 2. Assuming that m &gt; n , the voxel-to-voxel func-tional connectivity matrix A v can be defined and is equal to CAC  X  Algorithm 1 : S PARSE -S YS I D : Sparse System Identification of G E BM
Input: Training data in the form { y ( t ) , s ( t ) } T t =1
Output: G E BM matrices A (hidden connectivity matrix), B
Algorithm 2 : E IGEN S PARSIFY : Eigenvalue Preserving Spar-sification of System Matrix A .
 Input: Square matrix A (0) .

Output: Sparsified matrix A . 12: end while
Algorithm 3 : S INGULAR S PARSIFY : Singular Value Preserv-ing Sparsification Input: Matrix M (0) .

Output: Sparsified matrix M 10: end while
P ROOF . The observed voxel vector can be written as Matrix C is tall (i.e. m &gt; n ), thus we can write: y ( t ) = Cx ( t )  X  x ( t ) = C  X  y ( t ) Consequently, y ( t + 1) = CAC  X  y ( t ) + CBs ( t ) Therefore, it follows that CAC  X  is the voxel-to-voxel matrix
Finally, an interesting aspect of our proposed model G E BM is the fact that if we ignore the notion of the summarization, i.e. matrix C = I , then our model is reduced to the simple model M
ODEL 0 . In other words, G E BM contains M ODEL 0 as a spe-cial case. This observation demonstrates the importance of hidden states in G E BM.
The code for S PARSE -S YS I D has been written in Matlab. For the system identification part, initially we experimented with Matlab X  X  System Identification Toolbox and the algorithms in [10]. These algorithms worked well for smaller to medium scales, but were un-able to perform on our full dataset. Thus, in our final implementa-tion, we use the algorithms of [20]. Our code is publicly available at http://www.cs.cmu.edu/~epapalex/src/GeBM.zip .
In lieu of ground truth in our real data, we generated synthetic data to measure the performance of S PARSE -S YS I D .

The way we generate the ground truth system is as follows: First, given fixed n , we generate a matrix A that has 0.25 on the main di-agonal, 0.1 on the first upper diagonal (i.e. the ( i,i + 1) -0.15 on the first lower diagonal (i.e., the ( i  X  1 ,i ) 0 everywhere else. We then create randomly generated sparse ma-trices B and C , varying s and m respectively.
 After we generate a synthetic ground truth model, we generate Gaussian random input data to the system, and we obtain the sys-tem X  X  response to that data. Consequently, we use the input/output pairs with S PARSE -S YS I D , and we assess our algorithm X  X  ability to recover the ground truth. Here, we show the noiseless case due to space restrictions. In the noisy case, estimation performance is slowly degrading when n increases, however this is expected from estimation theory.

We evaluate S PARSE -S YS I D  X  X  accuracy with respect to the fol-lowing aspects:
Q1 : How well can S PARSE -S YS I D recover the true hidden con-
Q2 : How well can S PARSE -S YS I D recover the voxel-to-voxel con-
Q3 : Given that we know the the true number of hidden states In order to answer Q1 , we measure how well (in terms of RMSE) S
PARSE -S YS I D recovers the eigenvalues of A . We are mostly in-terested in recovering perfectly the real part of the eigenvalues, since even small errors could lead to instabilities. Figure 4(a) shows our results: We observe that the estimation of the real part of the eigenvalues of A is excellent. We are omitting the estimation re-sults for the imaginary parts, however they are within the we se-lected in our sparsification scheme of S PARSE -S YS S
PARSE -S YS I D is able to recover the true G E BM, for various val-ues of m and n .
 With respect to Q2 , it suffices to measure the RMSE of the true A v and the estimated one, since we have thoroughly tested the sys-tem X  X  behavior in Q1 . Figure 4(b) shows that the estimation of the voxel-to-voxel connectivity matrix using S PARSE -S YS I D accurate. Additionally, for ease of exposition, in Figure 3 we show an example a true matrix A v , and its estimation through S S
YS I D ; it is impossible to tell the difference between the two ma-trices, a fact also corroborated by the RMSE results.

The third dimension of S PARSE -S YS I D  X  X  performance is its sen-sitivity to the selection of the parameter n ; In order to test this, we generated a ground truth G E BM with a known n , and we varied our selection of n for S PARSE -S YS I D . The result of the experiment is shown in Fig. 4(c). We observe that for values of n smaller than the real one, S PARSE -S YS I D  X  X  performance is increasingly good, and still, for small values of n the estimation quality is good. When exceeds the value of the real n , the performance starts to degrade, due to overfitting . This provides an insight on how to choose S
PARSE -S YS I D in order to fit G E BM: it is better to under-estimate n rather than over-estimate it, thus, it is better to start with a small n and possibly increase it as soon as performance (e.g. qualitative assessment of how well the estimated model predicts brain activity) starts to degrade.
 Figure 3: Q2: Perfect estimation of A v :Comparison of true and estimated A v , for n = 3 and m = 4 . We can see, qualitatively, that G E BM is able to recover the true voxel-to-voxel functional connectivity. This section is focused on showing different aspects of G at work. In particular, we present the following discoveries:
D1: We provide insights on the obtained functional connectivity
D2: Given multiple human subjects, we discover regularities and D3: We demonstrate G E BM X  X  ability to simulate brain activity.
D4: We show how G E BM is able to capture two basic psycho-
We are using real brain activity data, measured using MEG. MEG (Magnetoencephalography) measures the magnetic field caused by many thousands of neurons firing together, and has good time res-olution (1000 Hz) but poor spatial resolution. fMRI (functional Magnetic Resonance Imaging) measures the change in blood oxy-genation that results from changes in neural activity, and has good spatial resolution but poor time resolution (0.5-1 Hz). Since we are interested in the temporal dynamics of the brain, we choose to operate on MEG data.
 All experiments were conducted at the University of Pittsburgh Medical Center (UPMC) Brain Mapping Center. The MEG ma-chine consists of m = 306 sensors, placed uniformly across the subject X  X  scalp. The temporal granularity of the measurements is 5ms, resulting in T = 340 time points; after experimenting with different aggregations in the temporal dimension, we decided to use (a) Q1 : RMSE of R EAL (eigenvalues) of A vs. n . Figure 4: Sub-figure (a) refers to Q1 , and show sthat S PARSE is able to estimate matrix A with high accuracy, in control-theoretic from this graph is that as long as n is under-estimated, S  X  X  performance is steadily good and is not greatly influenced by the particular choice of n . 50ms of time resolution, because this yielded the most interpretable results.

For the experiments, nine right handed human subjects were shown a set of 60 concrete English nouns ( apple, knife etc), and for each noun 20 simple yes/no questions ( Is it edible? Can you buy it? etc). The subject were asked to press the right button if their answer to each question was  X  X es X , or the left button if the an-swer was  X  X o X . After the subject pressed the button, the stimulus (i.e. the noun) would disappear from the screen. We also record the exact time that the subject pressed the button, relative to the ap-pearance of the stimulus on the screen. A more detailed description of the data can be found in [15].

In order to bring the above data to the format that our model ex-pects, we make the following design choices: In lack of sensors that measure the response of the eyes to the shown stimuli, we rep-resent each stimulus by a set of semantic features for that specific noun. This set of features is a superset of the 20 questions that we have already mentioned; the value for each feature comes from the answers given by Amazon Mechanical Turk workers. Thus, from time-tick 1 (when the stimulus starts showing), until the button is pressed, all the features that are active for the particular stimulus are set to 1 on our stimulus vector s , and all the rest features are equal to 0; when the button is pressed, all features are zeroed out. On top of the stimulus features, we also have to incorporate the task information in s , i.e. the particular question shown on the screen. In order to do that, we add 20 more rows to the stimulus vector each one corresponding to every question/task. At each given ex-periment, only one of those rows is set to 1 for all time ticks, and all other rows are set to 0. Thus, the number of input sensors in our formulation is s = 40 (i.e. 20 neurons for the noun/stimulus and 20 neurons for the task).

As a last step, we have to incorporate the button pressing in-formation to our model; to that end, we add two more voxels to our observed vector y , corresponding to left and right button press-ing; initially, those values are set to 0 and as soon as the button is pressed, they are set to 1.

Finally, we choose n = 15 for all the results we show in the following lines; particular choice of n did not incur qualitative changes in the results, however, as we highlight in the previous section, it is better to under-estimate n , and therefore we chose n = 15 as an adequately small choice which, at the same time, produces interpretable results.
The primary focus of this work is to estimate the functional con-nectivity of the human brain, i.e. the interaction pattern of groups of neurons. In the next few lines, we present our findings in a concise way and provide Neuroscientific insights regarding the interaction patterns that G E BM was able to infer.

In order to present our findings, we post-process the results ob-tained through G E BM in the following way: The data we collect come from 306 sensors, placed on the human scalp in a uniform fashion. Each of those 306 sensors is measuring activity from one of the four main regions of the brain, i.e.
Even though our sensors offer within-region resolution, for ex-position purposes, we chose to aggregate our findings per region; by doing so, we are still able to provide useful neuroscientific in-sights.
 Figure 5 shows the functional connectivity graph obtained using G
BM. The weights indicate the strength of the interaction, mea-sured by the number of distinct connections we identified. These results are consistent with current research regarding the nature of language processing in the brain. For example, Hickock and Poep-pel [9] have proposed a model of language comprehension that includes a  X  X orsal X  and  X  X entral X  pathway. The ventral pathway takes the input stimuli (spoken language in the case of Hickock and Poeppel, images and words in ours) and sends the informa-tion to the temporal lobe for semantic processing. Because the occipital cortex is responsible for the low level processing of vi-sual stimuli (including words) it is reasonable to see a strong set of connections between the occipital and temporal lobes. The dor-sal pathway sends processed sensory input through the parietal and frontal lobes where they are processed for planning and action pur-poses. The task performed during the collection of our MEG data required that subjects consider the meaning of the word in the con-text of a semantic question. This task would require the recruit-ment of the dorsal pathway (occipital-parietal and parietal-frontal connections). In addition, frontal involvement is indicated when the task performed by the subject requires the selection of semantic information [3], as in our question answering paradigm. It is in-teresting that the number of connections from parietal to occipital cortex is larger than from occipital to parietal, considering the flow of information is likely occipital to parietal. This could, however, be indicative of what is termed  X  X op down X  processing, wherein higher level cognitive processes can work to focus upstream sen-sory processes. Perhaps the semantic task causes the subjects to focus in anticipation of the upcoming word while keeping the se-mantic question in mind.
In our experiments, we have 9 participants, all of whom have un-dergone the same procedure, being presented with the same stimuli, and asked to carry out the same tasks. Availability of such a rich, multi-subject dataset inevitably begs the following question: are there any differences across people X  X  functional connectivity? Or is everyone, more or less, wired equally, at least with respect to the stimuli and tasks at hand?
By using G E BM, we are able (to the extent that our model is able to explain) to answer the above question. We trained G for each of the 9 human subjects, using the entire data from all stimuli and tasks, and obtained matrices A , B , C for each person. For the purposes of answering the above question, it suffices to look at matrix A (which is the hidden functional connectivity), since it dictates the temporal dynamics of the brain activity. At this point, we have to note that the exact location of each sensor can differ between human subjects, however, we assume that this difference is negligible, given the current voxel granularity dictated by the number of sensors.
 In this multi-subject study we have two very important findings:
In Fig. 6(a) &amp; (b) we show the real and imaginary parts of the eigenvalues of A . We can see that for 8 human subjects, the eigen-values are almost identical. This finding agrees with neuroscientific results on different experimental settings [18], further demonstrat-ing G E BM X  X  ability to provide useful insights on multi-subject ex-periments. For subject #3 there is a deviation on the real part of the first eigenvalue, as well as a slightly deviating pattern on the imagi-nary parts of its eigenvalues. Figures 6(c) &amp; (d) compare matrix for subjects 1 and 3. Subject 3 negative value on the diagonal (blue square at the (8 , 8) entry), a fact unique to this specific person X  X  connectivity.

Moreover, according to the person responsible for the data col-lection of Subject #3: This is a plausible explanation for the deviation of G E BM for Sub-ject #3.
An additional way to gain confidence on our model is to assess its ability to simulate/predict brain activity, given the inferred func-tional connectivity. In order to do so, we trained G E BM using data from all but one of the words, and then we simulated brain activity time-series for the left-out word. In lieu of competing methods, we compare our proposed method G E BM against our initial approach (whose unsuitability we have argued for in Section 3, but we use here in order to further solidify our case). As an initial state for G
BM, we use C  X  y (0) , and for M ODEL 0 , we simply use y (0) The final time-series we show, both for the real data and the es-timated ones are normalized to unit norm, and plotted in absolute values. For exposition purposes, we sorted the voxels according to the ` 2 norm of their time series vector, and we are displaying the high ranking ones (however, the same pattern holds for all voxels) In Fig. 7 we illustrate the simulated brain activity of G (solid red), compared against the ones of M ODEL 0 (using LS (dash-dot magenta) and CCA (dashed black) ), as well as the original brain activity time series (dashed blue) for the four highest ranking voxels. Clearly, the activity generated using G E BM is far more realistic than the results of M ODEL 0 .
As we briefly mentioned in the Introduction, we would like our proposed method to be able to capture some of the psychological phenomena that the human brain exhibits. We, by no means, claim that G E BM is able to capture convoluted and still under heavy in-vestigation psychological phenomena, however, in this section we demonstrate G E BM X  X  ability to simulate two very basic phenom-ena, habituation and priming . Unlike the previous discoveries, the following experiments are on synthetic data and their purpose is to showcase G E BM X  X  additional strengths.
 Habituation In our simplified version of habituation, we observe the demand behaviour: Given a repeated stimulus, the neurons ini-tially get activated, but their activation levels decline ( Fig. 8) if the stimulus persists for a long time ( 8). In Fig. 8, we show that G E BM is able to capture such behav-ior. In particular, we show the desired input and output for a few (observed) voxels, and we show, given the functional connectivity obtained according to G E BM, the simulated output, which exhibits the same, desired behavior.
  X 
AB  X  by (pseudo)inverting
ODEL 0 is not able to meet the re-Figure 8: G E BM captures Habituation : Given repeated exposure to a stimulus, the brain activity starts to fade.
 Priming In our simplified model on priming, first we give the stim-
Frontal lobe ! (attention) !
Temporal lobe ! (language) ! was supposed to be. Sub-figures (c) and (d) show matrices the first four high ranking voxels (in the ` 2 norm sense). ulus apple , which sets off neurons that are associated with the fruit  X  X pple X , as well as neurons that are associated with Apple inc. Con-sequently, we are showing a stimulus such as iPod ; this predisposes the regions of the brain that are associated with Apple inc. to dis-play some small level of activation, whereas suppressing the re-gions of the brain that are associate with apple (the fruit). Later on, the stimulus apple is repeated, which, given the aforementioned predisposition, activates the voxels associated with Apple (com-pany) and suppresses the ones associated with the homonymous fruit.

Figure 9 displays is a pictorial description of the above example of priming; given desired input/output pairs, we derive a model that obeys G E BM, such that we match the priming behavior. Brain Functional Connectivity Estimating the brain X  X  functional connectivity is an active field of study of computational neuro-science. Examples of works can be found in [13, 8, 7]. There have been a few works in the data mining community as well: In [16], the authors derive the brain region connections for Alzheimer X  X  pa-tients, and recently [5] that leverages tensor decomposition in order to discover the underlying network of the human brain. Most re-lated to the present work is the work of Valdes et al [19], wherein the authors propose an autoregressive model (similar to M and solve it using regularized regression. However, to the best of our knowledge, this work is the first to apply system identification concepts to this problem.  X 
AB
A , B . In the next lines we show two of the  X   X 
AB by (pseudo)inverting
ODEL 0 is not able to meet the re-
E BM are: Figure 9: G E BM captures Priming : When first shown the stimu-lus apple , both neurons associated with the fruit  X  X pple X  and Apple inc. get activated. When showing the stimulus iPod and then ap-ple , iPod predisposes the neurons associated with Apple inc. to get activated more quickly, while suppressing the ones associated with the fruit.
 Psychological Phenomena A concise overview of literature per-taining to habitutation can be found in [17]. A more recent study on habitutation can be found in [12]. The definition of priming, as we describe it in the lines above concurs with the definition found in [6]. Additionally, in [11], the authors conduct a study on the ef-fects of priming when the human subjects were asked to write sen-tences. The above concepts of priming and habituation have been also studied in the context of spreading activation [1, 4] which is a model of the cognitive process of memory.
 Control Theory &amp; System Identification System Identification is a field of control theory. In the appendix we provide more theo-retical details on subspace system identification, however, [10] and [21] are the most prominent sources for system identification algo-rithms.
 Network Discovery from Time Series Our work touches upon discovering underlying network structures from time series data; an exemplary work related to the present paper is [22] where the authors derive a who-calls-whom network from VoIP packet trans-mission time series.
The list of our contributions is: [1] J.R. Anderson. A spreading activation theory of memory. [2] Frederico AC Azevedo, Ludmila RB Carvalho, Lea T [3] Jeffrey R Binder and Rutvik H Desai. The neurobiology of [4] Allan M. Collins and Elizabeth F. Loftus. A [5] Ian Davidson, Sean Gilpin, Owen Carmichael, and Peter [6] Angela D Friederici, Karsten Steinhauer, and Stefan Frisch. [7] Alona Fyshe, Emily B Fox, David B Dunson, and Tom M [8] Michael D Greicius, Ben Krasnow, Allan L Reiss, and Vinod [9] Gregory Hickok and David Poeppel. Dorsal and ventral [10] Lennart Ljung. System identification . Wiley Online Library, [11] Martin J Pickering and Holly P Branigan. The representation [12] Catharine H Rankin, Thomas Abrams, Robert J Barry, Seema [13] Vangelis Sakkalis. Review of advanced techniques for the [14] Ioannis D Schizas, Georgios B Giannakis, and Zhi-Quan [15] G. Sudre, D. Pomerleau, M. Palatucci, L. Wehbe, A. Fyshe, [16] Liang Sun, Rinkal Patel, Jun Liu, Kewei Chen, Teresa Wu, [17] Richard F Thompson and William A Spencer. Habituation: a [18] Dardo Tomasi and Nora D Volkow. Resting functional [19] Pedro A Vald X s-Sosa, Jose M S X nchez-Bornot, Agust X n [20] Michel Verhaegen. Identification of the deterministic part of [21] Michel Verhaegen and Vincent Verdult. Filtering and system [22] Olivier Verscheure, Michail Vlachos, Aris Anagnostopoulos, [23] Robert W Williams and Karl Herrup. The control of neuron
Consider again the linear state-space model of G E BM Assuming x (0) = 0 , for simplicity, it is easy to see that and therefore y ( t ) = P t  X  1 read out the impulse response matrix H ( t ) = CA t  X  1 B .
As we mentioned in Section 3, matrices A , B , C can be identi-fied up to a similarity transformation. In order to show this, sup-pose that we have access to the system X  X  inputs { u ( t ) } outputs { y ( t ) } T ( A , B , C ) . A first important observation is the following. From x ( t + 1) = Ax ( t ) + Bu ( t ) we obtain Defining z ( t ) := Mx ( t ) ,  X  A := MAM  X  1 , and  X  B := MB obtain It follows that ( A , B , C ) and (  X  A ,  X  B ,  X  C ) = ( MAM are indistinguishable from input-output data alone. Thus, the sought parameters can only be (possibly) identified up to a basis (similar-ity) transformation, in the absence of any other prior or side infor-mation.

Under what conditions can ( A , B , C ) be identified up to such similarity transformation? It has been shown by Kalman that if the so-called controlability matrix is full row rank n , and the observability matrix is full column rank n , then it is possible to identify ( A , B , C ) such similarity transformation from (sufficiently  X  X iverse X ) input-output data, and the impulse response in particular. This can be accomplished by forming a block Hankel matrix out of the impulse response, as follows, This matrix can be factored into O MM  X  1 C , i.e., the  X  X rue X  C up to similarity transformation, using the singular value decom-position of the above Hankel matrix. It is then easy to recover  X  A ,  X  B ,  X  C from O M and M  X  1 C . This is the core of the Kalman-Ho algorithm for Hankel subspace-based identification [10].
This procedure enables us to identify  X  A ,  X  B ,  X  C , but, in our con-text, we are ultimately also interested in the true latent
P ROPOSITION 1. We can always, without loss of generality, trans-form  X  A to maximal sparsity while keeping  X  B and  X  C sparsity of A alone does not help in terms of identification. P ROOF . Suppose that we are interested in estimating a sparse while preserving the eigenvalues of of  X  A . We therefore seek a simi-larity transformation (a matrix M ) that will render A = M as sparse as possible. Towards this end, assume that  X  A set of linearly independent eigenvectors, collected in matrix let
 X  be the corresponding diagonal matrix of eigenvalues. Then, clearly,  X  AE = E  X  , and therefore E  X  1  X  AE =  X  -a diagonal ma-trix. Hence choosing M = E  X  1 we make A = M  X  AM  X  1 max-imally sparse. Note that A must have the same rank as  X  it has less nonzero elements it will also have lower rank. Finally, it is easy to see that if we apply this similarity transformation, the eigenvalues of  X  A do not change.
