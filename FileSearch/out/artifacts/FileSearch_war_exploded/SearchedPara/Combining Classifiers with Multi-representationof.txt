 Word sense disambiguation involves the association of a given word in a text or discourse with a particular sense among numerous potential senses of that word. As mentioned in [4], this is an  X  X ntermediate task X  necessarily to accom-plish most natural language processing tasks. Since its inception, many methods involving WSD have been developed in the literature (see, e.g., [4] for a survey). During the last decade, many supervised machine learning algorithms have been used for this task. As observed in studies of machine learning systems, although one could choose one of learning systems available to achieve the best perfor-mance for a given pattern recognition problem, the set of patterns misclassified by the different classification systems would not necessarily overlap. This means that different classifiers may potentially offer complementary information about patterns to be classified. This observation highly motivated the interest in com-bining classifiers during the recent years. Especially, classifier combination for WSD has been unsurprisingly received much attention recently from the com-munity as well, e.g., [3, 10, 6, 2, 11].
 the first scenario, all classifiers use the same representation of the input pat-tern. In the context of WSD, the work by Klein et al. [6], and Florian and Yarowsky [2] could be grouped into this first scenario. In the second scenario, each classifier uses its own representation of the input pattern. An important application of combining classifiers in this scenario is the possibility to integrate physically different types of features. In this sense, the work by Pedersen [10], Wang and Matsumoto [11] can be considered as belonging to this scenario. In this paper, we focus on classifier combination for WSD in the second scenario. Particularly, we consider various ways of using context in WSD as distinct repre-sentations of a polysemous word. This allows us to immediately use the common theoretical framework for combining classifiers developed in Kittler et al. [5] to WSD problem. The experimental result shows that combining classifiers with multi-representation of context significantly improves the accuracy of WSD. 2.1 WSD with Multi-representation of Context Given an ambiguous word w , which may have m possible senses (classes): c 1 , c ,..., c m , in a context C , the task is to determine the most appropriate sense of w . For a target word w , we may have different representations of context C corresponding to different views of context. Assume that we have such R representations: f 1 ,..., f R , serving for the aim of identifying the right sense of w .Thesetoffeatures f i is used by the i -th classifier. Due to the interpretation of f i  X  X  and the role of context in WSD, quite naturally, we shall assume that the classification models are mutually exclusive, i.e. that only one model can be associated with each target w .
 1 ,...,R ), the Bayesian theory suggests that the word w should be assigned to class c j provided the a posteriori probability of that class is maximum, namely Then the following decision rule is derived due to Bayes theorem: sifiers developed in [5] can be applied for WSD problem as in the following. 2.2 Basic Combination Schemes tribution of the representations extracted by the classifiers. Assume that the representations used are conditional independent, so that the decision rule (2) can be rewritten as follows: Using Bayes rule, we obtain the decision rule (4) that quantifies the likelihood of a hypothesis by combining a posteriori probabilities generated by the individual classifiers by means of a product rule: j = arg max Sum Rule. Let us return to the decision rule (4), in some application it may be appropriate further to assume that a posteriori probabilities computed by the respective classifiers will not deviate dramatically from the prior probabilities because of high levels of noise of information used for making decisions [5]. In such a situation it can be assumed that the a posteriori probabilities can be expressed as: where  X  ki 1 . If we expand the product and neglect any terms of second and higher order, we can obtain the sum rule as follows: 2.3 Derived Combination Strategies Mathematically, it is easy to see the following relation holds This relationship has suggested in [5] that the product and sum decision rules can be approximated by the upper or lower bounds appropriately, and under the assumption of equal priors, we can derive the following decision rules: In [10], Pedersen used the topic context with different sizes of context windows, grouping into three groups: small (with sizes of 0, 1, 2), medium (with sizes of 3, 4, 5), and large (with sizes of 10, 25, 50), for creating different representations of a polysemous word. For the purpose of experimental comparison, we simply con-sider the maximum window size in each group and generate nine representations by combining different sizes of left and right windows. We call this Pedersen X  X  multi-representation of context.
 sources for determining the sense of a polysemous word are the topic of con-text and relational information representing the structural relations between the target word and the surrounding words in a local context. Under such an obser-vation, we have experimentally designed five kinds of representation defined as follows: f 1 is a set of unordered words in the large context; f 2 is a set of words assigned with their positions in the local context; f 3 is a set of part-of-speech tags assigned with their positions in the local context; f 4 is a set of collocations of words; f 5 is a set of collocations of part-of-speech tags. Symbolically, we have  X  X  2 = { ( w  X  n 2 ,  X  n 2 ) ,..., ( w  X  1 ,  X  1) , ( w 1 , 1) ,..., ( w n 2 ,n 2 ) }  X  X  4 = { w  X  l  X  X  X  w  X  1 ww 1  X  X  X  w r | l + r  X  n 4 }  X  X  5 = { p  X  l  X  X  X  p  X  1 wp 1  X  X  X  p r | l + r  X  n 5 } where w i is the word at position i in the context of the ambiguous word w and p be the part-of-speech tag of w i , with the convention that the target word w appears precisely at position 0 and i will be negative (positive) if w i appears on the left (right) of w . In the experiment, we design the window size of topic context (for both left and right windows) as 50 for the representation f 1 , i.e. n 1 = 50, while the window size n i of local context as 3 for remaining representations. We tested on the datasets of four words, namely interest , line , serve ,and hard , which are used in numerous comparative studies of word sense disambiguation methodologies such as Pedersen [10], Ng and Lee [9], Bruce &amp; Wiebe [1], and Leacock and Chodorow [7]. In the experiments, we use a 10-folds cross validation and the experimental results are given in Tables 1 and 2.
 Pedersen X  X  multi-representation of context correspond to Sum rule and Median rule, that for our multi-representation of context in most cases corresponds to Product rule, with the exception of Median rule for hard . Turning back to Peder-sen X  X  method, we see that different representations have some overlaps between them, so that the conditional independence assumption imposed on individual classifiers may not be suitable for this multi-representation and, consequently, the Product rule does not yield the best result. On the contrary, in our multi-representation of context, each individual classifier corresponds to a distinct type of features so that the conditional independence assumption seems to be realistic.
 with Pedersen X  X  method and our method of multi-representation with previ-ous WSD studies tested on the same datasets. It is shown that the best clas-sifier combination according to our method gives the highest accuracy in all cases. In this paper we have argued that various ways of using context in WSD can be considered as distinct representations of a polysemous word for jointly us-ing to identify its meaning. This consideration allowed us to apply a common theoretical framework for combining classifiers developed in [5] to develop numer-ous strategies of classifier combination for WSD. In parallel with the experiment conducted on Pedersen X  X  multi-representation of context, we have experimentally designed a set of individual classifiers corresponding to distinct representation types of context considered in the WSD literature. It has been shown that this multi-representation of context significantly improves the accuracy of WSD by combining classifiers.
 This research is partly conducted as a program for the  X  X ostering Talent in Emergent Research Fields X  supported by the Japanese Ministry of Education, Culture, Sports, Science and Technology.

