 Due to the exponential growth of the web, an important challenge of web crawler is to siderable research attention. 
Some distributed crawling systems have been worked out to finish Web massive use a centralized server to manage the communication and synchronization of distributed crawling systems have been proposed in [4, 5, 6], that is, no central coor-dinator can exist in these systems. In these systems, large numbers of nodes collabo-rate dynamically in an ad-hoc manner and share information in large-scale distributed assignment and ignore the unbalance in crawling process. Another issue that has not been well resolved is scalability caused by the arrivals and departures of nodes. crawler based on a distributed crawling model. A structured architecture will be pro-posed and the mechanism to achieve load balance and scalability will be given. In our crawler, crawling nodes are organized as a structured ring network to offer the service of collecting Web pages. The ring is composed of several crawling nodes that partures. More importantly, they are simple to implement and incur virtually no over-Figure.1. 
Each crawling node in system is composed of 2 parts: crawling module and control module. The function of crawling module is to download web pages from Internet according to the URLs queue. The function of control module is to manage the com-munication and harmony with other crawling nodes. The inside organization of each crawling node can be described by Figure.2. implemented in control module. The core of the strategy is called distributed crawling model (DCM). As mention above, the model is composed of three parts which will be described as follow. 3.1 Tasks Assignment The sub-module of tasks assignment is used to divide whole crawling task into differ-ent parts, and allocate them to each node in order to achieve a parallel processing. We propose a new method of tasks assignment which is a dynamic consecutive division to the value space of hash function, and we explain why this method makes it possible to decentralize every task and to resolve the above problems. 
Let the value space of hash function be a rang from a . Let n denote the number of nodes, we can get a division with  X  will take charge of the URLs whose ) ( URL H are located in the range of ) , (
At the beginning, we initialize the value of 
Obviously, formula (1) is a n equivalent division on the range of ) , ( dynamically change the value of more detail will described in next section. 
The crawling nodes are organized as a ring. Each node has two neighbors which called  X  X receding-node X  and  X  X ollowing-node X . The hashing value of URLs on pre-URLs queues: local-queue, preceding-queue and following-queue. The URLs in pre-ceding-queue need to be sent to preceding-n ode, URLs in following-queue need to be work named  X  X orward-token X  and  X  X ackward-token X . The  X  X orward-token X  starts off from the first node in network which charges the set of the smallest hashing value of URLs. The node holding  X  X orward-token X  will operate as follow: 
The time of token walking a circle on the ring is called cycle T . In order to avoid URL will arrive at the corresponding node within the time T . 3.2 Dynamic Load Balance Management Load balance means that each node should be responsible for approximately the same number of URLs. But the n equivalent division on the range of ) , ( sure that there are same number URLs located in each part. So we provide a dynamic load balance model to achieve the characteristic of load balance. Our model is based on three principles: We use a token called DLBT (dynamic load balance token) to perform the function. and its  X  X ollowing-node X . The DLBT starts off from the first node and walk on the ring. 3.3 Scalability Maintenance High scalability means that the more crawling nodes, the higher performance. We and manage the arrivals and departures of crawling nodes. 
The mechanism is rather simple. Each node in the structured network not only closest nodes in up and down direction. If a node is failure, its neighbor will find the next node to rebuild the virtual link. If a node joins in the network, it will request for the connected node and get the information of neighbors to create the link, of course, the redundant link will be removed. The goal of this section is to analyze the load balance and scalability features of our dynamically assign URLs to each crawling node. The consequence can be obtained by analyzing colleted Web pages by each node every hour. 
In Jan 2007, we utilize our crawler to get experimental data which are about 7771402 Web pages with 21.75GB capacity within ten hours. And the number of Intel PCs with the P4 3.0GHZ Intel processors, 2GB of memory and 400GB hard SATA disk, the bandwidth is 100M. The operating system is Redhat Linux 9.0. 
Figure.3 shows the performance of load balance of our system. The experimental improving the load balance in distributed crawler systems. 
With the more number of crawling nodes, the crawling speed of our system is nodes, the overload of the synchronization and communication among the nodes may decrease the performance. work. A distributed crawling model (DCM) is proposed to achieve the merits of load balance and scalability. And a new method of tasks assignment is presented, which is a dynamic consecutive division on the value space of hash function. Also, a dynamic scalability in distributed crawling environment. This paper is supported by the Key Program of National Natural Science Foundation (No. 60435020). 
