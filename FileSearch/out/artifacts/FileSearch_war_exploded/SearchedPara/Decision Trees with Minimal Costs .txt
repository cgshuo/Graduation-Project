 Charles X. Ling CLING @ CSD . UWO . CA Qiang Yang QYANG @ CS . UST . HK Inductive learning techniques have had great success in building models that assign test cases to classes (Quinlan 1993; Mitchell 1997). However, much previous inductive learning research has focused on minimizing classification errors which are useful in deciding whether a learned model tends to make correct decisions when assigning class labels to new cases; thus, they are an important factor to consider. There are, however, different types of classification errors, and their costs are often very different. For example, in a binary classification task in the medical domain, the cost of false positive (FP) and the cost of false negative (FN) are often very different. In addition, misclassification costs are not the only costs to consider when applying the model to new cases; we also consider the  X  X est cost X  which is as important as the misclassification cost when test cases do not provide all the values for their attributes that may be necessary for classification. That is, the test strategies may suggest obtaining more information, at additional cost, for the missing values. Inductive learning methods that consider a variety of costs are often referred to as cost-sensitive learning (Turney 2002), and tasks that incur both misclassification and test costs abound in practice. As an example, consider again the task of a medical practice that examines incoming patients based on previous experiences. Suppose that these experiences have been compiled into a model such as a decision tree (Quinlan 1993). When dealing with a new patient, certain information about this patient may be unknown; for example, the blood tests or the X-ray test may not have been done yet. One obvious approach to solving this type of problem is to use C4.5 X  X  strategy (Quinlan 1993) in dealing with missing values. When a test case is classified by the decision tree, and is stopped at an attribute whose value is unknown, no test will be performed to obtain its value; instead, the test case is distributed along attribute branches and the classification results are weighted on all of the branches. But this approach ignores the possibility of obtaining the missing value at a cost to further reduce the misclassification cost, and thus the total cost. One of our test methods (the third method discussed in Section 4) uses the C4.5 X  X  strategy, and it is shown to be inferior to the new method proposed in this paper (see Sections 4 and 5). Another obvious approach is to perform all tests for unknown values. This is clearly not optimal either as the tests can be very expensive. A third obvious approach is to utilize the decision tree built by C4.5 to guide which tests should be performed. Again when a test case is classified by the decision tree, and is stopped at an attribute whose value is unknown, the tree naturally suggests that this test should be done at a cost, and the test case will follow the appropriate branch, until it can be classified in a leaf. The proble m with this approach is that when building the decision tree, the costs of obtaining these test results are completely ignored. As a consequence, tests that incur heavy costs may be placed on top of the tree, requiring all future patients to complete these tests. This may greatly increase the total test cost, and thus, the total cost. We compare this ap proach with our new methods, and show that the former is again inferior to the best method we propose (Sections 4 and 5). In this paper, we study a tree-building strategy that minimizes the misclassification and test costs. Also, we study a set of test strategies that may suggest additional tests at a cost to minimize the total cost on test cases. Our tree-building algorithm has a number of very desirable properties for cost-sensitive learning systems, including important properties as pointed out by (Turney, 2000; Turney 1995). For exam ple, if all test costs are larger than the misclassification cost, then no test should be performed and a one-node decision tree will be returned. As important as building a tree with minimal total cost, we formulate several strategies to deal with unknown values by taking into account both the misclassification and the test costs. These strategies are compared with each other and the best strategy is selected. The rest of the paper is organized as follows. We first review the related work in Section 2. Then we present our new tree-building algorithm, and show it has many desirable properties (Section 3). After that, we consider several test strategies and analyze their relative merits (Section 4). Finally, we present our experimental results (Section 5) and conclude the work with a discussion of future work (Section 6). Much work has been done in machine learning on minimizing the classification erro rs. This is equivalent to assigning the same cost to each type of classification error (for example, FP and FN), and then minimizing the total misclassification costs. In his survey article, (Turney 2000) analyzes a variety of co sts in machine learning, and the test cost is singled out as one of the least considered areas in machine learning. In particular, (Turney 2000) considered the following ty pes of costs in machine learning: z Misclassification costs: costs incurred by z Test costs: costs incurred for obtaining attribute In (Zubek and Dieterrich 2002), the cost-sensitive learning problem is cast as a Markov Decision Process (MDP), and an optimal solution is given as a search in a state space for optimal policies. For a given new case, depending on the values obtained thus far, the optimal policy can suggest a best acti on to perform to minimize both the misclassification and the test costs. While related to our work, their research adopts an optimal strategy, which may incur very high computational cost to conduct the search. In contra st, we adopt the local search algorithm of (Quinlan 1993) using a polynomial time algorithm to build a model, which returns a new decision tree. Then, when performing the test, our strategy (see later) together with the decision tree will suggest whether to conduct a test or not effectively. (Greiner et al. 2002) studied the theoretical aspects of active learning with test costs using a PAC learning framework. (Turney 1995) presented a system called ICET, which uses a genetic algorithm to build a decision tree to minimize the cost of tests and misclassification. Our work also considers the decision tree model, where we additionally consider both the minimization of misclassification cost on training data and the formulation of a test strategy for minimizing the test costs on the test data. As mentioned above, because our algorithm essentially adopts the same decision-tree building framework as in (Quinlan 1993), our algorithm is expected to be more efficient than Turney X  X  genetic algorithm based approach. We assume that the training data may consist of missing values (whose values cannot be obtained). We also assume a static cost struct ure where the cost is not a function of time or cases. Further, we assume that the test cost and the misclassification cost have been defined on the same cost scale, such as the dollar cost incurred in a medical diagnosis. Our new decision-tree learning algorithm is quite simple. We consider discrete attribute and binary class labels; extensions to other cases can also be made. We assume that FP is the cost of one false positive example, and FN is the cost of one false negative example. Our algorithm uses a new splitting criterion of minimal total cost on training data, instead of minimal entropy, to build decision trees. This cost measure is equivalent to the expected total cost measure us ed in the works of (Turney 1995; Zubek and Dietterich 2002; Greiner et al. 2002). At each step, rather than choosing an attribute that minimizes the entropy (as in C4.5 ), our algorithm chooses an attribute that reduces and minimizes the total cost, the sum of the test cost and the misclassification cost, for the split. Then, similar to C4.5 , our algorithm chooses a locally optimal attribute with out backtracking. Thus the resulting tree may not be globally optimal. However, the efficiency of the tree-building algorithm is generally high. A concrete example is given later in this section. A fine point of our new algorithm is the way it deals with attributes with unknown values in the training set. In many variations of decision tree algorithms, the unknown value is treated as an ordinary value. However, in our work, the strategy is that all unknown values (we use  X ? X ) are treated as a special  X  X alue X : no leaf or sub-tree will be built for examples with the  X ? X  value. This is because it is unrealistic to assume the unknown values would be as useful for classification as the known values. In addition, when a test example is stopped at an attribute whose value is unknown, if the attribute has a  X ? X  branch, it is impossible to decide whether the test should be performed by the tree. Therefore, the examples with unknown attribute values are not groupe d together as a leaf, or built into a sub-tree; instead, they are  X  X athered X  inside the node that represents that attribute. We then calculate the ratio of the positive and negative examples in the internal node. See the example given later for more details. Our second test algorithm (see Section 4) will incorporate such ratios in making predictions. Another important point is how the leaves are labeled. In traditional decision tree algorithms, the majority class is used to label the leaf node. In our case, as the decision tree is used to make predictions to minimize the total cost, the leaves are labeled also to minimize the total cost. That is, at each leaf, the algorithm labels the leaf as either positive or negative (in a binary decision case) by minimizing the misclassification cost. Suppose that the leaf has P positive examples, and N negative examples. If P  X  FN &gt; N  X  FP (i.e., the cost of predicting negative is greater than the cost of predicting positive), then the leaf is labeled as positive; otherwise it is labeled as negative. Therefore, the label of a leaf does not just depend on the majority class of the leaf, but also on the cost of misclassification. Let us look at a concrete example. Assume that during the tree building process, there is a set of P and N positive and negative examples respectively to be further classified by possibly buildin g a sub-tree. If we assume that P  X  FN &gt; N  X  FP , then, if no sub-tree is built, the set would be labeled as positive, and thus, the total misclassification cost is T = N  X  FP . Suppose that an attribute A with a test cost C is considered for a potential splitting attribute. Assume that A has two values, and there are P1 and N1 positive and negative examples with the first value, P2 and N2 positive and negative examples with the second value, and P0 and N0 positive and negative examples with A X  X  value unknown. Then the total test cost would be (P1+N1+P2+N2)  X  C (i.e., cases with unknown attribute values do not incur test costs). Assume that the first branch is labeled positive (as P1  X  FN &gt; N1  X  FP ), and the second branch is labeled negative, then the total misclassification cost of the two branches is N1  X  FP+P2  X  FN . As we have discussed earlier in this section, examples with the unknown value of A stay with the attribute A, and we have assumed that the original set of examples is labeled as positive. Thus, the misclassification cost of the unknowns is N0  X  FP . The total cost of choosing A as a splitting attribute would be: T = (P1+N1+P2+N2)  X  C + N1  X  FP + P2  X  FN + N0  X  FP If T A &lt; T, where T = N  X  FP , then splitting on A would reduce the total cost of the orig inal set, and we would then choose an attribute with the minimal total cost as a splitting attribute. We then apply this process recursively on examples falling into branches of this attribute. If T T for all remaining attributes, then no further sub-tree will be built, and the set would become a leaf, with a positive label. Finally, as our tree attempts to minimize the total cost, it may also overfit the training dataset. Traditional decision tree algorithms such as C4.5 incorporate a post-tree pruning procedure to simplify the tree. The current version of our algorithm, however, does not yet incorporate tree pruning. As all of our tree building algorithms (see Section 3) build unpruned trees, our experiment comparisons (Section 5) are still fair and valid. It remains our future work to include pruning in our tree-building algorithm with a minimal total cost. Aimed at minimizing the total cost of test and misclassification, our new d ecision-tree algorithm has several desirable features. We will discuss these features below, using the dataset  X  X coli X  as an example (Blake &amp; Merz 1998). This dataset, after pre-processing, has 332 labelled examples, which are described by six attributes. The numerical attributes are first discretized using the minimal entropy method (Fayyad &amp; Irani 1993), as our tree building algorithm can only accept discrete attributes (but it is straightforward to extend our algorithm to accept continuous attributes as C4.5 does). The attribute values are renamed as 1, 2, 3, and so on. More details on this and other datasets used in experiments can be found in Section 5. The first property, as discussed in the Introduction, is that the relative difference between misclassification and test costs can affect the tree drama tically. If the former is less than the latter, then no test should be performed, and the decision tree would be simply a one-node leaf. On the other hand, if the former is much larger than the latter, then all tests should be done, as long as they are relevant; i.e., they can improve the predictive accuracy. This can be seen clearly from the  X  X coli X  dataset. Indeed, if the misclassification cost is set to 200 for both FP and FN, and all test cost is set to 300, then the algorithm returns a one-leaf node as shown in Figure 1 (a). On the other hand,  X  X argest X ; in this case, the tree h as 13 nodes in to tal, and can be see n in Figure 1 (c ). As a n  X  X nterm ediate X  case, if all test co sts are set to 20 , then the decision tree with t he minim al cost has si x node s in total, an d t he t ree ca n be seen i n Fi gure 1 (b). Figure 1 . Three differen t d ecision trees bu ilt with diffe rent test c osts. The sec ond import ant an d desi rable p roperty is that fo r attrib utes with d ifferen t test co sts, our n ew alg orith m is lik ely to ch oo se an attribu te with zero or the sm allest co st at th e to p (or ro ot) of th e tree. Th is is b ecause th e attrib ute at th e roo t will b e tested by all ex am ples, and thus th e total attrib ute co st wo uld be relativ ely high . Choo sing an attrib ute with zero o r the sm allest co st help s red uce the total co st. Of course attribu te selectio n also depends on the d istri bution of attribu te values and class lab els of the training e xam ples. Table 1 s hows three cases in whic h attri bute costs are diffe rent. I n the first case (the baselin e), all attrib ute co sts are set to 20. In the sec ond a nd t hird cases attribute costs are set differently. Th e m iscla ssificatio n cost is set at 800 for bot h FP a nd FN. As we can see , in the second case , the attribu te A2 has the sm all est test co st, an d it is ind eed chosen as t he root of the t ree as sh ow n i n Fi gure 2 (b). In the th ird case, attrib ute A5 has th e sm alles t test co st, and it is ch osen as t he roo t (Figu re 2(c)). Th e third property, related to th e secon d one, is th at wh en the test cost of an attri bute is in creased, th at test attrib ute will b e  X  X u shed X  down in the tree, un til it  X  X alls o ut X  of the tree (wh en the test co st becomes to o large). If th e test other co sts ar e fi xed, we obtain trees (not sho wn he re) with A1 at t he roo t (sim ilar to Figu re 2(a)), in th e m iddle respectively. After t he m inimal-co st d ecisio n tree is bu ilt, th e n ext interestin g qu estio n is ho w t his tree can be u sed to deal with test exam ples with m any m issi ng val ues, i n order to predict th e class of th e test exam ples with th e m inimal total co st for t his case. Decid ing wh ich tests shou ld be per formed i s a part of the test strat egy. We will stud y fo ur test st rateg ies. We use th e decision test ex am ple in Tab le 3 to illu strate th e four strateg ies descri bed below. Bear in m ind th at th is is on ly for on e particula r test case. T he ov erall performance of these strateg ies will b e co mpared in the next sectio n with a large num ber of test exam ples. Th e first strateg y, called Op tim al Seq uential Test (OST), is v ery sim ple and intuitiv e. It u ses th e tree built with th e minim al cost to deci de w hat tests must be per formed i n sequence . More specifically, each test exam ple goes down the tree until an attribu te who se value is un kno wn is m et in the test exam ple. As th e tree was bu ilt to m inimize th e total cost, this tree would suggest that this test should be per formed at the c ost, a nd i ts val ue wo uld deci de which branch to go down the tree fur ther. For example, wh en the test exam ple in Ta ble 3 goes down the t ree in Figure cost 20, a nd it reveals the val ue 3. The n the exam ple goes down to node A1, a nd a test on A1 is perform ed at a cost 50, with the valu e 6. Thu s, it falls in to th e righ tm ost leaf under A 1, which p redi cts the class P. Th e pred ictio n is the sam e as the true class of the test case, so the re is no misclassificati on cost. Th us t he to tal co st is 20 + 50 = 70.
A1 A2 A3 A4 A5 A6 Class ? (6) 2 ? (1) 2 2 ? (3) P Th is strateg y, wh ile op tim al based on th e min imal co st o f the train ing set, is sequ ential . Th at is, on e must wait fo r the resu lt o f the first test before th e next test can b e det erm ined. In m edical di agnos es, doctors often ca nnot wait fo r the res ult of the first t est bef ore ot her tests can be done; they normally or der a set of tests to be done at once . A  X  X atch Test  X  strateg y can be m odeled easily in to ou r decision tree. Whe n a test case is stoppe d at the first attr ibute whose v alue is unknow n, all unk now n v alues under th at attrib ute m ust b e ob tain ed. Clearl y th is strateg y will return the sam e p redictio n as OST (i.e., sam e miscl assi ficat ion cost), but it w oul d i ncur a higher test cost. The second s trategy uses t he sam e deci sion to m ake done. More s pecifically, whe n a test exam ple is stopped at an at tribute w hos e val ue i s unknown, i t stops right there, and uses th e ratio of po sitive and negativ e ex am ples in that (in tern al) node to predict th e test ex am ple (recall th at these rati os a re calculated based on trai ning cases whic h also have un known val ues at this no de) . U sing the sam e exam ple, whe n the test e xample stops at the node A6, it would pre dict t hat t he test exa mple is of clas s for t he node total co st is t hus 0 here. The t hird strat egy is a va riation of the sec ond st rat egy. Inst ead of st oppi ng at the node whose at tribut e val ue is unkn own in the test case, th is strateg y will  X  X p lit X  th e test case into fracti ons according t o the t raining e xam ples, and go down all branche s sim ultane ously. The final class is a weighted sum of the class i n each branc h. Note that this is essen tially C4 .5 X  X  strateg y in dealin g wit h missin g values. Using th e same ex am ple to illustrate th is strateg y, th e test case will no t sto p at nod e A6 th is tim e; instead , it will distribute into four branche s with a rat io 107/ 108/6/111. The fi rst two bra nches make a correct pre diction with no misclassification cost. T he last bra nch m akes a wrong predictio n, with a m isclassifi catio n co st of 800. Th e t hird branch en counter s ano ther unk now n valu e, so it is distributed further down in the tree, with a ratio of 1/1/2/2. The fi rst two bra nches m ake a wr ong pre diction (c ost ing 800), while the ne xt two bra nches make a correct predictio n. With th e t otal n umber of 33 2 (23 0+10 2) train ing ex am ples in th e tree b uild ing, th e weigh ted co st for th is test ex am ple is th us: 80 0  X  (1+1 +111 )/332 = 272 .3. Th e fourth and f inal str ateg y also stipulates that no furthe r tests sho uld be don e, bu t it utilizes th e ex istin g attri bute values to th e full ex ten t. For each test exa mple, a new (and differen t) d ecision tree i s bu ilt d ynamically fro m all of th e train ing exam ples with on ly tho se attribu tes who se values are known in th e test ex am ple. In t his way, the new decision tree on ly u ses attri butes w ith kno wn v alues i n the test exam ple, and t hus, no new test is needed. As an exam ple, as A2, A4, a nd A5 a re the only known attr ibutes, a n ew decision tr ee (no t sh own her e) using the training exam ples with A2, A4 , and A5 as attrib utes will be built. Fro m this tree, we obtain th at th e to tal co st is 80 0. of lazy learni ng algorithm whe re t he learning m odel is built only du rin g test and can be affected by the test exam ples (see, for e xam ple, LazyDT by (Friedm an et al 1996)). He re, a s test exam ples m ay have a diffe rent set of known attributes, the trees from diffe rent test exam ples can be diffe rent, too . We ex pect t hat ou r first test strateg y, th e Op tim al Sequ ential Test, wou ld be th e b est with th e overall lo west total co st, as it is b ased on m inimizin g th e total co st in th e train ing set. Th e fou rth m ethod , bu ild ing differen t trees for differen t t est cases, wou ld be seco nd, as it utilizes fully th e train ing data, and lik e lazy learn ing , it exp lores the searc h space in the l ocal regi on. T he se con d and t hird methods woul d p robably pe rform the wo rst. In t he ne xt sectio n, we will p erform ex ten sive ex perim ents to com pare a nd eval uat e these m ethods with real -world datasets. We c onduct expe riments on five real -worl d dat aset s and com pare the four test strategi es against the baseline C4.5. In C4.5, we use th e inform atio n gain to bu ild a decisio n tree (wi thout pruning). M issing val ues ar e i gnored i n training e xam ples as done i n C4 .5. Th en the tree is used to pred ict th e test ex am ples with a process si milar to our first test strateg y (Op timal Seq uential Test). Th at is, wh en the test example is classifi ed by the tre e, and if a n test exam ple then goes down furt her acc ordi ng to t he value obtaine d, until it reaches the l eaf, whe re a predictio n is mad e. We use fi ve dataset s in our expe riments. These dat aset s are chose n because they have at least som e discrete attribut es, binary cl ass, an d a go od number of e xam ples. The num erical attributes in datasets are discretized fi rst usi ng m inim al ent ropy m ethod (Fay yad &amp; Ira ni 1993) as our al gorithm can curre ntl y only deal with discrete attrib utes. Th e datasets are listed in Tab le 4. Ecoli 6 332 23 0/102 Breast 9 683 44 4/239 Heart 8 161 98/ 16 3 Thy roid 24 2000 17 62/238 Au stralia 15 653 29 6/357 For the expe riments, each dataset is split into t wo parts: the train ing set (60 %) and the test set (4 0%). A decision tree is bu ilt from th e train ing set u sing our new al gorith m that minimizes th e to tal co st (Section 3). Fo r our fou rth lazy-style test m ethod, a di ffe rent tree is buil t for each test case. T he deci sion tree is t hen used to pred ict th e test exam ples, and to decide what tests, if a ny, should be perform ed to min imize th e total co st. Th is process is repeat ed five t imes, and resul ts, i n co nse quent figures , are avera ges of the five runs. As we d iscu ssed in the In trodu ctio n section , test exam ples would of ten hav e m ore un kn own values, as it is p art of the test proces s to de cide what tests nee d to be pe rformed. There fore, a ce rtain pe rcenta ge of attributes are ra ndom ly selected an d mar ked as unk now n. If th e test algo rith m decid es to perfo rm a test on an un kno wn attrib ute, th en its real val ue is revealed a nd a cost is inc urre d. Fig ure 4 shows differen t alg orith ms in term s o f the diffe rent pe rce ntages of unknown at tribute val ues i n the test exam ples. This figure s hows the graph for t he Ec oli dat aset , whe reas the ot her fi gures f or ot her dat aset s are sim ilar and thus are om itted. The scales on t he x-axis (20%, 4 0%, and so on ) represe nt the perce ntage of unkn own attri butes in th e test sets. The curv e represen ts the ave rage tot al cost of a te st case of fi ve differe nt test strat egies, a verage d ove r five r uns. In t his set of expe riments, the m iscl assi ficat ion cost is set as 40 0/400 (400 for FN and 400 for FP ), a nd t he test costs are set rando mly b etween zer o and 10 0. Figure 4 . C omparing t he total cost under di ffe rent unkn own s. Fro m th is experim ent, we can draw sev eral in terestin g concl usions. Fi rst , ou r first method ( M1), O ptimal Sequ ential Test (OST), is clearly th e wi nn er. Th e to tal cost is always the lowest, a nd it does not i ncrease m uch whe n the perce ntage of un known val ues increases. Thi s is mainly becaus e the test c osts are relatively cheap, a nd with the suggestions of tests per formed by OST, the final miscl assi ficat ion cost). Sec ond, ou r fo urth m ethod (M 4), a lazy-style decision tree al gorith m, is the second best when th e percentag e of unkn own attr ibutes is less t han 60% b ecau se it u tilizes fu lly th e kno wn attrib utes by buildi ng a ne w decision tree for eac h test exam ple. Ho we ver , w hen there a re t oo m any unk nown at tributes (su ch as 80 %), th e decision tree bu ilt fro m only 20% of the known att ributes is obvi ously i naccurate, thus the misclassification c ost i ncrea ses dram atically, increa sing overall, and as with OST, th e total co st d oes n ot in crease with m ore m issing v alues i n test cases. This sho ws th at doing tests (as in Op tim al S equential Test and C4.5) is better than not do ing tests (as in Method s 2, 3 an d 4) wh en th e test cost is no t to o larg e. However, C4.5 is not as good as OST because te st costs a re not take n i nto consid eration wh en the deci sio n tree is built. Fou rth, t he seco nd a nd third m ethods (M 2 a nd M 3) are wo rse, because they use a single decision tree built from the train ing set, an d it do es not p erfo rm tests to im prov e pre dictive ac curacy. He re we ca n s ee that thei r perform ance degra des as th e unknown val ues i ncrease . Fifth , it is clear fro m th e grap hs th at th e m ore un kno wn attrib ute v alues, th e h igher the to tal co st for test strateg ies with out doing th e test, and th e m ore ad van tageous our first m ethod Optimal Seque ntial Test an d C 4.5 wo uld be com pared t o other m ethods. Recall th at M3 is essen tially C4 .5 X  X  st rateg y in dealing with un kno wn v alues. It is su rprisi ng to see th at i n this and later ex periments M3 (the C4.5 strategy) seem s to be worse th an the n a X ve st rateg y M2 . We th ink th at th e m ain reason is th at distribu tio n into branc hes of t he tree due to unknown values accum ulates a large m isclassification cost ov erall. It wo uld be in terestin g to see if C4.5 wou ld be bet ter of f us ing the na X  ve strat egy, as in M 2, in deal ing with m issin g valu es. The ne xt se t o f ex peri ments com pares differe nt alg orith ms in term s o f their test co st m agnitud es wh ile th e misclassif icati on co st is f ixed at 400 /400. In Fi gure 5, wh ich plots the resu lt on th e Eco li dataset, t he co sts of t he tests (at tributes) range from 50 to 400. The pe rce ntage of unknown at tribut e values is set to 60%. Agai n we ca n mak e m any in terestin g con clusion s, so me of wh ich are sim ilar to what we stated earlier, bu t some are quite diffe rent. Fir st, our first m ethod (M 1), Optimal Seque ntial Test (OST), i s still clearly th e wi nner. Bu t o ther test strat egies we propose d (M 2, M 3, an d M4) becom e very sim ilar to OS T whe n the t est cost i ncre ases. T his is expecte d as t he test cost s inc rease, our tree-building alg orith m will prefer no t to bu ild a tree (o r only to bu ild a one-nod e tree) to sav e th e t otal co st wh ich m ay en d up with lower to tal co sts. When this h app ens, t he four test strat egies we propose d m ay becom e the sam e. Secon d, C4 .5 perf orms m uch wo rse when the test cost inc reases. This is also e xpecte d as C4.5 bu ild s the sa me decisio n becom e much larger whe n the test costs increase. Third, whe n the test cost is still re latively s mall (from 50 to 100), our fourt h m etho d (M 4), a lazy-style decision tree algorithm , is st ill next in ra nking because, a gain, it utilizes fully th e kn own attribu tes by build ing a new d ecision tree for eac h test e xam ple. Fourt h, it is clear from the gra phs that th e test st rateg ies X  to tal test co st do es no t increase much whe n the test cost i ncrease s beca use our tree -buildi ng al gori thm and test s trategies aim at m inimizing the to tal co st of m isclassificat ions and tests. Our last set of experim ents is si milar to th e on e ab ov e, bu t with m ore unbalance d m isclassification c osts. T he FP and FN costs are set to 400/ 1500, a nd t he perce ntage of unkn own attribu te valu es is 60 %. In t his cas e, we confirm our e xpect ation t hat C 4.5 w oul d not per form wel l as i t misclassification c osts while our m ethods do. Our test algorithms M 1, M 2, a nd M 4 per form sim ilarly an d bet ter than C4.5, as they are base d on the decision tree with min imal to tal c ost. Th e resu lt is p resen ted in Fig ure 6. On e of our si gn ifican t resu lts is th at th e best p erform ance as dem onst rated by the m ethod M 1 on the Ecoli dat aset is rep eatab le t hrough ou t all d atasets th at we con sider (see Tabl e 4). Fi gures 7 (a ) a nd ( b) c ompare t he per form ance of M 1 and C 4.5 for diffe rent dat aset s. As the percent age of unknowns and the test costs change, the ratio of the average total cost by M1 over C4.5 is always lower than one across different datasets. We can conclude that the superiority of M1 is a general phenomenon. In this paper, we present a simple and novel method for effectively building decision tr ees that minimizes the sum of the misclassification cost and the test cost. Our method utilizes a new cost-based splitting criterion for attribute selection, and incorporates several intelligent test strategies that can suggest how to obtain missing values with new tests. Our experiments show that our new decision-tree-building algorithm, together with the best test strategy, Optimal Sequential Test, can dramatically outperform a number of other competing algorithms, including C4.5. In addition, compared to other related works, our algorithm has a much lower computational complexity, and is thus more practical. In the future, we plan to consider methods of minimizing the total cost when all new tests must be decided together, rather than in a sequential manner. We did extend our Optimal Sequential Test to the Batch Test in Section 4, but it would be interesting to find more effective methods. Also pruning can be introduced in our tree-building algorithm to avoid overfitting the data. Finally, we plan to study ways to incorporate other types of costs in our decision tree learning and test algorithms. We would like to thank Peter Turney for his helpful discussions and suggestions during this research. We thank Anna Maria Davis and Robert J. Yan for their help in careful editing and preparation of the manuscript. We also thank reviewers for useful comments. Charles Ling thanks NSERC, and Qiang Yang thanks Hong Kong RGC, for support of their research. Blake, C.L., &amp; Merz, C.J. (1998). UCI Repository of [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, CA: University of California, Department of Information and Computer Science. Domingos, P. (1999) MetaCost: A General Method for Making Classifiers Cost-Sensitive. In Knowledge Discovery and Data Mining , Pages 155-164. Elkan. C. (2001) The Foundations of Cost-Sensitive Learning . In Proceedings of the Seventeenth 
International Joint Conference on Artificial Intelligence (IJCAI'01), pp. 973-978. Fayyad, U. M., &amp; Irani, K. B. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th 
International Joint Conference on Artificial Intelligence , pages 1022--1027. Morgan Kaufmann, Friedman, J. Yun, Y. and Koha vi, R. Lazy decision trees, in Proc. 13th Nat'l. Conf. Artificial Intelligence , 1996. Greiner, R, Grove A. and Roth D. (2002) Learning Cost-Sensitive Active Classifiers, Artificial Intelligence Journal 139:2, pp. 137-174. Kai, M.T. (1998) Inducing cost-sensitive trees via instance weighting. In Principles of Data Mining and Knowledge Discovery, Second European Symposium, Springer-Verlag. pp. 23-26 Margineantu, D. (2001). Methods for cost-sensitive learning. Dissertation , Oregon State Univ. Mitchell, T.M. (1997) Machine Learning McGraw Hill Nunez, M. (1991), The use of background knowledge in decision tree induction, Machine Learning , 6, pp. 231-250. Quinlan, J. R. (1993) C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers. Tan, M. (1993). Cost-sensitive learning of classification knowledge and its applications in robotics. Machine. Learning Journal , 13, 7--33. Turney, P.D. (2000), Types of cost in inductive concept learning, Workshop on Cost-Sensitive Learning at the Seventeenth International Conference on Machine Learning, Stanford University, California. Turney, P.D. (1995) Cost-Sensitive Classification: Empirical Evaluation of a Hy brid Genetic Decision Tree Induction Algorithm, Journal of Artificial Intelligence Research 2, pp. 369-409, 1995. Zubek, V. B., Dietterich, T. G. (2002). Pruning Improves Heuristic Search for Cost-Sensitive Learning. In 
Proceedings of the Nineteenth International Conference 
