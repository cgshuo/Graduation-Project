 We formulate a problem that arises in unstructured enterprise in-formation management, and has high commercial impact: retrieve knowledge-rich documents in a large textual collection of technical documents. We call such documents principal documents .

We exploit the properties of large sparse text collections in order to address this problem. It is known that the centroids of docu-ment clusters on such collections form so-called  X  X oncept vectors" for the collection. However, typically these centroids do not cor-respond to documents in the collection. How then should they be used for retrieving documents? An immediate approach is to col-lect documents that are closest to the centroid, which we call CTC. We also propose an algorithm called PrinDocs . The key insight behind PrinDocs is the following: replace distance functions by coverage. In other words, instead of finding the  X  X losest" docu-ments to a concept vector, find those that  X  X over" the concept vec-tor. PrinDocs employs greedy weighted set covering and uses the concept decomposition offered by centroids, but does not use the cosine distance on documents.

We compare CTC and PrinDocs for retrieving knowledge-rich documents in enterprise unstructured technical collections. We demonstrate that PrinDocs comprehensively outperforms CTC. Our work suggests that coverage based approaches might be preferable to distance based ones for similar retrieval tasks.
 Principal Documents; Knowledge-Rich Documents; Enterprise Un-structured Information
Over 70% of all information in an enterprise exists in unstruc-tured textual formats [5]. In light of this, the retrieval of defini-tive, knowledge-rich pieces of information from enterprise techni-cal corpora has emerged as a major problem in diverse applications including eDiscovery, knowledge gathering and project discovery, salesforce support, etc. We call the definitive, knowledge-rich doc-uments in an enterprise technical corpus its principal documents . Standing in the way of retrieving such documents are other doc-uments that are not comprehensive and may represent previous stages in the development of the technology. Frequently, these have already been culled into making the  X  X inal" definitive knowledge-rich versions that we seek.

Can we use known structural properties of large textual collec-tions [4] in order to address this problem? It is known that cluster-ing performs a  X  X oncept decomposition" of the major concepts in the collection. Furthermore, the centroids of the resulting clusters emerge as  X  X oncept vectors" in that their most significant coordi-nates correspond to significant concepts.

However, these concept vectors (or centroids) do not correspond to actual documents in the collection. Not only that, they typi-cally do not lie very close to actual documents either. How then do we use the aforementioned structural properties in order to retrieve documents from the collection?
The immediate approach is simply to use the distance function of the collection, and retrieve the closest documents to the centroid. These could, conceivably, contain the knowledge-rich documents that we seek. We formalize this approach using an algorithm we call CTC, for  X  X losest to centroid" X  X  known paradigm in retrieval.
We propose an alternative: retrieve documents that  X  X over" the concept vectors. In other words, ignore the standard distance func-tion, and use a metric that relies on how well a document  X  X overs" the concept vector. We formalize this approach using an algorithm we call PrinDocs . PrinDocs addresses several nuances: do we re-quire a document to cover all of the concept vector, or parts of it? How do we then collect documents that cover different parts of the concept vector? How much coverage do we need?
We compare CTC and PrinDocs in detail in this paper. Our re-sults are clear: PrinDocs outperforms CTC consistently, and by a widening margin as the number of clusters k increases. Our results suggest that coverage based approaches offer promise in enterprise retrieval tasks.
We now describe the stages of PrinDocs : our algorithm to dis-cover principal documents in an enterprise corpus. 1. We cluster the corpus into k clusters denoted by { C 1 The value of k is usually dictated by the enterprise application. For cluster C i , the concept vector truncated to its most significant ` terms will be denoted by where t  X  denotes the unit vector in the corresponding direction. 2. We begin by representing each document D in C i as a vector on the same ` -basis as C i . Namely, we restrict the term frequency vector of D to the ` terms that occur in C i , and drop all the remain-ing terms. Denote this vector by  X  D  X  = P ` j =1 n i j ( D ) t n (  X  ) is the corresponding term frequency. We also form the binary representation of D , which is obtained by replacing each non-zero coordinate n i j ( D ) in  X  D  X  by the Boolean 1. We denote this binary representation of D by  X  D  X  = P ` j =1 b i j ( D ) t i j 3. We weight the documents in C i using the list C i and the rep-resentations constructed above. The weight of D is simply the sum of the concept weights of all its non-zero terms. Namely, This step along with the next are called ClusterRankDocs . 4. The weighting above induces a ranking on the documents in C . We will denote this ranking by subscript, so that D 1 represents the document with the highest weight, and so on. In general, there will be ties in this ranking, which we break using the following rule: in the case of a tie, we traverse the list C i in decreasing order of concept weight until we reach a concept where the frequencies of the tied documents differ. We then rank the document whose frequency is higher above the other. We may need to iterate this in case of a tie between more than two documents. This choice also allows the user to control the number of principal documents. 5. We construct, on the set of concepts in C i a  X  X overage-profile" vector T i = P ` j =1 T i j t i j . Here, T i j denotes the desired coverage for the concept t i j in the cluster. We wish to set a higher coverage-level for terms that are more important for this cluster. We achieve this by setting the desired coverage for a concept to be the natural log of its concept weight plus one. Namely, T i j = ln( w This is the  X  X ogarithmic coverage" profile. Based on the needs of the application, other profiles may also be considered (for example, scaled logarithmic, linear, etc.). 6. We initialize an empty list P i . We will populate P i cipal documents from C i during the course of the algorithm. 7. Next, we cover each concept in the cluster to its desired cov-erage in the coverage-profile T i , using our ranked list of docu-ments and a greedy algorithm. We call this subroutine (encom-passing Stage 8 as well) ClusterPrinDocs . We begin by exam-ining the top ranked document from our ranked list, and identify concepts that are already covered to the desired coverage-level. Namely, whether n i j ( D 1 ) &gt; T i j for any j . If the top-ranked doc-ument does not cover any concept to the desired level, we go to the second-ranked document, sum its coverages with the top-ranked, and inspect the coverage levels again. Namely, we check whether n j ( D 1 ) + n i j ( D 2 ) &gt; T i j for any j . We proceed in this manner until at least one concept is covered. 8. Now, we remove the concepts thus covered from the set of concepts C i , giving us a smaller concept list that we call C also remove the top-ranked document from the ranked list of docu-ments, and insert it into P i . This gives us a smaller (by one) set of vectors of lower dimension than what we had before this stage. We now recompute the ranking of documents on this reduced dimen-sionality space and reiterate the steps outlined earlier. We iterate this process until either all concepts are covered or a maximum number of M documents have been added to P i . This maximum number M is pre-determined: we have experimented with values of M = [2 , 3 , 5 , 10] extensively, and recommend such low values in practice. 9. Finally, for each cluster C i , we return P i as a set of princi-pal documents that is complete with respect to the concepts in C Namely, PrinDocs returns P = P 1  X  . . .  X  X  k .

What if the coverage-level for a particular concept is not met by the time the algorithm terminates? In that case, we settle for the maximum coverage afforded to it until M documents have been re-moved. In practice, we found that all concepts are usually covered by fewer than 10 principal documents, and the algorithm terminates after meeting the full desired coverage-profile.

PrinDocs converges in time O ( f ) where f is the time complex-ity of the clustering algorithm. After clustering, the algorithm has to rank each document in each cluster, inspect the top ranked doc-uments in each cluster, and iterate until all concepts are covered. Each of these tasks has time complexity dominated by O ( f ) . Data : C i , C i , coverage-profile T i , M
Result : Set of principal documents for C i complete for
Initialize: P i  X  X  X  ; C  X  i  X  C i ; n  X  1 ; repeat until (All concepts in C i covered) OR ( n &gt; M ) ; Return P i ;
Algorithm 1: ClusterPrinDocs : Subroutine for identifying principal documents for a given cluster. Details in  X 2, 7-8.
The problem of clustering in retrieval has received much atten-tion, starting from when information retrieval was in its infancy. Various schemes were proposed earlier on for increasing the effec-tiveness of search. These schemes used a static clustering of the entire corpus; see [12] for an excellent review of the main ideas in these early works.

The application of clustering to browsing, as opposed to search, was initiated by [3], who proposed scatter-gather X  X  browsing scheme for large file collections. [7] provide the first discussion of scatter-gather on the results of a query; namely, they cluster the documents returned by a search engine as the results of a query, and then perform scatter-gather on it. The paper provides anecdo-tal evidence of the efficacy of scatter-gather using a few examples. [8] and [11] also consider the problem of using clustering on top-ranked search results to improve retrieval.

The CTC baseline algorithm that we compare to is related to these cluster based browsing techniques. The use of the cluster centroid for search (as opposed to browsing) was one of the early ideas in clustering in information retrieval. For example, [2] pro-posed a strategy in which a document that was representative of the query (using similarity metrics) would be used for the comparison to the cluster centroids in order to determine relevant clusters.
We should stress that the work on clustering in information re-trieval, though extensive, is not directly related to our work for the following two important reasons. Firstly, we address the problem of retrieving knowledge-rich documents from an enterprise corpus, which has not been addressed before. Secondly, our algorithm uses clustering in a very different manner than previous approaches: we do not use a cluster as a unit of retrieval, but rather use clustering and concept profiles within clusters as an intermediate technique to feed into our coverage algorithm. In particular, the coverage as-pects of our approach are novel, and do not appear in information retrieval literature.

Three works that do not fall under the rubric of clustering in re-trieval, but are related to ours are now described. [4] introduce  X  X oncept decompositions" of a corpus based on cluster centroids. Our work also uses a similar technique to create digests, which are then used later by the coverage part of our algorithm. [6] consider the choice of representatives from document clusters. The repre-sentatives they pick as  X  X ypical" are those that are closest to the centroid. This is the CTC algorithm that is our baseline. [1] con-sider a maximally diverse set of documents to represent clusters. The diversity is defined either in terms of distances, or categories. It would be interesting to apply their technique to diversity in terms of coverage. Since our work does not assume the notion of query, it is only peripherally related to the work on search diversification.
We also mention that [9] is related to our work because of the technique that they use: namely, centroid based summarization. Our baseline comparison algorithm CTC uses a closest to centroid criterion for choosing documents. Furthermore, the set of complete principal documents may be seen as a  X  X ocument-level summary" of the collection. Indeed, the algorithm is being considered for use in an enterprise-class leading retrieval platform in such a capacity.
In order to validate our approach, we constructed a dataset start-ing from a real-world corpus of enterprise origins. We then per-formed a series of processes on this corpus in order to recreate the  X  X ixed" nature of enterprise technical corpora. Now we describe this dataset. We began with the set of 301 technical reports generated by a Fortune-10 IT company  X  X orp-A X  in the years 2000 and 2005 that are publicly available. We spaced the two apart by five years to preclude significant concept overlaps. This yielded 301 finished documents, representing a roughly equal number of projects. This is called the Corp-A-TR dataset.

We then split each of these files between 0 (namely, no split) and 6 ways (namely, into 7 pieces), at random, into equal length components. These splits would then represent less knowledge-rich documents in the collection 1 .

We concatenated each of the components produced earlier with itself the same number of times as the split that caused it. For example, if file F was split into 3 files F A , F B each of the three F A , F B , F C would be concatenated three times, leading to F A F A F A , F B F B F B , and F C F C F C . This was done for length normalization, so that the original file F and the files F F A F A , F B F B F B , and F C F C F C would be comparable in length. Notice that this length normalization makes it strictly harder for PrinDocs to identify principal documents. This is because this nor-malization does not affect the ranking of the documents ( X 2, Step 3) since that ranking only takes into account the binary representation of a document. However, it makes it more likely that if a con-catenated document is at the top of the ranking, then it will cause concepts it contains to be removed from the list C This reduces the advantage that true principal documents have due to their coverage properties 2 .

Finally, we added all the split-concatenated files to the original text files to create our dataset, which we call Corp-A-TR-Mixed.
The principal documents in the Corp-A-TR-Mixed dataset are naturally the original finished documents that came from the Corp-A-TR dataset. Therefore, our retrieval target was these originals.
We used bisecting k -means to produce our clusters, since the quality of clusters produced by it are among the best of any cluster-ing algorithm [10].
We ran PrinDocs on the Corp-A-TR-Mixed dataset. We ran four versions of PrinDocs . In the first, we allowed PrinDocs to identify its full set of principal documents from each cluster, no matter how high that number may be. In other words, we set M to be higher than the size of the corpus. We denote this regime by M =  X  .
To compare to a benchmark, we used the closest to cluster cen-troid (CTC) algorithm. The document(s) closest to centroid is fre-quently used in literature as the best  X  X epresentative" of the clus-ter. We allowed the CTC algorithm to match ClusterPrinDocs for number of documents picked. Namely, if, say in cluster C
These could come, for example, from portions of the original document, being worked on separately by different members of a project; from smaller, less informative versions of the final docu-ment, and so on
We recognized this obstacle to PrinDocs during our extensive in-ternal validation on real-world enterprise corpora.
 ClusterPrinDocs subroutine identified m i principal documents, then the CTC algorithm was also allowed to take the m i whose cosine similarity to the centroid of cluster C i was the high-est. These would be the documents chosen by the CTC algorithm, and would be compared to the m i principal documents identified by ClusterPrinDocs . By taking the union over all clusters, we ob-tained a full set of principal documents for the Corp-A-TR-Mixed dataset, as well as an equal number of documents chosen by the CTC algorithm.

Precision was computed as follows: it was the proportion of the documents retrieved by each algorithm that was from the Corp-A-TR dataset. Namely, the precision was the proportion of original finished technical reports that each algorithm was able to retrieve. Each experiment was run three times, and averages are reported. We did not observe significant variations between runs.
 Finally, the design above was repeated, but with the parameter M set to 5, 3, and 2 in turn.

On an AMD opteron 2GHz processor, at k = 10 , the time taken to cluster our dataset was 1.98 sec, and the time taken to compute principal documents following it was 0.69 sec.
Our results are enumerated below. Each result is shown for k in [5 , 10 , 15 , 20 , 25 , 30] . M is varied in [  X  , 5 , 3 , 2] for each k . 1. Fig. 1 shows the actual number of principal documents re-2. Fig. 2 shows the precision of PrinDocs versus CTC.

Recall is immediately known once precision is, since the size of the Corp-A-TR dataset is known; therefore it is not reported sepa-rately.
We may draw the following observations from the results of  X  4 . 3 . The principal documents algorithm PrinDocs comprehen-sively outperforms the baseline CTC algorithm in identifying defini-tive documents in an enterprise corpus. 1. The improvement in precision demonstrated by PrinDocs 2. PrinDocs consistently outperforms CTC. In cases where both 3. As k is increased, the gap between the precisions produced
We formulate the problem of retrieving knowledge-rich docu-ments from enterprise technical corpora: a problem of high com-mercial impact. We compare two approaches to this problem: re-trieving documents that lie closest to the centroids of a clustering of the corpus, and retrieving documents that  X  X over" the concepts represented in the centroids. We find that the coverage-based ap-proach is significantly and consistently better than the closest to centroid approach. Distance based algorithms (such as CTC) may not give best results for the following reason. The space around a centroid in a cluster may be largely empty [4]. Thus, even though the centroid gives us a good concept decomposition of the cluster, it is not necessarily a good  X  X eometric representative" in the sense that it may lie in a largely empty area in the cluster. PrinDocs , on the other hand, does not use any geometric assumptions. It merely says  X  X oncept vectors provide good concept decompositions. Let us find documents only using these decompositions." clusters. M varied between [  X  , 5 , 3 , 2] .

Our results suggest that it might be advantageous to consider the coverage approach in other retrieval tasks, especially for the en-terprise domain, where  X  X ixed" corpora are the norm. PrinDocs adds an order or magnitude smaller time overhead on top of clus-tering, which makes it attractive for addition to existing clustering platforms. PrinDocs is currently being tested for deployment on a market leading enterprise unstructured information management platform from a Fortune-10 IT company.
 Acknowledgements. Some of the data in this paper was gathered using scripts written by Hernan Laffitte.
 [1] Z. Abbassi, V. S. Mirrokni, and M. Thakur. Diversity [2] W. Croft. A model of cluster searching based on [3] D. R. Cutting, J. O. Pedersen, D. Karger, and J. W. Tukey. [4] I. S. Dhillon and D. S. Modha. Concept decompositions for [5] Gartner. The top 10 strategic technology trends for 2012. [6] A. F. Gelbukh, M. Alexandrov, A. Bourek, and [7] M. A. Hearst and J. O. Pedersen. Reexamining the cluster [8] A. Leuski. Evaluating document clustering for interactive [9] D. R. Radev, H. Jing, M. Sty  X  s, and D. Tam. Centroid-based [10] M. Steinbach, G. Karypis, and V. Kumar. A comparison of [11] A. Tombros, R. Villa, and C. J. Van Rijsbergen. The [12] P. Willett. Recent trends in hierarchic document clustering: a
