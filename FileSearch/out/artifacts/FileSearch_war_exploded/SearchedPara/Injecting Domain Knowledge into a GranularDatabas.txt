
We discuss how to use techniques from such fields as text processing and knowledge management to better handle text attributes in the Infobright X  X  RDBMS engine. Our approach leads to a rich interface for domain experts who wish to share their knowledge about data content and, on the other hand, it remains unnoticeable to data users. It enables to improve data storage, data access, and data compression, with no changes required at the database schema level.
 H.2.4 [ Database Management ]: Systems Algorithms, Design Analytical Database Engines, Data Content Semantics, Compression, Infobright Community/Enterprise Editions
The amount of data that we need to operate on a daily basis is continually growing. For large data sets, the cost of storage, transfer, management (e.g. tuning, truncating, con-figuring) and query (e.g. computation, query formulation configuration) can be substantial. Our research is focused on how to address the above issues by solving the following variation of Physical Design Problem [3, 4]: Database Design Problem Given a query workload W,
While building Infobright X  X  database engine [6, 9], we have dealt with this problem by creating a system that:
In this study, we discuss how the knowledge about data content may be taken into account within our RDBMS solu-tion. By injecting such knowledge into the engine, we might influence data storage and processing. If we combined this with other aspects of domain knowledge  X  e.g., what the expected query workload will be  X  it can lead to a huge performance improvement. On the other hand, the way of injecting domain knowledge cannot make the whole system overcomplicated. It needs to be complementary  X  not con-tradictory  X  to the above Infobright X  X  features.

One needs to remember about an often unaddressed se-paration of concerns between data providers and data users. We term it as the Semantic Divide . Data providers have sig-nificant domain expertise that they usually want to share, although it is not easily translated into logical, application and physical models [5, 7]. We should provide them with interfaces at a different level than standard DBA tools. On the other hand, data users do not expect that anything in-jected into the system via appropriate interfaces will change models that they are working with on everyday basis. The needs of data providers and data users are equally important from the perspective of Database Design Problem and the overall framework for injecting domain knowledge.

The paper is organized as follows: Section 2 outlines the currently existing Infobright X  X  technology. Section 3 intro-duces further motivation for considering knowledge about data content. Section 4 shows the main steps of enhancing our software at the level of data storage and query process-ing. Section 5 includes some final remarks.
Our focus is to provide self-tuning analytical database en-gines that do not require advanced DBA expertise to tune or maintain. The impact of self-tuning processes is evident for large, growing data sets and analytic query workloads. Our software  X  available as Infobright Community Edition ( ICE , open source) and Infobright Enterprise Edition ( IEE , commercial)  X  is able to deal with such challenges. Ana-lytical query workload diversity is addressed by replacing standard indices with much smaller compact information annotating the groups of rows being created while loading data into ICE/IEE. We automatically create the granulated tables with rows  X  called rough rows  X  corresponding to the groups of original rows and attributes corresponding to various forms of compact information. One may look at ICE/IEE as a Granular Database Engine , where data oper-ations are designed based on two layers: 1. the granulated tables with rough rows and their rough 2. the underlying repository of data packs  X  collections of
When loading data from an external file to a previously declared table, the rows are decomposed onto their parti-cular attributes X  values. After gathering 2 16 of new rows, ICE/IEE automatically computes rough values and com-presses the blocks of 2 16 of values corresponding to each of the table X  X  attributes. Rough values and compressed blocks of values are stored on disk. When querying, all applicable rough values are put into memory. In case of, e.g., data fil-tering, rough values are applied to quickly exclude the blocks of values that do not satisfy a given SQL condition. Only not excluded blocks need to be decompressed and examined value by value. For further details, we refer to [8, 9].
The query speed in ICE/IEE depends on the ability of rough values to minimize and optimize access to the actual data stored on disk. One of our goals is to design types of rough values that allow us to minimize and optimize data access, while keeping the size of the rough values very small. In [6, 7], we introduced a framework for improving the qual-ity of rough values by more accurate, but still fast, grouping of rows into rough rows. However, data attributes are often too hard to be represented at the granulated level because of their complex content. Intuitively, the question is how to: Figure 2: URI structure and the corresponding lay-out of components of rough values and data packs.
By creating a framework that encodes the entire semantic structure of a given attribute and its values, we have the op-portunity to leverage automatic pattern matching at various stages of computation, e.g., compression, query plan formu-lation, and query plan execution. Such semantic knowledge may be also applicable during data preprocessing, integra-ting and modeling, but this usually makes the resulting data models too complicated for data users.

Our user and market analysis has strongly suggested that there is a need for interfaces which enable business savvy users to inject the domain knowledge directly into the data-base engine with no changes to data schemas. This way, data users are shielded from the complexity of semantic modeling, while reaping most of its benefits.

User interfaces should be designed for the purpose of do-main experts rather than database experts, as the knowledge about data content is usually tightly related to applications and often independent of general expertise in databases. Furthermore, from architectural point of view, there is a need of complete, consistent internal interface between the domain definitions and the algorithms of the core engine.
As an example, consider an alpha-numeric data attribute and assume that we know that some of its values may take the form of Uniform Resource Identifiers (URI). Some of the parts of URI (further called particles ) are as follows: Scheme Name consists of a sequence of characters begin-Hierarchical Part is intended to hold identification infor-Query is optional  X  it may contain additional identification Fragment provides additional information about secondary
Fig. 2 illustrates more detailed URI decomposition. 1 It also informally shows the idea of assembling each data pack
Full definition of the URI can be found in RFC 3986. from collections of its values X  particles that are, however, visible as a whole to the engine. Thus, from the engine X  X  perspective, there is still a single alpha-numeric attribute to be processed. Fig. 2 also shows that rough values may contain granules of information about values specified at dif-ferent levels [8, 10]. However, from the engine X  X  perspective, there is always a single rough value available for a single data pack, i.e., the engine shall be unaware of internal richness of rough values and shall be able to use the same internal interfaces as those currently implemented in ICE/IEE.
Surely, there is always a risk that the values of the same at-tribute will match different structures, the matching will be possible only partially or impossible completely. Indeed, one may imagine attributes such as, e.g., contact information with values taking the form of personal homepages, street addresses, or phone numbers. In Section 4, we introduce a more complete approach to such cases. We also proceed with another example of a multi-structure data attribute, as illustrated by Fig. 3 and Fig. 4.

In summary, domain knowledge is expected to help in defining rough values and compressing data. For example, collections of decomposed particles are far more likely to have low amounts of distinct values, which leads to their more efficient encoding and compression. The codes of par-ticles can be also used to build new components of rough values, e.g., statistics of the collections of values represented as concatenations of their particles X  codes.
Let us describe the general design considerations for cre-ating a domain aware database, and the needed data struc-tures for tying a given domain model to ICE/IEE. Note that a domain aware database does not imply a domain aware engine. As already suggested, we claim that the engine should work with data attributes as they are specified at the database schema level. This way, we can achieve neces-sary architectural scalability and stability. Let us break the problem into the following functional groupings: Domain Definition Defining interfaces that allow experts Data Load For a given data set, embedding the above se-Data Access Letting the engine operate on original values. Rough Data Access The engine should not be aware of
We chose the class of recursively enumerable languages , specifically regular expressions to define semantic rules for structures that can occur for attributes X  values. All such structure definitions are stored in the structure dictionary. One may say that it contains hierarchies of values corre-sponding to a given data attribute. We designed several prototypes with regards to how the structure dictionaries should be created and stored, with the assumption that their size is relatively small compared to the actual data. In our approach, all data packs that store collections of values of the same data attribute share the same structure dictionary. Internally, each data pack consists of the following: Match Table which encodes matches of whole values to Structure Sub-collections which contain values matched Outlier Sub-collection containing all values that were not
As an example, consider Table A that contains three at-tributes:  X  X imestamp X  of type  X  X IME X ,  X  X essionID X  of type  X  X nteger X , and  X  X aybeURL X  of type  X  X ARCHAR(700) X . As-sume that the structure dictionary of  X  X aybeURL X  contains the following structures:  X  X RL X  and  X  X nteger X . For each data pack, its match table may contain the following codes:
Fig. 3 illustrates the role of semantic matching when cre-ating match tables. The content of each of data packs is split onto sub-collections that can be managed by different compression algorithms and different procedures of compu-ting rough values, including those that are already in use in widely available versions of ICE/IEE [6, 11].

The semantic matcher applies regular expression(s) de-fined for an attribute to the input values, one at a time. As already noted, for values that match the regular expres-sion completely, the ordinal number of that expression in the structure dictionary is stored in match table. In in-stances where there are multiple possible expressions for an attribute, the semantic matcher may be more complex.
The semantic parser decomposes each of the previously derived structure sub-collections. Parsing, matching and en-coding is subsequently done when taking into account spe-cific definitions of expressions in the dictionary. For exam-ple, if we adapt Fig. 2 for URL values, then their sub-collections will take an internally decomposed form based on scheme, hierarchical part, query and fragment, or  X  if we go deeper in the hierarchy  X  path, authority, et cetera.
Whenever original values are needed by the engine, they can be reconstructed by employing definitions of particu-lar expressions together with decompression algorithms and match tables of the requested data packs. The engine may also request internal codes of the values instead of their ori-ginal form but in this case the overall framework remains the same. Rough values and structure definitions may be helpful in generating possibly most efficient codes that the engine may use instead of original values, which is an extension of the techniques already applied in ICE/IEE.

Similarly, we can generalize the way of building and using rough values. Besides information related to sub-collections of values corresponding to particular expressions, gathering basic statistics for each of expressions X  particles or more ad-vanced granules of information as outlined in the end of Sec-tion 3, rough values can also contain statistics of occurrence of expressions in particular data packs.

Fig. 4 displays how the engine should interact with rough values using the following query example:
The framework in Fig. 4 is quite analogous to the ICE/IEE foundations referred in Section 2. A difference is that con-ditions such as the one specified for  X  X aybeURL X  need to be additionally parsed using the same procedures as those ap-plied to the semantically complex attributes X  values on data load. During parsing we identify which granules of rough values are useful to resolve the query with a limited need of decompressing the corresponding data packs.

In case of the above query  X  which is of course a very sim-plified situation  X  the  X  X aybeURL X -based condition clearly focuses on scheme particles of values matching URL struc-ture. Additionally, we need to remember about outliers that can begin with  X  X ttp:// X . Thus, the most useful components of rough values are those related to the occurrence of URL values and outliers in particular data packs, as well as spe-cific statistics of scheme particles of URL values.
In the above study, the major point is that decisions about usefulness of particular granules contained in generally un-derstood rough values are made independently from the en-gine, in order to keep uniform internal interfaces between specific modules of the ICE/IEE architecture.
Allowing the users of sufficient expertise an interface to describe the content of their data provides substantial op-portunities for increasing both the performance and the sto-rage capabilities of general database systems. The challenge is to create an interface that is rich enough to provide a reasonable approximation of data semantics, while still not becoming onerous to either develop or maintain.

In this paper, we showed how the above idea can be put together with a specific case of Infobright X  X  software [6, 9, 8, 11]. We discussed how semantic richness of data can be addressed at the level of data storage and representation, transparently to the query engine (which is important for internal system X  X  architecture) and to data users who can work with unchanged schemas (which is important in view of Database Design Problem outlined in Section 1).

Our research is still incomplete, e.g., with respect to hu-man-computer interaction while injecting domain knowledge into the RDBMS system or categorization of the types of semantically-driven statistics that can represent data. On the other hand, we can already prove usefulness of the pro-posed methodology at a simplified prototype level. [1] D. J. Abadi, D. S. Myers, D. J. DeWitt, and S. [2] A. Ailamaki, D. J. DeWitt, and M. D. Hill. Data [3] N. Bruno and S. Chaudhuri. To Tune or not to [4] J. M. Hellerstein, M. Stonebraker, and J. R. Hamil-[5] L. T. Moss and S. Atre. Business Intelligence Road-[6] D.  X  Sl , ezak and V. Eastwood. Data Warehouse [7] D.  X  Sl , ezak and M. Kowalski. Intelligent Data Granu-[8] D.  X  Sl , ezak, P. Synak, J. Wr  X oblewski, and G. Toppin. [9] D.  X  Sl , ezak, J. Wr  X oblewski, V. Eastwood, and P. Synak. [10] J. Stepaniuk: Rough-Granular Computing in Know-[11] M. Wojnarski, C. Apanowicz, V. Eastwood, D.  X  Sl , [12] M. Zukowski, S. Heman, N. Nes, and P. Boncz.

