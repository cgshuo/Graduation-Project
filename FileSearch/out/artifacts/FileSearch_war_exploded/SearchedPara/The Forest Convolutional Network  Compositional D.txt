 For many natural language processing tasks we need to compute meaning representations for sen-tences from meaning representations of words. In a recent line of research on  X  X ecursive neural net-works X  (e.g., Socher et al. (2010)), both the word and sentence representations are vectors, and the word vectors ( X  X mbeddings X ) are borrowed from work in distributional semantics or neural lan-guage modelling. Sentence representations, in this approach, are computed by recursively applying a neural network that combines two vectors into one (typically according to the syntactic structure pro-vided by an external parser). The network, which thus implements a  X  X omposition function X , is opti-mized for delivering sentence representations that support a given semantic task: sentiment analysis (Irsoy and Cardie, 2014; Le and Zuidema, 2015), paraphrase detection (Socher et al., 2011), seman-tic relatedness (Tai et al., 2015) etc. Studies with recursive neural networks have yielded promising results on a variety of such tasks.

In this paper, we represent a new recursive neu-ral network architecture that fits squarely in this tradition, but aims to solve a number of difficulties that have arisen in existing work. In particular, the model we propose addresses three issues: 1. how to make the composition functions adap-2. how to deal with different branching factors 3. how to deal with uncertainty about the correct Figure 1: Recursive Neural Network. For simplic-ity, bias vectors are removed.

To solve these challenges we take inspiration from two other traditions: the convolutional neu-ral networks and classic parsing algorithms based on dynamic programming. Including convolution in our network provides a direct solution for is-sue (2), and turns out, somewhat unexpectedly, to also provide a solution for issue (1). Introduc-ing the chart representation from classic parsing into our architecture then allows us to tackle issue (3). The resulting model, the Forest Convolutional Network, outperforms all other models on a senti-ment analysis and question classification task. This section is to introduce the recursive neural network (RNN) and convolutional neural network (CNN) models, on which our work is based. 2.1 Recursive Neural Network A recursive neural network (RNN) (Goller and K  X  uchler, 1996) is a feed-forward neural network where, given a tree structure, we recursively ap-ply the same weight matrices at each inner node in a bottom-up manner. In order to see how an RNN works, consider the following exam-ple. Assume that there is a constituent with parse tree ( S I ( V P like it )) (Figure 1), and that x tions of the three words I , like and it , respec-tively. We use a neural network which consists of compute the vector for a parent node in a bottom up manner. Thus, we compute x V P where b is a bias vector and f is an (non-linear) activation function. Having computed x V P , we Figure 2: Convolutional Neural Network (one convolutional layer and one fully connected layer) with a window-size-3 kernel. can then move one level up in the hierarchy and compute x S This process is continued until we reach the root node.

For classification tasks, we put a softmax layer on the top of the root node, and compute the prob-ability of assigning a class c to an input x by
Pr ( c | x ) = softmax ( c ) = where u ( c 1 , y top ) ,...,u ( c | C | , y top ) T = W u y top + b u ; C is the set of all possible matrix and a bias vector.

Training an RNN uses the gradient descent method to minimize an objective function J (  X  ) . The gradient  X  X / X  X  is efficiently computed thanks to the back-propagation through structure algo-rithm (Goller and K  X  uchler, 1996).

Departing from the original RNN model, many extensions have been proposed to enhance its compositionality (Socher et al., 2013; Irsoy and Cardie, 2014; Le and Zuidema, 2015) and appli-cability (Le and Zuidema, 2014b). The model we are going to propose can be considered as an ex-tension of RNN with an ability to solve the three issues introduced in Section 1. 2.2 Convolutional Neural Network A convolutional neural network (CNN) (LeCun et al., 1998) is also a feed-forward neural network; it consists of one or more convolutional layers (of-ten with a pooling operation) followed by one or more fully connected layers. This architecture was invented for computer vision. It then has been widely applied to solve natural language process-ing tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).

To illustrate how a CNN works, the following example uses a simplified model proposed by Col-lobert et al. (2011) which consists of one con-volutional layer with the max pooling operation, followed by one fully connected layer (Figure 2). This CNN uses a kernel with window size 3; when we slide this kernel along the sentence  X   X  s  X  I like it very much  X  /s  X   X , we get five vectors: u u ... u ces, b c  X  R m is a bias vector. The max pooling operation is then applied to those resulted vectors in an element-wise manner: Finally, a fully connected layer is employed where W , b are a real weight matrix and bias vec-tor, respectively; f is an activation function.
Intuitively, a window-size-k kernel extracts (lo-cal) features from k -grams, and is thus able to cap-ture k -gram composition. The max pooling oper-ation is for reducing dimension, forcing the net-work to discriminate important features from oth-ers by assigning high values to them. For instance, if the network is used for sentiment analysis, local features corresponding to k -grams containing the word  X  X ike X  should receive high values in order to be propagated to the top layer. We now first propose a solution to the issues (1) and (2) (i.e., making the composition functions adaptive and dealing with different branching fac-tors), called Recursive convolutional neural net-work (RCNN), and then a solution to the third is-sue (i.e., dealing with uncertainty about the cor-rect parse), called Chart Neural Network (ChNN). A combination of them, Forest Convolutional Net-work (FCN), will be introduced lastly.
 Figure 3: Recursive Convolutional Neural Net-work with a nonlinear window-size-3 kernel. 3.1 Recursive Convolutional Neural Given a subtree p  X  x 1 ... x l , an RCNN (Fig-ure 3), like a CNN, slides a window-size-k kernel along the sequence of children ( x 1 ,...,x l ) to com-pute a pool of vectors. The max pooling operation followed by a fully connected layer is then applied to this pool to compute a vector for the parent p .
This RCNN differs from the CNN introduced in Section 2.2 at two points. First, we use a non-linear kernel: after linearly transforming in-put vectors, an activation function is applied. Sec-ond, we put k  X  1 padding tokens &lt; b &gt; at the be-ginning of the children sequence and k  X  1 padding tokens &lt; e &gt; at the end. This thus guarantees that all the children contribute equally to the resulted vector pool, which now has l + k  X  1 vectors.
It is obvious that this RCNN can solve the sec-ond issue (i.e., dealing with different branching factors), we now show how it can make the com-position functions adaptive. We first see what hap-pens if the window size k is larger than the number of children l , for instance k = 3 and l = 2 . There are four vectors in the pool u (1) = f ( W u (2) = f ( W u (3) = f ( W u (4) = f ( W where W 1 , W 2 , W 3 are weight matrices, b c is a bias vector, f is an activation function. These four resulted vectors correspond to four ways of com-posing the two children: (1) the first child stands alone (e.g., when the in-(2,3) the two children are composed with two dif-(4) the second child stands alone.
 Now, imagine that we must handle binary syn-tactic rules with different head positions such as S  X  NP V P (e.g.  X  X ones runs X ) where the second child is the head and V P  X  V BD NP (e.g.,  X  X te spaghetti X ) where the first child is the head. We can set those weight matrices such that when multiplying W 2 by the vector of a head, we have a vector with high-value entries. And when multiplying W 2 by the vector of a non-head, or when multiplying W 1 or W 3 by a vector, the re-sulted vector has low-value entries. This is possi-ble thanks to the max pooling operation and that heads are often more informative than non-heads.
If the window size k is smaller than the number of children l , the argument above is still valid in some cases such as head position. However, there is no longer a direct interaction between any two children whose distance is larger than k . 3 In prac-tice, this problem is not serious because rules with a large number of children are very rare. 3.2 Chart Neural Network Unseen sentences are always parsed by an auto-matic parser, which is far from perfect and task-independent. Therefore, a good solution is to give the system a set of parses and let it decide which parse is the best or to combine some of them. The RNN model handles one extreme where this set contains only one parse. We now consider the other extreme where the set contains all possible parses. Because the number of all possible bi-nary parse trees of a length-n sentence is the n -th Catalan number, processing individual parses is not practical. We thus propose a new model work-ing on charts in the CKY style (Younger, 1967), called Chart Neural Network (ChNN).

We describe this model by the following exam-ple. Given a phrase  X  X te pho with Milos X , a ChNN will process its parse chart as in Figure 4. Be-cause any 2-word constituent has only one parse, the computation for p 1 ,p 2 ,p 3 is identical to Equa-tion 1. For 3-word constituent p 4 , because there are two possible productions p 4  X  ate p 2 and p 4  X  p 1 with , we compute one vector for each production and then apply the max pooling operation to these two vectors to compute p 4 . We do the same to compute p 5 . Finally, at the top, there are three productions p 6  X  ate p 5 , p 6  X  p 1 p 3 and p 6  X  p 4 Milos . Similarly, we compute one vec-tor for each production and employ the max pool-ing operation to compute p 6 .
 Because this ChNN processes a chart like the CKY algorithm, its time complexity is O ( n 2 d 2 + n 3 d ) where n and d are the sentence length and is thus notably more complex than an RNN, whose complexity is O ( nd 2 ) . Like chart parsing, the complexity can be reduced significantly by prun-ing the chart before applying the ChNN. This will be discussed right below. 3.3 Forest Convolutional Network We now introduce the Forest Convolutional Net-work (FCN) model, which is a combination of the RCNN and the ChNN. The idea is to use an au-ductions (if applicable), and then apply a ChNN where the computation in Equation 3 is replaced by a convolutional layer followed by the max pool-ing operation and a fully connected layer as in the RCNN.
 Figure 5 shows an illustration how the FCN works on the phrase  X  X te pho with Milos X . A forest of parses, given by an external parser, comprises two parses ( V P ate pho ( PP with Milos )) (solid lines) and ( V P ate ( NP pho ( PP with Milos ))) (dash-dotted lines). The first parse is the preferred reading if Milos is a person, but the second one is a possible reading (for instance, if Milos is the name of a sauce). Instead of forcing the external parser to decide which one is correct, we let the FCN network do that because it has more information about the context and domain, which are embedded in training data. What the network should do is depicted in Figure 5-right.
 Training Training an FCN is similar to train-ing an RNN. We use the mini-batch gradient de-scent method to minimize an objective function J , which depends on which task this network is ap-plied to. For instance, if the task is sentiment anal-ysis, J is the cross-entropy over the training sen-tence set D plus an L2-norm regularization term
J (  X  ) =  X  where  X  is the parameter set, c p is the sentiment class of phrase p , p is the vector representation at the node covering p , Pr ( c p | p ) is computed by the softmax function, and  X  is the regularization parameter.

The gradient  X  X / X  X  is computed efficiently thanks to the back-propagation through structure (Goller and K  X  uchler, 1996). We use the AdaGrad method (Duchi et al., 2011) to automatically up-date the learning rate for each parameter. We evaluate the FCN model with two tasks: ques-tion classification and sentiment analysis. The evaluation metric is the classification accuracy. Our networks were initialized with the 300-D GloVe word embeddings trained on a corpus of tial values for a weight matrix were uniformly sampled from the symmetric interval  X  1  X  where n is the number of total input units. In each experiment, a development set was used to tune the model. We run the model ten times and chose the run with the highest performance on the devel-opment set. We employed early stopping: training is halted if performance on the development set does not improve after three consecutive epochs. 4.1 Sentiment Analysis et al., 2013) which consists of 5-way fine-grained sentiment labels (very negative, negative, neu-tral, positive, very positive) for 215,154 phrases of 11,855 sentences. We used the standard split-ting: 8544 sentences for training, 1101 for devel-opment, and 2210 for testing. The average sen-tence length is 19.1. In addition, the treebank Table 1: Accuracies at sentence level on the SST dataset. FCN (dep.) and FCN (const.) denote the FCN with dependency forests and with constituent forests, respectively. The accuracies of RNTN, CNN, DCNN, PV, DRNN, LSTM-RNN and CT-LSTM are copied from the corresponding papers (see text). also supports binary sentiment (positive, negative) classification by removing neutral labels, leading to: 6920 sentences for training, 872 for develop-ment, and 1821 for testing.

All sentences were parsed by Liang Huang X  X  de-used this parser because it generates parse forests and that dependency trees are less deep than con-stituent trees. In addition, because the SST was annotated in a constituency manner, we also em-ployed the Charniak X  X  constituent parser (Char-niak and Johnson, 2005) with Huang (2008) X  X  for-est pruner. We found that the beam width 16 for the dependency parser and the log probability beam 10 for the other worked best. Lower values harmed the system X  X  performance and higher val-ues were not beneficial.

Our FCN has the dimension of vectors at inner nodes 200, a window size for the convolutional kernel of 7, and the activation function tanh . It was trained with the learning rate 0.01, the regu-5. To reduce the average depth of the network, the fully connected layer following the convolutional layer was removed (i.e., p = x , see Figure 3).
We compare the FCN against other models: the Recursive neural tensor network (RNTN) (Socher et al., 2013), the Convolutional neural net-work (CNN) (Kim, 2014), the Dynamic convolu-tional neural network (DCNN) (Kalchbrenner et al., 2014), the Paragraph vectors (PV) (Le and Mikolov, 2014), the Deep recursive neural net-work (DRNN) (Irsoy and Cardie, 2014), the Re-cursive neural network with Long short term mem-ory (LSTM-RNN) (Le and Zuidema, 2015) and the Constituent Tree LSTM (CT-LSTM) (Tai et
Table 1 shows the results. Our FCN using con-stituent forests achieved the highest accuracies in both fine-grained task and binary task, 51% and 89.1%. Comparing to CT-LSTM, although there is no difference in the fine-grained task, the dif-ference in the binary task is significant (1.1%). Comparing to LSTM-RNN, the differences in both tasks are all remarkable (1.1% and 1.1%).

Constituent parsing is clearly more helpful than dependency parsing: the improvements that the FCN got are 0.6% in the fine-grained task and 0.9% in the binary task. We conjecture that, be-cause sentences in the treebank were parsed by a constituent parser (here is the Stanford parser), training with constituent forests is easier. 4.2 Question Classification (Li and Roth, 2002) which contains 5952 ques-tions (5452 questions for training and 500 ques-tions for testing). The task is to assign a ques-tion to one in six types: ABBREVIATION, EN-TITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC. The average length of the questions in the training set is 10.2 whereas in the test set is 7.5. This difference is due to the fact that those questions are from different sources. All questions were parsed by Liang Huang X  X  dependency parser with the beam width 16.

We randomly picked 5% of the training set (272 questions) for validation. Our FCN has the dimen-sion of vectors at inner nodes 200, a window size for the convolutional kernel of 5, and the activa-tion function tanh . It was trained with the learn-and the mini batch size 1. The vectors represent-to 0 .

We compare the FCN against the Convolutional neural network (CNN) (Kim, 2014), the Dynamic convolutional neural network (DCNN) (Kalch-Table 2: Accuracies on the TREC question type classification dataset. The accuracies of DCNN, MaxEnt H , CNN-non-static, and SVM S are copied from the corresponding papers (see text). brenner et al., 2014), MaxEnt H (Huang et al., 2008) (which uses MaxEnt with uni-bi-trigrams, POS, wh-word, head word, word shape, parser, hypernyms, WordNet) and the SVM S (Silva et al., 2011) (which uses SVM with, in addition to fea-tures used by MaxEnt H , 60 hand-coded rules). We also include the LSTM-RNN (Le and Zuidema, 2015) whose accuracy was computed by running This network was also initialized by the 300-D GloVe word embeddings.
 the second best accuracy, only lightly lower than SVM S (0.2%). This is a promising result because our network used only parse forests, unsupervis-edly pre-trained word embeddings whereas SVM S used heavily engineered resources. The differ-ence between FCN and the third best is remark-able (1.2%). Interestingly, LSTM-RNN did not perform well on this dataset. This is likely be-cause the questions are short and the parse trees quite shallow, such that the two problems that the LSTM was invented for (long range dependency and vanishing gradient) do not play much of a role. 4.3 Visualization We visualize the charts we obtained in the sen-timent analysis task as in Figure 6. To identify how important each cell is for determining the fi-nal vector at the root, we compute the number of features of each that are actually propagated all the way to the root in the successive max pooling op-erations. The circles in a graph are proportional to this number. Here, to make the contribution of each individual cell clearer, we have set the win-dow size to 1 to avoid direct interactions between cells.

At the lexical level, we can see that the FCN can discriminate important words from the others. Two words  X  X ost X  and  X  X ncoherent X  are the key of the sentiment of this sentence: if one of them is replaced by another word (e.g. replacing  X  X ost X  by  X  X ew X  or  X  X ncoherent X  by  X  X oherent X ), the sen-timent will flip. The punctuation  X . X  however also has a high contribution to the root. This happens to other charts as well. We conjecture that the net-work uses the vector of  X . X  to store neutral features and propagate them to the root whenever it can not find more useful features in other vectors. Our fu-ture work is to examine this.

At the phrasal level, the network tends to group words in grammatical constituents, such as  X  X ost of the action setups X ,  X  X re incoherent X . Ill-formed constituents such as  X  X f the action X  and  X  X ncoher-ent . X  receive little attention from the network.
Interestingly, we can see that the circle of  X  X nco-herent X  is larger than the circles of any inner cells, suggesting that the network is able to make use of parses containing direct links from that word to the root. This is evidence that the network has an ability of selecting (or combining) parses that are beneficial to this sentiment analysis task. The idea that a composition function must be able to change its behaviour on the fly according to in-put vectors is explored by Socher et al. (2013), Le and Zuidema (2015), among others. The tensor in the former is multiplied with the vector repre-sentations of the phrases it is going to combine to define a composition function (a matrix) on the fly, and then multiplies again with these vectors to yield a compound representation. In the LSTM architecture of the latter, there is one input gate for each child in order to control how the vector of the child affects the composition at the parent node. Because the input gate is a function of the vector of the child, the composition function has an infinite number of behaviours. In this paper, we instead slide a kernel function along the sequence of children to generate different ways of composi-tion. Although the number of behaviours is limited (and depends on the window size), it simultane-ously provides us with a solution to handle rules with different branching sizes.

Some approaches try to overcome the prob-lem of varying branching sizes. Le and Zuidema (2014b) use different sets of weight matrices for different branching sizes, thus requiring a large number of parameters. Because large branching-size rules are rare, many parameters are infre-quently updated during training. Socher et al. (2014), for dependency trees, use a weight matrix for each relative position to the head word (e.g., first-left, second-right). Le and Zuidema (2014a) replace relative positions by dependency relations (e.g., OBJ, SUBJ). These approaches strongly de-pend on input parse trees and are very sensitive to parsing errors. The approach presented in this pa-per, on the other hand, does not need the informa-tion about the head word position and is less sensi-tive to parsing errors. Moreover, its number of pa-rameters is independent from the maximal branch-ing size.

Convolutional networks have been widely ap-plied to solve natural language processing tasks. Collobert et al. (2011), Kalchbrenner et al. (2014), and Kim (2014) use convolutional networks to deal with varying length sequences. Recently, Zhu et al. (2015) and Ma et al. (2015) try to intergrate syntactic information by employing parse trees. Ma et al. (2015) extend the work of Kim (2014) by taking into acount dependency relations so that long range dependencies could be captured. The model proposed by Zhu et al. (2015), which is very similar to our Recursive convolutional neural network model, is to use a convolutional network for the composition purpose. Our work, although also employing a convolutional network and syn-tactic information, goes beyond them: we address the issue of how to deal with uncertainty about the correct parse inside the neural architecture. There-fore, instead of using a single parse, our proposed FCN model takes as input a forest of parses.
Related to our FCN is the Gated recursive con-volutional neural network model proposed by Cho et al. (2014) which is stacking n  X  1 convolutional neural layers using a window-size-2 gated kernel (where n is the sentence length). Mapping their network into a chart, each cell is only connected to the two cells right below it. What makes this network special is the gated kernel which is a 3-gate switcher for choosing one of three options: directly transmit the left/right child X  X  vector to the parent node, or compose the vectors of the two children. Thanks to this, the network can capture any binary parse trees by setting those gates prop-erly. However, because only one gate is allowed to open in a cell, the network is not able to capture an arbitrary forest. Our FCN is thus more expressive and flexible than their model. We proposed the Forest Convolutional Network (FCN) model that addresses the three issues: (1) how to make the composition functions adaptive, (2) how to deal with different branching factors of nodes in the relevant syntactic trees, (3) how to deal with uncertainty about the correct parse in-side the neural architecture. The key principle is to carry out many different ways of computation and then choose or combine some of them. For more details, the two first issues are solved by em-ploying a convolutional net for composition. To the third issue, the network takes input as a forest of parses instead of a single parse as in traditional approaches.

Our future work is to focus on how to choose/combine different ways of computation. For instance, we might replace the max pooling by different pooling operations such as mean pool-ing, k-max pooling (Kalchbrenner et al., 2014), and stochastic pooling (Zeiler and Fergus, 2013). We can even bias the selection/combination to-ward grammatical constituents by weighing cells by their inside probabilities.
 We thank our three anonymous reviewers for their comments. This work was funded by the Faculty of Humanities of the University of Amsterdam, through a Digital Humanities fellowship to PL and a position in the New Generation Initiative (NGO) for WZ.

