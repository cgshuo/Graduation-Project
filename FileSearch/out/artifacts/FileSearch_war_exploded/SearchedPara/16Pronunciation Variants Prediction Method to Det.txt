 JEESOO BANG, JONGHOON LEE and GARY GEUNBAE LEE ,PohangUniversity Computer-assisted pronunciation training (CAPT) applications can provide feedback to foreign language learners, who thereby gain an educational opportunity to practice their pronunciation at relatively low cost. Most CAPT applications ask a learner to perform some pronunciation tasks, then return corrective feedback for each response [Neri et al. 2008; Tsubota et al. 2004]. Many CAPT systems provide feedback with phoneme-level detail. Such feedback is effective in diagnosing mispronunciations for the learners, who can improve their oral proficiency by recognizing and correcting the mispronunciations. Therefore, detecting mispronunciations is the core function of a CAPT system.

Automatic speech recognition (ASR) module supports the detection of mispronun-ciation and generation of feedback; a CAPT system usually includes such a unit. Pronunciation is usually diagnosed at the phoneme level; to localize mispronunciation detection at this level, the ASR module uses phoneme recognition or forced-alignment to provide information on phone segmentation and the mapping between the phone segment and the best-matching phoneme.

The ASR module provides the CAPT system with information about (1) the phoneme sequence that best matches the given speech and model, and (2) the probability or con-fidence that this sequence is correct. The sequence may include boundary and dura-tion information on each phoneme as well as phoneme identities. The probability and confidence represent how well a phone segment matches the model pronunciation of the corresponding phoneme in the sequence. ASR-based CAPT systems use at least one of these types of information to analyze input speech and to detect and assess mispronunciations.
 Many studies [Harrison et al. 2008, 2009; Herron et al. 1999; Menzel et al. 2000; Tsubota et al. 2004] approached the mispronunciation detection problem in English by using the recognized phoneme sequence information directly. The direct approach com-pares phoneme recognition results with orthographic phoneme-level transcriptions and indicates the differences as mispronunciations. This approach often requires a pronunciation variation prediction method that can help the ASR module recognize the actual pronunciation correctly. The prediction method presents a well-trimmed decoding space to the ASR module for phoneme recognition. The set of pronunciation variants ( X  X ecoding space X ) bounds the ASR decoding and mispronunciation detection. This bounding may help ASR to detect mispronunciations efficiently, but it misses mis-pronunciations that do not occur in the space. For this approach to be effective, a well-designed method of predicting pronunciation variants is essential; it can be obtained by carefully analyzing related knowledge and data. Because direct approaches re-quire precise prediction methods to limit the ASR decoding space appropriately, these methods require knowledge or data to predict the target learners X  pronunciation.
Another group of studies [Strik et al. 2009; Witt and Young 1998] used scoring ap-proaches based on probability information rather than using the recognized phoneme sequence. The scoring approach to evaluating each phone is based on ASR scores in-cluding likelihood, posterior probability, or other confidence measures. The goodness of pronunciation (GOP) algorithm [Witt and Young 2000] is a well-known scoring ap-proach. It uses the duration-normalized log-likelihood ratio between forced and unlim-ited phoneme recognition to calculate a score for each phone. This approach does not require language-specific knowledge and has no limitation on detecting mispronun-ciations, and can therefore be applied to any language with little or no modification. However, to make the best decision about the mispronunciation of a phone, decision thresholds must be optimized.

The two approaches are not mutually exclusive and are often combined. Combin-ing the approaches can provide good synergy and improve detection. Witt and Young [2000] refined their original GOP score by considering systematic error, which is pre-dictable given knowledge about the types of pronunciation committed by nonnative speakers. Another study [Ito et al. 2005] combined the two approaches in a differ-ent manner, by first using predefined rules to generate mispronunciation candidates, then using the score calculation process to determine whether the candidates are mispronounced.

As in the previous approaches, our method combines the prediction and scor-ing approaches. However, our method is more advanced for two reasons. 1) We newly developed an effective pronunciation variants prediction method using gen-eralized transformation-based error-driven learning (GTBL). The set of predicted pronunciation variants can be used to generate an extended recognition network (ERN) [Harrison et al. 2009] which can provide a realistic decoding space for Korean learners. 2) We use modified GOP scores to improve mispronunciation detec-tion in realistic decoding space; these scores can be effectively integrated into our lo-gistic regression machine learning method that considers mispronunciation preference features.

The rest of this article is organized as follows. Section 2 introduces our data and some design points derived while analyzing the data for prediction of pronunciation variants and methods of detecting mispronunciations. Section 3 describes our GTBL-based pronunciation variants prediction method and section 4 explains our modi-fied GOP-based mispronunciation detection approach for Korean learners. Section 5 presents the experimental setup, and section 6 discusses the results of the experi-ments. Section 7 concludes this work. We collected English-read speech data spoken by Korean adult speakers with high oral proficiency levels. We then extracted knowledge about pronunciation variation in the phonemes to adapt the canonical English pronunciation dictionary to the Korean nonnative phoneme variants. We distributed 6,600 text segments of sentences, words, and phrases to 170 Korean people learning English. The phoneme distribution of the given text was not equally balanced. However, the texts for the data were collected by the phonetic experts, and the data is phonetically rich. We collected 12,000 speech segments; each person recorded an average of 74 text segments, and two different people recorded each text segment.

Two annotators trained 1 in phonetics and phonology annotated phoneme-level tran-scriptions of the collected data. The annotators were provided the automatic phoneme-level transcription corresponding to the text segment read. The information was extracted using HTK 3.4 [Young et al. 2006] by forced alignment, with an acoustic model developed using a Wall Street Journal (WSJ) [Marcus et al. 1993] and TIMIT corpora. The annotators revised the transcription, listening repeatedly to the record-ing, if necessary. The phoneme-level transcription is represented in ARPAbet The annotators strongly agreed with each other. Their phoneme level agreement had a Fleiss X  kappa value of 0.8685, and 86.90% agreement in 9,327 phones from 498 sen-tences [Ryu et al. 2011]. The mispronunciation labels were generated in postprocess-ing, because the annotators transcribed the actual phoneme sequences of each speech segment rather than marking mispronunciations. 2.2.1. Canonical Pronunciation Annotation. On the basis of the phoneme-level transcrip-tion of an actual pronunciation, we applied an automatic procedure to annotate a canonical pronunciation. A canonical pronunciation is a reference pronunciation that is in the pronouncing dictionary. 3 From the phoneme-level transcription and word in-stances in the text segment that corresponded to the transcription, we automatically annotated a canonical pronunciation by taking the closest pronunciation to the actual one from the pronunciations of each word instance in the canonical pronunciation dic-tionary. We aligned the phoneme-level transcriptions of each word instance by using the Levenshtein distance algorithm [Levenshtein 1966], with a minor modification to the substitution cost calculation. 2.2.2. Grapheme-Phoneme Alignment. As the last data preparation step, we aligned graphemes to the corresponding phonemes of the canonical phoneme-level transcrip-tion. The proposed method uses grapheme-phoneme alignment to predict pronunci-ation errors. This procedure predicts the types of pronunciation errors that occur in given grapheme-canonical phoneme factors. Some actual pronunciations are derived from graphemes not from the canonical pronunciations, because Korean people tend to pronounce English words with Korean alphabet reading rules. Many Koreans pro-nounce words similarly, especially when reading a text.

The Levenshtein distance algorithm, which aligned the canonical and actual tran-scriptions, is inappropriate for the grapheme-phoneme alignment task because the English grapheme-phoneme alignment gives n -to-m mappings. To obtain n -to-m map-pings between grapheme and phoneme sequences, we adopted the phrase alignment technique of statistical phrase-based machine translation [Koehn et al. 2003]. The alignment process involves the following steps.  X  1st step. Generate two IBM Model 4 [Brown et al. 1993] alignments using GIZA [Och and Ney 2003] software in opposing directions: phoneme-to-grapheme and grapheme-to-phoneme. IBM Model 4 is a statistical translation model that treats the translation process as a noisy-channel model. IBM Model 4 models the trans-lation process as a generative process of how a sequence of target words is gener-ated from a sequence of source words. GIZA ++ was developed to train word-based translation models from parallel corpora and is a part of the statistical machine translation toolkit used to train IBM Models.  X  2nd step. Apply the grow-diag-final symmetrization algorithm 2007; Och and Ney 2003] to generate an n -to-m alignment from the phoneme-to-grapheme and grapheme-to-phoneme alignments. The IBM Model 4 alignment should be applied in both directions because the model first generates only a 1-to-n alignment then combines the alignments to derive an n -to-m alignment. We addi-tionally grouped phoneme sequences if the resulting alignment reordered the origi-nal sequence.  X  3rd step. Apply postprocessing rules that attach un-aligned phonemes and graphemes to the prealigned adjacent phonemes and graphemes.

The alignment procedures pinned the phonemes of an actual transcription ( X  X ctual phonemes X ) and graphemes to the corresponding phonemes of a canonical pronunci-ation ( X  X anonical phonemes X ) and formed sequences of three-tiered tokens (Figure 1). The first tier is graphemes, the second tier is canonical phonemes, and the third tier is actual phonemes. After these procedures, most tokens formed n -1-1 structures, where each token contains zero or more graphemes, one canonical phoneme, and one actual phoneme (Figure 1(c), (e)). The grapheme-phoneme alignment sometimes resulted in an n -to m alignment and formed an n  X  m  X  m structured token. We replaced the n  X  m  X  m tokens with mn -1-1 tokens produced by repeating the grapheme component m times (Figure 1(b)). The n -1-1 token representation gives a clearer mapping between the canonical and actual phonemes compared with the original representation. We con-catenated the actual phonemes in the alignment and produced new phoneme repre-sentations if several actual phonemes were aligned to the same canonical phonemes (Figure 1(d)). The grapheme-phoneme alignment produced some erroneous results with no available relevant alignments, for instance, abbreviations; however, the canonical-actual phoneme alignment is still relevant regardless of the grapheme-phoneme alignment (Figure 1(a)). Mispronunciations (Figure 2) made by Korean speakers of English are concentrated in some specific phoneme classes and contexts, and are therefore predictable. Over 90% of the mispronunciations occur in one-third of the phoneme classes (Figure 2(a)). Most /ah/ mispronunciations are related to vowel nonreduction and are thus predictable from the corresponding graphemes. Because when a vowel in English is reduced, it is mostly reduced to /ah/, but Korean people are not familiar with vowel-reduction phenomenon, so they mispronounce /ah/ with different phoneme predictable from the corresponding grapheme. Over 95% of the /ih/ mispronunciations are replaced with one of three phoneme classes: /ah/, /iy/, or /eh/. Consonant mispronunciation may be predictable from the phoneme context, as with most /t/, /d/, and /z/ mispronunciations observed at the word final position. The statistics demonstrate that most mispronun-ciations are captured in a small number of predicted pronunciation variations, so we chose mispronunciations from the predicted variations. However, we need a supple-mentary method to weight each predicted pronunciation because the mispronuncia-tion expectations for each phoneme class have different error rates (Figure 2(b)). We thus designed a mispronunciation detection method to apply a scoring algorithm to a relatively small pronunciation space predicted by a pronunciation variants prediction algorithm. We inspected the data and designed a learning method to predict the observed pronun-ciation variations for Korean speakers of English. After postprocessing, the collected data contained 207k phoneme instances including approximately 21k differences be-tween canonical and actual phonemes. The pronunciation variations observed from the data are mostly phonemic phenomena, which either phoneme level rewrite rules or classification models can describe. The variations show that the factor that most af-fects the actual phone pronunciation is the canonical phoneme. Most tokens have the same actual and canonical phonemes, and many variations are bounded by certain phoneme groups with similar sounds as the canonical phoneme. Therefore, to de-sign the prediction method we began with the initial decision based on the canoni-cal phoneme, then revised this decision after considering additional factors, including phoneme context.

Although most actual pronunciations are derived from the canonical pronunciation, others are derived from graphemes. The canonical phoneme sequence of the word busi-ness is /b/ /ih/ /z/ /n/ /ah/ /s/, and we observed an actual phoneme sequence /b/ /ih/ /z/ /ih/ /n/ /ih/ /s/. Inserting the vowel /ih/ between /z/ and /n/ is possibly a realization of the grapheme  X  X  X  rather than groundless insertion within the phoneme context. The insertion of /ih/ is actually an overgeneralized application of Korean alphabet read-ing rules, and many Koreans pronounce the word similarly, especially when reading a text. Thus, graphemes present additional information to predict Korean speakers X  pronunciations.

The left and right phoneme contexts affect some variations of each phoneme. The pronunciation variants of the word only (Figure 3) show variations that depend on the left and right phoneme contexts. The phonemes of the second and third places mutu-ally affect their pronunciations: the second-place phoneme varies over /n/, /l/, and /ng/, but the /l/ variant never appears if the third place phoneme is /n/, and the /n/ variant in the third position does not appear unless the phoneme in the second place is /n/ or /ng/. This phenomenon is consonant assimilation, which is common in Korean native pronunciations [Cho 1988]. Thus, a method to predict Korean speakers X  pronunciation errors should consider left and right phoneme contexts, that is, the pronunciation vari-ants should depend on the phoneme contexts.

So far we have considered three factors (grapheme, canonical phoneme, and actual phoneme context), and they present important clues for predicting Korean mispronun-ciations. For identical situations, we have observed that different pronunciations can occur; many tasks of predicting human speech behavior, including phrase break and sentence stress, share this problem because of the variety and randomness of human articulatory behavior. Previous literature has mostly treated these multiple options as conflicts and has selected the most common option, relegating the others to secondary options or ignoring them. This approach works if the target task has a clear stan-dard or desired choice. The pronunciation prediction task should consider suboptimal choices in addition to the best choice. The coverage can be expanded by allowing multi-ple options to be output; the resulting expanded pronunciation dictionary is more likely to match a real Korean pronunciation of English than is the result of single options. We therefore design the learning method to consider multiple possible mispronunciations. Previous researchers who studied modeling pronunciation variations mainly focused on improving recognition accuracy at the word level; these studies have been in-tensively reviewed [Strik and Cucchiarini 1999]. Approaches to this problem can be categorized by information source into knowledge-based and data-driven methods. Knowledge-based approaches utilize expert knowledge to model non-native pronun-ciation, and focus on the language transfer effect. This well-studied linguistic knowl-edge provides a stable model for specific phonological phenomena. This knowledge is often represented as a set of context-sensitive rules. Although use of prior knowledge presents distinct advantages, this approach often supports automatic methods only because of its extensive coverage, which comes at high cost [Tomokiyo 2000] or inten-sively targets specific phonological phenomena [Goronzy et al. 2001].

Data-driven approaches directly derive variants from the speech to build a practical model for the group that recorded the data. Bouselmi et al. (2005), Goronzy et al. (2004) and Oh et al. (2007) utilized a phoneme-level ASR to capture pronunciation confusion actually observed in ASR results
We propose an error-driven learning approach that learns pronunciation variation rules from the three-tiered data described in Section 2 (Figure 1). The main idea of error-driven learning perfectly fits our goal of directly minimizing the difference between Korean mispronunciation and the phonemes of the predicted result. To apply the error-driven learning concept, we interpreted pronunciation prediction as a tag-ging task, such as part-of-speech tagging, that is, tagging a canonical phoneme with an actual phoneme. The overall procedure has two phases: (1) learning single-tag tag-ging rules without considering phoneme contexts, where phoneme context is left and right actual phonemes excluding graphemes and canonical phonemes; and (2) learning multi-tag rewrite rules while considering phoneme contexts. Input data in the first phase contain training examples that include factor and tag parts (Figure 4). A training example instance corresponds to a certain speech segment, such as a word or utterance, which is a series of multitiered tokens. The grapheme and canonical phoneme tiers compose the factor part, and the actual phoneme tier is used as the tag part. The first phase learns a set of tagging rules from the input data.
A tagging rule contains a triggering condition and tag (Figure 5). The triggering condition decides whether the tagging rule applies to a certain input. More specifically, this condition contains a series of factors, and the tagging rule applies if and only if all factors match the input. We defined the condition as a string of adjacent factors centered at the base factor f 0 . We use the notation f w condition that spans from the left to right factors of the relative positions where the base factor corresponds to the position of the actual phoneme to be tagged. Each factor contains grapheme g i and canonical phoneme c and we denote the factor of the relative position i as f w tags an actual pronunciation t 0 under the condition f w  X 
The overall procedure (Figure 6) of the first phase applies a separate learning pro-cedure for each factor observed in the training data. A factor X  X  learning procedure pro-duces a set of tagging rules with a window size and triggering condition centered at the factor. For simplicity, we used the same window size for the left and right spans for every triggering condition. A factor X  X  learning procedure iteratively extracts rules by using the majority selection principle, which minimizes the difference between the result and reference. The window size increases by one at each iteration. The iteration ends when increasing the window size does not increase the number of tags correctly predicted by the set of rules. The following score function (1) calculates the total agree-ment between the result and the reference by applying every rule of a given base-factor f and window size w , where the count function increments if both parameters, that is, tag and factor, occur together. We use angle brackets for notations that we defined: the factor, the tagging rule, the score function and the count function. score f 0 , w =
The first phase outputs a set of rules ( R ) and their factor window sizes ( W ) for each base factor. After the first step, the algorithm produces a set of generation rules and the sufficient window size for each phoneme, where the sufficient window size is the window size obtained at the end of the iteration process. Input data for the second phase contain training examples, as in the first phase. A training example for the second phase contains the tokens annotated by the rules learned from the first phase and their corresponding window sizes, that is, the example has five elements: grapheme, canonical phoneme, phoneme tagged by the rules of the first phase, actual phoneme, and sufficient window size.

The form of the rewrite rule (Figure 7) is similar to the tagging rule learned in the first phase; a rewrite rule also contains a triggering condition and tag. However, two important factors influence the rewrite and tagging rules. First, the  X  X andidate actual phoneme X  tier (annotated by the first phase) is included in the factor; therefore, the rule is context-sensitive. Second, each factor element is a set of labels rather than a single label. A rewrite rule represented as f w  X  w  X  t 0 phonemes t 0 with a different set of final actual phonemes t X  condition f w  X  w = f  X  w ... f  X  1 f 0 f 1 ... f w , where f
The second phase repeats four steps: collecting rewrite rules, merging the rules, drawing the best rewrite rule, and applying the best rule (Figure 8). The second phase outputs an ordered list of context-sensitive rewrite rules that can generate multiple tags, and appends a rewrite rule to the output rule list for each repetition.
The rewrite rules are extracted by comparing the machine-annotated candidate ac-tual phoneme t 0 to the reference actual phoneme a 0 .If t particular token, a rewrite rule f w  X  w  X  a 0 is formed, where w is the sufficient window size given by the first phase. In our definition, annotation t and only if t 0  X  a 0 =  X  . The following score function is used to evaluate the collected rewrite rules: score f w  X  w  X  t 0 = ( count f w  X  w , a 0  X  t 0  X  a 0  X 
The score function measures the number of agreements made by applying the given rule, and represents the error reduction multiplied by a penalty based on the num-ber of tags the rule generates, where  X 1/  X   X  indicates the maximum number of allowed tags. The penalty is necessary because the merged rule always scores better under the definition than does the original rule before merging; indeed, the merged rule becomes perfectly accurate if it replaces the tag of interest with the set of all possible tags. We thus introduced the penalty and tentatively set 1/  X  = 0.1.

Two or more rewrite rules sharing the same base-factor can be merged to form a new rewrite rule. Merge is an element-wise set union operation, which increases the applicability of the rule. The merged rule is added to the collection of rewrite rules only if the merged rule X  X  score is higher than the sum of the component rules.
After merging, a simple method draws the best rewrite rule from the rules in the list. Rule application is a simple process that checks whether the target example meets the triggering condition, then replaces the specified tag with a new one if the condi-tion is met. To prevent overgeneration, the tags replaced by applying rewrite rules are augmented by their application contexts. Each augmented context functions as a constraint based on multiple tags during the pronunciation generation process, that is, a pronunciation is discarded if one or more multi-tag decisions conflict with any augmented context. Our proposed method resembles transformation-based error-driven learning (TBL) [Brill 1995] and its fast version [Ngai and Florian 2001] in its overall algorithmic structure, but differs in two important points. First, the proposed method learns multi-tagging rules that generate multiple tags at certain contexts. The multi-tagging rule reflects the reality that the same context can cause diverse Korean mispronunciation variants. We redefined error measurement and factor matching to consider a set of tags rather than a single tag. Second, our method learns the sufficient window size in the first phase whereas the traditional TBL algorithm simply initializes with the ma-jority selection for a given word without considering additional information. The first phase of our method is the same as TBL X  X  initialization step if the iteration ends with window size zero. Our method fully uses context-independent information to learn the first phase rules and proceeds with the second phase to cover the examples that the context-independent rules cannot cover. The number of context-sensitive rules that generate multiple tags is thus controlled, as is the risk of overgeneration, which can cause a multi-tag decision conflict. Among a variety of approaches to mispronunciation detection, the GOP algorithm may be the best-known. Although the latest GOP algorithm version [Witt and Young 2000] considers systematic errors when it calculates the GOP 2 score, we adopt the original GOP 1 score (Equation (3)) because we do not have Korean-native pronunciation models of English sounds. The GOP 1 score of a phone segment is a duration-normalized acous-tic likelihood ratio of an expected canonical phoneme to the most probable phoneme. The acoustic likelihood when the phone is recognized as the canonical pronunciation, that is, the numerator is obtained by forced alignment, where O ment corresponding to phone p . The acoustic likelihood of the phone under the most probable phoneme choice, that is, the denominator, is obtained by free recognition on the unlimited phone-loop Q ,andNF( p ) is the number of frames in the acoustic segment O p ) to obtain a duration-normalized GOP 1 score.

We modify Equation (4) the GOP 1 score to make the best use of our prediction method using predicted phoneme recognition space S . We reduce the search space by replacing the entire phoneme set Q in Equation (3) with its subset S , which contains the phonemes predicted by our proposed pronunciation variants prediction method. This modification is implemented by replacing the unlimited phone loop which can generate any phoneme sequences using the entire phoneme set Q with the ERN gen-erated from our phoneme variants prediction method. This modification results in a trade-off between search error and prediction error, and can reduce overall error, depending on the accuracy of the prediction method. Two simple methods to detect mispronunciations are na  X   X ve comparison and a thresh-old test using the GOP score. Na  X   X ve comparison directly compares the results from two runs of phoneme recognition, one on a canonical path and one on an ERN. The canoni-cal phoneme recognition result comes from the phoneme decoding result of ASR, where the recognition result is bound to the canonical pronouncing dictionary and has a de-coding path only with the dictionary. The actual phoneme recognition result from the ERN comes from the ASR decoding result, where the decoding path is bound to the ERN generated from the predicted pronunciation variants by the proposed prediction method when the phoneme recognition runs. The two results are aligned phoneme-by-phoneme using the Levenshtein distance algorithm [Levenshtein 1966], considering the timelines to generate a sequence of canonical-actual phoneme pairs. The pairs of different canonical and actual phonemes are marked as mispronunciations. A thresh-old test classifies a phoneme as a mispronunciation if the GOP score of the phoneme exceeds a certain threshold. Machine learning approaches can blend these approaches if the features that describe them are properly defined.

We treated mispronunciation detection as a classification problem; thus we chose a machine learning approach to output a classification result. Logistic regression is a type of regression analysis that uses one or more predictor variables to predict the out-come of a categorically dependent variable. We chose logistic regression for mispronun-ciation detection for three reasons. First, the regression function outputs a real-valued score between zero and one that can be directly mapped into the decision probabil-ity, which may be important information for CAPT applications. Second, the feature functions designed are proportional to the mispronunciation probability, and we thus expect the decision boundary to be simple and nearly linear. Finally, the contribution of each feature can be easily controlled by adjusting its weight.
 For the logistic regression, we extracted two features that describe each approach. The first feature is a mispronunciation preference feature. We used the canonical-actual phoneme pairs obtained from direct comparison, and converted this information into a mispronunciation preference feature. We precalculated the preference for every phoneme pair by using the observed probability at which a canonical phoneme was mispronounced as a specific erroneous phoneme (Table I). For example, if a canonical phoneme recognition result is /aa/ and the actual phoneme recognition result is /ah/, the observed probability at which this specific substitution error occurred is 61.6%, so this value is used as the probability of /aa/ being mispronounced as /ah/. The second feature is a modified GOP score for every phoneme. For the scoring information, the modified GOP score is an off-the-shelf feature, and we used it directly. The classifica-tion model was built separately for each phoneme class because the mispronunciation distribution differs among phones. We used the data described in Section 2 to compare the effectiveness of the proposed method to those of the baseline methods. After removing utterances containing ex-ceptional words for which canonical pronunciations are unavailable, we obtained 167k phonemes from 170 speakers. We randomly grouped the data into five sets of 34 speak-ers and chose one set for testing and the others for training. We cumulated the four training data sets to build four training data sets of various sizes, that is, 34, 68, 102, or 136 speakers (Table II).

We cumulatively organized the training data to see the relationship between the pre-diction accuracy and the amount of supplied training data. Although we collected data on 167k phonemes, manually-corrected phoneme-level transcriptions were generally difficult to acquire. The prediction method should thus work even for small amounts of data, so we considered the case in which data were sparse. We did not prepare a specific data separation but used all data to evaluate the mispronunciation detection methods because the na  X   X ve comparison and GOP algorithm do not need to learn mod-els, and because the machine learning method is evaluated with a random split n -fold cross validation.
 The first baseline is the decision tree learning approach, which has shown success-ful results when used to model pronunciation variants [Humphries et al. 1996; Riley 1991; Riley et al. 1999]. We used C5.0 6 to learn and apply decision trees. C5.0 is a sophisticated data mining tool for discovering patterns that delineate categories, assembling them into classifiers, and using them to make predictions. C5.0 classi-fier is expressed as a decision tree or a set of if-then rules, forms that are generally easier to understand than neural networks. We supplied 21 factors f graphemes, canonical pronunciations, and actual pronunciation contexts, the trained decision trees to predict a Korean nonnative pronunciation phoneme for the given factors.
 The second baseline is the TBL approach, which is a method to learn rewrite rules. TBL effectively learns rewrite rules triggered by contextual patterns, but it requires some templates for the rules; these templates should be designed after carefully con-sidering the factors that provide information about the task: the design process re-quires some intuitive choices and knowledge about the task. We did not supply as many factors as for decision tree learning because if we supply overdetailed tem-plates covering large windows, the resulting transformations hardly apply to the new examples. Instead, we supplied the template of the five factors f the decisions in decision tree learning. We used our own implementations of the TBL algorithm.

We conducted an experiment to demonstrate the effectiveness of the proposed method. The experiment compares the contribution of the grapheme factor obtained from the alignment procedure and the proposed method X  X  pronunciation variants pre-diction accuracy against decision tree learning and TBL. To demonstrate the effectiveness of pronunciation variants prediction, we compared mispronunciation detection accuracy with and without pronunciation prediction for each method using na  X   X ve comparison, the GOP algorithm, and logistic regression (modified GOP). All three mispronunciation detection methods perform phoneme recognition, and we replaced the unlimited phone-loop network in GOP with an ERN generated from the predicted pronunciation variants by the proposed prediction method.

Mispronunciation detection performance can be measured in various ways, and we used several well-known measures. We first used cross-correlation ( CC )[Wittand Young 2000], which represents the similarity between the mispronunciation label from two difference sources, that is, machine-generated results and the corresponding hu-man label. Although the original CC considers fuzzy labels, we adapted it to use binary labels. Second, we used scoring accuracy ( SA ) [Kanters et al. 2009], which represents the overall correctness of the decisions. Finally, we computed precision ( P ), recall ( R ), and F1-score ( F ) for both mis-and correct-pronunciation labels.

We compared binary classification results for four cases: (1) true positive ( tp ), where both the classifier and reference label mark mispronunciation; (2) true negative ( tn ), where both the classifier and reference label mark correct pronunciation; (3) false pos-itive ( fp ), where the classifier marks mispronunciation but the reference label marks correct pronunciation; and (4) false negative ( fn ), where the classifier marks correct pronunciation but the reference label marks mispronunciation. From the counts of each case, the measures were calculated as where the subscripts denote correct pronunciation ( c ) and mispronunciation ( e ). Every method predicted the pronunciation variation at the phoneme level with and without the grapheme factor. Every method had lower error rate than the trivial pre-diction, which duplicates the canonical phonemes, and had much lower error rate with grapheme factor. The results of the methods differed from each other with increasing data size (Table III). The decision tree showed decreasing error rates for increasing data sizes; this trend is typical of machine-learning algorithms. TBL showed almost constant performance regardless of data size, because the resulting transformations were too specific to be applied to unseen cases; this phenomenon is caused by the lack of delicate templates that describe the TBL rewrite rule precisely. Our proposed method (GTBL) achieved the lowest error rate among the three methods, and the error rate decreased as the amount of training data increased. These results demonstrate that our method generates diverse variants and can cover almost twice the number of patterns as the decision tree or TBL (Table III, last column). The mispronunciation detection methods based on the predicted pronunciation varia-tions achieved consistently better results than did the methods based on the unlimited phone loop (Table IV). The most significant change in the numbers when using the predicted variations was the increase in R c ; this increase indicates that the predicted space prevented many false alarms by limiting the number of mispronunciation can-didates. The increase in R c is directly related to the increased P portant in CAPT system feedback provision and scoring system reliability. Although a trade-off exists between P e and R e , the increase in P e trade-off can be interpreted as the realization of the trade-off between prediction and search error. Thus the proposed mispronunciation detection method based on predic-tion of pronunciation variants achieved improved mispronunciation detection accuracy in the presented measures.

To find the main error sources, we examined the mispronunciation detection per-formance (Table V) of the logistic regression-based approach for each phoneme class. P e was poor for /d/, /v/, and /z/; the common characteristic of these phoneme classes is that they have few mispronunciation instances compared with their total num-bers of instances in the reference annotation. The balance between correct-and mis-pronunciation instances of the phoneme classes has a significant relation to P (Figure 9(a)) but not to R e (Figure 9(b)); this result indicates that the poor P /v/, and /z/ occurs not because of a prediction error but because of a search error. To balance the distribution of the positive and negative instances, we actually predicted the poor classification performance in an early stage and oversampled mispronounced instances before applying the learning models. Therefore, the performance degrada-tion in low error-rate phoneme classes is not because of data imbalance, but because of the lack of sufficient information for the mispronounced instances. By excluding phoneme classes that have a labeled mispronunciation rate of performance rating of our proposed method ( SA = 83.1%; CC number of mispronunciations are still covered, because the remaining phoneme classes have a relatively high labeled mispronunciation rate.

The machine-learning-based approach outperformed the other approaches, which had performances comparable to each other. Although the results were obtained by lo-gistic regression in this work, several other powerful methods to do this are available. We have tested the artificial neural network and the classification and regression tree algorithm and obtained slightly better results than with logistic regression. The main advantage of these methods over logistic regression is that they can handle nonlin-ear decision boundaries with possibly complex shapes. The advantage of a nonlinear boundary may increase classification accuracy. However, the features used in the ex-periments were designed to be proportional to the mispronunciation probability, and the relationship between these features and the mispronunciation probability is ex-pected to be monotonic; if this expectation is valid, the decision boundary may not be complex, so logistic regression is sufficient. However, if we increase the number of features considered, exploring the classification accuracy of machine learning algo-rithms will be interesting future work. We have presented an approach to modeling and predicting Korean learners X  English pronunciation variants. We collected a set of English-read speech data spoken by Korean speakers and extracted pronunciation variation knowledge from the differ-ences between their phonemes and a corresponding set of canonical phonemes. We designed a pronunciation-variants prediction method that uses multi-tagging GTBL. We considered the prediction method to begin with a simple initial decision based on the canonical phoneme, and to use a grapheme factor; Experiments demonstrate the method X  X  effectiveness. We also designed the prediction method to be sensitive to sur-rounding contextual phonemes and to learn multiple choices.

We compared our proposed prediction method to decision tree learning and TBL. The proposed method achieved the lowest error rate among the three prediction methods. Although the proposed method is designed to predict Korean pronunciation variants, the fundamental concept of multi-tagging GTBL applies to any other tasks that require diverse prediction with large coverage.

The pronunciation-variants prediction method was applied to each phoneme-recognition process, including na  X   X ve comparison, GOP algorithm, and logistic regres-sion machine-learning-based mispronunciation detection methods, and yielded a significant improvement compared with methods that use unlimited phone-loop de-coding. The overall accuracy of the compared mispronunciation detection methods was reasonable, except for some phoneme classes with low labeled mispronunciation rates. We expect that the proposed pronunciation variants prediction method will be adopted in CAPT systems which provide feedback to correct mispronunciations by nonnative speakers of English.

The pronunciation dictionary we use does not have various pronunciations gener-ated by coarticulation or reduction, so our mispronunciation detection method can-not distinguish the erroneous pronunciation and the pronunciations generated by coarticulation or reduction. Therefore, we address a vowel-reduction phenomenon as future work.

