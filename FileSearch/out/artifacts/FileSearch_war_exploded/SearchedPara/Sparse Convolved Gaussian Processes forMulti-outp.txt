 these overheads in the context of convolution processes [6, 2].
 process for the latent function.
 and computational demands. For Q output dimensions and N data points the covariance matrix putational overhead they imply.
 resenting a user specified value.
 synthetic and real datasets. Consider a set of Q functions { f between a smoothing kernel { k More generally, we can consider the influence of more than one latent function, { u clude a noise term), w The covariance between two different functions y where cov [ f q ( x ) ,f s ( x 0 )] = This equation is a general result; in [6, 2] the latent functions u white Gaussian noise processes, i.e. cov [ u simplified as an independent GP, i.e. cov [ u function for u As well as this correlation across outputs, the correlation between the latent function, u any given output, f of the model is given by where y = y &gt; K is the set of parameters of the covariance matrix and X = { x vectors at which the covariance is evaluated.
 The predictive distribution for a new set of input vectors X where we have used K ated at the inputs X the computation of the inverse of K predictive variance.
 model. If we had observed the entire length of each latent function, u each y independence will hold even if we have only observed M samples from u could be very well characterized from only a few samples.
 We define u = u &gt; [ u r ( z 1 ) ,...,u r ( z M )] functions u trices between the latent functions u (4) and Z = { z p ( y | u , Z , X ,  X  ) = We rewrite this product as a single Gaussian with a block diagonal covariance matrix, where D = blockdiag K elements should be set to zero. We can also write this as D = K is the Hadamard product and M = I their process priors, i.e. p ( u | Z ) = N ( 0 , K Notice that, compared to (5), the full covariance matrix K variance K is the multiple outputs.
 as where A = K integration of (6), evaluated at X with D approximation.
 the marginal likelihood wrt the matrices K convolution process framework, the semiparametric latent factor model (SLFM) proposed in [15] corresponds to a specific choice for the smoothing kernel function in (1) namely, k The latent functions are assumed to be independent GPs and in such a case, cov [ f P For computational speed up the informative vector machine (IVM) is employed [8]. K to SLFM, the convolution process is related with MTLM when the smoothing kernel function is given again by k [12] use a similar covariance function to the MTLM approach but use an IVM style approach to sparsification.
 more general case when neither kernel nor covariance function is given by the  X  function. cess of the form k form, k (see supplementary material).
 the following parameters, S for the outputs and L simply added white noise with variances  X  2 approximations we used M = 30 fixed inducing points equally spaced between the range of the input and R = 1 . We sought the kernel parameters through maximizing the marginal likelihood model is able to capture the correlations and predicts accurately the missing information. full GP with a considerable reduction of training times.
 Sotonmet, Cambermet and Chimet) each of which measures several environmental variables [12]. with an additional squared exponential independent kernel for each w of the behavior of the independent model, which is not able to follow the original signal. explains in some extent the different behaviors.
 GPs. However, as we have seen, the predictive power of independent GPs is lower. Linear dynamical systems responses can be expressed as a convolution between the impulse re-of obtaining approximate solutions and incorporating prior domain knowledge to the model. problem have already been proposed [14].
 Acknowledgments We thank the authors of [12] who kindly made the sensor network database available. References
