 Maximum margin discriminant analysis (MMDA) was pro-posed that uses the margin idea for feature extraction. It often outperforms traditional methods like kernel principal component analysis (KPCA) and kernel Fisher discriminant analysis (KFD). However, as in other kernel methods, its time complexity is cubic in the number of training points m , and is thus computationally inefficient on massive data sets. In this paper, we propose an (1 + ) 2 -approximation algorithm for obtaining the MMDA features by extending the core vector machines. The resultant time complexity is only linear in m , while its space complexity is indepen-dent of m . Extensive comparisons with the original MMDA, KPCA, and KFD on a number of large data sets show that the proposed feature extractor can improve classification ac-curacy, and is also faster than these kernel-based methods by more than an order of magnitude.
 Categories and Subject Descriptors: I.2.6 [Learning]: Kernel Methods; I.5.2 [Design Methodology]: Feature Ex-tractions.
 General Terms: Algorithms.
 Keywords: Kernel Feature Extraction, SVM, Scalability.
Superfluous features are abundant in real-world data. It is thus often worthwhile to perform dimensionality reduction that maps the original features to a new lower-dimensional feature space, while ensuring that the overall structure of the data points remains intact. A popular example is kernel principal component analysis (KPCA) [9]. While KPCA is unsupervised, the use of supervised information as in ker-nel Fisher discriminant analysis (KFD) [7] can lead to even better feature extractors. In the special case where the two classes are normally distributed with the same covariance, the direction found by KFD is Bayes optimal. However, when these assumptions are not met, the KFD directions may be far from optimal.
 On the other hand, SVM has good generalization per-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. formance by separating the classes with a large margin [9]. However, a single SVM is not always perfect, especially when one hyperplane may not fit the data well. Mangasarian et al. [6] proposed a multisurface version of the SVM that uses multiple hyperplanes to fit the patterns with both large mar-gin and small variance. Other proposals include the combi-nation of multiple SVMs [4].

Maximum margin discriminant analysis (MMDA) [5] is a recent method that exploits the key ideas of KFD and SVM. In contrast to KFD, MMDA does not require normal-ity assumptions on the data. Its goal is to project the data onto the normal of the hyperplane that best separates the classes. The first MMDA feature is obtained by using the standard SVM. Then, after obtaining d orthogonal MMDA features, the ( d + 1)th feature is found by optimizing the SVM in the remaining feature subspace. As in other feature extractors, this orthogonality constraint reduces the depen-dence among features, and thus usually only a few features are needed. Computationally, feature extraction in MMDA is formulated as a quadratic program (QP) which is similar to that of the SVM. However, given m training patterns, a naive QP solver requires O ( m 3 ) training time and at least O ( m 2 ) space. Thus, a major challenge is how to scale this up to massive data sets.

Recently, the core vector machines (CVM) is proposed that exploits the  X  X pproximateness X  in the design of SVM implementations [10]. By utilizing an approximation algo-rithm for the minimum enclosing ball problem in computa-tional geometry, the CVM has an asymptotic time complex-ity that is linear in m and a space complexity that is even independent of m . Experiments on large classification [10] and regression [11] data sets demonstrated that the CVM is as accurate as other state-of-the-art SVM implementations, but is much faster and can handle much larger data. In this paper, we attempt to integrate MMDA with the CVM. However, as the original CVM does not utilize orthog-onal constraints on the weight vectors, the QP of MMDA is not of the form required by the CVM. Thus, we propose an extension of the MEB problem that places orthogonality constraints on the MEB X  X  center. By adapting the CVM and its associated optimization problem, we can then per-form MMDA on massive data sets in an efficient manner.
The rest of this paper is organized as follows. Sections 2 and 3 provide introductions on MMDA and CVM, respec-tively. Section 4 then describes the proposed extension of CVM algorithm. Experimental results are presented in Sec-tion 5, and the last section gives some concluding remarks.
Given a training set D = { ( x i ,y i ) } m i =1 ,where x the input and y i  X  X  1 the class label 1 . The L2-SVM finds the hyperplane that maximizes the margin with minimum squared error. Its primal can be formulated as: Here,  X  is the nonlinear feature map associated with kernel k ,  X  i  X  X  are the slack variables and C weights the misclassi-fication cost. Note that  X  i  X  0 is automatically satisfied at optimality. In the following, we will denote this maximum margin separation problem by MMS( D ,C ).

MMDA extracts the features one by one. Let the fea-tures that have already been extracted be w 1 , w 2 ,..., w MMDA (Algorithm 1) requires the new feature w to be or-thogonal to all these previous w q  X  X , i.e., Notice that as MMS( D ,C )isaQP,sois(2).
 Algorithm 1 Margin Maximizing Discriminant Analysis w 1 = MMS( D ,C ) for s =1 ,...,d  X  1 do end for
As an illustration, Figure 1 shows the feature spaces ex-tractedbyKFDandMMDAonthe Optdigits data set 2 .As can be seen, both KFD (except for the two circled points) and MMDA separate the classes well. Figure 1: Digits 0, 6 and 9 in the 2D feature spaces extracted by KFD and MMDA.
Given a set of points S = { x 1 ,..., x m } , the minimum enclosing ball of S (denoted MEB( S )) is the smallest ball B ( c ,R ) in the feature space induced by the kernel k that contains all the points in S . Its dual is the QP: where  X  =[  X  i ,..., X  m ] is the vector of Lagrange multipli-ers, 0 , 1  X  R m are vectors of zeros and ones, and K =  X   X 
For multi-class problems, we use the standard one-vs-all technique to convert it to multiple binary classification prob-lems. MMDA features are then extracted from each of these pairwise classifiers.
Details of this data set will be given in Table 1. Here, we denote the ball with center c and radius R by B ( c ,R ). Moreover, we denote the center and radius of a ball B by c B and r B . (where  X  =[  X  ( x 1 ) ,..., X  ( x m )]) is the kernel matrix. As-sume that k satisfies a constant, for any pattern x . Using the constraint  X  1 =1, we have  X  diag( K )=  X  . By dropping this constant  X  from the objective in (3), we obtain a simpler QP: Conversely, whenever k satisfies (4), any QP of the form (5) can be regarded as a MEB problem. This establishes an im-portant relationship between the MEB problem and kernel methods. For example, it can be shown that the two-class L2-SVM, having training data { z i =( x i ,y i ) } m i =1 lent to finding the MEB in the feature space associated with the kernel  X  k ( z i , z j )= y i y j k ( x i , x j )+ y
While the learning problem can be formulated as a MEB problem, this offers no immediate computational advantage as traditional algorithms for finding exact MEBs do not scale well with dimensionality. However, recently, B  X  adoiu and Clarkson proposed a simple yet efficient (1+ )-approximation algorithm using the idea of core sets [1]. Let the MEB estimate at the t th iteration be B ( c t ,R t ). It is then ex-panded by including the furthest point outside the (1 + )-ball B ( c t , (1 + ) R t ). This is repeated until all the points in
S are covered by B ( c t , (1 + ) R t ). By incorporating this approximation algorithm into the CVM (Algorithm 2), the resultant asymptotic time complexity is only linear in the training set size m while its space complexity is even inde-pendent of m [10].
 Algorithm 2 Core Vector Machine 1: Initialize S 0 , c 0 and R 0 . 2: Terminate if there is no training point z i falling outside 3: Find z i such that  X   X  ( z i )isfurthestawayfrom c t 4: Find the new MEB( S t +1 )andset c t +1 = c MEB ( S t +1 5: Increment t by 1 and go back to Step 2.
A prerequisite 4 for the applicability of the CVM algorithm is that the QP must be of the form in (5). However, as will be shown in this section, this condition is not met by MMDA.
Consider (2) with the use of the L2-SVM in (1). As in the original CVM [10, 11], we slightly modify the formulation in (1) and write its primal as:
Recently, this is relaxed to allow QPs of a more general form [11]. However, even this extension cannot cover the type of QPs associated with MMDA.
 where u q =  X  w q /  X  w q . Introducing Lagrange multipliers  X   X   X  X  and  X   X  i  X  X  for the constraints in (7) and (8), respectively, we obtain the dual: max  X   X   X   X  K  X   X   X  2  X   X  Y X  U  X   X   X   X   X  U U  X   X  :  X  where  X   X  =[  X   X  1 ,...,  X   X  m ] and  X   X  =[  X   X  1 ,...,  X   X  variables, Y = diag( y 1 ,...,y m ), U =[ u 1 ,..., u s ], and is the transformed  X  X ernel X  matrix. By using the Karush-Kuhn-Tucker (KKT) conditions, the primal variables  X  w = can be recovered from the optimal  X   X  and  X   X  . Substituting u q =  X  w q /  X  w q back into (11) recursively, the extracted fea-ture  X  w is then expressed as a linear combination of  X  ( x
Should it happen that all the  X   X  q  X  X  in the optimal solution are zero, then only the first term in (9) remains and so the dual takes the form in (5). Moreover, it is easy to see that the diagonal entry of the  X  X ernel X  matrix  X  K in (10) (which plays the role of K in (5)) is [  X  K ] ii =  X  +1+ 1 C , and thus (4) is satisfied. Hence, as mentioned in Section 3, this MMDA problem can be regarded as a MEB problem. However, in general, not all  X   X  q  X  X  are zero. Thus, (9) will not have the form of (5), and an extension of the CVM is required.
Consider adding a set of constraints to the MEB problem such that the center c is required to be orthogonal to the existing u 1 , u 2 ,..., u s .Wethenhave Introducing Lagrangian multipliers  X  i ,  X  i for the constraints (13) and (14) respectively, the dual can be written as w.r.t.  X  =[  X  1 ,..., X  m ] and  X  =[  X  1 ,..., X  s ] . Proceeding as in Section 3, we combine (4) with the constraint  X  1 =1 and obtain  X  diag( K )=  X  . Dropping this constant from the objective in (15), we obtain the simpler problem: max  X   X  K  X   X  2  X   X  U  X   X   X  U U  X  :  X   X  0 ,  X  1 =1 . (16) The center c = p  X  diag( K )  X   X  K  X   X  2  X   X  U  X   X   X  U U  X  are then recov-ered from the optimal  X  and  X  . Conversely, whenever the kernel k satisfies (4), any QP of the form (16) can be re-garded as a MEB problem with orthogonality constraints on the center.

Returning to MMDA X  X  QP in (9), we can rewrite it as: where e i  X  R m is all zeros except that the i th position is h y 0 is the s  X  ( m +1) zero matrix. From (3), as  X  k ( z , z )=  X  +1+  X   X   X  is a constant,  X  k again satisfies (4). In other words, the optimization problem associated with MMDA in (6) can be viewed as a constrained MEB problem in (12), with  X  being replaced by the new feature map  X   X  . Once transformed to a MEB problem, the CVM procedure (Algorithm 2) can be easily adapted to cater for the new set of constraints in (8). Thus, the approximate MEB( S ) can be obtained by solving for MEB( S t ) iteratively.
Recall that finding the MEB( S t )involvesaQP.Inthe implementation of [10], we used an efficient decomposition method called sequential minimal optimization (SMO) [8] as the internal QP solver. However, here, the extra constraints in (14) lead to some Lagrangian multipliers  X   X  t that are not involved in the equality constraint of (17). This hinders the use of SMO. Moreover, the equality constraints in (17) leads to the infeasibility of  X   X  t =[  X   X  ( t ) 1 ,...,  X   X  ( of a single  X   X  ( t ) i is changed from a feasible  X   X  t
Here, we propose instead an efficient incremental update algorithm for finding the MEB( S t ). Due to the lack of space, only the major steps will be outlined. By considering its optimality conditions, the dual of (16) can be solved via the kernel adatron (KA) [2], which is essentially a variant of the Gauss-Seidel (GS) iteration approach for solving the linear system [3]: until the KKT conditions: 0  X   X   X   X   X  K  X   X  + Y X  U  X   X   X  1  X  0 and U  X Y  X   X  + U U  X  are met. [  X   X   X   X  ] is then normalized such that  X   X  1 =1.
To be more specific, this incremental update is used to solve the corresponding QP of the MEB( S t ), which is formed with  X  K t ,  X  t and Y t defined on the core-set S t analogous to  X  K ,  X  and Y on the whole training set S . The new feasible solution of the Lagrangian multipliers for constraints in (7) of the patterns from S t and the orthogonality constraints adapted from the optimal  X   X  t  X  1 and  X   X  t  X  1 of the MEB( as a warm start. For S 0 , we initialize  X   X  0 = [1] and  X  a feasible starting point. Then,  X   X  t and  X   X  t of the MEB( are updated by gradient descent for each variable by fixing others as in the GS iteration [3]: and the new value of  X   X  t are projected into the feasible region which satisfies the box constraints  X   X  t  X  0 .Thisprocessis repeated until the KKT conditions in (18) defined on S t are satisfied. Then, normalize [  X   X  t  X   X  t ] s.t.  X   X  t wards, c t and R t of the MEB( S t ) can be used to find the furthest point (Step 3 of Algorithm 2) to construct S t +1
Here, we list some properties of the modified CVM algo-rithm. The proofs are very similar to those in [10] and so are skipped here.
 Bound on the number of iterations: There exists a subset S t , with size 2 / , of the whole training set S such that the distance between c MEB ( S t ) and any point z i at most (1 + ) r MEB ( S ) .
 Recall that one point from S is added to the MEB at each iteration of Step 3 (Algorithm 2), this property thus ensures that the proposed method converges in at most 2 / iter-ations, independent of the feature dimensionality and the size of S .
 Convergence to (approximate) optimality: When = 0 , the algorithm finds the exact MMDA solution. When &gt; 0 and the algorithm terminates at the  X  th iteration, and we have max value of MMDA X  X  objective in (6).
 In other words, this is an (1 + ) 2 -approximation algorithm. As is usually very small, the approximate solution obtained is thus very close to the exact, optimal solution. Complexities: Recall that the main motivation for using an approximation algorithm is that its time and space com-plexities are much smaller than those of an exact algorithm. For the proposed algorithm, it can be shown that when prob-abilistic speedup is used in Step 3, the total time for solving the ( s + 1)th SVM is O gorithm takes O (1 / 2 ) space, which are independent of m for a fixed . Here, we ignore the O ( m ) space requirements for storing the m training patterns, as they may be stored outside the core memory.
In this section, we perform experiments on a number of real-world data sets 5 (Table 1). The following feature ex-tractors (all implemented in MATLAB) are compared: 1) the original MMDA, which is based on SMO [8]; 2) the pro-posed method, denoted MMDA(CVM), with probabilistic speedup and fixed 6 at 0.001; 3) kernel PCA (KPCA); and 4) kernel Fisher discriminant analysis (KFD). Methods that did not finish in 24 hours will be indicated by  X  X  X .
In our experiments, the C parameter in (6) is fixed at the value of 1. We use the Gaussian kernel exp(  X  x  X  z 2 / X  ), where  X  = 1 m 2 tance between patterns. Experiments are performed on an AMD Athlon 4400+ PC with 4GB of RAM.
As the performance of classification algorithms critically depend on the input features, we first examine the behav-ior of classification performance and extraction time of these
The first five data sets are from the UCI ma-chine learning repository, while the last two are from http://www.cs.ust.hk/  X  ivor/cvm.html .
Preliminary results show that this fixed value of leads to both fast training and good feature extraction. kernel feature extractors using different numbers of extracted features. Here, experiments are only performed on the first three smaller data sets in Table 1. The classification perfor-mance is obtained from the testing accuracy of an artificial neural network (ANN) using the extracted features as in-put. This ANN is a feed-forward multilayer perceptron with a single layer of 10 hidden units, and training is performed via standard back-propagation. Note that the rank of the between-class matrix in KFD is at most N c  X  1(where N c is the number of classes) [7], and so the number of features for KFD is always fixed at N c  X  1.

Figure 2 shows that the CPU time of MMDA extraction increases with the number of extracted features. On the other hand, the CPU time for KPCA and KFA is almost fixed, as both are dominated by the eigendecomposition of the m  X  m kernel matrix, which is always required no matter how many features are to be extracted. Furthermore, the results also confirm that the proposed CVM-based kernel MMDA implementation is often much faster than the other feature extractors.

As for testing accuracy, the performance with KPCA fea-tures usually improves at first, and then becomes stabilized or sometimes even degraded as features with lower classifi-cation information are included. For both MMDA feature extractors, their classification performance appear to be op-timal and better than the others when there are around 3 N to 5 N c features. Hence, in the sequel, we will only conduct experiments using N c ,3 N c and 5 N c MMDA features. In this section, we experiment with another classifier, the C4.5 decision tree, on all the data sets in Table 1. Recall that both KPCA and KFD involve solving an m  X  m eigen-system, which is computationally expensive on large data sets. To alleviate this problem, we will only use a random sample of size 3,500 when these two methods are used on the letters , mnist , usps and face data sets. Moreover, for simplicity, we fix the number of extracted features at 5 N for both KPCA and MMDA.

Results are shown in Tables 2 and 3. As can be seen, the features extracted by MMDA(CVM) often lead to small trees and high testing accuracy. As decision trees use the extracted features for node splitting. A small tree indicates that the set of extracted features carry useful classification information. Note also that KPCA and KFD perform poorly on letters , mnist and face . This demonstrates that, in gen-eral, random sampling is not a good approach to reduce the computational complexity.
In this section, we feed the extracted features to the 1-nearest neighbor (1-NN) classifier, and the artificial neural Table 3: Testing accuracies (in %) obtained by the decision tree using the extracted features. network (with the same ANN setting as in Section 5.1). Re-call that the first MMDA feature is obtained by using the standard SVM. Hence, to demonstrate the usefulness of the extra MMDA features, we also compare with the standard SVM as a baseline. Moreover, as mentioned in Section 5.1, we only experiment with N c , 3 N c and 5 N c MMDA features.
Table 4 shows the classification accuracies. The follow-ing general observations can be made: 1) Feature extraction can improve classification accuracy. In particular, the use of MMDA features can outperform a SVM. 2) MMDA, using either the original or new implementation, leads to better classification accuracies than the other feature extraction methods. 3) For MMDA, extracting several (3 N c  X  5 N c ) features is often beneficial. Moreover, note that KPCA and KFD sometimes perform miserably on the large data sets because of the random sampling problem mentioned in Sec-tion 5.2.
Table 5 shows the CPU time needed in the feature extrac-tion process. As expected, MMDA(CVM) is always faster than the original MMDA implementation, and the improve-ment can sometimes be of two orders of magnitude. Be-sides, on the three largest data sets, the original MMDA implementation cannot even converge in 24 hours, while MMDA(CVM) successfully extracts good features in only several hundred/thousand seconds. MMDA(CVM) is also always faster than KPCA and KFD on the small data sets. On the larger data sets, recall that we have used random sampling for KPCA and KFD and that explains why MMDA appears slower. However, one should also be reminded that such a random sampling scheme also leads to poor general-ization performance of KPCA/KFD in our experiments.
As mentioned in Section 4.1, each MMDA feature, in the same manner as KPCA features and KFD features, can be expressed as a linear combination of kernel evaluations. Ta-ble 6 compares the average numbers of kernel evaluations involved in the different types of features extracted. As can be seen, the MMDA(CVM) features are much sparser than the others, including the original MMDA features. As ker-nel evaluations often dominant the computational cost in testing, MMDA(CVM) is thus much faster.
In this paper, we investigated the problem of feature ex-traction in large classification tasks. Ideally, a good feature extractor should 1) produce features that can lead to a high classification accuracy; and 2) be computationally efficient during both training and testing. While the original MMDA can extract features useful for classification, it is computa-tionally inefficient on large data sets. Here, we extended the CVM algorithm and proposed an (1 + ) 2 -approximation al-gorithm for extracting kernel-based MMDA features. We ex-amined some of its theoretical aspects, and demonstrated its efficiency through various experiments. The training time complexity only depends on and, in practice, it is 10-100 times faster than the original MMDA implementation. The features extracted by the proposed method are also sparser, and involve fewer kernel evaluations. This in turn allows new features to be computed much faster during testing. This research has been partially supported by the Research Grants Council of the Hong Kong Special Administrative Region under grant 615005. [1] M. B  X  adoiu and K. L. Clarkson. Optimal core-sets for [2] T. Friess, N. Cristianini, and C. Campbell. The [3] W. Kienzle and B. Sch  X  olkopf. Training support vector [4] H.-C. Kim, S. Pang, H.-M. Je, D. Kim, and S. Bang. [5] A. Kocsor, K. Kov  X  acs, and C. Szepesv  X  ari. Margin [6] O. Mangasarian and E. Wild. Multisurface proximal [7] S. Mika, G. R  X  atsch, J. Weston, B. Scho  X  olkopf, and [8] J. Platt. Fast training of support vector machines [9] B. Sch  X  olkopf and A. Smola. Learning with Kernels . [10] I. W. Tsang, J. T. Kwok, and P.-M. Cheung. Core [11] I. W. Tsang, J. T. Kwok, and K. T. Lai. Core vector
