 Crowd based online work is leveraged in a variety of appli-cations such as semantic annotation of images, translation of texts in foreign languages, and labeling of training data for machine learning models. However, annotating large amounts of data through crowdsourcing can be slow and costly. In order to improve both cost and time efficiency of crowdsourcing we examine alternative reward mechanisms compared to the  X  X ay-per-HIT X  scheme commonly used in platforms such as Amazon Mechanical Turk. To this end, we explore a wide range of monetary reward schemes that are inspired by the success of competitions, lotteries, and games of luck. Our large-scale experimental evaluation with an overall budget of more than 1,000 USD and with 2,700 hours of work spent by crowd workers demonstrates that our alternative reward mechanisms are well accepted by online workers and lead to substantial performance boosts. K.4.4 [ Computers and Society ]: Electronic Commerce-Payment schemes Algorithms, Experimentation, Human Factors crowdsourcing, reward schemes, competitions, lotteries
Gathering and exploiting collective knowledge online has become increasingly popular over recent years. Projects such as Wikipedia or Open Street Maps have demonstrated that a large number of non-experts can, under certain con-ditions, be as effective and precise as a small group of ex-perts [24, 22]. Information providers make implicit use of collaborative knowledge for improving their services: Search engines leverage query logs for determining the popularity of web pages or for suggesting queries and advertisements to their customers. Online shops like Amazon and auctions like eBay correlate information from buyers to recommend new items. Games with a Purpose have been employed for gath-ering user input in large quantities: In the ESP game [26, 25], for instance, online players compete in image annota-tion tasks; in this way, the images indexed by Google in 2004 could be annotated in just 31 days. On the other hand, platforms like Amazon Mechanical Turk and CrowdFlower successfully make use of online workers and monetary incen-tives for accomplishing explicit tasks such as the annotation of multimedia content, translation of texts, or generation of training sets for machine learning.
 Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower are based on a reward scheme where the payment of online workers is proportional to the number of accomplished tasks ( X  X ay-per-HIT X ). Such platforms are used successfully in various contexts [29, 27], but can we get more value for money? In the  X  X eal world X  gaming based motivation has been shown to be very successful in the past. A prominent example are lotteries where the prize money is rather small in comparison to the revenue generated by millions of participants. In the former Eastern Germany construction workers received special lottery tickets for a certain amount of hours spent building houses. Another ex-ample is the Speed Camera Lottery where a portion of fines levied against speeders would be pooled in a jackpot, with a random winner periodically drawn from the group of speed-limit adherents. 1 Apart from games of luck, also competi-tion among workers can be a strong motivation. Inducement prizes awarded for instance by companies like Microsoft, Google, or Yahoo for ideas or code attract a huge numbers of participants and create an enormous overall value. The Netflix Prize 2 is an example for such an open competition, where 20,000 research teams were competing against each other, trying to implement the most effective collaborative filtering algorithm for a single prize of 1,000,000 USD.
To sum up, in the real world people are often attracted by scenarios where they can (a) compete, and/or, (b) can receive a relatively high reward with low probability. In this paper, we aim to carry over competitive and random-ized reward mechanisms to (online) crowdsourcing based on monetary incentives. Reward schemes studied in this work range from  X  X inner-Takes-It-All X  competitions, where only the top performer will earn a fixed prize money, over expo-nentially decreasing rewards distributed among an extended s et of top performers, to lottery-inspired mechanisms that introduce an additional random element. Furthermore, we study the influence of different information policies that vary the amount of knowledge and feedback given to a worker about the performance of their competitors. We use the classical  X  X ay-per-HIT X  mechanism (as employed e.g. in Mechanical Turk) as a baseline and show that it can be clearly outperformed using elements borrowed from compet-itive games and games of luck. Although theoretical gamifi-cation scenarios for crowdsourcing were considered in recent literature (see Section 2 for details), we are the first to per-form a large-scale empirical evaluation in this context.
In this work we shed light on how reward mechanisms can be employed in a cost effective manner for solving online tasks. We are aware that conventional games of luck are associated with increased life stress for players and can neg-atively affect their social environment [14, 18]. To this end, exploiting individuals depending on receiving living wages for the tasks they perform on crowdsourcing platforms would be inappropriate. As with almost every technology, gamifi-cation in crowdsourcing scenarios requires sensible handling and constructive usage to make it of great benefit for re-search communities operating under budget constraints on problems that are of interest for the public.
 lows: In Section 2 we discuss related work on crowdsourcing and games with a purpose. In Section 3 we describe our reward strategies and information policies in competitive as well as lottery games. The evaluation of our strategies is pre-sented in Section 4 where we first describe the experimental setup along with the core results, and then delve deeper into details about worker behavior and performance. Finally, in Section 5 we conclude and describe directions of our future work.
Literature in economics and computer science typically refers to crowdsourcing as a X  X rincipal-agent X  X roblem where the agents are workers with various skill levels which solve tasks for another person -the principal. Crowdsourcing has a wide range of applications, including the annotation of data sets, conduction of user surveys, and collaborative gathering of data collections. Online platforms such as Ama-zon Mechanical Turk 3 and CrowdFlower 4 provide principals and agents with a framework for publishing and selecting tasks and transferring payments. Since 2011, TREC offers a crowdsourcing trek [2] addressing various issues related to gathering document-relevance labels for information re-trieval systems [4, 3]. This includes HIT design, user fil-tering, and the fusion of worker judgments. In this paper, we embed crowdsourcing techniques within a competitive reward framework combined with random mechanisms in order to increase the effectiveness of annotations.
In their seminal work [15] Kazai et al. study how pay-ment, worker qualification, and required effort influence the output of tasks in Amazon Mechanical Turk. In the context of relevance labeling, the authors show that increasing the rewards, reducing the required effort, and filtering workers based on qualification requirements can increase the accu-racy of the output. In [16] the same authors show the corre-lation between behavior and personality of workers and the accuracy of their work. In [19] the influence of the amount of micro-payments on quantity and quality of annotations is studied. In contrast, our work focuses on how competitive reward mechanisms influence annotation behavior and the cost efficiency of crowdsourcing.

A recent workshop [1] has focused on principal aspects of game mechanics embedded in standard IR tasks includ-ing information seeking, crowdsourcing, and user engage-ment. He et al. [12] introduce a user interface for studying search behavior within a gamified setting where users receive points for finding relevant documents, and where scores are announced in leader boards. Dinesh et al. [20] discuss addi-tional incentive structures to encourage user activity within an online platform. A number of earlier works tackle the same problems. Eickhoff et al. [11] employ a game based approach for crowdsourcing of query result relevance labels and image cluster labels. The authors show that entertain-ment can be a powerful incentive in crowdsourcing and can partially replace financial rewards. In contrast, in our work we study monetary reward mechanisms combined with var-ious information policies in order to increase the effective-ness of crowdsourcing. In [9, 21] the authors propose using lottery tickets for engaging crowd workers. In [9] a small scale survey was conducted and the results show that about one third of the users prefer a payment strategy involving lotteries over the standard payment mechanism. This con-firms the claims in [23], where the author mentions that a non-negligible amount of workers engage themselves into crowdsourcing tasks for both money and fun. Our work pro-vides a large-scale experimental evaluation on boosting the cost efficiency of crowdsourcing through payment strategies (lottery as well as non-lottery) in combination with different information policies.

There is a body of work on crowdsourcing theory in the area of business, economics, and e-commerce. For in-stance, [5] provides an analysis of crowdsourcing contests within the software development portal TopCoder 5 . In [10] the authors study the influence of the reward amount; they find that participation rates increase as a function of the offered reward. Other work shows that, while workers are generally attracted by high rewards, they also tend to choose tasks with low rewards that better suit their abilities and to maximize their outcome by balancing reward and work-load [28]. In [13] this issue is further addressed by splitting the crowd into groups of workers with different abilities in a scenario where workers compete with each other in the context of bug detection. Although related to our research purposes, these works target crowd based software devel-opment contexts where just a single solution is selected at the end. In contrast, our work focuses on crowdsourcing of annotations in the context of information retrieval and data mining, where the workload is divided across multiple workers.

Finally, a number of works have proposed theoretical stochastic models in the context of crowdsourcing. In [8], the authors introduce models for the effectiveness of winner-take-all scenarios, and consider aspects such as the optimal choice of the prize money. In [7] the same authors concentrate on scenarios where every non-zero effort of w orkers is rewarded and, similar to the scenario studied in this paper, the final output consists of the cumulative effort of all workers. Archak et al. [6] present a model for designing crowdsourcing contests with optimized reward distributions to improve the quality of the best submission. However, in contrast to our paper, none of these works provide an experimental evaluation of their concepts. To the best of our knowledge, we are the first to sug-gest different information policies for crowdsourcing competitions, and to conduct systematic real-world studies of the effects of policies and reward distributions on both quantity and quality of annotations.
In this section, we formalize our crowdsourcing scenario and describe different strategies for distributing rewards among users both in a competitive and random fashion. Furthermore, we describe the different information policies (relating to information revealed about fellow workers) we employed in our framework.
We consider a scenario with a crowd consisting of n work-ers W = { w 1 , . . . , w n } , and with a fixed (monetary) budget M for paying workers. This budget is distributed among the workers depending on the values v ( w i ) produced by them. These values v ( w i ) can, for instance, correspond to the num-ber of correctly solved crowdsourcing tasks. A worker w i receives a reward r ( w i ) with P n i =1 r ( w i ) = M . Our goal is to maximize the overall value V = P n i =1 v ( w i ) produced by the workers in W .
In terms of strategies we distinguish between the baseline approach of paying per task, competitive approaches where payment is received depending on the performance based on the position of workers in a leaderboard, and random approaches where a set of winners is determined in a worker lottery.
 monly used strategy in crowdsourcing is to distribute re-wards proportional to the individual values produced by workers, i.e. each worker w i receives a reward r ( w i ) = ( v ( w i ) /V ) M . In practice this is typically implemented by fixing a reward rate c (e.g. money per task solved) and an overall value V = M/c to be produced (e.g. number of tasks to be solved), resulting in a reward r ( w i ) = v ( w worker w i . This strategy corresponds to the usual payment scheme as, for instance, employed for Amazon Mechanical Turk HITs ( X  X ay-per-HIT X ).
 itive strategies where workers are ranked according to their produced values, and the reward of a worker will depend on his rank. Formally, let rank ( w i )  X  { 1 , . . . , n } be the rank of worker w i , with a rank of j corresponding to the j th highest value produced across all workers. A simple special case con-sists of paying the whole budget to the top-ranked worker ( X  X inner-Takes-It-All X ), i.e. r ( w i ) = M if rank ( w i 0 otherwise. More generally, we compute the reward r ( w i as a monotonically decreasing function  X ( rank ( w i )) of the worker X  X  rank. Similar to  X  X eal world X  competitions we dis-count lower ranks, i.e. top performers receive more money per solved tasks, and  X  is a convex function. Examples for  X  could be (appropriately normalized) negative exponential or negative polynomial functions of the rank. In practice, instead of precise mathematical functions, one would rather provide workers with an easy to understand payment scheme containing  X  X ound X  numbers for rewards and number of win-ners; in addition, one has to account for the fact that very small rewards can become meaningless. In our concrete ex-periment with an  X  X xponential reward X -like strategy, for in-stance, we chose to pay out 25, 10, 5, 5, 1, 1, 1, 1, 0.5, and 0.5 USD to the top-10 users (overall budget: M = 50 USD). ized strategies which are inspired by scenarios such as gam-bling and lotteries where these types of rewards are success-fully used. Carrying this over to crowdsourcing, we explore reward schemes where workers can earn  X  X ottery X  tickets, with each ticket corresponding to a produced value  X  (e.g. a certain number of correctly solved tasks). In this way, worker w i producing a value v ( w i ) will obtain  X  v ( w lottery tickets. After the competition a number of lottery tickets are randomly drawn and winners obtain monetary awards r ( w i ) depending on the outcome. Similar to com-petitive strategies a simple special case consists of drawing a single lottery ticket and assigning the whole budget M to the winner, i.e. r ( w i ) = M if worker w i is an owner of the (unique) winning ticket and 0 otherwise. For this  X  X inner-Takes-It-All X  situation the probability p i of worker w i winning is proportional to the number of tickets earned randomly drawing k tickets and paying out rewards for each of these tickets. In our concrete experiment, for instance, we randomly selected k = 10 of the earned tickets and ran-domly distributed the prize money of 50 USD as previously described for the  X  X xponential reward X -like strategy. Note that, in contrast to  X  X raditional X  lotteries (with a randomly drawn set of numbers), in our payment scheme the whole budget M is finally paid out to workers because random selections are conducted over earned tickets only.
In contrast to the classical linear  X  X ay-per-HIT X  scheme, for both competitive and randomized strategies the payment of workers largely depends on the performance of their fellow workers: In the competitive case rewards are determined by the rank relative to other workers; in the randomized case the probabilities of winning rewards depend on the overall number of tickets earned by users. How much information about the performance of his fellow workers should we pro-vide to a worker during a competition? Our experiments show that such information can both motivate and demor-alize workers. In this section we describe the different infor-mation policies we explored.
 competitive strategies we distinguish between open, re-stricted, and medium policies. In the open information pol-icy we provide the worker, in principle, with all of the rele-vant information about his fellow workers during the compe-tition. This includes constantly showing the updated top-10 l eaderboard of workers along with their positions and scores (values produced). Knowing the position of other workers can trigger competitive behavior and motivate workers to  X  X atch up X  or to  X  X efend X  their status. On the other hand, workers that are too far X  X ehind X  X an be demotivated. In the restricted information policy no information about the fellow workers is provided. Workers can only see their own score and do not learn anything about scores of other workers and their relative position during the competition. While this can help avoiding the demoralization of players with low scores, it can also prevent desired competitive behav-ior. Situated in between the open and restricted policy is the medium policy where only part of the information about other workers is revealed. To this end, we tested scenarios where workers were shown their position along with a snap-shot consisting of their k neighbors above and below them in the leaderboard. The rationale was to trigger competitive behavior within part of the leaderboard while avoiding too much frustration for workers with lower scores.
 the randomized strategies described above the chances of winning do only depend on the number of tickets earned by the worker and the overall number of earned tickets. To this end, we tested an open policy where workers get continuous updates about their own tickets and the overall number of tickets in the system, and a restricted policy where a worker can just see his own tickets during the competition. In con-trast to the competitive strategies and due to the simpler information related structure of the lottery competitions, we did not see the potential for some medium policy. One interesting point to note is that, although the chances of winning do not depend on the specific distribution of tick-ets across workers, we received many comments requesting ticket based leaderboards. This is an example for irrational elements and artifacts in crowdsourcing that we plan to fur-ther explore in future work.
In this section we evaluate the reward distribution strate-gies and information policies described in Section 3 using two crowdsourcing scenarios: captcha translation and face recognition. The objective of our evaluation was to study the cost efficiency of strategies, competitive behavior of workers, and acceptance of the different reward schemes. Crowdsourcing Tasks. For the captcha translation sce-nario the tasks consisted of translating 10 captchas shown on a Web page, which were drawn from a pool of 100,000 captchas generated using the Cage 6 library. Our applica-tion checked the correctness of labeled captchas and pro-vided immediate feedback to the workers. Workers obtained one point per correctly translated captcha, and the applica-tion provided continuous feedback on their scores. With the captcha task we deliberately chose a rather dull scenario because we wanted to focus on monetary incentives in this work rather than gamification and intrinsic motivation. In our setup we do not have to consider annotation quality for our captcha translation task, and measure only the correctly solved instances.

However, in order to additionally study more quality re-lated aspects in our context we launched a face recognition task where workers were asked to identify a person on a given reference photo among a set of 10 test photos. The images were retrieved from the PubFig 7 database which was created for face verification [17]. Out of originally 58,797 im-ages with faces of 200 celebrities, 37,004 images where avail-able on the Web. We reviewed the dataset manually and removed 637 images showing placeholders as well as 133 im-ages we deemed unsuitable because the correct person was not shown on the image. As a quality check mechanism we randomly introduced a  X  X oneypot X  task within each batch of 100 tasks that was manually selected beforehand. After workers finished a batch they were shown the honeypot and their own input for it. If workers correctly solved the hon-eypot, 100 points were added to their score, otherwise 20 points were subtracted (with a cut-off threshold of 0 for the score, i.e. we did not introduce negative scores). sourcing tasks on the Amazon Mechanical Turk platform and on a mailing list consisting of participants from previous competitions about one day before the competition started. The workers were choosing the tasks autonomously, as com-mon for crowdsourcing platforms such as Mechanical Turk. As most of our tested reward strategies are not supported by Mechanical Turk or CrowdFlower, we ran the actual com-petition using an external application on servers at our in-stitute. Each worker was assigned a user code which he could enter in Mechanical Turk in order to claim his prize money after a competition was finished. The experiments were performed sequentially with a duration of four days per experiment (and one run per configuration) and a break of at least one day in between two competitions in order to avoid extensive user fatigue.
 strategies ( X  X inner-Takes-It-All X  ( wta ) and exponential reward strategy ( exp )) and the two random strategies ( X  X inner-Takes-It-All X  lottery ( wtaLott ) and exponential reward lottery ( expLott )) described in Section 3.2 with an overall prize money of M = 50 USD, and combined these strategies with the information policies described in Sec-tion 3.3 (open ( open ), restricted ( res ), and medium ( med ) policy for competitive strategies; open and restricted pol-icy of random strategies). A competition is thus defined by its reward strategy and information policy; in the following, we abbreviate, for instance, a X  X inner-takes-It-All X  X trategy conducted under an open information policy as  X  wta-open  X . This results in a total of 10 (reward strategy -information policy) combinations. In addition, we conducted baseline ex-periments using a linear reward assignment ( X  X ay-Per-HIT X ) with a fixed amount of money paid per solved task ( base-line ). In order to study the influence of the absolute value of monetary rewards, we also conducted experiments with increasing prize money amounts of 10, 25, 50, and 100 USD per competition.
I n our captcha experiments overall 988,054 captchas (amounting to 2,165 hours of work) were translated by 441 participants from 17 different countries. The majority of the participants where from India (51.25%) and the US (44.44%).
 lated captchas for each of the strategies along with the av-erage amount of money spent per captcha and per hour of work. The main observations are the following:
The lottery based strategies are outperformed by both the competitive strategies and the baseline. This might be explained by the relatively low rewards (with relative high probability of winning) in comparison to  X  X eal world X  lot-teries where typically risk affine persons are attracted by very high rewards paid out with low probability. Another explanation might be that the competitive character of our lotteries is just implicitly reflected by the odds of winning given the overall number of tickets in our system. captchas solved by the top-10 (1  X  R  X  10) and the re-maining workers ( R &gt; 10) for each run. For most of the runs (including the baseline) the top-10 workers translated over 70 percent of the captchas. Another interesting ob-servation is the performance of the top-1 users, who trans-lated between 10 and more than 40 percent of the captchas. The contribution of the top-1 workers is especially high for the winner-takes-it-all scenarios; a more detailed tem-poral analysis of worker contributions described below re-veals that these scenarios become quickly less attractive for
F or the baseline we chose a reward of 1 cent for 13 captchas; in contrast, for exp-med workers would have to annotate about 31 captchas for the same reward. Also note that within the given 4-day time frame workers where just willing to translate captchas for an overall amount of about 45 USD (i.e. the whole budget of 50 USD could not be utilized). Figure 1: Contributions of individual workers in different r ounds. lower ranked workers. In contrast, the exponential reward strategies exp-res and our best performing strategy exp-med , where lower ranked workers also have a chance of earning re-wards, result in a more balanced distribution of the workload across the top-10 workers. These strategies, along with the baseline, exhibit also the  X  X attest X  tail for the distribution of the number of translated captchas contributed by workers not appearing in the top-10.
F igure 2 shows the number of workers with respect to the number of rounds they participated in; workers are further divided into the ones that won a monetary prize in at least one of the rounds and the ones who didn X  X . The limited number of prizes paid out in our lottery and competitive scenarios naturally resulted in a large number of workers that did not receive any rewards. As expected, for workers participating in multiple rounds the fraction that won in one of the rounds is higher, consistent with our observation that winning workers were more motivated to participate in further rounds.
 tion of the cumulative counts for solved captchas aggregated over the whole set of workers. The numbers grow mostly lin-early over time. However, several of the strategies such as Figure 3: Cumulative number of annotated captchas per e xperiment over time for exponential reward based (top) and winner-takes-it-all based competitions (bottom). Figure 4: Temporal characteristics of annotations for the ex-ponential reward strategy with medium information policy ( exp-med ) and the baseline . our winning method exhibit a  X  X uick start X  phase in the first h ours, showing that people are highly motivated at the be-ginning if a fixed starting point is announced beforehand. This can be seen even more clearly in the non-cumulative plots for the annotated captcha counts per hour in Figure 4 (provided for the examples of our exponential reward strat-egy exp-med and the baseline ). In addition, we observe peri-odic phases where curves become almost  X  X lat X , i.e. less work is done in these phases (see also Figure 4 that makes that periodicity more explicit). This can be explained by the day cycle of the workers as there is a strong bias towards participants from India and the US.

We also studied the contribution of individual workers over time. Figure 5 breaks the cumulative captcha counts down for the top-10 workers. For our winning exponen-tial reward strategy exp-med (Figure 5d) we observe a large number of  X  X rossing curves X  during all phases of the exper-iment, indicating strong competitive behavior compared to other strategies. This holds both for the top-2 and the re-maining workers in the top-10. In contrast, activity is much lower towards the end for the lottery and winner-takes-it-all experiments. For wta-med (Figure 5a) we can observe a fierce competition between the top-2 workers; the remaining workers seem less motivated -presumably because they are already too far  X  X ehind X  and there is just a single reward issued. It is also interesting to note that some workers seem to exhibit irrational behavior: In the wta-open experiment (Figure 5j), for instance, the top worker keeps annotating until the end, although he is already far  X  X head X  and can monitor the activity level of his competitors due to the open information policy. reward strategy, exp-med , we also studied the influence of the total prize money by varying the budget M . Figure 6 indi-cates a roughly linear increase of overall amount of work con-ducted with respect to prize money spent. However studying the influence of the overall reward more in-depth requires ad-ditional experiments which we have to leave as future work, including both multiple repetitions of the conducted experi-ments and the introduction of an extended range of budgets.
In order to examine a more realistic scenario where quality-related issues have to be taken into account and where no immediate feedback about the correctness of annotations can be provided to workers, we evaluated our approach for the face recognition task described in Section 4.1. To this end, we compared the best performing configuration from the previous subsection (exponential reward strategy with medium information policy -exp-med ) with the  X  X ay-per-task X  baseline . As before, for exp-med we distributed a prize money of 50 USD among the top-10 workers. For the base-line we paid 15 cents per batch of 100 face annotations with correctly solved honeypot, and a penalty of 3 cents for each incorrect honeypot (corresponding to the 20% of the points subtracted in the competitive exp-med scenario). In total 72.29 USD were spent and 61,310 images were annotated correctly (amounting to 289.6 hours of work) by 155 par-ticipants from 9 countries (48.4% of them coming from the US and 47.1% from India). We considered only results from participants that completed at least one batch.

The main results are the following:
The exp-med setup also attracted more participants: 61 workers completed at least one batch, in comparison to 45 for the baseline. The mentioned differences between the methods are further illustrated in Figure 8 showing the cu-mulative number of correctly matched faces for both experi-ments. Figure 7 provides additional insights on the temporal dynamics of the top-10 performers. Similar to the captcha scenario we observe a large number of  X  X rossings X  as well as simultaneous bursts of activities in the first 36 hours of the exp-med experiment, indicating a vivid competition in contrast to the baseline scenario. 1k 2k 3k Matched Faces Figure 8: Cumulative number of recognized faces over time. observed variations in the number of participants per exper-iment. These numbers are shown in Figure 9 in chronological order. At the very beginning we observe a high amount of participants, which might be explained by curiosity of work-ers regarding the new reward models which are not common in Mechanical Turk. Most of the experiments (3-12) show rather moderate variation in the number of participants. We also observe that some of the experiments were exceptionally attractive for users (2 and 13 are both exp-med for captchas with a 50 USD budget, 14 is exp-med for captchas with a 100 USD budget, and 15 is exp-med for face recognition with a 50 USD budget). Finally No. 16 and 17 were the baseline experiments (for captchas and face recognition, respectively) which attracted a comparable number of workers.
 quite positive and the communication was very friendly. The tasks were described with terms like  X  X nteresting X  or  X  X ind exercise X  but also as challenging in the case of the captcha task. Multiple users commented favor-No. of Workers Figure 9: Participation over the course of the experiments. a bly on competitions ( "i love this game thank you very much [...]" , "it was good experience" ).

There were also critical responses, with some users having concerns about the reward mechanisms, reward amount, or duration of the competitions ( "is this trick for get-tin free of cost captcha solvers [...]" , "you are giving very little amount" , "i want duration 1 or 2 hour only. one week is not possible" ). In such cases the users did not participate further in the competitions. There were also a number of technical suggestions, ranging from requests for a mobile application to more subtle changes in the reward distributions.

In our experiments we noticed some workers dominat-ing the competitions for multiple consecutive rounds (com-ment from another worker: "So...you are saying that the same person won again??" ). In the preliminary rounds, concerns were raised regarding workers sharing the same ac-count, leading to an unfair advantage. We reacted to this by prohibiting simultaneous logins altogether, although it was still possible to take turns annotating using the same account. However, collaborations between workers are not necessarily negative and explicitly leveraging them opens promising avenues for future research.
W e have studied various alternative reward distribution strategies for crowdsourcing compared to the commonly used  X  X ay-per-task X  X pproach. These strategies are based on com-petitive as well as randomized aspects, and build on ideas borrowed from inducement prizes, lotteries, and games of luck. Our evaluation shows substantial performance boosts in both examined crowdsourcing scenarios compared to the baseline: Our best approach results in three times as many annotations than for the baseline within the same time frame and with a price per captcha reduced by a factor of almost 2.5 for the captcha translation task, and about 40% more annotations per dollar for the face recognition task. We also found that the amount of information revealed about other workers during the competitions plays a crucial role: Our results indicate that the sweet spot (a  X  X edium X  informa-tion policy) lies between a restrictive policy (no information about other workers revealed) and an open policy (all of the relevant information revealed). An in-depth analysis of the characteristics of individual workers sheds additional light on motivational and competitive aspects.

In our future work we aim to explore the influence of pa-rameters such as exact amount of prizes, duration of com-petitions, and amount information about fellow workers re-vealed. Furthermore, we plan to study quality insurance mechanisms in the context of competitive crowdsourcing. To this end, we plan to explore and compare different strategies based on honeypots, redundant inputs from different work-ers, and trust models for workers. Finally, we aim to study strategies for coping with worker fatigue -a phenomenon that might be further amplified in scenarios where compe-tition among workers creates additional pressure and stress. In order to tackle these problems we want to explore different mechanisms based on the introduction of strategic breaks, small guaranteed rewards, and task diversification. This work is partly funded by the European Research Council under ALEXANDRIA (ERC 339233) and by the European Commission FP7 under CUBRIK (grant agree-ment No. 287704). [1] GamifIR  X 14: 1st International Workshop on [2] TREC Crowdsourcing Task 2013. [3] O. Alonso and R. Baeza-Yates. Design and [4] O. Alonso and S. Mizzaro. Using crowdsourcing for [5] N. Archak. Money, glory and cheap talk: Analyzing [6] N. Archak and A. Sundararajan. Optimal design of [7] R. Cavallo and S. Jain. Efficient crowdsourcing [8] R. Cavallo and S. Jain. Winner-take-all crowdsourcing [9] L. E. Celis, S. Roy, and V. Mishra. Lottery-based [10] D. DiPalantino and M. Vojnovic. Crowdsourcing and [11] C. Eickhoff, C. G. Harris, A. P. de Vries, and [12] J. He, M. Bron, L. Azzopardi, and A. de Vries. [13] H. Jiang and S. Matsubara. Improving crowdsourcing [14] J. Joukhador, A. Blaszczynski, and F. Maccallum. [15] G. Kazai. In search of quality in crowdsourcing for [16] G. Kazai, J. Kamps, and N. Milic-Frayling. Worker [17] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. [18] D. Lam. An exploratory study of gambling [19] W. Mason and D. J. Watts. Financial incentives and [20] D. Pothineni, P. Mishra, A. Rasheed, and [21] J. P. Rula, V. Navda, F. E. Bustamante, R. Bhagwan, [22] N. Savage. Gaining wisdom from crowds. Magazine [23] K. Stoddart. Behind the scenes of crowdsourcing: [24] J. Surowiecki. The Wisdom of Crowds . Anchor, 2005. [25] L. von Ahn and L. Dabbish. Designing games with a [26] L. von Ahn and L. Dabbish. Labeling images with a [27] P. Welinder and P. Perona. Online crowdsourcing: [28] J. Yang, L. A. Adamic, and M. S. Ackerman.
 [29] M.-C. Yuen, I. King, and K.-S. Leung. A survey of
