 signals [2]. Corresponding spike-based learning rules of t he form have been proposed in [3], where w (1), where the eligibility trace c through computer experiments, see 5.1.
 work by Seung and others). The spike train of a neuron i which fires action potentials at times t (1) by a sum of Dirac delta functions S two integrals in (2)) are collected in an eligibility trace c for s  X  0 : c ji ( t ) = In our simulations, f time constant  X  where the positive constants A  X  S i , S We assume that weights are clipped at the lower boundary valu e 0 and an upper boundary w We use a linear Poisson neuron model whose output spike train S post process with the underlying instantaneous firing rate R taneous firing rate by an amount w for excitatory connections and assume that w the response kernel is normalized to R  X  inputs are summed up linearly: where S realization of the reward signal, denoted by the ensemble av erage hi where we used the abbreviation h f ( t ) i where D time t  X  s , and  X  In the following, we will always use the smooth time-average d vector h w pre spike pairs, since if a reward signal arrives at some time d will be proportional to f recurrent network with some delay d action potential where  X  forced neuron. We assume that the reward kernel  X  neglect the impact of the presynaptic spike and write given by d dt 4.1 Computer simulations within the recurrent circuit were randomly connected with p robabilities p p ie = 0 . 096 synapses which are plastic, instead of a static noise proces s. The function f had the form f The parameter values for  X  delayed 0.5s from the spike that caused it, and a long tailed n egative bump so that R  X  for 20 min simulated biological time with a simulation time step of 0 . 1 ms. in [1]. on how well the output spike train S post with arbitrary weights w  X  = ( w  X  spike trains S its weights w them receive. The reward d ( t ) at time t is given by where the function  X  ( r ) with  X   X  = R  X  on the time difference between a postsynaptic spike and a tar get spike and d synapse i is (see [6]) D ji ( t, s, r ) = h S slowly, we have w t assumed stationary,  X  if linear Poisson neuron model, the weight change at synapse ji is given by where  X  f  X  W  X  = R We will now bound the expected weight change for synapses ji with w  X  w i = 0 is small. We have to guarantee some minimal output rate  X  post w vector converges on average from any initial weight vector t o w  X  . are depressed while the third condition (21) ensures that we ights with w  X  = w kernel should be such that the integral R  X  (but positive). Hence,  X  experiments, we use a simple kernel which satisfies the afore mentioned constraints: where A  X  exponential functions the kernel is composed of, and t somewhat later, and negative if the neuron spikes much too ea rly. 5.1 Computer simulations that should converge to w  X  converge to w  X  target spike train without the additional noise inputs. A B had 10 additional excitatory input connections (these weights we re set to w were set to be plastic. The target neuron had a weight vector w ith w  X  w The reward was delayed by 0 . 5 s, and we used the same eligibility trace function f w additional simulations where the same spiking input S after we conducted the learning experiment (results are rep orted in Fig. 2B). generic cortical microcircuit models.
 S9102 and project # FP6-015879 (FACETS) of the European Union.
