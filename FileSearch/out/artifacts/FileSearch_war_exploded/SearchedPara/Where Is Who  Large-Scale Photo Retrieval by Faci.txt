 The ubiquitous availability of digital cameras has made it easier than ever to capture moments of life, especially the ones accompanied with friends and family. It is generally believed that most family photos are with faces that are sparsely tagged. Therefore, a better solution to manage and search in the tremendously growing personal or group pho-tos is highly anticipated. In this paper, we propose a novel way to search for face photos by simultaneously considering attributes (e.g., gender, age, and race), positions, and sizes of the target faces. To better match the content and layout of the multiple faces in mind, our system allows the user to graphically specify the face positions and sizes on a query  X  X anvas, X  where each attribute combination is defined as an icon for easier representation. As a secondary feature, the user can even place specific faces from the previous search re-sults for appearance-based retrieval. The scenario has been realized on a tablet device with an intuitive touch interface. Experimenting with a large-scale Flickr 1 dataset of more than 200k faces, the proposed formulation and joint ranking have made us achieve a hit rate of 0.420 at rank 100, sig-nificantly improving from 0.036 of the prior search scheme using attributes alone. We have also achieved an average running time of 0.0558 second by the proposed block-based indexing approach.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation; H.5.2 [ User Interfaces ]: Input devices and strate-gies
All of the face images presented in this paper except for those by Google Image Search in Fig. 2 (b) and Fig. 4 attribute to various Flickr users under a Creative Commons License.
 Figure 1: Example queries and top 5 retrieval results from our photo search system. (a) specifies two ar-bitrary faces with the larger one on the left and the smaller one on the right. (b) further constrains that the left face has attributes  X  X emale X  and  X  X outh X  and the right face has attribute  X  X id. X  (c) specifies two faces of  X  X ale X  and  X  X frican X  on the left and right, in addition to an arbitrary face on the center. (d) specifies a particular face in the database at the de-sired position and in the desired size. (e) specifies the previous database face on the left, and a face of  X  X emale X  and  X  X outh X  on the right.
 Face attributes, Face retrieval, Touch-based user interface, Block-based indexing
The ubiquitous availability of digital cameras has made it easier than ever to capture moments of life, especially the ones accompanied with friends and family. It is generally believed that most family photos are with faces that are sparsely tagged. Therefore, a better solution to manage and search in the tremendously growing personal or group photos is highly anticipated.

Psychology research in perception shows that images with certain kinds of subjects attract more attention of the eyes [8]. Among these subjects, human faces are the most mem-orable, followed by images of human-scale space and close-ups of objects [11]. The phenomena becomes more obvious in consumer photos because most of them contain family members or close friends that the user cares about and usu-ally keeps in mind. Therefore, they are able to make use of the face content and the face layout that they remember to effectively formulate their search intentions. Furthermore, viewing the retrieved images probably recalls more scenes in the user X  X  memory, so they expect to be able to refine their query interactively. For example,  X  X iewing a photo of Alice standing next to me, it reminds me of another photo with an African kid sitting in the middle of us. X  Although consumer photos generally lack annotations, automatic face analysis techniques would make the scenario economical and scalable.

In this paper, we propose a novel system for searching consumer photos by automatically analyzing  X  X ild photos X  (without tag information at all) through facial attribute de-tection (Sec. 4.1) and appearance similarity estimation (Sec. 4.2). To better match the content and layout of the multi-ple faces in mind, rather than laboriously sketching detailed outline or typing text, our system allows the user to graphi-cally specify the face positions and sizes on a query  X  X anvas, X  where each attribute combination is defined as an icon for easier representation. The query can be simply finding ar-bitrary faces in the desired layout (Fig. 1 (a)), or further constrained by facial attributes (Fig. 1 (b) and (c)). As a secondary feature, the user can even place specific faces from the previous search results for appearance-based re-trieval (Fig. 1 (d)), combined with other attributed faces (Fig. 1 (e)). Other complicated search intentions also ap-ply.

The scenario has been realized on a tablet device with an intuitive touch interface where the user can easily refine their query by interacting with the real-time search results. To provide effective matching in a large-scale Flickr dataset of more than 200k faces, the proposed formulation and joint ranking have made us achieve a hit rate of 0.420 at rank 100, significantly improving from 0.036 of the search scheme proposed by [15] using attributes alone. To provide effi-cient retrieval, we have also achieved an average running time of 0.0558 second by the proposed block-based indexing approach. The numbers are scalable to even larger photo collections.

The contributions of this paper are as follows:
In this section, we review various query formulations and query modalities in image search systems and their applica-bilities to face photo search. Fig. 2 is an illustration of such a scenario. The target image in the user X  X  mind (Fig. 2 (a)) is a boy X  X  face on the left and a larger girl X  X  face on the top right of it. The user vaguely remembers the face content and layout, but not the exact image file in the collection.
Existing commercial image search engines mostly rely on matching the query keywords with the surrounding text or manual tags of the target images. Fig. 2 (b) is obtained by Google Image Search using the keywords  X  X oy girl X  with advanced options of searching only face images. Directly matching text not only reveals little about the image con-tent, but in this particular case, it also happens to match the movie title  X  X t X  X  a Boy Girl Thing X  and retrieves some irrelevant images in the scene. What X  X  worse, tags are often inaccurate, incorrect, or ambiguous [12]. Due to the com-plex motivations behind tag usage [2], tags do not necessarily describe the content of an image [13].

In content-based image retrieval, Kumar [15] proposes fa-cial attribute classification by SVM and AdaBoost, and uses the confidence scores for image retrieval. Fig. 2 (c) is pro-duced in a similar way by enabling only the attribute modal-ity in our system. The corresponding attributes specified are  X  X ale + kid X  for boy and  X  X emale + kid X  for girl. While the attributes (especially the age) are mostly correct, this ap-proach does not consider the face layout in the user X  X  mind at all. On the other hand, Fig. 2 (d) is produced by en-abling only the position and size modalities in our system. While the face layouts are highly relevant due to accurate face detection, this approach does not consider about the face content. To utilize both the content and layout infor-mation, Fig. 2 (e) is produced by the full version of our system that combines all of these three modalities. The re-sults in Fig. 2 (e) best match the user X  X  search intention in terms of finding highly relevant (squared in green solid lines) and partially relevant (squared in blue dashed lines) images. The above illustration shows the power of multimodal fusion in retrieval systems.

Some efforts also attempt to capture the user X  X  search in-tention by visually describing both the image content and layout on a query canvas. Thanks to the growing popu-larity of touch devices, it has become more intuitive and convenient than ever to formulate such queries. [3] revisits the problem of sketch-based image search for scene photos. However, the gap between the user X  X  mind and their specified query can still be large even in such a system. For instance, users with poor drawing skills may have a hard time de-scribing their intention accurately. In addition, some object details are naturally difficult to sketch, and many concepts are even more difficult to describe by sketching, such as the age of a face. Therefore, the practicability of sketch-based retrieval for photo management is questionable, especially for face photos.

To deal with this sketching difficulty, [19] allows the user to formulate a 2-D  X  X emantic map X  by placing text boxes of various search concepts at desired positions and in desired sizes. However, it is intended for generic objects, not for faces of different individuals. To apply to face photo man-agement, [14] also allows the user to specify face positions, and face sizes on a canvas. These faces are further described by tagging names and even drawing social relationships [17]. However, non of these efforts proposes an efficient indexing method for large-scale photo retrieval. Meanwhile, typing text is not the most intuitive operation on touch devices even though these efforts aim for better user experience.
Specifically for photo management, some commercial ser-vices (e.g., Picasa [18] and iPhoto [10]) that exploit face recognition technologies to help face annotation in a semi-supervised or supervised manner have been shown promis-ing. However, people are mostly reluctant to annotate their photos, especially when photos are taken enormously due to the ubiquitous availability of digital cameras. Also, many of consumer photos are group photos, which makes the face annotation task even more tedious. In our work, we further consider spatial layout, attributes, and appearance for face Figure 3: An overview of our proposed system. Pho-tos are analyzed offline through face detection, facial attribute detection, and sparse coding for appear-ance similarities. The results are incorporated into the proposed block-based index and codeword index for efficient retrieval. photo retrieval. We believe it can be complementary existing face annotation solutions.
Fig. 3 is an overview of our proposed system named  X  X here is Who. X  In the offline process, the image database first goes through face detection to identify and locate frontal faces in the images. These faces are then analyzed through facial attribute detection (Sec. 4.1) and sparse coding for appearance similarities (Sec. 4.2). Finally, the attribute scores along with the position and size information are in-corporated into the block-based index (Sec. 5.4). The sparse codes of faces are also stored in the codeword index. These indices are pre-loaded for rapid online response. In the on-line process, the server retrieves candidate images in inverted lists, ranks them by relevance (Sec. 5.2), and returns the search results back to the user. Note that appearance-based retrieval is treated as a secondary feature and is not evalu-ated throughout this paper.
Facial attributes possess rich information about people and have been shown promising for seeking specific persons
Face recognition or face annotation information can be ex-ploited as another source of the  X  X ace content X  considered in this work.
 Table 1: The 3 attribute types and 8 corresponding attributes detected in our system.
 Figure 4: For each of the four face components (whole face, eyes, nose, and mouth), four low-level features (HoG, grid color moments, Gabor, and LBP) are extracted. Each of 16 combinations (e.g., &lt; mouth, LBP &gt; ) is treated as a mid-level feature for which an SVM is learned. in face retrieval and surveillance systems. In this work, we utilize 8 facial attributes (Table 1) including 2 of gender (male, female), 3 of age (kid, youth, elder) and 3 of race (Caucasian, Asian, African) to profile faces in large-scale photos.

In the training phase, each attribute classifier is learned separately through a combination of Support Vector Ma-chines (SVMs) and Adaboost [9] similar to [15]. Firstly, we crawl user-contributed photos from Flickr and extract facial regions by a face detector. The face images are annotated manually with positive and negative class labels. As illus-trated in Fig. 4, the faces are then automatically decom-posed into four different face components, i.e., whole face, eyes, nose, and mouth. From each of these components, four low-level features, i.e., histogram of oriented gradients (HoG) [6], grid color moments, Gabor filter, and local binary patterns (LBP) [1] are extracted.

A mid-level feature learned is an SVM with a specific low-level feature extracted from a specific face component, e.g., an SVM for &lt; mouth, LBP &gt; . Finally, the optimal weight-ing of the 16 (4  X  4) mid-level features for this attribute is determined through Adaboost. The combined strong clas-sifier represents the most important parts of that attribute. For example, &lt; whole face, Gabor &gt; is most effective for the female attribute while &lt; whole face, color &gt; is most effective for the African attribute.

Experimenting with the benchmark data [15], the approach can effectively detect facial attributes and achieve an accu-racy of more than 80% on average. Meanwhile, the training framework is generic for various cases thus providing a po-tential to extend to more attributes 3 .
To enable search through face appearance, we adopt the face retrieval framework of [5]. The advantage of this frame-work includes: (1) efficiency, which is achieved by using sparse representations of face image with inverted indexing, and (2) leveraging identity information, which is done by incorporating the partially-tagged identity information into the optimization process of codebook construction. Both of the above two points are suitable for our system. In de-tails, detected faces are first aligned into canonical position, and then component-based local binary patterns [1] are ex-
For example, the work of [16] has trained as many as 73 attribute classifiers.
 Figure 5: The image ranking problem as a maximum weighted bipartite matching between the query can-vas (set Q ) and the target image (set T ). The num-bering in the query canvas implies the order in which the faces are specified. The optimization in Eq. 1 can be carried out by (a) the optimal solution or (b) the greedy approximation. A red cross indicates a mismatched face, and match ( Q,T ) means the overall matching score between Q and T . tracted from the images to form feature vectors. After fea-ture extraction, sparse representations are computed from these feature vectors using an L1-regularized least square objective function. Non-zero entries of sparse representa-tions are considered as visual words for inverted indexing.
Due to the nature of faces, images of the same individ-ual may have high intra-class variation. To leverage the partially-tagged identity information, a regularization term is added to the objective function to force images of the same identity (tag) to have similar sparse representations. These images will propagate visual words to each other, and the query image will be able to find all images of the same individual if it is similar to at least one of them.
By incorporating such framework into our system, in ad-dition to attributes, the user can also use a face image itself as the face content.
As illustrated in Fig. 5, for a (query canvas, target image) pair, denoted as ( Q,T ), the image ranking problem is formu-lated as a maximum weighted bipartite matching between the two sets Q and T . The objective function match ( Q,T ), or the overall matching score between Q and T , is defined as the sum of the individual face matching scores match ( q,t ) (defined in Sec. 5.2) divided by max( | Q | , | T | ). The formu-lation is as the following constraint optimization problem: match ( Q,T ) = where  X  ( q,t ) (Eqs. 2a and 3) is an indicator variable of whether ( q,t ) is matched.

Note that the matching ensures each query face q = q 1 ,...,q matches at most one target face t = t 1 ,...,t | T | (Eq. 2b), and each t is matched at most once (Eq. 2c). We add the sub-scripts here to explicitly denote the individual faces in Q and T .

The numerator of Eq. 1 is the objective function in max-imum weighted bipartite matchings. Note the max( | Q | , | T | ) in the denominator. The positive weights (Eq. 2d) ensure that the number of matching pairs equals min( | Q | , | T | ). If the numbers of faces in Q and T are the same, dividing by | Q | or | T | is like averaging. But if | Q | and | T | are different, the overall matching score will be divided by a larger num-ber. Thus, this formulation much favors target images that have the same number of faces as the query canvas.

Fig. 5 shows an illustration of matching 4 query faces with 3 target faces. The optimal solution, by the above formula-tion, always comes up with the highest match ( Q,T ) among all the possible matches. As in Fig. 5 (a), match ( Q,T ) = 0 . 68 for this example. However, computing the optimal so-lution (e.g., by the Bellman-Ford algorithm) is inefficient if we have to repeat for all target images.
The inefficiency in solving Eq. 1 can be compromised by the proposed greedy approximation. By greedy, we mean the first query face q 1 is the first to match by choosing the best matching face remaining (i.e., the unmatched t which match ( q 1 ,t  X  ) is maximized), followed by q 2 , q The numbering in Q implies the order in which the faces are specified on the query canvas. The procedure is summarized in Algorithm 1.

In the example of Fig. 5 (b), the greedy approximation allows the first query face q 1 to match first, choosing t get face matching score of 0.73. The second query face q chooses t 1 of 0.96, followed by q 3 choosing the last target face t 3 remaining to get 0.91. q 4 then becomes a mismatched face (indicated by a red cross in Fig. 5 (b)), but it could have matched t 2 of 0.85 if specified in the first place. Even-tually, match ( Q,T ) = 0 . 65 in the greedy approximation. Algorithm 1 The procedure in greedy approximation.
 Input: The query canvas Q and the target image T .
 Output: The overall matching score match ( Q,T ). match ( Q,T )  X  0 /* Maintain a remaining set R . */
R  X  T for q  X  q 1 ,q 2 ,...,q | Q | do end for match ( Q,T )  X  match ( Q,T ) / max ( | Q | , | T | ) In general, although the greedy approximation has a lower match ( Q,T ) than the optimal solution, it significantly re-duces the computational cost and reflects the idea that the first face coming to the user X  X  mind is the most important.
Our work uses multimodal fusion to determine the face matching score match ( q,t ) between a query face q and a target face t . It is defined as a linear combination of the matching scores for facial attributes, appearance similarity, face position, and face size: where w attr , w app , w pos , and w size are the weights for these four modalities.

The first term in Eq. 4 weights the geometric mean of the matching scores Attr ( q  X  ,t  X  ) for all of the attribute types  X  , i.e., gender, age, and race ( |  X  | = 3). As in Eq. 5, if q or the attribute specification of the query face for type  X  , is some attribute k , then Attr ( q  X  ,t  X  ) = t  X k , the attribute score of k in type t . For instance, if q specifies the age  X  X outh X , then Attr ( q age ,t age ) takes the attribute score for youth of t . In notation, if for  X  = age, q age = youth, then In contrast, if for  X  , the attribute is not specified, then Attr ( q  X  ,t  X  ) = 1 . 0, the perfect score. The choice of geometric means rather than arithmetic means is to avoid outliers for some attribute type. The second term in Eq. 4 weights the appearance similarity score between q and t , obtained in Sec. 4.2. Note that in our user interface (Sec. 6.1), attributes and appearance similarity (by a specific face instance) of a query face are not specified at the same time.
The real-valued scores of each of the four query modal-ities, that is, attributes, appearances, positions, and sizes, are normalized into the range (0 , 1) for late fusion. 0 and 1 represent the worst score and the best score of a modality.
For an attribute score t  X k , we first normalize the strong classifier X  X  output to zero mean and unit variance for each attribute k . Then we apply a sigmoid function to map it to Figure 6: Quantization of ( x 0 ,y 0 ,w 0 ,h 0 ) into overlap-ping blocks of various positions and sizes, where the four variables represent the already quantized hori-zontal and vertical positions, width, and height. The mapping between a ( x 0 ,y 0 ,w 0 ,h 0 ) combination and a block ID should be unique throughout the system. (0 , 1). The appearance similarity scores App ( q,t ) are nor-malized in a similar way.

For the matching scores for face position Pos ( q,t ) and face size Size ( q,t ) between a query face q and a target face t , first note that in our system, coordinates are always represented as fractions of the width or height of the image (canvas). This fractional representation allows the computation to be adapted to the various aspect ratios in the the target images (query canvas). The definitions of Pos ( q,t ) and Size ( q,t ) are based on the distance errors between q and t as follows: where d center is the L2 distance between the face centers, and d width and d height are the L1 differences between the face widths and heights. The denominators 6 and 7 indicate the maximum (worst) distance between the face centers and the maximum width plus height differences between the faces, i.e., the diagonal line and the whole width plus whole height. Therefore, each term subtracted from 1 is now normalized into the range (0 , 1).
We apply a block-based method to spatially index all the database faces. Since the face center coordinates, width and height, denoted as x , y , w , and h , are fractions, the infinitely many numbers in the interval (0 , 1) make indexing compu-tationally infeasible and quantization too sensitive. There-fore, we first uniformly quantize each of the four variables into L levels, denoted as x 0 , y 0 , w 0 , and h 0 , each in the range [0 ,L  X  1]. We then quantize the valid ( x 0 ,y 0 ,w 0 ,h nations uniquely into overlapping blocks of various positions and sizes, as illustrated in Fig. 6. Note that not all the L combinations are valid (within-boundary) blocks. The map-ping between an ( x 0 ,y 0 ,w 0 ,h 0 ) tuple and a block ID should be unique throughout the system. One such mapping is easily achieved by representing the block ID as an L -nary number of 4 digits. For example: The ordering of digits does not matter as long as it is con-sistent. The mapping 4 in Eq. 8 is not only unique but also reversible and storage-free (no table lookup). As an example, suppose L = 20 levels, each being 0.05. Figure 7: The indexing structure in the proposed system. The block IDs are treated as the visual words in typical inverted indexing. Each of them corresponds to an inverted list of structures, each being a tuple of (image ID, the 8 attribute scores) that requires 36 bytes in the implementation.

To build the index, the block IDs are treated as visual words in typical inverted indexing. Each block then corre-sponds to an inverted list of structures, each being a tuple of (image ID, the 8 attribute scores), as in Fig. 7. In other words, each list contains all the faces and their attribute scores within this particular block.

Since retrieving only faces in the block of the query face is still too sensitive, in the online search, a query face runs a  X  X liding window X  to retrieve faces in W neighboring blocks. These neighbors are found by adjusting each of the ( x 0 ,y up and down for various quantization levels to produce new combinations. An example neighbor may be ( x 0  X  2 ,y 1 ,w 0 + 3 ,h 0 ). Then, we apply the mapping in Eq. 8 to get the neighboring block IDs and retrieve the corresponding inverted lists. The range of the sliding window, denoted by parameters tol pos and tol size , controls the level of tolerance in positions and sizes.

For multiple-face queries, each query face is processed sep-arately to collect relevance scores from inverted lists accord-ing to Eq. 4. The greedy manner still applies that the first query face scans the inverted lists first. Finally, the results are merged into a ranking list according to Eq. 1. The re-trieval results of block-based indexing and linear scan differ mostly by the quantization errors and the faces skipped by the sliding windows.
In this section, we describe the touch-based user interface of the proposed system named  X  X here is Who X  (short for WiW), followed by the dataset and implementations. We also conduct an estimation on storage cost. For a video demonstration of the system, please visit our project page: http://www.csie.ntu.edu.tw/~winston/projects/face/
The user interface of our system is shown in Figure 8. The user can drag faces from the top-right area onto the canvas An ( x,y,w,h ) combination of (0.11, 0.28, 0.42, 0.67) will be quantized into ( x 0 ,y 0 ,w 0 ,h 0 ) = (2, 5, 8, 13). The block ID is then 2 + 5  X  20 + 8  X  20 2 + 13  X  20 3 = 107302. The reverse mapping can restore ( x 0 ,y 0 ,w 0 ,h 0 ) from the block ID. Figure 8: The touch-based interface of our system. The user can formulate a query by dragging face icons from the top-right area onto the canvas at de-sired positions. They can also pinch their fingers to adjust the sizes of the icons and the canvas. When holding an icon, a pop-up menu will show up for at-tribute selection. When browsing the search results on the bottom, they can also hold a face and use the changed icon on the top-right to find similar faces (appearance-based) in other photos. For every can-vas modification, the system performs a search so that the user can refine their search intention inter-actively. at desired positions. They can also pinch their fingers to adjust the sizes of the icons and the canvas. Holding an icon invokes a popup attribute selector. We have designed a total of 48 face icons (3  X  4  X  4) to represent the various attribute combinations. For appearance-based retrieval, the user can hold a face in the result panel and use the changed icon on the top-right to find similar faces in other photos. For every canvas modification, the system performs a search and shows the results on the bottom so that the user can refine their search intention interactively viewing the current results. Since our system is naturally suitable for a touch-based interface, we have implemented the UI on a tablet device. The dataset is composed of two portions. As mentioned in Sec. 4.1, we crawl a large number of user-contributed pho-tos from Flickr as the first (main) portion. For appearance-based retrieval, 732 daily photos containing 1,248 faces are added to the dataset as the second portion. After face de-tection by a public API [7], together there are N = 115 , 487 images in the dataset where the average number of faces per image is F = 2 . 117, so the dataset contains N  X  F = 244 , 491 faces.

Since appearance-based retrieval is intended as a secondary feature of our system, we only estimate the appearance sim-ilarity scores in the second portion. Therefore, faces in the first portion always have zero appearance similarity scores if they are specified on the canvas.

In attribute detection, we adopt the LIBSVM software package [4] for learning the mid-level features. For the fu-sion weights in Eq. 4, we conduct a sensitivity test to se-lect w attr , w pos , and w size (that sum to 1) to optimize the evaluation criterion in Sec. 7.2. For block-based indexing, we empirically select the number of quantization levels as L = 20, and the tolerance (range) of the sliding window as tol pos =  X  4 levels and tol size =  X  4 levels 5 .

The server part of WiW is implemented on a 16-core, 2.40GHz Intel Xeon machine with 48GB of RAM.
Since appearance-based retrieval is considered as a sec-ondary feature, the storage cost of codeword index is not considered in this estimation. Following the format of the index structure in Fig. 7, for an inverted list structure, we require 4 bytes for an image ID and 4  X  8 = 32 bytes for the eight floating-point attribute scores. That is, 36 bytes for indexing a face. The cost of headers (block IDs and counts) can be neglected in the calculation. Multiplied by N  X  F , it requires approximately 244 . 5 K  X  36 B = 8 . 8MB in opti-mal implementations. Reusing F = 2 . 117 in our dataset, an 1-million image dataset requires a storage cost of around 1 M  X  2 . 117  X  36 B = 76 . 2MB.
In this section, we evaluate the performance of several variants of our proposed system. We have conducted an ex-periment to evaluate known-item search, in which the user tries to search for a specific target image in mind. Since appearance-based retrieval is treated as a secondary feature, refer to [5] for the corresponding evaluation. We also evalu-ate the efficiency of indexing by measuring the running time and the number of visited faces.
To the best of our knowledge, our system is the first work to address the problem of face image retrieval based on both facial attributes and face layout. So we compare four vari-ants of the proposed system: (1)  X  X ttr, X  by enabling only w attr in Eq. 4 with linear scan in order to resemble the search scheme in [15], (2)  X  X os + Size (index), X  by enabling w pos and w size with block-based indexing (Sec. 5.4), (3)  X  X ttr + Pos + Size, X  by enabling w attr , w pos , and w with linear scan, and (4)  X  X ttr + Pos + Size (index), X  same as (3) but with block-based indexing. (4) is the full version of WiW except for the appearance-based component.
Therefore, a sliding window visits W = (4  X  2 + 1) 2  X  (4  X  2 + 1) 2 = 6 , 561 neighboring blocks. Many of the blocks may be out-of-boundary or empty.
 Table 2: Distribution of the number of faces in the 500 query tasks. Figure 9: Hit rates@K of different methods over the 500 query tasks for known-item search. Adding layout information achieves a hit rate of 0.420 at rank 100 (purple line). This significantly outper-forms 0.036 of using attributes alone (blue line), the search scheme proposed by [15]. In addition, adding attribute information improves the hit rate from 0.320 (red line) to 0.420 (purple line).
In known-item search (KIS), the user aims to search for a specific target image that they have seen. To simulate such a scenario in a large-scale dataset, 500 target images, each containing at least one face (985 faces in total), were ran-domly selected from our dataset (portions 1 and 2) as query tasks. The distribution of the number of faces is summa-rized in Table 2. These query tasks were equally distributed among the participants of 20 subjects invited to the experi-ment.

For each query task, the subject was asked to first care-fully observe the target image, and then formulate a query canvas by graphically placing attributed icons at the corre-sponding positions and in the corresponding sizes for each query face. The subjects were asked to specify the posi-tions and sizes according to the bounding boxes detected by the system in order to minimize the effect of face de-tection errors. The attributes were specified according to their  X  X trengths X  to the subject. If either the gender, age, or race of the face was not obvious enough, the attribute would be  X  X ot specified X  for this type. Finally, the 500 submitted query canvases were collected for later evaluations.
Although this simulation does not reflect the reality that the user may not accurately remember the face layout or the face content in a large image collection over a long time, our user interface makes it easy to gradually refine the canvas by providing a real-time re-query for every canvas modification. This is useful in reality because the user usually performs several trials in the same way in typical retrieval systems.
To evaluate how well the target image is ranked in the results, we measure the  X  X it rate@K X  as in [3] 6 , the propor-
In KIS, the performance is often measured by mean recip-rocal rank. However, because there may be numerous other images with similar face content and face layout, especially images of 1 or 2 faces (Table 2), many of our target images Figure 10: The fusion weight selection to maximize hit rate. The three axes represent w attr , w pos (in Eq. 4), and hit rate@100, respectively. The non-negative weights are constrained by w attr + w pos + w size = 1 . tion of the 500 query tasks where the system can retrieve the target image within the top K search results (within rank K).

Fig. 9 shows the performance of the four compared meth-ods for all query tasks. Apparently, all three methods con-sidering face layout significantly outperform  X  X ttr, X  achiev-ing hit rates of 0.320, 0.428, and 0.420 at rank 100 are 8.8 to 11.8 times higher than 0.036 (blue line) of using attributes alone, the search scheme proposed by [15]. This clearly explains that when the user has the face layout in mind, specifying the positions and sizes on a canvas pro-vides much more information than specifying only the face content.

Also, the hit rate of  X  X ttr + Pos + Size (index) X  (purple line) is slightly lower than its linear-scan variant  X  X ttr + Pos + Size X  (green line). This is due to the quantization errors introduced by the block-based indexing, where the exact positions and sizes are quantized into nearby blocks.
We can also observe in Fig. 9 that  X  X ttr + Pos + Size (index) X  outperforms  X  X os + Size (index). X  In other words, adding attribute information can further improve the hit rate@100 from 0.320 (red line) to 0.420 (purple line), al-though in the fusion weight selection (Fig. 10), the contri-bution of w attr is only 5% of the total weight. The small weight can be explained by the fact that attribute detection is less robust than face detection and localization.
As reported in Sec. 4.1, a single attribute detector has an average accuracy of around 80%, but when three at-tributes are specified in a query face, we can expect only (0 . 80) 3 = 51% of the target faces to have all correctly de-are ranked up to number several thousand. Averaging by those near-zero reciprocal ranks would make it difficult to compare different methods.
Although a hit rate of 0.420 at rank 100 may not be high enough for practical photo management, the images returned by the system are often relevant to the query can-vas, as illustrated in Fig. 13. This high precision enables casual photo browsing when the user does not have a specific target in mind. Figure 11: Breakdown of the improvement of hit rate@100 from 0.320 to 0.420 (Fig. 9) by enabling different combinations of attribute types. G, A, and R stand for gender, age, and race, respectively. Starting from 0.320 of no attributes, we can observe that enabling more attribute types improves the hit rate towards the highest 0.420 (G+A+R). tected attributes. This is challenging for a system support-ing multi-attribute queries. Also, in the experiment of KIS, the subjects were instructed to specify the positions and sizes according to the bounding boxes returned by the face detector. This accurate layout information has compromised the contribution of attributes in the multimodal fusion.
Following Sec. 7.2.3, we break down the improvement of hit rate@100 from 0.320 to 0.420 (Fig. 9) by enabling different attribute combinations. In Fig. 11, G, A, and R stand for the three attribute types gender, age, and race, respectively.  X  X  + R, X  for example, enables attributes in faces in the query tasks where the user specified any age attribute or any race attribute.

Starting from 0.320 of no attributes, we can observe that enabling more attribute types improves the hit rate towards the highest 0.420 (G + A + R). Again, this shows the power of multi-modality in retrieval systems. With more attributes available, such as the 73 detected attributes in [16], we can expect such a system to achieve even better performance for practical usage.

It is also interesting to discuss the effect of using some attributes together in a query canvas. Fig. 12 shows the hit rates@100 by simultaneously enabling (hence the  X &amp; X  sym-bol) any gender attribute (G) and one age attribute. A red dot counts the percentage of faces with attributes that meet this requirement.

Generally, the higher the red dot, the more faces with attributes enabled, which is expected to raise the hit rate. However, we can observe that  X  X  &amp; kid X  performs the worst among all alternatives, even worse than  X  X  &amp; elder X  that has fewer faces with attributes. It reflects the intuition that it is relatively hard to tell the gender among kids.
From the 500 query tasks, we also record the average running time and the average number of visited faces, in-cluding repetitive visits, in the search. Table 3 shows the efficiency comparison between linear scan and block-based indexing. In both manners, block-based indexing speeds up Figure 12: Hit rates@100 by simultaneously enabling any gender attribute and one age attribute. A red dot represents the percentage of query faces with such attributes enabled. We can observe that  X  X  &amp; kid X  performs the worst among all alternatives, even worse than  X  X  &amp; elder X  that has fewer faces with attributes enabled. This reflects the intuition that it is relatively hard to tell the gender among kids.
 Table 3: The efficiency comparison between linear scan and block-based indexing, measured by the av-erage running time and the average number of vis-ited faces (including repetitive visits) in the search. Block-based index 0.0558 111,303
Indexing speedup 3.74x 2.98x around 3 times and requires only 0.0558 second in a dataset of more than 200k faces. Although there is still room for improvement (e.g., incorporating attribute scores into the visual words, or better quantization and search methods for the (x,y,w,h) information), we believe the proposed indexing method can be extended to a million-scale dataset.
Our work proposes a novel way to effectively organize and search for consumer photos by placing attributed face icons on a query canvas at desired positions and in desired sizes. With the help of automatic facial attribute detection and ap-pearance similarity estimation in the offline process, we are able to analyze wild photos without tagging at all. In the on-line process, the system simultaneously considers attributes, appearances, positions, and sizes of the target faces.
The scenario has been realized on a tablet device with a touch interface. Experimenting with a large-scale Flickr dataset of more 200k faces, we have achieved a hit rate@100 of 0.420, significantly improving from 0.036 of prior search scheme [15] using attributes alone. We have also achieved a fast retrieval response of 0.0558 second by the proposed block-based indexing approach. Experimental results from extensive search tasks (Fig. 13) reveal the potential for ef-fective and efficient photo management.

In the future work, we will exploit more facial attributes for the proposed search system. We will also include more context cues (e.g., time, geo-locations, etc.) for consumer photo management. Meanwhile, the human factors will be considered more in the integration with mobile devices. [1] T. Ahonen, A. Hadid, and M. Pietikainen. Face [2] M. Ames and M. Naaman. Why we tag: Motivations [3] Y. Cao, C. Wang, L. Zhang, and L. Zhang. Edgel [4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [5] B.-C. Chen, Y.-H. Kuo, Y.-Y. Chen, K.-Y. Chu, and [6] N. Dalal and B. Triggs. Histograms of oriented [7] face.com API. http://developers.face.com/ . [8] M. Freeman. The Photographer X  X  Eye: Composition [9] Y. Freund and R. E. Schapire. A decision-theoretic [10] iPhoto from Apple Inc. [11] P. Isola, J. Xiao, A. Torralba, and A. Oliva. What [12] L. Kennedy, M. Naaman, S. Ahern, R. Nair, and [13] L. S. Kennedy, S.-F. Chang, and I. V. Kozintsev. To [14] H.-N. Kim, A. E. Saddik, K.-S. Lee, Y.-H. Lee, and [15] N. Kumar, P. N. Belhumeur, and S. K. Nayar.
 [16] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. [17] K.-S. Lee, J.-G. Jung, K.-J. Oh, and G.-S. Jo. [18] Picasa from Google Inc. http://picasa.google.com . [19] H. Xu, J. Wang, X.-S. Hua, and S. Li. Image search
