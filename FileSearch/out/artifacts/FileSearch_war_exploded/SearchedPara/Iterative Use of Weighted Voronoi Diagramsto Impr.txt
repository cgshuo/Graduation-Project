 Recommender Systems (RS) have been developed to address the problem of information overloading, where people face difficulties in finding their required information from an overwhelming set of choices. Collaborative Filtering (CF) [ 1 , 13 ] is one of the most widely studied and widely adapted techniques behind rec-ommendation algorithms. It tries to recommend items to users based on user-user or item-item similarities computed from existing data, often in the form of ratings given by users. Existing CF methods based on correlation criteria [ 9 ], non-negative matrix factorization (NNMF) [ 6 ] and singular value decomposition (SVD) [ 10 ] typically predict ratings accurately. However, these CF techniques suffer from high computational complexity. As a result, the methods do not scale well on very large datasets. To address this scalability problem, we pro-pose a decomposition based CF algorithm. Our goal is to partition the entire users X  space into smaller regions and apply the Recommendation Algorithm sepa-rately to the regions. However, without using any arbitrary partitioning method, we employ an intelligent partitioning technique using Multiplicatively Weighted Voronoi Diagrams . In this work, we partition the users X  space according to the location of the user. The proposed work first find the weights (initial) associated with each of the voronoi cells formed by the ordinary (non-weighted) Voronoi Diagram, and then use these initial weights to construct the multiplicatively weighted Voronoi Diagram in an iterative manner. One of the objective of our work is to tessellate the users X  space into clusters in such a way that the cor-related users (users having similar preferences over items) end up in the same clusters. We find the Spatial Autocorrelation index (Geary X  X  index) value in the regions (clusters) formed by the decomposition process to measure the correla-tion among the users in the regions.
 In CF, finding similarity amongst N users is an O( N 2 ) process. If N is large then similarity computation becomes quite expensive. Decomposition avoids this quadratic blowup and allows us to process bigger datasets even with limited computational resources. As for example, if we partition a region with n users into k partitions with nearly equal sizes, then the overall time required for per-forming collaborative filtering in all those k partitions will be proportional to k. ( n/k ) 2 =( n 2 /k 2 ) .k = n 2 /k . So we can achieve a k order speed up by dividing the users X  space into k partitions. The advantage of the proposed method is that less similarity computations are needed as we apply the CF algorithm to the partitions and not to the entire users X  space. One disadvantage of this recom-mendation technique is that the recommendation quality may degrade, since we recommend only using the data of a particular region, and not the entire rating dataset. Our goal is to reduce the overall running time without sacrificing rec-ommendation quality much. Experiments conducted indicate that our method is effective in reducing the running time, while maintaining an acceptable quality of recommendation. Moreover our proposed decomposition scheme is oblivious of the underlying recommendation algorithm.
 The rest of the paper is organized as follows: In section 2, we provide back-ground information about Weighted Voronoi Diagram, Spatial Autocorrelation and also review some of the past works related to Collaborative Filtering based RS. Section 3 outlines our contribution while sections 4 and 5 present our Decom-position and Recommendation Algorithms respectively. In section 6, we report and analyze the experimental results. Section 7 concludes discussing our future research directions.
 2.1 Weighted Voronoi Diagram Voronoi Diagrams 1 are used in computational geometry to decompose a metric space into regions based on distances from a specified finite set of points. These points are also called sites on the plane that provide certain services. Suppose that there is a person located at point x on the plane. Now, the question is which site he should access to get service from. Generally it is the site which is nearest from the point x , based on a suitable distance function. Voronoi Diagrams tessel-late the plane into regions around each site, so that we know the service region for a particular site. Voronoi Diagrams can be generalized on the basis of the distance function that is used. One can define a distance function by assigning weights to the sites. The resultant diagrams are called Weighted Voronoi Dia-grams. They are of two types -multiplicatively weighted and additively weighted Voronoi Diagrams. In a multiplicatively weighted Voronoi Diagram, we define the distance function as where w i is the weight of the site s i . Here the effective distance is obtained by dividing the Euclidean distance by the weight or strength of the site, and therefore the region belonging to a site with higher weight will have a relatively larger area. Note that in the plane under the Euclidean metric, the edges of a multiplicatively weighted Voronoi Diagram can be circular arcs or straight line segments. Figure 1(a) shows an ordinary Voronoi Diagram consisting of 11 sites (from A to K), and the corresponding multiplicatively weighted Voronoi Diagram is shown in Figure 1(b). Applications of weighted Voronoi Diagrams can be found in various domains [ 2 , 3 , 8 ]. Dong developed a raster-based approach to generating and updating both ordinary and multiplicatively weighted Voronoi diagrams for point, line, and polygon features in a GIS environment [ 3 ]. Boots applied multiplicatively weighted Voronoi Diagrams to generate trade areas using store characteristics and assumptions about customer behaviour [ 2 ]. In this work, we use a multiplicatively weighted Voronoi Diagram to partition the users of our system with respect to location. 2.2 Spatial Autocorrelation Spatial Autocorrelation [ 7 ] measures the co-variance of properties within a geo-graphic space and it deals with both attributes and locations of spatial features. Spatial data tends to be highly self-correlated, i.e., people with similar character-istics, occupations and backgrounds tend to cluster in the same neighborhood. A commonly used measure of Spatial Autocorrelation is Geary X  X  index ( c )[ 4 ]. Geary X  X  index measures the similarity of i  X  X  and j  X  X  attributes, c calculated as follows: where z i and z j are the values of the attribute of interest for objects i and j . A locational similarity w ij is used in the calculation of Geary X  X  index, where w = 1 if i and j share a common boundary, and w ij = 0 if not. Geary X  X  index is expressed as follows: c = Here  X  2 is the variance of the attribute z values. If c = 1, the attributes are distributed independently of location. If c&lt; 1, similar attributes coincide with similar locations, and if c&gt; 1, attributes and locations are dissimilar. 2.3 Collaborative Filtering Based Recommender Systems CF systems generate recommendations based on a subset of users that are most similar to the active user. However, each time a recommendation is requested, the algorithm needs to compute the similarity between the active user and all other users, based on their co-rated items. This similarity computation becomes very expensive with the growth of both the number of users and items in the database. We briefly discuss some of the past works that address this scalability problem in the remaining of this section.
 Sarwar et al. [ 11 ] addressed the scalability issue associated with the CF task by clustering the complete user set on the basis of user-user similarity and used the cluster as the neighborhood. With the same perspective George and Merugu [ 5 ] use a collaborative filtering approach based on a weighted co-clustering algorithm that involves simultaneous clustering of users and items. Sarwat et al. [ 12 ] also pro-posed a scalable location-aware recommender system using location based rating. They employed user partitioning technique and produced recommendations twice as accurate compared to existing recommendation approaches. In this work, we address the scalability problem of the CF process by clustering the users X  space on the basis of a Weighted Voronoi diagram. The primary objective of our work is to deal with the computational complexity associated with the traditional CF algorithms. We propose a user partition-ing technique using multiplicatively weighted Voronoi Diagrams to partition the entire users X  space into smaller cells (regions) based on the location, and then apply the Recommendation Algorithm separately to these regions. In this work, we use two popular datasets MovieLens 2 and Book-Crossing tiveness of our proposed algorithm. However, our core approach can be easily adapted to the other scenarios. One of our main goal is to partition the users X  space in such a way that the correlated users are placed in the same regions. Our aim is to find the presence of spatial autocorrelation in the regions, and then recommend items with the idea that the suggested items will be liked by the user. In this work, we use a multiplicatively weighted Voronoi Diagram based approach for space decomposition. Space partitioning is done on the basis of locations (zip-codes or cities) of the users. In order to construct the Voronoi Diagram, we represent each city or zip-code by 2D coordinates. Our work use the longitude-latitude of the centroids of the polygonal regions representing the zip-codes as the coordinates.
 there are some weights associated with each voronoi site that we use in the distance function as described in Section 2.1. However, in our case, to start with we just have a flat set of user locations without any hierarchy. To identify some of them as voronoi sites, we introduce a threshold criterion described below. However, that is not enough to signify what weight should be associated with each of the sites. A natural expectation is that the weight associated with a particular site should be proportional to the number of users that will be present in the corresponding voronoi cell. With the above idea in mind, we proceed to construct the multiplicatively weighted Voronoi Diagram in two stages. users (threshold), and use those cities as the voronoi sites (facilities). Each site S corresponds to a voronoi cell V(S) consisting of all the points (zip-codes or cities) closer to S than any other site. For the remaining points (cities) in the plane, we calculate the distance of a point (city) from each of the site points, and the point is allocated to the region represented by the site that has the minimum distance from that point. In this way, we map each point onto some voronoi cell. The Haversine distance formula 4 that computes great-circle distances between two points on a sphere from their longitudes and latitudes is being used to find the distances. Now we have a set of voronoi polygons consisting of user cities. These voronoi polygons correspond to an ordinary Voronoi Diagram, i.e., weights are all considered to be uniform and the boundaries between all cells are straight lines. Next we find how many users are located in each of the voronoi cells formed, and then use these numbers as the preliminary weights of those cells. In stage II, we create the multiplicatively weighted Voronoi Diagram starting with the preliminary weights in an iterative manner. The preliminary weights of the cells are used to calculate the initial boundaries of the weighted Voronoi Diagram. As the boundaries change, due to the consideration of weights, the number of users belonging to each voronoi cell also change accordingly. We use these new numbers to modify the corresponding weights of each cell. Then we calculate the boundaries again on the basis of these new weights. We continue the above procedure for several iterations by which the boundaries get corrected again and again. Continuing this process for several iterations, we find that the weights of each voronoi cell reach a point of stability, when none of the weights change in two successive iterations. At this point, we accept the cell boundaries as the final boundaries of the weighted Voronoi Diagram. As we discuss later, the iterative process takes only a small part of the total time. We use these saturated weights as the final weights of the voronoi cells for further experimentations. The scheme is detailed in its algorithmic form as follows: Algorithm WeightedVoronoi Decomposition Step 1 : Select those zip-codes (or cities) as voronoi sites that satisfy the thresh-old criteria.
 Step 2 : Construct the initial Voronoi Diagram using these site points. Step 3 : Map the remaining cities to their destined voronoi cell (represented by the site closest to the city).
 Step 4 : Find the initial weights for the voronoi cells by calculating the total number of users in each cell.
 Step 5 : Create the weighted Voronoi Diagram in an iterative manner starting with the initial weights of the cells found in step 4.

Step 5.1 : The initial weights are used to define the initial boundaries of the weighted Voronoi Diagram.

Step 5.2 : Using the initial boundaries find the total number of users in each cell, and use these new numbers to modify the corresponding weights of each cell.
 Step 5.3 : Calculate the boundaries again on the basis of these new weights. Step 6 : Continue steps 5.2 and 5.3 until the weights for the cells reach a stable point (saturated weights).
 Note that for each user, we merely need to determine to which voronoi cell he or she belongs, which can be done in O(Number of cells) time. Therefore we determine this information implicitly, avoiding the explicit construction of the cell boundaries of the weighted Voronoi Diagram, which is otherwise expensive. We next compute the value of Geary X  X  index in the voronoi cells to verify whether users with similar tastes and preferences are grouped in the same neighborhood. In order to calculate the Geary X  X  index, we use movie (or book) ratings as the parameter for computing attribute similarity ( c ij ), while locational similarity ( w ij ) is measured by calculating the distance between the pairs of user city and then use the inverse of that distance (as discussed in sub-section 2.2). We investigate two popular CF methods -user-based and item-based and then combine these recommendation methods with our framework to verify whether their performance is improved. We use Pearson X  X  correlation coefficient [ 13 ]as the similarity metric for finding user-user similarity while item-item similarity is captured using Cosine-Based similarity metric [ 13 ].
 lar users of the target user by computing the similarities between the target user and all other users in the region (cluster). Next, we form a set of top rated items (movies or books) by using the ratings of the K similar users. This set include only those items whose average rating from all the K similar users is more than a threshold value. Then the items in this set are again ranked in order of their rating frequency (no. of users rating the item). The system recommends to the target user the top-N items from the item set not rated by the user. most similar items for each item present in the cluster according to the similarity score. Then we form a set, RC , as recommendation candidates by taking the union of the K most similar items and then removing the items already rated by the target user. Let RI denote the set of items rated by the target user. Next, we calculate the similarities between each item of the set RC and the set RI . Then the items in the set RC are sorted in descending order of the similarity and the Top-N items from this set are recommended to the target user. 6.1 Data Description Our experiments are performed on the MovieLens-1M and Book-Crossing data-sets. MovieLens-1M data consists of 1,000,209 anonymous ratings (1-5) of approx-imately 3,900 movies made by 6,040 MovieLens users where each user has rated at least 20 movies. Book-Crossing dataset contains 278,858 users (anonymized but with demographic information) providing 1,149,780 ratings (explicit/implicit) on 271,379 books. Ratings are either explicit, expressed on an integral scale from 1-10 (higher values denoting higher appreciation), or implicit, expressed by 0. 6.2 Evaluation Metric Discussion We use Mean Absolute Error (MAE) [ 13 ] and Root Mean Square Error (RMSE) [ 13 ] to evaluate the prediction accuracy while quality of the recommendation is measured using the Precision , Recall and F1 score metric. We have depicted the different combinations of recommendation that can be generated in a typical recommendation problem in Table 1. Note that a customer likes an item (movie or book) if he has given a rating of 4 or 5 to that item (in a scale of 1 to 5), otherwise dislikes it, i.e., his rating is 1, 2 or 3. A recommendation is positive if recommended rating coincides with the actual rating.
 Precision : Precision measures the degree of accuracy of the recommenda-tions produced by the algorithm. In our system, Precision measures what fraction of the recommended items are liked by the customers.
 Recall : In our Recommender System, Recall measures what fraction of the items liked by the customers, has been recommended by the algorithm. F1 score : F1 score is the harmonic mean of Precision and Recall.
 6.3 Experimentation with Decomposition Algorithm The Decomposition algorithm use threshold values (as discussed in section 4), which define the minimum number of users a city must have to be considered as a site in the Voronoi Diagram. For the MovieLens data, we use the threshold values, and for Book-Crossing we use { 50, 100, 150 values. Note that, we used higher threshold values for the Book-Crossing data than the MovieLens data because the total number of users in the Book-Crossing dataset is significantly more than the MovieLens dataset. If lower threshold values (like 5, 10 and 15) are used then we will have a number of small cells with very few users (and ratings), which in turn may affect the recommendation quality. Table 2 shows the result of the weighted Voronoi decomposition using the thresholds. It also shows the total number of iterations required for the weights of the voronoi cells to reach a stable point. From Table 2, it can be noted that the time required to generate the weighted Voronoi Diagram using the iterative process is really negligible ( &lt; 0 . 4%) compared to the total time required for recommendation as reported in Table 7.
 dataset in Table 3 and that of Book-Crossing dataset in Table 4. In the Tables, we compare the spatial autocorrelation values of the entire users X  space with the corresponding values of the regions formed by the initial and weighted Voronoi Diagrams. We know that if the Geary X  X  index value is less than 1, then spa-tial autocorrelation is present, otherwise absent. To measure correlation among the users in the regions we define two metrics -CI 1 and CI percentage of items (movies or books) that falls below the correlation value 0.75, and CI 2 reports the percentage of items that falls below the correlation value 1.0. As for example, in Table 3, we notice that on an average 14.46% of the movies of the entire users X  space fall below the correlation value 0.75 while 60.44% of the movies fall below 1.0. Similarly, for the Initial Voronoi Diagram, using Threshold = 5, 80.81% of the movies averaged across all the 172 regions fall below 0.75, and 94.53% of the movies fall below 1.0. We have the best results for spatial autocorrelation using the Weighted Voronoi approach ( CI and CI 2 = 97.18). Here, we achieve about 70.00% improvement in CI about 37.00% in CI 2 over the entire space. In Table 3, we can also observe that the Weighted Voronoi approach also gives us better correlation values over the Initial (non-weighted) Voronoi for all the threshold values. Similar results can also observed in Table 4. In the tables, one can also observe that as the number of cells increases, or in other words sizes of the cells decrease, the percentage of spatial autocorrelation increases within each cell. However we cannot decrease the cell sizes beyond a certain point, since too few users in a cell will result in fewer number of ratings based on which the recommendations will be made and this in turn will affect the quality of recommendations. 6.4 Experimentation with Recommendation Algorithm As we have already seen in the previous section that spatial autocorrelation exists in the regions, and therefore it seems very promising that if you recommended only using the users (or items) in the region of the target user, recommendation quality will improve. For both MovieLens and Book-Crossing datasets, we ran-domly split the user ratings into two sets -observed items (80%) for training and held-out items (20%) for testing. Ratings for the held-out items are to be predicted. We execute the algorithm with K = 100 and N = 10. That is we consider a maximum of 100 similar users or items and recommend top-10 items to the user.
 We report the results of the Recommendation Algorithm performed on the MovieLens dataset in Table 5, and that of the Book-Crossing dataset in Table 6. Here term Th is abbreviation for threshold. In the Tables, we make a com-parative analysis of the recommendation performance using different evaluation metrics. Here base performance indicates the performance of the algorithm using the entire users X  space (without decomposition). We compare the overall perfor-mance in the regions formed by weighted Voronoi decomposition with the base performance . We use MAE and RMSE to evaluate the prediction accuracy and also use Precision@K, Recall@K and F1@K to evaluate the quality of the top-K recommended items. Note that, we present Precision ( P @10), Recall ( R @10) and F1 ( F 1@10) score on position 10. The bold numbers indicate that its value has an improvement over the base value.
 In Tables 5 and 6, we have reported the performance of our recommendation algorithm averaged over all the regions. As for example, in Table 5, for thresh-old 5 (User-based case), we have an average Precision, Recall, F1, MAE and RMSE of 0.858, 0.830, 0.828, 0.419 and 0.571 respectively averaged across all the 172 regions. Here we can see that the algorithm performs better in terms of Recall and F1 score while in terms of Precision, MAE and RMSE, the base performance is slightly better. Similar results can also be observed in Table 6. Since we executed the algorithm only using the ratings of a particular region, it may sometimes compromise our recommendation quality as two users in two different regions may have similarity in the rating patterns. However, from the above tables, it is clear that our algorithm always performs better (in terms of Recall) than the base performance, while for the other evaluation metrics it has values which are nearly equal to the base. 6.5 Scalability We report the running time of our algorithm for the MovieLens and Book-Crossing datasets in Table 7. In the 3rd and 7th column of Table 7, we record the overall time required for testing the algorithm (in minutes) in all the regions formed by Weighted Voronoi (WV) decomposition using the different thresholds. Note that, the running time comprises of both the spatial correlation calculation time and recommendation generation time for all the users of a region. Here base represents the entire dataset without decomposition. Our experiments are run on a computer with Core i3 -2100 @ 3.10GHz x 4 CPU and 4 GB RAM.
 time improves significantly when we divide the entire dataset into smaller cells and apply the algorithm independently to those cells. As for example, for Movie-Lens dataset, the overall time required for recommending all the users in the 172 regions is 460.5 minutes while that of the entire dataset is 1233.3 minutes. Thus the running time reduces by about 63% over the base performance. Similarly for the Threshold value of 10 and 15, the runtime reduces by about 73% and 57% respectively. However, to improve the runtime further, we analyzed our algorithm and found that the weights associated with some of the voronoi cells are significantly higher than the rest of the cells. For this reason, the recom-mendation algorithm spends considerable amount of time in recommending the users in those cells, which in turn affects the overall performance. In order to distribute the cell weights evenly, we then modified the distance function defined in equation 1 as follows. Then we again constructed the diagram using the distance function in equation 2. Note that using square root of weight is intuitively justified as weight of each cell is directly related to its area, whose dimension varies with the square of distance. We report the overall running time of our recommendation algorithm considering the cells as per this modified Voronoi diagram in Table 7 (4th and 8th column). Here the term MV is abbreviation for Modified Voronoi. Comparing the results of MV with WV approach, we can clearly observe that this small change produces significantly faster recommendations. The bold numbers indicate improvement over WV. As for example, for MovieLens dataset, using MV approach, the overall time required for recommending all the users in the 172 regions is 20.45 minutes while the recommendation time for the corresponding 172 regions using WV approach is 460.5 minutes. Thus the runtime reduces by an order of 2. Similar results can also be seen for the other threshold values. Thus we can conclude that our MV based technique is effective in reducing the running time further. In figure 2, we report the average recommendation time (in seconds) per user in the entire users X  space (Base) and in the regions formed by both WV and MV based decomposition techniques. We can clearly observe that for both the MovieLens and Book-Crossing datasets, MV approach outperforms both the Base and WV based recommendation methods. In this paper, we have presented a scalable decomposition based Recommender System. We have implemented a decomposition technique that divides the users X  space into some smaller regions with respect to location and then use spatial autocorrelation measures to capture the correlation among the users in a region. Experimental analysis using real datasets show that our model is efficient and scalable. Our proposed approach deals with the Scalability problem of the CF based recommendation methods by applying the Recommendation Algorithm separately to the regions. The focus of our future work is to use other metrics for finding the spatial correlation and similarities between users with the aim of optimizing the splitting technique and the Recommendation Algorithm. Finally as noted earlier, our proposed decomposition scheme is oblivious of the underly-ing recommendation algorithm and hence applicable with other recommendation algorithms as well. How this can be leveraged is a matter of future research.
