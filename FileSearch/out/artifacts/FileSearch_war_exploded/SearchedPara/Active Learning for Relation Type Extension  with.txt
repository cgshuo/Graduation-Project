 Relation extraction is the process of identifying instances of specified types of semantic re lations in text; relation type extension involves extending a relation extraction system to recognize a new type of relation. We present LGCo-Testing , an active learning system for relation type extension based on local and global views of relation instances. Locally , we extract features from the sentence that contains the instance. Globally , we measure the distribu tional similarity between instances from a 2 billion token corpus. Evaluation on the ACE 2004 corpus shows that LGCo-Testing can reduce annotation cost by 97% while maintaining the performance level of supervised learning. H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Miscellaneous; I.2.7 [ARTIFICIAL INTELLIGENCE]: Natural Language Processing  X  Language parsing and understanding Information extraction, relation extraction, active learning, co-training, co-testing, distributional similarity Relation extraction is the process of identifying instances of specified types of semantic re lations in text; relation type extension involves extending a relation extraction system to recognize a new type of relation. Relation extraction aims to connect related entities in text and label them with the right semantic relationship. For ex ample, a relation extraction system needs to detect an Employment relation between the entities He and Arkansas in the sentence He was the governor of Arkansas . The task of relation type extension is to adapt an existing relation extraction system to be able to extract a new type of relation, often called the target relation , preferably in a fast, cheap and accurate way. __________________________________________ A supervised approach can tackle this problem in an accurate way. But it is slow and e xpensive as it relies on human annotation of a large quantity of examples. Semi-supervised learning, in contrast, does not require much human effort by automatically bootstrappi ng a system for the target relation from only a few labeled examples and a large unlabeled corpus. However, a large ga p still exists between the performance of semi-supervised and supervised systems. Moreover, their performance larg ely depends on the choice of seeds [10, 15]. Another attractive alternative is active learning, which reduces annotation cost by requesting labels of only the most informative examples while maintaining high learning performance. It is also shown to be robust to the choice of the seeds [7]. Specifically, we fo cus on relation type extension with co-testing [12], an active learning approach in the co-training [1] setting. It minimizes human annotation effort by building two classifiers based on two different views of the data and asking for human labels only for contention data points, points on which the two cl assifiers disagree about their labels. The key to the success of co-testing is to find a natural way of splitting a data point into two views that are uncorrelated and compatible (each view is sufficient in labeling the data point). To date, there is limited work on applying co-testing to relation type extension. The main difficulty, as we believe, is the lack of a natural way of partitioning the data into two uncorrelated and compatible views. Unlike named entity classification where one can rely on either the name string determine the type of the named entity [4, 7], the type of a relation is mostly determined by the context in which the two entities appear. For example, it is not possible to decide the type of relation between He and Arkansas without the local context was the governor of . If the context was traveled to , then the relation of the two entities would change entirely. Thus it is not desirable to separate the entities from their context to establish two views. Instead, we treat them together as a single view, the local view. Motivated by the idea of dist ributional similarity, we move beyond the local view to interpret the relation between two entities. Specifically, we compute from a 2 billion token corpus the distributional similarities between relational phrases. We take these similari ties as the global view upon which we build a classifier which classifies new examples based on the k -nearest neighbor algorithm. For example, if the phrase arrived in is more similar to traveled to than to was the governor of , the global view classifier classifies entities connected by arrived in as the same relation as those connected by traveled to . Armed with this global view classifier and a classifier trained with features extracted from the local view, applying co-testing to relation type extension becomes feasible. The main contributions of active learning with local and global views are: it indeed introduces two uncorrelated and compatible views for relation extraction. It provides substantial reduction of annotation effort as compared to various baselines based on an extensive experimental study. Furthermore, it leads to faster convergence of learning. The next section introduces our task. Sections 3 and 4 describe the local and global view cla ssifiers in detail. We present LGCo-Testing and baseline systems in Section 5 and 6, and evaluate them in Section 7. We discuss related work in Section 8 and conclude in Section 9. We choose to work on the well-de fined relation extraction task of the Automatic Content Extraction 1 (ACE) program in 2004, mostly driven by the purpose of system comparison as many published results on this data set are available. A relation is defined over a pair of entities within a single sentence. ACE 2004 defined 7 major relation types. Some examples are shown in Table 1. Following prev ious work, we only deal with relation mentions. Table 1: ACE relation examples from the annotation We consider two training si tuations: one where we are defining a new relation for a previously unannotated corpus; we call this the binary setting ; the other where we are adding a new relation to a corpus previously annotated with n relations, where the n+1 relations are mutually exclusive; we call this the multi-class setting . We refer to the new relation as the target relation . We use the ACE 2004 corpus, which is annotated with 7 different relati on types, to experiment with both tasks. For the binary setting , we pick one of the ACE relations as the target relation and pretend we don't know anything about the other relations. For the multi-class setting, we again pick one of the ACE relations as the target relation and treat the remaining 6 types as auxiliary relations which we already know about and can use to aid the learning of the new type. There are two common learning approaches for building a classifier based on the local view: feature-based [8, 6, 20] and kernel-based [2, 16, 17, 19, 21]. As we want to compare LGCo-Testing with co-testing based on a feature split at the local level, we choose to build a feature-based local classifier. Given a relation instance  X , X  X  X  X  X   X   X ,  X   X  , where  X  pair of entities and  X  is the sentence containing the pair, the local classifier starts with multiple level analyses of the sentence such as tokenizati on, syntactic parsing, and dependency parsing. It then extracts a feature vector v which contains a variety of lexical, s yntactic and semantic features for each relation instance. Our features are cherrypicked from previous feature based systems. Table 2 shows the feature set with examples. After feature engineering, the local classifier applies machine learning algorithms to learn a function which can estimate the conditional probability  X  X  X  X  X  X  , the probability of the type c given the feature vector v of the instance x . We used maximum entropy (MaxEnt) to build a binary classifier (for the binary setting ) and a multi-class classifier (for the multi-class setting ) because the training is fast, which is crucial for active learning as it is not desirable to keep the annotator waiting because of slow training. Building a classifier based on the global view involves three stages of process: extracting relational phrases, computing distributional similarities, and building the classifier based on similarities. We describe these stages in detail below. Extracting relational phrases . Given a relation instance  X , X  X  X  X  X   X   X ,  X   X  and assuming that  X   X  appears before  X  represent it as a relational phrase  X   X  , which is defined as the n-gram that spans the head 3 of  X   X  and that of  X   X   X _ X  X  X  X  X  X  X   X   X _ X  X  X  X ,  X   X  . For example, we extract Clinton traveled to the Irish border as the phrase for the example in Table 2. As our goal is to collect the tokens before and after a phrase as features to capture the similarity between phrases and long phrases are too sparse to be useful, we instead use the definition  X   X   X  X  X  X   X   X ,  X   X  (tokens between  X   X   X  and phrase contains more than 5 tokens. Thus for the example in Table 2, because the previously extracted phrase contains 6 tokens, we will instead use the phrase  X  traveled to  X  to represent that instance.
 Computing distributional similarities . We first compile our 2 billion token text corpus to a database of 7-grams and then form 7-gram queries to extract features for a phrase. Example queries for the phrase traveled to are shown in Table 3. &gt; traveled to &lt;  X   X  &gt;the Irish border&lt;/  X   X  &gt; for an ... X  Entity Sequence Syntactic Parsing Dependency Parsing We then collect the tokens that could instantiate the wild cards in the queries as features. Note that tokens are coupled with their positions. For example, if the matched 7-gram is President Clinton traveled to the Irish border , we will extract from it the following five features: President_-2 , Clinton_-1 , the_+1 , Irish_+2 and border_+3 . Each phrase P is represented as a feature vector of contextual tokens. To weight the importance of each feature f , we first collect its counts, and then compute an analogue of tf-idf : tf as the number of corpus instances of P having feature f divided by the number of instances of P ; idf as the total number of phrases in the corpus divided by the number of phrases with at least one instance with feature f . Now the token feature vector is transformed into a tf-idf feature vector. We compute the similarity between two vectors using Cosine similarity. The most similar phrases of traveled to and his family are shown in Table 4. Building the classifier . We build the global view classifier based on the k -nearest neighbor idea, classifying an unlabeled example based on closest labeled examples. The similarity between an unlabeled instance u and a labeled instance l is measured by the similarity between their phrases,  X  Note that we also incorporate the entity type constraints into the similarity computation. The similarity is defined to be zero if the entity types of u and l do not match. The similarity between u and a relation type c ,  X  X , X  X  X  X  X  , is estimated by the similarity between u and its k closest instances in the labeled instance set of c (we take the averaged similarity if  X 1 X  ; we will report results with k=3 as it works slightly better than 1, 2, 4 and 5). Let  X  X  X  X  X  be the classification function, we define it as follows: We first introduce a general co-testing procedure, then describe the details of the proposed LGCo-Testing . Let  X   X  denote unlabeled data, and  X   X  denote labeled data, the co-testing procedure repeats the following steps until it converges: 1. Train two classifiers  X   X  and  X   X  based on two data 2. Label  X   X  with  X   X  and  X   X  and build a contention set 3. Select  X   X   X  X  based on informativeness and request 4. Update:  X   X   X  X   X   X  X   X  and  X   X   X  X   X   X \  X  Initialization . Initialization first concerns the choice of the seeds. For the multi-class setting , it also needs to effectively introduce the instances of auxiliary relations. For the choice of the seeds, as we are doing simulated experiments on the ACE corpus, we take a random selection strategy and hope multiple runs of our experiments can approximate what will actually happen in real world active learning. Moreover, it was em pirically found that active learning is able to rectify itself from bad seeds [7]. In all experiments for both the binary and the multi-class settings, we use as seeds 5 randomly selected target relation instances and 5 randomly selected non-relation instances (entity pairs in a sentence not connected by an ACE relation). For the multi-class setting, we use a stratified strategy to introduce the auxiliary relation instances: the number of selected instances of a type is proportional to that of the total number of instances in the la beled data. Since we use the annotated ACE corpus and assume full knowledge of the auxiliary relations over the corpus, we can accurately estimate their distribution. We also make the assumption that our target relation is as important as the most frequent auxiliary relation and select these two types equally. For example, assuming that we only have two auxiliary types with 100 and 20 labeled instances respectively, we will randomly select 5 instances for the first type and 1 instance for the second type, given that we initialized our active learning with 5 target relation seeds. We also experimented with several other ways of introducing the auxiliary relation instances and none of them were as effective as the stratified strategy. For one example, using all the auxiliary instances to train the initial classifiers unfortunately generates an extremely unbalanced class distribution and tends to be biased towards the auxiliary relations. For another, selecting the same number of instances for the target relation type and all the auxiliary types does not take full advantage of the class distribution of the auxiliary types, which can be estimated with the labeled ACE data set. Informativeness measurement . It is straightforward to get the hard labels of an instance fr om both the local and global view classifiers. As the local classifier uses MaxEnt which is essentially logistic regression, we take the class with the highest probability as the hard label. The hard label of the global classifier is the relation type to which the instance is most similar. As long as the tw o classifiers disagree about an instance X  X  hard label and one of the labels is our targe t relation , we add it to the contention set. Quantifying the disagreement between the two classifiers is not as straightforward as getting the hard labels because the local classifier produces a probability distribution over the relation types while the global cl assifier produces a similarity distribution. So we first use the following formula to transform similarities to probabilities. relation type, and  X , X  X  X  X  X   X   X  is the similarity between u and one of the relation types  X   X  . We then use KL-divergence to quantify the degree of deviation between the two probability distributions. KL-divergence measures the divergence betwee n two probability distributions p and q over the same event space  X  : It is non-negative. It is zero for identical distributions and achieves its maximum value when distributions are peaked and prefer different labels. We rank the contention instances by descending order of KL-divergence and pick the top 5 instances to request human labels during a single iteration. It is worth mentioning that, for each iteration in the multi-class setting, auxiliary instances are introduced using the stratified strategy as in the initialization step. Convergence detection . We stop LGCo-Testing when we could not find contention instances. We compare our approach to a va riety of baselines, including six active learning baselines, on e supervised system and one semi-supervised system. We present the details of active learning baselines below, and refer the reader to the experiment section to learn more about other baselines. SPCo-Testing . One of the many competitive active learning approaches is to build two classifiers based on a feature split at the local level. As reported by [6], either the sequence features or the parsing features are generally sufficient to achieve state-of-the-art performance for relation extraction. So we build one classifier based on the sequence view and the other based on the parsing view. More precisely, one classifier is built with the feature set based on {entity, sequence} and the other based on {entity, syntactic parsi ng, dependency parsing} . We build these two classifiers with MaxEnt. The initialization is the same as in LGCo-Testing . KL-divergence is used to quantify the disagreement between the two probability distributions returned by th e two MaxEnt classifiers. Contention points are ranked in descending order of KL-divergence and the top 5 are used to query the annotator in one iteration. Like LGCo-Testing , SPCo-Testing stops when the contention set is empty. UncertaintyAL . This is an uncertainty-based active learning baseline. We build a single MaxEnt classifier based on the full feature set in Table 2 at the local level. It uses the same initialization as in LGCo-Testing . Informativeness measurement is based on uncertainty , which is approximated by the entropy  X  X  X  X  X  of the probability distribution of the MaxEnt classifier over all the relation types  X   X  . It is also non-negative. It is zero when one relation type is predicted with a probability of 1. It attains its maximum value when the distribution is a uniform one. So the higher the entropy, the more uncertain the classifier is. So we rank instances in descending order of entropy and pick the top 5 to request human labels. Stopping UncertaintyAL cannot be naturally done as with co-testing . A less appealing solution is to set a threshold based on the uncertainty measure. RandomAL . This is a random selec tion based active leaning baseline. 5 instances are selected randomly during a single iteration. There is no obvious way to stop RandomAL although one can use a fixed number of instances as a threshold, a number that might be related to the budget of a project. The next three baselines aim to investigate the benefits of incorporating features from th e global view into the local classifier. They are inspired by recent advances in using cluster level features to compensate for the sparseness of lexical features [11, 14]. Specifically, we use the distributional similarity as a distance measure to build a phrase hierarchy using Complete Linkage . The threshold for cutting the hierarchy into clusters is deter mined by its ability to place the initial seeds into a single cluster. We refer the reader to [13] for more details about how a threshold is selected. If a phrase cluster feature phraseCluster=c . UncertaintyAL+ . The only difference between UncertaintyAL+ and UncertaintyAL is its incorporation of cluster features in building its cl assifier. This is essentially the active learning approach presented by [11]. SPCo-Testing+ . It differs from SPCo-Testing only in its sequence view classifier, whic h is trained with additional phrase cluster features. LGCo-Testing+ . It differs from LGCo-Testing only in its local view classifier, which is trained with additional phrase cluster features. We experiment with the nwire and bnews genres of the ACE 2004 data set, which are benchmark evaluation data for relation extraction. There are 4374 relation instances and about 45K non-relation instances. Documents are preprocessed with the Stanford parser 4 and chunklink facilitate feature extraction. Note that following most previous work, we use the hand labeled entities in all our experiments. We do 5-fold cross-validation as most previous supervised systems do. Each round of cross-validation is repeated 10 times with randomly selected seeds. So, a total of 50 runs are performed (5 subsets times 10 experiments). We report average results of these 50 runs. Note that we do not experiment with the DISC (discourse) relation type which is syntactically different from ot her relations and was removed from ACE after 2004. The size of unlabeled data is approximately 36K instances (  X 5 X 4 X 45 ). Each iteration selects 5 instances to request human labels and 200 iterations are performed. So a total of 1,000 instances are presented to our annotator. This setting simulates satisfying a custom er X  X  demand for an adaptive relation extractor in a few hour s. Assuming two entities and their contexts (the annotation unit) are highlighted and an annotator only needs to mark it as a target relation or not, 4 instances per minute should be a reasonable annotation speed. And assuming that our annotator takes a 10-minute break in every hour, he or she can annotate 200 instances per hour. We are now ready to test the feasibility and quality of relation type extension in a few hours. We evaluate active learning on the target relation. Penalty will be given to cases where we predict target as auxiliary or non-relation, and vice versa. To measure the reduction of annotation cost, we compare active learning with the results of [14], which is a state-of-the-art feature-based supervised system. We use the number of labeled instances to approximate the cost of active l earning. So we list in Table 5 the F1 difference between an active learning system with different number of labeled instances and the supervised system trained on the entire corpus. The results of LGCo-Testing are simply based on the local classifier X  X  predictions on the test set. For the SPCo-Testing system, a third classifier is trained with the full feature set to get test results. There is a large gap between RandomAL and the supervised system (40% F1 difference regardless of the number of labeled instances). So we do not include its results in Table 5. The three baselines with cluster level features perform similarly as their corresponding baselines without cluster phrases, e.g. UncertaintyAL+ and UncertaintyAL perform similarly. We only report results for systems without cluster features. Comparing active learning with supervised learning: LGCo-Testing trained with 1,000 labeled examples achieves results comparable to supervised learning trained with more than 35K labeled instances in both the binary and the multi-class settings. This is true even for the two most frequent relations in ACE 2004, EMP-ORG and PHYS (about 1.6K instances for EMP-ORG and 1.2K for PHYS). Th is represents a substantial reduction in instances annota ted of 97%. So assuming our annotation speed is 200 instances per hour, we can build in five hours a competitive system for EMP-ORG and a slightly weak system for PHYS. Moreover, we can build comparable systems for the other four relations in less than 5 hours. Much of the contribution, as depicted in Fig. 1, can be attributed to the sharp increase in precision during early stages and the steady improvement of recall in later stages of learning. 
Figure 1: P-R curve of LGCo-Testing with the multi-class Comparing active learning systems: the clear trend is that LGCo-Testing outperforms UncertaintyAL and SPCo-Testing by a large margin in most cases for both experimental settings. This indicates its superiority in selective sampling for fast system development and adaptation. SPCo-Testing , which is based on the feature split at the local level, does not consistently beat the uncertainty based systems. Part of the reason, as we believe, is that the sequence and parsing views are highly correlated. For example, the token sequence feature  X  traveled to  X  and the dependency path feature  X  nsubj' traveled prep_to  X  are hardly conditionally independent. Figure 2. F1 difference of LGCo-Testing for GPE-AFF and Comparing LGCo-Testing in the multi-class setting with that in the binary setting, we observe that the reduction of annotation cost by incorporating auxiliary types is more pronounced in early learning stages (#labels &lt; 200) than in later ones, which is true for mo st relations. Figure 2 depicts this by plotting the F1 difference (between active learning and supervised learning) of LGCo-Testing in the two experimental settings against the number of la bels. Besides the two relations GPE-AFF and OTHER-AFF shown in Figure 2, taking ART as a third example relation type, with 50 labels, the F1 difference of the multi-class LGCo-Testing is -29.8 while the binary one is -48.4 , which represents a F1 improvement of 19.6 when using auxiliary types. As the number of labels increases, the multi-class setting incorporates more and more auxiliary instances, which might decrease the priors for the target relations. Hence the improvement for the target relation degrades in later learning stages. reprints for Governmental pur poses notwithstanding any copy-right annotation thereon. The views and conclusions contained herein are those of the author and should not be interpreted as necessarily representing the offi cial policies or endorsements, either expressed or implied, of IARPA, AFRL, or the U.S. 
Government. [1] Avrim Blum and Tom Mitchell. 1998. Combining labeled and [2] Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path [3] Yee Seng Chan and Dan Roth. 2010. Exploiting background [4] Michael Collins and Yoram Singer. 1999. Unsupervised Models for [5] Jing Jiang. 2009. Multi-task transfer learning for weakly-supervised [6] Jing Jiang and ChengXiang Zhai. 2007. A systematic exploration of [7] Rosie Jones , Rayid Ghani , Tom Mitchell , Ellen Riloff. 2003. [8] Nanda Kambhatla. 2004. Combining le xical, syntactic, and semantic [9] Zornista Kozareva and Eduard Hovy. 2010. Not all seeds are equal: [10] Dekang Lin and Xiaoyun Wu. 2009. Phrase Clustering for [11] Scott Miller, Jethran Guinness and Alex Zamanian. 2004. Name [12] Ion Muslea, Steve Minton, and Cr aig Knoblock. 2000. Selective [13] Ang Sun and Ralph Grishman. 2010. Semi-supervised Semantic [14] Ang Sun and Ralph Grishman. 2011. Semi-supervised Relation [15] Vishnu Vyas, Patrick Pantel, Eric Crestan. 2009. Helping Editors [16] Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. [17] Min Zhang, Jie Zhang, Jian Su, and GuoDong Zhou. 2006. A [18] Zhu Zhang. (2004). Weakly superv ised relation classification for [19] Shubin Zhao and Ralph Grishman. 2005. Extracting relations with [20] Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring [21] Guodong Zhou, Min Zhang, D ongHong Ji, and QiaoMing Zhu. 
