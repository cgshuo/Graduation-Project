 This paper studies the entity-centric document filtering task  X  given an entity represented by its identification page ( e.g. , a Wikpedia page), how to correctly identify its relevant documents. In particular, we are interested in learning an entity-centric document filter based on a small number of training entities, and the filter can predict document relevance for a large set of unseen entities at query time. Towards characterizing the relevance of a document, the problem boils down to learning keyword importance for query entities. Since the same keyword will have very different importance for different entities, we abstract the entity-centric document filtering problem as a transfer learning problem, and the challenge becomes how to appropriately transfer the keyword importance learned from training entities to query entities. Based on the insight that keywords sharing some similar  X  X roperties X  should have similar importance for their respective entities, we propose a novel concept of meta-feature to map keywords from different entities. To realize the idea of meta-feature-based feature mapping, we develop and contrast two different models, LinearMapping and BoostMapping. Experiments on three different datasets confirm the effectiveness of our proposed models, which show significant improvement compared with four state-of-the-art baseline methods.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models; I.2.7 [ Natural Language Processing ]: Text analysis Entity centric; meta feature; feature mapping; transfer learning
The immense scale of the Web has rendered itself as a huge repository storing information about various types of entities ( e.g. , persons, locations, companies, etc. ). Much of information sought on the Web nowadays is about retrieving information related to a particular entity. For example, consider the following scenarios: Scenario 1: Knowledge Base Acceleration [1]. Knowledge bases such as Wikipedia and Freebase grow very quickly on the Web; however, it is a heavy burden for editors to digest a huge amount of new information every day and keep knowledge bases up to date. To reduce Wikipedia editors X  burden, NIST proposed the TREC Knowledge Base Acceleration (TREC-KBA) problem to automatically recommend relevant documents for a Wikipedia entity based on the content of its Wikipedia page.
 Scenario 2: Business Intelligence. For a company, automatical-ly collecting useful user experiences about its product entities is helpful for future quality improvement. Moreover, from a crisis management perspective, constantly monitoring user opinions on the Web also helps the company detect potential or emergent crises in a timely manner.
 Scenario 3: Celebrity Tracking. Nowadays, the success of micro-blogs is largely due to their courtship of celebrity users, as a lot of people are interested in tracking the activities of their favorite celebrities (such as movie stars and securities analysts) on micro-blogs every day. For the same reason, it would be promising to design a system that can automatically track interesting event up-dates of celebrities from other Web data.

In these scenarios, people are interested in an entity-centric doc-ument filtering system which can automatically identify relevant documents from a large text corpus ( e.g. , the Web) or a contin-uous data stream ( e.g. , news feeds) for an entity ( e.g. , products, celebrities). In this paper, we will study such an entity-centric docu-ment filtering task, and in particular address its central issue of the learning to filter problem.

As the input, the entity-centric document filtering system needs knowledge about the query entity that we are keen on  X  only an entity name might not be sufficient, since it provides too limited information to help determine document relevance and there exist ambiguous entities sharing the same name. Nowadays, most entities have their identification pages accessible on the Web ( e.g. , Wikipedi-a pages for well-known entities, Amazon pages for products, etc. ), which usually contain rich information covering different aspects of the entities. In order to better characterize a target entity, we will feed its identification page as the input of the system. Based on such rich information, the system has to tackle how to determine if a doc-ument is relevant to a specific entity described by an identification page , i.e. , to realize entity-centric document filtering . We emphasize that, relevance here represents the sense of entity-relevance , which measures whether the document contains related information about the entity, rather than whether it repeats similar content of the entity identification page.

As a manually crafted formula might not well characterize the entity-relevance of a document, in this paper, we study the learning to filter problem to realize entity-centric document filtering in a learning framework. In the offline training phase, we are given some training entities ( e.g. ,  X  X ill Gates X ), each of which is represented by an identification page ( e.g. , the Wikipedia page of  X  X ill Gates X ) and associated with a set of documents labeled as relevant/irrelevant. Our goal is to learn an entity-centric document filter, such that at online query time, given a new query entity ( e.g. ,  X  X ichael Jordan X ) represented by its own identification page, the filter can correctly identify its relevant documents.

With entity identification pages as the input and measuring entity-relevance of documents as the goal, our problem needs a new methodology as the solution. Most existing document retrieval techniques ( e.g. , BM25 [19], learning to rank [5]) deal with short keyword queries and can not handle our problem where the input is a long entity identification page. With a document as input, one straightforward solution is to define the document relevance as the cosine similarity between the entity identification page and the tar-get document. However, such an idea can not well characterize the semantics of entity-relevance, because often times an entity-relevant document only discusses one aspect of the entity or reports a latest event not recorded in the identification page, and, thus would not have high similarity.

To judge the entity-relevance of a document, a better solution is to check if the document mentions about the most basic information that is commonly used to identify the entity in its relevant docu-ments. For example, for a document relevant to  X  X ill Gates, X  with high probability, it will mention important keywords such as  X  X i-crosoft, X   X  X eattle, X   X  X hilanthropist X  which hint its entity-relevance to  X  X ill Gates. X  We believe that an entity identification page, as a comprehensive summarization of an entity, should contain such im-portant keywords. Since entity identification pages are usually long and also contain other less important keywords, in entity-centric document filtering, we are mandated to discriminate the importance of keywords in the identification page, and use those important keywords to score the entity-relevance of a document .

However, although the problem boils down to the learning of key-word importance, it is non-trivial to generalize keyword importance from training entities to new unseen entities. We can easily learn the keyword importance for training entities ( e.g. ,  X  X icrosoft X  is an important keyword for  X  X ill Gates X ) by treating each keyword as a feature and building a document classifier [11] to learn the keyword weighting; however, for a new entity such as  X  X ichael Jordan, X  the learned weighting is not applicable as keyword  X  X icrosoft X  is no longer important. Towards learning a model that is generalizable to new entities, we abstract our problem as a transfer learning problem, and study how to appropriately transfer the keyword importance learned from training entities to new query entities which do not have labeled documents.

As the key contribution of this paper, we propose a novel con-cept of meta-feature to realize transfer learning. Intuitively, if two keywords from two entities ( e.g. ,  X  X icrosoft X  for  X  X ill Gates X  and  X  X hicago Bulls X  for  X  X ichael Jordan X ) share some similar proper-ties ( e.g. , both  X  X icrosoft X  and  X  X hicago Bulls X  are organization names and mentioned a lot in their identification pages), these two keywords should have similar importance for their respective enti-ties. We name such properties of keyword features as meta-features , based on which the keyword importance transfer is enabled. For example, although we do not have labeled documents for  X  X ichael Jordan, X  by mapping  X  X icrosoft X  to  X  X hicago Bulls X  based on their meta-features, we can infer that  X  X hicago Bulls X  is important for  X  X ichael Jordan, X  given the training result showing that  X  X icrosoft X  is important for  X  X ill Gates. X  We formalize such an intuition as the meta-feature-based feature mapping principle .

To support the idea of feature mapping, the design of meta-features should be related with the keyword importance. As an entity-identification page is usually structured ( e.g. , a Wikipedia page is composed of a title, an info box, references, etc. ), such structured information is helpful for determining the importance of keywords ( e.g. , keywords mentioned in the infobox tend to be more important). Therefore, we not only use traditional keyword impor-tance measures such as TF-IDF, but also design many meta-features characterizing such structural information, e.g. , meta-feature In-foxBoxTF denoting the frequency of a keyword in the infobox.
Based on the designed meta-features, we propose two different models, LinearMapping and BoostMapping , to realize the idea of feature mapping. Assuming the keyword importance as a simple linear function of meta-features, LinearMapping could be realized by a standard classifier. For BoostMapping, it clusters keyword based on their meta-features ( e.g. , by clustering keyword  X  X hicago Bulls X  and  X  X icrosoft X  together), and assumes that keywords from the same cluster share the same importance. Without relying on the linear assumption, BoostMapping has stronger representation power compared with LinearMapping.

In the design of BoostMapping, we propose to study a challenging distantly supervised clustering problem  X  as it is not possible to guar-antee the designed meta-features are all related with entity-centric document filtering, if we adopt traditional clustering algorithms ( e.g. , K-means), the generated clusters might be useless for docu-ment filtering. Since we do not have labels on the keyword level indicating how keywords should be clustered, we will study how to cluster keywords guided distantly by labels on the document lev-el , i.e. , realizing distantly supervised clustering , to generate useful keyword clusters in BoostMapping.

In the experiments, we performed our evaluation on three differ-ent datasets: TREC-KBA, Product and MilQuery, to validate the effectiveness of our models on different types of entity identification pages (Wikipedia pages for TREC-KBA and MilQuery, Amazon product pages for Product), and on different types of entities (entities are of similar types in TREC-KBA and Product, and of general types in MilQuery). We compared our models with four state-of-the-art baselines, and observed consistent improvement.
In terms of application, different from most information retrieval applications [19, 5] whose queries are a small number of keywords, the input of entity-centric document filtering is a document charac-terizing the target entity. With an entity identification page as input, our problem is closely related to verbose-query-based retrieval [14] and query by document [21, 20]. Kumaran et al. [14] propose to enumerate all possible short sub-queries of the input long query, and learn a query quality predictor to select the best one. Such an approach is not applicable for our problem because there are too many sub-queries when the input is a long document. Weng et al. [20] propose to calculate the relevance of a document based on its topical similarity to the query document ( e.g. , relevant if both documents are about politics); different from their work, our task aims at retrieving entity-relevant documents rather than ones dis-cussing about similar general topics. Yang et al. [21] propose to extract important phrases from the query document, and use the phrases as keyword queries. Although they also discriminate the importance of keywords in the query document, their solutions rely on a manually-crafted scoring function, which can hardly charac-terize the semantics of entity-relevance. In contrast, our work will study how to automatically learn the criteria of entity-relevance from training data.
In terms of abstraction, without labeled documents for query entities, we view entity-centric document filtering as a transfer learn-ing problem, to transfer keyword importance across entities. Most transfer learning works [4, 6, 23] assume the existence of domain-independent features, which have the same feature importance for different domains. For example, in sentiment classification [4], domain-independent features could be keywords like  X  X ood X  and  X  X ad X  which are strongly indicative of a positive/negative review in all domains ( e.g. , book and electronic product reviews). Their chal-lenges arise in the different distributions of domain-independent fea-tures in different domains and the interference of domain-dependent features. As their solutions, they propose approaches including in-stance re-weighting [23], feature representation transfer [6] etc. , to minimize the difference in domain-independent feature distributions across different domains and remove the interference of domain-dependent features. In our problem, with entity-relevance as the goal, the keyword weightings should represent their importance for a particular entity and, thus, are all entity-specific (domain-dependent). In the absence of domain-independent features that are strongly in-dicative of a relevant document for all entities, we propose a novel transfer learning framework relying on  X  X eta-features X  to bridge domain-dependent features.

In terms of solution, our idea of feature mapping is closely related to structural correspondence learning (SCL) [4] which proposes to bridge domain-dependent keywords based on domain-independent keywords (so-called  X  X ivot keywords X ). Their intuition is that, if two domain-dependent keywords ( e.g. ,  X  X edious X  for book reviews and  X  X ragile X  for product reviews) have similar co-occurrence frequency with some pivot keywords ( e.g. ,  X  X ad X ), these two keywords should represent similar semantics ( i.e. , indicative of a negative review) in their respective domains. For the aforementioned reason that we are lack of such domain-independent features (pivot keywords), SCL is not applicable to our problem. In selecting important keywords for the target entities, our solution has tie with feature selection in text classification [22], which aims at choosing the most informative key-words as features. Traditional feature selection [22] adopts various metrics, e.g. , information gain, mutual information, to measure the predictive power of each keyword, while in our work, we determine the importance of each keyword based on their meta-features.
In the design of BoostMapping, we study the distantly supervised clustering problem, to generate useful keyword clusters for predict-ing document relevance. Similar motivation has been studied by Blei et al. [3] who propose a supervised topic model (sLDA) to guide the clustering of keywords by document labels. Without the concept of meta-features, sLDA clusters keywords based on their co-occurrences in documents, rather than similarities on meta-features as our model. Our setting which labels and features are defined on d-ifferent levels is closely related to multi-instance learning [2] which, given a set of labeled bags containing unlabeled instances, aims at learning a model that can predict the label of a bag based on its instances (similar to our work if we view keywords as instances and documents as bags). As the key difference, multi-instance learning assumes that a bag is labeled as positive if it contains at least one positive instance, while, in BoostMapping, the label of a document relies on how it contains keywords of different importance.
This section formally defines the entity-centric document filtering problem. To transfer keyword importance across different entities, we abstract our problem as a transfer learning problem, and propose a novel concept of meta-feature to bridge keyword features across different entities.
 Problem: Learning to Filter. To realize entity-centric document filtering , we shall build a filter F to determine, for an entity e , the relevance of a document d , i.e. , if d provides information about e . Specifically, given an entity e ( e.g. ,  X  X ichael Jordan [Basket-ball Player] X ) as described by its identification page P e Wikipedia page of e ), if d is relevant to e , i.e. , F ( d | e, P or irrelevant , i.e. , F ( d | e, P e ) = -1, e.g. , as Figure 1 shows, d relevant but d 2 is not, since it refers to a computer scientist.
Our objective is to automatically learn such a filter, i.e. , learn-ing to filter . In the training phase , we are given a set of train-ing entities e = { e 1 ,e 2 ,...,e N } ( e.g. ,  X  X ill Gates X  and  X  X arack Obama X  in Figure 1), each of which is described by an identifi-cation page P e i , and associated with a set of example documents d where y ij = +1 (or -1) indicates that d ij is relevant (or irrelevant) to e i . E.g. , in Figure 1, for entity  X  X ill Gates, X  d 11 relevant since it describes Bill Gate X  X  activity, while d because it is mainly about Steve Jobs. Based on the training data, our goal is to learn a document filter F , such that, in the testing phase (or the  X  X uery time X ), F could correctly predict document relevance for a new entity e ( e.g. ,  X  X ichael Jordan X ) which does not appear in the training entities, i.e. , e /  X  e .
 Framework. As discussed in Section 1, the nature of entity rele-vance mandates us to check if it mentions about the basic informa-tion of the entity. To fulfill such a goal, we define the relevance of a document as the summation of the contribution of its keyword-s and, thus, a document that mentions more important keywords, which represent the basic information of the query entity, tends to be more relevant. Formally, if we define V ( P e ) as a set of keywords mentioned in the identification page P e , weight ( w,e ) to measure the importance of keyword w , tf ( w,d ) to denote the frequency of keyword w in document d and len ( d ) to denote the length of document d , the document relevance rel ( d | e, P e ) is defined as In Eq. 1, the relevance score is normalized by len ( d ) to avoid the bias towards long documents.

We note that Eq. 1 is both natural and widely adopted. Intuitively, to determine the relevance of a document, we need to check how the document is written, or more specifically, which keywords are used in the document. Such an idea of measuring the document relevance as the summation of its keyword contribution has been widely adopted in various IR applications. For example, in document classification [11], the document relevance takes the same form as Eq.1 with the keyword weighting measuring the relevance of keyword w to a category ( e.g. ,  X  X conomy X ).

With the simple relevance modeling, we can further concretize the filter F to learn. Since F is to judge if a document d is sufficiently relevant to an entity e identified by P e , we can simply base the decision on the relevance value, i.e. ,
Overall, we realize the entity-centric filtering task with the fol-lowing framework. The framework, as a learning approach, consists of two stages: at training time, it needs to learn the weight function, which recognizes the importance of each keyword w for a new enti-ty. With such a function learned, at testing time, we simply apply weight to realize the filtering function.
 Training Phase:
Given: Training entities e = { e 1 ,...,e N } , each entity e
Output: weight ( w,e ) measuring the importance of keyword w Testing Phase:
Given: Query entities e ; Identification page P e ; Candidate
Output: F ( d | e, P e ) by Eq. 2.
To learn the relevance function in Eq. 1, with tf ( w,d ) and len ( d ) both given, the entity-centric document filtering problem boils down to determining the value of weight ( w,e ) , i.e. , the importance of keyword w for entity e .

To clarify the challenge of our problem, we start with a simpler case which assumes that document training labels are available for query entities, i.e. , the original task studied in TREC-KBA which assumes e  X  e . Such a case could be simply solved by traditional document classification techniques [11], by learning one document filter for each query entity based on its labeled documents. Taking entity  X  X ichael Jordan X  as example, referring to [11], we define each len ( d ) as one feature corresponding to keyword w ( e.g. ,  X  X hicago Bulls X ), and learn feature weighting weight ( w,e ) by SVM; in the query time, weight ( w,e ) will be applied to predict the relevance of a new document for  X  X ichael Jordan X .

However, since we believe that the requirement of document la-bels for every entity is impracticable and assume that the query entity does not have labeled documents, traditional document clas-sification techniques no longer work. As the same keyword ( e.g. ,  X  X icrosoft X ) will have very different importance for different entities ( e.g. ,  X  X ill Gates X  and  X  X ichael Jordan X ), the keyword weighting learned from one entity could not be generalized to others. To tackle this problem, we will study how to transfer keyword importance across different entities , which is the core challenge in this paper.
The idea of transferring feature weightings across entities nat-urally connects our problem to transfer learning [4, 6, 23, 17]. Different from most machine learning problems which assume that the training and testing data must be in the same feature space with generalizable feature weightings, i.e. , in the same domain, transfer learning studies how to transfer knowledge across different domains. We abstract entity-centric document filtering as a transfer learning problem, with each entity corresponding to one domain . For dif-ferent entities, the domain difference is reflected by the fact that keyword features ( e.g. ,  X  X icrosoft X ) are domain-dependent , which have different importance for different entities/domains ( e.g. ,  X  X ill Gates X  and  X  X ichael Jordan X ).

To tackle the existence of domain-dependent features, existing transfer learning works either filter out such domain-dependent features and rely on domain-independent features [16, 7], or try to learn a new transferable feature representation for different domains [4]. However, all such works heavily rely on the existence of domain-independent features, while in entity-centric document filtering, as our goal is to identify the entity-relevance of a document, the keyword weightings should represent the importance of keywords for a particular entity and, thus, are all entity-specific. As there are no domain-independent keyword features that are commonly predictive of entity-relevance for all entities, the abstraction of entity-centric document filtering raises a novel transfer learning problem, i.e. , how to realize knowledge transfer in the absence of domain-independent features .
In order to realize transfer learning based on only domain-dependent features, we propose a novel concept of meta-feature . As our key insight, although a domain-dependent feature has different weight-ings in different domains, if the weighting is related with some domain-independent properties, we can use such properties to bridge domain-dependent features. For example, in entity-centric docu-ment filtering, given keyword  X  X icrosoft X  from entity  X  X ill Gates, X  and  X  X hicago Bulls X  from  X  X ichael Jordan, X  as both keywords are organization names mentioned a lot in their entity identifica-tion pages, intuitively they should have similar importance for their respective entities. As these properties are defined on keyword features w characterizing their potential importance for entity e , we name such  X  X eatures of features X  as meta-features , denoted by f ( w,e ) :=  X  f 1 ( w,e ) ,...,f K ( w,e )  X  where f k ( w,e ) indicates the k -th meta-feature function of keyword w with respect to e . Such an insight could be summarized as the Meta-Feature-based Feature Mapping principle , described as follows, Principle 1 (Meta-Feature-based Feature Mapping): Given two keyword-entity pairs  X  w 1 ,e 1  X  and  X  w 2 ,e 2  X  , if the two meta-feature vectors f ( w 1 ,e 1 ) and f ( w 2 ,e 2 ) are similar, weight ( w ilar to weight ( w 2 ,e 2 ) .
 Principle 1 proposes to map keywords sharing similar meta-features together to share similar weightings, which enables transfer learning across entities. In the training phase, we learn w ( e.g. ,  X  X icrosoft X ) is important for e i ( e.g. ,  X  X ill Gates X ) with high weight ( w,e whereas, at query time, although we do not have labeled doc-uments for new entity e ( e.g. ,  X  X ichael Jordan X ), we can still infer w 0 ( e.g. ,  X  X hicago Bulls X ) is important for e by observing f ( w,e i )  X  f ( w 0 ,e ) .
Figure 2 lists all meta-features we design for the TREC-KBA task, which characterize the potential importance of keyword w for entity e . For example, IdPagePos ( w,e ) denotes the first posi-tion of keyword w in the identification page of e , and for most of time, keywords mentioned earlier in the identification page ( e.g. , a Wikipedia article) tend to more important. We also incorporate the structural information of the entity-identification page in the design of meta-feature, e.g. , a keyword mentioned in the info-box with non-zero InfoBoxTF ( w,e ) is likely to be important.
Based on the insight proposed in Section 3.2, our objective boils down to mapping keyword features from different entities by their meta-features. In this section, we propose and contrast two feature mapping models  X  LinearMapping and BoostMapping  X  correspond-ing to defining weight ( w,e ) as a continuous function and a discrete function respectively. We compare the pros and cons of these two models at the end of the section.
To model Principle 1, one simple idea is to define weight ( w,e ) as a continuous function of the meta-feature vector of  X  w,e  X   X  the continuous property, which indicates that the function will produce similar output with similar input, naturally fulfills the requirement of Principle 1, i.e. , realizing the idea of feature mapping.
To facilitate the learning of parameters, we study the linear map-ping model (LinearMapping), which assumes the simplest linear form of weight ( w,e ) , formally defined as follows, where  X  k measures the importance of meta-feature f k ( w,e ) . By inserting Eq. 3 into Eq. 1, we obtain mation of one meta-feature of all keywords.

The linear assumption of weight ( w,e ) largely facilitates the learning of parameters. We can view  X  l 1 ( d,e ) ,...,l document feature vector generated by LinearMapping for document d , and the problem is transformed into a standard classification problem. We can apply any standard classification technique such as SVM, logistic regression to learn  X  k according to document labels.
Figure 3(a) intuitively shows how LinearMapping learns a linear curve ( i.e. , the dashed line) to fit the real curve ( i.e. , the solid curve, taking an L-shape because keywords appearing at the top of an Wikipedia page are more important). In addition to the continuous linear function, weight ( w,e ) could also be defined as a discrete stepwise function, plotted in Figure 3(b), which will be discussed in the following section. In this section, we propose a novel boosting mapping (Boost-Mapping) model which defines weight ( w,e ) as a discrete stepwise function. Such a design is motivated from the idea of constructing keyword clusters to realize feature mapping. In BoostMapping, we will study how to guide the clustering process such that the gen-erated clusters are useful for document filtering, i.e. , the distantly supervised clustering problem.
As the key idea, we cluster keywords based on their meta-features, and assume that keywords belonging to the same cluster contribute the same weight ( w,e ) to document relevance. For example, by clustering keyword  X  X icrosoft X  for  X  X ill Gates X  and  X  X hicagoBull X  for  X  X ichael Jordan X  together, we can learn the keyword weighting of  X  X icrosoft X  for  X  X ill Gates X  from the training data, and apply it to  X  X hicagoBull X  for  X  X ichael Jordan X  at query time.
 To formalize such an idea, we define L = { X  w,e  X  X  e  X  e ,w  X  V ( P e ) } containing all keyword-entity pairs, and c = { c as a set of keyword clusters, where each keyword cluster c contains keyword-entity pairs sharing similar meta-features. The keyword importance weight ( w,e ) depends on which clusters  X  w,e  X  belongs to, defined as, where I m ( w,e )  X  X  0 , 1 } is an indicator function denoting whether  X  w,e  X  belongs to keyword cluster c m , and  X  m measures the impor-tance of cluster c m . By inserting Eq. 5 into Eq. 1, we obtain could be viewed as a document feature generated by BoostMapping. Eq. 5 actually defines weight ( w,e ) as a discrete step function, as plotted in Figure 3(b). Each interval represents a keyword cluster across entities. Intuitively from the figure, with a good design of keyword clusters c , such a discrete function should better fit the underlying real curve of arbitrary shape compared with LinearMap-ping.
The idea of keyword-clustering-based feature mapping raises a new challenge for us  X  as we do not have keyword labels indicating how keywords should be clustered, if we adapt traditional clustering algorithms ( e.g. , Guassian Mixtures, K-Means) to cluster keywords based on their meta-features, the generated keyword clusters might be misleading for entity-centric document filtering. For example, a meta-feature which measures the length of a keyword might tempt traditional clustering algorithms to construct a keyword cluster con-taining only long keywords, which is not very helpful for identifying the keyword importance. Ideally, the process of keyword cluster-ing should be guided by document labels, to generate meaningful keyword clusters for document relevance prediction.

We abstract such a problem of finding meaningful keyword map-pings (or clusters) c for predicting y as the distantly supervised clustering problem. It is distantly supervised , because, without keyword labels indicating how keywords should be clustered, the only supervision comes from document labels at a different level. Formally, in the training phase, given a set of example documents d = { d 1 ,d 2 ,...,d | d | } with labels y = { y 1 ,y 2 ,...,y is to cluster L = { X  w,e  X  X  e  X  e ,w  X  P e } into a set of feature mappinps c = { c 1 ,...,c M } , such that, first, keywords in the same c m should share similar meta-features f ( w,e ) , and second, c should be useful for predicting y . In the testing phase, we will map new keywords to c i according to the same clustering criteria, and predict the document relevance based on Eq. 6. In this section, we propose a novel boosting mapping model, BoostMapping, to tackle the distantly supervised clustering problem.
As the key idea, in order to obtain useful keyword clusters c for predicting y , we will jointly optimize c together with parameters  X   X   X  to minimize the prediction error, rather than isolating the clustering process from the optimization. Much like other machine learning models, the objective of BoostMapping is to minimize the loss between document labels y and the document relevance predicted by Eq. 6, with respect to some loss function, i.e. , argmin In Eq. 7, different selections of loss function will largely affect the difficulty of optimization. For example, if we use the loss function of logistic regression or SVM, we have to jointly construct all clusters c to optimize the resulting objective function, which is complicated and intractable.

In order to derive a tractable objective function, BoostMapping adapts the idea of AdaBoost [9] as the solution. The main idea of AdaBoost is to iteratively learn a sequence of weak learners such that the subsequent learners are tweaked in favor of those instances misclassified by previous learners. Following the same weak learner corresponding to one cluster c m and, thus, the goal of finding predictive keyword clusters c m turns out to be the process of seeking the best weak learner h m . As the key advantage of such a boosting idea, the objective function of cluster construction becomes very simple which learns only one cluster in each iteration.
Formally, we use the same exponential loss function with Ad-aBoost, given by To minimize Eq. 8, BoostMapping defines an importance distri-bution D ( d,e ) over all entity-document pair  X  d,e  X  . Initially, the distribution is set to be uniform with D 1 ( d,e ) = 1 |T| is the total number of entity-document pairs. In the m -th iteration, we search the optimal keyword cluster c m (the detailed design of c m will be discussed later) whose corresponding weak learner h achieves the lowest error rate with respect to the current distribution D m ( d,e ) , where  X  ( y,h ) is an function indicating if the output of h ( d,e ) is inconsistent with document label y , defined as, After the best h m is chosen,  X  m could be found by a simple binary search [9]. The distribution D m ( d,e ) is updated by = D m ( d ij ,e i ) exp[  X   X  m y ij h m ( d ij ,e i )] Such a process is repeated until M keyword clusters are collected. The property of AdaBoost theoretically guarantees that Eq. 7 is minimized after each iteration and will finally converge.
Based on the above optimization procedure, the only problem left is how to appropriately define clusters c such that the optimization of Eq. 9 is simple. As our solution, we define a cluster as a set of keywords satisfying a set of predicates defined over meta-features, given as follows, where each predicate is a binary function measuring whether one meta-feature is greater or less than a threshold ( e.g. , DocIDF  X  1 . 45 , InfoBoxTF  X  1 ).
 The predicate-based design of c facilitates the optimization of Eq. 9  X  we adopt a simple greedy strategy to construct keyword cluster c m . In particular, we enumerate all meta-features and pos-sible thresholds (note that although a threshold can be any real number, it is enumerable because the number of meaningful thresh-olds is less than the number of keyword-entity pairs), and find out the predicate whose corresponding keyword clusters ( e.g. , one con-taining keywords satisfying DocIDF  X  1 . 45 ) minimizes Eq. 9. Such a process is repeated until L predicates are collected, e.g. , c could contain keywords satisfying DocIDF  X  1 . 45  X  InfoBoxTF  X  1  X  IDPageTF  X  10 .
In this section, we will compare the pros and cons of LinearMap-ping and BoostMapping in details.

As the key advantage of LinearMapping, by assuming that the weighting function takes the linear form, it could be transformed to a standard classification problem; however, its linearity assumption also results in the weaker representation power of LinearMapping compared with BoostMapping. We can intuitively understand such a drawback by checking if the generated features l k ( d,e ) by Lin-earMapping in Eq. 4 are meaningful document features. Still take meta-feature IdPagePos as example, and assume that there are three keywords: w 1 , w 2 and w 3 with IdPagePos 0 . 1 , 0 . 9 and 1 . 0 (mea-sured by percentage) respectively, and two documents of the same length: d 1 mentioning w 1 once and w 3 eight times, and d tioning w 2 nine times. Intuitively, as shown in Figure 3, w very important keyword as it is mentioned at the top of the entity identification page, while w 2 and w 3 are not, and, thus, d be more relevant compared with d 2 ; however, LinearMapping gener-ates the same l k = 0 . 81 for both d 1 and d 2 , failing to discriminate their importance. As LinearMapping simply sums up the meta-feature of all keywords, important keywords such as w 1 might be easily diluted by noisy ones such as w 2 and w 3 . Essentially, such a drawback stems from the linear assumptions of LinearMapping. Once the real keyword weighting curve takes some shapes ( e.g. , the L-shape curve in Figure 3) that can not be well fitted by a straight line, LinearMapping will perform poorly.

Different from LinearMapping, BoostMapping can better dis-criminate important keywords from noisy ones in documents. First, the document features h m ( d,e ) generated by BoostMapping in E-q. 6, which measure the percentage of keywords sharing similar meta-features in d , are intuitively more meaningful than l measuring the summation of meta-features in LinearMapping. In the aforementioned example, BoostingMapping will learn a good cluster contain only w 1 (how to learn good clusters is discussed in Section 4.2.3 by maximizing the overall likelihood), and the corre-sponding h m ( d,e ) measuring the percentage of w 1 mentioned in d will be helpful for discriminating d 1 from d 2 . Second, as shown in Figure 3, BoostMapping learns a discrete curve which can better fit the real curve of arbitrary shape.

However, the strong representation power of BoostMapping might also lead to a potential over-fitting problem  X  the generated keyword cluster might bias towards a small number of training entities, and could not be generalized to new entities. Currently, we require that the keywords in each cluster should appear in at least 20% of documents for each training entity. Such a heuristic helps filter those biased clusters and greatly relieves the over-fitting problem.
In this section, we compare the overall performance of LinearMap-ping and BoostMapping with four different state-of-the-art baselines to demonstrate the advantages of using meta-features for entity-centric document filtering. We further compare LinearMapping and BoostMapping on different feature sets, to better understand their differences. Finally, we verify the effect of the distantly supervised clustering technique in BoostMapping.
In order to demonstrate the capability of our models on general entity-centric filtering tasks, the evaluation collections should cover general types of entities, as well as different types of entity identi-fication pages. Based on such a concern, we chose three different data sets with specification shown in Figure 5. 1. TREC-KBA [1]. This dataset was built based on a stream corpus containing 134 million news and 322 million social articles. The KBA track chose 29 Wikipedia entities including living persons from different domains ( e.g. , basketball coach  X  X ill Coen X , professor  X  X im Steyer X ) and a few organizations ( e.g. ,  X  X asic Element (company) X ). For each entity, 100  X  3000 candidate documents were collected and labeled as garbage, neutral, relevant or central. Following the same procedure in [8], we got binary labels by viewing central and relevant documents as positive, and others negative. 2. Product. To demonstrate the capacity of algorithms on different types of entity identification pages, this dataset chose electronic products as query entities ( e.g. , TV, laptop, cellphone, etc. ) iden-tified by their Amazon product pages. We used the same stream corpus in the TREC-KBA dataset to collect candidate documents for each entity, and manually labeled each document as relevan-t/irrelevant. The Product dataset uses similar meta-feature design with TREC-KBA in Figure 2, by replacing Wikipedia-specific meta-features with ones designed for Amazon pages, e.g. , ReviewTF ( w,e ) denoting how many times w appears in the review section of e  X  X  Amazon page. 3. MilQuery. Different from the above datasets where almost all entities are person entities or electronic products, this dataset covers entities of different types ( e.g. , yahoo, yogurt, tetanus vaccine, etc. ). To build this dataset, we used the 2009 Million Query Track collec-tion, and chose a subset of entity queries  X  ones that have Wikipedia pages  X  together with their labeled documents. The documents in the original corpus has three levels of relevance: very relevant, relevant and irrelevant, while we view very relevant and relevant as positive, and irrelevant as negative. Since Wikipedia articles are used to describe entities, MilQuery uses the same meta-feature design as the TREC-KBA dataset.

All the text data was preprocessed by removing stopwords, lower-casing and stemming. The extracted features were first standardized to reduce the impact of different scales of features. To deal with unbalanced data, we changed the cost factor to balance positive and negative examples. To confirm the confidence of the comparison, we used 5-fold cross validation by dividing all entities into five sets, chose 4 entity sets together with their associated documents for training, and the remaining entity set as queries for testing. Thus, each entity would not be included in the training corpus and the testing corpus at the same time.

In our experiments, we used the implementation of SVMLight [12] to learn feature weightings for LinearMapping and transfor-m continuous score to binary output. As the entity-centric docu-ment filtering problem is expected to make a binary decision, we used classification-oriented metrics including precision, recall, F1-measure and accuracy as our evaluation criteria. For BoostMapping, we empirically set the number of feature mapping to be 150 and the predicate number to be 3 (later we would investigate their impact).
We introduce four baselines to incrementally validate the perfor-mance of the proposed LinearMapping and BoostMapping models. First, we experiment QueryByName to verify the necessity of lever-aging information in the identification pages. Second, by comparing our models with QBD-TFIDF , we verify the necessity of automati-cally learning a keyword importance function. Third, we compare our models with VectorSim to demonstrate that our models better characterize the entity-relevance semantics than empirical similarity functions. Finally, we show that our models outperform RelEntity , the best algorithm among all TREC-KBA submissions. The design details of baselines are introduced as follows, 1. Query by Entity Name ( QueryByName ). This baseline treat-s entity-centric filtering as a standard keyword-based information retrieval task by using only entity names as keyword queries. Fol-lowing the standard feature design in the LETOR benchmark [18], we extract ranking features such as TFIDF and BM25 for each docu-ment (note that not all LETOR features are covered, as some features like PageRank are only available in commercial search engines), and train a ranking model by SVMRank [13], which is the state-of-the-art learning to rank technique, to predict document relevance. We then feed the predicted score as one feature into SVMLight, which will learn a threshold from training data and output binary results. 2. Query by Document based on TFIDF Scoring ( QBD-TFIDF ) [21]. This work studies the retrieval task when the query is a docu-ment. As the solution, QBD-TFIDF first extracts noun phrases from documents, and scores those phrases by a TF-IDF-based formula. The noun phrases ( e.g. , top 20) with highest score will be chosen as an extended query. In the original implementation [21], such queries are submitted to a commercial search engine to retrieve documents, while in our implementation, as our candidate documents are given, we follow the same procedure in QueryByName to extract features and learn a document ranker for relevance prediction. 3. Keyword-Vector-based Similarity ( VectorSim ). This baseline first represents each document by a TF-IDF keyword vector, and then calculates document similarity using five different metrics [10], including Euclidean distance, cosine similarity, Jaccard coefficient, Pearson correlation coefficient and averaged Kullback-Leibler Di-vergence. These measures will be fed into SVMLight as features to learn a document filter. 4. Related-Entity-based Scoring ( RelEntity ) [15]. This is the al-gorithm that achieves the best performance among all TREC-KBA submissions. As the key idea, given an entity e , RelEntity first identifies its related entities  X e which are also Wikipedia entities and appear in e  X  X  Wikipedia page, and then score document d by an emperical formula S ( d ) = 100  X  N ( d,e )+ N ( d,  X e ) , where N ( d,e ) is the number of e in d . A threshold ( T = 100 ) is manually set to produce binary scores.

The result in Figure 6 demonstrates that BoostMapping signif-icantly outperforms all four baselines. As an ideal model should achieve both high precision and high recall, we compare the model performance on F-measure and accuracy. In particular, BoostMap-ping achieves encouraging improvement against runner-up RelEn-tity in TREC-KBA (F-measure +11 . 7% , accuracy +10 . 7% ), and runner-up VectorSim in Product (F-measure +35 . 0% , accuracy +7 . 4% ) and MilQuery (F-measure +8 . 6% and accuracy +10 . 3% ). LinearMapping also achieves improvements against runner-ups in the KBA dataset (F-measure +6 . 2% , accuracy +7 . 0% ) and the Mil-Query dataset (F-measure +2 . 9% , accuracy +5 . 7% ), while obtains similar performance with the runner-up in the product dataset.
We analyze the performance difference between our models and other baselines in more details:
First, QueryByName does not perform well. We can observe that QueryByName tends to achieve high recall but very low pre-cision in all three datasets. That X  X  because traditional information retrieval techniques tend to regard a document as relevant if it men-tions the query entity a lot, which might lead to incorrect results if the document does not contain related information of the query enti-ty. The comparison demonstrates that entity description pages do provide useful information to capture the sense of entity-relevance. Moreover, for ambiguous entities ( e.g. ,  X  X asic element [company] X  and  X  X asic element [music group] X  in TREC-KBA), without lever-aging their description pages, it is difficult for QueryByName to identify their relevant documents based on only entity names.
Second, although QBD-TFIDF leverages the information from the entity identification page by selecting keywords with high scores to extend the entity name query, its performance is very similar to QueryByName . As the scoring function of QBD-TFIDF is manu-ally crafted, it might not well characterize the keyword importance for an entity. The comparison between QBD-TFIDF and our models confirms the necessity of learning the keyword weighting function from the corpus.

Third, by comparing VectorSim with BoostMapping in Figure 6, we observe that, different from QueryByName and QBD-TFIDF , VectorSim tends to have high precision and low recall in both KBA and product datasets. That is because assessors tend to label documents that are very similar to the entity identification page as relevant, and VectorSim could correctly identify those documents with high accuracy. However, there also exist many documents that are not similar to the entity identification page, and the failure in identifying these low-similarity documents leads to low recall for VectorSim .
 Fourth, RelEntity achieves satisfactory performance in TREC-KBA, demonstrating that the number of related entities is a critically useful feature in the TREC-KBA corpus. However, such a method is not generalizable  X  in Product, the result of RelEntity is not listed, because it is unclear how to similarly define relevant entities in the context of Amazon product pages; in Mil-Query, RelEntity achieves poor performance, revealing that such a feature is no longer useful when the dataset is changed. The result shows it is difficult to adapt to different datasets by relying on only one feature, while in our framework, we can capture the same intuition of RelEntity by adding one meta-feature AnchorEntity as explained in Figure 2, and let the learning algorithm determine the importance of the meta-feature for different datasets.

In general, both LinearMapping and BoostMapping achieve satis-factory performance in three datasets, demonstrating the effective-ness of feature mapping for entity-centric document filtering. In particular the results on the Product and MilQuery datasets confirm our confidence that our models can not only handle different types of entity identification pages, but also work well even if the query entities cover very different types.
Compared with LinearMapping, with stronger representation pow-er, BoostMapping achieves more significant improvement against other baselines. Since LinearMapping assumes that weight ( w,e ) takes the linear form of meta-features, the performance of Lin-earMapping largely depends on whether the datasets follow such a linear assumption, while BoostMapping does not rely on such an assumption. To verify such a claim, for each meta-feature, we train a LinearMapping model, and use the prediction accuracy to estimate the linearity of the meta-feature, i.e. , how it fits the lin-ear assumption. We then divide meta-features into two sets of the same size, containing meta-features with low linearity and high linearity respectively. The results of LinearMapping and BoostMap-ping over these two datasets ( e.g. , denoted as LinearMapping-Low and LinearMapping-High) and all meta-features ( e.g. , denoted as LinearMapping-All) are compared in Figure 7.

Figure 7 offers an insightful understanding of the differences between LinearMapping and BoostMapping. First, by compar-ing BoostMapping-Low with LinearWeight-Low, BoostMapping-Low achieves significant improvement in F-Measure (TREC-KBA +19 . 5% , Product +39 . 4% , MilQuery +8 . 6% ), which verifies our claim in Section 4.3 that BoostMapping has stronger representation power compared with LinearMapping when the linear assumption does not hold. Second, LinearMapping-High achieves compara-ble performance with BoostMapping-High in both TREC-KBA and MilQuery, verifying our claim that LinearMapping could al-so achieve stratificatory performance over meta-features with high linearity; while for Product, even LinearMapping-High achieves low F-Measure, which implies that all meta-features in the product dataset have relatively low linearity and explains the general poor performance of LinearWeight-All on all meta-features. As Boost-Mapping could benefit from meta-features of different linearity, BoostMapping-All performs better compared with LinearMapping-All in F-measure (TREC-KBA +5 . 19% , Product +30 . 3% , Mil-Query +6 . 22% ).
As one important advantage of BoostMapping, it realizes distant-ly supervised clustering to guarantee the predictive power of the generated keyword clusters for entity-centric document filtering. To verify such an advantage, we design two baselines based on with two standard clustering algorithms, K-Means and Gaussian Mixture model, which directly cluster keywords based on their meta-features without guidance from document labels, and then use SVMLight to learn the importance of each cluster. The performance of Boost-Mapping and two baselines with 90 and 180 clusters is compared in Figure 8(a). Furthermore, in order to compare these algorithms X  per-formance when there exist some irrelevant meta-features, we build up a synthetic  X  X oisy X  dataset by adding 30 randomly generated meta-features for each keyword to the original dataset. The result on the synthetic noisy dataset is displayed in Figure 8(b). As Figure 8(a) displays, when using original meta-features, Boost-Mapping outperforms K-Means and Gaussian with different cluster number in different dataset. For example, when the cluster number is 180, the improvement against the runner-up is 3 . 7% in TREC-KBA, 4 . 7% in Product and 5 . 3% in MilQuery. The result demonstrates that our designed meta-features are relevant with the task such that standard clustering algorithms following the same keyword mapping idea could achieve satisfactory performance. We are also interested in the robustness of BoostMapping when there exist some irrelevant meta-features. From Figure 8(b), we observe that the performance of KMeans-Noise and Gaussian-Noise degrade significantly due to the existence of noisy meta-features, while for BoostMapping-Noise, it is only slightly affected, which achieves large improve-ment compared with KMeans-Noise and Gaussian-Noise ( 11 . 6% in TREC-KBA, 8 . 6% in Product and 11 . 4% in MilQuery when the cluster number is 180). Such a comparison confirms our confidence about the robustness of BoostMapping  X  by using document labels as guidance, BoostMapping generates clusters that are more useful for the entity-centric filtering problem.
In this section, we investigate the effect of cluster number M and predicate number L on the performance of BoostMapping. As Figure 9(a) shows, with respect to different number of clusters, we observe that BoostMapping X  X  F-Measure increases a lot from 30 to 120 clusters in all three datasets (TREC-KBA +17 . 4% , Prod-uct +5 . 52% , MilQuery +5 . 73% ), while from 120 to 180 clusters, the performance becomes much stabler. Figure 9(b) compares the performance of BoostMapping with different number of predicates. The result shows that BoostMapping usually achieves satisfactory performance when the predicate number is 2 or 3. When the predi-cate number is large, each cluster might contain too few keywords and the model tends to overfit to some particular queries.
To give an intuitive illustration of our results, in Figure 10, we perform case studies by showing 2 example entities in each dataset, and listing some of their important/unimportant keywords with high/low weight ( w,e ) returned by BoostMapping.
 First, with respect to important keywords, we observe that, for TREC-KBA and MilQuery, BoostMapping usually returns keywords that characterize the basic information of the entity, for example, keywords  X  X dvocacy X  and  X  X ttorney X  for entity  X  X im Steyer, X  as Jim Steyer is best known to be a child advocate and civil rights attorney. For the Product dataset, BoostMapping returns not only keywords describing the product specification, e.g. ,  X  X tabilizer X  and  X  X ybrid X  for  X  X anon S95 X , but also evaluation keywords such as  X  X ice X  and  X  X asier X  (which are usually mentioned in the review sections of Amazon product pages), because labelers tend to label documents including helpful comments of the products as relevant.
Second, for unimportant keywords, BoostMapping usually returns those keywords appearing in every Wikipedia page ( e.g. ,  X  X ategory X  and  X  X ncyclopedia X ), or in some less important positions ( e.g. ,  X  X log X  and  X  X itchen X  in the reference section of Jim Steyer X  X  Wikipedia page). For product entities, BoostMapping can successfully identify those keywords referring to closely related but different entities ( e.g. ,  X  X pod X ,  X  X pad X  for entity  X  X phone X , and  X  X lympus X  for  X  X anon X ) as unimportant keywords, which are very helpful for recognizing those irrelevant documents which occasionally mention about the target entity, and mainly discuss about other entities.
In this paper, we proposed to study the entity-centric documen-t filtering task  X  given an entity represented by its identification page (e.g., an Wikpedia page), how to correctly identify its rele-vant documents in a learning framework. As the key contribution, we proposed to leverage meta-features to map keywords between training entities and testing entities. To realize the idea of meta-feature-based feature mapping, we developed two different models  X  LinearMapping and BoostMapping. Experiments on three different datasets confirmed that our models achieved significant improve-ment compared with four different baseline methods.

We are planning to further improve our framework. First, the current design of meta-features only characterizes how a keyword appears in entity identification pages, while modeling how a key-word is mentioned in target documents is also useful. For example, an important keyword mentioned in a target document X  X  title should have more contribution compared with it appearing at the end. In-corporating such type of features is promising to further improve the model performance. Second, the current framework identifies rele-vant documents based on static content of entity identification pages; however, the information for a particular entity might evolve very fast. In our future work, we will study how to model the evolution of entities in document filtering. This material is based upon work partially supported by NSF Grant IIS 1018723, the Advanced Digital Science Center of the University of Illinois at Urbana-Champaign and the Multimodal Information Access and Synthesis Center at UIUC. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies. [1] Trec knowledge base acceleration 2012, [2] S. Andrews, I. Tsochantaridis, and T. Hofmann. Support [3] D. Blei and J. McAuliffe. Supervised topic models. arXiv [4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, [5] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. [6] W. Dai, G. Xue, Q. Yang, and Y. Yu. Co-clustering based [7] A. Evgeniou and M. Pontil. Multi-task feature learning. In [8] J. Frank, M. Kleiman-Weiner, D. Roberts, F. Niu, C. Zhang, [9] J. Friedman. Stochastic gradient boosting. Computational [10] A. Huang. Similarity measures for text document clustering. [11] T. Joachims. Text categorization with support vector [12] T. Joachims. Making large scale svm learning practical. 1999. [13] T. Joachims. Training linear svms in linear time. In [14] G. Kumaran and V. R. Carvalho. Reducing long queries using [15] X. Liu and H. Fang. Entity profile based approach in [16] S. Pan, J. Kwok, and Q. Yang. Transfer learning via [17] S. Pan and Q. Yang. A survey on transfer learning. Knowledge [18] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A benchmark [19] S. E. Robertson, S. Walker, S. Jones, M. M.
 [20] L. Weng, Z. Li, R. Cai, Y. Zhang, Y. Zhou, L. Yang, and [21] Y. Yang, N. Bansal, W. Dakka, P. Ipeirotis, N. Koudas, and [22] Y. Yang and J. Pedersen. A comparative study on feature [23] B. Zadrozny. Learning and evaluating classifiers under sample
