 In applications where large amount of data arrives in sequence, e.g., stock market prediction and email filtering, simple online learning such as Perceptron [1], second-order Perceptron [2], and Passive Aggressive (PA) [4] algorithms can be easily deployed with reasonable performance and low computational cost.
For some domains, data may originate from several different sources, also known as views. For example, a web page may have a content view comprising text contained within it, a link view expressing its relationships to other web pages, and a revision view that tracks the different changes that it has undergone.
When the various data sources are independent, running several instances of the same algorithm on it and combining the output via an ensemble learning framework works well. A simple concatenation of the two sources in a vector space model could unnecessarily favor sou rces with larger number of dimensions. On the other hand, training a separate model on each source fails to make good use of the relationship among the sources, even for a baseline ensemble classifier.
To take advantage of data with multiple views, various methods such as SVM-2K [7] and alternatives [9] have been proposed. However, the two-view methods proposed so far utilizes support vector machine (SVM) [3], which is fundamen-tally a batch learning algorithm that cannot be easily tailored to work well on large scale online streaming data.

One simple approach to extend the online learning model to handle two view data is to train one model for each view independently, and combine the clas-sifier outputs just like in classical ensemble learning. However, this approach ignores the relationship between the two views. Instead of using the same idea as SVM-2K where data in one view is used to improve the SVM performance [3] on another view (single view), we take advantage of the relationship between the two views to improve the combined perfor mance. Specifically, we propose a novel online learning algorithm based on the PA algorithm, called Two-view Passive Aggressive (Two-view PA) learning. Our approach minimizes the difference be-tween the two classifier outputs, but allows the outputs to differ as long as the weighted sum of each output leads to the correct result. In classical ensemble learning, the more diverse the classifier, the better the combined performance. In a way, the Two-view PA can be viewed as an ensemble of two online classifiers, except that the two views are jointly optimized. Online learning has been researched for more than 50 years. Back in 1962, Block proposed the seminal Perceptron [1] algorithm, while Novikoff [11] later provided in the mid twentieth century. The Perceptron is known to be one of the fastest online learning algorithms. However, its performance is still far from satisfactory in practice. Recently in 2005, Cesa-Bianc hi et al. [2] proposed the Second-order Perceptron (SOP) algorithm, which takes advantage of second-order data to improve the accuracy of the original P erceptron. Compare d with Perceptron, SOP works better in terms of accura cy but requires more time to train.
In 2006, Crammer et al. [4] proposed another Perceptron-based algorithm, namely the Passive Aggressive (PA) algorithm, which incorporates the margin maximizing criterion of modern machine learning algorithms. They not only have better performance than that of the SOP algorithm but also run significantly faster. Moreover, algorithms that improved upon the PA algorithm include the Passive-Aggressive Mahalanobis [10], the Confidence-Weight (CW) Linear Clas-sifier [6], and its latest version, multi-class CW [5]. The CW algorithm updates its weight by minimizing the Kullback-Leibler divergence between the new and old weights. However, similar to the SOP algorithm, these algorithms are time consuming compared to the first-order PA.

The PA algorithm works better than the SOP in terms of both speed and accuracy. However, it can only proces s one data stream at one time. On the other hand, in batch learning, Farquhar et al. [7] proposed a large margin two-view Support Vector Machine (SVM) [3] algorithm called the SVM-2K, which is an extension of the well-known SVM algorithm. The two-view learning algorithm was shown to give better performance compared to the original SVM on different image datasets [7]. Thus, SVM-2K provides the inspiration for our current work. 3.1 Problem Setting Online learning aims to learn the weight w of a linear prediction function f ( x )=sign( w  X  x ). The online learning algorithm operates in rounds, as in-put data arrives sequentially. Let x t  X  R n be an example arriving at round t . loss can be modeled using the hinge-loss function, which equals to zero when the margin exceeds 1, as follows. The overall objective is to minimize the cumulative loss over the entire sequence of examples. From this, Crammer et al. [4] formulated three optimization prob-lems; one based on hard margin and two using soft margins, respectively named PA, PA-I, and PA-II with weight update equations as follows. where the coefficient  X  t has one of three forms. The performance of the soft margin based PA-I and PA-II algorithms are al-most identical, and both performed be tter than the hard margin based PA al-gorithm [4]. Therefore, in this work, our proposed algorithm will be developed based on the PA-I algorithm.

For the two-view online learning setting, training data are triplets ( x t , x the first view vector, x B t  X  R m is the second view vector, and y t is their com-defined as follows. To incorporate the hybrid classifier, we modify the loss function as follows. (( w A t , w B t ); ( x A t , x B t ,y t )) =  X  3.2 Relationship between Views The primary challenge of multi-view learning is to properly define the related-ness among the different views. In other words, the relatedness quantifies the agreement among the views. Moreover, one could simply disregard the agree-ment between the two prediction function s, but instead learn the hybrid predic-to correctly predict the class label.

Generally, we want the two views to agree with one another. This can be enforced by minimizing their L1-norm or L2-norm disagreements as follows. where | X | denotes the absolute function. Here we use L1-norm instead of L2-norm because it is harder to find a close-form solution for the latter. In the next section, we will define an optimization problem based on the L1-norm relatedness measure. 3.3 Two-View Passive Aggressive Algorithm The ideal objective function should include both the new loss function in (2) and the view relatedness function in (3). Similar to the PA algorithm, the new weights of the two-view learning algorithm are updated based on the optimiza-tion problem as follows. where  X  and C are positive agreement and aggressiveness parameters respec-tively. While  X  is used to adjust the importance of the agreement between the two views, C is used to control the aggressiveness property of the PA algorithm. Note that the y t multiplier in the agreement is there just for subsequent deriva-tion convenience.
For the absolute function, we have | y expressed as follows. Next, we define the Lagrangian of the optimization problem as follows.
L = 1 where  X  ,  X  ,  X  ,and  X  are positive Lagrangian multipliers.

Setting the partial derivatives of L with respect to the weight w A to zero, we have, 0= Similarly, for the other view we have Setting the partial derivatives of L with respect to weight z to zero, we have Setting the partial derivatives of L with respect to weight  X  to zero, we have, Note that  X   X  0, thus we can conclude that 0  X   X   X  C .
 Substituting (5), (6), (7), and (8) into (4), we have, Setting the partial derivatives of L with respect to weight  X  to zero, we have, where the loss t =1  X  denote, As mentioned in Equation (8), we have  X  +  X  = C and  X   X  0 so we can conclude that  X   X  C .Now  X  can be determined as follows: Substituting (11) into (9), we have,
L =  X  1 Setting the partial derivatives of L with respect to weight  X  to zero, we have, 0= Therefore, we can conclude that Similarly, we have Recall that we have  X   X  0,  X   X  0, and  X  +  X  =  X  . Hence, we can conclude that  X   X   X  and  X   X   X  . Finally, we obtain our Two-view Passive Aggressive formulation as shown in Algorithm 1. The optimal value of the two tuning parameters C and  X  can be estimated via cross validation in practice.
 Algorithm 1. Two-view Passive Aggressive Algorithm In this section, we evaluate the online classification performance of our pro-posed Two-view PA on 3 benchmark datasets, Ads [8], Product Review [9], and WebKB [12]). The single-view PA algorithm serves as the baseline. We use a different PA model for each view, naming them PA View 1 and PA View 2 .We also concatenate the input feature vectors from each view to form a larger fea-ture set, and report the results. We denote this alternative approach as PA Cat . The dataset summary statistics are shown in Table 1. We note that the Ads and WebKB datasets are very imbalanced, which led us to use F-measure instead of accuracy to evaluate the classificat ion performance. To be fair, we choose C =0 . 1and  X  =0 . 5 for all PA algorithms. All experiments were conducted using 5-fold cross validation.
 4.1 View Difference Comparison the difference in prediction output betw een the two views. Figures 1(a), 2(a), and 3(a) show the view differences fo r the three datasets, respectively.
Figures 1(b), 2(b), and 3(b) plot the cumulative view difference at round t , views as the algorithm adapts to the dataset. The smaller it is, the more related the two views.

Compared to the Product Review and We bKB datasets, the view difference for the Ads dataset varies very much. Thi s means that the agreement between the two views is not stable. As expected, its cumulative view difference turns out to be the largest among the three datasets. Hence, we would expect a classifier based on simple concatenation of the two views to yield poor classification performance. This is in fact confirmed subsequently in the poor PA Cat result for the Ads dataset in Table 2.

On the other end of the spectrum, both the average and cumulative view difference for the WebKB dataset is the smallest. Therefore, one should be able to combine the two views into a single view and just run a simple PA algorithm to obtain a decent classification perform ance. This hypothesis is confirmed in Table 2, where the PA Cat result outperforms either view by more than 2%. 4.2 Ads Dataset The Ads dataset was first used by Kushmerick [8] to automatically filter ad-vertisement images from web pages. The Ads dataset comprises more than two views. In this experiment, we only use four views including image URL view , destination URL view , base URL view ,and alt view . The first and second orig-inal views were concatenated as View 1 and the remaining two original views were concatenated as View 2. This dataset has 3279 examples, including 459 positive examples (ads), with the remaining as negative examples (non-ads). The experimental results on the Ads dataset are shown in Table 2, where the F-measure of the proposed algorithm is the best. The Two-view PA performed up to 2% better than the runner-up, PA View 1. As previously discussed, PA View 1 is better than PA Cat since the two views have quite different classification outputs. 4.3 Product Review Dataset The Product Review dataset is crawled from popular online Chinese cell-phone forums [9]. The dataset has 1000 true reviews and 1000 spam reviews. It consists of two sets of features: one based on review content ( lexical view ) and the other basedonextractedcharacteri stics of the review sentences ( formal view ).
The experimental results on this dataset are shown in Table 2. Again, Two-view PA performs better than the other algorithms. The improvement is more than 2% compared with the runner-up. In this dataset, PA Cat performed better than either view alone. This is expected since the view difference between the two views are quite small, as shown in Figure 2.
 Moreover, PA Cat is only 0.41% better than the best individual PA View 1. This is because PA Cat does not take into account the view relatedness informa-tion. The best performer here is the Two-view PA, which beats the runner-up by almost 2%. 4.4 WebKB Course Dataset The WebKB course dataset has been freq uently used in the empirical study of multi-view learning. It comprises 1051 web pages collected from the computer science departments of four universities. Each page has a class label, course or non-course. The two views of each page are the textual content of a web page ( page view ) and the words that occur in the hyperlinks of other web pages point-ing to it ( link view ), respectively. We used a processed version of the WebKB course dataset [12] in our experiment. The performance of PA Cat here is also b etter than the best single view PA. However, the view difference of Two-view PA is much smaller than that of the PA algorithm as shown in Figure 3. Hence, Two-view PA performed more than 3% better than PA Cat, and 5% better than the best individual view PA.
Compared to the Ads and Product Revie w datasets, the view difference on the WebKB dataset is the smallest. It means that we are able to combine the two views into a single view. Therefore, the PA Cat performance on the WebKB dataset is improved more than 2% compared with the individual view PA. In this paper, we proposed a hybrid model for two-view passive aggressive al-gorithm, which is able to take advantage of multiple views of data to achieve an improvement in overall classification performance. We formulate our learning framework into an optimization problem and derive a closed form solution.
There remain some interesting open problems that warrant further investiga-tion. For one, at each round we could adjust the weight of each view so that the better view dominates. In the worst case where the two views are completely related or co-linear, e.g., view 1 is equal to view 2, our Two-view PA degener-ates nicely into a single view PA. We would also like to extend Two-view PA to handle multiple views and multiple classes. Formulating a multi-view PA is non-trivial, as it involves defining multi-view relatedness and minimizing (V choose 2) view agreements, for a V-view problem. Formulating a multi-class Two-view PA should be more feasible.

