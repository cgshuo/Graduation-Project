 for different groups.
 Model Group-level mixture components
Hierarchical DPs  X 
Transformed DPs  X 
Hierarchical DPs with random effects (  X  hierarchical DPs with random effects as proposed in this pap er. mixture models.
 Given a template mixture of two components with parameters  X  ture model for each group can have m of the copies of  X  no direct way to enforce  X  in our proposed model.
 transformed DP has been applied (Sudderth et al, 2005).
 learning the number of components in the model in a data-driv en fashion. 2.1 Dirichlet process mixture models A Dirichlet process DP (  X  when the number of components is unknown a priori (Rasmussen, 2000). The generative process for a mixture of Gaussian distributions with component mean  X  random effects. written, using a stick breaking construction (Sethuraman, 2004), as: where y the labels z where n  X  i assigned to a new component is proportional to  X  already assigned to it has a higher probability to attract th e next observation. 2.2 Hierarchical Dirichlet processes number of components in the model can be inferred from data.
 Let y portions,  X  Then, the hierarchical DP can be written as follows, using a s tick breaking construction: nents described by the  X  The hierarchical DP has clustering properties similar to th at for DP mixtures, i.e., where h l The probability that a new local cluster is generated within group j is proportional to  X  local cluster labels that is typically of interest. We take  X  random effects as follows: Each group j has its own component mean u eters come from a common prior distribution N (  X  u eters u the same global cluster k .
 Equations (2) and (3). In each iteration we sample labels h = { h and component parameters  X  = {  X  nately.
 We sample t where p ( y ji | h  X  ji u ,  X  ,  X  , X  ) = X In Equation (5a) the summation is over components in A = { k | some h k } In this case, since u Equation (5b) the summation is over B = { k | no h the unknown random effects parameter u and sample u approximate the integral by sampling new values for  X  evaluating p ( y Samples for l As in the sampling of h u new values for  X  the move based on a Metropolis-Hastings acceptance rule.
 from the data. From a template mixture model with three mixtu re components we generated 10 variability in both component means and mixing proportions of the mixture model. activation images.
 same type of model could be developed for the 3-dimensional c ase. y and the model parameters, we model the activation y where C = { c c using (b) DP mixtures, (c) hierarchical DPs, and (d) hierarc hical DP with random effects. Gaussian-shaped surface centered at b as Using Bayes X  rule we write this term as P ( c | x class prior probability P ( c ) . p ( x is a normal density with mean b expert is highest at the center of the activation and gradual ly decays as x center. p ( x for all positions in the brain. If x selects the background expert for the voxel.
 We place a hierarchical DP prior on  X  h  X  algorithm. We rely on an approximation of the integrals by sa mpling new values for b from their priors and new values for image-specific random ef fects parameters from N ( b and N ( h parameters. summarizes the brain activation) was produced using standa rd fMRI preprocessing. sampling algorithm was run for 3000 iterations. ford). (a) DP mixture, (b) hierarchical DP, and (c) hierarch ical DP with random effects. lation errors are shown as standard deviations.
 ellipses using one standard deviation of the width paramete rs  X  the estimated height h shared across images are drawn with the same color.
 The DPs shown in Figure 4(b) seem to overfit with many bumps and show a relatively poor gen-error.
 6(c). controls and patients for a particular disorder.
 Acknowledgments Research Network (FIRST BIRN; 1 U24 RR021992, www.nbirn.ne t); the Transdisciplinary Imag-ing Genetics Center (P20RR020837-01); and the National All iance for Medical Image Comput-Foundation under awards number IIS-0431085 and number SCI-0225642. References
