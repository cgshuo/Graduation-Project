 We consider the problem of document indexing and representa-tion. Recently, Locality Preserving Indexing (LPI) was proposed for learning a compact document subspace. Different from Latent Semantic Indexing (LSI) which is optimal in the sense of global Euclidean structure, LPI is optimal in the sense of local manifold structure. However, LPI is not efficient in time and memory which makes it difficult to be applied to very large data set. Specifi-cally, the computation of LPI involves eigen-decompositions of two dense matrices which is expensive. In this paper, we propose a new algorithm called Regularized Locality Preserving Indexing (RLPI). Benefit from recent progresses on spectral graph analysis, we cast the original LPI algorithm into a regression framework which en-able us to avoid eigen-decomposition of dense matrices. Also, with the regression based framework, different kinds of regularizers can be naturally incorporated into our algorithm which makes it more flexible. Extensive experimental results show that RLPI obtains similar or better results comparing to LPI and it is significantly faster, which makes it an efficient and effective data preprocessing method for large scale text clustering, classification and retrieval. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Indexing methods ; I.5.2 [ Pattern Recognition ]: De-sign Methodology  X  Feature evaluation and selection Algorithms, Performance, Theory Regularized Locality Preserving Indexing, Document Representa-tion and Indexing, Dimensionality Reduction The work was supported in part by the U.S. National Science Foundation NSF IIS-05-13678, NSF BDI-05-15813 and MIAS (a DHS Institute of Discrete Science Center for Multimodal Informa-tion Access and Synthesis). Any opinions, findings, and conclu-sions or recommendations expressed here are those of the authors and do not necessarily reflect the views of the funding agencies. Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00.
Document representation and indexing have been a key problem for document analysis and processing, such as clustering, classifi-cation and retrieval [8],[12],[23]. If we denote by document space the set of all the documents, different indexing algorithms see dif-ferent structures of the document space. The Vector Space Model (VSM) might be one of the most popular model for document rep-resentation. Each document is represented as a bag of words .Cor-respondingly, the document space is associated with a Euclidean structure and the inner product (or, cosine similarity) is used as the standard similarity measure for documents. Unfortunately, VSM suffers from some problems such as synonymy and polysemy .
To deal with these problems, Latent Semantic Indexing (LSI) was proposed [8]. Given a term-document matrix X , LSI applies Singular Value Decomposition (SVD) to project the document vec-tors into a subspace so that cosine similarity can accurately repre-sent semantic similarity. LSI aims to find the best subspace approx-imation to the original document space in the sense of minimizing the global reconstruction error. The basis functions obtained by LSI are the eigenvectors of the matrix XX T . It would be impor-tant to note that LSI is different from Principal Component Analy-sis (PCA) in that XX T is generally not the data covariance matrix. In fact, this occurs only when the documents has a zero mean. LSI received a lot of attentions during these years and many variants of LSI have been proposed [1], [13].

LSI is optimal in the sense of preserving the global geometric structure of the document space (inner product). However, it might not be optimal in the sense of discrimination. Specifically, LSI might not be optimal in separating documents with different top-ics. Recently, Locality Preserving Indexing (LPI) is proposed to discover the discriminant structure of the document space. It has shown that it can have more discriminative power than LSI. A rea-sonable assumption behind LPI is that, close inputs should have similar topics. The detailed discriminant analysis of LPI can be found in [12], [3]. Previous experiments on document clustering have demonstrated the effectiveness of LPI [4]. However, the com-putational complexity of LPI is very expensive because it involves eigen-decompositions of two dense matrices. It is almost infeasible to apply LPI on very large data set.
 In this paper, we propose a new algorithm called Regularized Locality Preserving Indexing (RLPI). RLPI is fundamentally based on LPI. It shares the same locality preserving character as LPI, but can be efficiently computed. Specifically, RLPI decomposes the LPI problem as a graph embedding problem plus a regularized least squares problem. Such modification avoids eigen-decomposition of dense matrices and can significantly reduce both time and mem-ory cost in computation. Moreover, with a specifically designed graph in supervised situation, the graph embedding problem in RLPI becomes trivial and RLPI only needs to solve the regularized least squares problem which is a further saving of time and memory. Such properties make RLPI an efficient data processing method for large scale text clustering, classification, retrieval, etc .
The rest of the paper is organized as follows: in Section 2, we give a brief review of LSI and LPI. Section 3 introduces our algo-rithm and give a theoretical analysis of the algorithm. Extensive experimental results on document clustering and categorization are presented in Section 4. Finally, we provide some concluding re-marks and suggestions for future work in Section 5. In this Section, we provide a brief review of Latent Semantic Indexing (LSI) [8] and Locality Preserving Indexing (LPI) [12], followed by a computational complexity analysis of these two al-gorithms.
LSI is one of the most popular algorithms for document indexing [8]. It is fundamentally based on Singular Value Decomposition (SVD) and try to extract the most representative features (minimize the reconstruction error). Given a set of documents { x i , which can be represented as a term-document matrix X = [ x , x 2 ,  X  X  X  , x m ] . Suppose the rank of X is r , LSI decompose the X by using SVD as follows: where  X = diag (  X  1 ,  X  X  X  , X  r )and  X  1  X   X  2  X  X  X  X  X  X   X  r are the vectors. We have U T U = V T V = I . LSI uses the first d vectors in U as the transformation matrix to embed the original document into a d -dimensional subspace. It can be easily checked that the column vectors of U are the eigenvectors of XX T . It would be important to note that XX T becomes the data covariance matrix if the data points have a zero mean, i.e. X e = 0 where e =[1 ,  X  X  X  , 1] such a case, LSI is identical to Principal Component Analysis [9]. More details on theoretical interpretations of LSI using SVD can refer to [2], [19].
Different from LSI which aims to extract the most representa-tive features, LPI aims to extract the most discriminative features. Given a similarity matrix W , LPI can be obtained by solving the following minimization problem: where D is a diagonal matrix whose entries are column sums of W ( D ii = j W ji )and L = D  X  W is the graph Laplacian [7]. LPI constructs the similarity matrix W as: where N p ( x i ) is the set of p nearest neighbors of x i jective function in LPI incurs a heavy penalty if neighboring points x and x j are mapped far apart. Therefore, minimizing it is an at-tempt to ensure that if x i and x j are  X  X lose X  then y i (= a (= a T x j ) are close as well [12]. Finally, the basis functions of LPI are the eigenvectors associated with the smallest eigenvalues of the following generalized eigen-problem: Sincewehave L = D  X  W , it is easy to check that the minimiza-tion problem in Eqn. (2) is equivalent to the following maximiza-tion problem: and the optimal a  X  X  are also the maximum eigenvectors of eigen-problem: which in some cases can provide a more numerically stable solution [10]. In the following of our paper, we will discuss LPI based on this formulation.
 To get a stable solution of the above eigen-problem, the matrix XDX T is required to be non-singular [10]. However, in many text analysis tasks, the number of features (terms) is usually larger than the number of documents which indicates the singularity of XDX T . In such a case, SVD can be used to solve this problem [4].

Suppose we have the SVD decomposition of X showninEqn. (1). Let X = U T X = X  V T and b = U T a ,wehave and Now, the objective function of LPI in (4) can be rewritten as: and the optimal b  X  X  are the maximum eigenvectors of eigen-problem: It is easy to check that XD X T is nonsingular and the above eigen-problem can be stably solved. After we get b  X  ,the a  X  can be ob-tained by solving a set of linear equations systems U T a = b noticing that given U , b  X  , there will be infinitely many solutions of a which satisfy this equations system 1 . Among all these solutions, is obviously one of them. This approach has been used in ordinary LPI algorithm [12], [4] to solve the singularity problem. Now let us analyze the computational complexity of LSI and LPI. We consider the case that the number of features ( n )islarger than the number of documents ( m ), which is usually the case for text processing tasks. The term flam [21], a compound operation consisting of one addition and one multiplication, is used to present operation counts.

Since the term-document matrix X is usually sparse, we can use the Lanczos algorithm to efficiently compute the first d singular vectors of X .Let s denote the average number of non-zero fea-tures per document, the Lanczos algorithm requires 2 q 1 dm ( s +8) flam, where q 1 is the number of iterations in Lanczos [22]. Thus,
Unless n&lt;m and rank ( X )= n . In this case, U will be an orthogonal matrix and there is an unique solution of equation U
T a = b  X  which is exactly U b  X  . computing the first d projective functions in LSI has linear-time complexity with respect to m .

The major computations in LPI include three steps: 1. p -nearest neighbor graph construction as in Eqn. (3). This 2. A complete SVD decomposition of term document matrix as 3. Computing the matrix XW X T (, XD X T ) and computing Thus, the overall time complexity of LPI measured by flam is at least which is cubic-time complexity with respect to m . Besides storing the term-document matrix, the additional memory requirement of LPI is Such a high computational cost (both on time and memory) makes it infeasible to apply LPI on document sets with large size.
Although LPI can learn a compact document representation which is beneficial for many text analysis tasks such as clustering [4] and categorization, the high computational cost restricts it to be applied to large scale data sets. In this section, we describe our approach which can overcome this difficulty.
In order to solve the eigen-problem in Eqn. (5) efficiently, we use the following theorem: T HEOREM 1. Let y be the eigenvector of eigen-problem with eigenvalue  X  .If X T a = y ,then a is the eigenvector of eigen-problem in Eqn. (5) with the same eigenvalue  X  .

P ROOF .Wehave W y =  X D y . At the left side of Eqn. (5), replace X T a by y ,wehave Thus, a is the eigenvector of eigen-problem Eqn. (5) with the same eigenvalue  X  .
 Theorem 1 shows that instead of solving the eigen-problem in Eqn. (5), the LPI basis functions can be acquired through two steps: 1. Solve the eigen-problem in Eqn. (8) to get y . 2. Find a which satisfies X T a = y . In reality, such a might
The advantages of this two-step approach are as follows: 1. The matrix D is guaranteed to be positive definite and there-2. There exist many efficient iterative algorithms ( e.g ., LSQR
In text processing tasks, the number of documents is often smaller than the number of features (words). Thus, the minimization prob-lem (9) is ill posed . We may have infinite many solutions for the linear equations system X T a = y (the system is underdetermined). The most popular way to solve this problem is to impose a penalty on the norm of a : This is so called regularization and is well studied in statistics [11]. The  X   X  0 is a parameter to control the amounts of shrinkage. Now we can see the third advantage of the two-step approach: 3. Since the regression is used as a building block, the regu-
The above two-step approach essentially performs regression af-ter the spectral analysis of the graph. Therefor, it can be named as Spectral Regression (SR) [5]. And we named our new algorithm as Regularized Locality Preserving Indexing (RLPI). Given a set of documents { x i } m i =1  X  R n , the algorithmic procedure of RLPI is stated below: 1. Adjacency graph construction :Let G denote a graph with 2. Eigen decomposition : Solve the eigen-problem 3. Regularized least squares : Find d vectors a 1 ,  X  X  X  , a 4. RLPI Embedding :Let A =[ a 1 ,  X  X  X  , a d ] , the embedding is
The regularized least squares in Eqn. (10) can be rewritten in the matrix form as: Requiring the derivative of right side with respect to a vanish, we get When  X &gt; 0 , this regularized solution will not satisfy the linear equations system X T a = y and a will not be the eigenvector of eigen-problem in Eqn. (5). It is interesting and important to see when (15) gives the exact solutions of eigen-problem (5). Specifi-cally, we have the following theorem: T HEOREM 2. Suppose y is the eigenvector of eigen-problem in corresponding projective function a calculated in Eqn. (15) will be the eigenvector of eigen-problem in Eqn. (5) as  X  deceases to zero. P ROOF . See Appendix A.

In text processing tasks, the number of documents is often smaller than the number of features (words) and the document vectors are usually linearly independent, i.e ., rank ( X )= m . In this case, we will have a stronger conclusion which is shown in the following Corollary.

C OROLLARY 3. If the document vectors are linearly indepen-dent, i.e ., rank ( X )= m , all the projective functions calculated by Eqn. (15) are the eigenvectors of eigen-problem in Eqn. (5) as  X  deceases to zero. Thus, the solutions of RLPI are identical to the solutions of LPI.

P ROOF . See Appendix B.
In supervised learning tasks, e.g ., text categorization, the label information can be used to build the graph W . In this subsection, we will examine how to build the graph W with label information to further reduce the computational cost of RLPI.

Suppose the m documents { x i } m i =1 belong to c classes. Let m be the number of documents in the j -th class, c j =1 m j With label information available, a natural way to define W can be: To simplify our exposition, we assume that the documents { are ordered according to their labels. It is easy to check that the matrix W defined in Eqn. (16) has a block-diagonal structure where W ( j ) is a m j  X  m j matrix with all the elements equal to 1 . We also have the D as the diagonal matrix. Thus, the eigenvalues and eigenvectors of W y =  X D y are the union of the eigenvalues and eigenvectors of its blocks (the latter padded appropriately with zeros) [10]: It is straightforward to show that the above eigen-problem has the eigenvector e ( j )  X  R m j associated with the eigenvalue 1, where e ) =[1 , 1 ,  X  X  X  , 1] T [7]. Also Rank ( W ( j ) )=1 , there is only one non-zero eigenvalue of W ( j ) . Thus there are exactly c eigen-vectors of eigen-problem W y =  X D y .Theyare These eigenvectors correspond to the same eigenvalue 1. Since 1 is a repeated eigenvalue, we could just pick any other c orthogonal vectors in the space spanned by { y j } in Eqn. (18), and define them to be our c eigenvectors [10]. The vector of all ones is naturally in the spanned space. This vector is useless since the corresponding projective function will embed all the documents to the same point. In reality, we can pick the vector of all ones as our first eigenvector and use Gram-Schmidt process to get the remaining c  X  1 orthog-onal eigenvectors. The vector of all ones can then be removed.
The above analysis shows that with the W defined as in Eqn. (16) in supervised case, the first two steps of RLPI become trivial. We can directly get the y  X  X  which is a significant saving of both time and memory for RLPI computation. It makes RLPI applicable for very large scale supervised learning tasks.
 any vector y in the space spanned by { y t } in Eqn. (18) are the same rows of Y are the same, where Y =[ y 1 ,  X  X  X  , y c  X  1 ] . Corollary (3) shows that when the sample vectors are linearly independent, the c  X  1 projective functions of LPI are exactly the solutions of the c linear equations systems X T a j = y j .Let A =[ a 1 ,  X  X  X  the transformation matrix which embeds the data points into the LPI subspace as: The columns of matrix Y T are the embedding results of samples in the LPP subspace. Thus, the documents with the same label are corresponding to the same point in the LPI subspace when the document vectors are linearly independent. This property will be very useful for designing efficient classifiers: We simply need to compare the distances of a test point to the centroids of each class in the subspace.
Now let us analyze the computational cost of RLPI. Let s denote the average number of non-zero features per document and we need calculate d projective functions in RLPI. The RLPI computation involves three steps: m 3 m ( p + m + n )+ nd s )+5 ndt 2 m ( p + d +1)+ n ( d +2) 1. p -nearest neighbor graph construction in Eqn. (11) which re-2. Computing the fist d eigenvectors of the generalized eigen-3. Solving d regularized least squares problems in Eqn. (13).
In supervised case, the first two steps are not necessary. We can directly get the responses through a Gram-Schmidt process on y in Eqn. (18) which requires ( mc 2  X  1 3 c 3 ) flam [21].
We summarize our complexity analysis of RLPI in Table 1, to-gether with LPI. The main conclusions include:
LSRQ converges very fast [17]. In our experiments, 15 iterations are enough.
In this section, several experiments on TDT2 and 20Newsgroups data sets were performed to show the effectiveness of our proposed algorithm. All of our experiments have been performed on an Intel Pentium D 3.20GHz Linux machine with 2GB memory. For the purpose of reproducibility, we provide our algorithms and data sets used in these experiments at: http://www.cs.uiuc.edu/homes/dengcai2/Data/data.html
Clustering is one of most crucial techniques to organize the doc-uments in an unsupervised manner. The ordinary clustering algo-rithms ( e.g . K-means) can be performed in the original document space or in the reduced document space (by using the dimension-ality reduction algorithms, e.g ., LSI, LPI). In this experiment, we investigate the use of dimensionality reduction algorithms for text clustering. The following six methods are compared in the experi-ment: Note that, the two methods LPI and RLPI need to construct a graph on the documents. In this experiment, we use the same graph for these two methods and the parameter p (number of nearest neigh-bors) was set to 7. The parameter  X  in RLPI was set to 0.1.
All these algorithms are tested on the TDT2 corpus. The TDT2 corpus 3 consists of data collected during the first half of 1998 and taken from 6 sources, including 2 newswires (APW, NYT), 2 radio programs (VOA, PRI) and 2 television programs (CNN, ABC). It consists of 11201 on-topic documents which are classified into 96 semantic categories. In this experiment, those documents appear-ing in two or more categories were removed, and only the largest 30 categories were kept, thus leaving us with 9,394 documents in total.
The clustering result is evaluated by comparing the obtained la-bel of each document with that provided by the document corpus. Two metrics, the accuracy ( AC ) and the normalized mutual in-formation metric ( MI ) are used to measure the clustering perfor-mance [4], [23]. Given a document x i ,let r i and s i be the obtained cluster label and the label provided by the corpus, respectively. The AC is defined as follows: where n is the total number of documents and  X  ( x, y ) is the delta function that equals one if x = y and equals zero otherwise, and map( r i ) is the permutation mapping function that maps each clus-ter label r i to the equivalent label from the data corpus. The best mapping can be found by using the Kuhn-Munkres algorithm [16].
Let C denote the set of clusters obtained from the ground truth and C obtained from our algorithm. Their mutual information metric MI ( C, C ) is defined as follows: trarily selected from the corpus belongs to the clusters c ily selected document belongs to the clusters c i as well as c the same time. In our experiments, we use the normalized mutual information MI as follows: where H ( C ) and H ( C ) are the entropies of C and C , respec-tively. It is easy to check that MI ( C, C ) ranges from 0 to 1. MI =1 if the two sets of clusters are identical, and MI =0 if the two sets are independent.
Besides clustering the whole data set into 30 clusters, the evalu-ations were also conducted with different number of clusters, rang-ing from 2 to 10. For each given cluster number c , 50 tests were conducted on different randomly chosen categories, and the aver-age performance was computed over these 50 tests (except the 30 cluster case). For each test, K-means algorithm was applied 10
Nist Topic Detection and Tracking corpus at http://www.nist.gov/speech/tests/tdt/tdt98/index.htm times with different start points and the best result in terms of the objective function of K-means was recorded. After LSI, LPI, or RLPI, how to determine the dimensions of the subspace is still an open problem. In this experiment, we keep c dimensions for all the three algorithms as suggested by previous study [4].

Table 2 and Figure 1 show the average accuracy and average mutual information of the six algorithms. LSI seems not promis-ing in dimension reduction for clustering because the K-means on the LSI subspace is even worse than K-means on the original doc-ument space. One may iterate all the possible dimensions for bet-ter performance of LSI as suggested in [4]. However, it may not possible to do so in a real case. Clustering using PLSI is even worse. NMF-NCW method achieves better performance than Base-line which is consistent with previous study [23], [4]. Both LPI and RLPI achieve significant improvements over other four algorithms. The reason is that LPI and RLPI try to reveal the local geometric structure of document space. More detailed analysis and experi-ments of document clustering using LPI are provided in [4].
Table 3 and Figure 2 show the processing time of the six algo-rithms. The processing time of LSI, LPI and RLPI include two parts: dimensionality reduction time and time of K-means on the reduced subspace. The processing time of Baseline and NMF-NCW methods are simply the time of clustering approaches (K-means and nonnegative matrix factorization). PLSI estimate the probability of each document belongs to each cluster, which can be directly used to infer the clustering result. Thus, the processing time of PLSI is only the dimensionality reduction (model estima-tion) time. After dimensionality reduction of LSI (LPI and RLPI), K-means is performed in a very low dimensional subspace thus is much more efficient then K-means in the original document space. The results here further show the advantage of dimensionality re-duction for clustering. Clustering based on LSI is the most efficient approach. However, the low clustering accuracy makes LSI ap-proach less attractable. Although the NMF-NCW method achieves better performance than Baseline method, the high computational cost (NMF-NCW spent more than 4 hours for clustering 9,394 doc-uments into 30 classes!) makes it not applicable on large document set. The same shortcoming exists for LPI approach. It can not be applied with 9,394 documents due to the memory limit. Consider both accuracy and efficiency, RLPI is obviously the best among the six compared algorithms for document clustering.
Text categorization is an active research area in machine learn-ing and information retrieval. Many statistical classification meth-ods have been applied to this problem, including Naive Bayes, k Nearest Neighbor (kNN) and Support Vector Machine (SVM) [14], [24]. In this experiment, we investigate the use of dimensionality reduction algorithms for text categorization. The following three classifiers are used in the experiment: 84.3  X  0.2 1.218 , 0.923 0.004 , 0.085 86.7  X  0.2 1.896 , 0.749 0.005 , 0.072 88.0  X  0.2 2.599 , 0.590 0.006 , 0.061 89.0  X  0.2 3.329 , 0.464 0.007 , 0.048 84.4  X  0.2 0 , 43.98 0 , 33.34 86.7  X  0.2 0 , 61.93 0 , 47.74 88.2  X  0.2 0 , 76.58 0 , 59.59 89.3  X  0.2 0 , 85.38 0 , 67.73 84.3  X  0.2 21.33 , 1.566 1.943 , 0.132 0.452 , 0.132 86.6  X  0.2 40.11 , 1.212 3.739 , 0.126 0.941 , 0.126 88.0  X  0.2 62.01 , 0.933 6.119 , 0.099 1.561 , 0.099 89.0  X  0.2 86.71 , 0.675 9.064 , 0.078 2.295 , 0.078 All the three classifiers are performed in original document space (Baseline) as well as LSI (PLSI, LPI and RLPI) subspace. The dimension of the LSI (PLSI and LPI) subspace is the number of categories c (= 20) and the dimension of the RLPI subspace is c 1(= 19) . The value of parameter  X  in RLPI is also set to 0.1. The parameter k in kNN and C in SVM are tuned to achieve the best Baseline performance.
 We use the popular 20 Newsgroups 4 as our data set. The 20 Newsgroups is a data set collected and originally used for document classification by Lang [15]. We use the  X  X ydate X  version which contains 18,846 documents, evenly distributed across 20 classes. This corpus contains 26214 distinct terms after stemming and stop word removal. The original split is separated in time, with 11,314 (60%) training documents and 7,532 (40%) testing documents. In order to examine the effectiveness of different algorithms with dif-ferent size of the training set, we further perform several tests that the training set contains 5%, 10%, 20%, 30%, 40% and 50% doc-http://people.csail.mit.edu/jrennie/20Newsgroups/ Table 5: Computational time of LSI, PLSI, LPI and RLPI (s)
Figure 4: Computational time of LSI, PLSI, LPI and RLPI uments. For these tests, we averaged the results over 10 random splits.

The classification results of the three classifiers on five document representation methods are listed in Table 4 and Figure 3. Table 5 and Figure 4 show the dimensionality reduction time of the four algorithms. The main observations from the performance compar-isons include:
We summarize the experiments below: 1. The low dimensionality of the document subspace obtained 2. The effectiveness and efficiency are two essential factors in
We have proposed a new algorithm for document indexing and representation, called Regularized Locality Preserving Indexing. RLPI shares the same locality preserving character as LPI while avoids the expensive computation. Specifically, RLPI can be com-puted by a sparse matrix eigen-decomposition followed with a reg-ularized least squares. Moreover, with a specific graph design in su-pervised case, the eigen-decomposition becomes trivial and RLPI only needs to solve a set of regularized least squares problems. Such property makes RLPI can be efficiently computed even for a large scale data set. Extensive experiments on TDT2 and 20News-groups datasets demonstrated the effectiveness and efficiency of RLPI.

There is a parameter  X  which controls the smoothness of the basis functions of RLPI. Our theoretical analysis shows that RLPI gives the same solution of LPI when  X  decreases to zero. For super-vised learning, when the training set is small, LPI tends to over-fit the training data. In such case, a smoother (with small norm) basis function is usually preferred and RLPI with  X &gt; 0 can have bet-ter performance than LPI. However, it remains unclear that how to automatically estimate the best  X  . [1] R. Ando. Latent semantic space: Iterative scaling improves [2] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Latent [3] D. Cai and X. He. Orthogonal locality preserving indexing. [4] D. Cai, X. He, and J. Han. Document clustering using [5] D. Cai, X. He, and J. Han. Spectral regression for [6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support [7] F. R. K. Chung. Spectral Graph Theory , volume 92 of [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [9] R.O.Duda,P.E.Hart,andD.G.Stork. Pattern [10] G. H. Golub and C. F. V. Loan. Matrix computations . Johns [11] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of [12] X. He, D. Cai, H. Liu, and W.-Y. Ma. Locality preserving [13] T. Hofmann. Probabilistic latent semantic indexing. In Proc. [14] T. Joachims. Text categorization with support vector [15] K. Lang. Newsweeder: Learning to filter netnews. In [16] L. Lovasz and M. Plummer. Matching Theory .Akad  X  e miai [17] C. C. Paige and M. A. Saunders. Algorithm 583 LSQR: [18] C. C. Paige and M. A. Saunders. LSQR: An algorithm for [19] C. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala. [20] R. Penrose. A generalized inverse for matrices. In [21] G. W. Stewart. Matrix Algorithms Volume I: Basic [22] G. W. Stewart. Matrix Algorithms Volume II: Eigensystems . [23] W. Xu, X. Liu, and Y. Gong. Document clustering based on [24] Y. Yang and X. Liu. A re-examination of text categoriztion
P ROOF . Suppose rank ( X )= r , the SVD decomposition of X is where  X = diag (  X  1 ,  X  X  X  , X  r ), U  X  R n  X  r , V  X  R m  X  r have U T U = V T V = I .The y is in the space spanned by row vectors of X , therefor, y is in the space spanned by column vectors of V . Thus, y can be represented as the linear combination of the column vectors of V . Moreover, the combination is unique because the column vectors of V are linear independent. Suppose the com-bination coefficients are b 1 ,  X  X  X  ,b r .Let b =[ b 1 ,  X  X  X  have: V b = y  X  V T V b = V T y  X  b = V T y  X  VV T y = y (19) To continue our proof, we need introduce the concept of pseudo inverse of a matrix [20], which we denote as (  X  ) + . Specifically, pseudo inverse of the matrix X can be computed by the following two ways: and The above limit exists even if X T X is singular and ( X T does not exist [20]. Thus, the regularized least squares solution in Eqn. (15) Combine with the equation in Eqn. (19), we have By Theorem (1), a is the eigenvector of eigen-problem in Eqn. (5).

P ROOF . The matrices W and D are of size m  X  m and there are m eigenvectors { y j } m j =1 of eigen-problem (8). Since rank ( X )= m , all these m eigenvectors y j are in the space spanned by row vectors of X . By Theorem (2), all m corresponding a j of RLPI in Eqn (15) are eigenvectors of eigen-problem in Eqn. (5) as  X  decreases to zero. They are Consider the eigen-problem in Eqn. (6), since the m eigenvectors y are also in the space spanned by row vectors of X = U T X =  X  V T , eigenvector b j will be the solution of linear equations sys-tem X T b j = y j . The row vectors of X = X  V T are linearly independent, thus b j is unique and Thus, the projective functions of LPI
