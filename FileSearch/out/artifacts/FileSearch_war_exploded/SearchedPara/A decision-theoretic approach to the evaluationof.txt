 1. Background
Information retrieval (IR) systems play an important role in information focusing and knowledge exten-sion. Evaluation is a major force in research, development and applications related to IR ( Saracevic, 1995 ).  X  X  X he challenge for the next decade, X  X  as Robertson and Hancock-Beaulieu (1992, p. 465) pointed out,  X  X  X s to explore the multiple dimensions and components of the new generation of information retrieval systems by experimenting with a diversity of evaluative approaches. X  X 
The overwhelming majority of IR evaluations have used the system-centered approach, which focuses on the assessment of search algorithms using statistical measures, with the best-known being recall (the ratio of relevant items retrieved to all relevant items) and precision (the ratio of relevant items retrieved to all retrieved items) ( Salton, 1992 ). Examples include projects such as Cranfield ( Cleverdon, Mills, &amp; Keen, 1966 ), SMART ( Salton, 1971, 1989 ), STAIRS ( Blair &amp; Maron, 1985, 1990 ) and TREC ( Harman, 1995 ). These studies have set a standard for experimental design, but their scope is incomplete from an evaluation perspective. First, these studies use topical relevance as the basic or sole criterion for IR evaluation. The static performance measures, such as recall and precision, are context free and do not reflect the interactive process of IR, but allow primarily for a binary relevance representation ( Borlund, 2000, 2003 ). Recall and 2002 ), but these relevance-based measures is still incapable of reflecting the multidimensional and dynamic nature of relevance ( Saracevic, 1996 ). Second, the user dimension is omitted from the evaluation. While rel-evance is a cognitive concept whose meaning is largely dependant on users  X  perceptions of information and their own information need situations ( Borlund, 2003 ), this approach deals only with the degree to which the query representation matches the contents of the retrieved information objects. In other words, the eval-uation studies of this sort are constrained by the system  X  s definition of information needs rather than the 1992 ).

With the paradigm shift toward the cognitive and behavioral aspect of IR, there is a growing body of user-centered studies that focus on evaluating end-user satisfaction, performance, and use of IR systems ( Ellis, 1996; Robertson &amp; Hancock-Beaulieu, 1992; Schamber, 1994 ). However, these user-centered studies, which are concerned with users and use levels, exist in relative isolation from other system-centered studies, which focus on outcomes and algorithms. As argued by Saracevic (1995) , this isolation of levels of evalu-ation could be considered a basic shortcoming of all IR evaluations.

As our understanding of IR grows, it is clear that IR exists as a social activity to link and enable inter-actions between producers/authors of information and users/readers of information ( Saracevic, 1995 ) and effectiveness of an IR system should be measured not only in a technical context, but also by users in ap-plied contexts. Moreover, a combination of outcome and process criteria should be incorporated within the evaluation to reflect the multidimensional and dynamic nature of relevance. A major challenge for IR eval-uation efforts is to apply the various criteria in some unified and comprehensive manner ( Saracevic, 1995 ). 2. Motivation
Evaluation should begin with an examination of the end-users  X  actual information-seeking and retrieval sentation of user  X  s cognitive activities and behaviors during the IR process (i.e., Belkin, 1980; Ingwersen, 1996; Kuhlthau, 1991 ). From this research, the information search process (ISP) from the user  X  s perspective has been identified as a six-stage constructive activity X  X nitiation, selection, exploration, formulation, col-actually a decision-making process that has a discrete beginning and ending and requires various user choices during the process. The process involves recognizing the information need, identifying and selecting the general topic or the approach to be pursued, investigating information on the general topic, forming a focus from the information encountered, gathering information related to the focused topic, making final decisions, and presenting the searched results. The activities involved in the ISP are consistent with the pro-cedures and steps that a decision maker normally follows in making a decision ( Forgionne, 1999 ). Similar to a decision-making process, the stages of the ISP are sequential, but the users may loop back, repeat steps, and modify results from previous steps under conditions of uncertainty and anxiety ( Kuhlthau, 1993 ).
Thinking of information searching as a decision-making process may be particularly important when evaluating IR systems. According to Soergel (1976) , the ultimate objective of any information storage and retrieval system is improved task performance/problem-solving/decision-making by the user. In other words, an effective IR system can assist the users to make good decisions in effectively and efficiently locat-ing the information relevant to their needs during their information search process. Thus, it would seem mation search process and its impact on users  X  decision making in retrieving relevant information. In this way, we might remedy the problems associated with the existing IR evaluation approaches by accounting for the process of, and outcomes from, IR system use.

The rest of the paper is organized as follows. First, there is an investigation of the decision-making steps involved in the users  X  information search process and the roles of decision-making theories in evaluating IR systems. Next, the paper develops a decision-theoretic model that combines both outcome-and process-ori-ented measures for the evaluation of IR systems. Using a multiple criteria decision making method, called the Analytic Hierarchy Process (AHP), the evaluation model then is illustrated in the context of a domain-specific IR application. The paper concludes with a discussion of the implications for IR evaluation, con-tributions to the literature, and the directions for future research. 3. Model framework for evaluation of IR systems 3.1. Information search process
From the user  X  s perspective, the information search process (ISP) is defined as  X  X  X he user  X  s constructive activity of finding meaning from information in order to extend his or her stage of knowledge on a partic-ular problem or topic X  X  (Kuhlthau, 1991, p. 361). The common patterns in users  X  ISP ( Kuhlthau, 1993 ) and the role of an IR system in facilitating the users with each stage of the search process are described as follows:
Stage 1: Task initiation: In this stage, user becomes aware of an information or knowledge deficiency and becomes uncertain and apprehensive. An IR system may help the user understand the information need or recognize the problem by providing access to information and possible topics and approaches.
Stage 2: Topic selection: During this stage, the user skims and scans for an overview of alternative topics and a general topic or approach to be investigated. A feeling of optimism often rises after the selection has been made. The IR system may facilitate this task by providing a range of viable topic alternatives and by assisting in the topic selection.

Stage 3: Prefocus exploration: The task is to investigate information on the general topic to extend per-sonal understanding by locating pertinent information, reading to become informed, or ultimately, incorporating the new information with previously-held knowledge and constructs. The IR system may help overcome difficulties by openly assisting with the formation of new constructs or relevant rela-tionships to the general topic.

Stage 4: Focus formulation: The central goal for this stage is to form a focus from the information explored about the general topic. The user  X  s feeling of confidence often increases with focus clarity, and the IR system may facilitate this activity by presenting engaging ideas that provide such clarity.
Stage 5: Information collection: In this stage, the user searches and gathers information related to the focused topic. Feelings of confidence continue to grow as understanding and interests deepen in the focused perspective. The IR system may help the user articulate a personalized information need relevant to the focus, facilitate a comprehensive search of all available resources, and identify relevant information.

Stage 6: Search closure: During this stage, users often take a summary search to recheck for information that may have been initially overlooked, confirm the search results, and prepare to present or use the findings. The IR system may be helpful in enabling the user to identify additional relevant information from rechecking and providing suggestive indications for the completion of the search. 3.2. Decision-making steps in ISP
Decision making theories can help in the IR evaluation by streamlining the user  X  s decision-making pro-cess during an ISP, providing a comprehensive, integrated, and consistent explanation of the supporting roles that an IR system plays in the decision-making process, and linking the process to decision outcomes ( Forgionne, 2000 ).
 From a decision-making perspective, the ISP can be viewed as the steps shown in Table 1 .

This identification of decision-making steps synthesizes the user  X  s cognitive construction and actions taken during an ISP. Since an outcome can occur only after the last decision-making step has been imple-mented, the decision outcome is a function of or largely explained by the decision making process ( Forgionne, 2000 ). Improvements made to the decision-making process, then, should lead to better out-comes. Process improvements could include gains in the user  X  s ability to perform the steps of decision making, or increased efficiency and effectiveness, while outcome improvements could involve both the sys-tem (e.g., improved system utility) and the user (e.g., learned knowledge or abilities). 3.3. Decision-theoretic model for IR evaluation
The ISP decision-making steps suggest a general approach to evaluate IR systems from a decision-mak-ing perspective. To evaluate an IR system  X  s ability in improving the user  X  s decision making in locating and retrieving relevant information, different evaluation measures can be identified from the major decision-making steps of the ISP. These measures can be formed into a hierarchy, where both the process and out-come criteria are established and sub-criteria are determined within process and outcome. Based on this approach, a decision-theoretic model for the evaluation of IR systems is proposed (see Fig. 1 ), where spe-cific evaluation measures are as follows: [T1] time to recognize the problem [N1] number of general topic alternatives [T2] time to identify a general topic [N2] number of articles generated for the general topic [T3] time to establish criteria (focus) [T4] time to identify a relevant article [N3] number of relevant articles identified [N4] number of additional articles identified from rechecking [P1] system utility [P2] user learning
This model associates the various evaluation measures relevant to the context of information searching in an integrated fashion. Because an IR system rarely assists the user with all the steps and procedures of decision making, only the relevant measures should be included in the evaluation. Some measures deal with process and others involve outcome. 3.4. Process
The process-oriented evaluation measures correspond to the decision-making steps previously identified from the ISP and assess each step in terms of both effectiveness and efficiency ( Forgionne, 1999 ). Effective-ness is concerned with decision outputs, such as the number of alternatives generated for the user or the number of relevant articles retrieved through the IR system, while efficiency is concerned with the use of resources to achieve those outputs, such as the time needed for the user to perform an information search decision-making step (e.g., recognizing the problem, establishing criteria). Effectiveness and efficiency assessments can be objectively recorded through three methods: (1) expert evaluation, in which domain experts would observe practice and rate the user  X  s proficiency and (2) directed self-examination, in which the user provides his or her own ratings, often with guidance from (3) process tracing, in which accounting tools embedded within the decision aid are utilized to record and 3.5. Outcome
The outcome-oriented measures include system utility and user learning , which manifest IR performance from both the system and the user aspects. The measure of system utility is based on Cooper  X  s work Cooper (1973a, 1973b) and represents the  X  X  X ltimate worth X  X  or whatever the user finds to be of value about the sys-tem output, whether its usefulness, appropriateness, or entertainingness. According to Cooper (1973a) , the document-utility, which is quantified in terms of the numbers of utiles (e.g., dollars, hours of labor). The sum of the document-utility obtained for all the relevant documents from a search is called search-utility. The researcher then divides the search-utility by the number of evaluated documents and generates a sys-tem-utility score for the IR system for a particular user. This procedure may also be repeated for all par-ticipating users and generate an average utility score for the system across all the users.

The measure of user leaning represents the user  X  s gained understanding of the current problem or acquired skills for future or further decision making ( Forgionne, 2000 ). The assessment of user learning involves subjective judgment, which requires users to assess their experiences or acquired knowledge and skills with the use of the system ( Adelman, 1991 ), and the employment of rating scales ( Tang, Shaw, &amp; Vevea, 1999 ).

By specifying the different evaluation measures and integrating them into a multi-criteria hierarchy, this model identifies the factors that must be measured to evaluate the effectiveness of an IR system from a deci-sion-making perspective. The hierarchy also isolates the specific cause of a decision outcome. 3.6. The analytic hierarchy process
As Fig. 1 illustrates, IR evaluation is a multiple criteria decision making (MCDM) problem. There are various MCDM methodologies that can be used to consolidate the various disparate criteria (and sub-cri-teria) into an overall measure of decision value. One of the most popular is the Analytic Hierarchy Process (AHP).

In the AHP methodology, the evaluation problem is broken down into a hierarchy of interrelated deci-the bottom of the hierarchy. For an IR decision-theoretic evaluation, the overall decision criterion is deci-sion value, attributes are outcome, process, and the sub-criteria for these main attributes, and the alterna-tives are the various IR system choices. Next, elements (alternatives or attributes) at one level are compared pair-wise for their relative importance in attaining the elements (attributes or overall criterion) at the next higher level. These data are used to estimate the relative weights of the attributes. Then, the weights are aggregated into a set of ratings for the decision alternatives (IR system choices).

The weight estimation and aggregation requires a complex series of mathematical calculations ( Harker, 1988; Saaty, 1987 ). For most IR system users, who may not be aware of, interested in, or technically pro-ficient with the mathematics, the calculations are best implemented through specialized computer software. Popular software for this task is EXPERT CHOICE ( Forgionne, 1999 ). 4. Evaluation illustration
Using the proposed model, various IR systems can be evaluated on the basis of their effectiveness in improving the user  X  s decision making during information retrieval. This section presents an IR system in the telemedicine domain and illustrates how the system can be evaluated in a realistic and testable form. Specifically, the evaluation considers both the outcomes from, and the process of, decision making by com-paring results with and without the IR system. 4.1. System
Information retrieval systems have been widely employed in the field of telemedicine, which relies on telecommunications to bridge geographic gaps and provide healthcare information and services to the users ( Demiris, 2004 ). An Internet-based IR system called Preventing Suicide Network (PSN) has been developed as an alternative mental healthcare application for preventing suicide (Wang et al., in press) . Unlike most other telemedicine retrieval systems, the PSN delivers information to intermediaries, such as mental health professionals, friends, and parents that are helping someone who may be at risk for suicide, rather than the patient themselves.

To ease the daunting task of searching and locating online resources on suicide prevention, the PSN has been designed with some features that aim for improved user  X  s decision making during information retrie-val. One dominant feature is called  X  X  X ailored information. X  X  In this mode, the PSN offers information about suicide prevention for a total of twenty intermediary roles, and the information is organized according to recognize a specific intermediary role and choose appropriate general topics to explore within a shortest possible time. In addition, the PSN is linked to a repository of medical databases and resources, such as the National Library of Medicine, through search terms that are presented to the user in a checkbox format as shown in Fig. 2 . In this way, the users are guided to make appropriate decisions in establishing search criteria and retrieving relevant information or articles to their needs. In sum, the PSN functions as an online resource center, through which the users can retrieve authoritative and specific suicide prevention informa-tion to help those at risk to suicide.
 4.2. Evaluation method
The PSN affects the process of user  X  s decision making in retrieving relevant information, and thus, affects the search outcomes as well. To assess the overall effectiveness of the system, the decision-theoretic evalu-ation model of IR effectiveness can be implemented with the Analytic Hierarchy Process (AHP) ( Harker, 1988; Saaty, 1987 ).

In the IR evaluation, there are two search alternatives. In our case, one alternative is to search and locate online information on suicide prevention with the decision and retrieval support provided by the PSN (IRS), and the other alternative is to have the user perform the search without the PSN (No-IRS). Using the AHP, these two alternatives can be compared by process and outcome criteria. The result would be an eigenvalue score assigned to each alternative, thereby establishing the decision value, if any, of the PSN to the ISP. 4.3. Performance results
To generate needed AHP data, a simulated scenario was developed. In the simulation, a user, in the hypothetical role of a concerned mother of an adult son who had suicidal tendencies, searched for relevant online information using a 19 00 screen on a 2.67 GHz Pentium  X  4 computer with an 11.0 Mbps Internet con-nection. Oral and written definitions of the evaluation measures were provided, and the measures were interpreted relative to the specific intermediary role of the user and the corresponding required tasks. For example, the variable T2 was explained to the user as  X  X  X he time to identify an intermediary role (the general topic), which is  X  X  X arent X  X  in her case; and the variable N2 was explained as the number of articles or links to web pages that are specifically helpful for parent to prevent suicide. X  X  To record the variables under the process criteria (N1 X  X 4, T1 X  X 4), the method of directed self-examination was employed, in which the user provided her own rating or counting with the verbal guidance and timing assistance of a facilitator.

Unlike the process-oriented variables that were assessed with actual measurements, the variables under the outcome criteria (P1, P2) were recorded and assessed based on subjective judgment. Due to the current stage of the research, the system utility and user learning measures were estimated by asking the user to give an overall rating on each measure with the use of a 7-point Likert scale. As described previously, both esti-mated judgments and ratios of actual measurements were used in deriving the overall priorities. According to Saaty (1987) , priorities can be derived based on pair-wise assessments using either judgment or ratios of actual measurements.

The user conducted the search with (IRS) and without the PSN (No-IRS). The PSN user was given a brief introduction on the system and some time to familiarize herself with the system. In particular,  X  X  X ar-was used in completing the information retrieval task. For the No-IRS case, the user searched the same kind of information through the basic interface of Google TM for Web search, which simply served as an access point to the online repository of information without providing any decision-making capabilities for the user. The user had to rely on her own judgment and knowledge in determining the search criteria (keywords) and finding relevant information. Due to the repetitive nature of the search results, the user only considered the first hundred results of each search in terms of timing and counting for the variables. The timing for the variables was estimated in seconds by the user using a stopwatch. Although gross estimates were often used for the variables, they provided adequate information for the AHP analysis. It has been suggested that the imprecision in the estimates of the same user  X  s pair-wise comparisons is unlikely to uation measures are shown in Table 2 . 4.4. AHP analyses
The values of the evaluation measures shown in Table 2 were computed with the AHP hierarchy shown in Fig. 3 to determine the priorities for the IRS and No-IRS cases. Specifically, the numerical values of the leaf nodes in Table 2 were first transformed, with equal weights, into ratio scales of relative magnitudes.
Perhaps some criteria are more important than others, but we chose to assign equal weights to avoid any potential perception biases. During implementation, the user can perform sensitivity analyses on the results using variable weights, if desired. The values of the nodes were in turn compared with a numerical preference scale and used to derive the priorities for the multiple criteria. The software program Expert Choice was utilized to facilitate the AHP analysis and generate results.

The values of the nodes obtained from the AHP analysis, as shown in Fig. 3 , indicate the relative sig-nificance of the two alternatives in improving the user  X  s decision making during information search.
Fig. 3 results indicate that decision-making effectiveness is considerably enhanced with the IRS com-pared to No-IRS (0.574 vs. 0.426), and decision-making efficiency is tremendously improved with the IRS (0.861 vs. 0.139). Information retrieval performance is also significantly improved with the IRS (0.608 vs. 0.392). The IRS alternative has been prioritized significantly higher than the No-IRS in both the process and outcome criteria, with the process showing the highest priority. Overall, a score of 0.663 was obtained for the IRS case compared to a score of 0.337 for the No-IRS case, indicating that the PSN improved the user  X  s decision making in retrieving relevant information on suicide prevention. 5. Discussion
The results illustrate that the use of the decision-theoretic model implemented with the AHP can be used to evaluate IR systems. The technique also helps detect possible design flaws in an existing IR system through the hierarchy priorities. For example, Fig. 3 indicates that IRS, although overall superior to the No-IRS alternative, is slightly inferior on the N2 and N4 measures. Updates of the PSN, then, may wish to incorporate features to improve N2 and N4 results.

This paper is a proof-of-concept based on an illustration with a single user in a hypothetical role. The data was collected in a simulated setting and is used for illustration purposes only. The foremost objectives of the paper are to present the decision-theoretic approach for evaluating IR systems and to report its workability. To further examine the concept empirically, the proposed method should be tested using a variety of potential users with different intermediary roles, experience levels, and other characteristics. The additional user characteristics should be accounted for, or controlled, in the empirical testing, and ple sets of data are collected, the sampling distribution of the priorities can be specified by a probability distribution (Phillips-Wren et al., in press) . In that case, statistical tests can be conducted to determine whether the overall priority of one alternative (e.g., IRS) is significantly different from that of the other alternative (e.g., No-IRS).

This paper also shows the applicability of the proposed method for IR evaluation by applying the deci-sion-theoretic model to the PSN, which is a newly developed IR system in the telemedicine domain, and testing its effectiveness. Similarly, the PSN was selected for illustration purposes only and simply functioned as an instrumental device for the evaluation method in this paper. Nevertheless, the methodology is appli-cable to a wider set of IR applications in various domains, such as library or legal information retrieval. In these other domains, the evaluation measures may have to be operationalized differently, even though the general formulation remains unchanged. 6. Conclusions Evaluation research on information retrieval systems has thus far been narrowly focused and disjointed. The research in this paper offers a methodology to consolidate IR evaluation. The novelty of this approach lies in the focus on the user aspect and the application of decision-making theories in the IR field. For the first time, users  X  information search processes and information seeking behaviors are investigated from a decision-making perspective, and specific decision-making steps are identified. Although the general no-tions adopted from decision-making theories have existed for several decades, the application of decision science into information retrieval is a relatively new contribution to the field. Second, this research offers a multi-criteria model that identifies the measures that can be used in IR evaluation and integrates the mea-sures into a holistic model that considers both process and outcome from ISP in IR evaluation. Third, the proposed approach involves users in the collection of data and IR evaluation. In this approach, both sub-jective and objective data are employed to provide a comprehensive picture of the effectiveness and value of an IR system. In addition, IR systems are evaluated from the user  X  s aspect by how well the systems can support decision-making and assist humans, rather than by how well the retrieved information matches a static query. Last, but not least, this research extends the Analytic Hierarchy Process to a new application area of IR evaluation. The AHP offers an accessible analysis method for the IR practitioners and shows promising future for its application the IR field.

Future research will concentrate on the empirical testing and refinement of this decision-theoretic ap-proach and its application in IR evaluation. The proposed model will be tested with multiple users in an experimental setting to provide empirical evidence and practical implications for the feasibility and gener-ality of the IR evaluation approach. Ultimately, a novel and accessible approach should be established, which specifies the following requirements for evaluating an IR system: (1) a system together with an infor-mation retrieval process; (2) the criteria used for evaluation; (3) the evaluation measures based on the cri-teria; (4) a measuring instrument (model) to register the measures, and (5) the methodology for obtaining measurements and conducting evaluation.
 Acknowledgements
The authors wish to express their gratitude to the vendors of Expert Choice for a grant to use the edu-cational version of Expert Choice 11 in this research effort.
 References
