 1. Introduction
In Information Retrieval (IR), query expansion usually refers to the technique that uses blind relevance feedback to expand a query with new query terms, and reweigh the query terms, by taking into account a pseudo relevance set ( Rocchio, 1971; Amati, 2003 ). Usually, the pseudo relevance set consists of the top-ranked documents returned by the first-pass retrieval. These top-ranked documents are assumed to be relevant to the topic. Query expansion has proved to be an effective technique for ad-hoc retrieval. For example, in previous Text Retrieval Conference (TREC) ad-hoc tasks, a consistent improvement brought by query expan-sion has been observed (for example, see Hawking, 2000; Hawking &amp; Craswell, 2001 ).
However, in some cases, query expansion can lead to little improvement in the retrieval performance. For example, in the TREC 2004 Terabyte track, some participants found that query expansion was not helpful for neto, and Romano (2004a) , the effectiveness of query expansion is correlated with the quality of the top-ranked documents returned by the first-pass retrieval, on which query expansion is based. The more closely the documents returned by the first-pass retrieval are related to the topic, the better the performance that the query expansion mechanism can achieve. In other words, if the top-ranked documents are poor, this can lead to a failure for query expansion to improve the retrieval performance ( Amati et al., 2004a; Yom-Tov, Fine, Carmel, &amp; Darlow, 2005 ).
 Improving the effectiveness and robustness of query expansion has been the focus of some previous work.
Two main approaches have been proposed. The first one is selective query expansion, which disables query expansion if query expansion is predicted to be detrimental to retrieval ( Amati et al., 2004a; Cronen-
Townsend, Zhou, &amp; Croft, 2004; Yom-Tov et al., 2005 ). The second one is the collection enrichment approach, which performs query expansion using a large high quality external collection, and then retrieves a collection, in which most documents are highly informative. For example, the TREC newswire collection is a high-quality one, since each of its documents usually contains mostly informative terms and very little uments are returned, and the notion of external collection refers to any other distinct collection. In this paper, we aim to further improve the effectiveness and robustness of query expansion by two different approaches.

First, we propose a novel query expansion mechanism on fields, which appropriately uses field evidence available in a corpora. For example, a Web document can be represented by combining fields, such as its title, the anchor text of its incoming links, and the body of its text. The queries can then be reweighed and expanded using this more refined available information. We suggest that the performance of query expan-sion is related not only to the quality of the top-ranked documents returned by the first-pass retrieval, but also to the quality of the query term reweighting. We believe that combining evidence from different doc-ument fields can refine the statistics of the query terms, and hence improve the query term reweighting. of the top-ranked documents.

Second, we propose a low-cost adaptive decision mechanism for the application of query expansion. The decision mechanism is based on the pre-retrieval performance prediction technique ( He &amp; Ounis, 2005 ).
The proposed mechanism applies both collection enrichment and selective query expansion techniques. In the proposed adaptive query expansion mechanism, the expansion of the query can be either local, using doc-uments in the local collection, or external, using an external resource. The adaptive mechanism predicts the to lead to the degradation of the query performance, then query expansion is disabled.

Our new query expansion techniques are based on the Divergence from Randomness (DFR) framework, which measures the informativeness of a term in a document by the divergence of the term X  X  distribution in val has been studied in Robertson, Zaragoza, and Taylor (2004) Zaragoza, Craswell, Taylor, Saria, and Rob-
BM25F model, and by extending PL2 ( Amati, 2003 ) to the field retrieval PL2F model ( Macdonald, He, Plac-houras, &amp; Ounis, 2005 ). In this paper, similarly to the approach in Macdonald et al. (2005) and Robertson et al. (2004) , we extend the parameter-free DLH model ( Amati, 2006 ) to field retrieval. The remainder of this paper is organised as follows. We introduce the related works in Section 2 .In
Sections 3 and 4 , we propose the query expansion on fields and the adaptive query expansion mechanism, respectively. The two new approaches are extensively evaluated on two standard TREC Web collections.
The document weighting model used in the experiments in Sections 3 and 4 is a DFR document weighting model. In Section 5 , we present experiments using BM25F to assess the impact of a different document weighting model on the experimental results. In Section 6 , we conclude the work and suggest future research directions.
 2. Related works
In Section 2.1 , we introduce the BM25F model on fields. We introduce a Divergence from Randomness (DFR) document weighting model for the first-pass retrieval in Section 2.2 , and introduce a DFR-based term weighting model for query expansion in Section 2.3 . These two DFR-based models will be later extended to which will be applied in our proposed adaptive query expansion mechanism. 2.1. The BM25F field retrieval document weighting model
In the BM25F model, the relevance score of a document d for a query Q is given by ( Robertson et al., 2004 ): where qtf is the query term frequency and k 1 and k 3 are the parameters. The default setting is k k = 1000 ( Robertson et al., 1995 ). w (1) is the idf factor, which is given by:
N is the number of documents in the whole collection and N
The normalised term frequency tfn is given by a so-called per-field normalisation component, where term frequency is normalised in a per-field basis ( Zaragoza et al., 2004 ). This per-field normalisation component applies a linear combination of the normalised term frequencies from different fields as follows: where w f is the weight of a field f, tf f the frequency of the query term in the field f of the document, b frequency normalisation hyper-parameter of field f, l f the number of tokens in field f of the document, and avg _ lf is the average length of field f in the collection, i.e. the average number of tokens in field f.
Note that the BM25 model X  X  formula ( Robertson et al., 1995 ) is the same as Eq. (1) , while its normalised term frequency tfn is given by its term frequency normalisation component: where tf is the term frequency in the documents, b the term frequency normalisation hyper-parameter, l the document length, and avg _ l is the average document length in the collection. 2.2. The DLH document weighting model
The DLH model is a generalisation of the parameter-free hypergeometric DFR model in a binomial case ( Amati, 2006 ). Previous probabilistic weighting models, e.g. BM25 ( Robertson et al., 1995 ), consider the occurrences of a query term in a document to be samples from the document. Unlike BM25, the hypergeo-metric model assumes that the occurrences of a query term in a document are samples from the whole collec-tion, instead of from the document. The DLH model does not have a term frequency normalisation component, and does not have any parameters that require relevance tuning. Therefore, it has no need for expensive training with relevance judgement, which is not always available in a practical setting. In other
Therefore, the hypergeometric model does not have the need for tuning its parameters, in order to achieve an optimised retrieval performance. In the DLH model, the relevance score of a document d for a query Q is given by: document length, and qtw is the query term weight. This is given by qtf/qtf frequency and qtf max is the maximum qtf among all the query terms. F is the frequency of the term in the col-lection and N is the number of documents in the collection. Note that the DLH model in Eq. (4) does not have a tf normalisation component, as this is assumed to be inherent to the model. 2.3. Divergence from randomness query expansion mechanism
The Divergence from Randomness (DFR) framework employs a query expansion mechanism that is a gen-eralisation of Rocchio X  X  method ( Rocchio, 1971 ). The DFR query expansion mechanism has two steps.
First, it applies a DFR term weighting model to measure the informativeness of the terms in the top-ranked documents. The idea of the DFR term weighting model is to infer the informativeness of a term by the diver-gence of its distribution in the top-ranked documents from a random distribution. The most effective DFR term weighting model is the Bo1 model that uses the Bose X  X instein statistics ( Amati, 2003; Macdonald is given by: where tf x is the frequency of the query term in the top-ranked documents, P the term in the collection, and N is the number of documents in the collection. In this paper, following the default setting of Amati (2003) , we extract the 10 most informative terms from the top 3 returned documents. The original query terms may also appear in the 10 extracted terms.

In the second step, the DFR query expansion mechanism expands the query by merging the extracted terms with the original query terms. The query term weight qtw is given by a parameter-free query expansion formula: term with the maximum w ( t ) in the top-ranked documents. If an original query term does not appear in the most informative terms extracted from the top-ranked documents, its query term weight remains equal to the original one. Note again that the above query expansion formula does not have any parameter that requires tuning. 2.4. Selective query expansion and collection enrichment
The basic idea of selective query expansion is to disable query expansion if the query is predicted to perform poorly. A selective query expansion mechanism was proposed by Amati, Carpineto, and Romano (2004b) in the context of the DFR framework. It predicts the performance of query expansion by the query difficulty , which looks at the divergence of a query term X  X  distribution in the top-ranked documents from this distribu-result in. An alternate selective query expansion mechanism was proposed by Cronen-Townsend et al. (2004) in the context of language modeling. In the latter approach, the query performance is predicted using the of the query model from the collection model. Again, the larger the divergence is, the better retrieval performance query expansion can provide.

In Kwok and Chan (1998) studied the idea of using an external resource for query expansion. They sug-
Therefore, the performance of query expansion can be improved by using a large external collection, which possibly contains more relevant documents and has better collection statistics for the query term reweighting.
This collection enrichment 1 approach was later applied in the TREC Robust tracks ( Grunfeld, Kwok, Dinstl, on the local collection, and the external collection is only used for query term reweighting.
In this paper, we extend the DLH document weighting model and the Bo1 term weighting model for query expansion to field retrieval. Moreover, we devise a novel adaptive query expansion mechanism that applies both selective query expansion and collection enrichment techniques. This mechanism selects the appropriate in Section 3 and the adaptive query expansion mechanism in Section 4 . 3. Combining field evidence for query expansion
In this section, we present a query expansion mechanism on fields that is based on an extension of the Bo1 model for query expansion. We also extend the DLH model to handle fields. In Robertson et al. (2004) , Rob-ertson et al. denote the BM25 model on fields as the BM25F model. In this work, we follow their notation by denoting the DLH model on fields as the DLHF model, and the Bo1 model on fields as the Bo1F model. We propose the DLHF model in Section 3.1 and the query expansion mechanism on fields in Section 3.2 .
The related evaluation is presented in Section 3.3 . 3.1. The DLHF field retrieval document weighting model
In this section, we describe how we extend the DLH model to field retrieval in order to improve the top-ranked documents from the first-pass retrieval.

Unlike the BM25 model (see Eq. (1) ), the DLH model does not have an explicit term frequency normali-cies in different fields without normalising the term frequency.

In our DLHF field retrieval model, the term frequency tf in the document is a linear combination of the term frequencies in different fields of the document. Similarly to the BM25F model, each field is associated with a weight. The linear combination is given below: where w f is the weight of a field f, and tf f is the term frequency in field f of the document.
The DLHF field retrieval model is defined as Eq. (4) , where the term frequency tf is given by the above linear combination. We use the DLHF model to perform both the first-pass retrieval, and the retrieval using the expanded query. Note that the DLHF model has fewer parameters than the BM25F model (see Section 2.1 ), in the sense that the DLHF model does not have any term frequency normalisation hyper-parameters ( b f in BM25F). 3.2. Query expansion framework on fields
In this section, we propose a query expansion mechanism on fields. We aim to improve the quality of the of the top-ranked documents, the quality of the query term reweighting is also related to the performance of query expansion. Combining field evidence can provide better statistics for the query terms. Therefore, it may improve query term reweighting and achieve a better performance of query expansion. This assumption will be evaluated in Section 3.3 .

In order to combine the statistics from different fields, we could compute a term weight for each field sep-of breaking the non-linear saturation of term frequency in the weighting model. Therefore, we combine the frequencies of the query term in the top-ranked documents directly, and then weight the term based on the combined statistics.

We apply a linear combination to sum the term frequencies in different fields. The term frequency in each field of the top-ranked documents is tf x f , and each field is associated with a weight wq in the top-ranked documents is given by: where wq f is the weight of a field f in the top-ranked documents. Each weight wq tance of the associated field in the top-ranked documents. tf documents.

We then define the Bo1F term weighting model on fields as Eq. (5) , where the term frequency in the top-ranked documents tf x is given by Eq. (8) .
 Using the Bo1F model, the proposed query expansion mechanism on fields can be described as follows: First, we run a first-pass retrieval and obtain the top-ranked documents.

Second, we compute the weights of the terms in the top-ranked documents using the Bo1F term weighting model on fields.

Third, we extract the terms with the highest term weights from the top-ranked documents, merge them with the original query terms, and then eweigh the terms in the expanded query using Eq. (6) .
We evaluate the proposed query expansion mechanism on fields in the next section. 3.3. Evaluation of the query expansion mechanism on fields
In Section 3.3.1 , we introduce the experimental setting of the evaluation of our query expansion mechanism on fields. We present the results in Section 3.3.2 . 3.3.1. Experimental setting
Our experiments are conducted using the Terrier platform. Terrier is a modular platform for the rapid development of large-scale Information Retrieval applications, providing indexing and retrieval functional-ities. It is developed at the University of Glasgow. 2
In this section, we present experiments that aim to evaluate our query expansion mechanism on fields. We of its incoming links, and the body of its text.

We use two standard TREC Web collections, namely the WT10G and .GOV2 collections. collection is a small crawl of Web documents, which was used in the TREC 9, 10 Web tracks. It contains 1,692,096 Web documents, and has 10 G of data. The .GOV2 collection, which contains 25,205,179 Web doc-uments and 426 G of data, is a crawl from the .gov domain. This collection has been employed in the TREC 13 largest TREC test collection. We use these two different standard Web collections to test the impact of the collection size on the query expansion mechanism on fields.

For the test topics, we use the TREC 9 &amp; 10 Web ad-hoc tasks, and the TREC 13 &amp; 14 Terabyte ad-hoc tasks, including the latest TREC ad-hoc topics in the TREC 14 Terabyte track. Table 1 lists the topic numbers associated with each TREC task on each Web collection used. Each task involves 50 topics, and we have 100 topics for each Web collection used.

Each TREC topic has three fields, namely title, description and narrative. In our experiments, we only use the Web are usually very short ( Silverstein, Henzinger, &amp; Marais, 1998 ).

We create indices for the three fields, i.e. body, title, and anchor text, using the WT10G and .GOV2 col-that the size of the body field is much larger than the other two.

The tokens in the collections used are stopped using a standard stop-word list, and stemmed using Porter X  X  stemming algorithm.

Our query expansion on fields involves six parameters, namely the weights ( w
DLHF model, and the weights ( wq f ) of the three used fields in the query expansion mechanism. Since it would be very time-consuming to optimise all six parameters, we make the following assumptions to reduce the num-ber of parameters to two: contribution of the field in the retrieval, which should be consistent in document weighting and in query expansion. 2. Similarly to the work in Robertson et al. (2004) , we set the weight of the body field to 1.
By making the above two assumptions, we reduce the number of parameters to two, namely the weights of the anchor text and title fields. In the rest of this paper, we use w text and title fields, respectively.

To optimise the weights of the anchor text and title fields, we do a two-dimensional data sweeping within [0,2] with an interval of 0.1, and then choose the weights that give the best mean average precision (MAP) using relevance assessments on the test topics.

Our baseline is the content-based query expansion, which is equivalent to applying a normal query expan-sion over the three document fields. Using the WT10G and .GOV2 collections, we compare our query expan-sion mechanism on fields with the baseline. Moreover, since real user queries usually do not have relevance assessments available, we conduct experiments to find out what happens if the field weights are trained based on a sample of test queries. Each of the two collections used is associated to 100 topics used in two TREC ad-hoc tasks. On each collection used, we conduct a two-fold holdout evaluation. In each fold, we train the field weights on the 50 topics used in one associated ad-hoc task, and test the effectiveness of our query expansion mechanism on the 50 topics used in another associated TREC ad-hoc task, using the field weights obtained in training. In the next section, we provide analysis on the experimental results. 3.3.2. Experimental results
Table 3 compares the mean average precision (MAP) obtained by the content-based query expansion (QE) and by our query expansion mechanism on fields (QEF). w a title, which result in the best obtained MAP by the query expansion mechanism on fields. D % indicates the difference between the two MAP values in percentage. The p -value is given by the Wilcoxon matched-pairs signed-ranks test.

From Table 3 , we observe an improvement brought by our query expansion mechanism on fields. Accord-on both collections. In our experiments, we do not observe a remarkable impact of the collection size on the performance of the query expansion mechanism on fields.

Moreover, we find that anchor text is not very useful in refining query term reweighting. Indeed, the opti-mal weight of the anchor text field is 0 or 0.1 for the four associated TREC ad-hoc tasks, (see column w
Table 3 ). This indicates that the anchor text information makes no ( w ( w = 0.1), to improving query term reweighting. We suggest that this is because a query term usually repeat-edly occurs in the documents where the term appears. Consequently, the frequency of the query term in the (see column w t of Table 3 ). Therefore, the title field is a good evidence for improving query expansion.
Table 4 contains the experimental results of the two-fold holdout evaluation, which uses a sample of the associated TREC topics for training, and uses the other ones for testing. Table 4 shows that when the topics used for training and testing are different, our query expansion mechanism on fields can still outperform the baseline. Overall, our query expansion mechanism on fields has achieved effective performance in our exper-iments on the two TREC Web collections used. 4. Adaptive query expansion
In this section, we propose a low-cost adaptive query expansion mechanism, which applies both the collec-tion enrichment and selective query expansion techniques. The idea is to choose an appropriate resource
The proposed adaptive mechanism also uses a query performance prediction technique. We describe the pro-posed mechanism in Section 4.1 , and present the related evaluation in Section 4.2 . 4.1. Decision mechanism
We propose a new low-cost adaptive decision mechanism for the application of query expansion on a per-query basis. The decision mechanism is based on a pre-retrieval performance prediction technique. We hypoth-
We use the Average Inverse Collection Term Frequency (AvICTF) ( He &amp; Ounis, 2005 ) to predict the qual-ity of the top-ranked documents, which is the average of the inverse collection term frequency (ICTF) ( Kwok et al., 1996 ) of each query term. The definition of AvICTF is as follows:
In the above definition, Q refers to the query, token coll a good query performance, and a low AvICTF value predicts a poor query performance. From this definition, given a query, the AvICTF X  X  value is comparable for different collections. Therefore, we can devise a unique thresholding mechanism over the two collections for the adaptive query expansion.

We could have used other query performance predictors, such as the query difficulty ( Amati, 2003 ) and the clarity score ( Cronen-Townsend et al., 2002 ). However, unlike these predictors, using AvICTF avoids the actual retrieval on both local and external collections before making a decision for query expansion. It uses only the collection statistics of the query terms to predict the query performance. Therefore, our adaptive query expansion mechanism is very low-cost. Besides, as shown in He and Ounis (2005) , it is a reliable query performance predictor.

Using the AvICTF predictor, the decision mechanism operates on a per-query basis, and can be described as follows:
First, given a local collection and an external one, we compute the AvICTF on both local and external collections.
 If the AvICTF values on both collections are lower than a threshold, we disable query expansion. Otherwise, we apply query expansion on the collection that has the highest AvICTF value.
In the next section, we conduct experiments to evaluate our adaptive query expansion mechanism. 4.2. Evaluation of the adaptive query expansion mechanism
In this section, we evaluate our adaptive query expansion mechanism. We introduce the experimental set-ting in Section 4.2.1 and present the results in Section 4.2.2 . 4.2.1. Experimental settings in Section 3.3 .

To create an external collection for the collection enrichment, we need to make sure that the external col-lection is of a high quality so that it can bring useful information for query expansion. We suggest that the
TREC newswire collections and the English Wikipedia domain are good resources. Therefore, we form an external collection by merging the TREC disks 1 X 5 collections July, 2005. The TREC disks 1 X 5 collections contain newswire articles from various sources, e.g. the Financial
Times, the Wall Street Journal, etc. The English Wikipedia contains explanations of various concepts, and is usually considered to be of a good quality, making it an appropriate external resource. The merged external collection consists of 3,445,344 documents, which is approximately twice as large as the WT10G collection, while being much smaller than the .GOV2 collection. As stressed in Kwok and Chan (1998) , a relatively large
Hence we expect the adaptive query expansion mechanism to improve the retrieval performance on WT10G, while having little impact on the large-scale .GOV2 collection.

For the optimisation of the threshold setting of the adaptive query expansion mechanism, we do a data sweeping in the range of [10,27] with an interval of 1. For all TREC topics used, the AvICTF values on both local and external collections are within this range. We choose the threshold setting that gives the best mean average precision (MAP) on the test topics using relevance assessments.

Our baseline is the best performing query expansion on fields using either the local collection (QEF) or the external collection (ExQEF). This allows us to assess whether the adaptive query expansion mechanism has the potential for further improvement of the retrieval performance based on our query expansion mechanism on fields. Section 3.3 , for comparison purposes, we compare the performance of our adaptive query expansion with the performance achieved by the best TREC submitted title-only runs on a per-task basis. 4.2.2. Experimental results
Table 5 presents the evaluation results. The p -value is computed by the sign test, which indicates if the adaptive query expansion mechanism provides a positive improvement, compared with the baseline, for a sta-tistically significant number of queries.

On the WT10G collection (see row WT10G in Table 5 ), we observe a 4.47% statistically significant improvement on the baseline. The optimal threshold setting is AvICTF = 10. The adaptive query expansion does not disable query expansion for any query. The performance of the adaptive query expansion is better than expanding all the queries locally or externally, which indicates that our adaptive query expansion mech-anism successfully selects the appropriate collection resource for query expansion. The adaptive query expan-sion mechanism makes the correct decision for 81 out of 100 queries. The sign test shows that the mechanism makes the correct decisions for a statistically significant number of queries at the 0.05 level.
However, for the large-scale .GOV2 collection, we do not observe an improvement using the adaptive query expansion mechanism. It results in the best MAP when the local query expansion is applied for all the queries.
We suggest that this is because the .GOV2 collection is so large that the used external resource cannot render suggestion in Kwok and Chan (1998) , and our expectation in Section 4.2.1 .

In summary, our adaptive query expansion is shown to be effective when the external collection is larger than the local collection. Our adaptive query expansion mechanism is shown to have the potential to further improve the retrieval performance based on the query expansion mechanism on fields. However, the experi-mental results suggest not to apply collection enrichment if the external collection is much smaller than the the adaptive query expansion mechanism. In particular, the choice of the external collection may change the performance, whatever the size of the local collection. 5. Experiments with BM25F
In this section, we conduct experiments with the BM25F model to see the impact of using a different doc-ument weighting model on the proposed query expansion techniques. In the query expansion mechanism on fields and the adaptive query expansion, we replace DLHF with BM25F for both the first-pass retrieval and the retrieval using the expanded query. The experimental settings, including the collections, topics, and the term weighting model (i.e. Bo1F) used, are the same as in Sections 3.3.1 and 4.2.1 .

Using BM25F, we need to optimise five parameters, which are the hyper-parameter b (see Eq. (2) ) and the weights of anchor text ( w a ) and title ( w each field f, we set w f to 1 and the weights of the other two fields to 0. Then, we choose the b the best mean average precision (MAP) on the test topics in a data sweeping within [0,1] with an interval of 0.05. The optimal b f values are given in Table 6 . When the hyper-parameters b mised, we perform a two-dimensional data sweeping to optimise w the optimal weights are not found in this range, we increase the upper bound of the range. Again, we choose the weights that give the best MAP on the test topics.

Using BM25F, Table 7 compares the performance between our query expansion mechanism on fields and the content-based query expansion (i.e. the baseline). Compared with the baseline, we observe a statistically significant improvement on the WT10G collection. However, on .GOV2, unlike the obtained results using DLHF ( Table 3 ), we do not observe a significant improvement using BM25F. Moreover, it seems that using
BM25F, it is necessary to give higher weights to the anchor text and title fields. In particular, the optimal weight of the title ( w t ) field is much larger than that of DLHF (see columns w
Table 8 contains the evaluation results of a two-fold holdout evaluation, which uses half of the test queries for training, and the other half for testing. From the results, we find that the query expansion mechanism on fields outperforms the baseline in all the four cases. However, the difference between the obtained MAPs are experimental results using DLHF in Section 3.3.2 , we find that BM25F leads to a relatively small improvement in the retrieval performance, brought by the query expansion mechanism on fields. This may be due to the fact that BM25F employs a per-field normalisation component, while DLHF does not have a term frequency nor-malisation component. Consequently, in BM25F, the field evidence is more exploited than in DLHF, which results in a better retrieval performance of the baseline.

Table 9 evaluates the adaptive query expansion mechanism using BM25F. From the table, we have similar observations with those obtained using DLHF. Again, we obtain a significant improvement on WT10G, but not on the large-scale .GOV2 collection.

In summary, for the query expansion mechanism on fields, BM25F seems to give more importance to the anchor text and title fields than DLHF. The optimal weights of these two fields are much larger than those of
DLHF. Moreover, using BM25F, the adaptive query expansion mechanism improves the retrieval perfor-mance on WT10G, but not on the large-scale .GOV2 collection, which is similar with what we obtained using
DLHF. Overall, the proposed query expansion techniques can achieve an improvement on the baselines using both document weighting models, although the improvement depends on the collections used. 6. Conclusions and future work
In this paper, we have improved query expansion by combining evidence from different fields. We suggest that the performance of query expansion is related not only to the quality of the top-ranked documents, but also to the quality of the reweighting of the query terms. Therefore, combining field evidence can refine the statistics for query term reweighting, and improve query expansion. The proposed query expansion mecha-nism on fields is evaluated on two standard TREC Web collections, namely WT10G and the large-scale .GOV2 collection. The experiments are conducted using two different document weighting models, namely DLHF and BM25F. The evaluation results show that our query expansion mechanism on fields has achieved a significant retrieval performance improvement compared with the content-based query expansion, except when using BM25F on WT10G, where its performance is comparable with the content-based query expansion.
Also, we have shown that the proposed query expansion mechanism on fields has a good potential for enhanc-properly set.

Moreover, we have devised an adaptive query expansion mechanism that automatically selects the appro-priate collection resource for query expansion. The proposed adaptive mechanism applies query performance prediction, selective query expansion, and collection enrichment techniques. In our experiments, we observe that the performance of the adaptive query expansion mechanism depends on the relative size of the external collection. This is consistent with the suggestion in Kwok and Chan (1998) . In addition, on the WT10G col-lection, the adaptive query expansion mechanism gives consistent retrieval performance improvement, regard-less of the document weighting model used.

In the future, we aim to further investigate the adaptive query expansion mechanism on very large-scale collections and using various external collections. For example, for .GOV2, we can use the SPIRIT collection ( Joho &amp; Sanderson, 2004 ) as an external resource for query expansion. The SPIRIT collection contains 94,552,870 documents and is approximately four times larger than the .GOV2 collection.

Moreover, this paper has focused on showing that query expansion on fields is beneficial and has a consid-crucial to the retrieval performance of the proposed query expansion techniques. For example, BM25F and
DLHF require different optimal weights for the anchor text and title fields. Moreover, for the same document ments are readily available. This allows the proposed techniques to be used in an operational setting, where little relevance information is available.

Finally, in our experiments, we found that the effectiveness of the adaptive query expansion mechanism reasons that explain this finding. For example, a large external collection may contain more relevant docu-ting process.
 References
