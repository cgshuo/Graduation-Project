 Online advertisements (ads) are a rapidly growing source of income for many Internet content providers. The content providers and the ad brokers who match ads to content are paid only when ads are clicked; this is commonly referred to as the pay-per-click model. In this setting, the goal of the ad brokers is to select ads to display from a large corpus, so as to generate the most ad clicks and revenue. The selection problem involves a natural exploration vs. exploitation tradeoff: balancing exploration for ads with better click rates against exploitation of the best ads found so far. Following [17, 16], we model the ad selection task as a multi-armed bandit problem [5]. A multi-armed bandit models a casino with k slot machines (one-armed bandits), where each machine (arm) has a different and unknown expected payoff. The goal is to sequentially select the optimal sequence of slot machines to play (i.e., slot machine arms to pull) to maximize the expected total reward. Considering each ad as a slot machine, that may or may not provide a reward when presented to users, allows any multi-armed bandit strategy to be used for the ad selection problem. A standard assumption in the multi-armed bandit setting, however, is that each arm exists perpetu-ally. Although the payoff function of an arm is allowed to evolve over time, the evolution is assumed to be slow. Ads, on the other hand, are regularly created while others are removed from circulation. This occurs as advertisers X  budgets run out, when advertising campaigns change, when holiday shopping seasons end, and due to other factors beyond the control of the ad selection system. The advertising problem is even more challenging as the set of available ads is often huge (in the tens of millions), while standard multi-armed bandit strategies converge only slowly and require time linear in the number of available options.
 In this paper we initiate the study of a rapidly changing variant of the multi-armed bandit problem. We call it the mortal multi-armed bandit problem since ads (or equivalently, available bandit arms) are assumed to be born and die regularly. In particular, we will show that while the standard multi-armed bandit setting allows for algorithms that only deviate from the optimal total payoff by O (ln t ) [21], in the mortal arm setting a regret of  X ( t ) is possible.
 Our analysis of the mortal multi-arm bandit problem considers two settings. First, in the less realistic but simpler state-aware (deterministic reward) case, pulling arm i always provides a reward that equals the expected payoff of the arm. Second, in the more realistic state-oblivious (stochastic reward) case, the reward from arm i is a binomial random variable indicating the true payoff of the arm only in expectation. We provide an optimal algorithm for the state-aware case. This algorithm is based on characterizing the precise payoff threshold below which repeated arm pulls become suboptimal. This characterization also shows that there are cases when a linear regret is inevitable. We then extend the algorithm to the state-oblivious case, and show that it is near-optimal. Following this, we provide a general heuristic recipe for modifying standard multi-armed bandit algorithms to be more suitable in the mortal-arm setting. We validate the efficacy of our algorithms on various payoff distributions including one empirically derived from real ads. In all cases, we show that the algorithms presented significantly outperform standard multi-armed bandit approaches. Suppose we wish to select the ads to display on a webpage. Every time a user visits this webpage, we may choose one ad to display. Each ad has a different potential to provide revenue, and we wish to sequentially select the ads to maximize the total expected revenue. Formally, say that at time t , we have ads A ( t ) = { ad 1 t ,...,ad kt } from which we must pick one to show. Each ad it has a payoff  X  it  X  [0 , 1] that is drawn from some known cumulative distribution F (  X  ) 1 . Presenting ad it at time t provides a (financial) reward R (  X  it ) ; the reward function R (  X  ) will be specified below. If the pool of available ads A ( t ) were static, or if the payoffs were only slowly changing with t , this problem could be solved using any standard multi-armed bandit approach. As described earlier, in reality the available ads are rapidly changing. We propose the following simple model for this change: at the end of each time step t , one or more ads may die and be replaced with new ads. The process then continues with time t + 1 . Note that since change happens only through replacement of ads, the number of ads k = | A ( t ) | remains fixed. Also, as long as an ad is alive, we assume that its payoff is fixed.
 Death can be modeled in two ways, and we will address both in this work. An ad i may have a budget L i that is known a priori and revealed to the algorithm. The ad dies immediately after it has been selected L i times; we assume that L i values are drawn from a geometric distribution, with an expected budget of L . We refer to this case as budgeted death . Alternatively, each ad may die with a fixed probability p after every time step, whether it was selected or not. This is equivalent to each ad being allocated a lifetime budget L i , drawn from a geometric distribution with parameter p , that is fixed when the arm is born but is never revealed to the algorithm; in this case new arms have an expected lifetime of L = 1 /p . We call this timed death . In both death settings, we assume in our theoretical analysis that at any time there is always at least one previously unexplored ad available. This reflects reality where the number of ads is practically unlimited.
 Finally, we model the reward function in two ways, the first being simpler to analyze and the latter more realistic. In the state-aware (deterministic reward) case, we assume R (  X  it ) =  X  it . This provides us with complete information about each ad immediately after it is chosen to be displayed. In the state-oblivious (stochastic reward) case, we take R (  X  it ) to be a random variable that is 1 with probability  X  it and 0 otherwise.
 The mortal multi-armed bandit setting requires different performance measures than the ones used with static multi-armed bandits. In the static setting, very little exploration is needed once an optimal arm is identified with near-certainty; therefore the quality measure is the total regret over time. In our setting the algorithm needs to continuously explore newly available arms. We therefore study the long term, steady-state, mean regret per time step of various solutions. We define this regret as the expected payoff of the best currently alive arm minus the payoff actually obtained by the algorithm. Our work is most related to the study of dynamic versions of the multi-arm bandit (MAB) paradigm where either the set of arms or their expected reward may change over time. Motivated by task scheduling, Gittins [10] proposed a policy where only the state of the active arm (the arm currently being played) can change in a given step, and proved its optimality for the Bayesian formulation with time discounting. This seminal result gave rise to a rich line of work, a proper review of which is beyond the scope of this paper. In particular, Whittle [23] introduced an extension termed restless bandits [23, 6, 15], where the states of all arms can change in each step according to a known (but arbitrary) stochastic transition function. Restless bandits have been shown to be intractable: e.g., even with deterministic transitions the problem of computing an (approximately) optimal strategy is PSPACE -hard [18]. Sleeping bandits problem, where the set of strategies is fixed but only a subset of them available in each step, were studied in [9, 7] and recently, using a different evaluation criteria, in [13]. Strategies with expected rewards that change gradually over time were studied in [19]. The mixture-of-experts paradigm is related [11], but assumes that data tuples are provided to each expert, instead of the tuples being picked by the algorithm, as in the bandit setting.
 Auer et al. [3] adopted an adversarial approach: they defined the adversarial MAB problem where the reward distributions are allowed to change arbitrarily over time, and the goal is to approach the performance of the best time-invariant policy. This formulation has been further studied in several other papers. Auer et al. [3, 1] also considered a more general definition of regret, where the comparison is to the best policy that can change arms a limited number of times. Due to the overwhelming strength of the adversary, the guarantees obtained in this line of work are relatively weak when applied to the setting that we consider in this paper.
 Another aspect of our model is that unexplored arms are always available. Related work broadly comes in three flavors. First, new arms can become available over time; the optimality of Gittins X  index was shown to extend to this case [22]. The second case is that of infinite-armed bandits with discrete arms, first studied by [4] and recently extended to the case of unknown payoff distributions and an unknown time horizon [20]. Finally, the bandit arms may be indexed by numbers from the real line, implying uncountably infinite bandit arms, but where  X  X earby X  arms (in terms of distance along the real line) have similar payoffs [12, 14]. However, none of these approaches allows for arms to appear then disappear, which as we show later critically affects any regret bounds. In this section we show that in the mortal multi-armed bandit setting, the regret per time step of any algorithm can never go to zero, unlike in the standard MAB setting. Specifically, we develop an upper bound on the mean reward per step of any such algorithm for the state-aware, budgeted death case. We then use reductions between the different models to show that this bound holds for the state-oblivious, timed death cases as well.
 We prove the bound assuming we always have new arms available. The expected reward of an arm is drawn from a cumulative distribution F (  X  ) with support in [0 , 1] . For X  X  F (  X  ) , let E [ X ] be the expectation of X over F (  X  ) . We assume that the lifetime of an arm has an exponential distribution with parameter p , and denote its expectation by L = 1 /p . The following function captures the tradeoff between exploration and exploitation in our setting and plays a major role in our analysis: Theorem 1. Let  X   X  ( t ) denote the maximum mean reward that any algorithm for the state-aware mortal multi-armed bandit problem can obtain in t steps in the budgeted death case. Then lim t  X  X  X   X   X  ( t )  X  max  X   X (  X  ) .
 Proof sketch. We distinguish between fresh arm pulls, i.e., pulls of arms that were not pulled before, and repeat arm pulls. Assume that the optimal algorithm pulls  X  ( t ) distinct (fresh) arms in t steps, and hence makes t  X   X  ( t ) repeat pulls. The expected number of repeat pulls to an arm before it expires is (1  X  p ) /p . Thus, using Wald X  X  equation [8], the expected number of different arms the algorithm must use for the repeat pulls is ( t  X   X  ( t ))  X  p/ (1  X  p ) . Let ` ( t )  X   X  ( t ) be the number of distinct arms that get pulled more than once. Using Chernoff bounds, we can show that for any p ( t  X   X  ( t )) / (1  X  p )  X  (1  X   X  ) different arms for the repeat pulls. Call this event E 1 (  X  ) . Next, we upper bound the expected reward of the best ` ( t ) arms found in  X  ( t ) fresh probes. For any with expected reward greater or equal to  X  ( h ) is ( ` ( t ) / X  ( t ))(1  X  h ) . Applying the Chernoff bound, for any  X ,h &gt; 0 there exists t (  X ,h ) such that for all t  X  t (  X ,h ) the probability that the algorithm this event E 2 (  X ,h ) .
 Let E (  X ,h ) be the event E 1 (  X  )  X  X E 2 (  X ,h ) . The expected reward of the algorithm in this event after Pr( E (  X ,h )) . As  X ,h  X  0 , Pr( E (  X ,h ))  X  1 , and the expected reward per step when the algorithm pulls  X  ( t ) fresh arms is given by lim sup t  X  X  X   X   X  ( t )  X  max  X   X (  X  ) .
 In Section 5.1 we present an algorithm that achieves this performance bound in the state-aware case. The following two simple reductions establish the lower bound for the timed death and the state-oblivious models.
 Lemma 2. Assuming that new arms are always available, any algorithm for the timed death model obtains at least the same reward per timestep in the budgeted death model.
 Although we omit the proof due to space constraints, the intuition behind this lemma is that an arm in the timed case can die no sooner than in the budgeted case (i.e., when it is always pulled). As a result, we get: Lemma 3. Let  X   X  det ( t ) and  X   X  sto ( t ) denote the respective maximum mean expected rewards that any algorithm for the state-aware and state-oblivious mortal multi-armed bandit problems can obtain after running for t steps. Then  X   X  sto ( t )  X   X   X  det ( t ) .
 We now present two applications of the upper bound. The first simply observes that if the time to find an optimal arm is greater than the lifetime of such an arm, the the mean reward per step of any algorithm must be smaller than the best value. This is in contrast to the standard MAB problem with the same reward distribution, where the mean regret per step tends to 0.
 Corollary 4. Assume that the expected reward of a bandit arms is 1 with probability p &lt; 1 / 2 and 1  X   X  otherwise, for some  X   X  (0 , 1] . Let the lifetime of arms have geometric distribution with the same parameter p . The mean reward per step of any algorithm for this supply of arms is at most 1  X   X  +  X p , while the maximum expected reward is 1 , yielding an expected regret per step of  X (1) . Corollary 5. Assume arm payoffs are drawn from a uniform distribution, F ( x ) = x,x  X  [0 , 1] . Consider the timed death case with parameter p  X  (0 , 1) . Then the mean reward per step in bounded In this section we present and analyze a number of algorithms specifically designed for the mortal multi-armed bandit task. We develop the optimal algorithm for the state-aware case and then modify the algorithm to the state-oblivious case, yielding near-optimal regret. We also study a subset ap-proach that can be used in tandem with any standard multi-armed bandit algorithm to substantially improve performance in the mortal multi-armed bandit setting. 5.1 The state-aware case We now show that the algorithm D ET O PT is optimal for this deterministic reward setting. Assume the same setting as in the previous section, with a constant supply of new arms. The expected reward of an arm is drawn from cumulative distribution F (  X  ) . Let X be a random variable with that distribution, and E [ X ] be its expectation over F (  X  ) . Assume that the lifetime of an arm has an exponential distribution with parameter p , and denote its expectation by L = 1 /p . Recall  X (  X  ) from (1) and let  X   X  = argmax  X   X (  X  ) . Now, Theorem 6. Let D ET O PT (t) denote the mean per turn reward obtained by D ET O PT after running for t steps with  X   X  = argmax  X   X (  X  ) , then lim t  X  X  X  D ET O PT ( t ) = max  X   X (  X  ) .
 Note that the analysis of the algorithm holds for both budgeted and timed death models. 5.2 The state-oblivious case We now present a modified version of D ET O PT for the state-oblivious case. The intuition behind this modification, S TOCHASTIC , is simple: instead of pulling an arm once to determine its payoff  X  , the algorithm pulls each arm n times and abandons it unless it looks promising. A variant, called S
TOCHASTIC WITH E ARLY S TOPPING , abandons the arm earlier if its maximum possible future reward will still not justify its retention. For n = O log L/ 2 , S TOCHASTIC gets an expected reward per step of  X (  X   X   X  ) and is thus near-optimal; the details are omitted due to space constraints. The subset heuristic. Why can X  X  we simply use a standard multi-armed bandit (MAB) algorithm for mortal bandits as well? Intuitively, MAB algorithms invest a lot of pulls on all arms (at least logarithmic in the total number of pulls) to guarantee convergence to the optimal arm. This is necessary in the traditional bandit settings, but in the limit as t  X  X  X  , the cost is recouped and leads to sublinear regret. However, such an investment is not justified for mortal bandits: the most gain we can get from an arm is L (if the arm has payoff 1 ), which reduces the importance of convergence to the best arm. In fact, as shown by Corollary 4, converging to a reasonably good arm suffices. However, standard MAB algorithms do identify better arms very well. This suggests the following epoch-based heuristic: (a) select a subset of k/c arms uniformly at random from the total k arms at the beginning of each epoch, (b) operate a standard bandit algorithm on these until the epoch ends, and repeat. Intuitively, step (a) reduces the load on the bandit algorithm, allowing it to explore less and converge faster, in return for finding an arm that is probably optimal only among the k/c subset. Picking the right c and the epoch length then depends on balancing the speed of convergence of the bandit algorithm, the arm lifetimes, and the difference between the k -th and the k/c -th order statistics of the arm payoff distribution; in our experiments, c is chosen empirically.
 Using the subset heuristic, we propose an extension of the UCB1 algorithm 2 [2], called UCB1 K / C , for the state-oblivious case. Note that this is just one example of the use of this heuristic; any stan-dard bandit algorithm could have been used in place of UCB1 here. In the next section, UCB1 K / C is shown to perform far better than UCB1 in the mortal arms setting.
 The A DAPTIVE G REEDY heuristic. Empirically, simple greedy MAB algorithms have previously been shown to perform well due to fast convergence. Hence for the purpose of evaluation, we also compare to an adaptive greedy heuristic for mortal bandits. Note that the n -greedy algorithm [2] does not apply directly to mortal bandits since the probability t of random exploration decays to zero for large t , which can leave the algorithm with no good choices should the best arm expire. In this section we evaluate the performance of UCB1 K / C , S TOCHASTIC , S TOCHASTIC WITH E
ARLY S TOPPING , and A DAPTIVE G REEDY in the mortal arm state-oblivious setting. We also com-pare these to the UCB1 algorithm [2], that does not consider arm mortality in its policy but is among the faster converging standard multi-armed bandit algorithms. We present the results of simulation studies using three different distributions of arm payoffs F (  X  ) .
 Uniform distributed arm payoffs. Our performance analyses assume that the cumulative payoff distribution F (  X  ) of new arms is known. A particularly simple one is the uniform distribution,  X  it  X  uniform(0 , 1) . Figure 1(a) shows the performance of these algorithms as a function of the expected lifetime of each arm, using a timed death and state-oblivious model. The evaluation was performed over k = 1000 arms, with each curve showing the mean regret per turn obtained by each algorithm when averaged over ten runs. Each run was simulated for ten times the expected lifetime of the arms, and all parameters were empirically optimized for each algorithm and each lifetime. Repeating the evaluation with k = 100 , 000 arms produces qualitatively very similar performance. We first note the striking difference between UCB1 and UCB1 K / C , with the latter performing far better. In particular, even with the longest lifetimes, each arm can be sampled in expectation at most 100 times. With such limited sampling, UCB1 spends almost all the time exploring and generates almost the same regret of 0.5 per turn as would an algorithm that pulls arms at random.
 In contrast, UCB1 K / C is able to obtain a substantially lower regret by limiting the exploration to a subset of the arms. This demonstrates the usefulness of the K / C idea: by running the UCB1 algorithm on an appropriately sized subset of arms, the overall regret per turn is reduced drastically. In practice, Figure 1: Comparison of the regret per turn obtained by five different algorithms assuming that new arm payoffs come from the (a) uniform distribution and (b) beta (1 , 3) distribution. with k = 1000 arms, the best performance was obtained with K / C between 4 and 40, depending on the arm lifetime.
 Second, we see that S TOCHASTIC outperformed UCB1 K / C with optimally chosen parameters. matches the best performance we were able to obtain by any algorithm. This demonstrates that (a) the state-oblivious versions of the optimal deterministic algorithm is effective in general, and (b) the early stopping criterion allows arms with poor payoff to be quickly weeded out.
 Beta distributed arm payoffs. While the strategies discussed perform well when arm payoffs are uniformly distributed, it is unlikely that in a real setting the payoffs would be so well distributed. In particular, if there are occasional arms with substantially higher payoffs, we could expect any algorithm that does not exhaustively search available arms may obtain very high regret per turn. Figure 1(b) shows the results when the arm payoff probabilities are drawn from the beta (1 , 3) distri-bution. We chose this distribution as it has finite support yet tends to select small payoffs for most arms while selecting high payoffs occasionally. Once again, we see that S TOCHASTIC WITH E ARLY S
TOPPING and A DAPTIVE G REEDY perform best, with the relative ranking of all other algorithms the same as in the uniform case above. The absolute regret of the algorithms we have proposed is increased relative to that seen in Figure 1(a), but still substantially better than that of the UCB1. In fact, the regret of the UCB1 has increased more under this distribution than any other algorithm. Real-world arm payoffs. Considering the application that motivated this work, we now evaluate the performance of the four new algorithms when the arm payoffs come from the empirically observed distribution of clickthrough rates on real ads served by a large ad broker.
 Figure 2(a) shows a histogram of the payoff probabilities for a random sample of approximately 300 real ads belonging to a shopping-related category when presented on web pages classified as belonging to the same category. The probabilities have been linearly scaled such that all ads have payoff between 0 and 1 . We see that the distribution is unimodal, and is fairly tightly concentrated. By sampling arm payoffs from a smoothed version of this empirical distribution, we evaluated the performance of the algorithms presented earlier. Figure 2(b) shows that the performance of all the algorithms is consistent with that seen for both the uniform and beta payoff distributions. In partic-ular, while the mean regret per turn is somewhat higher than that seen for the uniform distribution, it is still lower than when payoffs are from the beta distribution. As before, S TOCHASTIC WITH E
ARLY S TOPPING and A DAPTIVE G REEDY perform best, indistinguishable from each other. We have introduced a new formulation of the multi-armed bandit problem motivated by the real world problem of selecting ads to display on webpages. In this setting the set of strategies available to a multi-armed bandit algorithm changes rapidly over time. We provided a lower bound of linear regret under certain payoff distributions. Further, we presented a number of algorithms that perform substantially better in this setting than previous multi-armed bandit algorithms, including one that is optimal under the state-aware setting, and one that is near-optimal under the state-oblivious setting. Finally, we provided an extension that allows any previous multi-armed bandit algorithm to be used Figure 2: (a) Distribution of real world ad payoffs, scaled linearly such that the maximum payoff is 1 and (b) Regret per turn under the real-world ad payoff distribution. in the case of mortal arms. Simulations on multiple payoff distributions, including one derived from real-world ad serving application, demonstrate the efficacy of our approach.
 Acknowledgments We would like to thank the anonymous reviewers for their helpful comments and suggestions.
