 1. Introduction
There have been major changes on the ranking schemas of search engines over the last two years. 1 Google, Bing and Yahoo!, have started focusing on users and provide them with high quality content in order to get valuable user feedback. In parallel, Search Engine Optimization (SEO) has been under constant changes in order to capture the up-to-date search engine ranking strategies. This way SEO aspires to assist websites in achieving higher and better related rankings.

Search engines (SE) inherently are not able to produce popular content, only to promote it. Considering this fact, SEO strategies will always be effective, given that they recognize the factors that search engines value for their rankings and exploit them as much as possible.

Up until recently these factors were machine-related, i.e. metrics calculated by machines, not users. Lately, major SE players have been trying to promote the quality of the projected content to the users as the major characteristic for website evaluation.
Koningstein (2012) mentions a number of techniques followed by Google in order to identify documents that have been modi (in terms of content) for achieving higher rankings in a spamming fashion. Additionally, Lamping and Pearson (2011) discuss meth-ods that de fi ne the quality of documents according to their semantic relation with others and with given queries.

The major search engines have published their guides with some basic SEO suggestions. The guides of Google, 2 Bing 3 and Yahoo! portray technical differences, but they all conclude that content should be the focus of SEO. Content should be unique, of high quality and user oriented.

Nevertheless, due to the fact that search engines are not trans-parent regarding their ranking schemas, conclusions about them are only extracted through their result pages analysis and by exploration of their patents. Several organizations and companies have de fi ned their own metrics for the evaluation of web docu-ments from various perspectives. Moz 5 (former SEOmoz) metrics are very popular and are employed by a plethora of users in order to analyze their web presence and evaluate the popularity of their websites.
 previously concluded that Latent Dirichlet Allocation (LDA) ( Blei et al., 2003 ) has proven effective ( Mavridis and Symeonidis, 2012 ). LDA's added-value in SEO techniques is also strengthened by Patterson's ( Patterson, 2012 ) analysis on the mechanism that
Google employs in order to relate query phrases to documents for indexing, retrieval, description and analysis. The mechanism focuses on the extraction of top phrases out of documents and groups of documents, and calculates a score for the importance of each phrase and a total score for the top phrases; this is, practically, LDA's core objective.
 assess LDA's in fl uence for content production with respect to search engine rankings. We argue that the domain of a website and the metrics employed for ranking web documents is corre-lated to the effect of LDA on content generation. LSHrank extends our previous approach ( Mavridis and Symeonidis, 2012 ) and has been created in order to analyze search engine result pages (SERPs) and draw conclusions regarding the correlation of SE ranking schemas and the semantic analysis of content. Assessment is performed against Moz metrics, and the algorithm is tuned in order to produce the optimal content. The remainder of this paper discusses the state-of-the art regarding semantic analysis in SEs and presents the LDA's relation to SEO. Section 3 describes the architecture of LSHrank , while Section 4 discusses the experiments performed in order to assess LDA various parameters. Section 5 explores in depth LDA and LSHrank performance with thorough experiments based on the results of Section 4 . Furthermore,
Section 6 presents a set of experiments run to compare LSHrank against other semantic analysis SEO approaches. Finally, Section 7 summarizes work performed, probes on future work and concludes the paper. 2. Related work/background theory analysis in web search. There exist multiple approaches, from capturing user input and using ontologies to identifying concepts and performing semantic content analysis. Besides discussing state-of-the-art in all these approaches, we further elaborate on semantic content analysis, the use of LDA as a semantic analysis algorithm and the novelties of our approach. 2.1. User input to de fi ne semantics on content sis of web content largely exploits user input. Bakir and
Kulshreshtha (2012) provide details on Google's intention to cap-ture user-experience through the PandaUpdates ,withthe fi update 6 being announced by Google early 2011. Authors describe the deployment of human reviewers in order to capture problems that have to do with user-experience on web content and on data that an information system stores and indexes. In the same context,
Lawrence et al. (2005) presented a system on capturing user-related signals in order to determine relevance scores to the search results of Google. Going even further, Google attempts to perform hier-archical categorization of search results in topics by capturing user input, as described by Sandler et al. (2012) .Moreover, Pedersen et al. (2012) present the way Google uses access history of a user on content in order to calculate semantic similarities using topic vectors for the content of documents.
 Lei et al. (2006) follow a different approach; they propose
SemSearch , a search engine that provides users with precise answers to their queries through a simple interface. SemSearch captures user input and creates semantic entities in order to provide them with the desirable result. Still on the context of personalized search, Teevan et al. (2005) propose an automated information retrieval mechanism that exploits user search queries and their browsing history in order to provide ef fi cient client-side algorithms. In a similar fashion, Yin and Shah (2010) of Microsoft have proposed a methodology in order to use search logs into recognizing the user intent and semantics in search queries. The representative terms form a tree built based on directed maximum spanning, hierarchical agglomerative clustering and pachinko allocation model. The use of human raters via Amazon's Mechan-ical Turk update 7 veri fi ed the methodology's success on recogniz-ing signi fi cant representative terms. 2.2. Recognition of documents concepts and entities
Another approach widely applied strives to recognize entities and concepts related to documents. Hong et al. (2013) presented a methodology that identi fi es entities highly correlated to search query entities. It is based on various semantic factors that are not related to keywords nor link structure, indicating that Google can identify the semantic relatedness of results to entities.
Van Durme and Pasca (2008) discuss a method aiming to extract large numbers of concept classes and corresponding instances from documents using distributional similarity metrics based on the logic of Pantel and Ravichandran (2004) . Following this methodology,
Pasca (2013) analyzed Google's approach to recognize semantic classes/concepts in documents by de fi ning frequency and diversity scores on class-instance pairs, linked through  X  is-a  X  relationships.
Lee et al. (2013) described a framework of Google used to recognize candidate terms for an entity by analyzing the documents related to theentity.Theproposedframeworkemploysmeasurementofthe frequency of the appearance on various terms in web documents, focuses on the  X  known for  X  terms and recognizes the semantics of an entity in order to identify the important terms/words related to it. In the same context, Zhou et al. (2005) present the ability that
Google has in recognizing entity names in search results and in altering these results by providing documents that are related to these entities. On the other hand, Mishne et al. (2009) discuss how
Yahoo! can identify the most probable interpretation of a query in order to rank the documents according to their semantic relevance to the interpretation chosen. As Padovitz and Nagarajan (2012) state, Microsoft has also been exploring the fi eld from the entities perspective, attempting to de fi ne the complexity of extracting an entity identi fi cation from a corpora of documents.

Finally, Federici (2013) discuss how Google can calculate the transition probabilities that represent the relationship of the entities. In his approach, the term  X  entity  X  consists of the query, the documents related to the query, the search session related, the time of the query submission, anchor texts in the links of the documents, the advertisements presented and the associated domains of the documents. The calculation of the probabilities considers all the information of the entities in order to rank search results in an accurate fashion, identify semantic relation among queries and to improve the results of vertical search features. In general, the probabilities express the strength of the relationships of entities and the use of user behavior data. 2.3. Structure, ranking, ontologies and ground truth use
Approaches that attempt to recognize semantics through structure also exist. Kurland and Lee (2010) propose a number of methods for structural re-rankings using the inter-document generation relationships, while Xu et al. (2006) propose a colla-borative tag suggestion algorithm that employs a set of general tag criteria to spot high quality tags in order to form structured directories to unstructured web search engines.
 On the other hand, Haveliwala (2002) proposes Topic-Sensitive PageRank , an algorithm that follows the PageRank paradigm, and computes a set of PageRank vectors for a list of representative topics, in order to prove that users traverse to webpages semanti-cally correlated to the start page. Bao et al. (2007) propose the use of social annotations for the optimization of web searches. The optimization follows a two-stage process: (a) similarity ranking through SocialSimRank (SSR), which calculates the similarity between social annotations and queries and, (b) static ranking with SocialPageRank , which is relative to the popularity of a webpage.

Gauch et al. (2003) support the use of weighted ontologies to conceptually represent users and web content. In order to provide personalized browsing, classi fi cation techniques have been employed for mapping user-created ontologies to the reference ontology. Ahn et al. (2010) has described a methodology of Microsoft in order to identify relevant concepts for a query using an ontology of concepts as ground truth. The methodology tries to provide Bing search engine users with the most semantically related results to the concept.

Finally, Tran et al. (2007) use an ontology and lexical knowl-edge to map search queries to an expressive logical language, while Gabrilovich and Markovitch (2007) propose a mechanism called Explicit Semantic Analysis (ESA) that interprets text based on the concepts that are described in Wikipedia. 2.4. Content semantic analysis
All the above approaches provide different viewpoints of sem-antically based SEO. Authors argue that content analysis is also signi fi cant for webpage ranking and its potential is explored within the context of this work.

Cohen et al. (2012) has clearly stated that Google uses more advanced similarity scores than TF-IDF ( Salton and McGill, 1986 )in order to determine context value that identi fi es a context of an object, inter-context (proportional to number of different context values) and intra-context scores (proportional to the number of time the feature value is associated with each context value). The patent determines the similarity scores among object on a dataset based on contextual associations of them that are de fi ned through complex mathematical forms.

One of the fi rst approaches advancing content-related metrics is that of Latent Semantic Indexing (LSI). Deerwester et al. (1990) proposed LSI as a solution to the problem of TF-IDF regarding the small reduction in description length. LSI uses Singular Value Decomposition (SVD) and attempts to describe a statistical tech-nique to calculate semantic relatedness in unstructured text documents. It is signi fi cant to underline that LSI provides a solution regarding synonymy of words, but captures polysemy notions partially.
 Hofmann (1999) then developed an extension to LSI, called Probabilistic LSI (pLSI). pLSI matches each word of a given docu-ment in a mixture model consisting of the topic and associates of each document with a probability distribution over the topics. It creates an aspect/data model based on likelihood principle and achieves the minimization of word perplexity. The drawback of pLSI is the lack of a probabilistic model at the level of documents.
This limitation of pLSI provided the trigger for the introduction of Latent Dirichlet Allocation (LDA). LDA is the probabilistic model of a corpus which has its base on the idea that latent topics in random mixtures describe a document and each topic consists of a multi-dimensional distribution over words. In essence, LDA recognizes the prominent topics of the documents and the top words that characterize each topic. For a given corpus D of documents a set of k topics T  X f t 1 ; t 2 ; ... ; t k g is created. Each document D of p words based on an a priori distribution of words over the topic.
The original model of LDA had a major inferential problem in the calculation of a posterior distribution of the hidden variables of a goal has been the maximizing of the normalized version of it. Blei et al. (2003) suggested the use of an algorithm that is based on
Variational Expectatio n Maximization (EM). Minka and Lafferty (2002) proposed another approach called Expectation Propagation aiming to lead to better inferential accuracy. Then, Grif
Steyvers (2004) suggested the use of Gibbs sampling. The logic related to the assignment of all the other words on the given topic.
One of the major arguments against LDA and its alternatives has been their time performance in  X  big  X  domains, like web search and
SEO in our case. On the other hand, one could mention numerous approaches that improve the performance of LDA mechanics and, thus, could be used for the semantic analysis of web documents.
Nallapati et al. (2007) presented two parallel implementations of the variational EM algorithm for LDA; one using multiprocessing and one in distributed environment. They argue that both their implementations lead to speed-ups with the distributed version overcoming scalability issues successfully. Gomes et al. (2008) described a two level clustering process in order to reduce the computational cost of LDA. Their approach focused on using a bound on the amount of memory in the variational EM.

Newman et al. (2007) described synchronous methods to perform Gibbs Sampling in a distributed environment. Their approach showed promising results but with computational costs.
Furthermore, Smyth et al. (2008) approached Gibbs Sampling in a distributed environment with the communication to be purely asynchronous. Their choice not to be based on global synchroniza-tion led to a decrease in computational time and memory.
Porteous et al. (2008) presented a methodology that was based on better computational organization and the nature of topic association probabilities, in order to increase the speed of LDA with Gibbs Sampling.

Finally, one could mention work of Wang et al. (2009) analyzed two parallel implementations of LDA (PLDA); one using MPI ( Thakur et al., 2005 )andonewithMapReduce( Dean and Ghemawat, 2008 ).
Their results display that MPI-PLDA is more ef fi cient in in-memory communication and achieves a better speed-up in comparison to
Map-Reduce-PLDA. Nevertheless, the authors state that as the data size and the number of machines increase, both approaches could be used. Canini et al. (2009) approached the sampling issues of LDA by presenting two Markov chain Monte Carlo (MCMC) methodol-ogies. The fi rst one is based on performing incremental Gibbs sampling and the second using particle fi lters, which are a known sequential Monte Carlo method. Their results showed that these two algorithms did not perform as well as batch collapsed Gibbs sampling.

Hoffman et al. (2010) presented an online variational inference for LDA in order to deal with massively large data. The newly proposed algorithm outperforms the normal batch algorithm both considering the accuracy of the predictions and the computational cost. It should be outlined that perplexity is used as the evalua-tion metric. As the authors state there have been some ques-tions by Chang et al. (2009) regarding the ability of their approach to compare different topic models, but it is well suited for the measurements regarding the ef fi ciency of the inference algorithms. 2.5. Content semantic analysis for SEO it was fi rst reported as a possible factor in SEO by Bishop by Gruber et al. (2007) in 2007 in his GoogleTechTalks. LDA was found to have Spearman correlation of 0.17 ( Spearman, 1904 ) with the ranking of SERPs 9 which is signi fi cant score considering the values of Spearman correlation of other factors of SEO ( Fig. 17
Appendix). He and Chang (2013) presented a method that Google has designed in order to semantically associate a query to results based on topic. The method analyzes the use of conditionals probabilities in order to semantically map the words of a query, the domain topics and the possible results all together and to calculate a relevance rank of the results. Their method is similar to
LDA but it also considers possible search click data and query history in its computation component. Jain and et al. (2011) describes how Google uses the relationship among terms in a plethora of documents and the method under which related terms could be used to navigate to speci fi c documents. Bharat et al. (2007) , on the other hand, provide us with a clear view on the term semantic unit that Google uses in order to characterize certain combinations of multiple terms, while Dean et al. (2012) describe a method that is used to relate a group of search terms to particular topics by the appearance in search queries, the increase in a number of corresponding search results and rate of increase in the number of search results.
 the content of a web page in order to associate topics to it. For the extraction of topics a possible approach stated is the computation of a term vector for the content of the web page and the selection (2012) describes a system that Google created and that is similar to LDA's basic functionality.
 signi fi cant information on the semantics of given documents.
There have been multiple approaches on the optimization of the process of semantic content analysis, but they have focused on the performance and the accuracy of it. To our knowledge, there have not been any approaches on combining semantic content analysis with search engine results in order to extract valuable informa-tion regarding its possible in fl uence. Our proposed mechanism explores the use of semantic content analysis in a different direction: given that search engines play a major role in our online experience, and given that their query results are becoming more and more focused and ef fi cient, their corpus of documents present an excellent base to perform semantic analysis on. LDA was chosen as it is a proven over-time algorithm on semantic content analysis and its functionality is similar to mechanisms that the popular search engines employ for extraction of topics and recognition of semantics around the web. 2.6. LSHrank novelties in comparison to LDArank previous work, LDARank ( Mavridis and Symeonidis, 2012 ). Sum-marizing previous work, LDARank collects query results from top search engines and employs state-of-the-art metrics for the evaluation of webpages. Extending work on LDARank , LSHrank employs headless browsing of webpages in order to capture user experience, while also recording semantic-markup related metrics. A headless browser is a web browser without a visual interface.
There have been indications that the major SEs do use headless browsing in order to extract as much info as possible about the documents that the users are provided with upon a query. Maykov and Chellapilla (2007) imply that Microsoft's crawler
Bingbot is practically a browser. Bingbot interacts with the docu-ments it crawls by constructing their Document Object Model (DOM) (claim 12). Furthermore, it is able to provide speci responses to webpage scripts. Li et al. (2011) analyze mechanism of Microsoft used for the recognition of media elements in websites and describe how it ranks them based on their  X  dominance
Therefore, Microsoft parses media elements and has already de fi ned the mechanism to include them in its rankings schema.
Prabhakar et al. (2009) present a Yahoo! framework that exe-cutes scripts, completes forms in automatic fashion and is connected to a web browser and a crawler. Malpani et al. (2012) provides us with information about Google's headless browsing mechanism. It aims to rank non-text content items by creating labels/tags related to them based on info gathered from corresponding web pages.
Gokturk et al. (2007) further explore the functionalities of this mechanism and describe how it could analyze images and extract human-added information, beyond plain anchor text.

Moreover, should one explore Google's  X  webmasters help  X  section, he/she identi fi es that Google uses Instant Previews in its
SERPs in order to provide users with a  X  graphic representation of the web-page, generally highlighting the relevant sections based on the search query  X  . Instant Previews, though, are based on Google-bot and  X  the preview generator renders Javascript, AJAX, CSS3, frames, iframes and Flash  X  (as described in the Google Instant Previews FAQ section 10 ); AJAX by default is content loaded by JavaScript when an action takes place after a page is loaded.
Additionally, LSHrank includes Twitter as source of content. Java et al. (2007) have shown that people use Twitter in order to exchange information and comment on several topics and Google Authorship has revealed Google's goal towards SERPs that include
Social media signals. Minogue et al. (2009) address the signi cance of Authorship in SERP rankings Google. Social Media's in fl uence in SEs and Web is an essential trend to be captured.
Other than that, LSHrank follows the LDArank approach; results are analyzed based on Latent Dirichlet Analysis (LDA) in order to identify prominent topics and words, the creation of new queries, and thus content. One may refer to Mavridis and Symeonidis (2012) for more information.

Practically, our approach attempts to explore the use of LDA on actual search engine results in order to evaluate its effect as a semantic content analysis approach. The architecture of the LSHrank framework that has been developed for this purpose is discussed next. 3. The LSHrank architecture
LSHrank collects query results from the top search engines, employs state-of-the-art metrics, headless browsing on webpages and includes Twitter information in order to evaluate the content of webpages. Semantic analysis of the content extracted is per-formed using LDA and the creation of new queries is controlled by using NGD (Normalized Google Distance). The overall LSHrank approach is described in steps, as depicted in Fig. 1 .
LSHrank is a generic crawling evaluation mechanism that allows for a number of SEs to be employed, while the granularity of the analysis and the evaluation metrics can also be de user will. All parameters are easily de fi ned through a con fi le, which is parsed at the initiation phase of the mechanism. Table 1 depicts the con fi guration parameters and the LSHrank internal parameters.

The table is split in three sections: (a) parameters related to the search engine(s) and the crawling mechanism to be employed, (b) LDA-related parameters and (c) evaluation parameters and metrics. Given that focus of the paper is on the ef fi ciency of LDA mechanism, the selection of section (c) evaluation metrics is not discussed. Analysis on their signi fi cance is out of the scope of this work, and, thus, omitted. 3.1. Queries to search engines
Initially, one should de fi ne the queries of interest. These queries can be enhanced with the help of a Keyword Tool like Google AdWords. Google custom search API, Yahoo BOSS API and Bing Search API via Azure Marketplace are employed in order to perform number of m results returned by the APIs is de fi ned at the initiation for Yahoo!, B for Bing and G for Google) and is extracted in JSON format ( Crockford, 2006 ). Json-simple 11 is the employed parser in order to analyze the results and to extract the corresponding URLs. 3.2. Evaluation and selection of top results
The evaluation of the top  X  results  X  Rtop  X f rt 1 ; rt 2 performed against the following metrics:
Simple ranking  X  SR  X  : selection of the top  X  SR results according to their rank, where  X   X  m n 3
Visibility score  X  ViS  X  : selection of the top  X  ViS results according to their rank, after integrating the rankings of the three search engines, where  X   X  m n 3 # duplicate results
Moz Page Authority  X  PA  X  : selection of the top  X  PA results for each search engine, where  X   X  m or based on threshold option on the score of PA
Moz Domain Authority  X  DA  X  : selection of the top  X  DA results for each search engine, where  X   X  m or based on threshold option on the score of DA
Moz mozRank  X  MR  X  : selection of the top  X  MR results for each search engine, where  X   X  m or based on threshold option on the score of MR
Moz subdomain mozRank  X  sMR  X  : selection of the top  X  sMR for each search engine, where  X   X  m or based on threshold option on the score of sMR The values of MR , PA , DA , and sMR are calculated through the Moz
Mozscape free API. 12 The mechanism would have been more complete if it included the mozTrust and External mozRank metrics, which are however available only through the Moz Mozscape paid API, distributed under a certain cost. Nevertheless,
LSHRank follows an expandable architecture, thus their incorpora-tion is seamless if they are available. The values of the above
SEOmoz metrics are extracted in JSON format and the google-gson parser 13 is used for the analysis of the JSON fi les. 3.3. Headless browsing
During the last few years, headless browsing is uptaken by search engines, in the place of search engine robots, in order to capture user experience. This trend has unlocked the potential of headless browsing in the quest to rank webpages based on user experience.

The URLs included in Rtop are submitted to the headless browsing procedure, which is performed with the use of JxBrow-ser . Browsing is performed along with thorough DOM (Docu-ment Object Model) analysis in order to extract all information from each document. Statistics are kept regarding the human non-text readable content that is parsed by the headless browser. In speci fi c, the following statistics are calculated: the number of iframes Nif the number of embedded videos Ne the number of scripts Ns the number of internal links Li the number of redirect links Lr the total number of links Lt the number of semantic triples Str the number of schema.org uses Sch the number of rel tags Nr 3.4. Twitter text extraction the text ( TwTXT ) from the 100 most popular and recent tweets related to the given query. The metric according to which the tweets are ranked is de fi ned internally by Twitter, thus no details are available. Twitter4j provides inbuilt functions in order to extract the text from the tweets. 3.5. Webpage text content extraction and formation of content is extracted with the use of JSOUP html parser 16 . The body of the content BC along with the anchor text AT of internal links Lint , the metadata MDT such as alt image text  X  T , meta description MDESC and meta title MT are extracted and form the text content of the webpage TXTC which, along with TwTXT form the total content TC .
It has to be mentioned that from unstructured non-textual or textual data all possible HTML tags are parsed and included in TC .
Regular expressions, symbols and stop words are removed based on the symbols list SYL and stop words list STL provided by the
WEKA data mining suite 17 , 18 and Rainbow by Carnegie Mellon
Table 2 presents the special characters that are replaced. 3.6. Semantic analysis phase
Semantic analysis is performed on the text of the retained results for a given query, in order to recognize the dominant words that compose the dominant content for this query. The JGibbLDA implementation is employed, which is a Java implementation of
LDA that performs Gibbs Sampling for parameter estimation and inference. The output of the semantic analysis phase is a list that contains the most probable words mp for the given queries. The
LDA parameters that can be con fi gured in order to de fi of granularity of the analysis are the following ( Table 3 ): 3.7. Creation of new queries and selection process
Based on the list with the most probable words mp identi fi the previous step, a set of new queries is designed. The Combina-tion Generator algorithm ( Knuth, 2006 ) is coupled with the algorithm proposed by Rosen ( Rosen and Krithivasan, 1999 ), in order to calculate all possible combinations of words ( comb ) and permutations ( perm ). However, given that a user query typically contains a fi nite number of words and the order of the words in a query is important for the search engines, only the most powerful combinations with up to wmax words are retained. The similarity of the new possible queries NPQ with the original query is calculated in the context of NGD ( Cilibrasi and Vitanyi, 2007 ) and the top queries are selected according to the user-de threshold limit ( NGL ) and a max number of new queries maxNQ ,in order to form a new set of top queries Qnew i  X  q i 1 ; q In the context of our problem, NGD is de fi ned as NGD  X  x ; y  X  X  max f log f  X  x  X  x ; y ; f  X  x ; y  X  the number of results that include both x and y , and M the total number of results that Google considers. 3.8. LSHrank iteration and convergence
The above process (of creating queries and identifying domi-nant words) is repeated until the list of words from the current round of semantic analysis contains at least cwl % of common words with the previous round or a certain amount iLim of iterations is reached. Before convergence checking, stemming on the lists is performed with the use of the Porter Stemmer ( Van Rijsbergen et al., 1980 ). 3.9. LSHrank algorithm and activity diagram
The pseudocode for the LSHrank is displayed in Algorithm 1 , while Fig. 2 portrays the process through an activity diagram. Algorithm 1. LSHrank algorithm 2: counter _ iter  X  0 3: Conv _ flag  X  false 4: while counter _ iter o iLim jj Conv _ flag  X  X  false do 5: for i  X  1to n do 6: query  X  q i 7: if moz  X  X  false then 8: if SE _ G  X  X  true then 9: RS _ G  X  getGoogleResults  X  query ; m  X  10: end if 11: if SE _ B  X  X  true then 12: RS _ B  X  getBingResults  X  query ; m  X  13: end if 14: if SE _ Y  X  X  true then 15: RS _ Y  X  getYahooResults  X  query ; m  X  16: end if 17: end if 18: if moz then 19: threshold  X  0 20:  X   X  m 21: if SM _ o  X  X  2 then 22: threshold  X  SM _ th 23:  X   X  max _ Moz _ results 24: end if 25: SM _ metric  X  mR or DA or PA or mT or smR 26: m  X  max _ possible _ queries 27: if SE _ G  X  X  true then 28: RS _ G _ max  X  getGoogleResults  X  query ; m  X  29: RS _ G  X  sort  X  RS _ G _ max ;  X  ; SM _ metric  X  30: end if 31: if SE _ B  X  X  true then 32: RS _ B _ max  X  getBingResults  X  query ; m  X  33: RS _ B  X  sort  X  RS _ B _ max ;  X  ; SM _ metric  X  34: end if 35: if SE _ Y  X  X  true then 36: RS _ Y _ max  X  getYahooResults  X  query ; m  X  37: RS _ Y  X  sort  X  RS _ Y _ max ;  X  ; SM _ metric  X  38: end if 39: RS _ total  X  RS _ G  X  RS _ B  X  RS _ Y 40: end if 41: if ViS then 42: for i  X  1to m do 43: for j  X  1m do 44: if RS _ total  X  i  X  X  RS _ G  X  j jj RS _ total  X  i  X  X  45: RS _ total _ score  X  i  X  RS _ total _ score  X  i  X  j 46: end if 47: end for 48: end for 49: RS _ total  X  sort _ total  X  RS _ total ; RS _ total s 50: end if 51: TwTXT  X  tweets _ txt  X  query  X  52: for k  X  1to RS _ total _ size do 53: User _ xp _ stats  X  k  X  Headless  X  RS _ total  X  k  X  54: BC  X  k  X  getBody  X  RS _ total  X  k  X  55: AT  X  k  X  getAnchor  X  RS _ total  X  k  X  56: MDT  X  k  X  getMeta  X  RS _ total  X  k  X  57: TXTC  X  k  X  BC  X  k  X  AT  X  k  X  MDT  X  k  X  TwTXT 58: TXTC  X  k  X  removeStopwords  X  TXTC  X  k ; STL  X  59: TXTC  X  k  X  removeSymbols  X  TXTC  X  k ; SYL  X  60: end for 61: end for 62: mp  X  LDA _ Gibbs  X  TC ;  X  ;  X  ; M ; tw ; Lpt ;  X   X  63: CTotal  X  CTotal  X  mp 64: if counter _ iter 4 1 then 65: Conv _ flag  X  CheckConvergence  X  CTotal ; 66: end if 67: counter _ iter  X  X  68: for i  X  1to n do 69: comb  X  Possiblecomb  X  CTotal ; wmax  X  70: perm  X  Possibleperm  X  comb  X  71: end for 72: for i  X  1to n do 73: for j  X  1to perm : size do 74: NGD  X  i  X  j  X  getNGD  X  q i ; perm  X  j  X  75: end for 76: end for 77: Qnew  X  gettopNGD  X  NGD ; NPQ  X  78: Q  X  Qnew 79: end while 80: end procedure 4. De fi ning stable LSHrank parameters
In order to decide on the parameter values for LSHrank we performed numerous experiments. In the following section we discuss the process followed and identify the parameters used in the following experiments. Let us assume that we c onsider the software engineering domain as the base for experimentation. We form two sets of software of 15 more generic terms and the second one of 40, more focused performance evaluation set of the algorithm. 4.1. Setup
The fi rst set of generic SE terms is displayed in Table 4 and the second of more focused words in Table 5 .

Table 6 depicts the LSHrank parameter value combinations applied on the two sets. All the possible c ombinations were considered.
Based on the parameter values selected, experiments on the set of terms are separated in three groups: small scale (A), medium scale (B), and large scale (C), while experiments on the more focused terms are denoted as the (D) group. Table 7 illustrates the experi-ments performed (identi fi ed by their Case id ) and the respective experiment classi fi cation (experiment group). One should mention at this point that in our previous work ( Mavridis and Symeonidis, 2012 )wehavereachedtheconclusionthat specialized terms as input lead to different, more specialized, content ,thuswehavechosento perform experiments using such a set of terms.

For the experiments performed, LSHrank employs each time one of the SR, mR, PA, DA metrics, as well as these metrics on the merged results from all search engines ( Table 8 ). Thus, 2560 experiments (32 cases 8 metrics 10 times) were performed. 4.2. LSHrank convergence per metric and group
Different choice of evaluation metric and different group of cases leads to different convergence rates of LSHrank . The follow-ing four graphs ( Figs. 3  X  6 ) display the convergence rate per metric for groups (A)  X  (D), respectively.
 Groups of experiments: metrics, ( SR , mR ), ViS and mR_m, have values of convergence that are close to PA , DA , PA _ m and DA _ m .

For Group D, which could be considered as small-scale accord-ing to the values of its parameters, simple ranking ( SR ) and mozRank ( mR ) display the highest convergence rates both in non-merged results and merged results ( ViS and mR _ m , respectively).

Based on the results on convergence, the mean convergence of extracted. Using merged results leads to lower convergence percen-tages than non-merged results. In addition, convergence rates have higher values as the scale of the experiments becomes higher. It is also apparent that more focused terms lead to different convergence rates, if we compare the values of both small scale (Group D and
Group A) cases. From a standard deviation perspective, ViS has a constantly high value in all groups of experiments and could be indicator that maybe an unreliable metric. On the other hand, PA proves as the most reliable evaluation metric considering the fact that its standard deviation values are low in comparison to the other metrics. Obviously, more experiments should be performed in order to draw safe conclusions.

Apart from the metric employed, the type/content of the selected domain could also lead to varying performance of a mechanism such as LSHrank . The following section focuses on the signi fi producing content from different domains, which has proven to be insightful (see ( Mavridis and Symeonidis, 2012 )). 5. LSHrank performance on domains of varying content
In order to provide evidence on the added-value of the LSHrank approach, we have applied the mechanism on two domains: software engineering and sports . The fi rst domain was selected due to the fact that semantics in websites discussing software engineering are very ambiguous and, thus, not easy to identify. The second domain was selected exactly because of its wealth in semantic content. Parameter values de fi ned are based on the analysis performed in the previous section; however other para-meter values could be de fi ned for LSHrank initiation, based on the user hypothesis to be tested. 5.1. Experiment setup
The objectives of all the experiments performed with LSHrank in this section were to: (a) explore the difference in identi domains with respect to the values of the various metrics, (b) identify whether the size of the resulting word cloud is related to the SE ranking of webpages and (c) evaluate the differences between the several word clouds that could be produced from different LSHrank con fi gurations.

Two sets of terms were considered for the analysis: (a) a set comprising eight widely used software engineering terms and (b) a set comprising eight widely used sports terms related to basketball, football, soccer and hockey coupled with word in order to direct SEs to websites with web 2.0 and web 3.0 con-tent. Table 9 depicts the terms selected.

Experiments have been organized according to different LDA settings and based on our previous experience ( Mavridis and
Symeonidis, 2012 ). Three small-scale and three large-scale groups of experiments were conducted, given the number of topics and the number of top words of LDA. These groups are presented in
Table 10 . Since lower Lpt value corresponds to more content, the value is set to 0.002. Last but not least, since we would like to reach more accurate conclusions about SEs, m is set to 20 in order to gather more results. 5.2. LDA and convergence
The performance of LSHrank is evaluated against its convergence rate on the selected evaluation metric. Our initial hypothesis is that the domain choice (  X  sports  X  or  X  software engineering  X  the different groups of experiments (small-scale to large-scale) may lead to different convergence values. Moreover, we assume that the various top results selection metrics ( SR , mR , PA , DA , smR )may display signi fi cant difference on the convergence rate. Last but not least, we expect that the convergence rate should increase per round as the algorithm will recognize the semantically signi content. The evaluation of the convergence rate is performed by comparing the mean average convergence rate per group. 5.2.1. Comparison of domains
Fig. 9 depicts the mean average convergence rate in  X  sports and  X  software engineering  X  domain websites. It is important to recognize whether the choice of domain results to a signi statistical difference on the mean convergence rate. Given that data retrieved do not follow a normal distribution, we employ the
Mann  X  Whitney ( Mann and Whitney, 1947 ) approach to measure the signi fi cance of the domain of queries they belong to. This way we calculate the Mann  X  Whitney signi fi cance U and Z and consequently calculate the effect size r  X  z = total number of instances we check for the union of groups. The
Mann  X  Whitney U-test is a non-parametric test used as an alter-native to t-test in non-normal data and tests the mean values of two samples.
 corresponds to a signi fi cant attribute. In case the value of Monte 0 : 5 4 j r j 4 0 : 3 denote a medium effect, 0 : 8 4 j r j size if j r j 4 0 : 8 a very large effect ( Field, 2009 ). has a great effect on the convergence level; therefore analysis from this point on is performed per domain. 5.2.2. Comparison of groups vergence rate for each domain separately. Fig. 10 displays the mean average convergence rate for each categorization. In all small-scale experiments, sports-related documents convergence rate is close to the software engineering-related. However, in large-scale groups the difference the convergence rates of the two domains is larger, and reduces as the number of LDA topics increases.
 engineering  X  domain there is Pearson correlation 0.265 with signi fi cance 0.02 between the convergence rate and the variation from large scale group 1 to large scale group 3. This correlation portrays that there is a signi fi cant connection between the size of the group and the convergence rate; as the size of the group includes more topics and less words per topic, the convergence rate becomes higher.

Wallis testing ( Kruskal and Wallis,1952 )inordertoexplorewhether the differences between the groups are signi fi cant with respect to non-parametric test that could be used to 3 or more groups in a similar fashion as the Mann  X  Whitney U test. Signi fi cance is denoted metric is low. The results of the test are depicted in Table 12 .
From the above signi fi cance scores we conclude that the type of the group in fl uences the convergence in both domains. 5.2.3. Metrics comparison
Based on the analysis performed in Section 4 ,weassumedthatthe selection metrics for top results should have an in fl uence on the depicts the mean average convergence rate per metric grouped by domain choice. From the results above the following could be stated:
The convergence rate is higher in sports-related websites than in Software engineering ones for all metrics, implying that LDA performs better in sports-related content.

Mozrank ( mR ) and Subdomain Mozrank ( smR ) illustrate the highest convergence in sports-related websites.

In Software Engineering-related pages Simple Ranking ( SR ) leads to highest conversion.

In order to check if there is signi fi cant difference between the metrics Simple Rank, Mozrank, Page Authority, Domain Authority, subdomain mozRank and the convergence we perform Kruskal  X  Wallis testing. The results are displayed in Table 13 .
It could be stated that the convergence is in fl uenced by the different metrics. 5.2.4. Comparison of iterations/rounds
The other aspect that we would like to explore is the conver-gence rate per round. As already stated, the logic was that the convergence rate would increase per round. Fig. 12 displays the convergence rates per round.

From the graph it is clear that mean convergence increases per round in software engineering queries, but presents an unstable fashion in sports-related queries. The increase in software engi-neering validates our hypothesis. On the other hand, the unstable results in sports could have as their cause the variety of sports-related websites. The fact that the sports queries include various sports could have also affected the convergence level. The different convergence rate trend per round is another proof of the differ-ence between domains. 5.3. Content production analysis
Table 14 displays the number of words that each of the follo-wing subgroups produces categorized in duplicates, unique and total amount. The subgroups are separated according to their topics, number of top words, in a similar fashion to the grouping in Table 10 regarding LDA options, and domain.

From Table 14 , it is clear that in both sports and software engineering domains the  X  large 3  X  subgroups are the ones that produce the most content and have a low volume of duplicate words. Therefore,  X  large 3  X  subgroups are the most productive ones. It is worth mentioning that  X  small 1  X  subgroup in sports domain has a much lower percentage of duplicates than  X  small 1 subgroup in the software engineering domain.

In Table 15 we provide a comparison of the subgroups of software engineering and sports in terms of common unique words.
It is obvious that splitting the experiments into subgroups has similar effect on both software engineering and sports data. The small cases have a signi fi cant level of common words between them and the large cases present the same pattern. Moreover, the small cases present a high level of commons words with the large ones in some speci fi c subgroups. Therefore, in order to obtain a better understanding of the results, a comparison of the groups separated into two general groups, large and small, according to their topics is necessary.
 the small and large cases by selecting a different amount of top words according to the number of occurrences.
 of unique words and also different words than the small group.
The percentage of common words between the two groups decreases as the number of top words of both groups increases.
Sport data present lower percentages in comparison to software engineering terms. Figs. 13  X  16 illustrate the top 40 words of large and small groups in software engineering and sports according to their frequency. Up to this point, analysis performed is focused on identifying the parameters affecting the performance of LSHrank .
Next, we compare our algorithm against other approaches that perform semantic analysis of web documents. 6. Comparing LSHrank against other approaches we have chosen to compare it with other approaches that focus on performing semantic analysis on web documents and extracting word clusters. TF-IDF ( Salton and McGill, 1986 ) is one of the approaches for LSHrank to be compared against, since it has been a proven and widely used algorithm in the fi eld of text mining. TF-
IDF was incorporated in the semantic mechanism of LSHrank instead of LDA, in order to produce the prominent words related to the web documents parsed.
 tionality to extract semantic tags out of web documents. Diffbot was founded at Stanford University and is the fi rst company funded by StartX 22 , a non-pro fi t business incubator related to Stanford Uni-versity. Diffbot provides an API for access to its functionality and native integration with Semantria 23 in order to perform semantic text analysis on web documents.
 On the commercial fi eld, another signi fi cant approach has been Sensebot. 24 Sensebot also provides an API in order to perform semantic analysis on web pages and to extract semantic concepts/ terms from them.

In the following subsection, we analyze the setup of the experiments performed in order to compare LSHrank against the three approaches discussed above. 6.1. Experiment setup We have chosen six different domains for which LSHrank , TF-IDF, Diffbot and Sensebot produced clusters of the most prominent words. The six domains are Software Engineering, Sports, Linked Data, Law and courts, Java programming and Stem cells. Table 17 displays the 8 terms used as initial queries for these domains.
We compare the word clusters produced by all the four semantic approaches for each domain against their Normalized Mutual Information (NMI) ( Manning et al., 2008 ) values (JavaMI is employed for the implementation of NMI). We have chosen to compare each word cluster produced against a ground truth cluster for each domain. The ground truth for the Software Engineering domain is a glossary from the Universite de Mons for Law and court, a glossary by the of fi cial website of US courts for Linked Data, W3C was selected 28 ; for Java programming, a glossary provided by Oracle 29 was used; for Stem cells, a glossary by the National Institutes of Health, U.S Department of Health &amp; Human Services 30 . As far as the Sports domain is concerned, and since the sports-related queries cover multiple sports, the Wiki-pedia glossary for each sport is used(basketball, 31 football, american football 33 and ice hockey. 34
Table 18 depicts the settings chosen for LSHrank (parameters not necessary to be set were omitted), while Sensebot's amount of semantic concepts/terms recognized per document was set to 5 (in accordance with the LSHrank parameter  X  ). 6.2. Results on NMI scores In Table 19 we display the NMI scores of LSHrank , TF-IDF, Sensebot and Diffbot for the various domains. We provide three NMI scores for each domain; simple ( NMI _ SI ), after the removal of stopwords from the clusters ( NMI _ SR ) and after the removal of stopwords, stemming and exclusion of duplicates produced out of stemming ( NMI _ ST ). Obviously, the NMI _ ST score is the most accurate indicator for the comparison of clusters. We denote in bold the best NMI _ ST score for each domain.

LSHrank presents the highest NMI _ ST scores in all domains but two, in which it performs very close to the highest score. Table 19 is a clear indication of LSHrank 's robustness against other semantic analysis SEO approaches. 7. Conclusion and future work
Based on our basic argument that contemporary SEs strive to capture user experience and have started employing metrics that take semantic data into account, we have built LSHrank ,a mechanism that attempts to deliver high quality SEO based on LDA techniques and state-of-the-art SE metrics.

This paper attempts to capture the use of semantic analysis by search engines. We have explored the use of LDA for the produc-tion of optimal content. The convergence rate of the algorithm and the content that is produced based on LDA changes, as the number of topics recognized by LDA changes. The choice of domain also in fl uences the content produced.

The choice of metric to evaluate the results of the search engines
Moz has presented the highest convergence rate values. At a more general comparison context, large-scale word groups produced more content in terms of quantity and uniqueness than the small scale-ones.
Last but not least, LSHrank outperforms three signi fi cant approaches on the fi eld of semantic content extraction and proves through its high NMI scores that it could provide a reliable mean of recognition of semantically prominent content on web documents.
As a conclusion, we argue that we have gained an insight on the internal characteristics of the ranking mechanisms of search engines regarding semantic analysis of content and on the level that have implemented their already issued patents. The analysis of the experiments run has also provided us with the opportunity to analyze LDA's relation with search engines ranking schemas.
It has to be outlined that textual unstructured data, such as .pdf and .doc fi les, could provide with an insight on the semantics of
SERPs and the analysis of their content could be included in a future version of LSHrank . In the current version, their HTML tags are the elements included in the content gathered. Regarding non-textual structured data like images or videos we follow the same logic and extract all the available contents available in HTML tags such as alt text or description tags. The analysis of the content within the actual data was not included in our approach and could be a bene fi cial extension of LSHrank .

The next step is to analyze the correlation of LDA with the ranking of search engines with metrics such as Spearman correla-tion. Through the analysis regarding LDA's settings the focus could be solely on the relation of various domains, the convergence rates and NMI scores LSHrank presents on the content production.
Therefore, a plethora of experiments could be conducted in order to further ascertain the effect of LDA on the recognition of semantically related and optimal content for search engines. Appendix See Fig. 17.
 References
