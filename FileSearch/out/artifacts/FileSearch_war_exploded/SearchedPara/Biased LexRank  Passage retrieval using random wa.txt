 1. Introduction Text summarization is one of the hardest problems in information retrieval, mainly because it is not very well-defined.
There are various definitions of text summarization resulting from different approaches to solving the problem. Furthermore, there is often no agreement as to what a good summary is even when we are dealing with a particular definition of the prob-lem. In this paper, we focus on the query-based or focused summarization problem where we seek to generate a summary of a set of related documents given a specific aspect of their common topic formulated as a natural language query. This is in contrast to generic summarization, where a set of related documents is summarized without a query, with the aim of cov-ering as much salient information in the original documents as possible.

The motivation behind focused summarization is that readers often prefer to see specific information about a topic in a summary rather than a generic summary (e.g. Tombros &amp; Sanderson, 1998 ). An example summarization problem from the Document Understanding Conferences (DUC) 2006 1 is as follows:
Example 1  X  Topic: International adoption.  X  Focus: What are the laws, problems, and issues surrounding international adoption by American families?
Given a set of documents about a topic (e.g.  X  X  X nternational adoption X ), the systems are required to produce a summary mation retrieval. Passage retrieval also arises in question answering as a preliminary step: given a question that typically requires a short answer of one or a few words, most question answering systems first try to retrieve passages (sentences) that are relevant to the question and thus potentially contain the answer. This is quite similar to summarization with the key difference being that the summarization queries typically look for longer answers that are several sentences long.
In the current work, we propose a unified method for passage retrieval with applications to multi-document text sum-marization and passage retrieval for question answering. Our method is a query-based extension of the LexRank summari-zation method introduced in (Erkan &amp; Radev, 2004 ). LexRank is a random walk-based method that was proposed for generic summarization. Our contribution in this paper is to derive a graph-based sentence ranking method by incorporating the query information into the original LexRank algorithm, which is query-independent. The result is a very robust method that can generate passages from a set of documents given a query of interest.

An important advantage of the method is that it has only a single parameter to tune that effectively determines how much the resultant passage should be generic (query-independent) or query-based. Therefore, in comparison to supervised ture of the language in which the documents are written and does not require the use of any particular linguistic resources methods for sentence selection that primarily consider the similarity of the candidate sentences to the query (e.g. Allan, ploits the information gleaned from intra-sentence similarities as well. We previously presented this method in Otterb-acher, Erkan, and Radev (2005) . Here, we extend our experiments to include the summarization problem, and show that our approach is very general with promising results for more than one information retrieval problem. 2. Our approach: topic-sensitive LexRank
We formulate the summarization problem as sentence extraction, that is, the output of our system is simply a set of sen-tences retrieved from the documents to be summarized. To determine the sentences that are most relevant to the user X  X  query, we use a probabilistic model to rank them. After briefly describing the original version of the LexRank method, pre- X  X  X iased X ) version, which will be evaluated in two sentence retrieval experiments. More specifically, in Section 4 we apply
Biased LexRank to the problem of topic-focused summarization and in Section 5 we evaluate it in the context of passage re-trieval for question answering. 2.1. The LexRank method
In Erkanand Radev (2004) , the concept of graph-based centrality was used to rank a set of sentences for producing generic multi-document summaries. To compute LexRank, the documents are first segmented into sentences, and then a similarity graph is constructed where each node in the graph represents a sentence. The edge relation between the nodes is induced by a similarity metric of choice, as will be explained in the details of our experiments. In a generalized form the LexRank equa-tion can be written as: of the link from sentence v to sentence u . Therefore, the LexRank value of a node (sentence) is a constant term plus the (weighted) average of the LexRank values of its neighboring nodes.

An interesting interpretation of the LexRank value of a sentence can be understood in terms of the concept of a random distribution. Suppose we have a sentence similarity graph as described above. We define a random walk on this graph in such a way that it starts at a random sentence and then at each step, with probability d it jumps to a random sentence with uniform probability, with probability 1 d it visits a sentence that is adjacent to the current sentence with a probability in proportion to the outgoing edge (similarity) weights of the current sentence. The LexRank value of a sentence gives us the limiting probability that such a random walk will visit that sentence in the long run . Equivalently, the LexRank value is the fraction of the time such a random walk spends on the particular sentence. The LexRank Eq. (1) as described above is defined in a recursive manner, and can be computed via an iterative routine called the power method . method that is almost equivalent to LexRank with cosine links was independently proposed in ( Mihalcea &amp; Tarau, 2004 ).
The motivating assumption behind the LexRank method is that the information that is repeated many times in a cluster of sentences is the salient information that needs to be represented in a summary. This correlation between the repeated infor-mation and the salient information is the starting intuition that most summarization systems try to exploit. LexRank makes other sentences; therefore, it is a good candidate to be included in an extractive summary. Note that such a sentence will be strongly connected to a lot of other sentences in the similarity graph. The random walk we described above is more likely to visit a sentence that is better connected to the rest of the graph with strong links since the direction of the random walk is determined by the similarity-weighted edges in the graph. Thus the LexRank value of such a sentence will be higher. Fur-thermore, LexRank takes into account not only the similarity values of a sentence to its neighbors but also the individual importance of the neighbors of that sentence. This is achieved by the recursive formulation of LexRank and the propagation of the importance from node to node by the random walk. 2.2. Biased LexRank
In deriving a topic-sensitive or biased version of LexRank, we begin with the generalized form of the LexRank equation as
It can be noted that there is nothing in Eq. (1) that favors certain sentences based on a topic focus: LexRank is completely the matrix ergodic so that a solution to the equation exists. It does not have a big impact on the final ranking of the nodes since it favors all the nodes equally during the random walk. With probability d , the random walk jumps to any node with uniform probability. This suggests an alternative view of the random walk process. We can combine more than one random walk into a random walk process. Indeed, we could use a non-uniform distribution in combination with the random walk based on the weight function w  X  ;  X  .

Suppose, we have a prior belief about the ranking of the nodes in the graph. This belief might be derived from a baseline ranking method which we trust to a certain extent. For example, in the focused summarization task, we can rank the sen-od. We can then bias the random walk based on b  X  X  while computing LexRank as follows: tences during the random walk based on a prior distribution. When d  X  1, LR  X  X  ranks the nodes exactly the same as b  X  X  . When d &lt; 1, we have a mixture of the baseline scores and the LexRank scores derived from the unbiased structure of the graph. In other words, Biased LexRank ranks the sentences by looking at the baseline method and the inter-sentence similarities at the same time. Fig. 1 shows an illustration.
 3. A question answering example A problem closely related to focused summarization is question answering (QA). Essentially, the only difference between
QA and focused summarization is that QA addresses questions that require very specific and short answers that are usually only a few words long, whereas summarization involves questions that can be answered by composing a short document of a question (Gaizauskas, Hepple, &amp; Greenwood, 2004 ). Consider the following example in which answers to the given question are sought from a set of topically related documents.

Example 2 (1) The plane was destined for Italy X  X  capital Rome. (2) The plane was in route from Locarno in Switzerland, to its destination , Rome, Italy. (3) The pilot was on a 20-minutes flight from Locarno, Switzerland to Milan. (4) The aircraft had taken off from Locarno, Switzerland, and was heading to Milan X  X  Linate airport.

These four sentences were taken from various news articles about the same event  X  the crash of a small plane into a sky-scraper in Milan, Italy. As can be seen, there is some contradictory information among them since they come from different sources published at different points in time. However, in a question answering scenario all of them need to be retrieved. In the absence of any useful external information, a popular sentence scoring technique in QA is to look at the words in the question. If a sentence has some words in common with the question, it is considered to be a relevant sentence that may contain the answer to the question. Given a sentence u and a question q , an example sentence scoring formula, as used in
Allan et al. (2003) , is as follows: where tf w ; u and tf w ; q are the number of times w appears in u and q , respectively, and idf are somewhat related to the question since they include words from the question (shown in boldface). two sentences are also clearly related to the question and actually contain the answer. An important observation is that sen-tences 3 and 4 have some words in common with sentences 1 and 2. Knowing that sentences 1 and 2 are relevant or impor-tant, it is easy to infer that sentences 3 and 4 should also be relevant only by looking at the inter-sentence similarities between them. Hence, a suitable choice for the Biased LexRank formula is:
The random walk interpretation of this formula is as follows: With probability d , the random walk visits a sentence with a probability proportional to its relevance to the question, with probability  X  1 d  X  the random walk chooses a sentence that is a neighbor of the current sentence with a probability proportional to the link weight in the graph. As can be seen, the random walk is biased towards the neighborhoods of the highly relevant sentences in the graph. 4. Application to focused summarization
The Document Understanding Conferences summarization evaluations in 2005 and 2006 included a focused summariza-tion task. Given a topic and a set of 25 relevant documents, the participants were required  X  X  X o synthesize a fluent, well-or-ganized 250-word summary of the documents that answers the question(s) in the topic statement. X  An example topic statement and related questions are shown in Example 1 . In this section, we explain how we formulated the summarization tasks of DUC 2005 and 2006 based on the Biased LexRank technique detailed in Section 2.2. 4.1. Using generation probabilities as link weights
In approaching the task of focused summarization, we use language model-based similarity measures between the sen-tences as proposed by Kurlandand Lee (2005) . Here, we first recall the language modeling approach to information retrieval and adapt it to the summarization domain. Given a sentence v , we can compute a (unigram) language model from it. A straightforward way of computing this language model is the maximum likelihood estimation (MLE) of the probabilities of the words to occur in v : model since the words that do not occur in the text from which we compute the word frequencies get zero probability. This is an even bigger problem when we compute language models from a relatively shorter input text such as a sentence com-posed of only a few words. To account for the unseen words, we smooth the language model computed from a sentence using the language model computed from the entire cluster: where C is the entire document cluster. Eq. (6) is a special instance of the more general Jelinek-Mercer smoothing method and the MLE computed from the entire cluster. p JM  X  w j v  X  is nonzero for all words that occur in the document cluster to be summarized provided that k &gt; 0. We can also consider the generation probability of a sentence given the language model computed from another sentence.
For example, probabilities. Therefore, we normalize the generation probability of each sentence by the sentence length to make different pairwise similarities comparable to each other:
We use p norm  X  u j v  X  as the weight of the link from utov in the graph-based representation of the cluster. Note that p proportional to the (normalized) generation probability of u given the language model computed from v . The reason we are not using the value for the opposite direction  X  p norm evidence that the language model of that sentence can generate other sentences more successfully. Revisiting the random walk model of LexRank, the LexRank value of a sentence is a measure of its accumulated generation power , that is, how likely it is to generate the rest of the cluster from the language model of the specific sentence in the long run. We advocate that a sentence with a high generation power is a good candidate for the summary of its cluster.
 Extending the use of generation probabilities to Biased LexRank for the focused summarization task is straightforward.
For the baseline ranking method, we use the generation probability of the topic description from the sentences. A sentence is ranked higher if its language model can generate the topic description with a larger probability. This is analogous to the language modeling approach in information retrieval (Ponte &amp; Croft, 1998 ) where the documents are ranked with respect to the generation probability of the given query from each document X  X  language model. In our summarization method, given a topic description t , the final score for a sentence u is computed by the following Biased LexRank equation: 4.2. DUC 2005 and 2006 experiments
In the Document Understanding Conferences (DUC) 2005 and 2006 summarization evaluations, the task was to summa-rize a set of documents based on a particular aspect of their common topic. This particular aspect was provided as a  X  X  X opic description X  (see Example 1 ). Other than this change, the setting was similar to DUC 2003 and 2004 evaluations: there were 50 topical clusters to be summarized for each year. Each cluster had 25 English news documents that concerned the same topic.

There are two parameters in our framework summarized by Eq. (9). d is the biased jump probability in Biased LexRank,
Jelinek-Mercer smoothing parameter (Eq. (6)) that is used when computing the language model of each sentence. For both parameters, we experimented with several values in the [0.1,0.9] interval. Here, we report the results for one of the best parameter settings we obtained for the DUC 2005 dataset.
 It should be noted that we did not carry out an extensive parameter tuning. Rather, our goal was to show that the Biased
LexRank method is effective even when little or no parameter turning is possible. To further support this claim, we did not perform any parameter tuning for the DUC 2006 dataset at all, directly using the same parameter values from the DUC 2005 experiments. Overall, d 0 : 7 and k 0 : 6 performed well. Note that 0.7 is a relatively large value for d considering that we set it to 0.15 in the generic summarization experiments in Erkanand Radev (2004) . However, using large values for d makes perfect sense for focused summarization since we would certainly want to give more weight to a sentence X  X  similarity to the topic description than its similarity to other sentences. For small values of d , the summaries would be more generic rather than based on the topic description.

In the similarity graphs, we connected each node (sentence) to k other nodes that are most similar to it rather than con-necting to all the other nodes in the graph. We observed that this improves not only the running time of the algorithm but also the quality of the resultant summaries. One reason for this may be that small similarity values actually indicate  X  X  X is-similarity X  among sentences. Accumulating scores from dissimilar neighbors of a node is not intuitive no matter how small each node in the similarity graphs has exactly 20 outgoing links. Note that the number of incoming links for each node may vary depending on how similar a node is to the rest of the nodes in the graph. Indeed, a good summary sentence would typ-ically have more incoming links than outgoing links, which is an indication that its language model can better generate the rest of the sentences in the graph.
 In selecting the sentences for inclusion in the focused summaries, we did not use any features other than the Biased LexRank values in our experiments. To construct the final summaries, we ranked the sentences based on their Biased
LexRank scores. The ranked sentences were added to a summary one by one until the summary exceeded 250 words which was the limit in the DUC evaluations. When considering the ranked sentences for inclusion into the summary, we ignored any sentence which has a cosine similarity of larger than 0.5 to any of the sentences that were ranked above it. This simple re-ranking scheme ensures that the resulting summaries cover as much information as possible within the length limit. For the evaluation of our summaries, we used the official ROUGE metrics of DUC 2005 and 2006, i.e. ROUGE-2 and
ROUGE-SU4 ( Lin &amp; Hovy, 2003 ). ROUGE-2 compares the bigram overlap between the system summary and the manual sum-maries created by humans. ROUGE-SU4 does the same except that it introduces the relaxation of allowing as many as four words between the two words of a bigram.

Tables 1 and 2 show the results for DUC 2005 and 2006, respectively. In both datasets, it can be seen that the performance of Biased LexRank is comparable to that of the human summarizers. In some cases, its performance is not significantly dif-ferent from certain human summarizers considering the 95% confidence intervals. For comparison, we also include the top ranked system X  X  score each year. It can be seen that LexRank achieves almost the same scores or better. 5. Application to passage retrieval for question answering
In this section, we show how Biased LexRank can be applied effectively to the problem of passage retrieval for question is, we aim to extract sentences from a set of documents in response to a question.

We demonstrate that Biased LexRank significantly improves question-focused sentence selection over a baseline that only looks at the overlap between the sentences and the query. 5.1. Description of the problem
Our goal is to build a question-focused sentence retrieval mechanism using the Biased LexRank method. In contrast to uments for relevancy and then proceeds to find paragraphs related to a question, we address the finer-grained problem of finding sentences containing answers. In addition, the input is a set of documents relevant to the topic of the query that the user has already identified (e.g. via a search engine). Our method does not rank the input documents, nor is it restricted in terms of the number of sentences that may be selected from the same document.

The output produced by Biased LexRank, a ranked list of sentences relevant to the user X  X  question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the
Fan, 2004 ). However, in our approach, answers are extracted from a set of multiple documents rather than on a docu-ment-by-document basis. 5.2. Relevance to the question
We first stem all of the sentences in a set of articles and compute word IDFs by the following formula: where N is the total number of sentences in the cluster, and sf
We also stem the question and remove the stop words from it. Then the relevance of a sentence s to the question q is computed by query-based sentence retrieval (Allan et al., 2003 ), and is used as our competitive baseline in this study (e.g. Tables 6, 7 and 9 ). 5.3. The mixture model
The baseline system explained above does not make use of any inter-sentence information in a cluster. We hypothesize that a sentence that is similar to the high scoring sentences in the cluster should also have a high score. For instance, if a sentence that gets a high score in our baseline model is likely to contain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.
 mined as the sum of its relevance to the question (using the same measure as the baseline described above) and the simi-larity to the other sentences in the document cluster: off between two terms in the equation and is determined empirically. For higher values of d , we give more importance to the relevance to the question compared to the similarity to the other sentences in the cluster. The denominators in both terms are for normalization, which are described below. We use the cosine measure weighted by word IDFs as the similarity be-tween two sentences in a cluster:
Eq. (12) can be written in matrix notation as follows: 1. Note that as a result of this normalization, all rows of the resulting square matrix Q  X  X  d A  X  X  1 d  X  B also add up to 1. Such a matrix is called stochastic and defines a Markov chain. If we view each sentence as a state in a Markov chain, then ing for in Eq. (14) is the stationary distribution of the Markov chain.

An illustration of this representation is shown in Fig. 2 . The five input sentences are represented in the graph as nodes with the cosine similarity between each pair of sentences shown on the respective edge. In the example, node 1 is isolated since sentence 1 did not have a cosine similarity of greater than the threshold of 0.15 with any other sentence.
With probability d , a transition is made from the current node (sentence) to the nodes that are similar to the query. With probability (1 d ), a transition is made to the nodes that are lexically similar to the current node. Every transition is weighted according to the similarity distributions. Each element of the vector p gives the asymptotic probability of ending up at the corresponding state in the long run regardless of the starting state.

A simpler version of Eq. (14), where A is a uniform matrix and B is a normalized binary matrix, is known as PageRank (Brin &amp; Page, 1998, 1998 ) and used to rank the web pages by the Google search engine. It was also the model used to rank sen-tences for generic summarization in Erkan and Radev (2004) .

We experimented with different values of d on our training data. We also considered several threshold values for inter-sentence cosine similarities, where we ignored the similarities between the sentences that are below the threshold. In the training phase of the experiment, we evaluated all combinations of LexRank with d in the range of [0,1] (in increments of 0.10) and with a similarity threshold ranging from [0,0.9] (in increments of 0.05). We then found all configurations that out-performed the baseline. These configurations were then applied to our development/test set. Finally, our best sentence re-trieval system was applied to our test data set and evaluated against the baseline. The remainder of this section of the paper will explain this process and the results in detail. 5.4. Corpus
A key challenge for passage retrieval for QA is that, when attempting to retrieve answers to questions from a set of doc-uments published by multiple sources over time (e.g. in a Web-based environment), answers are typically lexically diverse and may change over time or even contradict one another. Therefore, in order to evaluate Biased LexRank on such challeng-ing questions, we built a corpus of 20 multi-document, multi-source clusters of complex news stories. The topics covered included plane crashes, political controversies and natural disasters. The data clusters and their characteristics are shown in Table 3 . The news articles were collected from the Web sites of various news agencies or from an automated news summarization system. In particular,  X  X  X ewstracker X  clusters were collected automatically by a Web-based news article collection and summarization system. The number of clusters randomly assigned to the training, development/test and test data sets were 11, 3 and 6, respectively.

Next, we assigned each cluster of articles to an annotator, who was asked to read all articles in the cluster. He or she then generated a list of factual questions key to understanding the story. Once we collected the questions for each cluster, two judges independently annotated nine of the training clusters. For each sentence and question pair in a given cluster, the judges were asked to indicate whether or not the sentence contained a complete answer to the question. Once an acceptable were annotated by one judge each.

In some cases, the judges did not find any sentences containing the answer for a given question. Such questions were re-moved from the corpus. The final number of questions annotated for answers over the entire corpus was 341, and the dis-tributions of questions per cluster can be found in Table 3 . 5.5. Evaluation metrics and methods
To evaluate our sentence retrieval mechanism, we produced extract files, which contain a list of sentences deemed to be relevant to the question, for the system and from human judgment. To compare different configurations of our system to the baseline system, we produced extracts at a fixed length of 20 sentences. While evaluations of question answering systems developing a passage retrieval system, of which the output can then serve as the input to an answer extraction system for previously mentioned, in our corpus the questions often have more than one relevant answer, so ideally, our passage retrie-val system would find many of the relevant sentences, sending them on to the answer component to decide which answer(s) should be returned to the user. Each system X  X  extract file lists the document and sentence numbers of the top 20 sentences.
The  X  X  X old standard X  extracts list the sentences judged as containing answers to a given question by the annotators (and therefore have variable sizes) in no particular order. 5
We evaluated the performance of the systems using two metrics  X  Mean Reciprocal Rank (MRR) ( Voorhees &amp; Tice, 2000 ) and Total Reciprocal Document Rank (TRDR) (Radev, Fan, Qi, Wu, &amp; Grewal, 2005 ). MRR, used in the TREC Q and A evalua-idea of how far down we must look in the ranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of all answers found by the system. In the context of answering questions from complex stories, where there is often more than one correct answer to a question, and where answers are typically time-dependent, we should focus on maximizing TRDR, which gives us a measure of how many of the relevant sentences were identified by the system. However, we report both the average MRR and TRDR over all questions in a given data set. 5.6. LexRank versus the baseline approach
In the training phase, we searched the parameter space for the values of d (the question bias) and the similarity threshold in order to optimize the resulting TRDR scores. For our problem, we expected that a relatively low similarity threshold pair with a high question bias would achieve the best results. Table 4 shows the effect of varying the similarity threshold. baseline system on the training data, based on mean TRDR scores over the 184 training questions. We applied all four of these configurations to our unseen development/test data, in order to see if we could further differentiate their performances. 5.6.1. Development/testing phase Having established the optimal ranges of the question bias and similarity threshold parameters on the training data, Biased LexRank was then evaluated on the unseen development/test data. As shown in Table 7 , all four LexRank systems out-performed the baseline, both in terms of average MRR and TRDR. A more detailed, cluster-by-cluster analysis of the perfor-mance of the best Biased LexRank configuration, LR[0.20,0.95], over the 72 questions for the three development/test data clusters is shown in Table 8 . While LexRank outperforms the baseline system on the first two clusters both in terms of
MRR and TRDR, their performances are not substantially different on the third cluster. Therefore, we examined properties of the questions within each cluster in order to see what effect they might have on system performance.
We hypothesized that the baseline system, which compares the similarity of each sentence to the question using IDF-weighted word overlap, should perform well on questions that provide many content words. To contrast, LexRank might per-form better when the question provides fewer content words, since it considers both similarity to the query and inter-sen-tence similarity. Out of the 72 questions in the development/test set, the baseline system outperformed LexRank on 22 of the questions. In fact, the average number of content words among these 22 questions was slightly, but not significantly, higher than the average on the remaining questions (3.63 words per question versus 3.46). Given this observation, we experimented with two mixed strategies, in which the number of content words in a question determined whether LexRank or the baseline system was used for sentence retrieval. We tried threshold values of 4 and 6 content words, however, this did not improve the performance over the pure strategy of system LR[0.20,0.95]. Therefore, we applied this system versus the baseline to our unseen test set of 134 questions. 5.6.2. Testing phase As shown in Table 9 , LR[0.20,0.95] outperformed the baseline system on the test data both in terms of average MRR and
TRDR scores. The improvement in average TRDR score was statistically significant with a p -value of 0.0619. Since we are interested in a passage retrieval mechanism that finds sentences relevant to a given question, providing input to the question answering component of our system, the improvement in average TRDR score is very promising. While we saw in Section 5.6.1 that LR[0.20,0.95] may perform better on some question or cluster types than others, we conclude that it beats the competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions. However, in future work, we will continue to improve the performance, perhaps by developing mixed strategies using different configurations of LexRank.
 5.7. Discussion
The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candi-date sentences and the input query, and not the similarity between the candidate sentences themselves, is likely to miss some important sentences. When using any metric to compare sentences and a query, there is always likely to be a tie be-tween multiple sentences (or, similarly, there may be cases where fewer than the number of desired sentences have simi-in Tables 10 and 11 , which show the top ranked sentences according to the baseline and LexRank respectively, for the ques-tion  X  X  X hat caused the Kursk to sink? X  from the Kursk submarine cluster. It can be seen that all top five sentences chosen by the baseline system have the same sentence score (similarity to the query), yet the top ranking two sentences are not actu-able to differentiate between them. As can be seen, LexRank has ordered the three relevant sentences first, followed by the two sentences that are not relevant. It should be noted that both for the LexRank and baseline systems, chronological order-ing of the documents and sentences is preserved, such that in cases where two sentences have the same score, the one pub-lished earlier is ranked higher. 6. Conclusion
We have presented a generic method for passage retrieval that is based on random walks on graphs. Unlike most ranking methods on graphs, LexRank can be tuned to be biased, such that the ranking of the nodes (sentences) in the graph is depen-dent on a given query. The method, Biased LexRank, has only one parameter to be trained, namely, the topic or query bias.
In the current paper, we have also demonstrated the effectiveness of our method as applied to two classical IR problems, extractive text summarization and passage retrieval for question answering. In the context of the Document Understanding
Conference 2005 and 2006 summarization tasks, we have shown that LexRank performed well in producing topic-focused summaries. Despite performing very limited parameter tuning for the evaluation, the LexRank method produced summaries comparable to those generated by the human summarizers. Biased LexRank was also shown to be effective in retrieving rel-evant passages from a set of related news articles, given a user X  X  unaltered natural language question. More specifically, by using inter-sentence similarity in addition to the similarity between the candidate sentences and the input question, Biased LexRank finds significantly more passages containing the desired answer and it also ranks them more accurately. Acknowledgements This paper is based upon work supported by the National Science Foundation under Grant No. 0534323,  X  X  X logoCenter:
Infrastructure for Collecting, Mining and Accessing Blogs X  and Grant No. 0329043,  X  X  X robabilistic and Link-based Methods for Exploiting Very Large Textual Repositories X .

Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation.

We would also like to thank the current and former members of the CLAIR group at Michigan and in particular Siwei Shen and Yang Ye for their assistance with this project.
 References
