 Heavy-tailed distributions naturally occur in many real life phenomena, for example in computer consumed network bandwidth. Equivalently, a small set of users generate a large fraction of the network traffic. Another common property of communication networks is that network traffic tends composed from the sum of distinct incoming flows.
 existing solutions in the area of network monitoring involve various approximations of the joint decomposition [13] historgrams [14], sketches [16], entropy [14], sampled moments [23], etc. distributions, we show how to compute both exact and approximate inference (Section 4). Using real data from the domain of computer networks we demonstrate the applicability of our proposed methods for computing inference in LCM (Section 5).
 We summarize our contributions below: 1.1 Related work work: Convolutional Factor Graphs (CFG), Copulas and Independent Component Analysis (ICA). Below we shortly review them and motivate why a new graphical model is needed. Convolutional Factor Graphs (CFG) [18, 19] are a graphical model for representing linear relation a product of factors in the cf domain. Unlike CFGs, LCMs are always defined, for any probability distribution, while CFG may are not defined when the inverse Fourier transform does not exist. A closely related technique is the Copula method [22, 17]. Similar to our work, Copulas assume a in the cdf domain.
 A third related technique is ICA (independent component analysis) on linear models [27]. Assum-dent. Both techniques (LCM and ICA) are complementary, since ICA can be used to learn the linear model, while LCM is used for computing inference in the learned model. lem domains, including economics, physics, geology and astronomy [24]. Stable distribution are soon show with our networking example, network flows exhibit empirical distribution which can be modeled remarkably well by stable distributions.
 stable with S ( 1 a unit scale, zero-centered stable random variable.
  X  1  X   X   X  1 , a,b 2 R , a 6 = 0 and Z is a random variable with characteristic function 2 Next we define a general stable random variable.
 Definition 2.2. [25, Def. 1.7] A random variable X is S (  X , X , X , X  ) if where Z is given by (1) . X has characteristic function formulates this linearity.
 Proposition 2.1. [25, Prop. 1.16] We propose a novel approach for modeling linear interactions between random variables distributed according to stable distributions, using a new linear probabilistic graphical model called LCM. A new graphical model is needed, since previous approaches like CFG or the Copula method can not be Definition 3.1. (Linear model) Let X Let Y where A Linear models are useful in many domains. For example, in linear channel decoding, X are the vations. When X are distributed using a Gaussian distribution, the channel model is called AWGN pendent when transmitting, given an observation Y , X are not independent any more, since they application to our current work is linear channel decoding with stable, non-Gaussian, noise. the pdf domain, we must work in the cf domain. Hence, we define a dual linear model in the cf domain. 3.1 Duality of LCM and CFG convolution: define LCM formally, and establish the duality to the factorization given in (2). Definition 3.2. (LCM) Given the linear model Y=AX, we define the linear characteristic model (LCM) where  X  ( t The following two theorems establish duality between the LCM and its dual representation in the derivation of LCM from the linear model.
 Theorem 3.3. Given a LCM, assuming p ( x,y ) as defined in (2) has a closed form and the Fourier transform F [ p ( x,y )] exists, then the F [ p ( x,y )] =  X  ( t Theorem 3.4. Given a LCM, when the inverse Fourier transform exists, then F  X  1 (  X  ( t 1 ,  X  X  X  X  ,t n ,s 1 ,  X  X  X  X  ,s m )) = p ( x,y ) as defined in (2).
 The proof of all theorem is deferred to the supplementary material. Whenever the inverse Fourier transform exists, LCM model has a dual CFG model. In contrast to the CFG model, LCM are always compute inference in either representations, whenever it is more convenient. (both exact and approximate) in linear model with underlying stable distributions. 4.1 Exact inference in LCM Marginal distribution of the node x where X \ i is the set of all nodes excluding node i . Unfortunately, when working with stable slicing, computed in the cf domain.
 Definition 4.1. (slicing/evaluation)[28, p. 110] (a) Joint cf . Given random variables X (b) Marginal cf . The marginal cf is derived from the joint cf by  X  This operation is called slicing or evaluation. We denote the slicing operation as  X  domain, by using the slicing operation. Theorem 4.2. Given a LCM, the marginal cf of the random variable X is given by p ( x Based on the results of Thm. 4.2 we propose an exact inference algorithm, LCM-Elimination, for computing the marginal cf (shown in Algorithm 1). We use the notation N ( k ) as the set of graph neighbors of node k , excluding k 5 . T is the set { t LCM-Elimination is dual to CFG-Elimination algorithm [19]. LCM-Elimination operates in the cf domain, by evaluating one variable at a time, and updating the remaining graphical model accord-the marginal cf  X  ( t the desired marginal probability p ( x 4.2 Exact inference in stable distributions After defining LCM and showing that inference can be computed in the cf domain, we are finally We assume that all observation nodes Y the linearity property of stable distribution, it is clear that the hidden variables X of this work, since as far as we know, no closed-form solution was previously derived. Theorem 4.3. Given a LCM, Y = AX + Z , with n i.i.d. hidden variables X n i.i.d. noise variables with known parameters Z assuming the matrix A a) the observations Y the following parameters: b) the result of exact inference for computing the marginals p ( x given in vector notation: (a) (b) log( A ) ,A  X  , sign( A ) are entrywise matrix operations and  X  x , [  X  x  X  4.3 Approximate Inference in LCM inference motivates us to devise a more efficient approximations.
 We propose two novel algorithms that are variants of belief propagation for computing approximate inference in LCM. The first, Characteristic-Slice-Product (CSP) is defined in LCM (shown in Algo-establishes this fact.
 Theorem 4.4. Given an LCM with underlying tree topology (the matrix A is an irreducible adja-marginal cf and the marginal distribution respectively.
 The basic property which allows us to devise the CSP algorithm is that LCM is defined as a prod-domain. Note that the original CFG work [18, 19] did not consider approximate inference. Hence our proposed approximate inference algorithm further extends the CFG model. 4.4 Approximate inference for stable distributions given in Algorithm 2(a). Like belief propagation, our approximate algorithm Stable-Jacobi is not guaranteed to converge on general graphs containing cycles. We have analyzed the evolution dynamics of the update equations Jacobi as well as closed-form equations for the fixed point.
 Theorem 4.5. Given a LCM with n i.i.d hidden variables X cording to stable distribution Y hold): are found in the supplementary material. Network monitoring is an important problem in monitoring and anomaly detection of communication networks [15, 16, 8]. We obtained Netflow PlanetLab net-testbed with around 1000 server nodes scattered in about 500 sites around the world. We define a network flow as a directed edge between a transmitting and receiving hosts. The number of packets transmitted in this flow is the scalar edge weight.
 of flows, sorted by their bandwidth, on a typical PlanetLab node. Empirically, we found out that For performing the fitting, we use Mark Veillette X  X  Matlab stable distribution package [31]. In contrast, by approximating network flow distribution with stable distributions, we need only 4 clusters of nodes.
 as the observed flows Y the unobserved remaining 376 flows X LCM-Elimination. We emphasize again, that using related techniques (Copula method , CFG, and terial, we provide a detailed comparison of two previous approximation algorithms: non-parametric BP (NBP) and expectation propagation (EP).
 Figure 2(c) plots convergence of the three parameters  X , X , X  as a function of iteration number of Regarding computation overhead, LCM-Exact algorithm requires 4  X  376 3 operations, while Stable-Source code of some of the algorithms presented here can be found on [3]. We have presented a novel linear graphical model called LCM, defined in the cf domain. We have shown for the first time how to perform exact and approximate inference in a linear multivariate our construction for computing inference of network flows.
 We believe that other problem domains may benefit from this construction, and plan to pursue this as a future work.
 butions like geometric stable distributions or Wishart can be analyzed in our model. The Fourier transform can be replaced with more general kernel transform, creating richer models.
