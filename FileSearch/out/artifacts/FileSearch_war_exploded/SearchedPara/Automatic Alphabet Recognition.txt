 1. Introduction As the Internet has become a part of contemporary life, people all over the world access the web and look for information on a daily basis. The importance of national language support is growing together with the increasing popularity and spread of the network.

Most languages are alphabetical, so they have a constant set of letters. Thus, all the words are composed from them. However, in computer documents, the same letter may be encoded in different ways. Some of these different methods are part of the language usage, such as the distinction between upper case and lower case letters in English. Other distinctions are formatting tools used by document authors, such as the distinction between boldface and italics .  X  X ncoding X  is a mapping of letters within the document to binary codes. In infor-mation technology, the letters are represented by byte-long codes. Unicode uses two bytes codes. Sometimes, different codes are assigned to the same letter of the alphabet according to the encoding type. For example, English is commonly represented by the ASCII code, which defines a 7-bit value for each lower case letter, upper case letter, and punctuation symbol. This has not always been so; early IBM mainframes used EBCDIC (IBM Character Data Representation Architecture, Reference and Registry, 26 GEFFET, WISEMAN AND FEITELSON SC09-2196-00, Dec. 1996), which utilizes a different encoding. The ISO-8859 standard stipulates that ASCII be used, and today practically all English documents use ASCII. When such a document is downloaded, there is no ambiguity with regard to which letters should be displayed.

As ASCII only defines 7-bit codes, the first bit in the byte-long encoding used by com-puters is always 0. The second set of 128 values, in which the first bit is 1, is undefined by ASCII. Thus fonts that translate the ASCII codes to graphical representations are free to use whatever representation they wish for the additional codes. A common usage is to use characters of foreign languages. Note that common punctuation symbols do not have to be included, as they are already defined by ASCII, and this has become a de-facto standard for other languages as well.

The ISO-8859 standard defines the mapping of the higher 128 codes for Latin alphabets and for many other languages, including Cyrillic, Arabic, Greek, and Hebrew alphabets (these use different numbers: 8859-5 is Cyrillic, 8859-6 is Arabic, etc.) (Information Tech-nology 1998). An alternative is the Unicode standard, which uses 16-bit codes to provide unique codes to the symbols needed for all commonly used alphabets in the world. Re-grettably, these standards have not come to dominate usage, as opposed to the dominance of ASCII for English. For example, the standard used by Microsoft software for Hebrew characters (known as Windows-1255) (Northrup 1999) does not conform to ISO-8859. Thus, it is relatively common to download a document in a non-English language and find that it uses an unexpected encoding of letters. The result is that the wrong glyphs are displayed and the document cannot be read (figure 1). Such cases occur quite often while exploring non-English sites on the web. The typical solution is to try different fonts supported by the browser with the hope that one of them uses the same mapping as the document.

Another typical problem for Semitic languages is bi-directional text, where some text is read right-to-left (e.g. words) and some other characters (e.g. numbers) are read left-to-right. There are two ways to represent this type of text: (i) logical, and (ii) visual. In the logically encoded text, the characters are stored in the order they are typed by the human user, using a need for a special conversion algorithm to display logically stored text. The most common algorithm is the Unicode Bi-directional algorithm (The Unicode Consortium 2000). The visual representation stores the text as it should be displayed on the screen device. As this method has many disadvantages (Northrup 1999), the logical representation has become a standard and is supported by the Internet Explorer 5 (Bracewell and Karp 1998) and the Netscape Navigator 6.1 Browsers (Smith 2001). The existing standards for web pages, HTML-4 (Graham 1998) and XHTML 1.0 (Kennedy and Musciano 2000) (latest version, which is a reformulation of HTML 4 in XML 1.0), support Bi-directional text as defined in Unicode X  X  Bi-directional algorithm (Northrup 1999, Jaeger 2002). According to this standard users supply a langu age, a charset (to identify the encoding standard) and a dir (the direction of a page X  X  rtl  X  X r X  ltr  X ) parameters in their HTML pages in order to allow a browser to process a document correctly. Unfortunately, many Hebrew documents are encoded visually in various non-standard ways, and therefore cannot be displayed even by AUTOMATIC ALPHABET RECOGNITION 27 standard-conforming browsers. Thus, even if the user knows the language of the document, it is not enough for correct decoding.

Our goal in this paper is to develop a methodology that can be used by a browser to automatically determine which encoding was used in a document. This will allow the browser to choose the correct font for displaying the document, without requiring a trial-and-error search by the user. We assume that the decoded document, and the language noted that the solutions for the language recognition and the bi-directional text problems 28 GEFFET, WISEMAN AND FEITELSON below. 2. Template-based recognition We aim to find an efficient statistical solution to the text encoding recognition problem, with no knowledge of morphological or syntactic rules. Therefore, this is a general method for any alphabetic language. The basic premise is to pre-compute universal templates of letter distribution in a language based on various corpora examples, and then compare them to the statistics of a given document that needs to be decoded. 2.1. Position vectors The first problem determining which kind of statistics analysis to use. It has been shown in Wiseman (2000) that the distribution of letters in a given human language is generally similar in many texts. There are always exceptions to this rule. For example, the first chapter of the Hebrew Bible doesn X  X  contain the letter  X  X amech X  (S). This was likely a purposeful omission, so that it does not reflect a regular Hebrew text.

Unfortunately, a good recognition cannot be achieved by this information. There are groups of 3 X 4 letters (e.g. A, B, $, and N) which have very close frequencies, so it will be hard or even impossible to distinguish between them (figure 2). We are interested in v ector-space-based model. Our first attempt was to construct a  X  position vector  X  for each letter, by counting its occurrences at every position in the word separately.
 AUTOMATIC ALPHABET RECOGNITION 29
More formally, define count l , i to be the number of times letter l appears in position i in the word. The 20th position in the vector is dedicated for the letter X  X  occurrences at the last position in the word. For example, for the word  X  X icycle X , the last X  X he 20th position ( a denotes the total number of all the letters in the text and words are assumed to be shorter than 20 letters long. Note, that practically all encodings agree on the ASCII standard, so the white spaces, newlines and punctuation marks will be encoded identically in all of them. Based on this information, the document can be parsed into words and the position of each letter in the word can be discovered. It should be noted that the direction of the text is given as an input.

Comparing the vectors of frequencies of the letters, rather than single values, provides much more accurate results (figures 3, 4 and 7). Some letters tend to appear more often in the beginning of a word, and others in the middle or in the end. For instance, articles, prepositions and verb prefixes are single letters which appear in the beginning of words very frequently in Hebrew, while plural form suffixes will be often found in the end. In Hebrew and Arabic there are also several letters that are written differently in different positions. In Hebrew, these are special forms in the final position, while in Arabic there may be special forms for both the first and the final positions. Such forms will therefore never appear in the middle, and their vectors will include all zeros except for the correct position. The non-final form of these letters, on the other hand, will have a zero at the first and/or last place, as for Mi n figure 3.
 30 GEFFET, WISEMAN AND FEITELSON 2.2. Environmental vectors So far, we looked at the text as a 0th order Markov Chain, as we treated each letter indepen-dently. However, it may be helpful to consider the closest neighbors of the letter in order to identify it, and for that purpose to extend our model to a higher order Markov Chain.
The basic approach of using statistical letters distribution in the language was proposed by Shannon (1948). In Benedetto (2002) the authors use statistical information on the dis-tribution of n -grams of letters in different languages in order to construct clusters of similar languages. Damashek and Huffman employed n -grams of letter statistics to classify docu-ments by their topic (Damashek 1995, Huffman 1995). Markov Chains are also commonly used in a human language processing to define compression rules (Bookstein and Klein 1990, Cormack and Horspool 1987) and to compute the probability of the next letter by its precedents (Horspool and Cormack 1986, Yoon et al. 1999). For example, in English  X  X  X  tends to be followed by  X  X  X , while  X  X  X  almost never occurs after  X  X  X  (Wiseman 2000, Ziv and Lempel 1978). Such rules are a very strong feature in English, but less so in Hebrew writing, which puts almost no restrictions on letters combinations, since it does not contain vo wels. Nevertheless, the differences in the probabilities of different pairs are sufficient to aid in recognition.
 First, we collect information about all possible pairs of letter occurrences in the corpus. This data may be viewed as a matrix M of the size: | Alphabet | X | Alphabet | , where every cell M ij contains the frequency, a i , j ,o f the corresponding pair of letters, where a i , j = letters. We notice that rows and columns of M represent the subsequent and preceding vectors of all the letters, respectively. So here again we took a vector-space-model to represent a letter X  X  closest environment (figures 5 and 8). For each letter l in the alphabet, we define its AUTOMATIC ALPHABET RECOGNITION 31 a complexity of the algorithm that constructs the environmental vectors is O ( | Alphabet | s ).
The main problem of both algorithms is ambiguity, when several distinct letters in the document are mapped to a single letter in the template.

Note that we still did not use the information contained in the columns of the matrix, the precedents  X  environmental vectors  X ( PM ). This redundant information is useful to dis-ambiguate the results, thus increasing our model to 2nd order Markov chains. Another wa yt o eliminate ambiguity is to combine the two proposed algorithms, the  X  environmental vectors  X  and the  X  position vectors  X , in the following way: the basic recognition is done by the former technique as it usually outperforms the latter one, followed by the  X  position vectors  X  results to correct errors obtained from the  X  environmental vectors  X  algorithm. The  X  combined  X  method resulted in better accuracy precentages, as shown in Table 1. 2.3. On-line matching Once the off-line construction of templates for various languages is completed, the system can start to work on-line, getting new documents and matching their vectors to the templates.
The matching procedure receives two sets of vectors, V 1 , V 2 generated from two texts, and output is a set of pairs, that are the closest to each other: PS ={ ( v, w ): v  X  V 1 ,w  X  V , f ( v, w ) = min w new text vectors set.

Another question to be discussed in this context is the f function, i.e. the vectors distance metries. We experimented with two versions of the norm formula: f 1 = V 1  X  V 2 1 = 32 GEFFET, WISEMAN AND FEITELSON the accuracy by up to 10% as demonstrated in figure 9, so we chose f 1 as the better metric for our purposes. We also tried to use the well-known Kullback-Leibler divergence metric, bu ti t produced very poor results (less than 10% matches). This can be explained by the fact that the KL-divergence metric works with probabilities instead of frequencies of the letters co-occurrences.

We noticed that sometimes the direction of mapping had a crucial influence on the accuracy. Therefore, mapping is executed in both directions: V 1  X  V 2 and V 2  X  V 1 to reduce ambiguity and to ensure that every letter gets a pair from the other set. The two results are then merged. Thus, the final number of errors is limited by the number of errors the best of the directions made. This helped increase the hit ratio by up to 6% in 25% of the cases as shown in figure 9.
 AUTOMATIC ALPHABET RECOGNITION 33 2.4. The combined method The final version of the proposed algorithm is summarized below: 1. Get a new document from the user. 2. Compute positions ( P 2 ) and environmental ( M 2 , PM 2 )v ectors for the document. 3. Pick a template, either the default  X  X ewspapers Style X  or according to a user selection. 4. Compare the successors X  environmental vectors of the template, M 1 ,t o those of the 5. For letters that got no or several mappings do: 6. For letters that are still not resolved: 7. If there is only one unidentified letter l (2) k left in the document alphabet: 2.5. Results of the combined method We tested our methods on various types of texts in three languages: Hebrew, Russian, and English. The source types in Hebrew experiments were on-line newspapers (HN, HN1, HN2), the Parliament protocols (HS), which represent the conversation language, several on-line scientific journals (HSJ) (Hebrew resources, http://www.snunit.k12.il/), and the Jewish Bible (HB). The English sources included Computer Science text (EC) (The Con-v ersation English resource, http://www.athel.com), Conversation language (ES) (The Sci-entific English resource, http://citeseer.nj.nec.com/cs), and the complete works of William Shakespeare (EL) (The English Literature resources, http://www.chemicool.com/). The Russian corpus contained on-line newspapers (RN), scientific articles collection (RS), 34 GEFFET, WISEMAN AND FEITELSON and the prose of A. S. Pushkin (RP), and F. M. Dostojevsky (RD) (Russian resources, http://ruslit.virtualave.net.).

We also examined the influence of the text size on the computed statistics in order to find the lower bound on the new document size. We ran the algorithms on texts of sizes varying from 200 Bytes (30 X 40 words) to 100 MB (  X  15 million words). Significant changes occur below 10 K (  X  1,500 words), mostly in the bottom half of this range; the difference between 5K and 10 KB was of 2 X 3 letters. Starting from 10 K the matching results never changed (as shown in figure 6).
 Below are illustrative graphs for different stages of the algorithm (figures 7, 8, and 9). The comparative results of the described algorithms are detailed in Table 1. Both vector methods in isolation succeeded for homogeneous corpora, but produced some mismatching for different types of text. The best case is, therefore, when we compared two similar sources, such as two newspapers, the same author X  X  books or two halves of the same source, (e.g. the Bible), that was divided into two parts and matched one to the other. The worst case is when comparing ancient text to modern one, or written to conversational language. The lower hit ratio for conversational language samples can be explained by their high number of participants, since conversational language has almost no norms or restrictions. In order to slove this problem, we need to find some common basis for all the language styles and genres. 3. Automatically generated dictionary A list of frequently used words can provide a common basis for a language. So we would like to construct a dictionary of the most common words constructed out of a large corpus. AUTOMATIC ALPHABET RECOGNITION 35 Building a dictionary is a very common method in other fields, like Data Compression (Ziv and Lempel 1978) and Speech Recognition (Sloboda 1995). For this purpose the al-gorithm takes a large text. We can use the text that was employed for template generation in Section 2. The text is split into words. We consider a  X  X ord X  any sequence of letters surrounded by non-letter characters. The algorithm counts the words and sorts them ac-cording to their number of appearances. Then, we can easily take the N most frequent w ords. 36 GEFFET, WISEMAN AND FEITELSON
From this point, the dictionary integrates into our algorithm. We have some letters that were mapped to several differnt letters in the template by the combined algorithm from dictionary the N most frequent words. Then these words are searched in the original text. The mapping of a letter with the highest number of found words is assessed to be the correct one.
Afterward, the  X  X isqualified X  mappings have to be assigned to alternative letters. For this we use the combined algorithm described in the previous section. We look for the best matching letter, excluding the previously disqualified ones. This procedure is repeated until there are no ambiguous mappings or when there is no change in the letters X  mapping table. As k etch of the algorithm is given below: 1. Automatic Dictionary Generation out of large text (off-line): (b) Build a dictionary containing these words. 2. Given a new document to be decoded: (b) For the disqualified mappings apply the combined method from Section 2 to assign AUTOMATIC ALPHABET RECOGNITION 37
We have run the new algorithm on the test files from Section 2, and the accuracy has been increased. The number of the mapping mistakes in the worst case (comparing different texts types) as a function of N most frequent words and the size of the text can be seen in figure 10. The conclusion from the graph is that 100% mapping accuracy can be achieved by either working with a large enough document or increasing the number of words fetched from the dictionary.

According to our method, there is no need for a special computerized linguistic resource, rather we generate our dictionary automatically from the text. We use any text of the lan-guage. Such a text can usually be found on the web. Moreover, the algorithm looks just for N w ords in the dictionary. Since N is a constant, the algorithm will perform | Alphabet | iterations of letters fixing in the worst case. Actually, this means that our algorithm X  X  com-the alphabet until it receives the correct interpretation. 4. Conclusions and future work We developed and presented a purely automatic method for the document encoding recog-nition, based on a vector-space model. It produced excellent results for languages from different families: Semitic, Slavic, and Indo-European. The time complexity of the vectors generation process is ( n ), where n is a number of characters in the new document. The matching procedure performance is bound by ( | Alphabet | 4 ).

We w ould like to suggest some possible extensions and further applications of our re-search. The method may be naturally used to easily identify the language of the document by comparing its statistics to pre-computed templates of given languages. Only one of them will have a relatively small number of unmatched or multiple-matched letters. It can also 38 GEFFET, WISEMAN AND FEITELSON be applied to determine document direction ( X  ltr  X ,  X  rtl  X ) and representation type ( visual , logical )b y simply running the  X  position vectors  X  algorithm in both ways and comparing the results to the templates. Obviously, only one of the two obtained vectors sets will match the templates, which reveals the correct direction and representation of the text. These are common difficulties since people tend to omit this information despite the fact that it is mandated by the HTML 4 standard. In summary, the only item our algorithm needs in order to recognize the encoding of a given piece of text is the list of candidate languages to choose from. Given this, it executes the three following stages, first it identifies the language, then the representation type and the direction, and finally the character set.

It is interesting to note that we received consistently different frequencies for different text types. Furthermore, it is a known fact in Linguistics (Yalin 1942) that every person uses certain very common stop words and prepositions with a constant individual frequency. There are also extreme examples, such as the first Prime Minister of Israel, David Ben-Gurion, who never used the case preposition  X  X T X . This feature can be very effective in identifying the authorship of written documents. We suggest to apply the presented method to classify texts according to their date, author, and style. First, we prepare sample vectors (templates) for various types of text and then compare them to a new document vectors and index it to the closest vectors category, respectively.
 The templates vectors as described so far were once calculated and then remain static. Another possible extension is to generate and update the templates as new documents arrive and are decoded, and thus make error correction dynamically.
 A ppendix In this paper we used the Latin transliteration of the Hebrew and Russian Letters. The Rus-sian alphabet consists of 33 letters: 21 consonants, 10 vowels and two letters without sound X  soft sign and hard sign (Russian transliteration, www.geocities.com/Colosseum/Track/ 7635/). Undotted Hebrew alphabet consists of 22 letters, all of them are consonants, and 5o f them have a special  X  X inal X  form (Segal and Itai, Hebrew transliteration, http://www. cs.technion. ac.il/  X  serelgl/bxi/hmntx/teud.html) for a total of 27 symbols.
 The Russian-Latin and Hebrew-Latin Transliteration Table: AUTOMATIC ALPHABET RECOGNITION 39 References 40 GEFFET, WISEMAN AND FEITELSON
