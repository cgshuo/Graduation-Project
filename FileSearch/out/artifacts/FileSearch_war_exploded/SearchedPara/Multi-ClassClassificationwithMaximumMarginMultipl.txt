 X X Y to R defined by  X  H = H  X   X  H  X  By the Rademacher complexity bound for functions with probability at least 1  X   X  , for all h  X  H , E  X  have  X  R S upper bounded as follows:  X  R  X  1]  X  hypotheses is upper bounded by the sum of the em-pirical Rademacher complexities of the sets to which are distributed in the same way leads to  X  R This concludes the proof.
 Proof. For any h  X  H K and x  X  X  , by the repro-H
S = span( (  X  1 ,..., X  m )  X   X  R m such that h  X  = Conversely, any Thus, for any y  X  X  , there exists  X  y = (  X  1 ,..., X  h ( x i ) = where K  X  is the kernel matrix associated to K  X  for  X   X  which concludes the proof.
 following equivalent form: min The Lagrangian L associated to this problem can be By the KKT conditions, the following holds: Slater X  X  condition holds for the convex primal prob-lem, which implies that strong duality holds and that  X  complementary slackness condition that guarantees for which is imposed on dual variables that correspond to inequality constraints, implies  X   X   X   X  1 . We first consider the subset of solutions to the dual optimization that are equal to T 1 = max k :  X   X  simplified by removing t when we impose  X   X   X  1 : the objective to max  X   X   X  Note this implies that  X   X  = 0 iff  X   X  k k inal problem in lemma 2 must be satisfied at the op- X  k &gt; 0.
 Now we seek an expression T 2 that is equal to the optimum of the dual optimization in the cases not ac- X  k and  X   X   X  found as convex combination. Now, fix any two co-least two such coordinates by the argument just dis-cussed). From condition ( 12 ) we know that the optimal objective value t  X  : since all constraints must hold at the optimum, which also implies, for all k and k  X  , t Thus, we can maximize over all feasible choice of k inequality is tight: which gives us the expression T 2 for the optimum in the intersection case. Finally, taking the maximum over T 1 and T 2 completes the lemma.
 p abilityatleast 1  X   X  overasampleofsize m ,thefol-lowingmulti-classclassificationgeneralizationbound holdsforall h  X  H 1 :
R ( h )  X   X  R  X  ( h ) + where T  X  and  X  max = max , Proof. Let M p =  X   X  lemmas 1 -3 and Jensen X  X  inequality, we can write: exp and Jensen X  X  inequality, we have e ing holds Therefore, we can write Taking the log of both sides and rearranging gives
E[ max  X  max Choosing t = the upper bound gives
E Plugging in this upper bound on the Rademacher com-concludes the proof.
 Proof. We introduce M 1 (  X  0 ) = max k  X  I can write T  X  max  X  M of that, we can rewrite T  X  N p (  X  0 ) =  X   X  k  X  )  X  (Tr[ K k  X  ] &lt; Tr[ K k ]) Now, let  X  0  X   X  0 , we will show that T  X  note that if T  X  M that T  X  Otherwise, T  X 
T bounded by M 2 (  X  0 )  X  T  X   X   X  M in all cases.
 linear program. Such a problem can be solved with any standard second order cone programming (SOCP) than the SILP formulation for smaller size problems, especially in the case of fewer classes.
 First, consider the dual formulation with the margin constraint appearing instead as an additional penalty an equivalent optimization problem. min both  X  and  X  are drawn from convex compact sets. unbounded from below. One can add a lower bound on achieve compactness and, thus, the minimax theorem applies and we permute the min and the max and solve on  X  and  X  , we have: where u k = troduces dual variables  X   X  0 : L =  X  At the optimum, it is guaranteed that  X  i, (since  X  min Note the minimax theorem applies once again (the ob-convex compact sets) and we can first solve the mini-is: min  X   X   X  (  X  C 2 u  X  min k  X (  X  C 2 u k  X  simplifies the objective further: constraint linear program:
