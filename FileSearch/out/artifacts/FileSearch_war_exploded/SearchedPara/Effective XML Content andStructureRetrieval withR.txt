 XML documents can be retrieved by means of not only content -only (CO) queries, but also content -and -structure (CAS) queries . Though promising better retrieval precisio n, CAS queries intro -duce several new challenges . To add ress these challenges , we propose a novel approach for XML CAS retrieval. The distin ctive feature of the approach is that it adopts a content -oriented point of view. Specifically, the approach first decompose s a CAS query into several fragments, then retrieves results for each query frag -ment in a content -centric way, and finally scores each answer node. The approach is adaptive to versatile homogeneous and heterog e-neous data environments. To assess the relevance of retrieval r e-sults to a query fragment, we present a scoring strategy that meas-ures relevance from both content and structure perspectives . In addition, an effective approach is pr opose d to infer a nswer nodes based on the C AS query and doc ument structure . An efficient algorithm is also presented for CAS retrieval. Finally, we demo n-strate the effectiveness of the proposed methods through c ompr e-hensive experimental studies.
 H.3.3 [Information Storage and Ret rieval]: Information Search and Retrieval Algorithms, Measurement, Performance XML , retrieval, content and structure (CAS) query , score 
As XML is more and more widely used in scientific data repos i-tories, digital libraries and web applications , there is an increas ing need for effective retriev al of information from these XML repos i-tories. XML as a data format differs from other document formats in that it has rich structure besides content. As a result, X ML doc-uments can be retrieved by means of not only content -only (CO) queries but also content -and -structure (CAS) queries [11] . CO queries, similar to traditional IR , contain only content related co n-ditions. CAS queries , on the other hand, contain both content and structur e conditions . For exa mple, through CAS query, one may specify that he is interested in papers whose title s are about "XML" in database journal s. Since CAS queries can specify more query conditions, they are more expres sive and promise better retrieval precision. 
Specifying exact structur e conditions in CAS queries, howe ver, is not easy , because the underlying structure of documents may be complex, and it is often difficult for users to gain complete kno w-ledge about the structure properties of XML doc uments . Without an accurate understanding of underlying structure, users are un a-ble to issue meaningful queries. Therefore, the structure cond i-tions in CAS queries should be interpreted a pproximately. For example, when one is searching for paper s with "information r e-trieval" in title s, a paper with "information r etrieval" in keywords should also be deemed relevant, although it does not match struc-ture condition in the query.

XML retrieval has been studied for several years, and several important characteristics h ave been identified [10,15] . The pre s-ence of structure conditions in CAS queries complicates the pro b-lems of query processing and results scoring, and introduces se v-eral challenges : (1) How to process structure conditions and resolve the impr e-ciseness in the structure constraints, especially when queries are issued on heterogeneous XML documents; (2) How to s core answers in terms of relevance to the query in both content and structure , and rank them accordingly; and (3) How to identify proper answer nodes that are relevant to a given CAS query and meet user's information need.

To address the challenges posed by CAS queries, we propose a novel approach for approximate XML CAS retrieval. The basic idea is to adopt a content -oriented point of view, and di ffere ntiate the roles of content conditions and structure condi tions based on the semantics of CAS query. To measure the relevance of an an-swer w.r.t a CAS query, we propose a co ntent -centric scoring method that takes both content relevance and structur al similar i-ties of query results into consideration. Finally, we present a n effective approach to infer meaningful answer nodes based on the CAS query and document structure. An algorithm is introduced which implements the proposed techniques efficiently. The pr o-posed approaches are in the same spirit as traditional IR and co n-form to query semantics . Moreover, the methods are highly flex i-ble in that they allow for signif icant structural variations and fit in versatile data environments with high structural heterogeneity . 
In this paper, we made the following contribu tions : (1) We propose an effective method for XML CAS retrieval. The method has several desired features:  X  It is flexible and adaptive to homogeneous or heterogene ous  X  It guarantees good s earch quality in terms of precision and  X  It is simple enough and efficient, and no relaxation is r e-(2) We propose a scoring strategy, which integrates structural relevance with content relevance effectively and suits CAS r e-trieval perfectly . (3) We propose to identify answer nodes using CAS queries and underlying documents structures. To the best of our know ledge, this is the first work to address the answer node inference problem in the context of XML CAS retrieval . (4) We design an algorit hm to retrieve results for CAS qu eries efficiently. Extensive experiments are conducted to invest igate the effectiveness of the proposed methods, and experime ntal results verify our techniques.

The rest of the paper is organized as follows. Section 2 dis-cusses examples to motivate our approaches. We present data model and CAS quer y semantics in Section 3 . Section 4 presents our approaches for CAS query processing and scoring. A method to infer answer nodes for CAS query is presented in Section 5. Section 6 describes algorithm s for CAS retrieval , followed by the exper imental studies in section 7. We review the related work in Section 8 and conclude in section 9.
In CAS queries, the structure conditions introduce new cha l-lenges . The first one is how to identify and locate the relevant fragments of XML documents when the structure conditions can-not be exactly matched.

Figure 1 gives two examples of XML document s, where the e l-lipse s represent element (attribute) nodes and the text o f a node is linked to the node. Consider a CAS query 
Q1: //article[about(author/name, widom )] where the about function is similar to the contains function in XPath [1] , except that it employ s a non-Boolean relevant s emantic. The semantics of CAS queries w ill be discussed later. The query Q1 searches for article s that ha ve authors relevant to " widom ". 
We notice that, the structural constraints in the query are not in consistency with the structural relationship s in Doc 1 showed in Figure 1(a) . Apparently , the article node in Doc 1 is a rel evant answer, but it cannot be found by exact match.

To handle the mismatch in structure, techniques based on tree pattern relaxations are proposed [7, 8] . The basic idea is to loose the structural constraints in queries s o that much more potential relevant answers can be matched.

For example, to find the article in Doc 1, the following relax a-tion [7] can be applied to Q1: (1) Edge Generalization , which generalizes article/author to article//author ; (2) " contains " Pro-motion , promot ing the content condition ( about widom ) from name to author ; and (3) Leaf deletion, delet ing the leaf node name . After these steps, the original query is relaxed to // article[about(.//author, widom )] , which can be match ed by the relevant node artic le in Doc 1. 
Although structural relaxation works in many cases , its effect is rather limited. Consider the document Doc 2 shown in Figure 1(b) . The information contained in that document is also closely rel evant to query intention of Q1 . However, using query relaxation, only when Q1 is relaxed to the most general form //article , the article node can be caught . Nevertheless , under the relaxed query, many irrelevant article nodes will appear in the answer set, in other words, there may be many false positives . Consider another query Q2: / /author[about(name, widom )]//article /title [about( .,xml )] Q2 can be matched perfectly by Doc 2, but it fails in Doc 1. The relaxed form //author[about(., widom )] is able to find relevant answer in Doc 1, but other conditions ar e lost in the relaxed query , as a result , many relevant answers may be discarded , in other words, there may be many false negatives.

To make a conclusion, query relaxation is useful only when structural constraints in the query are consistent with document structures to a degree. At least, the structures in the query should adhere to ancestor -descendant relationships of nodes in document. When structural constraints differ greatly with document stru c-ture s (e.g. the relationships are inverted as the case in Q1 w .r.t Doc 2), relaxation will introduce false positives or false negatives . In this paper , we propose a new vague approach for CAS retrieval. The approach proceeds in three steps: (1) Decomposition. We decompose a CAS queries into several fragments, each of them is composed of two parts: content condi-tion and structure condition. For example, Q1 has only one fra g-ment, in which there is a content condition " widom " and a stru c-ture condition article/author/name . (2) Retrieval. For each query fragment, we fi nd relevant r esults and score each result. For Q1, we try to find relevant matches using content condition " widom " and we get one in Doc 1. Then, we score the match according to how well it matches the content condition (content relevance) and how well it matches the stru c-tural constraint (structural relevance). (3) Combination. For each answer node, we combine scores from each fragment to produce an overall score for that answer.
Th is approach is flexible and adaptive enough while needs no relaxing or tra nsforming operations.

The second challenge posed by CAS queries is how to score an-swers so that the scores reflect the relevancies of answers w.r.t query intention. For example, Doc 1 and Doc 2 in Figure 1 should not only be deemed relevant to both Q1 and Q2, but also deserve high relevance scores because both are closely related to query intentions despite the internal structural heterogeneity. Another exa mple is that, given query Q2, an answer with " xml " in name than another answer with " xml " in title and "w idom " in name . In this paper, we propose to measure the relevance of an answer by means of content relevance combined with structure rele vance, where the latter is evaluated based on how we ll the doc ument context of a relevant piece of data matches the structure cond itions in a given query. 
The third challenge is to determine the most appropriate el e-ments as answer nodes . Although a user specif ies target node as return node, we believe it i s not proper to treat target nodes as answer node for Q2 obviously. In this case, we suggest that a u-thor be a good candidate for answer nodes. However, it is not straightforward to infer proper answer nodes. We present a m e-thod that is able to identify the most appropriate answer nodes based on query intention and underlying document struc ture. We adopt a common model used for XML, that is, w e model an XML document as an ordered, la beled tree where each el ement (attribute) is represented as a node and each element -to -subelement (element -to -attribute) relationship is represented as an edge between the corresponding nodes. We represent each node as label is the name of the corresponding element or attribute, and text is the corresponding element X  X  textual content or attribute X  X  value. The text is optional because not every el ement contains textual content. For example, F igure 1 shows tree repr esentation s of a sample XML document collection.

XML documents can be retrieved by means of several langua g-es with different flavors [5] . In this paper, we focus on stru cture and content retrieval with CAS query language. We use a content -oriented XPath -like query language called Narrowed Extended XPath I (NEXI) [24] as the basis of our CAS query language. NEXI narrows XPath in that only descen dant operato r is allowed in a tag path and many aspects of XPath are dro pped (e.g. functions) . I t extends XPath in that it adds an about () filter . The syntax of a CAS query is [18] : path 1 [abouts 1 ]//...//path n [abouts n ] where each path is a sequence of nodes connected by desce ndant ( X // X ) axes; each abouts is a Boolean combination of about fun c-tions. An about function is in the format of about(path, con tent ), where path is a tag path and content is a content condition co n-sisting of keywords. The about function, requir ing that a certain context (i.e., path ) should be relevant to a s pecific content d e-significant diversion of NEXI from XPath is semantics : there is no definite semantics defined for NEXI . 
Note that queries Q1 and Q2 in section 2 are almost but not cor-rect CAS queries according to the syntax described above . They can be corrected simply by replacing child axes with de scendant axes as follows: Q1: //article[about( .//author/ /name, widom )] Q2: / /author[about( .//name, widom )]//article //title [about( .,xml )]
When we use Q1 and Q2 below we refer to the standard CAS form above.

Tree patterns [6,7] are fundamental to XPath . We can use sim i-lar representation for CAS queries. 
Definition 1 . A CAS query pattern is a special tree pattern in that only one kind of edges is allowed which corresponds to the descendant axes (//) and each leaf node may have a content pred i-cate corresponding to the associated about function .

In tree pattern, there is a distinguished node corresponding to the output element . In CAS query pattern , the distinguished node is called target node and all other nodes are called support nodes . 
There is no predefined semantics for NEXI queries. To get a clear understanding of CAS quer y processing, and develop a basis for scoring and ra nking, we describe the semantics of CAS queries informally as follows . (1) The CAS queries specify information need through two kinds of constraints: structural constraints and content con straints. Content constraints specify what the match should be about while structural constraints specify what structural pro perties the match should observe . Thus, t he structure specifications should be con-sidered hints as to where to look [16] , it need s not to be fulfilled . Notice that th is is a vague interpret ation of s tructural constraints. (2) CAS queries return each answer within an answer node. The answer nodes of a CAS query should be inferred manually or a u-tomatically from query and/or documents. As motivated earlier, we believe it is not proper to treat target nod es as answer nodes in many cases, just as in text retrieval, it is not proper to return only the matches of query keywords to users. (3) Relevance of a result will be based on whether or not it s a-tisfies the information need. It will not be judged based on strict conformance to the structural constraints of the query . Since i n-formation retrieval is content -centric, t he information need is con-veyed mainly through content constraints. (4) The structural constraints may influence the score of query results. In XML documents, text ual content is nested under certain structural context. A piece of text appearing in different co ntext may have different meanings, which should be taken into account in scoring. 
In this section, we present a gener al approach to XML CAS r e-trieval , which consists of three steps: decomposition, retrieval and combinations.
Given a CAS query, the purpose of query decomposition is to divide the query into several fragments , such that different fra g-men ts are relativ ely independent to each other. 
Definition 2 (query context of a keyword ). Given a CAS tree pattern p , k is a keyword in content constraint associated with node v . The label path from root to v is called query context of k . 
Definition 3 (doc ument context of a keyword ). Given a leaf node l in XML document , k is a term in l , the tag path from doc u-ment root to l is a document context of k . 
Given a keyword k , its query context specif ies the structural properties of its potential matches , while its doc ument context is the actual context where the keyword ap pears.

Definition 4 (query fragment) . A query fragment is a pair &lt; con-constraint s; context is the query context of kws . Below we denote a query fragment as context : kws . 
Take Q2 as an example. Two query fragments can be derived from Q2, they are: q 1 : //author/ /name: widom , and q 2 : //author//article/title: xml
In this section, we will show how to find relevant matches of query fragments and how to measure the relevance of a match w.r.t a query fragment.
Two opposing strategies can be adopted as to find ing relevant match es of a query fragment. One is to locate targets using query context , then score the target s based on how well the content con-straints are satisfied. As discussed earlier, the problem with this strategy is that: due to the impreciseness of queries and heter o-geneity of documents, it will miss many relevant matches u nless relax or trans form the queries into general forms , which in turn will introduce false positives and false negatives . B esides, this strategy is not in conformance to CAS query sema ntics.

To obey CAS query semantics, we adopt another strategy , i.e. content -centric process ing of CAS queries . We start with finding relevant nodes that match content predicates . For each relevant node, we proceed to compute its score according to its similarity in content with content constraint and similarity in document con-text with query con text. By this strategy, we can avoid the weak-nesses of the first strategy while benefit from the flexibility, ada p-tability and simplicity.
Given a query fragment, t he relevance of a match can be meas-ured by its structural similarity and content sim ilarity with the query fragment. We first discuss how to measure the sim ilarity between a query context and document context, and then study how to measure the relevance on content.

A query context is a pattern, while a document context is a doc-ument path. Generally, a pattern relates to a document path in two ways: match, or not match. However, it is common that a doc u-ment path does not match a pattern exact ly, but they are very simi-lar . Consider a pattern p 1 : //author//article//title and a doc ument path d 1 : //article/author/title . Although d 1 does not match p they are similar in that d 1 is almost a match of p 1 methods try to relax the pattern s so that they can be match ed straightforwardly . Nevertheless, our proposal is to define a similar ity measure between a pattern and a document path so that the sim ilarity can be computed directly.

Definition 5 . Given a pattern p and document path d in doc u-ment D , the similarity b etween p and d is a value between 0 and 1 defined as: between two document paths d and d' , inst ( p ) represents the set of instances of p in D and an instance of p can be obtained by insta n-tiating each descendant axis with one or more child axis.

Intuitively , when d matches p , the similarity between p and d is defined to 1; otherwise, the similarity is the maximum sim ilarity between d and instances of p . 
The similar relation between a pattern and a document path can be reg arded as an extension to the Boolean match -or -not semantic. However, formula (1) does not lay a good foundation for compu-tation , since the instance set of a pattern may be either huge or empty . Below we try to compute the similarity in a tract able way. To begin with, we first evaluate how well a single node in a pat-tern is matched.

Definition 6 . Given a pattern p and a node v p in p , a node v document path d is a match of v p if label ( v match set of v p in d , denoted as M d ( v p ), is the se t of matches of v In the definition, label is a function mapping a node to its l abel. The "  X  " represent s similar relation . Labels can be similar or diss i-milar depending on the certain criteria of comparison speci fied by means of functions. Various simila rity functions can be considered, such as stem -based function, ontology -based func tion, etc. In this paper, we simply employ a stem -based function: two labels are similar if they share the common stem . 
Provided with v p in p , val d ( v p ) is a value measuring how well v is matched by nodes in d , val d ( v p ) can be defined as where sim ( v p , v d ) is the similarity between v p and v defined later.

Then, the overall similarity between pattern p and document path d can be computed as 
The key point of formula (2) and (3) is how to compute the s i-milarity between v p and v d . Here we present an approach , which computes similarity based on node label s as well as levels . In query pa ttern s and document structure s, node level represents structural properties. We notice that, the level of a node in a pat-tern is a relative value that does not represent a position in the document. For example, the levels of author , article and title in p is 0, 1 and 2. Consider an instance of p 1 , say d journal/article/title , where the level of a uthor , article and title is 1, 3 and 4. Obviously, directly compa ring levels in pattern with levels in document is meaningless. To make them comparab le, we use the relative levels in document.

Definition 7 . Given a document path d and path pattern p , d matches p and the matching nodes are a 0 ... a n , the relative level of a is determined by function rl () as follows : (1) rl ( a i )  X  {0,1, ..., n }; and (2) If a i appears before a j in document order , rl ( a
We use level ( v p ) and rl ( v d ) to denote the level of v fined as sim v v sim label v label v label -based similarity that takes disagreement of levels into co n-sideration. When plac ing formula (4) in formula (2) and (3), we get a substitute for formula (1), thus the similarity between query context and document context can be measured . 
Now let us look at the similarity of contents. Given query fra g-content constraint in q and textual content of a , respectively. The similarity between q .kws and a.text can be evaluated using trad i-tiona l methods, e.g. tf*idf methods , since both of them co ntain only keywords with no further structure s. We describe a.text via a term in a . Weight for a term t in a is determined using the follo w-ing tf*idf formula [14] : total number of text unit s in XML document s, n t is the number of text unit s in which term t exists . q.kws can be similarly described , except that the weight of term t in q.kws is determined by (1 + log( f q,t )) where f of term t in q.kws . Then the content similarity b etween a and q can be estimated via the cosine of their corres ponding content vectors:
Given a query fragment q and a data node a matching q , the overall similarity of q and a is a compound value of the stru ctural the following formula: While sim content ( a , q ) is computed by formula (6), sim the similarity between document context of a and query con text of q , thu s can be computed by formula (1).
In this section, we measure the relevance of an answer node with respect to given query. We assumed that we have obtained a set of potential answer nodes here, and we will show how to infer answer nodes in nex t section.

Given a CAS query Q , let q 1 ,..., q n be the query fragments in Q , ans is a (potential) answer node, a 1 , a 2 ,..., a q ,..., q n contained in ans respectively ( a i may be empty to denote that there is no match for q i ). Then the score of a ns is: 
The last but not least point to note is that, the scoring strategy presented in this paper is not restricted to certain computing m e-thods. It is a general framework , which allows other similar ity functions and computing m ethods.
In this section, we try to infer answer nodes automatically from query and document structure. 
For XML CAS query, answers should be returned at element level, and naturally, a question arises as to what kind of el ements are qualified as answer nodes. To answer this question, we present some guidelines that a good answer node should follow : (1) It should be relevant to the given query . (2) It should contain the target nodes . (3) It should embed the proofs that it matches the query. (4) It should not contain too much irrelevant information.

Guideline (1) is self -explanatory . According to guideline ( 2), answer nodes should have target nodes as descendant -or -self , because the target node s usually are important to user s. Guid eline matches of content predicates and structural predicates , since the user would like to check the predicate matches and make sure that the predicates are satis fied in a meaningful way. One of the strengths of XML re trieval is that it can return the most sp ecific answers to users, that is wh y guideline ( 4) should hold.

To infer answer nodes, we first differentiate node types from data nodes and infer answer node types before actually inferring answer nodes . We simply use node label s to indicate node types.
When XML documents are homogeneous and structural con-straints in query align with document structures, we can deduce acceptable answer node type as follows . Given a query pattern p , with nodes v 1 , v 2 ,..., v k . We use LCA( p ; t 1 , t t node type, depending on which one appears hig her in structure hierarchy. For example, when Q2 is issued against document s structured like Doc 2 in Figure 1(b), author can be chosen as answer node type. Consider another e xample : Q3: bib//article[about(.//author, widom )][about(.//title, xml)] For Q3, article is a valid answer node type if Q3 is issued on Doc 1-like documents . 
However, w hen XML documents are heterogeneous, or struc-tural constraints diverge from document structures, like the case when Q2 is issued on Doc 1 in Figure 1(a), we need to take speci f-ic document structure into consideration.

About XML documents, we have several observations : (1) While XML documents typically have a huge set of key-words, the set of tags is moderate in size. For example, there are only 30+ tag s in DBLP dataset, and the number of frequently used tag s is 10+. (2) In XML documents with similar themes, the tags used are similar. Although different tags may be used in different doc u-ments to express similar concept, this can be solved by tech niques such as synonym thesaurus, ontology, etc, and these tec hniques are orthogonal to our method. (3) In XML documents with similar theme, the heterogeneity of structure is revealed mainly by relationships between tags. 
According to the observations above, we know that XML doc-uments with similar themes are more likely to be stable in tag set , but non-stable in the relationships between tags. Pr ovided with these findings, we identify a subset of the stable tag set , which will be used to infer answer node type . 
Definition 8 . Given a query pattern p , t 1 , t words in content predicates of p , the Interested Tag Set (ITS) of p is a subset of tags in p which contains the tags of: LCA( p ; t t ), target node, and all support nodes that are desce ndants of the data node whose label is in ITS.
 For example, the ITS of Q2 is { author , name , article , title }. With ITS, we now present some heuristics about answer nodes inf e-rence: (1) The answe r nodes should contain as more ITS -labeled nodes in their subtree as possible . (2) In an answer node, not all labels from ITS appear multiple times. E.g. for Q2, author , name , article and title should not appear repeat edly in the same answer node.

Inspired by the heuristics above, we introduce a method to i nfer answer node type , which proceeds in two steps: (1) Compute the ITS for the given query pattern . (2) Compute the LCA of ITS -labeled nodes in document sche-ma. 
The node types obtained in the second ste p are the types of a n-swer node. If the document schema is not available, we can exact sch ema information from documents like [25] . In fact, the schema information we need is rather simple. 
For example, consider the document presented in Figure 1(a), we as sume that the schema can be obtained by simply dropping the texts. The ITS of Q1 is { article, author, name }, and in the schema derived from Doc 1 , LCA( article, author, name ) is a r-ticle , as a result, article is an answer node type. The ITS of Q2 is { author, name, article, title }, and LCA ( author, name, article, title ) is still article , so article is also an answer node type for Q2.
After the answer node types are inferred, we can go further to infer answer nodes. Initially, all nodes with answer node types ar e potential answer nodes. If a potential answer node contains rel e-vant matches of the given query, and its score is above a given thr eshold (if any), it will be picked out as an answer node. Note that techniques a nalyzing XML d ata structure [9, 19] is comp le-mentary to our method.
In this section , we present algorithm s that implement the pr o-posed techniques efficiently . 
We parse the input XML documents and assign a Dewey label to each node. We collect the schema information if available or extract from the documents. To speed up query processing, we build keyword inverted lists , which retrieve a list of nodes in do c-ument order whose text ual content contain the input ke yword . 
Below we present two algorithms. Both algorithms take as input a CAS query Q and an optional threshold value  X  value is set to 0. The algorithms output a ranked list of a nswer nodes. 
Algorithm 1 is a na X ve algorithm, which process es a CAS query in a rather straightforward way. It first decompose s Query Q into seve ral fragments as described in section 4.1, and identif ies an-swer node type s as discussed in section 5. Then for each potential answer node that is an instance of answer node type, it computes the score of the answer node with respect to each query fragment . Then overall score of the answer node is ob tained. If the answer node is a valid answer (i.e. its score is above  X  ), it will be added to final answer set. Finally , all a nswers are ranked and returned.
Algorithm 1 is inefficient in that it computes the sc ore of each potential answer node, even though a large part of potential an-swers is not relevant at all. To remedy this shortcoming, we pr o-pose another algorithm presented in Algorithm 2. It di ffers with Algorithm 1 in that only relevant answer nodes are c hecked and scored. Thus, it avoids a lot of useless computation. 
Algorithm 2 also begins with query decomposition and a nswer node type identification. Then, the getMatches procedure r etrieves all the XML nodes matching the keywords in content constraints. Based on the answers node type and matches of ke ywords, it calls the getAnswerNode function to obtain answer nodes. Then, for each answer node, the algorithm computes its score as discussed in section 4.3, if the score is less than prede fined threshold any), the answer node will be discarded. F inally, all the answer nodes are ranked and returned.

Function getAnswerNode infer s answer nodes based on the gi v-en answer node type T and matches of keywords. A cursor is a t-tached to each set S i and is initialized to the beginning of S it gets the answer nodes from S i [ c i ] (1  X  i  X  n ), one from each loop. S then a is an potential answer node, and wi tnesses of a are fetched from S i (1  X  i  X  n ) by calling function getWitness . The witnesses of query fragments. The witnesses will be used in scoring and out-put ting answer nodes. Note that the cursors will be forwarded monotonously in getWitness (line 5), so all matches will be scanned only once.
 Algorithm 1 : Na X ve algorithm for CAS query processing Input: CAS query Q ;  X  , a threshold value, set default to 0 Output: ranked list of answer nodes 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. Algorithm 2: efficient CAS query processing Input: CAS query Q ;  X  , a threshold value , set default to 0 Output: ranked list of answer nodes 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. Function getAnswerNode ( T , S 1 , S 2 , ... , S n ) 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Function getWitness( a , c 1 , ... , c n , S 1 , S 2 , ... , S 1. 2. 3. 4. 5. 7. 
In this section, we present our experimental results. We have implemented a prototype called XScore ( X ML  X  X  stored in an index build by Lemur toolkit [2] . We run our exper i-ments on a machine with Intel 2. 30G CPU and 2G RAM running Windows XP . All algorithms are implemented in C++. We have tested both real and synthetic datasets. The real dataset is DBLP 230M [3] . To simulate heterogeneous data enviro nment in which contents are similar , we changed the structure of DBLP dataset and increased structural complexity while kep t textual content unchanged to get synthetic dataset . Specifically, t he synthetic data is obtained by transforming a portion of DBLP using XQuery and its structure is shown in Figure 2, where the elements with * ap-pear repeatedly. The size of sy nthetic data is 150M.

We have tes ted several CAS queries. To investigate the i mpacts of different structure s on queries , we have constructed sev eral variants for each query. All the queries make up the query set. Each query was issued on both datasets. The queries are shown in Table 1. So me additional queries are used in some experimental sections and they will be pointed out later.
 Q1 //article[about(.//author/name, wang)] Q2 //author[about(.//article/title, xml)] Q3 //article[about(.//title, XML)][about(.//author, w ang ) ] Q4 //author[about(.//title, XML)][about(.//name, w ang)] Q5 //article[about(journal, ACM)][about(.//title, XML)] //author Q6 //journal[about(.//title, ACM)]//article[about(.//title, XML)] Q7 //author[about(., wang)]//journal[about(.//title, ACM)]// Q8 //dblp//author[about(.//article//title, xml)]// name Q9 //dblp//journal//article[about(title, xml)]
We used the following measures in experiments : the quality of the answer nodes, retrieval effectiveness measured by preci sion, recall and F -measure, ranking effectiveness measured by P@k precision, and efficiency as well as scalability.

We compared our approaches with that in [8] and [17, 18] . In [8] , three scoring methods are proposed: twig scoring, path scor-ing and binary scoring. Twig scoring is best in accuracy while worst in efficiency, binary scoring, on the opposite side, is not accurate but most efficient, and path scoring get s a balance b e-tween twig scoring and binary scoring. In this paper, we focus mainly on retrieval and ranking effectiveness, so we compare our methods with twig scoring. We do not compare with the scoring method in [17, 18] , because it interprets the structure of supporting nodes strictly, as we stated earlier, we are interested in appro x-imate match of queries, so the scoring methods in [17, 18] are not of too much interest to us.
Answer node inference is one of the bas ic problems in XML r e-trieval. We first test the effectiveness of the proposed methods for inferring answer nodes. Both [8] and [17, 18] do not regard a n-swer node inference as a research problem. It is interesting that, the two papers take totally different views on this problem: in [8] , the matches of roots of query patterns are viewed as an swer nodes, while [17, 18] regard target nodes as answer nodes. The differen c-es between our method and [8, 17, 18] are shown in Table 2. Queries DBLP Synthetic data XScore [8] [ 17, 18] XScore [8] [ 17, 18] Q1, Q3 article article article author article article Q2, Q4 article author author author author author
As we can see, our method is able to infer the most appropr iate answer nodes according to users' information need. More over, the answer node we infer will alter as documents structure s change. However, given the query, answer nodes determined by methods in [8, 17, 18] are fixed, no matter what the documents are. As a result, the answer nodes in [8, 17, 18] are usually meaningless, especially when the structural constraints in the query deviate from doc ument structure . 
It should be pointed out that answer node inference is diffe rent from retrieval results generation. It is usually the case that only part of the subtree rooted at an answer node needs to be output [13, 19, 20] . Therefore, the size of the subtree rooted at answer nodes should not be a concern provided that the answer nodes are a p-propriate and meaningful. 
To measure the retrieval effectiveness , we evaluate queries and summarize three metrics borrowed from IR field : precision , recall and F-measure. Precision measures the percentage of the output nodes that are desired, recall measures the percentage o f the d e-sired nodes that are output. F-measure is the weighted harmoni c mean of precision and recall . We use the default b alanced F -measure also known as F 1 which equally weights pr ecision and recall . We obtained the set of relevant nodes by running the sc he-ma -aware XML q uery (in XQuery). 
Note that the answer nodes used in XScore are different from that in twig scoring. In this section, we put the reasonableness of answer nodes aside, and focus on the retrieval effectiveness solely with the assumption that all answer nodes are acceptable. Other-wise, the retrieval effectiveness of twig scoring is hard to measure.
In this section, we use queries Q1~Q7, where Q1~Q2 are sim i-lar, Q3~Q4 belong to the same group, and Q5~Q7 are closely related. The seven queries ex hibit different sizes, query structures, and content predicates. We ch oose these queries to illustrate the different possible queries issued by different users in a real -world scenario with heterogeneous documents . 
Results are sh own in Figure 3-4, where TwigScore refers to twig scoring method. Figure 3 depicts the measurements ob tained from DBLP dataset. As we can see, XScore keeps almost perfect recall, precision and F -measure, while performance of T wig Score is not stable: it per forms fairly well on Q1, Q3 and Q5 in terms of both recall and precision as well as F -measure; Q2 gets a high recall rate while poor precision rate, which means most of r e-turned nodes are irrelevant; Q4, Q6 and Q7 on the other side r e-turn precise answers b ut a great quantity of relevant answer are lost. The reason for unstable performance of twig scoring is that it blindly relaxes the queries. In many cases, relaxed queries recall much more irrelevant nodes but little relevant answers, as a result, the precision degrades but recall improves little. The pe rformance of twig scoring is even worse when there are signi ficant structural variations in stru ctures of queries as observed in Q4, Q6 and Q7.
Figure 4 shows similar characteristics. XScore exhibits o ptimal and stable performance for all kinds of queries. It perform s signi f-icantly better than twig scoring for queries that do not align with document in structure. We compare the ranking strategies in XScore and twig sco ring. The measureme nt we use in this section is P@ k or top -k precision. P@ k precision measures the precision for top k answers. It is more meaningful to evaluate the performance by P@ k precision than overall precision since users are often only interested in the top -k answer s. The top-k precision reflects the ranking ability of the system.

While the parameter k is usually assigned constant values such as 10, 20 etc , we determine parameter k as follows. Given a query Q and document D , k =| Q ( D )|, i.e. k is the number of exact m atch es of Q in D . We run standard XML queries to get the value of k . The rationale is that, given a query Q and k computed as above, the exact matches of Q should be ranked at the top k pos itions . We believe that this is a fundamental demand to a mature re trie val system.

The results of top-k precision are shown in Table 3. We can find that XScore is accurate in top -k precision in contrast with twig scoring. The poor performance of twig scoring can be ex-plained by its scoring method. Take Q2 as an example. I n relax a-tion -based methods, if Q2 is issued on DBLP dataset, it will be relaxed to //author to get relevant answers. The score of an an-swer in twig scoring method depends on two factors: idf and tf . The idf scores are used to rank relaxed matches based on how closely they match the relaxed query. Since all answers of Q2 are actually matches of //author , their idf are identical. To di stinguish between matches of the same relaxed query tf measure is intr o-duced. Intuitively, the tf score of an answer quanti fies the number of distinct ways in which an answer matches a query. Since each author node can match //author in only one way , again, all an-swers have the same tf . As a result, all answers have equal score. In other words, the re is no ranking! In fact, the t op-k precision of Q2 on DBLP in Table 3 makes no sense. This situ ation exists for other queries as well. Conclusions can be drawn that the twig scoring method is defective in ranking.

We evaluate the efficiencies of proposed algorithms in this sec-tion. We use the na X ve algorithm as baseline, and measu re the improvement we gain in Algorithm 2 (see section 6). The results are shown in Figure 5, where XScore denotes the alg orithm used in XScore, i.e. Algorithm 2. The datasets we use is 3M DBLP dataset and 5M synthetic dataset. It is obvious that Algorithm 2 outperforms the na X ve algorithm significantly. XScore is more than 100 times and 200 times faster than na X ve algorithm on DBLP dataset and synthetic dataset, respectively. In fact, the na X ve algorithm co nsumes more than 1000 seconds on synthetic data for some queries ( That is why we use such small datasets).
We tested the scalability of XScore on the DBLP dataset over three parameters: number of query fragments, key word numbers and document size . 
To investigate the effect of query size in terms of query fra g-ments and content keywords (keywords in content predicates) on processing time, we use three groups of queries. Each group con-sists of four queries, with 1 to 4 query fragments in query respe c-tively . Queries from the same group contain the same set of key-words. The number of keywords for group 1, group 2 and group 3 is 4, 5 and 6 respectively. The i -th quer ies in three groups contain the same number of query fragme nts. For example, the first qu e-ries in all groups consist of only one fragment. In addition, the i -th queries in all groups are the same in stru ctures but different in keywords . The response time of the queries on DBLP 12M doc u-ment are shown in Figure 6. W e find that with the growth of query fragments in each group, the processing time increases very little. In fact, when the number of query fragments grows from 1 to 4, the time increases no more than 0. 2 seconds in all groups . On the other side, the i ncreased time gained by increasing the number of keywords is slightly greater. For example, the elapsed time is 0.4 seconds greater when we augment the fourth query in group 1 with two more keywords to get the fourth query in group 3. We can conclude from this set of experiments that the alg orithm is more sensitive to variance of keywords than variance of fra gments.
We have done another set of experiments to evaluate the ef fect of keywords on processing time as shown in Figure 6(b). We s e-lect two groups of keywords, with different selectivity. The ke y-words in group 1 appear frequently in dataset, whereas group 2 contains less frequent keywords. We notice that as the number of keywords increases from 1 to 4, the processing time increase s at different rate, dependi ng on the selectivity of keywords. Ge nerally, the algorithm scales well when the number of keywords i ncreases . 
We also test the efficiency of XScore with different document size. We extract different portion of DBLP dataset and get sev eral documents with size from 3M to 85M. We run two queries with moderate size but different selectivity (i.e.  X //article[about(.//title, XML search )] X  and  X //article [about(.//title, relevance feedback )] X ) on each doc uments, and take down the response time as well as the num ber of answer nodes. As shown in Figure 7(a), XScore scales linea rly with respect to document size. The time XScore consumes depends mainly on the size of query results. Figure 7(b) also shows that XScore scales well as the number of answer nodes increases . (a) time w.r.t no. of fragments (b) time w.r.t no. of keywords There is a large body of work on XML information retrieval . INEX [4] identifi ed the following two types of queries for XML IR: Content -only (CO) queries , and Content -and -structure (CAS) queries. CAS queries can be interpreted in two ways, either strictly (SCAS) or loosely (VCAS). The target structure of the inform a-tion need can be deduced exactly from the query in the SCAS interpretation . While in the VCAS interpretation, the path specif i-cations need not to be abide by and should be cons ider ed hints as to where to look. The queries considered in this work can be clas-sified into VCAS queries . 
Existing work concentrate on two aspects of VCAS retrieval: how to process the structural constraints in CAS queries, and how to score answers to CAS queries.

On CAS query processing, current work can be classified into the following categories: (1) CO approaches [23] . They ignore the structure conditions , and transform a CAS query to a CO query. Such approaches are simple but lose the precision bene fits that can be derived from XML structures. (2) Relaxation -based ap-proaches [7, 8] , where structural constraints in quer ies are relax ed and then strictly interprets structural constraints in relaxed queries, which are approximate answers to the original quer ies. While query relax ations work in some circumstances, it may fail in other circumstances and may lose precision . (3) Strict match -based ap-proaches. In [18] , a VCAS query is d ecomposed into a CO query and a SCAS query . Then they use existing XML engines t o r e-trieve results for the decomposed sub-queries, and finally co mbine results. In [22] , Structural constraints are classified as target co n-straints or context constraints and are matched separately. This category of methods still match all or part of stru cture cond itions in a strict way, they do not serve our purpose. Recently, I. Sanz et.al [21] developed an approach for the identi fication of subtrees similar to a given pattern in a collection of highly het erogeneous semi -structured documents , our work di ffers with [21] in that our approach is content -oriented, whereas [21] is not. 
On scoring CAS answers, several proposals have been brought forward. The scoring methods in [8] are inspired by tf*idf and consider both structure and content in scoring while accounting for query relaxation. As discussed in section 7 .3.3, their methods are defective in ranking . In [18] , results from SCAS retrieval and CO retrieval are combined to produce final approximate answers. The scor e of an answer is the score of a CO sub -query result ad-just ed by the relevancy of the result to the target path condition of the original query, which is mea sured by target path similarity. In [12] , scores for all support elements and result elements are co m-puted, and the scores of support elem ents are added to the scores of the result elements they support. These scoring methods do not consider the stru ctural relevance of answers when all structural constraints are vaguely matched . In [17] , weighted term frequen cy and inverted element frequency are proposed as the basis of XML ranking. These measures are complementary to our work. The work in [21] defines different structural similarity functions con-sidering different structural constraints . Their work , though inspi r-ing, did not deal with conten t similarity and relevance sco ring.
In this paper, we propose a new approach for XML CAS r e-trieval. Distinct from existing work, we differentiate the role s of structural constraints and content constraints, and adopt a co ntent -orient strategy. T he approach proceeds in three steps. It first d e-composes a CAS query into several query fragments. Then, It retrieves results for each query fragments in a content -orient way. At last, for each answer node, we combine the r esults from each query fragments and compute the relevance score of the answer w.r.t original query. To measure the relev ance of answers, we propose the similarity between a query context and a document context . The score of an answer is a combined score integrat ing structural similarity and content similarity. We propose an effe c-tive method to infer desired answer nodes based on some guide-lines. Efficient algorithm is also presented for CAS queries. Fina l-ly, we evaluate our ap proach es through extensive experiments on both real and sy nthet ic dataset. E xperimental results demonstrate the effectiv eness of our methods . 
Several i mprovements can be made in the future. One point is that , query fragments may not be equally important, e.g. fra gment containing target node may be deemed more importa nt. We can introduce a weight for each fragment, which should be set aut o-matically and will have impacts on scoring and ranking. What  X  X  more , we plan to probe into the top-k processing of CAS retrieval. This work is partly supported by the National Natural Science Foundation of China under Grant No. 60763001 and 60803105/ F020606, the National Social Science Foundation of China under Grant No. 07BTQ025. We also thank the anony mous referees for helpful suggestions . [1] XML Path Langua ge (XPath) Version 1.0. W3C Recomme n-[2] The Lemur Toolkit for Language Modeling and Inform ation [3] DBLP Bibliography . www.informatik.uni -trier.de/~ley/db/. [4] Initiative of Evaluation for XML Retr ieval . [5] S. Amer -Yahia, and M. Lalmas. XML search: languages, [6] S. Amer -Yahia, S. Cho, and D. Srivastava. Tree Pattern R e-[7] S. Amer -Yahia, L.V.S. Lakshmanan, and S. Pandit. Fle XPath: [8] S. Amer -Yahia, N. Koudas, A. Marian, et al. Structure and [9] Z. Bao, T.W. Ling, B. Chen, et al. Effective XML Ke yword [10] M.P. Consens, R.A. Baeza -Yates, M. Lalmas, et al. XML [11] N. G X vert, and G. Kazai. Overview of the INitiative for the [12] S. Geva. GPX -Gardens Point XML Information Retrieval at [13] Y. Huang, Z. Liu, and Y. Chen. Query Biased Snippet Ge n-[14] M. Iwayama, A. Fujii, N. Kando, et al. An empirical study on [15] J. Kamps, M. Marx, M. de Rijke, et al. XML Retrieval: What [16] J. Kamps, M. Marx, M. de Rijke, et al. Structured Queries in [17] S. Liu, Q. Zou, and W.W. Chu. Configurable indexing and [18] S. Liu, W.W. Chu, and R. Shahinian. Vague Content and [19] Z. Liu, and Y. Chen. Identifying meaningful return inform a-[20] Z. Liu, and Y. Chen. Reasoning and Identifying Relevant [21] I. Sanz, M. Mesiti, G. Guerrini, et al. Fragment -based ap-[22] B. Sigurbj X rnsson, and J. Kamps. The Effect of Structured [23] B. Sigurbj X rnsson, J. Kamps, and M. de Rijke. The Univers i-[24] A. Trotman, and B. Sigurbj X rnsson. Narrowed Extended [25] C. Yu, and H.V. Jagadish. Schema Summarization. In VLDB 
