 H.3.3 [Information storage and retr ieval]: Information Search and Retrieval  X  retrieval models, query formulation. Algorithms, Performance, Experimentation, Theory. CLIR, confidence estimation. Query translation is the key problem in Cross Language Information Retrieval (CLIR). This can be done by using Bilingual Dictionaries (BDs), parallel corpora or machine translation [4]. When several tr anslation tools or resources are combined, a crucial problem is to combine and re-weight all the translation candidates correctly. In the previous studies, simple methods are usually employed, i.e. one combines various translations for the same query term linearly by assigning or optimizing automatically a confidence weight to the translation tool or resource [3, 5]. However, we notice that a single confidence score is assigned to all the translations from the same translation resource. This score does not modify the relative importance of the translations from the same resource. In practice, sometimes when new criteria are considered, a translation with a low score suggested by a resource can turn out to be a better translation. In this paper, we propose a confidence estimation technique to adjust the weight of various possible translations of a query term. Given different transl ations from different resources, our approach estimates a confidence for each translation candidate of a query term according to additional informative features. According to these conf idence measures, the translation candidates are re-weighted and homogenous weights are assigned to the translation candidates from different resources. Therefore, the advantages of this approach are twofold. On one hand, the confidence measure allows us to adjust the original weight of the translations and to select the best translation terms. On the other hand, the confidence estimates also provide us with a comparable weighting for the translati on candidates across different translation resources. Confidence estimation was originally used in speech recognition and understanding (Hazen and al, 2002). It has been applied to improve recognition by incorpora ting extra information into the recognition process. In CLIR, query translation is performed with many resources for the same word. We then have the same Confidence for a translation is de fined as the posterior probability that this translation is correct P(C=1|X) , given X  X  the source word, a translation and a set of relative features. We use a Multi Layer Perceptron (MLP) to estimate the probability of correctness P(C=1|X) of a translation. The MLP was trained to minimize the negative log-likelihood (or cross entropy CE) assigned to the test corpus by the model normalized by the number of examples in the test corpus [1]. We selected intuitively seven classes of features hypothesized to be informative for the correctness of a translation. These features are: index of the translation s ource, translation probabilities and reverse translation probabilities, rank of the translation, source sentence-related features such as the frequency of the source word in the source sentence, language model features (unigram language model for source and targ et words on the training data) and Part-Of-Speech (POS) tags of both source word and translation candidate (i.e. lexical tag probabilities). The corpus used to train confidence is extracted from two parallel corpora: The Hansard corpus and the Web corpus. The former is composed of debates in the Canadian parliament, while the latter is automatically gathered from the Web [2]. It consists of around 60 K pairs of aligned sentences. Source sentences are translated word by word using baseline m odels (two STMs and a BD). We translated each source word with the five most probable translations for the STMs and all the translations provided by the BD. Translations are then compar ed to the reference sentence to build a labelled corpus: if a candidate translation appears in the reference translation sentence, then it is considered to be correct. We test with various numbers of hidden units (from 0 to 100). We used the Normalized Cross Entropy (NCE) to compare the performance of different architectures. The NCE measures the relative drop in negative log-likelihood compared to the baseline that depends on the prior probability of correctness. The MLP with 20 hidden units gave the best performance. To test the performance of individual features, we experimented with all features but with one feature removed at once. The best feature is the translation probabilities because when it is removed, we observe the largest decrease in NCE. The translation source, the source sentence-related features and the language model features provide some marginally useful information. In order to validate our confidence model for CLIR, we use English queries to retrieve French documents. We use two document collections: one from TREC6 and another from CLEF (SDA). We use 4 query sets: 3 from TREC (TREC 6, 7, 8) and one from CLEF2000. The query terms are translated with three sources: two STMs trained respectively from the Hansard and the Web corpus and one BD (http://www.freedict.com/). The resulting translations are then submitted to the information retrieval process. We tested with two ways to assign weights to translation candidates: linear combination and confidence estimation. In linear combination, each resource is assigned a coefficient denoting our confidence in it. In confidence estimation, we use confidence estimates as weights for translations instead of original probabilities. According to these 
