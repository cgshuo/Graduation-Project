 University of Minnesota University of Minnesota University of Minnesota
This article describes a framework for incorporating referential semantic information from a world model or ontology directly into a probabilistic language model of the sort commonly usedinspeechrecognition,whereitcanbeprobabilisticallyweightedtogetherwithphonological and syntactic factors as an integral part of the decoding process. Introducing world model referents into the decoding search greatly increases the search space, but by using a single integratedphonological,syntactic,andreferentialsemanticlanguagemodel,thedecoderisableto incrementallyprunethissearchbasedonprobabilitiesassociatedwiththesecombinedcontexts. domainsintheabsenceofexamplein-domaintrainingsentences. 1. Introduction
The capacity to rapidly connect language to referential meaning is an essential aspect of communication between humans. Eye-tracking studies show that humans listening to spoken directives are able to actively attend to the entities that the words in these directives might refer to, even while the words are still being pronounced (Tanenhaus et al. 1995; Brown-Schmidt, Campana, and Tanenhaus 2002). This timely access to referential information about input utterances may allow listeners to adjust their pref-erences among likely interpretations of noisy or ambiguous utterances to favor those that make sense in the current environment or discourse context, before any lower-level disambiguation decisions have been made. This same capability in a spoken language interface system could allow reliable human X  X achine interaction in the idiosyncratic language of day-to-day life, populated with proper names of co-workers, objects, and events not found in broad training corpora. When domain-specific training corpora are not available, a referential semantic interface could still exploit its model of the world: the data to which it is an interface, and patterns characterizing these data. tion from a world model or ontology directly into a statistical language model of the sort commonly used in speech recognition, where it can be probabilistically weighted together with phonological and syntactic factors as an integral part of the decoding process. Introducing world model referents into the decoding search greatly increases the search space, but by using a single integrated phonological, syntactic, and referential semantic language model, the decoder is able to incrementally prune this search based on probabilities associated with these combined contexts.
 tions over time from less constrained referents to more constrained referents. Because it is defined dynamically, interpretation in this framewor kcan incorporate dependencies on referential context X  X or example, constraining interpretations to a presumed set of entities, or a presumed setting X  X hich may be fixed prior to recognition, or dynam-ically hypothesized earlier in the recognition process. This contrasts with other recent systems which interpret constituents only given fixed inter-utterance contexts or explicit syntactic arguments (Schuler 2001; DeVault and Stone 2003; Gornia kand Roy 2004; Aist et al. 2007). Moreover, because it is defined dynamically, in terms of transitions, this context-dependent interpretation framewor kcan be directly integrated into a Viterbi decoding search, like ordinary state transitions in a Hidden Markov Model. The result is a single unified referential semantic probability model which brings several kinds of referential semantic context to bear in speech decoding, and performs accurate recognition in real time on large domains in the absence of example domain-specific training sentences.
 approaches to interleaving semantic interpretation with speech recognition. Section 3 will provide definitions for world models used in semantic interpretation, and language models used in speech decoding, which will form the basis of a referential semantic language model, defined in Section 4. Then Section 5 will describe an evaluation of this model in a sample spoken language interface application. 2. Related Work
Early approaches to incremental interpretation (Mellish 1985; Haddoc k1989) apply semantic constraints associated with each word in a sentence to progressively winnow the set of individuals that could serve as referents in that sentence. These incrementally constrained referents are then used to guide the syntactic analysis of the sentence, dis-preferring analyses with empty interpretations in the current environment or discourse context. Similar approaches were applied to broad-coverage text processing, querying a large commonsense knowledge base as a world model (Martin and Riesbeck 1986). But this winnowing is done deterministically, invoking default assumptions and potentially exponential backtracking when default assumptions fail.
 was later extended to pursue multiple interpretations at once by exploiting polynomial structure-sharing in a dynamic programming parser (Schuler 2001; DeVault and Stone 2003; Gornia kand Roy 2004; Aist et al. 2007). The resulting shared interpretation is similar to underspecified semantic representations (Bos 1996), except that the rep-resentation mainly preserves syntactic ambiguity rather than semantic (e.g., quanti-314 fier scoping) ambiguity, and the size complexity of the parser chart representation is polynomially bounded. This approach was further extended to support hypothetical referents (DeVault and Stone 2003), domains with continuous relations (Gornia kand
Roy 2004), and updates to the shared parser chart by components handling other levels of linguistic analysis in parallel, during real-time recognition (Aist et al. 2007). mapping between syntax and semantics using familiar compositional semantic rep-resentations. But the standard dynamic programming algorithm for parsing derives its complexity bounds from the fact that each recognized constituent can be analyzed independently of every other constituent. These independence assumptions must be relaxed if dynamic context dependencies are to be applied across sibling constituents (e.g., in the package data directory, open ..., where the files to be opened should be restricted to the contents of the package data directory). More importantly, from an engineering perspective, the dynamic programming algorithm for parsing runs in cubic time, not linear, which means this interpretation framewor kcannot be directly applied to continuous audio streams. Interface systems therefore typically perform utterance or sentence segmentation as a stand-alone pre-process, without integrating syntactic or referential semantic dependencies into this decision.
 language models that are pre-compiled into word n -grams for particular discourse or environment states, and swapped out between utterances (Young et al. 1989; Lemon and Gruenstein 2004; Seneff et al. 2004). But in some cases accurate interpretation will require spoken language interfaces to exploit context continuously during utterance recognition, not just between utterances. For example, the probability distribution over the next word in the utterance go to the package data directory and get the ... (or in the packagedatadirectorygetthe... ) will depend crucially on the linguistic and environment context leading up to this point: the meaning of packagedatadirectory in the first part of has been carried out. Moreover, in rich environments pre-compilation to word n -grams can be expensive, since all referents in the world model must be considered to build accurate n -grams. This will not be practical if environments change frequently. 3. Background
In contrast to the approaches described in Section 2, this article proposes an incremental interpretation framewor kwhich is entirely contained within a single-pass probabilistic decoding search. Essentially, this approach directly integrates model theoretic seman-tics, summarized in Section 3.1, with conventional probabilistic time-series models used in speech recognition, summarized in Section 3.2. 3.1 Referential Semantics
Semantic interpretation requires a framewor kwithin which a spea ker X  X  intended mean-ings can be formalized. Sections 3.1.1 and 3.1.2 describe a model theoretic approach to semantic interpretation that will later be extended in Section 4.1. The referential states defined here will then be incorporated into a representation of nested syntactic constituents in a hierarchic time-series model in Section 4.2. Some of the notation introduced here is summarized later in Table 1 (Section 4). 3.1.1 Model Theory. The language model described in this article defines semantic ref-erents in terms of a world model M . In model theory (Tarski 1933; Church 1940), a world model is defined as a tuple M = E ,  X  containing a domain of individuals {  X  ,  X  2 , ... } and an interpretation function  X  to interpret expressions in terms of those individuals. This interpretation function accepts expressions  X  of various types: logical statements, of simple type T T T (for example, the demo file is writable ) which may be true may refer to any individual in the world model; or functors of complex type  X  ,  X  , which take an argument of type  X  and produce output of type  X  . Functor expressions  X  of type  X  ,  X  can be applied to other expressions  X  of type  X  as arguments to yield expressions  X  (  X  )oftype  X  (for example, writable may take thedemofile as an argument and return true). By nesting functors, complex expressions can be defined, denoting (for example, a comparative adjective like larger ). 3.1.2 Ontological Promiscuity. First-order or higher models (in which functors can take sets as arguments) can be mapped to equivalent zero-order models (with functors defined only on entities). This is generally motivated by a desire to allow sets of individuals to be described in much the same way as individuals themselves (Hobbs 1985). Entities in a zero-order model M can be defined from individuals in a higher-order model M  X  by mapping or reifying each set S = {  X  1 set of sets in P ( P ( E M  X  )), etc.) as an entity e S in a new domain preted as zero-order functors in M can be defined directly from relations l as higher-order functors (over sets) in M  X  by mapping each instance of S l subsumption in M  X  can then be defined on entities made from reified sets in to  X  X  S A X  relations over concepts in knowledge representation systems (Brachman and Schmolze 1985).
 as shown in Figure 1, with supersets to the left connecting to subsets to the right. This representation will be used in Section 4 to define weighted transitions over first-order referents in a statistical time-series model of interpretation. 316 3.2 Language Modeling for Speech Recognition
The referential semantic language model described in this article is based on Hierar-chic Hidden Markov Models (HHMMs), an existing extension of the standard Hidden
Markov Model (HMM) language modeling framework used in speech recognition, which has been factored to represent hierarchic information about language structure over time. This section will review HMMs (Section 3.2.1) and Hierarchic HMMs (Sec-tions 3.2.2 and 3.2.3). This underlying framewor kwill then be extended to include random variables over semantic referents in Section 4.2. 3.2.1HMMsandLanguageModels. The model described in this article is a specialization of the HMM framework commonly used in speech recognition (Baker 1975; Jelinek,
Bahl, and Mercer 1975). HMMs characterize speech as a sequence of hidden states h (which may consist of speech sounds, words, or other hypothesized syntactic or se-mantic information), and observed states o t (typically finite, overlapping frames of an audio signal) at corresponding time steps t . A most-probable sequence of hidden states  X  h 1 .. T can then be hypothesized given any sequence of observed states o
Law (Equation 2) and Markov independence assumptions (Equation 3) to define the full probability P ( h 1 .. T | o 1 .. T )astheproductofa Language Model (LM) prior proba-
The initial hidden state h 0 may be defined as a constant. modeled using Weighted Finite State Automata (WFSAs), corresponding to regular expressions. An HMM state h t may then be defined as a WFSA state, or a symbol position in a corresponding regular expression. 3.2.2 Hierarchic HMMs. Language model transitions P  X  LM (  X  structured hidden states  X  t can be modeled using synchronized levels of stacked-up component HMMs in an HHMM (Murphy and Paskin 2001), generalized here as an abstract topology over unspecified random variables  X  and  X  . In this topol-ogy, HHMM transition probabilities are calculated in two phases: a  X  X educe X  phase (resulting in an intermediate, marginalized state  X  t at time step t ), in which compo-nent HMMs may terminate; and a  X  X hift X  phase (resulting in a modeled state  X  in which unterminated HMMs transition, and terminated HMMs are re-initialized from their parent HMMs. Variables over intermediate and modeled states are factored into sequences of depth-specific variables X  X ne for each of D levels in the HHMM hierarchy:
Transition probabilities are then calculated as a product of transition probabilities at each level, using level-specific  X  X educe X   X   X  and  X  X hift X   X  with  X  D + 1 t and  X  0 t defined as constants. In Viterbi (maximum likelihood) decoding, the marginals (sums) in this equation may be approximated using an argmax operator. A graphical representation of the dependencies in this model is shown in Figure 2. 3.2.3 Simple Hierarchic HMMs. The previous generalized definition can be considered a template for factoring HMMs into synchronized levels, using  X  and  X  as parameters.
The specific Murphy X  X askin definition of HHMMs can then be considered a  X  X imple X  instantiation of this template using FSA states for  X  and switching variables for  X  .In
Section 4, this instantiation will be augmented (or further factored) to incorporate addi-tional variables over semantic referents at each depth and time step, without changing the overall topology of the model.
 318 able f d  X  , t  X  X  0 , 1 } and each modeled state variable  X 
FSA state q d  X  , t : there is a transition at the level immediately below d and the stac kelement q final state, and false (equal to 0 ) with probability 1 otherwise: variables at and immediately below the current HHMM level: If there is no final state immediately below the current level (the first case above), it deterministically copies the current FSA state forward to the next time step; if there is a final state immediately below the current level (the second case presented), it transitions the FSA state at the current final (the third case presented), it re-initializes this state given the state at the level above, according to the distribution  X  Simple-Init . The overall effect is that higher-level
HMMs are allowed to transition only when lower-level HMMs terminate. An HHMM therefore behaves like a probabilistic implementation of a pushdown automaton (or  X  X hift X  X educe X  parser) with a finite stack, where the maximum stack depth is equal to the number of levels in the HHMM hierarchy.
 weighted FSA (WFSA) states or symbol positions in regular expressions, except that some states can be nonterminal states , which introduce corresponding sub-expressions or sub-WFSAs governing state transitions at the level below. The process of expanding each nonterminal state q d  X  1  X  , t to a sub-expression or WFSA (with start state q modeled in  X  Simple-Init . Transitions to adjacent (possibly final) states within each expression or WFSA are modeled in  X  Simple-Trans . ( q  X  , t ), and subphone ( q may be a position in a sequence of phones corresponding to a word, and a subphone state may be a position in a sequence of subphone states (e.g., onset, middle, and end) corresponding to a phone. In this case,  X  Simple-Init would define a prior model over words at level 1, a pronunciation model of phone sequences for each word at level 2, and a state-sequence model of subphone states for each phone at level 3; and  X  would define a word bigram model at level 1, and would deterministically advance along phone and subphone sequences at levels 2 and 3 (Bilmes and Bartels 2005). plementation of a cascaded FSA, used for modeling syntax in information extraction systems such as FASTUS (Hobbs et al. 1996). 4. A Referential Semantic Language Model A referential semantic language model can now be defined as an instantiation of an
HHMM (as described in Section 3.2), interpreting directives in a reified world model (as described in Section 3.1). This interpretation framewor kis novel in that it is defined dynamically in terms of transitions over referentialstates  X  X vocations of entity referents from a (e.g., first-order) world model X  X tacked up in a Hierarchic HMM. This allows (1) a straightforward fast implementation of semantic interpretation (as transition) that is compatible with conventional time-series models used in speech recognition; and (2) a broader notion of semantic composition that exploits referential context in time order (from previous constituents to later constituents) as well as bottom-up (from component constituents to composed constituents).
 a time-series model. Then Section 4.2 will apply these transitions to nested referents in a Hierarchic HMM. Section 4.3 will introduce a state-based syntactic representa-tion to lin kthis semantic representation with recognized words. Finally, Section 4.4 will demonstrate the expressive power of this model on some common linguistic constructions.
 particular, from formal semantics and statistical time-series modeling), a notation summary is provided in Table 1. 4.1 Dynamic Relations
Semantic interpretation may be easily integrated into a probabilistic time-series model if type at adjacent time steps. In other words, while relations in an ordinary Montagovian interpretation framewor k(Montague 1973) may be functions from entity referents to truth value referents, all relations in the world model defined here must be transition functions from entity referents to entity referents.
 preceding, unconstrained referents to referents constrained by l . The unconstrained referents can be thought of as context arguments: For example, in the context of the set of user-writable files, a property like E XECUTABLE evokes the subset of writable executables. In the subsumption lattice shown in Figure 1, this will define a rightward transition from each set referent to some subset referent, labeled with the traversed relation (see Figure 4 in Section 4.4). 320  X  : an individual  X   X  ,  X  : the type of a function from type  X  to type  X  (variables over types)  X  : a complex variable occurring in the reduce phase of processing;  X  : a complex variable occurring in the shift phase of processing;
 X  : a probability model mapping variable values to probabilities type of multi-source transition, distinguishing one argument of an original, ordinary relation l  X  as an output (destination) and leaving the rest as input (source); then intro-ducing a context referent as an additional input. Instead of defining simple transition arcs on a subsumption lattice, n -ary relations more accurately define hyperarcs ,with multiple source referents: zero or more conventional arguments and one additional con-text referent, leading to a destination referent intersectively constrained to this context. straints to be applied that occur prior to hypothesized constituents, in addition to those that occur as arguments. For example, in the sentence go to the package data directory andhidetheexecutablefile, the phrase gotothepackagedatadirectory provides a powerful constraint on the referent of theexecutablefile, although it does not occur as an argument sub-constituent of this noun phrase. In this framework, the referent of the package data the interpretation of theexecutablefile.
 higher-order model M  X  . The referential semantic language model described in this arti-cle interacts with this reified world model M through queries of the form l M ( e where l is a relation, e S 1 is an argument referent, and e context referent if there is no argument). Each query returns a destination referent e such that S is a subset of the context set in the original world model context-dependent relations l in M are then defined in terms of corresponding ordinary relations l  X  of various types in the original world model where relation products are defined to resemble matrix products:
The first case in Equation (12) casts this as a transition from an argument set S the set of individuals within S 1 that are executable. On the other hand, a relation like another individual, it would return true if the relation holds over the pair. The second case in Equation (12) casts this as a transition from a set of containers S set S 2 , to the subset of this context that are contained by an individual in S a set of individuals and then another individual, it would return true if the individual belongs to the (singleton) set of things that are the largest in the argument set. The last case in Equation (12) casts this as a transition from a set S the (singleton) subset of this context that are members of S individuals in S 1 . More detailed examples of each relation type in Equation (12) are provided in Section 4.4.
 sense that relations like C APTAIN that are traditionally one-place (denoting a set of enti-ties with ran kcaptain) are now two-place, dependent on an argument superconcept in the subsumption lattice. Relations can therefore be given different meanings at different places in the world model: in the context of a particular football team, C APTAIN will refer to a particular player; in the context of a different team, it will refer to someone else. One-place relations can still be defined using a subsumption lattice root concept 322  X   X  as a context argument of course, but this will increase the perplexity (number of choices) at the root concept, making recognition less reliable.
 cate Logic (Groenendijk and Stokhof 1991), except that only limited working memory for information states is assumed, containing only one referent (or variable binding in
DPL terms) per HHMM level. 4.2 Referential Semantic HHMM
Like the simple HHMM described in Section 3.2.3, the referential semantic language model described in this article (henceforth RSLM), is defined by instantiating the gen-eral HHMM  X  X emplate X  defined in Section 3.2.2. This RSLM instantiation incorporates both the switching variables f  X  X  0 , 1 } and FSA state variables q of the simple HHMM, and adds variables over semantic referents e to the  X  X educe X  and  X  X hift X  phases at each level. Thus, the RSLM decomposes each HHMM reduce variable  X  subsuming an intermediate referent e d  X  , t and a final-state switching variable f decomposes each HHMM shift variable  X  d t into a joint variable subsuming a modeled referent e d  X  , t and an ordinary FSA state q d  X  , t : A graphical representation of this referential semantic language model is shown in Figure 3.
 of compositional semantics (Frege 1892), in which meanings of composed constituents (at higher levels in the HHMM hierarchy) are derived from meanings of component constituents (at lower levels in the hierarchy). However, in addition to the referents of their component constituents, the intermediate referents in this framewor kare also constrained by the referents at the same depth in the previous time step X  X he referen-tial context described in Section 4.1. The modeled referents e correspond to a snapshot at each time step of the referential state of the recognizer, after all completed constituents have been composed (or reduced), and after any new constituents have been introduced (or shifted).
 in duce X  and  X  X hift X  HHMM operations via label functions L  X  map FSA states q to relation labels l .
 vious FSA state q d t -1 using a reduce relation l d  X  , t Reduce probabilities at each level (instantiating  X   X  as  X  vides a non-trivial constraint only when q d  X  , t is a final state; otherwise it returns an I DENTITY relation such that I DENTITY M ( e , e ) = e .
 rent FSA state q d  X  , t using a shift relation l d  X  , t Shift probabilities at each level (instantiating  X   X  as  X  labels using a  X  X escription X  model  X  Ref-Init , with referents e conditioned on (or deterministically dependent on) these labels. The probability distri-bution over modeled variables is therefore I DENTITY relation such that I DENTITY M ( e , e ) = e . The probability models  X   X  Syn-Init are induced from corpus observations or defined by hand.
 there is no final state immediately below the current level, referents and FSA states are simply propagated forward. In the second case, where there is a final state immediately below the current level, referents are propagated forward and the FSA state is advanced 324 final and must be re-initialized, a new referent and FSA state are chosen by: 1. selecting, according to a  X  X escription X  model  X  Ref-Init 2. deterministically generating a referent e d  X  , t given this label and the referent 3. selecting, according to a  X  X exicalization X  model  X  Syn-Init 4.3 Associating Semantic Relations with Syntactic Expressions
In this framework, semantic referents are constrained over time by instances of seman-tions between syntactic and semantic random variable values can be represented in expansion rules of the form where q q q 1 ... q q q n may be any regular expression initiating at state q q q and end with reduce relations. This is in order to keep the syntactic and referential semantic expansions synchronized.
 a context free grammar (CFG). However, unlike CFGs, HHMMs have memory limits on nesting, in the form of a maximum depth D beyond which no expansion may take place. As a result, the expressive power of an HHMM is restricted to the set of regular languages, whereas CFGs may recognize the set of context-free languages; and HHMM recognition is worst-case linear on the length of an utterance, whereas CFG recognition is cubic. 5 Similar limits have been proposed on syntax in natural languages, motivated by limits on short term memory observed in humans (Miller and Chomsky 1963;
Pulman 1986). These have been applied to obtain memory-limited parsers (e.g., Marcus 1980), and depth-limited right-corner grammars that are equivalent to CFGs, except that they restrict the number of internally recursive expansions allowed in recognition (Schuler and Miller 2005). 4.4 Expressivity The language model described herein defines referential semantics purely in terms of
HHMM shift and reduce operations over referent entities, made from reified sets of individuals in some original world model. This section will show that this basic model is sufficiently expressive to represent many commonly occurring linguistic phenomena, including intersective modifiers (e.g., adjectives like executable ), multi-argument rela-tions (e.g., prepositional phrases or relative clauses, involving trajector and landmark referents), negation (as in the adverb not ), and comparatives over continuous properties (e.g., larger ). 4.4.1Properties. Properties (traditionally unary relations like E can be represented in the world model as labeled edges l t defined by intersecting the set e t  X  1 with the set l t M satisfying the property l a reified world model can be cast as a subsumption lattice as described in Section 3.1.2.
The result of conjoining a property l with a context set e can therefore be found by downward traversal of an edge in this lattice labeled l and departing from e . traversing a R EAD -O NLY relation from the set of executables, or by traversing an E X -
O NLY  X  E XECUTABLE or E XECUTABLE  X  R EAD -O NLY from e . The resulting set may then serve as context for subsequent traversals. Property relations may also result in self-(e.g., D ATA F ILE  X  W RITABLE ). Property relations like E XECUTABLE can be defined using the dynamic relations in the first case of Equation (12) in Section 4.1, which simply ignore the non-context argument.
 phrase (NP) expansion using the following regular expression (where l l l relation labels constraining referents at the beginning and end of the NP):
NP  X  Det Adj 326 in which referents are successively constrained by the semantics of relations associated with adjective and noun expansions: (and are also constrained by the prepositional phrase (PP) and relative clause (RC) modifiers, as described below). Here the relation E XECUTABLE traverses from refer-4.4.2n-aryRelations. Sequences of properties (traditionally unary relations) can be inter-preted as simple nonbranching paths from referent to referent in a subsumption lattice, but higher-arity relations define more complex paths that for kand rejoin. For example, the referent of thedirectorycontainingtheexecutable in Figure 5 would be reachable only by: 1. storing the original set of directories e { d 4. traversing the inverse C ONTAIN of relation C ONTAIN to obtain the
This  X  X orking X  of referential semantic paths is handled via syntactic recursion: one path is explored by the recognizer while the other waits on the HHMM hierarchy (essentially functioning as a stack). A sample template for branching reduced relative clauses (or prepositional phrases) that exhibit this forking behavior can be expressed as below: where the inverse relation C ONTAIN is applied when the NP expansion concludes or reduces (when the forked paths are re-joined). Relations like C ONTAIN are covered in the second case of Equation (12) in Section 4.1, which define transitions from sets of individuals associated with the other argument of this relation, in the presence of a context set, which is a superset of the destination. The calculation of semantic tran-sition probabilities for n -ary relations thus resembles that for properties, except that the probability term associated with the relation l  X  and the inverse relation l depend on both context and argument referents (to its left and below it, in the HHMM hierarchy).
 even though there are two executable files in the world model used in these examples.
This illustrates an important advantage of a dynamic context-dependent (three referent) model of semantic composition over the strict compositional (two referent) model. In a dynamic context model, theexecutablefile is interpreted in the context of the files that are contained in a directory. In a strict compositional model, theexecutablefile is interpreted only in the context of fixed constraints covering the entire utterance, and the constraints related to the relation containing are applied only to the directories. This means that a generative model based on strict composition will assign some probability to an infi-nitely recursive description thedirectoriescontainingexecutablescontainedbydirectories...
In generation systems, this problem has been addressed by adding machinery to keep trac kof redundancy (Dale and Haddoc k1991). But in this framewor k, a description erent at the end of each departing labeled transition will be able to disprefer referential transitions that attempt to constrain already singleton referents, or that provide only trivial or vacuous (redundant) constraints in general. This solution is therefore more in line with graph-based models of generation (Krahmer, van Erk, and Verleg 2003), except that the graphs proposed here are over reified sets rather than individuals, and the goal is a generative probability model of language rather than generation per se. 4.4.3 Negation. Negation can be modeled in this framewor kas a relation between sets.
Although it does not require any syntactic memory, negation does require referential semantic memory, in that the complement of a specified set must be intersected with writable portion of this description should be negated.
 and is applied to a world model in Figure 6. Relations like N OT are covered in the third case of Equation (12) in Section 4.1, which define transitions between sets in an original 4.4.4 Comparatives, Superlatives, and Subsective Modifiers. Comparatives (e.g., larger ), 328 set) define relations from sets to sets, or from sets to individuals (singleton sets). They can be handled in much the same way as negation. Here the context is provided from previous words and from sub-structure, in contrast to DeVault and Stone (2003), which define the context of a comparative either from fixed inter-utterance constraints or as the referent of the portion of the noun phrase dominated by the comparative (in addition to inter-utterance constraints). One advantage of dynamic (time-order) constraints is that implicit comparatives ( in the Clark directory, select the file that is larger, with no complement) can be modeled with no additional machinery. If substructure context is not needed, then no additional HHMM storage is necessary.
 and is applied to a world model in Figure 7. Relations like L ARGEST are also covered in the third case of Equation (12), which defines transitions between sets in an original 5. Evaluation in a Spoken Language Interface
Much of the motivation for this approach has been to develop a human-like model of language processing. But there are practical advantages to this approach as well. One of the main practical advantages of the referential semantic language model described { in this article is that it may allow spoken language interfaces to be applied to content-creation domains that are substantially developed by individual users themselves. Such domains may include scheduling or reminder systems (organizing items containing idiosyncratic person or event names, added by the user), shopping lists (containing idiosyncratic brand names, added by the user), interactive design tools (containing new objects designed and named by the user), or programming interfaces for home or small business automation (containing new actions, defined by the user). Indeed, computers are frequently used for content creation as well as content browsing; there is every reason to expect that spoken language interfaces will be used this way as well. content-creation domains is that the vocabulary of possible proper names that users may add or invent is vast. Interface vocabularies in such domains must allow new words to be created, and once they are created, these new words must be incorpo-rated into the recognizer immediately, so that they can be used in the current context.
The standard tactic of training language models on example sentences prior to use is not practical in such domains X  X xcept for relatively skeletal abstractions, example sentences will often not be available. Even very large corpora gleaned from Internet documents are unlikely to provide reliable statistics for users X  made-up names with contextually appropriate usage, as a referential semantic language model provides. a means of improving accessibility to computers for disabled users. These domains also provide an ideal proving ground for a referential semantic language model, because directives in these domains mostly refer to a world model that is shared by the user and the interfaced application, and because the idiosyncratic language used in such domains makes it more resistant to domain-independent corpus training than other domains. In contrast, domains such as database query (e.g., of airline reservations), dictation, or information extraction are less likely to benefit from a referential semantic language model, because the world model in such domains is not shared by either the speaker (in database query) or by the interfaced application (in dictation or information extraction), 7 or because these domains are relatively fixed, so the expense of maintaining linguistic training corpora in these domains can often be justified.
 semantic language model as a spoken language interface in a very basic content-creation domain: that of a file organizer, similar to a Unix shell. model on this domain will be evaluated in large environments containing thousands of entities; more than will fit on the beam used in the Viterbi decoding search in this implementation.
 the effect on recognition time and accuracy of using a referential semantic language model to recognize common types of queries, generated by an experimenter and read by several speakers. A thorough evaluation of the possible coverage of this kind of system on spontaneous input (e.g., in usability experiments) would require a rich syntactic representation and attention to disfluencies and speech repairs which are beyond the scope of this article (see Section 6). 330 5.1 Ontology Navigation Test Domain
To evaluate the contribution to recognition accuracy of referential semantics over that of syntax and phonology alone, a baseline (syntax only) and test (baseline plus referential semantics) recognizer were run on sample ontology manipulation directives in a  X  X tu-dent activities X  domain. This domain has the form of a simple tree-like taxonomy, with some cross-listings (for example, students may be listed in homerooms and in activities). directories) can be mapped to reified world models of the sort described in Section 3.1.2.
Concepts C in such an ontology define sets of individuals described by that concept: {  X  |
C (  X  ) } . Subconcepts C of a concept C then define subsets of individuals: {  X  |
C (  X  ) } . These sets and subsets can be reified as referent entities and arranged on a subsumption lattice as described in Section 3.1.2. A sample taxonomic ontology is shown in Figure 8a (tilted on its side to match the subsumption lattices shown elsewhere in this article). Thus defined, such ontologies can be navigated using referent transitions described in Section 4.1 by entering concept referents via  X  X ownward X  (rightward in the figure) transitions, and leaving concept referents via  X  X pward X  (leftward) transitions.
For example, this ontology can be manipulated using directives such as: which are incrementally interpreted by transitioning down the subsumption lattice (e.g., to sports ).
 other referents e at the same level of the ontology as the most recently described refer-ent e , or at higher levels of the ontology than the most recently described entity, should be semantically accessible without restating the ontological context (the path from the root concept e )sharedby e and e . Thus, in the context of having recently referred to someone in Homeroom 2 at a particular campus in a school activities database, other students in the same homeroom or other activities at the same campus should be accessible without giving an explicit backup directive at each branch in the ontology.
To see the value of implicit upward transitions, compare Example (1) to a directive that makes upward transitions explicit using the keyword back (similar to  X .. X  in the syntax of
Unix paths) to exit the homeroomtwo and Clark folders: or if starting from the Duluth campus sports football directory: itly composed with downward transitions, resulting in transitions from source e destination e S via some ancestor e S
The composed transition function finds a referent e S then finds an ordinary (downward) transition l connecting e transition to every immediate child of an ancestor a referent (or in genealogical terms, to every sibling, ancestor, and sibling of ancestor), making these contextually salient concepts immediately accessible without explicit back-stepping (see Figure 8b).
Equation (12) in Section 4.1. 5.2 Scaling to Richer Domains
Although navigation in this domain is constrained to tree-like graphs, this domain tests all of the features of a referential semantic language model that would be required 332 in richer domains. As described in Section 4, rich domains (in particular, first-order domains, in which users can describe sets of individuals as referents) are mapped to transition edges on a simple graph, similar to the tree-like graphs used in this ontology.
In first-order domains, the size of this graph may be exponential on the number of individuals in the world model. But once the number of referents exceeds the size of the decoder beam, the time performance of the recognizer is constrained not by the number of entities in the world model, but by the beam width and the number of outgoing relations (labels) that can be traversed from each hypothesis. In a first-order system, just as in the simple ontology navigation system evaluated here, this number of relations is constrained to the set of words defined by the user up to that point. In both cases, although the interface may be used to describe any one of an arbitrarily large set of referents, the number of referents that can be evoked atthenexttimestep is bounded by a constant.
 quired to calculate sets of individuals or hypothetical planner states that result from a transition may be nontrivial, because it may not be possible in such domains to retain the entire referent transition model in memory. In first-order domains, for example, this may require evaluating certain binary relations over all pairs of individuals in the world model, with time complexity proportional to the square of the size of the world model domain. Fortunately the model described herein, like most generative language models, hypothesizes words before recognizing them. This means a recognizer based on this model will be able to compute transitions that might follow a hypothesized word during the time that word is being recognized. If just the current set of possible transitions is known (say, these have already been pre-fetched into a cache), the set of outgoing transitions that will be required at some time following one of these current transitions can be requested as soon as the beginning of this transition is hypothesized X  X s soon as any word associated with this transition makes its way onto the decoder beam.
From this point, the recognizer will have the entire duration of the word to compute (in parallel, in a separate thread, or on a separate server) the set of outgoing transitions that may follow this word. In other words, the model described herein may be scaled to richer domains because it is amenable to parallelization. 5.3 World Model
The student activities ontology used in this evaluation is a taxonomic world model defined with upward and downward transitions as described in Section 5.1. It organizes extracurricular activities under subcategories (e.g., offense organizes students into homerooms, in which context they can be identified by a single (first or last) name. Every student or activity is an entity e in the set of entities relations l are subcategory labels or student names. 5.3.1 World Model M 240 . In the original student activities world model of 240 entities were created in E : 158 concepts (groups or positions) and 82 instances (students), each connected via a labeled arc from a parent concept.
 possible to calculate a meaningful perplexity statistic for transitions in this model, assuming all referents are equally likely to be a source. The perplexity of this world model (the average number of departing arcs) is 16.79, after inserting  X  X P X  arcs as described in Section 5.1. 5.3.2WorldModel M 4175 . An expanded version of the students ontology, 4,175 entities from 717 concepts and 3,458 instances. This model contains subgraph, so that the same directives may be used in either domain; but it expands
M 240 from above, with additional campuses and schools, and below, with additional students in each class. The perplexity of this world model was 37.77, after inserting  X  X P X  arcs as described in Section 5.1. 5.4 Test Corpus A corpus of 144 test sentences (no training sentences) was collected from seven native
English speakers (5 male, 2 female), who were asked to make specific edits to the student activities ontology described previously. The subjects were all graduate students and native speakers of English, from various parts of the United States. The edit directives were recorded as isolated utterances, not as part of an interactive dialogue, and the target concepts were identified by name in written prompts, so the corpus has much of the character of read speech. The average sentence length in this collection is 7.17 words. 5.5 Acoustic Model
Baseline and test versions of this system were run using a Recurrent Neural Network (RNN) acoustic model (Robinson 1994). This acoustic model performs competitively with multi-state triphone models based on multivariate Gaussian mixtures, but has the advantage of using only uniphones with single subphone states. As a result, less of the
HMM trellis beam is occupied with subphone variations, so that a larger number of semantically distinct hypotheses may be considered at each frame.
 read speech (Fisher et al. 1987). This corpus yields several thousand examples for each of the relatively small set of single-state uniphones used in the RNN model. Read speech is also appropriate training data for this evaluation, because the test subjects are constrained to perform fixed edit tasks given written prompts, and the number of reasonable ways to perform these tasks is limited by the ontology, so hesitations and disfluencies are relatively rare. 5.6 Phone and Subphone Models
The language model used in these experiments is decomposed into five hierarchic levels, each with referent e and ordinary FSA state q components, as described in
Section 4.2. The top three levels of this model represent syntactic states as q (derived from regular expressions defined in Section 4.3) and associated semantic referents as e . The bottom two levels represent pronunciation and subphone states as q , and ignore e . associated with a word via a pronunciation model. The pronunciation model used in these experiments is taken from the CMU ARPABET dictionary (Weide 1998). Transi-tions across subphone states are defined in terms of sequences of subphones associated with a phone. Because this evaluation used an acoustic model trained on the TIMIT corpus (Fisher et al. 1987), the TIMIT phone set was used as subphones. In most cases, these subphones map directly to ARPABET phones, so each subphone HMM consists of
HMM consists of a stop subphone (e.g., bcl ) followed by a burst subphone (e.g., b ). 334
Referents are ignored in both the phone and subphone models, and therefore do not need to be calculated.
 vance along a sequence of phones in a pronunciation; and initial phone sequences de-pend on words in higher-level syntactic states q 3  X  , t , via a pronunciation model  X 
The student activities domain was developed with no synonymy X  X nly one word de-scribes each semantic relation. Alternate pronunciations are modeled using a uniform distribution over all listed pronunciations.
 current time step and the subphone at the previous time step. This model was trained directly using relative frequency estimation on the TIMIT corpus itself: 5.7 Syntax and Reference Models
The three upper levels of the HHMM comprise the syntactic and referential portion of the language model. Concept error rate tests were performed on three baseline and test versions of this portion of the language model, using the same acoustic, phone, and subphone models, as described in Sections 5.5 and 5.6. 5.7.1LanguageModel  X  LM-Sem . First, the syntactic and referential portion of the language model was implemented as described in Section 4.2. A subset of the regular expres-sion grammar appears in Figure 9. Any nondeterminism resulting from disjunction or Kleene-star repetition in the regular expressions was handled in  X  distributions over all available following states. Distributions over regular expression expansions in  X  Syn-Init were uniform over all available expansions. Distributions over that were compatible with the FSA state category generated by  X  5.7.2 Language Model  X  LM-NoSem . Second, in order to evaluate the contribution of refer-ential semantics to recognition, a baseline version of the model was tested with all relations defined to be equivalent to N IL , returning e at each depth and time step, with all relation labels reachable in M from e . This has the effect of eliminating all semantic constraints from the recognizer, while preserving the relation labels of the original model as a resource from which to calculate concept error rate. The decoding equations and grammar in Model  X  LM-NoSem are therefore the same as in Model  X  only the domain of possible referents is restricted.
  X  5.7.3 Language Model  X  LM-Trigram . Finally, the referential semantic language model (Lan-guage Model  X  LM-Sem ) was compiled into a word trigram model, in order to test how well the model would function as a pre-process to a conventional trigram-based speech recognizer. This was done by iterating over all possible sequences of hidden state transitions starting from every possible configuration of referents and FSA states on a stac kof depth D (where D = 3): first search using  X  LM-NoSem . Then every combination of three referents from hypothesized as a possible referent configuration. A complete set of possible initial values for  X  t  X  2 was then filled with combinations from the set of syntactic category configuration crossed with the set of referent configurations. From each possible  X   X 
LM-Sem was consulted to give a distribution over  X  t  X  1 (assuming a word-level transition occurs, with f 4  X  , t  X  1 = 1 ), and then again from each possible configuration of  X  a distribution over  X  t (again assuming a word-level transition). The product of these transition probabilities was then calculated and added to a trigram count, based on the words w t  X  2 , w t  X  1 ,and w t occurring in  X  t  X  2 ,  X  normalized over w t  X  2 and w t  X  1 to give P ( w t | w t 5.8 Results
The following results report Concept Error Rate (CER), as the sum of the percentages of insertions, deletions, and substitutions required to transform the most likely sequence of relation labels hypothesized by the system into the hand-annotated transcript, ex-pressed as a percentage of the total number of labels in the hand-annotated transcript.
Because there are few semantically unconstrained function words in this domain, this is essentially word error rate, with a few multi-word labels (e.g., firstchair,homeroomtwo ) concatenated together. 5.8.1 Language Model  X  LM-Sem and World Model M 240 . Results using Language Model  X  LM-Sem with the 240-entity world model ( 336
Here the size of the vocabulary was roughly equal to the number of referents in the world model. The sentence error rate for this experiment was 59.44%. 5.8.2 Language Model  X  LM-Sem and World Model M 4175 . With the number of entities (and words) increased to 4,175 ( M 4175 ), the CER increases slightly to 19.9% (Table 3). Here again, the size of the vocabulary was roughly equal to the number of referents in the world model. The sentence error rate for this experiment was 62.24%. Here, the use of a world model (Language Model  X  LM-Sem ) with no linguistic training data is comparable to that reported for other large-vocabulary systems (Seneff et al. 2004; Lemon and
Gruenstein 2004), which were trained on sample sentences. 5.8.3LanguageModel  X  LM-NoSem withnoWorldModel. In comparison, a baseline using only the grammar and vocabulary from the students domain M 240 information and no linguistic training data (Language Model  X  (Table 4). 9 The sentence error rate for this experiment was 93.01%.
  X  LM-Sem ( p &lt; 0 . 01 using pairwise t-test against Language model  X  grouping scores by subject), suggesting that syntactic constraints are poor predictors of concepts without considering reference. But this is not surprising: because the grammar by itself does not constrain the set of ontology labels that can be used to construct a path, the perplexity of this model is 240 (reflecting a uniform distribution over nearly the entire lexicon), whereas the perplexity of M 240 is only 16.79. 5.8.4 Language Model  X  LM-Trigram and World Model M 240 . In order to test how well the model would function as a pre-process to a conventional trigram-based speech recog-nizer, the referential semantic language model (Language Model  X  into a word trigram model. This word trigram language model (Language Model  X 
LM-Trigram ), compiled from the referential semantic model (in the 240-entity domain), shows a concept error rate of 26.6% on the students experiment (Table 5). The sentence error rate for this experiment was 66.43%.
 significant increases in error over Language Model  X  LM-Sem pairwise t-test, grouping scores by subject), showing that referential context is also more predictive than word n -grams derived from referential context. Moreover, the compilation to trigrams required to build Language Model  X  (requiring several hours of pre-processing) because it must consider all combinations of entities in the world model. This would make the pre-compiled model impractical in mutable domains.
 338  X   X   X   X  5.8.5 Summary of Results. Results in Table 6 summarize the results of the four experiments.
 plausible edits: for example, making one student a subset of another student. Domain information or meta-data could eliminate some of these kinds of errors, but in content-creation applications it is not always possible to provide this information in advance; that users would want to manage it themselves, or allow it to be automatically induced without supervision. 10 In any case, the comparison described in this section to a non-semantic model  X  LM-NoSem suggests that the world model by itself is able to apply useful constraints in the absence of domain knowledge. This suggests that, in an interpolated approach, direct world model information may relieve some of the burden on authored or induced domain knowledge to perform robustly, so that this domain knowledge may be authored more sparsely or induced more conservatively than it otherwise might. beam width of 1,000 hypotheses per frame. Differences in runtime performance were minimal, even between the simple trigram model and HHMM-based referential seman-tic language models. This was due to two factors: 1. All recognizers were run with the same beam width. Although it might 2. The implementation of the Viterbi decoder used in these experiments was 5.8.6 Statistical Significance vs. Magnitude of Gain. The experiments described in this article show a statistically significant increase in accuracy due to the incorporation of referential semantic information into speech decoding. But these results should not be interpreted to demonstrate any particular magnitude of error reduction (as might be claimed for the introduction of head words into parsing models, for example). a relatively small corpus (6,000 utterances), which introduces the possibility that the acoustic model was under-trained. As a result, the error rates for both baseline and test systems may be greater here than if a larger training corpus had been used, so the performance gain due to the introduction of referential semantics may be overstated. straints (a tree-like ontology, with a perplexity of about 17 for wea ksyntactic constraints (allowing virtually any sequence of relation labels, with a much higher perplexity of about 240), in order to highlight differences due to referential semantics. In general use, recognition accuracy gains due to the incorporation of ref-erential semantic information will depend crucially on the relative perplexity of the referential constraints combined with syntactic constraints, compared to that of syntac-tic constraints alone. This paper has argued that in content-creation applications this difference can be manipulated and exploited X  X n fact, by reorganizing folders into a binary branching tree (with perplexity 2), a user could achieve nearly perfect speech recognition X  X ut in applications involving fixed ontologies and purely hypothetical directives, as in database query applications, gains may be minimal or nonexistant. 6. Conclusion and Future Work
This article has described a referential semantic language model that achieves recogni-tion accuracy favorably comparable to a pre-compiled trigram baseline in user-defined domains with no available domain-specific training corpora, through the use of ex-plicit hypothesized semantic referents. This architecture requires that the interfaced application make available a queryable world model, but the combined phonological, syntactic, and referential semantic decoding process ensures the world model is only queried when necessary, allowing accurate real time performance even in large domains containing several thousand entities.
 als), making transition functions over referents equivalent to expressions in first-order logic. This framewor kcan be extended to model other kinds of references (e.g., to time intervals or events) by casting them as individuals (Hobbs 1985).
 strained by quantifiers: for example, thedirectorycontainingtwofiles. Because its referents are reified sets, the system can naturally model relations that are sensitive to cardinality
But a dynamic view of the referential semantics of nested quantifiers requires referents to be indexed to particular iterations of quantifiers at higher levels of nesting in the
HHMM hierarchy (corresponding to higher-scoping quantifiers). Extending the system to dynamically interpret nested quantifiers therefore requires that all semantic opera-tions preserve an  X  X teration context X  of nested outer-quantified individuals for each inner-quantified individual. This is left for future work.
 ample, toy in toy guns, which are not actually guns; or old in old friends, who are not necessarily elderly (Peters and Peters 2000) X  X nvolve referents corresponding to second-order sets (this allows these adjectives to be composed before being applied to anoun: oldbutcasualfriend ). Unfortunately, extending the framewor kdescribed in this article to use a similarly explicit representation of second-or higher-order sets would be impractical. Not only would the number of possible second-or higher-order sets be exponentially larger than the number of possible first-order sets (which is already 340 exponential on the number of individuals), but the length of the description of each referent itself would be exponential on the number of individuals (whereas the list of individuals describing a first-order referent is merely linear).
 interesting extensions to hypothetical reasoning and planning beyond the standard closed-world model-theoretic framework, however. Recall the sentence gotothepackage hidetheexecutablefiles, exemplifying the continuous context-sensitivity of the referential semantic language model. Here, the system focuses on the contents of this directory because a sequence of transitions resulting from the combined phonological, syntactic, and referential semantic context of the sentence led it to this state. One may characterize the referential semantic transitions leading to this state as a hypothetical sequence of changedirectory actions moving the active directory of the interface to this directory (for the purpose of understanding the consequences of the first part of this directive). The hypothesized context of this directory is then a world state or planning state resulting from these actions. Thus characterized, the referential semantic decoder is performing a kind of statistical plan recognition (Blaylock and Allen 2005). By viewing referents as world states, or as having world-state components, it would then be possible to use logical conclusions of other types of actions as implicit constraints X  X .g., unpack the tar file and hide the executable [which will result from this unpacking]  X  X ithout adding extra functionality to the recognizer implementation. Similarly, referents for hypothetical world model when the user describes them.
 to dynamically generate referents not in the current world model. The domain of referents in this extended system is therefore unbounded. Fortunately, as mentioned in Section 5.2, the number of referents that can be generated at each time step is still bounded by a constant, equal to the recognizer X  X  beam width multiplied by the num-ber of traversable relation labels. This means that distributions over outgoing relation labels are still well-defined for each referential state. The only difference is that, when modeling hypothetical referents, these distributions must be calculated dynamically. of referential semantics to speech recognition decisions. Ordinarily this is thought of as being mediated by syntax, which is covered in this article only through a rela-tively simple framewor kof bounded recursive HHMM state transitions. However, the bounded HHMM representation used in this paper has been applied (without seman-tics) to rich syntactic parsing as well, using a transformed grammar to minimize stack usage to cases of center-expansion (Schuler et al. 2008). Coverage experiments with this transformed grammar demonstrated that over 97% of the large syntactically annotated
Penn Treeban k(Marcus, Santorini, and Marcin kiewicz 1994) could be parsed using only three elements of stac kmemory, with four elements giving over 99% coverage. This suggests that the relatively tight bounds on recursion described in this paper might be expressively adequate if syntactic states are defined using this kind of transform. ing speech repairs, in which speakers repeat or edit mistakes in their directives: for example, selectthered,uh,thebluefolder (Miller and Schuler 2008). The resulting system models incomplete disfluent constituents using transitions associated with ordinary fluent speech until the repair point (the uh in the example), then processes the speech repair using only a small number of learned repair reductions. Coverage results for the same transform model on the Penn Treeban kSwitchboard Corpus of transcribed spontaneous speech showed a similar three-to four-element memory requirement. If this HHMM speech repair model were combined with the HHMM model of referen-tial semantics described in this article, referents associated with ultimately disfluent constituents could similarly be recognized using referential transitions associated with ordinary fluent speech until the repair point, then reduced using a repair rule that discards the referent. These results suggest that an HHMM-based semantic framework such as the one described in this article may be psycholinguistically plausible. Acknowledgments References 342
