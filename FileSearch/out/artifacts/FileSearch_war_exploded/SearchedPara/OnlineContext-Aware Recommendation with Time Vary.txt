 Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular con-text, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely hold-s in practice, since real-world problems often involve underlying processes that are dynamically evolving over time.

In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the map-ping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively cap-ture the context change and learn the latent parameters. In addi-tion, those learnt parameters can be naturally integrated into exist-ing multi-arm selection strategies such as LinUCB and Thompson sampling . Empirical studies on two real-world applications, in-cluding online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The ex-perimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate.
 Recommender System; Personalization; Time Varying Contextual Bandit; Probability Matching; Particle Learning
Online personalized recommender systems strive to promptly feed the consumers with appropriate items (e.g., advertisements, news articles) according to the current context including both the consumer and item content information, and try to continuously maximize the consumers X  satisfaction in the long run. To achieve this goal, it becomes a critical task for recommender systems to track the consumer preferences instantly and to recommend the in-teresting items to the users from a large item repository.
However, identifying the appropriate match between the con-sumer preferences and the target items is quite difficult for recom-mender systems due to several existing challenges in practice [18]. One is the well-known cold-start problem since a significant num-ber of users/items might be completely new to the system, that is, they may have no consumption history at all. This problem makes recommender systems ineffective unless additional informa-tion about both items and users is collected [9][7]. Second, both the popularity of item content and the consumer preferences are dynamically evolving over time. For example, the popularity of a movie usually keeps soaring for a while after its first release, then gradually fades away. Meanwhile, user interests may evolve over time.

Herein, a context-based exploration/exploitation dilemma is i-dentified in the aforementioned setting. A tradeoff between two competing goals needs to be considered in recommender systems: maximizing user satisfaction using the consumption history, while gathering new information for improving goodness of match be-tween user preference and items [16]. This dilemma is typically formulated as a contextual multi-armed bandit problem where each arm corresponds to one item. The recommendation algorithm de-termines the strategies for selecting an arm to pull according to the contextual information at each trial. Pulling an arm indicates the corresponding item is recommended. When an item matches the user preference (e.g., a recommended news article or ad is clicked), a reward is obtained; otherwise, there is no reward. The reward information is fed back to the algorithm to optimize the strategies. The optimal strategy is to pull the arm with the maximum expected reward with respect to the contextual information on each trial, and then to maximize the total accumulated reward for the whole series of trials.

Recently, a series of algorithms for contextual multi-armed ban-dit problems have been reported with promising performance under different settings, including unguided exploration (e.g.,  X  -greedy [26] and epoch-greedy [15]) and guided exploration (e.g., LinUCB [16] and Thompson Sampling [8]). These existing algorithms take the contextual information as the input and predict the expected reward for each arm, assuming the reward is invariant under the same con-text. However, this assumption rarely holds in practice since the real-world problems often involve some underlying processes that are dynamically evolving over time and not all latent influencing factors are included in the context information. As a result, the ex-pected reward of an arm is time varying even though the contextual information does not change.
A Motivated Example: Here we use a news recommendation example to illustrate the time varying behaviors of the reward. In the example, the click through rate (abbr., CTR) and the news arti-cles correspond to the reward and the arms, respectively. Five news articles are randomly selected and their corresponding user-article interaction records are extracted from the Yahoo! news reposito-ry [17, 16]. The context consists of both the user and article infor-mation. Although the context information of each article does not change, its average CTR varies dynamically over time as shown in Figure 1. The same contextual information may have different impacts on the CTR at different times. Figure 1: Given the same contextual information for each article, the average CTR distribution of five news articles from Yahoo! news repository is displayed. The CTR is aggregated by every hour.
In this paper, to capture the time varying behaviors of the re-ward in contextual multi-armed bandit problems, we propose a dy-namical context drift model based on particle learning and devel-op effective on-line inference algorithms. The dynamic behaviors of the reward is explicitly modeled as a set of random walk parti-cles. The fully adaptive inference strategy of particle learning al-lows our model to effectively capture the context change and learn the latent parameters. In addition, the learnt parameters can be nat-urally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling . We conduct empirical studies on two real-world applications, including online personal-ized advertising and news recommendation and the experimental results demonstrate the effectiveness of our proposed approach.
The rest of this paper is organized as follows. In Section 2, we describe a brief summary of prior work relevant to the contextual multi-armed bandit problem and the online inference with particle learning. We formulate the problem in Section 3. The solution to the problem is presented in Section 4. Extensive empirical e-valuation results are reported in Section 5. Finally, we reach the conclusion in Section 6.
In this paper, we come up with a context drift model to deal with the contextual multi-armed bandit problem, where the dynam-ic behaviors of reward is explicitly considered. A sequential online inference method is developed to learn the latent unknown param-eters and infer the latent states simultaneously. In this section, we highlight existing literature studies that are related to our proposed approach for online context-aware recommendation.
Our work is primarily relevant to the research area in the multi-armed bandit problem which was first introduced in [22]. The multi-armed bandit problem is identified in diverse applications, such as online advertising [20, 14], web content optimization [21, 1], and robotic routing [4]. The core task of the multi-armed ban-dit problem is to balance the tradeoff between exploration and ex-ploitation. A series of algorithms have been proposed to deal with this problem including  X  -greedy [26], upper confidence bound (UCB) [5, 19], EXP3 [3], and Thompson sampling [2].

Contextual multi-armed bandit problem is an instance of bandit problem, where the contextual information is utilized for arm se-lection. It is widely used for personalized recommendation service to address the cold-start problem [9]. Lots of existing multi-armed bandit algorithms have been extended to incorporating the contextual information.

Contextual  X  -greedy algorithm has been introduced by extending the  X  -greedy strategy with the consideration of context [5]. This algorithm chooses the best arm based on current knowledge with the probability 1  X   X  , while chooses one arm uniformly with the probability  X  .

Both LinUCB and LogUCB algorithms extend the UCB algo-rithm to contextual bandits [5, 19]. LinU CB assumes a linear mapping function between the expected reward of an arm and its corresponding context. In [19], the LogUCB algorithm is proposed to deal with the contextual bandit problem based on logistic regres-sion.

Thompson sampling [8], one of earliest heuristics for the ban-dit problem, belongs to the probability matching family. Its main idea is to randomly allocate the pulling chance according to the probability that an arm gives the largest expected reward given the context.

A most recent research work on the contextual bandit problem in [25] comes up with a novel parameter-free algorithm based on a principled sampling approach. This approach makes use of the on-line bootstrap sample to derive the distribution of estimated models in an on-line manner. In [24], an ensemble strategies combined with a meta learning paradigm is proposed to stabilize the output of contextual bandit algorithms.

These existing algorithms make use of the contextual informa-tion to predict the expected reward for each arm, with the assump-tion that the reward is invariant under the same context. However, this assumption rarely holds in real applications. Our paper pro-poses a context drift model to deal with the contextual multi-armed bandit problem by taking the dynamic behaviors of reward into ac-count.
Our proposed model makes use of sequential online inference to infer the latent state and learn unknown parameters. Popular sequential learning methods include sequential monte carlo sam-pling [12], and particle learning [6].
 Sequential Monte Carlo (SMC) methods consist of a set of Monte Carlo methodologies to solve the filtering problem [11]. It provides a set of simulation based methods for computing the posterior dis-tribution. These methods allow inference of full posterior distribu-tions in general state space models, which may be both nonlinear and non-Gaussian.

Particle learning provides state filtering, sequential parameter learning and smoothing in a general class of state space model-s [6]. Particle learning is for approximating the sequence of filter-ing and smoothing distributions in light of parameter uncertainty for a wide class of state space models. The central idea behind particle learning is the creation of a particle algorithm that directly samples from the particle approximation to the joint posterior dis-tribution of states and conditional sufficient statistics for fixed pa-rameters in a fully-adapted resample -propagate framework. We borrow the idea of particle learning for both latent state infer-ence and parameter learning.
In this section, we formally define the contextual multi-armed bandit problem first, and then model the time varying contextual multi-armed bandit problem. Some important notations mentioned in this paper are summarized in Table 1.

Notation Description a ( i ) the i -th arm.

A the set of arms, A = { a (1) , ..., a ( K ) } . x t the context at time t , and represented by a vec-r k;t the reward of pulling the arm a ( k ) at time t , y k;t the predicted reward for the arm a ( k ) at time t .
P k the set of particles for the arm a ( k ) and P ( i )
S ;t the sequence of ( x i ,  X  ( x i ) , r ( x i ) ) observed w k the coefficient vector used to predict reward of c w k the constant part of w k .  X  w ;t the drifting part of w k at time t .  X  k;t the standard Gaussian random walk at time t ,  X  k the scale parameters used to compute  X  w ;t .  X  the policy for pulling arm sequentially.

R the cumulative reward of the policy  X  . f a ( k ) ( x t ) the reward prediction function of the arm a  X  2 k the variance of reward prediction for the arm  X  ,  X  the hyper parameters determine the distribution  X  w , w the hyper parameters determine the distribution  X  c , c the hyper parameters determine the distribution  X  , the hyper parameters determine the distribution  X  , the hyper parameters determine the distribution
Let A be a set of arms, denoted as A = { a (1) , a (2) ..., a where K is the number of arms. A d -dimensional feature vector x t  X  X represents the contextual information at time t , and is the d -dimensional feature space. The contextual multi-armed problem involves a series of decisions over a finite but possibly unknown time horizon T . A policy  X  makes a decision at each time t  X  [1 , T ] to select the arm  X  ( x t ) , one of K arms, to pull based on the contextual information x t . After pulling an arm, the policy receives a reward from the selected arm. The reward of an arm a ( k ) at time t is denoted as r k;t , whose value is drawn from an unknown distribution determined by the context x t presented to arm a ( k ) . However the reward r k;t is not available unless arm a is pulled. The total reward received by the policy  X  is and the goal of the policy  X  is to maximize the total reward R .
Before selecting one arm at time t , a policy  X  typically learns a model to predict the reward for every arm according to the histori-cal observation, S ;t  X  1 = { ( x i ,  X  ( x i ) , r ( x i consists of a sequence of triplets. The reward prediction helps the policy  X  make decisions to increase the total reward.

Assume y k;t is the predicted reward of the arm a ( k ) , which is determined by where the context x t is input and f a ( k ) is the reward mapping func-tion for arm a ( k ) .

One popular mapping function is defined as the linear combina-tion of the feature vector x t , which has been successfully used in bandit problems [16][2]. Specifically, f a ( k ) ( x t ) is formally given as follows: where x | t is the transpose of contextual information x t d -dimensional coefficient vector, and  X  k is a zero-mean Gaussian noise with variance  X  2 k , i.e.,  X  k s N (0 ,  X  2 k ) . Accordingly, In this setting, a graphical model representation is provided in Fig-ure 2a. The context x t is observed at time t . The predicted reward value y k;t depends on random variable x t , w k , and  X  2 gate prior distribution for the random variables w k and  X  sumed and defined as NIG (i.e., Normal Inverse Gamma) distribu-tion with the hyper parameters  X  w , w ,  X  , and  X  . The distribution is denoted as NIG (  X  w , w ,  X ,  X  ) and shown below: where the hyper parameters are predefined.

A policy  X  selects one arm a ( k ) to pull according to the reward prediction model. After pulling arm a ( k ) at time t , a corresponding reward r k;t is observed, while the rewards of other arms are stil-sequence S ;t is formed by combining S ;t  X  1 with the new triplet. The posterior distribution of w k and  X  2 k given S ;t is a bution. Denoting the parameters of NIG distribution at time t are updated as follows:  X   X   X 
Note that, the posterior distribution of w k and  X  2 k at time t is considered as the prior distribution at time t + 1 . On the basis of the aforementioned inference, a series of algorithms, including Thompson sampling and LinUCB, are proposed to address the con-textual multi-armed bandit problem.
Thompson sampling , one of earliest heuristics for the bandit problem [8], belongs to the probability matching family. Its main idea is to randomly allocate the pulling chance according to the probability that an arm gives the largest expected reward given the context. Thompson sampling algorithm for the contextual multi-armed bandit problem involves the following general structure at time t : 1. For each arm a ( k ) , its corresponding  X  2 k and w k 2. The arm a  X  is selected to pull, and a reward of a  X  is obtained, 3. After observing the reward r a ;t , the posterior distribution is
LinUCB , another successful contextual bandit algorithm, is an extension of the UCB algorithm [16]. It pulls the arm with the largest score LinU CB (  X  ) , defined as below,
LinU CB (  X  ) = x | t  X  w t 1 | {z } where  X  is a parameter to combine the expectation and standard deviation of reward.

Both LinUCB [16] and Thompson Sampling [8] will be incor-porated into our dynamic context drift model. More details will be discussed in Section 4 after modeling the context drift.
As mentioned in Section 3.1, the reward prediction for arm a is conducted by a linear combination of contextual features x coefficient vector w k . Each element in the coefficient vector w indicates the contribution of the corresponding feature for reward prediction. The aforementioned model is based on the assumption that w k is unknown but fixed [2], which rarely holds in practice. The real-world problems often involve some underlying processes. These processes often lead to the dynamics in the contribution of each context feature to the reward prediction. To account for the dynamics, our goal is to come up with a model having the capability of capturing the drift of w k over time and subsequently obtain a better fitted model for the dynamic reward change. Let w k;t
Note that most exiting works for Thompson sampling assume  X  is known and w k is drawn from N (  X  w t 1 , w t 1 ) the coefficient vector for arm a ( k ) at time t . Taking the drift of w into account, w k;t is formulated as follows: where w k;t is decomposed into two components including both the stationary component c w k and the drift component  X  w components are d -dimensional vectors. Similar to modeling w Figure 2a, the stationary component c w k can be generated with a conjugate prior distribution where  X  c and c are predefined hyper parameters as shown in Fig-ure 2b.

However, it is difficult to model the drift component  X  w single function due to the diverse characteristics of the context. For instance, in Figure 1, given the same context, the CTRs of some articles change quickly, while some articles may have relatively stable CTRs. Moreover, the coefficients for different elements in the context feature can change with diverse scales. To simplify the inference, we assume that each element of  X  w k;t drifts indepen-dently. Due to the uncertainty of drifting, we formulate  X  a standard Gaussian random walk  X  k ; t and a scale variable  X  the following Equation: where  X  k ; t  X  X  d is the drift value at time t caused by the standard random walk and  X  k  X  X  d contains the changing scales for all the elements of  X  w k;t . The operator  X  is used to denote the element-wise product. The standard Gaussian random walk is defined with a Markov process as shown in Equation 10. where v is a standard Gaussian random variable defined by v N ( 0 , I d ) , and I d is a d  X  d -dimensional identity matrix. It is e-quivalent that  X  k ; t is drawn from the Gaussian distribution
The scale random variable  X  k is generated with a conjugate prior distribution where  X  and are predefined hyper parameters.  X  2 k is drawn from the Inverse Gamma (abbr., IG ) distribution provided in E-quation 4.
Combining Equations 7 and 9, we obtain
According to Equation 2, y k;t is computed as Accordingly, y k;t is modeled to be drawn from the following Gaussian distribution
The new context drift model is presented with a graphical model representation in Figure 2b. Compared with the model in Figure 2a, a standard Gaussian random walk  X  k;t and the corresponding scale  X  for each arm a ( k ) are introduced in the new model. The new model explicitly formulates the drift of the coefficients for the re-ward prediction, considering the dynamic behaviors of the reward in real-world application. From the new model, each element val-ue of c w k indicates the contribution of its corresponding feature in predicting the reward, while the element values of  X  k scales of context drifting for the reward prediction. A large ele-ment value of  X  k signifies a great context drifting occurring to the corresponding feature over time.
In this section, we present the methodology for online inference of the context drift model.

The posterior distribution inference involves four random vari-in Figure 2b, the four random variables are grouped into two cat-egories: parameter random variable and latent state random vari-able.  X  2 k , c w k ,  X  k are parameter random variables since they are assumed to be fixed but unknown, and their values do not depend on the time. Instead,  X  k ; t is referred to as a latent state random variable since it is not observable and its value is time dependent according to Equation 10. After pulling the arm a ( k ) according to the context x t at time t , a reward is observed as r k;t r k;t are referred to as observed random variables. Our goal is to in-fer both latent parameter variables and latent state random variables to sequentially fit the observed data. However, since the inference partially depends on the random walk which generates the latent state variable, we use the sequential sampling based inference s-trategy that are widely used sequential monte carlo sampling [23], particle filtering [10], and particle learning [6] to learn the distribu-tion of both parameter and state random variables.

Since state  X  k ; t  X  1 changes over time with a standard Gaussian random walk, it follows a Gaussian distribution after accumulat-ing t  X  1 standard Gaussian random walks. Assume  X  k ; t  X  N (  X  k ,  X  k ) , a particle is defined as follows.

D EFINITION 1 (P ARTICLE ). A particle of an arm a ( k ) container which maintains the current status information of a The status information comprises of random variables such as  X  c k ,  X  k , and  X  k ; t , and the parameters of their corresponding dis-tributions such as  X  and  X  ,  X  c and c ,  X  and ,  X  k and k
At time t  X  1 , each arm a ( k ) maintains a fixed-size set of parti-cles. We denote the particle set as P k;t  X  1 and assume the number of particles in P k;t  X  1 is p . Let P ( i ) k;t  X  1 be the i a ( k ) at time t  X  1 , where 1  X  i  X  p . Each particle P ( i ) weight, denoted as  X  ( i ) , indicating its fitness for the new observed data at time t . Note that cle P ( i ) k;t  X  1 is defined as the likelihood of the observed data x r k;t . Therefore, Further, y k;t is the predicted value of r k;t . The distribution of y Therefore, we can compute  X  ( i ) in proportional to the density value given y k;t = r k;t . Thus, where state variables  X  k;t and  X  k;t  X  1 are integrated out due to their change over time, and c w k ,  X  k ,  X  2 k are from P ( i ) obtain where Before updating any parameters, a re-sampling process is conduct-ed. We replace the particle set P k with a new set P  X  k , where generated from P k using sampling with replacement based on the weights of particles. Then sequential parameter updating is based on
At time t  X  1 , the sufficient statistics for state  X  k;t mean (i.e.,  X  k ) and the covariance (i.e., k ). Provided with the new observation data x t and r k;t at time t , the sufficient statistics for state  X  k;t need to be re-computed. We apply the Kalman fil-tering [13] method to recursively update the sufficient statistics for  X  k;t based on the new observation and the sufficient statistics at time t  X  1 . Let  X   X  state  X  k;t at time t . Then, where Q k is defined in Equation 18 and G k is Kalman Gain [13] defined as As shown in Equation 19, both  X   X  a correction using Kalman Gain G k (i.e., the last term in both two formulas). With the help of the sufficient statistics for the state random variable,  X  k;t can be draw from the Gaussian distribution
At time t  X  1 , the sufficient statistics for the parameter random variables (  X  2 k , c w k ,  X  k ) are (  X  ,  X  ,  X  c , c ,  X  , ). and  X  k = ( c w k | ,  X  k | ) | where z t ,  X  , and  X  are 2 d -dimensional v ector, is a 2 d  X  2 d -dimensional matrix. Therefore, the infer-ence of c w k and  X  k is equivalent to infer  X  k with its distribution  X  k s N (  X ,  X  2 k ) . Assume tatistics at time t which are updated based on the sufficient statistics at time t  X  1 and the new observation data. The sufficient statistics for parameters are updated as follows: At time t , the sampling process for  X  2 k and  X  k is summarized as follows:
As discussed in Section 3.1, both LinUCB and Thompson sam-pling allocate the pulling chance based on the posterior distribution of w k and  X  2 k with the hyper parameters  X  w , w ,  X  , and  X .
As to the context drifting model, when x t arrives at time t , the reward r k;t is unknown since it is not observed until one of arms is pulled. Without observed r k;t , the particle re-sampling, latent state inference, and parameter inference for time t can not be conducted. Furthermore, every arm has p independent particles. Within each particle, the posterior distributions of w k;t  X  1 are not available since w k;t  X  1 has been decomposed into c w k ,  X  k , and  X  k;t  X  Equation 13. We address these issues as follows.

Within a single particle of arm a ( k ) , the distribution of w can be derived by where i ( th ) particle. We use the mean of w k;t  X  1 , denoted as w infer the decision in the bandit algorithm. Therefore, where By virtual of Equation 25, both Thompson sampling and Lin-UCB can address the bandit problem as mentioned in Section 3.1. Specifically, Thompson sampling draws w k;t from Equation 25 and then predicts the reward for each arm with w k;t . The arm with maximum predicted reward is selected to pull. While LinUCB s-elects arm with a maximum score, where the score is defined as a combination of the expectation of y k;t and its standard deviation, i.e., where  X  is predefined parameter, E ( y k;t | x t ) and V ar ( y computed by
Putting all the aforementioned things together, an algorithm based on the context drifting model is provided below.
 Algorithm 1 The algorithm for context drift model ( Drift ) 1: pr ocedure MAIN ( p )  X  main entry 2: Initialize arms with p particles. 3: for t  X  1 , T do 4: Get x t . 6: Receive r k;t by pulling arm a ( k ) . 7: UPDATE( x t , a ( k ) , r k;t ). 8: end for 9: end procedure 10: procedure EVAL ( a ( k ) , x t )  X  get a score for a ( k ) 11: Learn the parameters based on all particles X  inferences of 12: Compute a score based on the parameters learnt. 13: return the score. 14: end procedure 15: procedure UPDATE ( x t , a ( k ) , r k;t )  X  update the inference. 16: for i  X  1 , p do  X  Compute weights for each particle. 17: Compute weight  X  ( i ) of particle P ( i ) k by Equation 17. 18: end for 19: Re-sample P  X  k from P according to the weights  X  ( i ) 20: for i  X  1 , p do  X  Update statistics for each particle. 21: Update the sufficient statistics for  X  k;t by Equation 19. 22: Sample  X  k;t according to Equation 20. 23: Update the statistics for  X  2 k , c w k ,  X  k by Equation 21. 24: Sample  X  2 k , c w k ,  X  k according to Equation 22. 25: end for 26: end procedure
Online inference for contextual multi-armed bandit problem s-tarts with MAIN procedure, as presented in Algorithm 1. As x arrives at time t , the EVAL procedure computes a score for each arm, where the definition of score depends on the specific policy. The arm with the highest score is selected to pull. After receiving a reward by pulling an arm, the new feedback is used to update the contextual drifting model by the UPDATE procedure. Especially in the UPDATE procedure, we use the resample -propagate strat-egy in particle learning [6] rather than the propagate -resample strategy in particle filtering [10]. With the resample -propagate strategy, the particles are re-sampled by taking  X  ( i ) as the i ticle X  X  weight, where the  X  ( i ) indicates the occurring probability of the observation at time t given the particle at time t resample -propagate strategy is considered as an optimal and ful-ly adapted strategy, avoiding an importance sampling step.
To demonstrate the efficacy of our proposed algorithm, we con-duct our experimental study over two real-world data sets including the online search advertising data from Track 2 of KDD Cup 2012, and the news recommendation data of Yahoo! Today News. Be-fore diving into the detail of the experiment on each data set, we first outline the general implementation of the baseline algorithms for comparison. Second, we start with a brief description of the data sets and their corresponding evaluation methods. We finally show and discuss the comparative experimental results of both the proposed algorithm and the baseline algorithms.
In the experiment, we demonstrate the performance of our method by comparing with the following algorithms. The baseline algo-rithms include: 1. Random : it randomly selects an arm to pull without consid-2.  X  -greedy (  X  ) (or EPSgreedy ): it randomly selects an arm 3. GenUCB (  X  ): it denotes the general UCB algorithm for con-4. TS ( q 0 ): Thompson sampling described in Section 3.1, ran-5. TSNR ( q 0 ): it is similar to TS ( q 0 ), but in the stochastic gradi-6. Bootstrap : it is non-Bayesian but an ensemble method
Our methods proposed in this paper include: 1. TVUCB (  X  ): it denotes the time varying UCB which integrates 2. TVTP ( q 0 ): it denotes the time varying Thompson sampling
Online advertising has become one of the major revenue sources of the Internet industry for many years. In order to maximize the Click-Though Rate (CTR) of displayed advertisements (ads), on-line advertising systems need to deliver these appropriate ads to individual users. Given the context information, sponsored search which is one type of online advertising will display a recommend-ed ad in the search result page. Practically, an enormous amount of new ads will be continuously brought into the ad pool. These new ads have to be displayed to users, and feedbacks have to be collected for improving the system X  X  CTR prediction. Thereby, the problem of ad recommendation can be regarded as an instance of contextual bandit problem. In this problem, an arm is an ad, a pull is an ad impression for a search activity, the context is the informa-tion vector of user profile and search keywords, and the reward is the feedbacks of user X  X  click on ads.

The experimental dataset is collected by a search engine and pub-lished by KDD Cup 2012 2 . In this dataset, each instance refers to an interaction between a user and the search engine. It is an ad im-pression, which consists of the user demographic information (age and gender), query keywords, some ads information returned by the search engine and click count on ads. In our work, the context is represented as a binary feature vector of dimension 1,070,866, in-cluding query entry and user X  X  profile information. And each query entry denotes whether a query token is contained in the search query or not. In the experiments, we use 1 million user visit events.
We use a simulation method to evaluate the KDD Cup 2012 on-line ads data, which is applied in [8] as well. The simulation and replayer [17] are two of the frequently used methods for the bandit problem evaluation. As discussed in [8] and [25], the simulation method performs better than replayer method when the item pool contains a large number of recommending items, especially larg-er than 50. The large number of recommending items leads to the CTR estimation with a large variance due to the small number of matched visits.
 In this data set, we build our ads pool by randomly selecting K = 100 ads from the entire set of ads. There is no explicit time stamp associated with each ad impression, and we assume the ad impression arrives in chronological order with a single time unit interval between two adjacent impressions. The context in-formation of these ads are real and obtained from the given data set. However, the reward of the k th ad is simulated with a coef-ficient vector w k;t , which dynamically changes over time. Let  X  be the change probability, where each coefficient keeps unchanged with probability 1  X   X  and varies dynamically with probability  X  . We model the dynamical change as a Gaussian random walk by w k;t = w k;t +  X  w where  X  w follows the standard Gaussian distribution, i.e.,  X  w s N ( 0 , I d ) . Given a context vector x at time t , the click of the k th ad is generated with a probability tial weight vector w k; 0 is drawn from a fixed normal distribution that is randomly generated before the evaluation.
With the help of the simulation method, we get a chance to know the ground truth of the coefficients. Therefore, we first explore the fitness of our model with respect to the true coefficient values over time. Then we conduct our experiment over the whole online ads data set containing 1 million impressions by using the CTR as the evaluation metric.

We simulate the dynamical change of coefficients in multiple d-ifferent ways including the random walk over a small segment of data set shown in Figure 3. Sampling a segment of data containing 120 k impressions from the whole data set, we assume a dynamical change occurring on only one dimension of the coefficient vector, http:/www . kddcup2012.org/c/kddcup2012-track2 Figure 3: A segment of data originated from the whole data set is provided. The reward is simulated by choosing one dimension of the coefficient vector, which is assumed to vary over time in three different ways. Each time bucket contains 100 time units. keeping other dimensions constant. In ( a ) , we divide the whole segment of data into four intervals, where each has a different co-efficient value. In ( b ) , we assume the coefficient value of the di-mension changes periodically. In ( c ) , a random walk mentioned above is assumed, where  X  = 0 . 0001 . We compare our algorithm Drift with the bandit algorithm such as LinUCB with Bayesian linear regression for reward prediction. We set Drift with 5 par-ticles. It shows that our algorithm can fit the coefficients better than Bayesian linear regression and can adaptively capture the dy-namical change instantly. The reason is that, Drift has a random walk for each particle at each time and estimates the coefficient by re-sampling these particles according to their goodness of fitting.
In this section, we evaluate our algorithm over the online ad-s data in terms of CTR. The performance of each baseline algo-rithm listed in Section 5.1 depends on the underlying reward pre-diction model (e.g., logistic regression, linear regression) and its corresponding parameters. Therefore, we first conduct the perfor-mance comparison for each algorithm with different reward pre-diction models and diverse parameter settings. Then the one with best performance is selected to compare with our proposed algo-rithm. The experimental result is presented in Figure 4. The al-gorithm LogBoostrap(10) achieves better performance than LinBootstrap(10) since our simulation method is based on the Logit function.

Although our algorithms TVTP(1) and TVUCB(1) are based on linear regression model, they can still achieve high CTRs and their performance is comparable to those algorithms based on logis-tic regression method such as, LogTS(0.001) , LogTSnr(10) . The reason is that both TVTP and TVUCB are capable of capturing the non-linear reward mapping function by explicitly considering the context drift. The algorithm LogEpsGreedy(0.5) does not perform well. The reason is that the value of parameter  X  is large, incurring lots of exploration.
The core task of personalized news recommendation is to display Figure 4: The CTR of KDD CUP 2012 online ads data is given for each time bucket. LogBooststrap , LogTS , LogTSnr , and LogEpsGreedy are bandit algorithms with logistic regression model. LinUCB , LinBoostrap , TVTP , and TVUCB are based on linear regression model. appropriate news articles on the web page for the users according to the potential interests of individuals. However, it is difficult to track the dynamical interests of users only based on the content. Therefore, the recommender system often takes the instant feed-backs from users into account to improve the prediction of the po-tential interests of individuals, where the user feedbacks are about whether the users click the recommended article or not. Addition-ally, every news article does not receive any feedbacks unless the news article is displayed to the user. Accordingly, we formulate the personalized news recommendation problem as an instance of con-textual multi-arm bandit problem, where each arm corresponds to a news article and the contextual information including both content and user information.

The experimental data set is a collection based on a sample of anonymized user interaction on the news feeds, collected by Ya-hoo! Today module and published by Yahoo! research lab 3 . The dataset contains 28,041,015 visit events of user-news item interac-tion data, collected by the Today Module from October 2nd, 2011 to October 16th, 2011 on Yahoo! Front Page. In addition to the interaction data, user X  X  information, e.g., demographic information (age and gender), behavior targeting features, etc., is provided for each visit event, and represented as a binary feature vector of di-mension 136. Besides, the interaction data is also stamped with the user X  X  local time, which is suitable for contextual recommendation and temporal data mining. This data set has been used for evaluat-ing contextual bandit algorithms in[16][8][17]. In our experiments, 2.5 million user visit events are used.
We apply the replayer method to evaluate our proposal method on the news data collection since the number of articles in the pool is not larger than 50 . The replayer method is first introduced in [17], which provides an unbiased offline evaluation via the histori-cal logs. The main idea of replayer is to replay each user visit to the algorithm under evaluation. If the recommended article by the test-ing algorithm is identical to the one in the historical log, this visit is considered as an impression of this article to the user. The ratio http://webscope.sandbox.yahoo.com/catalog.php between the number of user clicks and the number of impressions is referred as CTR. The work in [17] shows that the CTR estimated by the replayer method approaches the real CTR of the deployed online system if the items in historical user visits are randomly rec-ommended. Figure 5: The CTR of Yahoo! News data is given for each time bucket. Those baseline algorithms are configured with their best parameters settings.

Similar to the CTR optimization for online ads data in Section 5.2.4, we first conduct the performance evaluation for each algorithm with different regression models and parameter settings. The experi-mental result is displayed in Table 2. The setting of each algorithm with the highest reward is highlighted in bold. It can be observed that our algorithm TVUCB(0.5) achieves the best performance a-mong all algorithms. In four of all five parameter  X  settings, the performances of TVUCB consistently exceed the ones of LinUCB .
All baseline algorithms are configured with their best param-eter settings provided by Table 2. We conduct the performance comparison on different time buckets in Figure 5. The algorith-m TVUCB(0.5) and EpsGreedy(0.01) outperforms other-s among the first four buckets, known as cold-start phrase when the algorithms are not trained with sufficient observations. After the fourth bucket, the performance of both TVUCB(0.5) and LinUCB(0.5) constantly exceeds the ones of other algo-rithms. In general, TVTP(1.0) performs better than TS(1.0) and TSNR(100) , where all the three algorithms are based on the Thompson sampling . Overall, TVUCB(0.5) consistently achieves the best performance.
The time cost for TVUCB and TVTP on both two data sets are displayed in Figure 6. It shows that the time costs are increased linearly with the number of particles. In general, TVUCB is faster than TVTP since TVTP highly depends on the sampling process.
In this paper, we take the dynamic behavior of reward into ac-count and explicitly model the context drift as a random walk. We Figure 6: The time costs on different numbers of particles are given for both two data collections. propose a method based on the particle learning to efficiently in-fer both parameters and latent drift of the context. Integrated with existing bandit algorithms, our model is capable of tracking the contextual dynamics and consequently improve the performance of personalized recommendation in terms of CTRs, which is verified in two real applications,i.e., online advertising and news recom-mendation.

The recommend items, e.g., advertisements or news articles, may have some underlying relations with each other. For example, two advertisements may belong to the same categories, or come from business competitors, or have other same features. In the future, we plan to consider the potential correlations among different items, or say, arms. It is interesting to model these correlations as con-straints, and incorporate them into the contextual bandit modeling process. Moreover, the dynamically changing behaviors of two cor-related arms tend to be correlated with a time lag, where the change correlation can be interpreted as an event temporal pattern [27]. Therefore, another possible research direction is to extend our time varying model considering the correlated change behaviors with the time lag.
The work was supported in part by the National Science Founda-tion under grants CNS-1126619, IIS-1213026, and CNS-1461926, the U.S. Department of Homeland Securitys VACCINE Center un-der Award Number 2009-ST-061-CI0001, and a gift award from Huawei Technologies Co. Ltd.
