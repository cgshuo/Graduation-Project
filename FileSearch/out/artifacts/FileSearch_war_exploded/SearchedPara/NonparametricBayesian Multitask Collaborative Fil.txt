 The dramatic rates new digital content becomes available has brought collaborative ltering systems to the epicenter of computer science research in the last decade. One of the greatest challenges collaborative ltering systems are con-fronted with is the data sparsity problem: users typically rate only very few items; thus, availability of historical data is not adequate to effectively perform prediction. To allevi-ate these issues, in this paper we propose a novel multitask collaborative ltering approach. Our approach is based on a coupled latent factor model of the users rating functions, which allows for coming up with an agile information sharing mechanism that extracts much richer task-correlation infor-mation compared to existing approaches. Formulation of our method is based on concepts from the eld of Bayesian non-parametrics, speci cally Indian Buffet Process priors, which allow for data-driven determination of the optimal number of underlying latent features (item characteristics and user traits) assumed in the context of the model. We exper-iment on several real-world datasets, demonstrating both the efficacy of our method, and its superiority over existing approaches.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2 [ Arti cial Intelligence ]: Learn-ing Collaborative ltering, Indian Buffet Process, multitask learn-ing
The continuous explosion in the availability of content through the Internet renders content search a challenging task with ever-increasing difficulty. Ratings-based collab-orative ltering (CF) systems have served as an effective approach to address the problem of discovering items of in-terest [4]. They are based on the intuitive idea that the preferences of a user can be inferred by exploiting past rat-ings of that user as well as users with related behavior pat-terns. This thriving sub eld of machine learning has started becoming popular since the late 1990s with the spread of on-line services that use recommender systems, such as Ama-zon, Yahoo! Music, MovieLens, Net ix, and CiteULike.
Existing CF methods can be classi ed intro three main categories: memory-based methods, model-based methods, and hybrid methods. Memory-based systems generate pre-dictions by exploiting the original ratings matrix: Item rat-ing prediction for a target user comprises determination of a subset of the users of the system with similar ratings to the target user (target user neighbors), and computation of a weighted average of the ratings of each item provided by the target user neighbors. This is the earliest kind of algo-rithms used in the context of CF systems, and still comprises the basis of most of the ltering functionality performed on popular websites such as Amazon [10, 1]. A signi cant draw-back of such approaches is that, given the high sparsity of the ratings matrix, the neighborhood of a target user may contain only few, if any, ratings for a given item. Moreover, such approaches require availability of the whole ratings ma-trix to perform prediction in real-time. This might become too wasteful in terms of computational efficiency if a large number of users/items are registered with the system, thus limiting the scalability of the system.

Model-based CF methods attempt to ameliorate these is-sues by using the available ratings data to construct a model which expresses the rating decision function of the users. As such, model-based CF approaches entail an off-line training procedure. Then, given the trained model, prediction gen-eration becomes extremely efficient, thus affording scalable real-time operation. Among all model-based CF methods, matrix factorization (MF)-based methods are perhaps the most popular ones in recent years [5, 29, 25, 22, 26, 27, 15]. These methods assume that the registered users and items are related to sets of features that lie in some low-dimensional latent space; prediction is performed based on these latent features assigned to each user and item. More recently, several authors have considered application of al-ternative Bayesian latent factor models instead of matrix factorization (e.g., [9, 19]). As it has been shown, such Bayesian latent factor approaches allow for deriving scalable and robust model-based recommender systems with good performance in large datasets.
Fin ally, a number of hybrid CF techniques have also been proposed in the research literature, combining memory-based methods with model-based methods, or utilizing additional information such as content information. Such approaches hope to improve performance even further, by combining the strengths of different paradigms; characteristic examples are the works of [13, 14, 12].

Despite these advances and the great success CF sys-tems have been met with in the last decade, the data spar-sity problem [30], i.e. the difficulties resulting from the ex-tremely sparse nature of the ratings matrix in real-world systems, continues to pose a great challenge to existing CF systems. In an effort to better mitigate the effect of data sparsity on the performance of CF systems, few researchers have recently considered the possibility of combining infor-mation from multiple collaborative ltering tasks pertain-ing to different domains into a single multitask prediction system. The main notion underlying these efforts is the as-sumption that, by jointly modeling a collection of rating prediction tasks arising from multiple domains, CF systems can exploit the correlation between rating prediction prob-lems in different domains to alleviate the effect of data spar-sity. Multitask CF (MCF) systems are capable of extracting shared preference patterns among similar domains, exploita-tion of which allows for improving the obtained rating pre-diction performance in all the jointly modeled domains.
Existing approaches in the area of MCF mostly rely on matrix factorization techniques [34, 28], properly adapted so as to allow for automatically deriving and sharing correlated information across different domains. In this work, we fol-low a different approach: we introduce a novel latent factor model to perform MCF, leveraging the strengths of Bayesian nonparametrics. Speci cally, the rating function of the users in each domain is expressed by means of a two-component latent factor model, with the latent factors expressing the assignment of the users and the available items to latent classes (assignment of latent features), and the associated weights (factor loadings) expressing the joint user/item bi-ases of the model, which can be obtained through model training.

We impose appropriate nonparametric Bayesian priors over the considered latent features, namely Indian Buffet Process (IBP) priors. Such a nonparametric Bayesian model con-struction allows for assuming that each user or item may be associated with multiple latent features. It also allows for the number of latent features to be automatically discov-ered in the context of an efficient Bayesian inference scheme. Knowledge sharing between the jointly modeled tasks is ef-fectively performed by imposing a suitable matrix-variate prior over the domain-speci c model weights (factor load-ings) across all the domains, and deriving the corresponding posterior distributions in the context of the inference algo-rithms of our model. We evaluate the efficacy of our ap-proach by experimenting with several real-world datasets, and compare its performance to the MCF approaches that currently exist in the literature, as well as two related base-line MF-based and Bayesian latent factor model-based CF algorithms.
 The remainder of this paper is organized as follows: In Section 2, we brie y provide the background of our ap-proach. Speci cally, we rst review existing MCF algo-rithms; subsequently, we brie y present the nonparamet-ric Bayesian prior imposed over the latent variables of our model, namely the IBP prior. In Section 3, we introduce our proposed model, and derive its inference and prediction algorithms using a truncated variational Bayesian approach. In Section 4, we experimentally evaluate our method using two real-world datasets. Finally, in Section 5 we summarize our results, and conclude this paper.
Previous work on MCF systems is rather limited. An ap-proach related to our work is the multi-domain CF method of [34]. In that paper, the authors propose a probabilis-tic framework which uses probabilistic matrix factorization (PMF) [26] to model the rating prediction problem in each domain, and allows the extracted knowledge to be adaptively shared across different domains by automatically learning the correlation between domains. In addition to that work, [28] proposed a collective matrix factorization (CMF) ap-proach, closely related to [34]; speci cally, the work of [34] can be shown to reduce to CMF, thus incorporating CMF as a special case, by restricting the latent user feature matrices to be identical across domains.

In addition, an older related approach presented in [17] proposed a rating-matrix generative model to perform multi-task collaborative ltering. This approach establishes the re-latedness across multiple rating matrices by nding a shared implicit cluster-level rating matrix, which is next extended to a cluster-level rating model. On this basis, a rating matrix of any related task can be viewed as drawing a set of users and items from a user-item joint mixture model as well as drawing the corresponding ratings from the cluster-level rat-ing model. A major component of the work of [17] consists in the assumption that some tasks cluster together in a la-tent ratings subspace, and, hence, share common preference patterns, while others do not. However, a major drawback of this method is the absolute lack of a mechanism quantifying how much two tasks are related, and adapting knowledge-sharing based on this information. On the contrary, the ex-isting PMF-based approaches entail mechanisms that allow for achieving this level of exibility in knowledge sharing.
Furthermore, another effort toward effective knowledge sharing between tasks, as a means of mitigating data spar-sity, is the work of [33]. That paper essentially builds upon a previous work on Bayesian matrix factorization [24], where the user and item factor matrices are considered to be drawn from a Dirichlet process mixture prior [21], instead of simple multivariate Gaussians. The method of [33] achieves knowl-edge transfer by sharing model parameters among different tasks, and is fully nonparametric in that the dimension of latent feature vectors is automatically determined. Infer-ence is performed using the variational Bayesian algorithm, similar to our approach, which is much faster than Gibbs sampling used by most related Bayesian methods. A signi -cant downside of this approach is that it relies on parameter tying across tasks to perform knowledge sharing. As such, it does not allow for inferring task relatedness, and adapting the amount of knowledge sharing between any pair of tasks on the basis of their estimated level of relatedness. In con-trast, our approach is explicitly designed to allow for such an adaptive knowledge sharing capacity.

Finally, several researchers have recently considered a re-lated problem, namely transfer learning for collaborative ltering systems (e.g., [18, 23, 20, 16]). The aim of such systems is to allow for improving predictive performance in a new rating problem with very sparse data with the help of other rating problems which have denser rating data. Such an approach is very helpful when dealing with domains where too few ratings are available for all items (e.g., in ad-dressing the cold-start problem) [30]. However, the prob-lem we are considering in this paper, i.e. jointly address-ing the sparsity problem in multiple tasks, is quite differ-ent from the knowledge transfer application scenario, which aims at adaptively transferring existing knowledge only to new tasks, without affecting the model predictive mecha-nism pertaining to the existing tasks.
In many unsupervised learning problems it is necessary to derive a set of latent variables given a set of observations. A characteristic example is collaborative ltering: considering that users are associated with possibly multiple latent traits, and items are associated with possibly multiple latent fea-tures, we may wish to identify these sets of latent properties and determine which users/items have each property. Un-fortunately, most traditional machine learning approaches require the number of latent features as an input. In such cases, usually one has to resort to application of a model se-lection technique to come up with a trade-off between model complexity and model t.
 A solution to this problem is offered in the context of Bayesian nonparametrics. Nonparametric Bayesian approaches treat the number of latent features as a random quantity to be determined as part of the posterior inference procedure. The most common nonparametric prior for latent feature models is the Indian Buffet Process [8]. The IBP is a prior on in nite binary matrices that allows us to simultaneously infer which features in uence a set of observations and how many features there are. The form of the prior ensures that only a nite number of features will be present in any nite set of observations, but more features may appear as more observations are received.

Let us consider a set of N objects that may be assigned to a total of K features. Let Z = [ z nk ] N;K n;k =1 be a N K matrix of assignment variables, with z nk = 1 if the n th object is assigned to the k th feature (multiple z nk 's may be equal to 1 for a given object n ), z nk = 0 otherwise. The IBP imposes a prior over [ Z ], a canonical form of Z that is invariant to the ordering of the features [8]. The imposed prior takes the form Here, m k is the number of objects assigned to the k th fea-ture (s.t. z nk = 1), is the innovation hyperparameter of the IBP prior which regulates the number of effective model features K , H N is the N th harmonic number, and K h is the number of occurrences of the non-zero binary vector h among the columns in Z .

Apart from Markov chain Monte Carlo (MCMC) [8], in-ference for the IBP can also be performed by means of mean-eld variational Bayesian inference methods, which approxi-mate the true posterior via a simpler distribution [11]. Vari-ational Bayesian inference for the IBP is based on an al-ternative formulation of p ( Z ) [7], namely the stick-breaking construction of the IBP [31]: It has been shown that the prior (1) obtained by the IBP can be equivalently expressed under the following hierarchical Bayesian construction In other words, under the stick-breaking construction, an equivalent hierarchical expression for the prior p ( Z ) is ob-tained by introduction of the Beta-distributed stick variables v .
Before we introduce our model, let us rst formally de ne the problem we are aiming to address: Let us consider a recommender system the registered users of which comprise a set N = f n u g U u =1 . Let us also consider that the system produces predictions for D tasks, each one pertaining to a different domain. For each task, the corresponding domain comprises the items set C d = f c d m g M d m =1 , d = 1 ;:::;D . Each user is allowed by the system to provide a rating for each item in each domain; the rating variable r d in domain d is as-sumed to take values in the discrete set f 1 ;:::;R d g of possi-ble ratings. Each user may provide ratings for one or more of the items within each domain. In essence, at any time point, the considered recommender system has available a training ratings dataset D = f ( r d n ;u d n ;m d n ) g D;N d d;n =1 ples from each domain d , consisting of users with indexes u associated ratings r d n 2f 1 ;:::;R d g .

Typically, in real-world scenarios, users provide ratings only for a minuscule fraction of the items within each do-main. However, by exploiting information from different do-mains, and sharing correlated information across domains, it is probable that more (implicitly derived) information will be made available to the predictive models, thus allowing for enhancing model performance in all domains.

Based on this observation, what we aim to achieve in this paper is to derive a multitask learning approach to syner-gistically perform CF across domains. In this paper, we follow an approach inspired from the recent literature on Bayesian nonparametrics. Speci cally, we propose a novel nonparametric Bayesian latent factor model to achieve our ends. Under our approach, the rating function of each user for each task is expressed as a sum of three terms: a user bias, an item bias, and a hierarchical two-component latent factor term, with the latent factors expressing the assign-ment of the users and the available items to latent classes (assignment of latent features), and the factor weights (load-ings) taken as the joint user/item biases of the model, which can be obtained through model training.

We make the assumption that each user may have one or more traits which comprise latent variables of the sought model of unknown number, while each item may belong to o ne or more feature classes which also comprise latent vari-ables of the sought model of unknown number. To extract the underlying task correlation information, and perform in-formation sharing across the considered tasks, we impose a suitable joint prior over the factor weights (loadings) of all tasks, and perform inference of the corresponding posteriors given the training data of our algorithm.

Let us consider the d th task. The rating function r d of a user n u 2 N for item c d m 2 C d is expressed under our approach as follows where denotes the inner product. In the proposed model (5), d is the mean rating for any user/item pair in domain d . d u is the bias of the u th user w.r.t the task of domain d ; a positive value for d u expresses the propensity of the user for giving higher than average ratings to any given item from domain d , while a negative value expresses their propensity for giving lower than average ratings. d m is the bias of the m th item from domain d , and expresses an objective quality measure for the item: a high quality item is more likely to obtain higher than average ratings, even from a user with incompatible latent features; this is expressed by a positive m value. On the contrary, a low quality item is more likely to obtain lower than average ratings, even from a user with very compatible latent features; this is expressed by a nega-tive d m value. Further, x d m = [ x d mg ] 1 g =1 , where the x the latent variables of the items, with x d mg = 1 if the m th item from domain d is assigned to the g th latent feature (multiple feature assignments are possible for each item), x mg = 0 otherwise. Similar, the z d ui are the latent vari-ables of the users, with z d ui = 1 if the u th user is assigned to the i th feature (multiple feature assignments are possible for each user) in the context of the d th task, z d ui = 0 otherwise. Finally, the weights w d i = [ w d ig ] 1 g =1 are the joint user/item biases (factor loadings) that correspond to the combination of the i th latent user feature with the g th latent item feature in the context of the d th task.

The term  X  in (5) stands for the noise component of our model. Here, we consider an additive white noise term, with This selection for the prior of the noise variable  X  is clearly simplistic, since the observed ranking variables r d ( u;m ) are considered to take on a set of discrete values. However, the assumption of an additive white noise has the major advan-tage of allowing for a tractable model inference procedure, with simple model update expressions. As such, in the con-text of our work, we opt for such a simple noise prior se-lection, and take measures to alleviate any negative effects. Speci cally, for this purpose, instead of using the original in-teger ratings for model training, we employ a warping func-tion h ( ) : N ! R , which maps the observed integer rating values to the realm of real numbers, and use the so-obtained values as our observations r d ( u;m ) to perform inference in the context of our model. The employed warping function h ( ) is taken as a parametric function, i.e., h ( ) = h ( parameters of which are randomly initialized, and subse-quently optimized through model training as model hyper-parameters, as we shall explain in Section 3.2.2.
Note that in our model we assume positive factor weights associated with \compatible"' user and item characteristics (latent features), but also negative factor weights for \con-icting" latent features. We also note that, in the above de nition (5) of the proposed model, we have considered an in nite number of latent features. This assumption re ects our unawareness of the exact number of existing latent fea-tures, and demands imposition of an appropriate prior dis-tribution over the latent variables x d m and z d u = [ z so as to conduct Bayesian inference over the number of la-tent features. For this purpose, we adopt a suitable non-parametric Bayesian construction for our model: formula-tion of our model is performed by imposing IBP priors over the latent model variables x d m and z d u . Speci cally, we em-ploy the stick-breaking construction of the IBP, which allows for deriving a computationally efficient and scalable infer-ence algorithm for our model under the variational Bayesian paradigm. We consider and
Further, to conduct Bayesian inference, we also impose prior distributions over the model bias parameters and mean ratings. Speci cally, we consider where W d = [ w d i ] i .

Bayesian inference for such a model consists in derivation of a family of posterior distributions q ( : ) over the in nite sets v d = [ v d ] 1 =1 , ~ v d = [~ v d ] 1 =1 , and W d parently, under this in nite dimensional setting, Bayesian inference is not tractable. For this reason, we employ a com-mon strategy in the literature of Bayesian nonparametrics, formulated on the basis of a truncated stick-breaking repre-sentation of the IBP [31, 6]. That is, we x a value I letting the posterior over the v d i have the property q ( v d I +1 and, similar, we x a value G letting the posterior over the ~ v g have the property q (~ v d G +1 = 0) = 1. In other words, we set the d i and  X  d g equal to zero for i &gt; I and g &gt; G , 8 d , respectively. Note that, under this setting, our model continues to employ two full IBP priors: truncation is not im posed on the model itself, but only on the derived poste-rior distribution to allow for a tractable inference procedure [6]. In our work, the values of the truncation thresholds I and G are set equal to 1,000. This is quite high, and, hence, \close to in nity," since all related models have been shown to yield optimal performance with no more than 20 latent features. In practice, the IBP mechanisms will only retain the small fraction of this large number of latent features that is needed for optimal data modeling (see also [31]).
Finally, to perform multitask learning by deriving and learning the relationships between different domains and tasks, we impose a matrix-variate normal distribution over the joint matrix of the model factor loadings across tasks, i.e., the matrix W  X  [vec( W 1 ) ; vec( W 2 ) ;:::; vec( W where vec( ) stands for the operator which converts a matrix into a vector in a column-wise manner. This kind of mod-eling we adopt in our work re ects our assumption that, in cases of correlated tasks, users as well as items latent fea-tures may share common natural interpretations and similar interdependence patterns among tasks (expressed by the cor-responding factor loadings matrices W d ). This is in contrast to existing PMF-based approaches, which impose joint pri-ors over the latent feature assignments of the users across tasks, thus limiting the spectrum of their considered infor-mation sharing mechanisms only to user features, and ne-glecting the dynamics between user and item latent features (quanti ed by the W d ) which may also follow similar pat-terns among correlated tasks.

Speci cally, we use the prior where denotes the Kronecker product, and MN ( W j A ; B ) stands for the matrix-variate normal distribution with mean A 2 R a b , row covariance matrix B 2 R a a , and column covariance matrix 2 R b b , de ned as MN ( W j A ; B )  X  exp( 1 2 tr( B 1 ( W A ) 1 ( W A ) T )) We envisage the learnt hyperparameter matrix  X  as the component of our model that encodes the relationships be-tween domains. We shall elaborate on this aspect next.
Here, instead of MCMC, we opt for a variational Bayesian inference procedure, due to its better scalability on large datasets. Let us denote as the set of hidden variables and unknown parameters of our model over which a prior distri-bution has been imposed. Let us also denote as the set comprising all the hyperparameters of the imposed priors. Variational Bayesian inference consists in the introduction of an arbitrary distribution q () to approximate the actual posterior p ( j ; D ), which is computationally intractable [2]. The variational posterior q () is obtained by maxi-mization of the variational free energy of the model, which is de ned as [11] Note that L ( q ) comprises a lower bound of the log marginal likelihood (log evidence), log p ( D ), of the model [11].
Due to the considered conjugate prior con guration of our model, the variational posterior q () is expected to take the same functional form as the prior, p () [32]. Derivation of the variational posterior distribution q () involves max-imization of the variational free energy L ( q ) over each one of the factors of q () in turn, holding the others xed, in an iterative manner [2]. In addition, on each iteration, the estimates of the model hyperparameters are also updated. This latter procedure is performed by maximization of the variational free energy L ( q ) over each one of the hyperpa-rameters in , holding the others xed. By construction, this iterative, consecutive updating of the variational posterior distribution and the model hyperparameters is guaranteed to monotonically and maximally increase the free energy L ( q ) [32].
Let us denote as  X  :  X  the posterior expectation of a quantity (i.e., the expectation w.r.t. the variational posterior q () of the model). Optimization of L ( q ) yields the following variational posteriors: 1. Regarding the stick-breaking variables of the employed 2 . Regarding the posterior distributions over the users 3. Regarding the posterior distributions over the items 4. Regarding the joint user/item biases (factor loadings) 5. Regarding the user biases, we have 6. Regarding the item biases, we have 7. The mean ratings d yield
Having obtained the posterior updates q (), we now pro-ceed to derivation of the estimates of the hyperparameters of the model in . For this purpose, we resort to maximization of the variational free energy L ( q ) of our model over each one of them. We begin with the updates of the hyperpa-rameters matrix  X  of the matrix-normal joint prior imposed over the model factor loadings. As we discussed previously, the matrix  X  quanti es the shared information between do-mains. Taking the derivative of L ( q ) over  X  ; and setting equal to zero, we yield which implies This result shows that the proposed model learns the hyper-parameter matrix  X  as the correlation between the factor loading (weight) matrices of our model across tasks. This is a very interesting nding, since it is compatible with our conception of the matrix  X  as a quantity encoding the rela-tions between domains.

Further, for the noise variance hyperparameter 2 , we have Similar are the expressions of the variances of the user and item biases (we omit them for brevity).

Finally, we also need to perform estimation of the param-eters entailed in the non-linear transform h ( j ) employed to optimally map the originally observed integer ratings to the realm of real numbers. For this purpose, we substitute the (transformed into) real rating values r d n in the expres-sion (19) of the variational free energy of our model L ( q ) with the expression of the warping function h ( j ) given the original integer ratings, and optimize the resulting expres-sion over . Here, this optimization is performed by means of the scaled conjugate gradient (SCG) algorithm.
Having derived the inference algorithm for our model, we now proceed to derivation of its prediction algorithm. This consists in using the trained model to estimate the rating a user n u would assign to an item c d m from the d th domain that they have not rated before. For this purpose, we follow a MAP prediction approach: We use as the obtained pre-diction the posterior expectation (mean) of the introduced rating function. From (5), this posterior expectation yields ^ r d ( u;m ) = where  X  As we observe, the predictive function of our model yields a simple and computationally convenient expression, with very low computational costs, linear to the input size. In-deed, these costs are similar to PMF-based approaches, such as [34]. Hence, scalability of our model is very competitive.
Here, we evaluate the efficacy of our approach considering a number of experiments. Speci cally, we perform evalua-tions using two commonly used datasets in the CF literature, namely the MovieLens 100K dataset 1 , dealing with movie ratings, and the Book-Crossing dataset 2 , dealing with book ratings. To evaluate the multitask learning capabilities of our approach, we follow the experimental setup of [34], ex-ploiting the fact that, in both these datasets, the items can be divided into multiple heterogeneous domains. We utilize this feature to obtain multiple CF tasks from these datasets, which are jointly learned using our model.

In our experiments, apart from our method, we also evalu-ate two existing multitask CF methods related to our work, namely the MCF-LF method of [34], and the CMF method of [28]. As a baseline, we also evaluate two conventional CF approaches, namely the PMF method of [26], and the Bayesian latent factor-based BLITR method of [9]. The pa-rameters of the MCF-LF and CMF methods are selected as described in [34]; following the recommendations therein, we set the number of latent features equal to 10 in both these models. The PMF approach is trained with a learning rate of 0.005 and a momentum of 0.9, as described in [26]. We use 10 latent features for this model, similar to [26]. The settings of the BLITR model are adopted from [9]: a Rao-Blackwellised Gibbs sampler is used to draw 300 samples from the Markov chain, with a burn-in of 200 samples. In the case of BLITR we use 50 latent features, based on the ndings and guidelines of [9].

Finally, regarding the warping function h ( j ), we select where ~ r are the originally observed integer ratings, and r the transformed real values used by our model.

To assess the performance of our algorithm and its con-sidered rivals, we use as our evaluation metric the root mean square error (RMSE) of the predictions, which reads h ttp://www.grouplens.org/ http://www.informatik.uni-freiburg.de/~cziegler/BX/ T able 3: Minimum and maximum absolute error and its standard deviation for the evaluated methods.
 Ou r work 0 3 0 .49 0 2 0. 47 MC F-LF 0 3 0 .50 0 2 0. 47 BL ITR 0 4 0 .58 0 5 0. 68 wh ere r n is the true rating for the n th example, and ^ r the rating predicted by the evaluated model. Our source codes were developed in MATLAB R2012b. In our rst experiment, we evaluate our method using the MovieLens 100K dataset. MovieLens 100K is a widely used benchmark for movie recommendation systems. It contains 100,000 ratings of 1,682 movies provided by 943 users of the GroupLens website 3 . The ratings in this dataset take in a set of 10 discrete values. In addition to this rating information, MovieLens 100K dataset also provides genre information about the rated movies.

Based on this dataset, we construct an MCF scenario as follows: We rst determine the ve most popular genres in the dataset. These turn out to be the genres: 'Comedy'; 'Ro-mance'; 'Drama'; 'Action'; and, 'Thriller'. Subsequently, we de ne an MCF problem the tasks of which comprise domains that correspond to the selected ve genres of the dataset. We train the evaluated models using a randomly selected 80% of the rating data from each of the ve tasks, while the rest 20% is kept for testing. To alleviate the effects of random data selection on the obtained performance gures, we repeat our experiments 10 times, with different random training and test set con gurations each time.

In Table 1, we provide the RMSEs obtained by the eval-uated methods (means over the conducted repetitions). As we observe, the considered multitask CF models consistently yield considerably better performance compared to the PMF and BLITR approaches which do not possess multitask learn-h ttp://grouplens.org/node/73 ing capabilities. This nding provides strong indication that taking multiple domains into consideration allows for yield-ing signi cant competitive advantages compared to treat-ing different domains independently. We also observe that CMF always seems to be inferior to MCF-LF; this was ex-pectable enough, since MCF-LF is in essence a generaliza-tion of CMF, and reduces to it by simplifying its assump-tions. Note also that BLITR obtains better performance compared to PMF in this experiment. Finally, we observe that our approach yields a clear advantage over MCF-LF. This nding proves that our Bayesian latent factor model, utilizing two layers of interacting latent factors and two non-parametric Bayesian priors to optimize the latent feature construction of our model, can extract subtler shared pat-terns compared to MCF-LF.
Further, we evaluate our method using the Book-Crossing dataset. Book-Crossing is a public book ratings dataset which pertains to ratings from books available through sev-eral websites. In this experiment we use a subset of this dataset, namely the ratings on books with category informa-tion available on Amazon.com . This subset contains 56,148 ratings of 9,009 books provided by 28,503 users. The pro-vided ratings are in the scale 1{10 with increments of 1.
Based on this dataset, we construct an MCF scenario by considering the ve general book categories in this dataset, namely 'Mystery &amp; Thrillers'; 'Science Fiction &amp; Fantasy'; 'Science'; 'Business &amp; Investing'; and, 'Religion &amp; Spiritu-ality'. Speci cally, we de ne an MCF problem the tasks of which comprise domains that correspond to these ve dataset categories. We train the evaluated models using a randomly selected 80% of the rating data from each task, while the rest 20% is kept for testing. Similar to the previous experiment, to alleviate the effects of random data selection on the obtained performance gures, we repeat our experi-ments 10 times, with different random training and test set con gurations each time.

In Table 2, we provide the RMSEs obtained by the evalu-ated methods (means over the conducted repetitions). As we observe, MCF-LF as well as our approach consistently yield considerably better performance compared to the PMF and BLITR approaches, which do not possess multitask learning Figure 1:  X  matrices obtained by our method: (a) Movie-Lens dataset; (b) Book-Crossing dataset. capabilities. However, contrary to our ndings regarding the MovieLens dataset, the performance of the multitask CMF model is inferior to the considered single-task CF ap-proaches, probably due to its simplistic assumptions. Fi-nally, our method appears to work better than all its com-petitors in all cases.
In real-world recommender systems, apart from the av-erage error expressed by RMSE, another also signi cant quality aspect that determines system attractiveness is error variance. In other words, it is crucial that errors, whenever they (inevitably) happen, are not too large to make sys-tem performance seem way too poor. Indeed, even seldom cases of too poor a result being obtained may irrevocably harm user con dence in the system. Table 3 shows the min-imum and the maximum absolute prediction error obtained by the evaluated methods, as well as its standard deviation, in both the previous experiments. These statistics have been obtained by using the inverse of function h ( j ) in (49) to transform the real ratings predicted by our model to the corresponding discrete ones. As we observe, our approach yields the most competitive results in all cases.
Finally, in Figs. 1a-1b we illustrate the  X  matrices ob-tained by our method in the MovieLens and Book-Crossing dataset experiments, respectively. The obtained results are quite matching our intuition. For example, in the Movie-Lens dataset experiments, class #2:Romance appears to be strongly correlated with class #3:Drama, while class #1:Com-edy appears to have very low correlation with class #3:Drama. Similar, in the Book-Crossing dataset experiments, class #1:Mystery &amp; Thrillers appears to have close to zero cor-relation with class #5:Religion &amp; Spirituality, while class #2:Science Fiction &amp; Fantasy appears strongly correlated with both class #1:Mystery &amp; Thrillers and #3:Science.
To conclude, a question that naturally arises concerns how our method compares to the competition in terms of com-putational costs. To begin with, training our model imposes computational costs similar to the MCF-LF approach when the number of its latent features is equal to the truncation thresholds of our approach. In our elaborate implementa-tion, the number of latent features of MCF-LF was two or-ders of magnitude less than the initialization of our model. However, most of the initial features of our model were quickly purged; thus, our method yielded an only 27% in-crease in total computational time for model training, com-pared to MCF-LF. Regarding prediction generation using our model, as we already discussed in Section 3.3, the in-curred computational costs of our approach are similar to MCF-LF, due to the simple linear form of our prediction function; our experiments corroborated this theoretical nd-ing.
In this work, we presented a Bayesian latent factor model that aims at addressing the data sparsity problem CF sys-tems suffer from by utilizing information from multiple tasks, detecting and extracting common patterns, and sharing this information across tasks to enhance the obtained predictive performance in all cases. Its main difference from related existing works is that it models the users rating function by means of a two-component latent factor model, as well as its formulation on the basis of the nonparametric Bayesian paradigm.
 Speci cally, our approach is facilitated by imposing two Indian Buffet Process priors over the variables assigning users and items to latent features, which allow for doing inference for the appropriate number of latent features used in the context of the model. Information sharing is per-formed by imposition of a suitable matrix-variate prior over the factor loadings matrices of the model across the con-sidered tasks. This is in contrast to existing PMF-based approaches, which impose joint priors over the latent fea-ture assignments of the users across tasks. As we discussed, our information sharing mechanism allows for deriving richer shared information across tasks compared to existing PMF-based approaches.

We provided a highly-scalable algorithm for model in-ference using a truncated variational Bayesian approach. We evaluated our approach using two large commonly used real-world datasets. We compared the performance of our method to related existing MCF methods, as well as state-of-the-art single-task CF approaches. As we observed, our model manages to outperform the considered alternatives.
Our future research goal is to adapt our approach so as to also facilitate performing transfer learning, i.e. transferring the shared preference patterns extracted from a number of tasks and domains to new tasks, where very few ratings are available. Such a functionality is expected to be of great ben-e t, e.g., to the efforts of mitigating the cold-start problem. Exploring utilization of mixtures of latent factor models [3] is also part of our ongoing research. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] C. M. Bishop. Pattern Recognition and Machine [3] S. Chatzis, D. Kosmopoulos, and T. Varvarigou. [4 ] W. Y. Chen, J. C. Chu, J. Luan, H. Bai, Y. Wang, [5] D. DeCoste. Collaborative prediction using ensembles [6] F. Doshi-Velez, K. Miller, J. V. Gael, and Y. W. Teh. [7] F. Doshi-Velez, K. T. Miller, J. V. Gael, and Y. W. [8] T. Griffiths and Z. Ghahramani. In nite latent feature [9] M. Harvey, M. J. Carman, I. Ruthven, and [10] T. Hofmann. Latent semantic models for collaborative [11] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. [12] Y. Koren. Factorization meets the neighborhood: a [13] K.Yu, A.Schwaighofer, V.Tresp, X.Xu, and [14] K.Yu, S.Zhu, J.D.Lafferty, and Y.Gong. Fast [15] N. D. Lawrence and R. Urtasun. Non-linear matrix [16] B. Li, Q. Yang, and X. Xue. Can movies and books [17] B. Li, Q. Yang, and X. Xue. Transfer learning for [18] B. Li, Q. Yang, and X. Xue. Transfer learning for [19] E. Meeds, Z. Ghahramani, R. M. Neal, and S. T. [20] O. Moreno, B. Shapira, L. Rokach, and G. Shani. [21] P. Muller and F. Quintana. Nonparametric Bayesian [22] S. Nakajima and M. Sugiyama. Theoretical analysis of [23] W. Pan, E. W. Xiang, N. N. Liu, and Q. Yang. [24] I. Porteous, A. Asuncion, and M. Welling. Bayesian [25] J. D. M. Rennie and N. Srebro. Fast maximum margin [26] R. Salakhutdinov and A. Mnih. Probabilistic matrix [27] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [28] A. P. Singh and G. J. Gordon. Relational learning via [29] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. [30] X. Suand and T. Khoshgoftaar. A survey of [31] Y. W. Teh, D. Gorur, and Z. Ghahramani.
 [32] J. Winn and C. Bishop. Variational message passing. [33] C. Yuan. Multi-task learning for bayesian matrix [34] Y. Zhang, B. Cao, and D.-Y. Yeung. Multi-domain
