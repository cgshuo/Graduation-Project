 Assume a standard setting with data D = { ( x i , y The data dependent permutation complexity 1 for H is defined by: Here,  X  is a uniformly random permutation on { 1 , . . . , n } . P of the target values. The difficulty in analyzing P from y = [ y distribution B on y , where each sample y B Koltchinskii and Panchenko, 2000; Lozano, 2000; Lugosi and Nobel, 1999; Massart, 2000):  X 
H ( n, D ) those r satisfying P n Theorem 1 With probability at least 1  X   X  , for every h  X  X  , theorem is to accomodate samples ( y For a given permutation  X  , one can compute max to compute the permutation complexity. Let  X  be a single random permutation. Theorem 2 For an absolute constant c  X  6 + p 2 / ln 2 , with probability at least 1  X   X  , the bootstrap complexity (see Section 2.3).
 Empirical Results. For a single random permutation, with probability at least 1  X   X  , an empirical risk minimization on O ( n ) data points. uniform bound in terms of E 1.1 Related Work been well specified. Such methods are not our primary focus. model selection is ignored (as in Bartlett et al. (2002)). problems. For classification, their estimate is related to t he permutation complexity. in Bartlett et al. (2002) and ignoring terms which are O ( 1 of two; finally, (e) follows again by McDiarmid X  X  inequality because  X  happen with data dependent permutation sampling, which is w here the difficulty lies. main bounding tool will be McDiarmid X  X  inequality: Lemma 1 (McDiarmid (1989)) Let X for j = 1 , . . . , n . Then, with probability at least 1  X   X  , We also obtain E f  X  f + q 1 2.1 Permutation Complexity The out-sample permutation complexity of a model is: where the expectation is over the data D = ( x 1 , y D  X  differ from D only in one example, ( x j , y j )  X  ( x  X  Lemma 2 |P Proof: For any permutation  X  and every h  X  X  , the sum P n going from D to D  X  ; thus, the maximum over h  X  X  changes by at most 4 . Lemma 2 together with McDiarmid X  X  inequality implies a conc entration of P which means we can work with P Corollary 1 With probability at least 1  X   X  , P Since e in ( h ) = 1 y  X  can be used to compute P H ( n, D ) for a particular permutation  X  . 2.2 Bounding the Out-Sample Error To bound sup (2002); Shawe-Taylor and Cristianini (2004)). Let r  X  X  = [ r  X  X  Lemma 3 With probability at least 1  X   X  : Proof: We proceed as in the proof of the maximum discrepancy bound in Section 1.1: sup In (a), the O (( 1 because r  X  X  expectation (it amounts to relabeling of random variables) . 2.2.1 Generating Permutations with  X  1 Sequences Fix y ; for a given permutation  X  , define a corresponding  X  1 sequence r  X  by r  X  y ing  X  1 sequence r  X  i ; we thus obtain a multiset of sequences S have a mapping from permutations to the  X  1 sequences in S uniform on S that S (for example) r  X  S Theorem 3 With probability at least 1  X  5  X  , We obtain Theorem 1 by combining Theorem 3 with Corollary 1. 2.2.2 Proof of Theorem 3 Let D  X  X  be a second, independent ghost sample, and S Lemma 3, take the expectation over r  X  X  uniform on S where each permutation  X  induces a particular sequence r  X  X  (  X  )  X  S which is now r to the permutations over D, D  X  .
 S for every r  X  X   X  S mapped). Similarly, there exists such a mapping from S ing sets S y 1 = = y the first k are +1 , so y follows. For a given permutation  X  , we map r  X  X  (  X  )  X  S bijective since every permutation corresponds uniquely to a sequence in S Let r on exactly | k  X  k  X  X  | locations (and similarly for y disagree is therefore at most 2 | k  X  k  X  X  | . Thus, for any r  X  X  and any h  X  X  , We observe that P n where z We consider the function f ( z z using the symmetry of z r ( ( r i  X  r where r  X  X  (  X  ) cycles through the sequences in S under negation, we finally obtain the bound Using this in Lemma 3, with probability at least 1  X  5  X  , bound; the result is similar. (iii) One could bound P 2.3 Estimating P We now prove Theorem 2, which states that one can essentially estimate P all permutations) by sup Our proof is indirect: we will link P a random sequence y B of n independent uniform samples from y that y B Lemma 5 |B Proof: Let k be the number of y (
F denotes the flipping random process.) Since y F Thus, Since E thing about the bootstrap estimate is that the expectation i s over independent y B Lemma 6 For a random bootstrap sample B , with probability at least 1  X   X  , We now prove concentration for estimating P probability at least 1  X  2  X  , y B differs from y  X  in at most (2 n ln 1 the complexity by at most 2 , hence, with probability at least 1  X  2  X  , We conclude that for a random permutation  X  , with probability at least 1  X  3  X  , We have not only established that P 716 X 723.
 Asuncion, A. and Newman, D. (2007). UCI machine learning rep ository. structural results. Journal of Machine Learning Research , 3 , 463 X 482. Learning , 48 , 85 X 113.
 matik , 31 , 377 X 403.
 restraints and best predictor weights. Education and Psychology Measurement , 11 , 12 X 15. Journal of the American Statistical Association , 99 (467), 619 X 632. 66 (2-3), 165 X 207.
 929 X 989.
 Learning Theory , pages 501 X 515.
 Good, P. (2005). Permutation, parametric, and bootstrap tests of hypothese s . Springer. Proc. 14th European Conference on Machine Learning , pages 193 X 204. item analyses. Education and Psychology Measurement , 11 , 16 X 22. on Information Theory , 47 (5), 1902 X 1914.
 pages 443 X 459.
 Psychology , 22 , 45 X 55.
 Neural Comp.
 Statistics , 27 , 1830 X 1864.
 SIAM International Conference on Data Mining (SDM) .
 Facult  X  e des Sciencies de Toulouse , X , 245 X 303.
 148 X 188. Cambridge University Press.
 of cross validation. Education and Psychology Measurement , 11 , 5 X 11. Press.
 1940.
 Royal Statistical Society , 36 (2), 111 X 147.
 Neural Computation , 6 (5), 851 X 876.
 Sinica , 16 , 569 X 588.
 cient. Annals of Mathematical Statistics , 2 , 440 X 457.
 Psychology Measurement , 11 , 23 X 28.
 tion test for PLS component selection. Journal of Chemometrics , 21 (10-11), 427 X 439.
