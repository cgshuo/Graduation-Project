 To find near-duplicate documents, fingerprint-based para-digms such as Broder X  X  shingling and Charikar X  X  simhash algorithms have been recognized as effective approaches and are considered the state-of-the-art. Nevertheless, we see two aspects of these approaches which may be improved. First, high score under these algorithms X  similarity measurement implies high probability of similarity between documents, which is different from high similarity of the documents. But how similar two documents are is what we really need to know. Second, there has to be a tradeoff between hash-code length and hash-code multiplicity in fingerprint paradigms , which makes it hard to maintain a satisfactory recall level while improving precision.

In this paper our contributions are two-folded. First, we propose a framework for implementing the longest common subsequence (LCS) as a similarity measurement in reason-able computing time, which leads to both high precision and recall. Second, we present an algorithm to get a trustable partition from the LCS to reduce the negative impact from templates used in web page design. A comprehensive exper-iment was conducted to evaluate our method in terms of its effectiveness, efficiency, and quality of result. More specifi -cally, the method has been successfully used to partition a set of 430 million web pages into 68 million subsets of simi-lar pages, which demonstrates its effectiveness. For qualit y, we compared our method with simhash and a Cosine-based method through a sampling process (Cosine is compared to LCS as an alternative similarity measurement). The result showed that our algorithm reached an overall precision of 0 . 95 while simhash was 0 . 71 and Cosine was 0 . 82. At the same time our method obtains 1 . 86 times as much recall as simhash and 1 . 56 times as much recall as Cosine. Compar-ison experiment was also done for documents in the same  X 
This work is supported by NSFC Grants 60573166, 60773162 and National 863 Hi-Tech Grant 2007AA01Z100. web sites. For that, our algorithm, simhash and Cosine find almost the same number of true-positives at a precision of 0 . 91, 0 . 50 and 0 . 63 respectively. In terms of efficiency, our algorithm takes 118 hours to process the whole archive of 430 million topic-type pages on a cluster of six Linux boxes, at the same time the processing time of simhash and Cosine is 94 hours and 68 hours respectively. When considering the need of word segmentation for languages such as Chinese, the processing time of Cosine should be multiplied and in our experiment it is 602 hours.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Measurement, Experimentation Near-duplicate, longest common subsequence, fingerprint, sketch, similarity, web document repository
Detection of near-duplicate web documents is a recurrent task and plays an important role in web information related applications, such as clustering and sorting of query resul ts, web crawling, data extraction, plagiarism and spam detec-tion etc.

A variety of techniques have been developed and evalu-ated in recent years, among which Broder et al. X  X  shingling algorithm [2] and Charikar X  X  simhash [3] are considered X  X h e-state-of-the-art X  algorithms. These two algorithms use sh in-gles from page content and fingerprints over IR-based doc-ument vectors to measure similarity respectively. But both of them are not based on explicit content matching of the documents. As a result the measurements don X  X  imply the similarity but the possibility of similarity, which sets a l imit on accuracy. Meanwhile, the length of hash code controls the precision of hash based methods and the multiplicity controls the recall, so the conflict between code length and code multiplicity implicates a conflict between precision a nd recall [18].
 On the other hand, although the LCS (Longest Common Subsequence) is intuitively proper for similarity measure -ment, it is often considered not practical for near-duplica te detection. The primary concern is with efficiency. By the basic O ( N 2 ) time algorithm it needs an average of 0 . 1 second to obtain the LCS between two web documents. Moreover, finding near-duplicates by comparing each pair of two doc-uments in a large document repository usually means lots and lots of times of comparison which could not be finished in a reasonable time. A new framework is hence desirable for approaching both high precision and high recall at an acceptable computational cost. We believe our work is the first one which approaches this goal.

First, our approach has achieved high accuracy. We de-velop a LCS-based measurement which plays an essential role to achieve high accuracy. And we make further two con-tributions upon it: (1) an algorithm is presented to extract the most trustable portion from the LCS by considering the distribution of edit points in edit script. This method can reduce the false positive caused by the influence of template s to some extent, and hence provides better precision, and (2) our strategy to solve the position independence problem fur -ther improves the recall.

Second, our approach has reduced the computation time from years to days. We adopt Myers X  X  O ( ND ) difference algorithm [13] to compute LCS where N is the sum of the lengths of two sequences and D is the size of the shortest edit script (SES) for them. As this algorithm works fast on simi-lar documents, namely for small D , we develop the following approach to reduce the computation effort of LCS: (1) we develop a fingerprinting algorithm to divide the web doc-uments repository into sets of perhaps similar documents. Sentences instead of fixed size tokens are chosen to get pri-mal fingerprints, and a projection method is used to compute signatures by considering the global distribution of sente nce length. Documents with at least one signature in common are clustered into a set by a fast clustering method, and (2) before computing LCS, we develop a filtering algorithm to compute a sketch for each one of the two documents to compare by training away all the distinct fragments which exist in only one document. After this two heuristic steps the difference of two documents to be compared has been reduced significantly, and thus the O ( ND ) complexity ap-proaches O ( N ) in practice.

Our approach has been successfully used to partition a set of 430 million Chinese web pages into 68 million subsets of similar pages, For overall evaluation, a random sampling of 1000 subsets from that was conducted. This sample was ex-amined manually and a precision of 0 . 95 has been observed. For comparison purpose, we run Charikar X  X  simhash algo-rithm and the Cosine similarity on the same data set with the same sampling procedure, which yields a precision of 0 . 71 and 0 . 82 respectively. In terms of recall, the ratio of our algorithm X  X  performance to that of simhash is 1 . 86, and the ratio of our algorithm X  X  performance to that of Cosine similarity is 1 . 56. For document detection in the same web-site, while recalls levels are almost the same, our algorith m gives precision 0 . 91, and simhash and Cosine yields 0 . 50 and 0 . 63 respectively. In terms of efficiency, our algorithm takes 118 hours to process the whole archive on a cluster of six Linux boxes. The processing time of simhash algorithm is 94 hours, and for Cosine similarity it is 68 hours.
The rest of the paper is organized as follows: Section 2 describes related work and background. Section 3 presents our similarity measurement and the framework of our al-gorithm, and then describes our algorithm in detail. The evaluation methodology and the experiment results are pre-sented in section 4, and section 5 gives the conclusion and discusses about future works.
A variety of methods have been designed to detect dupli-cates or near-duplicates. These methods differ mainly in two aspects: the features extracted from each document and the strategy to get signatures from the features. On the first as-pect, shingles and document vector are the most commonly used features. Brin et al. [1] used sentences to create shin-gles in the COPS system while Broder et al. [2] adopted a sliding window on words. Hoad [8] and Chowdhury et al. [11, 4] made their contributions on document vector based methods by developing similarity measurements and com-puting a lexicon to modify the document vector respectively . On the second aspect, shingles-based methods [2, 10] usu-ally get signatures from their features using MOD m or MIN methods, while Charikar X  X  simhash [3] and Chowdhury et al. X  X  I-match [11] methods compute signatures over docu-ment vectors. Among these techniques, Broder X  X  shingling algorithm [2] and Charikar X  X  simhash algorithm [3] have bee n implemented on popular search engines.

In shingling algorithm [2] a shingle is defined as the fin-gerprint of a k -gram consisted by a subsequence of k suc-cessive tokens. The percentage of unique shingles on which two documents agree is used as a measurement for the sim-ilarity between them. Two simple compression schemes for shingling algorithm are MOD m and MIN s methods, which retain only the shingles which have the m smallest values or result in 0 when modulus m . Fetterly et al. [6] applied a fix number of different Rabin [16] fingerprinting functions to each shingle, and retain only the shingles whose finger-print is the smallest one returned by each Rabin function to form a feature vector. It has the property that the size of the feature vector is constant even if the document is short, and it needs only a position-to-position comparison on the feature vector to estimate the similarity. Schleimer et al.  X  X  winnowing algorithm [17] extended the basic shingling ap-proach by implementing a sliding window on shingles. The smallest shingle in each window-scope is chosen to form the signature of the document. It makes a guarantee that at least part of any sufficient long match is detected.
In simhash algorithm [3] high-dimensional feature vectors are mapped to small-sized fingerprints. It has a property that the cosine similarity of two documents is proportional to the number of bits in which the two corresponding fin-gerprints agree. Manku et al. [12] designed a technique to solve the hamming distance problem of identifying existing fingerprints that differ from a given fingerprint in at most a certain number of bit-positions, in order to implement the simhash algorithm for a multi-billion page repository.
Henzinger [7] implemented a lager-scale evaluation to com-pare the precision of shingling and simhash algorithms by adjusting their parameters to maintain almost same recall. Her experiment shows that neither of the algorithms works well for finding near-duplicate pairs on the same site be-cause of the influence of templates, while both achieve a higher precision for near-duplicate pairs on different site s. Simhash method achieves a better overall precision since it finds more near-duplicates on different sites.
There are several valuable works in recent two years. Yang and Callan [19] considered the near-duplicate detection as an instance-level constrained clustering problem, and de-veloped a semi-supervised clustering algorithm consideri ng three types of constraints. Huffman et al. [10] addressed the duplicate detection problem as a part of search evalua-tion, and improved accuracy by computing multiple pairwise signals for each pair of documents.
The basic O ( N 2 ) algorithm to solve the dual problems of finding a LCS of two sequences A and B and a shortest edit script (SES) for transforming A into B was presented by Wagner &amp; Fischer, where N is the sum of the lengths of two sequences and SES for transforming B into A may be different from that of A into B but of the same length [14]. Several subsequent works improved upon the basic O ( N 2 ) algorithm by being sensitive to other problem size parameters such as the length L of the LCS and the length D of the SES, where D = N  X  2 L [15, 13].

Among these algorithms Myers X  X  O ( ND ) difference algo-rithm [13] performs best when the parameter D is small. Myers solved the problem by transforming it into an equiv-alent problem of finding a shortest path in an edit graph. Let A = a 1 a 2 , . . . , a N and B = b 1 b 2 , . . . , b for A and B has a vertex at each point in the grid ( x, y ), x  X  [0 , N ] and y  X  [0 , M ]. Each vertex is connected to its right neighbor by a horizontal edge and to its below neigh-bor by a vertical edge, and there is a diagonal edge connect-ing vertex ( x  X  1 , y  X  1) to vertex ( x, y ) if a x = b longest subsequence problem is equivalent to finding a path from (0 , 0) to ( N, M ) with the maximum number of diagonal edges, and the SES problem is equivalent to finding a path with the minimum number of non-diagonal edges. Myers de-veloped a greedy algorithm to solve this kind of single-sour ce shortest path problem.

Figure 1 illustrates an edit graph of two sequences A = abcabba and B = cbabac presented in [13]. The LCS be-tween A and B is caba . The shortest edit script from A to B is 1 D 2 D 3 I b 6 D 7 I c, in other words to delete the 1 character and a  X  X  X  next to the 7 th character.
We define a document as a sequence of characters. Given two documents A and B , let | LCS | be the length of their longest common subsequence and | SES | be the length of their shortest edit script (The problems of finding a longest common subsequence of two sequences of A and B and a shortest edit script for transforming A into B have long been known to be dual problems). As illustrated above, given A = abcabba and B = cbabac, the longest edit script of A and B is caba, and the shortest edit script for transforming A into B is 1 D 2 D 3 I b 6 D 7 I c. The length of A and B is represented as | A | and | B | respectively. We have The resemblance of the two documents is defined as The containment of B in A is defined as We judge two documents as near-duplicate if the resem-blance or containment exceeds some thresholds.

Given the measurement above, we have three problems to solve. The first two are about how to compute the simi-larity according to the measurement efficiently. Considerin g the property that Myers X  X  O ( ND ) difference algorithm works fast for similar sequences, these two problems can be defined as: (1) how to identify the documents which are perhaps similar to each others that need LCS to verify their simi-larity, to compute LCS just between them, and (2) how to further reduce their differences before computing their LCS . The third problem is how to further improve accuracy upon the measurement. Here, an important observation is that web pages from the same website often adopt some com-mon template, which makes the pages have many identical non-content sequences recognized by the computer. As such, applying LCS to those pages without any additional mea-sure may cause false positive, and thus the third problem is equal to (3) how to extract a trustable portion from the LCS to compute similarity.

The framework of our algorithm can be divided into three steps: (1) divide the web documents repository into sets of perhaps-similar documents, and (2) before computing LCS of two documents in each set of perhaps-similar documents, filter out the portions that are unlikely to contribute to Trustable LCS, and (3) compute the LCS and choose its trustable portion to compute similarity. The framework of the entire algorithm involving all this three steps is illus -trated in Figure 2 with the input as a web documents cor-pus. The following three sections describe these three step s in detail.
The first step of our algorithm framework is to divide the web document repository into sets of perhaps-similar docu-ments, in order to constrain the computation of LCS occur-ing only between perhaps-similar documents in which My-ers X  X  O ( ND ) difference algorithm works fast. Documents in different sets are considered as not-similar and thus will no t perform the LCS computation. The algorithm consists of three steps: preprocessing the documents, computing signa -tures for each document and clustering the documents based on their signatures.

In the first phase, we preprocess the original web docu-ments into documents consisting of sentences. Visible text blocks are extracted from web documents by removing all HTML markups and formatting directives. Then the visi-ble text blocks are partitioned by punctuations to produce sentences.

In the second phase, we compute signatures for each doc-ument. We fingerprint every sentence in the document using 128-bit MD5 fingerprinting function. We disperse the finger-prints in the following way: (1) we view each fingerprint as a sequence of 16 bytes. By connecting the tail of the sequence to its head we get a ring of bytes, in which every contin-uous 16 bytes starting at each one of the 16 bytes form a derivative fingerprint, and thus we can get at most 16 finger-prints for each sentence and at most 16 sets of fingerprints for each document, and (2) we define W as one of the n sets of derivative fingerprints of a document (1  X  n  X  16). Given an integer m , all the signatures in W are treated as integers and then grouped by their module on m . We define MOD i m ( W ) as a subset of W in which the module of the fingerprints on m is i , (0  X  i  X  m  X  1), and thus we can get m  X  n subset for each document.

For each subset, we then choose a certain number of small-est fingerprints from it to compute a signature: (1) we trans-form the bit-string of each selected fingerprint into a vecto r by project each 1-bit to w and each 0-bit to  X  w , where w is the weight of the fingerprint. The computation of w is based on the length of the sentence corresponding to the fingerprint. Our statistics show that the overall distribut ion of the lengths of sentences in our archive obeys power-law , while the local distribution we care most is closer to a log-normal distribution. Then we set w = 1 + log e l , where l is the length of the sentence, and (2) we add the vectors together to get a single vector. The final signature is cre-ated by setting every positive entry in the vector to 1 and every non-positive entry to 0. We get m  X  n subsets in the foregoing steps, and thus the total number of signatures for a document is also m  X  n . For a given value of m , when we set different values from 1 to 16 to n , we could get m signa-tures, 2  X  m signatures, . . . , and at most 16  X  m signatures for each document.

In the third phase, we index the documents by signatures and cluster them into perhaps similar sets. Since we want to divide the perhaps-similar documents into a same set as much as possible with small computational cost, so each two documents have one or more signatures in common are con-sidered as perhaps-similar documents. The grouping proces s is as follows: (1) we compute a weight W d for each document according to its signature number (other values of page qual -ities such as pagerank values may be used also). Together with the document number I d we get a key &lt; W d , I d to sort documents, and (2) we sort documents in the list indexed by each signature and choose the document which has  X  X argest X  key as the  X  X epresentative X  of the list. The li sts with the same representative will be merged into a single list. Then we sort all the lists according to their represent a-tives in descending order. Begin from the first list, find the lists whose representatives exist in this list and merge the m into it. The final lists are the perhaps-similar sets.
There are two enhancements in implementing the above three phases: (1) we only choose the sentences whose length exceed some threshold ( &gt; 16 bytes) to compute fingerprints, and the sentences whose fingerprints are chosen to compute a signature should disperse in the document, so some doc-uments couldn X  X  generate the exact number of signatures as the longer documents. In our archive there are about 1 . 03% documents which are very short and worthless generate 0 signature, so we abnegate them directly, and (2) we have a trick to reduce the influence of templates on same web sites and detect spam in this stage. We delete the signatures which exist very frequently ( &gt; 10000 times) and which ex-ist frequently ( &gt; 400 times) and focus on some certain web sites. These kind of signatures mostly come from templates and spam web pages.
The second step of our algorithm framework is to filter each two documents that are to compute LCS to strain away all the distinct fragments which exist in only one of the two document, in order to further reduce the compu-tational cost of LCS. This step is necessary. Although di-viding the repository into perhaps-similar sets can reduce the computing time significantly, there are two reasons for which it doesn X  X  satisfy our request completely: (1) the doc u-ments in each set are just perhaps-similar, so there would be some false-positives which have large differences from oth-ers and consume huge LCS computing time, and (2) even for two documents which are indeed similar, if their differ-ences reach a relatively large rate, for example 20% of sum of their lengths, the O ( ND ) time for computing LCS is then approaching a O ( N 2 ) time for D = N/ 5. So a filtering pro-cess is needed to further reduce the differences between two documents before computing LCS.

Denehy and Hsu X  X  work [5] on managing duplicate data give us much inspiration in designing the filtering algorith m. Our algorithm is as follows: given two documents A and B to compare, we compute a sketch for A and B respectively consisting of sufficiently long fragments that exist in both of them in the following way: (1) a sliding window of size S is used to compute the fingerprint for each overlapping S characters of each document using Rabin X  X  fingerprinting functions [16], and (2) each fingerprint of A is hashed into a hash table T which chains hash collisions for each entry by a list of fingerprints hashed into it. The number of entries of T is | A | + | B | X  2 S + 2, and (3) for each fingerprint of B , if it finds a match in T with the fingerprints of A , the corresponding sequence in B is reserved. The intersection of such reservations forms the filtered sketch of B . We can get the filtered sketch of A through a similar process. The LCS computation is implemented upon the two sketches if necessary. The time and space cost of this algorithm are both O ( N ).

It is important to note that the LCS computation on fil-tered sketches is not always necessary. Since | LCS | of the two filtered sketches is certainly less than the length of the minimal one, the later can be used as a replacement of the former to test if the two similarity measurements could ex-ceed their thresholds. Only if that is true should the LCS computation be performed, or the two documents can be de-termined as not similar directly. This optimization furthe r improves the performance effectively since LCS computa-tion is known as time-costly even if performed on similar documents.

There are two properties of our algorithm: (1) if the inter-val between two different characters is less than S in one of the documents, the characters in this interval will be elimi -nated regardless whether they are common sequences of the two documents or not. This seems to be a loss of informa-tion, but in fact it is good for filtering meaningless charact ers to reserve just sequences long enough for semantic contribu -tions, which makes the extraction of Trustable LCS (TLCS) easier, and (2) due to the sliding window, the algorithm is robust for small changes on original common sequences, such as a loss or insertion of characters when typing or copying a paragraph, or when transforming a PDF document into a HTML document etc. So the loss for computing TLCS is small.
The third step of our algorithm framework is to compute the LCS and extract a trustable portion from the LCS to compute the similarity of documents. Myers X  X  O ( ND ) differ-ence algorithm was adopted to compute LCS, and thus the algorithm presented here is about how to compute TLCS from LCS. There are some approximations: after comput-ing the filtered sketches of documents A and B , we compute the LCS and SES of the two sketches using Myers X  X  differ-ence algorithm [13]. The LCS of A and B is approximated Figure 4: Curve of the distribution of edit opera-tions. by the LCS of the two sketches. The SES from A to B can be approximated by a combination of the SES of the two sketches and the edit scripts from the two documents to their sketches, which can be computed in linear time. The following LCS and SES both refer to these approximations.
We define the TLCS based on the following observations: the templates of websites usually exist at the top and/or bot -tom positions of documents. For two different documents in the same website, the portion of their LCS from template and that from main text are rarely contiguous to each other in the original documents. So a TLCS is defined as a con-tinuous portion of LCS which satisfies the following three properties: (1) continuity: the characters of this portion have a high continuity in the original documents, and (2) centricity: the characters of this portion cover the centra l or near-central positions of the original documents, and (3) e x-tensity: it should be the longest or nearly-longest one whic h satisfies the preceding two properties.

The properties of TLCS can be reflected intuitively through the curve of SES as illustrated in Figure 4, in which the d -axis represents number of edit operations in SES, and the x -axis represents their corresponding occur-positions in a n original document. The slope of a region (that is the slope decided by two points on the curve whose x -coordinates are the two endpoints of the region) reflects the continuity of the portion of LCS in this region, while the region X  X  positio n and length decide the centricity and extensity respectivel y. We have developed a heuristic algorithm to find the longest region whose rope is under a threshold (the threshold of rope in Figure 4 is 0 . 2) and satisfying centricity property. This region is defined as the trustable region, as illustrated in Figure 5. The corresponding portion of LCS is TLCS.
We get total ( k 2 + k ) / 2 base regions by k blocks, so the computing time to get base regions is O ( k 2 ). There are at most two regions with the same largest weight and fewest blocks, and thus there are at most two regions in the final R
B . Extending a left or right endpoint needs s  X  1 times of comparison, but in fact we need not compare all of the s  X  1 offsets but just those where the number of edit operations d increases, so in worst case the number is D and on average it is D/k . Then the total computing time in worst case is O ( k 2 + D ). Figure 5: Algorithm for extracting trustable LCS.
There are two further considerations: (1) for regions with the same weight, the fewer blocks a region covers the higher centricity it has. This is the reason that we reserve regions with fewest block when further filtering R B , and (2) the longest extended region is not the real trustable region but an approximation which may be a little shorter as illustrate d in Figure 4. This seems to be a loss, but in fact the lost ones are those which may make the edges of the trustable region more serrated, which makes the longest extended re-gion more trustable.

Some applications require the algorithm to be robust for position changes such as a coarse permutation of the para-graphs in a document. This can be solved by an iterative strategy: after extracting a trustable region for document A , eliminate it from A and eliminate the TLCS from document B . Then use the remaining portions of A and B to compute a second trustable region and LCS and so on. For two doc-uments with n paragraphs, the worst case is that A is the negative sequence of B which needs n iterations to detect all the same paragraphs. For n is usually small, the total computing time is still O ( ND ). Besides, it has the property that the length of LCS got in each iterating is monotonous descending, which allows setting of thresholds to limit the iteration depth to control the computing time.
We think near-duplicate detection is valuable mainly on topic-type web documents rather than navigation-type doc-uments, so the experiments were performed on a set of 430 million topic-type web pages extracted from a repository of 2 . 4 billion web pages collected by Web Infomall [9], which is a Chinese web archive that has been built at Peking Univer-sity since 2001. The 2 . 4 billion web pages were crawled from 24 . 7 million hosts over years mainly based on breadth-first search crawling, and the topic-type web pages that we are interested in mainly include news, reports, blogs and sim-ilar. Visible contents are first extracted from web pages, and we clean out those noisy data from the content, which obviously has little correlation with the topic before dete ct-ing near-duplicates. The following  X  X eb document X  refers to the remaining content. Since many web documents are very long, we only use the first 10KB (if longer) to compute similarity for efficiency.

For human evaluation, we define two documents to be near-duplicates if they are identical at the main portion of content, and manually tag each pair as: (1) true positive, (2) false positive, or (3) unknown. In particular, we use the guidelines similar as [19] to define several subcategori es (maybe overlap) of true positive: The guidelines for false positive is same as [7], where a near -duplicate pair is false positive if the main item(s) of the tw o documents is (are) different, such as two shopping pages with same boilerplate but a different product in the page center. The remaining situations are considered as unknown.
The experiments were organized as follows: First, we com-pared our algorithm with simhash and Cosine in precision, recall and efficiency in processing the 430 million web pages. Second, we evaluated the sensitivity of key parameters deci d-ing the effectiveness of accuracy and efficiency in each step of our algorithm, which involves the impact of the number of signatures in computing perhaps-similar sets, the impact o f the length of sliding window in computing filtered sketches, and the impact of the threshold of the slope of trustable region for precision and recall.
We compared the overall precision , overall recall and effi-ciency of our algorithm with simhash algorithm and Cosine in this subsection. Simhash algorithm achieves a better ove r-all precision than shingling algorithm at almost the same recall in Henzinger X  X  [7] evaluation and thus represents the top level of near-duplicate detection algorithms intuitiv ely; While Cosine was considered to be more efficient than LCS as a pair-wise similarity measurement and more accurate than simhash. Further more, since in Henziger X  X  evaluation neither simhash nor shingling worked well for finding near-duplicate pairs on the same web site because of the influence of templates, we also made a comparison about the precision and recall of documents on the same web sites.

The criteria for comparison are defined as follows: (1) The precision is defined as the fraction of the true-positive pairs in the near-duplicate pairs detected by each algorithm. Af-ter detecting near-duplicates on the 430 million documents by each algorithm, we randomly selected 1000 documents from the results for evaluation. For each selected document , we computed the ratio of the number of true-positives to the total number of near-duplicates of this document announced by the algorithm. The average ratio is considered as preci-sion . (2) The recall is defined as the ratio of the number of true-near-duplicate pairs detected by each algorithm to th e total number of all existing near-duplicate pairs. It is im-possible to find all true-near-duplicates manually from the set of 430 million web documents, thus we can not get the exact value of overall recall . Then we regarded the recall of simhash algorithm as a baseline which was set to 1. For each sampled document we compute the ratio between the true-positives detected by our algorithm and that detected by simhash algorithm, and the recall of our algorithm is de-noted by the average ratio of all sampled documents. The recall of Cosine is measured in the similar way. (3) The efficiency is measured by the total computing time of pro-cessing the whole 430 million documents. Experiments were performed on a cluster of six ordinary linux boxes, each of which has 2 CPUs (Intel Xeon 3.0GHz) and 8GB main mem-ories. The contenders are described as below:
LCS : For this algorithm proposed in this paper, the num-ber of signatures per document is 112, the length of slid-ing window is 16, the slope of Trustable Region is 0 . 1, the threshold of resemblance is 0 . 28, and the threshold of con-tainment is 0 . 7.

Cosine : The computing time of comparing 430 million web documents pair-by-pair is also unaffordable, and thus we implemented Cosine under our heuristics framework. Co-sine can be seen as an alternative similarity measurement fo r our framework. That is, it only includes two steps: first is to generate perhaps-similar sets same as LCS; second is to compute similarity by Cosine in stead of LCS. Computing filtered sketches is not needed in this case. Two web doc-uments were regarded as similar if their Cosine similarity exceeded 0 . 95.

Simhash : The parameters in implementing simhash were different from Henzinger X  X  evaluation [7] in some place sinc e we had different guidelines for human evaluation. A vector of 64 bits was stored for each document and the threshold of the number of agreeing bits was set to 59, which reaches the highest precision with a certain recall . We adopted Manku X  X  [12] algorithm, the most efficient one we could find, to solve the hamming distance problem in implementing Simhash algorithm.

The experimental results is illustrated in table 1. Our algorithm performed better on both precision and recall : (1) our algorithm reached an overall precision of 0 . 95 while simhash X  X  overall precision was 0 . 71 and that of Cosine was 0 . 82. At the same time, the ratio between the overall recall of our algorithm and that of simhash is 1 . 86 and this ration between Cosine and simhash is 1 . 13, and (2) for documents on same web sites, our algorithm, simhash and Cosine find almost the same number of true-positives at a precision of 0 . 91, 0 . 50 and 0 . 63 respectively. The improvement on pre-cision compared with simhash is mainly due to that we use the  X  X egree of true match X  instead of the  X  X ossibility of sim -ilarity X  and the influence of templates on same web site are reduced. While compared with Cosine this improvement is mainly due to that LCS includes the sequence of words be-sides the words X  frequency, as well as the influence of tem-plates. The improvement on recall is mainly due to two reasons: (1) our measurement is able to evaluate the con-tainment between document and judge them to be similar if they satisfy some other conditions, and (2) our algorithm is robust for documents that are true similar but with dif-ferent templates and other changes, and this also may be the reason that the more true-positives our algorithm found mostly reside on different sites.

Efficiency is as important as, if not more important than, accuracy for large scale near-duplicate detection. Althou gh at first thought our LCS-based method might be much slower than simhash and even Cosine, it turns out to be not much difference in computation cost. Our algorithm spent 118 hours to process all the 430 million documents, while Co-sine and simhash spent 68 hours and 94 hours respectively. If not considering the same step 1 X  X omputing perhaps-similar sets X , Cosine spent 21 hours while LCS spent 71 hours. How-ever, we still think LCS is better for our application since i t is much more precise.

What X  X  more, there are three points that should be no-ticed: (1) Cosine is also performed in our heuristic frame-work in the experiment. If not, the computing time should be amount of years also. Moreover, in some languages such as Chinese and Japanese there are no space between words and thus it needs  X  X ord segmentation X  before implementing Cosine measurement, which is a computation costly task. The whole computing time of processing the whole 430 mil-lion documents is 602 hours when the time to segment words is added. (2) Under limited computing resources especially main memory, the efficiency of simhash is sensitive to the similarity threshold of the number of agreeing bits of two fingerprints. In our experiments, if this threshold is set to 58 instead of 59, the computing time is more than 400 hours rather than 94 hours. On the other hand, stricter similarity threshold is able to reduce computing time but that also re-sults in poorer recall. And thus, simhash seems to be proper for detecting near-copied web pages different with tiny dif-ference. (3) The average number of near-duplicates of each document on the web is fixed on the whole, deciding a basi-cally stable size of each perhaps-similar set on average whi ch means that the average computing time of step 2 and step 3 to process each perhaps-similar set is fixed regardless of the size of the archive. And thus the whole computing time of our algorithm is linear to the number of perhaps-similar sets got after step 1, namely the size of the archive, which is a promise for the scalability.
The false-positives in our algorithm mainly include four portions: (1) web documents which describe similar but not same objects, such as pages selling computers of same se-ries but different models, and (2) web documents with won-drously large boilerplate, (3) web documents on forum which differ only on some small but critical replies, and (4) in or-der to reduce complexity of implementing, we assumed that the similarity between documents satisfied transitivity wh ich brought a fall in precision. This false-positives caused by this four reasons occupied 60%, 20%, 15%, and 5% of the total false-positives respectively.
The effectiveness of each step of our heuristics algorithm in improving accuracy and speeding up the computation is shown in table 2: (1) for efficiency, without step 1 (comput-ing perhaps-similar sets) we should need amount of years to compare all documents in pairwise fashion, which is not af-fordable anyway; without step 2 (computing filtered sketche s) we should need more than 5700 hours to compute edit scripts because the difference between each pair in the perhaps-similar set is still large, which is not affordable still, and (2) for precision, after step 1 average 67% documents in each perhaps-similar set are substantially similar, and after s tep 3 the overall precision is 95%. We analysis the effective-ness and parameter sensitivity of each step in detail in the following portions.

Computing perhaps-similar set : There are two goals in the stage of computing perhaps-similar sets: (1) for each selected document, all of its similar documents should be clustered into one set as fully as possible, and (2) the doc-uments put into each set should be as similar as possible. We use coverage rate and trustable rate to evaluate the two goals respectively. The coverage rate is defined as the ra-tio between the number of true near-duplicates of a selected document that can be detected from its perhaps-similar set (including itself) and that can be detected directly from th e whole original document repository, both using TLCS mea-surement. The trustable rate is defined as the ratio between the number of near-duplicates of a selected document de-tected from its perhaps-similar set by the TLCS measure-ment and the total document number of that set.

We randomly selected 150 documents from the whole repos-itory for evaluation. First, for each selected document we ran a near-duplicate detection on the 430 million documents to find all its similar documents measured by TLCS, that is, every document was compared to the selected document, and TLCS between them was computed to measure if they are similar. We then manually checked the found documents to see if they are true-similar to the selected one. Next, we ran near-duplicate detections on perhaps-similar sets onl y, measured by TLCS also. Setting signature number to differ-ent values produced different perhaps-similar sets. For the parameters mentioned in subsection 3.2, we set the modular parameter m to 7, and thus when we change n from 1 to 16 we get different signature number from 7 to 112 (in steps of 7). For the parameters of TLCS measurement, the threshold of resemblance is 0 . 28, and the threshold of containment is 0 . 7.

The result is illustrated as in figure 6. When the number of signatures increases, the coverage rate ascends while the trustable rate descends, which indicates that higher recall can be achieved at the cost of more computation resource since lower trustable rate means higher computational cost. The trustable rate and coverage rate seems to reach a balance when signature number is 14, which may be a proper choice considering the tradeoff between overall recall and compu-tational cost. In our system we have a higher requirement on recall compared with computational cost so we choose 112 signatures for each document. On the other hand, 112 is almost the largest number we can afford. The signatures for 430 million web pages occupy 1 . 2TB disk space and costs 33 hours for indexing and clustering.

Computing filtered sketches: In the stage of com-puting filtered sketches in each perhaps-similar set we also have two goals: (1) most of the time in computing LCS is spent on not-true-similar pairs, so their contents should b e strained away as much as possible, and (2) the contents that are to exist in the TLCS of the true-similar pairs should be reserved in the filtered sketch as much as possible. We use strainaway rate and reserve rate to evaluate the two goals respectively. We divide the pairs in each perhaps-similar set into true-similar pairs and not-true-similar pairs. Th e strainaway rate is defined as the ratio between the sum of length of the strained away portions of a not-true-similar pair and the sum of length of this pair. The reserve rate is defined as the ratio between the TLCS X  X  length of the fil-tered sketches of a true-similar pair and the TLCS X  X  length of this original pair.

We used 112 fingerprints for each document in the first stage resulting in 46 million perhaps-similar sets. We ran-domly selected 100 documents from the whole repository for evaluation and their corresponding perhaps-similar se ts were selected too. For each selected document we manu-ally tagged each document of its perhaps-similar set as true -similar or not-true-similar to it. They were then used as
Figure 7: Impact of the length of sliding window. true-similar pairs and not-true-similar pairs to compute t he average strainaway rate and reserve rate when the length of sliding window increased from 2 bytes to 100 bytes.
The result is illustrated as figure 7. The strain-away rate increases sharply in the range corresponding to dif-ferent length of sliding window between 2 and 6, reaches 0 . 8 when the length is 6 bytes, and then stays roughly the same afterwards. The main reason may be that most of the Chinese words are less than 3 Chinese characters (that is 6 bytes). The reserve rate decreases slowly and is above 0 . 85 at its lowest value. In our system we choose 16 bytes as the length of sliding window resulting in a strainaway rate at 0 . 87 and a reserve rate at 0 . 975, which significantly reduces the difference between not similar documents before com-puting TLCS and thus reduces the computing time, while has little influence on the TLCS of true similar documents.
Computing trustable LCS: We evaluated the impact of different values of slope of the trustable region on overal l precision at a certain recall by sampling 1000 documents randomly each time. The precision is computed as the above  X  X omputing perhaps-similar sets X , while the measure -ment of recall used the following approximation: assuming that all true-near-duplicates of each selected document ha d been included in its perhaps-similar set. Thus the recall of a selected document was computed as the ratio between the number of true-near-duplicates detected by our algo-rithm and the total number of all true-near-duplicates in th e perhaps-similar set. For each value of the slope threshold o f trustable region, we adjusted the threshold of resemblance to maintain the recall at 0 . 90 to evaluate the influence on precision . The measurement of containment was not used in this evaluation for simplicity.

The result is illustrated as figure 8, which shows the change of precision when the slope threshold increases from 0 . 02 to 0 . 2. The precision reaches its peak which is 0 . 948 when the slope threshold is 0 . 10. And we adopt this value of slope threshold in our system. The reason for the significant lower precision in the range corresponding to slope threshold be-tween 0 . 02 and 0 . 05 may be that the threshold is too strict to extract sufficient long TLCS, and thus increase the chance of mismatch. The reason for the lower precision in the range corresponding to slope threshold between 0 . 15 and 0 . 2 may be that the threshold is loose, so that some portion of the templates was included into the TLCS. The proper value of slope threshold may be different for different type of docu-ment, and thus should be adjusted according to applications .
The framework we developed adopts the  X  X rue matches X  between documents to evaluate their similarity. Experi-ments on 430 millions of topic-type web documents showed that it provided both higher precision and recall, compared with the two state-of-the-art algorithms. Further more, th e continuity, centricity and extensity features of TLCS redu ce the influence of templates, with which our algorithm ensures high accuracy for documents on the same web site where both shingling and simhash algorithms failed. Although the evaluations were performed on web documents, we believe our algorithm is applicable to broader document types.
Intuitively our work may contribute to two important fur-ther researches: (1) the algorithm to compute TLCS may be altered and improved for the extraction of templates and texts, and (2) the most valuable matches retrieved from TLCS may be used with algorithms of storage compression of similar documents for web archive. [1] S. Brin, J. Davis, and H. Garcia-Molina. Copy [2] A. Z. Broder, S. C. Glassman, M. S. Manasse, and [3] M. S. Charikar. Similarity estimation techniques from [4] A. Chowdhury, O. Frieder, and D. Grossman.
 [5] T. E.Denehy and W. W.Hsu. Duplicate Management [6] D. Fetterly, M. Manasse, and M. Najork. On the [7] M. Henzinger. Finding near-duplicate web pages: A [8] T. C. Hoad and J. Zobel. Methods for identifying [9] L. Huang, H. Yan, and X. Li. Engineering of web [10] S. Huffman, A. Lehman, and A. Stolboushkin.
 [11] A. Kolcz, A. Chowdhury, and J. Alspector. Improved [12] G. S. Manku, A. Jain, and A. D. Sarma. Detecting [13] E. W. Myers. An o(nd) difference algorithm and its [14] N. Nakatsu, Y. Kambayashi, and S. Yajima. The [15] N. Nakatsu, Y. Kambayashi, and S. Yajima. A lcs [16] M. O. Rabin. Fingerprinting by Random Polynomials . [17] S. Schleimer, D. S. Wilkerson, and A. Aiken. [18] B. Stein. Principles of hash-based text retrieval. In [19] H. Yang and J. Callan. Near-duplicate detection by
