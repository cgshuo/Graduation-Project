 Twitter has become an increasingly important source of in-formation, with more than 400 million tweets posted per day. The task to link the named entity mentions detected from tweets with the corresponding real world entities in the knowledge base is called tweet entity linking. This task is of practical importance and can facilitate many different tasks, such as personalized recommendation and user inter-est discovery. The tweet entity linking task is challenging due to the noisy, short, and informal nature of tweets. Pre-vious methods focus on linking entities in Web documents, and largely rely on the context around the entity mention and the topical coherence between entities in the document. However, these methods cannot be effectively applied to the tweet entity linking task due to the insufficient context in-formation contained in a tweet. In this paper, we propose KAURI, a graph-based framework to collectively linK all the nA med entity mentions in all tweets posted by a user via modeling the U se R  X  X  topics of I n terest. Our assumption is that each user has an underlying topic interest distribution over various named entities. KAURI integrates the intra-tweet local information with the inter-tweet user interest in-formation into a unified graph-based framework. We exten-sively evaluated the performance of KAURI over manually annotated tweet corpus, and the experimental results show that KAURI significantly outperforms the baseline methods in terms of accuracy, and KAURI is efficient and scales well to tweet stream.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Tweet entity linking; Knowledge base; User interest model-ing
Twitter, a popular micro-blogging platform, has been grow-ing nearly exponentially recently. With more than 400 mil-lion tweets posted daily, Twitter offers an abundant informa-tion source of personal data. On Twitter, users can publish and share information in short posts or tweets (with a limi-tation of 140 characters) about topics ranging from daily life to news events and other interests. This huge collection of tweets embody invaluable information and knowledge about named entities, which appear in tweets frequently. However, as being free text form, named entity mentions in tweets are potentially ambiguous: the same textual name may refer to several different real world entities. As the example shown in Figure 1, the entity mention  X  X un X  in tweet t 2 can refer to the star at the center of the Solar System, a multinational computer company, a fictional character named  X  X un-Hwa Kwon X  or many other entities named  X  X un X .

Recently, the success of Wikipedia and the proposed vision of Linked Open Data (LOD) [2] have facilitated the auto-mated construction of large scale general-purpose machine-understanding knowledge base about the world X  X  entities, their semantic categories and the relationships between them. Such kind of notable endeavors include DBpedia [1], YAGO [21], Freebase [3], and Probase [23]. Bridging these knowl-edge bases with the collection of tweets is beneficial for exploiting and understanding this huge corpus of valuable personal data on the Web, and also helps to populate and enrich the existing knowledge bases [18]. We define tweet entity linking as the task to link the textual named entity mentions detected from tweets with their mapping entities existing in the knowledge base. If the matching entity of certain entity mention does not exist in the knowledge base, NIL (denoting an unlinkable mention) should be returned for this entity mention.

The tweet entity linking task is of practical importance and can be beneficial for various applications of Twitter mining, such as news and trend detection, commercial brand management, and personalized recommendation [22, 5]. For instance, detecting and linking named entities Twitter users mention in their tweets are significantly helpful for the task of user interest discovery [16, 24], which further allows for recommending and searching Twitter users based on their topics of interest [22]. In addition, there are also applications that utilize user interest to recommend and rank tweets for Twitter users [5, 6]. As another example, the needs to col-lect opinions or information about some products, celebrities or some other named entities from Twitter also require the process of linking entities in tweets with knowledge base.
In recent years, considerable progresses have been made in linking named entities in Web documents with a knowl-edge base [4, 7, 12, 11, 9, 20]. These previous methods mainly focus on entity mentions detected from Web docu-ments, and the main idea is to define a context similarity measure between the text around the entity mention and the document associated with the entity in the knowledge base. However, for the tweet entity linking task, these con-text similarity based methods may result in unsatisfactory performance owing to the informal language usage and the limited length of each individual tweet, which is confirmed by our experiments. For the example in Figure 1, the con-texts in tweets t 1 and t 3 are particularly short and ambigu-ous, which lack sufficient context information in order to link mentions  X  X ulls X  in t 1 and  X  X cott X  in t 3 correctly.
Besides leveraging context similarity, many previous ap-proaches assume that entities mentioned from a single doc-ument are likely to be topically coherent, and they consider the interdependence between different entity assignments within a document to jointly link these mentions [7, 12, 11, 9, 20]. Despite that each individual tweet satisfies this as-sumption, these previous methods still may not work well for the tweet entity linking task due to the limited number of entities appearing within each single tweet, which is also confirmed by our experiments. For the example in Figure 1, there is only one entity detected in tweet t 1 and tweet t , respectively. Therefore, we cannot leverage the topical coherence between different entities within the single tweet to help link these mentions.

To solve the problem of insufficient information in a single tweet, many Twitter mining approaches simply gather the tweets published by an individual user into a big document for processing [22, 5]. However, this simple method can-not be applied to the tweet entity linking task because this aggregated document violates the assumption that entities mentioned from a single document are topically coherent. As the example shown in Figure 1, tweets t 1 and t 4 talk about the NBA teams and basketball players, while tweets t and t 3 talk about IT companies and businessmen which are totally unrelated to the topics in tweets t 1 and t 4 sum up, it can be seen that the tweet entity linking task is challenging and different from the task of linking entities in Web documents.

Given a knowledge base about the world X  X  entities, for the named entity mentions detected from all tweets of a given Twitter user, our goal is to collectively link them to their mapping entities existing in the knowledge base by leverag-ing the following two categories of information: Intra-tweet local information. Intuitively, a candidate mapping entity is  X  X ood X  with respect to some entity men-tion within a single tweet if it has three key properties: (1) the prior probability (its definition is given in Formula 2) of the entity being mentioned is high; (2) the similarity be-tween the context around the entity mention in the tweet and the context associated with the candidate mapping en-tity is high; (3) the candidate mapping entity is topically coherent with the mapping entities of the other entity men-tions (if the tweet has) within the same tweet. Therefore, we could leverage the three intra-tweet local features to link the entity mentions in tweets. For example, in Figure 1, for the entity mentions in tweet t 2 , we can easily figure out the mention  X  X cNealy X  refers to Scott McNealy , the CEO of Sun Microsystems, and the mention  X  X un X  refers to the company entity Sun Microsystems , as the prior probabil-ity of the entity Scott McNealy for the mention  X  X cNealy X  is very high, and the mapping entities Scott McNealy and Sun Microsystems are highly topically coherent. Similarly, for the entity mentions in tweet t 4 , knowing the mention  X  X yson Chandler X  refers to the NBA player Tyson Chandler and the mention  X  X BA X  refers to National Basketball Asso-ciation , it could help us link the mention  X  X ony Allen X  to the NBA player Tony Allen (basketball) rather than the mu-sician that has higher prior probability compared with the NBA player entity.
 Inter-tweet user interest information. As stated above, a single tweet may be too short and noisy to provide suf-ficient context information for linking the entity mentions with their corresponding mapping entities correctly (e.g., the entity mentions  X  X ulls X  in t 1 and  X  X cott X  in t 3 ). To deal with this problem, we exploit the user interest information across tweets published by one individual user by model-ing the user X  X  topics of interest. We assume each user has an underlying topic interest distribution over various topics of named entities. If a candidate mapping entity is highly to pically related to entities the user is interested in, we as-sume this user is likely to be interested in this candidate entity as well. For the example in Figure 1, since the NBA players Tyson Chandler , Tony Allen (basketball) and the en-tity National Basketball Association are deemed likely to be the entities the user mentioned in tweet t 4 , we consider this user is interested in these entities. Henceforth, with regard to the entity mention  X  X ulls X  in t 1 , we predict that this user is more likely to be interested in the NBA team Chicago Bulls rather than the South African rugby union team Bulls (rugby) and the small town Bulls, New Zealand , as the en-tity Chicago Bulls is highly topically related to the entities mentioned in t 4 that the user is interested in. Similarly, since the CEO Scott McNealy is likely to be the entity user mentioned in tweet t 2 , among the candidate entities for the mention X  X cott X  X n t 3 , the user is more likely to be interested in the same entity Scott McNealy , rather than the wrestler Scott Steiner and the novelist Walter Scott .
 Contributions. The main contributions of this paper are summarized as follows.
In this section, we begin by describing the tweet and the task of tweet entity linking. Next, we introduce the genera-tion of candidate mapping entities for each entity mention. Tweet. A tweet is a short textual post comprising a max-imum of 140 characters published by a Twitter user. In Figure 1, there are four tweets published by a Twitter user. When a user wants to refer to another user in his tweet, he needs to add the  X  X  X  symbol before the referred user name to mention that user according to Twitter standards. Here, the meaning of mention is different from the mean-ing of entity mention. From now on, we call it Twitter mention . In tweet t 2 in Figure 1,  X  X jniccolai X  is a Twit-ter mention of the Twitter user whose name is  X  X niccolai X . Word prefixed with the  X # X  symbol, like  X #NBA X  in tweet t in Figure 1, is hashtag, usually referring to a topic. In addition, there are some shortened URLs in tweets, such as  X  X ttp://t.co/YGmByJMC X  in tweet t 4 in Figure 1. Twitter users often mention named entities in their tweets. As shown in the example in Figure 1, there are totally seven named en-tities in these four tweets. According to the statistics of the manually annotated tweet corpus used in our experiments, about 45.08% of tweets have at least one named entity. Re-cently, several methods [14, 13] have been proposed to ad-dress the problem of named entity recognition for tweets. However, this problem is orthogonal to our task because our task takes the recognized named entity mentions as input. Tweet entity linking. According to the task setting, we take (1) a collection of tweets posted by some Twitter user (denoted by T ), and (2) named entity mentions recognized in the given tweets T (denoted by M ) as input. Let | T | the number of tweets in T . We use 1  X  i  X  | T | to index the tweet in T , and the tweet with index i is denoted by t i set of named entity mentions recognized in each tweet t i is denoted by M i , and M i  X  M . Let | M i | be the number of entity mentions in the set M i . We use 1  X  j  X  | M i | index the entity mention in M i , and the entity mention with index j in mention set M i is denoted by m i j . Each entity mention m i j is a token sequence of a named entity that is po-tentially linked with an entity in the knowledge base. Here, we formally state the tweet entity linking task as follows:
Definition 1 ( Tweet entity linking ). Given the tweet collection T posted by some Twitter user and named entity mention set M , the goal is to identify the mapping entity e in the knowledge base for each entity mention m i j  X  M . If the mapping entity of entity mention m i j does not exist in the knowledge base, we should return NIL.
 In this paper, the knowledge base we adopt is YAGO [21], an open-domain ontology combining Wikipedia and WordNet with high coverage and quality. YAGO uses unique canoni-cal strings from Wikipedia as the entity names. Currently, YAGO contains over one million entities. For illustration, we show a running example of the tweet entity linking task.
Example 1. In this example, we just consider the tweet entity linking task for the tweets t 1 and t 4 shown in Figure 1 for the purpose of simplicity. There are four entity men-tions (i.e.,  X  X ulls X  in t 1 ,  X  X yson Chandler X ,  X  X ony Allen X , and  X  X BA X  in t 4 ), which need to be linked in this example. The candidate mapping entities for each entity mention are shown using arrows in Figure 1. For each entity mention, we should output its true mapping entity, which is underlined in Figure 1.
 Candidate mapping entity. For each entity mention m may be referred by the token sequence of m i j . Therefore, we firstly identify all the entities that may be referred by m j , and denote this set of entities as the candidate map-ping entity set R i j for the entity mention m i j . To iden-tify R i j for each m i j , we need to build a dictionary D that contains vast amount of information about various surface forms of the named entities, like name variations, abbrevi-ations, spelling variations, nicknames, etc. The dictionary D is a  X  key, value  X  mapping, where the column of the key K is a list of surface forms and the column of the mapping value K:value is the set of named entities which can be re-ferred by the key K . We construct the dictionary D by leveraging the four structures of Wikipedia: Entity page , Redirect page , Disambiguation page and Hyperlink in Wikipedia article . The detailed construction method is introduced in [19, 20]. A part of the dictionary D is shown in Table 1.

For each entity mention m i j  X  M , we look up the dictio-nary D and search for m i j in the column of key K . If a hit is found, i.e., m i j  X  K , we add the set of entities m i to the candidate mapping entity set R i j . We denote the size entity in R i j . The candidate mapping entity with index q in R j is denoted by r i j,q . Let R = ca ndidate mapping entity sets for all the entity mentions in M . For the example in Figure 1, for each entity mention, we show its candidate mapping entity set generated from the dictionary D using arrow, and the true mapping entity is underlined. It is noted that in Figure 1 each candidate entity has the name which is the unique canonical string for that entity in Wikipedia, and the size of the candidate map-ping entity set for most of the entity mentions (5 out of 7) is larger than 1. Henceforth, for each entity mention, we have to figure out which entity in its candidate mapping entity set is the mostly proper link for it, which will be addressed in the next section.
To address the tweet entity linking task, we propose a novel graph-based framework KAURI to collectively link all the entity mentions in all tweets of one user by modeling this user X  X  topics of interest. We have the following three assumptions:
Assumption 1. Each Twitter user has an underlying topic interest distribution over various topics of named enti-ties. That is to say, for each Twitter user, each named entity is associated with an interest score, indicating the strength of the user X  X  interest in it.

Assumption 2. If some named entity is mentioned by a user in his tweet, that user is likely to be interested in this named entity.

Assumption 3. If one named entity is highly topically related to the entities that a user is interested in, that user is likely to be interested in this named entity as well.
Based on these three assumptions, we model the tweet entity linking problem into a graph-based interest propaga-tion problem. Firstly, for each Twitter user, we construct a graph of which the structure encodes the interdependence information between different named entities, which will be introduced in Section 3.1. Next, we estimate the initial in-terest score for each named entity in the graph based on the intra-tweet local information according to Assumption 2, which will be depicted in Section 3.2. Lastly, we propose a graph-based algorithm, called user interest propagation al-gorithm , to propagate the user interest score among differ-ent named entities across tweets using the interdependence structure of the constructed graph according to Assump-tion 3, which will be illustrated in Section 3.3. Thus, our proposed framework KAURI integrates the intra-tweet local information with the inter-tweet user interest information into a unified graph-based model via modeling the user X  X  interest.
We propose to turn the tweet entity linking problem into a graph-based interest propagation problem. Thus, we firstly introduce how to construct the graph that captures the in-terdependence information between different named entities.
For one Twitter user, given the candidate mapping entity set R , we construct a graph G = ( V; A; W ) to represent all the interdependence information between these candidate mapping entities in R , where V denotes the set of nodes, A is the set of edges, and W : A  X  R + is the weight func-tion which assigns a positive weight to each edge in A . The node set V consists of the nodes which come from all the candidate mapping entities in R . One node in the graph represents a candidate mapping entity r i j,q and it is associ-ated with an interest score s i j,q indicating the strength of the user X  X  interest in it, as well as an initial interest score p estimated from the intra-tweet local information. For each them is larger than 0, we add an undirected edge ( r i j,q pendence between them. If the two candidate entities of mention (i.e., i = i  X  and j = j  X  ), we disallow the propagation between them and do not add an edge between them, since only one of them may be the true mapping entity.

According to Assumption 3, if two candidate entities are more interdependent or topically related to each other, the interest score should be propagated in a larger extent to each other. Thus, the edge weight is defined as the topi-cal relatedness between the two candidate entities. In our framework, since all the candidate entities are Wikipedia ar-ticles, we adopt the Wikipedia Link-based Measure (WLM) described in [17] to calculate the topical relatedness between Wikipedia articles. The WLM is based on the Wikipedia X  X  hyperlink structure. The basic idea of this measure is that two Wikipedia articles are considered to be topically related if there are many Wikipedia articles that link to both. Given two Wikipedia articles u 1 and u 2 , we define the topical re-latedness between them as follows:
T R ( u 1 ; u 2 ) = 1  X  log ( max ( where U 1 and U 2 are the sets of Wikipedia articles that link to u 1 and u 2 respectively, and W P is the set of all articles in Wikipedia. This definition gives higher value to more topically related article pair and the value of T R ( u 1 varied from 0.0 to 1.0.
 In Figure 2, we show the constructed graph for Example 1. Each node in Figure 2 represents a candidate mapping entity for some entity mention in Example 1. Each edge in Fig-ure 2 indicates the topical relatedness relationship between ca ndidate entities, and the value shown beside the edge is the edge weight calculated using Formula 1. For illustration, each true mapping entity is shaded in the figure. From Fig-ure 2, we can see that there is a strong topical relatedness relationship between any two of the true mapping entities, which demonstrates that the measure of topical relatedness in Formula 1 effectively captures the interdependence rela-tionship between them. On the contrary, between the true mapping entity and false mapping entity, the topic related-ness is either none (e.g., between Tyson Chandler and Bulls, New Zealand ), or a little weak (e.g., between National Bas-ketball Association and Tony Allen (musician) ).

It is noted that in our framework, the edge weight can be measured by other features or combination of them, such as the temporal and spatial information embedded in tweets, or the content of the Web page corresponding to the shortened URLs in tweets. This gives flexibility to our framework in an efficient and simple way.
Each node r i j,q in the graph is associated with an initial interest score p i j,q . Based on Assumption 2, we can see that the more likely the candidate entity is the true mapping entity, the more interested the user is in this candidate en-tity. Thus, given tweet t i where entity mention m i j appears, we estimate the initial interest score p i j,q for the candidate entity r i j,q based on the intra-tweet local information in t Specifically, we leverage the following three intra-tweet lo-cal features in t i : (1) the prior probability of the candidate entity r i j,q being mentioned; (2) the similarity between the context associated with the candidate entity r i j,q and the context around the entity mention m i j in tweet t i ; (3) the topical coherence between candidate entity r i j,q and the map-ping entities for the other entity mentions within tweet t Prior probability: We have the observation that each can-didate mapping entity r i j,q  X  R i j having the same surface form m i j has different popularity, and some entities are very obscure and rare for the given surface form m i j . For the example in Figure 1, for the entity mention  X  X un X , the can-didate entity Sun-Hwa Kwon , a fictional character on the ABC television series Lost, is much rarer than the candi-date entity Sun , the star at the center of the Solar System, and in most cases when people mention  X  X un X , they mean the star rather than the fictional character simply known as  X  X un X . We formalize this observation via taking advantage of the count information from Wikipedia, and define the prior probability P p ( r i j,q ) for each candidate mapping entity r portion of the links with the surface form m i j as the anchor text which point to the candidate mapping entity r i j,q : where count ( r i j,q ) is the number of links which point to en-tity r i j,q and have the surface form m i j as the anchor text. For example, in Figure 1, the candidate mapping entities for each entity mention are ranked by their prior probabilities shown on the right of the figure in decreasing order. It can be seen that the notion of prior probability suitably expresses the popularity of the candidate mapping entity being men-tioned given a surface form.
 Context similarity: To calculate the context similarity feature Sim ( r i j,q ) for each candidate mapping entity r compare the context associated with the candidate entity r j,q with the context around the entity mention m i j in tweet t . For each candidate entity r i j,q , we collect each occurrence of entity r i j,q in the Wikipedia page corpus, and extract the context of r i j,q in a short window to compose the bag of words vector for r i j,q . To get the bag of words vector for the entity mention m i j in tweet t i , we firstly preprocess the tweet (i.e., remove the Twitter mentions ,  X # X  symbols, URLs, and all punctuation symbols). Then we extract the short window of words around each occurrence of m i j in tweet t i to compose the bag of words vector for entity mention m i j . Finally, we measure the cosine similarity of these two vectors weighted by TF-IDF as the context similarity feature Sim ( r i j,q each candidate entity r i j,q  X  R i j .
 Topical coherence: Recall that a single tweet satisfies the assumption that entities mentioned in it are likely to be topically coherent. Here, we exploit this topical coherence between entities in the single tweet to define the third fea-ture topical coherence for each candidate mapping entity. The topical coherence feature Coh ( r i j,q ) for each candidate mapping entity r i j,q is measured as the topical coherence be-tween the candidate mapping entity r i j,q and the mapping entities for the other entity mentions within tweet t i . In this paper, we measure the topical coherence between entities as the topical relatedness between them defined in Formula 1. Thus, given tweet t i where the entity mention m i j appears, we formally define the notion of topical coherence Coh ( r for each candidate mapping entity r i j,q  X  R i j as: where | M i | is the number of entity mentions in tweet t e is the mapping entity for the entity mention m i c  X  M i , and T R ( r i j,q ; e i c ) is the topical relatedness between the can-didate entity r i j,q and the mapping entity e i c for other entity mention m i c  X  M i (where c  X  = j ) which can be calculated by Formula 1. However, the mapping entity e i c for entity mention m i c is unknown to us and needs to be assigned in this task. To solve this problem, we utilize a practical and effective greedy algorithm, called iterative substitution algo-rithm , to jointly optimize the identification of the mapping entities for the entity mentions in the single tweet by lever-aging the intra-tweet local information in this tweet. We proposed this iterative substitution algorithm to deal with the list linking task [19], and this greedy algorithm starts with a good guess of the initial mapping entity set, and then iteratively improves the quality of the mapping entity set until the algorithm converges. For the details about this algorithm, you could refer to [19].
 Initial interest score: Based on these three intra-tweet local features illustrated above, we calculate the initial in-terest score p i j,q for each candidate mapping entity r i as the weighted sum of prior probability , context similarity and topical coherence as follows: where + + = 1, and , and are weight factors that give different weights for different feature values in the calculation of initial interest score p i j,q . Here, we denote  X  X  X  w as the weight vector, and automatically learn the weight vector ma rgin technique based on the training data set, which is similar to the max-margin technique utilized in [19].
Using the above method, we can compute the initial inter-est score p i j,q for each candidate mapping entity r i j,q ample 1 based on its intra-tweet local information according to Formula 4. In Table 2, we show the result of the initial interest score for each candidate mapping entity in Exam-ple 1. From Table 2, we can see that the notion of initial interest score effectively captures the intra-tweet local infor-mation. Specifically, despite that the prior probability of the candidate entity Tony Allen (basketball) (i.e., 0.278) is much lower than the prior probability of the candidate entity Tony Allen (musician) (i.e., 0.722) with respect to the entity men-tion  X  X ony Allen X  in tweet t 4 , the initial interest score of the true mapping entity Tony Allen (basketball) (i.e., 0.155) is higher than the initial interest score of the candidate entity Tony Allen (musician) (i.e., 0.145). On the other hand, for tweet t 1 which lacks sufficient intra-tweet context informa-tion to link entity mention  X  X ulls X , the definition of initial interest score is unable to give the higher score to the true mapping entity (i.e., Chicago Bulls ) compared with the can-didate entity (i.e., Bulls (rugby) ), since the initial interest score only encodes the intra-tweet local information. Thus, it is very necessary to consider the inter-tweet user interest information for linking this mention.
In this section, we introduce the user interest propaga-tion algorithm , a graph-based algorithm to propagate the interest score among different candidate mapping entities across tweets using the interdependence structure of the con-structed graph G . For each Twitter user, we construct the graph G = ( V; A; W ), and let | V | be the number of nodes in V . We use 1  X  k  X  | V | to index the node in V , and the node with index k is denoted by v k . For each node v k in graph G , we can compute the initial interest score p k according to Formula 4. Then we can easily normalize it by the sum of the initial interest scores of all nodes in graph G : The edge between each pair of nodes  X  v k ; v k  X   X  in the graph G provides a path to propagate interest between these two nodes, and the edge weight W ( v k ; v k  X  ) indicates the strength of the interest propagation between each other. Therefore, a node will propagate more interest score to the node which has a stronger edge with it. However, we cannot directly the interest propagation strength because it is unnormalized raw weight. Formally, we define the interest propagation strength N W ( v k ; v k  X  ) from node v k to node v k  X  as: where V v k is the set of nodes each of which has an edge with node v k . For the graph shown in Figure 2, we can com-pute the interest propagation strength between nodes using Formula 6. For example, the interest propagation strength from node National Basketball Association to node Bulls (rugby) is 0.111, while the interest propagation strength from node Bulls (rugby) to node National Basketball As-sociation is 1.0.

Suppose that we want to compute the final interest score vector score for node v k to indicate the strength of the user X  X  inter-est in it. The final interest score s k of each node is decided by the combination of its initial interest score and the prop-agated score of other related nodes, and the final interest score vector tiplication. Let  X  X  X  p = ( np 1 ; :::; np k ; :::; np | V | ), where np k is the normalized initial interest score for node v k which can be calculated using Formula 5. Let B be a | V |  X  | V | interest propaga-tion strength matrix, where b k  X  ,k is the interest propagation strength from node v k to node v k  X  and b k  X  ,k = N W ( v which can be calculated using Formula 6. Thus, the matrix B is a column-normalized matrix. Formally, we define the computation of the final interest score vector where  X  [0 ; 1] is a parameter that balances the relative importance of the two parts (i.e., the initial interest score  X  X  X  p and the propagated score of other related nodes B the experiments, we show that the performance of our frame-work is insensitive to the parameter . From this definition, we can see that the final interest score vector evidences from the initial interest score vector dependence information between nodes B and the interest propagation process encoded as the product of B and Formula 7 is a recursive form, in order to calculate the final interest score vector rithm firstly initializes the final interest score vector setting til  X  X  X  can see that our user interest propagation algorithm is a little similar to the topic-sensitive PageRank algorithm [10] and the PageRank-like collective inference algorithm used in [9]. Since the interest propagation strength matrix B is square, stochastic, irreducible and aperiodic, our user interest prop-agation algorithm is guaranteed to converge according to [10].

Using the user interest propagation algorithm introduced above, we can compute the final interest score s i j,q for each candidate mapping entity r i j,q in Example 1 based on the g raph shown in Figure 2 and the initial interest scores shown in Table 2. We show the results in Table 3. From Table 3, we can see that the user interest propagation algorithm effectively gives the highest final interest score to the true mapping entity among all the candidate mapping entities for each entity mention in Example 1. In addition, for the true mapping entity Chicago Bulls , of which the initial interest score is not the highest among all the candidate mapping entities with regard to the entity mention  X  X ulls X  shown in Table 2, the user interest propagation algorithm can effec-tively leverage the inter-tweet user interest information to give it the highest final interest score among all the candi-date entities.
 Output of the tweet entity linking task: For each entity mention m i j  X  M and its corresponding candidate mapping entity set R i j , we compute the final interest score s i candidate mapping entity r i j,q  X  R i j using our framework in-troduced above. Then we identify the mapping entity e i j entity mention m i j as:
Since the mapping entity of some entity mention does not exist in the knowledge base, we have to deal with the prob-lem of predicting unlinkable mention. Firstly, if the size of candidate mapping entity set R i j generated for entity men-tion m i j is equal to zero, we predict entity mention m i an unlinkable mention and return NIL for mention m i j un-doubtedly. Otherwise, we can obtain e i j for entity mention m j according to Formula 8. Subsequently, we have to val-idate whether the entity e i j is the true mapping entity for mention m i j . We adopt a simple method and learn a nil threshold to validate the entity e i j . If the final interest score s i j,q of entity e i j is greater than the learned nil thresh-old , we return e i j as the mapping entity for entity mention m j , otherwise we return NIL for m i j . The nil threshold is learned by linear search based on the training data set.
To evaluate the effectiveness and efficiency of our frame-work KAURI, we present a thorough experimental study in this section. All the programs were implemented in JAVA and all the experiments were conducted on an IBM server (eight 2.00GHz CPU cores, 10GB memory) with 64-bit Win-dows. In this paper, we do not utilize the parallel computing technique and just consider single machine implementation.
To the best of our knowledge, there is no publicly avail-able benchmark data set for the tweet entity linking task. Thus, we created a gold standard data set for the tweet en-tity linking task. We used the Twitter API to collect 3,200 most recent tweets for each of randomly sampled 71,937 Twitter users. Since the annotation task for tweet entity linking consists of detecting all the named entity mentions in all the tweets and identifying their corresponding map-ping entities existing in YAGO(1) 1 of version 2008-w40-2, the annotation task is very time consuming. Therefore, la-beling all tweets for all users is impractical. To create the gold standard data set, we randomly sampled 20 non-verified Twitter users and for each sampled user, we annotated the h ttp://www.mpi-inf.mpg.de/yago-naga/yago/ 20 0 most recent tweets. For the Twitter user who has fewer than 200 tweets, we annotated all his tweets. Finally, we obtained 3,818 tweets from 20 users forming the gold stan-dard data set. A summary of this data set is shown in Table 4. From Table 4, we can see that 1,721 out of 3,818 tweets (about 45.08%) have at least one named entity mention. For 241 named entity mentions among the total 2,918 detected named entity mentions, we are uncertain to point out their mapping entities as the information contained in their tweets is so limited and noisy. Thus we dropped them and used the remaining 2677 mentions for evaluation.

We downloaded the May 2011 version of Wikipedia to construct the dictionary D introduced in Section 2. Mean-while, we employed these downloaded 3.5 million Wikipedia pages to obtain the context for each candidate entity for the calculation of context similarity feature introduced in Section 3.2. We set = 0 : 4 in the following experiments. The weight vector using 2-fold cross validation. To evaluate the effectiveness of KAURI, we calculated the accuracy as the number of cor-rectly linked entity mentions divided by the total number of all entity mentions.
Evaluation of effectiveness. To the best of our knowl-edge, KAURI is the first framework to deal with the tweet entity linking task. However, if we regard each tweet as a single document, we can apply all the previous methods for linking entities in Web documents [4, 7, 12, 11, 9, 20] to our data set. As it is impractical to implement all these previ-ous methods, we chose one of the state-of-the-art methods LINDEN [20] as our baseline method, which leverages both the semantic context similarity and the topical coherence between entities within a single Web document to link en-tity mentions with knowledge base. Besides our proposed framework KAURI, we also evaluated the performance of our framework which regards the initial interest score as the final score, and we call this truncated framework as LOCAL.
The experimental results are shown in Table 5. Besides the accuracy, we also show the number of correctly linked en-tity mentions for all methods with different types (i.e., link-able, unlinkable, and all). For the framework LOCAL and the framework KAURI, we not only show their performance by leveraging all the intra-tweet local features introduced in Section 3.2, which we refer as LOCAL full and KAURI full respectively, but also present their performance by leverag-ing a subset of the intra-tweet local features. For instance, culate the initial interest score using Formula 4, we set = 0 and = 0. Thus, in these two methods, we only leveraged the feature of prior probability to calculate the initial interest score.

The experimental results in Table 5 demonstrate that our proposed graph-based framework KAURI full significantly outperforms the baseline method LINDEN and all the differ-ent configurations of the framework LOCAL. Also, we can see that each configuration of the framework KAURI ob-tains much higher accuracy compared with the same config-uration of the truncated framework LOCAL, since we utilize the constructed graph described in Section 3.1 and the user interest propagation algorithm introduced in Section 3.3 in the framework KAURI. This means, the inter-tweet user in-terest information is very important and our user interest propagation algorithm is very effective for the tweet entity linking task, and integrating intra-tweet local information with the inter-tweet user interest information considerably boosts the performance. Moreover, it can be also seen from Table 5 that every intra-tweet local feature has a positive impact on the performance of the framework KAURI and LOCAL, and with the combination of all intra-tweet local features, KAURI full and LOCAL full can obtain the best results among the different configurations of the framework KAURI and LOCAL, respectively.

Sensitivity analysis. To better understand the perfor-mance characteristics of our proposed framework, we con-ducted sensitivity analysis to understand the influence of parameter to the performance of our framework. In Fig-ure 3, we show the performance of KAURI full with varied parameter . From the trend plotted in Figure 3, it can be seen that when 0 : 15  X   X  0 : 65, the accuracy achieved by KAURI full is all greater than 0.85. Thus, we can say that when is varied from 0.15 to 0.65, the performance of our framework is not very sensitive to parameter and we set = 0 : 4 in all the other experiments.

Evaluation of efficiency. For the purpose of efficiency, when we want to link the entity mentions in the recent tweet stream published by some Twitter user, it is unnecessary to consider the interdependence relationship between the re-cent tweet and the very old tweets published by the same user a long time ago. Thus, we define the tweet sliding win-dow as the set of sequential tweets published by one Twit-ter user, and the size of the tweet sliding window means the number of tweets contained in this window. Then we want to know the proper size of the tweet sliding window, which means just considering the interdependence informa-tion across tweets within such size of window, our framework can achieve high and stable accuracy for this task. There-fore, we conducted an experiment to find the proper size for the tweet sliding window. We show the accuracy achieved by KAURI full with varied size of the tweet sliding window in Figure 4. For illustration, for each user, we assign a tweet index to each tweet sequentially from 1 to 200. For the ex-perimental results shown in Figure 4, all the tweet sliding windows with different size are set to start from the tweet with index 1 for each user. From Figure 4, we can see that when the size of the tweet sliding window is larger than or equal to 150, the accuracy achieved by KAURI full is greater than 0.855 and remains relatively stable.

Accordingly, when we receive some new tweet stream posted by some user, we just need to consider the interdependence relationship across tweets within the latest tweet sliding win-dow of size 150 in order to link the entity mention in the new tweet stream. Moreover, since our graph-based framework KAURI naturally supports the incremental update opera-tion and we can store the constructed graph for the for-mer tweet sliding window, in order to construct the new graph for the latest tweet sliding window, we just need to incrementally modify the existing graph (i.e., remove some nodes out of the latest window and add some new nodes appearing in the latest window), which will save lots of time and cost. To evaluate the incremental update performance of KAURI full , we firstly preformed the tweet entity link-ing task for the initial tweet sliding window of size 150 for each user (starting from the tweet with index 1 to the tweet with index 150). The overall accuracy is 0.856, and the total annotation time is 251.35s for 1,952 entity mentions contained in this window. Roughly 70% of the annotation time is spent in graph construction. Then, we evaluated the incremental update performance of KAURI full and as-sume that at each time we received 10 new tweets for each user. Table 6 shows the performance of KAURI full at each time. From Table 6, we can see that at each time when new tweets are received, KAURI full achieves stable high accu-racy and spends about linear incremental annotation time to the number of added edges upon incrementally modifying the existing graph, which shows the scalability of KAURI. On average, the incremental annotation time per entity men-tion is between 14.03ms and 22.53ms, which demonstrates th e efficiency of KAURI. In addition, taking time 5 as an example, if we construct the graph for the tweet sliding window from empty rather than incrementally modify the existing graph for the tweet sliding window at time 4, the needed annotation time is 304.51s, about 7.2 times of the incremental annotation time at time 5 (i.e., 42.50s), which also demonstrates the efficiency of KAURI.
Twitter has received more and more attention from re-search community recently. The work relevant to our tweet entity linking task include named entity recognition for tweets [14, 13], influential Twitter user identification [22], tweet rec-ommendation [5, 6], and user interest discovery [16, 24]. The task similar to our tweet entity linking task is concept linking with Wikipedia for tweets [15]. The goal is to identify rel-evant concepts (such as Education , Shopping , Meeting , Stu-dent , etc.) in tweets and generate links to the corresponding Wikipedia articles. Another recent work related to our work is object matching for tweets proposed in [8]. In [8], the au-thors exploited the geographic aspects of tweets, and used a probabilistic model to infer the matches between tweets and restaurants from a list of restaurants. Compared with these two work, our tweet entity linking task focuses on linking named entities of general types (e.g., person, location, orga-nization, and event) rather than the concepts or the tweets, and links named entity mentions with the general-purpose knowledge base rather than the restaurant list.

Another thread of related work is linking named entities in Web documents (or Web lists) with knowledge base [4, 7, 12, 11, 9, 19, 20]. The work [12] models local mention-to-entity compatibility in combination with the pairwise intra-document topical coherence of candidate entities using a probabilistic graphical model. In [9], they proposed a graph-based collective entity linking method to model the topical interdependence between different entity linking decisions within one document. LINDEN [20] gives a rank to the can-didate entities with the linear combination of four features (i.e., the entity popularity, semantic associativity, semantic similarity and the global topical coherence between map-ping entities). The intra-tweet local features adopted by KAURI, prior probability , context similarity , and topical co-herence , have been used in various ways by these previous work. However, applying the local features alone leads to unsatisfactory performance over the tweets due to the noisy, short and informal nature of tweets, which has been dis-cussed before and confirmed by our experiments. In KAURI we propose a unified graph-based framework by combining intra-tweet local information with the inter-tweet user inter-est information, which is very effective in the new problem setting of tweet entity linking.
In this paper, we have studied a new problem on tweet en-tity linking. We propose KAURI, a framework which can ef-fectively link named entity mentions detected in tweets with a knowledge base via user interest modeling. KAURI com-bines both the intra-tweet local information and the inter-tweet user interest information into a unified graph-based framework. The experimental results show that KAURI achieves high performance in terms of both accuracy and efficiency, and scales well to tweet stream.
This work was supported in part by National Natural Sci-ence Foundation of China under Grant No. 61272088 and National Basic Research Program of China (973 Program) under Grant No. 2011CB302206.
