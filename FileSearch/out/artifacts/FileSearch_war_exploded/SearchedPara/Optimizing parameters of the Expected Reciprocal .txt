 Most popular IR metrics are parameterized. Usually pa-rameters of these metrics are chosen on the basis of general considerations and not adjusted by experiments with real users. Particularly, the parameters of the Expected Recip-rocal Rank measure are the normalized parameters of the DCG metric, and the latter are chosen in an ad-hoc man-ner. We suggest an approach for adjusting parameters of the ERR metric that allows to reach maximum agreement with the real users behavior. More exactly, we optimized the parameters by maximizing Pearson weighted correlation between ERR and several online click metrics. For each click metric we managed to find the parameters of ERR that re-sult into its higher correlation with the given online click metric.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Metrics, Experimentation, Performance information retrieval measures, evaluation One of the most challenging problems in the field of Web Search is choosing an appropriate metric for learning and evaluating retrieval algorithms. Chapelle et al. suggested the Expected Reciprocal Rank (ERR) metric [1], which re-ceived wide recognition in the community. They calculated a correlation of the ERR and other IR measures with on-line click metrics to prove significance of the ERR metric. Chapelle et al. discovered that correlation of the ERR (and other measures) with click metrics varies for different types of queries (Navigational vs. Non-navigational, queries of various lengths) and different markets. It implies that for different tasks and different purposes different measures (or different parameters of the same measure) should be used. We suppose that one way to set a purpose of an IR sys-tem is to choose the target click-metric. For example, if we want our users not to visit a competitor X  X  search engine, then abandonment rate is an adequate click metric. If the focus is on the fast satisfaction of the users, then the position of the first click is an appropriate metric.

The ERR metric has a set of parameters. Each param-eter (weight) means the probability of getting completely satisfied after reaching a document with a certain relevance grade. Chapelle et al. suggested a method of setting these parameters using the gain parameters of the DCG metric: grades. Thereby, commonly used parameters of the ERR metric for a 5-grade scheme with grades Perfect , Excellent , The same set of parameters (also for a 5-grades scheme) was used at TREC 2010/2011 [2] and de facto became a standard. We argue that these parameters should be ad-justed more accurately and depend on the purpose (target click-metric) and market. Thus we suggest a method for optimizing these parameters by maximizing Pearson corre-lation between ERR and a target online click metric.
We followed Chapelle et al. and optimized weighted Pear-son correlation. Suppose that there are N configurations (a configuration is a query and an ordered set of results). For the i -th configuration, let x i be the value of ERR metric, y the value of the click metric, and n i the number of times this configuration is present in the data set. Then, the weighted correlation is computed as following: C ( x, y, n ) = where m x and m y are the weighted averages: x i as the value of ERR metric may be considered as a function of five variables: x i = x i ( P, E, G, F, B ), where params = ( P, E, G, F, B ) corresponds to the weights of the original parameters respectively.
 Perfect , Excellent , Good , Fair , Bad documents respectively. Thus C ( x, y, n ) may be considered as C x,y,n ( params ). So the optimization problem is formulated as following: where
Q = C x,y,n ( params )  X 
We added an extra summand to the target function Q to encourage the following essential requirement:
This requirement follows from the nature of the param-eters ( P, E, G, F, B ). Parameters a = 100 , k = 400 were selected experimentally to meet the latter requirement.
Any click metric may be used to optimize the correlation with. We examined the same 6 metrics as Chapelle et al.: MinRR, MaxRR, MeanRR, UCTR, SS and PLC [1] (Section 6.2).
We used query logs of a popular search engine for three months period. Queries generated by search bots were fil-tered using a proprietary bot filtering algorithm. We fol-lowed Chapelle et al. and considered only one-query ses-sions (60 minutes period was used to delimit sessions), that did not have clicks on additional SERP elements (such as ads). We sampled random 13,755 unique queries and fil-tered such of them for which click rate on additional SERP elements is higher than 0.10. We were guided by the fol-lowing reasons: if the CTR on the additional elements is high, then the cases with no such clicks (as mentioned we consider only such queries) are probably outliers. We then asked our judges to assess all result documents (with the common 5-grade system) that were actually shown to the users. As a result we got 10,134 queries (32,239 configura-tions) in 9,500,687 search sessions.
For each configuration from the data set the value of the target click metric y i was calculated as the average over all the sessions belonging to the given configuration. Next, for each configuration the ERR@10 measure was calculated and stored as the polynomial x i ( params ). For example, if the i -th configuration looks like &lt; Perfect , Good , Bad &gt; (we take 3 documents instead of the 10 in this example for the simplicity) then according to the definition of ERR:
Thus each configuration specifies the pair ( x ( params ) , y i ). That allows to calculate Q ( params ) as a function of params . Finally that function was opti-mized by the truncated Newton algorithm (SciPy.optimize package 1 was used). The results are presented in Table 1.
We described a method of tuning parameters of the ERR measure. For each target click metric the correlation of ERR with new parameters is higher. The most noticeable im-provement was obtained for the SS (Search Success) click metric. The reason is probably that this metric takes less noisy clicks into account and consequently it is easier to op-timize correlation with it.

We demonstrated that for different purposes (i.e. target click metric) there are different optimal parameters of the ERR measure. It is clear that the parameters that we ob-tained in the experiment are not universal and depend on the market and other specifics of the search engine under study. However, we believe that it is worthwhile to tune them in each case, if online click metrics are assumed to be indicators of search engine user satisfaction. In the future, we plan to experiment with other metrics and markets, dif-ferent types of queries. Besides, we are going to develop a method to optimize correlation with several click metrics simultaneously. [1] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [2] C. L. A. Clarke, N. Craswell, N. Craswell, and G. V. docs.scipy.org/doc/scipy/reference/tutorial/optimize.html
