 Crowdsourcing enables programmers to incorporate  X  X uman com-putation X  as a building block in algorithms that cannot be fully automated, such as text analysis and image recognition. Simi-larly, humans can be used as a building block in data-intensive applications X  X roviding, comparing, and verifying data used by applications. Building upon the decades-long success of declara-tive approaches to conventional data management, we use a similar approach for data-intensive applications that incorporate humans. Specifically, declarative queries are posed over stored relational data as well as data computed on-demand from the crowd, and the underlying system orchestrates the computation of query answers.
We present Deco , a database system for declarative crowdsourc-ing. We describe Deco X  X  data model, query language, and our pro-totype. Deco X  X  data model was designed to be general (it can be instantiated to other proposed models), flexible (it allows methods for data cleansing and external access to be plugged in), and prin-cipled (it has a precisely-defined semantics). Syntactically, Deco X  X  query language is a simple extension to SQL. Based on Deco X  X  data model, we define a precise semantics for arbitrary queries involv-ing both stored data and data obtained from the crowd. We then describe the Deco query processor which uses a novel push-pull hybrid execution model to respect the Deco semantics while coping with the unique combination of latency, monetary cost, and uncer-tainty introduced in the crowdsourcing environment. Finally, we experimentally explore the query processing alternatives provided by Deco using our current prototype.
 H.2.1 [ Database Management ]: Logical Design X  data models ; H.2.3 [ Database Management ]: Languages X  query languages declarative crowdsourcing, human computation
Crowdsourcing [7, 26] uses human workers to capture or gener-ate data on demand and/or to classify, rank, label or enhance ex-isting data. Often, the tasks performed by humans are hard for a computer to do, e.g., rating a new restaurant or identifying features of interest in a video. We can view the human-generated data as a data source , so naturally one would like to seamlessly integrate the crowd data source with other conventional sources, so the end user can interact with a single, unified database. And naturally one would like a declarative system, where the end user describes the needs, and the system dynamically figures out what and how to obtain crowd data, and how it must be integrated with other data. This overall vision, and the underlying issues and challenges, were outlined in our earlier paper [23].

In this paper, we realize our earlier vision to present Deco (short for  X  X eclarative crowdsourcing X ), a database system that answers declarative queries posed over stored relational data, the collective knowledge of the crowd, as well as other external data sources. Our goal is to make Deco appear to the end user as similar as possible to a conventional database system (a relational one in our case), while hiding many of the complexities of dealing with humans as data sources (e.g., breaking down large tasks into smaller ones, posting tasks to a marketplace and pricing them, dealing with latency, and handling errors and inconsistencies in human-provided data).
We describe the Deco data model, the query language and se-mantics, and the query processor.

While the idea of declarative access to crowd data is appealing and natural, there are significant challenges to address:
Our Deco design addresses these questions, trying to strike a bal-ance between too much generality, and achieving an elegant, im-plementable system. While it may not be immediately apparent to the reader, the design of Deco required significant effort as well as consideration of many possible alternative designs. In particular, we will argue in our related work section (Section 6) that Deco is more principled, general, and flexible than recent approaches for declarative crowdsourcing [9, 19, 20].

In summary, our contributions are the following:
Throughout the paper we use a running example that is by de-sign simple and a bit contrived, yet serves to illustrate the major challenges of declarative crowdsourcing, and to motivate our solu-tions to these challenges. Our users are interested in querying two logical databases: We have a restaurant relation, where each restaurant may have one or more addresses (multiple addresses indicate a chain), and each restaurant serves one or more cuisines . We also have ratings , as-sociated with a restaurant-address pair, since different outlets of a chain may be rated differently. We have a second relation contain-ing address information. We assume a restaurant X  X  address may be encoded in some fashion X  X erhaps as a string or a geolocation X  but we may want to pose queries involving, say, cities or zipcodes.
There are several components to Deco X  X  data model: In the remainder of this section we begin by illustrating each of the data model components informally, then we step through each component formally.

We use the term designer to refer to both the schema designer as well as the database administrator. We use the term user to refer to the end user or the application developer.
Consider the restaurant relation introduced in Section 1.1: Rest-aurant (name, address, rating, cuisine) . The designer designates name and address as anchor attributes , and rating and cuisine as dependent attributes . Informally, we can see that the pair of name and address attributes together identify the  X  X ntities X  in our relation while the other two attributes are properties of entities; we will see shortly the specific roles of anchor versus dependent attributes.
The raw schema corresponding to this specification of Restau-rant is shown in the lower half of Figure 1. These relations are the ones actually stored as tables in the back-end RDBMS. There is one anchor table ( RestA ) containing the anchor attributes, and one de-pendent table for each dependent attribute ( RestD1 and RestD2 ); dependent tables also contain some anchor attributes. (In general, both anchor and dependent attributes can be a group of attributes.) Recall that we associate cuisines with the restaurant name, and rat-ing with a name-address pair, since different branches of a restau-rant (such as NY and SF in Figure 1 X  X o save space, we use abbre-viated addresses) can have different ratings, but all branches serve the same kind of food. We will see in Section 2.5 how the raw schema are generated.

The top of the figure shows the original conceptual relation, which is the outerjoin of the raw tables with certain attribute values  X  X e-solved X  (explained shortly).

Now let us consider how our database might be populated. Per-haps we already have some restaurant name-address pairs, with or without ratings and/or cuisines. If so, Deco might ask human work-ers to specify ratings and/or cuisines given a restaurant name and/or address. Alternatively, Deco might ask human workers to specify restaurant names and addresses given a cuisine and/or rating, or to provide restaurant names without regard to ratings or cuisines. Referring to Figure 1, the designer can specify fetch rules that: These fetch rules are depicted at the bottom of the raw tables in Figure 1. There are many more fetch rules that may be used to populate this database, we return to this point later on.
Now suppose we X  X e obtained values for our raw tables, but we have inconsistencies or uncertainty in the collected data. One de-cision we made in Deco is to provide a conceptual schema that does not have uncertainty as a first-class component, however meta-data in both the raw and conceptual schemas (described later in Section 2.7) can be used to encode information about confidence, worker quality, or other aspects of collected data that may be use-ful to applications. To obtain conceptual relations that are  X  X lean X  from raw tables that may contain inconsistencies, we use resolu-tion rules , specified by the designer. In Figure 1 we illustrate two resolution rules: The semantics of a Deco database is defined based on a Fetch-Resolve-Join sequence. Every Deco database has a (typically in-finite) set of valid instances . A valid instance is obtained by log-ically: (1) Fetching additional data for the raw tables using fetch rules; this step may be skipped. (2) Resolving inconsistencies using resolution rules for each of the raw tables. (3) Outerjoining the re-solved raw tables to produce the conceptual relations. Note that the  X  X ntermediate X  relations between steps (2) and (3) are not depicted in Figure 1; in the figure we resolve and join in one step. Also it is critical to understand that the Fetch-Resolve-Join sequence is a logical concept only. When Deco queries are executed, not only may these steps be interleaved, but typically no conceptual data is materialized except the query result.

Note that valid instances could contain wildly varying amounts of data, from no tuples at all to several million tuples, and they are all valid. So, when a user poses a query to the database, the valid instance used to answer his query may be the one with no tuples at all. We therefore need a mechanism to allow the user to request that at least a certain number of tuples are returned, discussed further in Section 3.
Now let us begin formalizing the concepts illustrated in Sec-tion 2.1. The designer declares two conceptual relations, and des-ignates their anchor and dependent attributes. In our example: Each attribute is either an anchor attribute or a member of ex-actly one dependent attribute-group . Thus, to partition a relation X  X  attributes, it suffices to enclose dependent attribute-groups within square brackets. In our example, the pair of anchor attributes name and address identify individual restaurants, while rating and cui-sine are dependent attributes that are independent of each other. (We assume ratings are not associated with specific cuisines of a restaurant.) In relation AddrInfo , address is the anchor attribute; at-tributes city and zip are not independent of each other with respect to an address, so they form a single dependent attribute-group. The next subsection clarifies the purpose of these designations within the Deco data model.
For each conceptual relation, the schema designer must spec-ify one resolution rule for each dependent attribute-group, and one resolution rule for the anchor attributes treated as a group. Thus, a resolution rule takes one of the following two forms, where A , A and D are sets of attributes and f is a function. 1. A 0  X  D : f 2.  X   X  A : f In the first form, resolution function f takes as input a tuple of val-ues for the anchor ( A 0 ) attributes, and a set of tuples with values for the dependent ( D ) attributes. It produces as output a new (pos-sibly empty, possibly unchanged) set of dependent tuples. The idea is that function f  X  X leans X  the set of dependent values associated with specific anchor values, when the dependent values may be in-consistent or uncertain. In the second form, function f  X  X leans X  a set of anchor ( A ) values. In either case, if no cleaning is necessary then f can simply be the identity function.

Let us look at a set of resolution rules for our running example. (Rules RR 1 and RR 2 are displayed in Figure 1.) (Note the function specifications here are abstracted; in practice the Deco system requires resolution functions to adhere to a spe-cific API.) The interpretation of all these rules is straightforward from their description, and can be found in our extended techni-cal report [22]. As an example, the three tuples corresponding to (Subway,SF,*) in raw table RestD1 in Figure 1 are resolved us-ing resolution function RR 1 into a single tuple (Subway,SF,3.9) , which then participates in the join with RestA and RestD2 .
Readers inclined towards dependency theory may already have noticed that that resolution rules suggest multivalued dependen-cies on the conceptual relations. In relation Restaurant we have the multivalued dependencies name,address  X  X  X  rating and name  X   X  cuisine . Furthermore, when a resolution rule for a dependent attribute-group is guaranteed to produce exactly one value, we have a functional dependency. For instance, name and address function-ally determine rating , since the resolution rule for rating produces exactly one value.
The schema designer may specify any number of fetch rules. Un-like resolution rules, fetch rules may be added or removed at any time during the lifetime of a database X  X hey are more akin to  X  X c-cess methods X  than to part of the permanent schema. A fetch rule takes the following form: where A 1 and A 2 are sets of attributes from one relation (with A 1 =  X  or A 2 =  X  permitted, but not both), and P is a fetch procedure that implements access to human workers or other exter-nal sources. ( P might generate HITs (Human Intelligence Tasks) to Amazon X  X  Mechanical Turk [1], for example, but nothing in our model or system is tied to AMT specifically.) The only restriction on fetch rules is that if A 1 or A 2 includes a dependent attribute D , then A 1  X  A 2 must include all anchor attributes from the left-hand side of the resolution rule containing D . In other words, if A the left hand side of a resolution rule containing dependent attribute D , then, if D  X  A 1  X  A 2 , then A D  X  A 1  X  A 2 . We will see the reason for this restriction in Section 2.6.

Let us look at a set of possible fetch rules for our running ex-ample. We use R and A to abbreviate Restaurant and AddrInfo respectively. (Rules FR 1 , FR 2 , FR 4 , and FR 5 are displayed in Fig-ure 1.) The interpretation of all these rules is straightforward from their description, and can be found in the extended technical report. We describe a couple of them here: FR 5 accesses humans or other ex-ternal sources to get restaurant name-address pairs, without any input, while FR 8 performs verification: it gets the input pair of name X  X ddress verified with a yes/no answer.

Invocations of the Fetch Rule FR 1 (with (Limon,SF) ), FR Limon ), FR 4 (with French ), FR 5 (with no input) are depicted in Figure 1. Note that these fetch rules affect at most one dependent raw table; other fetch rules, such as FR 9 , affect multiple dependent raw tables.

There are many, many more possible fetch rules for our running example. Which fetch rules are used in practice may depend on the capabilities (and perhaps costs) of the human workers and other ex-ternal data sources, and perhaps the programming burden of imple-menting a large number of fetch procedures. Also remember that fetch rules are not set in stone X  X hey can be added, removed, and modified as capabilities change or tuning considerations dictate. The raw schema X  X or the tables actually stored in the underlying RDBMS X  X epends only on the definitions of the conceptual rela-tions, anchor and dependent attributes, and resolution rules. Specif-ically, for each relation R in the conceptual schema, the raw schema contains: From the conceptual schema in Section 2.2, and the resolution rules in Section 2.3, it is straightforward to derive the following raw schema: For readers inclined towards dependency theory: Continuing from the discussion at the end of Section 2.3, we can see that the raw schema is in fact a Fourth Normal Form (4NF) decomposition of the conceptual schema based on the multivalued dependencies im-plied by the resolution rules.

Next we define the data model semantics, which centers around the mapping from raw tables to conceptual relations.
The semantics of a Deco database at a given point in time is de-fined as a set of valid instances for the conceptual relations. The valid instances are determined by the current contents of the raw tables, the potential of the fetch rules, and the invocation of resolu-tion rules before outerjoining the raw data to produce the concep-tual relations. Let us consider this Fetch-Resolve-Join sequence in detail. Note that these three steps may not actually be performed, but query answers must reflect a valid instance that could have been derived by these three steps, as we will discuss in Section 3. The current contents of the raw tables may be extended by in-voking any number of fetch rules any number of times, inserting the obtained values into the raw database. Consider a fetch rule A 1  X  A 2 : P with A 2 6 =  X  . By definition, procedure P takes as input a tuple of values for the attributes in A 1 , and produces as output zero or more tuples of values for the attributes in A can equivalently think of an invocation of the fetch rule as return-ing a set T of tuples for A 1  X  A 2 . Note the input values for A may come from the database or from a query (see Section 4), but for defining semantics we can assume any input values.

The returned tuple set T with schema A 1  X  A 2 may include any number of attributes from any number of raw tables. Consider any of the raw tables T ( A ) , and let B = A  X  ( A 1  X  A attributes of T present in T . If B is nonempty, we insert  X  into table T , assigning NULL values to the attributes of T that are not present in T (i.e., the attributes in B  X  A ). Informally, we can think of the process as vertically partitioning T and inserting the partitions into the corresponding raw tables, with anchor attributes typically replicated across multiple raw tables. The one guarantee we have, based on our one fetch rule restriction (Section 2.4), is that no NULL values are inserted into anchor attributes of dependent tables. This property simplifies the resolution process, discussed shortly. As an example, fetch rule FR 9 above would insert a tuple in all three restaurant raw tables. Note that inserting tuples in this manner may end up proliferating duplicate tuples, especially in the anchor raw table. As an example, if we have a name-address pair in RestA , then every time we use the name-address pair in the fetch rule name,address  X  rating , we end up adding an extra copy of the same name-address pair to RestA . If duplicates may pose a problem, then the origin of the tuples (i.e., the fetch rule invocation that added them) can be recorded in the metadata (Section 2.7) and used by the resolution function.

Lastly, consider a fetch rule A 1  X   X  : P . In this special case where a fetch rule is being used for verification (recall Section 2.4), we proceed with the insertions as described above, but only when P returns a yes value. If the no values are important for resolution, a more general scheme for verification may be used, and is described in Section 2.7 below. After gathering additional data via fetch rules, each anchor and dependent table in the raw schema is logically replaced by a  X  X e-solved X  table. Consider a dependent table T ( A 0 , D ) with corre-sponding resolution rule A 0  X  D : f . We logically: (a) group the tuples in T based A 0 values; (b) call function f for each group; (c) replace each group with the output of f for that group. (Recall from Section 2.3 that f takes as input a tuple of values for the A attributes and a set of tuples with values for the D attributes. It pro-duces as output a new, possibly empty or unchanged, set of tuples for D .) Resolving an anchor table T ( A ) simply involves invoking the function f in the resolution rule  X   X  A : f with all tuples in T .

Note that NULL values may appear in anchor tables, and for dependent attributes in dependent tables. We assume if NULL val-ues are possible, then the corresponding resolution functions are equipped to deal with them. We need not worry about NULLs when grouping anchor attribute values (step (a) above), since our fetch-rule restriction (Section 2.4) ensures that all anchor values in dependent tables are non-NULL.

In Figure 1, resolution on the three RestD1 tuples correspond-ing to Subway, SF returns an average rating of 3.9. Since cuisine has a duplicate-elimination resolution function, resolution on the three tuples corresponding to Bouchon in RestD2 returns two tu-ples (one for French , one for Continental ). The final step is simple: For each conceptual relation R , a left out-erjoin of the resolved anchor and dependent tables for R is per-formed to obtain the final tuples in R . Note that an outerjoin (as opposed to a regular join) is necessary to retain all resolved val-ues, since values for some attributes may not be present. The left outerjoin must have the anchor table as its first operand, but other-wise the resolved dependent tables may be left outerjoined in any order. In Figure 1, resolved tuples (Subway,SF) from RestA , (Sub-way,SF,3.9) from RestD1 and (Subway,Burgers) from RestD2 join to give the tuple: (Subway,SF,3.9,Burgers) .

To recap, a valid instance of a Deco database is obtained by starting with the current contents of the raw tables and logically performing the three steps above, resulting in a set of data for the conceptual relations. Since in step 1 any number of fetch rules may be invoked any number of times (including none), we typically have an infinite number of valid instances. This unboundedness is not a problem; as we will see shortly, our goal is to deliver the result of a query over some valid instance, not over all valid instances.
We emphasize again that these three steps are logical X  X hey are used only to define the semantics of the data model. Of course some amount of fetching, resolving, and joining does need to oc-cur in order to answer queries, but not necessarily as  X  X ompletely X  as defined above, and not necessarily in Fetch-Resolve-Join order. Section 4 describes our query processing strategies that respect the semantics while meeting cost, performance, and quality objectives.
The Deco data model as described so far has precise formal un-derpinnings and relies heavily on the relational model with all of its benefits. However in reality there are several messy aspects of using crowdsourced (or other external) data that also must be accommodated X  X ieces of the crowdsourcing puzzle that we have chosen not to make first-class in our data model, yet are crucial for some applications. Examples include:
All of these examples can be handled by allowing additional metadata columns in the raw tables. Metadata columns can be used for timeout values, to store information about worker quality, to tally votes, and for confidence values. Metadata attributes can be included in the input and/or output of fetch procedures and resolu-tion functions. In our examples, data expiration and worker quality values might be returned by a fetch function; a resolution function might take worker quality and fetch rules as part of its input and re-turn confidence scores as part of its output. Metadata columns can be exposed in the conceptual relations too, if their contents may be of direct use to applications. The Deco query language and semantics is straightforward: We assume either relational algebra or SQL for posing queries. Since the Deco system supports SQL, we use SQL for examples in this paper.

Referring back to Section 2.6, recall that one valid instance of the database can be obtained by resolving and joining the current con-tents of the raw tables, without invoking any fetch rules. Thus, it appears a query Q can be always answered correctly without con-sulting human workers or other outside sources. The problem is that often times this  X  X orrect X  query result will also be empty. For example, if our query is: and there are currently no highly-rated Thai restaurants in our database, then the query result is empty.

To retain our straightforward semantics over valid instances while avoiding the empty-answer syndrome, we simply add to our query language an  X  AtLeast n  X  clause. This clause says that not only must the user receive the answer to Q over some valid instance of the database, but it must be a valid instance for which the answer has at least n tuples with non-NULL attributes. Adding  X  AtLeast 5 X  to the query above, for example, forces the query processor to collect additional data using fetch rules until it obtains the name and address values for at least five highly-rated Thai restaurants.
Our basic problem formulation for query processing, discussed in detail in the next section, thus requires a minimum number of tu-ples in query results, while attempting to optimize other dimensions such as number of fetches, monetary cost, response time, and/or re-sult quality. These considerations suggest other possibilities at the language level, for example: There are many interesting variants, but it is important to note that all of them rely on the same query language semantics: return the relational answer to Q over some valid instance of the conceptual relations while satisfying any performance-related constraints.
Having defined the Deco data model and query language, we now consider how to build a query processor that implements the defined semantics. The query processor must support constraints such as AtLeast while dealing with the inherent challenges in crowd-sourcing, such as latency, cost, and uncertainty.

We first describe Deco X  X  execution model, then we present a space of alternate plans that demonstrate Deco X  X  flexibility. Plan costing and selection is a topic of ongoing work.
The Deco data model and query semantics gives us some unique challenges to address at the execution-model level. To address these challenges, Deco uses a novel Push-Pull Hybrid Execution Model, drawing on a combination of ideas from incre-mental view maintenance [3] and asynchronous iteration (devel-oped in WSQ-DSQ [10]). It has the following features: Significant additional details of the Push-Pull Hybrid Execution Model are given in the extended technical report.
To show the flexibility of our execution model, we present four different query plans for the following query on the Restaurant relation from Section 1.1: As resolution functions, we use duplicate elimination for name-address , average of 3 (or more) tuples for rating , and majority of 3 (or more) for cuisine . We will see in Section 5 how these different query plans perform in terms of execution time and monetary cost. Basic Plan: Figure 2(a) shows a basic query plan that uses the fetch rules  X   X  name,addr (operator 7), name,addr  X  rating (operator 10), and name  X  cuisine (operator 13). At a high level, the plan performs an outerjoin (operator 4) of a resolved version of RestA (operator 5) and a resolved version of RestD1 (operator 8), followed by a filter on rating (operator 3). Then the result is out-erjoined (operator 2) with a resolved version of RestD2 (operator 11). The root operator (operator 1) stops the execution once five output tuples are generated.
 Predicate Placement: Alternatively, we can place the Filter oper-ator above both of the DLOJoin or Dependent Left Outerjoin oper-ators 16 and 17 (Figure 2(b)).

Due to the nature of our execution model, this simple transfor-mation has more significant implication. The plan in Figure 2(a) will fetch cuisine only for the restaurants that satisfy rating &gt; 4 , so it may have lower monetary cost. On the other hand, the plan in Figure 2(b) increases the degree of parallelism by fetching rating and cuisine at the same time, while having a higher monetary cost. Reverse Fetches: Another interesting alternative plan can use re-verse fetch rules. For our query Q , suppose the predicate rating &gt; 4 is very selective. If we use the query plans in Figure 2, even ob-taining a single answer could be expensive in terms of latency and monetary cost, because we are likely to end up fetching a number of restaurants that do not satisfy the predicate.

Instead, we can use the reverse fetch rule rating  X  name,address underneath the Resolve operator to start with a restaurant with a cer-tain rating (according to at least one worker) instead of a random restaurant. Figure 3(a) shows a query plan that uses this reverse fetch rule. Notice that the Fetch operator 7  X  X ushes X  tuples to both RestA and RestD1 via the Scan operators 6 and 9 (dotted arrows). Combined Fetches: It may be less expensive to use a fetch rule that gathers multiple dependent attributes at the same time, rather than fetching one attribute at a time. For the example query Q , we can use a combined fetch rule name,address  X  rating,cuisine in-stead of two fetch rules name,address  X  rating and name,address  X  cuisine . Figure 3(b) shows a query plan that uses this combined fetch rule. Notice that both Resolve operators 21 and 24  X  X ull X  more tuples from the Fetch operator 23.
To illustrate the types of performance results that can be obtained from Deco, we present two experiments. One studies how different fetch rule configurations affect query performance, while the sec-ond experiment evaluates different query plans given a particular fetch rule configuration. Given our space limitations, it is impossi-ble to present here more comprehensive results.
 Experimental Setup: We used the following conceptual relation: The anchor attribute name identifies a country, while the two de-pendent attributes language and capital are independent properties of the country. For resolution functions, we used dupElim (du-plicate elimination) for name , and majority-of-3 for language and capital . (We assume one language per country.) The resolution function majority-of-3 finds the majority of three or more tuples.
In addition, we created six fetch rules that use the Mechanical Turk fetch procedure to fetch data. Our experiments use one of the following fetch rule configurations: For attributes name and capital , workers were allowed to enter free text, but our question server validated this input to ensure that the text had no leading or trailing whitespaces and that all letters were uppercase. For language , workers were allowed to select one lan-guage from a drop-down list of about 90 languages.
 On Mechanical Turk, we paid a fixed amount of five cents per HIT (i.e., per fetch rule invocation) as compensation for completed work. All experiments were conducted on weekends in Feb 2012.
Our benchmark query requested eight Spanish-speaking coun-tries along with their capital cities.
 For each experiment, we begin with empty raw tables. According to Wikipedia, there are 20 Spanish-speaking countries in total. Experiment 1: Performance for Different Fetch Configurations: In our first experiment, we evaluated the query performance of different fetch rule configurations to study the usefulness of the flexibility of Deco fetch rules. Our fetch configurations are by default translated into query plans similar to Figure 2(a) (Basic), Figure 3(a) (Reverse), Figure 3(b) (Hybrid), where the raw table containing names is left outerjoined with the raw table containing names and languages, followed by a filter on language, and then the result is left outerjoined with the raw table containing names and capitals. However, the fetch operators in each of these plans use different fetch rules. For instance, for the Hybrid configuration, the fetch operator for the name raw table uses language  X  name,capital , while the (shared) fetch operator for language and capital raw ta-bles uses name  X  language,capital as the fetch rule.

We ran the same benchmark query with the three sets of fetch rules (therefore different query plans) on our Deco prototype [25]. For each set, we ran the query twice and noted similar results. Both runs are included in our graphs.

Figure 4(a) depicts the number of non-NULL result tuples ob-tained over time. Since our query specified AtLeast 8, reaching eight output tuples signifies the completion of query. In addition, Figure 4(b) shows the number of HITs submitted by workers over time. Note that the monetary cost of a query is proportional to the total number of HITs submitted.

Using the hybrid configuration, the query took 10.5 minutes and cost $1.35 for 27 worker answers on average (across two runs). Us-ing the reverse configuration, the query took 15 minutes and cost $2.30 for 46 worker answers on average. In comparison, the ba-sic configuration performed very poorly: the query took two hours overall and cost around $12.05. (We ended up collecting 64 coun-tries and their languages.)
Thus, we find that it is important to consider a variety of query plans, and the decisions of the query optimizer can significantly impact query performance in terms of latency and total monetary cost. Choosing fetch rules is a significant decision under the con-trol of the optimizer. In particular, for this benchmark query, those fetch rules that get multiple pieces of information at once (such as name  X  language,capital ), and those that operate in the reverse di-rection (such as language  X  name ) offer significant benefits over basic fetch rules. In general, it is important for a declarative crowd-sourcing system (like Deco) to handle fetch rules of different types, and thus increase the opportunities for finding a good execution plan. (Of course, we depend on the schema designer to take advan-tage of the flexibility and provide multiple fetch rules.)
Overall, our findings validate the role of fetch rules as  X  X ccess methods X  for the crowd, and reinforce the importance of including them in a principled query optimization framework.

We repeated our experiment with the constraint AtLeast 12 in-stead of AtLeast 8 (graphs not shown). While the relative per-formance of our configurations was similar, it was interesting to observe that AtLeast 12 queries did not take significantly longer than AtLeast 8 despite producing more answers, but they did in-cur higher cost. This behavior is because our execution model is-sues more fetches (i.e., HITs) in parallel (via bind messages) for AtLeast 12 than for AtLeast 8 , and a larger number of HITs in the same HIT group attract more workers. We plan to investigate trading off cost for time by varying the number of bind messages in future work.

In terms of the quality, the results were clean and correct except one typo ( X  X dominican Republic X ). Not suprisingly, individual an-swers had several errors that are not exposed in the result. Common errors for capital cities were for Spain and Bolivia.
 Experiment 2: Performance for Different Query Plans: In this experiment, we evaluated the query performance of different query plans under the reverse fetch rule configuration. Specifically, we studied the performance implication of pulling up the (language =  X  X panish X ) predicate. Our Deco prototype pushes all predicates down as much as possible by default and produces a query plan similar to Figure 3(a) in the reverse configuration. Under this plan (termed  X  X own X ), Deco does not invoke the fetch rule name  X  cap-ital until language is resolved to Spanish. On the other hand, under the plan (termed  X  X p X ) that is produced by applying the predicate pull-up transformation (similar to Figure 2(b)), Deco invokes fetch rules name  X  language and name  X  capital simultaneously as soon as a new country name is fetched.
 We ran the same benchmark query with these two query plans. Figure 4(c) depicts the number of non-NULL result tuples obtained over time for two runs each. The completion times for the  X  X p X  and  X  X own X  plans were 9 minutes and 15 minutes, respectively. In both cases, the cost of query was about $2.20. In general, the predicate pull-up transformation makes Deco query plans invoke fetch rules more  X  X peculatively, X  which results in more parallelism. More parallelism brings down latency, as we see in Figure 4(c). On the downside of pull-up, speculative fetch rule invocations can lead to higher overall cost. In this particular scenario, we did not observe a significant cost increase because the worker answers for the fetch rule language  X  name were quite accurate, and the speculative invocations of name  X  capital were not wasted.

The results reveal that, much like conventional query optimiza-tion, the placement of operators is crucial for plan performance, and in addition there are interesting interactions between fetch rules and other operators. Of course, the ability to explore alternative plans hinges on a principled framework for query optimization.
The prior work related to Deco fits into one of the following categories. We describe each category in turn.
 Crowdsourcing and Databases: There has been recent interest in the database community in using crowdsourcing as part of database query processing [5, 9, 19, 23]. The work on CrowdDB [9] is per-haps the most similar to the one presented in this paper. Over-all, Deco opts for more flexibility and generality, while CrowdDB made some fixed choices at the outset to enable a simpler design. A detailed comparison between the two systems can be found in our technical report [22]. We describe the key differences between Deco and CrowdDB here:
Qurk [19] is a workflow system that implements declarative crowd-sourcing, unlike Deco which is a database. (Like Deco, however, Qurk may retain outcomes of prior tasks for reuse or fitting clas-sifiers with the aim of reducing cost, using an underlying storage engine.) Since Qurk uses crowdsourcing primarily as part of its op-erators (filtering, joins), it is not as general in the kind of data it can obtain from the crowd.

Our prior work [23] outlines a wide variety of challenges in inte-grating crowdsourcing within a database. In this paper, we build on this work to design a principled and flexible data model and query processor to meet these challenges. Reference [5], while also deal-ing with crowdsourcing focuses mainly on the challenges underly-ing user feedback for information integration rather than a system for general declarative crowdsourcing.
 Crowdsourcing Programming Libraries: Turkit [17] and HProc [13] are programming libraries designed to allow programmers to interface with MTurk. These libraries support a procedural ap-proach where the programmer needs to specify all the tasks, com-bine their answers, orchestrate their execution and so on, whereas we advocate a declarative approach where the DBMS is responsible for these goals and also performs transparent optimizations. Prior work in Databases: There has been prior work in the database area on expensive predicate or user-defined function (UDF) evalua-tion as part of query processing [6, 12]. While calls to crowdsourc-ing services can be viewed as UDFs, UDF evaluations are much simpler than crowdsourcing calls, disallowing direct application of prior work in this area: (a) UDF evaluations return the same  X  X or-rect X  answer every time they are run. (b) Apart from the computa-tional cost of a UDF, there is no monetary cost. (c) There is only one way to evaluate a UDF, and two UDFs cannot be combined. (d) There is no advantage to evaluating multiple UDFs on differ-ent items at the same time. The work on WSQ-DSQ [10] is more relevant than the others in this area since it allows UDF calls to be issued  X  X n parallel X , and we do leverage the asynchronous query processing elements from WSQ-DSQ in our system.

There has been a lot of prior work over the last decade on uncer-tain databases with systems such as Trio [28], MystiQ [4], MayBMS [2] and PrDb [27]. In Deco, we have made a conscious decision not to expose uncertainty as a first-class construct to the end user. In-stead, we provide explicit control to the schema designer on how uncertainty should be resolved for each attribute group. This ap-proach is not only simpler (and similar to the forms of uncertainty resolution being used in crowdsourcing today), but it is more flex-ible, and also eliminates the computationally intractable aspects of dealing with uncertainty correlations between different attributes. Intuitively, our intention is less to manage and query uncertain data as it is to fetch and clean data.

Our data model is also related to the field of Global-As-View (GAV) data integration [14]. In particular, our fetch rules can be viewed as binding patterns or capabilities of sources [8, 15, 16], and the conceptual schema can be viewed as a mediated database. How-ever, the properties and behavior of humans and sources are differ-ent, necessitating different query processing techniques: First, hu-mans, unlike external data sources, need to be compensated mon-etarily. Second, subsequent invocations of the same fetch rule im-proves the quality of results since multiple humans would be con-sulted, unlike external sources where repeating the same query re-turns the same results. Third, different invocations of fetch rules can be done in parallel (i.e., many humans may address tasks in parallel), while external data sources have a single point of access. Crowdsourcing Algorithms: Recently, there has been an effort to design fundamental algorithms for crowdsourcing. In other words, the algorithms, such as sorting, filtering and searching use human workers to perform basic operations, such as comparisons, evalu-ating a predicate on an item, or rating or ranking items. This work is orthogonal to ours and can be used as part of the query proces-sor of our Deco system. So far, there have been papers on filtering items [21], finding max [11], searching in a graph [24], and sorting and joins [18].
We presented Deco, a system for declarative crowdsourcing. Deco offers a practical and principled approach for accessing crowd data and integrating it with conventional data. We believe that our no-tions of fetch and resolution rules provide simple but powerful mechanisms for describing crowd access methods. We also believe that our split conceptual/raw data model (top and bottom parts of Figure 1), together with our  X  X etch-Resolve-Join X  semantics, yield an elegant way to manage the data before and after cleansing. Our query processor utilizes a novel push-pull execution model to al-low worker requests to proceed in parallel, a critical aspect when dealing with humans and other high-latency external data sources. While our current Deco prototype does not yet perform sophisti-cated query optimization, our design and system do provide a solid foundation to study the cost, latency and accuracy tradeoffs that make crowdsourced data such an interesting challenge. [1] Mechanical Turk. http://mturk.com . [2] L. Antova, C. Koch, and D. Olteanu. MayBMS: Managing [3] J. A. Blakeley, P. Larson, and F. W. Tompa. Efficiently [4] J. Boulos, N. Dalvi, B. Mandhani, S. Mathur, C. Re, and [5] X. Chai, B. Vuong, A. Doan, and J. F. Naughton. Efficiently [6] S. Chaudhuri and K. Shim. Query optimization in the [7] A. Doan, R. Ramakrishnan, and A. Halevy. Crowdsourcing [8] D. Florescu, A. Levy, I. Manolescu, and D. Suciu. Query [9] M. J. Franklin, D. Kossmann, T. Kraska, S. Ramesh, and [10] R. Goldman and J. Widom. Wsq/dsq: A practical approach [11] S. Guo, A. Parameswaran, and H. Garcia-Molina. So who [12] J. M. Hellerstein and M. Stonebraker. Predicate migration: [13] P. Heymann and H. Garcia-Molina. Turkalytics: analytics for [14] M. Lenzerini. Data integration: a theoretical perspective. In [15] A. Levy, A. Rajaraman, and J. Ordille. Querying [16] C. Li and E. Chang. Query planning with limited source [17] G. Little, L. B. Chilton, M. Goldman, and R. C. Miller. [18] A. Marcus, E. Wu, D. Karger, S. Madden, and R. Miller. [19] A. Marcus, E. Wu, S. Madden, and R. Miller. Crowdsourced [20] R. McCann, W. Shen, and A. Doan. Matching schemas in [21] A. Parameswaran, H. Garcia-Molina, H. Park, N. Polyzotis, [22] A. Parameswaran, H. Park, H. Garcia-Molina, N. Polyzotis, [23] A. Parameswaran and N. Polyzotis. Answering queries using [24] A. Parameswaran, A. D. Sarma, H. Garcia-Molina, [25] H. Park, R. Pang, A. Parameswaran, H. Garcia-Molina, [26] A. Quinn and B. Bederson. Human computation: a survey [27] P. Sen and A. Deshpande. Representing and Querying [28] J. Widom. Trio: A System for Integrated Management of
