 Data Stream Management Systems performing on-line ana-lytics rely on the efficient execution of large numbers of Ag-gregate Continuous Queries ( ACQs ). The state-of-the-art WeaveShare optimizer uses the Weavability concept in or-der to selectively combine ACQs for partial aggregation and produce high quality execution plans. However, WeaveShare does not scale well with the number of ACQs . In this pa-per we propose a novel closed formula, F 1, that accelerates Weavability calculations, and thus allows WeaveShare to achieve exceptional scalability in systems with heavy work-loads. In general, F 1 can reduce the computation time of any technique that combines partial aggregations within composite slides of multiple ACQs . We theoretically ana-lyze the Bit Set approach currently used by WeaveShare and show that F 1 is superior in both time and space complexi-ties. We show that F 1 performs  X  10 62 times less operations compared to Bit Set to produce the same execution plan for the same input. We experimentally show that F 1 executes up to 60,000 times faster and can handle 1,000,000 ACQs in a setting where the limit for the current technique is 550.
Nowadays more and more applications are becoming avail-able to wider audiences, resulting in an increase in the amount of data produced. In order to cope with the sheer volume of information, enterprises move to Cloud Infrastructures to minimize the purchase and maintenance cost of machinery, and be able to scale their services including on-line analytics.
On-line data analytics have gained momentum in many applications that need to ingest data fast and apply some form of computation. Data streams are part of this broader category of data management, where Data Stream Manage-ment Systems ( DSMS ) [1, 19, 3, 2, 14] have been proposed as suitable systems for handling large amounts of data, ar-riving with high velocity.

A representative example of on-line analytics can be found in stock market web applications, where multiple clients monitor price fluctuations of stocks. In these settings, a system needs to be able to answer analytical queries (i.e. average stock revenue, profit margin per stock, etc.) for dif-ferent clients, each one with (possibly) different relaxation levels in terms of accuracy.

In DSMS , clients register their analytics queries on in-coming data streams. Since these queries continuously ag-gregate streaming data to produce answers, they are called Aggregate Continuous Queries ( ACQs ). The accuracy of an ACQ can be thought of as the window in which the ag-gregation takes place, and the period at which the answer is re-calculated. Periodic properties that are often used to describe ACQs are range ( r ) and slide ( s ) (sometimes also referred to as window and shift [9]). A slide denotes the period at which an ACQ updates its answer; a range is the time window for which the statistics are calculated. For ex-ample, if a stock monitoring application has a slide of 3 sec and a range of 5 sec, it means that the application needs an updated result every 3 sec, and the result should be derived from data accumulated over the past 5 sec. An ACQ re-quires the DSMS to keep state over time, while performing aggregations. Often, it is useful to run partial aggregations on the data while accumulating it, and then produce the answer by performing the final aggregation over the partial results [6, 11, 12, 13]. It is clear that the greater the range and the smaller the slide of the ACQ , the higher its cost is to maintain (memory) and process (CPU).
 Problem Statement In this work we focus on environ-ments where a large number of long-running ACQs with different periodic properties (accuracies) are operating on the same data stream, calculating similar aggregate oper-ations. Example of such an environment can be a DSMS deployed to Cloud Infrastructure, which results in a multi-tentant setting, where muliple ACQs with a wide range of different periodic features are executed on the same hard-ware. Since the ACQs are executed periodically (unlike one-shot queries), the opportunity to reduce the long-term overall processing costs by sharing final and partial results arises. For instance, assume that two ACQs monitor an av-erage stock value over the same data stream. Both have a slide of 3 sec. The first one has a range of 5 sec, whereas the second one has a range of 10 sec. In this setting, it is beneficial to keep calculating the results for the first query and combine the last two for generating the results for the second query. This should take place every 3 sec, which is the common slide among those ACQs . Partial results shar-ing is applicable for all matching aggregate operations, such as max , count , sum , average , etc., and for different but com-patible aggregate operations, for example sum , count and average can share results by treating average as sum/count .
Typically, the number of ACQs with similar aggregation types can be overwhelming in on-line systems [3]. Therefore, it is crucial to be able to make decisions quickly on combin-ing different ACQs into execution plans that would benefit the system. Unfortunately, the former has been proven to be NP-hard [24], and currently only approximation algorithms can produce acceptable execution plans.

The state-of-the-art WeaveShare algorithm produces very high quality execution plans by utilizing the Weavability concept [8], which is used to decide which ACQs are similar enough to be combined. WeaveShare is theoretically guar-anteed to approximate the optimal cost-savings to within a factor of four for practical variants of the problem [4]. How-ever, when we tried to implement it in a multiple-tenant DSMS , we observed that the current approach of calculating Weavability using Bit Set is very computationally expensive.
This motivated us to explore a more efficient algorithm to accelerate the calculation process in order to make the WeaveShare algorithm more scalable for systems with heavy workloads. Towards this, in this paper we propose a mathe-matical solution Formula 1 (or F 1), which reduces the num-ber of operations needed to produce the efficient execution plan by  X  10 62 times for the same set of 500 queries, and speeds up the plan generation time in our experiments by up to 60,000 times. F 1 also eliminates concerns over the amount of system memory as it does not need to store any large data during its operation. In fact, F 1 acceleration has enabled us to explore additional cost savings that can be achieved by utilizing the distributed nature of the Cloud Infrastructure by intelligently colocating ACQs on differ-ent computing nodes [17]. In general, F 1 can reduce the computation time of any technique that combines partial aggregations within composite slides of multiple ACQs . Contributions We make the following contributions:  X  We propose a novel closed formula, F 1, for calculating  X  We theoretically evaluate the state-of-the-art Weavabil- X  We experimentally evaluate F 1 and show that we can  X  We show experimentally that F 1 allows significantly bet-Roadmap In the next section, we provide the background of our work. We introduce our new formula, F 1, for the Weavability calculation and its additional optimization in Section 3. The complexity analysis on it is presented in Section 4. The evaluation platform and the experiments are discussed in Section 5. We summarize related work in Section 6 and conclude in Section 7.

In this paper, without a loss of generality, we target ACQs installed on the same data stream. Specifically, we scale the WeaveShare algorithm that has been shown to produce high quality execution plans of multiple ACQs by intelligently sharing partial aggregations.

Partial aggregation was proposed to improve the pro-cessing of ACQs [6, 11, 12, 13]. The idea behind partial ag-gregation is that we assemble the final answer from partial aggregates by performing a final aggregation over partials. For example, if an ACQ needs to calculate the maximum value over a specific time period, it can compute the maxi-mum on each separate partition of streaming data, and then perform the final maximum operation on all obtained partial answers in order to get the result. Therefore, if the data set for the operation is split into n partitions, the system needs to perform n partial aggregations and one final aggregation to obtain the answer.

The question that immediately arises is how many par-tial aggregations are beneficial to perform before each final aggregation for a specific ACQ . This depends on the time properties of the ACQ .  X  The ACQ slide is greater or equal to the ACQ range . In  X  The ACQ slide is less than the ACQ range, and it divides  X  The ACQ slide is less than the ACQ range, and it does Shared Processing of ACQs Several shared processing schemes as well as multiple ACQs optimizers utilize the Paired Window Technique [11, 8]. To show the benefits of sharing partial aggregations consider the following example: Example 1 There are two ACQs that perform the count aggregate operation on the same data stream. The first ACQ has a slide of 2 sec and a range of 6 sec, the second one has a slide of 4 sec and a range of 8 sec. Therefore, the first ACQ is computing partial aggregates every 2 sec, and the second is computing the same partial aggregates every 4 sec. Clearly, the calculation producing partial aggregates only needs to be performed once every 2 sec, and both ACQs can use these partial aggregates for their corresponding final aggregations. The first ACQ then will run each final aggregation over the last three partial aggregates, and the second ACQ will run each final aggregation over the last 4 partial aggregates.
The procedure to determine how many partial aggrega-tions is needed after combining n ACQs using a Bit Set is formalized as follows:  X  Find the length of the new combined (composite) slide,  X  Each slide is then repeated LCM/slide times to fit the  X  If the location is already marked, it cannot be marked
Weavability is a metric that measures the benefit of shar-ing partial aggregations between any number of ACQs . If it is beneficial to share computations between these ACQs , then these ACQs are known to weave well together. In-tuitively, two ACQs weave perfectly when their combined LCM contains only common edges. The following formula can be used to calculate the cost ( C ) of the execution plan before and after combining ACQs from their own trees into shared trees. The difference between these costs tells us if the combination was a good choice or not.
 Note that m is the number of the trees in the plan,  X  is input rate in tuples per second, E i is edge rate of tree i , and  X  is the overlap factor of tree i . Edge rate is the number of partial aggregations performed per second, and the overlap factor is the total number of final-aggregation operations performed on each fragment.

The WeaveShare optimizer utilizes the concept of Weav-ability to produce an execution plan for a number of ACQs . It selectively partitions the ACQs into multiple disjoint ex-ecution trees (i.e., groups), resulting in a dramatic reduc-tion in the total processing costs of the query plan. Weave-Share starts with a no share plan where each ACQ has its own individual execution tree. Then, it iteratively consid-ers all possible pairs of execution trees, and combines those which reduce the total plan cost the most, into a single tree. WeaveShare produces a final execution plan when it cannot find a pair that would reduce the total plan cost further.
The most complex part of the calculation occurs when the system is scheduling each partial aggregation operation ( edge ) and is tracking these operations using a Bit Set as mentioned above. The size of the Bit Set increases rapidly if the ACQs  X  time properties differ. For each ACQ added to the execution tree, WeaveShare needs to traverse the whole Bit Set to make sure that all partial aggregations necessary for this ACQ are marked in a Bit Set for future execution. Since the size of a Bit Set increases exponentially with the increase of the input size, traversing it becomes prohibitively time-consuming as we show in Section 4. Additionally, the exponential increase of the size of the Bit Set puts a hard limit on a system X  X  capabilities, based on the amount of memory available.
In this section, we describe our new formula F 1 that sig-nificantly speeds up the edge rate calculation in a composite slide. We target two scenarios for ACQs with matching or compatible aggregate operations: 1) when all ACQ slides are factors of their corresponding ranges, and 2) when some of the ranges are not multiples of their corresponding slides.
In the case when all of the ranges of the ACQs that are being installed onto the DSMS are divisible by their cor-responding slides, we can store partial aggregates at every slide. For example, if we have an ACQ with a slide of 3 sec and a range of 9 sec, we can store partial results every 3 sec, and perform the final aggregations on the 3 last saved partial aggregations to get the answer for the last 9 sec. In order to calculate the edge rate in WeaveShare after weaving together n ACQs , we need a Bit Set of the length equal to the LCM of all n slides. At first, the Bit Set is populated with zeros. For each one of n ACQs we traverse the whole Bit Set and mark all bits whose indexes are divisible by the corresponding ACQ  X  X  slide with ones. If the bit was already marked, the algorithm does nothing and just moves to the next bit.
 Example 2 Consider five stock monitoring ACQs with the following slides: 2, 3, 4, 5, and 6. Their LCM is 60, therefore we need a Bit Set of size 60. First, we traverse the Bit Set and mark all indexes divisible by 2 (all even numbers up to 60). Now the Bit Set has 30 bits marked. Next we mark all indexes that are divisible by 3. The Bit Set already has 40 bits marked (10 overlapped with already marked ones). Next we mark all indexes that are divisible by 4. The Bit Set still has 40 bits marked since all of the bits we were trying to mark were already marked by a slide of 2. After repeating the same for slides of 5 and 6, we calculate how many bits we have in our Bit Set , and the answer is 44. This method is illustrated in the Figure 2.

To accelerate this calculation process we propose the For-mula 1 (or F 1 ): Where LCM n = LCM ( s 1 ,s 2 ,...,s n ), and function G 1 is a sum of the inversed LCMs of all possible groups of slides of size i from a set of size n . For example: G 1 (3 , 2) = 1 F 1 can be expanded as follows:
LCM n [ G 1 ( n, 1)  X  G 1 ( n, 2)+ ...  X  G 1 ( n,n  X  1)  X  G Equation 3 is composed of an alternating series of function G 1 multiplied by the LCM n . LCM n  X  G 1 ( n, 1) and represents the number of all edges produced by all ACQs and therefore includes all overlapping edges. The goal of the calculation is to count every edge only once, even if it overlaps multiple times in different ACQs . Therefore, the rest of the elements of the series will eliminate all of the overlapping edges from the current result. LCM n  X  G 1 ( n, 2) represents the number of edges that overlap in all different pairs of slides and after subtracting it, we get a smaller number than the number we are looking for, because there are potentially some edges where more than two slides overlap at the same time. For example, if slides a , b , and c overlap at some specific edge e , we add it three times: for pairs ( a,b ), ( a,c ), and ( b,c ). The following element LCM n  X  G 1 ( n, 3) compensates for these cases by adding back all edges that overlap in each set of three slides. After adding it, we have again a larger num-ber than the sought-after number, because we might have four or more slides overlapping at the same edge. Therefore, each element compensates for the previous ones X  inaccura-cies up to the point when we add/subtract the final edge of the composite slide, which clearly occurs only once, since edge has an index equal to the LCM n .

Equation 3 is an alternating series and we know in advance that the number of elements is always equal to the number of ACQs in the execution tree and is a finite number. There-fore, by definition, the sequence always converges.
The following is an example of using F 1 to calculate the number of edges: Example 3 Consider the same set of stock monitoring ACQs as we had in Example 1: slides are 2 , 3 , 4 , 5 , and 6. As a first step of our algorithm we calculate the LCM n of the whole set of slides. LCM n = LCM (2 , 3 , 4 , 5 , 6) = 60. Next we substitute our values into Equation 11: 60  X  G 1 (5 , 1)  X  60  X  G 1 (5 , 2) + 60  X  G 1 (5 , 3)  X  60  X  G Every element is expanded as shown above. For example, the expansion of element 60  X  G 1 (5 , 2) is as follows. (Note Figure 3: F 1 converging to the solution for 20 ACQs in 20 steps that LCM ab denotes LCM ( a,b )). Finally we have: 87  X  70 + 36  X  10 + 1 = 44 This answer matches the solution from Example 1.

Notice that the elements of the alternating series are in-terchangeably increasing and decreasing the solution as we approach the end of the calculation. For 20 different ACQs , the calculation of overlapping edges using F 1 consists of 20 addition operations, causing the total number to change as depicted in Figure 3.
In case some of the ranges of the ACQs that are being installed onto the DSMS are not divisible by their corre-sponding slides, according to the Paired Window approach, the slides should be broken into fragments. This enables us to store partial aggregates for every fragment. For example, if we have an ACQ with slide 5 sec and range 7 sec, the slide is split into two fragments: f 2 = 7 (mod 5) = 2 and f = 5  X  2 = 3. Now we can store partial results for first 3 sec, then for following 2 sec, then again for the following 3 sec and so on.

In the original WeaveShare [8], in order to calculate the edge rate after weaving together n ACQs with fragments, we again need to work with a Bit Set of the length equal to the LCM of all n slides. The Bit Set is pre-populated with zeros again. For each ACQ we traverse the whole Bit Set and mark bits corresponding to the times when partial aggregations will happen with ones. If the bit was already marked, the algorithm does nothing and just moves to the next location.
 Example 4 Consider four stock monitoring ACQs with the following slides: 3 , 4 , 6 , and 9. ACQs with slides of 4 and 6 consist of fragments (3 , 1) and (2 , 4) respectively. ACQs 3 and 9 do not have fragments. The overall LCM of all slides together is 36, therefore we need a Bit Set of size 36. First, we traverse the Bit Set and mark all indexes that are divisible by 3 for the ACQ with a slide of 3 and no fragments. Now the Bit Set has 12 bits marked. Next consider an ACQ with a slide of 4 and fragments (3 , 1). We traverse the Bit Set by starting from 0 and adding fragment 3 followed by fragment 1 repeatedly. Thus, the Bit Set will be marked at Figure 5: (slide 3, shift 0) and (slide 6, shift 3) DO overlap , indexes 3 , 4 , 7 , 8 , 11 , 12 , etc. Now there are 24 marked bits. Next we continue to an ACQ with a slide of 6 and fragments (2 , 4). Again, we start at 0 and by adding 2 and 4 repeatedly we mark the following bits: 2 , 6 , 8 , 12 , 14 , etc., marking 27 bits in total. For the last ACQ with a slide of 9 and no fragments, we traverse the Bit Set at increments of size 9 and mark each 9th bit with one. The total number of set bits stays 27, because the last ACQ did not add any new bits, therefore our answer is 27. This method is illustrated in Figure 4.

To generalize F 1 for both cases (if we do have ACQs with fragments and if we do not) we introduce the notion of shifts . Each ACQ that does not have fragments has a shift of zero. Each ACQ that does have fragments must be presented as two ACQs with the same slides, but different shifts . First one has a shift of zero, and the second one has a shift equal to the first fragment of the original ACQ .
When counting overlapping edges of ACQs , and when at least one of their shifts is not zero, we can encounter two different cases:  X  ACQs overlap, and the number of common edges is the  X  ACQs do not overlap at all. Since the shifts are not Example 5 Assume two ACQs with slides of 3 and 6. If their corresponding shifts are 0 and 3, there is an overlap-ping edge every 6 time units. However, if the corresponding shifts are 0 and 2, there are no overlapping edges. This is illustrated in Figure 5.

To decide whether two ACQs q 1 and q 2 (if at least one of them has non-zero shift) will overlap, we propose the fol-lowing Overlap Check Formula based on GCD ( Greatest Common Divisor ): | q 1 .shift  X  q 2 .shift | mod GCD ( q 1 .slide,q 2 .slide ) (4)  X  If the Overlap Check Formula resolves to zero then the  X  Otherwise the ACQs DO NOT overlap Proof of the Overlap Check Formula (by contradiction) Assume that we have two ACQs q 1 and q 2 , with correspond-ing slides s 1 and s 2 , and shift difference h . Assume further that h mod GCD ( s 1 ,s 2 ) 6 = 0 and (for the sake of contra-diction) the ACQs DO overlap. Let us denote all edges produced by q 1 as { e 1  X  1 ,e 1  X  2 ,...,e 1  X  n } , and edges of q { e 2  X  1 ,e 2  X  2 ,...,e 2  X  n } . Let us first look at the two ACQs separately. Since every edge produced by q 1 is divisible by s , and every edge produced by q 2 is divisible by s both s 1 and s 2 are divisible by GCD ( s 1 ,s 2 ) (by definition of GCD), every edge produced by q 1 and q 2 is divisible by GCD ( s 1 ,s 2 ). Therefore, all edges that are not divisible by GCD ( s 1 ,s 2 ) cannot possibly overlap any of the edges pro-duced by either q 1 or q 2 . Without loss of generality, let us consider q 2 from the standpoint of q 1 . Then, all edges of q are shifted by h with respect to edges of q 1 , and they can be written as follows: { e 2  X  1 + h,e 2  X  2 + h,...,e These edges should be divisible by GCD ( s 1 ,s 2 ) in order for them to overlap the edges of q 1 : { e 1  X  1 ,e 1  X  2 ,...,e know that the edges { e 2  X  1 ,e 2  X  2 ,...,e 2  X  n } are divisible by GCD ( s 1 ,s 2 ). However, by the initial assumption, the shift h that is being added to them is not divisible by GCD ( s 1 Thus, edges { e 2  X  1 + h,e 2  X  2 + h,...,e 2  X  n + h } cannot pos-sibly be divisible by GCD ( s 1 ,s 2 ). Therefore, none of the edges of q 2 can possibly overlap with the edges of q the case that h would actually be divisible by GCD ( s 1 ,s all shifted edges of q 2 that are divisible by s 1 would overlap with the edges of q 1 . However, the initial assumption states that h is not divisible by GCD ( s 1 ,s 2 ), which leads us to the conclusion that the ACQs q 1 and q 2 do not overlap, which is a contradiction. Hence, the initial formula is correct. Next we generalize our formula F 1 for use in cases when ACQs have fragments, and cases when none of the ACQs have fragments. The general F1 is: Where again LCM n = LCM ( s 1 ,s 2 ,...,s n ), and function G 2 is the same as function G 1 , however all elements pro-duced by G 2 have to be checked with the Overlap Check Formula for redundancy as described below. Prior to using this formula, for each ACQ that has fragments, we create two new ACQs : one of them has a shift of zero, another one has a shift equal to the first fragment of the original ACQ . All new ACQs are added back to the set of the origi-nal ACQs replacing the originals. To calculate each G 2 we find all possible groups of size x from the new set of ACQs just like in the case with no fragments. Some of these groups are redundant because they do not have overlapping edges (because of the shifts). To remove all redundant groups, we check all possible pairs within each group using the Overlap Check Formula , and if any of the pairs return a non-zero value, then the whole group is discarded. Otherwise, G is calculated and used the same way as in the case with NO fragments. The generalized formula F 1 still converges, which can be proven using the same strategy as in the case with no fragments.
 Equation 5 expands into an alternating series likewise: We show how Equation 6 works with the following example. Example 6 Assume the same set of stock monitoring ACQs as in Example 4: slides are 3, 4, 6, and 9, and ACQs with slides of 4 and 6 consist of fragments (3 , 1) and (2 , 4) respectively. As a first step of our algorithm we calculate the LCM n of the whole set of slides. LCM LCM (3 , 4 , 6 , 9) = 36. Next we replace the ACQs that have fragments with the ACQs that have corresponding shifts. In our set we now have two ACQs with a slide of 4 (shifts 0 and 3), and two ACQs with a slide of 6 (shift 0 and shift 2). The rest of the ACQs stay the same. We can substitute our values into the generalized formula F 1: The calculation is almost identical to the case with no frag-ments, except every group produced by G 2 has to be checked with the Overlap Check Formula to see if it is redundant or not. For example, the expansion of the second group is shown below. Note that 3 0 4 3 denotes a group of ACQs with slides of 3 and 4 and shifts of 0 and 3, respectively. The fractions that have been crossed out did not pass the test with the Overlap Check Formula . Finally we have: 46  X  26 + 8  X  1 = 27 This solution matches the solution from Example 4.
Since we are using the Euclidean GCD algorithm for all of our LCM calculations, we found that we can achieve a significant additional speed up by utilizing the technique of memorization. We adopted this technique by preloading a table of GCD s into main memory before the execution begins. If the user is willing to allocate b bytes of memory to store the GCD table and each GCD takes g bytes of memory, we can store in memory GCD s of all the possible pairs of numbers up to p 2 b/g . In our implementation we are using 8 byte numbers of the Long type for calculations, so if we want to allocate 4 GB of main memory to store GCDs , we can fit GCD s of all the pairs of numbers up to 32 , 768. If we calculate the GCD for numbers that are larger than the above limit, the GCD table still save us some time by taking advantage of the recursive nature of the Euclidean algorithm. The effects of the optimization are shown in Section 5.
In this section, we calculate the difference between the complexities of Bit Set calculation and our F 1 method. Time Complexities To compare the time complexities we start by identifying the initial calculations needed by both algorithms. We denote the number of ACQs as n , and the max slide as max . The following steps need to be done at the beginning of both algorithms.  X  Remove all duplicate slides, since the same slides produce  X  Precalculate the LCM n , which is the LCM of all slides  X  Remove all slides that are multiples of other slides in-
Therefore, precalculation takes n  X  logn +( n  X  1)  X  log ( max )+ n  X  ( n  X  1) / 2 operations, however since it is performed by both algorithms, we can ignore it for the matter of comparison. After completing the initial computation, we now have LCM n stored in main memory, and a set of slides which does not contain any duplicates or multiples. Therefore, the set can now only have either prime numbers or numbers for which their multiples do not appear in the set.

To better illustrate differences in complexity we utilize two different sets of slides:  X  Working Set ( S w ) is a set of slides that includes only  X  Auxiliary set ( S a ) is a set of slides, that consists of se-
Next we show that the lower bound of the Bit Set calcula-tion is higher than the upper bound of the F 1 computation.
The complexity of the Bit Set calculation is:
Where LCM n = LCM ( s 1 ,s 2 ,...,s n ). The complexity holds since for each of n ACQs we would need to traverse the whole Bit Set , whose length is equal to the LCM of all slides of all ACQs , with a step equal to each ACQ  X  X  slide. We can expand the Equation 7 to the following:
First, we perform complexity analysis using the Auxiliary set S a . Let us focus on the first part of the product in Equa-tion 8: LCM n . We know that the LCM of all numbers in this set is the product of the highest prime powers occurring in the set. The log of the LCM is therefore the sum of the logs of the prime powers in the set: wheref ( i ) =
This sum has significance in the Prime Number Theorem and it is well known to be asymptotically equal to e n [23].
Next we focus on the second part of the product in Equa-tion 8: ( 1 s quential numbers from one to n , this part will look like: ( + 1 2 + ... + 1 n ), which is a classic example of a diverging harmonic series P  X  n =1 1 n . For any n , this series can be cal-culated as follows: P  X  n =1 1 n = ln ( k ) +  X  + k , where  X  is the Euler-Mascheroni constant (  X   X  0 . 577) and k  X  1 2 k , which approaches zero as k goes to infinity[22]. For our purposes, since  X  is a constant and k is negligible, they are ignored. Also, we can say that time complexity e n  X  ln ( n ) is asymp-totically equal to e n , therefore we can assume asymptotical time complexity of the Bit Set computation for set S a is e
When we use set S w , the time complexity for LCM n is larger than when using set S a . This is true because we replace n sequential numbers that start from 1 with n non-duplicate primes and non-multiples, which are larger and have a larger total LCM . The time complexity for the part P bers in the denominator, however it is still insignificant, be-cause even in the worst case we can lower bound it with ln ( max )  X  ln ( max  X  n ). Thus, the time complexity of the Bit Set computation for the working set S w is at least e
To calculate the complexity of F 1, we need to determine the number of operations that need to be performed based on the size of the input. First, we know that the number of elements in our alternating series is equal to the number of ACQs in the set. Let us take Equation 3 and expand LCM n into the parentheses: As we previously mentioned, LCM n  X  G 1 ( n,n ) = 1, therefore: To determine how many groups will be produced by each one of these elements we use binomial coefficients. Each el-ement of type LCM n  X  G 1 ( n,k ) therefore produces n distinct k -element groups of type LCM n LCM LCM ( s 1 ,s 2 ,...s k ). Therefore, the total number of all of these groups is: n 1 + n 2 + n 3 + ... + n n  X  1 + n n . By the additive property of binomial coefficients, this sum equals 2  X  1. Next we determine how many calculations are per-formed in each group. The numerator of all groups is LCM and since it is kept in main memory, we do not need to re-calculate it every time. The denominator is LCM k , and it Figure 6: Number of operations needed by Bit Set and F 1 for plan generation. Top labels show BitSet/ F 1 ratio is determined by calculating the LCM of the first two el-ements, and then iteratively calculating the LCMs of the resulting number with the rest of the elements in the group. Therefore, for each group we need to perform k  X  1 LCM cal-culations, and one calculation to add the group to the total number, which makes k calculations total. Since each group with k elements needs k calculations, the total number of calculations needed for all groups becomes: 1 n 1 + 2 n 2 3 3 + ... + ( n  X  1) which can be calculated by taking the generalization of bi-nomial series: (1 + x ) a = P  X  k =0 a k  X  x k and differentiating it with respect to x and then substituting x = 1 [20]. Due to the use of the Euclidean algorithm to calculate the LCM , the complexity of each LCM calculation is log ( min ( a,b )) at most [21]. Therefore, at worst F 1 has a time complexity of 2 n  X  1  X  n  X  log ( max ), which asymptotically equals 2 n .
Thus, we have determined that the Bit Set calculation has a time complexity of at least e n , and F 1 has a time complexity of at worst 2 n . Clearly, when n goes to infinity, it is increasingly beneficial to use F 1 versus Bit Set .
Additionally, since we have calculated the formulas for determining the exact number of operations done by both Bit Set and F 1, we can compare the increase in the amount of operations performed by Bit Set and F 1 with the increase of the number of input ACQs . The comparison is shown in Figure 6. ACQs for this comparison were sequentially drawn from the Auxiliary set ( S a ) introduced above. Note that since the difference between Bit Set and F 1 operation numbers is drastic and grows exponentially we had to use a logarithmic scale to still see the operations of F 1. This comparison shows that F 1 is much more scalable than Bit Set in terms of the number of operations required.
 Space Complexities The space complexity of the Bit Set calculation is LCM n , since we have already shown that the Bit Set grows at the rate of e n . The space complexity of the F 1 calculation is O (1) ( constant ) since it does not require storing edges. Edge overlaps are calculated strictly math-ematically. Since F 1 expands into a sum, we only need to keep one number in memory, which is increased or decreased by the elements of the alternating series sequentially. The improvement in space complexity is extremely important for the WeaveShare algorithm, since the leading cause of its fail-ures with large workloads is  X  X ut of memory X  errors.
In this section, we summarize the results of our experi-mental evaluation of the scalability of F 1 in terms of the size of the input set of the ACQs , the diversity of their time properties, and the input rate of the data stream.
In order to show the significance of our Weavability cal-culation optimization we built an experimental platform in Java. Specifically, we implemented the WeaveShare opti-mizer as described in [8] with different options for calculat-ing Weavability . Our workload is composed of a number of ACQs with different characteristics. We are generating our workload synthetically in order to be able to fine-tune sys-tem parameters and get a more detailed sensitivity analysis of the optimizer X  X  performance. Moreover, it allows us to target possible real-life scenarios and analyze them.
Our system X  X  experimental parameters are: [ Algorithm ] specifies which technique is used for Weav-ability calculations. The available techniques are: (a) Bit Set ( BS ), (b) Formula 1 ( F 1), and (c) Formula 1 + Opti-mization ( F 1 + Opt ). The F 1 + Opt technique uses a 4 GB table for keeping GCD s in main memory. [ Q num ] Number of ACQs . We assume that all ACQs are installed on the same data stream and their aggregate func-tions allow them to share partial aggregations among them. The actual function does not have any effect on performance other than the ability to share partial aggregations. [ S max ] Maximum slide length, which provides an upper bound on how large slides of our ACQs can be. The mini-mum slide allowed by the system always equals one. [  X  ] The input rate, which describes how fast tuples arrive through the input stream in our system. [ Z skew ] Zipf distribution skew, which depicts the popularity of each slide length in the final set of ACQs . A Zipf skew of zero produces uniform distribution, and a greater Zipf skew is skewed towards large slides (for more realistic examples). [ X  max ] Maximum overlap factor, which defines the upper bound for the overlap factor. The overlap factor of each ACQ is drawn from a uniform distribution between one and the maximum overlap factor. [ Gen ] Generator type, which defines whether our workload is normal ( Nrm ) and includes any slides, or diverse ( Div ) and includes only prime slides.
To test the scalability of our approaches F 1 and F 1 + Opt versus BS in terms of the parameters Q num , S max ,  X  , Z and  X  max , we ran five experiments, where we varied each one of these parameters while keeping the rest of them fixed. The parameters were selected separately for each experiment in a way that would highlight the differences in the scalabilities of the three approaches the best. The experimental parameters are specified in the Table 1.

All results are taken as averages of running each experi-ment five times. Please note that since F 1 and F 1 + Opt showed to have significantly smaller runtimes compared to BS , we had to use logarithmic scale to be able to display all techniques X  performances in the same graphs.
 We ran all our experiments on a dual Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz server with 96 GB of RAM.
 Exp 1: Number of ACQs Scalability (Figure 7) In this test we varied the Q num from 100 to 1,000,000. Figure 8: Exp 2 Scalability of the maximum slide length Clearly, increasing the Q num also increases the amount of required calculations, causing higher runtimes for all three algorithms. The results are depicted in Figure 7. The Bit Set approach did not finish execution, because after we crossed Q num of 550 it started running out of memory (on a 96GB RAM machine) and eventually crashed (on all runs). Oth-erwise, the growth rates of these techniques are similar to what we expected from the theoretical analysis of the time complexities of their underlying algorithms. The statistics show that our techniques X  runtimes are on average 350 times faster than runtimes of BS with a maximum of 790 times, and our techniques are able to scale up to 1,000,000 ACQs on this setting without running out of memory. Also, the F 1 + Opt plan outperformed the F 1 plan by approximately 28% on average, validating our optimization expectations. Exp 2: Max Slide Scalability (Figure 8) In this test we varied the S max from 100 to 10,000. Similarly to Exp 1, increasing the S max also increases the amount of required calculations. This happens because with a higher max slide parameter, the generated ACQs have longer slides, which results in higher LCMs and fewer overlapping edges. In Figure 8 we see that the BS approach did not finish execution again after we crossed S max of 800 because of the Figure 10: Exp 4 Sensitivity to the zipf distribution skew Figure 11: Exp 5 Sensitivity to the maximum overlap factor  X  X ut of memory X  error. The growth rates of our techniques however are again similar to what we expected from our theoretical analysis of their time complexities. Our proposed techniques X  runtimes in this experiment are on average 2,200 times faster than runtimes of BS with a maximum of 10,000 times, and our techniques are able to scale up to the S max 10,000 ACQs on this setting and finish the plan generation successfully. The F 1 + Opt plan outperformed the F 1 plan by an average of 18%.
 Exp 3: Input Rate Scalability (Figure 9) In this test we varied  X  from 100 to 100,000. Increasing  X  also increases the amount of required calculations because with higher input rates, according to the Equation 1, it be-comes more beneficial to combine more execution trees. This forces WeaveShare to combine some trees with different time properties that would not have been combined if the input rate was lower. Thus, increasing  X  leads to higher LCMs and higher runtimes. The average number of execution trees formed at the end of the plan generation is 71 when  X  = 100, 29 when  X  = 400, 15 when  X  = 900, 4 when  X  = 10 , 000 and 1 when  X  = 100 , 000 or 1,000,000. The fact that at some  X  all trees get merged into one explains why runtimes stop increasing after this  X  . In this setting it happened at  X  = 100 , 000 (see Figure 9). In this experiment the BS ap-proach crashed when  X  reached 900, and the average number of trees at that point was 15. Our approaches demonstrated good scalability again and were able to increase input rate to the point where all trees are Weaved into one. On aver-age our techniques ran 3,800 times faster than BS with a maximum of 16,000. The F 1 + Opt plan outperformed the F 1 plan by the average of 19%.
 Exp 4: Slide Skew Sensitivity (Figure 10) In this test we varied the Z skew from 0 to 100. This ex-periment is similar to the max slide scalability experiment, because in both experiments we are gradually increasing the amount of ACQs with large slides and therefore increasing the amount of required calculations. The difference is that, when skewing all slides drawn from the same set to the larger side, at some point they start repeating, which then reduces the amount of the required calculations. In our experiment (see Figure 10) we first observe the initial increase in the amount of computation, which leads the BS approach to crash with an  X  X ut of memory X  error (at Z skew = 2 . 25), and then we see gradual decrease in computation, because there are many repeating slides in the input set. In this setting our proposed techniques X  runtimes are on average 14,000 times faster than runtimes of BS with a maximum of 60,000 times, and our techniques are able to scale up to the Z skew of 100 and finish the plan generation successfully. The F 1 + Opt plan outperformed the F 1 plan by the average of 18% again. Exp 5: Overlap Factor Sensitivity (Figure 11) In this test we varied the  X  from 100 to 1. We did it in reverse order since its value is inversely proportional to the amount of computation required to generate an execution plan using WeaveShare . Based on Equation 1 we can see that smaller  X s benefit the total cost if their corresponding ACQs are combined to fewer execution trees, which causes WeaveShare to Weave more trees with different time prop-erties together. In our experiment (see Figure 11) the BS approach crashed when  X  reached 40. Our approaches again demonstrated good scalability and were able to finish the plan generation successfully even with the minimum value of  X  = 1. On average our techniques ran 5,600 times faster than BS with a maximum of 16,000. The F 1 + Opt plan outperformed the F 1 plan by the average of 26%.
 Experimental Results Summary Clearly, the above ex-perimental results show that our techniques F 1 and F 1+ Opt deliver the best performance in terms of plan generation runtimes and scalability, while producing same high quality execution plans as the original WeaveShare optimizer. The results of our experiments are summarized in Table 2.
Techniques for the efficient processing of ACQs could be broadly classified into techniques for: 1) the implementation of the continuous aggregation operator, and 2) the multi-query optimization of multiple continuous aggregate queries.
Under the operator implementation techniques, partial ag-gregation has been proposed to minimize the repeated pro-cessing of overlapping data windows within a single aggre-gate (e.g., [12, 13, 6, 11, 24, 25, 18]) by processing each input tuple only once. As discussed in Section 2, ACQ process-ing is typically modeled as a two-level (i.e., two-operator) query execution plan. In order to minimize the cost of final aggregation, TriOps [7] uses intermediate function levels to pipeline partial aggregates to final-aggregate functions.
Under the multi-query optimization techniques, the gen-eral principle is to minimize the repeated processing of over-lapping operations across multiple aggregate queries. This repetition occurs when queries exhibit an overlap in at least one of the following specifications: 1) predicate conditions, 2) group-by attributes, or 3) window settings.

Techniques leveraging overlaps in predicates and group-by attributes across ACQs are similar to classical multi-query optimization [16] that detects common subexpressions.
Techniques leveraging shared processing of overlapping windows across ACQs emerged with the paradigm shift for continuous queries. Shared time slices ( SLS ) [11] is one such a technique, which was also extended into shared data shards in order to share the processing of varying predicates, in ad-dition to varying windows. Orthogonally, [15] extends classi-cal subsumption-based multi-query optimization techniques towards sharing the processing of multiple ACQs with vary-ing group-by attributes and similar windows.

Like SLS , WeaveShare [8] optimizes the shared processing of ACQs with varying windows by selectively partitioning them into execution trees resulting in a dramatic reduction in total processing costs. WeaveShare was also applied in distributed environments [17].

In [5], a demonstration of implementing event monitor-ing applications using the modified Hadoop framework was presented. Along the same lines are schemes for scaling op-erators/queries out when nodes get overloaded [9, 10].
The main contribution of this paper is a novel closed for-mula, F 1, for accelerating Weavability calculations required for determining the best execution plans for sharing partial aggregations of ACQs . Our approach replaces the counting of the edges within a Bit Set with mathematical compu-tation. We proved theoretically that F 1 significantly de-creases the number of operations required for the execution plan generation while reducing the algorithm X  X  space con-sumption to the bare minimum. We showed experimentally that the F 1 approach achieves up to 60,000 times faster plan generation times compared to the current state of the art, and is able to achieve much better scalability in terms of the number of input ACQs , their diversity, and the input rate of the data stream. It should be noted that F 1 can reduce the computation time of any optimization technique that requires scheduling partial aggregations within compos-ite slides of multiple ACQs .
 Acknowledgments We would like to thank Profs. K. Pruhs, J. Sorenson, and E. Bach for their help with our theoreti-cal analysis and C. Thoma and the anonymous reviewers for the insightful feedback. This work was supported in part by NSF award CBET-1250171 and a gift from EMC/Greenplum.
