 How can we reasonably expect to learn given possibly adversarial data? Overcoming this obstacle has been one of the major successes of the Online Learning framework or, more generally, the so-called competitive analysis of algorithms: rather than measure an algorithm only by the cost it incurs, consider this cost relative to an optimal  X  X omparator algorithm X  which has knowledge of the data in advance. A classic example is the so-called  X  X xperts setting X : assume we must predict a sequence of binary outcomes and we are given access to a set of experts , each of which reveals their own prediction for each outcome. After each round we learn the true outcome and, hence, which experts predicted correctly or incorrectly. The expert setting is based around a simple assumption, that while some experts X  predictions may be adversarial, we have an a priori belief that there is at least one good expert whose predictions will be reasonably accurate. Under this relatively weak good-expert assumption, one can construct algorithms that have quite strong loss guarantees. Another way to interpret this sequential prediction model is to treat it as a repeated two-player zero-sum game against an adversary on a budget ; that is, the adversary X  X  sequence of actions is restricted in that play ceases once the adversary exceeds the budget. In the experts setting, the assumption  X  X here is a good expert X  can be reinterpreted as a  X  X ature shall not let the best expert err too frequently X , perhaps more than some fixed number of times.
 In the present paper, we develop a general framework for repeated game-playing against an adver-sary on a budget, and we provide a simple randomized strategy for the learner/player for a particular class of these games. The proposed algorithms are based on a technique, which we refer to as a  X  X andom playout X , that has become a very popular heuristic for solving games with massively-large state spaces. Roughly speaking, a random playout in an extensive-form game is a way to measure the likely outcome at a given state by finishing the game randomly from this state. Random play-outs, often known simply as Monte Carlo methods, have become particularly popular for solving the game of Go [5], which has led to much follow-up work for general games [12, 11]. The Bud-geted Adversary game we consider also involves exponentially large state spaces, yet we achieve efficiency using these random playouts. The key result of this paper is that the proposed random playout is not simply a good heuristic, it is indeed minimax optimal for the games we consider. Abernethy et al [1] was the first to use a random playout strategy to optimally solve an adversarial learning problem, namely for the case of the so-called Hedge Setting introduced by Freund and Schapire [10]. Indeed, their model can be interpreted as a particular special case of a Budgeted Adversary problem. The generalized framework that we give in the first half of the paper, however, has a much larger range of applications. We give three such examples, described briefly below. More details are given in the second half of the paper.
 Cost-sensitive Hedge Setting. In the standard Hedge setting, it is assumed that each expert suffers a cost in [0 , 1] on each round. But a surprisingly-overlooked case is when the cost ranges differ, use a generic bound of max i c i , is extremely loose, and we know of no better bounds for this case. Our results provide the optimal strategy for this cost-sensitive Hedge setting.
 Metrical Task Systems (MTS). The MTS problem is decision/learning problem similar to the Hedge Setting above but with an added difficulty: the learner is required to pay the cost of moving through a given metric space. Finding even a near-optimal generic algorithm has remained elusive for some time, with recent encouraging progress made in one special case [2], for the so-called  X  X eighted-star X  metric. Our results provide a simple minimax optimal algorithm for this problem. Combinatorial Prediction Market Design: There has been a great deal of work in designing so-called prediction markets, where bettors may purchase contracts that pay off when the outcome of a future event is correctly guessed. One important goal of such markets is to minimize the potential risk of the  X  X arket maker X  who sells the contracts and pays the winning bettors. Another goal is to design  X  X ombinatorial X  markets, that is where the outcome space might be complex. The latter has proven quite challenging, and there are few positive results within this area. We show how to translate the market-design problem into a Budgeted Adversary problem, and from here how to incorporate certain kinds of combinatorial outcomes. sequences of elements of [ n ] . We will use the greek symbols  X  and  X  to denote such sequences i i 2 . . . i T , where i t  X  [ n ] . We let  X  denote the empty sequence. When we have defined some T -length sequence  X  = i 1 i 2 . . . i T , we may write  X  t to refer to the t -length prefix of  X  , namely  X  n -simplex, where w i denotes the i th coordinate of w . We use the symbol e i to denote the i th basis vector in n dimensions, namely a vector with a 1 in the i th coordinate, and 0 X  X  elsewhere. We shall use 1 [  X  ] to denote the  X  X ndicator function X , where 1 [ predicate ] is 1 if predicate is true, and 0 if it is false. It may be that predicate is a random variable, in which case 1 [ predicate ] is a random variable as well. 2.1 The Setting: Budgeted Adversary Games We will now describe the generic sequential decision problem, where a problem instance is char-acterized by the following triple: an n  X  n loss matrix M , a monotonic  X  X ost function X  cost : [ n ]  X   X  R + , and a cost budget k . A cost function is monotonic as long as it satisfies the relation cost (  X  X  )  X  cost (  X i X  ) for all  X ,  X   X  [ n ]  X  and all i  X  [ n ] . Play proceeds as follows: The goal of the Player is to choose each w t in order to minimize the total cost of this repeated game on all sequences of outcomes. Note, importantly, that the player can learn from the past, and hence would like an efficiently computable function w : [ n ]  X   X   X  n , where on round t the player is given  X  w : [ n ]  X   X   X  n by its performance against a worst-case sequence, that is Note that above T is a parameter chosen according to  X  and the budget. We can also define the min-imax loss, which is defined by choosing the w (  X  ) which minimizes WorstCaseLoss (  X  ) . Specifically, In the next section, we describe the optimal algorithm for a restricted class of M . That is, we obtain the mapping w which optimizes WorstCaseLoss ( w ; M, cost , k ) . and c i &gt; 0 for all i . With these values c i , define the distribution q  X   X  n with q i := 1 /c i P Given a current state  X  , the algorithm will rely heavily on our ability to compute the following function  X (  X  ) . For any  X   X  [ n ]  X  such that cost (  X  ) &gt; k , define  X (  X  ) := 0 . Otherwise, let Notice, this is the expected length of a random process. Of course, we must impose the natural con-dition that the length of this process has a finite expectation. Also, since we assume that the cost in-k } , has an exponentially decaying tail. Under these weak conditions, the following m -trial Monte Carlo method will provide a high probability estimate to error within O ( m  X  1 / 2 ) . Algorithm 1 Efficient Estimation of  X (  X  ) for i=1. . . m do end for Notice that the infinite sequence  X  does not have to be fully generated. Instead, we can continue to sample the sequence and simply stop when the condition cost (  X  X  t  X  1 )  X  k is reached. We can now define our algorithm in terms of  X (  X  ) .
 Algorithm 2 Player X  X  optimal strategy Input: state  X  Compute:  X (  X  ) ,  X (  X , 1) ,  X (  X , 2) , . . . ,  X (  X , n ) Now we prove that Algorithm 2 is both  X  X egal X  and minimax optimal.
 Lemma 4.1. The vector w (  X  ) computed in Algorithm 2 is always a valid distribution. Proof. It must first be established that w i (  X  )  X  0 for all i and  X  . This, however, follows because we assume that the function cost () is monotonic, which implies that cost (  X  X  )  X  cost (  X i X  ) and hence expected difference of the infinite sum of these two indicators leads to  X (  X  )  X   X (  X i )  X  0 , which implies w i (  X  )  X  0 as desired.
 We must also show that P i w i (  X  ) = 1 . We claim that the following recurrence relation holds for the function  X (  X  ) whenever cost (  X  )  X  k : This is clear from noticing that  X  is an expected random walk length, with transition probabilities defined by q , and scaled by the constant ( P i 1 /c i )  X  1 . Hence, where the last equality holds because q i = 1 /c i P Theorem 4.1. For M = diag ( c 1 , . . . , c n ) , Algorithm 2 is minimax optimal for the Budgeted Adver-sary problem. Furthermore,  X (  X  ) = MinimaxLoss ( M, cost , k ) .
 of Algorithm 2 will be
X and hence the total cost of the algorithm is always bounded by  X (  X  ) .
 On the other hand, we claim that  X (  X  ) can always be achieved by an adversary for any algorithm w (  X  ) . Construct a sequence  X  as follows. Given that  X  t  X  1 has been constructed so far, select any coordinate i t  X  [ n ] for which w i t (  X  t  X  1 )  X  w 0 i least as much weight on i t as the proposed algorithm w we defined in Algorithm 2. This must always let us check the loss of w 0 on this sequence  X  : Hence, an adversary can achieve at least  X (  X  ) loss for any algorithm w 0 . 4.1 Extensions For simplicity of exposition, we proved Theorem 4.1 under a somewhat limited scope: only for diagonal matrices M , known budget k and cost () . But with some work, these restrictions can be lifted. We sketch a few extensions of the result, although we omit the details due to lack of space. First, the concept of a cost () function and a budget k is not entirely necessary. Indeed, we can redefine the Budgeted Adversary game in terms of an arbitrary stopping criterion  X  : [ n ]  X   X  X  0 , 1 } , where  X  (  X  ) = 0 is equivalent to  X  X he budget has been exceeded X . The only requirement is that  X  () is monotonic, which is naturally defined as  X  (  X i X  ) = 1 =  X   X  (  X  X  ) = 1 for all  X ,  X   X  [ n ]  X  and all i  X  [ n ] . This alternative budget interpretation lets us consider the sequence  X  as a path through a game tree. At a given node  X  t of the tree, the adversary X  X  action i t +1 determines which branch to follow. As soon as  X  (  X  t ) = 0 we have reached a terminal node of this tree.
 Second, we need not assume that the budget k , or even the generalized stopping criterion  X  () , is known in advance. Instead, we can work with the following generalization: the stopping criterion  X  is drawn from a known prior  X  and given to the adversary before the start of the game. The resulting optimal algorithm depends simply on estimating a new version of  X (  X  ) .  X (  X  ) is now redefined as both an expectation over a random  X  and a random  X  drawn from the posterior of  X  , that is where we condition on the event  X  (  X  ) = 1 .
 Third, Theorem 4.1 can be extended to a more general class of M , namely inverse-nonnegative matrices , where M is invertible and M  X  1 has all nonnegative entries. (In all the examples we give we need only diagonal M , but we sketch this generalization for completeness). If we let 1 n be the vector of n ones, then define D = diag  X  1 ( M  X  1 1 n ) , which is a nonnegative diagonal matrix. Also let N = DM  X  1 and notice that the rows of N are the normalized rows of M  X  1 . We can use Algorithm 2 with the diagonal matrix D , and attain distribution w 0 (  X  ) for any  X  . To obtain an Fourth, we have only discussed minimizing loss against a budgeted adversary. But all the results can be extended easily to the case where the player is instead maximizing gain (and the adversary is minimizing). A particularly surprising result is that the minimax strategy is identical in either case; that is, the the recursive definition of w i (  X  ) is the same whether the player is maximizing or minimizing. However, the termination condition might change depending on whether we are minimizing or maximizing. For example in the expert setting, the game stops when all experts have cost larger than k versus at least one expert has gain at least k . Therefore for the same budget size k , the minimax value of the gain version is typically smaller than the value of the loss version. Simplified Notation. For many examples, including two that we consider below, recording the entire sequence  X  is unnecessary X  X he only relevant information is the number of times each i occurs in  X  and not where it occurs. This is the case precisely when the function cost (  X  ) is unchanged up to permutations of  X  . In such situations, we can consider a smaller state space, which records the for the sequence  X  t = i 1 i 2 . . . i t . A straightforward application of Budgeted Adversary games is the  X  X edge setting X  introduced by Freund and Schapire [10], a version of the aforementioned experts setting. The minimax algorithm for this special case was already thoroughly developed by Abernethy et al [1]. We describe an interesting extension that can be achieved using our techniques which has not yet been solved. The Hedge game goes as follows. A learner must predict a sequence of distributions w t  X   X  n , and game ceases only once the best expert has more than k errors, i.e. min i P t ` t,i &gt; k . The learner wants to minimize his total loss.
 The natural way to transform the Hedge game into a Budgeted Adversary problem is as follows. We X  X l let s be the state, defined as the vector of cumulative losses of all the experts. The proposed reduction almost works, except for one key issue: this only allows cost vectors of the form ` t = M e i t = e i t , since by definition Nature chooses columns of M . However, as shown in Abernethy et al, this is not a problem.
 Lemma 5.1 (Lemma 11 and Theorem 12 of [1]) . In the Hedge game, the worst case adversary always chooses ` t  X  X  e 1 , . . . , e n } .
 The standard and more well-known, although non-minimax, algorithm for the Hedge setting [10] uses a simple modification of the Weighted Majority Algorithm [14], and is described simply by loss of this algorithm by k + Abernethy et al [1] provide the minimax optimal algorithm, but state the bound in terms of an expected length of a random walk. This is essentially equivalent to our description of the minimax cost in terms of  X (  X  ) .
 A significant drawback of the Hedge result, however, is that it requires the losses to be uniformly non-uniform cost ranges, i.e. where expert i suffers loss in some range [0 , c i ] . The ` t,i  X  [0 , 1] assumption is fundamental to the Hedge analysis, and we see no simple way of modifying it to achieve a tight bound. The simplest trick, which is just to take c max := max i c i , leads to a bound of the form k + only a single  X  X isky X  expert, with a large c i , should not affect the bound significantly. In our Budgeted Adversary framework, this case can be dealt with trivially: letting M = rem 4.1, we know to be minimax optimal. According to the same theorem, the minimax loss bound is simply  X (  X  ) which, unfortunately, is in terms of a random walk length. We do not know how to obtain a closed form estimate of this expression, and we leave this as an intriguing open question. A classic problem from the Online Algorithms community is known as Metrical Task Systems (MTS), which we now describe. A player (decision-maker, algorithm, etc.) is presented with a finite metric space and on each of a sequence of rounds will occupy a single state (or point) within this metric space. At the beginning of each round the player is presented with a cost vector , describ-ing the cost of occupying each point in the metric space. The player has the option to remain at the his present state and pay this states associated cost, or he can decide to switch to another point in the metric and pay the cost of the new state. In the latter case, however, the player must also pay the switching cost which is exactly the metric distance between the two points.
 The MTS problem is a useful abstraction for a number of problems; among these is job-scheduling. An algorithm would like to determine on which machine, across a large network, it should process a job. At any given time point, the algorithm observes the number of available cycles on each machine, and can choose to migrate the job to another machine. Of course, if the subsequent machine is a great distance, then the algorithm also pays the travel time of the job migration through the network. Notice that, were we given a sequence of cost vectors in advance, we could compute the optimal path of the algorithm that minimized total cost. Indeed, this is efficiently solved by dynamic program-ming, and we will refer to this as the optimal offline cost , or just the offline cost. What we would like is an algorithm that performs well relative to the offline cost without knowledge of the sequence of cost vectors. The standard measure of performance for an online algorithm is the competitive discussed below, we assume that the online algorithm can maintain a randomized state X  X  distri-bution over the metric X  X nd pays the expected cost according to this random choice (Randomized algorithms tend to exhibit much better competitive ratios than deterministic algorithms). When the metric is uniform, i.e. where all pairs of points are at unit distance, it is known that the competitive ratio is O (log n ) , where n is the number of points in the metric; this was shown by Borodin, Linial and Saks who introduced the problem [4]. For general metric spaces, Bartal et al achieved a competitive ratio of O (log 6 n ) [3], and this was improved to O (log 2 n ) by Fiat and Mendel [9]. The latter two techniques, however, rely on a scheme of randomly approximating the metric space with a hierarchical tree metric, adding a (likely-unnecessary) multiplicative cost factor of log n . It is widely believed that the minimax competitive ratio is O (log n ) in general, but this gap has remained elusive for at least 10 years.
 The most significant progress towards O (log n ) is the 2007 work of Bansal et al [2] who achieved such a ratio for the case of  X  X eighted-star metrics X . A weighted star is a metric such that each point i has a fixed distance d i from some  X  X enter state X , and traveling between any state i and j requires going through the center, hence incurring a switching cost of d i + d j . For weighted-star metrics, Bansal et al managed to justify two simplifications which are quite useful: Bansal et al provide an efficient algorithm for this setting using primal-dual techniques developed for solving linear programs. With the methods developed in the present paper, however, we can give the minimax optimal online algorithm under the above simplifications. Notice that the adversary is MTS cost of  X  , which we will call cost (  X  ) . Assume 2 we know the cost of the offline in advance, say which minimizes As we have shown, Algorithm 2 is minimax optimal for this setting. The competitive ratio of this algorithm is precisely lim sup k  X  X  X  1 k MinimaxLoss ( M, cost , k ) . Notice the convenient trick here: by bounding a priori the cost of the offline at k , we can simply imagine playing this repeated game until the budget k is achieved. Then the competitive ratio is just the worst-case loss over the offline cost, k . On the downside, we don X  X  know of any easy way to bound the worst-case loss  X (  X  ) . We now consider the design of so-called cost-function-based information markets, a popular type of prediction market. This work is well-developed by Chen and Pennock [7], with much useful discussion by Chen and Vaughn [8]. We refer the reader to the latter work, which provides a very clear picture of the nice relationship between online learning and the design of information markets. In the simplest setting, a prediction market is a mechanism for selling n types of contract, where a contract of type i corresponds to some potential future outcome, say  X  X vent i will occur X . The standard assumption is that the set of possible outcomes are mutually exclusive, so only one of the n events will occur X  X or example, a pending election with n competing candidates and one eventual winner. When a bettor purchases a contract of type i , the manager of the market, or  X  X arket maker X , promises to pay out $1 if the outcome is i and $0 otherwise.
 A popular research question in recent years is how to design such prediction markets when the out-come has a combinatorial structure. An election might produce a complex outcome like a group of candidates winning, and a bettor may desire to bet on a complex predicate, such as  X  X one of the winning candidates will be from my state X . This question is explored in Hanson [13], although without much discussion of the relevant computational issues. The computational aspects of com-binatorial information markets are addressed in Chen et al [6], who provide a particular hardness result regarding computation of certain price functions, as well as a positive result for an alternative type of combinatorial market. In the present section, we propose a new technique for designing combinatorial markets using the techniques laid out in the present work.
 In this type of information market, the task of a market maker is to choose a price for each of the n contracts, but where the prices may be set adaptively according to the present demand. Let s  X  N n denote the current volume, where s i is the number of contracts sold of type i . In a cost-function-based market, these prices are set according to a given convex  X  X ost function X  C ( s ) which represents a potential on the demand. It is assumed that C (  X  ) satisfies the relation C ( s +  X  1 ) = C ( s ) +  X  for all s , and  X  &gt; 0 and  X  2 C  X  X  2 easy to check this function satisfies the desired properties.
 has the advantage that the total money earned in this market is easy to compute: it X  X  exactly C ( s ) regardless of the order in which the contracts were purchased. A disadvantage of this market, how-ever, is that the posted prices (typically) sum to greater than $1! A primary goal of an information market is to incentivize bettors to reveal their private knowledge of the outcome of an event. If a given bettor believes the true distribution of the outcome to be q  X   X  n , he will have an incentive to purchase any contract i for which the current price p i is smaller than q i , thus providing positive ex-pected reward (relative to his predicted distribution). Using this cost-function scheme, it is possible that q i &lt; C ( s + e i )  X  C ( s ) for all i and hence a bettor will have no incentive to bet. We propose instead an alternative market mechanism that avoids this difficulty: for every given volume state s , the market maker will advertise a price vector w ( s )  X   X  n . If a contract of type i is i i other hand, if the final demand is s , in the worst case the market maker may have to payout a total of max i s i dollars. If we assume the market maker has a fixed budget k on the max number of contracts he is willing to sell, and wants to maximize the total earned money from selling contracts subject to this constraint, then we have 3 exactly a Budgeted Adversary problem: let M be the identity and let cost ( s ) := max i s i .
 This looks quite similar to the Budgeted Adversary reduction in the Hedge Setting described above, which is perhaps not too surprising given the strong connections discovered in Chen and Vaughn [8] between learning with experts and market design. But this reduction gives us additional power: we now have a natural way to design combinatorial prediction markets. We sketch one such example, but we note that many more can be worked out also.
 Assume we are in a setting where we have n election candidates, but some subset of size m &lt; n will become the  X  X inners X , and any such subset is possible. In this case, we can imagine a market maker selling a contract of type i with the following promise: if candidate i is in the winning subset, the payout is 1 /m and 0 otherwise. For similar reasons as above, the market maker should sell contracts at prices p i where P i p i = 1 . If we assume that market maker has a budget constraint of k for the final payout, then we can handle this new setting within the Budgeted Adversary framework by simply modifying the cost function appropriately: This solution looks quite simple, so what did we gain? The benefit of our Budgeted Adversary framework is that we can handle arbitrary monotonic budget constraints, and the combinatorial nature of this problem can be encoded within the budget. We showed this for the case of  X  X ubset betting X , but it can be applied to a wide range of settings with combinatorial outcomes. We have provided a very general framework for solving repeated zero-sum games against a budgeted adversary. Unfortunately, the generality of these results only go as far as games with payoff matrices that are inverse-nonnegative. For one-shot games, of course, Von Neumann X  X  minimax theorem leads us to an efficient algorithm, i.e. linear programming, which can handle any payoff matrix, and we would hope this is achievable here. We thus pose the following open question: Is there an efficient algorithm for solving Budgeted Adversary games for arbitrary matrices M ? [1] J. Abernethy, M. K. Warmuth, and J. Yellin. Optimal strategies from random walks. In Pro-[2] Nikhil Bansal, Niv Buchbinder, and Joseph (Seffi) Naor. A Primal-Dual randomized algorithm [3] Y. Bartal, A. Blum, C. Burch, and A. Tomkins. A polylog (n)-competitive algorithm for met-[4] A. Borodin, N. Linial, and M. E Saks. An optimal on-line algorithm for metrical task system. [5] B. Br  X  ugmann. Monte carlo go. Master X  X  Thesis, Unpublished , 1993. [6] Y. Chen, L. Fortnow, N. Lambert, D. M Pennock, and J. Wortman. Complexity of combina-[7] Y. Chen and D. M Pennock. A utility framework for bounded-loss market makers. In Proceed-[8] Y. Chen and J. W Vaughan. A new understanding of prediction markets via No-Regret learning. [9] A. Fiat and M. Mendel. Better algorithms for unfair metrical task systems and applications. [10] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning [11] S. Gelly and D. Silver. Combining online and offline knowledge in UCT. In Proceedings of [12] S. Gelly, Y. Wang, R. Munos, and O. Teytaud. Modification of UCT with patterns in Monte-[13] R. Hanson. Combinatorial information market design. Information Systems Frontiers , [14] N. Littlestone and M. K. Warmuth. The Weighted Majority algorithm. Inform. Comput. ,
