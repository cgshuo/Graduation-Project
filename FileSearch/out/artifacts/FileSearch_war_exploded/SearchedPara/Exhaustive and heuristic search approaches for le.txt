 Parag C. Pendharkar n 1. Introduction
Software development and maintenance constitutes a signifi-cant cost for organizations, and the delivery of high quality software requires extensive testing for software defects ( Banker and Slaughter, 1997 ). Software testing consumes 40 X 50% of total software development resources, and recent research suggests that optimal resource allocation strategies are necessary to contain software testing cost and increase effectiveness of soft-ware testing ( Huang and Lo, 2006 ). The growing complexity and modular nature of commercial software products makes it difficult to test all software modules in limited time frame and resources ( Huang and Lo, 2006 ; Ostrand and Weyuker, 2005 ). A simple approach to contain cost is to test only the high risk software components ( Zhou and Leung, 2006 ). These high risk software components are identified by using manual inspections, automated sampling, or machine learning methods ( Menzies and
Di Stefano, 2004 ; Khoshgoftaar and Lanning, 1995 ). A common sampling policy for identifying high risk software components is based on static code measures. One such static code measure can be represented as follows: IF cyclomatic complexity Z 10 THEN software has defects :
Several such static code measures form defect detectors which identify the software components that may contain defects. These (2003) note that the information acquisition cost for different input metrics is not exactly same, and the cost of advanced metrics is more than the cost of simple metrics. While it costs more to acquire information on advanced metrics, it is not clear that these advanced metrics actually improve the prediction accuracy of machine learning models. Thus, there is a need to develop an automated procedure that can simultaneously identify appropriate inputs and learn an effective software prediction model. When an automated procedure is used to identify appropriate inputs, it will save cost of human verification and benchmarking of relevant inputs and, for irrelevant inputs, it will allow an organization to save administrative costs for data acquisition of irrelevant inputs. A good prediction accuracy of a classifier will further allow software managers to save software testing cost due to better resource allocation.

In this paper, we propose two hybrid exhaustive search and probabilistic neural network (ES X  X NN), and simulated annealing and probabilistic neural network (SA X  X NN) models for learning a combination of inputs, from all the available inputs, that provide the best prediction accuracy for identifying high risk software components. The rest of the paper is organized as follows. Section 2 describes the software defect prediction model learning problem. Section 3 describes the two hybrid approaches. Section 4 describes the real-world datasets and results of our experiments on hybrid approaches. Section 5 compares our hybrid approaches to traditional classification algorithms and prior studies. Section 6 concludes the paper with a summary and conclusions. 2. A software defect prediction model learning problem
In order to mathematically describe the software defect prediction model learning problem (SDPMLP), we assume that u where d A {0,1} represents whether a software component is low risk (free of defects) or high risk (contains defects), then the SDPMLP can be represented as f ( Z ) -D . The problem consists in finding a combination of a classification function f() and a vector Z such that f( Z ) has the best prediction accuracy among all possible values of vector Z . Since Z depends on U , which is a binary vector of cardinality n , there are a total of 2 n 1 non-null unique values for the vector Z . All of these values have to be considered for identifying the best software defect prediction model. To avoid solutions with only one input (metric), we impose a constraint 9
Z to a total of 2 n ( n +1) possible solution search space for learning the best software defect prediction model. Since using the Binomial theorem: 2 n  X  P n g  X  0 ng  X  X  , the search space complexity of the SDPMLP is factorial.

The solution of SDPMLP consists of systematically identifying learning algorithm, and selecting the value of U n that provides the best prediction accuracy (optimal solution). In case of multiple optimal solutions, all values of U n that provide the optimal solutions can be identified. For small values of n , all possible approach can be used for a large value of n , and a sub-optimal heuristic solution can be obtained for SDPMLP in realistic time. Fig. 1 illustrates a general framework for solving the SDPMLP. There are two stages in the general framework. In the first stage at the top of the figure, we identify different values of U and related random integer j such that 1 r j r n , and then defining u  X  (
It can be shown that if K is feasible then U is feasible whenever k =0 because the cardinality of K is greater than one. Special operations are required to check for the feasibility of U when K is not feasible. Fig. 3 illustrates the pseudo-code for SA algorithm z  X  0
T  X 
T
K  X  U  X  K BestClassification  X   X  PNN( U Best )
U  X 
K while z  X  do where Z i is the i th training pattern and s is a smoothing parameter for the Gaussian kernel. The third layer contains summation nodes which sum the outputs o i of the patterns belonging to each class in the training data (we assume two the summation layer output to one of the classes using prior probabilities of each class and applying the Bayesian decision rule.
The key design parameter in the performance of PNN is the selection of appropriate s . While a constant value of s can be chosen by trial and error, we use a bisection method to optimize the value of sigma using training data and the leave-one-out sampling approach. One bisection method approach is similar to the one described in Pendharkar (2008) and is illustrated in Fig. 5 . The method PNN ( a ) returns the number of correct classifications using leave-one-out sampling on training data for the value of s = a .

The PNN memorizes all training data patterns, and allows online training and updating capability ( Saad et al., 1998 ). For a 1998 ). Since convergence of a bisection method is exponential, we find that dynamically selecting a value of s during training phase of the PNN does not substantially increase computational over-head.

We implement the two hybrid ES X  X NN and SA X  X NN ap-proaches using the C++ programming language in Visual Studio .Net. We also implemented and tested our system in SUN Solaris operating system and GNU complier collection (GCC) on a SUN Blade 2000 workstation. The system was tested rigorously for its accuracy and performance. During our tests, we obtained best results when the variable e was assigned a value of 0.1. Since the number of inputs for the PNN at second stage is determined by the first stage algorithm, values of j and b were computed dynamically. We found that initial values of j =2 n 9 Z 9 and b =200 n 9 Z 9 worked well for data sets used in our study. For our SA implementation, we obtained the best results for z max =10, T =1000, and a =0.799. 4. Real-world software engineering data, experiments and results
We use two real-world data sets for our experiments. Both of our data sets are publicly available from the Promise Software Engineering Repository (PSER) at http://promise.site.uottawa.ca/ SERepository/datasets-page.html . The first data set is related to the DATATRIEVE product, which was developed in a hybrid BLISS and C programming language. There are a total of 9 attributes and 130 examples in this data set. The first 8 attributes contain various LOC measures and project team knowledge of modules, and the last attribute is the decision attribute which takes a binary value depending on whether faults were found in a module or not. Of the total of 130 modules, 119 modules did not contain any faults and remaining 11 modules contained some faults. Table 1 details different attributes for the DATATRIVE product data set.
Our second data set comes from a C programming application called JM1. JM1 is a real-time predictive ground system, which uses simulations to generate predictions. The data for JM1 were the remaining 4389 modules for training data and 4390 modules for the test data did not contain any defects.

The different sizes of the two data sets represent different search space complexities. For the DATATRIEVE product the search space complexity was 2 8 (8+1)=247. However, for JM1 product the search space complexity was much larger and was equal to 2 (21+1)=2,097,130 possible solutions. For JM1 data set, a single run of the PNN, at the second stage in Fig. 1 , typically took 0 2 4 6 8 10 12
Frequency lines of code reused from version 6.0. The frequency analysis also indicates that solution vectors containing low frequency variables numbered 5, 6, 7 can be avoided.

For our JM1 data set, given factorial complexity of the SDPMLP, we do not have the luxury of finding all the optimal solutions. Since ES X  X NN always finds all the optimal solutions at the termination of its run, and given an approximate estimate of 120 years of run time to termination, we did not use ES X  X NN for the JM1 data set. We only used the SA X  X NN for the JM1 data set. SA X  X NN provided us one optimal solution with 100% correct classification on the holdout sample. This optimal solution vector for JM1 was U n = [110110001010111111001] with the number of correct classifications for holdout sample equal to 5443. 5. Comparison of our results with other studies and techniques, and lessons learnt
We compare the results of our experiments with other studies and algorithms; and draw some valuable conclusions. Menzies et al. dataset and their results show approximately 75% correct classifica-tion of software defects. Morasca and Ruhe (2000) use logistic regression and rough sets to classify software defects for DATATRIVE regression and 93.75% for rough sets. Since we get 100% correct classification in our holdout sampl e, the results of our experiments outperform the results of prior studies on these datasets.
One of the limitations of compa ring the performance of our might be different from sampling of holdout data in previous studies. A better comparison can be obtained if we test different classification techniques on training and holdout data samples used in our study. For such a comparison, we use Clementine data mining software and compare the results of our experiments with a quick propagation neural network, CART, and C5.0 classification ap-proaches. All of these classification approaches are available in Clementine data mining software. Fig. 7 illustrates our experimental setup in the Clementine software. We used the default settings for our experiments. Table 4 illustrates the results of our holdout data set experiments. Comparing the re sults of hybrid approaches from previous section to the results of standard classification approaches  X  CART, ANN, and C 5.0  X  from Table 4 indicates that the hybrid approaches outperform standard approaches.

From our comparisons, we learn following lessons: 1. Since standard classification approaches do not perform variable selection, it appears that some of the variables in the datasets are irrelevant and these variables degrade the performance of traditional classification approaches. The performance degrada-tion due to irrelevant variables was noted by critics of traditional machine learning approaches and our study supports this assertion ( Stracuzzi and Utgoff, 2004 ; Chan et al., 2006 ). 2. Hybrid approaches can identify irrelevant/redundant data.
Assuming that cost of data collection was same for all variables, any future efforts to omit collection of irrelevant data can result in administrative cost savings of between 38% and 62%.
