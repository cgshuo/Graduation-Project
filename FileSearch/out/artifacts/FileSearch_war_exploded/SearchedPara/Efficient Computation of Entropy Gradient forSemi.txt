 Semi-supervised learning is of growing importance in machine learning and NLP (Zhu, 2005). Condi-tional random fields (CRFs) (Lafferty et al., 2001) are an appealing target for semi-supervised learning because they achieve state-of-the-art performance across a broad spectrum of sequence labeling tasks, and yet, like many other machine learning methods, training them by supervised learning typically re-quires large annotated data sets.

Entropy regularization (ER) is a method of semi-supervised learning first proposed for classification tasks (Grandvalet and Bengio, 2004). In addition to maximizing conditional likelihood of the available labels, ER also aims to minimize the entropy of the predicted label distribution on unlabeled data. By in-sisting on peaked, confident predictions, ER guides the decision boundary away from dense regions of input space. It is simple and compelling X  X o pre-clustering, no  X  X uxiliary functions, X  tuning of only one meta-parameter and it is discriminative.
Jiao et al. (2006) apply this method to linear-chain CRFs and demonstrate encouraging accuracy improvements on a gene-name-tagging task. How-ever, the method they present for calculating the gradient of the entropy takes substantially greater time than the traditional supervised-only gradient. Whereas supervised training requires only classic forward/backward, taking time O ( ns 2 ) (sequence length times the square of the number of labels), their training method takes O ( n 2 s 3 )  X  X  factor of O ( ns ) more. This greatly reduces the practicality of using large amounts of unlabeled data, which is exactly the desired use-case.

This paper presents a new, more efficient entropy gradient derivation and dynamic program that has the same asymptotic time complexity as the gradient for traditional CRF training, O ( ns 2 ) . In order to de-scribe this calculation, the paper introduces the con-cept of subsequence constrained entropy  X  X he en-tropy of a CRF for an observed data sequence when part of the label sequence is fixed. These meth-ods will allow training on larger unannotated data set sizes than previously possible and support active learning. Lafferty et al. (2001) present linear-chain CRFs, a discriminative probabilistic model over observation sequences x and label sequences Y =  X  Y 1 ..Y n  X  , where | x | = | Y | = n , and each label Y i has s differ-ent possible discrete values. For a linear-chain CRF of Markov order one: where F k ( x, Y ) = P i f k ( x, Y i , Y i +1 , i ) , and the partition function Z ( x ) = P
Y exp( P k  X  k F k ( x, Y )) . Given training data D =  X  d 1 ..d n  X  , the model is trained by maximizing the log-likelihood of the data L (  X  ; D ) = P d log p  X  ( Y ( d ) | x ( d ) ) by gradient methods (e.g. Limited Memory BFGS), where the gradient of the likelihood is: The second term (the expected counts of the features given the model) can be computed in a tractable amount of time, since according to the Markov as-sumption, the feature expectations can be rewritten: A dynamic program (the forward/backward algo-rithm) then computes in time O ( ns 2 ) all the needed probabilities p  X  ( Y i , Y i +1 ) , where n is the sequence length, and s is the number of labels.

For semi-supervised training by entropy regular-ization , we change the objective function by adding the negative entropy of the unannotated data U =  X  u 1 ..u n  X  . (Here Gaussian prior is also shown.) L (  X  ; D, U ) = X This negative entropy term increases as the decision boundary is moved into sparsely-populated regions of input space. In order to maximize the above objective function, the gradient for the entropy term must be computed. Jiao et al. (2006) perform this computation by: where
While the second term of the covariance is easy to compute, the first term requires calculation of quadratic feature expectations. The algorithm they propose to compute this term is O ( n 2 s 3 ) as it re-quires an extra nested loop in forward/backward.
However, the above form of the gradient is not the only possibility. We present here an alternative derivation of the gradient:
Since P Y p  X  ( Y | x ) P Y 0 p  X  ( Y 0 | X ) F k ( x, Y 0 P cels, leaving:
Like the gradient obtained by Jiao et al. (2006), there are two terms, and the second is easily com-putable given the feature expectations obtained by forward/backward and the entropy for the sequence. However, unlike the previous method, here the first term can be efficiently calculated as well. First, the term must be further factored into a form more amenable to analysis: X = X = X to efficiently calculate this term, it is sufficient to calculate P Y pairs y i , y i +1 . The next section presents a dynamic program which can perform these computations in O ( ns 2 ) . We define subsequence constrained entropy as H  X  ( Y The key to the efficient calculation for all subsets is to note that the entropy can be factored given a linear-chain CRF of Markov order 1, since Y i +2 is independent of Y i given Y i +1 .
 X
Given the H  X  (  X  ) and H  X  (  X  ) lattices, any sequence entropy can be computed in constant time. Figure 1 illustrates an example in which the constrained se-quence is of size two, but the method applies to arbitrary-length contiguous label sequences.
Computing the H  X  (  X  ) and H  X  (  X  ) lattices is easily performed using the probabilities obtained by for-ward/backward. First recall the decomposition for-mulas for entropy: Using this decomposition, we can define a dynamic program over the entropy lattices similar to for-ward/backward: The base case for the dynamic program is H  X  (  X  X  y 1 ) = p ( y 1 ) log p ( y 1 ) . The backward entropy is computed in a similar fashion. The conditional programs are available by marginalizing over the per-transition marginal probabilities obtained from forward/backward.

The computational complexity of this calcula-tion for one label sequence requires one run of for-ward/backward at O ( ns 2 ) , and equivalent time to calculate the lattices for H  X  and H  X  . To calculate the gradient requires one final iteration over all label pairs at each position, which is again time O ( ns 2 ) , but no greater, as forward/backward and the en-tropy calculations need only to be done once. The complete asymptotic computational cost of calcu-lating the entropy gradient is O ( ns 2 ) , which is the same time as supervised training, and a factor of O ( ns ) faster than the method proposed by Jiao et al. (2006).

Wall clock timing experiments show that this method takes approximately 1.5 times as long as traditional supervised training X  X ess than the con-stant factors would suggest. 1 In practice, since the three extra dynamic programs do not require re-calculation of the dot-product between parameters and input features (typically the most expensive part of inference), they are significantly faster than cal-culating the original forward/backward lattice. In addition to its merits for computing the entropy gradient, subsequence constrained entropy has other uses, including confidence estimation. Kim et al. (2006) propose using entropy as a confidence esti-mator in active learning in CRFs, where examples with the most uncertainty are selected for presenta-tion to humans labelers. In practice, they approxi-mate the entropy of the labels given the N-best la-bels. Not only could our method quickly and ex-actly compute the true entropy, but it could also be used to find the sub sequence that has the highest un-certainty, which could further reduce the additional human tagging effort. Hernando et al. (2005) present a dynamic program for calculating the entropy of a HMM, which has some loose similarities to the forward pass of the algorithm proposed in this paper. Notably, our algo-rithm allows for efficient calculation of entropy for any label subsequence.

Semi-supervised learning has been used in many models, predominantly for classification, as opposed to structured output models like CRFs. Zhu (2005) provides a comprehensive survey of popular semi-supervised learning techniques. This paper presents two algorithmic advances. First, it introduces an efficient method for calculating subsequence constrained entropies in linear-chain CRFs, (useful for active learning). Second, it demonstrates how these subsequence constrained entropies can be used to efficiently calculate the gradient of the CRF entropy in time O ( ns 2 )  X  the same asymptotic time complexity as the for-ward/backward algorithm, and a O ( ns ) improve-ment over previous algorithms X  X nabling the prac-tical application of CRF entropy regularization to large unlabeled data sets.

