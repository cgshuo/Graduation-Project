 Sashank J. Reddi sjakkamr@cs.cmu.edu Barnab  X as P  X oczos bapoczos@cs.cmu.edu Measuring dependencies and conditional dependencies are of great importance in many scientific fields in-cluding machine learning, and statistics. There are numerous problems where we want to know how large the dependence is between random variables, and how this dependence changes if we observe other random variables. Correlated random variables might become independent when we observe a third random variable, and the opposite situation is also possible where inde-pendent variables become dependent after observing other random variables.
 Thanks to the wide potential application range (e.g., in bioinformatics, pharmacoinformatics, epidemiol-ogy, psychology, econometrics), finding e ffi cient depen-dence and conditional dependence measures has been an active research area for decades. These measures have been used, for example, in causality detection, feature selection, active learning, structure learning, boosting, image registration, independent component and subspace analysis. Although the theory of these estimators is actively researched, there are still several fundamental open questions.
 The estimation of certain dependence and conditional dependence measures is easy in a few cases: For ex-ample, (i) when the random variables have discrete distributions with finitely many possible values, (ii) when there is a known simple relationship between them (e.g., a linear model describes their behavior), or (iii) if they have joint distributions that belong to a parametric family that is easy to estimate (e.g. nor-mal distributions). In this paper we consider the more challenging nonparametric estimation problem when the random variables have continuous distributions, and we do not have any other information about them. Numerical experiments indicate, that a recently pro-posed kernel measure based on normalized cross-covariance operators ( D HS ) appears to be very pow-erful in measuring both dependencies and conditional dependencies ( Fukumizu et al. , 2008 ). Nonetheless, even this method has a few drawbacks and several open fundamental questions. In particular, lower and upper bounds on the convergence rates of the estima-tors are not known. It is also not clear if the D HS measure or its existing estimator D HS are invariant to any invertible transformations of the random vari-ables. This kind of invariance property is so impor-tant that Schweizer &amp; Wol ff ( 1981 )evenputitinto the axioms of dependence. One reason for this is that in many scenarios we need to compare the estimated dependencies. If certain variables are measured on dif-ferent scales, the dependence can be much di ff erent in absence of this invariance property. As a result, it might happen that in a dependence based feature se-lection algorithm di ff erent features would be selected if we measured a quantity e.g. in grams, kilograms, or if we used log-scale. This is an odd situation that can be avoided with dependence measures that are invariant to invertible transformations of the variables. Main contributions: The goal of this paper is to provide new theoretical insights in this field. Our con-tributions can be summarized in the following points: (i) We prove that the dependence and conditional de-pendence measures ( D HS ) are invariant to any invert-ible transformations, but its estimator D HS does not have this property. (ii) Under some conditions we de-rive an upper bound on the rate of convergence of D
HS . (iii) We show that if we apply D HS on the empirical copula transformed points, then the result-ing estimator D C will be invariant to any monotone increasing transformations. (iv) We show that under some conditions the estimator is consistent and derive an upper bound on the rate. (v) We prove that if the conditions are less restrictive, then convergence of both D HS and D C can be arbitrarily slow. (vi) We also generalize these dependence measures as well as their estimators to sets of random variables and pro-vide an upper bound on the convergence rate of the estimators.
 Related work: Since the literature on dependence measure is huge, we only mention few prominent exam-ples here. The most well-known dependence measure is probably the Shannon mutual information, which has been generalized to the R  X enyi- X  ( R  X enyi , 1961 ) and Tsallis- X  mutual information ( Tsallis , 1988 ). Other interesting dependence measures are the maximal cor-relation coe ffi cient ( R  X enyi , 1959 ), kernel mutual infor-mation ( Gretton et al. , 2003 ), the generalized variance and kernel canonical correlation analysis ( Bach , 2002 ), the Hilbert-Schmidt independence criterion ( Gretton et al. , 2005 ), the Schweizer-Wol ff measure ( Schweizer &amp; Wol ff , 1981 ), maximum-mean discrepancy (MMD) ( Borgwardt et al. , 2006 ; Fortet &amp; Mourier , 1953 ), Copula-MMD ( P  X oczos et al. , 2012 ), and the distance based correlation ( Sz  X ekely et al. , 2007 ). Some of these measures, e.g. the Shannon mutual information, are invariant to any invertible transformations of the ran-dom variables. The Copula-MMD and Schweizer-Wol ff measures are invariant to monotone increasing transformations, while the MMD and many other de-pendence measures are not invariant to any of these transformations. The conditional dependence estima-tion is an even more challenging problem, and only very few dependence measures have been generalized to the conditional case ( Fukumizu et al. , 2008 ; Poczos &amp;Schneider , 2012 ).
 Notation: We use X  X  P to denote that the random variable X has probability distribution P . The symbol X  X  X  X  Y | Z indicates the conditional independence of X and Y given Z. Let X i : j denote the tuple ( X i ,...,X j E [ X ] stands for the expectation of random variable X . The symbols  X  X and B X denote measure and Borel  X   X  field on X respectively. We use D ( X i ,...,X j )to denote the dependence measure of set of random vari-ables { X i ,...,X j } . With slight abuse of notation, we will use D ( X, Y ) to denote the dependence measure between sets of the random variables X and Y . { A j } denotes the set { A 1 ,...,A k } where k will be clear from the context. The null space and range of an operator L are denoted by N ( L ) and R ( L ) respectively, and A stands for the closure of set A . In this section, we review the theory behind the Hilbert Schimdt (HS) norm and its use in defining depen-dence measures. Suppose ( X, Y ) is a random vari-able on X  X  Y . Let H X = { f : X  X  R } be a repro-ducing kernel Hilbert Space (RKHS) associated with X , feature map  X  ( x )  X  H X ( x  X  X ) and kernel k
X ( x, y )=  X  ( x ) ,  X  ( y ) H X ( x, y  X  X ). The kernel sat-isfies the property f ( x )= f, k X ( x, . ) H which is called the reproducing property of the kernel. We can similarly define RKHS H Y and kernel k Y as-sociated with Y . Let us define a class of kernels known as universal kernels, which are critical to this paper. Definition 1. A kernel k X : X  X  X  X  R is called universal whenever the associated RKHS H X is dense in C ( X )  X  the space of bounded continuous functions over X  X  with respect to the L  X  norm.
 Gaussian and Laplace kernels are two popular kernels which belong to the class of universal kernels. The cross-covariance operators on these RKHSs cap-ture the dependence of random variables X and Y ( Baker , 1973 ). In order to ensure existence of these operators, we assume that E [ k X ( X, X )] &lt;  X  and E [ k Y ( Y, Y )] &lt;  X  . We also assume that all the ker-nels defined in this paper satisfy the above assump-tion. The cross-covariance operator (COCO)  X  YX : H
X  X  H Y is an operator such that holds for all f  X  H X and g  X  H Y . Existence and uniqueness of such an operator can be shown using the Riesz representation theorem ( Reed &amp; Simon , 1980 ). If X and Y are identical, then the operator  X  XX is called the covariance operator. Both operators above are natural generalizations of the covariance matrix in Euclidean space to Hilbert space. Analogous to cor-relation matrix in the Euclidean space, we can define operator V YX , where R ( V YX )  X  R (  X  YY ) and N ( V YX )  X   X  R (  X  XX This operator is called the normalized cross-covariance operator (NOCCO). Similar to  X  YX , the existence of NOCCO can be proved using Riesz representation the-orem. Intuitively, the normalized cross-covariance op-erator captures the dependence of random variables X and Y discounting the influence of the marginals. We would like to point out that the notation we adopted leads to a few technical di ffi culties which can easily be addressed (refer ( Gr  X unew  X alder et al. , 2012 )). We also define the conditional covariance operators which will be useful to capture conditional indepen-dence. Suppose we have another random variable Z on Z with RHKS H Z and kernel k Z . The normalized conditional cross-covariance operator is defined as: Similar to the cross-covariance operator, the condi-tional cross-covariance operator is a natural extension of conditional covariance matrix to Hilbert space. An interesting aspect of the normalized conditional cross-covariance operator is that it can be expressed in terms of simple products of normalized cross-covariance op-erators ( Fukumizu et al. , 2008 ).
 It is not surprising that covariance operators described above can be used for measuring dependence between random variables since they capture the dependence between them. While one can use  X  YX or V YX in defining the dependence measure ( Gretton et al. , 2005 ; Fukumizu et al. , 2008 ), we will use the latter in this paper. Let the variable X denote ( X, Z ), which will be useful for defining conditional dependence mea-sures. We define the HS norm of a linear operator L : H X  X  H Y as follows.
 Definition 2. (Hilbert-Schmidt Norm) The Hilbert-Schmidt norm of L is defined as where { u i } and { v j } are an orthonormal bases of H X and H Y respectively, provided the sum converges. The HS norm is a generalization of the Frobenius norm on matrices and is independent of the choice of the orthonormal bases. An operator is called Hilbert-Schmidt if its HS norm is finite. The covariance oper-ators defined in this paper are assumed to be Hilbert-Schmidt Operators. Fukumizu et al. ( 2008 )definethe following dependence measures: Note that the measures above are defined for a pair of random variables X and Y . In Section 5 we will generalize this approach and provide dependence and conditional measures with estimators that operate on sets of random variables. Theorem 10 (in the supple-mentary material) justifies the use of the above depen-dence measures. The result can be equivalently stated in terms of HS norm of the covariance operators since HS norm of an operator is zero if and only if the oper-ator itself is a null operator. At first it might appear that these measures are strongly linked to the kernel used in constructing the measure but Fukumizu et al. ( 2008 ) show a remarkable property that the measures are independent of the kernels, which is captured by the following result. Let E Z [ P X | Z  X  P Y | Z ( A  X  B )] = E [ B ( Y ) | Z = z ] E [ A ( X ) | Z = z ] dP Z ( z ) for A  X  B and B  X  B Y .
 Theorem 1. (Kernel-Free property) Assume that the probabilities P XY and E Z [ P X | Z  X  P Y | Z ] are absolutely continuous with respect to  X  X  X   X  Y with probability density functions p XY and p X  X  X  X  Y | Z , respectively, then we have D HS ( X, Y | Z )= Suppose Z =  X  , we have D HS ( X, Y )= The above result shows that these measures bear an uncanny resemblance to mutual information and might possibly inherit some of its desirable properties. We show that this intuition is, in fact, true by proving an important consequence of the above result. In partic-ular, the following result holds when the assumptions stated in Theorem 1 are satisfied.
 Theorem 2. (Invariance of dependence measure) As-sume that the probabilities P XY and E Z [ P X | Z  X  P Y | Z are absolutely continuous. Let X  X  R d , and let  X  X : X  X  R d be a di ff erentiable injective function. Let  X  Y and  X  Z be defined similarly. Under the above assumptions, we have As a special case of Z =  X  , we have The proof is in the supplementary material. Though we proved the result for Euclidean spaces, it can be generalized to certain measurable spaces under mild conditions ( Hewitt &amp; Stromberg , 1975 ). We now look at the empirical estimators for the dependence mea-sures defined above. Let X 1: m ,Y 1: m and Z 1: m be i.i.d samples from the joint distribution. Let  X  ( m ) X = note the empirical mean maps respectively. The em-pirical estimator of  X  YX is  X  The empirical covariance operators  X  ( m ) XX and  X  ( m ) be defined in a similar fashion. The empirical normal-ized cross-covariance operator V YX is
V where m &gt; 0 is the regularization constant ( Bach , 2002 ; Fukumizu et al. , 2004 ). In the later sections, we look at a particular choice of m which provides good convergence rates. The empirical conditional cross-covariance operator is Let G X be the centered gram matrix such that G R
X = G X ( G X + m m I ) G Y ,G Z and R Y ,R Z for random variables Y and Z . The empirical dependence measures are then Fukumizu et al. ( 2008 ) show the consistency of the above estimators. We now provide an upper bound on the convergence rates of these estimators under certain assumptions.
 Theorem 3. (Consistency of operators) Assume  X  isfies m  X  0 and 3 m m  X  X  X  . Then, we have conver-gence in probability in HS norm i.e An upper bound on convergence rate of V ( m ) YX  X  V Proof Sketch (refer supplementary for complete proof). The first term can be shown to be O p  X  3 / 2 m m  X  1 / 2 using Lemma 3 (in the supplementary material). To prove the second part, consider the complete orthogo-nal systems {  X  i }  X  i =1 and {  X  i }  X  i =1 for H X and H an eigenvalue  X  i  X  0 and  X  i  X  0 respectively. Using the definition of Hilbert-Schimdt norm, we can bound the square of second term by Using AM-GM inequality, and assuming m  X  1 , m  X  1 and that  X  Schmidt, the theorem follows.
 Theorem 4. (Consistency of estimators) As- X  satisfies m  X  0 and 3 m m  X  X  X  . Then, we have An upper bound on convergence rate of the estimator D
HS ( X, Y | Z ) is O p ( The proof is in the supplementary material. The as-sumptions used in Theorems 3 and 4 depend on the probability distribution and the kernel. A natural question arises if such assumptions are necessary for estimating these measures. The following result an-swers this question a ffi rmatively. The crux of the re-sult lies in the fact that these dependence measures are intricately connected to functionals of probability distributions that are typically hard to estimate. In particular, we have D HS ( X, Y )= Since estimation of these functionals is tightly coupled with smoothness assumptions on probability distribu-tions of the random variables, it is reasonable to as-sume that our assumptions have a similar e ff ect. We prove the result for the special case where X , Y  X  R d and Z =  X  . Let P denote the set of all distributions in [0 , 1] d .
 Theorem 5. Let D n denote an estimator of D HS on sample size n. For any sequence of estimates { D n } and any sequence { a n } converging to 0, there exists a compact subset P 0  X  P for which the uniform rate of convergence is slower than { a n } . In other words, where D = D HS ( X, Y ) .
 The proof is in the supplementary material. An impor-tant point to note is that while the dependence mea-sures themselves are invariant to invertible transfor-mations, the estimators do not possess this property. It is often desirable to have this invariance property for the estimators as well since we generally deal with fi-nite sample estimators. This property can be achieved using copula transformation, which is our focus in the next section. Along with the theoretical analysis, we also provide a compelling justification for using cop-ula transformation from a practical point of view in Section 6 . We review important properties of the copula of mul-tivariate distributions in this section. The use of the transformation will be clear in the later sections. The copula plays an important role in the study of depen-dence among random variables. They not only cap-ture the dependence between random variables but also help us construct dependence measures which are invariant to any strictly increasing transformation of the marginal variables.
 Sklar X  X  theorem is central to the theory of copulas. It gives the relationship between a multivariate random variable and its univariate marginals. Suppose X =
X 1 ,...,X d  X  R d is a d -dimensional multivariate random variable. Let us denote the marginal cumu-lative distribution function (cdf) of X j by F j X : R  X  [0 , 1]. In this paper, we assume that the marginals F
X are invertible. The copula is defined by Sklar X  X  theorem as follows: Theorem 6. (Sklar X  X  theorem). Let H ( x 1 ,...,x d ) = Pr X 1  X  x 1 ,...,X d  X  x d be the multivariate cumu-lative distribution function with continuous marginals F j X . Then there exists a unique copula C such that Conversely, if C is a copula and F j X are marginal cdfs, then H given in Equation ( 1 ) is the joint distri-bution with marginals F j X .
 Let T X = T 1 X ,...,T d X denote the transformed vari-ables where T X = F X ( X )= F 1 X ( X 1 ) ,...,F d X ( X [0 , 1] d . Here, F X is called copula transformation. The above theorem gives a one-to-one correspondence be-tween the joint distribution of X and T X .Further-more, it provides a way to construct dependence mea-sures over the transformed variables since we have information about the copula distribution. An in-teresting consequence of the copula transformation is that we can get invariance of dependence measures to any strictly increasing transformations of the marginal variables. Consider random variables X = X 1 ,...,X d , Y =
Y 1 ,...,Y d and Z = Z 1 ,...,Z d . We focus on the problem of defining dependence measure D C ( X, Y ) and conditional dependence measure D C ( X, Y | Z )us-ing copula transformation. The next section gener-alizes the dependence measure to any set of random variables. Note that we have assumed that the ran-dom variables X , Y and Z are all d  X  dimensional for simplicity, but our results hold for random variables with di ff erent dimensions.
 Let T X ,T Y and T Z be the copula transformed vari-ables of X, Y and Z respectively. With slight abuse of notation, we use k X ,k Y and k Z to denote the ker-nels over transformed variables T X , T Y and T Z respec-tively. In what follows, the kernels are functions of the form [0 , 1] d  X  [0 , 1] d  X  R since they are defined over the transformed random variables. We define the de-pendence among random variables X and Y as: The conditional dependence measure is defined as: D C ( X, Y | Z )= D HS ( T X ,T Y | T Z )= V T where T X =( T X ,T Z ). By Theorem 10 (in supplemen-tary) and the fact that the marginal cdfs are invert-ible, it is easy to see that D C ( X, Y )=0  X  X  X  X  X  Y and D C ( X, Y | Z )=0  X  X  X  X  X  Y | Z . Our goal is to estimate the dependence measures D C ( X, Y ) and D C ( X, Y | Z ) using the i.i.d samples X 1: m , Y 1: m and Z 1: m . Suppose we have the copula transformed vari-ables T X ,T Y and T Z , then we can use the estimators for dependence and conditional dependence measures respectively. However, we only have i.i.d samples X 1: m , Y 1: m , Z 1: m , and the marginal cdfs are unknown to us. We have to get the empirical copula trans-formed variables through these samples by estimating the marginals distribution functions F j X , F j Y and
F j Z . These distribution functions can be estimated e ffi ciently using the rank statistics. For x  X  R and x j  X  R for 1  X  j  X  d ,let F
X is called the empirical copula transforma-tion of X . The samples T X 1 ,..., T Xm =
F X ( X 1 ) ,..., F X ( X m ) , called the empirical copula, are estimates of true copula transformation. We can similarly define empirical copula transformations and empirical copula for random variables Y and Z .It should be noted that the samples of empirical cop-ula T X 1 ,..., T Xm are not independent even though X 1: m are i.i.d samples. We can now use the depen-dence estimators in ( Fukumizu et al. , 2008 )usingem-pirical copula T X 1 ,..., T Xm instead of the samples ( X 1 ,...,X m ) . Lemma 2 (in supplementary) shows that the empirical copula is a good approximation of i.i.d. samples ( T X 1 ,...,T Xm ).
 It is important to note the relationship between mea-sures D HS and D C . The copula transformation can also be viewed as an invertible transformation and hence, by Theorem 2 we have D HS = D C . Though the measures are identical, their corresponding estimators D
HS and D C are di ff erent. At this point, we should also emphasize the di ff erence between our work and P  X oczos et al. ( 2012 ). Although both these works use copula trick to obtain invariance, in contrast to P  X oczos et al. ( 2012 ), we essentially get the same measure even after copula transformation. In other words, the cop-ula transformation in our case does not change the de-pendence measure and therefore, provides an invariant finite sample estimator to D HS .Thus,weprovidea compelling case to use copulas for D HS . Moreover, the invariance property extends naturally to conditional dependence measure in our case.
 We now focus on the consistency of the proposed esti-mators D C ( X, Y ) and D C ( X, Y | Z ). We assume that the kernel functions k X ,k Y and k Z are bounded ker-nel functions and are Lipschitz continuous on [0 , 1] d i.e there exists a B&gt; 0 such that for all x, x 1 ,x 2  X  [0 , 1] d . The gaussian kernel is one of the popular kernels which is not only universal but also bounded and Lipschitz continuous. In what follows, we assume that conditions required for Theorem 3 and 4 hold for the transformed variables as well. We now show the consistency of the dependence estimators and provide upper bounds on their rates of convergence. Theorem 7. (Consistency of copula dependence es-timators) Assume kernels k X ,k Y and k Z are bounded and Lipschitz continuous. Suppose m satisfies m  X  0 and 3 m m  X  X  X  , then (i) D C ( X, Y ) P  X  X  X  D C ( X, Y ) . (ii) D C ( X, Y | Z ) P  X  X  X  D C ( X, Y | Z ) . An upper bound on convergence rate of estimators D
C ( X, Y ) and D C ( X, Y | Z ) is O p ( Proof Sketch (refer supplementary for complete proof). We first show V ( m ) the following upper bound of V ( m ) From Theorem 3 , it is easy to see that the second term can be proved to be bounded by C  X  3 / 2 m  X  ( m )  X  prove that  X  ( m ) (refer Lemma 1 in the supplementary material), thereby proving the overall rate to be O p (  X  3 / 2 m m  X  1 / 2 The convergence of the dependence estimators follows from the convergence of the operators.
 The above result shows that by using copula trans-formation, we can ensure that the invariance property holds for finite sample estimators without loss in statis-tical e ffi ciency. It should be noted that slightly better rates of convergence can be obtained by more restric-tive assumptions. It is also noteworthy that under the conditions assumed in this paper, the estimators do not su ff er from the curse of dimensionality, and hence, can be used in high dimensions. Moreover, the estimators only use rank statistics rather than the ac-tual values of X 1: m , Y 1: m and Z 1: m .Thisprovidesus with robust estimators since an arbitrarily large outlier sample cannot a ff ect the statistics badly. In addition, this also makes the estimators invariant to monotone increasing transformations.
 Let us call the dependence measure defined above as pairwise dependence since it measures dependence be-tween two random variables. Now, the question arises if this approach can be generalized to measure depen-dence amongst a set of random variables rather than just two random variables. We answer this question a ffi rmatively in the next section. Suppose S = { S 1 ,...,S n } is a set of random variables. Similar to our previous assumptions, we assume that random variables { S j } are d-dimensional. We would like to measure the dependence amongst the set of ran-dom variables. Recall S i : j represents the random vari-able ( S i ,...,S j ). Note that S i : j is a random variable of ( j  X  i ) d dimensions. With slight abuse of notation, the kernel k i : j corresponding to variable S i : j is defined appropriately. We now express the generalized depen-dence measure as a sum of pairwise dependence mea-sures. For simplicity, we denote D C ( S 1 ,...,S n )by D C ( S ). The dependence measures are defined as: Note that the set dependence measure is a sum of n  X  1 pairwise dependence measures. The following result justifies the use of these dependence measures. Theorem 8. (Generalized dependence measure) If the product kernels k j k j +1: n for j = { 1 ,...,n  X  1 } are universal, we have (i) D C ( S )= 0  X  ( S 1 ,...,S n ) are independent. (ii) D C ( S | Z )=0  X  ( S 1 ,...,S n ) are independent given Z .
 The proof is in the supplementary material. We can now use the pairwise dependence estimators for esti-mating set dependence. The following estimators are used for measuring dependence Let us assume that the conditions required for The-orem 7 are satisfied. The following theorem states the consistency of the dependence estimators proposed above, and provides an upper bound on their rates of convergence.
 Theorem 9. (Consistency of generalized estimators) Suppose the kernels defined above are bounded and Lip-schitz continuous, then (i) D C ( S ) P  X  X  X  D C ( S ) . (ii) D C ( S | Z ) P  X  X  X  D C ( S | Z ) .
 An upper bound on convergence rate of D C ( S ) and D
C ( S | Z ) is O p ( n Proof. The theorem follows easily from Theorem 7 and Equations ( 3 ), ( 4 ), ( 5 ) and ( 6 ). In this section, we empirically illustrate the theoreti-cal contributions of the paper. We compare the perfor-mance of D HS ( X, Y ) and D HS ( X, Y | Z ) (referred to as NHS) with D C ( X, Y ) and D C ( X, Y | Z ) (referred to as CHS), that is with and without copula respectively. In the following experiments, we choose gaussian kernels and choose  X  by median heuristic. We fix m = 10  X  6 for our experiments. 6.1. Synthetic Dataset In the first simulation, we constructed the following random variables: X 1  X  U [0 , 4  X  ], X 2 = (500 V 1 , 1000 tanh( V 2 ) , 500 sinh( V 3 )), where V ,V 2 ,V 3  X  U [0 , 1], and Y = 1000 tanh( X 1 ). 200 sample points are generated using the distributions specified above. The task in this experiment was to choose a feature between X 1 and X 2 that contains the most information about Y . Note that Y is a deterministic function of X 1 and independent of X 2 . Therefore, we expect the dependence for ( X 1 ,Y )to be high and that of ( X 2 ,Y ) to be low. Figure 1 shows dependence measure (DM) comparison of NHS and CHS. It can be seen that while CHS chooses the correct feature X 1 , NHS chooses the incorrect feature X The next simulation is designed to prove the signif-icance of invariance property on finite samples. As mentioned earlier, though NHS is invariant to invert-ible transformation, its estimators do not retain this property. Thanks to copula transformation, CHS does not su ff er from this issue. Let X  X  U [0 , 4  X  ], Y = 50 X and Z = 10 tanh( Y/ 100). Note that Z is an invertible transformation of Y . The dependence of ( X, Z )under CHS is not reported here as CHS is invariant to any monotone increasing transformations. We now demon-strate the asymptotic nature of the invariance property of NHS on the same data. Figure 2 clearly shows that while both methods have almost the same dependence measure for ( X, Y ), this dependence measured by NHS is reduced significantly when Y is transformed to Z . We can clearly see that NHS requires a large sample before it exhibits the invariance property. This prob-lem furthur amplifies as we move to higher dimensions, making it undesirable for higher dimensional tasks. We demonstrate the performance of conditional de-pendence measures in the following experiment. Let X, V  X  U [0 , 4  X  ], Y = 50 sin( V ) and Z = log( X + Y ). Observe that though X and Y are independent, they become dependent when conditioned on Z. The results in Figure 3 clearly indicate that NHS fails to detect the dependence between X and Y when conditioned on Z while CHS successfully captures this dependence. 6.2. Housing Dataset We evaluate the performance of the dependence mea-sures on the Housing dataset from the UCI repository. The importance of scale invariance on real-world data is demonstrated through this experiment. This dataset consists of 506 sample points each of which has 12 real valued and 1 integer valued attributes. We will only consider the real value attributes for this experiment and discard the integer attribute. Our aim is to pre-dict the median value of owner-occupied homes based on other attributes like per capital crime, percentage of lower status of the population etc. This dataset is particularly interesting since it contains features of dif-ferent nature and scale. In this experiment, we would like to predict the single most relevant feature for pre-dicting the median value. Features 13 and 6 achieve the least prediction errors amongst all features using linear regressors (see supplementary material for more details). While CHS predicts these two features as the most relevant features, NHS performs poorly by se-lecting features 1 and 6 as the most relevant features. In this paper we developed new dependence and con-dition dependence measures and estimators which are invariant to any monotone increasing transformations of the variables. We showed that under certain condi-tions the convergence rates of the estimators are poly-nomial, but when the conditions are less restrictive, then similarly to other existing estimators the conver-gence can be arbitrarily slow. We generalized these measures and estimators to sets of variables as well, and illustrated the applicability of our method with a few numerical experiments.
 Bach, Francis R. Kernel independent component anal-ysis. JMLR , 3:1 X 48, 2002.
 Baker, Charles R. Joint measures and cross-covariance operators. Transactions of the American Mathemat-ical Society , 186:pp. 273 X 289, 1973.
 Borgwardt, K., Gretton, A., Rasch, M., Kriegel, H.,
Sch  X olkopf, B., and Smola, A. Integrating structured biological data by kernel maximum mean discrep-ancy. Bioinformatics , 22(14):e49 X  X 57, 2006. Fortet, R. and Mourier, E. Convergence de lar  X eparation empirique vers la r  X eparation th  X eorique. Ann. Scient.  X  Ecole Norm , 70:266 X 285, 1953. Fukumizu, K., Gretton, A., Sun, X., and Schoelkopf, B. Kernel measures of conditional dependence. In NIPS 20 , pp. 489 X 496, Cambridge, MA, 2008. MIT Press.
 Fukumizu, Kenji, Bach, Francis R., and Jordan,
Michael I. Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces.
Journal of Machine Learning Research , 5:73 X 99, 2004.
 Fukumizu, Kenji, Bach, Francis R., and Gretton, Arthur. Statistical convergence of kernel cca. In
Advances in Neural Information Processing Systems 18 , 2005.
 Gretton, A., Herbrich, R., and Smola, A. The kernel mutual information. In Proc. ICASSP , 2003.
 Gretton, A., Bousquet, O., Smola, A., and Sch  X olkopf, B. Measuring statistical dependence with Hilbert-Schmidt norms. In ALT , pp. 63 X 77, 2005.
 Gr  X unew  X alder, S, Lever, G, Baldassarre, L, Patterson,
S, Gretton, A, and Pontil, M. Conditional mean em-beddings as regressors. In Proceedings of the 29th In-ternational Conference on Machine Learning, ICML 2012 , volume 2, pp. 1823 X 1830, 2012.
 Hewitt, E. and Stromberg, K. Real and Abstract Anal-ysis: A Modern Treatment of the Theory of Func-tions of a Real Variable . Graduate Texts in Mathe-matics. Springer, 1975. ISBN 9780387901381.
 Poczos, B. and Schneider, J. Nonparametric estima-tion of conditional information and divergences. In International Conference on AI and Statistics (AIS-
TATS) , JMLR Workshop and Conference Proceed-ings, 2012.
 P  X oczos, Barnab  X as, Ghahramani, Zoubin, and Schnei-der, Je ff G. Copula-based kernel dependency mea-sures. In ICML , 2012.
 Reed, M. and Simon, B. Functional Analysis . Aca-demic Press, 1980. ISBN 9780080570488.
 R  X enyi, A. On measures of dependence. Acta. Math. Acad. Sci. Hungar , 10:441 X 451, 1959.
 R  X enyi, A. On measure of entropy and information. In 4th Berkeley Symposium on Math., Stat., and Prob. , pp. 547 X 561, 1961.
 Ritov, Y. and Bickel, P. J. Achieving information bounds in non and semiparametric models. The An-nals of Statistics , 18(2):pp. 925 X 938, 1990. Schweizer, B. and Wol ff , E. On nonparametric mea-sures of dependence for random variables. The An-nals of Statistics , 9, 1981.
 Sz  X ekely, G. J., Rizzo, M. L., and Bakirov, N. K. Mea-suring and testing dependence by correlation of dis-tances. Annals of Statistics , 35:2769 X 2794, 2007. Tsallis, C. Possible generalization of boltzmann-gibbs
