 Word alignments over parallel corpora have be-come an essential supporting technology to a va-riety of natural language processing (NLP) appli-cations most prominent among which is statisti-cal machine translation (SMT). 1 Although phrase-based approaches to SMT tend to be rob ust to word-alignment errors (Lopez and Resnik, 2006), impro v-ing word-alignment is still useful for other NLP re-search that is more sensiti ve to alignment quality , e.g., projection of information across parallel cor -pora (Yaro wsk y et al., 2001).

In this paper , we present a novel approach to using and combining multiple preprocessing (tok-enization) schemes to impro ve word alignment. The intuition here is similar to the combination of dif-ferent preprocessing schemes for a morphologically rich language as part of SMT (Sadat and Habash, 2006) except that the focus is on impro ving the alignment quality . The language pair we work with is Arabic-English.

In the follo wing two sections, we present related work and Arabic preprocessing schemes. Section 4 and 5 present our approach to alignment preprocess-ing and combination, respecti vely . Results are pre-sented in Section 6. Recently , several successful attempts have been made at using supervised machine learning for word alignment (Liu et al., 2005; Taskar et al., 2005; Itty-cheriah and Rouk os, 2005; Fraser and Marcu, 2006). In contrast to generati ve models, this frame work is easier to extend with new features. With the ex-ception of Fraser and Marcu (2006), these pre vious publications do not entirely discard the generati ve models in that the y inte grate IBM model predictions as features. We extend on this approach by includ-ing alignment information based on multiple prepro-cessing schemes in the alignment process.

In other related work, Tillmann et al. (1997) use several preprocessing strate gies on both source and tar get language to mak e them more alik e with re-gards to sentence length and word order . Lee (2004) only changes the word segmentation of the morpho-logically comple x language (Arabic) to induce mor -phological and syntactic symmetry between the par -allel sentences. We dif fer from these two in that we do not decide on a certain scheme to mak e source and tar get sentences more symmetrical. Instead, it is left to the alignment algorithm to decide under which circumstances alignment information based on a specific scheme is more lik ely to be correct than information based on other schemes. Arabic is a morphologically comple x language with a lar ge set of morphological features. As such, the set of possible preprocessing schemes is rather lar ge (Habash and Sadat, 2006). We focus here on a subset of schemes pertaining to Arabic attachable clitics. There are three de-grees of cliticization that apply to a word BASE: ( [CONJ+ [PART+ [Al+ BASE +PRON]]] ).
 At the deepest level, the BASE can have a def-inite article + (Al+ the ) 2 or a member of the class of pronominal clitics, +PR ON, (e.g., +hA her/it/its ). Ne xt comes the class of particles (PAR T+), (e.g., + s+ will [futur e] ). Most shallo w is the class of conjunctions (CONJ+), (e.g., + w+ and ). We use the follo wing five schemes: AR , D 1 , D 2 , D 3 and T B . Definitions and contrasti ve examples of these schemes are presented in Ta-ble 1. To create these schemes, we use M ADA , an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambo w, 2005), and T OKAN , a general Arabic tok enizer (Habash and Sadat, 2006). Using a preprocessing scheme for word alignment breaks the process of applying Giza++ (Och and Ne y, 2003) on some parallel text into three steps: preprocessing, alignment and remapping. In prepro-cessing, the words are tok enized into smaller units. Then, the y are passed along to Giza++ for alignment (def ault settings). Finally , the Giza++ alignments are mapped back (remapped) to the original word form which is AR tok ens in this work. For instance, tak e the first word in Table 1, wsyktbhA ; if the D 3 preprocesssing scheme is applied to it before align-ment, it is turned into four tok ens ( w+ s+ yktb +hA ). Giza++ will link these tok ens to dif ferent words on the English side. In the remapping step, the union of these links is assigned to the original word wsyk-tbhA . We refer to such alignments as remappings. After creating the multiple remappings, we pass them as features into an alignment combiner . The combiner is also given a variety of additional fea-tures, which we discuss later in this section. The combiner is simply a binary classifier that deter -mines for each source-tar get pair whether the y are link ed or not. Given the lar ge size of the data used, we use a simplifying heuristic that allo ws us to mini-mize the number of source-tar get pairs used in train-ing. Only links evidenced by at least one of the ini-tial alignments and their immediate neighbors are in-cluded. All other links are considered non-e xistent. The combiner we use here is implemented using a rule-based classifier , Ripper (Cohen, 1996). The reasons we use Ripper as opposed other machine learning approaches are: (a) Ripper produces human readable rules that allo w better understanding of the kind of decisions being made; and (b) Ripper is rel-atively fast compared to other machine learning ap-proaches we examined given the very lar ge nature of the training data we use. The combiner is trained us-ing supervised data (human annotated alignments), which we discuss in Section 6.1.

In the rest of this section we describe the dif fer -ent machine learning features given to the combiner . We break the combination features in two types: word/sentence level and remapping features. Word/Sentence Featur es:  X 
Word Form : The source and tar get word forms.  X 
POS : The source and tar get part-of-speech tags.  X 
Location : The source and tar get relative sentence position (the ratio of absolute position to sentence length). We also use the dif ference between these values for both source and tar get.  X 
Fr equency : The source and tar get word frequenc y computed as the number of occurrences of the word form in training data. We also use the ratio of source to tar get frequenc y.
 Similarity : This feature is moti vated by the fact that proper nouns in dif ferent languages often resemble each other , e.g. dam hussein X . We use the equi valence classes pro-posed by Freeman et al. (2006) to normalize Ara-bic and English word forms. Then, we emplo y the longest common substring as a similarity measure. Remapping Featur es:  X 
Link : for each source-tar get link, we include (a) a binary value indicating whether the link exists ac-cording to each remapping; (b) a cumulati ve sum of the dif ferent remappings supporting this link; and (c) co-occurrence information for this link. This last value is calculated for each source-tar get word pair as a weighted average of the product of the rela-tive frequenc y of co-occurrence in both directions for each remapping. The weight assigned to each remapping is computed empirically . 3  X 
Neighbor : The same information as Link, but for each of the immediate neighbors of the current link.  X 
Cr oss : These include (a) the number of source words link ed to the current tar get word, the same for tar get to source, and the number of words link ed to either of the current words; and (b) the ratio of the co-occurrence mass placed in this link to the total mass assigned to the source word, the same for the tar get word and the union of both. 6.1 Experimental Data and Metrics The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMA C) 4 (Ittycheriah and Rouk os, 2005). We only use 8.8K sentences from IBMA C because the rest (smaller portion) of the corpus uses dif ferent normalizations for numerals that mak e the two sets incompatible. We break this data into 6.6K sentences for training and 2.2K sentences for development. As for test data, we use the IBMA C X  X  test set: NIST MTEv al 2003 (663 Arabic sentences each human aligned to four English references).

To get initial Giza++ alignments, we use a lar ger parallel corpus together with the annotated set. The Arabic-English parallel corpus has about 5 million words. 5 The Arabic text in IBMA C is preprocessed in the AR preprocessing scheme with some ad-ditional character normalizations. We match the preprocessing and normalizations on our additional data to that of IBMA C X  X  Arabic and English prepro-cessing (Ittycheriah and Rouk os, 2005).

The standard evaluation metric within word align-ment is the Alignment Error Rate (AER) (Och and Ne y, 2000), which requires gold alignments that are mark ed as  X  X ure X  or  X  X robable X . Since the IBMA C gold alignments we use are not mark ed as such, AER reduces to 1 -F-score (Ittycheriah and Rouk os, 2005): where A links are proposed and S links are gold. NULL links are not included in the evaluation (Ayan, 2005; Ittycheriah and Rouk os, 2005). 6.2 Results We conducted three experiments on our develop-ment data: (a) to assess the contrib ution of align-ment remapping, (b) to assess the contrib ution of combination features for a single alignment (i.e., in-dependent of the combination task) and (c) to deter -mine the best performing combination of alignment remappings. Experiments (b) and (c) used only 2.2K of the gold alignment training data to minimize com-putation time. As for our test data experiment, we use our best system with all of the available data. We also present an error analysis of our best system. The baseline we measure against in all of these exper -iments is the state-of-the-art gro w-diag-final ( gdf ) alignment refinement heuristic commonly used in phrase-based SMT (K oehn et al., 2003). This heuris-tic adds links to the intersection of two asymmetrical statistical alignments in an attempt to assign every word a link. The AER of this baseline is 24.77%. The Contrib ution of Alignment Remapping We experimented with five alignment remappings in two directions: dir (Ar -En) and inv (En-Ar). We also constructed their corresponding gdf alignment. The more verbose a preprocessing scheme, the lower the AER for either direction and for gdf of the corre-sponding remapping. The order of the schemes from worst to best is AR , D 1 , D 2 , T B and D 3 . The best result we obtained through remapping is that of D 3 gdf which had a 20.45% AER (17.4% relati ve de-crease from the baseline).
 The Contrib ution of Combination Featur es For each of the basic ten (non gdf ) alignment remap-pings, we trained a version of the combiner that uses all the rele vant features but has access to one align-ment at a time. We saw a substantial impro vement for all alignment remappings averaging 29.9% rel-ative decrease in AER against the basic remapped version. The range of AER values is from 14.5% ( D 3 dir ) to 20.79% ( AR inv ).
 Alignment Combination Experiments To deter -mine the best subset of alignment remappings to combine, we ordered the alignments given their AER performance in the last experiment described (using combination features). Starting with the best performer ( D 3 in the order of their performance so long the com-Table 2: Combining the Alignment Remappings Alignment Remapping combination AER D 3 dir 14.50 bination X  s AER score is decreased. Our best com-bination results are listed in Table 2. All additional alignments not listed in this table caused an increase in AER. The best alignment combination used align-ments from four dif ferent schemes which confirms our intuition that such combination is useful. Test Set Ev aluation We ran our best system trained on all of the IBMA C data (training &amp; devel-opment), on all the unseen IBMA C test set. On this data we achie ve a substantial relati ve impro vement of 38.3% from an AER of 22.99 to 14.19.

Ittycheriah and Rouk os (2005) used only the top 50 sentences in IBMA C test data. Our best AER re-sult on their test set is 14.02% (baseline is 22.48%) which is higher than their reported result (12.2% with 20.5% baseline (unrefined GIZA++)). The two results are not comparable because: (a) Ittycheriah and Rouk os (2005) used additional gold aligned data that was not released and (b) the y use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set  X  the details of such adaptation were not pro vided and thus it is not clear how to replicate them to compare fairly . Clearly this ad-ditional data is helpful since even their baseline is higher than ours. 6 Err or Analysis We conducted error analysis on 50 sentences from our development set. The ma-jority of the errors involv ed high frequenc y closed-class words (54%) and comple x phrases (non-compositional or divergent translations) (23%). Both kinds of errors could be partly addressed by introducing phrasal constraints which are currently lacking in our system. Orthogonally , about 18% of all errors involv ed gold-standard inconsistencies and errors. These gold errors are split equally between closed-class and comple x-phrase errors. We have presented an approach for using and com-bining multiple alignments created using dif ferent preprocessing schemes. We have sho wn a relati ve reduction of AER of about 38% on a blind test set. In the future, we plan to extend our system with ad-ditional models at the phrase and multi-w ord levels for both alignment and alignment combination im-pro vement. We plan to use more sophisticated ma-chine learning models such as support vector ma-chines for combination and mak e use of more avail-able parallel data. We also plan to evaluate the influ-ence of our alignment impro vement on MT quality .
