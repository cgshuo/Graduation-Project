 Eduardo F. Morales eduardo.morales@itesm.mx Claude Sammut claude@cse.unsw.edu.au tralia Suppose that we want to learn how to control an air-craft in a high-fidelity simulator. This task can be easily formulated as a reinf orcement learning problem, where we want to learn which action to perform in any particular state to maximize the total expected reward reflecting a successful flight. However, this problem, whether the aircraft is real or simulated, typically has 20 to 30 variables, most of them continuous, describ-ing an aircraft moving in a potentially  X  X nfinite X  three dimensional space, hampering the applicability of re-inforcement learning.
 Several approaches have been suggested to deal with large search spaces, such as state abstraction, func-tion approximation, and hierarchical decomposition. In this paper, states are abstracted using a relational representation (see (Morales, 2003)). This has several advantages: it is easy to represent powerful abstrac-tions, simplifying the learning process; domain knowl-edge can be easily incorporat ed into the learning task; and once a policy has been learned, it can be re-used, without any further learning in another similar do-main. Our state abstraction requires the definition of an adequate set of relations and a set of actions that operate on these relations. We call these sets of ac-tions r-actions . In this paper, it is shown how r-actions can be automatically induced from traces of flights us-ing behavioural cloning. Since turbulence is added to flights, the pilot is not always consistent in his/her ac-tions and due to the characteristics of our particular behavioural cloning algorithm, a single state can have several applicable r-actions . Our behavioural cloning approach, however, induces only a small subset of pos-sible r-actions per state, from which reinforcement learning obtains an optimal policy in a small number of episodes. The policy learned to fly the aircraft is tested under different turbulence conditions and it is shown that it can be used to fly a completely differ-ent mission including new manoeuvres not presented in the original traces without any further learning. Section 2 describes the characteristics of the flight sim-ulator. Section 3 reviews r einforcement learning us-ing a relational representa tion. Section 4 describes the behavioural cloning approach used in the exper-iments. Section 5 provides experimental results with the flight simulator. Sectio n 6 reviews some relevant related work. Finally, in section 7, conclusions and future research directions are given. A flight simulator based on a high fidelity model of a high performance aircraft (a Pilatus PC-9 acrobatic air plane) was used in our experiments. The PC-9 is an extremely fast and ma noeuvrable aircraft used for pilot training. The model, provided by the Aus-tralian Defense Science and Technology Organization (DSTO), is based on wind tunnel and in-flight perfor-mance data.
 The aircraft can be controlled with the ailerons, el-evators, throttle, flaps an d gear levers. The ailerons control the roll and yaw of the aircraft. The elevators control the pitch, the throttle controls the thrust of the plane, the flaps are used during landing and takeoff to increase lift and the gear is r etracted during the flight. Since the flight simulator is of an aerobatic aircraft, small changes in control can result in large deviations in the aircraft position and orientation. This paper only deals with controlling the ailerons and elevators, which are the two most difficult tasks to learn. In all the experiments, it was assumed that the aircraft was already in the air with a constant throttle, flat flaps and retracted gear. Turbulence was added during the learning process (both behavioural cloning and rein-forcement learning) as a random offset to the velocity components of the aircraf t, with a maximum displace-ment of 10 ft/s in the vertical d irection and 5 ft/s in the horizontal direction. 1 The flight simulator produces a symbolic output given as Prolog facts. It includes information about the po-sition of the aircraft, its velocity and orientation, roll, pitch and yaw rates, and the position of objects, such as buildings, that appear in the visual field. A flight is specified by a sequence of ways points. The aircraft is required to fly through each way point with a tol-erance of 100 ft vertically and horizontally. Thus the aircraft must fly through a  X  X indow X  centred at the way point. Trying to directly learn in the search space produced by the output information of the flight simulator is challenging for any reinforcement learning approach. Even with a very gross discretization of the space it is easy to produce millions of state-action pairs 2 .Also, once a policy has been learned to achieve a particu-lar goal or sets of goal, a new policy has to be learned again if the goal changes its location. What we want to learn is a single policy that can be used to fly from any initial state to any achievable goal position in space un-der different turbulence condi tions. In this research we use a relational representation to achieve this, where it is easy to encode the relative position of an agent with respect to a goal or to other objects in the en-vironment. The main idea i s to represent states as sets of properties that ca n be used to characterize a particular state and which may be common to other states.
 An r-state is a conjunction of first-order predicates . The extension of an r-state is the set of states that are covered by its description. Each state is an instance of one and only one r-state . In the flight simulator, these properties could represent the relative distance to the target, the relative orientation of the aircraft to the target, the current plane roll, etc. For exam-ple, an r-state ,suchas distance target ( State, close ) and orientation target ( State,left ), covers all the states where the current target is close to and to the left of the aircraft. In this paper it is assumed that the rele-vant relations to characterize the space are previously defined by the user.
 When learning a policy by reinforcement learning, we would like to only try those actions that are relevant to the current state. An r-action is defined by a set of pre-conditions and a generalized action . The pre-conditions consist of a conjunction of predicates that must hold for the r-action to be applicable. The gener-alized action represents all the instantiations of prim-itive actions that satisfy the conditions. When several primitive actions satisfy the conditions of an r-action , one of them is chosen randomly. An r-action can be applied to many states, and not all the r-actions apply to all the r-states .
 For example, the following r-action with id number 23 (first argument), for controlling the aileron (second argument) says, if the aircraft is near its goal, the goal is to the left, the aircraft is rolled to the right and the roll rate is increasing, then move the stick to the far left. Initialize Q ( S, A ) arbitrarily (where S is an r-state and A is an r-action ) Repeat (for each episode): A policy consistent with our representation, which we will refer to as an r-space policy (  X  R ), is a scheme for deciding which r-action to select when entering an r-state .An r-space optimal policy (  X   X  R ) is a policy that achieves the highest cumulative reward among all r-space policies.
 In the experiments reported below, we use Q-learning in r-space . However, other reinforcement learning al-gorithms may also be used. It is also easy to incor-porate eligibility traces and function approximation into our approach. Table 1 gives the pseudo-code for the rQ-learning algorithm. This is very similar to the Q-learning algorithm, except that the states and ac-tions are characterized by relations. The algorithm still takes primitive actions ( a  X  X ) and moves over prim-itive states ( s  X  X ), but learns over r-state -r-action pairs. This process is, in general, non Markovian, neverthe-less, it can be shown to converge to an optimal r-space policy, using Singh X  X  results (Singh et al., 1996) (see (Morales, 2003) for a more complete description). The aircraft is controlled by performing left-right and forward-backward movements on the stick. We de-cided to divide the task into two independent rein-forcement learning tasks: (i) forward-backward move-ments to control the elevation of the aircraft and (ii) left-right movements to control the roll and heading of the aircraft. We assume that in normal flight, the air-craft is approximately level so that the elevators have their greatest effect on elevation and the ailerons on roll.
 To characterize the states for elevation control the fol-lowing predicates and discretized values were defined:  X  distance goal : relative distance between the plane  X  elevation goal : difference between current eleva-For aileron control, in addition to distance goal ,the following predicates and dis cretized values were also defined:  X  orientation goal : relative difference between cur- X  plane rol : current inclination of the plane. Possi- X  plane rol trend : current trend in the inclination of The ranges of the discretized values were chosen arbi-trarily at the beginning of the experiments and defined consistently across different variables with no further tuning. The exact values appear not to be too rele-vant, but further tests are needed.
 The above definitions discretize our state space. In simple domains, a suitable set of r-actions to perform on each r-state is relatively easy to define by hand. However, in more complex domains this is much more difficult and it is desirable to learn the r-actions .In this paper the r-actions were learned from traces of human pilots using behavioural cloning. Behavioural cloning induces control rules from traces of skilled operators, e.g., (Sammut et al., 1992). The general idea is that if the human is capable of perform-ing a task, rather than asking him/her to explain how it is performed, he/she is asked to perform it. Machine learning is then used to produce a symbolic description of the skill.
 Flying an aircraft has been a benchmark problem in behavioural cloning. One of the main limitations of the original behavioural cloning approaches was that the clones produced were not robust to variations (Bratko et al., 1998). Several improvements have been made in recent years to produc e more robust clones, by adopting a hierarchical decomposition of the learning problem and adopting different machine learning tech-niques. Some of the differences between our research and previous work are that we learn under high turbu-lence conditions in a sophisticated flight model (as in (Isaac &amp; Sammut, 2003)). Our behavioural cloning ap-proach is incremental with an easy to interpret repre-sentation language and we use an exploration phase to provide robustness to the system. Furthermore, in this research, behavioural cloning is not used as a means to learn how to perform a control task, but as guidance for reinforcement learning by inducing a small subset of relevant actions per state.
 A very simple behavioural cloning approach is used to gather relevant r-actions .Fromalogofahuman flight, for each state description, the algorithm evalu-ates a set of predefined predicates, such as those given in the previous section, and observes the control ac-tion. It the control action is an instance of an already defined r-action that r-action is used. Otherwise a new r-action is created with the conjunction of the predefined predicates using the format described be-low. Thisschemecanalsobeusedinanincremental way, where new traces can b e incorporated at any time (possibly) increasing the current r-action set (see also (Morales, 1997) for a similar approach used in chess end-games).
 The actions were discretized as follows. The X com-ponent of the stick can have the following values: far-left (if stick X component value is less than -0.1), left (if it is between -0.1 and -0.03), nil (between -0.03 and 0.03), right (between 0.03 and 0.1), and farright (greater than 0.1). For the Y component of the stick movements the following discretization was used: far-down (above 0.4), down (between 0.3 and 0.4), nil (be-tween 0.2 and 0.3), up (between 0.1 and 0.2), and farup (below 0.1). These discretizations were based on the actions performed by human pilots.
 Elevation r-actions have the following format: where Num is an identification number, StickMove is one of the possible values for stick on the Y coordi-nate, DistGoal is one of the possible values for dis-tance goal ,and ElevGoal is one of the possible values for elevation goal . For elevation there can be 75 pos-sible r-actions (3 possible values for DistGoal ,5for ElevGoal ,and5for StickMovement ).
 Similarly, the format for the aileron r-actions is as follows: where there can be 1,125 possible aileron r-actions . A total of 222 r-actions (180 for aileron and 42 for elevation) were learned after 5 consecutive mission logs over the flight plan shown in figure 1. 4.1. Exploration Mode The trace logs were produced with high turbulence. Since the pilot does not always behave consistently, several conflicting actions may be produced for the same state description. As we are learning only from seen cases, there may be some states descriptions not covered by the r-actions but which may occur in other flight manoeuvres. To compensate for this, the learned r-actions were used to fly the aircraft to try to reach previously unseen situations. In cases where there were several applicable r-actions , one was chosen ran-domly. Whenever the aircraft reached a new state description (where there was no applicable r-action ), the system prompted the user for a suitable action, from which a new r-action was induced. Also the user was able to perform a different action in any state if he/she wished, even if there were some applicable r-actions . Exploration mode continued until almost no new r-actions were learned, which was after 20 consec-utive exploratory flights. In total, 407 r-actions were learned, 359 for aileron (out of 1,125 which is  X  32%) and 48 for elevation (out of 75, which is  X  64%). So although, we are still learning a substantial number of r-actions behavioural cloning helps us to learn only a subset of the possible r-actions (only one third) fo-cusing the search space and simplifying the subsequent reinforcement learning task (an average of 1.6 r-actions per aileron state and 3.2 per elevation state). The actual value of the stick position was assigned as the mid point of the intervals, except for the extreme ranges, as follows. For the X coordinate: farleft =  X  0 . 15, left =  X  0 . 05, nil =  X  0 . 01, right =0 . 05, and far-right =0 . 15. For the Y coordinate: fardown =0 . 45, down =0 . 35, nil =0 . 25, up =0 . 15, and farup =0 . 05. We have left for future work the use of continuous ac-tions Once the state has been abstracted and the r-actions induced, rQ-learning is used to learn a suitable policy to fly the aircraft. In all the experiments, the Q values were initialized to -1, =0 . 1 , X  =0 . 9 , X  =0 . 1, and  X  =0 . 9 (since we used eligibility traces). The experiments were performed on the 8 goals mission shown in figure 1. If the aircraft increases its distance to the current goal, after 20 time steps have elapsed from the previous goal, it is assumed that it has passed the goal and it changes to the next goal.
 The following experiments were performed: 1. Positive reinforcement (+20) was given only when 2. Same as (1) without turbulence. 3. Same as (1) but only with the r-actions learned 4. Same as (1) but we automatically generate all the 5. Same as (4) but we use the original traces to Figures 2 and 3 show the learning curves of the above experiments for aileron and elevation respectively. In particular, how many times the aircraft crosses suc-cessfully the eight goals with maximum turbulence (ev-ery 500 flights) for aileron and elevation control. We continued the experiments for 20,000 episodes without any clear improvements in any of the experiments after the first 3,000 episodes.
 As can be seen from the figures, without focusing the search with behavioural clo ning, reinforcement learn-ing is unable to learn an adequate strategy in a reason-able time. In complex environments, spurious actions
Stage 0/ 0/ 5/ 5/ 10/ 10/ Goal1 0 100 31 75 49 89 Goal2 100 100 16 41 26 46 Goal3 0 100 53 62 51 70 Goal4 0 0 23 35 27 46 Goal5 0 100 57 91 59 95 Goal6 0 0 16 33 24 47 Goal7 100 100 47 74 33 66 Goal8 0 100 35 58 45 61
Aver. 25 75.0 34.75 58.625 39.25 65.0 can very easily lead an agent to miss the goal. In this particular domain, going away from the current goal at some intermediate state can lead the agent into a situation where it is impossible to recover and reach the goal without first going away from the goal. The exploratory phase, where new r-actions were learned using random exploration, also proved to be useful as the initial traces substantially biased the learning pro-cess and limited its applicability.
 The policy learned in experiment 1 after 1,500 flights was able to fly the whole mission successfully. Its ro-bustness was tested under different turbulence condi-tions. Figure 1 shows a human trace of the mission and the trace followed by the learned policy. Table 2 shows the results, averaged over 100 trials, of flying the learned policy on the mission with different lev-els of turbulence. Two columns are shown per turbu-lence level, one with percentages passing the way point within 100 ft. (which was used in the reward scheme) and one with a 200 ft. The important point to note is that the aircraft can recover even if it misses one or more goals, and although it occasionally misses some of the goals, it gets quite close to them, as can be seen from figure 1.
 Table 3 shows the average performance of the learned policies with and without turbulence (experiment 1 after 1500 episodes and experiment 2 after 3,000 episodes) using the r-actions learned with our be-havioural cloning approach. It also compares the per-formance of a random selection of the r-actions learned with behavioural cloning and a random selection of all the possible r-actions .
 The learned policies were t hentestedonacompletely Turb./ Policy Policy Rand. Rand. all
Toler. w/turb. no/turb. r-actions r-actions 5/100 34.75 30.25 16.25 1.125 5/200 58.625 56.00 38.00 1.875 10/100 39.25 26.75 16.50 0.625 10/200 65.00 50.50 38.875 2.00 different mission, consisting of four way points. The intention was to try manoeuvres not previously seen before. The new mission included: a right turn 3 ,a sharper left climb turn of what it has previously seen before, another quick right turn, and a sharp descend-ing right turn.
 Figure 4 shows a human trace and the trace using the previously learned policy (experiment 1) on the new mission with the maximum level of turbulence. The learned policy of the previous mission is clearly able to fly the aircraft on a completely new mission. Ta-ble 4 shows the performance of the policy on this new mission with different turbulence levels averaged over 100 trials. Table 5 shows the average performance of the learned policies with and without turbulence us-ing the r-actions learned with our behavioural cloning approach on the new mission. It also compares the per-formance of a random selection of the r-actions learned with behavioural cloning and a random selection of all the possible r-actions . State aggregation clusters  X  X imilar X  states together and assigns them the same value, effectively reducing
Stage 0/ 0/ 5/ 5/ 10/ 10/ Goal1 0 100 66 99 74 100 Goal2 100 100 17 39 29 44 Goal3 100 100 38 70 46 70 Goal4 0 100 51 77 39 58 Aver. 50 100 43 71.25 47 68 Turb./ Policy Policy Rand. Rand. all
Toler. w/turb. no/turb. r-actions r-actions 5/100 43.00 9.25 17.25 1.75 5/200 71.25 29.75 44.75 5.25 10/100 47.00 12.75 18.00 0.75 10/200 68.00 21.25 46.75 3.75 the state space. Work on tile-coding, Kanerva cod-ing, and soft-state aggregation are some of the repre-sentatives of this approach. We also do state aggre-gation, but we use a relational representation, where it is easy to define powerful abstractions, to incorpo-rate domain knowledge, and the learned policies can be directly used to other similar domains without any further learning.
 Relational Reinforcement Learning (RRL) (D  X  zeroski et al., 2001) also uses a relational representation for states and actions, however the main focus has been on approximating value functions with a relational rep-resentation. We are not trying to approximate a value function with a relational representation but rather, use a relational representation to structure and ab-stract the search space and then approximate a value function over this abstracted space. This makes our learning task much simpler and allows us to re-use previously learned policies more effectively. Also, our combination with behavioural cloning simplifies the re-inforcement learning task a s it provides a bias towards useful actions.
 Traces from humans or possibly some available poli-cies have been used by other researchers to provide initial guidance to reinforcem ent learning (e.g., (Ryan, 1998; Smart &amp; Kaelbling, 2000; Driessens &amp; D  X  zeroski, 2002)). Our traces are used to learn a subset of ac-tions per state. We also tried seeding initial Q values with all the possible r-actions using the original human traces, but we did not observe any clear improvements. In (Ryan, 1998; Ryan, 2002), the user manually en-codes a set of teleo-reactiv e operators (or TOPs), de-scribed using a relational representation. These are used by a planning system to produce sequences of subgoals at a high abstraction level. In our case, we only provide a set of relations, and learn the r-actions from logs of human traces. When applied to the flight simulator, they reported very poor results: less than 30% of successful flights with a large proportion of crashes (46%) in their best performances.
 In (Ng et al., 2004) a reinforcement learning approach is decribed that learns how to fly an autonomous heli-copter. Data from human pilots is used to fit a model of the helicopter X  X  dynamics using locally weigthed re-gression and a reinforcement learning algorithm based on a Monte Carlo approach which pre-samples all the random numbers used in the stochastic simulation to find a good policy estimate. A careful selection of state variables and potential-based shaping rewards were used to learn how to hover and perform partic-ular maneuvers. Maneuvers need to be specified by a set of contiguous points to follow. The human per-formance is used quite differently in this work com-pared with ours. Ng and Bagnell use data from hu-man flights to constrict a model of the plant whereas, we use the data to constrai n reinforcement learning. Our symbolic state abstraction approach in conjunc-tion with behavioural cloning gives us a much simpler state-action space, although our actions are discrete. Also our maneuvers are specified just with positions of way points in space.
 In (Isaac &amp; Sammut, 2003) a robust behavioural cloning approach is described to learn how to fly. It is, to our knowledge, the only other work that uses a high fidelity flight model with different turbulence lev-els. The approach assumes that a flight should have a constant climb rate and turn rate to achieve a goal and divides the task into two parts: (i) given a goal position learn the value of the turn and climb rates for the air-craft to achieve the goal, and (ii) given a turn/climb-rate learn which actions to perform to achieve that particular turn/climb-rate. Both parts are learned us-ing model trees, and in particular, the second part induces PID controllers at the leaves. Similar to our approach, once their system learns to perform particu-lar manoeuvres, it can be used to fly different missions. Besides the obvious differences in the machine learn-ing approaches, one of their advantages is that they can produce smoother flights as they use regression models, whether our behaviour is more  X  X umpy X  due to the discretization used in the actions of the stick. However, to produce an adequate clone, they require an expensive wrapper to set up adequate parameters for the learning algorithms. Our research shows that reinforcement learning can be used in a complex con-trol task and achieve comparable performance to the state-of-the-art behavioural cloning approach. In this paper we have shown how reinforcement learn-ing can be used to solve a complex control problem. The strategy used is to define relations to abstract the search space, use logs of human traces to learn a subset of relevant actions, and use reinforcement learning over this abstracted and redu ced search space to learn an optimal policy. The learned policy is shown to perform reasonably well under different turbulence conditions and on different missions.
 There are two key elements that contributed to the success of this research: (i) a relational representa-tion to produce powerful abstractions and descriptions including the relative position of the agent and (ii) the use of a behavioural cloning approach with an ex-ploration phase to learn a subset of the possible ac-tions per state, substantially reducing the reinforce-ment learning task.
 There are several future res earch directions that can be followed. An obvious initial step is perform a more careful selection of the discretization ranges for the state variables. Another possibility is to incorpo-rate regression models into the r-actions to produce  X  X moother X  flights. We would like to explore how to learn new relations using an ILP algorithm.
 The authors would like to thank Andrew Isaac for his help and feedback during the development of this work. This work was developed while the first author was on sabbatical leave at UNSW and supported by a grant from Conacyt (Mexico) and the ARC Centre of Excel-lence for Autonomous Systems (CAS).
 Bratko, I., Urban  X  ci  X  c, T., &amp; Sammut, C. (1998). Be-havioural cloning: phenomena, results and prob-lems. automated systems based on human skill. IFAC Symposium . Berlin.
 Driessens, K., &amp; D  X  zeroski, S. (2002). Integrating ex-perimentation and guidance in relational reinforce-ment learning. Proc. of the nineteenth international conference on machine learning (pp. 115 X 122). Mor-gan Kaufmann.
 D  X  zeroski, S., Raedt, L. D., &amp; Driessens, K. (2001). Re-lational reinfor cement learning. Machine Learning , 43(2) , 5 X 52.
 Isaac, A., &amp; Sammut, C. (2003). Goal-directed learn-ing to fly. Proc. of the Twentieth International Con-ference on Machine Learning (pp. 258 X 265). AAAI Press.
 Morales, E. (1997). On learning how to play. In van den H. Herik and J. Uiterwijk (Eds.), Advances in computer chess 8 , 235 X 250. The Netherlands: Universiteit Maastricht.
 Morales, E. (2003). Scaling up reinforcement learn-ing with a relational representation. Proc. of the
Workshop on Adaptability in Multi-agent Systems (AORC-2003) (pp. 15 X 26).
 Ng, A. Y., Kim, H. J., Jordan, M. I., &amp; Sastry, S. (2004). Autonomous helicopter flight via reinforce-ment learning. Advances in Neural Information Pro-cessing Systems 16 . Cambridge, MA: MIT Press. Ryan, M. (1998). Rl-tops: An architecture for modu-larity and re-use in reinforcement learning. Proc. of the Fifteenth International Conference on Machine Learning (pp. 481 X 487). San Francisco: Morgan Kaufmann.
 Ryan, M. (2002). Using abstract models of behaviours to automatically generate re inforcement learning hi-erarchies. Proc. of the Nineteenth International Conference on Machine Learning (pp. 522 X 529). San Francisco: Morgan Kaufmann.
 Sammut, C., Hurst, S., Kedzier, D., &amp; Michie, D. (1992). Learning to fly. Proc. of the Ninth Inter-national Conference on Machine Learning (pp. 385 X  393). Morgan Kaufmann.
 Singh, S., Jaakkola, T., &amp; Jordan, M. (1996). Rein-forcement learning with soft state aggregation. Neu-ral Information Processing Systems 7 . Cambridge, MA: MIT Press.
 Smart, W., &amp; Kaelbling, L. (2000). Practical rein-forcement learning in continuous spaces. Proc. of the International Conference on Machine Learning
