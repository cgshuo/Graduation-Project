 In this paper we present a hybrid recommendation system that combines ontological knowledge w ith content-extracted linguistic information, derived from pre-trained lexical graphs, in order to produce high quality, personalized recommendations. In the described approach, such recommendations are exemplified in an advertising scenario. We propose a distributed system architecture that uses semantic knowledge, ba sed on terminologically enriched domain ontologies, to learn ont ological user profiles and consequently infer recommendati ons through fuzzy semantic reasoning. A real world user study demonstrates the recommendations with the aid of semantic profiles. H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  Distributed systems.
 Algorithms. Ad recommendation, ontology population, lexical graph, ontological user profile, fuzzy reasoning. Personalized recommendation systems aim to present users with online content, tailored to each user X  X  specific interests. The personalization of ad recommenda tion has received particular interest in recent years, since the internet business model relies heavily on advertising. Personalizing ad recommendation however is particularly challenging, since the scarcity and briefness of the text and metadata accompanying ads leave recommenders with insufficient criteria to adequately filter ads. Our choice for using ontological user profiles in the recommendation process aims to remedy the vocabulary impedance problem [1] as well as the cold-start problem in recommender systems [2]. The first problem is addressed by extracting semantic metadata from textual content and expressing them in a uniform, machine-understandable vocabulary, via domain reference ontologies. The pr ocess of classifying text to ontology concepts, formulated in semantic axioms, involves a novel combination of linguistic anal ysis through the use of lexical graphs. The latter problem is thus alleviated by the fundamental domain information provided in the reference ontologies. The effectiveness of this syst em depends on advanced textual analysis used to term inologically enrich th e semantic knowledge. We extend a previously implemented approach by [3], where graphs carrying information about interrelated domain terms were employed for content-based recommendation in order to ameliorate the vocabular y impedance problem. The use of explicit domain knowledge, represented by an ontology or taxonomy in order to improve recommendation accuracy and completeness has been explored in the past. The population techniques in this paper adopt the methodology proposed in [4] and [2], where concept vectors are constructed for each ontology concept by indexing a training set of web pages. In these approaches, the terms in the vectors are essentially a bag-of-words and there is no accounting for relations between them. The aforementioned techniques place the load directly on the server and refrain from utilizing much of the formal semantics offered via ontologies, possibly due to performance limitations. However, advanced inference capabilities through the use of reasoning over formal semantics have been explored in [5], with acceptable performance. Our approach extends this technique to fuzziness, enabling us to handle fuzzy annotation and user preference weights. This paper discusses techniqu es which combine semantic unobtrusively extracted from the cont ent a user consumes in order to match and rank advertisements based on the interest score in the semantic profile of the user. A typical usage scenario is as follows: the user consumes a content item (ad, article, annotated video, short text). The textual da ta of the item are analyzed in order to extract its semantic information based on domain ontologies, enriched with terms in an offline process. This information translates into a set of user preferences which are captured in the semantic user profile through an automated procedure. User preferences are then matched semantically to a set of supplied, automatically an notated ads to determine whether to recommend an ad as well as the degree of confidence that the ad is useful to the user. The match confidence degree is used to rank recommended ads to achieve more accurate recommendations. The framework X  X  architecture is depicted in Figure 1. Populating an ontology with statis tical contextual information is performed in an offline training process which involves the construction of at least one le xical graph per topic, based on linguistic analysis over a large set of web corpora relevant to the domain in question, and requires at least one reference ontology per domain. The approach does not require extensive ontologies or mapping every detail of the domain. For recommendation purposes, it suffices to identify the domain for the specific recommendation problem and model (or reprodu ce from existing ontologies) the basic information. The expressivity of the ontologies supported by our system rests within the DLP (Description Logic Programs) fragment as defined in [6]. An ontology O consists of a set of concepts C and roles R and includes axioms that comprise semantic rules by right of the ontology X  X  expressivity. Figure 2 illustrates a portion of an example ontology for the soccer domain. The lexical graph creation a nd update process follows the principles in [3], where the gra phs used within our system have the form of a network of c onnected words (terms) and are progressively built up through processing textual content found on web. The basic elements of this model, denoted by G , are the set of graph nodes (or vertices) V and the set of graph edges E , connecting pairs of nodes, which in short can be defined as G  X  {V,E} . Each node t k  X  V in the graph is thus connected to a set of neighborhood. After training the graph as depicted in [3], a set of tokens is derived. Each token, denoted as lemma, consists of the graph nodes, assigned with attributes su ch as the lemma X  X  part-of-speech (POS) tag, and statistical inform ation such as its term frequency ( tf ) and node degree ( deg ) (i.e. the number of neighbors). The edges of the graph carry the degree denoting the co-occurrence ( cooc ) between a pair of terms in the same sentence. The topic graphs serve as enriched dictionaries for each domain and enable us to define contextual relations between the concepts and the terms in the domain. These relations are used to classify text to ontology concepts. Hence, each ontology concept c terminologically classified to a lemma in the graph with the same string classification method, where possible. Each concept can then be enriched by a vector weighted terms, where each term t i  X  V is in the graph neighborhood of the term mapped to the concept c weight w i  X  E , represents the cooc of the neighboring terms to the term mapped to c i .
 However, matching absolute strings to graph nodes leads to a loss of valuable information, dispersed in the variations and synonyms of terms represented by different lemmas in the graph. Therefore, we employ a series of linguistic analyses to detect and bring together information on closely related or identical data. Wikipedia Named Entities Normalization (NEN) . This mechanism is used to group the neighborhoods of all variations of a named entity in the ontology under a single reference name. We accept the Wikipedia representation of a NE as the reference name for that entity and assume that all NEs in the ontology are expressed in or can be converted to such a reference name. For example, the terms  X  X an Utd X  and  X  X UFC X  are variations of the English soccer team Manchester United FC . If some or all of these terms are present in separate lemmas in the graph, it is essential that all the neighbors of these variations are merged under a common neighborhood in order to achieve richer and more consistent concept vectors. We attempt to retrieve all variations for a single NE from a token list extracted from a set of texts relevant to the NE. The texts are comprised by the top N results returned from querying the reference name to a search engine. Tokenization of the texts retrieves a set of terms (referred to as variation candidates ) that are then further filtered in order to discard lingui stically incompatible terms. The heuristic filter used compares the order of letters in the retrieved candidates against the order of letters in the wiki-reference name. For example, the token  X  X UFC X  contains the letters m,u,f and c in the same order as the reference name  X  M anchester U nited FC  X  and is therefore accepted, while  X  F eat u red C o m ic Strips X  contains letters found in the reference name but in the wrong order and is therefore rejected. Remaining candidates are then queried to Wikipedia. A candidate is finally accepted as a valid variation of the concept only if the query redirects to (a) a page whose header is the original reference name, or (b) a Wikipedia disambiguation page that contains a hyperlink to the original reference name in its body text. WordNet-based Synonym Detection. Similarly, we attempt to identify and merge neighborhoods of semantically identical nouns in the graph. For that purpose, all WordNet synonyms of a reference concept are retrieved and classified as variations of that term. The initial concept vector is updated w ith the terms in the neighborhoods of all the synonyms retrieved in the graph. Neighborhood Selection. All graph nodes representing variations (or the reference name) of a concept c i are assembled in a single vector c v , along with the terms in their joint neighborhood, as it was formed after the abovementioned analysis. A lower degree threshold is employed to dispense with the most common terms of the topic and prune them from the concept X  X  neighborhood. The lower deg threshold formula is given in (1). where L deg G is the lower deg threshold, avg (deg deg of all terms in the graph and stdDeviation(deg standard deviation of the deg all terms in the graph. Similarly, to avoid circumstantial co-occurrence between two terms, terms whose cooc is lower than the average neighborhood cooc of the examined concept X  X  neighborhood are discarded. Weighting. All terms in the concept vector are assigned a weight confidence degree with which the term describes the concept. All NE variations are assigned with the maximum degree, i.e. 1, of participation to the concept. All synonyms of a noun or adjective are assigned 0.9, to allow some uncertainty with respect to the sense of the particular synonym. All other neighborhood terms in the vector are assigned with the normalized cooc degree of the neighboring term with the concept X  X  name variation term. The normalized weight is calculated with formula (2). If a term appears in the neighborhood of two or more variations, the maximum of the normalized term degrees is retained. Where n is adjacent to the root lemma t , co-occurrence(t, n co-occurrence degree between t and n, and tf (t) is the tf of the root lemma t . The recommendation system uses the concept vectors derived from the previous step to discern the semantics of user-consumed content. The individual component s exploiting these semantics to result in a set of ranked recommendations are described below. The pre-trained concept vectors are used to classify ads and user consumed content to ontology concepts. A vector of terms is retained for each individual cont ent item or advertisement. Each term is assigned with a tf value analogous to its appearance to the text, which represents the participa tion weight of that term to the content item or ad. The classification process is based on a look-up scheme, where concept mappings for the extracted text terms are retrieved, i.e. each extracted term is looked-up with an inverted index scheme in all the concept vectors. The weight w i of the term t i and its tf in the text determine the weight of participation of the concept c i to the content. If the same concept emerges more than once in a single content item, the concluding weight acquired is the maximum from all occurrences of the concept in the content. After all concept vectors are examined, the set of retrieved concepts and their standing weights constitute the classification set Raw ads can be annotated semantically based on the aforementioned process. For each con cept in the classification set of the ad, a unique instance of the concept will be automatically generated, i.e. C c c atom processing determines whethe r some ontology property might quantify each concept, based on th e range and sub-domain of the property. Such properties are assigned with connecting ground values, thus relating a unique atom and the atom of the instance of User transactions with web conten t are tracked through a transaction listener. Refraining from exploring the non-trivial issue of tracking disinterests, we consider as positive transactions, hence interests, viewed content items and clicked advertisements. Upon consumption, the textual content is classified and the emerging concepts are added to the user profile, along with their classification weight. Concepts are incrementally appended, and/or eliminated, or are weight-modified in the profile. A semantic rule is created and n  X  hasInterest.Interest i  X  n disjunction of all preferences n in the user X  X  profile. Concepts are existentially quantified with general descriptor properties of the ontology (where applicable) based on the given property X  X  range, forming complex concepts of the type are the ones not assigned to a specific sub-domain. Preferences are depicted in the user profile in Profile Concepts which might consist of not only individual ontology concepts, but also persistent combinations of ontology concepts, that represent occurrences of atomic concepts with strong correlation. Each concept combination is expressed as a conjunction of the individual correlating concepts in the user profile, such as  X  k Concept combinations for each set of classi fied concepts are produced for every set of terms in a consumed item based on formula (3). Where N is the number of individual concepts participating in the combination and C total is the sum of all combinations C i=1,N. Empirical results have demonstrated that N=3 is sufficient to adequately represent plausible persistent combinations. Profile concepts are updated upon user transaction. The weights of the profile concepts are also decayed by a temporal factor, expressed in formula (4). Where  X  is the last recorded normalized weight of the profile concept, f is the concept X  X  frequency of appearance in the profile, t is the current timestamp, t last is the timestamp of the last transaction milliseconds). This weight is then normalized in [0,1] with respect to the user X  X  transaction total. A semantic reasoning service is employed to match complex semantic user profiles to extract ed ad annotation. The devised reasoner is an extension of the Pocket KRHyper [5] mobile reasoner. The reasoner, called f-PocketKRHyper 1 previous implementation to fuzziness, thus supporting management of annotation and user preferences X  uncertainty. Consequently, by appending user implicit information, formulated with formal semantics, in given a domain ontology, f-PocketKRHyper can decide whether a given ad item matches the user profile and to what degree. In addition, the reasoner supports weighted concepts, based on Str accia X  X  concept weight modifiers [7], thus the participation degree of the produced entailments can be controlled by the confidence weight with which the user preferences participate to the profile. For the implementation of the approach, a single ontology outlining the soccer domain was develope d for proof-of-concept purposes, represented by 547 concepts and 18 roles. A lexical graph has been trained for this topic from a manually collected dataset of web-crawled soccer articles and ads. A large corpus of additional web-crawled articles and ads were collected and used for test purposes. A user study was conducted by 38 users, not chosen with any specific constraint, in order to evaluate the inter-dependent components of the overall system 2 . A profile was learned per user after consumption of 20 soccer articles and a set of ranked ads was recommended at the end, both of which the users were asked to rate. Results have demonstrated an average success rate of 69.74% for the top 10 final recommendations and of 79.47% for the derived profile. 75.26% was the average sa tisfaction score for the system X  X  performance as a whole. The users were also asked to rate ads presented to them during article viewing. The ads were provided by the reasoner-based recommender ( RBR ) described herein and the content-based recommender ( CBR ) described in [3] while the users were unaware of the recommendation method. Results demonstrate that the RBR performs well from the beginning of the trials, achieving scores comparable to the straightforward CBR . Comparison of the two recommenders verifies that the RBR shows progressive and rather steady improvement as the user profile evolves and becomes more stable. The CBR  X  X  performance, on the other hand, fluctuates significantly since it depends on successfully identifying ads relevant to the text, while we notice that the accuracy gradually decreases as the users expect targeted recommendations. The work described in this paper has presented evidence that semantic knowledge in combination with the use of statistical terminological data captured in lexical graphs can be beneficial to the recommendation of content items expressed by text. Furthermore, it has confirmed the efficiency of employing formal semantics that capture the underlying substance of user preferences to achieve richer and more me aningful recommendations, even methodology described allows for recommendation of any kind of content item for which there are textual data and for any domain for which semantic knowledge is available. [1] Ribeiro-Neto B., Cristo M., Go lgher B.G., de Moura E.S. 2005. [2] Sieg A., Mobasher B., Burke R. 2007. Learning Ontology-Based [3] Papadopoulos S., Menemenis F., Ko mpatsiaris Y., Bratu B. 2009. [4] Trajkova J., Gauch S. 2003 . Improving Ontology-Based User [5] Kleemann T., Sinner A. 2005. User Profiles and Matchmaking on [6] Grosof B. N., Horrocks I.,. Volz R, Decker S. 2003. Description [7] Straccia, U. 2005. Towards a Fuzzy Description Logic for the 
