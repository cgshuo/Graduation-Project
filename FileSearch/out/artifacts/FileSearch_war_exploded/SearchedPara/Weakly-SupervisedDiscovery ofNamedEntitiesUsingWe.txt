 A seed-based framew ork for textual information extraction allo ws for weakly sup ervised extraction of named entities from anon ymized Web searc h queries. The extraction is guided by a small set of seed named entities, without any need for handcrafted extraction patterns or domain-sp eci c kno wledge, allo wing for the acquisition of named entities pertaining to various classes of interest to Web searc h users. Inheren tly noisy searc h queries are sho wn to be a highly valu-able, alb eit little explored, resource for Web-based named entity disco very.
 H.3.1 [ Information Storage and Retriev al ]: Con ten t Analysis and Indexing; I.2.7 [ Arti cial Intelligence ]: Nat-ural Language Pro cessing; I.2.6 [ Arti cial Intelligence ]: Learning; H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al Algorithms, Exp erimen tation Named entities, kno wledge acquisition, query logs, weakly sup ervised information extraction, unstructured text
Although the information in large textual collections suc h as the Web is available in the form of individual textual documen ts, the human kno wledge enco ded within the doc-umen ts can be seen as a hidden, implicit Web of classes of instances (e.g., named entities), interconnected by relations applying to those instances (e.g., facts). The accurate iden-ti cation of named entities pertaining to div erse classes is an essen tial step towards automatically constructing kno wledge bases from unstructured text [16], particularly in the case of the acquisition of large fact rep ositories from Web docu-men ts [2]. Besides their role in populating larger rep ositories of human kno wledge, named entities constitute a signi can t fraction of the queries submitted as part of an activit y that permeates mo dern societies and is undertak en by millions of people every day, namely Web searc h. A variet y of natural language pro cessing tasks, including parsing, prep ositional attac hmen t [1] and coreference resolution [9], also bene t from the availabilit y of information about named entities. Similarly , the performance of named entity recognizers im-pro ves when the systems have access to lists of accurate named entities [18], even with state of the art statistical rec-ognizers [19]. Named entity disco very is also a powerful tool in building new verticals in Web searc h semi-automatically , for example to impro ve or pro vide alternativ e views of searc h results for popular classes suc h as Drug , Vide oGame etc.
In a formal ackno wledgmen t of the imp ortance of named entities in Web searc h in particular, and text pro cessing in general, numerous previous studies address the task of ac-quiring clean lists of named entities occurring within un-structured text. Traditionally , the recognition of names and their asso ciated categories within unstructured text relies on man ually compiled seman tic lexicons and gazetteers. The amoun t of e ort required to assem ble large lexicons some-times con nes the recognition to a limited domain (e.g., medical imaging ) for whic h large-co verage resources already exist. It is also possible to build recognizers that iden tify names automatically in text [3, 4, 18], although the targeted named entities are usually limited to a small set of coarse-grained classes, e.g. Person or Location . Recen t work on large-scale information extraction holds much promise, as it scales well to Web-sized collections through an emphasis on ligh tweigh t extraction metho ds [2]. In particular, a few studies tak e adv antage of non-traditional documen t collec-tions for the purp ose of mining named entities, in the form of comparable news articles [17] or multilingual corp ora [6].
In a departure from previous work on named entity disco v-ery from unstructured text, this pap er introduces a weakly sup ervised metho d for mining Web search queries in order to explicitly extract named entities that are exp ected to be relev ant and suitable for later use. The metho d is inspired by recen t work on weakly sup ervised acquisition of class at-tributes from query logs [12]. In con trast, previous work that looks at query logs as a data resource does so only to implic-itly deriv e signals impro ving the qualit y of various tasks suc h as information retriev al, whether through re-ranking of the retriev ed documen ts [21], query expansion [5], or the dev el-opmen t of spelling correction mo dels [8]. Con versely , previ-ous studies in large-scale information extraction, including named entity disco very, uniformly choose to capitalize on documen t collections [10] rather than queries as preferred data source, thus failing to tak e adv antage of the collec-tive kno wledge embedded haphazardly within noisy searc h queries.

The prop osed extraction metho d is guided by a small set of seed instances, without any need for handcrafted extrac-tion patterns or further domain-sp eci c kno wledge. To en-sure varied exp erimen tation on sev eral dimensions despite the burden of man ually assessing the correctness of the out-put, the evaluation computes the precision of instances ex-tracted for ten di eren t target classes ( City , Drug etc.). The classes constitute a realistic sample of the wide range of domains of interest to Web searc h users. One of the pre-requisites of the evaluation, illustrating its scop e and time-intensiv e nature, is the man ual assessmen t of the correctness of more than 4,000 candidate instances. The resulting pre-cision scores are signi can t both in absolute value (0.96 for prec@50, 0.90 for prec@150, and 0.80 for prec@250), and relativ e to the qualit y of instances extracted from Web doc-umen ts based on handcrafted patterns (with precision im-pro ving by 29% for prec@50, 26% for prec@150, and 15% for prec@250).
Giv en a set of target classes and a set of seed instances for eac h class, the goal is to extract relev ant class instances from query logs, without any further domain kno wledge. As sho wn in Figure 1, the extraction metho d consists of ve stages: iden ti cation of query templates that matc h the seed instances (Step 1); iden ti cation of candidate instances (Step 2); internal represen tation of candidate instances (Step 3) and seed instances (Step 4); and instance ranking (Step 5).

A target class (e.g., Drug ), for whic h instances need to be extracted, is available in the form of a set of represen-tativ e instances (e.g., phentermine and vioxx ). In Step 1, eac h query that con tains a seed instance generates a query template comp osed of the remainder of the query , that is, the pre x and the post x around the matc hed instance. For example, the occurrence of the instance vioxx in the query \long term vioxx use" corresp onds to the query template [ long term ] prefix [ use ] postf ix . During matc hing, all string comparisons are case-insensitiv e.

In another pass over the query logs, Step 2 collects a very large (and noisy) pool of candidate instances, by iden tify-ing the queries (e.g., \long term xanax use" ) whic h matc h any of the query templates generated in the previous step, and collecting the query in xes (e.g., xanax ) as candidate instances.
Step 3 (see Figure 1) builds an internal represen tation of eac h candidate instance. Whenev er a query con tains a candidate instance collected in Step 2, the remainder of the query , that is, the concatenation of the remaining pre x and post x, becomes an entry in a query template vector that acts as a search signatur e of the candidate instance with resp ect to the class. The weigh t of eac h entry in a vector is the frequency of occurrence of the con tributing query in the query logs. For instance, the query \can lamictal be crushe d" results in a new entry being added to the searc h-signature vector of lamictal with resp ect to the class Drug . The new entry corresp onds to the query template [ can ] prefix entity disco very ( I stands for a class instance) [ be crushe d ] postf ix . The weigh t of the new entry is set to the frequency of the query .

Step 4 of Figure 1 injects weak sup ervision in the extrac-tion pro cess, through the small input set of per-class in-stances. For eac h class (e.g., Drug ), the vectors generated in Step 3 are insp ected to iden tify those asso ciated to a seed instance (e.g., xanax and vioxx ), rather than any other candi-date instance. The vectors thus selected are added together, forming a reference searc h-signature vector. Whereas Step 3 generates a vector for eac h candidate instance of eac h class, Step 4 pro duces one reference searc h-signature vector for eac h class. A reference vector can be though t of as a loose searc h ngerprin t of the desired output type with resp ect to the class [12].

In Step 5, the similarit y scores among the searc h-signature vector of eac h candidate instance, on one hand, and the reference searc h-signature vector of the class, on the other hand, determine the ordered list of candidate instances along with their ranking. The similarit y scores are computed based on a similarit y function, namely Jensen-Shannon, that is used frequen tly in text pro cessing [7].

One of the possible interpretations of the reference searc h-signature vectors of a class is to view them as a series of queries (or questions, when form ulated in natural language) that can be ask ed about the instances in the class. Table 1 sho ws some of the query templates in the reference searc h-signature vectors collected during named entity disco very for a sample of the target classes. Note how it is incremen tally easier to guess to whic h class an instance belongs, solely by insp ecting more questions that various people (users) ask about the instance. For example, if valid questions about an instance I include \what type of government does I have" and \what do people in I eat" , then it is relativ ely easy for people to guess that I must be a geop olitical entity of some sort, and later re ne that guess as they are presen ted with more questions about the instance. The intuition behind named entity disco very from query logs is similar: given a set of candidate phrases, the system must guess whic h of the candidate phrases are more likely to belong to the target class, by looking at queries that can be ask ed about the candidate phrase, and more generally about all instances in the target class.
The input to the exp erimen ts is a random sample of around 50 million unique, fully-anon ymized queries in English sub-mitted by Web users to the Google searc h engine in 2006. All queries are considered indep enden tly of one another, whether they were submitted by the same user or di eren t users, within the same or di eren t searc h sessions.
To better test the abilit y of the extraction metho d to dis-cover useful named entities, eac h target class is speci ed through only ve seed instances, whic h are case-insensitiv e: trix g ; street journal, washington post g ; pablo picasso, vincen t van gogh g ; univ ersit y of chicago, univ ersit y of pennsylv ania, univ ersit y of texas at austin g ; sup er mario bros., warcraft g .

The resulting set of ten target classes is a mix of coarse-grained classes ( Person and Location ) used hea vily in liter-ature on named entity disco very and extraction, and ner-grained classes (e.g., Drug and Vide oGame ) that more real-istically mo del the inheren t div ersit y of Web documen ts and queries. Furthermore, the classes di er from one another with resp ect to domains of interest (e.g., Health for Drug vs. Entertainmen t for Movie ). Overall, the target classes pro vide varied exp erimen tation on sev eral dimensions, while taking into accoun t the time intensiv e nature of man ual ac-curacy judgmen ts often required in the evaluation of infor-mation extraction systems [2].
Multiple lists of candidate instances (named entities) are evaluated for eac h class. The top 250 elemen ts of eac h output list to be evaluated are sorted alphab etically into a merged list. Eac h instance of the merged list is man ually assigned a correctness lab el with two possible values ( correct or incorrect ) within its resp ectiv e class. Thus, a correctness lab el is man ually assigned to a total of 4,468 instances ex-tracted for the ten target classes, in a pro cess that once again con rms that evaluation of information extraction metho ds can be quite time consuming. Table 3: Performance of named entity disco very from query logs (Q seed )
To compute the precision score over a rank ed list of ex-tracted instances, the correctness lab els are con verted to nu-meric values: correct to 1, and incorrect to 0. Precision at some rank N in the list is thus measured as the sum of the assigned values of the rst N candidate instances, divided by N .
Using only the ve seed instances per class and no further domain kno wledge, the extraction metho d prop osed in this pap er (denoted Q seed ) mines the query logs for other useful instances, some of whic h are sho wn in Table 2. As exp ected, not all extracted instances are correct; for example, europe is an incorrect extraction in the case of the class Country . In a more rigorous analysis, Table 3 captures precision num bers at various ranks for eac h of the target classes. The top 250 instances extracted for both Location and Person con tain no erroneous instances. For other classes, the accuracy of the extracted instances is quite good, at least at lower ranks (up to rank 50) regardless of the class. The variation in precision across di eren t classes is more pronounced further down in the rank ed lists of instances. In particular, the precision at rank 250 varies from a minim um of 0.54 for Newsp aper , to a maxim um of 1.00 for Location and Person , with a precision of 0.80 as an average over all target classes.

One may argue that the more queries con tain any of the seed instances of a given target class, the better the internal represen tation (that is, reference searc h-signature vector) of that target class would be. In turn, better internal repre-Figure 2: Correlation between the prec@250 score of a target class (see Table 3), and the corresp ond-ing num ber of all queries (top graph) and unique queries (bottom graph) con taining any of the ve seed instances of the target class sen tations could lead to more accurate scoring, and hence to higher-qualit y lists of extracted instances. In other words, the extracted instances are more likely to be accurate, if the seed instances that de ne the class occur more frequen tly in the query logs. Figure 2 illustrates the correlation between the precision score of an individual target class, and the total num ber of all queries (top graph) or unique queries (bottom graph) that con tain any of the ve seed instances of the class. The corresp onding correlation values between the precision of a class, on one hand, and the num ber of queries con taining some seed instance of the class, on the other hand, is -0.17 when computed over all queries, or -0.19 when computed over unique queries. Both values sho w that, in fact, preci-sion scores are not correlated with the popularit y of the seed instances in the query logs. For instance, in the top graph of Figure 2, a total of more than 21 million queries con tain seeds ( london , paris , san francisc o , tokyo or toronto ) of the class City , whose list of extracted instances has prec@250 of 0.75. Comparativ ely, a lower num ber of queries, that is, around 4 million, con tain seeds of the class Vide oGame , yet the prec@250 score for that class is 0.98.

At ranks 250 and higher, lower precision num bers for classes suc h as Country are not necessarily meaningful, since there cannot be that man y correct instances in those classes in the rst place. On the other hand, we feel that it is not unrealistic to exp ect precision levels in excess of 0.90 at rank 250 and even higher, for the extraction metho d to be truly useful in accurately collecting large instance sets, in the or-der of thousands or even tens of thousands of instances (e.g., for Vide oGame and Movie , not to men tion Person and Lo-cation ). With suc h an aggressiv e goal in mind, the curren t precision value of 0.80 at rank 250 is encouraging, but leaves room for impro vemen t.
A large body of previous work focuses on compiling lists of named entities exclusiv ely from documen t collections, either iterativ ely by starting from a few seed extraction rules [3], or by mining named entities from comparable news articles [17] or from multilingual corp ora [6]. In comparison, our pap er is the rst endea vor in named entity disco very from inheren tly-noisy Web searc h queries.

A set of extraction patterns relying on syn tactically parsed text (e.g., &lt; subj &gt; was kidnapp ed ) are acquired automati-cally from unstructured text in [14]. After man ual post-ltering, the patterns extract relations in the terrorism do-main (perp etrator, victim, target of a terrorist event) from a set of 100 annotated documen ts, with an average precision of 58%. A more sophisticated bootstrapping metho d [15] cautiously gro ws very small seed sets of ve items, to less than 300 items after 50 consecutiv e iterations, with a nal precision varying between 46% and 76% dep ending on the type of seman tic lexicon. By adding the ve best items ex-tracted from 1700 text documen ts to the seed set after eac h iteration, 1000 seman tic lexicon entries are collected after 200 iterations in [20], at precision between 4.5% and 82.9%, again as a function of the target seman tic type.

In [2], handcrafted extraction patterns are applied to a collection of 60 million Web documen ts to extract instances of the classes Comp any and Country . Based on the man ual evaluation of samples of extracted instances, the authors in-dicate that an estimated num ber of 1,116 instances of Com-pany are extracted at a precision score of 0.90. Although these num bers are indicativ e of the qualit y of previous meth-ods designed for Web documen ts, they are not directly com-parable to the scores from Table 3 for two reasons. First, the num bers rep orted in [2] are computed from a sample of larger lists of extracted named entities, whereas the preci-sion scores in Table 3 assume the systematic assessmen t of consecutiv e named entities from the extracted lists. Second, a detailed comparison of precision scores at various ranks cannot be obtained, unless the actual lists of named entities extracted from Web documen ts in previous studies are also already available, for eac h of the ten target classes. Nat-urally , this is not the case for [2], and in general for most Table 4: Man ual one-to-one mappings from target classes to textual class lab els collected from Web documen ts and deemed to be equiv alen t for the pur-pose of comparativ e evaluation previous work on named entity extraction from Web docu-men ts, since the authors rep ort aggregated results on small, non-o verlapping, pre-de ned sets of target classes.
In an e ort to comparativ ely assess the usefulness of query logs versus Web documen ts in the task of named entity dis-covery, the evaluation tak es adv antage of lists of instances as they were extracted in an earlier exp erimen t on the acquisi-tion of categorized named entities from Web documen ts [11]. As opp osed to other previous work, the set of target classes in [11] is not speci ed in adv ance. Instead, it is incremen tally acquired from Web documen ts along with the resp ectiv e in-stances, based on handcrafted extraction patterns (denoted D patt ). For simplicit y and similarly to previous studies on acquiring instances of arbitrary classes from textual docu-men ts [13, 2], the extraction patterns can be summarized as hC [suc h as j including] Ii , where I is a candidate instance and C is the corresp onding class lab el, as found in a docu-men t fragmen t that matc hes the patterns (e.g., \[..] drugs such as Vioxx [..]" ).

A large num ber of class lab els (e.g., drugs ) are thus eac h asso ciated with a rank ed list of candidate instances, in the lists extracted with D patt in [11]. As sho wn in the two left-most columns of Table 4, it is trivial to map a class lab el de-rived in D patt from Web documen ts, into exactly one target class deemed as lexically equiv alen t. Most mappings sim-ply involve plural forms (e.g., cities ) of the abstract target classes (e.g., City ), with two exceptions dictated by the avail-abilit y of a better mapping in the case of Person (mapp ed to people as a more natural alternativ e to the initial choice per-sons ) and Vide oGame (mapp ed logically to the phrase vide o games ). The righ t column in Table 4 indicates the rank of eac h class lab el C among all class lab els acquired in D patt , when the ranking criterion is the frequency of occurrence of the class lab el C across all documen t fragmen ts matc hing the extraction patterns. The values in Table 4 con rm that the chosen set of target classes pro vides varied exp erimen tation, with ranks varying from 1 for people (corresp onding to Per-son ) to 1532 for vide o games (corresp onding to Vide oGame ).
The instances extracted from Web documen ts (D patt ) are man ually evaluated as correct or incorrect , follo wing the same evaluation pro cedure used earlier for the instances acquired from query logs (Q seed ). The resulting precision scores exhibit a larger variation across target classes when compared to Q seed . At rank 50, the minim um and maxim um scores with D patt are 0.30 and 0.98 (for Food and Country all target classes (last graph) resp ectiv ely). In con trast, the scores at the same rank 50 vary between 0.86 and 1.00 in the case of Q seed , as sho wn earlier in Table 3.

Figure 3 plots precision values for ranks 1 through 250 for pattern-based extraction from Web documen ts (D patt ) against seed-based extraction from searc h queries (Q seed ). The rst ten graphs corresp ond to eac h of the ten target classes. The output of Q seed is better for most target classes, with the exception of the classes City and Newsp aper for whic h D patt gives better precision at higher ranks, and Coun-try for whic h D patt is better across all ranks. The last graph in Figure 3 sho ws the comparativ e precision as an average over all target classes. Overall, the Q seed metho d prop osed in this pap er outp erforms D patt , with relativ e precision score boosts of 29% (0.96 vs. 0.74) at rank 50, 26% (0.90 vs. 0.71) at rank 150, and 15% (0.80 vs. 0.69) at rank 250. Table 5 pro vides a more detailed view on the top items in the actual lists of instances extracted from Web documen ts and query logs resp ectiv ely.

The extracted instances sometimes capture condensed vari-ants that would otherwise be dicult to collect since they are rarely available in unstructured text, even within Web documen ts. For example, a varian t suc h as nfsmw , whic h is actually extracted from searc h queries, may be less formal than its equiv alen t Need for Speed: Most Wante d men tioned on the ocial Web site of the game dev elop er, but nfsmw is certainly quite popular among avid users searc hing for a Vide oGame .
Traditional wisdom suggests that textual documen ts tend to assert information (statemen ts or facts) about the world in the form of exp ository text. Comparativ ely, searc h queries can be though t of as being nothing more than noisy , keyw ord-based appro ximations of often-undersp eci ed user informa-tion needs (interrogations). Despite this apparen t disadv an-tage, and in a departure from previous approac hes to large-scale acquisition of named entities from the Web, this pap er illustrates the usefulness of a weakly-sup ervised extraction framew ork for mining named entities from query logs, rather than Web documen ts. Since the extracted instances relate to one another through various types of facts, they constitute a building blo ck towards acquiring large sets of relations con-necting named entities. Next steps include the acquisition of named entities for languages other than English and for target classes organized hierarc hically (e.g., Impr essionist , Painter , Artist and Person ), as well as the exploitation of Web documen ts in general and Web searc h results in par-ticular for additional extraction clues in conjunction with those deriv ed from query logs. [1] E. Brill and P. Resnik. A transformation-based [2] M. Cafarella, D. Downey , S. Soderland, and [3] M. Collins and Y. Singer. Unsup ervised mo dels for [4] S. Cucerzan and D. Yaro wsky . Language indep enden t [5] H. Cui, J. Wen, J. Nie, and W. Ma. Probabilistic [6] A. Klemen tiev and D. Roth. Weakly sup ervised [7] L. Lee. Measures of distributional similarit y. In [8] M. Li, M. Zhu, Y. Zhang, and M. Zhou. Exploring [9] K. McCarth y and W. Lehnert. Using decision trees for [10] R. Mo oney and R. Bunescu. Mining kno wledge from [11] M. Pasca. Acquisition of categorized named entities [12] M. Pasca. Organizing and searc hing the World Wide [13] P. Pantel and D. Ravichandran. Automatically [14] E. Rilo . Automatically generating extraction [15] E. Rilo and R. Jones. Learning dictionaries for [16] L. Schubert. Turing's dream and the kno wledge [17] Y. Shin yama and S. Sekine. Named entity disco very [18] M. Stev enson and R. Gaizausk as. Using corpus-deriv ed [19] P. Talukdar, T. Bran ts, M. Lib erman, and F. Pereira. [20] M. Thelen and E. Rilo . A bootstrapping metho d for [21] Z. Zhuang and S. Cucerzan. Re-ranking searc h results
