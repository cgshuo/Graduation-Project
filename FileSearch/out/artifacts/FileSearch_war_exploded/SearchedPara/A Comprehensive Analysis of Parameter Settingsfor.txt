 In the TREC Web Diversity track, novelty-biased cumula-tive gain (  X  -NDCG) is one of the official measures to as-sess retrieval performance of IR systems. The measure is characterised by a parameter,  X  , the effect of which has not been thoroughly investigated. We find that common settings of  X  , i.e.  X  =0.5, may prevent the measure from behaving as desired when evaluating result diversification. This is because it excessively penalises systems that cover many intents while it rewards those that redundantly cover only few intents. This issue is crucial since it highly influ-ences systems at top ranks. We revisit our previously pro-posed threshold, suggesting  X  be set on a query-basis. The intuitiveness of the measure is then studied by examining ac-tual rankings from TREC 09-10 Web track submissions. By varying  X  according to our query-based threshold, the dis-criminative power of  X  -NDCG is not harmed and in fact, our approach improves  X  -NDCG X  X  robustness. Experimental re-sults show that the threshold for  X  can turn the measure to be more intuitive than using its common settings.
 Categories and Subject Descriptors: H.3.3 [ Information Storage and Retrieval ] -Informa-tion Search and Retrieval General Terms: Measurement, Experimentation Keywords: Diversity, Evaluation Measure
The task of novelty and diversity has recently attracted increasing interest. Within this context, the TREC 09-11 Web Diversity tracks 1 aim to investigate and evaluate the performance of systems that can retrieve relevant documents while providing diversity of intents (sub-topics 2 ) within the search results. IR systems designed for such task should re-trieve documents covering a complete array of potentially relevant query-intents (i.e.  X  X rovide complete coverage for a query X , while  X  X voiding excessive redundancy X  3 ). By ad-hering to this policy, the likelihood of retrieving information relevant to each query-intent is maximised. We formalise two user models of diversity-based retrieval in order to com-pare the intuitiveness of evaluation measures. These user models are devised based upon common assumptions in in-formation retrieval theory [9] and in accordance with the diversity categories defined by Radlinski et al.[8], i.e. ex-trinsic and intrinsic diversity.

Within this context,  X  -NDCG [4] is one of the official mea-sures used at TREC 09-11. The measure is characterised by a parameter  X  , which represents user tolerance to redun-dant information; in practice, it determines the balance be-tween rewarding relevancy and intent coverage. In common IR evaluation contexts,  X  is set to an arbitrary value of 0.5 for evaluating and tuning diversification methods. Although other alternative diversity-based measures, including ERR-IA [2] and #D-measures [11], are available and recently used in TREC and NTCIR 4 , we here focus on a comprehensive analysis of  X  -NDCG. This is because, to the best of our knowledge, no thorough study has attempted to examine the effect of the parameter  X  on the intuitiveness and re-liability of  X  -NDCG. Furthermore, gaining a thorough un-derstanding of  X  -NDCG may be of use in the investigation and understanding of other measures, that may or may not resemble behaviours similar to that of  X  -NDCG.
Zhai et al.[14] proposed a set-based measure, called sub-topic recall ( s-recall ) for the evaluation context of novelty and diversity retrieval. The measure is defined as the per-centage of relevant sub-topics that are covered by documents retrieved up to rank r . S-recall explicitly evaluates the diver-sity of relevant information in terms of sub-topic coverage. However, the measure has three major drawbacks. Firstly, s-recall@ r does not distinguish between retrieving a relevant document at rank r  X  1 or at rank r  X  2. Whereas, users pre-fer relevant information to be retrieved as early as possible. Secondly, once a sub-topic has been covered, s-recall does not distinguish between subsequent retrievals of (documents covering) the same sub-topic. This issue generalises into the third drawback of s-recall: once total sub-topic coverage is achieved, s-recall does not further distinguish between re-trieving relevant or not-relevant documents.

IR measures that stem from models of user-browsing be-haviour, such as NDCG , RBP , and ERR [2], have been ex-tended to the evaluation context of novelty and diversity retrieval (i.e.  X  -NDCG [4], NRBP [5], and ERR-IA [2]). These measures are characterised by a similar gain function that models the documents X  utility; measures are however distinguishable because of the different discounting func-tions that progressively reduce utility with respect to rank positions or users effort [1].  X  -NDCG belongs to this family of measures and is an extension of NDCG to the new eval-uation context. Sakai and Song [12] argued that  X  -NDCG is counter-intuitive; however, they did not identify in which circumstances this is the case. While we agree with Sakai and Song, in this paper we take a step further in the anal-ysis of the measure. We in fact identify the circumstances where  X  -NDCG is counter-intuitive and explain why this is so. Then we analyse and validate a solution defining a safe threshold for a parameter,  X  , as developed in our previous work [6]. Furthermore, we show that the intuitiveness of  X  -NDCG is improved without resorting to developing a new measure, as opposed to [12].
We outline two different user models derived from differ-ent interpretations of the TREC diversity task X  X  guidelines. These models are theoretically grounded in the interpre-tations of relevance suggested by Robertson et al. [9] (see also [7, 15] for further details).
 User Model 1 (UM1)  X  a set of users and a single document. Consider a set of different users (e.g. u 1 ,...,u who submit the same ambiguous query but with different query-intents. This user model exploits extrinsic diversity [8], where diversity is considered as a means to deal with the uncertainty about the representation of a user X  X  informa-tion need. Each user aims to find only a single document relevant to their intent and judges as non-relevant the doc-uments that do not contain such intent. These users would consider a system effective if it retrieves at least one relevant document for their intent.
 User Model 2 (UM2)  X  a single user and a set of doc-uments. Consider a single user (e.g. u 5 ), who is interested in a complete coverage of intents and is reluctant to see doc-uments containing redundant (despite relevant) intents that have been seen before. However, this user still prefers doc-uments covering redundant relevant intents to non-relevant documents as the former documents are still considered more useful than the latter. In addition, the user expects to dis-cover all relevant intents after examining documents up to a cut-off rank 5 r . Thus the following chain of preferences holds for this user model:  X  X ntent coverage X  &gt;  X  X edundant relevant X  &gt;  X  X on-relevant X 
In other words, at rank r , this user deems a system that retrieves fewer relevant documents but covers all intents as more effective than a system that retrieves only relevant but redundant documents without providing a broad cover-age of all query intents. Additionally, given the same level of intent coverage at a specific rank r , this user prefers sys-tems that retrieve more relevant documents to systems that retrieve fewer relevant documents. UM2 falls into the cate-gory of intrinsic diversity [8], in which diversity is considered a property of information need. Under this interpretation, the user requires a set of different results, taken together, to fulfil their single well-defined information need. Note that most rank-based measures, such as  X  -NDCG and ERR-IA, are developed based on the notion of a single user and a set of documents. It is yet unclear whether such measures eval-uate rankings according to the information need of multiple users in UM1 too. The fact that Robertson et al. unveiled equivalences between the two interpretations of relevance UM1 and UM2 are based upon may suggest that measures of diversity for UM2 may be suited for UM1 as well.
The same chain of preferences can also be applied for users of UM1. For instance, consider the case of user u 1 : if an IR system retrieved all and only documents relevant to other users X  intents (e.g. d 2 , d 3 , d 4 ), user u 1 would consider such system ineffective because it did not return any document (i.e. d 1 ) relevant to his need. Table 1 shows an example of relevance judgements with respect to UM1 and UM2.
In [6], we considered the user models of section 3 and illus-trated a simple situation, where  X  -NDCG behaves counter-intuitively. We showed that  X  -NDCG with a common set-ting of  X  =0.5 evaluates a system, which retrieves redundant sub-topics, as more effective than another system, which re-trieves novel relevant sub-topics. To avoid such a scenario, a formal safe threshold is derived by defining boundaries on  X  so that a system returning novel relevant sub-topics is awarded with an higher  X  -NDCG than a system return-ing redundant sub-topics. As suggested by Leelanupab et. al. [6], the safe threshold (st) for  X  is defined as: Eq (1) is the necessary and sufficient condition that has to be satisfied if we expect  X  -NDCG to reward systems retriev-ing novel relevant sub-topics more than systems retrieving redundant sub-topics. The threshold of eq (1) is a function of the number of intents associated to a query. Queries that differ for the number of intents generate different values of the threshold. Therefore, if  X  is set to 0.5 regardless of the queries,  X  -NDCG may misjudge documents conveying novel information. This is crucial, in particular, when analysing high quality 6 ranking results @2, @3, etc., or when the re-dundancy difference of the rankings (  X  ) at lower positions is small. For queries containing 2 sub-topics this problem does not occur, as  X  =0.5 is greater than the safe threshold. For queries with more intents, values of  X  lower than the threshold violate the user preferences set by the user model derived from the TREC guidelines. Values of  X   X  st + ( being a very small positive number) are the minimum values that satisfy the user model. Finally, for  X  st , increasing importance is given to diversity at the expense of relevance.
To understand the behaviour of  X  -NDCG with respect to the different settings of  X  , we analyse the measure aiming to answer the following questions:
To answer these questions, we analyse document rankings at rank 10, and we focus on two cases: when  X  is set to 0.5, and when  X  is set to st +0 . 01 (and with  X  = 1). We use data from the TREC 2009 and 2010 Web Diversity tracks [3].
In section 5.2 we examine a real case scenario where com-mon settings of  X  provide a counter-intuitive systems rank-ing, while  X  = st +0 . 01 provides a systems ranking consistent with the user model of section 3. This observation is fur-ther generalised in section 5.3, where we consider Kendall X  X   X  rank correlation and AP correlation (  X  ap ) [13] between the system rankings obtained by the two different settings of  X  .
In particular, while  X  treats discrepancies among systems to have equal impact regardless of their positions in the sys-tems rankings,  X  ap is an asymmetrical and top-heavy coef-ficient, giving thus more weight to ranking differences oc-curring at top ranks. This analysis suggests that systems rankings formed with  X  = 0 . 5 are different from those ob-tained with  X  = st + 0 . 01: specifically, differences are found in the top positions of the rankings.

Finally, in section 5.4 we study the discriminative power of  X  -NDCG under the two settings of  X  , so as to assess whether the intuitiveness and the reliability of the measure are degraded when following our proposal.
Table 2 presents the document rankings and the corre-sponding relevant sub-topics of the top six systems for query 35 of TREC 2009. The top and bottom rows of the table report the order of the systems according to  X  -NDCG@10 (whose value is reported in brackets) with  X  = 0 . 5 and  X  = st + 0 . 01 (with st = 0 . 8), respectively. Note that when
TREC 2009 s-r  X  =0.5  X &gt; st
TREC 2010 s-r  X  =0.5  X &gt; st  X  = 0 . 5,  X  -NDCG@10 suggests that  X  X ogTrDPCQcdB X  is the best performing system as all retrieved documents are relevant. However, these documents cover only 2 of the 6 sub-topics. This ranking is highly effective in retrieval tasks such as ad-hoc retrieval, where there is no notion of sub-topics. However, with the TREC guidelines for the Web Di-versity task indicating that systems should provide complete sub-topics coverage, while avoiding excessive redundancy, the  X  X ogTrDPCQcdB X  run is far from being highly effective, as it does not provide a broad coverage of the sub-topics, i.e. it does not diversify the document ranking. On the contrary, the runs identified as  X  X wgym X ,  X  X udvimp X , and  X  X SDiv2 X  should be ranked higher than  X  X ogTrDPCQcdB X , as they cover more sub-topics (i.e. 4, 4, and 5) although retrieving some non-relevant documents. This is because in the diver-sity retrieval task relevance is not the only evaluation crite-ria: rankings should also address different sub-topics. While systems as  X  X wgym X  retrieve less relevant documents than  X  X ogTrDPCQcdB X , they provide a broader coverage of the query X  X  sub-topics. Whereas,  X  X ogTrDPCQcdB X  provides a very relevant ranking, but not at all diverse .

When  X  is set as st + 0 . 01,  X  -NDCG provides a system or-der in line with the TREC Web Diversity guidelines. In fact,  X  X wgym X ,  X  X udvimp X , and  X  X SDiv2 X  obtain higher scores than  X  X ogTrDPCQcdB X . In particular,  X  X wgym X  is assessed as being the best system for query 35 as it retrieves at rank one a document that covers more sub-topics than that re-trieved by  X  X udvimp X  and  X  X SDiv2 X . Similarly  X  X SDiv2 X  is ranked lower than  X  X udvimp X , although it covers 5 sub-topics against the 4 covered by  X  X udvimp X , because the lat-ter system achieves high sub-topic coverage at earlier ranks
In this section we study the correlations between the sys-tems rankings obtained when employing different settings of  X  -NDCG and when compared with s-recall. Sakai and Song [12] reported a similar analysis based on correlations between  X  -NDCG (with  X  = 0 . 5) and s-recall. These how-ever were computed using systems rankings obtained aver-aging the performances over all the queries in the dataset. Here instead, we consider systems rankings generated by the different measures on a query-by-query basis, averaging af-terwards their correlations over all the queries in the dataset.
Table 3 reports  X  and  X  ap between systems ranking ob-tained by  X  -NDCG with  X  = 0 . 5 and  X  &gt; st , and s-recall. The rank correlation analysis reveals that there are differ-ences (despite small) between the systems rankings, and on average  X  =0.951 and  X  ap =0.918 for TREC 2009, and  X  =0.933 and  X  ap =0.899 for TREC 2010. While these val-#1 3 6 3 3 6 3 6 3 6 2 #2 3 6 1 4 3 6 #3 3 6 3 6 3 6 3 6 #4 3 6 3 1 3 3 6 #5 3 6 2 3 6 #6 3 6 1 3 6 #7 3 6 1 5 3 6 #8 3 6 3 6 1 4 #9 3 6 3 6 #10 3 6 3 6 ues may suggest that the systems rankings are very simi-lar, a further analysis reveals that the systems at the top of the ranking vary considerably when considering  X  = 0 . 5 or  X  &gt; st . This is evident when examining the results re-ported in table 4. In fact, when only the top 10 systems are considered, the two systems rankings are only weakly correlated (for both  X  and  X  ap and in both TREC tracks). Correlations increase as the number of considered systems increases: this is often due to poorly performing systems that do not retrieve relevant documents and for which no difference in evaluation is found between the two  X  -NDCG settings.

We also analysed the correlations between s-r@10 and the two settings of  X  -NDCG@10: these are reported in ta-ble 3. Both  X  and  X  ap of rankings obtained using  X  &gt; st are higher than those obtained with  X  = 0 . 5, suggesting that our method delivers systems rankings that are more adherent to those obtained with s-recall. While this does not guarantee specific advantages, it witnesses that more weight is given to sub-topic coverage, and by reflection to diversity, when considering  X  &gt; st rather than the common setting.
Next, we verify that varying  X  according to our proposal does not decrease the reliability of  X  -NDCG in terms of dis-criminative power . To this aim, we use the method intro-duced by Sakai [10] and based on the bootstrap hypothesis test. The method consists in conducting a statistical sig-nificance test for different pairs of experimental runs. In particular, it computes the percentage of pairs that are sig-nificantly different at specific fixed significance levels. In our experiments, we use all the systems submitted to the TREC 2009 and 2010 Web Diversity Tracks to generate pairs. Thus we obtained 48*(48-1)/2=1,128 pairs for TREC 2009 and 32*(32-1)/2=496 pairs for TREC 2010. As query set, we only considered queries that contains 3 or more sub-topics. As significance test, we employed a two-tailed paired boot-strap test with 1,000 samples and a fixed significant level of 0.05. The bootstrap samples were obtained by sampling queries with replacement.
 In our experiment, we found that the two settings of  X  -NDCG produce sightly different levels of discriminative power. For example, in TREC 2009 the discriminative power of  X  -NDCG with  X  = 0 . 5 is 60.72% while that with  X  &gt; st is 61.08%. The small difference between the discriminative powers obtained by the two settings is not surprising, as our results are based on a comparison of the same measure. However, it can be noticed that setting  X  according to the safe threshold does not harm the discriminative power of  X  -NDCG. Instead, it slightly improves its reliability.
Since in the previous sections it has been observed that systems rankings generated by the different settings of  X  differ within the top positions, we further analyse the differ-ences in terms of discriminative power by considering only the top 20 systems wrt. combined precision 8 and s-recall. This produced 20*(20-1)/2=190 pairs to be examined.
Figure 1 illustrates the Achieved Significance Level (ASL) curves of  X  -NDCG with the two setting of  X  for TREC 09-10. The horizontal axis represents the 190 run-pairs sorted in decreasing order of ASL. The vertical axis represents the ASL (i.e. p -value). When considering ASL plots, metrics whose curves are closer to the origin are considered having more discriminative power than the others, i.e. they can detect more significant differences. By examining the plots of Figure 1, it can be stated that  X  -NDCG with  X  &gt; st is able to discriminate more consistently than  X  -NDCG with  X  = 0 . 5. This finding is valid for both TREC 09-10.
Table 5 reports, for two settings of  X  -NDCG, how many pairs of TREC systems satisfied the condition ASL &lt; 0 . 05. Table 5: Discriminative power of traditional metrics at The second column reports the discriminative power; while, the third reports the estimated difference required for sat-isfying the condition ASL &lt; 0 . 05. For TREC 2010, the discriminative power of  X  -NDCG with  X  &gt; st at 0.05 level is 29.47%. If the difference between two systems is 0.09 or larger, then the performances of the two systems are signif-icantly different [10]. The comparison between the two set-ting of  X  across TREC 2009 and 2010 shows that setting  X  on a query-by-query basis increases the discriminative power of  X  -NDCG with respect to the top performing systems.
In this paper we have investigated a state-of-the-art evalu-ation measure,  X  -NDCG, for the TREC Web Diversity task. In this context, we reported the following findings: 1.  X  is not only a user dependent parameter, but it also 2. Common settings of  X  (i.e.  X  = 0 . 5) prevent  X  -NDCG 3. This issue affects many topics in the TREC 2009 and 4. A formal threshold for  X  can be derived so as to set  X  5. Empirical evidences suggest that by setting  X  accord-Acknowledgments : This research is partially supported by the EU funded project LiMoSINe (288024).
