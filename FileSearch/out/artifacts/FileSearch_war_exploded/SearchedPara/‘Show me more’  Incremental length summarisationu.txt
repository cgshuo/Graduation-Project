 1. Introduction
As device technologies improve and advance, so too does the services that they provide. Combined with the wealth of electronic information currently available, additional digital services add to the problem often referred to as information overload. Frequently associated with the information consumer, information over-load describes the effects of having more information available than can be readily assimilated. It is not only the quantity of information items that contribute to this problem, but also the way in which information is presented.
The design and presentation of content is of particular importance when accessing information in a non-traditional setting, a case being, mobile information access. Here, to meet the demands of anytime, anywhere information access, information has to be delivered in a form that can be readily and easily digested whilst on the move. Further, additional considerations are needed to account for the inherent interaction constraints of mobile devices.

Automatic summarisation can be employed to condense a textual document, presenting only the important parts of a full text thereby reducing the need to refer to the source document. Therefore, at a document level, summarisation may be considered as a means of reducing overheads in digesting information. Traditionally, summaries can be classified as, those that are indicative of the content of the source document, and summaries that are informative, providing information contained in the document ( Brandow, Mitze, &amp; Rau, 1995 ). A summary can also be described according to its orientation, being document-based, containing generic infor-mation from the author X  X  perspective, or query-based, containing content tailored to a particular user X  X  inter-ests. Other dimensions of summarisation have been highlighted in Mani (1999) , but are not relevant to the work reported here.

The intended use, and consequent type of summarisation employed is an important characteristic, another is the length of the summary. Summary length is particularly important for mobile information access, given restrictions in screen displays and the associated navigational costs of scrolling vertically, or  X  X aging X  to view content. Vertical scrolling describes the action of viewing content in a progressive manner, serially. By con-trast, paging permits access to the next full screen worth of content without any further action by the user.
In terms of an optimal summary size, according to findings of our previous work ( Sweeney &amp; Crestani, 2006 ), it would appear that short summaries (7% of the document length) perform well for a range of display screen sizes.

Aside from summary length, another factor that could improve the effectiveness of summaries, particularly in the task of identifying relevant items with respect to an information need, is the novelty of information. In this paper we consider summarisation with novelty detection, where information is not only condensed but also an attempt is made to remove redundant information. Whilst the combination of summarisation paired with novelty detection is not a new concept ( Carbonell &amp; Goldstein, 1998 ), we concern ourselves with the mechanism of delivering the information. Our focus is the notion of  X  X how me more X , where given interest in a topic, or theme of a document a reader wishes to satisfy further interests in that document. To satisfy the request to  X  X how me more X  we generate and deliver novel summaries, with the aim to provide additional novel information. However, there may be a negative effect by concentrating on only novel information, as there is a greater potential to misrepresent the content of document if essential contextual information is removed from summaries. With this in mind, we investigate whether summaries that contain novel sentences alone provide sufficient basis to determine relevance of a document, or if we need to include additional sen-tences in the summaries to provide context.

The framework we adopt to investigate novelty detection in summarisation is a user study. Our, objec-tive therefore, is to evaluate whether or not successful novelty detection methods at sentence level are use-ful for the purpose of fulfilling a practical information seeking task, as reflecting in the tasks we set our users.

The remainder of the paper is structured as follows. Section 2 describes our use of  X  X how me more X  as a framework for displaying summaries and briefly outlines the motivations of our work. Section 3 expands the motivations, and presents the research questions that the paper sets out to investigate. Section 4 outlines existing work in novelty detection and how it can be combined with summarisation. Section 5 describes the methods used to generate our query-biased novel summaries. In Section 6 , we present the details of the user study we carried out to evaluate query-biased novel summaries. And in Section 7 , we present the results and analysis of the experiments. Finally, Section 8 concludes the paper with a short discussion of the implications of our findings and indicates directions for future work. 2.  X  X how me more X 
The strategy we adopt to deliver information can be characterised as fulfilling the request to  X  X how me more X . In particular, we focus on the situation where, given interest in a topic, or theme of a document a reader wishes to satisfy further interests in that document. An example is a news brief that provides details of a capturing news headline, or the body text of an article that expands the details of a news brief. The  X  X how me more X  paradigm then assumes a willingness to invest effort in viewing additional iterations of content to get more information. In our case  X  X how me more X  describes the delivery of additional summa-ries. In the example of the news article, while the body text provides more details of the news contained in the brief, there will undoubtedly be overlapping, and possibly repetition of information contained in the brief. By adopting a strategy to detect novelty in the generation of summaries, we aim to reduce the amount of redundant information contained in subsequent summaries. In previous studies, we have inves-tigated the use of hierarchical query-biased summarisation using summaries of increasing length on a mobile phone ( Sweeney, Crestani, &amp; Tombros, 2002 ) and a PDA ( Sweeney &amp; Crestani, 2003 ). In those studies our assumption of  X  X how me more X  was simply as providing a summary of increasing length. By contrast, in the work we report here  X  X ore X  is taken to be not just a function of summary length, the size of the summary, but also the information content. This then can be considered a more intuitive approach, where  X  X ore X  (the next summary to be shown) will not only be query-biased (presenting those sentences that are relevant to the query) but also contain only novel information with respect to previously seen content.

If we consider the full text of a document consists of 3 types of sentences: (i) relevant sentences, (ii) novel sentences and the (iii) remaining sentences. A summary based on relevance will have sentences that contain content relevant to an information need. Within a summary based on relevance there may be redundant information since sentences appearing later in the summary may repeat earlier concepts. In con-trast, a summary based on novelty will contain only sentences that are both relevant and novel. However, a possible shortcoming of a summarisation strategy that focuses on presenting only novel information is the potential for a loss of context. In this sense, we refer to context as the background, or more specifically the information digested from previously seen content, which may have bearing on the correct interpretation given the source document. This constitutes the basis of the research questions that the work presented in this paper sets out to investigate. These research questions will be explained in more detail in the next section. 3. Relevance, novelty and context: research questions
To assess the notion of  X  X how me more X  and any potential for a loss of context in novel summaries, we adopt two strategies to produce summaries that incorporate novelty in different ways; an incremental length summary, and a constant length summary. Constant length summaries contain only novel sentences, whereas the incremental length summaries contain additional sentences that provide context. To evaluate the perfor-mance of both strategies we carried out a set of user experiments. In the experiments we measure users X  per-ception of relevance of displayed documents, in the form of the automatically generated summaries, in response to a simulated submitted query. We measure performance as users X  ability to correctly identify rel-evant documents. The aim is to study experimentally how users X  perception of relevance varies depending on the type of summary used. This should permit us to determine if summaries that contain novel sentences alone provide sufficient basis to determine relevance, or if we need to include additional sentences in the summaries to provide context.

Our aim can be characterised by the following research questions. Given the task we set our users: 1. Do query-biased summaries that take account of novelty perform better than those without novelty? 2. Do query-biased summaries that have a constant length, containing only novel sentences, perform better than those with an increasing length, where the additional sentences provide context? 3. Finally, which of the summary configurations achieve the highest level of performance?
Answering the above questions will allow us to fulfil our underlying overall objective, which is to determine if there is an optimal strategy for showing summaries to users in response to the request to  X  X how me more X .
 4. Background and related work 4.1. Novelty detection and summarisation
A large proportion of work in novelty detection has been carried out in topic detection and tracking (TDT) ( Allan, 2002 ). In the domain of news, TDT refers to the detection of breaking news in the form of new event, or first story detection and tracking the reappearance and evolution of these stories from a news stream ( Allan,
Carbonell, Doddington, Yamron, &amp; Yang, 1998 ). Since the application of TDT to news is concerned with event-based novelty detection, the emphasis then is on detecting overlaps in event coverage in news stories, and to identify whether two news stories cover the same event. It is often the case that many of the techniques applied in TDT to detect events make use of temporal clues and other features that are particular to the struc-ture of stories in news reporting.
 Another area where novelty detection research has been actively pursued is at the Novelty tracks of the
Text REtrieval Conferences X 02 X 04 (TREC). 1 In contrast to TDT, the novelty track is concerned with topic-based novelty detection. Here, the focus is novelty detection at a sentence level where the importance is not only on finding whether two sentences discuss the same topic, but also identifying where there is new infor-mation on the topic. Track participants are required to build a ranked list of novel relevant sentences, which consists of a two part process: (i) identify relevant sentences from a set of retrieved documents for a topic; and (ii) using the list of relevant sentences, identify those that contain new information. It is implicitly assumed that the process of topic learning happens within the task, and effects of prior knowledge are ignored.
Techniques that have been demonstrated at the Novelty track include those that are word-based and those that make use of other textual features. Using TREC X 02 data, UMass experimented with a range of techniques from a simple count of new words to more complex approaches that use language models and Kullback-Lei-et al., 2003 ) found that simple word counting methods (e.g. NewWords ) performed no worse than other tested techniques to detect novelty at a sentence level; indeed performed best in the case where non-relevant sentences were present. More recent approaches have investigated features in sentences, such as various types of patterns of word combinations ranging from named entities and phrases, to other natural language structures.
A successful recent technique used at the track detected focus discourse in combination with new words counts ( Schiffman &amp; McKeown, 2005 ). This additional evidence aims to improve performance particularly in regards to achieving high precision with high compression rates, a key goal in summarisation. Another approach makes use of query-related patterns to detect expected answer types ( Li &amp; Croft, 2005 ). For this approach, the task of recognising novelty is interpreted as new answers to potential questions posed in a query that expresses a users X  information need. Similar to techniques applied in the open-domain automatic Ques-tion Answering 2 (QA) an initial stage is required to transform the query into a question to establish the expected answer type(s). The aim then is to dramatically reduce the set of relevant items by removing those that do not match the expected answer type. Novelty is further boosted by accepting only those sentences con-taining answers that have not already been seen.
 Other research in novelty detection at a topic-level is in adaptive information filtering ( Zhang, Callan, &amp;
Minka, 2002 ) where document streams are monitored to find documents that match changing information needs specified by user profiles.

In terms of summarisation paired with novelty detection, early work combining query-relevance and infor-mation-novelty was in Carbonell and Goldstein (1998) . Here, maximal marginal relevance (MMR) was used to reduce redundancy while maintaining query-relevance in re-ranking retrieved documents and in selecting appropriate passages for text summarisation. However, for the work reported in this paper, we approach nov-elty detection in a slightly different way. Rather than treat each sentence independently and assess novelty at a sentence level, we instead apply novelty detection at a summary level, with respect to previously seen summa-ries. In this way we provide the most relevant important parts of the document in response to the query first, and for any subsequent requests for more content, we present only novel information with respect to what has been already seen.
 In relation to other work in novelty detection, we make use of a similar approach to NewWords , used by
Allan et al. (2003) , as our first account of detecting novelty in generating summaries. We justify this decision based on its simplicity to implement, and on the basis that this approach performed no worse then more com-plex methods in situations that more accurately reflect use in a real environment, which is applicable in our work. The state of the art in novelty detection at the sentence level is actually quite simplistic. Whilst one might expect that an elaborated model should be able to capture the different subtopics of the text and, then, produce the novel sentences accordingly, the simpler  X  X  X ag-of-sentences model of text X  X  works as good as any other more evolved method ( Allan et al., 2003 ). In the future we might expect that more evolved methods (e.g. based on subtopic structure in a text) can reach reasonable performance, and hence, out-perform simple word-based methods. Nevertheless, as demonstrated by Allan et al. (2003) this was not the case in the reported experiments. 4.2. Information access in non-traditional environments
The experience of accessing information in non-traditional computing environments is very different from that in a conventional setting, using a desktop PC. Comparing information access in a mobile environment to the conventional setting there are substantial differences ( Loudon, Sacher, &amp; Kew, 2002 ). Aside from inherent device constraints there are additional factors, such as, user multi-tasking, carrying out a number of tasks con-currently; increased potential for distractions from outside factors, such as, noise and interruptions; and the need to fulfil users X  requests in a timely manner given increased temporal and/or locational dependencies of a transient environment. There exists a large volume of research into provisions and support for accessing infor-mation on mobile phones, PDAs, mobile communicators (telephone/PDA) and Pocket PCs. For the purposes of this paper, we restrict the scope of our review to consider devices that have display screens and can present textual information. However, we recognise that there are many other effective modes to communicate infor-mation on a mobile devices (e.g. interaction via aural interfaces).

Research investigating the effects of using small screen devices on search task performance have found that increased scrolling due to limited display size can lead to an increase in cognitive load, and impeded search task performance ( Jones, Marsden, Mohd-Nasir, &amp; Boone, 1999b ). In particular, the negative effects associ-ated with horizontal scrolling to view content outside the screen display area. When required to scroll hori-zontally, users reported experiencing disorientation within the information space. In another study traversing pages led to an increase in cognitive load. Other studies have found that increased within page nav-igation also has the effect of increasing task completion times ( Jones et al., 1999b, 2001, Jones, Buchanan, &amp; Thimbleby, 2003 ). However, this may not translate to an increase in task error rates ( Kim &amp; Albers, 2001 ).
These studies illustrate the issues encountered when accessing information on small screen devices. They sug-gest that many of problems can be associated with the within page navigation in order to view content. The studies also provide evidence that content for small screen devices should undergo some form of processing to reduce effects due to inherent device constraints.

A variety of strategies have been investigated to make content more device-friendly when viewed on the small screen. Techniques range from the manual creation of device-specific content, to automated re-authoring approaches that apply transcoding, or transformation strategies. For a discussion of a range of adaption tech-niques for the small screen refer to ( MacKay &amp; Watters, 2003 ). Many of the approaches that exist concentrate on presenting web pages. However, other uses include access to digital library services ( Buchanan, Jones, &amp; Marsden, 2002 ) and support for email processing/viewing ( Corston-Oliver, 2001 ).

Among the first to directly addresses the need of automatic tools for layout adaptation is the Digestor pro-ject ( Bickmore &amp; Schilit, 1997 ). Classifying automatic adaptation techniques into two categories: syntactic techniques, based on the structure of a web page, and semantic techniques, accounting for the content of a web page, the authors describe a number of alternatives to automatically adapt the content of web pages.
One of the methods described relies on a mechanism for text outlining, which supports linking to paragraphs of text within a document, and is aimed to permit quicker access to content in small devices. More recently
WebTwig ( Jones, Buchanan, &amp; Mohd-Nasir, 1999a ) and PowerBrowser ( Buyukkokten, Garcia-Molina, Pae-pcke, &amp; Winograd, 2000 ) have adopted a similar strategy. Both are designed to take account of limited display screens by allowing collapsing views of textual content. Here, the mechanism is used to provide an outline view to convey high level information, while details are concealed/revelled to display further text regions of the ori-ginal document. The result is a more direct and systematic approach to viewing content that requires much less scrolling. Interestingly both these schemes have more recently incorporated features that use forms of sum-marisation ( Buyukkokten, Garcia-Molina, &amp; Paepcke, 2001, 2004 ).

Using summarisation to adapt content for small screen delivery, ( Buyukkokten et al., 2001 ) used an approach that, given an initial phase of content segmentation and extraction, can hide, partially display, make fully visible, or summarise text units. Described as an  X  X  X ccordian X  X  structure the method combines summari-sation with supporting the outlining action of being able to reveal/conceal content. They experimented with a variety of methods to summarise the text units of a web page, evaluating the relative performance of the sum-marisation methods in a user study involving information searching tasks.

An alternative, but similar style of presentation is hierarchical text summarisation. ( Sweeney et al., 2002, 2003 ), investigate the use of automatically generated hierarchical query-biased text summaries of newspaper articles presented to on WAP mobile phones, and PDAs. They describe hierarchical text summaries as having a root, or top level summary, which corresponds to the minimum level of information; each hierachy, or sum-mary level, is then intended to provide more information. Proceeding down the hierarchy, more and more infor-mation is made available, up to a maximum, which corresponds to the full text of the document. For the studies, summaries were produced using a query-biased sentence extraction algorithm, where a score was assigned to a sentence to reflect its importance for inclusion in the document X  X  summary. Scores were assigned based on examining the structural organisation of a document, utilising within-document term frequency information, and the distribution of contained query words. The final summary being generated as the desired number of top-scoring sentences, outputted in the order in which they appear in the original document. To evaluate the utility of the hierarchical query-biased summaries a user study carried out in a task-based setting. Summaries evaluated in the study ranged from title only, 7%, 15 X 30% of the original document length. Results from the study suggest that hierarchical query-biased summaries are useful when dealing with small screens.
Radev, Kareem, and Otterbacher (2005) also use hierarchical text summarisation to summarise web docu-ments for viewing on small, mobile devices. They describe the top level summary as presenting the most impor-tant sentences in an document, providing a gist of the content contained in the reminder of the document. Following this initial summary, users can then choose to  X  X  X rill down X  X  into the details by expanding nodes.
To generate summaries, document sentences are first ranked in order of salience; a tree is then constructed using the ranking, where the root node is the highest scoring sentences. Summaries then consists of the highest scoring sentences up to a cut-off salience level. Each sentence in a summary may also act as a node that links to other sentences that have a lower salience. The salience of a sentence is computed as a linear combination of four features: centroid (similarity of the sentence to the overall document); position; length and SimWithFirst (sim-ilarity to the first sentence of the document, most cases the title/headline). In a later paper, ( Otterbacher,
Radev, &amp; Kareem, 2006 ), evaluate the approach when used to summarise news articles sent to a web mail account (in plain text format) and accessed via a cellular phone. Comparing the hierarchical text summaries to that in which subjects were given the full text articles, there was no significant difference in task accuracy or the time taken to complete the task. Also, compared to three other summarisation methods, their users achieved significantly better accuracy on the tasks when using hierarchical summaries.

Other related work that employs summarisation as a mechanism for delivering textual information for small screen viewing include, the integration of linguistic analysis in the summarisation process for the custom information delivery for handheld devices ( Boguraev, Bellamy, &amp; Swart, 2001 ); and the application of fractal theory to summarisation for the delivery of financial news ( Yang &amp; Wang, 2003 ). 5. Query-biased summarisation using novel detection
We now report the methods used to generate our query-biased novel summaries. We start by describing query-biased summarisation, which forms an initial phase of the overall process, and then describe how we include novelty detection in the summary generation process. 5.1. Query-biased summarisation
Query-biased summarisation methods generate summaries in the context of an information need expressed as a query by a user. Such methods aim to identify and present to the user individual parts of a document X  X  text that are more focused towards this particular information need rather than a generic, non-query-sensitive summary. Summaries of this type can then serve as an indicative function, providing a preview format to sup-port relevance assessments on the full text of documents ( Rush, Salvador, &amp; Zamora, 1971 ). The application of query-biased summarisation to aid information retrieval tasks was investigated by
Tombros and Sanderson (1998) . The summarisation system employed in the study we report in this paper is similar to one described in Sweeney and Crestani (2006) and is based on the one developed by Tombros and Sanderson. The system uses a number of sentence extraction methods ( Paice, 1990 ) that utilise informa-tion both from the documents of the collection and from the queries used.

The underlying summarisation process relies on scoring sentences in a document to reflect their importance for inclusion in the document X  X  summary. Scores are assigned based on evidence from the structural organi-sation of the document (title, leading text and heading scores), within document term-frequency information (significant term score) and the presence of query terms (query score). The final score for a sentence is com-puted as the sum of the partial scores. The inclusion of a query score, which is based on the distribution of query words in a sentence, is of particular importance and distinguishes a query-biased summary from a gen-eric summary. Finally, the summary for a document is generated by selecting the desired number of top-scor-ing sentences, and outputting them in the order in which they appear in the original document. Summary length, that is, the number of sentences picked, can be controlled to restrict the level of information a user would be presented with in relation to the original document.

Fig. 1 provides an example of a query-biased summary generated for a sample newswire document, which is taken from the Associated Press Wire of TREC (refer to later Section 6.3 ). This document was used in the experiment, which we describe later in the paper. Annotations have been added, for the purposes of reporting here, to denote the summary and to identify sentences according to their ordinal position in the source document. 5.2. Summarisation with novelty detection
The starting point for generating our novel summaries is an initial seed summary, Sum biased summary. The length of this summary, l 1 , is determined as a percentage of the original document length. Given a ranked set of sentences, s r top l 1 sentences ordered as they appear in the original document.

Subsequent summaries are generated to include only novel information, and reflect previously seen sum-mary content. In this way, a request to  X  X how me more X  would produce a novel summary, SumN tained sentences with minimal overlap with those constituting the first summary. To avoid the presentation of material that the user has already seen the focus is on the sentences which, in the original (relevance-based) rank, were ranked right after the ones selected for Sum 1 selected from s r of selecting sentences from lower rank positions.

To estimate how novel the candidate sentences are, a history log, composed of previously seen sentences is formed. Each candidate sentence has a relevance score greater than zero. Sentences with a zero relevance score are not included to remove those sentences considered  X  X ot relevant X  which, may be novel but off-topic with respect to the query. For the case where the number of candidate sentences is less than the number required to generate a summary then those candidate sentences available are used, supplemented with addi-tional sentences from the history log to the number required. When there are no candidate sentences avail-able, we assume the summary to be the same as the previous summary. However, this is a pathological case which could happen only for very short documents; steps were taken to avoid such occurrence in the evaluation.

Next, a WordsSeen list is generated from the history log. The novelty score is based on the proportion of new words with respect to the WordsSeen and compared to all words in the sentence. We compute this as the count of the number of new words divided by the sentence size, including only those words in the sentence that have been stopped and stemmed. To combine the novelty score with the relevance-based score we apply weighting to the novelty score to emphasize novelty scoring over the previous scoring matrix for a sentence. The final score for a candidate sentence is then, the sum of the novelty score with the existing relevance score. Candidate sentences are then ranked according to the combined score.

On the basis of the score ranking and on the required size, a summary is produced. The top-scoring can-didate sentences form the final summary. The final stage of the process involves reordering summary sentences according to their ordinal position as they occurred in the original document. Fig. 2 provides example query-biased novel summaries generated for the same sample newswire document shown in the previous section. In the figure, the summary SumN 2 serves the request to  X  X how me more X  following the query-biased summary ( Fig. 1 ). The summary SumN 3 is then the result of a further request, and contains novel information with respect to both Sum 1 and SumN 2 . 6. Experimental settings
We now provide details of the experimental framework used to investigate the application of query-biased novel summarisation. To illustrate the process, a listing of summary sentences for the baseline and novel sum-maries of a typical document from the experimental collection is reported. This is followed by details of the document collection, and the measures used to compare the performance of the experimental summaries. The section concludes with an outline of the experimental procedure that was employed.
 6.1. Experimental arrangement
To evaluate our method of query-biased novel summarisation we carried out a set of user experiments with groups of users. To fulfil the requirements set out in our research questions, the experimental arrangement comprises two parts. Firstly, to assess the relative performance of query-biased novel summarisation com-pared to query-biased summarisation. And secondly, to evaluate any effects on performance due to a loss of context in a summary that contains only novel information.

To provide a point of reference for the rest of this section it is helpful to first illustrate the complete range of summaries built for the user study. Fig. 3 serves to describe the arrangement of the summaries that were gen-erated for the experiment, whilst Table 1 serves to illustrate the summaries created for a typical document.
Fig. 3 shows both the levels and types of summaries prepared. Reading in a vertical perspective the diagram can be divided along an imaginary central axis (beneath Sum novelty (left of centre), SumN i and SumN c ; and the baseline query-biased summaries, which do not (right of centre), SumB i and SumB c . The horizontally dotted lines indicate additional levels of summary, which depend-ing on their type may increase in length ( SumN i Example summaries, again for the same sample document, are given in Fig. 4 .

Key decisions made at the outset, and influence the production of summaries, relate to the number of sum-mary levels and the length of summaries. We restrict the number of summary levels to 3, primarily to avoid overburdening users in the experimental tasks. Also, including the document title with summaries we aim to assist users in associating summary levels with the source text. In terms of summary length, for each document a number of sentences equal to 7% of its length (with a minimum of 2 sentences and maximum of 6 sentences) were used. This is supported by our previous experiments with summary length, where we found short sum-maries to be suited and performed well in similar tasks ( Sweeney &amp; Crestani, 2006 ).

A further feature shown in the diagram ( Fig. 3 ) is an indication of differences in how information content is presented. In the diagram, x represents information gained from the summary at level 1. The contrasting methods of delivery are apparent then at levels 2 and 3. For SumN seen previously, and the additional new information, whereas for SumN
A similar situation happens for the baseline summaries. The overall pattern then is that the same information is conveyed in both cases, and only the method of delivery is varied.

We now describe the process of generating the summaries for the experiment. The idea is that Sum first summary presented to the user and, then, they can ask to see more information. There are two different ways to produce the next summaries. The first method increases length ( N summary to be l 2 ,= K * l 1 , where K = 2, for example, as is the case reported here. This method produces a new summary where all of the material which appeared in Sum 1 maintains a constant length ( N c ) and takes a very different approach producing a new summary, SumN and instead focus on the sentences which, in the original (relevance-based) rank, were ranked right after the ones selected for Sum 1 . That is, SumN c trast, the increasing length method includes both the new sentences and the material already seen, which we consider as the context.

The generation process for both SumN i and SumN c is for the most part the same with the key difference at the final stage. The generation process then differs depending on the summary type, as follows:
Increasing length summaries: A combination of the sentences taken from the history log, and the top N scor-ing candidate sentences form the final summary. Therefore, given SumN
SumN i Constant length summaries: The top N scoring candidate sentences form the final summary. Given
SumN c
We used query-biased summarisation to generate the baseline summaries, and they form the basis of our comparisons. 6.2. Sample summaries for a typical document
To illustrate the described process for building novel and baseline summaries, Table 1 shows the output of the summarisation processes for a typical document. The table highlights the difference between the summaries generated using the different settings, and at each distinct level, the associated sentence identifiers. The differences between Sum c and Sum i are clearly shown, with the increasing length summary containing previ-ously seen summary sentences. Also evident is the shared seed summary at level 1, which is a generic query-biased summary (recall Sum 1 in Fig. 3 ). A final point of interest is the overlap in summary sentences between the novelty and baseline methods. The overlap, sentence 15 occurring at level 2, is most easily seen in the con-stant length summaries ( SumN c and SumB c ). For some documents, the number of overlapping sentences is greater.

Fig. 4 contains the summaries generated for the sample document. Annotations marking the type of sum-mary have been added for the purposes of reporting here. For ease of cross-referencing with Table 1 , sentence identifiers have also been included in the summary text. 6.3. The test collection
Documents used in the experiment were taken from the AQUAINT collection, from the Novelty track, and consist of newswire stories from the New York Times (NYT) and Associated Press Wire (APW). Topics selected were used as a data source, providing users the necessary orientation and background with which to make their decisions. The TREC relevance assessments that are part of the collection, and made by TREC assessors, are used to enable precision and recall figures to be calculated.

It is worth noting that a feature of the novelty track is the assessment of relevance at the sentence level. In the novelty track assessors review each sentence in a document and mark it as either relevant or not relevant with respect to the topic. Therefore, we are able to make use of this sentence level information in our experiment.

A total of 5 randomly selected TREC queries and for each query, the 10 top-ranking documents were used as an input to the summarisation system. To ensure suitability of the documents for the experiment, a minimum of 5 relevant documents were present in each test set. The test collection then consisted of a total of 50 news articles. 6.4. Experimental measures
The experimental measures used to assess the effectiveness of user relevance assessments were the time to complete the task and accuracy . We quantify accuracy as precision, recall and decision correctness. In the experiment we focused on the variation of these measures in relation to the different experimental conditions val (IR) research.

We define precision (P) as the number of documents marked correctly as relevant (in other words, found to be relevant in agreement with the TREC judges X  assessments) out of the total number of documents marked.
This definition corresponds to the standard definition of precision. The use of TREC relevance assessments as the  X  X  X round truth X  X  towards which the users X  decisions are measured is a procedure used by many researchers in IR. In fact, for example, we used the same procedure in our papers in Tombros and Crestani (2000) . Recall (R) is defined as the number of documents marked correctly as relevant out of the total number of relevant documents seen. A further measure we used to quantify the accuracy of a user X  X  assessment was decision cor-rectness (DC), that is users X  ability to identify correctly both the relevant document and the non-relevant (irrel-evant) documents. We define decision correctness as the sum of the number of documents marked correctly as relevant, plus the number of documents correctly marked as non-relevant out of the total number of docu-ments marked for that query. 6.5. Experimental design
For the experiment we recruited 20 users to form four experimental groups ( Group were recruited from members of staff and postgraduate students of the Department of Computer and Infor-mation Sciences at the University of Strathclyde. The experiment was divided into two sessions, with two of the user groups completing the experimental tasks in each of the sessions. Care was taken to ensure consis-tency in the conditions experienced by all groups.
 For the experiment, each user was given 5 queries, and for each query, the top 10 retrieved documents.
There were on average 5.6 relevant documents among the documents for queries. The 10 documents were rep-resented as 5 documents summarised using technique which included novelty, SumN , and 5 summarised using the baseline query-biased summarisation, SumB . For each document there are three summary levels as, Sum Sum 2 , and Sum 3 ( Fig. 3 ).

The experiment was conducted in such a way that each user group used the different system settings. The system configurations that were shown to users alternated so as to mix the different types. For example, the first document might be SumB i , then the next document SumN the experimental conditions used. The allocation of summary types were assigned in such a way as to avoid users X  gaining preference for a type of summary over another. Both the user group and session assignments were selected randomly.

To summarise, each user was given a total of 50 documents to work through, each represented by 3 sum-maries. At the end of the experiment, a user had visited a total of 150 document summaries (75 with novelty
SumN and 75 without novelty SumB ). 6.6. Experimental procedure
Each user was presented with a retrieved document list in response to a simulated query (TREC topic), and tasked with identifying correctly relevant and non-relevant documents for that particular query. Further, so as not to biased quick decisions, the importance of making accurate responses was stressed to users. The infor-mation presented for each document was the automatically generated summaries.

Following an initially briefing about the experimental process and instructions by the experimenter, users were presented with a list of 5 queries. To start the experiment users were asked to select the first query from
TREC topic 3 ) provided the necessary background to their  X  X nformation need X  to allow users to make relevance assessments. For each query, an initial period was allowed to read and digest the query details. Following this, the first of the 10 highest ranked documents were presented and timing for that specific document started.
Users were shown documents from the list where the content for a document consisted of the level 1, 2 and 3 summaries (e.g. SumN c 1 , SumN c 2 , and SumN c 3 ). This order, based on level, was the order that the content was presented to users. Having seen summary SumN c 3 users were required to make a decision as to whether to mark the document as relevant, or non-relevant. After indicating their decision users were presented with the first summary of the next document. On completing the final document for a query users were returned to the list of queries. The process was repeated until all queries have been evaluated.

Once all query tasks were completed a simple online questionnaire was given to the users. The key quan-tative data of interest, user decisions and the individual summary timing data, were recorded in logs file.
Some shortcomings to the methodology used in our experiment relate to the use of TREC topics to simulate information needs imposes an unnatural overhead on users to carry out relevance assessments. Added to this, is the use of TREC relevance assessments as the basis for comparing user decisions in order to obtain precision and recall values. However, despite this limitation the same experimental conditions applied to all of the test systems. A further factor imposed as part of the experimental design corresponds to permitting relevance deci-sions only after viewing all of the summaries, and not at individual summary levels. In removing the ability to make an early decision, it could be argued that we are not giving users a true representation of the case for  X  X how me more X . The motivation for the restriction was to ensure a consistent basis for comparing all systems.
It was an assumption of the study that users would make better decisions if shown more of the original doc-ument contents. With this in mind, we evaluate the best strategy for showing the user more. Therefore, we do not expect to evaluate the system at intermediate steps (before presenting the 3rd summary) but our aim is instead to evaluate the different production strategies (incremental versus constant length) with and without novelty. To evaluate the effects of intermediatory decisions, before the 3rd summary, we carried out some additional experiments, which are reported in Section 7.5 . 7. Results
We now report the results of the experiment described in the previous section. The results are reported from a number of view points. We start from an overall view of users X  performance, and then consider the perfor-mance at both query and document levels. Isolating relevant documents, we report how the make up of these summaries may have influenced users X  decision making. We then consider performance at the different sum-mary levels. Finally, we end the section by discussing the findings from the results. 7.1. Overall performance
Table 3 provides a view of the results in the context of the experimental methodology, depicting the allo-cation of users to groups and associated summary types. Focusing on the different summary settings the rel-ative performance across the experimental queries in terms of DC, P , R and average time spent is shown.
The results show a slight increase in DC and R performance with summaries that provide novelty with additional context, SumN i . For P , the baseline summary with a constant length, SumB ever, the margins of improvement are somewhat minimal.

Interestingly, the margin of difference in the time spent on SumN what we might normally expect. The additional effort to digest a longer summary (e.g. SumN suppose to translate into more time spent compared to shorter summaries (e.g. SumN show that is not necessarily the case and the times are instead very similar. A possible reason to explain the similarity could be that users may skim the longer summaries, glancing over familiar parts, content already seen, and instead focusing on the new parts. The baseline summaries follow a more expected pattern, though again the margin of difference is small. A further observation from the table is the similarity in time spent view-ing summaries between SumN i and SumN c , compared to the greater level of separation observed between
SumB i and SumB c . However, we cannot extract significant conclusions on the basis of task completion times, since intangibles, such as, user fatigue and individual differences among users might also be important.
An alternate view of the results is given in Table 4 where the measures are separated accordingly (DC, P , R ) and the results presented in a form that permits easy evaluation of the original hypotheses. In this way, by evaluating the columns we gain insight into hypothesis 1, and the rows hypothesis 2.

If we consider the case of novelty versus baseline summaries, relating to hypothesis 1, then for DC and R we observe that the best performing summary is from among the novel approaches ( N
R = 0.852). For P , the inverse is the case where the best performing summary setting is from the baseline ( B c , P = 0.850). However, as mentioned the margins of performance improvement are small and as such inconclusive. Further inspection of Table 4 shows that evidence for a constant versus increasing length sum-mary, relating to hypothesis 2, to be inconclusive. Carrying out appropriate statistical tests (Chi-Squared test) we found no significance difference in the overall results for the different approaches. 7.2. Query level performance
If we consider results at a query level, then Fig. 5 reports the performance for each query separately. In terms of DC and P then performance levels show a degree of alignment according to whether they contain novelty, or are from the baseline. On the whole there is a pattern of improvement over the first query, with performance levelling out for intermediate queries and a drop in performance for the final query. However, an exception to this pattern is DC for the baseline approaches in the second query seen by users, query 58 (Q58), where there is a drop in performance. For R , the different summary types share a similar performance profile but with a greater spread in the range of performance levels. However, SumN worse in R compared to all other approaches, particularly in query 78 (Q78).

Indeed, the poor performance in R for SumN c may be in part due to its summarisation strategy and a ten-dency to  X  X ove away X  from the relevance ranking in favor of new/novel information. This fact, combined with the fact that information from the previous summary is not presented, may result in users loosing the context of sentences in the summary. During decision making then, given a degree of indecision by users based on a lack of context, they may be more inclined to mark a document as irrelevant (and, thus, some relevant doc-uments are wrongly classified as irrelevant). Therefore, recall is harmed. On the other hand, precision is not especially worse than the other approaches as it is not as likely that many irrelevant documents will be marked as relevant.

Comparing queries in terms of the average time spent, the first query takes the greatest amount of time, with a decrease in time spent on all other queries, Fig. 6 . Interestingly, despite spending less time, users per-form no worse in making relevance decisions for the later queries. This may be attributed to learning effects as users become more efficient in completing experimental tasks. Beyond the second query there is little variation in the times for the remaining queries, which may suggest a threshold in task efficiency.

The degree of query topic difficulty, and the language and writing style of documents, are the main factors behind the fluctuation in the observed query level performance. A further contributing factor being a period of learning as users become familiar with the experimental task. This pattern may also be observed at a document level for queries, as users X  refine their interpretations of relevance. The performance drop for the final query may be explained by an element of user fatigue. 7.3. Document level performance
We now provide some indication of performance at a document level, where possible effects due to docu-ment characteristics, as well as factors relating to how users have approached the experimental task, are more apparent. However, the results should be interpreted as tentative due to limitations in the experimental design. The experimental arrangement does not allow the extraction of definite conclusions at a document level. To gain insight into users X  performance we present the results of two experimental queries: Q58 and Q78.
These two queries form opposite ends of the scale in terms of the observed performance results. The former being the worst performing query in the experiment, while the later being among the best performing queries.
We focus on reporting DC results only, since this measure can account for both correct relevant and non-rel-evant decisions. Before we present results, it is worth recalling the mix of relevant and non-relevant documents for the two experimental queries. In Q58 there was a ratio of 6 relevant to 4 non-relevant documents and for Q78 an equal ratio, with 5 relevant and 5 non-relevant documents.

Fig. 7 reports the DC levels for all documents in Q58, comparing the different types of summary and dis-tinguishing both relevant and non-relevant documents. The results show high levels of DC for documents 5, 6, 9 and 10, and given such consistency in performance across users suggests there was little problem in correctly identifying these documents. There is a drop in performance for documents 2, 7 and 8. Indeed, for documents 2 and 8 most users consistently made incorrect decisions. This may be an indication that users have experi-enced difficulty in making decisions for these documents.

A factor that could contribute to a low performance for relevant documents is the presence of few (TREC deemed) relevant sentences in summaries. For a non-relevant document the presence of sentences that suggest relevance, or are partially relevant, but in fact do not fulfil the full requirements of a relevant document, as deemed by the TREC assessors, could mislead users. This may explain the low levels of performance for the summaries of documents 2 and 8, both non-relevant documents. For the case of document 7, in the next section (Section 7.4 ) we shall analyse the composition of relevant document summaries to establish the pro-portion of relevant and non-relevant sentences.

Fig. 8 shows the DC levels for all documents in Q78. Here, we can see that for most documents a high level of accuracy is achieved with the exception of documents 3, 5 and 9. Interestingly, all of these documents are relevant documents and as a result, for this query, it would seem that users appear to perform better with the non-relevant documents. This pattern is opposite to that in Q58, where users performed better with relevant documents. Achieving a greater level of accuracy in correctly identifying non-relevant documents could be attributed to a clearer distinction in a summary X  X  content being off-topic.

Table 5 draws a comparison between users X  performance with relevant and non-relevant documents for Q58 and Q78. Here, the average performance for all documents is reported according to the different summary types. Results from the table show, for Q58, despite the dip in performance for documents 7 and 8, seen in
Fig. 7 , both novel approaches perform slightly better than the baselines. Also, that relevant documents mar-ginally out-perform the non-relevant documents. For Q78, there is less distinction between novel and baseline summaries. However, increasing length summaries seem to fair better than the constant length summaries and, as previously mentioned, the improved performance with non-relevant documents is clearly evident.
Other factors that may influence users X  performance at a document level include, the length of summaries and the time taken to complete the experimental tasks. Our initial intuition was that users would be more accurate in making decisions using longer summaries, since they would see more of the original document X  X  content. Further, that greater decision accuracy would be attained from longer viewing times. For both Q58 and Q78 there was little observed differences in performance among long and short summary lengths.
Also, despite differences in summary viewing times among users, comparing hastier decisions to a longer time viewing summaries, there was again little performance variation. 7.4. Relevant sentences in summaries
We now report on the effects of relevant material in the experimental summaries, and whether having a greater proportion of relevant sentences has an influence on making correct decisions.

A further part of the test collection used for the experiment is relevance judgements at a sentence level. For relevant documents in the TREC Novelty track collection, all sentences are manually assessed and annotated as being either relevant or non-relevant. Using this listing of relevant sentences we were able to establish rel-evant sentences contained in the experimental summaries. Table 6 shows, for relevant documents in Q58 and
Q78, the percentage of summaries containing relevant and non-relevant sentences, based on all summary levels combined. In addition, details of source documents: the document length as the number of sentences, and the number of contained relevant sentences are reported.

Apparent in Table 6 is the lack of relevant sentences in summaries where the percentage of relevant sen-tences in the text of a document is low. This effect is most clear for documents 3 and 9 in Q58 and appears to be independent of the summary type. As the number of relevant sentences increases in the text of a docu-ment both types of summary gain more relevant sentences. This can be seen for document 10 in Q58, where 47% of the summaries contain relevant sentences. Similar patterns are evident for relevant documents in Q78.
In terms of influence on performance, we can observe that the percentage of relevant sentences in summa-ries does impact on users X  accuracy in DC. Combining the findings from the previous section, with insights gained from Table 6 shows that summaries that contain many relevant sentences out-perform those with low numbers of relevant sentences. In Q58, summaries for documents 2, 7 and 9 contain few relevant sentence for which users X  achieve low levels of DC. By comparison, the remaining relevant documents in Q58, containing more relevant sentences and all perform better. A similar case is in Q78, comparing the low performance with document 5 to the other relevant documents.

Figs. 9 and 10 provide insight into the numbers of sentences that make up the reported percentage of sum-maries. The figures show, for each of the relevant documents, the number of relevant and non-relevant sen-tences at each summary level and evidence to explain the poor performance for certain documents. If we return to document 7 in Q58, highlighted in the previous section, there is only a single relevant sentence in the baseline summaries, and only a minor improvement of two relevant sentences for novel summaries. It therefore not unexpected that users made mistakes for this document.
 We can also gauge the differences between summary types, evident beyond the first generic summary level. For Q58, there is no difference in the total number of relevant sentences for the query as a whole, whereas for
Q78, there are 5 more relevant sentences for SumB c compared to SumN relevant sentences SumN c performed no worse than SumB c . It would seem then, aside from the quantity of relevant sentences in summaries, that other factors, such as, the style of writing, technical details, and any assumed previous knowledge may have an impact on users decision accuracy. 7.5. Performance at different decision levels
It was an assumption of the study that users would make better decisions if shown more of the original document contents. As such, a study of the decision level, or  X  X  X topping point X  X  was not an objective in the paper. Nevertheless, we conducted a preliminary study (four users) to analyse whether this issue can have a significant effect. While in the previous section we reported the number of relevant sentences at the different levels of summary, here the investigation is concerned with the patterns of users X  decisions given the freedom to select summary levels.

We now report the findings of a small additional experiment, similar to the one described, with the differ-ence that users were able to make relevance decisions at any level. The purpose of this study was to investigate addition, observe the utility of summary levels since users can select which summaries to based their decisions and determine the accuracy of their decisions. Our initial assumption, which underpinned the reason for restricting the decision level, was that shown more of the source document users would achieve a higher level of performance, compared to an early decision on the basis of seeing less of the source document.
In much the same experimental conditions as used previously, four users were given the same experimental task, to correctly identify relevant documents and assigned one of the test settings which were associated with the previous experimental groups.

Results from this new experiment indicate somewhat similar performance levels to those found in the pre-vious experiment in terms of overall DC, P and R . An exception being in the time spent viewing summaries, which was reduced. Table 7 summarises the above finding, comparing the overall average DC, P , R and Time for both experiments. In the table, a fixed decision level refers to the previous experiment results, while a var-iable decision level refers to the setting described here, with the decision level being controlled by users. On the basis of these results it would seem that permitting users choice in the level of summary to make decisions does not improve the accuracy of making relevance decisions.

A further interesting insight is the utility of the summaries. For this purpose we assume those levels seen by users to be an indicator of the utility of the summaries. Table 8 presents the decisions made at the distinct summary levels by users. The table shows variation among users in the levels used to make decisions. How-ever, a pattern most evident being that most users make fewer decisions with the lower levels (levels 2 and 3), with a large proportion of the decisions are made at level 1; in most cases half the of the total decisions. At the same time, as users do go beyond the first level, there is greater variation among users in decisions between levels 2 and 3. An alternative view of summary utility is given in Table 9 . Comparing the levels of decision on the basis of summary type Table 9 focuses on the decisions made at level 2 and 3 summaries (level 1 is included only for completeness). The table shows a consistent trend in use for all summary types, with the as more decisions are made at level 2. Indeed, for novel summaries, the combined use of levels 2 and 3 is greater than at the first level of summary. Comparing summary use on the basis of length, then constant length summaries have slightly greater levels of access, compared to incremental length summaries. Interestingly, the greater frequencies of access of constant length summaries at level 3 (24% and 22%) may suggest that there is an increased loss of context in constant length level 2 summaries.

In terms of effectiveness in making relevance decisions at the distinct summary levels, Table 10 provides a comparison of summary types on the basis of DC performance. Here, we can see that by isolating the sum-mary types SumN i stands out as the best performing with the pattern of an increase in accuracy, with increas-ing level. However, this does not hold for SumB i summaries, and is indeed the inverse of the pattern observed for the overall picture ( Table 9 ). Comparing the performance with novel summaries to the baselines then there is not much in the way of difference. In terms of performance at the levels, then users seem to perform best with level 1 summaries.

We now summarise the findings of the additional experiments reported in this section. The results suggest a preference among users to make decisions at the first summary level, however, in the case of novel summaries, a greater tendency to use further levels. Whilst achieving slight improvements in performance by going beyond the first level for novel summaries, on the whole, users are most accurate with the first summary. Therefore, despite the freedom to select summaries for decision making afforded by the new experimental setting this does not translating into greater levels of performance. Since users are traditionally reluctant to spend much effort to make relevance decisions and, thus, it is not surprising that our users decided often to assess the relevance of the documents right after seeing the first piece of material. Nevertheless, in this study our main aim was to determine which methods are effective to supply additional material to users when they want to spend more time reviewing a document X  X  contents. With this in mind, we consider that our original experimental design to be valid. However, we recognise that in a more realistic deployment, that a setting more similar to the one described in this section would be more appropriate. 7.6. Discussions
We now discuss the findings in the experimental results in response to our initial intuitions held at the out-set. The experimental hypotheses, which we describe in Section 3 can be briefly summarised as follows: (1) given the benefits of seeing more new information from a document, that novel summaries would out-perform summaries that contained redundant information; (2) also, based on viewing more a document, that longer summaries, including context, would perform better than short summaries.

In terms of novel query-biased summarisation, the results show no benefit of generating summaries that are novel over the baseline query-biased summaries. The inclusion of novelty in summaries, despite our initial intuitions, does not seem to be beneficial in terms of performance, or in terms of time savings. Key factors that are integral to the performance of the experimental systems and influence the results we observe are: the novelty detection algorithm adopted, and the quality of the baseline used.

If we recall how the experimental summaries were produced, a core part of the (query-biased) summary generation process was the ranking of sentences by relevance. Novelty detection was integrated as an addi-tional feature. The baseline summaries then by definition contain a majority of relevant sentences, and, there-fore, it is not unexpected that they perform well for the task we set our users. For relevant documents, the average number of relevant sentences in SumB c and in SumN sentences in all summaries combined, for all queries, in SumB sentences.

An explanation for the lack of performance difference in experimental systems, despite the inclusion of nov-elty detection, could be due to a deficiency in the distinction of the summaries generated; that users X  could not discriminate between the different types of summary, and perceived all summaries as being the same. This could also be a feature of the specific collection that comprise relatively short news documents. Therefore, given the baseline query-biased summaries are well suited to the experimental task, and difficult to improve upon, combined with the similarity in performance levels; we could surmise that the simple method of novelty detection we used, though proved to be no worse than more complex approaches in Allan et al. (2003) , is not suitable for our purpose. A more complex method of detecting novelty, that is more selective of the sentences that make up a summary, is needed for the tasks we set our users.

Considering a constant length summary compared to an increasing length summary, the experimental results show that retaining contextual information in the summaries does not improve performance. Instead, it would seem that users are able to maintain the context of what has been previously seen, without explicit prompts in the content of a summary. Whilst there is little benefit in time savings between the two summari-sation approaches, there is a benefit in savings of bandwidth for constant length summaries. This is an impor-tant finding from the point of view of mobile information access, where factors relating to costs of communication are more prominent. Benefits of a short summary then would include: reduced costs, both financially for pay-per-view content and in transmissions overheads; less navigation requirements in terms of scrolling and paging for content; and less cognitive effort to assimilate the information contained in a sum-mary, due to a smaller amount of text to digest.

Finally, given the freedom to select summaries to base decisions, the results show a pattern of preference to make decisions at the first summary level. Users were also found to be accurate in their decisions using the first level. This supports the argument that the query-biased summaries, used for the generic first level summary and the baseline summaries, are difficult to improve on. Though again, this finding might be a consequence of the specific collections and topics used. 8. Conclusions and future work Automatic text summarisation condenses a document, thereby reducing the need to refer to the full text.
This can be seen as being beneficial given the problems associated with information overload. An effective way to produce a short summary maybe to include only novel information. However, producing a summary that only contains novel sentences (assuming we employ sentence extraction to build summaries) might imply a loss of context for the reader.

In this paper we considered summarisation with novelty detection, where information is not only condensed but also attempt is made to remove redundancy. Whilst the combination of summarisation paired with novelty detection is not a new concept, in the paper we concern ourselves with the mechanism of delivery, more spe-cifically, to investigate if there a optimal strategy for showing summaries to users in response to the request to  X  X how me more X .

We adopted two strategies to produce summaries that incorporate novelty in different ways: an incremental summary and a constant length summary. We compared the performance of groups of users with each of the summarisation strategies, and baseline query-biased summaries that did not include novelty. The aim was to establish whether a summary that contains only novel sentences provides sufficient basis to determine rele-vance of a document, or whether additional sentences need to be present to provide context.

Findings from the user study suggest that there is little difference in performance (DC, P , R and Time) between the different summaries. That, the inclusion of novelty in summaries does not seem to offer benefits in terms of performance, or in terms of time savings. However, this may be due in part to the high levels of performance achieved with the baseline query-biased summaries, and the suitability of the novelty detection algorithm we adopted. In terms of a constant length summary compared to an increasing length summary, the experimental results show that retaining contextual information in the summaries does not improve perfor-mance. Instead, it would seem that users are able to maintain the context of what has been previously seen, without explicit prompts in the content of a summary. Whilst again there is little benefit in time savings between the two approaches to summary length, there is a benefit in savings of bandwidth for constant length summaries. This is an important finding from the point of view of mobile information access, where factors relating to costs of communication are more prominent. Finally, given the freedom to select summaries to make decisions, the results show both a pattern of preference, and high level of accuracy, for initial summaries.
If we revisit our initial objective to evaluate the usefulness of incorporating novelty detection in summari-sation at a sentence level. On the basis of our results, it seems that users do not perceive major differences between approaches that account for novelty and those that do not. Therefore, the state of the art in sentence level novelty detection is not helpful in these circumstances.

Extensions to the work we have presented include investigating the performance of a more refined approach to novelty detection beyond a simple count of new words. Also, it would be interesting to measure users X  opin-ions on their confidence and perceived accuracy in making relevance decisions. In addition, investigating the use of different collections and topics could also be interesting. Finally, given the open issue of information overload, there remains the motivations to investigate strategies to assist in accessing information. Therefore, we intend to continue to explore the theme of  X  X how me more X  to investigate other parameters of summarisa-tion, such as, personalisation.
 Acknowledgement
This work is supported by the EU Commission under the IST Project PErsonalised News content program-minG (PENG) (IST-004597). More information about PENG can be found at http://www.peng-project.org/ . References
