 AVail te I z z Attfibut | Low to Moderate De~iw Noise Figure 1: Variation in cluster shape, size and density across data localities and subspaces of such cases in Figure 1. In many cases, a region of low density can be dearly distinguished as a separate cluster in one subspace, but regions with similar density correspond to noise in another subspace. Often, even within a subspace, the clusters can be distinguished from one another only on a case-by-case basis. Such clusters are difficult to isolate using fully automated methods in which simple mathematical for-realizations are used as the only criterion in order to define all clusters. Since there is so much variation across differ-ent data localities and projections, it is difficult to recon-cile these differences without the use of human intervention. At the same time, since there are a very large I number of subspaces in the high dimensional case, human involvement necessitates the exploration of only a small fraction of the subspaces. Thus, computational support is required in order to minimize the effort in finding clusters in optimally chosen subspaces. Therefore, a natural solution is to divide the clus-tering task in such a way that each entity performs the task that it is most well suited to. In the system thus devised, the computer performs the high dimensional data analysis which is used in order to provide the user with summary feedback; this feedback is given in a way so that the human is facili-tated in his intuitive task of characterizing the clusters. The reactions of the user are utilized in order to determine and quantify the meaningfulness of the final set of clusters which are highly interpretable in terms of the user-reactions. The result of this cooperative technique is a system which can perform the task of high dimensional clustering better than either a human or a computer. We note an interesting tech-nique discussed in [7], which describes a visual tool for users to interact with clusters in lower dimensional views of the data. However, our system actively finds views of the data in which projected clusters exist, whereas the technique in 1There are infinitely many if arbitrary subspaces of the data are picked. Algorithm ComputePolarizedProjec~ion(Data Set: 9 , begin lc ----d/2; gc = Lt; while ic &gt; 2 do begin end return(&amp;); end put to the system we provide a user-defined support, which is the minimum fraction of database points in a cluster for it to be considered statistically significant. 
The overall algorithm is illustrated in Figure 2. The in-teractive projected clustering algorithm works in a series of iterations in each of which a projection is determined in which there are distinct sets of points which can be clearly distinguished from one another. We refer to such projec-tions as well polarized. In a well polarized projection, it is easier for the user to clearly distinguish a set of clusters from the rest of the data. In order to create the polarizations, we pick a set of k records from the database which are referred to as the polarization anchors. A subspace of the data is determined such that the data is clustered around each of these polarization anchors. The data is repeatedly sampled for polarization anchors in an iterative process, so that the most dominant subspace clusters containing these anchors are discovered. 
In each iteration, when the projection subspace has been found, kernel density estimation techniques can be used in order to determine the data density at each point in this projection. A visual profile of the data density is provided to the user, who uses this aid in order to find the most intuitive separation of the data into clusters. The cluster separation by the user is then recorded in the form of a set of N Identity Strings (IdStrings), where N is the total number of records in the database. These set of identity strings are denoted by 27 = (11... IN). We assume that the rth string I~ corresponds to the rth data record. In the nth iteration, the value of the nth position in the identity string is recorded. If the rth data point does not belong to any of the clusters corresponding to the polarization points, then this position value is set at * (don't care). Otherwise, we set the value to the index of the corresponding cluster in that particular projection. We will provide a more detailed discussion on this slightly later. As the termination criterion, we ensure that most points in the data are included in a cluster in some view. Specifically, we define the coverage of the data set as the average number of views in which a data point occurs in majority of the polarization points. Of course, clusters may also exist in that projection which do not contain any of the polarization points. However, once a good projection is de-termined, all relevant clusters are used by the algorithm. It is an interesting problem to find the projection subspace E in which the data shows this polarized behavior X  An exam-ple of such a projection is illustrated in Figure 4, in which the data is nicely separated out into different clusters using two polarization points. We emphasize that the main pur-pose of randomly sampling the data for polarization points is to discover projected clusters (if any) which contain these points X  F~zrthermore, even if some clearly separated clusters have low density in all possible projections, the correspond-ing subspaces would still be discovered when a polarization point from the cluster is sampled. In this sense, polarization points serve a similar function to medoids in many cluster-ing algorithms [3] X  As in medoid based algorithms, it is expected that the user would sample at least one point from the prominent (projected) clusters in the data, if enough number of points are sampled for polarization. 
In order to actually find the projection subspace we use an iterative process in which we start off with the universal d-dimensional subspace L/. In each iteration, we maintain a current subspace gc of dimeusionality Ic which is gradually reduced to a 2-dimeusional projection. In each iteration, we find the set of data points J~i which is closest to the po-larization anchor yt based on the distance measurements in the subspace  X ~. We assume that the centroid of .Adi is zi. The number of data points in each subset A41 is determined by the user-defined threshold s. In turn, the data points in Uk__.~lJ~i are used to determine the subspace in which these points form well distinguished clusters around the polariza-tion points X  This can be done only by finding the subspace in which the momentum of these sets about their centroids is minimal. Some principal component analysis techniques exist [8] which can find the optimum subspace in which the momentum of a single set of points about their centroid is minimal. However, here we have k sets ~i ...Mk, and E. Therefore, we will use a simple transformation trick in order to use the method in [8]. 
Let O(zi) be the set of points obtained by subtracting zi from each point in .A4i. Thus, each of the sets O(zi) is now centered at the origin O. Since this transformation is a simple translation, the covariance and momentum about the centroid is preserved by the transformation. Therefore, we can show that TC(Mi, E, zi) = 7~(Oi(zi),g,'O). But we Now all we need to do is to use the method of [8] in order to find the subspace in which the momentum of the single set of points U~=lOi(zi) about their centroid is minimal X  
In order to do, we first determine the covariance matrix A of the data points in U/k=10i (zi). Specifically, the covariance matrix is a d* d matrix, in which the entry (i,j) is equal to the covariance between the dimensions i and j of the points in U~=lOi(zi). This matrix is positive semi-definite and can be diagonalized in order to determine a set of eigenvectors which will be orthogonal to one another X  It has been shown in [8] that the eigenvectors corresponding to the smallest eigenvalues define the subspace with the least momentum about the centroid. Therefore, the subspace ~c corresponds to the eigenvectors for the ic smallest eigenvaiues. Corre-spondingly, this means that the data points in J~4i are likely 
Since the density at every point in the continuous space cannot be calculated, we pick a set of p * p grid-points at which the density of the data is estimated. The density val-ues at these grid points are used in order to create a surface plot. An example of such a plot is illustrated in Figure 5. Since clusters correspond to dense regions in the data, they are represented by peaks in the density profile. Similarly, the regions which separate out the different clusters have low density values and are represented by valleys in the den-sity profile. In order to actually separate out the clusters, the user can visually specify density value thresholds which correspond to noise level at which clusters can be separated from one another. Specifically, a cluster may be defined to be a connected region in the space with density above a cer-tain noise threshold ~/, which is specified by the user. In order to provide the visual perspective of this separation, a hyperplane at a density value of r I can be superposed on the density profile. We shall refer to this as the density separa-tor plane. The intersection between the separator plane and the density profile creates a number of connected regions at which the density is above the specified noise threshold. The contours of the intersections between the separator planes and the density profiles are also the contours of the clusters in the data. All the data points which lie within a contour correspond to the same duster in a given projection. For example, in Figure 5, we have illustrated a case in which by specifying the noise threshold r/= 27, we find two clusters above the noise threshold. Note that the resulting clusters may be of arbitrary shape. Furthermore, by specifying dif-ferent values of the noise threshold y one may have different number, sizes and shapes of clusters. For example, in Figure 5, there are two clusters at the specified noise threshold of 27, whereas if we reduced the noise threshold to 15, there are three clusters. This is because in this case the cluster corresponding to the low density peak is also found. If we reduced the noise threshold further, then two of the clus-ters may get merged, and a larger portion of the low density cluster is found. Thus, different noise thresholds provide a division of the data into clusters of different levels of gran-ularity. It is often difficult to settle on the use of single density, since clusters in different localities will have differ-ent densities. In order to handle this, we allow the user the flexibility to specify multiple values of 0 in a single projec-tion. The smaller values of the density will reveal even the low density dusters in the data but will not reveal the finer separations among different clusters. The larger values of the density will reveal the fine grained separations into dif-ferent clusters, but will not reveal the low density clusters. We assume that the final set of densities picked by the user is denoted by/C = nl ... n,. In some projections, the data may not be amenable to clustering. In such cases, the user may choose not to specify any value of the noise threshold r/, and the set/C is null. This will not happen too often if the subspace determination procedure of the previous section is effective in finding well polarized projections. We note that the number of different separations r may depend upon the nature of a given projection and a user's understanding of the data behavior. Such intuition cannot be matched by any fully automated system effectively; this is an example of the criticality of the user in the cooperative process of high dimensional clustering. We note that in Figure 5, the polarization point occurs at the peak of a local optimum in the density profiles. It is typical that polarization points throughout the data. 
Consider an IdString I~ and pattern S of the same length l. A pattern S is a subpattern of I~ if and only if for each position i E {1,... l} in S which is not *, the ith position in I~ also has the same value. Thus, the string *2*5*** is a subpattern of the string *2*5*4*, but it is not a subpattern of the string *2*3*4*. The support of the pattern S is equal to the percentage of the IdStrings in 2:, for which the string S is a subpattern. The larger the number of fixed positions in S, the smaller the support of the string. We note that the minimum support s provided by the user is a measure of the minimum number of data points which a cluster must contain for it to be considered useful. Therefore we find all the mammal subpatterns which have support greater than the user defined threshold s. We also refer to such subpat-terns as itemsets. Methods for finding such itemsets have been proposed in [5]. Let us denote the final patterns found by QI... QK. Note that each of these patterns Qi can be mapped onto all the data points Ci F whose IdStrin9s are supersets of these subpatterns. We shall refer to the sub-pattern Qi for cluster C~ as the cluster template. A position value m r on this template Qi which is not * (a fixed position) corresponds to a projection in which the points of Cf belong to duster id m ~ separated out by the user in that view. We note that when the projections are chosen from the original set of dimensions, it is possible to interpret the clusters using the duster template. This is because the cluster template Q~ of the cluster Cf provides the different combinations of dimensions in which the user always classified all of these points to belong to the same cluster. This provides an intu-itive interpretation of how the final clusters relate both to the attributes in the data and the history of user interac-tion. The subspace in which the set of points Cf is a cluster corresponds to the union of all the 2-dimensional subspaces for fixed positions in Q~. It now remains to discuss how the meaningfulness of each of these clusters is quantified. 
The procedure for evaluation of meaningfulness of clus-ters is denoted by EvalMeaningful in Figure 2. Since the final dusters are created by determination of the sets of points which occur together as clusters in multiple projec-tions, it is useful to evaluate the consistency of the user behavior across these different views in order to evaluate meaningfulness. In order to do so, we calculate the interest ratios of the patterns which define the clusters. The interest ratio of a pattern is the ratio of its actual support to the expected support based on the assumption of statistical in-dependence. Let S = rnl ... m~ be a given cluster template, created by the I iterations. Let 0 be the fraction of database points supported by S. Let/3i be the fractional support of the number of points corresponding to mi. (Specifically,/3i is the fraction of points that belong to cluster Id mi for the visual projection i. When mi is "*", then the value of/3i is 1.) Then the interest ratio IR(S) of the cluster template S is the ratio of the support of template S to its support assuming statistical independence. When the user behavior does not show any meaningful cor-relation across the different projections, the interest ratio for the clusters discovered will be close to one. An interest ratio larger than 1 is indicative of a cluster which reveals to provide an intuitive understanding of the actual quality of the clusters found, we use a measure which we refer to as the cluster purity. Each record in the data sets had a class label associated with it. The cluster purity was defined as the average percentage of the dominant class label in each cluster. Since the class label was not used in the interactive clustering process, this provides an intuitive idea of the final quality of the clusters. For the case of the ionosphere data set, the average cluster purity was found to be 86.3%. We also tested the algorithm when more than one polarization point was used. Example of a sample projection on the iono-sphere data set with two polarization points is illustrated in Figure 6. The interactive experience and final result for the case of two polarization points was similar to the case when one point was used. The algorithm was also tested on the segmentation data set from the UCI machine learn-ing repository. The overall clustering experience was quite similar to the ionosphere data set. A detailed description of these empirical results and the effect of using different number of polarization points is provided in a long version of this paper [2]. 
High dimensional clustering is a very challenging prob-lem because of the sparsity of the data. In this paper we discussed the problem of discovering meaningful clusters in high dimensional space with the utilization of a cooperative process between the human and the computer. The clusters are discovered in different subspaces with the use of polar-ization points and a visual profile of the data is provided to the user. This visual profile may be used in order sepa-rate out the different clusters with the use of human inter-vention. The attraction behind such an interactive process is that the problem of high dimensional clustering requires both the computational power of a computer and the in-tuition of a human; consequently a system which allocates the tasks according to the abilities of each entity is more effective than a fully automated process. [1] C. C. Aggarwal. Re-designing distance functions and distance based applications for high dimensional data. 
ACM SIGMOD Record, March 2001. [2] C. C. Aggarwal. A Human-Computer Cooperative System for Effective High Dimensional Clustering, IBM [3] C. C. Aggarwal et al. Fast algorithms for projected clustering. A CM SIGMOD Conference, 1999. [4] C. C. Aggarwal, P. S. Yu. Finding Generalized [5] R. Srikant, R. Agrawal. Mining Quantitative [6] It. Agrawal et al. Automatic Subspace Clustering of [7] A. Hinneburg, D. A. Keim, M. Wawryniuk. HD-Eye: [8] I. T. Jolliffe. Principal Component Analysis, 
