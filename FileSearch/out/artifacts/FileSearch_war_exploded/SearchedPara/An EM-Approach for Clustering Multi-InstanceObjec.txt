 In modern data mining applications, the complexity of analyzed data objects is increasing rapidly. Molecu les are analyzed more precisely and with respect to all of their possible spatial conformations [1]. Earth observation satellites are able to take images with higher resolutions and in a variety of spectra which was not possible some years before. Data mining started to analyze complete websites instead of single documents [2]. All of these application domains are examples for which the complexity demands a richer object representation than single feature vectors. Thus, for these application dom ains, an object is often described as a set of feature vectors or a multi-instance (M I) object. For example, a molecule can be represented by a set of feature vectors w here each vector describes one spatial conformation or a website can be analyzed as a set of word vectors corresponding to its HTML documents.

As a result the research community started to develop techniques for multi-instance learning that where capable to analyze multi-instance objects. One of the first publications in this area [1, 3] was focussed to a special task called multi-instance learning. In this task the appearance of one positive instance within a multi-instance object is sufficient to indicate that the object belongs to the positive class. Besides classical multi-instance learning, some approaches like [4, 5] aim at more general problems. However, all of the mentioned approaches are based on a setting having a set of labeled bags to train a learning algorithms.
In this paper, we focus on clustering unlabeled sets of feature vectors. To cluster those objects, the common approach so far is to select some distance measures for point sets like [6, 7] and then apply a distance-based clustering al-gorithm e.g. k-medoid methods like CLARANS [8] or a density-based algorithm like DBSCAN[9]. However, this approach does not yield expressive cluster mod-els. Depending on the used algorithm, we might have some representative for some cluster, but we do not have a good model for describing the mechanism behind this clustering. To overcome this problem, we will refer to the model of multi-instance objects that was introduced in [5] stating that a multi-instance object of a particular class (or in our problem each cluster) needs to provide instances belonging to a cer tain concept or several co ncepts. We will adapt this view of multi-instance obj ects to clustering. Therefor e, we propose a statistical model that is based on 2 steps. In the first step, we use a standard EM Cluster-ing algorithm on the union set of all mult i-instance objects. Thus, we determine a mixture model describing the instances of all multi-instance objects. Assum-ing that each of the found clusters within each mixture model corresponds to some valid concept, we now can derive distributions for the clustering of multi-instance objects. For this second step , we assume that a multi-instance object containing k instances can be modeled as k draws from the mixture model over the instances. Thus, each cluster of mul ti-instance objects is described by a distribution over the instance clusters derived in the first step and some prior probability. For example, for the classical multi-instance learning task, it can be expected that there is at least one ins tance cluster that is very unlikely to appear in the multi-instance clusters corresponding to the negative bags.
The rest of the paper is organized as following: In section 2, we will survey previous work in data mining with multi-instance objects and give a brief in-troduction to EM clustering. Section 3 will describe our statistical model for multi-instance data. In section 4, this model is employed for EM clustering. To demonstrate the usefulness of our approach, section 5 contains the results on several real world data sets. Section 6 concludes the paper with a summary and directions for future work. Data Mining in multi-instance objects h as so far been predominantly examined in the classification section. In [1] Dietterich et al. defined the problem of multi-instance learning for drug prediction a nd provided a specialized algorithm to solve this particular task by learning axis parallel rectangles. In the following years, new algorithms increasing the performance for this special task were in-troduces [3]. In [5] a more general method for handling multi-instance objects was introduced that is applicable for a wider variety of multi-instance problems. This model considers several concepts fo r each class and requires certain cardi-nalities for the instances belonging to the concepts in order to specify a class of MI objects. Additionally, to this model [10] proposes more general kernel functions for MI comparing MI objects.

For clustering multi-instance objects, it is possible to use distance functions for sets of objects like [6, 7]. Having such a distance measure, it is possible to cluster multi-instance objects with k-medoid methods like PAM and CLARANS [11] or employ density-based clustering approaches like DBSCAN [9]. Though this method yields the possibility to partition multi-instance objects into clus-ters, the clustering model consists of representative objects in the best case. Another problem of this approach is that the selection of a meaningful dis-tance measure has an important impact of the resulting clustering. For example, netflow-distance [7] demands that all instances within two compared objects are somehow similar, whereas for the minimal Hausdorff [12] distance the indication of similarity is only dependent on the closest pair.

In this paper, we introduce an algorithm for clustering multi-instance objects that optimizes probability distributions to describe the data set. Part of this work is based on expectation maximization (EM) clustering for ordinary feature vectors using Gaussians. Details about this algorithm can be found in [13]. In [14], a method for producing a good initial mixture is presented which is based on multiple sampling. It is empirically shown that using this method, the EM algorithm achieves accura te clustering results. In this section, we will introduce our model for multi-instance clustering. There-fore, we will first of all define the terms instance and multi-instance (MI) object. Definition 1 (instance and MI object). Let F be a feature space. Then, i  X  F is cal led an instance in F . A multi-instance (MI) object o in F is given by an arbitrary sized set of instances o = i 1 , .., i k with i j  X  F . To denote the unique MI object an instance i belongs to, we will write MiObj ( i ) .
 To cluster multi-instance objects using an EM approach, we first of all need a sta-tistical process that models sets of multi-instance objects. Since multi-instance objects consist of single instances in some feature space, we begin with modeling the data distribution in the feature space of instances. Therefore, we first of all define the instance set of a set of multi-instance objects: Definition 2 (Instance Set). Given a database DB of multi-instance Objects o = i 1 ,...,i k , the corresponding instance set I DB = DB o is the union of all multi-instance objects.
 To model the data distribution in the instance space, we assume a mixture model of k independent statistical processes. For example, an instance set consisting of feature vectors could be described by a mixture of Gaussians.
 Definition 3 (Instance Model). Let DB be a data set consisting of multi-instance objects o and let I DB be its instance set. Then, an instance model IM for DB is given by a mixture model of k statistical processes that can be described by a prior probability Pr [ k j ] for each component k j and the necessary parameters for the process corresponding to k j , e.g. a mean vector  X  j and co-variance matrix M j for Gaussian processes. After describing the instance set, we ca n now turn to the description of multi-instance objects. Our solution is based on the idea of modeling a cluster of multi-instance objects as a multinomial distribution over the components of the mixture model of instances. For each instance and each concept, the probability that the instance belongs to this concept is considered as result of one draw. If the number n of instances within an object o is considered to be important as well, we can integrate this into our model as well by considering some distribution over the number of draws, e.g. a binomial distribution. To conclude, a mixture model of multi-instance clusters can be described by a set of multinomial distributions over the components of a mixture model of instances. A multi-instance object is thus derived in the following way: 1. Select a multi-instance cluster c i w.r.t. some prior distribution over the set 2. Derive the number of instances n within the multi-instance object w.r.t some 3. Repeat n -times:
Formally, the underlying model for multi-instance data sets can be defined as follows: Definition 4 (Multi-Instance Model). A multi-instance model M over the instance model IM is defined by a set C of l processes over I DB . Each of these number of instances in the bag Pr [ Card ( o ) | c i ] and an conditional probability describing the likelihood that a multi-instance object o belonging to process c i contains an instance belonging to the component k l  X  IM . The probability of an object o in the model M is calculated as following:
The conditional probability of process c i under the condition of a given multi-instance object o can be calculated by: Let us note that the occurrence of an instance within the data object is only dependent on the cluster of instances it is derived from. Thus, we do not assume any dependencies between the instances of the same objects. Another important characteristic of the model is that we assume the same set of instance clusters for all multi-instance clusters. Figure 3 displays an example of a two dimensional multi-instance data set corresponding to this model. This assumption leads to the following 3 step approach for multi-instance EM clustering.
 After introducing a general statistical pro cess for multi-instance objects, we will now introduce an EM algorithm that fits the distribution parameters to a given set of multi-instance objects. Our method works in 3 steps: 1. Derive a Mixture Model for the Instance Set. 2. Calculate a start partitioning. 3. Use the new EM algorithm to optimize the start partitioning. 4.1 Generating a Mixture Model for the Instance Set To find a mixture of the instance space, we can employ a standard EM approach as proposed in section 2. For general featu re vectors, we can describe the instance set as a mixture of Gaussians. If the fea ture space is sparse using a mixture of multinomial processes usually provides better results. If the number of clusters in the instance is already known, we can simply employ EM clustering. However, if we do not know how many clusters are hidden within the instance set, we need to employ a method for determining a suitable number of processes like [15]. 4.2 Finding a Start Partitioning of Multi-Instance Objects After deriving a description of the in stance space, we now determine a good start partitioning for the final clustering step. A good start partitioning is very important for finding a good cluster model. Since EM algorithms usually do not achieve a global maximum likelihood, a suitable start partitioning has an important impact on both, the likelihood of the cluster and the runtime of the algorithm. The versions for EM in ordinary feature spaces often use k -means clustering for finding a suitable start partitioning. However, since we cluster sets of instances instead of single instances, we cannot use this approach directly.
To overcome this proble m, we proceed as follows. For each multi-instance object we determine a so-called confidence summary vector in the following way. Definition 5 (Confidence Summary Vector). Let IM be an instance model over database DB containing k processes and let o be a multi-instance object. Then the confidence summary vector is calculated as follows: After building the confidence summary vector for each object, we can now em-ploy k -means to cluster the multi-instance objects. Though the resulting clus-tering might not be optimal, the objects within one cluster should yield similar distributions over the components of the underlying instance model. 4.3 EM for Clustering Multi-Instance Objects In this final step, the start partitioning for the data set is optimized using the EM algorithm. We therefore describe a suitable expectation and maximization step and then employ an iterative method. The likelihood of the complete model M can be calculated by adding up the log-likelihoods of the occurrence of each data object in each clusters. Thus, our model is (locally) optimal if we obtain a maximum for the the following log-likelihood term.
 Definition 6 (Log-Likelihood for M).
 To determine Pr [ c i | o ], we proceed as mentioned in definition 4. Thus, we can easily calculate E ( M ) in the expectation step for a given set of distribution parameters and an instance model. To improve the distribution parameters, we employ the following updates to the distribution parameters in the maximization step: where W c i denotes the prior probability of a cluster of multi-instance objects.
To estimate the number of instances co ntained in an MI object belonging to cluster c i , we can employ a binomial distribution determined by the parameter l . The parameters are updated as follows: where MAXLENGTH is the maximum number of instances for any MI object in the database.

Finally, to estimate the relative number of instances drawn from concept k j for MI objects belonging to cluster c i , we derive the parameter updates in the following way:
Using these update steps, the algorithm is terminated after the improvement of E ( M ) is less than a given value  X  . Since the last step of our algorithm is a modification of EM clustering based on multinomial processes, our algorithm always converges against a local maximum value for E ( M ). All algorithms are implemented in Java 1 .5. The experiments described below are carried out on a work station that is equipped with two 1.8 GHz Opteron processors and 8 GB main memory.

Our experiments were performed on 3 different real world data sets. The prop-erties of each test bed are illustrated in Table 1. The Brenda data set contains of enzymes taken from the protein data bank (PDB) 1 . Each enzyme comprises several chains given by amino acid seque nces. In order to derive feature vectors from the amino acid sequences, we emplo yed the approach described in [16]. The basic idea is to use local (20 amino acids) and global (6 exchange groups) characterization of amino acid sequences. In order to construct a meaningful fea-ture space, we formed all possible 1-gra ms for each kind of characteristic. This approach provided us with 26 dimensional histograms for each chain. To obtain the class labels for each enzyme we used a mapping from PDB to the enzyme class numbers from the comprehensive enzyme information system BRENDA 2 .
MUSK 1 and MUSK 2 data sets come from UCI repository [17] and describe a set of molecules. The MI-objects in MUSK 1 and MUSK 2 data sets are judged by human experts to be in musks or non-m usks class. The feature vectors of MUSK data sets have 166 numerical attributes that describe these molecules depending on the exact shape or conformation of the molecule.

To measure the effectiveness, we consid ered the agreement of the calculated clusterings to the given class systems. To do so, we calculated three quality measures namely precision, F-measure and average entropy. In order to calculate the precision and F-Mea sure, we proceeded as follows. For each cluster c i found by a clustering algorithm, its class assignment Class ( c i ) is determined by the class label of objects belonging to c i that are in the majority. Then, we calculated the Precision within all clusters w.r.t. the determined class assignments by using the following formulas.
 In addition, we measured the average entropy over all clusters. This quality measure is based on the impurity of a cluster c i w.r.t. the class labels of objects belonging to c i .Let p j,i be the relative frequency of the class label Class j in the cluster c i . We calculate average entropy as following.

In order to demonstrate that the proposed clustering approach for multi-instance objects outperforms standard clustering algorithms working on a suit-able distance functions, we compared precision, F-Measure and average entropy of the MI-EM with that of k -medoid clustering algorithm (PAM). To enable clus-ter analysis of multi-instance objects by PAM, we used the Hausdorff distance (HD)[6], the minimum Hausdorff distance (mHD)[12] and the Sum of Minimum Distances (SMD)[6]. Due to the fact that the data set DS1 has 6 classes and the data sets DS2 and DS3 have 2 classes, we investigated the effectiveness of the cluster analysis where the number of clusters is equal to or slightly than the number of the desired classes. Thus, we set in our experiments the number of clusters equal to 6 and 8 for DS1, and equal to 2, 6 and 8 for the data sets DS2 and DS3. The results of our comparison are illustrated in Figures 1,3 and 2.
In all our experiments, PAM working on distance functions suitable for multi-instance objects achieved a significantly lower precision than MI-EM. For exam-ple, the MI-EM algorithm reached a precision of 0.833 on DS1 and the number of clusters equal to 8 (cf. Figure 2(a)). In contrast to the result of MI-EM, the precision calculated for clusterings found by all competitors lies between 0.478 and 0.48. Furthermore, MI-EM obtained in all experiments higher or compara-ble values of F-Measures. This fact indicates that the cluster structure found by applying of the proposed EM-based approach is more exact w.r.t. precision and recall than that found by PAM with 3 different MI distance functions. For ex-ample, the F-Measure calculated for MI-EM clustering of DS2 with 8 clusters is 0.63 whereas PAM clustering with different MI distance functions shows values between 0.341 and 0.41 (cf. Figure 2(b)). Finally, the values of average entropy observed by the MI-EM results are considerably lower than those of PAM on HD, mHD and SMD. The lower values of average entropy imply a lower level of impurity in the cluster structures detected by applying MI-EM.

To summarize, the values of the different quality measures observed on real world data sets when varying the number of clusters show that the proposed EM-based approach for cluster analysis of MI-objects outperforms the considered competitors w.r.t. effectiveness. In this paper, we described an approach for statistical clustering of MI objects. Our approach models instances as members of concepts in some underlying fea-ture space. Each concept is modeled by a sta tistical process in this feature space, e.g. a Gaussian. A multi-instance object can now be considered as the result of selecting several times a concept and ge nerating an instance with the corre-sponding process. Clusters of multi-in stance objects can now be described as multinomial distributions over the concepts. In other words, different clusters are described by having different probabilities for the underlying concepts. An additional aspect is the length of the MI object. To derive MI clusters corre-sponding to this model, we introduce a three step approach. In the first step we derive a mixture model describing con cepts in the instanc e space. The second step finds a good initialization for the target distribution by subsuming each MI object by a so-called confidence summary vector (csv) and afterwards cluster-ing these csvs using the k -means method. In the final, step we employ a final EM clustering step optimizing the distr ibution for each cluster of MI objects. To evaluate our method, we compared ou r clustering approach to clustering MI objects with the k -medoid clustering algorithm PAM for 3 different similarity measures. The results demonstrate that the found clustering model offers better cluster qualities w.r.t. to the provided reference clusterings.
