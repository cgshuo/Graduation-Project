 Labeling text data is quite time-consuming but essential for automatic text classification. Especially, manually creating multiple labels for each document may become impractical when a very large amount of data is needed for training multi-label text classifiers. To minimize the human-labeling efforts, we propose a novel multi-label active learning ap-proach which can reduce the required labeled data with-out sacrificing the classification accuracy. Traditional active learning algorithms can only handle single-label problems, that is, each data is restricted to have one label. Our ap-proach takes into account the multi-label information, and select the unlabeled data which can lead to the largest re-duction of the expected model loss. Specifically, the model loss is approximated by the size of version space, and the reduction rate of the size of version space is optimized with Support Vector Machines (SVM). An effective label predic-tion method is designed to predict possible labels for each unlabeled data point, and the expected loss for multi-label data is approximated by summing up losses on all labels according to the most confident result of label prediction. Experiments on several real-world data sets (all are pub-licly available) demonstrate that our approach can obtain promising classification result with much fewer labeled data than state-of-the-art methods.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval; I.5.2 [ Design Methodology ]: Classifier Design and Evaluation Algorithms, Performance, Experimentation
This work was finished when the first author conducted her internship at Microsoft Research Asia.
 Active Learning, Text Classification, Multi-label Classifica-tion, Support Vector Machines
As text data becomes a major information source in our daily life, many research efforts have been conducted in text classification to better organize text data, in applications like document filtering, email classification, Web search, etc. In particular, multi-label text classification problems have received considerable attention, since many text classifica-tion tasks are multi-labeled, i.e., each document can belong to more than one category. Take news classification as an ex-ample, one news article talking about the effect of Olympic games on tourism industry might belong to the following topic categories: sports, economy and travel .

In the literature, supervised learning algorithms are widely used in text classification. It requires a sufficient amount of labeled data for training a high quality model. However, labeling is usually a time-consuming and expensive process done by domain experts. Active learning is an approach to reduce the labeling cost. The active learner iteratively se-lects a sample of data to label based on some selection strate-gies suggesting that the data most deserves to be labeled. Thus it can achieve comparable performance with supervised learners while using much less labeled data. Active learning is particularly important for the multi-label text classifica-tion task. The reason is that, in the single-label case, a human judge can stop labeling an instance once its category is identified. But in the multi-label case, human judges need to decide all possible categories for each instance. Thus the effort of assigning labels for multi-label data is much larger than for the single-label data.

Despite the value and significance of this problem, there is very limited research on multi-label active learning. Most of the active learning research focuses on the single-label classification problem [10, 21, 14, 22]. The sample selec-tion strategy strictly follows the assumption that each in-stance has only one label, and thus cannot directly applied in multi-label active learning. The reason can be explained by the following example. Suppose there are three categories c , c 2 , c 3 in the multi-label classification task. The popu-lar one-versus-all technique [3] is used and the classification probabilities on all possible classes are given. Assume the probabilities on instance x 1 are [ c 1 :0.8, c 2 :0.5, c x 2 is [ c 1 :0.7, c 2 :0.1, c 3 :0.1]. x 1 actually has two labels c c , and x 2 has one label c 1 . It can be found that correctly predicting labels for x 1 is harder than x 2 . However, if we assume each instance only has one label and take the most uncertainty strategy, x 2 would be considered to be harder to classify, since the probability score on the predicted label of x 2 is 0 . 7, which is lower than that of x 1 0 . 8. Thus consider-ing multi-label information in the sample selection strategy is very important.

In this paper, we propose a novel multi-label active learn-ing approach for text classification. The sample selection strategy aims to label data which can help maximize the reduction rate of the expected model loss. To measure the loss reduction, we use Support Vector Machines (SVM) in terms of version space [21] due to the effectiveness of SVM active learning on text classification. In the original work, the loss is modeled for single-label case, and here we ex-tend it to multi-label case. We also propose an effective method to predict labels for multi-label data. The expected loss is approximated with the loss associated with the most confident result of label prediction. We will show that a proper label prediction method is critical in measuring loss for multi-label data.

We empirically evaluate the effectiveness of the proposed approach using several real-world data sets that are pub-licly available. The results demonstrate that our method is superior to the state-of-the-art active learning algorithms for multi-label text classification, and can significantly re-duce the demand of labeled data while maintaining promis-ing classification results.

The remainder of this paper is organized as follows. Sec-tion 2 discusses related work. Section 3 presents the defi-nition of multi-label active learning problem. Section 4 in-troduces our SVM-based active learner, including the loss optimization framework and the sample selection strategy. Section 5 shows experimental results of our algorithm on several real-world data sets compared with other baseline methods. Section 6 presents conclusions and future work.
Active learning on text classification has been well re-searched. Based on the adopted sample selection strategy, they can be grouped into three types: 1) Uncertainty sam-pling [10, 14]. The active learner iteratively labels the un-labeled data on which the current hypothesis is most uncer-tain. 2) Expected-error reduction [2, 18, 22]. The strategy aims to label data to minimize the expected error on the unlabeled data. Usually it requires expensive computational effort on estimating the expected error, since each of the un-labeled data associated with each possible labeling needs to be evaluated. 3) Committee-based active learner. It has the similar idea with uncertainty sampling strategy. The active learner selects data to label that have the largest disagree-ment among several committee members (classifiers) from the version space. The work of query by committee [19] is the first algorithm of this kind. In [21], the idea is extended to Support Vector Machine active learning, and it models the reduction of version space size with SVM.

However, most of the previous research targets single-label classification problems. The sample selection strategy eval-uates each unlabeled data by assuming it has only one label. For instance, the uncertainty sampling strategy will focus on measuring the confidence of the most likely class, and the error reduction strategy will estimate the expected error by just considering single-label cases. Thus these strategies can not be directly applied in multi-label text classification.
There is very limited research on multi-label active learn-ing. The research work of [9] is the one most related to our paper with respect to the studied problem. It decomposes the multi-label classification problem to several binary ones using one-versus-all approach. The selection strategy mini-mizes the smallest SVM margin among all binary classifica-tion problems. The approach does not consider the multi-label information, and treats all classes equally. In [12], an SVM active learning method was proposed for multi-label image classification. It selects unlabeled data which has the maximum mean loss value over the predicted classes. The multi-label classification problem is also viewed as several binary classification tasks. A threshold of loss value is esti-mated for each binary classifier, and then used to decide the predicted classes for unlabeled data. According to our exper-iments, this threshold cutting method is not effective on the text classification data sets we used. Also for image classifi-cation, [16, 17] developed a two-dimensional active learning algorithm, which selects sample-label pairs to minimize the Bayesian classification error bound. It is reasonable to label picture-category pairs since judging a picture X  X  label is very efficient. However, this method is not suitable for text clas-sification task. Because it will introduce much additional cost if a document is read several times. Obviously, the cost of reading a document and judging its label is much bigger than that of a picture. Recently, [4] proposed several active learning strategies for multi-label text classification. Each selection strategy consists of one rule to combine the output of individual binary classifiers, including three orthogonal dimensions:  X  X vidence X ,  X  X lass X  and  X  X eight X . Also, they do not take account of the label prediction result for each in-stance in the selection strategy.
Multi-label text classification is the task of automatically classifying text documents into a subset of predefined classes. Denote training examples as x 1 , ..., x n and the k classes as 1 , ..., k . We represent the label set of x i by a binary vector y = [ y 1 i , ..., y k i ] , y j i  X  { X  1 , +1 } , where y j to class j , otherwise y j i =  X  1. Denote the set of all pos-sible class combinations as Y ( |Y| = 2 k ). The multi-label classifier can be expressed as a decision function f : X  X  X  .
In our active learning study, we consider SVM as the ba-sic multi-label classifier, since SVM has demonstrated sig-nificant success on text classification tasks [7, 23]. Usu-ally, multi-label SVM adopts the one-versus-all approach, which trains a separate binary classifier for each possible class against the rest of classes, and combines the output of all the binary classifiers to determine the final labels of the given data. In binary classification, SVM tries to find the hyperplane that can separate the training data by a max-imal margin. Denote f i as the binary classifier associated with target class i . Given a test instance x 0 , if f i ( x then x 0 belongs to class i , otherwise, the labels of x 0 include class i .

In this paper, we adopt the pool-based active learning ap-proach which is usually used in the literature. Assume we are given a pool of partially labeled data. Denote the data with labels by D l , which is typically small in size, and the remaining data without labels by D u . At the beginning, a classifier is trained using the initial labeled set D l . Based on this classifier, the learner selects a sample from D u queries for its true labels according to some criterion. Then the newly labeled data is incorporated into D l . The training and labeling process runs iteratively after a certain number of iterations or when the classifier reaches a sufficient accu-racy.

The key issue of active learning is how to select the most informative data examples to label, which is also called sam-ple selection strategy. So, the research problem studied in this work can be described as follows: in order to train an effective multi-label active learner, how to design the sam-ple selection strategy to reduce the human labeling cost as much as possible?
In this section, we will first introduce the optimization framework for multi-label active learning. Next we will de-scribe our sample selection strategy with SVM.
The optimization goal of our multi-label active learner is to label data which can contribute the largest reduction of the expected model loss.

Let P ( x ) be the input distribution. Denote the multi-label prediction function given training set D l as f D l predicted label set of x is f D l ( x ). Suppose the true label set of x is y , then the estimated loss on x can be writ-L ( f D l ) in the following discussion), and the expected loss of the learner can be expressed as follows: As it is rather difficult to estimate P ( x ) directly, a practical way to estimate d  X  D l is to measure it over all the examples in D u . Therefore we have The active learner will evaluate each possible set of unla-beled data D s to find the optimal query set D  X  s . When D obtains its labels, it can be incorporated to the training set. Denote the new training set as D 0 l = D l + D s , and the ex-pected loss for the classifier trained on D 0 l as d  X  D 0 mization problem is to find the optimal query set D  X  s , which once added, will generate the largest reduction on expected loss.
 As in [1], we assume that any x in D u  X  D s has equal impact on the learner trained from D l and D 0 l . Then we will have
According to Equation 4, the optimization problem can be divided into two parts: how to measure the loss reduction of the multi-label classifier and how to provide a good proba-bility estimation for the conditional probability p ( y | x ). We will address these two issues respectively in the following subsections.
As discussed in Section 3, we decompose the multi-label problem to one-versus-all subproblems and use SVM as the base binary classifier in active learning. By decomposing the classifier into several binary ones, the overall model loss can be measured by gathering the model loss of all binary classifiers.
 where l ( f i ) is the model loss of binary classifier f i problem becomes how to estimate the model loss of each binary classifier. As suggested by S. Tong et al. [21], we measure the model loss by the size of version space of a binary SVM. According to [21], the version space of SVM can be defined as follows:
V = { w  X  W | k w k = 1 , y i ( w  X  x i ) &gt; 0 , i = 1 , ..., n } (6) where W denotes the parameter space. The size of a version space is defined as the surface area of the hypersphere k w k = 1 in W .

Based on the work in [21], we can use SVM margin as the measure of the version space size. When a new labeled example is added, we can approximate the new version space size by computing the SVM margin of the updated classifier. However, it is too expensive in computation when each data in the unlabeled pool associated with each possible label set needs to be evaluated. To make it more practical, we apply the heuristics idea in [20] to simplify the approximation by mapping the SVM margin of the current classifier to the size of the new version space.

In multi-label settings, denote V i D space of the binary classifier f i D i and learnt from labeled data D l . After adding new data point ( x , y i ), where y i  X  X  X  1 , +1 } is the true label for data x on class i , the new model loss versus the old one on the binary classifier f i D Then the loss reduction part in Equation 4 can be re-written by: Note that l ( f i D beled example x , so we can focus on optimizing the reduction rate, which can be approximated as
Intuitively, the idea of the above estimation can be ex-plained as follows. Consider an unlabeled data example x , if x can be correctly predicted by the binary classifier f then the smaller the value of | f i ( x ) | is, the more uncertain the classifier is on x , and x deserves more to label. This is consistent with the result of the above measure, since x will contribute more in reducing the size of the version space. On the other hand, if the classifier provides wrong predic-tion result for x , then the larger | f i ( x ) | is, the more mistake the classifier will make, and in another view, adding x will greatly help reduce the size of the version space.
Now we come to the issue of estimating the conditional probability p ( y | x ), y  X  Y . Note that for k labels, there are 2 k possible label combinations. It is intractable for ac-tive learner to provide estimation on all these possibilities. Particularly, it will become harder when the training data is quite limited, which is common in active learning. To simplify the estimation, we approximate the expected loss with the loss on the most possible label combination, since the predicted labels with the largest confidence will be most likely to be correct. Thus the problem becomes how to pro-duce better label prediction on the unlabeled data. We pro-pose a novel prediction approach to address this problem. Instead of directly estimating the possible labels for each data, we first try to decide the possible label number each data may have, and then determine the final labels based on the probability on each label obtained by the corresponding binary classifier.

Suppose there are k classes. Using the one-versus-all ap-proach, we can have k binary classifiers. Given data x , de-note p ( y i = 1 | x ) as the probability of x belonging to class i . We can obtain k classification probabilities on x produced by the k binary classifiers. Sort these k probabilities in decreas-ing order. If x actually has m labels, the first m probabilities are expected to be large while the other k  X  m probabilities are expected to be small. Based on this assumption, we want to predict the number of labels for each data based on the probabilities output by the binary classifiers.
Specifically, we predict the number of labels by tackling a multi-class classification problem. Logistic regression (LR) algorithm is used to train a multi-class model and predict the probabilities of having different number of labels for each data. For k classes, there are k possible number of labels : 1 , ..., k . So we have k classes in the multi-class classification problem. Before LR is used, we transform the decision out-put on the training data to classification probabilities. Here, we use the sigmoid function [13] to transform the SVM out-put to probability values. For a data example x , we have where f i is the binary SVM classifier associated with class i , A and B are scalar values fit by maximum likelihood es-timation.

The process of predicting number of labels can be de-scribed as follows: 1. Use the SVM classifier to assign classification proba-2. For each instance x , sort the classification probabil-3. Train logistic regression classifier. For each training 4. For each data in the unlabeled pool, apply the LR Suppose the most possible number of labels for data x is m , and i 1 , ..., i m are the m classes associated with the largest probabilities produced by the m corresponding binary SVM classifiers. Then the predicted label vector b y can be rep-resented by the binary vector [ b y i 1 = 1 , .., b y i j = 1 , b y  X  1 , ..., b y i k =  X  1]. We call this approach LR  X  based label prediction.

By incorporating the predicted label vector into the ex-pected loss estimation, we obtain our data selection strategy, Maximum loss reduction with Maximal Confidence(MMC). It can be written as
Based on the above discussion, the proposed active learn-ing algorithm is described in Algorithm 1.
 Algorithm 1 Multi-label Active Learning Input: Labeled set D l Unlabeled set D u Number of classes k Number of iterations T Number of selected examples per iteration S 1: for t = 1 to T do 2: Train k binary SVM classifiers f 1 , ..., f k based on 3: for each instance x in D u do 4: Predict its label vector using the LR-based predic-5: Calculate the expected loss reduction with the 6: Sort score ( x ) in decreasing order for all x in D u 7: Select a set of S examples D  X  s with the largest scores,
In this section, we will evaluate our proposed multi-label active learning approach for multi-label text classification task on seven real-world data sets, comparing with the state-of-the-art active learning approaches. Table 1: Statistics on RCV1-V2 and Yahoo! Data Sets Arts&amp;Humanities 7,484 23,146 26 Business&amp;Economy 11,214 21,924 30 Computers&amp;Internet 12,444 34,096 33
The first data set 1 we used is the RCV1-V2 [11] text data set, which has been widely used as a benchmark data set to evaluate text classification algorithms. It contains Reuters newswire stories which are organized by three different cat-egory sets: Topics, Industries, and Regions. Each document is assigned with at least one label in the related category set. A sample of 3,000 documents in the Topics category set is chosen for our experiments, including 101 labels.
The other 6 data sets 2 are web pages collected through the hyperlinks from Yahoo! X  X  top directory (www.yahoo.com). They are used in [15, 8] to evaluate multi-label text clas-sification algorithms. Each data set is associated with one of Yahoo! X  X  top categories, and each page is labeled with one or more second level sub-categories. We choose 6 data sets for our experiments, which are Arts&amp;Humanities, Busi-ness&amp;Economy, Computers&amp;Internet, Education, Entertain-ment, and Health.

The details of all the 7 data sets are given in Table 1.  X #Samples X  X s the number of samples in each data set.  X #Fea-tures X  is the feature dimension of each data set.  X #Label X  is the number of labels in each data set.

On all data sets, the documents are transformed to vec-tors with TF-IDF format, and each vector has unit modulus with L-2 length normalization. One-versus-all classification is conducted for each category and the multi-label classi-fication problem is treated as several binary classification problems, where the documents from the target category are given positive label (i.e. y = 1 ), and the rest of the doc-uments are given negative label (i.e. y =  X  1). SV M Light package [7] is downloaded and used to train the binary clas-sifier. Linear kernel is used due to its good performance in text classification task [6]. The penalty parameter C is set to 1 . 0 by default.

In our active learning experiments on each data set, we first randomly selected a small set of documents to form the initial labeled set, and left the remaining documents as the unlabeled pool. Then the active learner selects a given num-ber of examples from the unlabeled pool in each iteration, and then add them to the labeled set with their labels. We performed several active learning iterations on each data set until the learner achieves sufficient accuracy. In every iter-ation, once the selected data being incorporated, the active learner retrained a new classifier on the expanded labeled set and its performance was evaluated on the remaining data ex-amples. We used Micro-Average F1 score as the evaluation http://trec.nist.gov/data/reuters/reuters.html http://www.kecl.ntt.co.jp/as/members/ueda/yahoo.tar.gz measure, since it is a standard evaluation used in most previ-ous text classification research. As defined in [23], micro-F1 score in multi-label case is given as follows where n is the number of test data, y i is the true label vec-tor of the i -th data instance, y j i = 1 if the instance belongs to category j ; otherwise y j i =  X  1. b y i is the predicted la-bel vector. We computed the average of micro-F1 scores for each active learning iteration based on 10 randomized experiments.

In our experiments, we will evaluate and compare four active learning methods:
In this section, we will present and discuss the experiment results on the RCV1-V2 data set as well as the 6 Yahoo! data sets. Figure 1: Comparison between label prediction methods on RCV1-V2 data set (no active learning)
In the first experiment, we would like to verify whether our method of label prediction (presented in Section 4.2.2) is effective when only a small amount of training data is available, as this is very typical in active learning. Two popular prediction methods for multi-label classification are implemented for comparison purposes. In previous studies, the SCut method is widely used and proved very effective for predicting labels in multi-label classification tasks [11]. In [11], a binary classifier is first trained for each label. A threshold score is tuned for each binary classification task and then used to decide if an unlabeled data example be-longs to the corresponding class or not. The second pre-diction method is simply setting the threshold score to be zero for each binary problem. If the classification score is positive, then the data belongs to this class, and vice versa. This simple method has its theoretical foundation, as when SVM is used, zero score corresponds with the classification hyperplane induced from statistical learning theory.
In order to verify the effectiveness of the LR-based method in predicting labels, we varied the number of training data from 100 to 1,000 (with 100 as step size). The correspond-ing micro-F1 curves for predicting labels are plot in Fig-ure 1. We can observe that, as the number of training data varies, the LR-based method achieves substantially better performance than both baseline methods. When less train-ing data is available, the advantage of LR is more obvious. This demonstrates that the LR-based method is more effec-tive for label prediction in multi-label active learning frame-work.

In the following we will report the active learning experi-ment results. We randomly selected 500 examples as the ini-tial labeled data. Active learning was iteratively performed for 50 iterations, selecting 20 examples from the unlabeled pool each time. Figure 2 and Table 2 show the experimen-tal results of micro-F1 scores averaging over 10 random tri-als. The proposed MMC strategy outperforms other base-line methods by a large margin. Surprisingly, we can see that MML performs even worse than Random at the begin-ning, and worse than MMC and BinMin for all cases. Since MML adopts the same label prediction approach as MMC, the observation above indicates that the loss optimization Figure 2: Micro-F1 score on RCV1-V2 data set Table 2: Micro-F1 score at different iterations on RCV1-V2 data set(%) approach used in MML is not effective on multi-label text data. Instead, our approach optimizes the loss reduction rate over all labels based on the most confident label vec-tor, and it can successfully pick out useful data examples to label. We can also find that our proposed method out-performs all baseline methods more significantly than Bin-Min. An explanation is that the BinMin strategy does not take advantage of the multi-label information, while our ap-proach effectively estimates possible labels for each instance and incorporates the multi-label information to optimize the expected loss reduction.

Table 2 shows the performance results with the number of training samples added. We can find that as the number of selected data increases, the improvement becomes more and more significant. For example, when 1,000 examples are added, the micro-F1 score of our method achieves 82.88%, while that of BinMin, MML and Random are 77.19%, 75.77% and 75.12% respectively. We can find that MMC achieves the similar performance with BinMin by using about 600 se-lected examples, while BinMin needs to select 1,000 exam-ples. It indicates that MMC can save about 40% labeling effort compared with BinMin.

In order to investigate if our MMC algorithm is sensitive to the size of initial labeled data set, we varied the number of initial training data from 100 to 1,000, with 100 as step size. For each fixed initial labeled set, we applied active learning and selected 20 examples at each iteration. Then we com-Figure 3: Micro-F1 score on RCV1-V2 data set after adding 1000 examples Figure 4: Micro-F1 score of MMC on RCV1-V2 data set with different sampling sizes per run pared the performance of the final classifier after 50 active learning iterations. Figure 3 presents the micro-F1 scores of final classifiers with the size of initial training data set. We can see that our proposed MMC algorithm consistently outperforms all other methods when the initial training data set varies in size. The consistent improvement indicates that our MMC strategy is robust with different size of the initial labeled data set.

We also varied the sampling size per run and investigated its impact on the performance of the active learner. In this experiment, we started with 500 training examples and stopped after 1,000 examples are added. The sampling size S was set to 1, 20, 50, 100 and 200. The results of MMC with various sampling size are depicted in Figure 4. We can see that generally the performance improves as the sampling size decreases. A possible explanation is that having more chances to query labels enables the learner to make better evaluation on unlabeled examples, and to choose more in-formative examples to label.
 Table 3: Micro-F1 score on the Yahoo! data sets with 2,500 training samples added (%) Data sets MMC BinMin MML Random Arts&amp;Humanities 65.03 61.67 60.26 58.74 Business&amp;Economy 80.54 78.37 77.08 75.90 Computers&amp;Internet 77.13 75.05 74.37 73.94 Education 71.28 69.29 66.97 67.65 Entertainment 75.46 74.46 73.14 70.82 Health 81.05 79.56 74.74 74.60
The following experiments are conducted with the 6 Ya-hoo! data sets. On each data set, we randomly selected 500 data instances as the initial training data, and set the sampling size in each active learning run to 50. The learn-ing process was repeated for 50 rounds. The active learning results were averaged over 10 random trials. Fig 5 presents the performance of all active learners with the number of training data added. We can observe that our proposed method MMC outperforms other baseline methods on all six data sets. The most noticeable case is the Comput-ers&amp;Internet data set, where the BinMin method only pro-vides slight improvement over the Random method. How-ever, MMC achieves substantially better performance. It can be observed that MMC only requires labeling 900 ex-amples to achieve the similar performance with BinMin and MML which require labeling about 1,600 and 2,100 exam-ples respectively. We can also see that MML has worse performance compared with Random on most of the cases. This implies that the loss optimization framework of MML is worse than that of MMC on multi-label text data. Com-pared with MMC, BinMin is less effective to enhance the ac-tive learner as the training example grows. This underscores the importance of considering multi-label information when evaluating unlabeled examples. The promising results of MMC confirm that the proposed method can provide proper evaluation on the unlabeled data examples, and select the informative ones which can help enhance the learner more ef-fectively. Table 3 summarizes the classification results mea-sured by micro-F1 after 50 active learning iterations on the six Yahoo! data sets. It shows that the proposed MMC method provides more favorable performance than all other baseline methods for all six data sets, and the improvement of MMC over Random is more significant than that of Bin-Min and MML.

From the above experiments, we can observe that MMC provides promising performance on diverse data sets. This indicates that it is more effective and robust for training multi-label text classifier than the state-of-the-art active learn-ing methods.
In this paper, we try to address the problem of multi-label active learning for text classification. The goal is to reduce the required size of labeled data in multi-label classifica-tion while maintaining favorable accuracy performance. We propose a novel multi-label active learning algorithm with Support Vector Machines (SVM). The optimization goal is to select data to label which can maximize the reduction in the expected model loss. Our approach provides proper ap-proximation on the loss reduction and the expected loss in the optimization framework. Experiments on several real-world data sets show that our proposed method outperforms the state-of-the art active learning techniques on multi-label text classification by a large margin and can significantly re-duce the labeling cost.

Note that our active learning approach should evaluate each of the unlabeled data at every active learning itera-tion. The computation would be expensive when the size of unlabeled pool is very large and the number of categories is very big. So it would be interesting to study how to evaluate only a subset of the unlabeled pool and also be able to pick out informative data to label. We plan to explore this exten-sion in the future. Also, we will apply our method on other multi-label classification tasks, e.g., image classification. We express our grateful thanks to Prof. Jian Pei from Simon Fraser University for his valuable suggestions on this work.
