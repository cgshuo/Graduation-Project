 Document clustering is one of the oldest and most studied problems of information retrieval (van Rijsbergen, 1979). Almost all document clustering approaches to date have represented documents as vectors in a bag-of-words vec-tor space model , where each dimension of a document vector corresponds to a term in the corpus (Salton and McGill, 1983). General clustering algorithms are then applied to these vectors to cluster the given corpus. There have been attempts to use bigrams or even higher-order n-grams to represent documents in text categorization, the supervised counterpart of document clustering, with little success (Caropreso et al., 2001; Tan et al., 2002).
Clustering can be viewed as partitioning a set of data objects into groups such that the similarities between the objects in a same group is high while inter-group simi-larities are weaker. The fundamental assumption in this work is that the documents that are likely to have been generated from similar language models are likely to be in the same cluster . Under this assumption, we propose a new representation for document vectors specifically de-signed for clustering purposes.

Given a corpus, we are interested in the generation probabilities of a document based on the language models induced by other documents in the corpus. Using these probabilities, we propose a vector representation where each dimension of a document vector corresponds to a document in the corpus instead of a term in the classical representation. In other words, our document vectors are -dimensional, where is the number of documents in the corpus to be clustered. For the vector of docu-ment , the th element of is closely related to the generation probability of based on the language model induced by document . The main steps of our method are as follows: 2.1 Language Models The language modeling approach to information retrieval was first introduced by Ponte and Croft (1998) as an al-ternative (or an improvement) to the traditional relevance models. In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities. For each term that occurs in a document , the ML estimation of with respect to is defined as where is the number of occurences of term in document . This estimation is often smoothed based on the following general formula: where an entire corpus which usually is a member of. general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two im-portant roles (Zhai and Lafferty, 2004). First, it accounts for terms unseen in the document preventing zero prob-abilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has an -like effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai and Lafferty, 2004; Liu and Croft, 2004): Here, mean more aggressive smoothing.

Assuming the terms in a text are independent from each other, the generation probability of a text sequence given the document is the product of the generation probabilities of the terms of
In the context of information retrieval, usually composed of few terms. In this work, we are interested in the generation probabilities of entire docu-ments that usually have in the order of hundreds of unique terms. If we use Equation 1, we end up having unnatural probabilities which are irrepresentably small and cause floating point underflow. More importantly, longer docu-ments tend to have much smaller generation probabilities no matter how closely related they are to the generating language model. However, as we are interested in the generation probabilities between all pairs of documents, we want to be able to compare two different generation probabilities from a fixed language model regardless of the target document sizes. This is not a problem in the classical document retrieval setting since the given query is fixed, and generation probabilities for different queries are not compared against each other. To address these problems, following (Lavrenko et al., 2002; Kurland and Lee, 2005), we  X  X latten X  the probabilities by normalizing them with respect to the document size: where us with meaningful values which are comparable among documents of different sizes. 2.2 Using Generation Probabilities as Document Equation 2 suggests a representation of the relation-ship of a document with the other documents in a corpus. Given a corpus of documents to cluster, we form an -dimensional generation vector
We can use these generation vectors in any clustering algorithm we prefer instead of the classical term-based clearer when we consider the underlying directed graph representation, where each document is a node and the weight of the link from to is equal to flat .
 An appropriate analogy here is the citation graph of sci-entific papers. The generation graph can be viewed as a model where documents cite each other. However, un-like real citations, the generation links are weighted and automatically induced from the content.

The similarity function used in a clustering algorithm over the generation vectors becomes a measure of struc-tural similarity of two nodes in the generation graph. Work on bibliometrics uses various similarity metrics to assess the relatedness of scientific papers by looking at the citation vectors (Boyack et al., 2005). Graph-based similarity metrics are also used to detect semantic simi-larity of two documents on the Web (Maguitman et al., 2005). Cosine, also the standard metric used in based document clustering, is one of these metrics. In-tuitively, the cosine of the citation vectors (i.e. vector of outgoing link weights) of two nodes is high when they link to similar sets of nodes with similar link weights. Hence, the cosine of two generation vectors is a measure of how likely two documents are generated from the same documents X  language models.

The generation probability in Equation 2 with a smoothed language model is never zero. This creates two potential problems if we want to use the vector of Equa-tion 3 directly in a clustering algorithm. First, we only want strong generation links to contribute in the similar-ity function since a low generation probability is not an evidence for semantic relatedness. This intuition is sim-ilar to throwing out the stopwords from the documents before constructing the vectors to avoid coinci-dental similarities between documents. Second, having a dense vector with lots of non-zero elements will cause efficiency problems. Vector length is assumed to be a constant factor in analyzing the complexity of the clus-tering algorithms. However, our generation vectors are -dimensional, where is the number of documents. In other words, vector size is not a constant factor anymore, which causes a problem of scalability to large data sets. To address these problems, we use what Kurland and Lee (2005) define as top generators : Given a document , we consider only eration probabilities and discard others. The resultant -dimensional vector, denoted elements, which are the largest given constant tain operations (e.g. cosine) on such vectors can be done in constant time independent of . 2.3 Reinforcing Links with Random Walks Generation probabilities are only an approximation of se-mantic relatedness. Using the underlying directed graph interpretation of the generation probabilities, we aim to get better approximations by accumulating the generation link information in the graph. We start with some defini-tions. We denote a (directed) graph as is the set of nodes and weight function . We formally define a generation graph as follows: Definition 1 Given a corpus documents, and a constant is a directed graph Definition 2 A -step random walk on a graph that starts at node as node For example, for a generation graph 1-step random walks that start at a given node with probabilities proportional to the weights of the outgoing generation links of that node.
 Suppose there are three documents generation graph. Suppose also that there are  X  X trong X  generation links from from cally related to generation link between them depending on model. We approximate this relation by considering the probabilities of 2-step (or longer) random walks from to .
 Let walk starts at of random walks is that for a given node depend on walk ending up at on its starting point (Seneta, 1981). This limiting prob-ability distribution of an infinite random walk over the nodes is called the stationary distribution of the graph. The stationary distribution is uninteresting to us for clus-tering purposes since it gives an information related to the global structure of the graph. It is often used as a measure to rank the structural importance of the nodes in a graph (Brin and Page, 1998). For clustering, we are more inter-ested in the local similarities inside a  X  X luster X  of nodes that separate them from the rest of the graph. Further-more, the generation probabilities lose their significance during long random walks since they get multiplied at each step. Therefore, we compute . Finally, we define the following: Definition 3 The -step generation probability of docu-ment from the language model of : the -step generation vector of document . We will often write talking about the vector of a specific document. gen that starts at will visit in or fewer steps. It helps us to discover  X  X idden X  similarities between documents that are not immediately obvious from 1-step generation links. Note that when normalized such that the sum of the elements of the vec-tor is 1. The two are practically the same representations since we compute the cosine of the vectors during clus-tering. Our work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encour-aging for a similar twist to document representation for clustering purposes. Second, graph-based inference tech-niques to discover  X  X idden X  textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as sum-marization (Erkan and Radev, 2004; Mihalcea and Ta-rau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sen-tences for extractive summarization. Their link weight function is based on cosine similarity. Our graph con-struction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a sim-ilar generation graph to rerank the documents returned by a retrieval system based on the stationary distribu-tion of the graph. Finally, previous research on clustering graphs with restricted random walks inspired us to clus-ter the generation graph using a similar approach. Our -step random walk approach is similar to the one pro-posed by Harel and Koren (2001). However, their algo-rithm is proposed for  X  X patial data X  where the nodes of the graph are connected by undirected links that are de-termined by a (symmetric) similarity function. Our con-tribution in this paper is to use their approach on textual data by using generation links, and extend the method to directed graphs.

There is an extensive amount of research on document clustering or clustering algorithms in general that we can not possibly review here. After all, we do not present a new clustering algorithm, but rather a new representation of textual data. We explain some popular clustering algo-rithms and evaluate our representation using them in Sec-tion 4. Few methods have been proposed to cluster doc-uments using a representation other than the traditional ing a bipartite graph of terms and documents and then clustering this graph based on spectral methods is one of them (Dhillon, 2001; Zha et al., 2001). There are also general spectral methods that start with vectors, then map them to a new space with fewer dimensions be-fore initiating the clustering algorithm (Ng et al., 2001).
The information-theoretic clustering algorithms are relevant to our framework in the sense that they involve probability distributions over words just like the language models. However, instead of looking at the word distri-butions at the individual document level, they make use of the joint distribution of words and documents. For ex-ample, given the set of documents in the document collection, Slonim and Tishby (2000) first try to find a word clustering information sion) while maximizing the original information). Then the same procedure is used for clustering documents using the word clusters from the first step. Dhillon et. al. (2003) propose a co-clustering version of this information-theoretic method where they cluster the words and the documents concurrently. We evaluated our new vector representation by compar-ing it against the traditional vector space repre-sentation. We ran k-means, single-link, average-link, and complete-link clustering algorithms on various data sets using both representations. These algorithms are among the most popular ones that are used in document cluster-ing. 4.1 General Experimental Setting Given a corpus, we stemmed all the documents, removed the stopwords and constructed the vector for each document by using the bow toolkit (McCallum, 1996). We computed the of each term using the following formula: where is the total number of documents and df is the number of documents that the term appears in.
We computed flattened generation probabilities (Equa-tion 2) for all ordered pairs of documents in a corpus, and then constructed the corresponding generation graph (Definition 1). We used Dirichlet-smoothed language models with the smoothing parameter can be considered as a typical value used in information retrieval. While computing the generation link vectors, we did not perform extensive parameter tuning at any stage of our method. However, we observed the follow-ing: Under these observations, we will only report results us-ing vectors of the data set and the clustering algorithm. 4.2 Experiments with k-means 4.2.1 Algorithm k-means is a clustering algorithm popular for its sim-plicity and efficiency. It requires ters, as input, and partitions the data set into exactly clusters. We used a version of k-means that uses cosine similarity to compute the distance between the vectors. The algorithm can be summarized as follows: 1. randomly select 2. assign each document to the cluster whose centroid 3. recompute the centroid of each cluster. (centroid 4. stop if none of the centroid vectors has changed at 4.2.2 Data k-means is known to work better on data sets in which the documents are nearly evenly distributed among dif-corpora for this experiment to be able to get a fair com-parison between different document representations. The first corpus we used is classic3 , 1 which is a collection of technical paper abstracts in three different areas. We used two corpora, bbc and bbcsport , that are composed of BBC news articles in general and sports news, respec-is a corpus of newsgroup articles composed of 20 classes. Table 1 summarizes the corpora we used together with the sizes of the smallest and largest class in each of them. 4.2.3 Results
We used two different metrics to evaluate the results of the k-means algorithm; accuracy and mutual informa-tion. Let algorithm, and where wise. map bel set of the k-means algorithm to the actual label set of the corpus. Given the confusion matrix of the output, best such mapping function can be efficiently found by Munkres X  X  algorithm (Munkres, 1957).

Mutual information is a metric that does not require a mapping function. Let output label set of the k-means algorithm, and with the underlying assignments of documents to these sets. Mutual information (MI) of these two labelings is defined as:
MI where ment is labeled as actual corpus, respectively; that these two events occur at the same time. These val-ues can be derived from the confusion matrix. We map the MI metric to the the maximum possible MI that can be achieved with the corpus. Normalized MI is defined as
One disadvantage of k-means is that its performance is very dependent on the initial selection of cluster cen-troids. Two approaches are usually used when reporting the performance of k-means. The algorithm is run mul-tiple times; then either the average performance of these runs or the best performance achieved is reported. Re-porting the best performance is not very realistic since we would not be clustering a corpus if we already knew the class labels. Reporting the average may not be very informative since the variance of multiple runs is usually large. We adopt an approach that is somewhere in be-tween. We use  X  X rue seeds X  to initialize k-means, that is, we randomly select each of the true classes as the initial centroids. This is not an unrealistic assumption since we initially know the number of classes, ing one example document from each class is not usually high. This way, we also aim to reduce the variance of the performance of different runs for a better analysis.
Table 2 shows the results of k-means algorithm us-ing vectors versus generation vectors flattened generation probabilities), walks), of the relatively larger size and number of classes of 20news corpus, we randomly divided it into disjoint par-titions with 4, 5, and 10 classes which provided us with 5, 4, and 2 new corpora, respectively. We named them 4news-1 , 4news-2 , means with 30 distinct initial seed sets for each corpus.
The first observation we draw from Table 2 is that even particularly surprising given that than the representation for most documents. 4 All a wide margin. The performance also gets better (not al-ways significantly though) in almost all data sets as we in-crease the random walk length, which indicates that ran-dom walks are useful in reinforcing generation links and inducing new relationships. Another interesting observa-tion is that the confidence intervals are also narrower for generation vectors, and tend to get even narrower as we increase . 4.3 Experiments with Hierarchical Clustering 4.3.1 Algorithms
Hierarchical clustering algorithms start with the triv-ial clustering of the corpus where each document de-fines a separate cluster by itself. At each iteration, two  X  X ost similar X  separate clusters are merged. The algo-rithm stops after are merged into a single cluster.

Hierarchical clustering algorithms differ in how they define the similarity between two clusters at each merg-ing step. We experimented with three of the most popular algorithms using cosine as the similarity metric between two vectors. Single-link clustering merges two clusters whose most similar members have the highest similarity. Complete-link clustering merges two clusters whose least similar members have the highest similarity. Average-link clustering merges two clusters that yield the highest av-erage similarity between all pairs of documents. 4.3.2 Data Table 3: The corpora used in the hierarchical clustering exper-
Although hierarchical algorithms are not very efficient, they are useful when the documents are not evenly dis-tributed among the classes in the corpus and some classes exhibit a  X  X ierarchical X  nature; that is, some classes in the data might be semantically overlapping or they might be in a subset/superset relation with each other. We picked two corpora that may exhibit such nature to a certain ex-lected from six news agencies in 1998. They contain doc-uments labeled with zero, one or more class labels. For each corpus, we used only the documents with exactly one label. We also eliminated classes with only one doc-ument since clustering such classes is trivial. We ended up with two collections summarized in Table 3. 4.3.3 Results
The output of a hierarchical clustering algorithm is a tree where leaves are the documents and each node in the tree shows a cluster merging operation. Therefore each subtree represents a cluster. We assume that each class of documents in the corpus form a cluster subtree at some point during the construction of the tree. To evaluate the cluster tree, we use F-measure proposed in (Larsen and Aone, 1999). F-measure for a class subtree where cision of of subtrees in the output cluster tree, and of classes. F-measure of the entire tree is the weighted average of the maximum F-measures of all the classes: where .

We ran all three algorithms for both corpora. Unlike k-means, hierarchical algorithms we used are deterministic. Table 4 summarizes our results. An immediate observa-tion is that average-link clustering performs much bet-ter than other two algorithms independent of the data set or the document representation, which is consistent with earlier research (Zhao and Karypis, 2002). The high-est result (shown boldface) for each algorithm and cor-pus was achieved by using generation vectors. However, unlike in the k-means experiments, was able to outperform yielded the best result instead of cases. We have presented a language model inspired approach to document clustering. Our results show that even the simplest version of our approach with nearly no parame-ter tuning can outperform traditional models by a wide margin. Random walk iterations on our graph-based model have improved our results even more. Based on the success of our model, we will investigate various graph-based relationships for explaining semantic structure of text collections in the future. Possible applications in-clude information retrieval, text clustering/classification and summarization.
 I would like to thank Dragomir Radev for his useful com-ments. This work was partially supported by the U.S. National Science Foundation under the following two grants: 0329043  X  X robabilistic and link-based Methods for Exploiting Very Large Textual Repositories X  admin-istered through the IDM program and 0308024  X  X ollab-orative Research: Semantic Entity and Relation Extrac-tion from Web-Scale Text Document Collections X  admin-istered by the HLT program. All opinions, findings, con-clusions, and recommendations in this paper are made by the authors and do not necessarily reflect the views of the National Science Foundation.

