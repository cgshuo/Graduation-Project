 The effectiveness of knowledge transfer using classification algorithms depends on the difference between the distribu-tion that generates the training examples and the one from which test examples are to be drawn. The task can be es-pecially difficult when the training examples are from one or several domains different from the test domain. In this paper, we propose a locally weighted ensemble framework to combine multiple models for transfer learning, where the weights are dynamically assigned according to a model X  X  pre-dictive power on each test example. It can integrate the advantages of various learning algorithms and the labeled information from multiple training domains into one unified classification model, which can then be applied on a different domain. Importantly, different from many previously pro-posed methods, none of the base learning method is required to be specifically designed for transfer learning. We show the optimality of a locally weighted ensemble framework as a general approach to combine multiple models for domain transfer. We then propose an implementation of the local weight assignments by mapping the structures of a model onto the structures of the test domain, and then weight-ing each model locally according to its consistency with the neighborhood structure around the test example. Experi-mental results on text classification, spam filtering and in-trusion detection data sets demonstrate significant improve-ments in classification accuracy gained by the framework. On a transfer learning task of newsgroup message catego-rization, the proposed locally weighted ensemble framework achieves 97% accuracy when the best single model predicts correctly only on 73% of the test examples. In summary, the improvement in accuracy is over 10% and up to 30% across different problems.
The work was supported in part by the U.S. National Sci-ence Foundation grants IIS-05-13678/06-42771 and BDI-05-15813. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of the funding agencies.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms
We are interested in transfer learning scenarios where we learn from one or several training domains and make predic-tions in a different but related test domain. Such knowledge transfer is possible when the training domain(s) and the test domain have the same set of categories or class labels. We further assume that we are only exposed to some labeled examples from the training domains but do not have any la-beled example from the test domain. The study of transfer learning is motivated by the fact that people often exploit knowledge gained from related domains where labeled data are abundant to classify examples in a new domain. Unfor-tunately, traditional supervised learning techniques usually fail to transfer knowledge in this scenario because it requires the training and the test data to be i.i.d. samples from the same distribution.

There are a few important observations about this prob-lem. We notice that there are usually several classification models available from the training domains. For example, the classifiers can be trained from several relevant domains or built using different learning algorithms on the same do-main. Different models usually contain different knowledge and thus have different advantages, due to the inductive bias of the specific learning technique as well as the distributional differences among the training domains. Therefore, differ-ent models may be effective at different regions or structures in the new and different test domain, and no single model can perform well in all regions. We refer to these different models as base models. Ideally, we may wish to combine the knowledge from these base models rather than using any single model alone to more effectively transfer the use-ful knowledge to the new domain. For this task, one would naturally consider model averaging that additively combines the predictions of multiple models. However, the existing model averaging methods in traditional supervised learning usually assign global weights to models, which are either uni-form (e.g., in Bagging), or proportional to the training accu-racy (e.g., in Boosting), or fixed by favoring certain model (e.g., in single-model classification). Such a global weight-ing scheme may not perform well in transfer learning because different test examples may favor predictions from different base models. For example, when the base models carry con-flicting concepts at a test example, it is essential to select the model that better represents the true target distribution underlying the example. In fact, based on principles of risk minimization, we can derive that there exists a solution to assign per model and per example weights to combine multi-ple base models to maximize their combined accuracy on the new domain, and the combined accuracy is higher than any single model acting alone. However, it is impossible to dy-namically assign the optimal model weights for each example precisely because P ( y | x ), the true conditional probability of class label y given a test example x , is not known a priori. Past practice of cross-validation based weight assignment is inapplicable since the weights would be assigned based on labels in the given training domain(s) whose P ( y | x ) could be different from that of the test domain. Therefore the fo-cus of this paper is to find an approximation to this optimal local weight assignment for each test example.

We propose a graph-based approach to approximate the optimal model weights where the local weight for a base model is computed by first mapping and then measuring the similarity between the model and the test domain X  X  lo-cal structure around the test example. This similarity is measured by comparing neighborhood graphs, and quanti-fied in the weight assignment equation. Intuitively, it favors classifiers whose mapped local structure is similar to the local structure around the test example. For a particular example, if none of the mapped local structures is similar to the original local structure in the target domain, the pre-dicted label will be obtained by voting among its neighbors inside the same local structure of the test set. This strat-egy ensures that the maximum amount of predictive powers of the labeled information are extracted and transferred to the test domain to make the predictions consistent with its underlying manifold structure.

Our main contributions to the task of transfer learning include the following: (1) We propose a locally weighted en-semble framework to address the transfer learning problem, and demonstrate its superiority over single models in terms of risk minimization when the weights are set optimally. (2) None of the base models is required to be specifically de-signed for transfer learning, thus providing great flexibility and freedom on what models to use. (3) We propose to ap-proximate the model weights based on the local manifold structures in the test domain, and provide neighborhood graph-based estimation. (4) We provide a prediction adjust-ment step to propagate labels from nearby examples when all base models are inconsistent with certain test examples.
We evaluated the proposed framework on three real tasks: spam filtering, text categorization, and network intrusion detection. In each task, the test examples come from a dif-ferent domain than the training set. Our experiment results show that the locally weighted ensemble framework signifi-cantly improved the performance over a number of baseline methods on all three data sets, which shows the effectiveness of the proposed framework for transfer learning.
Let us first look at a toy learning problem with two train-ing sets and a test set shown in Figure 1. The two training sets have partially conflicting concepts and their decision boundaries are the straight lines. For the test set, however, the optimal decision boundary is the V-shape solid line. As can be seen, the regions R 1 and R 2 are  X  X ncertain, X  because the two training sets are conflicting there. If we either sim-ply collapse the two data sets and try to train a classifier on the merged examples, or combine the two linear classifiers M 1 and M 2 trained from the training set 1 and set 2 respec-tively, then those negative examples in R 1 and R 2 will be hard to predict. Those semi-supervised learning algorithms do not work either because they only propagate the labels of the training examples to the unlabeled examples. In this case, there are conflicting labels in R 1 and R 2 , causing am-biguous and incorrect information to be propagated. But it is obvious that, if M 1 is used for predicting test examples in R 1 and M 2 used for examples in R 2 , then we can label all test examples correctly. Therefore, ideally, one wish to have a  X  X ocally weighted X  ensemble framework that combines the two models, and weighs M 1 higher at R 1 and M 2 higher at R 2 . We also observe that this data set has a property that neighbors along the same  X  X lustering-manifold struc-ture X  share the same class labels, which is a commonly-held assumption for reasonable problems. Below, we first intro-duce a locally weighted ensemble framework with weights dynamically adjusted according to the model behaviors at each test example. We then present an effective way of ap-proximating the model weights via local structure mapping around each example. The success of the proposed method on this toy data set is demonstrated in Section 4.2.
Let x be the feature vector and y be the class label where x and y are drawn from feature space X and label space Y respectively. For a set of k models M 1 , . . . , M k , the general Bayesian model averaging approach computes the posterior distribution of y as P ( y | x ) = where P ( y | x , D, M i ) = P ( y | x , M i ) is the prediction made by each model and P ( M i | D ) is the posterior of model M after observing the training set D . However, in transfer learning, since training and test domains are different, we may wish to incorporate information about the test domain and update the model prior for P ( M i | T ), where T is the test set. So P ( M i | D ) should be replaced by P ( M i the weighted combination of model predictions. By this re-placement, we take the difference between training and test domains into consideration during learning. If the true dis-tribution P ( y | x ) is known, then for predictions on x , the other examples in the test set are irrelevant to the model per-formance at x . In other words, the model weight P ( M i | T ) is actually P ( M i | x ) at x when P ( y | x ) is available. Different from traditional ensemble approaches, this locally weighted model averaging method weights individual models accord-ing to their local behaviors at each test example. The final prediction for x is: where w M i , x = P ( M i | x ) is the true model weight that is locally adjusted for x representing the model X  X  effectiveness on the test domain.

The benefits of this locally weighted model averaging ap-proach can be shown as follows. To simplify the problem, we map the label space Y to { 1 , . . . , c } where c is the number of classes. We then use a c  X  1 vector f to denote the true conditional probability in the test domain where the i -th el-ement is f i = P ( y = i | x ) . Supervised learning can output a c  X  1 vector h that is close to f for x . Let w i = w M i the weight for model M i at test example x , and let w denote the k  X  1 weight vector. h i represents the predictions made by model M i at x and is again a c  X  1 vector where the j -th element is h i j = P ( y = j | x , M i ) . H is used to represent a c  X  k matrix with all the model predictions made for x where the ij entry is model M i  X  X  predicted P ( y = j | x , M i Then the output of the model averaging framework for x is a vector h e = Hw . Note that w satisfies the constraints that w i  X  [0 , 1] and from a single model M i is a special case of h e when w i and other weights are zero. But we wish to find a weight vector w which minimizes the distance between f and h e . Under squared-error loss, the following objective function should be minimized to obtain the optimal w : where I is a k  X  1 vector of 1 and  X  is the regularization term. It is obvious that Eq. (2) represents a least-square linear regression problem and the optimal solution is  X  can be further calculated by substituting the above w  X  the constraint ( w  X  ) T I = 1 . Usually w  X  i is a value between 0 and 1 so the weight vector of the optimal ensemble is different from that of the single model. Therefore, the error of the model averaging framework on each test example x will not be greater than that of any single model: Thus, for each test example, there is a smaller chance to make a mistake if we combine the predictions from different models using the optimal weight vector. It is important to note that the optimal weight vectors are different for differ-ent test examples, so weights should be decided locally.
This locally weighted ensemble framework differs from tra-ditional model averaging methods in the following ways: 1) In transfer learning problems, the traditional methods of assigning model weights based on training set or assigning fixed prior weights are undesirable. Instead, we do not as-sume that training and test domains follow same distribu-tions but rather focus on the test set when deriving the best model weights to transfer knowledge across domains. 2) Existing work usually weights each model globally, but the proposed method assigns per example weights to each model to identify variations in model performance for dif-ferent test examples. As discussed, there may not exist one model globally optimal for all the test examples. Usually, different test examples favor different models and therefore the per example weighting scheme is better than the global weighting scheme in terms of classification accuracy.
One challenge is that the optimal per example weight vec-tors cannot be computed exactly in reality, since the true target vector f for each test example x is not known a pri-ori. Importantly however, from its solution in Eq. (3), a model will have a higher weight if its prediction on x is closer to the true P ( y | x ) . In the rest of this paper, we pro-pose a graph-based approach to approximate the optimal per example weight w M i , x under the  X  X lustering-manifold X  as-sumption that P ( x ) is related to P ( y | x ) . Other approxima-tion heuristics can be developed under this locally weighted framework as long as the weights reasonably approximate the model performance for given test examples.
As discussed in Section 2.1, the optimal weights can be approximated by assigning a higher weight to a model that produces a more accurate label prediction for x . So the main task is to formulate similarity between the model pre-dictions and the unknown true target function. To achieve this goal, we can model the underlying P ( x ) from the un-labeled test set in order to infer P ( y | x ) . Specifically, we make a X  X lustering-manifold X  X ssumption, as commonly held in semi-supervised learning, that P ( y | x ) is not expected to change much when the marginal density P ( x ) is high. In other words, the decision boundary should lie in areas where P ( x ) is low. Under such an assumption, we can compare the difference in P ( y | x ) between the training and the test data locally with only unlabeled test data. However, probabil-ity density estimates are hard to obtain precisely, especially when x is high-dimensional. Instead, we propose to clus-ter the test data and assume that the boundaries between the clusters represent the low density areas. As a result, if the local cluster boundaries agree with the classification boundary of M around x , then we assume that P ( y | x , M ) is similar to the true P ( y | x ) around x , and thus the weight for model M ought to be high at x . In the following, we formally give a procedure of computing the weight and illustrate the procedure with an example.

For a test example x and a base model M to be com-bined, we first construct two graphs: G T = ( V, E T ) and G
M = ( V, E M ) . In both graphs, the vertex set V contains all the test examples. For G M , there is an edge connecting two test examples if and only if the examples are classified into the same class by M . On the other hand, to construct G
T , we cluster the test examples into c 0 clusters and again, connect two test examples with an edge if and only if the two examples are in the same cluster. Then we can approx-imate the model weight as the similarity between the local structures around x in G T and G M . Specifically, under the clustering assumption, it is probable that two examples are in the same class if they belong to the same cluster in G So we could use the percentage of common neighbors of x found in G M and G T to approximate the model accuracy on x and set the weight. Suppose the sets of neighbors for x in G
M and G T are V M and V T respectively. The model weight at x is proportional to the similarity of its local structures between G M and G T : w M, x  X  s ( G M , G T ; x ) = Figure 2: Local Neighborhood Graphs around x According to its definition, s ( G M , G T ; x ) reflects the degree of consistency in labeling the test examples. If x has similar sets of neighbors in G M and G T , it is likely that the model M is consistent with the underlying structure around x . As an example, Figure 2 shows the neighborhood graphs of a test example x constructed from two supervised models and the clustering algorithm on the test set. According to Eq. (5), the similarity between model 1 and the clustering structure is 0.75 at x , but that between model 2 and the structure is 0.5. Therefore, for x , model 1 X  X  weight will be set higher since it is more consistent with the local structure around x . This is a simple and effective method to compute the similarity.

The weight approximation is based on the clustering as-sumption which requires that the manifold structure of the data is related to the conditional probability P ( y | x ) . Though this is a reasonable assumption for many problems, it may not always hold. Without knowing P ( y | x ) a priori, it is impossible to verify the assumption. But this property is usually determined by the nature of the learning tasks. An example where the assumption does not hold is sentiment classification, where the clustering structure of a set of prod-uct reviews reveals the topics but may have nothing to do with whether the users like or dislike the product. Therefore, we propose to check the validity of the clustering assump-tion by evaluating the clustering quality on the training set using purity, entropy or F measure. If the task fails the test, we will ignore the weight approximation step, but simply combine the models using uniform weights. This strategy restricts the use of the graph-based weight estimation only to the cases where the clustering assumption is satisfied on both training and test sets. However, the strict checking criteria could guarantee the high accuracy of the proposed method. For the cases where the clustering assumption does not hold, other techniques need to be explored.

When the condition holds, we compute the per-example model weights based on Eq. (5) with a normalization term: where M i is one of the k models. Then the final prediction of the weighted ensemble E for x is: where P ( y | M i , x ) is the prediction made by model M the predicted label for x goes to y  X  which minimizes the risk: where  X  ( y 0 , y ) is the cost incurred when the true class label is y 0 but the prediction goes to y . With the most commonly used zero-one loss function, y  X  = arg max y P ( y | E, x ) .
The weighting scheme shown in Eq. (7) works on the ba-sis that at least some of the models do reasonably well on predicting the label for x . However, if the concepts carried by all the models conflict with the actual concept at x , the similarity measure s ( G M , G T ; x ) is expected to be low for each model M . But after the normalization in Eq. (6), the locally weighted ensemble framework would still make de-cisions based on these models for x and it is probable that the combined output is still in conflict with the true one. In such a scenario, it is reasonable to abandon the labeled information conveyed by the supervised models but rather rely on the local structure around x only.

Since the similarity measure s ( G M , G T ; x ) reflects the de-gree of consistency between model M  X  X  prediction and x  X  X  neighborhood structure, we can use the average s ( G M , G over all M to judge whether the labeled information is reli-able or not. In fact, s ( G M , G T ; x ) , representing the average percentage of common neighbors shared by supervised mod-els and clustering results, is within [0 , 1] . To be exact, when a test example shares the same neighbors in two graphs, their similarity is 1, whereas if no common neighbor is found, it is 0. So for example, if only two models are used and their s ( G M , G T ; x ) are both 0.01 at x , then we should avoid normalizing the weights into 0.5 since both models should rather be discarded. Let s avg ( x ) = 1 k be the average similarity between the base models X  predic-tions on x and the clustering structure around x . Then if s avg ( x )  X   X  , where  X  is the threshold, we believe in the prediction obtained from Eq. (7); otherwise, we discard all the supervised classifiers and construct an  X  X nsupervised X  classifier based on the neighborhood of x .

The  X  X nsupervised X  classifier U is not trained on any la-beled training set. Its prediction on x is mainly determined by the neighbors of x with labels predicted by the combined classifier. Specifically, P ( y | U, x ) can be decomposed as: Here, C is one of the clusters in the test set. We assume that the cluster membership is deterministic, then P ( x  X  C | x ) is approximated as follows: Hence, P ( y | U, x ) is approximately the same as P ( y | U, x  X  C ) when x belongs to cluster C . We can further approx-imate P ( y | U, x  X  C ) as the average P ( y | E, x ) for x  X  C where C 0 contains test examples which satisfy both x  X  C and s avg ( x )  X   X  . In other words, only examples that have reliable predictions from the weighted ensemble will count in this procedure. Therefore, where | C 0 | is the size of C 0 . The above strategy can be simplified if we set P ( y | E, x ) = 1 when y is the label for x predicted by E . So P ( y | U, x  X  C ) can be estimated by a
Figure 3: Locally Weighted Ensemble Framework majority vote among examples in C 0 : where c ( y, C 0 | E ) is the number of examples with label y pre-dicted by ensemble E in C 0 . So the probability of x having label y is the percentage of examples in the cluster C 0 that have y as their class labels, where C 0 is the cluster that x belongs to and contains test examples with predicted labels. The final predicted label for x is determined by Eq. (8) with P ( y | E, x ) replaced by P ( y | U, x ) . If zero-one loss function is applied, the class label for x whose cluster is C should be the majority label prediction among the test examples which satisfy both x  X  C and s avg ( x )  X   X  .
The framework is summarized in Figure 3. We first verify whether the clustering structure is relevant to the classifica-tion task by performing clustering on the training set. If the purity of clustering on the training set is below 0.5, we sim-ply combine models using uniform weights. Otherwise, if the clustering quality is satisfactory, in step 2, we construct the neighborhood graphs for both the supervised models and the clustering results. Then in step 3, the weight of each model at each test example is computed, which reflects the consistency of model predictions among the test example X  X  neighborhood. We then separate the test examples by check-ing if its average model weight is greater than a confidence threshold. For those test examples on which cross domain models can make sufficiently accurate predictions, the final label predictions are decided by the locally weighted ensem-ble. But, for the test examples that the models are not expected to classify correctly, the labels are determined by majority voting among those neighbors with highly confi-dent predictions within the same cluster structure.
In this part, we demonstrate the effectiveness of the locally weighted ensemble framework. The algorithms are evaluated on various data sets covering many application domains. Re-sults show that the proposed framework could combine the predictive powers obtained from multiple sources and gain great improvements in classification accuracy. The software, datasets and more details about the experiments are avail-able at http://ews.uiuc.edu/  X  jinggao3/kdd08transfer.htm.
We conduct experiments on one synthetic and four real data sets, where training and test distributions are different.
The two training sets and the test set as shown in Figure 1 are generated from several Gaussian distributions with the same variance. In each training set, there are 40 positive and 20 negative examples and in the test set, the number of positive and negative examples are 20 and 40 respectively.
The email spam data set, released by ECML/PKDD 2006 discovery challenge, contains a training set of publicly avail-able messages and three sets of email messages from indi-vidual users as test sets. The 4000 labeled examples in the training set and the 2500 test examples for each of the three different users differ in the word distribution. The aim is to design a server-based spam filter learned from public sources and transfer it to individual users.

The 20 newsgroups data set contains approximately 20,000 newsgroup documents, partitioned across 20 different news-groups nearly evenly. The Reuters-21758 corpus contains Reuters news articles from 1987. From the two text collec-tions, we generate nine cross-domain learning tasks. Both text collections have a two-level hierarchy so that each learn-ing task involves a top category classification problem but the training and test data are drawn from different sub cat-egories. For example, the goal is to distinguish documents from two top newsgroup categories: rec and talk. So a train-ing set involves documents from X  X ec.autos, X  X  X ec.motorcycles, X   X  X alk.politics X  and  X  X alk.politics.misc, X  whereas the test set includes sub-categories X  X ec.sport.baseball, X  X  X ec.sport.hockey, X   X  X alk.politics.mideast X  and  X  X alk.religions.misc X . The strat-egy is to split the sub-categories among the training and the test sets so that the distributions of the two sets are similar but not exactly the same. The tasks are generated in the same way as in [9] and more details can be found there.
The KDD cup X 99 data set consists of a series of TCP connection records for a local area network. Each exam-ple in the data set corresponds to a connection, which is labeled as either normal or an attack, with exactly one spe-cific attack type. Some high level features are used to distin-guish normal connections from attacks, including host, ser-vice and traffic features. In the experiments, we use the 34 continuous features. Attacks fall into four main categories: DOS(denial-of-service), R2L(unauthorized access from a re-mote machine), U2R(unauthorized access to local superuser privileges), Probing(surveillance and other probing). Since in reality, we usually encounter the problem of detecting the variants of known attacks, it is realistic to have one type of intrusions in the training set but another type in the test set. We create three data sets, each contains a set of randomly selected normal examples and a set of attacks from one cat-egory. Since the number of U2R attacks is small, we only use examples from DOS, R2L and Probing categories. Then three cross-domain learning tasks are generated by training from two types of attacks to detect another type of attack. The details of the four real tasks are presented in Table 1.
We compare the weighted ensemble framework with dif-ferent learning algorithms. In particular, since most data sets are high-dimensional, the following commonly used al-gorithms are appropriate choices: 1) Winnow ( WNN ) from learning package SNoW [6], 2) Logistic Regression ( LR ) implemented in BBR package [16]; and 3) Support Vec-tor Machines ( SVM ) implemented in LibSVM [8]. When we only have a single source domain in the training, three single classifiers are trained using the above learning algo-rithms and combined according to the proposed weighted ensemble framework. But note that the proposed method is a general framework so that any kind of models could be plugged in and transferred to the test domain. Since semi-supervised learning (transductive learning) is closely related to the problem, we compare the proposed method with Transductive Support Vector Machines (TSVM) im-plemented in SVM light [20]. Furthermore, in the pro-posed framework, the two main steps are, predicting labels using weighted classifiers if the classifiers are sufficiently accurate in terms of alignment with clustering structures; and propagating the labels of predicted test examples to the unpredicted ones through the clustering structure. To demonstrate the effectiveness of both steps, we include the following three methods in the comparison: 1) A simple model averaging framework ( SMA ) where all model pre-dictions are combined using uniform weights; 2) The lo-cally weighted ensemble framework without the adjustment step, which simply adopts the weighted prediction for each test example. We call it partial locally weighted ensemble method ( pLWE ); 3) The locally weighted ensemble frame-work ( LWE ) involving both classifier combination and local structure based adjustment. Note that SMA is one of the global ensemble methods where the model weights are set the same for all the test examples. Suppose there are k example. We use the clustering package CLUTO [21], which is designed for high-dimensional data clustering, to cluster the test set. Again, other clustering algorithms could be used as long as the  X  X lustering X  assumption is satisfied.
We compare with a set of different baseline methods on the synthetic and intrusion detection data sets. In each task, we have two source domains for training and the remaining one for the testing. The proposed weighted ensemble methods (pLWE and LWE) are built upon two single models trained from the two source domains using SVM. First, we compare pLWE and LWE with the simple averaging method (SMA) based on the two SVM models. Second, we can choose the training set as 1) one of the two source data sets, or 2) the union of the two source data sets. On the three possible training sets, we study the performance of supervised learn-ing models (SVM) and semi-supervised models (TSVM) and compare them with the proposed methods.

To compare the performance of the classification methods, we look at a set of standard evaluation metrics. First, we use classification accuracy, which is simply defined as the percentage of correct predictions among all test examples. Second, under squared loss function, the algorithms can be evaluated using Mean Squared Errors defined as follows: of the classifier, which is the estimated posterior probability of x i belonging to positive class, P (+ | x i ) is the true poste-rior probability and { x i } n i =1 represents the test set. Another measure is used in evaluating the intrusion detection task: the area under ROC curve (AUC), the best of which is 1 corresponding to 100% detection and 0% false alarm. In the experiments, we focus on binary classification, but the framework can be easily applied on multi-class tasks.
In this part, we report the experimental results regarding the effectiveness of the locally weighted ensemble. The re-sults clearly demonstrate that on the transfer learning prob-lems where training and testing data have different distri-butions, the proposed locally weighted ensemble approach greatly outperforms supervised, semi-supervised single-model algorithms, and a simple averaging ensemble.

The results of the toy problem introduced in Figure 1 are summarized in Figure 4. The results of linear SVM on the training sets from two domains are the top two on the left, denoted as M 1 and M 2 . Due to the difference between train-ing and test distributions, both make incorrect predictions at  X  X irrored X  areas. After merging the training sets, the SVM model ( X  X LL X  on top right) still does not work and the constructed hyperplane is obviously a horizontal line. This is due to the fact that there exist conflicting concepts in the merged training set. On the other hand, transduc-tive SVM (TSVM bottom left) trained on merged training sets fails as well since the label propagation is confused by the conflicting training examples. Simple averaging of M 1 and M 2 , shown as  X  X MA X  (bottom middle) also makes mis-takes in the uncertain areas. However, examples incorrectly classified by these methods are now correctly predicted by the locally weighted ensemble approach (LWE) and the de-cision boundary matches the V-shape well. To see how this works, first, the clustering algorithm discovers the two clus-ters above and below the V-shape. For any example x  X  R 1 its neighbors in the cluster contain the examples in all three regions R 1 , R 2 and R 3 . At the same time, its neighbors predicted by M 1 are those examples  X  R 1 and R 3 . Im-portantly, its neighbors predicted by M 2 are only examples  X  R 1 . Since there are more common neighbors between the clustering structure and M 1 , M 1 will be given higher weight at x . Thus, according to M 1 , the examples in R 1 are classi-fied to be negative. Similarly, M 2 will be chosen to predict examples  X  R 2 as negative. In summary, by weighting the two models locally according to the degree of consistency between models and clusters, the examples at the uncertain areas are predicted correctly.

Results of all the methods on the Email Spam Filtering, 20 Newsgroup and Reuters sets are summarized in Table 2 with best results shown in bold font. Refer to Table 1 for the details of each task. It is clearly seen that, for all tasks and using any performance measure, the locally weighted ensem-ble method (LWE) significantly improves the transfer learn-ing performance compared with other baseline methods. We can observe that most of the transfer learning problems are tough due to the unknown discrepancy between the train-ing and the test distributions. The single-model methods (WNN, LR, SVM) usually have poor performance with accu-racy around 0.7 and mean squared error greater than 0.1 on most of the tasks. The simple model averaging algorithm us-ing uniform weights can help reduce the expected error com-pared with single models. However, its performance is not quite satisfactory since they only rely on the labeled informa-tion from the source domain and make no efforts in selecting useful information and transferring the knowledge into the test domain. By incorporating the structure information of the test set into learning, the transductive learning approach can beat the supervised learning methods most of the times. But we can see more improvement achieved by using the pro-posed locally weighted ensemble framework. After the first step of combining classifiers by weighting them judiciously, both accuracy and mean squared error are improved over all the baselines. Then propagating confident predictions along the clustering structure in the test set can significantly boost the performance further. As an example, on the  X  X  vs S X  data set in the 20 newsgroup collection, the worst single model only achieves around 66% accuracy whereas the best single model makes correct predictions for 73% of the test examples. The tranductive SVM improves the accuracy to around 77% and LWE outperforms all the other methods by an impressive 97% accuracy. In most of the experiments, the improvement in accuracy after utilizing weighted ensemble is over 10% and up to 30% for some problems. The experi-mental results on these transfer learning tasks demonstrate the benefits of the empirical approximation of the optimal locally weighted ensemble framework. Both per-example weighting scheme and the adjustment step in the framework can successfully filter out the  X  X armful X  labeled information, and thus help make the most reliable predictions.
Table 3 presents the performance of all methods on the three tasks of intrusion detection. Each row corresponds to a learning problem characterized by the test domain and the other two domains act as training, as discussed in Sec-tion 4.1. Besides the two training domains, a simple com-bination of examples from the two domains (represented as  X  X LL X ) could be another source of training. Based on each training source, we test the performance of SVM and TSVM on the test domain. We also build two single models from each training domain and combine them using uniform weights, which corresponds to SMA. The proposed pLWE and LWE are shown in the last two columns. For the first two learning tasks, it is obvious that the proposed LWE shows dominance for both accuracy and AUC. Especially on the test set of  X  X robing X , the two training domains seem to be conflicting with each other, thus both the models trained from a union of the two domains and the simple averaging of the two models result in an accuracy around 50% to 60%. LWE achieves 96.36% accuracy by choosing the useful in-formation from the two models. On the last learning task, the algorithm TSVM trained on the combination of training domains wins over the proposed method, which may be due to the fact that one of the single models we are combining has insufficient amount of examples to be relied on. We note that the worst single model X  X  accuracy is around 56% and the simple averaging method even degrades to having 54% accuracy. Based on such weak classifiers, we could still im-prove the accuracy to 80%. In the future, we will explore more strategies to detect the cases when we should combine the source domains rather than building individual models.
There are two important parameters in the proposed al-gorithm, the number of clusters c 0 in the test set and the selection threshold  X  to filter the predictions with low con-fidence. The traditional way of setting parameters through cross-validation cannot work when the training and test dis-tributions are different. Again, since the true target function of the test domain is not known, there may not have effective methods to find the optimal values of the parameters. So here, we just give some sensitivity experimental results and state some basic principles in setting the parameters. We choose one cross-domain learning problem from each of the three data sets: email spam filtering, and 20 newsgroup and Reuters set, and the results are shown in Figure 5. We vary c from 2 to 10 and  X  from 0.1 to 0.9, and put both of them on the x -axis. We compare the accuracy of LWE approach when the parameters vary, with that of the best accuracy achieved by the baseline methods. We fix  X  = 0 . 7 when changing c 0 , and let c 0 = 2 when tuning  X  . It is clearly seen that when the threshold rises from 0.1 to 0.5, the learning performances on all three sets are gradually improving. Af-ter the point of 0.5, the performances maintain stable. This suggests that a low threshold is not desirable since many inaccurate predictions from the supervised models would be used in the adjustment step. Therefore 0.5 up to 1 could be a reasonable range to select the threshold  X  . However, the users could choose to lower down or raise the threshold to match their beliefs in the abilities of the supervised models. As for the number of clusters c 0 , the best performances in the experiments are achieved when c 0 = 2 . When c 0 goes up, the over-fitting could occur when the number of examples in each cluster is not sufficient enough to give an accurate estimate of the model weights, and thus we could observe a drop in accuracy. We could also note that in spite of the changes caused by parameter variation, the proposed LWE improves over the best baseline method most of the time.
The problem with different training and test distributions started gaining much attention very recently. When it is as-sumed that the two distributions differ only in P ( x ) but not in P ( y | x ), the problem is referred to as covariate shift [25, 18] or sample selection bias [14]. The instance weighting ap-proaches [25, 18, 5] try to re-weight each training example Another line of work tries to change the representation of the observation x hoping that the distributions of the train-ing and the test examples will become very similar after the transformation [3, 24]. [22] transforms the model learned from the training examples into a Bayesian prior to be ap-plied to the learning process on the test domain. The major difference between our work and these studies is that they depend on a single source of information and try to learn a global single model that adapts well to the test set.
Constructing a good ensemble of classifiers has been an active research area in supervised learning [12]. By com-bining decisions from individual classifiers, ensembles can usually reduce variance and achieve higher accuracy than individual classifiers. Such methods include Bayesian av-eraging [17], bagging, boosting and many variants of en-semble approaches [2, 27, 13, 15]. Some ensemble methods assign weights locally [1, 19], but such weights are deter-mined based on training data only. There has not been much work on ensemble methods to address the transfer learning problem. In [11, 26], it is assumed that the train-ing and the test examples are generated from a mixture of different models, and the test distribution has different mix-ture coefficients than the training distribution. In [23], a Dirichlet Process prior is used to couple the parameters of several models from the same parameterized family of dis-tributions. [10] extends the boosting method to perform transfer learning. Bennett et al. [4] proposed a methodol-ogy for building a meta-classifier which combines multiple distinct classifiers through the use of reliability indicators. The proposed weighted ensemble provides a more general framework for transfer learning because 1) the base models can be heterogeneous and can be any generative or discrim-inative models, and 2) the method does not depend on spe-cific applications and makes no assumption about the form of distributions generating the training or the test data.
Multi-task learning(MTL) [7], which learns several related tasks at the same time with a shared representation, consid-ers single P ( x ) and multiple output variables, so the basic setting is different from our problem. The  X  X lustering X  as-sumption in our work is exploited in some transfer learning and semi-supervised learning works [9, 28], where clustering structure is utilized in smoothing predictions among neigh-bors. Our paper differs from these papers by utilizing the assumption in weighting different models locally to combine all sources of labeled information for knowledge transfer.
Knowledge transfer across domains with different distri-butions is an important problem in data mining that has not been fully investigated. In this work, we take advantage of the different predictive powers of several models trained on different domains or using different learning algorithms. We propose a locally weighted ensemble framework to transfer the combined knowledge to a new domain that is different from all the training domains. Importantly, the base mod-els can be constructed by traditional learning algorithms not specifically designed for transfer learning. We analyze the optimality on expected error reduction by utilizing the locally weighted ensemble framework as compared to both single models and globally weighted ensembles. Based on the  X  X lustering X  assumption that the local structure of the test set is related to P ( y | x ) , we design an effective weighting scheme to approximate the optimal model weights. This is formulated by comparing the neighborhood graphs of each model with those from clustering. The experimental results on four real transfer learning data sets show that the pro-posed method improves over each base model 10% to 30% in accuracy and is more accurate than both semi-supervised learning and simple model averaging models. These results indicate that: 1) the locally weighted ensemble could suc-cessfully identify the knowledge from each model that is useful to predict in the test domain and transfer such infor-mation from all available base models; and 2) the proposed graph-based weight estimation method makes the framework practical by effectively approximating the optimal model weights. In the future, we plan to compare LWE with exist-ing single-model based transfer learning algorithms, as well as to explore effective methods to set parameter values.
