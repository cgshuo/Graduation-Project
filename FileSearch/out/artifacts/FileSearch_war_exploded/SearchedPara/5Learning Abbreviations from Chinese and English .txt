 Abbreviations represent fully expanded forms (e.g., Hidden Markov Model) through the use of shortened forms (e.g., HMM). Abbreviations result from a highly productive type of term variation, defining alternative expressions to fully expanded forms. At the same time, abbreviations increase the ambiguity in a text. For example, in com-putational linguistics, the acronym HMM stands for Hidden Markov Model, whereas, in the field of biochemistry, HMM is generally an abbreviation for heavy meromyosin. Associating abbreviations with their fully expanded forms is of great importance in various natural language processing (NLP) applications [HaCohen-Kerner et al. 2008; Pakhomov 2002; Yu et al. 2006].

The core technology for abbreviation disambiguation is to recognize the abbrevia-tion definitions in the actual text. Chang and Sch  X  utze [2006] reported that 64,242 new abbreviations were introduced into the biomedical literatures in 2004. As such, it is important to maintain sense inventories (lists of abbreviation definitions) that are updated with the newly coined word or phrase. In addition, based on the one-sense-per-discourse assumption [Yarowsky 1993], the recognition of abbreviation definitions assumes senses of abbreviations that are locally defined in a document. Therefore, a number of studies have attempted to model the generation processes of abbreviations, for example, inferring the abbreviating mechanism of the Hidden Markov Model into HMM.

An obvious approach is to manually design rules for abbreviating terms. Early stud-ies attempted to determine the generic rules that humans use to intuitively abbre-viate given words [Barrett and Grems 1960; Bourne and Ford 1961]. Since the late 1990s, researchers have presented various methods by which abbreviation definitions appearing in actual texts are extracted [Adar 2004; Ao and Takagi 2005; Park and Byrd 2001; Schwartz and Hearst 2003; Taghva and Gilbreth 1999; Wren and Garner 2002]. These studies proposed various heuristics for abbreviation recognition, for ex-ample, use of initials, capital letters, syllable boundaries, stop words, length of ab-breviation, etc. These studies performed highly, especially for English abbreviations. However, a more extensive investigation of abbreviations is needed in order to further improve definition extraction. In addition, we cannot simply transfer the knowledge of the hand-crafted rules from one language to another. For instance, in English, abbre-viation characters are preferably chosen from the initial and/or capital characters in their full forms, whereas some other languages, including Chinese and Japanese, do not have word boundaries or case sensitivity.

A number of recent studies have investigated the use of machine learning tech-niques. For example, Tsuruoka et al. [2005] formalized the processes of abbreviation generation as a sequence labeling problem. In the present study, each character in the expanded form is tagged with a label, y  X  X  P , S } 1 , where the label character and the label S skips the current character . In Figure 1 (upper), the abbre-viation PGA is generated from the full form poly glycolic acid because the underlined characters are tagged with P labels. In Figure 1 (lower), the abbreviation is generated using the second and third characters, skipping the subsequent three characters, and then using the seventh character.

In order to formalize this task as a sequential labeling problem, we have assumed that the label of a character is determined by the local information of the character and its previous label. However, this assumption has a limitation for modeling abbre-viations, and adding the global information addresses this limitation. For example, the model cannot make use of the number of words in a full form to determine and gen-erate a suitable number of letters for the abbreviation. In addition, the model would be able to recognize the abbreviating process in Figure 1 more reasonably if it is able to segment the word polyglycolic into smaller regions, for example, poly-glycolic (see Figure 2). In this article, non-local information means the long range information that is located outside of the local observations. For example, for the labeling on the seg-ment poly in Figure 2, the labeling on the word acid is a type of non-local information. Modeling non-local information can enable long-range dependencies in abbreviating words. Even though humans may use global or non-local information to abbreviate words, previous studies have rarely incorporated this information into a sequential labeling model.

In the present article, we propose implicit and explicit solutions for incorporating non-local information. The implicit solution is based on the latent conditional random field model (LCRF) [Sun and Tsujii 2009; Sun et al. 2008b], in which non-local in-formation is modeled by latent variables. We manually encode non-local information into the labels in order to provide an explicit solution. We evaluate the models on the task of abbreviation generation, in which a model produces an abbreviation for a given full form. Experimental results indicate that the proposed models outperform previous abbreviation generation studies.

Not surprisingly, non-local information increases the computational complexity of training and slows down the training speed. In particular, training LCRFs is quite challenging. Traditional batch training methods are very slow on training LCRFs [Petrov and Klein 2008; Sun et al. 2009a]. For example, Petrov and Klein [2008] high-lighted both time and memory cost problems on training LCRFs for natural language parsing. To accelerate training speed, we further present an online training method for optimizing LCRFs. The improved online training method can arrive the same objective optimum with significantly accelerated training speed. Online optimization methods for non-convex log-linear models are interesting because few existing studies applied online training for LCRFs. Abbreviation generation (i.e., predicting abbreviation of a given full form) is important in Chinese NLP applications. For example, in an information retrieval (IR) system, it will be helpful if we can estimate abbreviations of a query. For the data of People X  X  Daily in Chinese, a large number of the articles contain only the abbreviations. Hence, successful abbreviation generation may improve the recall of IR systems. In addition, [Yang et al. 2009] showed that Chinese abbreviation generation can be used for vocab-ulary expansion and can improve the performance of Chinese voice search.

Abbreviation generation is closely related to abbreviation recognition [Sun et al. 2009b], and abbreviation recognition aims at extracting abbreviation-definition can-didate pairs from texts and then get correct abbreviation-definition pairs. We will in-troduce related work on both abbreviation generation and recognition. Compared with abbreviation recognition, there was much less previous work on abbreviation gener-ation. Early studies on abbreviation generation/recognition attempted to define the generic rules that humans abbreviate given words [Barrett and Grems 1960; Bourne and Ford 1961]. For example, Barrett and Grems [1960] defines 20 rules for abbreviat-ing terms; for example,  X  X lways save the first letter for each single word and the first letter for each phrase. X  However, it was unrealistic to design a set of generic rules that are applicable to any abbreviation definitions on different text corpora.
Since the late 1990s, researchers have presented various methods by which abbrevi-ation definitions appearing in actual texts are extracted [Torii et al. 2007]. Most studies share pattern (1) to locate a textual fragment with an abbreviation and its expanded form [Schwartz and Hearst 2003; Wren and Garner 2002]. Assuming we take ( l + 4 ) words appearing before the pare nthetical expression [Adar 2004], where l is the number of letters in the short form, the sentence,  X  X he exact route was determined by magnetic resonance imaging (MRI) X , could yield the textual fragment marked with the italic letters. The most challenging part of this task is to identify the boundary of an expanded form in the textual fragment if exists.
Existing methods of abbreviation recognition can be categorized into three groups: using hand-crafted heuristics and/or scoring rules [Adar 2004; Ao and Takagi 2005; Pakhomov 2002; Park and Byrd 2001; Schwartz and Hearst 2003; Taghva and Gilbreth 1999; Wren and Garner 2002; Yamamoto et al. 2011; Yu et al. 2002], statistics [Hisamitsu and Niwa 2001; Liu and Friedman 2003; Okazaki and Ananiadou 2006; Zhou et al. 2006], and machine learning [Chang and Sch  X  utze 2006; Kuo et al. 2009; Nadeau and Turney 2005; Okazaki et al. 2008; Xu and Huang 2006; Yeganova et al. 2010].

The first category proposed a variety of characteristics to identify abbreviation def-initions, for example, use of initials, capital letters, syllable boundaries, stop words, length of abbreviation, etc. Schwartz and Hearst [2003] implemented a simple algo-rithm that obtains the shortest expression containing all alpha-numerical letters of an abbreviation. Adar [2004] presented scoring rules to choose the most likely expanded form in multiple candidates, for example,  X  X dd one to the score for every abbrevia-tion character that is at the start of a definition word. X  Ao and Takagi [2005] designed more detailed (two-pages long in their article) conditions for accepting or discarding candidates of abbreviation definitions. However, hand-crafted rules are fragile to in-correct abbreviation definitions, for example, a patient with human immuno deficiency syndrome (AIDS) . Hand-crafted rules also encounter difficulties in finding an ex-panded form whose abbreviation is arranged in a different word order, for example, beta 2 adrenergic receptor (ADRB2) .

The second category focuses on the redundancy (repetition) of abbreviation defini-tions in a corpus. Hisamitsu and Niwa [2001] proposed a method that measures the co-occurrence strength between the inner and outer phrases of a parenthetical expression via mutual information,  X  2 test with Yate X  X  correction, Di ce coefficient, log-likelihood ratio, etc. Liu and Friedman [2003] based their method on collocations occurring before the parenthetical expressions. Enumerating candidates of expanded forms as colloca-tions appearing more than once in a text collection, their method eliminates unlikely candidates with rules such as  X  X emove a set of candidates T word to a candidate w if the number of such candidates T w and Ananiadou [2006] formalized the task of abbreviation recognition to extract tech-nical terms appearing before parenthetical expressions. They used a variant of C -value method [Frantzi and Ananiadou 1999] to score candidates of expanded forms.
To improve the accuracy of abbreviation recognition, the third category obtains ab-breviation rules by using a machine learning algorithm. The most popular approach is to classify or score each candidate of expanded forms by defining features that assess the appropriateness of the abbreviation definition. Chang and Sch  X  utze [2006] applied a logistic regression to combine nine features including  X  X he percentage of letters of an expanded form aligned at the beginning of a word, X   X  X he percentage of abbreviation let-ters aligned to the expanded form, X  etc. Nadeau and Turney [2005] employed Support Vector Machine (SVM) to classify candidates of abbreviation definitions into positive (definition) or negative (non-definition). The feature design is similar to that of Chang and Sch  X  utze [2006], for example,  X  X he number of participating definition letters that are capitalized, X  and  X  X he length (in words) of the definition. X  Xu and Huang [2006] also presented a similar method using SVM. Because these studies used a small num-ber of features that roughly correspond to the scoring rules used in the first category, these studies did not yield significantly better results than those with hand-crafted rules.

Recently, Kuo et al. [2009] proposed a set of rich features that represent the literal information, character properties, positional information, existence of stop words, recognition results from hand-crafted rules, etc. They reported that their machine learning method outperformed all baseline rule-based systems. Yeganova et al. [2010] explored to use  X  X aturally labeled data X , in which positive instances are naturally occurring potential abbreviation definition pairs in text, and negative instances are generated by randomly mixing potential abbreviations with unrelated potential definitions.

The previous studies presented so far focused only on global features that charac-terize an expanded form and abbreviation as a whole, for example, the number of abbreviation letters that appear at the head of a full form. In other words, these stud-ies did not model associations between abbreviation letters and their origins explicitly. In contrast, Okazaki et al. [2008] formalize the task of abbreviation recognition as a sequential alignment problem, which finds the optimal alignment (origins of abbrevi-ation letters) between an abbreviation and a full form. They designed a large num-ber features that directly express the events where letters produce or do not produce abbreviations. This approach also outperformed previous studies, but did not incor-porate global information (e.g., number of abbreviation letters) to the feature set be-cause the method assumes Markov assumption on the events producing abbreviation letters.

We close this section by describing the previous work on abbreviation genera-tion, which predicts an abbreviation for a given expanded form. Compared to the well-studied abbreviation recognition task, there was much less prior work on abbre-viation generation. Abbreviation generation is more difficult than abbreviation recog-nition, having broader search space (2 n candidates for a given expanded form of n letters) than that for abbreviation recognition (usually w candidates for a window of w words). Basically, abbreviation generation is formalized as a sequential labeling prob-lem: each character in the expanded form is tagged with a label, y produces the current character and the label S skips the current character .Tsuruoka et al. [2005] used Maximum Entropy Markov Model (MEMM), and Wakaki et al. [2009] used Conditional Random Fields (CRFs) for modeling the generation processes of abbreviations. Although these efforts have been made to advance the approach for abbreviation generation, abbreviation recognition generally yields better performance than generation. Conditional Random Fields are proposed as an alternative solution for structured clas-sification by solving  X  X he label bias problem X  [Lafferty et al. 2001]. Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f , the probability of a label sequence y conditioned on the observation sequence x is modeled as the following [Lafferty et al. 2001].
 where w is a parameter vector.
 Typically, computing  X  y exp w f ( y , x ) could be computationally intractable. This summation can be computed using dynamic programming techniques [Lafferty et al. 2001]. To make the dynamic programming techniques applicable, the depen-dencies of labels are chosen to obey the Markov property. This has a computational complexity of O ( NK M ) . N is the length of the sequence, K is the dimension of the label set, and M is the length of the Markov order used by local features.

Given a training set consisting of n labeled sequences, ( eter estimation is performed by maximizing the objective function The first term of this equation represents a conditional log-likelihood of a training data. The second term is a regularizer for reducing overfitting. We employed an L prior, R ( w ) = || w || 2 sample, log P ( y i | x i , w ) ,as ( i , w ) . The final objective function is the following. Real-world problems may contain hidden structures that are difficult to be captured by conventional structured classification models without latent variables. In such cases, models that exploit latent variables are advantageous in learning [Morency et al. 2007; Petrov and Klein 2008; Sun et al. 2009a, 2009b]. Therefore, as a representative struc-tured classification model with latent variables, the latent conditional random fields have become popular for performing a variety of tasks with hidden structures, for ex-ample, vision recognition [Morency et al. 2007], syntactic parsing [Petrov and Klein 2008], abbreviation generation/recognition [Sun et al. 2009b], and biomedical named entity recognition [Sun et al. 2009a]. For example, Morency et al. [2007] demonstrated that LCRF models can learn latent structures of vision recognition problems efficiently, and outperform several widely-used conventional models, such as support vector ma-chines, conditional random fields and hidden Markov models (HMMs). Petrov and Klein [2008] reported on a syntactic parsing task that LCRF models can learn more accurate grammars than that of the conventional techniques without latent variables.
To implicitly incorporate non-local information, we employ LCRFs [Morency et al. 2007; Petrov and Klein 2008] for abbreviating terms. The LCRF model uses latent vari-ables to capture additional information that may not be expressed by the observable labels. For example, using LCRFs, a possible feature could be  X  X he current character x = X , the label y effectively modeled in LCRFs, and the additional information at the previous position or many of the other positions in the past could be transferred via the latent variables. Figure 3 illustrates an example of hidden state sequences in LCRF-based abbreviation generator.

Using the label set Y ={ P , S } , abbreviation generation is formalized as the task of assigning a sequence of labels y = y 1 , y 2 , ... , y x = x Y . For each sequence, we also assume a sequence of latent variables h which are unobservable in training examples.
 The LCRF model is defined as the following [Morency et al. 2007].
 where w represents the parameter vector of the model. To make the training effi-cient, a restriction is made for the model: for each label, the latent variables associated with it have no intersection with the latent variables from other labels [Morency et al. 2007; Petrov and Klein 2008]. This simplification is also a popular practice in other latent conditional models, including hidden-state conditional random fields (HCRF) [Quattoni et al. 2007]. Each h is a member in a set H ( y the class label y ,and H ( y j )  X  H ( y k ) =  X  if y j = latent variables; that is, the union of all H ( y ) sets: can have any value from H ,but P ( y | h ) is zero except for only one of y in restriction indicates a discrete simplification of P ( y | where m is the length of the labeling: m =| y | . The formula h indicates that the latent-labeling h is a latent-labeling of the labeling y ,whichcanbe more formally defined as the following. Since sequences that have any h j /  X  H ( y j ) will by definition have P model can be simplified as the following.
 The item P ( h | x , w ) is defined by the usual conditional random field formulation. where f ( h , x ) is a global feature vector.

Given a training set consisting of n labeled sequences, ( eter estimation is performed by optimizing the objective function Training LCRFs is quite challenging. Traditional gradient-based batch training meth-ods, like Limited-memory BFGS (LBFGS) [Nocedal and Wright 1999], are slow on training LCRFs [Petrov and Klein 2008; Sun et al. 2009a]. For example, Sun et al. [2009a] showed that the training of LCRFs is quite costly on biomedical named entity recognition. To accelerate training speed, we try to employ online optimization meth-ods for training LCRFs. First, we review the literature on stochastic gradient descent (SGD). 3.3.1. Stochastic Gradient Descent. The SGD [Bottou and LeCun 2004] uses a small randomly-drawn subset of the training samples to approximate the gradient of an objective function. In this way, one can u pdate the model parameters much more fre-quently and speed up the convergence. Suppose  X  S is a randomly drawn subset of the full training set S , the stochastic objective function is then given by The extreme case is a batch size of 1, and it gives the maximum frequency of updates, which we adopt in this work. In this case, |  X  S |= 1and | S |= set contains n samples). In this case, we have where  X  S ={ i } . The model parameters are updated in the following way. where k is the update counter,  X  k is the learning rate. 3.3.2. SGD with Modifications (SGDM). In preliminary experiments, we find the standard SGD training is still slow in this task. The reason of the slowness is from the setting of the learning rates. The setting of learning rates often has a big impact on the conver-gence speed of the SGD training. A typical choice of learning rate is as follows [Collins et al. 2008]. where  X  0 is a constant. Although this scheduling guarantees theoretical ultimate con-vergence, the actual convergence speed can be slow in practice [Tsuruoka et al. 2009]. For our task, we want a more effective scheduling of learning rates to achieve faster convergence speed of the SGD training. We tested the following simple exponential decay [Tsuruoka et al. 2009]. where  X  is constant, with 0 &lt; X &lt; 1. Our experiments demonstrate that this expo-nential decay is more effective than the traditional decay (Equation 10), and arrives at a empirical convergence state faster. The main reason is that the exponential decay is more smooth than the traditional decay. The traditional decay schedule drops too quickly at the beginning and too slowly at the end. Although the exponential decay has no guarantee on theoretical ultimate convergence, this is not a problem for prac-tical applications. The reason is that the theoretical ultimate convergence is normally very slow, and the training has to be terminated when the training reaches empirical convergence. Our experiments demonstrate that the exponential decay schedule can reach empirical convergence 2 faster than the traditional decay schedule.
In addition, when the update gradients are sparse and the overall feature dimension is high, it will be wasteful to use the standard SGD updates, because at each update we need to apply regularization to all features, including those features that are not used in the current training sample. To make training faster, we use a heuristic lazy regularization schedule for the SGD. We can design the labels such that they explicitly incorporate non-local information. In this approach, the label y i at position i attaches the information of the abbreviation length generated by its previous labels, y 1 , y 2 , ... , y a Chinese abbreviation. In this encoding, a label not only contains the produceorskip information, but also the abbreviation-length information, that is, the label includes the number of all P labels preceding the current position. We refer to this method as label encoding with global information (hereinafter, GI ).

Note that the model-complexity is increased only by the increase in the number of labels. Since the length of the abbreviations is usually quite short (less than 5 for Chinese abbreviations and less than 10 for English abbreviations), the model is still tractable even when using the GI encoding.

The implicit (LCRF) and explicit (GI) solutions address the same issue concerning the incorporation of non-local information, and there are advantages to combining these two solutions. Therefore, we will combine the implicit and explicit solutions by employing the GI encoding in the LCRF (LCRF+GI). The effects of this combination will be demonstrated through experiments. Next, we design two types of features: language-independent features and language-specific features. Language-independent features can be used for abbreviating terms in English and Chinese. We use the features from #1 to #3 listed in Table I.
Feature templates #4 to #7 in Table I are used for Chinese abbreviations. Templates #4 and #5 express the Pinyin reading of the characters, which represents a Roman-ization of the sound. Templates #6 and #7 are designed to detect character duplica-tion, because identical characters will normally be skipped in the abbreviation process. On the other hand, such duplication detection features are not so useful for English abbreviations.

Feature templates #8 X #11 are designed for English abbreviations. Features #8 and #9 encode the orthographic information of expanded forms. Features #10 and #11 rep-resent a contextual n -gram with a large window size. Since the number of letters in Chinese (more than 10 K characters) is much larger than the number of letters in En-glish (26 letters), in order to avoid a possible overfitting problem, we did not apply these feature templates to Chinese abbreviations.
Chinese newswire data. For Chinese newswire abbreviation generation, we used the corpus of Sun et al. [2008a], which contains 2,914 abbreviation definitions for train-ing, and 729 pairs for testing. This corpus consists primarily of noun phrases (38%), organization names (32%), and verb phrases (21%).

English biomedical data. For English biomedical abbreviation generation, we eval-uated on the corpus of Tsuruoka et al. [2005]. This corpus contains 1,200 aligned pairs extracted from MEDLINE biomedical abstr acts published in 2001. For both tasks, we converted the aligned pairs of the corpora into labeled full forms and used the labeled full forms as the training/testing data. We employ the feature templates defined in Section 3.5, taking into account these 81,827 features for the Chinese abbreviation generation task, and the 50,149 features for the English abbreviation generation task. We use three latent variables for each original label in LCRFs. 3 We will show the experimental results on varying the number of latent variables.
 For numerical optimization, we performed a gradient descent with the Limited-Memory BFGS (LBFGS) optimization technique [Nocedal and Wright 1999]. LBFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approx-imation, LBFGS can handle large-scale problems efficiently. Since the objective func-tion of the LCRF model is non-convex, different parameter initializations normally bring different optimization results. Therefore, to approach closer to the global opti-mal point, it is recommended to perform multiple experiments on LCRFs with random initialization and then select a good start point.

Note that, for the label encoding with global information, many label transitions (e.g., P 2 S 3 ) are actually impossible: the label transitions are strictly constrained, that is, y i y i + 1  X  X  P j S j , P j P j+1 , S j P j+1 , backward lattice are enforced by giving appropriate features a weight of forcing all forbidden labelings to have zero probability. Sha and Pereira [2003] origi-nally proposed this concept of implementing transition restrictions.

The evaluation metrics used in the abbreviation generation are exact-match accu-racy (hereinafter accuracy ), including top-1 accuracy, top-2 accuracy, and top-3 accu-racy. The top-N accuracy represents the percentage of correct abbreviations that are covered, if we take the top N candidates from the ranked labelings of an abbreviation generator. First, we present the results of the Chinese abbreviation generation task, as listed in Table II. To evaluate the impact of using latent variables, we chose a representative non-latent model, the CRF model, as a baseline. We compared the performance of the LCRF with the CRFs and other baseline systems, including the heuristic system (Heu), the HMM model, and the SVM model described in Sun08 , that is, Sun et al. [2008a]. The heuristic method is a simple rule that produces the initial character of each word to generate the corresponding abbreviation. The SVM method described by Sun et al. [2008a] is formalized as a regression problem, in which the abbreviation candidates are scored and ranked.

The results revealed that the latent variable model considerably improved the performance over the CRF model. All of its top-1, top-2, and top-3 accuracies were consistently better than those of the CRF model. Therefore, this demonstrated the effectiveness of using the latent variables in Chinese abbreviation generation. The experimental results on varying the number of latent variables are shown in Figure 5. Since LCRF+GI performed well, the experiments are based on LCRF+GI.

As the case for the two alternative approaches for incorporating non-local informa-tion, the latent variable method and the label encoding method competed with one another (see LCRF vs. CRF+GI). The results showed that the latent variable method outperformed the GI encoding method by +0.8% on the top-1 accuracy. The reason for this could be that the label encoding approach is a solution without the adaptiv-ity on different instances. We will present a detailed discussion comparing LCRF and CRF+GI for the English abbreviation generation task in the next subsection, where the difference is more significant.

In contrast, to a larger extent, the results demonstrate that these two alternative approaches are complementary. Using the GI encoding further improved the perfor-mance of the LCRF, with +4.7% on top-1 accuracy. We found that major improvements were achieved through the more exact control of the output length. To perform a de-tailed analysis, we collected the statistics of the length distribution (see Figure 5) and found that the GI encoding improved the abbreviation length distribution of the LCRF.
The proposed method, the latent variable model with GI encoding, is 9.6% better with respect to the top-1 accuracy compared to the best system on this corpus, namely, the SVM regression method. Furthermore, the top-3 accuracy of the latent variable model with GI encoding is as high as 94.9%, which is quite encouraging for practical usage. In the English abbreviation generation task, we randomly selected 1,481 instances from the generation corpus for training, and 370 instances for testing. Table III shows the experimental results. We compared the performance of the LCRF with the perfor-mance of the CRFs. Whereas the use of the latent variables still improves the gen-eration performance, using the GI encoding hurted the performance in this task. In comparing the implicit and explicit solutions for incorporating non-local information, we can see that the implicit approach with LCRFs performs much better than the ex-plicit approach with the GI encoding. An example is shown in Figure 6. The CRF+GI produced a Viterbi labeling with a low probability, which is an incorrect abbrevia-tion. The LCRF produced the correct labeling. The experimental results on varying the number of latent variables are shown in Figure 7. Since LCRF performed well in this dataset, the experiments are based on LCRF.
 To perform a systematic analysis of the superior-performance of LCRF compare to CRF+GI, we collected the probability distributions (see Figure 7) of the Viterbi label-ings from these models (see  X  X CRF vs. CRF+GI X ). The curves suggest that the data sparseness problem could be the reason for the differences in performance. A large percentage (37.9%) of the Viterbi labelings from the CRF+GI (ENG) have very small probability values ( p &lt; 0.1). For the LCRF (ENG), there were only a few (0.5%) Viterbi labelings with small probabilities. Since English abbreviations are often longer than Chinese abbreviations ( length &lt; 10 in English, whereas length the GI encoding resulted in a larger label set in English. Hence, the features become more sparse than in the Chinese case. In addition, the training data of the English task is much smaller than the Chinese task, which could make the models more sen-sitive to data sparseness. Therefore, a significant number of features could have been inadequately trained, resulting in Viterbi labelings with low probabilities. For the la-tent variable approach, its curve demonstrates that it did not cause a severe data sparseness problem.
 The aforementioned analysis also explains the poor performance of the LCRF+GI. However, the LCRF+GI can actually produce correct abbreviations with high probabil-ities in some difficult instances. In Figure 8, the LCRF produced an incorrect labeling for the difficult long form, whereas the L CRF+GI produced the co rrect labeling con-taining non-initials.

Hence, we present a simple voting method to better combine the latent variable ap-proach with the GI encoding method. We refer to this new combination as GI encoding with back-off (hereinafter GIB ): when the abbreviation generated by the LCRF+GI has a confident probability ( p &gt; 0.3 in the present case), the LCRF+GI then outputs it. Otherwise, the system backs-offs to the parameters trained without the GI encoding, that is, the LCRFs.

The results in Table III demonstrate that the LCRF+GIB model significantly out-performed the other models because the LCRF+GI model improved the performance in some difficult instances. The LCRF+GIB model was robust even when the data sparseness problem was severe.
 To strictly compare with the previous results in Tsuruoka et al. [2005], following Tsuruoka et al. [2005], we performed a five-fold cross-validation on the corpus. Our LCRF method achieved the T1A of 57.5%, which is better than the T1A in Tsuruoka et al. [2005]. In Tsuruoka et al. [2005], the highest T1A report is 55.2% by using a maximum entropy Markov model (MEMM). Here, we perform experiments on optimizing objective functions of CRFs and LCRFs on different data sets. We compare our SGD with modifications (SGDM) with the pop-ular LBFGS batch training method and the standard SGD training method. For the SGDM method, we need to tune the value of  X  0 and  X  . The tuning of performed in such a way that follows previous work [Tsuruoka et al. 2009]. We tested the three training methods on CRF/LCRF and different data sets, with English biomedical data and Chinese newswire data. The curves of the objective functions by varying training iterations are shown in Figure 9. As we can see, in all cases compared with the LBFGS batch training method, the SGDM method achieved the same level of objective values on convergence. Most importantly, in most of the cases, the SGDM method converges much faster than the LBFGS batch training method. The detailed training-time and iterations of SGDM and LBFGS are shown in Table IV 4 .

It is noteworthy that the weights produced by the SGDM and the LBFGS training are very similar when the two methods optimized the objective function to the same optimum. Hence, in spite of the significant acceleration of training speed, abbreviation generation accuracies are almost the same when we use the SGDM training instead of the LBFGS training. We do not repeat the accuracy results here. We have presented the latent conditional random fields and the GI encoding to incor-porate non-local information in abbreviating terms. We have shown that each of those two methods can be used to model non-local information. On the other hand, more im-portantly, we showed that the two approaches were complementary to each other. By combining the two approaches, we were able to achieve state-of-the-art performance in abbreviation generation in the same model, across different languages, and with a simple feature set. Not surprisingly, experimental results showed that learning non-local information may slow down the training speed. To solve this problem, we further presented an online training method, which can arrive at the same optimum with ac-celerated training speed.

As discussed earlier herein, the training data is relatively small. Since there are numerous unlabeled full forms on the web, it is possible to use a semi-supervised ap-proach in order to make use of such raw data. This is an area for future research.
