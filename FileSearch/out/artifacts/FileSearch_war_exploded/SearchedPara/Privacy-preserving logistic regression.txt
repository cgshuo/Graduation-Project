 Privacy-preserving machine learning is an emerging problem, due in part to the increased reliance on the internet for day-to-day tasks such as banking, shopping, and social networking. Moreover, pri-vate data such as medical and financial records are increasingly being digitized, stored, and managed by independent companies. In the literature on cryptography and information security, data privacy definitions have been proposed, however designing machine learning algorithms that adhere to them has not been well-explored. On the other hand, data-mining algorithms have been introduced that aim to respect other notions of privacy that may be less formally justified.
 Our goal is to bridge the gap between approaches in the cryptography and information security com-munity, and those in the data-mining community. This is necessary, as there is a tradeoff between the privacy of a protocol, and the learnability of functions that respect the protocol. In addition to the specific contributions of our paper, we hope to encourage the machine learning community to embrace the goals of privacy-preserving machine learning, as it is still a fledgling endeavor. In this work, we provide algorithms for learning in a privacy model introduced by Dwork et al. [6]. The -differential privacy model limits how much information an adversary can gain about a par-ticular private value, by observing a function learned from a database containing that value, even if she knows every other value in the database. An initial positive result [6] in this setting depends on the sensitivity of the function to be learned, which is the maximum amount the function value can change due to an arbitrary change in one input. Using this method requires bounding the sensitivity of the function class to be learned, and then adding noise proportional to the sensitivity. This might be difficult for some functions that are important for machine learning. The contributions of this paper are as follows. First we apply the sensitivity-based method of design-ing privacy-preserving algorithms [6] to a specific machine learning algorithm, logistic regression. Then we present a second privacy-preserving logistic regression algorithm. The second algorithm is based on solving a perturbed objective function, and does not depend on the sensitivity. We prove that the new method is private in the -differential privacy model. We provide learning performance guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. Finally, we provide experiments demonstrating superior learning performance of our new method, with respect to the algorithm based on [6]. Our technique may have broader applications, and we show that it can be applied to certain classes of optimization problems. 1.1 Overview and related work At the first glance, it may seem that anonymizing a data-set  X  namely, stripping it of identifying However, this is problematic, because an adversary may have some auxiliary information, which may even be publicly available, and which can be used to breach privacy. For more details on such attacks, see [12].
 auxiliary knowledge by the adversary. The definition we use is due to Dwork et al. [6], and has been used in several applications [4, 11, 2]. We explain this definition and privacy model in more detail in Section 2.
 Privacy and learning. The work most related to ours is [8] and [3]. [8] shows how to find classifiers that preserve -differential privacy; however, their algorithm takes time exponential in d for inputs in R d . [3] provides a general method for publishing data-sets while preserving -differential privacy such that the answers to all queries of a certain type with low VC-dimension are approximately correct. However, their algorithm can also be computationally inefficient.
 Additional related work. There has been a substantial amount of work on privacy in the literature, spanning several communities. Much work on privacy has been done in the data-mining community [1, 7], [14, 10], however the privacy definitions used in these papers are different, and some are sus-ceptible to attacks when the adversary has some prior information. In contrast, the privacy definition we use avoids these attacks, and is very strong. Before we define the privacy model that we study, we will note a few preliminary points. Both in that model, and for our algorithm and analyses, we assume that each value in the database is a real vector with norm at most one. That is, a database contains values x 1 ,...,x n , where x i  X  R d , and k x i k  X  1 for all i  X  { 1 ,...,n } . This assumption is used in the privacy model. In addition, we assume that when learning linear separators, the best separator passes through the origin. Note that this is not an assumption that the data is separable, but instead an assumption that a vector X  X  classification is based on its angle, regardless of its norm.
 In both privacy-preserving logistic regression algorithms that we state, the output is a parameter its Euclidean norm. For a function G ( x ) defined on R d , we use  X  G to denote its gradient and  X  2 G to denote its Hessian.
 Privacy Definition. The privacy definition we use is due to Dwork et al. [6, 5]. In this model, users have access to private data about individuals through a sanitization mechanism , usually denoted by M . The interaction between the sanitization mechanism and an adversary is modelled as a sequence of queries, made by the adversary, and responses, made by the sanitizer. The sanitizer, which is typically a randomized algorithm, is said to preserve -differential privacy, if the private value of any one individual in the data set does not affect the likelihood of a specific answer by the sanitizer by more than .
 More formally, -differential privacy can be defined as follows. Definition 1 A randomized mechanism M provides -differential privacy, if, for all databases D 1 and D 2 which differ by at most one element, and for any t , It was shown in [6] that if a mechanism satisfies -differential privacy, then an adversary who knows the private value of all the individuals in the data-set, except for one single individual, cannot figure out the private value of the unknown individual, with sufficient confidence, from the responses of the sanitizer. -differential privacy is therefore a very strong notion of privacy. [6] also provides a general method for computing an approximation to any function f while preserv-ing -differential privacy. Before we can describe their method, we need a definition.
 Definition 2 For any function f with n inputs, we define the sensitivity S ( f ) as the maximum, over all inputs, of the difference in the value of f when one input of f is changed. That is, [6] shows that for any input x 1 ,...,x n , releasing f ( x 1 ,...,x n ) +  X  , where  X  is a random variable drawn from a Laplace distribution with mean 0 and standard deviation S ( f ) preserves -differential privacy.
 In [13], Nissim et al. showed that given any input x to a function, and a function f , it is sufficient to draw  X  from a Laplace distribution with standard deviation SS ( f ) , where SS ( f ) is the smoothed-sensitivity of f around x . Although this method sometimes requires adding a smaller amount of noise to preserve privacy, in general, smoothed sensitivity of a function can be hard to compute. Based on [6], one can come up with a simple algorithm for privacy-preserving logistic regression, which adds noise to the classifier obtained by logistic regression, proportional to its sensitivity. From Corollary 2, the sensitivity of logistic regression is at most 2 n X  . This leads to Algorithm 1, which obeys the privacy guarantees in Theorem 1.
 Algorithm 1: all i . Then, Algorithm 1 preserves -differential privacy.
 P
ROOF : The proof follows by a combination of [6], and Corollary 2, which states that the sensitivity of logistic regression is at most 2 n X  .
 Lemma 1 Let G ( w ) and g ( w ) be two convex functions, which are continuous and differentiable at all points. If w 1 = argmin w G ( w ) and w 2 = argmin w G ( w ) + g ( w ) , then, || w 1  X  w 2 || X  g 1 G g = max w || X  g ( w ) || and G 2 = min v min w v T  X  2 G ( w ) v , for any unit vector v . The main idea of the proof is to examine the gradient and the Hessian of the functions G and g around w 1 and w 2 . Due to lack of space, the full proof appears in the full version of our paper. || x i || X  1 , the sensitivity of logistic regression with regularization parameter  X  is at most 2 n X  . P ROOF : We use a triangle inequality and the fact that G 2  X   X  and g 1  X  1 n .
 Learning Performance. In order to assess the performance of Algorithm 1, we first try to bound the performance of Algorithm 1 on the training data. To do this, we need to define some notation. For a classifier w , we use L ( w ) to denote the expected loss of w over the data distribution, and  X  L ( w ) to denote the empirical average loss of w over the training data. In other words,  X  L ( w ) =  X  f by the following lemma.
 Lemma 3 Given a logistic regression problem with regularization parameter  X  , let w 1 be the classi-fier that minimizes  X  f  X  , and w 2 be the classifier output by Algorithm 1 respectively. Then, with prob-Due to lack of space, the proof is deferred to the full version.
 From Lemma 3, we see that performance of Algorithm 1 degrades with decreasing  X  , and is poor in particular when  X  is very small. One question is, can we get a privacy-preserving approximation to logistic regression, which has better performance bounds for small  X  ? To explore this, in the next section, we look at a different algorithm. In this section, we provide a new privacy-preserving algorithm for logistic regression. The input to y ,...,y n for the examples, a regularization constant  X  and a privacy parameter , and the output is a vector w  X  in R d . Our algorithm works as follows.
 Algorithm 2: We observe that our method solves a convex programming problem very similar to the logistic regression convex program, and therefore it has running time similar to that of logistic regression. In the sequel, we show that the output of Algorithm 2 is privacy preserving.
 Theorem 2 Given a set of n examples x 1 ,...,x n over R d , with labels y 1 ,...,y n , where for each i , || x i || X  1 , the output of Algorithm 2 preserves -differential privacy.
 P
ROOF : Let a and a 0 be any two vectors over R d with norm at most 1 , and y,y 0  X  ( x value of b that maps the input to the output. This uniqueness holds, because both the regularization function and the loss functions are differentiable everywhere.
 Let the values of b for the first and second cases respectively, be b 1 and b 2 .
 Since w  X  is the value that minimizes both the optimization problems, the derivative of both opti-mization functions at w  X  is 0 .
 This implies that for every b 1 in the first case, there exists a b 2 in the second case such that: b 1  X  for any pair ( a,y ) , ( a 0 ,y 0 ) , theorem follows.
 We notice that the privacy guarantee for our algorithm does not depend on  X  ; in other words, for any value of  X  , our algorithm is private. On the other hand, as we show in Section 5, the performance of our algorithm does degrade with decreasing  X  in the worst case, although the degradation is better than that of Algorithm 1 for  X  &lt; 1 .
 Other Applications. Our algorithm for privacy-preserving logistic regression can be generalized to provide privacy-preserving outputs for more general convex optimization problems, so long as the problems satisfy certain conditions. These conditions can be formalized in the theorem below. Theorem 3 Let X = { x 1 ,...,x n } be a database containing private data of individuals. Suppose we would like to compute a vector w that minimizes the function F ( w ) = G ( w ) + P n i =1 l ( w,x i ) , over w  X  R d for some d , such that all of the following hold: Let b = B  X   X  b , where B is drawn from  X ( d, 2  X  ) , and  X  b is drawn uniformly from the surface of a d -provides -differential privacy. In this section, we show theoretical bounds on the number of samples required by the algorithms to learn a linear classifier. For the rest of the section, we use the same notation used in Section 3. First we show that, for Algorithm 2, the values of  X  f  X  ( w 2 ) and  X  f  X  ( w 1 ) are close. Lemma 4 Given a logistic regression problem with regularization parameter  X  , let w 1 be the clas-sifier that minimizes  X  f  X  , and w 2 be the classifier output by Algorithm 2 respectively. Then, with The proof is in the full version of our paper. As desired, for  X  &lt; 1 , we have attained a tighter bound using Algorithm 2, than Lemma 3 for Algorithm 1.
 Now we give a performance guarantee for Algorithm 2.
 Theorem 4 Let w 0 be a classifier with expected loss L over the data distribution. If the training ex-amples are drawn independently from the data distribution, and if n &gt; C max( || w 0 || 2 2 for some constant C , then, with probability 1  X   X  , the classifier output by Algorithm 2 has loss at most L + g over the data distribution.
 P
ROOF : Let w  X  be the classifier that minimizes f  X  ( w ) over the data distribution, and let w 1 and w 2 can use an analysis as in [15] to write that: w output by Algorithm 2 is at most L ( w 0 ) + g .
 The same technique can be used to analyze the sensitivity-based algorithm, using Lemma 3, which yields the following.
 Theorem 5 Let w 0 be a classifier with expected loss L over the data distribution. If classifier output by Algorithm 2 has loss at most L + g over the data distribution.
 It is clear that this bound is never lower than the bound for Algorithm 2. Note that for problems in which (non-private) logistic regression performs well, k w 0 k X  1 if w 0 has low loss, since otherwise one can show that the loss of w 0 would be lower bounded by log(1 + 1 e ) . Thus the performance guarantee for Algorithm 2 is significantly stronger than for Algorithm 1, for problems in which one would typically apply logistic regression. We include some simulations that compare the two privacy-preserving methods, and demonstrate formance terribly, from that of standard logistic regression. Performance degradation is inevitable however, as in both cases, in order to address privacy concerns, we are adding noise, either to the learned classifier, or to the objective.
 In order to obtain a clean comparison between the various logistic regression variants, we first ex-perimented with artificial data that is separable through the origin. Because the classification of a vector by a linear separator through the origin depends only its angle, not its norm, we sampled the data from the unit hypersphere. We used a uniform distribution on the hypersphere in 10 dimensions with zero mass within a small margin ( 0 . 03 ) from the generating linear separator. Then we experi-mented on uniform data that is not linearly separable. We sampled data from the surface of the unit ball in 10 dimensions, and labeled it with a classifier through the origin. In the band of margin  X  0 . 1 with respect to the perfect classifier, we performed random label flipping with probability 0 . 2 . For our experiments, we used convex optimization software provided by [9].
 Figure 1 gives mean and standard deviation of test error over five-fold cross-validation, on 17,500 points. In both simulations, our new method is superior to the sensitivity method, although incurs more error than standard (non-private) logistic regression. For both problems, we tuned the logistic regression parameter,  X  , to minimize the test error of standard logistic regression, using five-fold cross-validation on a holdout set of 10,000 points (the tuned values are:  X  = 0 . 01 in both cases). For each test error computation, the performance of each of the privacy-preserving algorithms was evaluated by averaging over 200 random restarts, since they are both randomized algorithms. In Figure 2a)-b) we provide learning curves. We graph the test error after each increment of 1000 points, averaged over five-fold cross validation. The learning curves reveal that, not only does the Figure 2: Learning curves: a) Uniform distribution, margin=0.03, b) Unseparable data.
 Epsilon curves: c) Uniform distribution, margin=0.03, d) Unseparable data. new method reach a lower final error than the sensitivity method, but it also has better performance at most smaller training set sizes.
 In order to observe the effect of the level of privacy on the learning performance of the privacy-preserving learning algorithms, in Figure 2c)-d) we vary , the privacy parameter to the two algo-rithms, on both the uniform, low margin data, and the unseparable data. As per the definition of -differential privacy in Section 2, strengthening the privacy guarantee corresponds to reducing . Both algorithms X  learning performance degrades in this direction. For the majority of values of that we tested, the new method is superior in managing the tradeoff between privacy and learning performance. For very small , corresponding to extremely stringent privacy requirements, the sen-sitivity method performs better but also has a predication accuracy close to chance, which is not useful for machine learning purposes. In conclusion, we show two ways to construct a privacy-preserving linear classifier through logistic ing the -differential privacy definition of Dwork et al. [6], we prove that our new algorithm is privacy-preserving. We provide learning performance guarantees for the two algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. In simulations, our new algorithm outperforms the method based on [6].
 Our work reveals an interesting connection between regularization and privacy: the larger the reg-ularization constant, the less sensitive the logistic regression function is to any one individual ex-ample, and as a result, the less noise one needs to add to make it privacy-preserving. Therefore, regularization not only prevents overfitting, but also helps with privacy, by making the classifier less sensitive. An interesting future direction would be to explore whether other methods that prevent overfitting also have such properties.
 Other future directions would be to apply our techniques to other commonly used machine-learning algorithms, and to explore whether our techniques can be applied to more general optimization problems. Theorem 3 shows that our method can be applied to a class of optimization problems with certain restrictions. An open question would be to remove some of these restrictions. Acknowledgements. We thank Sanjoy Dasgupta and Daniel Hsu for several pointers.

