 Most of the research on Arabic is focused on Mo d-ern Standard Arabic . D ialectal varieties have not received much attention due to the lack of dialectal tools and annotated texts (Duh and Kirchoff, 2005). In this paper, we present a rule -based m e-thod to generate Colloquial Egyptian Arabic (CEA) from Modern Standard Arabic (MSA), relying on seg ment -based part -of -speech tags. The transf o r-mation process relies on the observation that d i-alectal varieties of Arabic differ mainly in the use of affixes and function words while the word stem mostly remains unchanged. For example , g iven the Buckwalter -encoded MSA sentence  X  AlA xwAn Almslm wn lm yfwzwA fy AlAntxbAt  X  the rules pr o-duce  X  AlAxwAn Almslm yn mfAzw$ f AlAntxAbAt  X  (  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X   X  X  X  X  X  X   X   X  X  X  X  X  X  X  X  X  X  , The Muslim Br o-therhood did not win the elections). The availabil i-ty of segment -based part -of -speech tags is essential since many of the affixes in MSA are ambiguous. For example, lm could be either a negative particle or a question work, and the word AlAxwAn could be either made of two segments ( Al+ &lt; xwAn , the brothers), or three segments ( Al+&gt;xw+An , the two brothers).

We first introduce the transfor mation rules, and show that in many cases it is feasible to transform MSA to CEA, although there are cases that requ ire much more than POS tags. We then provide a ty p-ical case in which we utilize the transformed text of the Arabic Treebank (Bies and Maamouri, 2003) to build a part -of -speech tagger for CEA. The ta g-ger improves the accuracy of POS tagging on a u-thentic Egyptian Arabic by 13% absolute (from 73.24% to 86.84%) and reduces the percent age of out -of -vocabul ary words from 28 .98% to 16.66% . 2. MSA to CEA Conversion Rules Table 1 shows a sentence in MSA and its CEA counterpart. Both can be translated into:  X  X e did not write it for them.  X  MSA has three words while CEA is more synthetic as the preposition and the negative particle turn into clitics. Table 1 ill u-strates the end product of one of the Imperfect transformation rules, namely the case where the Imperfect Verb is preceded by the negative particle lm .
 Our 103 rules cover nominals (number and case affixes), verbs (tense, number, gender, and modal i-ty), pronouns (number and gender), and demo n-str a tive pronouns (number and gender). 
The rules also cover certain lexical items as 400 words in MSA have been converted to their co m-mon CEA counterparts. Examples of lexical co n-versions include ZlAm and Dlmp (darkness), rjl and rAjl (man), rjAl and rjAlp (men), and kvyr and ktyr (many), where the first word is the MSA ve r-sion and the second is the CEA version.
 Many of the lexical ma ppings are ambiguous. For example, the word rjl can either mean man or leg . When it means man , the CEA form is rAjl , but the word for leg is the same in both MSA and CEA. While they have different vowel patterns ( rajul and rijol respectively), the vowel inform a-tion is harder to get correctly than POS tags. The problem may arise especially when dealing with raw data for which we need to provide POS tags (and vowels) so we may be able to convert it to the colloquial form . Below, w e provide two sample rules : the negated past, for which CEA uses the perfect v erb. What makes things more complicated is that CEA treats negative particles and prepositional phrases as clitics. An example of this is the word mktbthlhm$ (I did not write it for the m ) in T able 1 above. It is made of the negative particle m , the stem ktb (to write), the object pronoun h , the pr e-position l , the pronoun hm (them) and the negative particle $. Figure 1, and the following steps show the conversions of lm nktbhA lhm to mktbnhAlhm $ : orthography is not standardized, many affixes and clitics can be written in different ways. For exa m-ple, the word mktbnhlhm$, can be written in 24 ways . All these forms are legal and possible, as attested by their existence in a CEA corpus (the Arabic Online Commentary Dataset v1.1 ), which we also use for building a language model later . MSA possessive pronouns inflect for gender, nu m-ber (singular, dual, and plural), and person. In CEA, there is no distinction between the dual and the plural, and a single pr onoun is used for the plural feminine and masculine . The three MSA forms ktAb hm , ktAb hmA and ktAb hn ( their book for the masculine plural, the dual, and the feminine plural respectively ) all collapse to ktAb hm . Table 2 has examples of some other rules we have applied. We note that the stem, in bold, hardly changes, and that the changes mainly affect fun c-tion segments . The last example is a lexical rule in which the stem has to change.
 3. POS Tagging Egyptian Arabic We use the conversion above to build a POS tagger for Egyptian Arabic. We follow Mohamed and Kuebler (2010) in using whole word tagging, i.e., without any word segmentation . We use the C o-lumbia Arabic Treebank 6 -tag tag set: PRT (Pa r-ticle) , NOM (Nouns, Adjectives, and Adverbs) , PROP (Proper Nouns) , VRB (Verb) , VRB -pass (Passive Verb) , and PNX (Punctuation) (Habash and Roth, 2009). For example, the word wHnktblhm ( and we will write to them ,  X  X  X  X  X  X  X  X  X  ) receives the tag PRT+PRT+VRB+PRT+NOM.
 This results in 58 composite tags, 9 of which occur 5 times or less in the converted ECA training set.
We converted two sections of the Arabic Tre e-bank (ATB): p2v3 and p3v2. For all the POS ta g-ging experiments, we use the m emory -based POS tagger (MBT) (Daelemans et al., 1996) The best results, tuned on a dev set, were obtained, in non -exhaustive search, with the Modified Value Di f-ference Metric as a distance metric and with k (the number of nearest neighbors) = 25. For known words, we use the IGTree algorithm and 2 words to the left, their POS tags, the focus word and its list of possible tags, 1 right context word and its list of possible tags as features. For unknown words, we use the IB1 algorithm and the word itself, its first 5 and last 3 characters, 1 left context word and its POS tag, and 1 right context word and its list of possible tags as features. 3.1. Development and Test Data As a development set, we use 100 user -contributed comments (2757 words) from the website m a-srawy.com , which were judged to be highly coll o-quial. The test set contains 192 comments (7092 words) from the same website with the same crit e-rion. The dev elopment and test sets were hand -annotated with composite tags as illustrated above by two native Arabic -speaking students .
 ling errors ( mostly run -on words). The most co m-mon of these is the vocative particle yA , which is usually attached to following word (e.g. yArAjl , (you man ,  X  X  X  X  X  X  ) ). It is not clear whether it should be treated as a proclitic , since it also occurs as a separate word, which is the standard way of wri t-ing. The same holds true for the variation between the letters * and z , (  X  and  X  in Arabic ) which are pronounced exactly the same way in CEA to the extent that the substitution may not be considered a spelling error. 3.2. Experiments and Results We ran five experiments to test the effect of MSA to CEA conversion on POS tagging: (a) Standard , where we train the tagger on the ATB MSA data, (b) 3 -gram LM , where for each MSA sentence we generate all transformed sentences (see Section 2.1 and Figure 1) and pick the most probable sentence according to a trigram language model built from an 11.5 million words of user contributed comments. 1 This corpus is highly dialectal Egyptian Arabic, but like all similar collections, it is diglossic and demonstrates a high degree of code -switching between MSA and CEA. We use the SRILM toolkit (Stolcke, 2002) for language modeling and sentence scoring , (c) R andom , where we choose a random sentence from all the correct sentences gene rated for each MSA sentence, (d) Hybrid , where we combine the data in a) with the best settings (as measured on the dev set) using the converted colloquial data (namely experiment c) . Hybridization is necessary since most Ara bic data in blogs and comments are a mix of MSA and CEA, and (e) Hybrid + dev , where we enrich the Hybrid training set with the dev data . We use the following metric s for evaluation: KWA : Known Word Accuracy (%) , UW A : Unknown Word Accuracy (%) , TA : Total Accuracy (%) , and UW : unknown words (%) in the respective set in the respective experiment . Table 3(a) presents the results on the dev elopment set while Table 3(b) the results on the test set.
Experiment KWA UWA TA UW (a) Standard 92.75 39.68 75.77 31.99 (b) 3 -gram LM 89.12 43.46 76.21 28.29 (c) Random 92.36 43.51 79.25 26.84 (d) Hybrid 94.13 52.22 84.87 22.09 We notice that randomly selecting a sentence from the correct generated sentences yields better results than choosing the most probable sentence accor d-ing to a language model. The reason for this may be that randomization guarantees more coverage of the various form s . We have found that the vocab u-lary size (the number of unique word ty pes) for the training set generated for the Random experiment is considerably la rger than the vocabulary size for the 3 -gram LM experiment (55367 unique word types in Random versus 51306 in 3 -gram LM ) , which results in a drop of 4.6% absolute in the pe r-cen tage of unknown words: 27.31% versus 22.30 % ). This drop in the percentage of unknown words may indicate that generating a ll possible variations of CEA may be more useful than using a language model in general. Even in a CEA corpus of 35 million words , one third of the words gene r-ated by the rules are not in the corpus, while many of these are in both the test set and the develo p-ment set.
 (a) Standard 89.03 40.67 73.24 28.98 (b) 3 -gram LM 84.33 47.7 0 74.32 27.31 (c) Random 90.24 48.90 79.67 22.70 (d) Hybrid 92.22 53.92 83.81 19.45 (e) Hybrid+dev 94.87 56.46 86.84 16.66
We also notice that the conversion alone i m-proves tagging accuracy from 75.77% to 79.25% on the dev elopment set, and from 73.24% to 79. 67% on the test set. Combining the original MSA and the best scoring converted data (Ra n-dom ) raises the accuracies to 84.87% and 83.81% respectively. T he percentage of unknown words drops from 29.98% to 19.45% in the test set when we used the hybrid data . The fact that the perce n-tage of unknown words drops further to 16.66% in the Hybrid+dev experiment points out the authe n-tic colloquial data contains elements that have not been captured using conversion alone. 4. Related Work To the best of our knowledge, ours is the first work that generates CEA automatically from morpholo g-ically disambiguated MSA, but Habash et al . (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. MAGED incorporates the morphology, phonology, and o r-thography of several Arabic dialects. Diab et al . (2010) worked on the annotation of dialectal Ara b-ic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval.

Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morph o-logical analyzer and information gleaned from the intersection of several Arabic dialects. This is di f-ferent from our approach for which POS tagging is only an application. Our focus is to use any exis t-ing MSA data to generate colloquial Arabic r e-sources that can be used in virtually any NLP task. At a higher level, our work resem bles that of Kundu and Roth (2011), in which they chose to adapt the text rather than the model. While they ada pted the test set, we do so at the training set level. 5. Conclusions and Future Work We have a presented a method to convert Modern Standard Arabic to Egyptian Colloquial Arabic with an example application to the POS tagging task. This approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation, for example.
 morphological in nature, they have proved useful in handling colloquial data. However, morphology alone is not enough for handling key points of di f-ference between CEA and MSA. While CEA is mainly an SVO language, MSA is mainly VSO, and while demonstratives are pre -nominal in MSA, they are post -nominal in CEA. These phenomena can be handled only through syntactic conversion. We expect that convert ing a dependency -based treebank to CEA can account for many of the ph e-nomena part -of -speech tags alone cannot handle
We are planning to extend the rules to other li n-guistic phenomena and dialects, with possible a p-plications to various NLP tasks for which MSA annotated data exist. When no gold standard se g-ment -based POS tags are available, tools that pr o-duce segment -based annotation can be used, e.g. s egment -based POS tagging (Mohamed and Kue b-ler, 2010) or MADA (Habash et al, 2009) , although these are not expected to yield the same results as gold standard part -of -speech tags.
 This publication was made possible by a NPRP grant (NPRP 09 -1140 -1 -177) from the Qatar N a-tional Research Fund (a member of The Qatar Foundation). The statements made herein are sol e-ly the responsibility of the authors.

We thank the two native speaker annotators and the anonymous r e viewers for their instru c tive and enriching feedback.
