 1. Introduction
An active area of research in machine learning is the combination of classifiers, the commonly called
Several methods have been developed concerning the construction of an ensemble classifier [1]. These methods are grouped into those that cope with: a) training examples to generate multiple hypotheses, b) the set of input features available to the learning algorithm, c) the output targets that are given to the learning algorithm, d) those that inject randomness into the learning algorithm and e) those that use Bayesian voting. The most popular variants of ensemble methods are Bagging [4], Boosting [5], Random Subspace methods [6] and, the last decade, Random Forests [7].

In Bagging [4] each base classifier is trained using a set generated by randomly drawing with replacement of N examples, where N produces a series of classifiers and the training set used for each classifier is chosen based on the performance of the earlier classifier(s) in the series. The classifiers are added one at a time and are trained on the data which have been ensemble members. This is achieved by assigning a weight to each trainingexample and may adaptively change the weight at the end of each boosting round. In the Random Subspace methods the training data are also modified. However, this modification is the original feature set. The outputs of the models are then combined, usually by a simple majority voting process.
Random Forests algorithm [7] constructs a set of tree-based learners. Each base learner is grown on a bootstrap sample of the predictors remains constant throughout the construction of the forest. The split of the node is based on the best of the randomly forest casts a vote for the instance being classified and the predicted class is determined by a majority voting procedure.
Random Forests ( RF ) presents a variety of advantages over other ensemble methods. It provides estimates about the importance of the input variables and it detects the interactions between them. In Random Forests, there is no need for a separate test set to get an unbiased estimate of the generalization error. It is estimated internally through the use of the out-of-bag noise data compared to other ensemble methods and according to Breiman [7] it does not overfit. The first two advantages are attributed to the fact the algorithms has its root to Bagging [2].

Although, the mechanisms that explain the high performance of the Random Forests have been detected, they are not fully decrease of the correlation or the improvement of the combination of tree-based classifiers.
 For this purpose several studies have been reported in the literature each one addressing, separately, one of the above issues. the base classifiers either affecting the number of features selected at each node or the evaluation measures which determine the finding the best combination function of base classifiers, since each one of the base classifiers has a different impact on the processing of different instances.

Bernard et al. [9] ( RK  X  RF ) focused on the setting of the hyperparameter m (a parameter that is not automatically tuned by the algorithm and in RF expresses the number of features which are used to determine the decision at a node of the tree [7]).
Linear Discriminant Analysis ( LDA ) to create a linear combination of features [11]. Robnik-Sikonja [8] proposed the replacement and resulted in decreased performance. Thus, moving one step forward, Robnik-Sikonja [8] replaced the Gini index as the sole attribute evaluation measure with several others, decreasing in this way the correlation but retaining the strength of the tree classifiers (RF with multiple estimators  X  RF with me ).

Robnik-Sikonja [8] uses internal estimates to identify the instances that are most similar to the one we wish to classify and then weights the votes of the trees with the strength they demonstrate on these near instances ( RF with wv -1 ). The similarity measure used is given by the ratio of the number of times the unknown instance and the training instance are in the same terminal node over the number of trees, while the similarity with nearest neighbors is expressed through different distance measures in RF with wv-2 and RF with wv-3 . Another approach based on the performance of the base classifiers is the one [12] modifies the combination of trees by taking into account their local predictive performance ( RF with wv-5 ). One such technique is the dynamic integration where the local predictive performance is first estimated for each base model based on the performance of similar instances, and then it is used to calculate the corresponding weight for combining predictions with followed on the sixth weighted voting scheme ( RF with wv-6 ). The weights of the trees are determined using genetic algorithms [14]. The fitness function is defined as the recognition rate of the forest when weighted voting is used.
The contribution of this work is twofold. First, it proposes modifications of the Random Forests algorithm which aim to improve either the construction of the forest or the voting mechanism as those reported previously and second, it proposes modifications that address both factors which affect the performance of the algorithm (construction and voting). Regarding the construction of the forest, a combination of a variable number of features, selected at each node, and different evaluation trees, by adopting nearest neighbors, dynamic integration and optimization techniques. Finally, modifications of the Random
Forests algorithm addressing the number of the predictors at each node, the evaluation measure that determines the best split and the voting scheme are proposed for the first time. More specifically, they address the three aforementioned factors jointly or their combination in pairs. The proposed modifications provide a strong ensemble classifier (having small generalization error) since they construct accurate, non-correlated base classifiers which are combined in an optimal way in order to classify an unknown instance. The resulted forest is characterized by robustness since the modifications incorporate mechanisms that also ensured by the existence of procedures which determine the optimal configuration (parameterization) of the forest.
The paper is organized as follows: Section 2 describes the Random Forests classification algorithm. In Section 3 the proposed modifications of the algorithm are presented. Section 4 describes the 24 datasets which are used for the evaluation of the proposed modifications. Section 5 provides information on experimental setting and implementation of the algorithms. Section 6 presents the results produced by the application of classical Random Forests and its modifications to 24 classification datasets.
Finally, in Section 7 the modifications of the Random Forests presented in this work are compared, using the evaluation for the 24 datasets. 2. The Random Forests algorithm the maximum size without pruning. The tree is just grown until each terminal node contains only members of a single class.
However, a lot of studies in the literature concentrate on the estimation of class probabilities in the leafs of the trees [15 features ( m ) remains constant throughout the construction of the forest. If the popularity of the trees agrees on a given whether there are many dissenters.

The Random Forests algorithm has also an internal mechanism for the estimation of the misclassification error, called  X  out-of-bag  X  error estimate. The idea is to estimate the expected predictive accuracy by running each observation through all the trees which were formed without this observation. It has been shown that for each data point approximately one-third of the the class that gets most of the votes for the x data point, the proportion of times that y point x is the out-of-bag estimate. The out-of-bag proportion of votes for class y where I (  X  ) is the indicator function, h k ( x ) is the prediction of the k th tree classifier for instance x , O instances for the classifier h k and K is the number of the tree classifiers.

According to the Breiman theorem [7] the generalization error, PE , of the Random Forests algorithm (which is the average of the out-of-bag errors of the trees) can be bounded by: where s is the  X  strength  X  of the set of classifiers and (on average), normalized by their variability (average correlation of the forest).

The strength, s , is the expected value of the margin and is given by: mr is the margin function and it measures the extent to which the average number of votes for the right class exceeds the average number of votes for any other class. It is given as: where c is the number of the classes, h ( x ) the joint classifier and P expresses the possibility.
The average correlation is computed as the ratio of the variance of the margin (var( mr ( X , Y ))) to the square of the standard deviation of the forest h ,( std ( h )). Thus, it is given as:
An appealing characteristic, which makes the Random Forests algorithm efficient, is the random selection of features. Recall that, at each step when growing a tree, classic recursive partitioning examines all variables to determine the best split. The
Random Forests, usually, picks the sqrt ( M ) of the variables randomly, taking the best split among them. This extra level of (otherwise, the slightly more predictive variable would always be chosen). Thus, random feature selection de-correlates the trees, lowering the prediction error. Consequently, an important parameter of the Random Forests algorithm is the number of features, m , randomly selected at each node during the tree induction process (reducing m leads to the reduction of both the correlation
These values are sqrt ( M ), 2 * sqrt ( M ), 1 2 sqrt M  X  X 
To improve the performance of the Random Forests algorithm several modifications, which aim to decrease the generalization error of the forest, can be proposed. The modifications proposed in this work are described in the next section. 3. Modi fi cations of the Random Forests algorithm modify the voting procedure, and C) those that address both the construction of the forest and the voting mechanism. The proposed the first time (Category A, Category B and Category C). The extensions of the existing modifications concern the distance function ( Modified SFS-RF , Modified SBS-RF ) and the creation of the input data and the automatic determination of the number of clusters ( Clustering-RF ). The categories of those modifications are shown in Table 1 . 3.1. Category A: Modi fi cations affecting the construction of the forest
The methods described in this section aim to increase the strength of each tree and decrease the inter-tree correlation, which can be achieved by addressing several issues concerning the construction of the trees of the forest. Those methods mainly modify the procedureof selectingthe number of features in eachsplitting node and the evaluation measure which determines thenode impurity.
All the approaches reported in the literature investigate either the influence of the number of features selected at each node ( RF with ReliefF , RF with me ). The application of those modifications affects positively the performance of the Random Forests algorithm. To our knowledge, there is no method which addresses both the influence of the number of features and the evaluation measure. We propose, an approach inducing both the number of selected predictors and the evaluation measure. More specifically, the random selection of the number of variables at each node of the trees and the use of a different evaluation measure at each tree are utilized. This modification is called RK-RF with me .

More specifically, the value of the parameter m is not the same throughout the construction of the forest but it is randomly attributes. An empirical study on attribute selection measures in decision tree learning is provided in [18]. In our case, the estimators which are used are the Gini index, the Gain Ratio ( GR ), the ReliefF , the Minimum Description Length ( MDL ) and the myopic ReliefF ( mRelifF ) which are the most popular for classification problem [8]. The formulas for the above evaluation measures are: where p ( y i ) is the probability for the class y i , p ( v for the class y i given that the feature A i has the value v training set, n . j is the number of training instance with the j th value of a given attribute, n and then searches for l of its nearest neighbors from the same class, called nearest hits H values for R i , hits H j and misses M j ( c ). The update formula is given by: where p ( c ) is the prior probability of the class c , g is the number of randomly chosen instances, and dif is given by: where I 1 and I 2 two different instances. The specific modification exploits the advantages of the two separate aforementioned approaches in order to decrease the correlation and increase the strength to the maximum degree. The decrease of the forest. This characteristic of the algorithm leads to the differentiation between the trees of the forest. We modify the Rotation Forest , proposed by Rodriguez et al. [10], by replacing the induction process of the trees of the forest. a new extracted feature set is reassembled, while all the principal components are retained. The data is transformed linearly into the new features. A tree classifier is trained using this new dataset. The J48 algorithm which is used for the construction of the
The reason for retaining all the principal components is the preservation of the variability information of the data in the new feature space. The low inter-tree correlation is achieved by r rotations which take place to form the new features. 3.2. Category B: Modi fi cations affecting the voting mechanism
Since not all trees are equally successful in labeling all instances, different weighted voting procedures were developed which the trees or selecting a subset of trees of the forest that are most predictive and less correlated. For this purpose, procedures assigning weights to the votes of the trees, feature selection and clustering techniques were employed. 3.2.1. Modi fi cations assigning weights to the votes of the trees
The modifications concerning the assignment of weights to the votes of the trees can be divided into those that are based on: a) nearest neighbor techniques, b) dynamic integration techniques and c) optimization techniques.

The core idea of most of the above methods is the determination of the similarity between the instance to be classified and the training instances. Thus, the characteristic which mainly differentiates them is the distance function which determines the degree of similarity. For this purpose we tested the weighted voting schemes reported previously with several different distance functions. These metrics were also tested using the weighted voting formula described in [19] (Eq. (3) described in [19]), given as: where two forms of vdm f ( vdm af , vdm bf ) were used: and f is a feature of the set of features F ( f  X  F ), w f is the weight assigned to feature f , x instance x and x jf is the value of feature f in the training instance x that the attribute f has value x f . c is the number of classes and x and x Using this distance measure two values are considered to be closer if they have similar classifications.
The l nearest neighbors are selected, based on this distance metric, to determine the class of x . The difference of the proposed modification with RF with wv-2 , described in [19] , lies in Eq. (12) .In RF with wv-2 the distance between x and x where  X  is given as:
The experimental validation of the RF with wv-3 (see Section 6 ) indicates that the specific distance metric is independent of only numerical but also nominal attributes.

A detailed description of the modifications assigning weights to the votes of the trees are provided in [21]. 3.2.2. Modi fi cations selecting a subset of trees
The second subgroup of modifications that affect the voting mechanism is based on the idea of feature selection and clustering this forest.

The tree selection approaches proposed by Bernard et al. [22] ( SFS-RF and SBS-RF ) are based on the classical feature selection terminated when a maximum number of iterations is reached. This number determines the number of base classifiers in the final subset. Another, commonly used, criterion is the convergence of the accuracy.

Recall that we are interested not only in finding strong classifiers but also less correlated. For this purpose a modified version added or discarded ( Modified SFS-RF and Modified SBS-RF , respectively).

Since the above methods are known to be suboptimal [22], because the sequential process makes each iteration depending on the previous one and not all possible combinations are explored, another modification is proposed in this work, Optimal RF . then it is retained, otherwise all the combinations with previous trees are checked, by adding trees that were discarded and removing trees that were added. If the newly added tree does not affect the accuracy or the correlation of the subset, the Brier score [23] is checked. Brier score is a function which measures the average square deviation between predicted probabilities for a set of events and their outcomes (the lowest score corresponds to the highest accuracy). Thus, if the newly added tree decreases the Brier score remains to the forest.

Finally, clustering techniques are employed in order to select trees from the forest. In order for the clustering techniques to be applied the following issues must be addressed: a) what must be clustered (nature of data)? b) what clustering algorithm must be used? c) what is the number of clusters? and d) how is the number of clusters determined?. Gatnar [24] used the results of Yule's and Hamann's diversity measures in order to guide combination of classifiers. The data were grouped into 30 clusters. From each cluster, the tree base classifier with the highest accuracy has been selected to form the ensemble. Finally, majority voting was used for the final classification.
 dataset. b) Determination of the number of clusters: nine validity indices are tested to determine the optimum number of clusters. indices = 72 combinations); c) clustering algorithm: k -means algorithm is applied to 72 combinations; d) determination of the subset: a member of each cluster, the one with the highest accuracy, is selected; e) Combination of the selected classifiers: the majority voting procedure is used. The diversity measures are the following [24,25] : Yule's Q statistic It is used to assess the similarity of the two classifiers' outputs. It is given as: where N ab is the number of instances, classi fi ed correctly ( a = 1) or incorrectly ( a = 0) by classi incorrectly ( b = 0) by classi fi er j .
 Correlation coefficient
It expresses the correlation between the outputs of two classifiers: Double Fault measure It is the percentage of test instances for which both classifiers make wrong predictions: Fail non fail disagreement measure correct: Hamann measure
It is a binary similarity coefficient that is given by the difference between the matches and mismatches as a proportion of the total number of entries: kappa measure kappa ij is equal to 0 when the agreement of the two classifiers ( i and j ) equals to that expected by chance, and kappa to 1 when the two classifiers ( i and j ) agree on every example. It is defined as: where  X  1 and  X  2 are given as:
N is the number of instances in the dataset, recognized as class i by the number of instances recognized as i by the fi rst classi fi Interrater agreement measure
It measures the level of agreement between two classifiers ( i and j ) with the correction for chance: Plain disagreement measure
It is equal to the proportion of the instances on which the classifiers make different predictions: The validity indices are the following [26  X  28]: Hartigan where N is the total number of objects in the dataset,  X  ( c )= N optimal number of clusters.
 Davies Bouldin where  X  ( C r ) is the intra-cluster distance, de fi ned as the average distance of all the cluster objects C  X  ( C r , C j ) is the distance between the clusters C r and C DB ( c ).
 Krzanowski and Lai where and x represents an object that belongs to the j -th cluster, the c optimum c corresponds to the maximum of Krazanovski and Lai index.

Silhouette objects of the closest cluster and C r is the objects belonging to cluster r .
 Calinski and Harabasz where SSB ( c ) and SSW ( c ) are the sum-of-squares within and between clusters, respectively.
 Homogeneity where C is a clustering solution and S ( F ( i ), F ( j )) is the similarity between the Separation which measures the average similarity between clusters.
 Weighted inter-to-intra-cluster ratio where inter _ sim and intra _ sim are the similarity between and within clusters, respectively.
 Under  X  Over partition measure matrix and the dataset, respectively. The under partition measure is de over partition measure is given as the ratio of the number of clusters to the minimum inter cluster distance. 3.3. Category C: Modi fi cations affecting the construction and the voting mechanism
In the above sections, various modifications of the Random Forests algorithm are described. The modifications reported in the first section aimed to increase the strength and diversity of the base classifiers while the modifications described in the second section aimed to optimize the voting mechanism.
 Although, increasingstrength and diversity of the ensemble or optimizingthe voting mechanism,increases the performance of the
Random Forests algorithm, this is not enough. An ensemble integration method which decides how to combine strength and less correlated base classifiers is needed. The methods of the third category ( Table 1 parameter m , b) the evaluation measure which determines thebest split and c) thecombination mechanismof the output of thetrees.
The first two concern thenode splitting procedure while the second oneconcerns the votingmechanism. The proposed modifications, wv-1 , RF with me and wv-3 and Optimal RF with me examine how the combined changes of factors b and c improve the performance of the Random Forests algorithm. The above mentioned modifications employ multiple estimators and weighted voting approaches, one reported in the literature and two proposed in this work. The RK-RF with wv-1 and Optimal RF with RK-RF modifications address simultaneously factors a and c. The random selection of the number of splitting attributes is combined with two different weighted selected). Finally, the RK-RFwithmeandwv-1 and Optimal RF with me and RK-RF aim to improve the Random Forests algorithm taking modifications, which address separately the factors a, b and c mentioned above, was based on the advantages that each modification provides, as well as, on the comparison of each modification with Classical RF and other modifications of the same category. 4. The datasets
For the evaluation of the Random Forests algorithm and its modifications 24 datasets were used. 23 datasets are publicly available at the UCI Machine Learning Repository [29] and the last one consists of data from patients with Alzheimer's disease features, attributes and classes. A short description of the datasets is provided in Table 2 . 5. Implementation and experimental setting
The software used for the implementation of the modifications of the Random Forests algorithm has been developed in C++ and can be found in ( ftp://195.130.121.65 ). The original implementation of the Random Forests algorithm was taken from [8]. The algorithms are implemented to take as input datasets which include numerical and categorical features and to address the existence of missing values. The replacement of missing values varies depending on the type of the attribute (numerical or
Then the algorithm uses this value to replace all missing values of the attribute in the corresponding class. If the attribute is parameterized in order to work with datasets with different values as far as it concerns the number of instances, the number of features and the number of classes. The value of these parameters is determined during loading of the dataset.
The implementation of Classical RF and modifications that do not employ RK-RF allow the user to select the value of parameter m . The choices are the following: sqrt ( M ), 2 * sqrt ( M ), 1 randomly selected at each node ( m ) is set equal to sqrt ( M ) since the difference between sqrt ( M ) and log almost zero, in most of the datasets incorporated in the experiments. The value of this parameter is not the same throughout the construction of the forest and it is randomly chosen for each node in the following modifications: RK-RF , RK-RF with me , RK-RF with wv-1 , RK-RF with me and wv-1 , Optimal RF and RK-RF and Optimal RF with me and RK-RF .

The parameter that interferes in the modifications, where weights are assigned to the votes of the trees, is the number of nearest neighbors. In RF with wv-1 and RF with me and wv-1 this parameter is determined through a test and error technique, while in RF with wv-2 , RF with wv-3 , RF with wv-5 and RF with me and wv-3 the value of the parameter depends on the distance between the new instance and all training instances [21].

In the case of modifications where multiple estimators are employed each fifth of the trees is built with a different attribute evaluation measure. In Clustering RF for each number of trees (ranging from 5 to 100) 72 combinations are tested (8 diversity measures  X  9 indices). Finally, in the Rotation Forest the following parameters are employed: (a) the number of the features set equal to 3. Different values in the range [10,95] with step 5 are tested for the second and the third parameter. Principal Component Analysis is used as filter for the projection of the data and the J48 and random tree are employed as base classifiers. The experiments that are conducted for all combinations of parameters are 648 and were made using the WEKA platform [37].
In order for the best number of trees to be selected the following process is followed. We run the Random Forests algorithm and each modification (except from those belonging to the feature selection category, clustering techniques and those belonging difference between iterations is that the number of trees is increased by one starting with 5 trees ending with 100. In each
When the procedure is completed we select the best number of trees based on accuracy. If the accuracy (accuracy was calculated from the confusion matrix of the 10 fold cross validation procedure) is the same between the iterations the selection is made based on Brier score. We select the forest with the smallest Brier score. If the accuracy and the Brier score are the same between iterations we check the correlation and we select the case with the lowest correlation. Once the optimal number of tree is determined the datasets are partitioned again and the classifier is rebuilt and evaluated (in terms of accuracy
The experiments were run on an Intel (R) Core (TM)2 Duo CPU 3.17 GHz personal computer with 4 GB RAM. 6. Results
As mentioned above the performance of the Random Forests algorithm is influenced by two factors: the strength of the trees and their diversity. These two factors interfere with the procedure of the construction of the trees of the forest. Part of the modifications is concentrated on increasing the strength and decreasing the correlation by changing the number of the features
Rotation Forest . As far as it concerns the feature evaluation measure, the Gini index is used in the classical Random Forests with me , RF with me and wv-1 , RF with me and wv-3 , RK-RF with me , Optimal RF with me and Optimal RF with me and RK-RF ) multiple estimators were employed instead of one. Another critical issue concerning the performance of the Random Forests voting, that is used in the classical Random Forests algorithm, are presented. The modifications are based on weighted voting schemes, feature selection and clustering techniques. In addition, modifications which aim to address all the above issues and to provide an overall improvement of the classical Random Forests algorithm are presented.

The results produced from each modification for each data set are reported in Table 3 (modifications which belong to Category A), which is achieved using this optimal number. To determine the optimal number of trees, a grid search technique is employed. Each second run the number of trees is set equal to 6, while in the last run the number of trees is equal to 100. The same grid search from those belonging to the feature selection category and clustering techniques and those belonging to weighted voting using weighted voting modifications, it is considered preferable for the comparison to be made under the same construction circumstances of the forest (using the same construction procedure and the same number of trees).

According to Table 1 , for each one of the three basic categories of the modifications there are some subcategories which are issues ( RK-RF with me ). Rotation Forest and RF with me are the the classical Random Forests (random tree algorithm). The best results are obtained using the random tree algorithm. The numbers same category.

Feature selection, clustering, nearest neighbors, and genetic algorithms have been utilized to determine which tree of the forest should vote for the class of an unknown instance. In the case of the modifications based on feature selection techniques
Modified SFS-RF and Optimal RF produce the optimal classification accuracy. Recall that the specific category attempts to find the best combination of trees (over 100) that has the highest accuracy and the lowest correlation. Although, someone would expect same results with other modifications of the same category. For this purpose the algorithms are compared also according to the average correlation of the forest they constructed. The comparison showed that the Optimal RF results to less correlated trees. As clustering algorithm and the number of clusters to be determined. However, there is not a specific combination of diversity measure and validity index that produces the best results. It depends on the dataset. Among the modifications that are based on the distance between the instance being classified and its nearest neighbors, the RF with wv-3 proposed in this work presents significant improvement in the predictive accuracy for all datasets.

The combination of the above modifications results in variations of the Random Forests algorithm which affect both the construction of the forest and the voting procedure ( Table 5 ). Among those modifications RF with me and wv-3 , proposed in this work, produces the best results for 19 of the datasets.
 modifications, 14 of them proposed in this work, improve the performance of the classical Random Forests algorithm for more than
More specifically, RF with wv-2  X  won  X  against Classical RF in 14 datasets and wv-4  X  won  X  against Classical RF in 11 datasets,  X  lost  X  (11/10/3).

It must be noted that the modifications proposed in this work present significant improvement for all datasets. More specifically, they improve the classification accuracy by 3% on average (all datasets). However, the improvement in some datasets is more than 10%. The potentiality of the modifications proposed in this work is also expressed through the use of statistical experiments we use the accuracy measured and averaged over 10 fold cross validation. The results of individual folds could be used [32], however, the variation of Friedman test that considers multiple observations per run makes the assumption that the observations are independent [33], which does not hold here.

Friedman test is a non-parametric test equivalent of the repeated measures ANOVA. Under the null-hypothesis, it states that all the algorithms are equivalent, so a rejection of this hypothesis implies the existence of differences among the performance of best performing algorithm getting rank 1, the second best rank 2, and so on. In the case of ties average ranks are assigned. It checks whether the measured average ranks are significantly different from the mean rank expected under the null hypothesis.
Friedman test is given as: where D is the number of the datasets, cl is the number of classifiers and AR datasets. It is distributed according to the  X  F 2 with cl D N 10 and cl N 5). For a smaller number of algorithms and datasets, the exact critical values have been computed [31].
The Iman and Davenport test is a non-parametric test, derived from the Friedman test. It is less conservative that the Friedman statistic and it is given as:
It is distributed according to the F-distribution with cl
Table 7 indicate the existence of significant differences (the Friedman value is greater than the value is greater than the F value). Since the null hypothesis (all classifiers perform the same and the observed differences are merely random) is rejected, the Bonferroni  X  Dunn test is performed. It is a post-hoc test that can be used after Friedman or Iman
Davenport tests when they reject the null hypothesis. According to this test the quality of the two algorithms is significantly different if the corresponding average ranking is at least as great as its critical difference given by:
The results of the Bonferroni  X  Dunn test are depicted in Fig. 1 . The height of the bars is proportional to the average ranking obtained for each modification. We subtract the critical difference obtained by Bonferroni belong to the modifications whose behavior is significantly worse than that contributed by the control algorithm ( Classical RF ) the line. In Fig. 2 the comparison of the modifications with the control algorithm is presented using the Bonferroni rank of the control algorithm. All the modifications with ranks outside the marked interval are significantly different ( p from the control [35].

According to the analysis of the results presented until know the modifications belong mainly to the category of weighted voting techniques where nearest neighbors determine the classification of the unknown instance ( RF with wv-2 and RF with wv-4 ). This indicates that the nature of the dataset and the distance metric that is used affect the performance of the weighted voting algorithm. In addition, the number of the trees that compose the forest might affect the performance of the weighted voting schemes since the specific modifications run with a fixed number of trees (equal to the number of trees for which the classical Random Forests had the best accuracy). In addition, the SBS-RF modification does not present better results than the classical Random Forests algorithm. A reason for this result can be attributed to the backward sequential process which is followed in the specific modification. From the modifications of the same category, Clustering RF does not improve the results of
The fact that the RF with me and wv-3 modification outperforms the other modifications is also supported by the results reported in Tables 8 and 9 . Table 8 presents the results from three evaluation heuristics, average macro accuracy, average micro accuracy and average rank. Macro-averaged accuracy is the standard average of the accuracies on the 24 individual datasets.
Micro-averaged accuracy is the fractions of correctly classified examples in all examples in the union of all examples of the different datasets. The average rank is the average of all the individual ranks on each dataset [36]. The number in brackets indicates the ranking of the methods according to each modification. It must be noted that the numbers in brackets in the column of average ranking have the opposite meaning. More specifically, the best algorithm is the one with rank 25 and the worst algorithm is the one with rank 1. The last column of the table presents the sum of the values of the columns and the rows of the
Win/Loss/Tie analysis. The sum shows how often the modification in this row/column has outperformed any of the modifications in the columns/rows. Table 9 presents the results of the F-measure [36] for Classical RF and RF with me and wv-3 , which is the  X  winning  X  modification according to the above analysis.

The above results are also verified by the results of the statistical comparison of modifications with state of the art ensemble methods, such as Boosting and Bagging. More specifically, the Bonferroni control classifier Boosting and Bagging ensemble methods, respectively, instead of the classical Random Forests algorithm. The classifier. The results agree with the ones presented in Fig. 1 .

The analysis reported previously was repeated using as input the predictive performance of the modifications computed by keeping the number of trees constant and equal to 100 (maximum size of the forest). The graphical representation of the
Bonferroni  X  Dunn procedure is presented in Fig. 3 . The results indicate that the modifications that are affected by skipping the of these modifications, such as RK-RF with me , RF with me and wv-1 , RK-RF with wv-1 and RK-RF with me and wv-1 . The comparison trees), reveals that the ranking of the modifications is affected, especially in the case of RF with wv-1 and RK-RF with wv-1. 7. Discussion
In this study we consider different modifications of RF algorithm, including 14 modifications which were not previously considered. The proposed modifications aim to improve the construction of the trees, the procedure of combining the trees or both mechanisms. The motivation of the Random Forests algorithm is that the combination of multiple models will produce better results than a single model. When multiple models are generated they are normally combined by voting (classification) or averaging (regression). In this work, modifications incorporating weighted voting schemes, feature selection and clustering techniques are employed. However, combining models, in our case trees, will only be beneficial if the individual models are the above conditions depend on the number of predictors selected randomly at each node and the evaluation measure that determines the best predictor. With a large number of predictors the eligible predictor set are quite different from node to node and the inter-tree dependency are minimized. In addition, the selection of the right evaluation measure ensures the selection of a good splitter and thus the good performance of the base model. We propose modifications affecting the node splitting procedure, either the number of features selected at each node or evaluation measure or both.

RK-RF [9] inserts a new source of randomness in the construction of the trees and examines if the performance of the RF algorithm is improved . Rotation Forest aims to encourage the accuracy and diversity within the ensemble through the feature extraction that are observed in many problems. RK-RF with me examines how the different number of features at each node in combination with measures [21] . Inorder for theweightingvoting proceduretobeindependent from theinstancetobeclassifiedand traininginstances,
RF with wv-4 , RF with wv-6 and modifications based on feature selection and clustering techniques were employed. In RF with wv-4 [21] the weight of the tree depends on its accuracy while in RF with wv-6 [21] is determined through an optimization procedure classifiers is achieved by employing diversity measures. Through the qualitative and quantitative analysis of the above mentioned modifications we conclude for the modifications of the Category C which combine the advantages of the approach described above and, jointly, address all the factors that affect the performance of the RF algorithm.

For the evaluation of the modifications reported in this work, a 10 fold stratified cross validation procedure is applied on 24 different datasets covering different areas. The comparison is based, not only on the resulted predictive accuracy, but also on results indicate the existence of statistically significant differences especially in most of the cases. It must be noted that all of the modifications proposed in this work belong to those that outperform the classical Random
Forests algorithm. The rest of the methods not reported belong mainly to the category of weighted voting schemes which are based on nearest neighbors. The absence of statistically significant differences between this subcategory of modifications and the classical Random Forests algorithm is mainly due to the fact that the methods of this category are not used with the optimal parameters values, as far as it concerns the number of trees, but with the same parameters with those of classical random forests with the optimal number of trees. The reason for this approach, fixing the number of trees, is the evaluation of whether the distance metric, that is used, affects the performance of the Random Forests algorithm.

The results reported above indicate also the superiority of some modifications for each subcategory reported in Table 1 . What is encouraging is the fact that most of the methods proposed in this work address all the parameters that influence the performance of the Random Forest algorithm (strength of classifiers, inter-tree correlation, voting procedure). The superiority is are classified, the number of features and the number of classes which are used.

A comparison of the proposed modifications, in terms of time complexity, leads to an expected conclusion. More specifically, the specific modifications can be probably improved by the implementation of the parallel version [37] of the modifications.
The modifications reported in the literature concentrated either on determining the optimal number of features randomly selected at each node during the integration of the tree or on finding the optimal index for determining the node impurity. Also, many works deal with the combination of base classifiers since each of the classifier participates in the classification of an unknown instance with different weight. An important limitation of these methods is that none of them addresses all the factors that affect the performance of the Random Forests algorithm. In this work, except from modifications that affect the above factors separately, we introduce methodologies which address the above mentioned parameters all together. Thus, a robust and strong ensemble classifier is produced, since procedures that ensure both high accuracy and low correlation are utilized.
The modifications already reported in the literature increase on average the accuracy by 1% while the modifications proposed in this work do so by 3%. The improvement varies depending on the modification and the datasets and in some cases is over 10%. The modificationsare concentrated on various issues concerningthe integrationof theforest, the compositionof thefinaldecision or both.
For the evaluation of the modifications 24 datasets are used, with number of instances ranging from 90 to 6598, number of features datasets, such as those related to medicine and biology. However, in the future such modifications must be used in large datasets experiments led to limiting the number of the datasets to 24. The results reported underline the predominance of the modifications will be conducted (36 UCI datasets published on the main web site of the WEKA platform [38] ) and issues such as, the automatic named entity recognition [42] will be studied in the future. 8. Conclusions
The modifications of the Random Forests algorithm reported in this work aim to decrease the generalization error of the algorithm statistical significant improvement in the performance of the Random Forests algorithm. More specifically, among the modifications belonging to Category A, modified Rotation Forest produced the best results. RF with wv-3 is the while in Category C RF with me and wv-3 has the best performance, compared to not only the Classical RF butalsotothe modifications of the other categories (Category A and B). This leads to the conclusion that the joint improvement of all the mechanisms that affect the performance of the Random Forests algorithm provides a robust solution for classification problems.
However, before suggesting such a modification further study is needed concerning the optimization of parameters that affect the trees, affect the performance of the forest.

References
