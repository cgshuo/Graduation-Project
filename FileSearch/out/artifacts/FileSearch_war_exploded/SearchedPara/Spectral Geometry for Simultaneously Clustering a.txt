 How best to present query search results is an important problem in search engines and information retrieval system s. When a single query retrieves many results, simply showing them as a long list will provide users with poor overview. Nowadays, ranking and clustering query search results have been two useful separate post-processing techniques to or-ganize retrieved documents. In this paper, we proposed a spectral analysis method based on the content similarity ne t-works to integrate the clustering and ranking techniques fo r improving literature search. The new approach organizes all these search results into categories intelligently and si-multaneously rank the results in each category. A variety of theoretical and empirical studies have demonstrated tha t the presented method performs well in real applications, es -pecially in biomedical literature retrieval. Moreover, an y free text information can be analyzed with the new method, i.e., the proposed approach can be applied to various infor-mation systems, such as Web search engines and literature search service.
 H.3.3 [ Information Search and Retrieval ]: Clustering and Ranking Algorithm, Theory, Experimentation Spectral geometry, Clustering, Ranking, Query Search
Search engines attempt to identify, within large text col-lections, the useful text documents whose content pertains to some specific topics or information needs. Such topics or needs are inherently specific to a given user or situation via a query. Usually, a large number of documents will be relevant to the query. Simply showing the search results as a long list always provides users with a poor overview. Therefore, providing a well-organized document set becomes a major goal in search engines. In this case, the retrieved document s can be categorized into different topical groups based on textual information, linkage and other information. In thi s way, users can select, analyze, and focus on only a reduced set of documents in one or more topical groups of interest.
Recently, several effective and efficient clustering technol o-gies for Web search result organization have been developed [25, 13, 26, 4]. Meanwhile, two Web search engines with clus-tering function, Vivisimo( http://vivisimo.com ) and Eigen-cluster( http://eigencluster.csail.mit.edu/about.html ), are well designed and applied in the internet or enterprise intranet . Clustering techniques group a set of documents into differ-ent subsets based on their textual similarities, so that the documents in the same subset are much similar with each other, while documents in different subsets are much dissim-ilar. Documents clustering have been well studied [11, 24, 27, 28, 22]. Ranking is also an important post-processing technique for Web search engines. It estimates the quality of a set of results retrieved by a search engine. For Web search engines, hyperlink information uniquely existing i n Web pages is used to rank the search results [12, 19, 1]. In addition, the number of citations to the articles and the impact factor of the publication venues (eg., journal) are taken as the criteria to rank the search results in literatur e retrieval [2].

However, how to use both clustering and ranking tech-niques to improve the search result presentation has not been well studied [10, 23, 15]. One strategy to integrating them is to firstly cluster retrieved documents and then rank documents within each cluster. For example, in [15], docu-ment clustering algorithms and ranking algorithms with ci-tation count per year were separately applied to MEDLINE database for efficiently explore biomedical citations. In ot her words, in this strategy, clustering and ranking can not shar e the information with each other, in fact, they could improve the performance with each other.

In this paper, we propose a method to discover the cluster-ing characteristics (called as  X  clustering structure  X ) of a text document collection before the actual clustering algorith m is performed. Usually, these special  X  clustering structure  X  re-veals a natural relationship between clustering and rankin g information, but is hidden in most query search results. The new method is implemented by analyzing the spectral geom-etry of the documents similarity network. First of all, the retrieved documents for each query search are represented as a content similarity network according to some similarit y measure, say cosine similarity. By analyzing the spectral g e-ometry of this similarity network, we discover  X  X he structu re of quasi-orthogonal beams X  which illustrates the distribu -tion of documents, and each beam represents a cluster [7]. In other words, the documents projected on the same beam have similar content, while the documents projected on the different beams have smaller content relationship with each other. At the same time, the documents with large projec-tion lengths on a beam play a leading role in the correspond-ing cluster. Based on this pre-existing clustering structu re, we design a new algorithm with both clustering and ranking functions to present the query search results. To our knowl-edge, there is no study that systematically investigates bo th pervasive properties and their applications to algorithm d e-sign for presenting query search results.

The rest of the paper is organized as follows. In Sec-tion 2, some preliminaries are given for a similarity networ k construction and some characteristics of the network, at th e same time, a concept  X  X ormalized Path Length X  is intro-duced. In Section 3, we analyze the spectral geometry of the similarity network and shows the relationship between the clustering structure and the spectral geometries. In Se c-tion 4, we proposed our new algorithm to simultaneously cluster and rank the query search result based on the an-alyzed result in Section 3. Several experimental results on the real-world query search results are listed in Section 5 to demonstrate that the proposed algorithm performs well in real applications. Section 6 gives a short survey of spec-tral techniques and applications to understanding network s. Finally, conclusions and future works are discussed in Sec-tion 7.
In this paper, we aim to organize (including cluster and rank) the query search results based on their content re-lationships. Two retrieved documents are considered con-nected if their similarity in text content is high enough to exceed a value. In information retrieval, there are many sim -ilarity measures which can be used to determine the simi-larity space. Among them, cosine similarity is the most popular one [11], usually, this similarity between documen ts is calculated based on the preprocessed documents. Sev-eral typical steps in information retrieval are used such as , segmentation into single terms, word stemming, stop words removal and term weighting with TFIDF (frequency-inverse document frequency).

Before constructing the content similarity network, we build a similarity space matrix S = ( s ij ) n  X  n , where s dicates the similarity between the i th and j th document (In our experiments, we use cosine similarity measure to calcu-late s ij .). Meanwhile, 0 6 s ij 6 1, s ii = 1 and s ij = s i.e., S is symmetric shown as follows.
 Based on this matrix, a content similarity network G ( S ) = &lt; V, E, S &gt; can be constructed, where V is the set of n nodes (i.e., documents) and E is the set of weighted edges. Each node v i of G ( S ) corresponds to the i -th column (or row) of S , and the weight of each edge d v i v j corresponds to the non-diagonal entry s ij . For any two nodes ( v i , v j ), a larger value of s ij indicates a higher connectivity between them, and vice versa. In order to speed up the analyzing process, some weighted edges will be removed if the similarity be-tween the nodes connected by these edges are less than a given threshold (we set the threshold as 0.3 in our exper-iments). In other words, the similarity threshold plays a role of filtering those trivial relationships of the retriev ed documents.
In this subsection, we introduce the main concept  X  X or-malized Path Length X  (NPL) in the similarity networks. Estrada and Rodr  X  X guez-Vel  X azquez [7] firstly pointed out t he relationship between NPL and clustering structures of node s. However, the authors in [7] focus on the subgraph central-ity of a node which was used as a metric to evaluate if a node play an important role in a cluster, although it can not identify which cluster this node belongs to. Here, we proposed to make use of normalized path to characterize the neighborhood relationship of two nodes in terms of clus-tering structure. One of the important properties of the normalized path is that, the large length of this normalized path means that these two nodes most probably belong to the same cluster; otherwise, they may be in different clus-ters.

We define  X  X otal path X  P ( i, j ) between the vertices i and j as the number of all paths starting from the i -th node and ending at the j -th node. Supposing that P k ( i, j ) is the number of all k -length paths between the vertices i and j , we have the total path P ( i, j ) = P ( i, j ) can be used to measure the neighborhood relation-ship of two nodes [7]. There are two important factors on applying  X  normalized path  X  to analyzing networks: 1. With the increasing of the length of path, the contri-2. The sum of all paths with different length leads to a
In other words, simply using the sum of all paths presup-poses a mathematical problem of the divergence of the series P k =1 P k ( i, j ). Thus, a straightforward way to address this problem is to normalize or scale each P k ( i, j ) with the order of the spectral moment k . That is, we divided the sum of all k -length walks by the factorial of k as follows,
We call this normalized path as  X  Normalized Path Metric  X  between two nodes i and j .
In this section, we analyze the spectral geometry of the similarity network constructed based on the document con-tent relationships.
Since the k -th power of the adjacency matrix W of the graph compute the sum of all k -length paths for any two vertices, we have P k ( i, j ) = ( W k ) ij , where ( W k try value in the i -th row and j -th column of W k . Based on the close relationship ( Let  X  be an eigenvalue of W and x is its corresponding eigenvector, we have W x =  X  x . Then we can obtain W k x =  X  k x , which means W k has the same eigenvector as W and the k -th power of the same eigen-value  X  k . ) between the power of a matrix and its spectral information (e.g. eigenvalues and eigenvectors), the foll ow-ing theorem gives a quick computation for P ( i, j ) from the spectral information of W .

Theorem 1 (Spectral Computation of P ( i, j ) ). Gi-ven an undirected weighted graph G ( V, E, W ) (where n = | V | and W is the adjacency matrix of the graph), let x 1 , x 2 be eigenvectors of W associated to the eigenvalues  X  1 ,  X   X  point v i = ( x 1 ( i ) , x 2 ( i ) , . . . , x n ( i )) T (the relationship between x i and v j is shown in Figure 1), where x 1 ( i ) denotes the i -th component of x 1 . Then for any two vertices i and j , normalized path metric is, Figure 1: Illustration of eigenvectors and points representing vertices: the column vector x j repre-sents the j -th eigenvector and the row vector v i is the point standing for the i -th vertex. Therefore, the entry value in the i -th row and j -th column is v ( j ) = x j ( i ) .

Proof. We first project each unit vector e i (whose i -th component is 1 and the rest of components are 0) on x r and get, where x r ( i ) denote the i -th component of x r . Since n eigen-vectors form an orthonormal basis of n -dimensional space, each vector e i can be represented as e i = Therefore, P k ( i, j ) can be expressed by the spectral infor-mation of the adjacency matrix of the graph as follows, where &lt; s , t &gt; is the dot product of two vectors. With Eq.(4), Eq.(5) can be written as, Replacing Eq.(2) by Eq.(6), we have, By reordering the terms of series (7), we can get the conver-gent series
P ( i, j ) =
From Theorem 1, we can see that pairwise normalized path can be computed by the spectral decomposition of the adjacency matrix of the network. If we analyze this compu-tation carefully, i.e., Eq. (3), it can be rewritten as, i is the i -th weighted points, whose r -th dimension has Therefore, Eq. (9) indicates that P ( i, j ) is the dot product of v 0 i and v 0 j , which reflects the geometric relationships of these two vectors. As shown in Figure 2, the dot product &lt; the cosine of their angle cos(  X  ). Recall the facts, &lt; v 0 Figure 2: Illustration of dot product &lt; x , y &gt; , and projection b y ( x  X  b y ) of two vectors x and y. The dot product can be defined as &lt; x , y &gt; =  X  &lt; x , x &gt; the vectors. mentioned in [7], and P ( i, j ) reveals the degree of how two vertices i and j belong to the same cluster, we can get the following conclusions. but do not consider the pre-existing cluster structures of the content similarity networks. Furthermore, the propose d method can not only effectively cluster similarity networks , but point out the importance of each node in forming the cluster (refer to Line 11-13). Based on these importance values, the proposed method can rank nodes in each cluster. Therefore, our clustering algorithm is a specific method in exploring content similarity networks of query search resu lts by providing both clustering and ranking capabilities.
In order to show that the proposed approach performs well, we conducted several experiments on real-world query search results. Among them, four query search results refer -ring to four cancers X  names ( X  X reast Cancer X (BC), X  X olorec -tal Cancer X  (CC),  X  X ndocrine Cancer X  (ENC), and  X  X ung Cancer X (LC)) are obtained from PubMed ( http://www.ncbi. nlm.nih.gov/entrez ). These cancers X  name are correspond-ing to the categories defined by Society of Surgical Oncol-ogy ( http://www.surgonc.org ) Annotated Bibliography. The fifth search result is about  X  X ognitive X  which is commonly used in HCI Bibliography ( http://www.hcibib.org ). The last query  X  X aguar X  is a popular search in Google [12]. All of the retrieved documents for each query were used in the ex-periments. As mentioned in Section 2.1, we preprocessed these documents by segmenting into single terms, stemming words, removing stop words and weighting term with TFIDF. And then, each data set was represented by a document-by-term matrix, where each row shows a document, each col-umn represents a term, and the cell value means the weight of the current term in the corresponding document. Table 1 gives the size of the data matrices in detail.

Based on the document-by-term matrix, we constructed one content similarity network for each query search result , and then applied our proposed algorithm on these networks to find the clusters and ranking order of the retrieved doc-uments. So far, there are no gold standard query data sets that can evaluate document clustering and ranking algo-rithms. Therefore, we only report our own results on the spectral geometry to demonstrate the effectiveness of the proposed algorithm.

In Section 3.2, we listed part of the experimental results as shown in Figure 3 and Figure 4. In order to analyze the performance of the proposed algorithm, we give the more re-sult details on the query search result with  X  breast Cancer  X  (BC),  X  colorectal Cancer  X  (CC) and  X  jaguar  X .

As it is difficult to visualize the distribution of documents X  points in more than 3-dimensional space, we only show 3D spectral geometry of the query  X  breast cancer  X  and  X  col-orectal Cancer  X . Figure 5(a) and (c) illustrates the beams detected with different colors, where each nodes X  point is as -signed to the beam with the same color. From Figure 5(a) and (c), we can see that the documents X  points are very close to the discovered beams and these three beams are almost orthogonal to each other. This indicates that the content similarity network strictly follows the distribution of qu asi-orthogonal beams.

To further investigate the network relationships of these documents X  points in beams, we can rank the documents X  points assigned to a beam by their projection lengths on that beam. We selected the top 500 documents X  points of each beam and visualized their corresponding adjacency matrix in Figure 5(b) and (d). By comparing Figure 5(a) with Figure 5(b) and Figure 5(c) with Figure 5(d) we can get the following conclusions: (1). Points/Documents with large projection lengths (2). Nodes in different beams have sparse connec-
We further read these selected documents in Figure 5(b) and (d), and found that each cluster has a specific and nar-row topic. In Figure 5(b) for  X  breast cancer  X , the 1st clus-ter is about  X  X ammography and screening X , the 2nd clus-ter is about  X  X enetic study, especially genes BRCA1 and BRCA2 X , and the 3rd cluster is about  X  X reatment of breast cancer, especially using the drug tamoxifen X . In Figure 5(d ) for  X  colorectal cancer  X , the three clusters represent  X  X he-motheraphy X ,  X  X rostate Cancer X  and  X  X olonoscopy X  respec-tively. Moreover, we found that, the higher a document is ranked in a cluster, the closer to the cluster X  X  topic it is. This fact does match the meaning of the documents X  rank-ing mentioned in the above Point (1).

To illustrate that such special clustering structure also exists in more than three dimensions, we firstly applied our algorithm to identify beams in k -dimensional spectral geom-etry ( k &gt; 3), and then visualized the network relationships of top-ranked points/documents in each beam by their cor-responding adjacency matrix. We used the query  X  jaguar  X  searched in Google as an example, and applied our algorithm to its 4-dimensional spectral geometry. In the output of the algorithm, we tested the direction vectors of the 4 beams dis -covered in this 4D spectral geometry, and found that these 4 beams have quasi-orthogonal angles between each other. Hence, we can say that the clustering structure in the doc-ument points follows Corollary 3 in k -dimensional spectral geometry ( k &gt; 3). Next, we will verify whether these four beams discovered represent the meaningful clusters.
As there are only few documents X  points with large projec-tion lengths in each beam, we select the top 10 ranked doc-uments X  points of each beam and visualize their correspond-ing adjacency matrix in Figure 6. It is clearly that there are four dense clusters in Figure 6. We further investigated the Webpages in these clusters and found four distinct topics:  X  X nimal jaguar X ,  X  X ideo game Atari Jaguar  X ,  X  X aguar cars X  spectral geometry and the corresponding adjacency matrix. Figure 6: The clustering structure of the content similarity network of query  X  X aguar X  in 4D NPL spectral geometry and the corresponding adjacency matrix. and  X  X aguar graphics X . From these four topics, we found that the word  X  X aguar X  has different meanings in different topics. We showed the links of the top-ranked two Web-pages in each cluster in Figure 6 for readers X  interests. The y do represent the main topics of their corresponding cluster s. Moreover, when we carefully read Webpages ranked from highly to lowly, we found that the higher ranked Webpages are, the closer the meanings expressed in Wegpages are to the main topic of the cluster; otherwise, they stray farther from the main topic.
There have been a lot of methods in applying spectral techniques to understand and analyze networks in different domains. It began with the exploration of algebraic proper-ties of a graph by Miroslav Fiedler since 70 X  X  [8, 9]. Since hi s pioneering work, algebraic view of a graph has increasingly attracted more and more attentions and studies from math-ematicians. Among these studies, spectral methods were intensively studied and discovered to be closely related to many invariants and properties of graphs. Therefore, alge-braic graph theory [20, 3] and spectral graph theory [5] were consequently studied and many theoretical results were con -secutively worked out. However, until the end of last cen-tury, these theoretical and mathematical results have not started to be applied to different domains with more and more large and autonomous networks from increasingly de-veloped information and communication technologies [17]. Notable cases are information networks (the graph of World Wide Web and Internet autonomous system level graph), where the spectral information (including both eigenvalue s or eigenvectors) of a graph was used to analyze the graph structure [21, 16]. Recent works were conducted by Ding et al. [6] and Newman [18]. However, these works are different from ours, as they focused on partitioning networks without considering the characteristics of networks, and thus can n ot provide both clustering and ranking capabilities to explor e networks. On the contrary, our proposed algorithm, in this paper, employed the spectral information to simultaneousl y perform clustering and ranking.
In this paper, we presented a algorithm which can simul-taneously cluster and rank the query search result by in-vestigating the spectral geometry of the similarity networ k which is constructed based on the retrieved documents. The proposed algorithm identifies the quasi-orthogonal beams i n the network according to the normalized path length met-ric. These quasi-orthogonal beams represent the clusterin g structure. The document points projected onto one beam direction will belong to same cluster. Meanwhile, the docu-ment points with a larger NPL play an important role in the corresponding cluster because they have much larger similarity with each other. Furthermore, the proposed al-gorithm is very efficient because we adopted spectral anal-ysis method, for more detail on this point can refer to our previous work [14].

Both theoretical and empirical studies have demonstrated that the clustering structure exists in the content similar ity networks. The experimental results showed that our method can appropriately discover the clustering structure of que ry search results, where both clusters and documents X  roles in clusters are clearly indicated. To our knowledge, we can say that our discovery in spectral geometries and cluster-ing structure of query search results opens a new door to organize the retrieved documents in IR system.

There are a lot of venues of future works. For example, our discovery of clustering structure in query search resul ts does not indicate how many possible clusters there are. The work of estimating number of clusters in networks [14] with spectra analysis may be used to complement our method. Also, a large number of experiments over various queries need to be conducted. Conducting experiments to compare clustering results between our proposed algorithm and othe r existing popular algorithms (eg., k-means) is another dire c-tion we can do in the future. [1] H. Almeida, M. Goncalves, M. Cristo, and P. Calado. [2] E. Bernstam, J. Herskovig, Y. Aphinyaphongs, [3] N. Biggs. Algebraic Graph Theory . Cambridge [4] D. Cheng, R. Kannan, S. Vempala, and G. Wang. A [5] F. R. K. Chung. Spectral Graph Theory . Amer. Math. [6] C. Ding, X. He, and H. Zha. A spectral method to [7] E. Estrada and J. A. Rodr  X  X guez-Vel  X azquez. Subgraph [8] M. Fiedler. Algebraic connectivity of graphs. [9] M. Fiedler. A property of eigenvectors of non-negative [10] A. Gulli. On Two Web IR Boosting Tools: Clustering [11] A. K. Jain, M. N. Murty, and P. J. Flynn. Data [12] J. M. Kleinberg. Authoritative sources in a [13] A. Leuski and J. Allan. Improving interactive retrieva l [14] W. Li, W.-K. Ng, Y. Liu, and K.-L. Ong. Enhancing [15] Y. Lin, W. Li, K. Chen, and Y. Liu. A document [16] M. Mihail, C. Gkantsidis, and E. Zegura. Spectral [17] M. E. J. Newman. The structure and function of [18] M. E. J. Newman. Finding community structure in [19] L. Page, S. Brin, R. Motwani, and T. Winograd. The [20] G. Royle and C. Godsil. Algebraic Graph Theory . [21] D. Vukadinovi, P. Huang, and T. Erlebach. On the [22] F. Wang, C. Zhang, and T. Li. Clustering of document [23] J. Xu and H. Li. A boosting algorithm for information [24] W. Xu, X. Liu, and Y. Gong. Document clustering [25] O. Zamir and O. Etzioni. Web document clustering: A [26] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma. [27] Y. Zhao and G. Karypis. Empirical and theoretical [28] Y. Zhao and G. Karypis. Hierarchical clustering
