 This thesis focuses on explanations of recommendations. Explanations can have many adva ntages, from inspiring user trust to helping users make good decisi ons. We have identified seven different aims of explanations, and in this thesis we will consider how explanations can be optimized for some of these aims. We will consider both an explanation X  s content and its presentation. As a domain, we are currently i nvestigating explanations for a movie recommender, and developi ng a prototype system. This paper summarizes the goals of the thesis, the methodology we are using, the work done so far and our intended future work. H.5.2 [ User Interfaces ]: User-centered design, Evaluation/Methodology Design, Experimenta tion, Human Factors Recommender systems, explanations The recommender systems community is reaching a consensus that accuracy metrics such as mean average error (MAE), precision and recall, can only partially evaluate a recommender system [15]. User satisfaction and derivatives thereof such as serendipity [15], diversity [21] a nd trust [4] are increasingly seen as important. Explanations of recommendations can play an important role in improving the user experience. However, the definition of a good explanation is still largely open and depends on the general aim of the recommender system. Previous recommender systems with explanation facilities have been evaluated in a number of ways, which we have reviewed and discussed in-depth in [19]. Among other things, good explanations could help inspire user trust and loyalty, increase contrast, Herlocker et al. [6] evaluated twenty-one different explanation interfaces for a movie recommendation, but measured them according to how likely a user thought they would be to see this movie at the cinema ( Persuasiveness ). Carenini and Moore [3] also focused on persua sion, and found concise and personalized explanations to be more persuasive in the house domain. Other systems aimed at decreasing the time it takes a user to find a good item, i.e. increasing recommendation Efficiency. This has been done by helping the user understand the relation between competing options [13, 14]. It is unlikely for an explanation facility in a recommender system to be optimized for all seven aims , and thus all the more important that this is a conscious choice. Occasionally, the choice of aim is not conscious and leads to trade-offs the creators of the system initially did not consider. For example consider the work of [9], which found that while medical staff in a neonatal unit preferred graphics ( Satisfaction ), they actually made better choices (Effectiveness) as well as were able to make decisions quicker (Efficiency) for the textual version. As discussed above, we have c onducted a comprehensive review of explanations [19]. In our review, we argue that explanations are intrinsically linked with the way recommendations are presented (for instance, top item, top N-items, etc), and with the degree of interactivity offered. The tables therein offer an overview of explanation facilities in existing commercial and academic recommender systems respectively. In addition we considered how to measure the  X  X oodness X  of explanations for each of the aims in Table 1. For instance, one way to evaluate how effective explanations are w ould be to measure the liking of the recommended item prior to and after consumption. Persuasiveness can be measured as the effect on the likelihood of selecting an item. We refer back to [19] for an in detail discussion for each of the aims. Using a user-centered design appro ach, we have investigated what characterizes general properties of useful, or Effective, explanations of recommendations . We have used a methodology of corpus analyses and focus groups [20] to elicit important features in our movie domain, as well as to obtain heuristics such as the optimal number of featur es to mention. The rationale behind studying user X  X  utilization of item features is that simply stating that two items are simila r does not always help users see the commonality between items, while an explanation using feature-based information may be tter help a user understand how two items are related. For exam ple Hingston [7] who studied the perceived Effectiveness of explanations found that participants requested information about why ite ms were judged to be similar to one another in an explanation interface which compared the recommended item to similar items the user had liked in the past. Similarly, Bilgic and Mooney [1 ] failed to show a significant effect on Effectiveness for an explanation interface which used information about previously ra ted items, but where the explicit relations between these previous ly rated items and the current recommendation are not clear. study of explanations in co llaborative recommender systems. Firstly, we perceived that the study often did not control for content, i.e. although a graphical interface gained the most acceptance, it did not have a textual counterpart. In this study we therefore often compared graphical and textual equivalents. In fact, participants initially preferred an equivalent textual description to the bar chart preferred by most participants in [6], until we changed the graphic to a pie chart! We received a great deal of detailed qualitative feedback, including the relevance of the content. For example, most of the participants felt that information about recommendation confidence should be omitted altogether, or at least not be shown as a justification for a recommended item (in any medium). Rather, the participants felt that uncertain recommendations should simply be omitted. In addition, participants sugge sted more detailed possible improvements to interfaces. As participants told us what they liked and preferred, this study focused on perceived Effectiveness of explanations. This thesis started in the dom ain of News recommendations, but changed domain after feedback in the Recommender Systems summer school in Bilbao last year. We had already explored different ways to measure similar ity for news headlines [18]. We compared human judgements of similarity with Lin X  X  taxonomy-based measure [12] and the WASP measure that uses annotated corpus data [11]. The main aim of this work was to better understand similarity, so that it can be used to explain recommendations to users. We found that both the Lin and WASP measures were feasible options for calculating similarity in the context of real-world news headlines. WASP has the advantage of not needing word senses. Howeve r, the Lin measure is better suited for constructing explanati ons that can be understood by users. We proposed a hybrid approach, in which we calculate both the WASP and the Lin measures, us ing word sense 1 for all words (to avoid manual annotation). We can then use the WASP measure as the similarity measure to decide on recommendations, but whenever the measures give similar results, use the least common subsumer as used by Lin to explain the similarity to the user. Though this work was done in the news domain, we believe that it will be applicable to the movie domain as well, and expect to use this in future vers ions of our prototype. Currently, we are developing a prototype which generates explanations for movie recommendations. Figure 1 shows an explanation for a single movie, based on the features that a user found most important features (other people X  X  ratings, and actors), and specific genre preference (action and adventure). Our findings suggest that plot and genre informa tion is important to most if not all users, so these are made available by default. However, to cater for potential space restrictions, the full plot description only extends when explicitly requested by the user. Figure 2 illustrates the basic structure of this prototype. The user model used in this system weighs the movies features elicited by our studies, according to specified user utility. this movie because... X ). More specifically, we would like to find out whether or not this can increase the Trustworthiness of the system, or help the user make a correct decision whether to watch a movie or not (increase Effectiveness). Secondly, we would like to tackle a few presentational issues of explanations. We would like to answer questions such as, in regards to e.g. Trust and Effectiveness, when is text preferable to graphics and vice versa? When do the two media compliment each other? Does the order in which features are mentioned have an effect? We also plan to consider how different types of explanation content (e.g. content-based, collaborative-based, and case-based) affect these types of preferences (e.g. choice of media). Last, but not least, remains the question of the underlying recommender engine. The focus of this thesis lies with the optimal presentation and content for explanations rather than designing a strong recommendation sy stems engine. For the sake of feasibility it would however be beneficial if our prototype simulates or uses a pre-existing engine. A possible solution would be to extend the prototype to use Amazon X  X  recommendation engine, via a similarity l ookup on Amazon X  X  ECS. However, while our explanations are based on content-based preferences, Amazon X  X  recommendations are computed according to a collaborative algorithm. One of the main challenges for the coming year will be to bridge this gap, possibly by augmenting limited meta-data with information from customer and editorial reviews, extracted with the help of semantic similarity measures adapted from [18]. [1] Bilgic, M. and Mooney, R.J. Explaining recommendations: [2] Buchanan, B.G. &amp; Shortliffe, E.H. (ed.) The Rule-Based [3] Carenini, G. and Moore, J. An empirical study of the [4] Chen, L. and Pu, P. Trust building in recommender agents. [5] Czarkowski, M. A Scrutable Adaptive Hypertext . PhD thesis, [6] Herlocker, J. L., Konstan, J. A. and Riedl, J. Explaining [7] Hingston, M. User frie ndly recommender systems. Honours 
