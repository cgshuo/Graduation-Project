 In this paper we present learning models for the class ra-tio estimation problem, which takes as input an unlabeled set of instances and predicts the proportions of instances in the set belonging to the different classes. This prob-lem has applications in social and commercial data analysis. Existing models for class-ratio estimation however require instance-level supervision. Whereas in domains like politics, and demography, set-level supervision is more common. We present a new method for directly estimating class-ratios us-ing set-level supervision. Another serious limitation in ap-plying these techniques to sensitive domains like health is data privacy. We propose a novel label privacy-preserving mechanism that is well-suited for supervised class ratio esti-mation and has guarantees for achieving efficient differential privacy, provided the per-class counts are large enough. We derive learning bounds for the estimation with and with-out privacy constraints, which lead to important insights for the data-publisher. Extensive empirical evaluation shows that our model is more accurate than existing methods and that the proposed privacy mechanism and learning model are well-suited for each other.
In this work we study statistical learning models for es-timating the proportion of instances belonging to different classes in a given set of instances. Many real-world appli-cations motivate this problem: a health analyst wants to estimate the proportion of individuals susceptible to a dis-ease in a locality, a political analyst wants to estimate the proportion of votes to different parties, and so on. Recent work [13, 29] shows how to train such models when pre-sented with a set of instances each attached with its correct label. Unfortunately, such instance-labeled data is not easily  X  Work done while on sabbatical at Microsoft, Hyderabad, India.  X  Work done while on sabbatical at Google, Mountain View, USA.
 accessible in domains like health and social analysis. Often labels are available only for groups of instances and not in-dividual instances in the group. For instance in voting, we only have access to aggregate number of votes to each party, and not the vote of any one individual. [23] lists other sce-narios where instance-level supervision in not plausible. In other cases (example, in health [11]), the labels are private and not easily accessible to an analyst. Much work [15, 24, 4, 14, 16, 1, 30, 7, 27] exists on creating machine learning models over private data. But most of these assume that learning happens within the trust boundary of private data. We are targeting cases where the training data might have to be aggregated from many private organizations for un-derstanding broader trends. Thus, our goal in this work is to learn a model for estimating class proportions under these two constraints: (i) supervision is in the form of label counts on sets of instances, and (ii) the labels of data is pri-vate and model training happens outside the trust boundary of owners of private data.

In the first part of the paper we present a new learning model for estimating class ratios with set-labeled data for supervision. Our model directly estimates class ratios in a given set of instances using the principle of Maximum Mean Discrepancy in a Reproducible Kernel Hilbert Space [10]. We show this paradigm to be significantly more accurate than first building classifiers with set-labeled data using the techniques of [22, 28, 23, 25, 21] and then estimating propor-tions in predicted instance labels. We theoretically analyze our model and show that our model is statistically consis-tent. More interestingly, the analysis shows that our model performs best when trained on few sets, each with a large number of examples. In contrast, existing models for train-ing with set-labeled data (e.g., [28]) prefer many, small sets. Since large sets imply easier hiding of private labels, our model is particularly compatible with the goals of learning under privacy constraints. More importantly, the existing models assume that the training and test distributions are the same, which defeats the very purpose of class-ratio esti-mation. Our empirical results confirm that existing models perform poorly when the class-ratios in the training and test are different; while the proposed models handle such distri-bution shift well.

Next we extend our model to handle data privacy. In this work we focus only on protecting the privacy of the class labels. This setting has been proposed earlier in [3, 21] and is of interest in domains like health and finance where some fields are more private than others. In this work, we consider the popular ( , X  )-differential privacy [8, 2] as the definition of privacy. A widely used mechanism for enforcing differential privacy is based on adding a Laplace noise [8, 20, 18]. We show that this mechanism distorts proportions too much. We propose a new label privacy preserving mecha-nism that is well-suited for the supervised class-ratio esti-mation problem. We theoretically analyze our mechanism and show that our method guarantees  X  0 , X   X  0 as long as each class has large enough counts, independent of the number of classes. In contrast, existing mechanisms based on Lapace noise have a (non-zero) lower bound on , which worsens with the number of classes. Empirically also, we show that our mechanism provides much lower distortion particularly for large number of classes.

Our privacy mechanism and learning algorithm are de-signed to be maximally compatible with each other and aligned to the goals of creating an accurate model while guaranteeing differential privacy. Our privacy mechanism preserves class ratios and incurs low distortion when set sizes are large. Our learning algorithm yields high accuracy when each set is large and the number of sets is small. These to-gether lead to much lower estimation error than can be ob-tained using existing learning models and existing privacy mechanisms.

In summary, the main contributions of this work are: 1. We designed a model for estimating class ratios that 2. We theoretically bound the estimation error of our 3. We propose a new mechanism for achieving ( , X  ) dif-4. We extend the learning model as well as its analy-5. Empirical evaluation of our method on several large
The rest of the paper is organized as follows. We first present and analyze our learning model for class ratio esti-mation with set-labeled data in Section 2. Next in Section 3, we present our mechanism of enforcing differential privacy on label proportions and analyze our learning model with privacy protected data. In Section 4 we empirically compare our learning model and privacy mechanism. We present re-lated work and conclusions in Sections 5 and 6 respectively.
In this section we define the supervised class ratio estima-tion problem and discuss models for it.
We start with a formal definition of the supervised class ratio estimation problem. Let X be the set of all instances and Y = { 1 ,...,c } be the set of all labels. Our goal is to design an estimator M that for any given set U (  X  X ) es-timates the true probabilities of the various classes:  X  u [  X  u 1 ,..., X  uc ] &gt; in the distribution from which U was sam-pled. In this paper we use class-ratios or class-proportions to refer to such  X  . Since U is a finite sample, in practice we will only be able to estimate the sample proportions  X   X  in U belonging to the various classes.

To facilitate the estimation, supervised training data D consisting of M sets of instances, S i  X  X , i = 1 ,...,M , and the corresponding fractions of instances belong to the various classes,  X   X  i are provided, i.e., D = { ( S i 1 ...M } . We will use n i to denote the number of instances in set S i . We call such D set-labeled data. Such set-labeled supervision is much weaker than in standard classification where labels are associated with each instance in the training data 1 .

Needless to say, we need to assume that the affine-hull of the label proportions,  X   X  i  X  i = 1 ,...,M , contains the c -dimensional simplex 2 : If this is not the case, then it is easy to see that some classes may go totally un-represented.

A standard assumption in supervised learning is that the joint-distribution over X  X Y is unchanged between train-ing and test data. In contrast, in this setting we allow the distribution of the class labels to differ among the sets in training and test. For instance, the class proportions  X  the unlabeled set U can be arbitrary and be very different
However this setting still falls under standard supervised learning as the prediction/estimation is also over sets and not over instances
The affine hull of a set of vectors V, denoted by Aff( V ), is the set of all linear combinations of vectors in V such that the combining coefficients sum to unity. from those in the training set. Our only distributional as-sumption about the training and test sets is that the class conditional distribution P ( x | y ) is unchanged. That is, for any two sets S 0 , S in the training and/or test set Note this is a much weaker assumption than in classification and in existing models for class ratio estimation (e.g., [23, 28]) which assume that both P ( x | y ) and P ( y ) are preserved in the training and test sets. Requiring the P ( y ) distribution to be unchanged defeats the very goal of estimating class proportions.
One method to solve the above problem using existing lit-erature, is to tap into the recent work on learning classifiers from set-labeled training data [22, 28, 23, 25]. The goals of individual instances given weak supervision in the form of set-labeled data during training. Using such a classifier, we can estimate label proportions in any set U as follows: for each x j  X  U , invoke classifier to get predicted label  X  y and then estimate/approximate the class proportions as the fractions of instances belonging to the various classes.
Firstly, such methods assume that the training and test distributions are the same and hence will perform poorly. Also, recent work [29, 13] shows that learning methods that directly estimate the class-ratios out-perform such per-instance predictive models. The models of [29, 13] require instance-labeled data and cannot be trained with set-labeled training data, limiting their pragmatic applicability [23]. We are aware of no other work that proposes a method of direct class ratio estimation with set-labeled data. In the next section we present the first such model.
In this section we detail the proposed class ratio estima-tion algorithm.

We begin by recalling the basic assumption A 0, which motivates us to parameterize the class ratios in the unlabeled set using: Then the problem of class ratio estimation boils down to that of estimating the parameters  X  i .

We now make the following identifiability assumption with-out which the class ratio estimation problem is undefined and no algorithm can identify the true class ratios: in other words, we are assuming that the class conditionals are linearly independent. Let us denote the class conditional distribution that is common for all sets by P ( x | y ).
With the above in place, we note that:  X  where P ( x , S i ) denotes the marginal distribution over X that generated S i .

Thus, to solve for  X  we need a method to represent and compare the distributions of each set. Recently, a powerful tool for such algebraic operations on distributions has been provided by the concept of Maximum Mean Discrepency (MMD) on a Reproducible Kernel Hilbert Space (RKHS) [10]. Examples of some algorithms based on MMD are: handling covariance shift [10], the two-sample problem [9], class ratio estimation [29, 13] and training deep generative neural net-works [19]. MMD uses the notion of embedding distributions in the RKHS of a kernel. Using this we find  X   X  X  by minimiz-ing the distance between P ( x ,U ) and P M i =1  X  i P ( x , S
Accordingly, we define K to be a characteristic kernel and let H denote the RKHS induced by K . Let  X  : X 7 X  H denote the canonical feature map induced by the kernel onto the RKHS. Let and under the distribution P ( x , S ).

Our objective of finding  X   X  X  such that P M i =1  X  i P ( x ,S P ( x ,U ) can now be posed as the following optimization problem: The class-ratios can then be computed using  X  u = P  X  where P is the matrix with i th column as  X  i and  X   X  is the optimal solution of (4). Now since  X   X  i ,  X   X  U as well as P are not available, we instead approximate them using the fol-lowing empirical estimates: and where n i = | S i | and n u = | U | . And, This leads to the following estimate for  X   X  X : optimal solution of (5).

Now, because of the approximations it may as well happen that  X  P  X   X  /  X   X  c . Hence we finally propose to employ the following estimate:  X   X  u  X  Proj  X  c  X  P  X   X  , where Proj denotes the projection of vector v onto the set V .
The analysis presented in 2.4 shows that the above two-step approximation indeed leads to a statistically consistent alogrithm. More importantly, it shows that: i) M = c num-ber of sets, each with large number of examples leads to efficient estimation. Note that this is completely in contrast with the  X  -SVM [28]. This feature of our algorithm, as we shall see later, naturally leads to high-levels of label privacy; thus achieving good trade-off between estimation accuracy and label privacy, and, ii) the more the label purity 3 in the sets, the better the estimation.
In this section we present the learning bounds associated with the proposed algorithm. We begin by rewriting (4) and (5) respectively as: where  X  A = [  X   X  1 ( x ) ,...,  X   X  M ( x )] and  X  a = [ where  X  A  X  [  X   X  1 ,...,  X   X  M ] and  X  a = [  X   X  U ].
For the analysis we make another very mild assumption: Note that this is satisfied whenever the instances in D x unique, since the kernel is universal. To simplify the no-tation in the learning bound we assume that the kernel is normalized and hence max x  X  X  k  X ( x ) k = 1.

Theorem 1. Given the notation, and assumptions stated above, the error of our estimated proportions  X   X  u true proportions  X  u is upper bounded with at least probabil-ity 1  X   X  as follows: k  X   X  u  X   X  u k X  where C  X  ( n ) is a confidence term defined by C  X  ( n )  X  q of  X 
A (maxsing is analogously defined), and Q ( D ) is, what we call the condition number of the training set, defined by
We detail in Section 2.4.1 the valuable insights provided by the bound and then in Section 2.4.2 present a sketch of the proof.
First, the confidence term decays at O ( 1  X  n ) with growing sizes of the sets, and shows that the estimation error reduces with increasing number of points in both the training and test sets. The model prefers smaller number of sets with large number of points in each set 4 . Note that this is in contrast with  X  -SVM and is an attractive feature of our algorithm wrt. privacy.

Second, we notice that the upper bound vanishes to zero as the training set sizes n i and test set size n u infinity. This indicates that our estimator is consistent.
Purity of set increases as the proportion of instances with the majority label approaches unity.
However, since we want to span the entire convex hull spanned by the class conditional distributions, the minimum number of sets we must have is c .

The condition number Q ( D ) brings out more desirable properties of our estimator. This term simply measures how  X  X lose-by X  the sets are with respect to the y distribution (via the term maxsing  X  P in the numerator) and the x dis-tribution (via the term minsing  X  A in the denominator). The more diverse the class-ratios are across the sets, less is the condition number and vice-versa. It is easy to see that the condition number is the highest, reaching  X  , when the class-ratios are almost the same across the sets (because the minsing will then be near zero). And is the least, ap-proaching unity, when the class-ratios are orthogonal to each other. This indicates that the estimator performs best when the class proportions in the training set are skewed towards any one class.
To prove the theorem, we begin with the observation that 5 We now proceed to bound the two difference terms above. The first difference term accounts for the error that may arise from the errors in the estimates of the class proportions of each set calculated from finite sets. Lemma 1 gives the error bound for this difference term. The proof for the bound for the second difference term proceeds in two steps. In the first step via Lemma 2, we bound the difference term with the difference in the objective function of the respective optimization problems in Equations 6 and 7. In the second step in Lemma 3, we bound the difference in the objective functions in terms of known quantities.

Lemma 1. With probability 1  X   X  M 2 M +1
Proof. Notice that, k  X  P X  X k F =  X  is the true class proportion in the set S i . We can now pro-ceed to bound each term of this summation using Theorem 27 from [17].

Lemma 2. k  X   X   X   X   X  k X 
Proof. The proof for the above lemma is identical to proof of Lemma 1 in [13].

Lemma 3. With probability 1  X   X  M +1 2 M +1 ,
Here, the matrix norm is the maximum singular value and the Frobenius norm is highlighted with an  X  X  X  subscript.
Proof. Note that, k  X  A  X   X   X   X  a k 2  X  X   X  A  X   X   X   X  a k 2  X  a k 2 . From our discussion in Section 2.3, we know that P Applying Theorem 27 from [17] to each individual difference term gives us the result.
 Combining results from Lemma 1, 2, and 3 and the fact that k  X 
Let S = { ( x 1 ,y 1 ) ,..., ( x m ,y m ) } represent a typical set in the un-published training data. Now, let us denote the set consisting of only the input instances from S i.e., { x 1 by S x . Similary, define S y = { y 1 ,...,y m } . Let  X  and  X  de-note the vectors with entries as the number and proportions respectively of the instances belonging to the various classes in S . Note that P c i =1  X  i = m and thus each  X  i =  X  i
In this section we present a label privacy preserving mech-anism g , which takes as input S y and outputs a vector  X  is a proxy for  X  , the proportions of instances belonging to the various classes in S . Our goal is for g to satisfy the stan-dard differential privacy requirements [8] which states that the output of g should be randomized such that neighbor-ing data sets assign similar probabilities to any output  X  . More formally, let S y ( i,j ) denote the dataset obtained from S y by changing the label of an instance from class i to class j . Then g is said to satisfy ( , X  )-differential privacy (DP) iff: P [ g ( S y )  X  B ]  X   X  + e P h g ( S y ( i,j ) )  X  B i ,  X  B  X   X  We seek a mechanism that achieves this privacy while min-imally distorting the true  X  -s so that they continue to be useful for the learning algorithm. A popular recipe for ( , X  )-differential privacy is to add to each  X  i an independent ran-dom noise z i generated from a Laplace or Gaussian distri-bution[8]. The resultant  X   X  i =  X  i + z i may not be positive or sum to m . Recently, [18] proposed a constrained least square step that can be used to convert such  X   X  i to a valid output in a post-processing step. Another option is to use the technique of [20] that uses  X   X  i to define the parameters of a Dirichlet distribution and sample a valid proportion  X  from this distribution. While all these mechanisms achieve ( , X  ) privacy, they do so at the cost of a large distortion to the true proportions as we show in Section 4. In this paper we propose a new mechanism that achieves ( , X  ) pri-vacy with significantly smaller distortion, provided no class is under-represented. Our mechanism achieves differential privacy with  X  0 , X   X  0 asymptotically 6 . In contrast, in the mechanism of [20] is bounded from below by a value that grows with k .
 The key point of our mechanism is to sample  X  from a Dirichlet distribution given by: As  X  i  X  X  X   X  i = 1 ,...,k .
 Here  X  is an input-dependent parameter that is tuned to achieve the desired trade-off between the level of privacy and accuracy of the output. We next show how to choose  X  for a given ( , X  ) and S y .

Let f 0  X  Dir (  X  X  1 , X  X  2 ,..., X  X  i ,..., X  X  j ,..., X  X  k ) and f Dir (  X  X  1 , X  X  2 ,..., X  (  X  i  X  1) ,..., X  (  X  j + 1) ,..., X  X  probability density functions of g ( S y ) and g ( S y ( i,j ) tively. The key steps in the analysis are: 1. Finding  X  ij such that f 0 (  X  )  X  e f ij (  X  )  X   X   X   X  2. For B 6 X   X  ij , it is easy to see that ( , X  )-differential Accordingly, let us first find  X  i,j : Now, f 0 (  X  )  X  e f
Now, for bounding  X  , we calculate
The above integral is not easy to solve in closed form but we can use the Chebyshev X  X  inequality to bound it as follows.
Let Z ij denote the random variable  X  i  X   X  1  X  ij e  X  expectation of Z ij is E [ Z ij ] =  X   X  i  X   X  1  X  ij e  X   X   X  is given by Using the Chebyshev X  X  inequality, we obtain  X  as: max
For a given , we perform line search over  X  to find the largest  X  for which we achieve a target  X  . We may find no  X  for a given  X  when any of the  X  i -s is too small, in which case one can fallback to any standard mechanism like [20]. In our experiments we performed the search exactly via Equation 9 but for faster search the approximation in Equation 10 might be more useful. level ( , X  ) for given counts  X  . When any  X  i is too small, not all choices of , X  , may yield a valid  X  . The algorithm detects such cases and returns an error. When all  X  i -s are large, our mechanism guarantees that differential privacy with arbitrarily small , and  X  is possible as we show in the asympototic analysis below.
In the asymptotic case i.e., as each  X  i  X   X  ,  X  1  X  ij behaves From the above it is clear that in the asymptotic case, dif-ferential privacy with  X  0 , X   X  0 is possible as long as the parameter  X   X  0 such that  X  X  i  X   X   X  i and  X   X   X  , for some constant  X  &gt; 0. Also, the above analysis shows that sets with near uniform distribution of labels lead to lower  X  , and hence better privacy. However, more interestingly, if  X  is chosen properly, low values of  X  are achievable even with sets with skewed class-ratios (which lead to better class-ratio estimation).

In Section 4 we show that even in the finite set case our mechanism provides much smaller error for a given ( , X  ) requirement than existing methods.
In this section we extend the proposed class-ratio esti-mation algorithm as well as its analysis for the case where the training set is perturbed using the privacy preserving mechanism proposed in the previous section. Let  X  i be the Dirichlet mechanism parameter set to achieve ( , X  )-differential privacy over the set S i .

The estimation algorithm is now given by:  X   X  u  X  Proj  X  where  X  P is the matrix with i th column as g ( S y i ). In other words, the algorithm simply uses the output of the Dirichlet mechanism as a proxy for the fraction of labels in the set. It is easy to see that the only modification in the analysis is that the k  X  P X  X k F term in (8) is now replaced by k  X  P X  X k Again, k  X  P  X  X k F  X  k  X  P  X  X k F + k  X  P  X   X  Pk F . Hence, in the following we analyze only the new term k  X  P X   X  Pk F and leave details of the other analogous terms to the reader. More importantly, analysis of this term illustrates the suitability of the proposed Dirichlet mechanism for estimating class-ratios using the proposed MMD based algorithm. Towards this goal we present the following lemma:
Lemma 4. With probability 1  X   X  we have: where all notations are as defined in Section 2.
 Before we present the proof, it is insightful to note the fol-lowing: Proof. We begin by noting that: P h k  X  P  X   X  Pk &gt;  X  i  X  Also, The result follows by choosing the RHS of the last inequality
We conclude this section by noting that the insights pro-vided from the learning bounds are indeed very useful for the data-publisher, who may have access to instance-level su-pervised data. The bounds suggest that the data-publisher must create c sets, with almost equal number of instances in each set, and such that each set has moderate level of skew in the class-proportions (while ensuring A 0 is satisfied). As the skew increases, the learning becomes more efficient but privacy might suffer. Thus, the data publisher could pre-fer the largest skew that satisfies his privacy requirements possibly as per guidelines laid in [12].
In this section we first show that our mechanism for en-forcing ( , X  ) privacy on a set of labels induces less distortion than existing methods (Section 4.1). We next show that our learning model for estimating class proportions is more ac-curate than existing models for both undistorted training data (Section 4.2) and privacy protected data (Section 4.3)
In this section we compare different privacy preserving mechanisms for publishing a set of labels as discussed in Section 3. For these experiments, the input is a set of c counts  X  = (  X  1 ,..., X  c ) and we generate perturbed  X   X  via dif-ferent mechanisms of ( , X  ) differential privacy. We measure distortion as the L1 distance between the true and perturbed counts scaled by the size of the set.

We compare the following three methods 1. Laplace: In this method we first add Laplace noise [8] 2. Gaussian: Same as above but with the noise gener-3. Laplace Prior: This is the mechanism proposed in 4. Scaled Dirichlet: This is our mechanism described
For these experiments we set the default value of c to 5, set size ( m ) to 1000, class proportion for first c  X  1 classes to a  X  1 = 0 . 05 and the last class to 1  X  ( c  X  1)  X  ( , X  ) to (0.05, 0.05). All reported numbers are averaged over twenty random samples of perturbed outputs for the same input. We study distortion between the true and perturbed proportions under varying values of the default parameters in Figure 1. 0.05 0.15 0.25 0.35 Distortion increasing on five classes.

The first plot in Figure 1 shows distortion in the output of different privacy mechanisms with increasing number of classes ( c ). We observe that as the number of classes in-creases, our method increasingly scores over all existing ap-proaches. The key reason is that all other methods distort by adding independent noise to each class count. In con-trast, our method preserves class ratios by using the same multiplicative parameter to increase variance when required.
Next in the middle plot of Figure 1 we fix the number of classes to 5 and vary the class proportions (  X  1 , X  1 , X  4  X  1 ) by varying  X  1 from 0.025 (skewed proportions) to 0.2 (uniform proportions). We observe that the distortion achieved by our mechanism is the lowest under all settings.
 In the last plot in Figure 1 we vary from 0.01 to 0.2. As expected, the output is distorted more when the pri-vacy requirements is more stringent ( small). The Laplace mechanism is quite competitive with ours (Scaled Dirichlet) when is large but for small , our method provides much smaller distortion. Also, from the error bars we note that the variance of our method is the lowest.

We thus conclude that our privacy model has smaller dis-tortion and smaller variance compared to other models, and is particularly useful when the number of classes is large.
In this section we compare various class ratio estimation models on set-labeled data without any perturbation of the class proportions.

We evaluate our method, which we call MMD , presented in Section 2.3 using a RBF kernel chosen through cross-validation. We vary the bandwidth of the kernels in the range 2  X  5 to 2 5 . The only existing method that we are aware of that can be trained with set-labeled supervision and work with universal kernels is [28]. As discussed in Section 2, we can estimate class ratios using this method by aggregating per-instance predictions from the SVM classi-fier. We call this the pSVM method. The classifier we used was  X  SVM 7 [28] which trains a SVM like model from sets of instances and their class proportions. The parameters for this classifier are chosen via cross-validation. We vary the bandwidth of the kernels in the range 2  X  5 to 2 5 . The other parameters of varied in the range as per [28].
 Datasets.

The method pSVM only works for binary classes. So, for the comparisons in this section we restrict to datasets with two classes. Table 1 summarizes the datasets we used. Census: Census dataset consists of records from 1994 cen-sus database with features that include age, workclass, oc-cupation, relationship, race, sex etc. The target label is income which is 1 if the person earns more than 50000 in a year and 0 otherwise. This dataset is available from the UCI repository 8 .
 Higgs: In a particle accelerator, not all collisions are likely to produce interesting particles. This dataset captures fea-tures of several collisions or processes. The label indicates whether a given collision or process is going to produce an interesting particle or not. 21 out of the 28 features are prop-erties measured by the particle detectors in the accelerator and the remaining 7 are abstract features hand-designed by physicists. This dataset is available from the UCI repository. Youtube: A dataset based on YouTube comments that we created based on this 9 collection. The goal in the YouTube dataset is to estimate the fraction of comments that are
Code taken from https://github.com/felixyu/pSVM https://archive.ics.uci.edu/ml/datasets/Census+Income http://mlg.ucd.ie/yt 0.15 0.25 Estimation Error 0.05 Estimation Error spams on a YouTube video. The dataset was crawled by tracking 6407 popular YouTube videos over 77 days and comprises of 6,431,471 comments labeled spam or not. The feature set is a normalized TF-IDF vector over 1000 words + a comment length feature.

If the number of classes is c , we create c training sets in the following way: we initially choose a value  X  which is usually 0.05 or 0.1. Then, we create c sets with the following class proportions [  X , X ,..., 1  X  ( c  X  1)  X  ] , [  X , X ,..., 1  X  ( c  X  training sets are all class conditionally sampled. In addition, we add c more sets with the same class proportions as above to support cross validation for kernel selection.

Since, we are dealing with only binary datasets in this experiment, we create 4 sets in training, 2 of them have pro-portions (0 . 1 , 0 . 9) and the other 2 had proportions 0 . 9 , 0 . 1. This is similar to the process described above with  X  = 0 . 1. For testing, we created various test sets whose positive class proportions varied from 0 . 1 to 0 . 9.

For each method we measure estimation error as the L1 distance between the correct class proportions in the test set and the proportions estimated by the method. In Figure 2 we show the errors of the baseline pSVM and our MMD method on test sizes with increasing fraction of instances in the first class. We observe that MMD estimates have much lower error than pSVM especially for extreme class skews. The pSVM method assumes that the training and test distributions of P ( y ) remain unchanged, and therefore this method returns accurate class ratios only in the range of ratios where this assumption holds. Our method allows the P ( y ) distribution to shift and therefore provides lower error on a wider range.

We conclude from this section with set-labeled training data, our proposed MMD method provides the lowest error among existing options for a large range of test set ratios.
In this section, we evaluate if our estimates continue to remain accurate when the label proportions are distorted to protect privacy. Our experiments in Section 4.1 showed that the distortion is within tolerable limits for binary datasets but is large for non-binary datasets. Therefore, in this sec-tion we report experiments on real-world non-binary datasets. The three we considered are described below and summa-rized in Table 1.
 Mnist: This is a handwritten digit recognition dataset. The target labels are the digits 0 to 9 and inputs are fixed size input containing the handwritten image of a digit.
 Acoustic: Acoustic is a three class dataset about classi-fying military vehicles from microphone recordings. This dataset as well as Mnist are available from LibSVM multi-class dataset repository 10 .
 Twitter: This twitter dataset was created for the task of classifying each tweet into one of three sentiment classes: positive, negative and neutral. We use the dataset and fea-ture extraction mechanism described in [26]. The authors of [26] have made code and data available online 11 .

In Figure 3 we show the MMD estimation error when trained with data distorted by three privacy mechanisms: ours (scaled dirichlet), Laplace, and Laplace Prior as de-fined in Section 4.1. We drop the Gaussian method since it provides no advantage over the Laplace method as per our experiments in Section 4.1. Instead we add as a refer-ence the errors when MMD is trained with undistorted sets. Note this method does not guarantee any privacy and is just included to get a lower bound. The test set for these exper-iments where created with varying class proportions using the parameter  X  1 as described in Section 4.1. The X -axis in Figure 3 indicates these class proportions.

We can make the following observations from these figures. 1. As expected, our estimation model is sensitive to the 2. When the number of classes is large (as in Mnist), https://www.csie.ntu.edu.tw/  X cjlin/libsvmtools/datasets/ multiclass.html https://github.com/duytinvo/ijcai2015 0.3 0.4 0.5 0.6 Estimation Error 0.1 0.2 Estimation Error 3. The effect of bad training data is more pronounced
In this paper we presented a learning model for estimating proportions of labels in a set of instances using set-labeled training data with privacy constraints on the true label.
Our use of set-labeled data for training is related to the models proposed in [22, 28, 23, 25, 21] for training classifi-cation models with the same kind of supervision. However, the way we wrap an MMD objective around set-labeled data allows effective learning from a few, large sets. In contrast, the classification models of [28], while being kernel-based like us, prefer many small sets. Since small sets is anti-thetical to privacy, our method is particularly suitable for learning under privacy constraints.

The use of MMD for estimating class proportions has been explored before in [13, 29]. But these models require instance-labeled data during training which does not work under privacy constraints. One option would have been to train the model with private data while enforcing privacy during the model creation phase as used in [24, 4, 14, 16, 1] and several others [15]. We did not consider this option be-cause we are targeting scenarios where the training data is aggregated from several private organizations and the model creation happens outside the trust boundary.

Another class of methods attempt to first create DP joint distribution of the data [6, 20] and then sample instances from the distribution for down-stream tasks like model-creation. These summaries are for general-purpose analysis, and not a specific classification task, and are expected to be less ac-curate. Ours and others like [21, 30] of creating summaries is geared for the prediction task at hand while allowing data from multiple agencies to be aggregated. We have shown that our mechanism for publishing label proportions pro-vides much higher accuracy for the same ( , X  ) guarantees than existing approaches based adding Gaussian noise to each component as suggested in [8] and sanitizing the output using either the recently proposed constrained least square approach [18] or the Dirichlet samples [20].

A different category of approach attempt to publish com-binatorial summaries of data, for instance [5] proposes to create trees to publish set-valued data for differential pri-vacy. Even though the method is designed for publishing sets of items, their raw data is a large set of records each of which is small set of items. Consequently their definition of differential privacy (DP) is to be insensitive to the removal of a record. In contrast, our raw data is a single set of labels and our notion of DP is to be insensitive to the change of any single element of the set.
In this paper we designed a model for estimating class ratios and devised mechanisms for training it in scenarios where labels are provided on sets of instances and where labels are private. We theoretically analyzed our model and showed it to be consistent and accurate when the number of training sets is large. This is in contrast to existing methods that prefer many, small sets. Empirical evaluation on three real-world datasets show that our estimator provides lower error than existing methods, particularly when test class distributions are skewed.

We proposed a new mechanism for achieving differential privacy of labels that is more effective in preserving class ra-tios than existing mechanisms, particularly when the num-ber of classes is large. We extend the learning model as well as its analysis for privacy-protected data. We show that the proposed learning and privacy mechanisms are well-suited for each other. In particular we show common conditions for achieving efficiency in both these phases. Empirical eval-uation on several large real-datasets shows that the combi-nation of our learning algorithm and privacy mechanism is able to provide significantly more accurate estimates than existing methods.

Our future work includes extending our model for the case of continuous y values (regression setting). Also, we would like to extend our mechanism to protect the privacy of the x part of an instance. Since our estimator is based on kernels, one idea is to use the technique of [24] for preserving privacy of the x and our current technique to protect the privacy of the y s. [1] R. Bassily, A. Smith, and A. Thakurta. Private [2] A. Blum, C. Dwork, F. McSherry, and K. Nissim. [3] K. Chaudhuri and D. Hsu. Sample complexity bounds [4] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. [5] R. Chen, N. Mohammed, B. C. M. Fung, B. C. Desai, [6] R. Chen, Q. Xiao, Y. Zhang, and J. Xu. Differentially [7] J. C. Duchi, M. I. Jordan, and M. J. Wainwright. [8] C. Dwork and A. Roth. The algorithmic foundations [9] A. Gretton, K. M. Borgwardt, M. J. Rasch, [10] A. Gretton, A. J. Smola, J. Huang, M. Schmittfull, [11] C. Guttmann, X. Sun, C. Rao, C. Queiroz, and B. I. [12] J. Hsu, M. Gaboardi, A. Haeberlen, S. Khanna, [13] A. Iyer, S. Nath, and S. Sarawagi. Maximum mean [14] P. Jain and A. Thakurta. Differentially private [15] Z. Ji, Z. C. Lipton, and C. Elkan. Differential privacy [16] D. Kifer, A. Smith, and A. Thakurta. Private convex [17] S. Le. Learning via Hilbert space embedding of [18] J. Lee, Y. Wang, and D. Kifer. Maximum likelihood [19] Y. Li, K. Swersky, and R. S. Zemel. Generative [20] A. Machanavajjhala, D. Kifer, J. M. Abowd, [21] R. Nock, G. Patrini, and A. Friedman. Rademacher [22] G. Patrini, R. Nock, T. Caetano, and P. Rivera. [23] N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. [24] B. I. P. Rubinstein, P. L. Bartlett, L. Huang, and [25] S. R  X  uping. SVM classifier estimation from group [26] D.-T. Vo and Y. Zhang. Target-dependent twitter [27] Y. Xin and T. Jaakkola. Controlling privacy in [28] F. X. Yu, D. Liu, S. Kumar, T. Jebara, and S. Chang. [29] K. Zhang, B. Sch  X  olkopf, K. Muandet, and Z. Wang. [30] S. Zhou, J. D. Lafferty, and L. A. Wasserman.
