 To facilitate direct comparisons between different products, we present an approach to constructing short and compar-ative summaries based on product reviews. In particular, the user can view automatically aligned pairs of snippets describing reviewers X  opinions on different features (also se-lected automatically by our approach) for two selected prod-ucts. We propose a submodular objective function that avoids redundancy, that is efficient to optimize, and that aligns the snippets into pairs. Snippets are chosen from product reviews and thus easy to obtain. In our experi-ments, we show that the method constructs qualitatively good summaries, and that it can be tuned via supervised learning.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models summarization; reviews; comparison; snippets; pairs; sub-modular
After deciding what kind of product to buy (e.g. a cell phone), it is often still hard to make a good choice due to abundance of different brands and models. Using the inter-net as a convenient source of information, we can usually find a wealth of data describing many products  X  even more so if we plan to make our purchase online too. First, read-ing in-depth professional reviews might help us to familiarize ourselves with strengths and weaknesses of a certain prod-uct, but does not directly give us direct understanding of how it compares to other choices. Second, we can find ta-bles comparing products, but they usually list only product feature specifications as documented by the manufacturer and do not provide insight into how those features translate into usefulness during the actual use. Third, we can find user reviews of products which describe experiences of using the product and are often helpful in making the purchasing decision. However, the major drawbacks are the need to read many reviews to get an estimate of agreement between users and the need to manually compare different products (by keeping the contents of the reviews in mind).

Our aim is to fill-in this space and provide a way of explic-itly comparing products of the same type (e.g., competing cell phones). These comparisons should reflect users X  expe-riences (i.e., are based on product reviews), so that they can complement specification-based comparisons tables.
 Many online retailer websites already provide user reviews. In our approach, we offer an additional view of these re-views by providing a compact (to avoid the need to read many long reviews) and comparative (to facilitate decision support) summary of them. In other words, we select impor-tant snippets from the reviews of one product and present them aligned with snippets talking about the same aspect from the reviews of the other product. This allows users to quickly read through the aligned pairs describing reviewers X  opinions on important aspects of the two products they are trying to compare.
 Product Comparative Snippet Pairs A battery lasted for about 7h of web browsing B I got about 8h but only if I disabled wireless A screen has a good uniform lighting B there was a slight light bleed on the screen A buttons were hard to press B despite small buttons, they were easy to use Figure 1: Illustrative example of a comparative sum-mary for products A and B .

Figure 1 shows an example of what we would expect from the system. Each pair talks about one aspect of the prod-uct and the selection should represent the most important ones (as defined by the reviewers X  selection of which ones to mention). The snippets give insight into how well the product specifications translate into real-world utility based on reviewers X  experiences and how do they compare to a competing product. Reading such summary provides an al-ternative to time-consuming reading of many reviews and at the same time provides direct comparison between different purchasing choices.
We assume that the user selects two products A &amp; B to compare against each other, and that each product p  X  { A,B } comes with a respective set of reviews R p = r p 1 (obtained from e.g. online retailer X  X  website). Each set of reviews R p is then split into snippets S p = s p 1 ,...,s e.g. using sentences as snippets). Finally, we use our ob-jective function F to select a set of snippet pairs ( s (where both snippets describe the same aspect but for two different products) that represent the final summary.
Each snippet is represented as a vector (in the simplest case using a bag-of-words TFIDF scores; otherwise snippets can be represented using any features) and v p k ( x ) represents the value of feature x in snippet s p k (e.g. TFIDF score of word x ). To construct a pair, we want to select two snip-pets (one for each product) such that they talk about the same aspect (i.e. are aligned). To promote good alignment we use sum of features in the intersection of the two snip-pets (Eq. 1). Furthermore, we want to also account for the information outside the intersection to promote showing the differences (instead of just finding the most similar snip-pets). To achieve this we use the sum of features outside of the intersection (Eq. 2), but clamped to be less or equal to the value of the intersection. Clamping the score avoids as-signing high scores to some bad corner-cases (e.g. two long snippets being aligned only on the word  X  X he X ). Finally, for any candidate ( s A i  X  S A ,s B j  X  S B ) we then use Eq. 3 (with the favorable properties of having a good alignment and in-cluding useful comparative information) to score the given pairing of snippets.
Let X  X  consider some examples of bad snippet pairs to bet-ter understand why we selected this scoring function. Snip-pet pair (the battery life is good, the screen had some light bleed) is a bad choice because the snippets overlap only on the word  X  X he X . It does not get chosen because the score is limited by the intersection. Pair (all buttons are big and very easy on the fingers, has buttons) is highly unbalanced (left snippet adds a lot of weight if we naively summed the word weights) but the score is again limited by the inter-section. In the third example (and the screen has fantastic colors, while the screen has fantastic colors) we have a large intersection, but still a low score because we do not have much product specific information (and thus parts outside the intersection do not add much weight). Our approach still includes coverage terms (and thus gives weight to fre-quently occurring phrases), however it prefers to select and point out the differences (to facilitate deciding between the products).

In contrast to scoring a single pair of snippets (Eq. 3), the main objective function (Eq. 7) is a submodular set function. In addition to selecting aligned pairs talking about important aspects, we can now select diverse pairs that give a good coverage of the source information (original reviews) while avoiding redundancy. We parametrize this objective with weight vector w (where scalar w x corresponds to the feature x ), which allows for supervised learning, but can be substituted with 1 in the simple case of uniform weights .
The objective function F (Eq. 7) follows a similar pattern to the individual pair scoring. To simplify the notation we use  X  ( S ) to represent the union of all words present in snip-pets for product A included in S but not for B , similarly  X  ( S ) for words in B but not A and  X  ( S ) for the union of all words in the intersections. Function H sums the largest weights for all words (thus resulting in diminishing returns for covering a word multiple times) in the target set (i.e. intersection X =  X  ( S ) or the remainder corresponding to A or B ). The final objective (Eq. 7) is composed of four parts (corresponding to intersections and remainders using weights for A or B ).

Eq. 7 can be efficiently maximized using a greedy algo-rithm (linear in the number of candidate pairs and selected set cardinality) which achieves a constant factor approxima-tion [8] and works well in practice. Data. To evaluate our approach we used reviews from Amazon X  X  web site. We scraped reviews for 8 tablets (from different manufacturers) and split them into sentences based on punctuation. Then we parsed those sentences and, if nec-essary, further split them into smaller snippets (e.g. in the case of two clauses connected by  X  X nd X ). By doing this we obtain in the order of 10000 snippets per product. Exper-iments were performed using TFIDF weighting, where we treat each review as one document.

Filtering. Because we are selecting pairs of snippets, the number of candidate pairs (and thus running time of the greedy algorithm) grows quadratically with the number of snippets obtained from the reviews. To speed up the selec-tion of which pairs to present to the user, we precomputed the set of the top most similar snippets (using cosine simi-larity). For each product pair (for which we want to show a comparison) we limit ourselves to the top 10000 most sim-ilar pairs while running the greedy algorithm. We believe that this is a reasonable number based on our empirical ob-servations and the fact that even within this limited set the similarities in the bottom part already became very weak.
Labels. For the purposes of the evaluation and super-vised learning we require labeled data. Our labels are per snippet pair and are defined as follows: +1.0 a good pair (the two snippets are talking about the +0.5 a misaligned pair (at least one snippet contains useful -0.5 irrelevant comments (e.g. reviewer discussing seller X  X  -1.0 a bad pair (the pair contains no useful information, is
Qualitative Evaluation. On Figure 2 we show the top 5 pairs selected by our method comparing the Apple iPad with the Google Nexus tablet. All selected pairs are  X  X ood X  according to the labeling and they also fit our goals: they are aligned (both snippets talk about the same aspect, e.g. microsd slot), they are balanced (no very short snippets paired with extremely long ones), and they talk about a non-redundant set of topics (e.g. storage space, expansion slots, email accounts etc.).

Uniform weights. Our approach can be used in a non-learning setting (where we do not require labels except for the evaluation purposes). In this case we use uniform weights by setting w = 1 . We compare the results of this scoring function against the following baseline , which simply selects the top most similar snippets as the final output. The only additional restriction is that no snippet may be selected more than once. Note that this simple baseline does not account for redundancy (except for not repeating the same snippet) and coverage of what is important, but only strives to maximize the intersections of snippets.

Comparison of our method with the baseline for selecting 40 pairs is shown in Table 1. It demonstrates that the simple baseline results in substantially worse performance than us-ing our approach with uniform weights. Furthermore, after manually comparing them we believe that the pairs selected by the baseline are qualitatively worse overall than the ones selected by our approach.

Learning. The goal of learning in our approach is to generalize across product pairs. For example, if we get some labels for comparison of camera models A and B , we would like to use this information to improve the performance for the comparisons of C and D as well. We can expect this to be possible because, for example, indicating that resolution is an important factor most likely applies across all models.
In our experiments for the supervised learning case, we simulate an online setting as we would expect it in a de-ployed version. For a given product pair that we want to compare, we select 5 pairs to be presented to the user us-ing our approach. For each individual pair, we receive user feedback expressed as labels as defined in the Labels para-graph. After each such iteration, we retrain our model using all the labels obtained so far. This is easily doable due to small number of training examples, but one could also use an incremental learning approach. Furthermore, to facili-tate exploration and to simplify the labeling process, we do not allow the same pair to be selected again in the following iterations. In this way we obtain 100 labels by doing 20 iter-ations of presenting 5 pairs (using weights computed in the previous iteration). Altogether, one annotator labeled 100 training snippet pairs for each of 4 distinct product pairs. For training we use a linear support vector regression (us-ing the label values) on a bag-of-words representation (with TFIDF weighting).
 Table 1: Average performance scores of baseline , us-ing uniform weights and learned weights across prod-uct pairs. Our approach outperforms the baseline and achieves more than 20% improvement on previ-ously unseen product pairs through learning.
The performance is measured by selecting 40 pairs from a different product pair with a disjoint set of reviews (to avoid any possible overlap in the data) but using the same weights, and computing the score according to the labeling. The reason for selecting a larger set of pairs (40) is to obtain a more robust score, because measuring smaller performance changes on only 5 pairs is unreliable due to low granularity. The results are then averaged across all combinations of one training and one testing product pair. The comparison in Table 1 shows more than 20% increase in the performance above the uniformly weighted case (which is already good in itself by looking at the qualitative evaluation on Figure 2).
Model variants. We experimented with other possible features in addition to bag-of-words TFIDF scores, but the ones we tried did not noticeably improve the score (which is already high for the basic model). Furthermore, minor changes to the scoring function do not immediately break the model from what we observed. Also, selecting pairs in-dividually instead of using the global submodular objective still produces reasonable results, but introduces noticeable amounts of redundancy into the summary as expected.
Instead of performing generic summarization [2] or select-ing one representative sentence [1] based on a single corpus, we are constructing summaries based on two sets of docu-ments (in our case snippets from reviews).

There is existing work that extracts features based on product specification [7] and another one that discovers as-pects and associated opinions [13] which can then be used to summarize reviewer X  X  opinions for a single product, while we use coverage to automatically select snippets talking about important aspects. Or we can look at retrieving consensus opinion [5, 3] on product features, while our approach tries to show snippets covering the most important facts or opin-ions. We are using a coverage based objective to achieve diversity, because balance of presented aspects is important [6].

Other research has already been done in presenting sum-maries as pairs of items. For example, contrastive pairs [4, 9] aligns positive and negative opinions on the same aspect. Our approach does not produce a summary that contrasts sentiment, but we construct pairs that contrast products.
Another relevant topic to our work are approaches that summarize differences, such as comparative summaries con-structed by using dominating sets [11] and scoring terms based on how likely they are to appear in the other collec-tion [10]. Similar ideas can also be found in summarizing differences in multilingual news [12]. have to download specific apps to be able to download anything since safari doesnt handle downloads and you cant add music to the ipod app without first synching with itunes unless youre purchasing from itunes on the device itself setup was easy and it synced flawlessly with my gmail account automatically downloading apps expansion slots that so many android tablets have you plan on gaming and storing music you may have a bit of trouble with running out of space if you think you might like some extra storage space then i suggest getting the new 32gb model that was recently released
If we compare related work with our approach, we are closer to summarizing differences except that snippets still have to be aligned on aspect. Compared to opinion focused approaches we do not distinguish between positive and neg-ative, but focus on how is one product (perceivably) differ-ent (or similar) from the other. Although we implicitly try to cover all important aspects, we do not explicitly extract them and rely on submodular objective to achieve balanced coverage and avoid redundancy.
In this paper we presented an approach to selecting pairs of snippets from reviews in a way that creates a summa-rizing product comparison. Our scoring function strives to select aligned pairs (both snippets are about the same as-pect) with good coverage of important aspects and low re-dundancy. The objective function is submodular and thus efficient to optimize with a constant factor approximation. Our experiments show that we outperform a naive baseline even with the uniform weights model. Using a supervised learning approach we can achieve generalization across dif-ferent product pairs by using user feedback on the presented pairs.
This research was funded in part by NSF Awards IIS-0905467, IIS-1142251, and IIS-1247696. [1] P. Beineke, T. Hastie, C. Manning, and [2] J. Carbonell and J. Goldstein. The use of mmr, [3] J. Choi, D. Kim, S. Kim, J. Lee, S. Lim, S. Lee, and [4] H. D. Kim and C. Zhai. Generating comparative [5] T. Lappas and D. Gunopulos. Efficient confident [6] M. Mahajan, P. Nguyen, and G. Zweig.
 [7] X. Meng and H. Wang. Mining user reviews: from [8] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis [9] M. J. Paul, C. Zhai, and R. Girju. Summarizing [10] G. Raveendran and C. L. Clarke. Lightweight [11] C. Shen and T. Li. Multi-document summarization via [12] X. Wan, H. Jia, S. Huang, and J. Xiao. Summarizing [13] H. Wang, Y. Lu, and C. Zhai. Latent aspect rating
