 Most of the former or existing IR methods are based on the occurrences of terms in a know, a word may have a lot of senses. A sense can also be represented by more than one word. So, we doubted whether using the formalized meaning of the word, instead of the word itself, as the processing object can improve the accuracy of the IR system. Meanwhile, some research teams have fully studied the semantic space [1-3] and employed a formalized symbolic system to represent the space [2-3] . This symbolic system provide an important foundation for us to represent the word sense. On the task for us is to propose an approach which can accurately draw the meaning of a word (namely concept) according to the co ntext based on the symbolic system. After the concept can be successfully extract ed, a suitable model for IR is required. Statistical model becomes the preferred approach since we are still not able to measure the semantic relevance of a query against a document within a collection using linguistic or even semantic method. A number of researches have, recently, confirmed that the language model is an effective and promising approach for IR [7, 8] . Therefore, we attempt to construct a suitable statistical model and apply varying degrees of NLU(Nature Language Understanding) scheme to the basic retrieval model. Consequently, we propose a domain language model. The main idea of a domain language model is derived from the Aspect Model [10] . A formalized symbolic system which is used to express the meaning has been designed and constructed according to the needs of the NLP engineering recently [2-4] . It can be classified into two parts: the basal member sub-system and the sentence category sub-system. The basal member sub-system is designed for describing the meaning of the terms. There are 108 concept trees within this su b-system. Each tree describes a category of the concepts. Each node of a tree, indexed by an exclusive character strings, represents a concept (namely, a meaning). The connotation of the child node represents a narrower sense than his father node, but more concrete. An algorithm for semantic relativity calculation based on the basal member sub-system is also addressed [9] . It can be used to measure the semantic relativity between two concepts. Sentence category sub-system is designed for describing the meaning of sentences. The fundamental of this sub-system is using the finite expressions to express the meanings of the infinite sentences. These fi nite expressions are designed in advance. Huang concluded 57 types of primitive sentence category expressions and 57*56 compound ones [2, 3] . As a result, the meaning of sentences can be formalized. The details of the symbolic system follow the definition described by MIAO [4] . Language modeling has been applied su ccessfully in information retrieval [5, 6] . Given a document d and a query q , the basic principle of this approach is to compute the maximum conditional probability ) | ( Q D P as follows. and to improve the maximum likelihood estimation, smoothing is involved. Most of the language models use the collection model as the reference model to interpolate or smooth the document model. The extracting approach is based on the semantic and linguistic relationships among the sentence category expressions, the semantic chunks and the words. Three knowledge bases are involved in the approach. They are TCK (Term Concept Knowledge base), SRK (Scheduler Rule Knowledge base) and SREK (Semantic Relativity Knowledge base). These knowledge bases stored all the useful knowledge refined in advance by an assistant system [9] . They are the foundations of functions, () f and () g . 
TCK is the key knowledge base for the extracting approach. It is like a special vocabulary. It stores the terms and all thei r corresponding concept candidates. It also restrictions of selecting a concept candidate of a term in a certain context. The affiliation information between the terms and the sentence category expressions is also stored in TCK. It provides the information about which sentence category expression should be hypothesized and which character can be used to validate the hypothesis according to the given terms the computer reads. SRK determines which sentence category expression should be hypothesized first, which one second, calculation. Relativity calculation can help to determine the term concepts within a local range. SREK was constructed based on the algorithm (called AlgZ) [7] for semantic relativity calculation. It is somewhat like the relation definition between two words in WordNet. The details of the three knowledge bases are described in [9]. 
Processing strategy focuses mainly on the processing logic which tells computer how to get the sentence category expressions and the term concepts through some specific procedures. The whole processing strategy is an iterative procedure of () f and () g . Due to the limitations of space, we only provide the block diagram here. The strategy is shown schematically in Fig. 1 
The process can be divided into four sub-stages. The first sub-stage is called pretreatment. The assignments in this sub-stage depend on the language it processes. If Chinese is processed, the word segmenta tion will be performed. In the second sub-stage, the semantic chunks in the sentence and their corresponding sentence category expressions will be hypothesized according to the terms in the sentence and their concepts recorded in CE of the TCK. Th e possible sentence category expressions are calculation will be involved. It is used to derive the modifier of a headword or locate the juxtaposition of terms even without the use of coordinating or subordinating conjunctions. In the third sub-stage, the hypothesis made in the second sub-stage will be tested according to the  X  X C X  in the TCK. The right sentence category expression will be proved and obtained. Composition analysis of chunks is the last sub-stage. Through this sub-stage, we can get all components of the chunks. Components are expressed by the exact term concepts. After processing, sentence category expression and the concept expression of each term in the sentence can be obtained. The basic idea of the domain language model approach is to estimate the domain language model for a document and then to compute the likelihood that the query would have been generated from the estimated model. Therefore, the key issue is how to estimate the domain language model for a document based on the observation of a collection of documents. Consequently, the idea of Aspect Model [10] is introduced into the new model. The domain in domain language model is similar to the unobserved class variable in Aspect Model. As a result we can translate the domain language model into a joint probability model shown as following. of domain p generating the concept t . 
In order to obtain the unobserved domain variable p , clustering method is estimated based on the distribution of the domain-based clusters: ) | ( d p P is estimated according to the frequency of the concept t in the cluster p . 
In the proposed cluster generation method, K-Means is adopted for its ease of implementation. Profited from the concept trees defined in the basal member sub-system, the proposed method, different from the traditional method, assigned the data 108 concept trees have strong and discrete domain characters [3] . Therefore, we create 24 data points as domain seeds manually according to the concep t trees. Each data As a result, initial domain-based clusters are assigned. After that, we use two stage K-means (which means the algorithm only alternates two times) to generate the clusters. When the clustering procedure is finished, we remove the seeds from the clusters. 
The proposed method uses Kullback-Liebler distance algorithm to measure the correlation between a document and a cluster. The objective function follows. where ) , ( c d KL defines the degree of correlation between a document (or a collection of documents: cluster) d and a document (cluster) c ; ) , ( x t n i is the document-concept weight, which is a measure of the number of occurrences of concept i t in the document (cluster) x ; | | x is the number of concepts in the document (cluster) x . The procedure for the clustering algorithm is simply described in Algorithm 1. 10. Calculate each KL( i d , k p ) and save the values to a file, where D d q  X  and The proposed approach computes document query similarity in two stages. 
In stage 1. All the documents will be indexed into the concepts. In stage 2, the query will be translated into their conceptual form. 
To accomplish stage 1, we define the conditional probability ) | ( D C P as the probability of using concept C as the domain for document D , which is given below. 
As can be seen from Eq. (3), the domain language model the role of collection estimates to compute the probability of a concept term. and cluster p . The distance has been calculated in Algorithm 1 of section 4.2.2. cluster s P .
In stage 2, the query should be translated into its conceptual form in order to match algorithms are simply described in Algorithm 2. This approach are based on the algorithm (called AlgZ) [10] for semantic relativity calculation. From Algorithm 2, we can see that sometimes the output is not one set of concepts. In this situation, we compute the concept document similarity for each set of scoring of the similarity. We have implemented a research prototype retrieval engine (called HNCIR) to test our approach. We now provide experimental results to illustrate the behavior of HNCIR. The Chinese test collections are chosen from TREC6. It was 170 Mb as raw texts. There were 26 topics (CH 29-54) constructed. The TF-IDF has shown its superior performance for document indexing [12] . Therefore, this scheme is used as a standard for comparison with HNCIR. Some modification was properly applied in order to enable it to implement the Chinese IR. 
We also try to improve our probability estimates since this should yield better retrieval performance. We called this model HNCIR-X. This improvement of the estimate is to translate the queries into their conceptual form manually, and then input them as the new queries to the system. This can greatly help to find out the effect that the query translation brings us and the estimate precision of the query translation. 
We measured the recall level precisions. The test results of these 3 approaches and the uninterpolated average precision over all relevant docs are given in Table 1. The precision results show that the proposed system outperforms the TF-IDF method. HNCIR and HNCIR-X increased the precision of the traditional TF-IDF method by more There are two possible conclusions can be made. One is that the sense ambiguities of the query keys are not serious. The second is that the AlgZ can well match the needs of the query translation. Considering all the test topics, we find the first reason effects more on producing such a result. It seems that the query translation does not contribute a lot to the improvement of the system performance in the case of the TREC6 test collection. Nevertheless, the AlgZ-based query translation has played an important role in CH37, CH41, CH42, Ch46 and CH47. It greatly helps the system to explicate the user X  X  intentions through analyzing the semantic relationship between the query keys. Certainly, it costs some additional time to complete the estimation. The cost of the computational time will be explicated in the following experiments. Table 3 shows the comparison between TF-IDF and HNCIR. It shows that the sub-total processing time for Chinese IR of HNCIR are more than 1.5 times than that of TF-IDF, but the retrieval time of the HNCIR are less longer. So we can conclude that on the time criterion, IR using HNCIR has advantages over IR using TF-IDF from the point of view of the information seekers, but has disadvantages from the point of view of the document processing users (sometimes, the service providers). However, For a commercial system, the satisfaction of the searchers is the top goal of the service providers. Therefore, the overall time cost of HNCIR is not inferior to that of TF-IDF. In this paper, we have proposed an information retrieval model. In this model, we tend to apply the NLU schemes to the SNLP (Statistical NLP) methods. The goal of addressing this issue is to study a new approach, which can take advantage of both the NLU and the SNLP, to better serve the IR. The experiments in which the proposed approach was compared with the traditio nal TF-IDF method highlighted the better performance of the proposed scheme, especially with regard to the average precision. 
