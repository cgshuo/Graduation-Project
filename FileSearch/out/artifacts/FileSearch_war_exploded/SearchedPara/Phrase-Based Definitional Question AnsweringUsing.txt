 Definitional question answering is a task of answering definition questions, such as What are fractals? and Who is Andrew Carnegie? , initiated by TREC Ques-tion Answering Track[1]. Most of the definitional question answering systems consist of the following three component s: question analysis, passage retrieval, and answer extraction. Contrary to factoid or list questions, having a narrow question focus, definition questions do not have expected answer type but con-tain only the question target in the quest ion, so relatively simple works are done in question analysis phase. In the questio n analysis phase, a question target is ex-tracted from the question sentence. For example a question target ETA or ETA in Spain should be extracted from the question What is the ETA in Spain? .Most efforts to the retrieval phase are concentrated on expanding the question, char-acterized by the insufficient information and vague question focus. Most systems regard all sentences in the retrieved passages as answer candidates, and rank the candidates using several evidences.
 sources such as online dictionaries, are k nown to be so useful for ranking answer candidates[2]. Most researches use the manually constructed lexical patterns, and the construction task is labor-intensive. Although the lexical patterns can be automatically trained and collected[3], they suffer from little coverage. More-over, external resources may not contain any information about the question target, although the external definitions, if exist, provide relevant information so much. Therefore, we propose ranking methods using syntactic patterns and the characteristics of the definition itself, which can be applied to the question targets for which there is no lexical pattern matching and external definition entry.
 answering, many systems participating in the past TREC QA Track definitional question answering only used lexical and POS information. The main reason is that further syntactic or semantic analysis using NLP techniques requires a lot of resources as well as usually shows the disappointing result in terms of accuracy or precision. However, even naive syntactic or semantic analysis can significantly help some process. For instance, phrase detection using a syntactic parser could be very useful to reduce the length of the answer by just taking exact noun phrase describing the definition of the target, and semantic matching using the thesaurus such as WordNet could be helpful for eliminating redundant information between phrases or sentences.
 candidate was adopted by a few researchers[4][5][6]. [5] extracted linguistic con-structs called linguistic features using information extraction tools. The linguis-tic features, including relations and propositions, are more fine-grained than our phrases. [6] also uses the predicate set defined by semantic categories such as genus, species, cause, and effect. However, we are to show that using a lit-tle linguistic information could increase system performance although so many resources and tools are not applied.
 method mainly characterized as follows:  X  Anaphora resolution technique is used to expand the retrieved passages  X  Syntactic patterns are used to find the boundary of short and exact an- X  WordNet is used to check the redundancy of the answer candidates.  X  Term statistics peculiar to the general definitions, not the target-specific These contributions are explained in detail in section 2, and experimental results and analyses are given in section 3. We conclude our work in section 4. 2.1 Question Analysis In the question analysis phase, the question sentence is parsed to extract the head word of the target. Then, the type of the target is identified using named entity tagger. We classified the target into one of the three types: persons, orga-nizations, and other things. We use the target type for calculating the weights of the words in definitional phrases in later stage. 2.2 Passage Retrieval Two-Phase Retrieval. As the target tends to be used with different expression in documents from that in the question, a lot of relevant information could not be retrieved or lots of irrelevant information would be retrieved by one phase passage retrieval. For example, for a target Bill Clinton , it would be expressed in a text by Clinton , president Clinton , he , etc. A strict phrase query bill clinton would suffer from low recall because of differently-represented phrases in documents, while a relaxed query clinton would be overloaded by a plenty of irrelevant information such as George Clinton .
 atively strict query, and then extract relevant sentences by using more relaxed one. The query for the document retrieval consists of the words and phrases of the target filtered with a stop word list. If there is a sequence of two words starting with a capital letter, a phrase query is generated with the two words. The remaining words are also used as sing le query words. For example, for a tar-get Berkman Center for Internet and Society , the query would include a phrase berkman center and two words internet and society .
 one or more sentences containing head word of the target, initially set to one sentence. Then, we check whether the passage can be expanded to the multiple-sentence passage using anaphora resolution technique described in the next sub-section.
 Passage Expansion Using Target-Focused Anaphora Resolution. In many cases, the definition of the given target does not occur with the target word or phrase, but with its anaphora. For example, it is easy to observe that an anaphora refers to the target if the anaphora is used as a subject and the target is also used as a subject in the previous sentence like: (a) Former president Bill Clinton was born in Hope, Arkansas. (b) He was named William Jefferson Blythe IV after his father, William Jeffer-As shown in the above example, if the sentence (b) is next to the sentence (a) in a document, the anaphora he in (b) refers to Bill Clinton in (a). anaphora are also extracted by using simple rules. When the head word is used as a subject in a sentence, the following anaphora resolution rules are applied to the next aw sentences according to the target type. The aw is the anaphora resolution window size.
 Persons. If a first word of the sentence is he or she , then it is replaced with the Organizations or things. If a first word of the sentence is it or they ,thenit can extract efficiently the informative sentences related to the question target by using the target-focused anaphora resolution. 2.3 Candidates Extraction Using Syntactic Patterns We suggest an answer candidate extraction method based on syntactic patterns, easily constructed manually and freer from the coverage problem than lexical patterns. Another advantage of syntactic patterns is that the answer candidates extracted using the syntactic patterns can reduce answer granularity. A shorter text unit than a sentence is necessary for generating a more fine-grained answer because the sentences in news articles are generally so long that the answer length limit is used up quickly by sentences.
 tic patterns shown in Table 1. In this study, we use the syntactic information generated by a dependency parser, Conexor FDG parser 1 [7].
 ing the syntactic patterns: (1) Former president (2) born in Hope, Arkansas (3) named William Jefferson Blythe IV after his father, William Jefferson Blythe Third candidate clearly shows the advantage of using syntactic patterns rather than lexical patterns. While the lexical patterns capable of extracting the verb phrases such as the candidate can be hardly constructed, our syntactic patterns are useful for extracting such verb phrases, specifically distant one from the target, although the patterns sometimes might extract unimportant phrases. erates erroneous results or there are sen tences from which the information is not obtained. In order to alleviate the problem, we complement the syntactic information with POS information.  X  If any word between the first word and the last of a phrase in the sentence  X  If the last word of extracted phrase is labeled with noun-dependent POSs  X  If the extracted phrase is incomplete one, that is, ended with one of the POSs noun or number is considered to be valid. 2.4 Redundancy Elimination Using WordNet In order to eliminate the redundancy among answer candidates, at least two decisions have to be made: which candidates are redundant one another, and which candidates has to be eliminated from them. The word overlap ratio of content words is calculated in each candidate. If any word overlap ratio between two candidates is greater than high threshold T high , the two candidates are determined redundant. The candidates of which no word overlap ratio amount to low threshold T low are regarded not redundant. If any word overlap is between the two threshold value, T high and T low , the redundancy decision is not made based only the overlap. Instead, the semantic class of the head word is checked in WordNet, and two candidates are considered redundant if the two head words from each candidate belong to same synset in WordNet. Once the redundancy is detected, the highly overlapped candidate is eliminated.
 the redundant information are likely to be important, which is also used as an effective ranking measure in factoid question answering system[8]. Therefore, the redundant count of the eliminated candidates is inherited by the survived one, used in the candidate ranking phase. 2.5 Answer Selection The decision which candidates are definition or not is so difficult that ranking the candidates according to the definition likelihood can be an alternative. Our system used several evidences to rank answer candidates: head redundancy, term statistics in the relevant passages, exte rnal definitions, and definition terminol-ogy.
 Head Redundancy. The important facts or events are usually mentioned re-peatedly, and the head word is the core of each answer candidate. Mentioned in the previous section, a pair of candidates is redundant when one of them is heav-ily overlapped by lexical or the semantic classes of each head word are identical. Therefore, we consider the redundancy of answer candidate C by using following formula.
 where r represents the redundant count of answer candidate C in the candidate set, and n is the total number of answer candidates. For most terms, the fraction r/n is so smaller than 1 that the Rdd ( C )ishardlyover1.
 Local Term Statistics. In addition to the head word, the frequent words in the retrieved passages are important. The Loc ( C ) presents a local score based on the term statistics in the retrieved sentences, local sentences, and is calculated as follows: where sf i is the number of sentences in which the term t i is occurred, maxsf is the maximum value of sf among all terms, and | C | is the number of all content words in the answer candidate C .
 External Definitions. We tried to use external definitions from various online resources by designing a scoring formula based on the probability model. If we introduce binary random variable A which is true when the candidate C is a real answer, the probability that answer candidate C is an answer to the question target is as follows: As the prior probability P ( A = 1) is independent of ranking the candidates, the score of C by external definition is defined by: where P ( C ) is a priori probability of the candidate.
 the probability P ( C | A = 1) using following external definition model: where freq E i is the number of occurrence of term t i in the external definitions E in which there are the total | E | term occurrences. freq B i and | B | are those of in the background collection. | C | is the number of content word in the answer candidate C , used for normalizing the probabilities.
 Definition Terminology. Although external definitions are useful for ranking candidates, it is obvious that they cannot cover all the possible targets. For the targets of which definition do not exist in external resources, we device another score called definition terminology score , reflecting how the candidate phrase is definition-like. The definition terminologies depend on the type of question target. For example, born or died is widely used to describe essential informa-tion about persons, while found or locate is used to describe information about organizations.
 ence is that we identify the target type and build definition terminology accord-ing to the type. In order to get the definition terminology, we collected external definitions according to the three target types: persons, organizations, and other things. These external definitions must be a good training data for learning about the definition generation. We compare the term statistics in the definitions to those in the general text, believing the difference of the term statistics to be a measure for the definition terminology.
 the logarithm for combining with other scores described in the above sections appropriately.
 where P D ( t ) is the probability of term t in the definitions, and P ( t )isthatof in the general text. The probability P ( t ) is estimated as the equation 4, except the length normalization factor. The terms having a high value of Pratio ( t )is considered to be important for generating definitions.
 follows: The score Tmn ( C ) measures the average probability ratio of content terms in the answer candidate C .
 3.1 Experiments Setup We experimented with 50 TREC 2003 topics and the AQUAINT corpus used in TREC Question Answering Track evalu ation. As the manual evaluation such as TREC evaluation requires a lot of cost, we evaluated our system automati-cally. The evaluation of definition answer is very similar to that of summaries, so we used a package for automatic evaluation of summaries called ROUGE[10]. ROUGE has been used for automatic evaluation of summary in Document Un-derstanding Conference (DUC), and was successfully applied for evaluation of definitional question answering[5]. We used ROUGE-L among several measures because it is known to be highly correlated with human judgement.
 where LCS ( A, S ) is the length of the longest common subsequence of the refer-ence answer A and the system result S ,and | A | and | S | are the length of them respectively. The LCS-based F-measure F lcs is called ROUGE-L, and  X  controls the relative importance of recall and precision. We evaluated the systems with stop words excluded.
 Columbia Encyclopedia, Wikipedia, FOLDOC, The American Heritage Dic-tionary of the English Language, Online Medical Dictionary, and Web pages returned by Google search engine. The Google search results are used for sup-plying wider coverage of the external definitions. The external definitions are collected in query time by throwing a query consisting of the head words of the question target into the site. If the Google search results are not considered, 14% (7 out of 50) topics are not covered by the external definitions.
 cording to the target type. We collected 1,174 person entries from the above sites using the person name list gathered from Identifinder resources and Columbia Encyclopedia, 545 organizations, and 696 things entries. The AQUAINT collec-tion is used for general text.
 processed top 200 documents retrieved in all experiments. 3.2 Target-Focused Anaphora Resolution Table 2 shows the effect of target-focused anaphora resolution, when the system outputs the definition up to 500 non-white-space characters. In this experiment, we did not extract the phrases, but rank the retrieved sentences. The system 500B.Exp0 , 500B.Exp1 ,and 500B.Exp2 are the systems in which the anaphora resolution size aw is 0, 1, and 2, respectively. The +S column shows the number of sentences added to the answer candidates using the anaphora resolution. show that the added sentences are useful one for definition generation. For only 20 questions, the anaphora resolution condition is satisfied and the anaphora is resolved. The resolution added on the average 8.95 sentences per the applied question. Although the resolution is not frequently applied and few sentences are added, the performance increased a b it. It says that the added sentences are related to the question target, and the target-focused anaphora resolution is useful for definitional question answering. When we resolve the anaphora, distant more than one sentence from the target, the expansion did not affect the performance, though few sentences are added. It is necessary to increase the application ratio of the resolution method. 3.3 Candidate and Answer Text Unit Table 3 displays the system performance according to the answer length mea-sured by the number of non-white-space characters, and Figure 1 shows the per-formance change according to the an swer length. The system name suffix S.S , S.P , P.S ,and P.P denote the text unit for answer candidate and final answer. S and P mean the sentences and phrases, respectively. The 500B.S.P represents the system where sentences are used as the candidates and phrases are used as the final answer limited up to 500 non-white-space characters.
 based system S.S . As the answer length limit gets longer, the performance differ-ence becomes smaller, because, as shown in Table 3, the recall of S.S increases much more than that of P.P . As the sentences contain the contents which are not closely related to the question target, the precision of S.S also decreases significantly. Considering TREC reference answer is about 250 non-white-space characters long on the average, we had better use P.P rather than S.S .These results support our claim that the phrases shorter than sentences are useful for concise answer.
 better than the sentence extraction system from them S.S . The result also shows that the phrase extraction is superior to the sentence extraction specifically for brief answer.
 because the extracted phrases are not sufficient. If the set of retrieved sentences are very small, the phrase-based system could not do well, because of the in-sufficient number of the answer candidates. In order to alleviate this problem, it is an alternative to use the phrases and sentences together. The retrieved sentences from which no phrase is extracted could be used with the extracted phrases together as answer candidates. Ac tually, experimental results show that the combination P+.P+ outperforms the single phras e-or sentence-based system, asshownintheTable4.
 3.4 Redundancy Elimination The effect of the redundancy elimination is shown in Table 5 where NORE , LRE , and SRE denote no redundancy elimination, lexical overlap-based method, and semantic-based method, respectively. The length limit is set to 500 non-white-space characters, and the threshold values T high and T low are set to 0.7 and 0.5, respectively.
 mance very slightly. As reported in [5], the redundancy elimination seems not to have great effect on the performance.
 3.5 Answer Ranking Measures Table 6 shows the system performance according to the ranking score combi-nations. The length limit is set to 500 non-white-space characters, and T high and T low is set to 0.8 and 0.5, respectively. The system ALL uses all the equally-combined scores, and -Rdd , -Loc , -Ext ,and -Tmn is the system which each score is excluded from the ALL score, respectively. The system Rdd , Loc , Ext ,and Tmn is one used the own score only, respectively.
 itions are useful as a single scoring measure, and the external definitions and definition terminology have a great influence on the combined score. Although the definition terminology is not so good measure used alone, it shows true value when used with other ranking measures. The significant performance degradation without the definition terminology shows it has useful information for ranking candidates, which cannot be replaced with other measures. From this point of view, the external definitions are the best measures of them, but the incomplete coverage has to be complemented. The combination of the external definitions and the definitional terminology Ext+Tmn shows that the definition terminology is adequate for the role. 3.6 TREC 2004 Evaluation Results We participated in TREC 2004 Question Answering Track with a preliminary system[11]. Our TREC 2004 system used the definition terminology only for per-sons based on an encyclopedia, not using the external definition score for answer ranking. The evaluation results of our system was 0.246 based on the F (  X  =3) measure, compared to 0.184 of the median performance of all participation sys-tems. Ours was among top 10 systems. We proposed a definitional question answering system using linguistic informa-tion, and tried various ranking measures, specifically definition terminology. Our interesting findings in this study can be summarized as follows:  X  Target-focused anaphora resolution can be applied to expand the retrieved  X  Phrases are likely to be a good processing unit rather than sentences for  X  Redundancy elimination seems not to have great effect on the performance,  X  External definitions and definition terminology are efficient and harmonic so the conditions in which the method can be applied are very strict. Our fu-ture work is that the constraint will be gradually relaxed without performance degradation.

