 Statistical machine translation (SMT ) [ 1 ] is one of the most popular machine transl a-tion frameworks where translations are generated on the basis of statistical models whose parameters are derived from the analysis of bilingual parallel corpora. The statistical machine translation technologies have been extended from word -based model to p hrase -based model, and nowadays, as the advent of strong stochastic parsers , synta x -based models are also built [ 2 ]. Due to lack of syntax -annotated bilingual pa r-phrase -based tran slation [ 3 ] combines the strengths of phrase -based and syntax -based grammars as rules. Phrase -based and hierarchical -based models are often used in recent days. The trans lation model training, parameters tuning and decoding are all based on the output of word alignment model. To some extent, the performance of word alig n-ment model affects the quality of statistical machine translation.
 Research on machine translation betwe en m inority language s and Chinese like low -resource and word forming distinction s between Uyghur and Chinese [ 4 ], we cannot get a desired translation performance with traditional word alignment models. 
In this paper, we propose a co -occurrence degree based Uyghur -Chinese word alignment method to alleviate the data sparseness problem during models training, which combines co -occurrence counts and fuzzy co -occurrence weights to trai n word alignment models. Experiments show that our method outperforms traditional word alignment models both in Uyghur -Chinese word alignment and Uyghur -Chinese m a-chine translation. The original work of bilingual word alignment IBM models 1 -5 were proposed by Brown et.al, which described a series of five statistical models of the translation pr o-cess and gave algorithms for estimating the parameters (Expectation -Maximum alg o-rithm (EM) [ 5 ]) of these models given a set of pairs of sentences th at were translations of each other. In 1996, Vogel presented the HMM -based word alignment model [6] , which made the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions. Liu et.al proposed a log -linear model for word alignment [ 7 ], which treats all knowledge sources as feature functions; Liang X  X  work focused on word alignment agreement [ 8 ]. a clue -based word alignment method, which added several features like string simila r-ities between source language words and target language words into the training of word alignment models, performance of word alignment model and quality of machine translation were both imp roved. The method proposed by Tiedemann et.al only pe r-formed well on cognate languages, but for language pairs like Uyghur -Chinese, the improvement of word alignment performance is not significantly. in a situation that source language and target language are not cognate languages ( Uyghur and Chinese) and the data sparseness problem is relatively serious. In our method, we replace the traditional word co -occurrence counts wi th word co -occurrence degree in word alignment model; the word co -occurrence degree consists of co -occurrence counts and fuzzy co -occurrence weights. Chinese is one part of Sino -Tibetan language family, and Uyghur belongs to A ltaic language family. Because of differences among language families, Chinese and U y-ghur have some significant distinctions in word forming and syntactic structure. In this part, we first introduce features of Uyghur language, and then we compare with Ch i-nese and describe problems which exist in Uyghur -Chinese word alignment. 3.1 Feature s of Uyghur Language Uyghur is an agglutinative language [1 0 ], which is a type of synthetic language with morphology that primarily uses agglutination: words are formed by join ing phonet i-cally unchangeable suffix morphemes to the stem. In agglutinative languages, each suffix is a bound morpheme for one unit of meaning, instead of morphological mod i-fications with internal changes of the root of the word, or changes in stress or t one. The syntax structure of Uyghur is S (Subject) -O (Object) -V (Verb), which is significantly different with Chinese (S -V -O). We give some examples about the Uyghur words forming and syntax structure as follows: 3.2 Data Sparseness in Uyghur -Chinese Word Alignment . Due to rare of Uyghur -Chinese parallel corpora , there exist data sparseness problem s during training of Uyghur -Chinese word alignment models.
 We train word alignment models based on bilingual parallel corpora. Compared with English -Chinese and English -French, Uyghur -Chinese parallel corpora are relatively rare. Additionally, due to the word forming of Uyghur , a stem in Uyg hur may derive several Uyghur words; we cannot expect a Uyghur -Chinese dictionary or U y-ghur -Chinese bilingual corpora can collect every word that a certain stem can forming. Therefore, data sparseness will occur during training of Uyghur -Chinese word alig n-ment model, which affects the performance of word alignment model, even the quality of Uyghur -Chinese machine translation.

In this paper, we try to alleviate the data sparseness problem in Uyghur -Chinese word alignment models training. Most word alignment models like IBM Model 1 -5, HMM are trained based on word co -occurrence information which is obtained by counting word co -occurrence in pa r-allel texts. When a certain Uyghur -Chine se word pair appeared in Uyghur -Chinese parallel corpora, the co -occurren ce count of this word pair increased . Due to shortage of Uyghur -Chinese parallel texts, data sparseness may occur during word alignment models training. For fully use of Uyghur -Chines e parallel texts, we propose a co -occurrence degree based word alignment method to replace traditional word co -occurrence counts based methods. 4.1 Word Co -occurrence Degree Definition of Word Co -occurrence Degree We obtain Uyghur -Chinese word co -occurrence degree by combine word co -occurrence counts and fuzzy co -occurrence weights. The word co -occurrence counts can be gotten as a common way -number of times a certain Uyghur -Chinese word appeared in corpora; we compute fuzzy co -occu rrence weights as summing up length of Uyghur words that have the s ame stem, meanwhile there exist same Chinese word ( s ) in Chinese sentences.
 Computation of Co -occurrence Degree in Uyghur -Chinese Word Alignment The co -occurrence degree can be computed as:
In (1), is the count of word co -occurrence, and is the fuzzy co -occurrence weight . Measure the Word Co -occurrence Counts .

As the traditional way, we simply get the word co -occurrence counts by counting number of times a certain Uyghur -Chinese word pair appeared in Uyghur -Chinese parallel corpora : Measure the Fuzzy Word Co -occurrence Weights . 
Uyghur words are formed by joining phonetically unchangeable suffix morphemes to a certain stem. We compute the fuzzy word co -occurrence weights of a U y-ghur -Chinese word pair based on bilingual corpora. In this paper, we suppose that if a word in current Uyghur sentence has the same stem with word in another Uyghur sentence, meanwhile, there are same Chinese words in Chinese sentences which aligned to above two Uyghur sentences; we c onsider these two Uyghur -Chinese word pairs are reference word aligned. These kinds of alignments are measured by fuzzy co -occurrence weights, which can be obtained as following two parts. 1) Searching for Fuzzy Aligned Word Pairs Suppose we have three align ed Uyghur -Chinese sentence pairs: ( SENT -UYG1 , SENT -CHN1 ) and ( SENT -UYG2 , SENT -CHN2 ), words of these sentences distribute as follows: SENT -UYG1: SENT -CHN1 : SENT -UYG 2 SENT -CHN2 : SENT -UYG 3 : SENT -CHN3 : sentence). k, l , h , n, p, q are length of the 1st Uyghur sentence, length of 1st Chinese sentence, length of 2nd Uyghur sentence, length of 2nd Chinese sentence, length of 3rd Uyghur sentence and length of 3rd Chinese sentence, respectively. If a Uyghur word ( ) in SENT -UYG1 have the same stem with a Uyghur word ( ) in SENT -UYG2 and a Uyghur word ( there exist a same Chinese word ( ) in SENT -CHN1, SENT -CHN2 and SENT -CHN3, the word pair &lt; &gt; can be considered as fuzzy aligned in sentence pair SENT -UYG 1 and SENT -CHN1. 2) Computation of Fuzzy Co -occurrence Weights According to method described in 1 ) , with the help of Uyghur -Chinese dictionary, we fuzzy co -occurrence weights of current word pair as combine differences of length between current Uyghur word and other Uyghur words in fuzzy ali gned word pairs which obtain from 1 ) : method de scribed in 1 ). k is the number of fuzzy co -occurrence word pair s , is the length of Uyghur word in i th word pair, is the length of current Uyghur word. 4.2 Combine the Word Co -occurrence Degree into Word Alignment Models IBM models are traditionally trained based on word co -occurrence counts; IBM model 1 is the first and important model to collect lexical information for following models , which can be indicat ed as follows : sentence ( Chinese ) , respectively ; a is the word alignment function, a: j -&gt;i means source word is align with target word ; t(e|f) is the translation probability of source word f and target word e . When training the IBM model 1, t( e|f) can be co m-puted based on word co -occurrence counts: word e in bilingual parallel corpora, is the number of times source word f appeared in corpora. In this paper, we replace t(e|f) (which based on U y-ghur -Chinese word co -occurrence counts) with t  X  (e|f) (which based on U y-ghur -Chinese word co -occurrence degree), and t  X  (e|f) can be computed as follows: 5.1 Set up We use GIZA++ 1 which implement s IBM models and HMM as the baseline in word alignment experiments and evaluates word alignment results by (Recall), (Pr e-cision) and AER (Alignment Error Rate). A indicates a set of word alignment results computation of AER requires gold al ignments annotated as  X  sure  X  or  X  possible  X  , in this paper, we don  X  t distinguish them. Therefore, we can compute AER as: We extract 200 sentence pairs from CWMT 2013 Uyghur -Chinese corpora (which collected from news reports and government documents) as the word alignment val i-date set, and annotated alignment association s by hands. For Uyghur -Chinese machine translation experiments, we use the CWMT 2013 corpora as training set and tune set , and select 1500 sentence pairs as the test set, Table 1.

We use Moses 2 [1 1 ] as machine translation system and SRILM 3 [ 12 ] as language modeling tool. The results of Uyghur -Chinese machine translation are evaluated by BLEU [ 13 ]. 5.2 Experiments Uyghur -Chinese Word Alignment Experiments We use the GIZA++ as the baseline in word alignment experiments. Uyghur -Chinese sentence pairs in word alignment validate set were preprocessed by methods described in 5.3.1. Then, we search for fuzzy aligned word p airs in bilingual corpus, and obtain co -occurrence degree as introduce in 4.1. The co -occurrence counts are replaced with co -occurrence degree in GIZA++. We take the co -occurrence degree as input in training of IBM Model 1. The performance of Uyghur -Chinese word alignment is evaluated by Recall, Precision and AER, respectively. Uyghur -Chinese Machine Translation Experiments In Uyghur -Chinese machine translation experiments, we take the results by co -occurrence counts based word alignment mode l and co -occurrence degree based word alignment model as inputs of model training of Moses, respectively. Parameters maximum length of phrases (rules) is 11. We evaluate the quality of Uyghur -Chinese machine translation by the script multi -bleu.perl which included in Moses. 5.3 Analysis of Results Table 2 and Table 3 are the experiment results of Uyghur -Chinese word alignment and Uyghur -Chinese machine translation , respectively .
 recall (R) o f the stem based method (G Stemmer) achieved highest improv e ment (1.37%), which may because the Uyghur words stemming reduce the data sparseness, to some extent; but its improvement of precision ( P) (1.93% ) cannot ou t perform co -occurre nce degree ba sed method (G Co -degree) (2.39%), one possible reason is that the stem based method missing some importan t information of Uyghur words. The AER of co -occurre nce degree based method (G Co -degree) achieved lowest among three methods (14.94%). Notice that the decrease of AER betwee n the stem based method (G Stemmer) and co -occurrence degree based metho d (G Co -degree) is not very significantly (( -1.68) -( -1.81) = 0.13), which means two methods both e n hancing ass o-ciations between source words ( Uyghur words) and target words (Chinese words) that related with each other. under different word alignment methods in test set and tune set. The stem base d method (G Stemmer) and the co -occurre nce degree based method (G Co -degree) are both ou t-perform the baseline (G Baseline) in U y ghur -C hinese machine translation. And the performance of co -occurre nce degree based method (G Co -degree) achieved higher than st em based method (G Stemmer), the most important reason is that the stem based method missing some information in Uyghur , and quality of stem based machine translation also rely on the performance of stemmers. Because of local reo r dering and generalization abilities , h ierarchical phrase -based models ou t perform phrase based models. Although there are some different ideas about re latio n ship b e tween AER and BLEU in statistical machine translation, we validate some researchers  X  opinions that the BLEU of Uyghur -Chinese machine translation is related with the precision of Uyghur -Chinese word alignment: with the increase of precision of word alignment by our method, the improvement of Uyghur -Chinese machine tran s lation performance increases correspondingly. This may because the precision of word alignment partly decide the alignment of translated words in Uyghur -Chinese bilingual corpora . In this paper, we propose a word co -occurrence degree based method for U y-ghur -Chinese word alignment in SMT, which is different from traditional co -occurrence counts based word alignment methods. We obtain the word co -occurrenc e degree by combine word co -occurrence counts and fuzzy co -occurrence weights. Experimental results show that with the method we present in this article, data sparseness in Uyghur -Chinese word alignment is alleviated effectively ; comparing with stem based word alignment method, o ur approach maintain the i ntegrity of U y-ghur words. Performance of co -occurrence degree based word alignment model is significantly outperforming the word co -occurrence counts based method and stem based word alignment method; quality of Uyghur -Chinese machine translation also improved by our method. For future work, we will further investigate relationships between Chinese word segmentation and Uy ghur -Chinese word alignment; we also plan to test our approach in other domains and on other language pairs.
 Acknowledgements. This work is supported by the Strategic Priority Research Program of the Chinese Academy of Sciences ( Grant No. XDA06030400 ) , We st Light Foundation of Chinese Academy of Sciences ( Grant No. LHXZ201301 XBBS201216 ), the Xinjiang High -Tech Industrialization Project ( Grant No. 201412101 ) and Young Creative Sci -Tech Talents Cultivation Project of Xinjiang Uyghur Autonomous Region ( Grant No. 2013731021 ).

