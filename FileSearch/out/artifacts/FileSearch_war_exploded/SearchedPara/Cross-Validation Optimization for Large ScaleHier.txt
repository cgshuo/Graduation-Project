 hyperparameters.
 a large number of hyperparameters and does not need user inte raction. w.r.t. loss functions different from zero-one.
 gistic regression model , consisting of C latent (unobserved) class functions u multiple logistic (or softmax) likelihood P ( y u fitting. k X k Our notation for n C vectors 2 (and matrices) uses the ordering y = ( y We set u = ( u indexes I are applied to i only: y Since the likelihood depends on the f expansion: f becomes (1 / 2)  X  T K X  + (1 / 2)  X   X  2 k b k 2 . K ( c ) = ( K ( c ) ( x is block-diagonal. We refer to this setup as flat classification model. The b b =  X  2 ( I  X  1 T )  X  . Thus, if  X  K = K +  X  2 ( I  X  1 )( I  X  1 T ) , then  X  becomes  X   X   X  . The corresponding kernel expansions are  X  u conditional probability on test points x where  X  log P ( y the primary fitting of  X  does not require constrained convex optimization. The complexity of our fitting algorithm is dominated by k with K , where k be chosen small. k are sufficient (see Section 6.2). modifications and minor extra cost.
 In flat classification, the latent class functions u is determined by P and n nodes, | L | = C . Assign a pair of latent functions u assumed a priori independent, as in flat classification. u u leafs L ( c ) , L ( c 0 ) have the common parent p , then u the class functions share the effect u smooth, the classes c, c 0 are urged to behave similarly a priori . Let u = ( u Algorithm 1, and MVM with  X  T works in a similar manner [8].
 Under the hierarchical model, the class functions u  X  is cheap, and  X  K is block-diagonal just as in the flat case. We note that the step from flat to hierarchical classification requires minor modifications of existing code only. If code for representing a block-diagonal K is available, we can use it to represent the inner  X  K , just replacing C by P . This simplicity carries through to the hyper-parameter learning case (see Section 4). The cost of a kernel MVM is increased by a factor P/C &lt; 2 , which in most hierarchies in prac-tice is close to 1 . However, it would be wrong to claim that hierarchical classification in gen-eral comes as cheap as flat classification.
 C (marginal) inference is typically used as subroutine for pa rameter learning. validation log likelihood  X  instead. Let { I and let  X  data. Here, u CV criterion is where  X  overfitting.
 w.r.t. h . To this end, we keep the set {  X  for each fold I us to solve a linear system with the Hessian matrix I + V T in Section 5. As for the complexity, suppose there are q folds. The update of the  X  q primary fitting applications, but since they are initialize d with the previous values  X  thus q k run very efficiently using specialized numerical linear alg ebra code. minimal. Given a method for computing  X  and  X  as Quasi-Newton in order to learn h . applications, but of interest to Machine Learning.
 Given (  X  , u ) ,  X  and  X  the line searches basically come for free! have  X  X  X  the equivalent symmetric system (details are in [8]). Note that P x = ( P K We now show how to compute the gradient  X   X  metrized as above: ( I + V T V we use Eq. 2:  X  E = (  X  T using a standard sparse matrix format for X . If A is stored row-major ( a hyperparameter learning, we work on subsets J sentation shuffling X  permutes the representation s.t.  X  K at r n C for a small constant r , and avoiding frequent reallocations. 5.1 Matrix-Vector Multiplication may share kernels: K ( c ) = v The linear kernel K ( c ) ( x , x 0 ) = v n .
 matrices instead of the K ( c ) [10], whence MVM is O ( C n d ) , d the rank. In Section 4 we also need MVM with the derivatives (  X / X  X  K with little additional cost) for computing derivative MVMs . 6.1 Flat Classification: Remote Sensing We use the isotropic Gaussian (RBF) kernel 6.2 Hierarchical Classification: Patent Text Classificatio n We use linear kernels (see Section 5.1) with variance parame ters v 5 p j  X  R accuracy, classes are ranked as r accuracy (acc) m  X  1 P m the shortest path between leafs L ( c ) , L ( c 0 ) . The taxo-loss (taxo) is m  X  1 P scores are motivated in [1]. For taxo-loss and parent accura cy, we better choose y ( x We compare methods F1, F2, H1, H2 (F: flat; H: hierarchical). F 1: all v across each level of the tree (3). F2, H2: v hyperparameter learning: k argmax c p j,c rule, rather than minimize expected loss.
 of  X  [ J ] , gradient accumulation for single fold.
 For final fitting: k from v many different v leads to significantly worse results in all scores, the v optimization ( 5 of them are required for  X  ,  X  available (see Section 1).
 as incorporating uncertain data.
 probabilities which are asymptotically correct.
 Acknowledgments of the European Community, under the PASCAL Network of Excel lence, IST-2002-506778.
