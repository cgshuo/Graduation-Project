 ing the maximum a posteriori (MAP) assignment in probabilis tic graphical models, relaxations play a key role in a variety of algorithms. For example, tree-rewe ighted belief propagation (TRW-BP) can be thought of as a linear programming relaxation of an intege r program for a given MAP problem [1, 2]. Branch-and-bound search algorithms for finding opti mal MAP solutions, such as [3, 4], rely on structural relaxations, such as mini-bucket approximat ions, to provide upper bounds [4, 5]. Whether a relaxation is used as an approximation on its own, or as a guide for finding optimal solutions, a trade-off is typically made between the qualit y of an approximation and the complexity without impacting its structural complexity.
 More specifically, we propose here an approach to approximat ing a given MAP problem by perform-a simpler model whose MAP solution provides an upper bound on that of the original. Second, we compensate for the relaxation by introducing auxiliary parameters, wh ich we use to restore certain properties, leading to a tighter approximation. We shall in fact propose two distinct properties on which a compensation can be based. The first is based on a simpl ified case where a compensation can be guaranteed to yield exact results. The second is based on a notion of an ideal compensation, new semantics for the max-product belief propagation algor ithm. The second approach leads to another approximation that further yields upper bounds on t he MAP solution. We further propose monotonically decreasing upper bounds on the MAP solution ( at least empirically).
 Proofs of results are given in the auxiliary Appendix. where a is an index to the factor  X  a ( X a ) over the domain X a  X  X . We seek the maximum a posteriori (MAP) assignment x  X  = argmax assignment x  X  by: which we refer to more simply as the MAP value. Note that there may be multiple MAP assignments x , so we may refer to just the value map  X  when the particular assignment is not relevant. Next, if z is an assignment over variables Z  X  X , then let x  X  z denote that x and z are compatible assignments, i.e., they set their common variables to the sa me states. Consider then the MAP value under a partial assignment z : We will, in particular, be interested in the MAP value map ( X = x ) where we assume a single vari-able X is set to a particular state x . We shall also refer to these MAP values more generally as map ( . ) , without reference to any particular assignment. The structural relaxations that we consider here are based o n the relaxation of equivalence con-variables X i and X j to the same state, and invalid otherwise. Note that when we remove an equiva-log 1 = 0 . However, the values map ( x ) for invalid configurations can increase, since they are  X  X  X  prior to the removal. In fact, they could overtake the optima l value map  X  . Thus, the MAP value after relaxing an equivalence constraint in M is an upper bound on the original MAP value. It is straightforward to augment a model M to another where equivalence constraints can be relaxed. Consider, for example, a factor  X  1 ( A,B,C ) . We can replace the variable C in this factor with a equivalence constraint C  X  C  X  , we have a new model M  X  which is equivalent to the original model M , in that an assignment x in M corresponds to an assignment x  X  in M  X  , where assignment x  X  sets a variable and its clone to the same state. Moreover, the valu e map ( x ) in model M is the same as the value map  X  ( x  X  ) in model M  X  .
 We note that a number of structural relaxations can be reduce d to the removal of equivalence con-[5, 4]. In fact, the example above can be considered a relaxat ion where we delete a factor graph edge C  X   X  1 , substituting clone C  X  in place of variable C . Note that mini-bucket approximations in particular have enabled algorithms for solving MAP probl ems via branch-and-bound search [3, 4]. Suppose that we have a model M with MAP values map ( . ) . Say that we remove the equivalence a compensated model M  X  with MAP values c -map ( . ) that is as tractable to compute as the values r -map ( . ) , but yielding tighter approximations of the original value s map ( . ) . equivalence constraint X i  X  X j that we remove. Equivalently, we can introduce the log facto rs will be unambiguous from the context). These new factors add new parameters into the approxima-the relaxation r -map ( . ) and the compensation c -map ( . ) , where: Note that the auxiliary factors  X  of the compensation do not introduce additional complexity to the Consider then the case where an optimal assignment x  X  for the relaxation happens to set variables X i and X j to the same state x , for each equivalence constraint X i  X  X j that we relaxed. In this r -states, then it is not a valid assignment for the original mod el M , as it violates the equivalence constraint and thus has log probability  X  X  X  .
 the MAP value when we set X j to the same state. We can then ask of a compensation, for all st ates to the same state at the same time.
 We propose two approaches: (1) based on a condition for exact ness in a special case, and (2) based the simplified case where a single equivalence constraint is relaxed. 4.1 Intuitions: Splitting a Model into Two independent sub-models, M i and M j , where sub-model M i contains variable X i and sub-model M j contains variable X j . Intuitively, we would like the parameters added in one sub-model to summarize the relevant information about the other sub-mod el. In this way, each sub-model could independently identify their optimal sub-assignments. Fo r example, we can use the parameters: Since sub-models M i and M j become independent after relaxing the single equivalence c onstraint X i  X  X j , computing these parameters is sufficient to reconstruct th e MAP solution for the original that map  X  = max x [  X  ( X i = x ) +  X  ( X j = x )] .
 the following condition: simpler semantics. The following proposition confirms that this choice of parameters does indeed idealized case when a model is split into two.
 into two independent sub-models. Then the compensation has parameters satisfying Equation 1 iff c -map ( X i = x ) = c -map ( X j = x ) = map ( X i = x,X j = x ) +  X  . the case where relaxing an equivalent constraint splits a mo del into two.
 compensation may lead to more meaningful, and hopefully mor e accurate, approximations than a re-approximations. Thus, we call a compensation satisfying Eq uation 1 a REC -BP approximation. 4.2 Intuitions: An Ideal Compensation of an  X  X deal X  compensation where, as far as computing the MAP solution is concerned, a compen-sated model is as good as a model where the equivalence constr aint was not relaxed. Consider then the following proposal of an ideal compensation, which has t he following two properties. First, it has valid configurations : for all states x . Second it has scaled values for valid configurations: solution sets variables X i and X j to the same state, and is thus a valid assignment for the orig-compensation further allows us to recover the MAP value as we ll. A compensation having valid It may not always be possible to find parameters that lead to an ideal compensation. However, we propose that a compensation X  X  parameters should satisfy: an ideal one, then it must at least satisfy Equation 2.
 values of a compensation that results from relaxing an equiv alence constraint X i  X  X j in M . If c -map ( . ) has valid configurations and scaled values, then c -map ( . ) satisfies Equation 2. We thus call a compensation satisfying Equation 2 a REC -I compensation.
 We note that other values of  X  &gt; 1 can be used, but the choice  X  = 2 given above re-c -map ( X i = x,X j = x ) (i.e., the parameters alone can recover an original MAP valu e). Before we discuss the general case where we relax multiple eq uivalence constraints, we highlight the relaxation alone. In particular: 4.3 General Properties where multiple equivalence constraints are relaxed, and fu rther highlight some of their properties. Suppose that k equivalence constraints X i  X  X j are relaxed from a given model M . Then compen-sations REC -BP and REC -I seek to recover into the relaxation two weaker notions of equ ivalence. First, a REC -BP compensation has auxiliary parameters satisfying: The following theorem relates REC -BP to max-product belief propagation.
 Theorem 1 Let map ( . ) denote the MAP values of a model M , and let c -map ( . ) denote the MAP values of a compensation that results from relaxing enough e quivalence constraints X i  X  X j in M to render it fully disconnected. Then a compensation whose p arameters satisfy Equation 3 has belief propagation run on M , and vice-versa.
 Loopy max-product belief propagation is thus the degenerat e case of a REC -BP compensation, when the approximation is fully disconnected (by deleting every factor graph edge, as defined in Sec-tion 3). Approximations need not be this extreme, and more st ructured approximations correspond to instances in the more general class of iterative joingrap h propagation approximations [8, 6]. Next, a REC -I compensation has parameters satisfying: We again approximate the exact MAP value map  X  with the value 1 1+ k c -map  X  .
 an optimal assignment for the original model M : we need only check that it is a valid assignment. This result is analogous to results for max-product BP, TRW-BP, and related algorithms [9, 2, 10]. A
REC -I compensation has additional properties over a REC -BP compensation. First, a REC -I com-pensation yields upper bounds on the MAP value, whereas REC -BP does not yield a bound in general. Theorem 3 Let map ( . ) denote the MAP values of a model M , and let c -map ( . ) denote the MAP pensation has parameters satisfying Equation 4, then map  X   X  1 1+ k c -map  X  .
 We remark now that a relaxation alone has analogous properti es. If an assignment x  X  is optimal it does not violate the equivalence constraints X i  X  X j ), then x  X  is also optimal for M , where r -a relaxation is not valid for model M , then the MAP value of the relaxation is an upper bound on the original MAP value. On the other hand, REC -I compensations are tighter approximations than the corresponding relaxation, at least in the case when a sin gle equivalence constraint is relaxed: The following theorem has implications for MAP solvers that rely on relaxations for upper bounds. MAP solution. They rely on upper bounds of a MAP solution, und er partial assignments, in order to prune the search space. Thus, any method capable of providin g upper bounds tighter than those of a relaxation, can potentially have an impact in the performan ce of a branch-and-bound MAP solver. Algorithm 1 RelaxEq-and-Compensate ( REC ) input: a model M with k equivalence constraints X i  X  X j output: a compensation M  X  main: 1: M  X  0  X  result of relaxing all X i  X  X j in M 2: add to M  X  0 the factors  X  ( X i ) , X  ( X j ) , for each X i  X  X j 3: initialize all parameters  X  0 ( X i = x ) , X  0 ( X j = x ) , e.g., to 1 2 r -map  X  4: t  X  0 5: while parameters have not converged do 6: t  X  t + 1 7: for each equivalence constraint X i  X  X j do 8: update parameters  X  ( X i = x ) t , X  ( X j = x ) t , computed using compensation M  X  t  X  1 , by: 9: for REC -BP : Equations 5 &amp; 6 10: for REC -I : Equations 7 &amp; 8 12: return M  X  t satisfy Equation 3 iff they satisfy: This suggests an iterative fixed-point procedure for finding the parameters of a compensation that using the compensation from the previous iteration: the next, then we can say that the iterations have converged, and that the compensation satisfies Equation 3. Similarly, for REC -I compensations, we use the update equations: to identify compensations that satisfy Equation 4.
 Algorithm 1 summarizes our proposal to compensate for a rela xation, using the iterative procedures for REC -BP and REC -I . We refer to this algorithm more generically as RelaxEq-and -Compensate (
REC ). Note that in Line 11, we further damp the updates by q , which is typical for such algorithms both algorithms tend to have compensations with decreasing MAP values. REC -BP may eventually have MAP values that oscillate however, and may not converge . On the other hand, by Theorem 3, we know that a REC -I compensation must yield an upper bound on the true MAP value map  X  . monotonically decreasing upper bounds on the true MAP value from iteration to iteration. We explore this point further in the following section. Figure 1: The REC algorithm in 10  X  10 grids. Left column: random grids, using REC -BP (top) and (bottom) with a relaxation with max cluster size 3. Our goal in this section is to highlight the degree to which di fferent types of compensations can We evaluated our compensations using randomly parametrize d 10  X  10 grid networks. We judge the quality of an approximation by the degree to which a compensa tion is able to improve a relaxation. In particular, we measured the error E = 1 1+ k c REC algorithm, for both types of compensations, with parameter s that led to an initial compensation exact solution.
 values drawn uniformly at random from 0 to 1 (we assigned no fa ctors to nodes). We assumed first the coarsest possible relaxation, one that results in a full y disconnected approximation, and where the MAP value is found by maximizing factors independently. 2 We expect a relaxation X  X  upper bound to be quite loose in this case.
 Consider first Figure 1 (left), where we generated ten random grid networks (we plotted only ten We see that, once we start iterating, that both methods of com pensation can tighten the approxima-convergence is slower, but the compensation is still a signi ficant improvement over the relaxation. Moreover, it is apparent that further iterations would bene fit the compensation further. We next generated random grid networks with frustrated inte ractions. In particular, each edge was not converge in networks with frustrated interactions [11] . Non-convergence is the primary failure mode for belief propagation, and in such cases, we may try to u se instead REC -I . We generated 10 random grid networks with p = 1 2 and another 10 networks with p = 1 3 . Although the frustration in these networks is relatively mild, REC -BP did not converge in any of these cases. On the other hand, REC -I compensations were relatively well behaved, and produced m onotonically decreasing upper bounds on the MAP value; see Figure 1 (center). Althoug h the degree of compensation is not as dramatic, we note that we are compensating for a very coars e relaxation (fully disconnected). in only one of 10 networks generated. Moreover, we can see in t hat one instance, REC -BP converges below the true MAP value; remember that by Theorem 3, REC -I compensations always yield upper example, we used the mini-buckets-based approach to relaxa tion proposed in [4], and identified re-There are two basic concepts underlying our proposed framew ork. The first is to relax a problem by dropping equivalence constraints. The second is that of com pensating for a relaxation in ways that models goes back to mini-bucket approximations [13], which can be considered to be a particular way of relaxing equivalence constraints from a model [4]. In this paper, we propose further a way to compensate for these relaxations, by restoring a weaker n otion of equivalence. One approach to compensation identified a generalized class of max-product belief propagation approximations. We then identified a second approach that led to another class of approximations that we have observed to yield tighter upper bounds on MAP solutions as compared to a relaxation alone.
 An orthogonal approach to upper-bounding MAP solutions is b ased on linear programming (LP) formulating MAP problems as integer programs, whose soluti ons are upper-bounded by tractable LP relaxations. A related approach based on Lagrangian relaxa tions is further capable of incorporating connection between belief propagation and LP relaxations [ 2, 10].
 In contrast to the above approaches, compensations further guarantee, in Theorem 4, upper bounds on MAP solutions under any partial assignment (without rerunning the algorithm). Thi s property assignments, to perform a branch-and-bound search for opti mal MAP solutions. 3 Further, as we approximate MAP by computing it exactly in a compensated mod el, we avoid the difficulties that al-gorithms such as max-product BP and related algorithms face , which infer MAP assignments using max-marginals (which may not have unique maximal states), w hich is based on local information tween belief propagation and an upper-bound approximation , namely that they arise from different notions of compensation. We hope that this perspective will enable the design of new approxima-tions, especially in domains where specific notions of compe nsation may suggest themselves. Acknowledgments This work has been partially supported by NSF grant #IIS-091 6161. [1] Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsk y. MAP estimation via agreement [2] Amir Globerson and Tommi Jaakkola. Fixing max-product: Convergent message passing al-[3] Radu Marinescu, Kalev Kask, and Rina Dechter. Systemati c vs. non-systematic algorithms for [4] Arthur Choi, Mark Chavira, and Adnan Darwiche. Node spli tting: A scheme for generating [5] Rina Dechter and Irina Rish. Mini-buckets: A general sch eme for bounded inference. J. ACM , [6] Arthur Choi and Adnan Darwiche. An edge deletion semanti cs for belief propagation and its [7] Arthur Choi and Adnan Darwiche. Approximating the parti tion function by deleting and then [8] Rina Dechter, Kalev Kask, and Robert Mateescu. Iterativ e join-graph propagation. In UAI , [9] Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsk y. Tree consistency and bounds on [10] Yair Weiss, Chen Yanover, and Talya Meltzer. MAP estima tion, linear programming and belief [11] Gal Elidan, Ian McGraw, and Daphne Koller. Residual bel ief propagation: Informed schedul-[12] David Sontag, Talya Meltzer, Amir Globerson, Tommi Jaa kkola, and Yair Weiss. Tightening [13] Rina Dechter. Mini-buckets: a general scheme for appro ximation in automated reasoning. [14] Jason K. Johnson, Dmitry M. Malioutov, and Alan S. Wills ky. Lagrangian relaxation for [15] Arthur Choi, Trevor Standley, and Adnan Darwiche. Appr oximating weighted Max-SAT prob-
