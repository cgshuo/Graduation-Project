 In recent years, there has been much rese arch work on data mining in seeking for better performance. Better performance includes mining from structured data, which is a new challenge. Since structure is represented by proper relations and a graph can easily represent relations, kno wledge discovery fro m graph-structured data poses a general problem for mining from structured data.

Chunkingless Graph-Based Induction (Cl-GBI) [4] is a machine learning tech-nique which was devised for the purpose of extracting typical patterns (sub-graphs) from graph-structured data. Cl-GBI is regarded as an improved version of Graph-Based Induction (GBI) [8] which extracts typical patterns from graph-structured data by recursively chunkin g two adjoining nodes. However, Cl-GBI does not employ this pairwise chunking strategy. Instead, the most frequent pairs are regarded as new nodes and given new node labels in the subsequent steps but none of them is chunked. In other words, they are used as pseudo nodes, thus allowing extraction of overlapping subgraphs. It was shown in [4] that Cl-GBI can extract more typical substructures than Beam-wise Graph-Based Induction (B-GBI) [4] which is an enhanced version of GBI adopting the beam search.
On the other hand, a majority of methods widely used for data mining are for data that do not have structure and that are represented by attribute-value pairs. Decision trees [5, 6], and induction rules [1] relate attribute values to target classes. Association rules often used in data mining also use this attribute-value pair representation. These methods can induce rules such that they are easy to understand. However, the attribute-value pair representation is not suitable to represent a more general data structure such as graph-structured data, which means that most of useful methods in data mining are not directly applicable to graph-structured data.

In this paper, we propose an algorithm to construct decision trees for graph structured data using Cl-GBI. This decision tree construction algorithm, called Decision Tree Chunkingless Graph-Based Induction (DT-ClGBI), is a revised version of our previous algorithm called Decision Tree Graph-Based Induction (DT-GBI) [2], and can construct decisi on trees for graph-st ructured datasets while simultaneously constructing substr uctures used as attributes for the clas-sification task by means of Cl-GBI instead of B-GBI adopted in DT-GBI. In this context, substructures mean subgraphs or patterns that appear in a given graph database. Patterns extracted by Cl-GBI are regarded as attributes of graphs and their existence/non-existenc e are used as attribute values. Namely, DT-ClGBI does not require the user to define available substructures in advance. Since attributes (features) are constructed while a classifier is being constructed, DT-ClGBI can be conceived as a method for feature construction. Using both synthetic and real-world graph-structured datasets, we experimentally show DT-ClGBI can construct decision trees from gr aph-structured data that achieve rea-sonably good predic tive accuracy.

This paper is organized as follows: Section 2 briefly describes the framework of Cl-GBI. Section 3 proposes DT-ClGBI and explains its working mechanism of how a decision tree is constructed and used for classification. The performance of DT-ClGBI is experimentally evaluated and reported in Section 4. Finally, Section 5 concludes the paper. 2.1 Graph-Based Induction (GBI) GBI [8] employs stepwise pair expansion (pairwise chunking) to extract typical patterns from graph-structured data. Later an enhanced version of GBI, named Beam-wise GBI (B-GBI) [3], adopting the beam search was proposed to increase the search space, thus extracting more discriminative patterns while keeping the computational complexity within a tolerant level. Since the search in GBI is greedy and no backtracking is made, which patterns are extracted by GBI depends on which pairs are selected for chunking. This means that patterns that partially overlap can no longer be extracted, and thus there can be many patterns which are not extracted by GBI. B-GBI can help alleviate this problem, but cannot solve it completely because the chunking process is still involved. 2.2 Chunkingless Graph-Based Induction (Cl-GBI) Cl-GBI [4] was developed to cope with the problem of overlapping subgraphs incurred by both GBI and B-GBI. Cl-GBI employs a  X  X hunkingless chunking X  strategy, where frequent pairs are never chunked but used as pseudo nodes in the subsequent steps, thus allowing extraction of overlapping subgraphs. As in B-GBI, the Cl-GBI approach can handle both directed and undirected graphs as well as both general and induced subgraphs. It can also extract typical patterns in either a single large graph or a graph database. The algorithm of Cl-GBI is briefly described as follows. For a detailed mathematical treatment of Cl-GBI the reader is referred to [4].

Given a graph database, two natural numbers b (beam width) and N e ,anda frequency threshold  X  , the  X  X hunkingless chunking X  strategy repeats the follow-ing three steps N e times, each of which is referred to as a level ( N e is thus the number of levels).
 Step 1. Extract all the pairs consisting of two connected nodes in the graphs, Step 2. Select the b most frequent pairs from among the pairs extracted at Step Step 3. Assign a new label to each pair sel ected at Step 2 but do not rewrite
All the pairs extracted at Step 1 in all the levels (i.e. level 1 to level N e ), including those that are not used as pseudo nodes, are ranked based on a typ-icality criterion using a discriminative function such as information gain [5] or gain ratio [6]. Those pairs that have frequency count below  X  are eliminated, which means that there a re three parameters b , N e ,  X  to control the search.
The output of Cl-GBI algorithm is a set of ranked typical patterns, each of which comes together with the positions of all its occurrences in each transaction of the graph database as well as the numbers of occurrences. 3.1 Decision Tree for Gr aph-Structured Data As mentioned in Section 1, the attribute-value pair representation is not suitable for graph-structured data, although both attributes and their values are essential for a classification or prediction task beca use a class is related to some attribute values in most cases. In a decision tr ee, each node and a branch connecting the node to its child node correspond to an attribute and one of its attribute values, respectively. Thus, to formulate the construction of a decision tree for a graph-structured dataset, we define attributes and their values as follows:  X  attribute: a pattern/subgraph in graph-structured data,  X  value of an attribute: existence/non-existence of the pattern in each graph.
Since the value of an attribute is either yes (the pattern corresponding to the attribute exists in the graph) or no (the pattern does not exist), the resulting decision tree is represented as a binary tree. Namely, data (graphs) are divided into two groups: one consists of graphs with the pattern, and the other consists of graphs without it. Figure 1 illustrates the decision tree constructed based on this approach. One remaining question is how to determine patterns which are used as attributes for graph-structured data. Our approach to this question is described in the next subsection. 3.2 Feature Construction by Cl-GBI The algorithm we propose here, called D ecision Tree Chunkingless Graph-Based Induction (DT-ClGBI), utilizes Cl-GBI to extract patterns from graph-structured data and use them as attributes for a classification task, whereas our previous algorithm, Decision Tree Graph-Based Induction (DT-GBI), adopted B-GBI to extract patterns. Namely, DT-ClGBI invokes Cl-GBI at each node of a decision tree, and selects the most discriminative pattern from those which were extracted by Cl-GBI. Then the data (graphs) are divided into two groups, i.e., one with the pattern and the other without the pattern as described above. For each group, the same process is recursively applied until the group contains graphs of a single class like the ordinary decision tree construction method such as C4.5 [6]. The algorithm of DT-ClGBI is summarized in Fig. 2.

In DT-ClGBI, each of the parameters of Cl-GBI, b , N e ,and  X  ,canbesetto different values at different nodes in a d ecision tree. All patterns extracted at a node are inherited to its descendant nodes to prevent a pattern that has already been extracted in the node from being extracted again in its descendants. This means that, as the construction of a decision tree progresses, the number of patterns to be considered at a node progressively increases, and the size of a pattern newly extracted can be larger than existing patterns. Thus, although initial patterns at the start of search consist of two nodes and the link between them, attributes useful for the classification task can be gradually grown up into larger patterns (subgraphs) by applying Cl-GBI recursively. In this sense, DT-ClGBI can be conceived as a method for featu re construction, since features, i.e., attributes (patterns) useful for the classification task, are constructed during the application of DT-ClGBI.

However, recursive partitioning of data until each subset in the partition con-tains data of a single class often results in overfitting to the training data and thus degrades the predictive accuracy of resulting decision trees. To avoid over-fitting, and improve predictive accuracy, DT-ClGBI incorporates  X  X essimistic pruning X  used in C4.5 [6] that prunes an overfitted tree based on the confidence interval for binomial distribution. This pruning is a postprocess that follows the algorithm in Fig. 2.
 Note that the criterion for selecting a pair that becomes a pseudo node in Cl-GBI and the criterion for selecting a discriminative pattern in DT-ClGBI can be different. In the following experiments, frequency of a pair is used as the former criterion, and information gain of a pattern is used as the latter criterion 1 . 3.3 Classification Using the Constructed Decision Tree Unseen new graph data must be classified once the decision tree has been con-structed. Here the problem of subgraph isomorphism arises to test if the input graph contains the pattern (subgraph) specified in the test node of the tree. To alleviate this problem, we utilize Cl-GB I again. Theoretically, if the test pattern actually exists in the input graph, Cl-GBI can find it by setting the beam width b and the number of levels N e large enough and by setting the frequency threshold to 0. However, note that nodes and links that never appear in the test pattern are never used to form the test pattern in Cl-GBI. Therefore, we can remove such nodes and links from the input graph before applying Cl-GBI to reduce its running time. This approach is summarized as follows: Step 1. Remove nodes and links that never appear in the test pattern from the Step 2. Apply Cl-GBI to the resulting input graph setting the parameters b Step 3. Test if one of the canonical labels o f extracted patterns with the same
In general, Step 1 results in a small graph and Cl-GBI can run very quickly without any constraints on N e and b . However, if we need to set these constraints, we may not be able to obtain the correct answer because we don X  X  know how large these parameters should be. In that sense, this procedure can be regarded as an approximate solution to the subgraph isomorphism problem. To evaluate the performance of DT-ClGBI, we conducted some experiments on both synthetic and real-world datasets consisting of directed graphs. 4.1 Synthetic Datasets Data Preparation. Synthetic datasets were artificially generated in a random manner. The number of nodes in a graph is determined by the gaussian distri-bution having the average of T and the standard deviation of 1. The links are attached randomly with the probability of p . The node labels and link labels are randomly determined with equal probability. The number of node labels and the number of link labels are denoted as L V and L E , respectively. The total number of transactions is kept fixed as GD .

Two datasets of directed graphs having the average size of 30 and 40, each of which has GD = 300, L V =5, L E = 10, p = 20%, were generated and are represented as T 30 and T 40, respectively. Each dataset was equally divided into two classes, namely  X  X ctive X  and  X  X nactive X . Similarly, L basic patterns of connected subgraphs having the average size of I ,where L =4and I =4,were generated. The number of basic patterns to be embedded in a transaction G t of the class  X  X ctive X , N t , was randomly selected in the range between 1 and L .Eachofthese N t basic patterns was in turn chosen from the set of L basic patterns by equal probability, i.e. 1/ L , and overlaid on that transaction. This means that each transaction of the class  X  X ctive X  includes from 1 to L basic subgraphs, some of them may happen to be the same. We also check if there is any basic subgraph included in a transaction of the class  X  X nactive X  by Cl-GBI as described in Section 3.3. If there is, the involved node and link labels are changed in a way that the basic pattern no longer exists in the transaction. In other words, basic subgraphs are those which discriminate the two classes. This does not necessarily mean that any subgraph of the basic patterns is not discriminative enough. We have not checked that all the subgraphs of the basic patterns appear in both active and inactive data. Figure 3 shows the 4 basic subgraphs which were embedded in the transactions of the class  X  X ctive X . Experiments. For these two synthetic datasets, the classification task is to classify two classes  X  X ctive X  and  X  X nact ive X  using DT-ClGBI by a single run of 10-fold cross validation (CV). The final prediction error rate was evaluated by the average of 10 estimates of the prediction error (a total of 10 decision trees).
The first experiment was conducted to confirm that the most discriminative patterns with respect to the used index can be extracted by Cl-GBI not only at the root node, but also at each internal node itself. To this end, we compared the predictive accuracy and the tree size obtained by two different settings for DT-ClGBI described as follows. In the first setting, i.e. setting 1, a decision tree is constructed by applying Cl-GBI at the root node only, with N e =2.Atthe other nodes, we simply recalculate information gain for those patterns that have already been discovered at the root node. In the other case, i.e., setting 2, Cl-GBI is invoked at the root node with N e = 2 and at other nodes with N e =1.In addition, the total number of levels of Cl-GBI in the second setting is limited to 6 to keep the computation time at a tolera nt level. Whenever the total number of levels reaches this limitation, Cl-GBI is no longer used for extracting patterns. Instead, only the existing patterns are employed for constru cting the decision tree thereafter. Not e that beam width is set to 5 in both settings.
Results of the first experiment are summarized in Table 1, and it is shown that the second setting obtains higher pre dictive accuracy. Moreover, we observe that the decision trees constructed by t he second setting have smaller sizes in most cycles of the 10-fold CV for both dat asets. The result reveals that invoking of Cl-GBI at internal nodes is needed to im prove the predictive accuracy of DT-ClGBI, as well as to reduce the tree size. Intu itively, the search space increases by applying Cl-GBI at the internal nodes in addition to the root node. As a result, more discriminative patterns which have not been discovered in the previous steps are discovered at these nodes. I n other words, applying Cl-GBI at only the root node cannot help enumerate all the necessary patterns unless N e and b are set large enough. For example, in the decision tree constructed by the first run of 10-fold CV on the dataset T 30 using the second setting, the classifying pattern for a node at the third level was not found at the root node but its parent node. If N e is set large enough, the necessary pattern should be able to be found at the root node. This pattern, if found at the root node, should give smaller information gain at the root node but Cl-GBI retains this and passes down to the lower node. The question is how to find this pattern where it is needed without running Cl-GBI using all the dataset.
 The second experiment focused on the comparisons between DT-ClGBI and DT-GBI [2], also in terms of the predictive accuracy and the tree size. Here beam width is also set to 5 in both cases. For DT-GBI, the number of levels of B-GBI at any node of a decision tree is kept fixed as 4. It should be noted that, whenever being invoked for constructing a decision tree by DT-GBI, B-GBI starts extracting typical patterns from the beginning, i.e. no inheritance is employed, because the graphs that pass down to the yes branch have been chunked by the test pattern. On the other hand, the number of levels of Cl-GBI is 4 at the root node and 1 at the other nodes of a decision tree in the case of DT-ClGBI. In addition, the total number of levels of Cl-GBI is limited to 8, which means that the number of levels p erformed by the feature construction tool in DT-ClGBI is much less than that in DT-GBI.
 Table 2 shows the results of the second experiment. It can be seen that DT-ClGBI achieves lower prediction error for both datasets. We also observe that, for each dataset, the decision trees cons tructed by DT-ClGBI have smaller sizes in most cycles of the 10-fold CV. The hi gher predictive accuracy of DT-ClGBI and the simpler decision trees obtained by this method can be explained by the improvement of Cl-GBI over B-GBI, and the inheritance of previously extracted patterns at an internal node (in a decision tree) from its predecessors. It is known that Cl-GBI resolves the problem of overlapping patterns incurred by B-GBI, thus resulting in more typical patterns extracted by Cl-GBI. In addition, the computation time of DT-ClGBI was found to be less than half of that required by DT-GBI with the above settings, which is mainly due to the fact that the feature construction tool for DT-ClGBI was not run from the scratch at any internal node as in DT-GBI. This implies that DT-ClGBI performs better than DT-GBI while requiring much less computation resource.

It should also be noted that the size of the embedded graphs in these two datasets is 4 or 5. Setting N e = 2 as in the first experiment means that the maximum size of patterns we can get at the root node is 4. Considering the beam width, it is unlikely that the embedded patterns are found at the root node. Even N e = 4 as in the second experiment, these basic patterns cannot be found. However, the substructures of the embedded graphs are discriminative enough as shown in the two experiments. Due to the downward closure property, these substructures were embedded in the transactions of the class  X  X ctive X . 4.2 Real-World Datasets Finally, we verified if DT-ClGBI can const ruct decision trees that also achieve reasonably good predictive accuracy on a real-world dataset. For that purpose, we used the hepatitis dataset as in [2]. The classification task here is to clas-sify patients into two classes,  X  X C X  (Liver Cirrhosis) and  X  X onLC X  (non Liver Cirrhosis) based on their fibrosis stages, which are categorized into five stages in the dataset: F0 (normal), F1, F2, F3, and F4 (severe = Liver Cirrhosis). All 43 patients at F4 stage were used as the class  X  X C X , while all 4 patients at F0 stage and 61 patients at F1 stage were used as the class  X  X onLC X . This ratio of  X  X C X  to  X  X onLC X  was determined based on [7]. The records for each patient were converted into a directed graph as d escribed in [2]. The resulting graph database has 108 graph transactions, and the average size of a graph transaction is 316.2 and 386.4 in terms of the number of nodes and of links, respectively.
Through some preliminary experiments on this database using DT-ClGBI, we found that existence of some graphs often makes the resulting decision tree too complicated and worsen the predictive accuracy. This has led us to adopt a two step approach, first to divide the patients into  X  X ypical X  and  X  X on-typical X , and second to construct a decision tree for each group of the patients. To divide the patients in the first step, we ran 10-fold cross validation of DT-ClGBI on this database, varying its parameters b and N e in the ranges of { 5 , 6 , 8 , 10 } and { 6 , 8 , 10 , 12 } , respectively. Note that these values are only for the root node and we did not run Cl-GBI at the succeed ing nodes. The frequency threshold  X  was fixed to 10%. Namely, we conducted 10-fold cross validation 16 times with different combinations of these parameters, and obtained totally 160 decision trees in this step. Then we classified graphs whose average error rate is 0% into  X  X ypical X , and the others into  X  X on-typical X . As a result, for the class  X  X C X , 28 graphs are classified into the subset  X  X ypical X  and the other 15 graphs into  X  X on-typical X , while for the class  X  X onLC X , 48 graphs are classified into  X  X ypical X  and 17 graphs into  X  X on-typical X .

In the second step, we applied DT-ClGBI to each subset again adopting the best parameter setting in the first step with respect to the predictive accuracy, where b =8and N e = 10. The predictive accuracy (average of 10-CV) for the subset  X  X ypical X  is 97.4%, and that for  X  X on-typical X  is 78.1%. The overall accuracy is 91.7%, which is much better t han the accuracy obtained by applying DT-ClGBI to the original dataset with b =8and N e = 10, i.e., 83.4%. We can find typical features for a patient with Liver Cirrhosis in the extracted patterns such as  X  X OT is High X  or  X  X LT is Low X . From these results, we can say that DT-ClGBI can achieve reasonably good predic tive accuracy on a real-world dataset and extract discriminative features embedded in the dataset as subpatterns. We have proposed an algorithm called DT-ClGBI, which can construct decision trees for graph-structured data using Cl-GBI. In DT-ClGBI, substructures, or patterns useful for a classification task are constructed on the fly by means of Cl-GBI during the construction process of a decision tree. The experimental results using synthetic and real-world dataset s showed that decision trees constructed by DT-ClGBI achieve good predictive a ccuracy for graph-st ructured data. The good predictive accuracy of DT-ClGBI is mainly attributed to the fact that Cl-GBI can give the correct number of occurre nces of a pattern in each transaction of the graph database due to its capability of extracting overlapping patterns, which is very useful for algorithms such a s DT-ClGBI that need correct counting. Also, the inheritance of previously extracted patterns at an internal node from its predecessors is shown helpful.

