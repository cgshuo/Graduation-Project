 which is crucial in deciding whether features are similar or not.
 For example, Nowak and Jurie [21] establish whether one can d raw conclusions on two never seen will considerably improve or speed-up such tasks.
 algorithms significantly boost performance for object and s cene categorization [12]. Knowledge distributions will strongly deviate from the Gaussian prob ability distribution. enables a remarkable speed-up in the search for matching ref erence points and hence for matching be applied in the construction of fast search trees, see e.g. [16].
 derived in this paper are directly applicable in object and s cene recognition. The density function has only three parameters: mean, stand ard deviation and skewness. We will accurately.
 Our work on density distributions of similarity values comp ares to the work by Pekalska and Duin when considering the minima and maxima of many measurements . The major result of the field in order to derive the distribution family of similarity mea sures.
 for image feature vectors. Finally, conclusions are given i n Section 5. 2.1 Similarity distance measures To measure the similarity between two feature vectors, many distance measures have been proposed [15]. A common metric class of measures is the L feature vector s to one other feature vector t can be formalized as: where n and i are the dimensionality and indices of the vectors. Let the ra ndom variable D Independent of the reference feature vector s , the probability density function of L be denoted by f ( D 2.2 Distance distributions Ferencz et al. [7] have considered the Gamma distribution to model the L regions to one reference region: f ( D and  X  the scale parameter;  X ( ) denotes the Gamma function. In [7], the distance function wa s represent the region distances to some extent, the method la cks a theoretical motivation. Based on the central limit theorem, Pekalska and Duin [23] as sumed that L feature vectors are normally distributed: f ( D followed. 2.3 Contribution of this paper Our contribution is the theoretical derivation of a paramet erized distribution for L region types [18]. In this section, we derive the distribution function family of L a feature vector drawn from the random variable T . For each vector t , we consider the value at index i , t be interpreted as a sample of the random variable T other vectors involves manipulations of the random variabl e T X variables, and a reparameterization: D = ( X . 3.1 Statistics of sums As a starting point to derive the L tics about the sum of random variables.
 Lemma 1 For non-identical and correlated random variables X Weibull distribution.
 confine the distribution function further, we also need the f ollowing lemma. Lemma 2 If in the above lemma the random variable X sum of the variables is Weibull distributed (and not Gumbel n or Frechet): with  X  the shape parameter and  X  the scale parameter.
  X  upper-bounded indeed.
 upper-bounded random variables, summarized in the followi ng theorem. Theorem 1 For non-identical, correlated and upper-bounded random va riables X variable Y = mas 1 and 2. Theorem 1 is the starting point to derive the distr ibution of L reference vector to other feature vectors. 3.2 L Theorem 1 states that Y is Weibull-distributed, given that { X L -distances, achieved by the transformation ( ) 1 /p : The consequence of the substitution Z = Y 1 /p for the distribution of Y is a change of variables distribution still of the Weibull type: We argue that the random variables X correlated and upper-bounded random variables when consid ering a set of values extracted from feature vectors at indices i and j : We have obtained the following result.
 ues, L the Weibull distribution. 3.3 Extending the class of features transform g ( ) applied to a single feature vector as s  X  = g ( s ) . For the random variable T . We are still dealing with upper-bounded variables X  X  The experimental verification of the assumption that PCA-tr ansformed feature values T  X  longer dealing with correlated feature values T  X  whether X  X  axes. The tuples ( X  X  Obviously, variances  X  ( X  X  expected to be observed. Under the assumed case, we have obta ined the following result. ues, and for PCA-transformations thereof, L adhere to the Weibull distribution. 3.4 Heterogeneous feature vector data posite dataset modelled by random variables { T each of the T T heterogeneous distance data { T with non-identical, correlated and upper-bounded values, and for PCA-transformations thereof, L Weibull mixture distribution: f ( D = d ) = and  X  different mechanisms to describe an image region [17]. The r egion features are computed from regions detected by the Harris-and Hessian-affine regions, maximally stable regions (MSER), and intensity extrema-based regions (IBR) [18]. Also, we consi der PCA-transformed versions for each on the L typical distributions of distances between features taken from single images. 4.1 Validation of the corollary assumptions for image featu res 4.1.1 Intrinsic feature assumptions in practice. Differences between feature values are correlated. We consider a set of feature vectors T the differences at index i to a reference vector s : X of Pearson X  X  correlation [4] between the difference values X at an other feature vector index.
 significantly correlated. For SPIN and GLOH, we obtain 98% and 96% , respectively. Also PCA-across the random samplings and across the various region ty pes.
 We repeat the experiment for PCA-transformed feature value s. Although the resulting values are and 75% , respectively.
 Differences between feature values are non-identically di stributed. We repeat the same proce-of significantly differently distributed difference value s X versions of SIFT, SPIN, GLOH, and PCA-SIFT, we find: 62% , 40% , 64% , and 51% , respectively. 4.1.2 Inter-feature assumptions 2a shows Harris-affine regions from a natural scene which are described by the SIFT feature. The the distances of one to other regions computed from a composi te image containing two types of in practice, and that the Weibull function, or a mixture, fits distance distributions well. 4.2 Validation of Weibull-shaped distance distributions feature vector to other vectors. We consider the same data as before. Over 1,000 repetitions we consider the goodness-of-fit of L test has also proven to be suited to measure the goodness-of-fit of mixture distributions [19]. Table 1 indicates that for most of the feature types computed from various regions, more than 90% of the SPIN, SIFT, PCA-SIFT and GLOH features, are fitted well by Weibull distributions. The exception here is the low number of fits for the SIFT and SPIN fe atures computed from Hessian-have multiple modes. Likewise, there is a low percentage of fi ts of L Figure 2: Distance distributions from one randomly selecte d image region to other regions, each is bimodal (c). Samples from each of the distributions are sh own in the upper images. SPIN feature computed from IBR regions. Again, multiple mod es in the distributions are observed. For these distributions, a mixture of two Weibull functions provides a good fit (  X  97% ). and PCA-SIFT features, and for a large variety of images from the COREL image collection, we have demonstrated that the similarity distances from one to other features, computed from L between PCA-transformed feature vectors, the distances ar e Weibull-distributed. The Malahanobis distance is very similar to the L [7] and normal [23] distributions are positively and non-sk ewed, respectively. similarity, with projected improvements and speed-ups in o bject/scene recognition algorithms. Acknowledgments project Multimedian, and by the EU Network of Excellence MUS CLE.

