 Anomaly detection involves detecting statistically significant deviations of test data from nominal distribution. In typical applications the nominal distribution is unknown and generally cannot be reliably estimated from nominal training data due to a combination of factors such as limited data size and high dimensionality.
 We propose an adaptive non-parametric method for anomaly detection based on score functions that maps data samples to the interval [0 , 1] . Our score function is derived from a K-nearest neighbor graph (K-NNG) on n -point nominal data. Anomaly is declared whenever the score of a test sample falls below  X  (the desired false alarm error). The efficacy of our method rests upon its close connec-tion to multivariate p-values. In statistical hypothesis testing, p-value is any transformation of the feature space to the interval [0 , 1] that induces a uniform distribution on the nominal data. When test samples with p-values smaller than  X  are declared as anomalies, false alarm error is less than  X  . We develop a novel notion of p-values based on measures of level sets of likelihood ratio functions. Our notion provides a characterization of the optimal anomaly detector, in that, it is uniformly most powerful for a specified false alarm level for the case when the anomaly density is a mixture of the nominal and a known density. We show that our score function is asymptotically consistent, namely, it converges to our multivariate p-value as data length approaches infinity.
 Anomaly detection has been extensively studied. It is also referred to as novelty detection [1, 2], outlier detection [3], one-class classification [4, 5] and single-class classification [6] in the liter-approaches [7] the nominal densities are assumed to come from a parameterized family and gen-eralized likelihood ratio tests are used for detecting deviations from nominal. It is difficult to use parametric approaches when the distribution is unknown and data is limited. A K-nearest neighbor (K-NN) anomaly detection approach is presented in [3, 8]. There an anomaly is declared whenever the distance to the K-th nearest neighbor of the test sample falls outside a threshold. In comparison our anomaly detector utilizes the global information available from the entire K-NN graph to detect deviations from the nominal. In addition it has provable optimality properties. Learning theoretic approaches attempt to find decision regions, based on nominal data, that separate nominal instances from their outliers. These include one-class SVM of Sch  X o lkopf et. al. [9] where the basic idea is to map the training data into the kernel space and to separate them from the origin with maxi-mum margin. Other algorithms along this line of research include support vector data description [10], linear programming approach [1], and single class minimax probability machine [11]. While these approaches provide impressive computationally efficient solutions on real data, it is generally difficult to precisely relate tuning parameter choices to desired false alarm probability. Scott and Nowak [12] derive decision regions based on minimum volume (MV) sets, which does provide Type I and Type II error control. They approximate (in appropriate function classes) level sets of the unknown nominal multivariate density from training samples. Related work by Hero [13] based on geometric entropic minimization (GEM) detects outliers by comparing test samples to the most concentrated subset of points in the training sample. This most concentrated set is the K -point minimum spanning tree(MST) for n -point nominal data and converges asymptotically to the minimum entropy set (which is also the MV set). Nevertheless, computing K -MST for n -point data is generally intractable. To overcome these computational limitations [13] proposes heuristic greedy algorithms based on leave-one out K-NN graph, which while inspired by K -MST algorithm is no longer provably optimal. Our approach is related to these latter techniques, namely, MV sets of [12] and GEM approach of [13]. We develop score functions on K-NNG which turn out to be the empirical estimates of the volume of the MV sets containing the test point. The volume, which is a real number, is a sufficient statistic for ensuring optimal guarantees. In this way we avoid explicit high-dimensional level set computation. Yet our algorithms lead to statistically optimal solutions with the ability to control false alarm and miss error probabilities.
 linearly with dimension and quadratic with data size and can be applied to high dimensional feature spaces. (2) Like [12] our algorithm is provably optimal in that it is uniformly most powerful for the specified false alarm level,  X  , for the case that the anomaly density is a mixture of the nominal and any other density (not necessarily uniform). (3) We do not require assumptions of linearity, smoothness, continuity of the densities or the convexity of the level sets. Furthermore, our algorithm adapts to the inherent manifold structure or local dimensionality of the nominal density. (4) Like [13] and unlike other learning theoretic approaches such as [9, 12] we do not require choosing complex tuning parameters or function approximation classes. In this section we present our basic algorithm devoid of any statistical context. Statistical analysis the unit cube [0 , 1] d . For notational convenience we use  X  and x n +1 interchangeably to denote a test point. Our task is to declare whether the test point is consistent with nominal data or deviates from the nominal data. If the test point is an anomaly it is assumed to come from a mixture of nominal distribution underlying the training data and another known density (see Section 3).
 Let d ( x, y ) be a distance function denoting the distance between any two points x, y  X  [0 , 1] d . For function to be Euclidean. However, we also consider geodesic distances to exploit the underly-ing manifold structure. The geodesic distance is defined as the shortest distance on the manifold. The Geodesic Learning algorithm, a subroutine in Isomap [14, 15] can be used to efficiently and consistently estimate the geodesic distances. In addition by means of selective weighting of differ-ent coordinates note that the distance function could also account for pronounced changes in local dimensionality. This can be accomplished for instance through Mahalanobis distances or as a by product of local linear embedding [16]. However, we skip these details here and assume that a suitable distance metric is chosen.
 Once a distance function is defined our next step is to form a K nearest neighbor graph (K-NNG) or alternatively an  X  neighbor graph (  X  -NG). K-NNG is formed by connecting each x i to the K closest points { x i order d i,i nearest neighbor. We construct  X  -NG where x i and x j are connected if and only if d ij  X   X  . In this case we define N S ( x i ) as the degree of point x i in the  X  -NG.
 For the simple case when the anomalous density is an arbitrary mixture of nominal and uniform  X  -NNG respectively. The score functions map the test data  X  to the interval [0 , 1] . where I { X } is the indicator function.
  X  p
K (  X  ) ,  X  p  X  (  X  )  X   X  . We call this algorithm Localized p-value Estimation (LPE) algorithm. This choice is motivated by its close connection to multivariate p-values(see Section 3).
 The score function K-LPE (or  X  -LPE) measures the relative concentration of point  X  compared to the training set. Section 3 establishes that the scores for nominally generated data is asymptotically uniformly distributed in [0 , 1] . Scores for anomalous data are clustered around 0 . Hence when scores below level  X  are declared as anomalous the false alarm error is smaller than  X  asymptotically (since the integral of a uniform distribution from 0 to  X  is  X  ).
 uniform while scores for the test points drawn from 2D uniform distribution cluster around zero. Figure 1 illustrates the use of K-LPE algorithm for anomaly detection when the nominal data is a 2D Gaussian mixture. The middle panel of figure 1 shows the detection results based on K-LPE are consistent with the theoretical contour for significance level  X  = 0 . 05 . The right panel of figure 1 shows the empirical distribution (derived from the kernel density estimation) of the score function K-LPE for the nominal (solid blue) and the anomaly (dashed red) data. We can see that the curve for the nominal data is approximately uniform in the interval [0 , 1] and the curve for the anomaly data has a peak at 0 . Therefore choosing the threshold  X  = 0 . 05 will approximately control the Type I error within 0 . 05 and minimize the Type II error. We also take note of the inherent robustness of our algorithm. As seen from the figure (right) small changes in  X  lead to small changes in actual false alarm and miss levels. To summarize the above discussion, our LPE algorithm has three steps: (1) Inputs: Significance level  X  , distance metric (Euclidean, geodesic, weighted etc.). (2) Score computation: Construct K-NNG (or  X  -NG) based on d ij and compute the score function K-LPE from Equation 1 (or  X  -LPE from Equation 2). (3) Make Decision: Declare  X  to be anomalous if and only if  X  p K (  X  )  X   X  (or  X  p  X  (  X  )  X   X  ). Computational Complexity: To compute each pairwise distance requires O(d) operations; and O( n 2 d ) operations for all the nodes in the training set. In the worst-case computing the K-NN graph training data. Finally, computing the score for each test data requires O(nd+n) operations(given that R S (  X  ) , N S (  X  ) have already been computed).
 Remark: LPE is fundamentally different from non-parametric density estimation or level set esti-mation schemes (e.g., MV-set). These approaches involve explicit estimation of high dimensional quantities and thus hard to apply in high dimensional problems. By computing scores for each test sample we avoid high-dimensional computation. Furthermore, as we will see in the following sec-tion the scores are estimates of multivariate p-values. These turn out to be sufficient statistics for optimal anomaly detection. A statistical framework for the anomaly detection problem is presented in this section. We establish that anomaly detection is equivalent to thresholding p-values for multivariate data. We will then show that the score functions developed in the previous section is an asymptotically consistent esti-mator of the p-values. Consequently, it will follow that the strategy of declaring an anomaly when a test sample has a low score is asymptotically optimal.
 Assume that the data belongs to the d-dimensional unit cube [0 , 1] d and the nominal data is sam-pled from a multivariate density f 0 ( x ) supported on the d-dimensional unit cube [0 , 1] d . Anomaly detection can be formulated as a composite hypothesis testing problem. Suppose test data,  X  comes supported on [0 , 1] d . Anomaly detection involves testing the nominal hypotheses H 0 :  X  = 0 versus the alternative (anomaly) H 1 :  X  &gt; 0 . The goal is to maximize the detection power subject to false alarm level  X  , namely, P ( declare H 1 | H 0 )  X   X  .
 Definition 1. Let P 0 be the nominal probability measure and f 1 (  X  ) be P 0 measurable. Suppose the the p-value of a data point  X  as Note that the definition naturally accounts for singularities which may arise if the support of f 0 (  X  ) p (  X  ) = 0 . Here anomaly is always declared(low score).
 The above formula can be thought of as a mapping of  X   X  [0 , 1] . Furthermore, the distribution of formations. To build intuition about the above transformation and its utility consider the following example. When the mixing density is uniform, namely, f 1 (  X  ) = U (  X  ) where U (  X  ) is uniform over that such a density level set is equivalent to a minimum volume set of level  X  . The minimum volume set at level  X  is known to be the uniformly most powerful decision region for testing H 0 :  X  = 0 Theorem 1. The uniformly most powerful test for testing H 0 :  X  = 0 versus the alternative (anomaly) H 1 :  X  &gt; 0 at a prescribed level  X  of significance P ( declare H 1 | H 0 )  X   X  is: establish p ( X ) as a random variable over [0 , 1] under both nominal and anomalous distributions. is a monotonically decreasing PDF supported on [0 , 1] . Consequently, the uniformly most powerful test for a significance level  X  is to declare p-values smaller than  X  as anomalies.
 Next we derive the relationship between the p-values and our score function. By definition, R S (  X  ) and R S ( x i ) are correlated because the neighborhood of  X  and x i might overlap. We modify our algorithm to simplify our analysis. We assume n is odd (say) and can be written as n = 2 m + 1 . We divide training set S into two parts: We modify  X  -LPE to  X  p  X  (  X  ) = 1 m
P Furthermore, we assume f 0 (  X  ) satisfies the following two smoothness conditions: We have the following theorem.
 Let  X   X  [0 , 1] d be an arbitrary test sample. It follows that for a suitable choice K and under the above smoothness conditions, For simplicity, we limit ourselves to the case when f 1 is uniform. The proof of Theorem 2 consists of two steps: Lemma 3 (  X  -LPE) . By picking  X  = m  X  3 5 d where  X  Proof. We only prove the lower bound since the upper bound follows along similar lines. By inter-changing the expectation with the summation, where the last inequality follows from the symmetric structure of { x 0 , x 1 ,  X  X  X  , x m } . Clearly the objective of the proof is to show P S random variable with success probability q ( x 1 ) := N smoothness condition. The details of these two steps are shown in the below.
 Note that N S that is, N S We choose  X  x Next, we relate q ( x 1 )( or condition of f 0 , and then equation (5) becomes By applying the same steps to N S Lemma 4 ( K -LPE) . By picking K = Proof. The proof is very similar to the proof to Lemma 3 and we only give a brief outline here. Now the objective is to show P S the result of Lemma 3. To accomplish this, we note that { R S { N S By the tail probability of Binomial distribution, the probability of the above two events converges to techniques developed in the proof to Lemma 3, these two inequalities are implied by Therefore if we choose K = Remark: Lemma 3 and Lemma 4 were proved with specific choices for  X  and K . However, they can be chosen in a range of values, but will lead to different lower and upper bounds. We will show in Section 4 via simulation that our LPE algorithm is generally robust to choice of parameter K . Lemma 5. Suppose K = cm 2 / 5 and denote  X  p K (  X  ) = 1 m where  X  d is a constant and is defined as the minimal number of cones centered at the origin of angle  X / 6 that cover R d .
 Proof. We can not apply Law of Large Number in this case because I { R as MacDiarmid X  X  inequality[17]. Denote F ( x 0 ,  X  X  X  , x m ) = 1 m Corollary 11.1 in [18], Then the lemma directly follows from applying McDiarmid X  X  inequality.
 Theorem 2 directly follows from the combination of Lemma 4 and Lemma 5 and a standard appli-cation of the first Borel-Cantelli lemma. We have used Euclidean distance in Theorem 2. When the support of f 0 lies on a lower dimensional manifold (say d 0 &lt; d ) adopting the geodesic metric leads to faster convergence. It turns out that d 0 replaces d in the expression for  X  1 in Lemma 3. First, to test the sensitivity of K-LPE to parameter changes, we run K -LPE on the benchmark data-set Banana [19] with K varying from 2 to 12 . We randomly pick 109 points with  X  +1  X  label and regard them as the nominal training data. The test data comprises of 108  X  +1  X  points and 183  X   X  1  X  points (ground truth) and the algorithm is supposed to predict  X  +1  X  data as nominal and  X   X  1  X  data as anomalous. Scores computed for test set using Equation 1 is oblivious to true f 1 density ( X   X  1  X  labels). Euclidean distance metric is adopted for this experiment.
 To control false alarm at level  X  , points with score smaller than  X  are predicted as anomaly. Empiri-cal false alarm and true positives are computed from ground truth. We vary  X  to obtain the empirical ROC curve. The above procedure is followed for the rest of the experiments in this section. As shown in 2(a), the LPE algorithm is insensitive to K . For comparison we plot the empirical ROC curve of the one-class SVM of [9]. For our OC-SVM implementation, for a fixed bandwidth, c , we obtain the empirical ROC curve by varying  X  . We then vary the bandwidth, c , to obtain the best (in terms of AUC) ROC curve. The optimal bandwidth turns out to be c = 1 . 5 . In LPE if we set  X  = 0 . 05 we get empirical FA = 0 . 06 and for  X  = 0 . 08 , empirical FA = 0 . 09 . For OC-SVM we are unaware of any natural way of picking c and  X  to control FA rate based on training data. Next, we apply our K -LPE to the problem where the nominal and anomalous data are generated in the following way: We call ROC curve corresponding to the optimal Bayesian classifier as the Clairvoyant ROC (the red dashed curve in Figure 2(b)). The other two curves are averaged (over 15 trials) empirical ROC curves via LPE. Here we set K = 6 and n = 40 or n = 160 . We see that for a relatively small training set of size 160 the average empirical ROC curve is very close to the clairvoyant ROC curve. Finally, we ran LPE on three real-world datasets: Wine , Ionosphere [20] and MNIST US Postal Service ( USPS ) database of handwritten digits. If there are more than 2 labels in the data set, we artificially regard points with one particular label as nominal and regard the points with other labels as anomalous. For example, for the USPS dataset, we regard instances of digit 0 as nominal and instances of digits 1 ,  X  X  X  , 9 as anomaly. The data points are normalized to be within [0 , 1] d and we use geodesic distance [14]. The ROC curves are shown in Figure 3. The feature dimension of Wine is 13 and we apply the  X  -LPE algorithm with  X  = 0 . 9 and n = 39 . The test set is a mixture of 20 nominal points and 158 anomaly points. The feature dimension of Ionosphere is 34 and we apply the K -LPE algorithm with K = 9 and n = 175 . The test set is a mixture of 50 nominal points and 126 anomaly points. The feature dimension of USPS is 256 and we apply the K -LPE algorithm with K = 9 and n = 400 . The test set is a mixture of 367 nominal points and 33 anomaly points. In USPS , setting  X  = 0 . 5 induces empirical false-positive 6 . 1% and empirical false alarm rate 5 . 7% (In contrast FP = 7% and FA = 9% with  X  = 5% for OC-SVM as reported in [9]). Practically we find that K -LPE is more preferable to  X  -LPE and as a rule of thumb setting K  X  n 2 / 5 is generally effective. In this paper, we proposed a novel non-parametric adaptive anomaly detection algorithm which leads to a computationally efficient solution with provable optimality guarantees. Our algorithm takes a K-nearest neighbor graph as an input and produces a score for each test point. Scores turn out to be empirical estimates of the volume of minimum volume level sets containing the test point. While minimum volume level sets provide an optimal characterization for anomaly detection, they are high dimensional quantities and generally difficult to reliably compute in high dimensional feature spaces. Nevertheless, a sufficient statistic for optimal tradeoff between false alarms and misses is the volume of the MV set itself, which is a real number. By computing score functions we avoid computing high dimensional quantities and still ensure optimal control of false alarms and misses. The computational cost of our algorithm scales linearly in dimension and quadratically in data size.
