 There are increasingly visible demands for structured/ un-structured information integration and advanced analytics. However, conventional database technology has not been able to present a robust and practical implementation of a truly integrated architecture for such purposes. After work-ing on several industrial applications (in particular, in the healthcare and life sciences area), we have identified funda-mental issues and technical approaches to tackle the issues. In this paper, we propose data representations and alge-braic operations for integrating semantic information (e.g., ontologies) into OLAP systems, which allow us to analyze a huge set of textual documents with their underlying se-mantic information. The performance of the prototype im-plementation has been evaluated using real world datasets, and the high scalability and flexibility of our approach have been confirmed with respect to the computation time. H.2 [ DATABASE MANAGEMENT ]: Database appli-cations Performance OLAP, Text Mining
Since many of business intelligence applications have been incorporating unstructured (primarily textual) information for more context-oriented analysis and decision-making [4],  X 
This research was conducted when A. Inokuchi was affili-ated with Tokyo Research Laboratory, IBM Japan.
 database technology has been seriously challenged to in-gest, map, store, and access such text-originated informa-tion along with the structured information in a way that two types of information can mutually enhance information dis-covery and analysis capability. One of the most critical prob-lems is that most of semantics underlying the unstructured information (such as ontological hierarchy, synonymous and antonymous relationship) cannot be effectively managed by conventional database systems. Another significant prob-lem is that a rigid schematic representation (and associated queries and analytic processing) of unstructured informa-tion often suffers frequent modifications due to updates to dictionary and ontology for adequately categorized words, phrases, and entities described in the unstructured informa-tion. Therefore, it is very important to propose a more flex-ible representation, which reduces the cost and workload of frequent revision and re-population of the database schema.
The multidimensional database technology has been con-sidered for the interactive analysis of large amounts of data for decision making purposes [23, 13, 1, 2, 10, 12]. Multidi-mensional data models categorize data either as facts with associated numerical measures or as dimensions that charac-terize the facts. In a retail business, for example, a purchase transaction would be a fact and the purchase amount and price would be measures, and the type of purchased prod-uct, the purchase time and location would be dimensions. Queries for Online Analytical Processing (OLAP) aggregate measures over a range of dimensional values to provide re-sults such as the total sales per month of a given product, leading to overall trends. An important feature of the mul-tidimensional data model is to use hierarchical dimensions to provide as much context as possible for the facts. Di-mensions are used for selecting and aggregating data at the desired level of detail. Most of traditional multidimensional databases assume that the dimensional hierarchies are bal-anced and non-ragged trees.

The star and snowflake schemas, which are representative schemas for the multidimensional data model, store data in fact and dimension tables. A fact table holds a row for each fact in the database and it has a column for each measure, containing the measured value for the particular fact, as well as a column for each dimension that contains a foreign key referring to a dimension table for the particular dimension.
When analyzing unstructured information in a multidi-mensional data model, a document would be typically rep-resented as a fact, and categories of keywords, such as pro-tein, gene, or disease in the life science domain, would be selected as axis for the interactive analysis as shown in Fig-455 ure 1. Each cell of the cube in the figure stores the number of the corresponding documents. Operations, such as drill down, roll up, slice, dice, pivoting or drill through, are avail-able for analyzing/aggregating large amounts of documents and their contextual information to obtain insights. It is of-ten very difficult, however, to define a set of dimensions and their hierarchies for a huge set of keywords such as protein name, gene names. For example, the number of distinct keywords in data used in our experiments in Section 5 is 13,640,593. To design a hierarchy for OLAP, we use on-tologies such as Unified Medical Language System (UMLS: http://umlsinfo.nlm.nih.gov) and the Gene Ontology (GO: http://www.geneontology.org), each of which needs to be represented as a kind of a directed acyclic graph, rather than a set of balanced and non-ragged trees. When we assume that each node of the hierarchy corresponds to a dimension, many missing values and a set of multiple values for the node could possibly be introduced. In addition, because the number of nodes in the hierarchy becomes very large and a complex relationship among the nodes exists, we cannot store the data in the star schema and efficiently aggregate the data within the hierarchy under a straightforward im-plementation.

In this paper, we propose a data representation and alge-braic operations to integrate a multidimensional model with ontologies to analyze a huge set of textual documents. This paper describes
The rest of this paper is organized as follows. Section 2 addresses a hierarchy and an ontology. Section 3 defines our data representation and its algebraic operations. In Sec-tion 4, we introduce our schemas to efficiently compute the distributions and their implementations. Section 5 presents experiments using about 500,000 abstracts. Section 6 dis-cusses related works. Finally, Section 7 concludes this paper.
In this section, we give formal definitions of a hierarchy and an ontology according to [3]. If S is a nonempty set, and  X  X  X  S  X  S ,then( S,  X  )isanordering 1 .If x  X  x for x  X  S ,then S is reflexive. If x  X  y and y  X  z  X  x  X  z for x, y, z  X  S ,then S is transitive. If x  X  y and y  X  x  X  x = y for x, y  X  S ,then S is anti-symmetric. ( S,  X  )isapartial ordering if S is a reflexive, transitive, and anti-symmetric binary relation on S . better: Let ( S,  X  1 )and( S,  X  2 ) be two orderings. We say ( S,  X  1 ) is better than ( S,  X  2 )iff  X  x, y  X  S ( x  X  1 y ). In addition, we say that ( S,  X  1 ) is strictly better than ( S,  X  2 )iff( S,  X  1 ) is better than ( S,  X  2 )and( S,  X  better than ( S,  X  1 ). hierarchy: Let ( S,  X  ) be a partial ordering. A hierarchy of S is an ordering ( S, ) such that (1) ( S, ) is better than and (3) there is no other ordering ( S, ) satisfying the above two conditions and that ( S, ) is strictly better than ( S, ontology: Suppose  X  is some finite set of strings and S is some set. An ontology w.r.t.  X  is a partial mapping  X  from  X  to hierarchies for S .

Example 1: When S is given as { tire, car, hubcap } ,where tire is a part of car, hubcap is a part of car, and hubcap is a part of a tire. In addition, everything is a part of itself. For
Hierarchies can be classified according to their generality as shown in Figure 2 [21].
 DAG: Directed acyclic graph (DAG) which is a directed graph with no directed cycles is the most general class in the taxonomy of hierarchy. The hierarchy introduced above to define the ontology is a proper subclass of this class. transitive anti-closed digraph: Theanti-closurecanbe produced as follows. If there are an edge of length 1 and a path from node A to node B of length 2 or more, then remove the path of length 1 from A to B. The hierarchy de-fined above is in this class. tree: A tree is a DAG where each node can only have one parent, except for one node which has no parents and which is called the root. balanced tree: Levels in an unbalanced hierarchy have a consistent parent-child relationship but have a logically in-consistent levels. The hierarchy branches also can have in-
This paper uses  X  to represent a direct relation between two elements in the set S , &lt; + to represent its transitive clo-sure, and  X  + to represent its transitive closure or represent that the elements are equal. 456
Figure 3: An Unbalanced Tree and a Ragged Tree consistent depths. For example, an unbalanced hierarchy in Figure 3 (a) shows a chief executive officer on the top level of the hierarchy and at least two of the people that might branch off below including the chief operating officer and the executive secretary. The chief operating officer has more people branching off also, but the executive secretary does not. The parent-child relationships on both branches of the hierarchy are consistent. However, an executive secre-tary is not the logical equivalent of a chief operating officer non-ragged tree: Each level in a ragged hierarchy has a consistent meaning, but the branches have inconsistent depths because at least one branch has no corresponding node at some level. Figure 3 (b) shows a geographic hier-archy that has Country, State, and City levels defined. The hierarchy becomes ragged when some member does not have an entry at all of the levels. For example, a branch has no entry for the State level because this level is not applicable to Greece for the business model in this example. In this example, the Greece and United States branches have dif-ferent depths to the leaf, creating a ragged hierarchy. balanced and non-ragged tree: Most of traditional mul-tidimensional databases use hierarchies of this class.
In this section, we give formal definitions of our data rep-resentation and operations according to [22].
Given a hierarchy (or an ontology) ( S,  X  ), a fact schema is defined as S =( F , T ), where F is a fact type and T is a hierarchy type T =( C ,  X  T , T ) which is strictly better than ( S,  X  ) and the relations in ( S,  X  ) required for analyzing the documents are remaining in T . The hierarchy type is a three-tuple ( C ,  X  T , T ), where C = {C j ,j =1 ,  X  X  X  ,n set of category types of T ,and  X  T is a partial order on with T  X  X  being the top element of the ordering. The intuition is that the top element of the ordering logically contains all other elements, that is  X C j  X  X  , C j  X  + T .
A hierarchy instance T of type T is a two-tuple T =( C,  X  ), where C is a set of categories c j such that Type ( c j and  X  is a partial order on C . Functions to give the set of immediate predecessors and successors of a category c j are defined as pred : C  X  2 C and succ : C  X  2 C .That is, pred ( c j )= { c | c &gt;c j } and succ ( c j )= { c respectively. Each category c  X  C has an associated set dom ( c ) called its domain. The members of dom ( c ) are called http://publib.boulder.ibm.com/infocenter/db2luw/v8/ index.jsp?topic= /com.ibm.db2.udb.db2 olap.doc/ cmdhierarchy.htm values of the category c . An element in dom ( c ) is represented as c : v . In addition, a function below to give a set of values is also defined as below ( c )= { dom ( c ) | c  X  + c } .
Example 2: We have a hierarchy instance T ,apartof which is depicted in Figure 4 (a). Categories such as  X  X oft-ware X ,  X  X S X ,  X  X iddleware X ,  X  X pplication X ,  X  X indows X ,  X  X inux X ,  X  X IX X  are contained in C . pred ( OS ) has only one element  X  X oftware X , and succ ( OS )contains {  X  X indows X ,  X  X inux X ,  X  X IX X  } . dom ( Windows ) contains  X  X indows XP X ,  X  X indows Me X ,  X  X indows 2000 X , and so on. below ( All )con-tains all values of all categories.

Let F = { f i ,i =1 ,  X  X  X  ,m } be a set of facts. A fact-hierarchy relationship between F and T is a set R = { ( f, c : v ) where f  X  F , c  X  C ,and v  X  dom ( c ). Thus, R links facts to hierarchical values. We say that fact f is char-acterized by a hierarchical value c : v , written by f c : v , if  X  c  X  C (( f, c : v )  X  R  X  c  X  + c  X  v = v ). Our data object is a four tuple D = {S ,F,T,R } ,where S =( F , T ) is the fact schema, F is a set of facts where Type ( f )= T =( C,  X  ) is a hierarchy instance where Type ( c j )= C c  X  C and C j  X  X  ,and R is a set of fact-hierarchy relations such that ( f, c : v )  X  R  X  f  X  F  X  X  X  c  X  C ( v  X  dom ( c )).
Example 3: We have the hierarchy instance T and an analyzed document which is depicted in Figure 4 (b). F contains a set of document identifiers. Terms in the doc-ument whose document ID is 1 in Figure 4 are annotated in preprocessing, e.g., categories  X  X indows X  and  X  X orksta-tion X  are assigned to terms  X  X indows 2000 X  and  X  X ntelliSta-tion 6217 X , respectively, and (1 ,windows : windows 2000) and (1 , workstation : IntelliStation 6217) are stored in R .
Conceptually, R corresponds to a relation R  X  2 dom ( c 1  X  X  X  X  2 dom ( c n ) which is not a normalized relation. R corre-sponds to a fact table for a star schema, and each row and column in R correspond to a document (fact) and a category (dimension value in the star schema), respectively. A naive method cannot store the data in a relational database and efficiently aggregate the data along the hierarchy, because the relation has many missing values and a set of multi-ple values for each attribute c j , the number of attributes in the relation becomes very large and a complex relationship among the attributes (columns) exists.

Example 4: A hierarchy instance T used in Section 5 is a transitive anti-closed digraph which has more than 240,000 nodes (categories) and more than 340,000 edges and whose depth is 24. If the hierarchy instance is a tree, V = E +1, where V and E are the numbers of categories and edges of the hierarchy, respectively. Ho wever, because the difference between the numbers of the categories and edges in T is so large, the hie rarchy instance T used in Section 5 have a very complex relationship. In addition, about 36,400,000 elements in a conceptual relation R have values. Since the number of attributes, n ,for R is greater than 240,000 and R has more tuples than 500,000 in Section 5, most of elements in R are missing values. Furthermore, more than 7,600,000 elements in R has a set of values, and dom ( ) has about 193,000,000 distinct values.

The function g ( c ) is defined as a user-defined function to return a set of fact-hierarchy re lations, and another function G ( g ( c )) is defined as G ( g ( c )) = { ( c : v ) | ( f, c : v ) function Group as Group ( c 1 : v 1 ,  X  X  X  ,c q : v q )= { F  X  used to aggregate the distributions of documents for each 457 Figure 4: Examples of a Hierarchy and Fact-Hierarchy Relations g (1) ( c )= { ( f, c : v ) | ( f, c : v )  X  R  X  c = c  X  v = dom ( c ) g (2) ( c )= { ( f, c : c ) | ( f, c : v )  X  R  X  c  X  succ ( c ) g (3) ( c )= { ( f, c : v ) | ( f, c : v )  X  R  X  c  X  + c  X  v
Figure 5: Examples of User-Defined Functions keyword and category. For example, g ( c )isprovidedas in Figure 5. First, g (1) ( c ) is used to aggregate the distribu-tions for keywords belonging to the specified category c .The second function g (2) ( c ) is used to aggregate the distributions for the immediate successors (subcategories) of the specified category c . The third one g (3) ( c ) is used to aggregate the distributions for keywords belonging to below ( c ). Users can define any additional functions as required for the intended analysis of a set of documents.
This subsection defines operations on our data object. selection  X  : Given a compound predicate P = p 1 or  X  X  X  or p where each atomic predicate p i is represented in the form of c : v or c :  X  . The selection  X  is defined as  X  P ( D )= (
S ,F ,T,R ), where F = { f | f  X  F  X  ( f p 1  X  X  X  X  X  f p ) } ,and R = { ( f ,c : v )  X  R | f  X  F } . For example, a set of documents having any keywords belonging to a cate-gory  X  X oftware X  is given by  X  sof tware :  X  ( D ). Other examples  X  of documents having a term  X  X IKE X  belonging to a category  X  X ene name X  and a set of documents having a term v 1 and v belonging to a category c , respectively. difference  X  : Given two data objects D 1 =( S 1 ,F 1 ,T 1 and D 2 =( S 2 ,F 2 ,T 2 ,R 2 ) such that S 1 = S 2 = S ,the difference is defined as ( S ,F 1 ,T 1 ,R 1 )  X  ( S ,F (
S ,F ,T 1 ,R ), where F = F 1  X  F 2 and R = { ( f, c : v ) | f  X  F , ( f ,c : v )  X  R } . For example, a set of docu-ments which has a term v 1 and does not have the term v 2 projection  X  : The projection is defined as  X  c 1  X  X  X  X  X  c (
S ,F,T,R ), where R = { ( f, c : v )  X  R | f  X  F  X  ( f c :  X  X  X  X  X  X  X  f c l :  X  ) } . aggregation  X  : Given a set of categories and functions  X  [ T, count ]( D )=( S ,F ,T ,R ), where S , F , T ,and R are defined in Figure 6. ( count , ) represents a partial relation count &lt; .
 Example 5: We give some examples of the aggregation S =( F , T ) F =2 F T =( C ,  X  T , T )
C = C X  X  count }  X  T =  X  T  X  X  ( count , ) }
F = { Group ( c 1 : v 1 ,  X  X  X  ,c q : v q ) | ( c 1 : v 1 , T =( C ,  X  )
C = C  X  X  count }  X  =  X  T  X  X  ( count , ) } R = R 1  X  R 2
R 1 = { ( f ,c : v ) | X  ( c 1 : v 1 ,  X  X  X  ,c q : v q )  X  G ( g  X  , and will give detailed procedures using a Document-Term Matrix in Section 5.2. Table 1 shows a top N ranking of pro-tein names based on document frequencies. It is obtained by calculating  X  [ protein ,g, count 1 ](  X  disease : diabetes sorting the result in descending order of frequency, and fetch-ing N rows. Table 2 shows the results of the query  X  [ company , GeneSymbol ,g 1 ,g 2 , count 2 ]( D ). If the re-sult is analyzed for a set of patent documents, a strategist for a pharmaceutical company might be able to find associ-ations between companies and genes. The query  X  [ protein , protein ,g 1 ,g 2 , count 3 ]( D ) may be useful to find interactions between proteins. Note that we can se-lect the same categories as cube X  X  axes unlike the traditional multidimensional database.

Other operations are similarly defined, although they are omitted because of lack of space.
 By using above operators, we will show how common OLAP operators can be defined. roll-up &amp; drill-down: In the traditional multidimensional database, there are two types of rolling up operation, one is dimensional rolling up and the other is hierarchical rolling up. For example, let S be the fact table S ( product, city, time, 458 sale ), where sale is a measure, and L ( city, state, country ) be one of the dimension tables. The dimensional rolling up case of one dimension being dropped, and city  X  city,SU M in the case of two dimensions being dropped 3 . The hierar-chical rolling up is represented as possible to define more than 2 k roll-up queries for the k di-mensions of the traditional multidimensional database. In our case, the dimensional rolling up corresponds to moving from  X  [ c 1 ,  X  X  X  ,c q ,g 1 ,  X  X  X  ,g q , count ]( D )into  X  [ c c of one dimension being dropped, and the hierarchical rolling up corresponds to moving from  X  [ c 1 ,  X  X  X  ,c q ,g (1) 1 count ]( D )to  X  [ c 1 ,  X  X  X  ,c h ,c h +1 ,c h +2 ,  X  X  X  ,c g slice &amp; dice: The slice operation performs a selection on one dimension of the given cube, resulting in a subcube, and the dice operation defines a subcube by performing a selec-tion on two or more dimension. For example, in the tradi-tional multidimensional database, slice and dice operations P =( product  X  X  p 1 ,p 2 } and city  X  X  c 3 ,c 4 } ). In our case, the slice is represented as  X  [ T, count ](  X  p ( D )), and the dice pivot: The pivot operation is a visualization operation that rotates the data axes in view in order to provide an alterna-tive presentation of the data, which corresponds to moving from  X  [ c 1 ,c 2 ,g 1 ,g 2 , count ]into  X  [ c 2 ,c 1 ,g
A key strategy for speeding up cube view processing for the traditional multidimensional database is to use pre-computed cube views. The pre-computation reduces the response times to queries potentially involving huge amounts of data and al-lows interactive data analysis in the traditional approaches. However, it is impossible to pre-compute or pre-aggregate in advance of receiving queries for all of the combinations of values in our situation, because the situation where each document has many values and there are a lot of categories is combinatorially explosive. For example, the average num-ber of annotated terms which each documents have is about 380 and the number of categories is more than 240,000 for the data used in Section 5.

In this section, we design table schemas and data struc-tures to achieve query response times that are as fast as possible. Since a hierarchy for analyzed documents consti-tutes a transitive anti-closed digraph rather than a set of balanced and non-ragged trees, it cannot be stored in a star schema or snowflake schema. For computation efficiency in aggregating the distributions of documents, the hierarchy is indexed as follows. A depth first search traverses the hi-erarchy from root category c root whose type Type ( c root equal to T assigning a preorder, postorder, and depth to each category, and it backtracks if and only if it reaches leaf nodes. This means that it does not backtrack when it reaches any internal nodes which it has already visited.
The assigned preorders and postorders make it possible to  X  a,sum ( b ) represents an SQL query  X  X ELECT a, SUM(b) FROM ... GROUP BY a X . handle ancestor-descendant containment in the hierarchy [7]. In other words, it can check the containment by assigning a preorder and a postorder to each node in a hierarchy and comparing the preorder and postorder values assigned to given two nodes. If a node A is an ancestor of a node B, A spreorder&lt;B spreorder &amp; A spostorder&gt;B spostorder. We use the preorder-postorder method to index the category hierarchy, since we assume that updates to a category hierar-chy (ontology) happens much less frequently than document insertions that the cost of preorder and postorder recalcula-tions is negligible.

Example 6: The hierarchy in Figure 7 (a) is traversed to return a tree shown in Figure 7 (b) where each node has a category, an assigned preorder, postorder, and depth. In this figure, all descendants of a category c 2 have preorders which are greater than the preorder of c 2 and have pos-torders which are less than the postorder of c 2 .Wecallthe tree in Figure 7 (b) a traversed tree.

We define two tables, CATEGORY H and KEYWORD V ,tostore the traversed tree and fact-hierarchy relations as CATEGORY (CATEGORYNAME CHARCTER, KEYWORD (ID INTEGER, respectively. Each record in the table H corresponds to a node in the traversed tree and CATEGORYNAME , PATH , PREORDER1 , PREORDER2 ,and PARENT in H are a name of the category, a path from the root node to the corresponding node, a pre-order, a value for a postorder plus a depth, and a preorder of its parent of the corresponding node. The reason why we use PREORDER2 instead of the postorder is that we can check ancestor-descendant containment in the hierarchy as
A spreorder 1(= A spreorder ) &lt;B spreorder instead of using condition (1). Each record in the table V corresponds to ( f, c : v )in R ,and ID , PREORDER ,and VALUE in the table V are a document ID f , a preorder of the category c , and a value v in dom ( c ), respectively.

By using these tables, we can implement the operations introduced in Section 3.2. In the following definitions, c is provided as input. Although there are multiple records whose values of CATEGORYNAME in H are c , a record arbitrarily chosen from the records is used in the following operations. In other words,  X   X  P ( H ) X  for P =( categoryname = c )is 459 g (1) ( c )=  X  g (2) ( c )=  X  g (3) ( c )=  X  Figure 8: Implementation of User-defined Functions as H c . The choice has no influence on its result. selection  X  : The selection is defined as  X  c : v ( D )=( H, V ), where V =  X  id in I ( V ), I =  X  id ( V P H c )and P = ( preorder 1  X  preorder  X  preorder 2 and value = v ). In the definition, the condition (2) is used as  X  preorder 1 preorder  X  preorder 2 X . Although this calculation needs to join H with V , this calculation runs as fast as the selection of V , since only one record is returned from H c . difference  X  : The difference is defined as ( H, V 1 )  X  ( H, V projection  X  : The projection is defined as  X  c ( D )=( H, V ),  X  preorder  X  preorder 2). aggregation  X  : The aggregation is defined as  X  [( c 1 ,  X  X  X  ,c q ,g 1 ,  X  X  X  ,g q ) , count ]( D ) =
Example 7: Figure 9 shows how to compute  X  [ Software ,g (2) , count ]( D ) for the data shown in Fig-tions A 1 of the category  X  X oftware X  and its child categories. combinations A 2 of the child categories and the correspond-ing document IDs. The output contains a tuple in the first row, because  X  X indows 2000 X  is a descendant of  X  X S X  in the category hierarchy in Figure 4 and preorders for  X  X in-dows 2000 X  and  X  X S X  fulfill the condition (2). Finally, the ) for the immediate subcategories of  X  X oftware X  is returned. For testing, we used biomedical documents from MED-LINE (http://www.nlm.nih.gov/databases/databases medline.html). Life science researchers typically use MED-LINE, a bibliography database that covers the biomedical area. MEDLINE is administered by the National Center for Biotechnology Information (NCBI: http://www.ncbi.nlm.nih. gov) of the United States National Library of Medicine (NLM: http://www.nlm.nih.gov/). It contains approximately 17 million biomedical citations, dating from the mid-1960s up to the present time. Citations in MEDLINE are collected from over 5,000 biomedical journals published worldwide. Biomedical citations in MEDLINE are available to the gen-eral public at the PubMed (http://www.ncbi.nlm.nih.gov /entrez). We selected 503,989 abstracts from Medline which contain structured information such as authors and Mesh Terms and unstructured information such as titles and ab-stracts.

To prepare a fact-hierarchy relation from the documents, the documents written in English are parsed by CCAT [5], a shallow syntactic parser. Because this is a general-purpose parser that has not been trained for biomedical documents, it is difficult to obtain optimized results by parsing docu-ments from various domains [27]. We solve this problem by first annotating the text with domain dictionaries. The an-notations facilitate the parsing of the documents even when the parser has not been specifically trained for the domains.
In the first step of the preprocessing, the term annota-tor finds words in the input text using the term dictionary and identifies these words with their canonical forms. The term dictionary contains pairs of surface forms and canoni-cal forms. Fox example, most of the technical terms in the medical domain are compound words. The compound noun  X  X epetitive sequence-based polymerase chain reaction X  con-sists of an adjective (repetitive), a past participle of a verb (sequence-based) and three nouns (polymerase, chain, reac-tion). Thus, biomedical terms tend to consist of a combina-tion of numerals, symbols, and verbs, making it very difficult to find term boundaries. In addition, there are often con-siderable numbers of expressions that are synonymous with a particular technical term. These can arise from abbrevia-tions or acronyms as well as from spelling variations. If these variations are recognized as different entities, it can often cause problems when aggregating unstructured information in documents. For instance,  X  X NA X  and  X  X eoxyribonucleic acid X  are synonyms. The dictionary contains spelling and abbreviation variants and their canonical forms. By reduc-ing these variants to a single canonical form, we treat them as the same entity.

In the second step, the text annotated with a technical term dictionary is passed to the syntactic parser. The parser outputs segments of phrases labeled with their syntactic roles, for example NP (noun phrase) or VG (verb group). In the third step, the category annotator assigns categories to the terms in these segments and phrases. The category dictionary consists of a set of canonical forms and their cat-egories, which also indicate the node labels in the category hierarchy (ontology). A category assigned to each term is an internal node or leaf in the hierarchy.

Example 8: Figure 10 shows an example of the pre-processing of a sentence. When  X  X epetitive sequence-based polymerase chain reaction effects deoxyribonucleic acids X  is given as input, an annotator assigns  X  X NA X  as canonical and  X  X roper noun X  as part-of-speech to  X  X eoxyribonucleic acids X . After parsing the annotated text, categories are assigned to each term. In Figure 10,  X .A.1.2.23.4 X  represents PATH from a root node to the corresponding node in a category hierarchy.
After preprocessing the 503,989 abstracts, the numbers of ( f, c : v ), records in the table H , and distinct canonical forms of terms were 193185919, 340154, and 13640593, re-spectively. The categories contain categories for publication dates, authors, affiliations for the authors, and so on. replaced into  X   X  P ( H ) FETCH FIRST 1 ROWS ONLY  X , denoted 460
To compare with the method mentioned in Section 4, we used a method with a Document-Term Matrix (DTM) as a proprietary method. In general, a proprietary algorithm and index can compute faster than a method with a persis-tent store, e.g., the method in Section 4, although it is more difficult to add some functions into the proprietary method and to integrate the proprietary method with other systems compared to the persistent-store method. We explain the method using the proprietary algorithm and index, and the next subsection will explain that the method using the per-sistent store is comparable to the proprietary method.
We focus how to compute  X  [ c, g, count ](  X  P ( D )) by a DTM using a simple example, because this is the most fundamen-tal computation. Let the sets of terms and documents be ADTMisamatrix M =( m ij )of m  X  n , and an elements m ij represents how many times the term t j appears in the document d i . Although storing the whole matrix requires a lot of memory, it can be compressed by storing a pair for each element that is not zero and its corresponding index in each row or column, because the matrix is very sparse.
As mentioned in the previous section, since some cate-gories are assigned to each term, we use a modified DTM. Rows in our DTM correspond to a set of documents D = { d 1 ,...,d m } similar to the conventional DTM, and columns correspond to a set of pairs of categories and terms P = { v jk | v jk  X  dom ( c j ) ,j =1 ,  X  X  X  ,n } X  X  c j :  X  X  j =1 , The value c j :  X  is used to facilitate aggregating the number of documents for each subcategory, and an element for d i and c j :  X  is not zero when d i c j :  X  .

Figure 11 presents how the method using DTM computes has specified the category c 3 after narrowing down to the documents containing the term v 11 whose category is c 1 . First, the method narrows th e search down to the document this Process 1, a set of terms { c 3 : v 31 ,c 3 : v 32 ,c c : v 35 } whose category is c 3 is output (2). After Pro-cesses 1 and 2, the distribution of the documents for the terms appearing in the documents { d 2 ,d 6 ,d 10 } is returned as { ( c 3 : v 31 ):2 , ( c 3 : v 35 ):1 } (3). Process 3 requires much more computation time than Processes 1 and 2. For ex-ample, when the user selects a  X  X ommon noun X  category, Process 2 returns 340,154 terms for the dataset used in the experiments described in Section 5.3.

When the user specified a category which is an internal node in the category tree, it also computes the distribution of the documents for each subcategory of the specified cat-egory, which corresponds to  X  [ c 3 ,g (2) , count ](  X  P this case, a set of categories { c 3 :  X  X  c 3  X  succ ( c 3 turned in Process 2.
The method in Section 4 was implemented in Java. It generates SQL queries and accesses a relational database via JDBC (Java Database Connectivity). The method in Section 5.2 was implemented in C++ to compare with the above method. For the evaluation, an IBM IntelliStation with Windows XP, an Opteron-2.2 GHz CPU, and 2 GB of main memory was used. The efficiency of our approach has been confirmed with respect to the computation time.
Figure 12 shows the results of the response time for a query  X  [ c, g, count ]( D ) for all the 340,154 categories. The DB and DTM in the figure correspond to implementations of the methods mentioned in Section 4 and Section 5.2, re-spectively. KW and SUB represent the cases where g (1) and g (2) as g in  X  [ c, g, count ]( D ) were used, respectively.  X  X TM SUB 1k X  shows the results for 1,000 documents sam-pled randomly from all of the documents. The method of the DB does not compute for the sampled documents but for all of the documents. Each point ( x, y ) in the figure means that the method returns the result within x seconds for y % of all of the 340,154 categories. The ideal method is at the upper left corner. While DB could return the result for about 89% of the categories within 0.1 second, DTM could only return results for about 60% of the categories for 1,000 sampled documents, and for about 0.01% of cat-egories for 10,000 sampled documents for KW. In addition,  X  [ c, g, count ](  X  : cancer ( D )) was calculated for the various categories, and the results for 11,914 documents containing  X  X ancer X  were similar to Figure 12. As shown in Figure 12, DB is superior to DTM for most of the categories.
From another viewpoint of the empirical 8-second rule say-ing that a webpage should be loaded within 8 seconds of a request [20], we can conclude that the better method is the one whose coverage rate in about 10 seconds is better. Fig-ure 13 shows the coverage rates of DB and DTM -between 99.97% and 100% in 5-10 second response time for the query  X  [ c, g, count ](  X  : cancer ( D )). Figure 13 shows that DB is inferior to DTM within 8-second constraint. Tables 3 and 4 summarize the experimental results for  X  [ c, g, count ]( D ) and  X  [ c, g, count ](  X  : cancer ( D )), respectively. Although the average computation time of DB is lower than for DTM, the number of categories for which DB cannot return within 10 seconds may become greater than for DTM.

As shown in Table 4 and Figure 13, the averages of the 461 Figure 13: Experimental Results for Documents Containing  X  X ancer X  computation times for DB are superior to DTM. However, the number of categories for which DB cannot respond within 10 seconds is greater than for DTM. When documents con-taining  X  X ancer X  are selected, an SQL query to obtain V for  X  : cancer ( D )=( H, V ) is represented as where I =  X  id ( V P 1 H c ), P 1 =( value = cancer ), P 2 ( V ( a ) .id = V ( b ) .id and V ( a ) .value = cancer ), and V = V a ) = V ( b ) . The query (3) is represented as # 10 means the number of categories for which the results are not returned within 10 seconds.
 Table 4: Computation Times for Documents Con-taining  X  X ancer X  Figure 14: Computation Times for KEYWORD Di-vided into 10 Tables The reason why DB requires so much computation time for certain categories is the self-join of the table KEYWORD V which contains 193,185,919 records. Therefore, we divided the table into multiple tables in preprocess .Let id ( V )bea function which returns a set of document IDs in a table V . We divide V into multiple tables V i which satisfies id ( V )= S i id ( V i )and id ( V i ) query (3) contains V ( a ) .id = V ( b ) .id in its WHERE clause, the following SQL query can avoid joining tables that do not contain the same documents IDs. sents UNION ALL operation, and each V i is calculated by the SQL query (3).

Figure 14 shows the results when we compared the aggre-gation with KEYWORD divided into 10 tables with DTM and the aggregation using a single table for KEYWORD . By divid-ing the table, the coverage rate within 10 seconds rises from 99.993% to 99.999% for KW and from 99.79% to 99.93% for SUB. Although these experiments were run with a single computer, we can easily run on multiple computers, because such commercial database systems support parallelization.
Figure 15 shows the average computation time for the number of documents for all of the categories. The com-putation time is observed to be proportional to the number of documents. The high scalability of our method has been confirmed for the amount of data. 462 Figure 15: Average Computation Time for the Number of Documents
A top N ranking query often returns a trivial result, which contains only frequent terms in the documents. To measure more informative terms that could show strong relevance to a given subset of documents, the following relative frequency is used [27]. This measure compares the current document subset to the initial document set. Assume D is the initial document set. A selection operation due to query c : v re-turns D s . The relative frequency for a term p jk =( c j in the document set D s is calculated as relative frequency ( p jk ,D s )=( c ( p jk ,D s ) | where c ( p jk ,D ) is the number of documents that contain the term p jk in the set D . For example in Figure 11 in Section 5.2, relative frequency (( c 3 : v 31 ) ,D s )is 2 where D s is { d 2 ,d 6 ,d 10 } .

It is very critical to achieve query response times that are as fast as possible for interactively analyzing a huge amount of text data. A key strategy for speeding up to aggregate the data is to use indexing technology. As mentioned in Sec-tion 4, we use the preorder and postorder to check ancestor-descendant containment in a category hierarchy. If we do not use preorder and postorder in a traversed tree, we need to join the table CATEGORY n times to check where a node A is an ancestor of a node B ,where n is the length of a path from the A to the B . However, we do not need any join operations to ancestor-descendant containment in the hierarchy. The method to index the tree was proposed in 1982 [7], and it recently draws attention as the method to index XML (eXtensible Markup Language) database and to map XML data into the relational database [11], since each XML document is modeled as a DOM (Document Object Model) tree. Several methods such as prefix label [6], Dewey order [26], prime label [28], VLEI code [14], embedding into a k-ary tree [15] are used to index XML. A disadvantage of the methods such as preorder-postorder method and prime label is to need a re-assignment of preorder and postorder of nodes when inserting some nodes into a tree. Since each node has the same label as a prefix of its children in the methods such as prefix label and Dewey order, they do not need to be reassigned the labels when inserting some nodes. However, because they need to compute functions to pro-cess string to check ancestor-descendant containment, they need more computation time than the preorder-postorder method. Since we assume that a category hierarchy is far less frequently updated than new records are inserted into the table V , we used the preorder-postorder method to index the category hierarchy.

Our data representation is similar to the bag-of-words ap-proach [16], although each term is assigned categories. In the bag-of-words approach, the following sentences are treated as the same content,:  X (a) X did fail X ,  X (b) X did not fail X , and  X (c) Did X fail? X  [19]. Besides the negation and in-terrogative mood, some auxiliary verbs such as  X  X an X  and some verbs such as  X  X ant X  often indicate the author X  X  com-municative intentions. It is important to associate commu-nicative intentions with predicates by analyzing grammat-ical features and lexical information.  X  X ail X  in the previous examples (a) to (c) are assigned categories and stored in R as (a) complaint:fail, (b) commendation:not fail, and (c) ques-tion:fail. These distinctions are instrumental in facilitating problem detection and workload reduction for analysts at customer help centers, for example.

Some papers such as [17] and [18] proposed OLAP systems to analyze a set of documents. The following MDX (Multi-Dimensional eXpression) query which was used in [17] finds all the documents which contain a term  X  X orests X  and are published in New York in the first quarter of 1998.
SELECT not empty [DocId].members on rows, FROM docInfo
WHERE ([Term].[forest],[1998][quarter 1], In the query, [Term].[forest] means that a depth of a hierarchy in a TERM dimension is 2, and term  X  X orests X  is a child node of  X  X ERM X  corresponding to in the TERM dimension. Hierarchies that the existing OLAP systems for texts assume are so simple that it is difficult to integrate the hierarchies with a complex ontology with a huge set of nodes. Fagin et al. proposed Multi-Structural Database (MSDB) to support efficient analysis of large, complex data set. Our method provides a user interactive analysis, although MSDB can semi-automatically segment the data by using analytics operators among some correlated dimensions. In addition, we demonstrated our method by much larger set of data than those of MSDB.

Other papers proposed an OLAP system to analyze a set of documents [25, 24]. Since sales figures mentioned in mul-tiple documents (newspapers) would be a fact in the papers, operations in the system is similar to the conventional OLAP system for structured data. An example operation in the system is to analyze the average sales per product and year. In our case, since a document would be a fact, we can find a change in the number of documents with time. For example, in a call center in a company, call takers make reports of each call by typing in customer information such as name and phone number, selecting call categories such as  X  X echnical QA X  and typing in brief descriptions of questions or mes-sages from the customer and brief descriptions of answers and/or actions taken. The brief descriptions are written in natural language. The manager of the call center wants to improve productivity, reduce cost, improve customer satis-faction, etc. For example, in a large number of documents related to customers X  calls, he or she would like to find what kinds of topics have recently been increasingly mentioned and which product is associated with specific topics, so that we can take appropriate actions for the improvement of call center productivity and product quality, or create a FAQ (frequently asked question) database. 463
Our method relies on natural language processing and in-formation extraction, and to build a general purpose extrac-tion module is not a trivial task. IBM makes UIMA (Un-structured Information Management Architecture, http:// uima-framework.sourceforge.net/) available as an open source software development kit [9]. The UIMA framework is an open, scalable and extensible platform for building analytic applications or search solutions that process text or other unstructured information. It enables developers to build an-alytic modules, e.g. entity extraction modules. It can also enable us to extract various technical and meaningful terms to analyze a huge set of documents by composing modules from multiple annotator providers.
In this paper, we proposed a data representation and its algebra operations to integrate ontologies with OLAP sys-tems to analyze a huge set of textual documents. By using our method, two types of information (structured and un-structured information) can mutually enhance information discovery and analysis capability. The proposed method was implemented with a persistent store using preorder and pos-torder in a hierarchy. The efficiency of our approach has been confirmed with respect to the computation time. Our method is so efficient and robust that it enables an analyst to interactively analyze a large amount of text data for more context-oriented analysis and decision-making.
We thank members of text mining group at Tokyo Re-search Laboratory, IBM Japan for their supports and ad-vices. [1] S. Agarwal et al. On the Computation of Multi-[2] P. Baumann et al. Spatio-Temporal Retrieval with [3] P. Bonatti et al. An Ontology-extended Relational [4] V. Chakaravarthy et al. Efficiently Linking Text [5] E. Charniak. Statistical Language Learning .TheMIT [6] E. Cohen et al. Labeling Dynamic XML Trees. Proc. [7] P. Dietz. Maintaining Order in a Linked List. Proc. of [8] R. Fagin et al. Multi-Structural Databases. Proc. of [9] D. Ferrucci &amp; A. Lally. Building an Example [10] S. Goil &amp; A. N. Choudhary. High Performance Multi-[11] T. Grust. Accelerating XPath Location Steps. Proc. of [12] H. Gupta et al. Index Selection for OLAP. Proc. of [13] M. Gyssens &amp; L. Lakshmanan. A Foundation for [14] K. Kobayashi et al. VLEI code: An Efficient Labeling [15] Y. Lee et al. Index Structures for Structured [16] C. Manning &amp; H. Sch  X  u tze. Foundations of Statistical [17] M. McCabe et al. On the Design and Evaluation of a [18] J. Mothe et al. DocCube: Multi-Dimensional [19] T. Nasukawa &amp; T. Nagano. Text Analysis and [20] J. Nielsen. Designing Web Usability: The Practice of [21] T. Niemi et al. Logical Multidimensional Database [22] T. Pedersen &amp; C. Jensen. Multidimensional Data [23] T. Pedersen &amp; C. Jensen. Multidimensional Database [24] J. P  X  erez et al. A Relevance-extended Multi-[25] J. P  X  erez et al. IR and OLAP in XML Document [26] I. Tatarinov et al. Storing and Querying Ordered [27] N. Uramoto et al. A Text-Mining System for [28] X. Wu et al. A Prime Number Labeling Scheme for
