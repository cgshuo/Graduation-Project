 Lexical resources are repositories of machine-readable knowledge that can be used in virtually any Natural Language Processing task. Notable examples are WordNet, Wikipedia and, more re-cently, collaboratively-curated resources such as OmegaWiki and Wiktionary (Hovy et al., 2013). On the one hand, these resources are heteroge-neous in design, structure and content, but, on the other hand, they often provide complemen-tary knowledge which we would like to see inte-grated. Given the large scale this intrinsic issue can only be addressed automatically, by means of lexical resource alignment algorithms. Owing to its ability to bring together features like multilin-guality and increasing coverage, over the past few years resource alignment has proven beneficial to a wide spectrum of tasks, such as Semantic Pars-ing (Shi and Mihalcea, 2005), Semantic Role La-beling (Palmer et al., 2010), and Word Sense Dis-ambiguation (Navigli and Ponzetto, 2012).

Nevertheless, when it comes to aligning textual definitions in different resources, the lexical ap-proach (Ruiz-Casado et al., 2005; de Melo and Weikum, 2010; Henrich et al., 2011) falls short because of the potential use of totally different wordings to define the same concept. Deeper ap-proaches leverage semantic similarity to go be-yond the surface realization of definitions (Nav-igli, 2006; Meyer and Gurevych, 2011; Niemann and Gurevych, 2011). While providing good re-sults in general, these approaches fail when the definitions of a given word are not of adequate quality and expressiveness to be distinguishable from one another. When a lexical resource can be viewed as a semantic graph, as with WordNet or Wikipedia, this limit can be overcome by means of alignment algorithms that exploit the network structure to determine the similarity of concept pairs. However, not all lexical resources provide explicit semantic relations between concepts and, hence, machine-readable dictionaries like Wik-tionary have first to be transformed into semantic graphs before such graph-based approaches can be applied to them. To do this, recent work has pro-posed graph construction by monosemous linking, where a concept is linked to all the concepts asso-ciated with the monosemous words in its definition (Matuschek and Gurevych, 2013). However, this alignment method still involves tuning of parame-ters which are highly dependent on the character-istics of the generated graphs and, hence, requires hand-crafted sense alignments for the specific pair of resources to be aligned, a task which has to be replicated every time the resources are updated.
In this paper we propose a unified approach to aligning arbitrary pairs of lexical resources which is independent of their specific structure. Thanks to a novel modeling of the sense entries and an effective ontologization algorithm, our ap-proach also fares well when resources lack rela-tional structure or pair-specific training data is ab-sent, meaning that it is applicable to arbitrary pairs without adaptation. We report state-of-the-art per-formance when aligning WordNet to Wikipedia, OmegaWiki and Wiktionary. Preliminaries. Our approach for aligning lexi-cal resources exploits the graph structure of each resource. Therefore, we assume that a lexical resource L can be represented as an undirected graph G = ( V,E ) where V is the set of nodes, i.e., the concepts defined in the resource, and E is the set of undirected edges, i.e., seman-tic relations between concepts. Each concept c  X  V is associated with a set of lexicalizations L G ( c ) = { w 1 ,w 2 ,...,w n } . For instance, Word-Net can be readily represented as an undirected graph G whose nodes are synsets and edges are modeled after the relations between synsets de-fined in WordNet (e.g., hypernymy, meronymy, etc.), and L G is the mapping between each synset node and the set of synonyms which express the concept. However, other resources such as Wik-tionary do not provide semantic relations between concepts and, therefore, have first to be trans-formed into semantic networks before they can be aligned using our alignment algorithm. We ex-plain in Section 3 how a semi-structured resource which does not exhibit a graph structure can be transformed into a semantic network.
 Alignment algorithm. Given a pair of lexical resources L 1 and L 2 , we align each concept in L 1 by mapping it to its corresponding concept(s) in the target lexicon L 2 . Algorithm 1 formalizes the alignment process: the algorithm takes as input the semantic graphs G 1 and G 2 corresponding to the two resources, as explained above, and produces as output an alignment in the form of a set A of concept pairs. The algorithm iterates over all con-cepts c 1  X  V 1 and, for each of them, obtains the set of concepts C  X  V 2 , which can be considered as alignment candidates for c 1 (line 3). For a concept c , alignment candidates in G 2 usually consist of every concept c 2  X  V 2 that shares at least one lex-icalization with c 1 in the same part of speech tag, i.e., L G Meyer and Gurevych, 2011). Once the set of target candidates C for a source concept c 1 is obtained, the alignment task can be cast as that of identifying those concepts in C to which c 1 should be aligned. To do this, the algorithm calculates the similarity between c 1 and each c 2  X  C (line 5). If their sim-ilarity score exceeds a certain value denoted by  X  Algorithm 1 Lexical Resource Aligner (line 6), the two concepts c 1 and c 2 are aligned and the pair ( c 1 ,c 2 ) is added to A (line 7).
Different resource alignment techniques usually vary in the way they compute the similarity of a pair of concepts across two resources (line 5 in Al-gorithm 1). In the following, we present our novel approach for measuring the similarity of concept pairs. 2.1 Measuring the Similarity of Concepts Figure 1 illustrates the procedure underlying our cross-resource concept similarity measurement technique. As can be seen, the approach consists of two main components: definitional similarity and structural similarity . Each of these compo-nents gets, as its input, a pair of concepts belong-ing to two different semantic networks and pro-duces a similarity score. These two scores are then combined into an overall score (part (e) of Figure 1) which quantifies the semantic similarity of the two input concepts c 1 and c 2 .

The definitional similarity component computes the similarity of two concepts in terms of the simi-larity of their definitions, a method that has also been used in previous work for aligning lexical resources (Niemann and Gurevych, 2011; Hen-rich et al., 2012). In spite of its simplicity, the mere calculation of the similarity of concept defi-nitions provides a strong baseline, especially for cases where the definitional texts for a pair of concepts to be aligned are lexically similar, yet distinguishable from the other definitions. How-ever, as mentioned in the introduction, definition similarity-based techniques fail at identifying the correct alignments in cases where different word-ings are used or definitions are not of high qual-ity. The structural similarity component, instead, is a novel graph-based similarity measurement technique which calculates the similarity between a pair of concepts across the semantic networks of the two resources by leveraging the semantic structure of those networks. This component goes beyond the surface realization of concepts, thus providing a deeper measure of concept similarity.
The two components share the same backbone (parts (b) and (d) of Figure 1), but differ in some stages (parts (a) and (c) in Figure 1). In the follow-ing, we explain all the stages involved in the two components (gray blocks in the figure). 2.1.1 Semantic signature generation The aim of this stage is to model a given concept or set of concepts through a vectorial semantic representation, which we refer to as the seman-tic signature of the input. We utilized Person-alized PageRank (Haveliwala, 2002, PPR ), a ran-dom walk graph algorithm, for calculating seman-tic signatures. The original PageRank ( PR ) algo-rithm (Brin and Page, 1998) computes, for a given graph, a single vector wherein each node is as-sociated with a weight denoting its structural im-portance in that graph. PPR is a variation of PR where the computation is biased towards a set of initial nodes in order to capture the notion of im-portance with respect to those particular nodes. PPR has been previously used in a wide variety of tasks such as definition similarity-based resource alignment (Niemann and Gurevych, 2011), textual semantic similarity (Hughes and Ramage, 2007; Pilehvar et al., 2013), Word Sense Disambigua-tion (Agirre and Soroa, 2009; Faralli and Navigli, 2012) and semantic text categorization (Navigli et al., 2011). When applied to a semantic graph by initializing the random walks from a set of con-cepts (nodes), PPR yields a vector in which each concept is associated with a weight denoting its semantic relevance to the initial concepts.

Formally, we first represent a semantic network consisting of N concepts as a row-stochastic tran-sition matrix M  X  R N  X  N . The cell ( i,j ) in the matrix denotes the probability of moving from a concept i to j in the graph: 0 if no edge exists from i to j and 1 /degree ( i ) otherwise. Then the PPR vector, hence the semantic signature S v of vector v is the unique solution to the linear sys-tem: S v = (1  X   X  ) v +  X  M S v , where v is the personalization vector of size N in which all the probability mass is put on the concepts for which a semantic signature is to be computed and  X  is the damping factor, which is usually set to 0.85 (Brin and Page, 1998). We used the UKB 1 off-the-shelf implementation of PPR .
 Definitional similarity signature. In the defini-tional similarity component, the two concepts c 1 and c 2 are first represented by their corresponding definitions d 1 and d 2 in the respective resources L 1 and L 2 (Figure 1(a), top). To improve expressive-ness, we follow Niemann and Gurevych (2011) and further extend d i with all the word forms asso-ciated with concept c i and its neighbours, i.e., the union of all lexicalizations L G x  X  X  c 0  X  V i : ( c,c 0 )  X  E i } X  X  c } , where E i is the set of edges in G i . In this component the person-alization vector v i is set by uniformly distributing the probability mass over the nodes correspond-ing to the senses of all the content words in the extended definition of d i according to the sense inventory of a semantic network H . We use the same semantic graph H for computing the seman-tic signatures of both definitions. Any semantic network with a dense relational structure, provid-ing good coverage of the words appearing in the definitions, is a suitable candidate for H . For this purpose we used the WordNet (Fellbaum, 1998) graph which was further enriched by connecting each concept to all the concepts appearing in its Structural similarity signature. In the struc-tural similarity component (Figure 1(b), bottom), the semantic signature for each concept c i is com-puted by running the PPR algorithm on its corre-sponding graph G i , hence a different M i is built for each of the two concepts. 2.1.2 Signature unification As mentioned earlier, semantic signatures are vec-tors with dimension equal to the number of nodes in the semantic graph. Since the structural similar-ity signatures S v ent graphs and thus have different dimensions, we need to make them comparable by unifying them. We therefore propose an approach (part (c) of Fig-ure 1) that finds a common ground between the two signatures: to this end we consider all the concepts associated with monosemous words in the two signatures as landmarks and restrict the two signatures exclusively to those common con-cepts. Leveraging monosemous words as bridges between two signatures is a particularly reliable technique as typically a significant portion of all
Formally, let I G ( w ) be an inventory mapping function that maps a term w to the set of con-cepts which are expressed by w in graph G . Then, given two signatures S v the respective graphs G 1 and G 2 , we first obtain the set M of words that are monosemous accord-ing to both semantic networks, i.e., M = { w : |I
G 1 ( w ) | =1  X |I G 2 ( w ) | =1 } . We then transform each of the two signatures S v of the only concept of w k in I G ple, assume we are given two semantic signatures computed for two concepts in WordNet and Wik-tionary. Also, consider the noun tradeoff which is monosemous according to both these resources. Then, each of the two unified sub-signatures will contain a component whose weight is determined by the weight of the only concept associated with tradeoff n in the corresponding semantic signature. As a result of the unification process, we obtain a pair of equally-sized semantic signatures with comparable components. 2.1.3 Signature comparison Having at hand the semantic signatures for the two input concepts, we proceed to comparing them (part (d) in Figure 1). We leverage a non-parametric measure proposed by Pilehvar et al. (2013) which first transforms each signature into a list of sorted elements and then calculates the similarity on the basis of the average ranking of elements across the two lists: where T is the intersection of all concepts with non-zero probability in the two signatures and r j is the rank of the i th entry in the j th sorted list. The denominator is a normalization factor to guar-antee a maximum value of one. The method pe-nalizes the differences in the higher rankings more than it does for the lower ones. The measure was shown to outperform the conventional cosine dis-tance when comparing different semantic signa-tures in multiple textual similarity tasks (Pilehvar et al., 2013). 2.1.4 Score combination Finally (part (e) of Figure 1), we calculate the overall similarity between two concepts as a lin-ear combination of their definitional and struc-tural similarities:  X  Sim def ( S v  X  ) Sim str ( S v how we set, in our experiments, the values of  X  and the similarity threshold  X  (cf. alignment algo-rithm in Section 2). In Section 2, we presented our approach for align-ing lexical resources. However, the approach as-sumes that the input resources can be viewed as semantic networks, which seems to limit its ap-plicability to structured resources only. In or-der to address this issue and hence generalize our alignment approach to any given lexical resource, we propose a method for transforming a given machine-readable dictionary into a semantic net-work, a process we refer to as ontologization .
Our ontologization algorithm takes as input a lexicon L and outputs a semantic graph G = ( V,E ) where, as already defined in Section 2, V is the set of concepts in L and E is the set of seman-tic relations between these concepts. Introducing relational links into a lexicon can be achieved in different ways. A first option is to extract binary relations between pairs of words from raw text. Both words in these relations, however, should be disambiguated according to the given lexicon (Pantel and Pennacchiotti, 2008), making the task particularly prone to mistakes due to the high num-ber of possible sense pairings.

Here, we take an alternative approach which requires disambiguation on the target side only, hence reducing the size of the search space sig-nificantly. We first create the empty undirected graph G L = ( V,E ) such that V is the set of con-cepts in L and E =  X  . For each source con-cept c  X  V we create a bag of content words W = { w 1 ,...,w n } which includes all the con-tent words in its definition d and, if available, ad-ditional related words obtained from lexicon rela-tions (e.g., synonyms in Wiktionary). The prob-lem is then cast as a disambiguation task whose goal is to identify the intended sense of each word w i  X  W according to the sense inventory of L : if w i is monosemous, i.e., |{I G L ( w i ) }| = 1 , we con-nect our source concept c to the only sense c w w i and set E := E  X  X { c,c w i }} ; else, w i has mul-tiple senses in L . In this latter case, we choose the most appropriate concept c i  X  X  G the maximal similarity between the definition of c and the definitions of each sense of w i . To do this, we apply our definitional similarity measure intro-duced in Section 2.1. Having found the intended sense  X  c w As a result of this procedure, we obtain a semantic graph representation G for the lexicon L .
 noun cone in Wiktionary (i.e., cone 4 n ) which is de-fined as  X  X he fruit of a conifer X  . The definition contains two content words: fruit n and conifer n . The latter word is monosemous in Wiktionary, hence we directly connect cone 4 n to the only sense of conifer n . The noun fruit , however, has 5 senses in Wiktionary. We therefore measure the similar-ity between the definition of cone 4 n and all the 5 definitions of fruit and introduce a link from cone 4 n to the sense of fruit which yields the maximal similarity value (defined as  X (botany) The seed-bearing part of a plant... X  ). Lexical resources. To enable a comparison with the state of the art, we followed Matuschek and Gurevych (2013) and performed an align-ment of WordNet synsets ( WN ) to three different collaboratively-constructed resources: Wikipedia ( WP ), Wiktionary ( WT ), and OmegaWiki ( OW ). We utilized the DKPro software (Zesch et al., 2008; Gurevych et al., 2012) to access the infor-mation in the foregoing three resources. For WP , WT , OW we used the dump versions 20090822 , 20131002 , and 20131115 , respectively.
 Evaluation measures. We followed previous work (Navigli and Ponzetto, 2012; Matuschek and Gurevych, 2013) and evaluated the alignment per-formance in terms of four measures: precision, re-call, F1, and accuracy. Precision is the fraction of correct alignment judgments returned by the sys-tem and recall is the fraction of alignment judg-ments in the gold standard dataset that are cor-rectly returned by the system. F1 is the harmonic mean of precision and recall. We also report re-sults for accuracy which, in addition to true posi-tives, takes into account true negatives, i.e., pairs which are correctly judged as unaligned.
 Lexicons and semantic graphs. Here, we de-scribe how the four semantic graphs for our four lexical resources (i.e., WN , WP , WT , OW ) were constructed. As mentioned in Section 2.1.1, we build the WN graph by including all the synsets and semantic relations defined in WordNet (e.g., hypernymy and meronymy) and further populate the relation set by connecting a synset to all the other synsets that appear in its disambiguated gloss. For WP , we used the graph provided by Matuschek and Gurevych (2013), constructed by directly connecting an article (concept) to all the hyperlinks in its first paragraph, together with the category links. Our WN and WP graphs have 118K and 2.8M nodes, respectively, with the average node degree being roughly 9 in both resources.
The other two resources, i.e., WT and OW , do not provide a reliable network of semantic rela-tions, therefore we used our ontologization ap-proach to construct their corresponding semantic graphs. We report, in the following subsection, the experiments carried out to assess the accuracy of our ontologization method, together with the statistics of the obtained graphs for WT and OW . 4.1 Ontologization Experiments For ontologizing WT and OW , the bag of con-tent words W is given by the content words in sense definitions and, if available, additional re-lated words obtained from lexicon relations (see Section 3). In WT , both of these are in word sur-face form and hence had to be disambiguated. For OW , however, the encoded relations, though rela-Table 1: The statistics of the generated graphs for WT and OW . We report the distribution of the edges across types (i.e., ambiguous and un-ambiguous) and sources (i.e., definitions and rela-tions) from which candidate words were obtained. tively small in number, are already disambiguated and, therefore, the ontologization was just per-formed on the definition X  X  content words.

The resulting graphs for WT and OW contain 430K and 48K nodes, respectively, each provid-ing more than 95% coverage of concepts, with the average node degree being around 10 for both re-sources. We present in Table 1, for WT and OW , the total number of edges together with their dis-tribution across types (i.e., ambiguous and unam-biguous) and sources (i.e., definitions and rela-tions) from which candidate words were obtained.
The edges obtained from unambiguous entries are essentially sense disambiguated on both sides whereas those obtained from ambiguous terms are a result of our similarity-based disambigua-tion. Hence, given that a large portion of edges came from ambiguous words (see Table 1), we carried out an experiment to evaluate the accu-racy of our disambiguation method. To this end, we took as our benchmark the dataset provided by Meyer and Gurevych (2010) for evaluating re-lation disambiguation in WT . The dataset con-tains 394 manually-disambiguated relations. We compared our similarity-based disambiguation ap-proach against the state of the art on this dataset, i.e., the WKTWSD system, which is a WT rela-tion disambiguation algorithm based on a series of rules (Meyer and Gurevych, 2012b).

Table 2 shows the performance of our disam-biguation method, together with that of WKTWSD , in terms of Precision (P), Recall (R), F1, and ac-curacy. The  X  X uman X  row corresponds to the inter-rater F1 and accuracy scores, i.e., the upper-bound performance on this dataset, as calculated by Meyer and Gurevych (2010). As can be seen, our method proves to be very accurate, surpassing the performance of the WKTWSD system in terms of precision, F1, and accuracy. This is particularly Table 2: The performance of relation disam-biguation for our similarity-based disambiguation method, as well as for the WKTWSD system. interesting as the WKTWSD system uses a rule-based technique specific to relation disambigua-tion in WT , whereas our method is resource inde-pendent and can be applied to arbitrary words in the definition of any concept. We also note that the graph constructed by Meyer and Gurevych (2010) had an average node degree of around 1.

More recently, Matuschek and Gurevych (2013) leveraged monosemous linking (cf. Section 5) in order to create denser semantic graphs for OW and WT . Our approach, however, thanks to the con-nections obtained through ambiguous words, can provide graphs with significantly higher coverage. As an example, for WT , Matuschek and Gurevych (2013) generated a graph where around 30% of the nodes were in isolation, whereas this number drops to around 5% in our corresponding graph.
These results show that our ontologization ap-proach can be used to obtain dense semantic graph representations of lexical resources, while at the same time preserving a high level of accuracy. Now that all the four resources are transformed into semantic graphs, we move to our alignment experiments. 4.2 Alignment Experiments 4.2.1 Experimental setup Datasets. As our benchmark we tested on the gold standard datasets used in Matuschek and Gurevych (2013) for three alignment tasks: WordNet-Wikipedia ( WN -WP ), WordNet-Wiktionary ( WN -WT ), and WordNet-OmegaWiki (
WN -OW ). However, the dataset for WN -OW was originally built for the German language and, hence, was missing many English OW concepts that could be considered as candidate target alignments. We therefore fixed the dataset for the English language and reproduced the performance of previous work on the new dataset. The three datasets contained 320, 484, and 315 WN concepts that were manually mapped to their corresponding concepts in WP , WT , and OW , respectively. the art in definition similarity-based alignment approaches ( Configurations. Recall from Section 2 that our resource alignment technique has two parameters: the similarity threshold  X  and the combination pa-rameter  X  , both defined in [0, 1]. We performed experiments with three different configurations:  X  Unsupervised , where the two parameters are  X  Tuning , where we follow Matuschek and  X  Cross-validation , where a 5-fold cross vali-4.2.2 Results We show in Table 3 the alignment performance of different systems on the task of aligning WN -WP , WN -WT , and WN -OW in terms of Precision (P), Re-call (R), F1, and Accuracy. The SB system corre-sponds to the state-of-the-art definition similarity approaches for WN -WP (Niemann and Gurevych, 2011), WN -WT (Meyer and Gurevych, 2011), and WN -OW (Gurevych et al., 2012). DWSA stands for Dijkstra-WSA, the state-of-the-art graph-based alignment approach of Matuschek and Gurevych (2013). The authors also provided results for SB+Dijkstra-WSA, a hybrid system where DWSA was tuned for high precision and, in the case when no alignment target could be found, the algorithm fell back on SB judgments. We also show the re-sults for this system as SB + DWSA in the table.
For our approach (SemAlign) we show the re-sults of six different runs each corresponding to a different setting. The first three (middle part of the table) correspond to the results obtained with the three configurations of SemAlign: unsupervised, with tuning on subset, and cross-validation (see Section 4.2.1). In addition to these, we performed experiments where the two parameters of SemA-lign were tuned on pair-independent training data, i.e., a training dataset for a pair of resources dif-ferent from the one being aligned. For this setting, we used the whole dataset of the corresponding re-source pair to tune the two parameters of our sys-tem. We show the results for this setting in the bottom part of the table (last three lines).
The main feature worth remarking upon is the consistency in the results across different resource pairs: the unsupervised system gains the best re-call among the three configurations (with the im-provement over SB + DWSA being always statisti-or through cross-validation, consistently leads to the best performance in terms of F1 and accuracy (with the latter being statistically significant with respect to SB + DWSA on WN -WP and WN -WT ).
Moreover, the unsupervised system proves to be very robust inasmuch as it provides competitive results on all the three datasets, while it surpasses the performance of SB + DWSA on WN -WT . This and OmegaWiki ( WN -OW ). is particularly interesting as the latter system in-volves tuning of several parameters, whereas Se-mAlign, in its unsupervised configuration, does not need any training data nor does it involve any tuning. In addition, as can be seen in the table, SemAlign benefits from pair-independent training data in most cases across the three resource pairs with performance surpassing that of SB + DWSA , a system which is dependent on pair-specific train-ing data. The consistency in the performance of SemAlign in its different configurations and across different resource pairs indicates its robustness and shows that our system can be utilized effec-tively for aligning any pair of lexical resources, ir-respective of their structure or availability of train-ing data.

The system performance is generally higher on the alignment task for WP compared to WT and OW . We attribute this difference to the dictionary nature of the latter two, where sense distinctions are more fine-grained, as opposed to the relatively concrete concepts in the WP encyclopedia. 4.3 Similarity Measure Analysis We explained in Section 2.1 that our concept sim-ilarity measure consists of two components: the definitional and the structural similarities. Mea-suring the similarity of two concepts in terms of their definitions has been investigated in previ-ous work (Niemann and Gurevych, 2011; Hen-rich et al., 2012). The structural similarity compo-nent of our approach, however, is novel, but at the same time one of the very few measures which en-ables the computation of the similarity of concepts across two resources directly and independently of the similarity of their definitions. A comparable approach is the Dijkstra-WSA proposed by Ma-tuschek and Gurevych (2013) which, as also men-tioned earlier in the Introduction, first connects the two resources X  graphs by leveraging monosemous linking and then aligns two concepts across the two graphs on the basis of their shortest distance. To gain more insight into the effectiveness of our structural similarity measure in comparison to the Dijkstra-WSA method, we carried out an experi-ment where our alignment system used only the structural similarity component, a variant of our system we refer to as SemAlign str . Both systems (i.e., SemAlign str and Dijkstra-WSA) were tuned on 100-item subsets of the corresponding datasets.
We show in Table 4 the performance of the two systems on our three datasets. As can be seen in the table, SemAlign str consistently improves over Dijkstra-WSA according to recall, F1 and accu-racy with all the differences in recall and accu-racy being statistically significant (p &lt; 0.05). The improvement is especially noticeable for pairs in-volving either WT or OW where, thanks to the rel-atively denser semantic graphs obtained by means of our ontologization technique, the gap in F1 is about 0.23 ( WN -WT ) and 0.15 ( WN -OW ).
In addition, as we mentioned earlier, for WN -WP we used the same graph as that of Dijkstra-WSA, since both WN and WP provide a full-fledged se-mantic network and thus neither needed to be ontologized. Therefore, the considerable perfor-mance improvement over Dijkstra-WSA on this resource pair shows the effectiveness of our novel concept similarity measure independently of the underlying semantic network. Resource ontologization. Having lexical re-sources represented as semantic networks is highly beneficial. A good example is WordNet, which has been exploited as a semantic network in dozens of NLP tasks (Fellbaum, 1998). A re-cent prominent case is Wikipedia (Medelyan et al., 2009; Hovy et al., 2013) which, thanks to its inter-article hyperlink structure, provides a rich backbone for structuring additional information (Auer et al., 2007; Suchanek et al., 2008; Moro and Navigli, 2013; Flati et al., 2014). How-ever, there are many large-scale resources, such as Wiktionary for instance, which by their very nature are not in the form of a graph. This is usually the case with machine-readable dictionar-ies, where structuring the resource involves the arduous task of connecting lexicographic senses by means of semantic relations. Surprisingly, despite their vast potential, little research has been conducted on the automatic ontologization of collaboratively-constructed dictionaries like Wik-tionary and OmegaWiki. Meyer and Gurevych (2012a) and Matuschek and Gurevych (2013) pro-vided approaches for building graph representa-tions of Wiktionary and OmegaWiki. The result-ing graphs, however, were either sparse or had a considerable portion of the nodes left in isolation. Our approach, in contrast, aims at transforming a lexical resource into a full-fledged semantic net-work, hence providing a denser graph with most of its nodes connected.
 Resource alignment. Aligning lexical resources has been a very active field of research in the last decade. One of the main objectives in this area has been to enrich existing ontologies by means of complementary information from other resources. As a matter of fact, most efforts have been concentrated on aligning the de facto com-munity standard sense inventory, i.e. WordNet, to other resources. These include: the Roget X  X  the-saurus and Longman Dictionary of Contemporary English (Kwong, 1998), FrameNet (Laparra and Rigau, 2009), VerbNet (Shi and Mihalcea, 2005) or domain-specific terminologies such as the Uni-fied Medical Language System (Burgun and Bo-denreider, 2001). More recently, the growth of collaboratively-constructed resources has seen the development of alignment approaches with Wikipedia (Ruiz-Casado et al., 2005; Auer et al., 2007; Suchanek et al., 2008; Reiter et al., 2008; Navigli and Ponzetto, 2012), Wiktionary (Meyer and Gurevych, 2011) and OmegaWiki (Gurevych et al., 2012). Last year Matuschek and Gurevych (2013) proposed Dijkstra-WSA, a graph-based ap-proach relying on shortest paths between two concepts when the two corresponding resources graphs were combined by leveraging monosemous linking. Their method when backed off with other definition similarity based approaches (Niemann and Gurevych, 2011; Meyer and Gurevych, 2011), achieved state-of-the-art results on the mapping of WordNet to different collaboratively-constructed resources. This approach, however, in addition to setting the threshold for the definition similarity component by means of cross validation, also re-quired other parameters to be tuned, such as the allowed path length (  X  ) and the maximum num-ber of edges in a graph. The optimal value for the  X  parameter varied from one resource pair to an-other, and even for a specific resource pair it had to be tuned for each configuration. This made the approach dependent on the training data for the specific pair of resources that were to be aligned. Instead of measuring the similarity of two con-cepts on the basis of their distance in the com-bined graph, our approach models each concept through a rich vectorial representation we refer to as semantic signature and compares the two con-cepts in terms of the similarity of their semantic signatures. This rich representation leads to our approach having a good degree of robustness such that it can achieve competitive results even in the absence of training data. This enables our system to be applied effectively for aligning new pairs of resources for which no training data is available, with state-of-the-art performance. This paper presents a unified approach for align-ing lexical resources. Our method leverages a novel similarity measure which enables a di-rect structural comparison of concepts across dif-ferent lexical resources. Thanks to an effec-tive ontologization method, our alignment ap-proach can be applied to any pair of lexical re-sources independently of whether they provide a full-fledged network structure. We demon-strate that our approach achieves state-of-the-art performance on aligning WordNet to three collaboratively-constructed resources with differ-ent characteristics, i.e., Wikipedia, Wiktionary, and OmegaWiki. We also show that our approach is robust across its different configurations, even when the training data is absent, enabling it to be used effectively for aligning new pairs of lexical resources for which no resource-specific training data is available. In future work, we plan to ex-tend our concept similarity measure across differ-ent natural languages. We release all our data at
We would like to thank Michael Matuschek for providing us with Wikipedia graphs and alignment datasets.
