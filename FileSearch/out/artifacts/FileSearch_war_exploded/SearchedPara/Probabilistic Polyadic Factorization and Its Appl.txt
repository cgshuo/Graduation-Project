 Multiple-dimensional, i.e., polyadic, data exist in many a p-plications, such as personalized recommendation and multi ple-dimensional data summarization. Analyzing all the dimen-sions of polyadic data in a principled way is a challenging research problem. Most existing methods separately ana-lyze the marginal relationships among pairwise dimensions and then combine the results afterwards. Motivated by the fact that various dimensions of polyadic data jointly affect each other, we propose a probabilistic polyadic factorizat ion approach to directly model all the dimensions simultane-ously in a unified framework. We then show the connec-tion between the probabilistic polyadic factorization and a non-negative version of the Tucker tensor factorization. W e provide detailed theoretical analysis of the new modeling framework, discuss implementation techniques for our mod-els, and propose several extensions to the basic framework. We then apply the proposed models to the application of personalized recommendation. Extensive experiments on a social bookmarking dataset, Delicious , and a paper citation dataset, CiteSeer , demonstrate the effectiveness of the pro-posed models.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; J.4 [ Computer Applications ]: Social and Behavioral Sciences X  Economics Algorithms, Experimentation, Measurement, Theory Probabilistic Polyadic Factorization, Multiple-dimensi onal Data, Non-negative Tensor Factorization, Personalized Re c-ommendation, Social Bookmarking
Binary relationship exists in many applications and dyadic data analysis has been studied extensively by various re-searchers. For example, the well known Latent Semantic Indexing (LSI) focuses on dyadic data consisting of term-document pairs. For another example, singular value de-composition has been used to model dyadic data consist-ing of user-item pairs in collaborative filtering. However, in many applications, data are polyadic, i.e., they are of mul-tiple (greater than 2) dimensions. Social bookmarking is such an example: in a social bookmarking system such as Del.icio.us, a user assigns a set of tags to a given url (which corresponds to a Web page). Here data records are user-tag-url three-dimensional triples. Paper citation analys is is another example: in a paper corpus such as CiteSeer, an au-thor , in an article on a specific keyword , cites a reference . In this example, a data record is an author-keyword-reference triple. This data can have even more dimensions if more information such as second author, publication venue, and publication year, is available.

To analyze polyadic data, an important and challenging issue is how to combine heterogeneous information from dif-ferent dimensions. Various approaches have been proposed to tackle this challenge. However, most existing research work adopts a two-step approach: first, only relationships between pairs of dimensions are analyzed; second, the ob-tained pairwise relationships are combined in certain ways . A main weak point of such a two-step approach is that it considers the relationships among different pairs of dimen-sions independently while in reality, different dimensions of polyadic data affect each other in a joint way. There are some recent studies [10, 15, 16], that combine the two steps mentioned above into a single-step process. The key idea of these studies is to use the same set of concepts to si-multaneously capture all the pairwise relationships among different dimensions. These approaches are more accurate in modeling polyadic data, however they have two major weaknesses. First, they usually use a linear combination to fuse all pairwise relationships among different dimensions . Such a linear combination is somewhat ad hoc, which makes it difficult to find a good intuition behind the coefficients as well as a principled way to set the values of the coefficients. Second, they ignore valuable information on higher-order correlation (other than the second order correlation) amon g various dimensions of data.

In this study, we propose a probabilistic factorization mod el for analyzing polyadic data in a coherent way. Our main contributions are summarized as follows.
There are many potential applications to the factor model we proposed, such as extracting and monitoring coherent groups of people and topics in social networks, ranking and recommending important people and documents, as well as summarizing and visualizing data. In this paper, we fo-cus on applying the proposed factor model to an important information retrieval task: personalized recommendation . We first show that our probabilistic factorization model of-fers a natural way for making personalized recommenda-tions. Then, we use extensive experimental studies on a social bookmarking dataset (Delicious) and a paper citatio n dataset (CiteSeer) to demonstrate the effectiveness of the proposed model.
 The rest of the paper is organized as the following. In Section 2, we describe our factor models in detail. In Sec-tion 3, we present two extensions to our basic framework. In Section 4, we discuss some practical issues on efficient com-putation of the model parameters. In Section 5, we show how the proposed model can be applied to the application of personalized recommendation. In Section 6, we survey re-lated work. We show experimental results in Section 7 and finally give conclusions in Section 8.
In this section, the main theoretical framework of our fac-tor model is described. First, the factor model based on a probabilistic polyadic factorization is presented. Then a n interpretation using a non-negative tensor factorization is given. Finally, the connection between the two is proved. For ease of discussion we show the cases of 3-dimensional data in the rest of the paper, although our factor model can be defined on multiple-dimensional data with arbitrary number of dimensions.
We describe the probabilistic framework of our factor model by using the CiteSeer dataset as a motivational example. In the CiteSeer data, each data record is an author-keyword-reference triple. Assume that the dataset contains I dis-tinct authors, J distinct keywords, and K distinct refer-ences. Furthermore, assume that there are L latent factors (groups) of authors where each author belongs to these L groups with different probabilities. Note that these L latent factors are not explicit, i.e., they are hidden and need to be learned from training data. A random variable C  X  is used to represent the prior distribution for the L latent factors of authors. Similarly, it is assumed that there are M la-tent factors of keywords and N latent factors of references, and random variables C  X  X  and C  X  X  X  are used to represent the prior distributions of these two sets of latent factors. It i s worth noting that the cardinalities of C  X  , C  X  X  , and C not have to be identical. In the following discussion, i , j , and k are used to represent the indices for author, keyword, and reference, respectively; l , m and n are used to represent the indices for the factors of author, keyword, and referenc e, respectively.

With these latent factors defined, the following generative model is used to model how each data record is generated. To generate a data record (i.e., an author-keyword-referen ce triple h i, j, k i ), a factor c  X  l for author, a factor c word, and a factor c  X  X  X  n for reference are selected with proba-an author i is picked with probability P ( i | c  X  l ), a keyword j with probability P ( j | c  X  X  m ), and a reference k with probability P ( k | c  X  X  X  n ). The assumption of this generative model is that the selections of author, keyword and reference are indepen -dent with each other, given the latent factors c  X  l , c  X  X  been chosen. In the following discussion, to avoid notation al clutter, we write P ( c  X  l , c  X  X  m , c  X  X  X  n ) as P ( c
Furthermore, it is assumed that the data records are gen-erated following an identical independent distribution. B y using a ijk to denoted the number of occurrences of the triple h i, j, k i in the data, the likelihood of observing all the data records is Then to complete the generative model, the parameters  X  are to be learned in order to maximize the above data like-lihood, or equivalently, the logarithm of the data likeliho od arg max
It can be shown that the following EM algorithm can be used to compute the MLE parameters  X  in our generative model. The detailed proof is skipped due to the space limit.
E-Step:  X  p M-Step:
P ( c lmn ) = X
This probabilistic polyadic factor model is related to the aspect model proposed by Hofmann et al [7]. However, it is different from the aspect model in two ways. First, the aspect model is defined on dyadic data (matrices) while our factor model can handle polyadic data with arbitrary num-ber of dimensions. Second, in the aspect model, there is a one-one mapping between the factors in the row space and the factors in the column space while in our model, such a one-one mapping is not enforced. Instead, in our factor model a joint probability, P ( c lmn ), is used to capture the correlation among factors in different dimensions. As a re-sult, our factor model is more flexible and it allows many-to-many mapping between factors in different dimensions.
Now we connect the probabilistic polyadic factor model in the previous section with a non-negative version of the Tucker tensor factorization. For this purpose, we re-inter pret the the generative model in a different way. Assume that each author i is encoded with an L -dimensional vector ~x R + , where R + represents non-negative real numbers. Sim-ilarly, each keyword j is encoded by ~y j  X  R M + and each reference k by ~z k  X  R N + . In addition, a ijk is approximated by a function f ( ~x i  X  ~y j  X  ~z k ) where f belongs to a certain family of functions F and  X  denotes the Kronecker product. The objective is to find the best encoding, in terms of ~x ~y , ~z k , and f , that minimizes the following loss: Now two questions arise: which family of functions F to choose and what distance function to use. For the first question, we simply choose the family of linear functions, i.e., f ~c ( ~x i  X  ~y j  X  ~z k ) = h ~c, ~x i  X  ~y j  X  ~z sents the vector inner product. For the second question, we choose the KL-divergence between a ijk and f ( ~x i  X  ~y j where the KL-divergence is defined by Then, we can rewrite the objective function (7) in terms of tensor factorization as follows. Data is first put into a data tensor A  X  R I  X  J  X  K + where ( A ) ijk = a ijk . Next, the vectors ~x i  X  X  are concatenated to a matrix X  X  X  I  X  L + the i -th row of X is the transpose of ~x i . Similarly ~y ~z  X  X  are concatenated into matrices Y  X  R J  X  M + and Z  X  R + . Furthermore, ~c is folded into a tensor C  X  X  tensor . After the concatenation and the folding, we have f ( ~x i  X  ~y j  X  ~z k ) = ( C X  1 X  X  2 Y  X  3 Z ) ijk where  X  and  X  3 are the mode-1, mode-2, and mode-3 multiplications of the tensor C by the matrices X , Y , and Z , respectively. (Please refer [1] for details about the terminologies.) The n the problem of best encoding becomes the problem of finding the best approximation, in terms of X, Y, Z , and C , that minimizes the following loss However, there is a tricky issue: because C X  1 X  X  2 Y  X  3 for arbitrary orthogonal matrices Q X , Q Y , and Q Z , as a result the solution is not uniquely defined. To make the problem well posed, we add the additional constraint that the columns of X , Y , and Z must all sum to ones. Equation (9) turns out to be a non-negative version of the Tucker tensor decomposition [14]. By directly extending th e algorithm given by Lee and Seung [9], which was used for solving the non-negative matrix factorization (NMF) prob-lem, we have the following iterative update algorithm.
Theorem 1. For a given A  X  R I  X  J  X  K + , we first define another tensor B as Then the following update rules are guaranteed to converge to an optimal solutions to the objective function defined in Eq. (9) .

We provide the proof for the correctness of this algorithm in the Appendix.
It turns out that the model we defined using the NTF framework is equivalent to the probabilistic polyadic factor model and therefore, it is just a different interpretation of the probabilistic factor model. We prove this claim in the following theorem.

Theorem 2. The problem of minimizing the loss in Equa-tion (9) is equivalent to the MLE problem described by Equa-tion (1) .

Proof. We first define D = C X  1 X  X  2 Y  X  3 Z and denote ( D ) ijk by d ijk . Then we can expand the loss function (9) as loss KL = KL ( A||C  X  1 X  X  2 Y  X  3 Z )
Several observations can be obtained from the above equa-tion. First, a necessary condition for a solution D to min-implies that a necessary condition for an optimal solution i s that P i,j,k a ijk = P l,m,n ( C ) lmn , because of the constraints on X , Y , and Z .
A second observation is A can be scaled so that P i,j,k a 1 without changing the optimization problem. Similarly, be -cause Equation (1) can be multiplied by a constant without affecting the MLE problem, without loss of generality, we can assume that P i,j,k a ijk = 1. As a result, minimizing loss KL is equivalent to maximizing P i,j,k a ijk log d ijk
By comparing the term P i,j,k a ijk log d ijk with the target function Equation (1) for the MLE problem, and by set-P ( c lmn ), we prove the theorem.

As can be expected, there is a close connection between the EM algorithm in Equations (2) X (6) and the NTF factor-ization algorithm in Equations (10) X (14). With some simple derivations we can show that the two algorithms share al-most the identical form. However, there is a subtle differenc e between the two. In the EM algorithm, all parameters are updated together in the M-step. In comparison, in the NTF factorization algorithm, X, Y, Z, and C must be updated al-ternatively , which makes it a block descent algorithm.
Having both the probabilistic MLE and NTF interpreta-tions of the factor model is beneficial, because we can furthe r extend the proposed model based on two different perspec-tives. In this section we present two such kind of extensions to the basic framework.
One extension is to consider the maximum a posteriori (MAP) estimation for the basic probabilistic model. Here, we consider the Dirichlet distribution as the prior distrib u-tion for the parameters.

First, we consider the prior for X . Since X il = P ( i | l ), where P (  X  X  l ) is a multinomial distribution, the prior of each column of X could be a Dirichlet distribution with hyper-parameter  X  X 1 . The logarithm of the prior probability is where c X is a value irrelevant to X .
 Similarly, we assume the priors for Y , Z and C are all Dirichlet distributions with hyper-parameters  X  Y ,  X  Z and  X 
C respectively. The logarithm of the prior probabilities are
The logarithm loss of the MAP estimation is that of MLE plus the logarithm of prior probabilities. Therefore,
With some computations, we can derive the following Corol-lary (for convenience, we chose an expression in NTF).
Dirichlet distribution is a conjugate prior for multinomia l distribution.

Corollary 1. The following update rules are guaranteed to converge to an optimal solutions to the objective functio n defined in Equation (15) .
 and normalize s.t. columns of X , Y , Z sum to ones.
This Dirichlet-prior-based MAP framework is useful in that it provides a way for us to add smoothing to the ob-served data when the data are sparse. We will demonstrate this in the experimental studies.
From the NTF interpretation of the factor model, we can choose any matrix (tensor) norm instead of the KL-divergence to measure the loss. For example, if the Frobe-nius norm is used, the loss becomes where the Frobenius norm for a tensor A is defined by kAk hA , Ai = P i,j,k a 2 ijk . For this loss function, we can show that the following algorithm can be used to find an optimal solution.

Theorem 3. For a given A X  X  I  X  J  X  K + , the following up-date rules are guaranteed to converge to an optimal solution s to the objective function defined in Equation (16) . This algorithm is a direct extension of an algorithm by Lee and Seung [9] for NMF and we provide the proof for the correctness of this algorithm in the Appendix.
In this section, we describe some practical details in the implementation of our factor models and provide time com-plexity analysis.
In the following discussion, without loss of generality, it is assumed that L  X  M  X  N and I  X  J  X  K . We assume that there are nz data records in the dataset, which implies that there are nz non-zero entries in the data tensor A . In addi-tion, we assume that there are np distinct ( i, j ) pairs in the dataset. For ease of discussion, we use the NTF framework in the following discussion, although the same ideas apply to the EM algorithm.

As a matter of fact, most components in Equations (10) X  (14), especially those time-consuming parts, can be com-puted in the same computational framework. We describe this computation framework by showing how to compute B in Equation (10), while other computations can be con-ducted in a similar fashion. A naive implementation for computing B will expand C X  1 X  X  2 Y  X  3 Z , which turns out to be a dense tensor in R I  X  J  X  K + . However, because such a dense tensor contains IJK entries, it is impractical to make such an explicit expansion (e.g., consider a dataset with 10 K authors, 10K keywords, and 10K references, where IJK = 1 trillion). We design an implementation that takes advantag e of the sparseness of the dataset and has a time complexity that is linear (per iteration) in nz , the number of non-zero entries in the raw data tensor A .

Assuming each data record of the data is stored in the are the indices of the data record in the first, second, and third dimensions, respectively; v is the value of the data record, which can be, for example, the number of times that an author cites a reference on a keyword. In the first step of the algorithm, the data records are sorted according to key 1 as the major key and then key 2 and then key 3 as the minor keys. This sorting takes time linear in nz because the keys are integers with known upper-bounds and therefore a linear sorting algorithm, such as bucket sort, can be applie d. In addition, to simplify the discussion, it is assumed that h key 1 , key 2 , key 3 i is a primary (unique) key for the data records.

There are two key ideas in our implementation. First, our implementation is a lazy one where only the absolute necessary computations are performed. In such a lazy fash-ion, the implementation takes advantage of the sparseness of the data and only performs computations on entries cor-responding to non-zero values in the dataset. The second key idea of our implementation is that the computations are carefully ordered in a way such that repeated computations are minimized. Figure 1 illustrates these two key ideas usin g pseudo-codes. In the algorithm, D is an M  X  N matrix and ~ d is a vector of length N . Note that fold ( ) is a function for folding a vector into a matrix. This function does not have to be materialized in the real implementation and is shown here only to make it easier to understand. The idea of lazy computation is reflected by the fact that the for loop enumerates only non-zero entries in the dataset (or equiv-alently, in A ). To see the idea of ordered computation, we note that the D matrix computed at line 5 for a given i is usually re-used several times at line 8 for different j  X  X , and the ~ d computed at line 8 is usually re-used several times at line 10 for different k  X  X .
It can be easily seen that the algorithm in Figure 1 has a time complexity of O ( nz L + np L 2 + I L 3 ), where nz is the number of data records and np is the number of distinct ( i, j ) pairs in the dataset. Here are some observations on the time complexity:
Algorithm for computing B 1: i  X  X  X  1, j  X  X  X  1; 2: for each data record h key 1 , key 2 , key 3 , v i do 3: if i 6 = key 1 4: i  X  key 1, j  X  X  X  1; 5: D  X  fold ( X row i C (1) ); 6: if j 6 = key 2 7: j  X  key 2; 8: ~ d  X  Y row j D ; 9: k  X  key 3; 10: b ijk  X  v/ ( Z row k ~ d ); 11: return B ;
Our algorithms are iterative updating ones and therefore the losses in Equations (9) and (16) are needed in each iter-ation to check convergence. Again, to explicitly expanding C  X  1 X  X  2 Y  X  3 Z is impractical. Here we give details on how we efficiently compute the two losses.

The loss in the KL-divergence can be easily computed while computing at no extra cost. The loss in the Frobenius norm is more tricky. We first rewrite the loss in the Frobenius norm as the following In the last equation, the first term is only related to the non-zero entries in A and so can be computed with time linear in nz ; the second term involves an inner product between two tensors with small sizes in all the dimensions and so it can be handled quickly; the third term has a form similar to the computation in the update algorithm that has been shown in Figure 1, and therefore the time complexity for computing this term is again O ( nz L + np L 2 + I L 3 ). In summary, computing the losses does not change the total time complexity of the algorithms.
There can be many applications using our factor mod-els, such as discovering coherent groups in social networks , summarizing multiple-dimensional data, ranking and recom -mending important people and documents, etc. In this pa-per, we focus on an important information retrieval task: personalized recommendation.

We again use the CiteSeer dataset as a motivation ex-ample where the task is to recommend the most relevant references to a given author for a given keyword. A straight-forward solution to this problem is to simply recommend the most popular references associated with the given keyword. Such an approach, however, makes global recommendations to all the authors and therefore ignores any personal infor-mation about the author to whom the recommendation is made. Personal information can be very useful for recom-mendations because different authors have different areas of expertise and focuses of research. So for a given keyword, the most globally popular references on the keyword may not be the most relevant ones for a particular author.
Our polyadic factor model provides a natural way to in-corporate an author X  X  background in order to make person-alized recommendation. Given a probabilistic model, for a given author i and a given keyword j , a reasonable solution is to recommend the references k  X  X  that have the highest con-ditional probabilities predicted by the model. In the naive method based on the global popularity, the conditional prob -ability is only conditioned on the keyword j , i.e., the naive method uses P ( k | j ) for recommendation. In comparison, in our probabilistic polyadic factor model, the conditiona l probability P ( k | i, j ) that is conditioned on both the author i and the keyword j is used for personalized recommenda-tion, where P ( k | ij ) = P ( ijk ) P ( ij )  X  P ( ijk ) = C X  1 X row
One research area that is closely related to our work is general tensor factorizations where the non-negative con-straint is removed. Two examples of such tensor decompo-sitions are the Tucker model [14], which has recently been generalized to the Higher Order Singular Value Decomposi-tion [1]) and the PARAFAC model [5]. These tensor fac-torization techniques show some good properties in terms of minimizing certain losses in the Frobenius norm. However, the factors and core tensors obtained by these general fac-torizations have negative values, which make them difficult to interpret. In addition, factors obtained from these tens or factorizations usually have some orthogonality constrain ts. All of these restrict the applications of these general tens or factorizations in the IR field.

As we have mentioned before, our factor model for polyadic data is closely related to the aspect model (PLSA) proposed by Hofmann et al [7], which can be solved by the non-negative matrix factorization (NMF) algorithms proposed by Lee and Seung [9]. The MLE interpretation of our factor model is an extension to PLSA and the NTF interpretation is an extension to NMF. Recently, Gaussier and Goutte [4] and Ding et.al. [3] have shown the equivalence between the PLSA and the NMF algorithms. Therefore it comes at no surprise that the MLE and the NTF interpretations of our factor model are equivalent. Also, we are not the first ones to notice that the NMF algorithms can be directly extended to solving NTF problems. For example, M X rup et al [11] have explored this connection and developed similar algo-rithms. In addition, there also exist non-negative version s of the PARAFAC model [6, 12]. However, a weak point of such algorithms is that they require the number of factors in each dimension to be identical and the core tensor to be diagonal. This limits the usage of the model in many appli-cations.

Personalized recommender systems has been studied for many years in three sub areas in IR: relevance feedback, adaptive filtering and collaborative filtering. Adaptive fil-tering and relevance feedback focus on identifying top-ically relevant documents based on the content of the doc-uments and update the user profile using content based re-trieval models (e.g. Boolean models, vector space models, traditional probabilistic models, inference networks and lan-guage models) or machine learning algorithms (e.g. Support Vector Machine (SVM), K nearest neighbors (K-NN), neu-ral network, logistic regression and Winnow). Collabora-tive filtering recommends items to a user using informa-tion from other users with similar tastes and preferences in the past. Memory based heuristics and model based ap-proaches have been used in collaborative filtering tasks [2, 8, 13]. However, collaborative filtering algorithms usually o nly use two dimensional data represented as user-item matrix. The model proposed in this paper enables us to go beyond the user-item matrix and use more information, such as so-cial networks, for better personalized recommendation.
In this section, we apply our factor models to the problem of personalized recommendation and study the performance by using two datasets. The first dataset is obtained from the social bookmarking website Del.icio.us 2 . At Del.icio.us, registered users assign tags to urls . Therefore, data records in this dataset, which we call Delicious , consist of user-tag-url triples. In our study, we first preprocess the data by removing users, tags, and urls that have occurrences less than a threshold of 20. This results a Delicious dataset with 3,908 users, 867 tags, 3,570 urls, and 180,246 data records. Figure 2 shows the histograms of the data. As can be seen, users, tags, and urls all clearly follow power-law distribu -tions. On the Delicious dataset we define two tasks of per-sonalized recommendation. In the first task, for a given url, relevant tags are recommended to a particular user. This task is directly applicable to personalized tag recommenda -tion in social bookmarking systems such as Del.icio.us. The http://del.icio.us/ Figure 2: The histograms for users, tags, and urls in the Delicious dataset. second task is in a dual form of the first one: for a given tag, relevant urls are recommended to a particular user. This task can be of importance in a personalized search engine. That is, if a tag is considered as a query, then the collabora-tive tagging reflects people X  X  tendency to associate differe nt urls to the query and the history of a particular user reflects that user X  X  preference among the urls for the given query.
The second dataset is obtained from the CiteSeer web-site 3 . A set of articles from various areas are selected first. The author(s), abstract, and references of each article are extracted and the abstracts are split into keywords. Finall y the author-keyword-reference triples are extracted. Simi-lar preprocessing process is applied to the CiteSeer data, which results in a dataset, which we call CiteSeer , contain-ing 16,466 authors, 920 keywords, 29,309 references, and 3,636,020 data records. On the CiteSeer dataset, we define the task as recommending relevant references for a given au-thor on a given keyword. This task is useful for an author to collect important references when writing an article on a particular topic.
For the performance studies, data are randomly split into 80% training data and 20% testing data. However, a subtle issue here is that the split should not be totally random. To illustrate this issue, we take the task of tag recommenda-tion as an example. For this task, for fair comparison, the training and testing data should be split in such a way that a user-url pair either shows up in the training data or in the testing data, but not in both. In other words, if we mea-sure the performance by how good the recommended tags are on a given url for a particular user, then the user should not have seen the url in the training data. As a result, for different tasks, the data are split in slightly different ways .
Factorizations are applied to the training data to get the parameters for our factor models, i.e., the factors in each dimensions and their correlation. Then the learned factor models are applied to the testing data for top-K person-alized recommendation with various K  X  X . The recommen-dation performance is measured by the average Normalized Discounted Cumulative Gain (NDCG) where NDCG is de-fined as NDCG is a relatively new measure used extensively in Web searches. It allows different levels of relevance and weighs http://citeseer.ist.psu.edu/ the recommendations according to their ranks in the ranked list. In our study, we use a binary relevance score for rel ( j ) where rel ( j ) = 1 if item j occurs in the testing data and 0 otherwise. More specifically, we use top-3 tag recommenda-tion as an example to illustrate how the average NDCG is computed. Assume that for a user i on a url k , our model recommends an ordered list of three tags { j 1 , j 2 , j 3 testing data but h i, j 2 , k i does not. Then the NDCG for the ( i, k ) pair is 1 / log(1 + 1) + 1 / log(3 + 1) = 1 . 5. The average NDCG is the NDCG scores averaged over all distinct ( i, k ) pairs in the testing data.

We compare the performance of our factor models with that of two baseline algorithms. These baseline algorithms offer global rather than personalized recommendations. We explain these baseline algorithms using the task of tag rec-ommendation. First, the user-tag-url triples are aggregat ed into tag-url pairs together with the count of each pair. The aggregated results are put into a matrix M where the rows and columns of M correspond to tags and urls, respectively, and the jk -th entry in M represents the count of occur-rences of tag j associated with url k . For the first baseline algorithm, the recommendation is only based on the count of occurrences. That is, for url k , this baseline recommends the tags with highest magnitudes in the k -th column of M . This baseline algorithm corresponds to recommending by popu-larity and such a straightforward method usually shows very good performance when enough observations are sampled from data. In the second baseline algorithm, a non-negative matrix factorization (NMF) is applied to M to compute an approximation  X  M to M . After  X  M is obtained, the recom-mendations are made in a similar way to the first baseline, except that  X  M is used instead of M . The intuition behind this baseline is similar to the intuition behind the latent semantic indexing (LSI)  X  by capturing the major compo-nents from the raw data, we may be able to reduce the noise in the raw data and therefore discover the hidden relations.
In the first task, namely recommending relevant tags to a given user on a given url, our factor models do not perform as well as the baseline algorithms. Table 1 gives the average NDCG scores for different algorithms. For our factor mod-els, 100 factors are used in each of the three dimensions. We believe that the main reason for the poor performance of the factor models is that personalization does not help in this task. That is, different users usually have consensus on the tags for a url. For example, for a homepage of a professor Table 1: Performance of different algorithms for the task of recommending tags on Delicious dataset. whose research is on IR, most likely a user will assign tags such as  X  X R X  and  X  X eople X , despite the background of the user. Another observation from the results is that because the data are sparse, adding smoothing (the case reported as Factor KL 100 S in Table 1, with  X   X  X  set to 0.001) improves the performance over the case where no smoothing is used (the case reported as Factor KL 100 in Table 1).

On the other hand, for the second task, namely recom-mending relevant urls to a given user on a given tag, person-alization does help. As can be see from Table 2, the factor model using the KL-divergence has better performance than the baseline algorithms. Intuitively, on the same tag, diffe r-ent users may have different ideas on what urls are relevant, depending on the user X  X  interests and background and as a result, personalization offers certain help in this task. Table 2: Performance of different algorithms for the task of recommending urls on Delicious dataset.

Another observation from Tables 1 and 2 is the factor model that uses KL-divergence to measure the loss clearly outperforms the one that uses Frobenius norm to measure the loss. Puzzled by this discrepancy, we investigated the factors obtained by the two models. It turns out, as shown in Figure 3, using Frobenius norm to measure the loss will result in factors that are much more sparser than those ob-tained by using KL-divergence. This result is because that Frobenius norm focuses heavily on entries with large values in the raw data while KL-divergence focuses on all entries relatively evenly. This difference in the resulting factors sug-gests that KL-divergence is a more appropriate loss measure when the task is recommending or ranking. Figure 3: A typical tag factor obtained by using KL divergence (upper) and a typical tag factor obtained by using the Frobenius norm (lower).
For the CiteSeer dataset, the main task is to recommend relevant references to a given author on a given keyword. Table 3 shows the performance in average NDCG scores. For the factor model using KL-divergence, 10 factors are used for each of the three dimensions while for the factor model using Frobenius norm, we report two cases where 10 factors and 30 factors are used respectively. As can be seen from the results, in this task our factor models are able to outperform the baseline algorithms using very small number of factors (10 for the model using KL-divergence and 30 for the model using Frobenius norm). Table 3: Performance of different algorithms for the task of recommending references on CiteSeer data. This good performance suggests that this dataset, Cite-Seer, is  X  X asier X  to handle than the pervious Delicious data . It is easy in the sense that personalization tells us a lot abo ut an author. To validate this point, we show in Tables 4, 5, and 6 the top members in some factors obtained by our factor model by using KL-divergence. As can be seen, the factors obviously form clusters of authors focusing on dif-ferent research areas with different relevant references. I n comparison, top members in the factors obtained from the Delicious dataset (not shown due to the space limit) do not show such clear clusters. It is worth mentioning that these factors for the three dimensions are simultaneously obtained by our factor models from the polyadic data. That is, our factor models are effective in simultaneously analyzing all di-mensions as well as their correlations among polyadic data. In addition, these factors and their correlations can be ver y useful for other tasks such as clustering and summarization .
We designed several experiments to study the performance of our implementation in terms of running time. For the purpose of experimental study, we generate a second Deli-cious dataset, which we call Delicious-2 , by using a lower threshold (10) in the pre-processing steps which results in dataset that is 3 times larger (with 482,600 data records) than the previous Delicious dataset (which we refer to as Delicious-1 ).

The factor model based on KL-divergence is applied to the two datasets over a variety of factor numbers ranging from
Table 5: Top members in some keyword factors 1. Fast algorithms for mining association rules 2. Mining association rules between sets of items in large 3. The anatomy of a large-scale hypertextual Web search 4. Authoritative sources in a hyperlinked environment 5. The stable model semantics for logic programming 6. Information retrieval 1. Dynamic source routing in ad hoc wireless networks 2. A performance comparison of multi-hop wireless ad 3. Ad-hoc on-demand distance vector routing 4. Highly dynamic destination-sequenced distance-vector 5. Next century challenges: scalable coordination in sen-6. A highly adaptive distributed routing algorithm for 1. Reinforcement learning I: introduction 2. Bagging predictors 3. Support-vector networks 4. Optimization by simulated annealing 5. A decision-theoretic generalization of on-line learnin g 6. Experiments with a new boosting algorithm
Table 6: Top members in some reference factors 10 to 50. First, the running time is shown in Figure 4(a) in a log-log scale. As can be seen, the running time scales in a polynomial fashion with respect to L , the number of factors in each dimensions. From the slope of the curves in Figure 4(a), it can be shown that the running time is proportional to L 2 . Second, it can be seen that the ratio between the running time on the two datasets, which is given in Figure 4(b), remains a constant 3, which is about the ratio between the sizes of the two datasets. All these result s validate our theoretical time complexity analysis.
In this paper, we propose a polyadic factor model to ana-lyze multiple-dimensional data such as networked data in so -cial networks. This probabilistic generative model is rela ted to non-negative tensor factorizations. Based on the prob-abilistic and NTF interpretation of the model, we propose two extensions to the basic framework. The factor model can be learned efficiently with a time complexity linear to the number of data records. This model has many potential applications. As an example, we apply the polyadic factor Figure 4: (a) The running time per iteration for the two Delicious datasets, (b) The ratio of running time between the two Delicious datasets. model to the task of personalized recommendation, and ob-tain encouraging experimental results on social bookmarki ng dataset Del.icio.us and paper citation dataset CiteSeer . We thank Professor C. Lee Giles for providing us the Cite-Seer dataset and Haward Jie for sharing the De.li.cio.us dat a. The last author X  X  research is sponsored by National Science Foundation IIS-0713111. Any opinions, findings, conclusio ns or recommendations expressed in this paper do not neces-sarily reflect those of the sponsor. [1] L. De Lathauwer, B. De Moor, and J. Vandewalle. A [2] J. Delgado and N. Ishii. Memory-based [3] C. Ding, X. He, and H. D. Simon. On the equivalence [4] E. Gaussier and C. Goutte. Relation between plsa and [5] R. A. Harshman. Foundations of the parafac [6] T. Hazan, S. Polak, and A. Shashua. Sparse image [7] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning [8] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, [9] D. D. Lee and H. S. Seung. Algorithms for [10] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [11] M. M X rup, L. K. Hansen, and S. M. Arnfred.
 [12] A. Shashua and T. Hazan. Non-negative tensor [13] L. Si and R. Jin. A flexible mixture model for [14] L. R. Tucker. Some mathematical notes on three-mode [15] D. Zhou, S. Zhu, K. Yu, X. Song, B. L. Tseng, H. Zha, [16] S. Zhu, K. Yu, Y. Chi, and Y. Gong. Combining Proof. We start with the proof for the update rule for X . In the proof, we use  X  X ,  X  Y ,  X  Z ,  X  B , and  X  current values of X , Y , Z , B and C before the update. With Y , Z and C fixed to the values  X  Y ,  X  Z and  X  C , we consider KL ( A||  X  C X  1 X  X  2  X  Y  X  3  X  Z ) as a function of X , i.e., D In addition, to avoid notation clutter, we define Then we have
D X ( X ) =
X  X  X = X =
X . = Q X ( X ;  X  X ) .
 Therefore, Q X ( X ;  X  X ) is an auxiliary function of D the sense that 1. D X ( X )  X  Q X ( X ;  X  X ), and 2. D X ( X ) = Q X ( X ; X ).
 So the problem is reduced to minimizing Q X ( X ;  X  X ) with respect to X , under the constraint that all the columns of X sum to ones. We define the Lagrangian and by taking its derivative and setting the result to zero, we have
Applying similar procedures to Y , Z , and C we have which give the update rules in Theorem 1.

Proof. We proof the update rules for X and C . In addi-current values of X , Y , Z , and C before the update.
With Y , Z and C fixed to the values  X  Y ,  X  Z and  X  C , we D
X ( X ). In addition, by unfolding the tensor with respect to the first mode, we have Then we can apply the results by Lee and Seung [9] to get the update rule which give the update rule for X in Theorem 3.

For the update rule for C , we rewrite the objective function as Then again we can apply the results by Lee and Seung [9] to get the update rule vec ( C ) i  X  vec ( C ) i which give the update rule for C in Theorem 3.
