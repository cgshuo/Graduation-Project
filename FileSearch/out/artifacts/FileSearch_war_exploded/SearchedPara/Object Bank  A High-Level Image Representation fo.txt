 Understanding the meanings and contents of images remains one of the most challenging problems in machine intelligence and statistical learning. Contrast to inference tasks in other domains, such as NLP, where the basic feature space in which the data lie usually bears explicit human perceivable meaning, e.g., each dimension of a document embedding space could correspond to a word [21], or a topic, common representations of visual data seem to primarily build on raw physical metrics of the pixels such as color and intensity, or their mathematical transformations such as various filters, or simple image statistics such as shape, edges orientations etc. Depending on the specific visual inference task, such as classification, a predictive method is deployed to pool together and model the statistics of the image features, and make use of them to build some hypothesis for the predictor. For example, Fig.1 illustrates the gradient-based GIST features [25] and texture-based Spatial Pyramid representation [19] of two different scenes (foresty mountain vs. street). But such schemes often fail to offer sufficient discriminative power, as one can see from the very similar image statistics in the examples in Fig.1.
 Figure 1: (Best viewed in colors and magnification.) Comparison of object bank (OB) representation with While more sophisticated low-level feature engineering and recognition model design remain impor-tant sources of future developments, we argue that the use of semantically more meaningful feature space, such as one that is directly based on the content (e.g., objects) of the images, as words for tex-tual documents, may offer another promising venue to empower a computational visual recognizer to potentially handle arbitrary natural images, especially in our current era where visual knowledge of millions of common objects are readily available from various easy sources on the Internet. In this paper, we propose  X  X bject Bank X  (OB), a new representation of natural images based on objects, or more rigorously, a collection of object sensing filters built on a generic collection of la-beled objects. We explore how a simple linear hypothesis classifier, combined with a sparse-coding superior predictive power over similar linear prediction models trained on conventional representa-tions. We show that an image representation based on objects can be very useful in high-level visual recognition tasks for scenes cluttered with objects. It provides complementary information to that of the low-level features. As illustrated in Fig.1, these two different scenes show very different image responses to objects such as tree, street, water, sky, etc. Given the availability of large-scale image datasets such as LabelMe [30] and ImageNet [5], it is no longer inconceivable to obtain trained ob-ject detectors for a large number of visual concepts. In fact we envision the usage of thousands if not millions of these available object detectors as the building block of such image representation in the future.
 While the OB representation offers a rich, high-level description of images, a key technical chal-lenge due to this representation is the  X  X urse of dimensionality X , which is severe because of the size (i.e., number of objects) of the object bank and the dimensionality of the response vector for each object. Typically, for a modest sized picture, even hundreds of object detectors would result into a representation of tens of thousands of dimensions. Therefore to achieve robust predictor on practi-cal dataset with typically only dozens or a couple of hundreds of instances per class, structural risk minimization via appropriate regularization of the predictive model is essential.
 In this paper, we propose a regularized logistic regression method, akin to the group lasso approach for structured sparsity, to explore both feature sparsity and object sparsity in the Object Bank repre-sentation for learning and classifying complex scenes. We show that by using this high-level image representation and a simple sparse coding regularization, our algorithm not only achieves superior image classification results in a number of challenging scene datasets, but also can discover seman-tically meaningful descriptions of the learned scene classes. A plethora of image descriptors have been developed for object recognition and image classifica-tion [25, 1, 23]. We particularly draw the analogy between our object bank and the texture filter banks [26, 10].
 Object detection and recognition also entail a large body of literature [7]. In this work, we mainly use the current state-of-the-art object detectors of Felzenszwalb et. al. [9], as well as the geometric context classifiers ( X  X tuff X  detectors) of Hoeim et. al. [13] for pre-training the object detectors. The idea of using object detectors as the basic representation of images is analogous [12, 33, 35]. In contrast to our work, in [12] and [33] each semantic concept is trained by using the entire images or frames of video. As there is no localization of object concepts in scenes, understanding cluttered im-ages composed of many objects will be challenging. In [35], a small number of concepts are trained and only the most probable concept is used to form the representation for each region, whereas in our approach all the detector responses are used to encode richer semantic information. The idea of using many object detectors as the basic representation of images is analogous to ap-proaches applying a large number of  X  X emantic concepts X  to video and image annotation and re-trieval [12, 33, 35]. In contrast to our work, in [12, 33, 35] each semantic concept is trained by using entire images or frames of videos. There is no sense of localized representation of meaningful object concepts in scenes. As a result, this approach is difficult to use for understanding cluttered images composed of many objects.
 Combinations of small set of (  X  a dozen of) off-the-shelf object detectors with global scene context have been used to improve object detection [14, 28, 29]. Also related to our work is a very recent exploration of using attributes for recognition [17, 8, 16]. But we emphasize such usage is not a universal representation of images as we have proposed. To our knowledge, this is the first work that use such high-level image features at different image location and scale. Object Bank (OB) is an image representation constructed from the responses of many object de-tectors, which can be viewed as the response of a  X  X eneralized object convolution. X  We use two blobby objects such as tables, cars, humans, etc, and a texture classifier by Hoiem [13] for more texture-and material-based objects such as sky, road, sand, etc. We point out here that we use the word  X  X bject X  in its very general form  X  while cars and dogs are objects, so are sky and water. Our image representation is agnostic to any specific type of object detector; we take the  X  X utsourcing X  approach and assume the availability of these pre-trained detectors.
 Fig. 2 illustrates the general setup for obtaining the OB representation. A large number of object detectors are run across an image at different scales. For each scale and each detector, we obtain an initial response map of the image (see Appendix for more details of using the object detectors [9, 13]). In this paper, we use 200 object detectors at 12 detection scales and 3 spatial pyramid levels (L=0,1,2) [19]. We note that this is a universal representation of any images for any tasks. We use the same set of object detectors regardless of the scenes or the testing dataset. 3.1 Implementation Details of Object Bank So what are the  X  X bjects X  to use in the object bank? And how many? An obvious answer to this question is to use all objects. As the detectors become more robust, especially with the emergence of large-scale datasets such as LabelMe [30] and ImageNet [5], this goal becomes more reachable. But time is not fully ripe yet to consider using all objects in, say, the LabelMe dataset. Not enough research has yet gone into building robust object detector for tens of thousands of generic objects. And even more importantly, not all objects are of equal importance and prominence in natural im-ages. As Fig.1 in Appendix shows, the distribution of objects follows Zipf X  X  Law, which implies that a small proportion of object classes account for the majority of object instances. tant practical consideration for our study is to ensure the availability of enough training images for each object detectors. We therefore focus our attention on obtaining the objects from popular image datasets such as ESP [31], LabelMe [30], ImageNet [5] and the Flickr online photo sharing com-munity. After ranking the objects according to their frequencies in each of these datasets, we take the intersection set of the most frequent 1000 objects, resulting in 200 objects, where the identities and semantic relations of some of them are illustrated in Fig.2 in the Appendix. To train each of the 200 object detectors, we use 100  X  200 images and their object bounding box information from the LabelMe [30] (86 objects) and ImageNet [5] datasets (177 objects). We use a subset of LabelMe scene dataset to evaluate the object detector performance. Final object detectors are selected based on their performance on the validation set from LabelMe (see Appendix for more details). We envisage that with the avalanche of annotated objects on the web, the number of object detec-tors in our object bank will increase quickly from hundreds to thousands or even millions, offering increasingly rich signatures for each images based on the identity, location, and scale of the object-based content of the scene. However, from a learning point of view, it also poses a challenge on how to train predictive models built on such high-dimensional representation with limited number of ex-amples. We argue that, with an  X  X vercomplete X  OB representation, it is possible to compress ultra-high dimensional image vector without losing semantic saliency. We refer this semantic-preserving compression as content-based compression to contrast the conventional information-theoretic com-pression that aims at lossless reconstruction of the data.
 In this paper, we intend to explore the power of OB representation in the context of Scene Clas-sification, and we are also interested in discovering meaningful (possibly small subset of) dimen-sions during regularized learning for different classes of scenes. For simplicity, here we present our model in the context of linear binary classier in a 1-versus-all classification scheme for K classes. matrix, represent the design built on the J -dimensional object bank representation of N images;  X  = (  X  1 , . . . ,  X  J )  X  R J is a vector of parameters to be estimated. This leads to the following learning problem min  X   X  R J  X R (  X  ) + 1 m convex loss , m is the number of training images, R (  X  ) is a regularizer that avoids overfitting, and  X   X  R is the regularization coefficient, whose value can be determined by cross validation. been widely studied and understood in the literature. In particular, recent asymptotic analysis of the ` norm and ` 1 /` 2 mixed norm regularized LR proved that under certain conditions the estimated sparse coefficient vector  X  enjoys a property called sparsistency [34], suggesting their applicabil-ity for meaningful variable selection in high-dimensional feature space. In this paper, we employ an LR classifier for our scene classification problem. We investigate content-based compression of the high-dimensional OB representation that exploits raw feature-, object-, and (feature+object)-sparsity, respectively, using LR with appropriate regularization.
 Feature sparsity via ` 1 regularized LR (LR1) By letting R (  X  ) , k  X  k 1 = obtain an estimator of  X  that is sparse. The shrinkage function on  X  is applied indistinguishably to all dimensions in the OB representation, and it does not have a mechanism to incorporate any potential coupling of multiple features that are possibly synergistic, e.g., features induced by the coefficient estimator by  X  F .
 Object sparsity via ` 1 /` 2 (group) regularized LR (LRG) Recently, a mixed-norm (e.g., ` 1 /` 2 ) regularization [36] has been used for recovery of joint sparsity across input dimensions. By letting R (  X  ) , k  X  k 1 , 2 = and k X k 2 is the vector ` 2 -norm, we set the feature group to be corresponding to that of all features induced by the same object in the OB. This shrinkage tends to encourage features in the same group to be jointly zero. Therefore, the sparsity is now imposed on object level, rather than merely on raw feature level. Such structured sparsity is often desired because it is expected to generate semantically more meaningful lossless compression, that is, out of all the objects in the OB, only a few are needed to represent any given natural image. We call such a sparsity pattern object sparsity , and denote the resultant coefficient estimator by  X  O . Joint object/feature sparsity via ` 1 /` 2 + ` 1 (sparse group) regularized LR (LRG1) The group-regularized LR does not, however, yield sparsity within a group (object) for those groups with non-zero total weights. That is, if a group of parameters is non-zero, they will all be non-zero. Translating to the OB representation, this means there is no scale or spatial location selection for an object. To sparsification effects of both shrinkage functions, and yields sparsity at both the group and individual feature levels. This regularizer necessitates determination of two regularization parameters  X  1 and  X  , and therefore is more difficult to optimize. Furthermore, although the optimization problem for ` /` 2 + ` 1 regularized LR is convex, the non-smooth penalty function makes the optimization highly nontrivial. In the Appendix, we derive a coordinate descent algorithm for solving this problem. To conclude, we call the sparse group shrinkage patten object/feature sparsity , and denote the resultant coefficient estimator by  X  OF . Dataset We evaluate the OB representation on 4 scene datasets, ranging from generic natural scene and to complex event and activity images (UIUC-Sports). Scene classification performance is eval-uated by average multi-way classification accuracy over all scene classes in each dataset. We list below the experiment setting for each dataset: Experiment Setup We compare OB in scene classification tasks with different types of conven-tional image features, such as SIFT-BoW [23, 3], GIST [25] and SPM [19]. An off-the-shelf SVM classifier, and an in-house implementation of the logistic regression (LR) classifier were used on all feature representations being compared. We investigate the behaviors of different structural risk minimization schemes over LR on the OB representation. As introduced in Sec 4, we experimented ` regularized LR (LR1), ` 1 /` 2 regularized LR (LRG) and ` 1 /` 2 + ` 1 regularized LR (LRG1). 5.1 Scene Classification Fig.3 summarizes the results on scene classification based on OB and a set of well known low-level feature representations: GIST [25], Bag of Words (BOW) [3] and Spatial Pyramid Matching (SPM) [19] on four challenging scene datasets. We show the results of OB using both an LR classi-and are on par with the 15-Scene dataset. The substantial performance gain on the UIUC-Sports and the MIT-Indoor scene datasets illustrates the importance of using a semantically meaningful representation for complex scenes cluttered with objects. For example, the difference between a liv-ingroom and a bedroom is less so in the overall texture (easily captured by BoW or GIST), but more so in the different objects and their arrangements. This result underscores the effectiveness of OB, highlighting the fact that in high-level visual tasks such as complex scene recognition, a higher level image representation can be very useful. We further decompose the spatial structure and semantic meaning encoded in OB by using a  X  X seudo X  OB without semantic meaning. The significant im-provement of OB in classification performance over the  X  X seudo object bank X  is largely attributed to the effectiveness of using object detectors trained from image. For each of the existing scene datasets (UIUC-Sports, 15-Scene and MIT-Indoor), we also compare the reported state of the arts 5.2 Control Experiment: Object Recognition by OB vs. Classemes [33] OB is constructed from the responses of many objects, which encodes the semantic and spatial information of objects within images. It can be naturally applied to ob-ject recognition task. We compare the object recognition performance on the Caltech 256 dataset to [33], a high level image representation obtained as the output of a large number of weakly trained object classifiers on the image. By encoding the spatial locations of the objects within an image, OB (39%) significantly outperforms [33] (36%) on the 256-way classification task, where per-formance is measured as the average of the diagonal values of a 256  X  256 confusion matrix. 5.3 Semantic Feature Sparsification Over OB In this subsection, we systematically investigate semantic feature sparsification of the OB represen-tation. We focus on the practical issues directly relevant to the effectiveness of OB representation and quality of feature sparsification, and study the following three aspects of the scene classifier: OB.interpretability of predictive features. 5.3.1 Robustness with Respect to Training Sample Size The intrinsic high-dimensionness of the OB representation raises a legitimate concern on its demand on training sample size. We investigate the robustness of the logistic regression classifier built on features selected by LR1 and LRG in this experiment. We train LR1 and LRG on the UIUC-Sports dataset by using multiple sizes of training examples, ranging from 25%, 50%, 75% to 100% of the full training data.
 As shown in Fig. 4(a), we observe only moderate drop of performance when the number of training samples decreases from 100% to 25% of the training examples, suggesting that the OB representa-tion is a rich representation where discriminating information residing in a lower dimensional  X  X n-formative X  feature space, which are likely to be retained during feature sparsification, and thereby ensuring robustness under small training data. We explore this issue further in the next experiment. 5.3.2 Near Losslessness of Content-based Compression via Regularized Learning We believe that the OB can offer an over complete representation of any natural image. Therefore, there is great room for possibly (near) lossless content-based compression of the image features into a much lower-dimensional, but equally discriminative subspace where key semantic information of the images are preserved, and the quality of inference on images such as scene classification are not compromised significantly. Such compression can be attractive in reducing representation cost of image query, and improving the speed of query inference.
 In this experiment, we use the classification performance as a measurement to show how different regularization schemes over LR can preserve the discriminative power. For LR1, LRG and LRG1, cross-validation is used to decide the best regularization parameters. To study the extend of infor-mation loss as a function of different number of features being retained in the classifier, we re-train an LR classifier using features from the top x % percentile of the rank list, where x is a compression scale ranging from 0.05% to 100%. One might think that LR itself when fitted on full input dimen-sional can also produce a rank list of features for subsequent selection. For comparison purpose, we also include results from the LR-ranked features, as can be seen in Fig.4(b,c), indeed its performance drops faster than all the regularization methods.
 In Fig.4 (b), we observe that the classification accuracy drops very slowly as the number of selected features decreases. By excluding 75% feature dimensions, classification performance of each algo-rithm decreases less than 3%. One point to notice here is that, the non-zero entries only appear in dimensions corresponding to no more than 45 objects for LRG at this point. Even more surprisingly, LR1 and LRG preserve accuracies above 70% when 99% of the feature dimensions are excluded. Fig. 4 (c) shows more detailed information in the low feature dimension range, which corresponds to a high compression ratio. We observe that algorithms imposing sparsity in features (LR1, LRG, and LRG1) outperform unregularized algorithm (LR) with a larger margin when the compression ratio becomes higher. This reflects that the sparsity learning algorithms are capable of learning the much lower-dimensional, but highly discriminative subspace. 5.3.3 Profitability Over Growing OB We envisage the Object Bank will grow rapidly and constantly as more and more labeled web images become available. This will naturally lead to increasingly richer and higher-dimensional representa-tion of images. We ask, are image inference tasks such as scene classification going to benefit from this trend? As group regularized LR imposes sparsity on object level, we choose to use it to investigate how the number of objects will affect the discriminative power of OB representation. To simulate what hap-pens when the size of OB grows, we randomly sample subsets of object detectors at 1%, 5%, 10%, 25%, 50% and 75% of total number of objects for multiple rounds. As in Fig.4(d), the classification performance of LRG continuously increases when more objects are incorporated in the OB repre-sentation. We conjecture that this is due to the accumulation of discriminative object features, and we believe that future growth of OB will lead to stronger representation power and discriminability of images models build on OB. 5.4 Interpretability of the Compressed Representation Intuitively, a few key objects can discriminate a scene class from another. In this experiment, we aim to discover the object sparsity and investigate its interpretability. Again, we use group regularized LR (LRG) since the sparsity is imposed on object level and hence generates a more semantically meaningful compression. We show in Fig.5 the object-wise coefficients of the com-pression results for 4 sample scene classes. The object from the feature dimensions of each object (at different scales and spatial locations) learned by LRG. Objects with all zero coefficients in the resultant coefficient esti-mator are not displayed. Fig.5 shows that objects that are  X  X epresentative X  for each scene are retained by LRG. For example,  X  X ailboat X ,  X  X oat X , and  X  X ky X  are objects with very high weight in the  X  X ailing X  scene class. This sug-gests that the representation compression via LRG is vir-tually based upon the image content and is semantically meaningful; therefore, it is nearly  X  X emantically lossless X .
 Knowing the important objects learned by the compres-sion algorithm, we further investigate the discriminative dimensions within the object level. We use LRG1 to examine the learned weights within an ob-ject. In Sec.3, we introduce that each feature dimension in the OB representation is directly related to a specific scale, geometric location and object identity. Hence, the weights in  X  OF reflects the importance of an object at a certain scale and location. To verify the hypothesis, we examine the im-portance of objects across scales by summing up the weights of related spatial locations and pyramid resolutions. We show one representative object in a scene and visualize the feature patterns within the object group. As it is shown in Fig.6(Top), LRG1 has achieved joint object/feature sparsification by zero-out less relevant scales, thus only the most discriminative scales are retained. To analyze how  X  OF reflects the geometric location, we further project the learned coefficient back to the im-age space by reversing the OB representation extraction procedure. In Fig.6(Middle), we observe that the regions with high intensities are also the locations where the object frequently appears. For example, cloud usually appears in the upper half of a scene in the beach class. As we try to tackle higher level visual recognition problems, we show that Object Bank representa-tion is powerful on scene classification tasks because it carries rich semantic level image informa-tion. We also apply structured regularization schemes on the OB representation, and achieve nearly lossless semantic-preserving compression. In the future, we will further test OB representation in other useful vision applications, as well as other interesting structural regularization schemes.
