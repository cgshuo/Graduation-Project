 With the rapid development of the information technology, there exists the difficulty in deploying state-of-the-art re-trieval models in environments such as peer-to-peer networks and pervasive computing, where it is expensive or even in-feasible to maintain the global statistics. To this end, this paper presents an investigation in the validity of different statistical assumptions of term distributions. Based on the findings in this investigation, a variety of weighting mod-els, called NG (standing for  X  X o global statistics X ) models, are derived from the Divergence from Randomness frame-work, in which only the within-document statistics are used in the relevance weighting. Compared to the state-of-the-art weighting models in extensive experiments on various stan-dard TREC test collections, our proposed NG models can provide acceptable retrieval performance in ad-hoc search, without the use of global statistics.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Experimentation, Performance, Algorithms Probabilistic models, Term frequency distribution fitting, Within-document statistics
The probabilistic models are among the most popular in-formation retrieval models for their efficiency and effective-ness. The BM25 probabilistic weighting function [9], the PL2 Divergence from Randomness (DFR) model [1] and the KL-divergence language model (KLLM) [13], have been showneffectiveinTRECexperimentation 1 .Allofthese Although these three weighting models are based on differ-three models consider the global term statistics, e.g. the document frequency and the collection-wide term frequency, for the relevance weighting. The use of global statistics in the weighting models can be beneficial to the retrieval ef-fectiveness. However, in large-scale Web applications such as the distribution IR, peer-to-peer network and pervasive computing, it is difficult or even infeasible to maintain the global statistics [2, 11]. To this end, the aim of this paper is to explore the possibility of developing the fairly simple retrieval models that use only within-document statistics for the relevance weighting.

The main contribution of this paper is tri-fold. First, we study the term frequency distribution on various recent IR datasets, results show that other than the classical Poisson assumption proposed by Harter in 1975 [4], there are quite a few appropriate approximations of the actual term frequency distribution in recent datasets. Second, we propose a family of NG (No Global statistics) weighting models that use only the within-document statistics for the relevance weighting. Evaluation shows that the weighting models based on the Weibull approximation of the term frequency distribution demonstrates the best retrieval performance and robustness. Third, we develop a new weighting framework that is a sim-plified form of the DFR framework which can improve not only the retrieval performance, but also the robustness by reducing the parameter sensitivity.

The remainder of this paper is organized as follows. Sec-tion 2 introduces the related work. Section 3 applies a list of distribution functions to fit the actual term frequencies in standard TREC collections. A list of new NG weighting models using only the within-document statistics for rele-vance weighting are proposed in Section 4. Evaluation re-sults of these new models are then presented in Section 5. Finally, Section 6 concludes the work and suggests possible future research directions.
A well-known empirical description of the term frequency distribution in text collection is the so-called Zipf X  X  law, in which a given term X  X  collection-wide frequency is inversely proportional to its rank in the frequency table [15]. Luhn X  X  ent assumptions of relevance, they are usually considered to be within the category of probabilistic models [7]. work in 1957 is among the earliest research on the term fre-quency distribution and IR. His research gave an outline for building a system based on statistical method for literature searching by machine [5]. Later in 1975, Harter proposed the 2-Poisson assumption for keyword indexing, in which the informative terms in the documents, namely specialty words, follow Poisson distribution in both the entire docu-ment collection, and the elite set [4].

The 2-Poisson assumption has been widely recognized as the standard approximation of the term frequency distribu-tion. The BM25 and PL2 DFR models, both derived based on the 2-Poisson assumption, are currently among the most popular and effective IR models. The formulas of these two models, as well as the KLLM model, are introduced below. These three popular probabilistic models are used as the baselines in our evaluation.

As one of the most established weighting models, BM25 computes the relevance score of a document d for a query Q by the following formula [9]: where qtf is the query term frequency; w (1) is the idf factor, which is given by: N is the number of documents in the whole collection. N t is the document frequency of term t . K is given by k ((1  X  b )+ b l avg l ), where l and avg l are the document length and the average document length in the collection, respectively. The document length refers to the number of tokens in a document. k 1 , k 3 and b are parameters. The default setting is k 1 =1 . 2, k 3 = 1000 and b =0 . 75 [9]. qtf is the number of occurrences of a given term in the query; tf is the within document frequency of the given term. term frequency, we obtain: Hence the term frequency normalization component of the BM25 formula can be seen as:
The Kullback-Leibler divergence language model (KLLM) with Dirichlet smoothing assigns the relevance score as fol-lows [13, 14]: score ( d, Q )= where p ( t |  X   X  Q ) is the maximum likelihood of generating query term t from a query model. p ( t |C ) is the generation proba-bility from the collection model. In this paper, the free pa-rameter  X  is set by simulated annealing on training queries.
PL2 is one of the weighting models derived from the Di-vergence from Randomness (DFR) framework. The DFR framework assigns the relevance score of a document d for a query Q as follows [1]: where qtf is the query term frequency. The first measure-ment of the information amount Inf 1 is given by the infor-ity of having tf occurrences of query term t in document d . The second measurement of the information amount Inf 2 is given by Laplace succession 1 tfn +1 . The normalized term frequency tfn is given by Normalization 2: where the recommended setting of free parameter c ranges from 1 to 7, depending on the search task.

Assuming the Poisson distribution of the term frequency in the collection leads to the PL2 model, where the relevance score of a document d for a query Q is given by [1]: where  X  is the mean and variance of the assumed Poisson distribution. Estimation of  X  requires the global statistics of the term X  X  distribution in the whole document collection.
In this paper, we propose to eliminate the need for global statistics in the DFR models by treating the P ( t, tf | d )as a function of the within-document term frequency tf ,and its parameters of the term frequency distribution function. Various distribution functions are tested in our study on the distribution fitting in the following sections. In addition, as explained in Section 4.2, the average document length avg l is obtained by averaging the document length of random samples from the collection.
Harter X  X  [4] study on the term frequency distribution was based on the analysis on a sample of the archives at the Graduate Library of University of Chicago. With the fast growth of the internet in both size and the amount of in-formation in text documents or Web pages, an interesting research question arises: do the terms still follow the Poisson distribution in recent datasets?
In this section, we attempt to answer the above research question by using a list of statistical distribution functions (Section 3.1) to fit the term frequency distribution in stan-dard TREC collections, as described in Section 3.2. Detailed analysis on the distribution fitting is provided in Section 3.3.
We study a variety of distribution functions as follows: fitting.
We use four standard TREC test collections in our study: disk1&amp;2, disk4&amp;5, WT10G and .GOV2. For all four test collections used, each term is stemmed using Porter X  X  En-glish stemmer, and standard English stopwords are removed. Only the query terms in the title field are used.
Weighted Least Squares (WLS) regression is used to deter-mine the optimal parameters in a given distribution function by fitting the target function, namely the empirical cumu-lative distribution function (ECDF). The use of the CDF function instead of the probability density function (PDF) Table 1: Information about the test collections used. as the target of the distribution fitting is a common practise because CDF can uniquely determine a distribution, while PDF may not, for example when the PDF of a given dis-tribution cannot be derived, or simply does not exists. The mean sum of square error ( SE ) between the estimated dis-tribution and the observed distribution of all query terms in the title field is used to indicate the goodness of the fitting.
In Harter X  X  work, the distribution fitting is done on the raw term frequencies [4]. We base our investigation on the distribution of the normalized term frequency ( tfn )instead. For space reason, we only report the results obtained using BM25 X  X  default setting b =0 . 75.

Figure 1 plots the P-P figures between the fitted proba-bilities and the empirical probabilities on the four datasets used. The average linear correlation coefficients between ECDF and CDF of the Poisson,  X  2 , Exponential, Gamma, Rayleigh, Weibull distribution are 0.9908, 0.9735, 0.9743, 0.9904, 0.9874, and 0.9832 respectively. It shows that all the six distributions can fit the term frequency distribution to some extent on the four test collections being used.
Finally, we conduct ANOVA to investigate the differences of the fitness among the distributions being used. In Fig-ure 2), objects on the left have better fitting goodness than its right ones. Vertical dashed lines separating the objects shows the significant fitness differences in the group. Ac-cording to Figure 2, Gamma and Weibull distributions demon-strate better fitness than the others.
We describe our proposed NG models (no global statistics models) in Section 4.1, and estimate the average document length, which is a global variable, in Section 4.2.
Our proposed NG models follow the DFR framework as introduced in Equation (4). Under the general probabil-ity platform of DFR framework, we can generate new NG functions on the four datasets used. weighting models by replacing the probability P ( t, tf | the proper function form which can fit the real normalized term frequency distribution well.

Table 7 explains how the proposed NG models are gen-erated named. For example, the WL2d model assigns rele-vance score as follows: score ( d, Q )= where the probability P ( tf, t | d ) is estimated by Weibull (W) distributioninEquation(10), Inf 2 is given by Laplace suc-cession (L) as explained in Table 7. The normalized term frequency tfn is given by Normalization 2 in Equation (5). A d at the end of the model name stands for within-document term statistics, indicating that the model is derived from the DFR framework in Equation (4). The distribution pa-rameters  X  and k are treated as free parameters that require tuning on training queries.

A notable difference between our proposed NG models and the state-of-the-art probabilistic models is that the for-mer does not involve the use of the global statistics. How-ever, we need to apply term frequency normalization, such as the normalization 2 in Equation (5) and BM25 X  X  normaliza-tion method in Equation (2), to cope with the bias towards long documents. Both of the above mentioned normaliza-tion methods use an expected document length, given by the average document length in the entire document collec-tion, as a normalization factor. In this paper, we propose a method to get the average documents length by estimating through random sampling in the collection and our stimu-lating experiments have shown good estimating results.
We use Systematic Sampling to estimate the average doc-ument length in the tf normalization part. A unique feature of Systematic Sampling is that it can give a stable estimate of the random variable, i.e. the average document length Table 2: The estimated ( EstL ) and the actual aver-age document length ( avg l ) on the four test collec-tions used, and the error in percentage.
 Table 3: The average error rate (Avg.), maximum positive error rate (MaxPos), minimum negative er-ror rate (MinNeg) of the 10,000 estimated average document length. Avg. is the mean of the absolute values of MaxPos and MinNeg.
 ( avg l ) in our case, with only few samples from the dataset, which is therefore very suitable for our study.

Our stimulated tests of the estimates of the average docu-ment length on four datasets show that most of the sampled average document length EstL fall around the actual aver-age document length, showing the reliability of the sampling method. Some detailed statistics of the tests are shown in Table 3. The table shows that a descent approximation re-sult is observed in the 10,000 times of systematic sampling experiments. When sampling 2.5% of the collection X  X  docu-ment length, the average error rate is less than 4%, and the maximum error rate is less than 10%.
Section 5.1 introduces the evaluation settings and Section 5.2 presents the experimental results. Our proposed NG models are evaluated against the KLLM, BM25, and PL2 weighting models, which are among the most effective weighting models as shown by the evaluation results in the literature such as the TREC experimentation [12]. The experiments are conducted using an in-house ver-sion of the Terrier toolkit [8].

We again use the same TREC collections and their associ-ated title-only queries as in Section 3.2. On each collection, (a) Gamma distribution X  X  pa-rameter  X  (see Eq. 8) (c) Weibull distribution X  X  pa-rameter k (see Eq. 10) we evaluate the NG models by a two-fold cross-validation. The test topics associated to each collection are split into two equal-size subsets by parity. We use one subset of top-ics for training, and use the remaining subset for testing. The overall retrieval performance is averaged over the two test subsets of topics. We use the TREC official evalua-tion measures in our experiments, namely the Mean Aver-age Precision (MAP) [12]. All statistical tests are based on Wilcoxon matched-pairs signed-rank test at the 0.05 level. Table 4: Experimental results of the baselines.

The evaluation results of the NG models are presented in this section. Tables 4 and 5 contain the evaluation results of the three state-of-the-art probabilistic models and our proposed NG models, respectively. According the results, we have the observations as follows:
First, the NG models based on Weibull and Poisson distri-butions provide the best retrieval performance, while those based on the Rayleigh and  X  2 distributions are not as good as expected. This indicates that the effectiveness of the mod-els in the term frequency distribution fitting and the retrieval is not necessarily completely related.

Second, the use of two different normalization methods leads to in general comparable retrieval performance of our proposed models, which conforms to our previous finding in Section 3.3 that these two different normalization methods lead to similar fitting effectiveness.
 Third, our proposed NG models based on Weibull and Poisson distribution can lead to comparable retrieval per-formance with the baseline models as the performance is sometimes not significant different between our models and the BM25 model 5. As a matter of fact, without using global statistics, we indeed have less information than the baseline models, for instance the collection information, e.g. the doc-ument frequency and the term frequency in the collection. So our models X  performances mainly depend on what distri-bution function to use and how to use them to describe the real situation.

Finally, Figure 3 examines how the parameter setting af-fects the NG models X  retrieval performance. From the ex-periments, we find that the retrieval performance of the NG models are highly sensitive to their parameters of their un-derlying distribution functions, which can potentially hurt the robustness of the models. In the next section, we im-prove the robustness by a simplified DFR framework.
In this section, we propose to improve the robustness of the NG models. The underlying idea of our method is to fit the relevance scores produced by a given NG model using a function that is more simple than the information content components in the original DFR framework (i.e. Inf 1  X  Inf in Equation (4)). In IR applications, it is a common prac-tise to simplify the mathematical forms that are relatively complicated, in order to achieve an easier implementation or a better robustness. For example, Robertson &amp; Walker simplified the 2-Poisson model by fitting the actual term fre-quency distribution, which eventually led to the proposal of the BM25 model [9, 10].

With an idea similar to [10], we propose a simple form of the DFR framework as given by the following function:
Following the similar steps of the distribution function fitting in Section 3, we obtain  X  = 1. Thus, a simplified DFR framework is given as follows:
In Table 6, the retrieval performance of the simplified NG models is compared to the state-of-the-art models, and the NG models. The Poisson distribution is not suitable to the simplified DFR framework because of the difficulty caused by expanding the factorials using Stirling formula [1]. From Table 6, we can see that the simplified NG models achieve statistically significant improvement over the NG models on most cases, showing that the simplified DFR framework can indeed enhance the retrieval performance of the NG models.
Moreover,fromFigure3,wecanseethatthesimpli-fied NG models markedly reduce the parameter sensitivity. Overall, the simplified NG models based on the Weibull esti-mation of the term frequency distribution, especially W2dS, has demonstrated the best retrieval effectiveness and robust-ness out of all the NG models proposed. Therefore, it is rec-ommended to apply the W2dS model in large-scale IR appli-cations where it is difficult to maintain the global statistics.
We have conducted a thorough study of the term fre-quency distribution on recent TREC collections. Six differ-ent distribution functions are used to fit the actual frequency distribution of query terms from TREC collections. Our ex-perimental results show that apart from Poisson distribution there are other probabilistic models of term occurrences are suitable for describing the term frequency distribution in document collections.
 Based on the above finding, we have proposed a list of the NG models generated from the DFR framework. A unique feature of the NG models is the exclusion of global statistics. Extensive experiments on four TREC test collections show that our proposed NG models can provide acceptable re-trieval performance for ad-hoc search. In addition, we have improved the robustness of the NG models by fitting rele-vance scoring fitting using simplified NG models. Finally, it is recommended to apply th W2dS model based on the Weibull distribution in large-scale IR applications, for its effectiveness and robustness.

While our proposed models X  evaluation results are not good enough, and as to we have conducted a wide investiga-tion about which distributions can be deployed, we plan to refine the form of the distributions appeared in the weighting formula in the future to improve the performance.Besides. Devising a method that automatically estimates the param-eters on a per-term basis without the use of global statistics is also scheming. Another future research direction is to in-vestigate the application of query expansion and the term proximity models [6] on top of the NG models. Finally, we plan to investigate the effectiveness of the proposed NG models in large-scale distributed IR applications such as in peer-to-peer networks or sensor networks in practise.
