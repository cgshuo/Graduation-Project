 2 Zhao Xu, 2 and Daniel Schulz 2
Data mining systems that process noisy sensory input often treats tasks involved independently of each other. Although this makes the systems comparatively easy to assemble, and helps to contain computational complexity, it comes at a high price: information from one task cannot be utilized to help processing the other so that errors made in one task cannot be corrected, see e.g. [1]. Examples of this can be found in information extraction, natural language processing, speech recognition, vision, robotics, etc. In this paper, we are triggered by a market relevant application of pricing outdoor poster sites.

The German outdoor advertising market has a yearly turnover of 1,200 million dollars. The  X  X achverband Au X en-werbung e.V. X  (FAW), which is the governing organization of German outdoor advertising, provides performance indi-cators on which pricing of poster sites is based. The value of each site is composed of a quantitative measure, namely the number of passing vehicles, pedestrians, and public transport, cf. Fig. 1, and of a qualitative measure which specifies the expected notice of passers-by. This is a very rare if not unique case where a spatial data mining model [2], [3], [4] is business critical for a whole branch of industry, comprising a large number of companies. Specifically, the spatial data mining model proposed has become the basis for pricing of posters for that branch of industry and can be used for selecting new poster sites and for planning cam-paigns. Moreover, the outdoor advertising sector considers the model to be a milestone for their business; the reliability of the prediction result seems to be accepted by dozens of companies in this area.

Although much effort has gone into developing the model for pricing posters and indeed it has improved steadily over the years, it still has important drawbacks. In the present paper, we focus on the following ones:  X  Regression tasks involved are treated independently.  X  Furthermore, there is a lot of uncertainty involved So, to address both problems, we would ideally like to per-form joint Bayesian inference for both tasks simultaneously. Current systems, however, treat the regression tasks inde-pendently and employ non-Bayesian regression approaches. Although recent progress in probabilistic inference and machine learning has begun to make joint inference pos-sible. For example, multi-output and multi-task regression approaches [5], [6], [7], [8], transfer learning [9], [10], [11], [12], and the emerging field of statistical relational learning (SRL) [13], [14] are essentially concerned with learning from data points that are not independent and identically distributed. However, setting up a joint inference model is usually rather complex, and the computational cost of running it can be prohibitive. This is exactly the problem we will address in this paper.

Specifically, we propose stacked Gaussian process learn-ing , a novel meta-learning scheme in which a base Gaussian process is enhanced by adding the posterior covariance functions of other related tasks to its covariance function in a stage-wise optimization. The benefits are three-fold:  X  In terms of the application, Gaussian processes are at- X  Sometimes, however, it can be quite difficult  X  if not  X  Finally, stacked Gaussian process learning is very effi-We proceed as follows. We start of by touching upon related work. Then, we review regression with standard Gaussian processes in Section III and relational ones in Section IV. In Section V, we develop stacked Gaussian process learning. Afterwards, Section VI presents our experimental evaluation on several real-world dataset. Then, we conclude.

Stacked Gaussian process learning is the first application of relational Gaussian processes to a market relevant regres-sion problem, namely flow prediction in public transporta-tion and street networks. Scheider et al. [3] proposed to use off-the-shelves linear regression and model tree approaches whereas May et al. [4] investigated non-stationary k-nearest-neighbour approaches. In contrast to stacked Gaussian pro-cesses, they neither provide a principled non-linear Bayesian framework nor cross-domain/joint learning of both tasks.
Algorithmically, stacked Gaussian process learning is re-lated to several recent machine learning and data mining approaches. Probably the closest work is stacked graphical learning [18], [19], [20]. So far, this line of research has focused on stacking Markov networks. There is, however, no reason why Markov networks should be the only repre-sentation of choice for symmetric dependence, i.e., relational structures. In this paper, we extend stacking to the case when relationships are postulated to exist due to hidden common causes [16]. As Silva et al. [16] have shown, this corresponds to a graphical representation called directed mixed graph (DMG), with bi-directed edges representing the relationship of having hidden common causes between a pair of vertices. However, regression has not been considered for the mixed-graph Gaussian process framework so far.

Finally, stacked Gaussian processes are related to multi-output and multi-task regression approaches [5], [6], [7], [8], transfer learning [9], [10], [11], [12], collaborative learning [21], [22], and the emerging field of statistical relational learning (SRL) [13], [14]. Here, the work of Yu et al. [5] is probably the closest. Whereas Yu et al. propose an hierarchical Bayes approach, learning the shared hyperparameters using EM, we turn the EM into a boosting like stage-wise optimization of the intra-task covariance matrices. This simplifies the approach and allows one to im-plement a simple version using any off-the-shelves Gaussian process approach, even relational ones.
 The flow prediction problems can be formulated as a Bayesian regression task. In this section, we will show how to encode it with Gaussian process (GP) regression models; here using local attributes only and then utilizing relations in Sec. IV. For a comprehensive introduction and further details on GPs, we refer to [23].

Assume that each segment, i.e., a street or segment of a public transportation line (represented as the center of its bounding box) at which we want to predict a flow is associated with attributes x i  X  R D and an observation y  X  R . The attributes include, e.g., the numbers of public buildings, touristic sites, restaurants near the segment, and so on. The observation y i is some log-transformed flow measurement, i.e., numbers of people passing the segment. The log-transformation of the measurements is necessary since flows are restricted to be positive. Fig.2 illustrates the GP regression model. In a GP framework, the observation is modeled as a noise contaminated function value f ( x (shortened as f i in the rest of the paper) of its attributes x that is where i is Gaussian noise with variance  X  2 . The function value f i is the real, i.e., noise-free flow, which is unob-servable. Since the form and parameters of the function is unknown and random, f i is also unknown and random. Now, the function values { f 1 ,f 2 ,... } of an infinite number of segments can be represented as an infinite dimensional vector, i.e., the i  X  X h dimension is the function value f We assume that the infinite dimensional random vector follows a Gaussian process prior with mean function m a ( x and covariance function k a ( x i ,x j ) . Here, the subscript a emphasizes that the mean and covariance functions are attribute-wise. In turn, any finite set of function values { f i : i = 1 ,...,n } has a multivariate Gaussian distribution with mean vector and covariance matrix defined in terms of the mean and covariance functions of the GP, see e.g. [23]. Without loss of generality, we assume zero mean so that the GP is completely specified by the covariance function. A typical choice is the squared exponential covariance function with isotropic distance measure: where  X  and  X  are parameters of the covariance function. x i,d denotes the d -th dimension of the attribute vector x
Formally, for a set of n segments with attributes X = { x 1 ,...,x n } , the multivariate Gaussian prior distribution of the function values f = ( f 1 ,...,f n ) T is written as: Here, K a denotes the n  X  n covariance matrix whose ij -th entry is computed in terms of the covariance function with the corresponding attributes x i and x j .

Since the observations Y = ( y 1 ,...,y n ) T are the func-tion values with noise, cf. Eq. (1), their joint distribution is: where I is n  X  n identity matrix.

For a new segment with attributes x  X  , the predictive distri-bution of the function value f  X  given the noisy observations Y can be computed as follows. Due to the properties of GPs, ( f,f  X  ) T is still Gaussian with zero-mean and covariance matrix where k a ( x  X  ,x  X  ) is the variance of the new function values f , k a ( X,x  X  ) is a n -dimensional column vector, which k ( X,x  X  ) , i.e. k a ( x  X  ,X ) = k a ( X,x  X  ) T . Introducing the noise terms i , we can write the joint distribution of the observations Y and the new function values f  X  as Finally, the predictive distribution of f  X  is computed in terms of the formula on conditional Gaussian distributions. Specifically, we have where
Public transportation systems can elegantly be represented using entities and relations. Specifically, they are collections of segments (entities) that are interconnected by streets or public transit lines (relations). Intuitively, we would like to make use of this additional knowledge: our prediction at one segment should help us to reach conclusions about flows at other, related segments. Reconsider Fig. 1. There, the bus line denoted as thick, black line connects several segments.
Several GP variants exactly doing this have recently been proposed [24], [25], [16], [17]. In this paper, we extend Silva et al.  X  X  mixed graph Gaussian processes (XGP) [16] to model flows in public transportation system. The XGP model, which represents the relational information as hidden common causes, is a straightforward way to incorporate relations into Gaussian processes, and has successfully been applied to classification and ranking problems.

Assume there are not only segment attributes X and flow observations Y but also relations R = { r i,j : i,j  X  1 ,...,n } such as bus lines or streets between segments. All relations the segment i participates in are denoted as r is natural to assume that the flow of a segment depends on both the attributes x i of the segment and the relations r i the segment participates in. To include r i into the GP model, we introduce another function value g ( r i ) (shorten as g i ) for each segment shown as Fig.3. Now, a segment is associated with two function values: f i representing the flow predicted based on segment attributes, and g i representing the flow based on relations. The overall function value (noise-free flow)  X  i of a segment is a weighted sum of the two components where  X  0 is a mixture weight. The function value g i encodes some hidden common causes from relations. The term hidden is due to the fact that the relational information encoded over segments is only implicit in contrast to the commonly used information on segment attributes. Now, the observation y i is the overall, noisy function value
Similarly to the attribute-wise function values, we place a zero-mean GP prior over { g 1 ,g 2 ,... } . Again, { g 1 ,...,n } follow a multivariate Gaussian distribution. In contrast to the attribute-wise GPs, however, the covariance function k r ( r i ,r j ) should represent correlation of segments i and j due to the relations. There are essentially two strategies to define such kernel functions. The simplest way is to represent the known relations of segment i as a vector. The kernel function k r ( r i ,r j ) can then be any Mercer kernel function, and the computations are essentially the same as for the attributes. Alternatively, we notice that segments and relations form a graph, and we can naturally employ graph-based kernels to obtain the covariances, see e.g. [26], [27], [16]. The simplest graph kernel might be the regularized Laplacian kernel where  X  and  X  are two parameters of the graph kernel.  X  denotes the combinatorial Laplacian, which is computed as  X  = D  X  W , where W denotes the adjacency matrix of a weighted, undirected graph, i.e., w i,j is taken to be the weight associated with the edge between i and j . D is a diagonal matrix with entries d i,i = P j w i,j .

The function values f = ( f 1 ,...,f n ) T and g = ( g 1 ,...,g n ) T are both Gaussian, thus their weighted sum  X  = (  X  1 ,..., X  n ) T also follows a multivariate Gaussian distribution. Its mean is the weighted sum of the means of the two independent Gaussian distributions. Since we assume zero means for f and g , the mean of  X  is also zero. The covariance between  X  i and  X  j is computed as Thus, the covariance matrix for  X  is Here, K a (resp. K r ) denotes the n  X  n covariance matrix whose ij -th entry is computed with the corresponding co-variance function. Finally, the prior distribution of the overall function value  X  is defined as The distribution for the noisy observations Y is Compared to Eq. (4), the only difference is replacing K with K , the enhanced covariances between interrelated seg-ments, Eq. (12).

In the XGP framework, making flow predictions on segments is considered in a transductive learning setting, i.e. there is no new segment introduced in prediction particular, the flows of some of the n segments are missing and the task is to predict them given the observed segments. Let  X  = (  X  L ,  X  U ) T be the vector composed of  X  and  X  U , i.e., the function values (noise-free flows) for the observed and unobserved segments, respectively. In analogy to predictive inference in standard GP regression models, see Eq. (6), we introduce the noise terms i and obtain the joint distribution of the observations Y and the function values  X  U of the unobserved segments as follows where K U,L are the corresponding entries of K between the unobserved segments and observed ones. K L,L , K and K L,U are defined equivalently. I | L | is an identity matrix, where the dimensionality is the number of observed segments. Finally, the predictive distribution of  X  U given Y is computed as where So far, we have assumed that the relations are given. But where do the relations come from? Often, a domain expert determines the relations. This can be difficult and expensive. Collecting additional data that can be used to learn the relations, however, is often easier. Specifically, we often have data for several, related regression tasks. In our application of pricing posters, for example, we not only have observed public transit flows but also pedestrian flows: when we predict a high number of pedestrians at one location, it is very likely that we also observe a larger number of people on buses, and vice versa. In this Section, we will show how to extract automatically hidden common cause relations from Gaussian process models of the related regression tasks. Specifically, we extend the XGP framework to stacked Gaussian processes (SGPs). SGPs are a meta-learning scheme in which a base Gaussian process is enhanced by adding the posterior covariance functions of other related tasks to its covariance function in a stage-wise optimization. The idea is that the stacked posterior covariances encode the hidden common causes among variables of interest that are shared across the related regression tasks. In other words, all tasks influence each other so that the learning and prediction on one task is based on the other ones. The process is iterated until convergence.

Assume that for each segment, there is pedestrian in-formation z i associated, including pedestrian attributes  X  x and pedestrian relations  X  r i . Additionally we have a flow observation  X  y i . We introduce a new function value (shorten as  X   X  i ) to encode the additional hidden common causes from the pedestrian data. Thus, the noise-free flow  X  on public transportation is the weighted sum of all causes (Fig.4): where  X  1 is the mixture weight for the new cause.

The distributions of f i and g i are given. To obtain the distribution of  X  i , however, we need the distribution of To solve the problem, we start off with an XGP model for the pedestrian data without considering the public transportation data. As discussed in Section IV,  X   X  i can be decomposed as where  X  f i and  X  g i denote the function values of  X  x i.e.,  X  f i = f ( X  x i ) ,  X  g i = g ( X  r i ) , which follow zero-mean Gaussian with covariance matrices  X  K a and  X  K r , respectively. For the computations of  X  K a and  X  K r , we refer back to Section IV. The main idea for exploiting the observations on the pedestrian flow is now as follows: More formally, the posterior of  X   X  i is used to compute the distribution of  X  i (Eq. (17)). In this way, all the infor-mation on pedestrian attributes  X  X , pedestrian relations and pedestrian observations  X  Y is transfered to the public transportation task.

To be more precise, the posterior distribution of the function values  X   X  = (  X   X  1 ,...,  X   X  n ) T (noise-free pedestrian flows) can be found to be: where  X   X  2 is the variance of the noise for the pedestrian mean  X   X  i and variance  X   X  2 . P (  X   X  |  X  X ,  X  R ) is the prior of which is zero-mean Gaussian with the covariance matrix  X  K =  X  K a +  X   X  2 0  X  K r (referring to Eq.12).  X   X  and mean and covariance matrix of the posterior distribution where Since the function values f , g and  X   X  are three independent Gaussians, their sum  X  is still Gaussian with mean m and Y )  X   X  =  X   X (  X   X   X  1  X  m +  X   X   X  2  X  Y ) ) covariance  X  where K a and K r are covariances on transportation at-tributes and relations, respectively. The matrix  X   X  is the posterior covariance of the pedestrian data.

Making predictions is straightforward. Given some ob-served public transportation frequencies Y , the predictive distribution of the unobservable noise-free frequencies  X  essentially computed as in Eq. (16). We only have to replace the covariance matrix K by  X  . Again, the prediction exploits transportation attributes ( K a ), transportation relations ( K and the additional hidden common cause relations ( transfered from the related task.

Indeed, nothing prevents us from also  X  X oosting X  the public transportation hidden common causes into the pedes-trian model in an equivalent way. As the new posterior covariances of both GPs might change, stacked Gaussian process (SGP) learning iterates the processes until conver-gence (probably discounting mixture weights over time) as summarized in Table I.

Note that in the iteration step, the update of transportation flow prior (  X  and m ) in Task 1 uses the pedestrian posterior (  X 
 X  and  X   X  ) in Task 2, and vice versa, by which the infor-mation propagates between the tasks. Thus, SGP essentially implements a simple form of joint learning. For numerical stability, one can use the formula ( A  X  1 + B  X  1 ) A  X  A ( A + B )  X  1 A to compute the covariance matrices.
The computational complexity of the proposed stacked learning algorithm is O ( n 3 ) per task and iteration, where n is the number of training cases. Thus, stacked Gaussian processes scale as standard Gaussian processes with a linear overhead depending on the number of iterations. Experi-ments have shown that already one iteration of stacking often yields significant improvement.

Our intention is to investigate the following question: (Q) Does stacked Gaussian process learning improve To answer the question, we implemented the stacked Gaus-sian process learning in Matlab using Rasmussen and Williams X  Gaussian Process library [23] and compared base models with their stacked counterparts.
 A. Real-World Datasets We used the datasets described in detail by Scheider and May [3] and by May et al. [4]. They consist of pedes-trian flow respectively public transport flow observations of N several German cities together with the underlying network information, i.e., attributes describing stops and segments as well as the involved bus and tram lines. Sufficient observed data for sound evaluation is only available for a few cities. So, we considered the subset containing the 4 German cities Frankfurt, Kaiserslautern, Ulm, and Konstanz.

Each segment is described in terms of a set of attributes that are deduced from the characteristics of the segments X  bounding stops and the respective line information. The most important feature is the schedule frequency of lines which measures the traffic pressure between stops. An additional attribute marks the type of the line, i.e., bus, tram, or underground. Furthermore, there are attributes characterising the attractiveness of stops measured by the spatial density of several points of interest (POI) like touristic and cul-tural facilities, public utilities, restaurants, and hotels. In particular, the attribute values are aggregated number of POI within a POI specific spatial radius around stops. We refer to [3], [4] for more details. Note, however, that the pedestrian data provided slightly different attributes than the public transportation data such as street type and boolean values for footpath and pedestrian area.

Additionally, we also extracted graphs/relations among the segments from the available network information. More precisely, for the pedestrian data, we compute an  X  nearest neighbour graph encoding relations among segments on the basis of the pedestrian data in the underlying street network. The public transportation network, however, had to be first turned into a segment-based representation. Consider Fig. 5. It shows the original network on the left and the segment-based representation on the right. Essentially, each edge in the original representation becomes an node in the new, segment-based representation. Edges between two segments i and j exist due to the following expert-defined relations weighted with w ij : The general statistics of both the public transport and the pedestrian dataset for the different cities are summarised in Table II, which shows range ( y min ,y max ) , mean  X  y and standard deviation  X  y of the noisy observations and the number of labeled and unlabeled data ( N L ,N U ) . Frankfurt is the only considered city having an underground. In Ulm we predict passenger frequencies for trams and buses considering the joint public transport network and in the other two cities only buses serve as public transport means. B. Methodology On the resulting 4 datasets, we compared several models. Specifically, we trained GPs taking no relational information into account (denoted as GP) as well as relational variants (denoted as XGP) as base learners. For both pedestrian and public transport models, we used the squared exponential covariance function with different length-scales for each input dimension. To treat the relational information, we used a regularized Laplacian kernel [26]. The mixture weights were selected using cross-validation on the training set. Hyperparameter optimization was carried out using Rasmussen and Williams X  conjugate gradient maximization of the log marginal likelihood. Since traffic flows are constrained to be non-negative, a log-transformation of the observation space was used. Each dataset was split randomly into 2/3 training and 1/3 test set. On the test set, we computed several evaluation measures to gain a complete picture of the predictive performance of the trained models. Specifically, we used: Coefficient of Determination: Root Mean Squared Error: Mean Absolute Error: Negative Log Predictive Density: Then, stacked learning was initiated using the learned base models yielding their stacked counterparts, denoted as SGP and SXGP. As a baseline approach, we also evaluated a distance-weighted 5 -nearest-neighbour method. Each exper-iment was ran 10 times and we report the averaged values. Statistical significance was assessed using a two-sample t-tests with 95% confidence intervals. All experiments were conducted on a standard Windows laptop PC.
 C. Results
Table III summarizes the experimental results. Addition-ally to the performance measures averaged over 10 reruns with random initialization, it also indicates whether the results are significant or not. Specifically, we tested on sig-nificant performance improvements for the following pairs of learners: kNN vs. GP, GP vs. SGP, GP vs. XGP, and XGP vs. SXGP.

As one can see, standard GPs significantly improve the kNN method by 3 error measures in 3 out of 4 cases for the task of predicting public transport passenger flows. More-over, the results clearly show that GP regression benefits from relational information. In particular, stacked GPs per-formed significantly better than standard GPs for half of the considered datasets on both tasks. The XGP using  X  X xpert X  relations yields significantly better predictive performance on nearly all evaluation measures. Moreover, stacking XGP clearly shows a significant performance gain, too. In 3 out of 4 cases the stacked XGP significantly outperformed un-stacked XGP on both tasks, the prediction of public transport passenger and pedestrian flows. Table IV summarizes this overall cumulative  X  X ignificant win X  statistics for all consid-ered pairs of learners (see above).
To summarize, the extensive set of experiments clearly answers (Q) affirmatively.

Joint inference is currently an area of great interest in information extraction, natural language processing, statisti-cal relational learning, and other fields. Despite its promise, joint inference is often difficult to perform effectively, due to its complexity, computational cost, and sometimes mixed effect on accuracy.

In this paper, we have shown how these problems can be addressed in a market relevant application of pricing outdoor poster sites. Specifically, we have introduced stacked Gaussian process learning, a meta-learning scheme in which a base Gaussian process is enhanced by adding the posterior covariance functions of other related tasks to its covariance function in a stage-wise optimization. The idea is that the stacked posterior covariances encode the hidden common cause relations among variables of interest that are shared across the related tasks. The evaluations on several real-world datasets indicate that regression with stacked Gaussian processes can indeed improve upon the performance of non-joint approaches significantly.

Compared to other joint Bayesian learning approaches, stacked Gaussian process learning is efficient as it can be implemented with virtually no overhead using of-the-shelves Gaussian process models. This property allows it to be easily used even by non-experts and to be very competitive in applications where efficient Bayesian inference algorithms are important.

Future work will compare stacked Gaussian process learn-ing to other joint Gaussian process models and (relational) graphical models such as Markov logic networks and re-lational Markov networks. We are also considering further applications of stacked Gaussian process learning to joint inference problems in domains such as information retrieval and natural language processing.

From the application perspective, we modelled an impor-tant and interesting aspect. Nevertheless, there are a lot of other open machine learning and data mining problems as for instance how to extrapolate flows to cities where we do not have observations for one or even both of the prediction tasks.

The authors would like to thank Michael May for the constructive discussions and the anonymous reviewers for their helpful comments. We also thank the German professional association for outdoor advertising  X  X achverband Au X enwerbung e.V. X  (FAW) for kindly providing the data. Kristian Kersting and Zhao Xu were supported by the Fraunhofer ATTRACT Fellowship STREAM.

