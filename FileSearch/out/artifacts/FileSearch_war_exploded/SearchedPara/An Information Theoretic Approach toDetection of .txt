
Detection of rare and exceptional occurrences in large-scale databases have become an important practice in the field of knowledge discovery and information retrieval. Many databases include large amount of noise or irrelevant data, whose distribution often overlaps with the subsets of exceptional data containing useful knowledge. This paper addresses the problem of finding a small subset of  X  X inor-ity X  data whose distribution overlaps with, but are excep-tional to or inconsistent with that of the majority of the database. In such a case, conventional distance-based or density-based approaches in Outlier Detection are ineffec-tive due to their dependence on the structure of the majority or the prerequisite of critical parameters.

We formalize the task as an estimation of a model of the minority subset which provides a simple description of the subset and yet maintains divergence from that of the major-ity. This estimation is formalized as a minimization problem using an information theoretic framework of Rate Distor-tion theory. We further introduce conditions of the majority to derive an objective function which factorizes the prop-erty of the minority and dependence to the structure of the majority. The proposed method shows improvements from conventional approaches in artificial data and a promising result in document retrieval problem.
Recent advances in data processing and the availability of storage devices have increased the number of large scale database in practical domains, along with the need for data mining techniques to discover useful knowledge from such databases. Many of these databases include a large amount of noise or irrelevant data. For example, in applications of genome informatics, gene expression profiles are used in classification of patients or estimation of gene regulatory networks. These profiles consists of thousands of gene ex-pression levels simultaneously observed at various tissues. However, most of the genes are irrelevant to the bio-medical process of interest, and even the relevant genes are often in-trinsically redundant. Thus, the selection of feature genes is critical to obtaining practical, feasible results using such data. In information retrieval, one is often asked to fetch a set of relevant documents based on few keywords, from the database with an enormous number of documents. More-over, since the cost of users to read the documents is very high, in practice, a high precision is expected of the fetched documents. Generally, the distribution of noise or irrelevant instances is uncorrelated to the rare instances which contain useful information. Further, it often has a large variance, overlapping with and submerging the set of rare instances, causing significant difficulty in detection of such instances. In this paper, we address the problem of finding a small sub-set which contains useful knowledge from a database con-sisting largely of high variance-noise or irrelevant samples. In the following, we refer to the subset of rare instances as the minority and the large set of irrelevant data as the ma-jority .

In the field of database engineering and knowledge dis-covery, finding exceptional instances in the database has be-come an important practice in many applicational domains. It is studied in various topics such as outlier detection[1], exception mining[22], rare class mining[13], and chance discovery[17]. However, for the problem addressed in this paper, the presence of overlapping and abundant majority is problematic for conventional approaches, e.g. distance-based and density estimation-based outlier detection and mixture density estimation. In distance-based outlier de-tection methods, outliers are define based on the distance from the majority of the dataset. This definition is depen-dent on the global property of the dataset, i.e., the property of the majority, thus is ineffective for detecting an overlap-ping, uncorrelated subset. The mixture density estimation [18] is a widely used approach to estimation of overlapping distribution. However, its objective function is also largely dependent on the majority of the data, and has difficulty in learning the model of the minority subset at the pres-ence of large noise. The behaviors of these approaches are demonstrated in Section 2. The outlier detection or clus-tering methods based on non-parametric density estimation [10, 20], require much knowledge or analysis of the data to determine parameters such as smoothing parameter, which are critical to their performance.

In this paper, we propose a new approach to detection of the minority subset. First, the minority is defined as a subset whose distribution is exceptional or inconsistent to that of the majority. Using this definition, the detection of minority subset is translated to estimating a distribution which gives a good description of the subset and a large divergence from that of the majority. Here, we assume that both majority and the minority subset can be sufficiently described with prob-abilistic models. If the minority subset has a sufficient num-ber of elements, it can be detected by the local comparison of the statistical properties of the data.

We then formalized the estimation of minority subset distribution as a minimization problem using the frame-work of Rate Distortion theory [4]. Recently, informa-tion theoretic approaches to machine learning tasks [9, 23]. have been employed in practical applications, e.g., voice recognition, image analysis, text classification. [3, 11].The Rate Distortion theory addresses the limits of quantizing a stochastic source signal and provides principles for solving the Rate Distortion problem, a problem of finding the min-imum possible rate, i.e., the average amount of informa-tion, to quantize the source signal subject to the tolerable amount of distortion. Theoretical works [2] have provided relevance between unsupervised learning and rate distortion problems, and principles for formalizing an unsupervised learning task as an optimization of the simplicity and the descriptiveness of the model.

In this paper, we propose a method for detecting and es-timating the distribution of the minority subset based on the minimization problem derived from the above framework. The proposed method achieves the minority subset, which was difficult for conventional methods, by (a) eliminates the global property of the majority from the objective func-tion, (b) evaluates the divergence from the majority, and (c) avoiding false positive detection of the subset of majority. (a) is attained by the approximation on the distribution of the majority and evaluation of the simplicity of the model by Rate Distortion framework. (b) is attained by the formaliza-tion within information theoretic framework and (c) by the introduction of hard clustering technique and the boundary density condition. The proposed method is capable of mod-eling a small-scale subset and achieves high precision and recall in detecting a small subset that cannot be separated by a hard margin in arbitrary subspaces. In the experiments using artificial data, the proposed method performed more effectively than conventional methods, even with 50 times as denser noise. Using test data from standard text docu-ment corpus, the proposed method demonstrated high pre-cision and recall in retrieving a set of documents of with the same category based on one sample document.

The rest of the paper is organized as follows. Section 2 discusses the problem setting and the related works. Sec-tion 3 shows the formalization of the task and derivation of the objective function using an information theoretic frame-work. Section 4 gives the algorithm which minimizes the objective function. Section 5 evaluates the performance of the proposed method using artificial and text classification data and compares to the conventional methods. Finally, Section 6 gives our conclusion.
The conventional objective of outlier detection methods is to remove inconsistent or exceptional instances to im-prove the predictive accuracy and robustness of the machine learning techniques.In the conventional distance-based out-lier detection, the outliers are defined based on its distance from the majority [14]. Support Vector Machine (SVM) [25] is one of the well-studied and powerful machine learn-ing technique which employs a distance-based approach. One-class SVM, an extension of SVM for one-class classi-fication problem, have been suggested for outliers detection [19]. By definition, the distance-based outlier detection is highly sensitive to the structure of the majority. It has been pointed out in [8] that outliers detected by one-class SVM is strongly biased toward the center of the entire dataset. Such approach is ineffective when aiming to detect the minority subset which is intrinsically uncorrelated to the majority.
On the other hand, density-based outlier detection [10, 6] and clustering based on kernel density estimation [12] are the examples of methods using a non-parametric definition of outliers. These methods estimate the local density using nearest-neighbor or kernel density estimation and identify outliers or clusters as the data with exceptional density.
We consider a dataset illustrated in Figure 1 to demon-strate the behavior of the conventional approaches in the minority detection problem. The observed variable x is the vector in a two dimensional Euclidean domain. It consists of 4000 points uniformly distributed in  X   X  = [  X  1 : 1] 2 100 points generated from a normal distribution with mean  X   X  (0 . 5 , 0 . 5) , standard deviations and correlation coefficient  X   X  = 0 . In this setting, the uni-formly distributed samples simulate the majority and the normally distributed samples the minority subset. Each sample is denoted by a dot in the figure, and the arrow indi-cated the position of the minority subset mean.
Figure 1. Detection of minority subset using conventional methods
First, we demonstrate how one-class SVM performs in this setting. The degree 2 polynomial kernel, and the value of 0 . 99 and 0 . 01 were chosen. The results were gen-erally robust to the values of the parameters, which are close to the true parameters of the data. Figure 1 shows the outliers detected at respective  X  with solid and dotted circles. When the ratio of the outlier is large (  X  = 0 . 99 center of majority is detected as outliers. Meanwhile, the marginals of the majority are detected as outliers while the ratio of the outliers is small (  X  = 0 . 01 ).

We also applied the method of mixture density estima-tion based on maximum likelihood estimation (MLE). In the standard procedure of mixture density estimation, we assume a hidden variable Y = { y } , | Y | = k whose value corresponds to k components of the mixture. The mixture P ( x ) is given by the marginal of joint probability distribu-tion P ( x , y ) . The EM algorithm is often used to estimate the component distribution P ( x , y ) , which locally maxi-mizes the mean log likelihood, by alternatively updating the parameters of P ( y | x ) and P ( x , y ) .

Here, we employ EM algorithm with the presumption that the mixture consists of two components P ( x |  X  ) P ( x |  X  ) , where  X  and  X  denote the parameters of the normal and the uniform distribution, respectively. We estimated the maximum likelihood parameter by EM algorithm, initializ-ing the parameters  X ,  X  with  X   X  and  X   X  = Though the true parameters were initially given, the likeli-hood converged at a much larger variance. In Figure 1, the arc shows the two standard deviation of the maximum like-lihood estimate. In the two methods demonstrated above, either the loss function or the objective function is predomi-nantly determined by the majority, which presents difficulty in detecting or modeling the minority subset.

Methods which define outliers or cluster by a non-parametric loss function, e.g., the density-based outlier de-tection or clustering based on kernel density estimation, employ objective/loss functions defined locally or by non-parametric means, thus independent of the global property of the majority. The problem in using locally defined ob-jective function to detect minority subset is that agglom-erations or fluctuations in the majority can be detected as outliers or a cluster. Circumventing this problem may in-voke further difficulties, such as tuning the scale or smooth-ing parameters or performing a statistical tests to decide whether the detected subset is the minority or a subset of the majority.
Unsupervised learning and data compression share a common ground in their process, assuming a probability distribution (explicitly or implicitly) over a set of inputs and generating a simplified representation based on such a distribution. Generally, an arbitrary unsupervised learn-ing can contribute to a form of data compression method. In Information Theory, the branch of Rate Distortion the-ory addresses vector quantization and lossy data compres-sion, providing the fundamental limitation of representing a stochastic information source with a finite alphabet set. Re-cently, several analytical works [2, 23] have provided rele-vance between the unsupervised learning task and the Rate Distortion problem, the central problem in Rate Distortion theory. Rate Distortion problem require encoding a source signal X to a quantization signal Y with minimal average information, subject to the tolerable distortion of the origi-nal signal. This presents a new formalization for unsuper-vised learning problem, which facilitate the simplicity as well as the accuracy of the data description into the objec-tive function. The solution can be obtained using the prin-ciples and the formulation extended from that of the Rate Distortion Theory. This methodology is often referred to as the information theoretic framework and is applied to vari-ous learning tasks [11, 3].

Denoting the source signal by X = { x i } N i =1 , the en-coded signal by Y = { y i } N i =1 , and the distortion function of assigning signal x to y by d ( x , y ) , The encoding from X to Y is probabilistic, and denoted by P ( Y | X ) . The con-strained minimization of the rate and the distortion is for-malized as follows. I ( Y ; X ) , the mutual information between X and Y , is de-fined as I ( Y ; X ) = H ( Y )  X  H ( Y | X ) , using the entropy H ( Y ) and conditional entropy H ( Y | X ) . E X,Y [ d ( x , y )] is the expected distortion defined as (1) includes a positive tradeoff coefficient  X  between the rate and the expected distortion. The Rate Distortion theory provides that for arbitrary positive  X  , there exists an encod-ing P ( Y | X ) , which minimize equation (1) [7].
One of the standard procedure in unsupervised learn-ing problems is to assume a hidden variable Y for the ob-served variable X and estimate the conditional probability P ( Y | X ) . Transferring these notations to (1), the first term represent the simplicity of the learned result and the second term represent the deviation between the data and the de-scription of the model. Then, minimization of (1) can be translated to estimating a simple, descriptive model of the data. [2] has shown that substituting the empirical distribu-tion of the observed variable for the source signal distribu-tion, the solution of rate distortion problem is equivalent to the maximum likelihood estimation of the family of expo-nential distribution.
In this section, we introduce the settings of the minority detection problem into (1). First, we denote the empirical distribution of the observed variable X as P ( X ) . Assum-ing X consists of the majority and the minority subset, we denote labels Y as a hidden variable over y i  X  X  l n , l s further use P ( y i | x i ) to denote the conditional probability of x i being assigned to the majority ( y i = l n ), or the minor-ity ( y i = l s ).

Next, we introduce several approximations regarding the property of the majority. The first term in (1) can be ex-panded as Since X is mostly composed of the majority, we approxi-mate distributions P ( X ) and P ( X | l n ) are equivalent, i.e., Hence, we can rewrite (2) ignoring the first term, Meanwhile, the second term in (1) is decomposed as
E X,Y [ d ( x , y )] = E X,l In (5), the first term, which represents the expected distor-tion for the majority, can be considered a constant and negli-gible in the minimization. This is due to the approximation that the distribution of the majority is constant in (3), and also that the number of data labeled l s is negligible to the entire dataset. Then, approximation of (5) is From (4) and (6), we derive the objective function F as Next, we introduce the hard assignment approximation restricting the conditional probability P ( y | x ) to a dis-crete value { 0 , 1 } , thus making the assignment determin-istic rather than probabilistic. This hard assignment is used generally in hard clustering methods such as k-means, and represents a special case of the soft or probabilistic assign-ment, in which the conditional probabilities of hidden vari-able are continuous.

By introducing (8), the expectation over X, l s in (7) be-comes a mean over the subset with label l s . Denoting the minority subset by Z = { z i } M i =1 , such that  X  x i  X  Z, y l , the second term in (7) is rewritten as
Furthermore, the hard assignment approximation simpli-fies the conditional entropy to H ( Y | X ) = 0 , thus the mu-tual information to I ( Y ; X ) = H ( Y ) . As a result, the first term in (7) becomes Substituting (9) and (10) to (7), we obtain
Intuitively, the first term of F 0 represents the simplicity of the model, while the second term represents the deviation of the data from the model. As a result of simplifications (3), (6), and (8), the objective function has been localized to the subset with label l s , such that the global structure of the majority has been eliminated in (11). In mixture esti-mation, [15, 26]have shown that hard assignment approxi-mation factorizes the likelihood of the component distribu-tions over the respective components of the mixture, which can further be independently maximized. In our work, the factorization is achieved by the combination of the majority approximation (3) and hard assignment.
In this section, we parameterize the distributions of the minority subset and the majority and also the distortion function. First, we introduce parameters  X  and  X   X  respec-tively parameterizing the distribution of minority subset and the majority as We define the distortion function d (  X  ) using the logarithm of probability density, Subject to  X  and  X   X  , F 0 is rewritten as follows. F 0 (  X ,  X   X  ) =
Intuitive interpretation of the second term in (11) is the divergence between the distribution of the minority subset and that of the majority. Hence, minimizing F 0 will simul-taneously increases the simplicity of the model of minority and the divergence from the model of the majority.
The objective function (1) includes a tradeoff coefficient  X  for the rate and the expected distortion. The value of  X  is determined based on the specifications of the problems and cannot be determined within the information theoretic framework. In this section, we introduce a boundary condi-tion to determine the hard assignment of the labels. Then, we use the condition along with the variational principle of the rate distortion theory to derive a feasible value of  X 
First, we use F to denote the functional in (1), as a func-ory, p ( y | x ) which minimize F is obtained as the solution of a variational problem: In (3) and (6), we assumed that I ( l n | X ) and E [ d ( l are independent of the variation of P ( y | x ) . Under this ap-proximation, is derived from (12) with regards to the variation of F 0 ther, in the solution of the problem (13), the variational of F 0 with regards to P ( Y | X ) is 0, i.e.,  X F 0 = 0 .
Using  X  x to denote the instance on the boundary of the minority subset and the majority,  X F 0 is written as follows.
Next, we give a boundary condition on the densities of the minority subset and the majority. The densities of the minority subset  X  s ( x ) and the majority  X  n ( x ) at defined as
We mandate the following condition that the respective densities are equivalent i.e., at an arbitrary boundary  X  x . Substituting (15), (16), and to (14) and setting the variation to zero, from which we obtain  X  = 1 . Using this value, (11) is rewritten as follows.

Additionally, boundary condition (16) enables this method to avoid detecting a part of the majority as a mi-nority subset. This is intuitively described as follows: since the majority has a significantly larger variance than the mi-nority, we can assume that the density of the majority is approximately uniform around the margin of the subset Z . When a center-heavy distribution such as exponential dis-tribution families is employed as the model of the minority subset, the estimated density  X  s of subset Z which consists of the majority will be larger than the actual density of the majority toward the center of the distribution. In return, the estimated density is smaller at the margin compared to the actual density.

In short, if the subset Z consists entirely of the major-ity instances, the estimated density is always smaller than that of the rest at the boundary. By iteratively updating the parameters to satisfy the boundary condition (17), Z will degenerate to an empty set in such case. By rejecting which degenerated to below cardinality threshold, we can avoid detecting a part of the majority as a minority subset. The conditional probability P ( y | x ) is constrained by In rate distortion theory, the constraints (12) and (17) lead to the Blahut-Arimoto (BA) algorithm [5], which alternatively updates p ( y ) and p ( y | x ) to satisfy the constraints to find the local minimum of F .

However, the analytical solution of (13) and (16) does not give us a hard assignment solution. Thus, we designed an algorithm for incremental detection of minority ( in-D-Minor ) which alternatively satisfies (17) and decrease F We show the pseudo code of in-D-Minor in Figure 2.
We provide the following proposition which proves the convergence of F s .
 Proposition 1. Algorithm in-D-Minor converges at the P ( Y | X ) which gives a local minimum of F s within a finite number of iterations.
 Proof. We denote the parameter of the minority distribution after t th iteration of the E Step and the M Step using  X  t labels by Y t , and the minority subset by Z ( Y t ) . First, since is fixed in the E Step, follows trivially from the criteria used to decide y 0 j . In the M Step, F s is dependent only on since P ( Y | X ) is fixed. Then, stands for  X  t +1 which maximizes the likelihood.

The monotonic decrease of F s by in-D-Minor follows from (18) and (19). Further, since the number of possi-ble combinations of Y is finite, in-D-Minor converges to a solution P ( Y | X ) which locally minimizes F s in a finite number of iterations.
 INPUT: observed variable X = { x i } N i =1 OUTPUT: subset of outliers Z  X  X METHOD: Y = { y i } N i =1 , Z = { x i | y i = l s } X  X
M = # ( Z ) ,  X   X  = arg max
Initialize labels Y using some initial parameter  X  0 , s.t.  X  i, j | y i = l s , y j = l n , P ( x i |  X  0 ) &gt; P ( x repeat until F s ( Z,  X ,  X  ) converge return Z When  X  is initialized near the true parameter, in-D-Minor minimizes F s and converges at  X   X  which satisfies the boundary condition (16). Meanwhile, when  X  is far from  X   X  and the subset Z consists only of the majority instances, converges at the initial subset.
In this section, we evaluate the convergence property of the objective function and the classification performance of the proposed method using artificially generated test data. 5.1.1 Convergence Property We prepare test datasets A 0 , A 1 , A 2 in a two dimensional Euclidean domain as follows. A 0 consists of 4000 points uniformly distributed over U ([  X  1 : 1] 2 ) . A 1 , A 2 , and are mergers of 4000 uniformly distributed samples and 100, 200, and 300 samples generated by a bivariate normal distri-bution respectively. The mean, standard deviation, and the correlation of the normal distribution are (0 . 3 , 0 . 3) 0, respectively. In these datasets, the uniformly distributed samples represent the majority, and the normally distributed samples the minority subset.
 For each sample, we compute the relative mean density R  X  . Using the density of the uniformly distributed samples  X 
U and the mean density of normally distributed samples  X  G , the relative mean density is defined as R  X  =  X  G /  X 
Figure 3. Convergence curve: objective func-tion by cardinality of subset  X 
G is the mean density of the normally distributed sam-ples within two standard deviation of the mean. R  X  repre-sents the relative density of the minority subset, therefore the level of difficulty in their detection. The values of for A 0  X  A 3 are 0 , 0 . 076 , 0 . 15 , 0 . 23 .

We evaluate the convergence property of the function F s by the following procedure. Z 0 is initialized with a subset of 50 samples near the true mean of the normally distributed samples. Following the Algorithm 2, we compute F s . while incrementing the subset Z . Figure 3 shows the convergence curve of F s against the cardinality of Z . The x-axis shows the cardinality of Z , while y-axis shows the inverse of F
Figure 3 shows that the cardinality of subset Z , which minimizes F s , correlates to the number of normally dis-tributed samples despite the abundant presence of irrelevant majority. Further, when the entire samples are uniformly distributed, F s does not have a local minimum and mono-tonically decreases, against the cardinality of Z .
In addition, we compute the log likelihood of the entire dataset X by P ( x ,  X  ) and P ( x ,  X   X  ) . Figure 4 shows the convergence curve of log likelihood against the cardinality of
Z . As shown in Figure 4, the log likelihood maximizes at largest Z independent of the number of normally distributed samples. This result comes from the fact that likelihood is determined mostly by the majority, suggesting that the dif-ficulty of modeling the minority subset by maximum likeli-hood estimation.

Figure 5 and 6 illustrate the dataset A 1 and the subset which maximizes F s in the above procedure. In Figure 5, the normally distributed samples are represented by and the uniformly distributed samples are represented by
Figure 4. Convergence curve: log likelihood by cardinality of subset  X  . In Figure 6, the minority subset is represented by + and the rest of the data by  X  s. Figure 6 shows significant improvement in modeling minority subset compared to that of the conventional methods in Figure 1. 5.1.2 Quantitative comparison of classification result In this section, we compare the proposed method to two conventional methods: one-class SVM and maximum like-lihood estimation (MLE) by EM algorithm. We compose the test datasets from 4000 samples uniformly distributed within [  X  1 : 1] 2 and 100 normally distributed samples. The mean  X  , standard deviation  X  , and correlation  X  of the nor-mal distribution are randomly chosen from [  X  0 . 5 : 0 . 5] [0 . 1 : 0 . 2] 2 , and [  X  0 . 5 : 0 . 5] , respectively.
LIBSVM package 1 is used to evaluate one-class SVM in this experiment. We chose the polynomial kernel over linear and RBF kernels based on the comparison of mean precisions. For  X  parameter, which defines the maximum ratio of outliers, we enumerated over range [0 . 01 : 0 . 1] [0 . 9 : 0 . 99] with a step size of 0.01, evaluated the result with maximum F-measure [24]. F-measure is a harmonic mean of precision and sensitivity defined as
For MLE, we used a mixture of a uniform and a Gaussian distributions and used the EM algorithm to estimate parameters  X   X   X  . We detect samples whose probability density is higher for normal distribution than the uniform distribution, i.e., P In the proposed method, we set the initial cardinality of Z to M 0 = 50 . For each dataset, MLE and the proposed method are applied 20 times with different initializations, and the result with smallest objective function is chosen. We compare the precision p , sensitivity Sn , specificity Sp of each method. These measures are defined using true positive TP , false positive FP , true negative TN , and false negative FN as follows. p =
Since the two distributions overlap in the test datasets, the maximum possible precision is not one and differs for respective datasets. We define the standard precision p z the relative precision p r as follows. Assume an ellipsoid which encloses the 2-standard-deviation range of the nor-mal distribution. The standard precision p z is defined as the precision of the hard margin classification by the ellipsoid. Then, the relative precision p r is defined as p r = p/p z Table 1 shows the average result with 20 test datasets. In Table 1, the proposed method shows the best relative preci-sion and sensitivity by far, and also a very high specificity. In comparison, one-class SVM on the second row shows a very low precision and sensitivity. This indicates that there are very few true positive samples, thus the failure of de-tection. The high sensitivity is the probable result of large number of majority samples. MLE on the third row shows better precision and sensitivity than one-class SVM, but the
Figure 6. Example of detection result by pro-posed method( r = 0 . 076 ) specificity is significantly low considering the large ratio of the majority instances. These results imply that the detec-tion by MLE induces large false positive samples and trades off specificity for sensitivity.

The bottom two rows of Table 1 shows the results of comparative performance by conventional methods. For these experiments, the difficulty of the problem is reduced by decreasing the cardinality of majority to the same num-ber as that of the minority subset. Comparing the rela-tive density, these results show that the proposed method is equally or more effective than conventional methods with 50 times as dense irrelevant data.
In this section, we apply the proposed method to a standard text classification dataset, Reuters 21578 corpus which has been used in numerous preceding works. This corpus consists of 10,789 documents, each of which has one or more categories.

We employed the multinomial distribution as the proba-bility model, which is one of the standard event models for text documents [16]. Given a vocabulary W = { w i } n i =1 document x is represented by the vector of frequencies v i for word w i ,
The words in vocabulary W are selected based on their 0.32 (0.16) 0.66  X  0.069 0.97  X  0.012  X  0.12 (0.16) 0.025  X  0.057 0.98  X  0.0015 0.067 (0.16) 0.50  X  0.51 0.41  X  0.42 0.044 (0.89) 0.69  X  0.029 0.69  X  0.028  X  0.24 (0.84) 0.28  X  0.23 0.57  X  0.099 contribution to I ( W ; X ) , i.e., This selection scheme was proposed in [21] and does not require the category of the documents. For the following experiment, we used the vocabulary size of k = 200 .
The conditional probability of x is given by a multino-mial distribution where, V = the vocabulary. The conditional probabilities P ( w j | l P ( w j | l n ) are given by
P ( w j | l s ) = where Z is the subset of X such that y = l s .

The test data was prepared as follows. First, we selected the topic acq , which has a large number of documents, and topics oilseed , money-supply , sugar , and gnp all with less than one-tenth the number of elements as acq . The number of documents having respective categories as its topic are 2448, 192, 190, 184, and 163. We merge the documents with these categories to create the dataset C . The goal of the experiment is to detect a document subset with the same category as a sample. Among the detected documents, the documents with same category are the true positive samples and the rest are false positive samples.

For each run, we randomly choose a sample document x 0 from one category, and run the Algorithm 2 with ini-tial subset Z 0 = x 0 . For each category, we repeat the run 20 times with different sample documents. The results are compared in terms of specificity and F-measure.

Not many methods are implemented to retrieve docu-ment from a one positive sample. For reference, we demon-strated how the EM algorithm performs with the same procedure, i.e., setting the initial conditional probabilities P ( w j | l s ) by the sample document and P ( w j | l rest, respectively.

Table 2 shows the average specificity and F-measure of retrieved documents using the samples of respective cate-gories. The proposed method shows very high specificity and F-measure, consistently for all categories. Meanwhile, the retrieval by the maximum likelihood EM was in favor of high sensitivity at the expense of low precision and speci-ficity. Its results were also somewhat inconsistent, showing good specificity and relatively high F-measure for gnp , but very low for others.
This paper addressed the problem of detecting minority subset from a database consisting largely of noise or irrele-vant samples. The proposed method can model a subset of very small scale, which was problematic for conventional approaches, and present a new method for detecting rare instances in the databases that are submerged by the over-whelming irrelevant instances.

Our contribution is formulating the task as an estima-tion of the distribution which provides a good description of the minority subset and has significant divergence from the distribution of the majority using the information the-oretic framework of Rate Distortion. Furthermore, we de-rived an objective function which focuses on the local prop-erty of the data using hard assignment and majority approxi-mations, and also induced a desirable convergence property using the variational principle and a boundary condition of the data densities.

In the experiments using artificially generated data, the proposed method was more effective than the conventional methods even with 50 as dense noise. In the experiment us-ing reuters21578 corpus, the proposed method showed very promising result in retrieving document subsets from a sin-gle sample document.

