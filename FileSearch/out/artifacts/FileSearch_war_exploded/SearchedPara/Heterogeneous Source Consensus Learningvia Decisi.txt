 Nowadays, enormous amounts of data are continuously gen-erated not only in massive scale, but also from different, sometimes conflicting, views. Therefore, it is important to consolidate different concepts for intelligent decision mak-ing. For example, to predict the research areas of some people, the best results are usually achieved by combining and consolidating predictions obtained from the publication network, co-authorship network and the textual content of their publications. Multiple supervised and unsupervised hypotheses can be drawn from these information sources, and negotiating their differences and consolidating decisions usually yields a much more accurate model due to the di-versity and heterogeneity of these models. In this paper, we address the problem of  X  X onsensus learning X  among compet-ing hypotheses, which either rely on outside knowledge (su-pervised learning) or internal structure (unsupervised clus-tering). We argue that consensus learning is an NP-hard problem and thus propose to solve it by an efficient heuris-tic method. We construct a belief graph to first propagate predictions from supervised models to the unsupervised, and then negotiate and reach consensus among them. Their final decision is further consolidated by calculating each model X  X  weight based on its degree of consistency with other models. Experiments are conducted on 20 Newsgroups data, Cora research papers, DBLP author-conference network, and Ya-hoo! Movies datasets, and the results show that the proposed method improves the classification accuracy and the cluster-ing quality measure (NMI) over the best base model by up to 10%. Furthermore, it runs in time proportional to the number of instances, which is very efficient for large-scale data sets.
The work was supported in part by the U.S. National Sci-ence Foundation grants IIS-08-42769 and BDI-05-15813, Of-fice of Naval Research (ONR) grant N00014-08-1-0565, and the Air Force Office of Scientific Research MURI award FA9550-08-1-0265. Any opinions, findings, and conclusions expressed here are those of the authors and do not necessar-ily reflect the views of the funding agencies.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms
As information networks become ubiquitous, the same set of objects can be represented and categorized by different in-formation sources, which could be either the outside knowl-edge or the internal information. Example scenarios include (a) user viewing interest analysis based on their ratings of movies, movie synopses and movie genres; (b) buyers pro-filing based on their purchase history and personal informa-tion in e-commerce; (c) advertisement campaign based on the click rates, webpage content, advertisement content and location in the web page; and (d) research paper catego-rization based on paper content, citations and citation con-texts. In these applications, we are interested in classifying a set of objects, and the predictive information comes from multiple information sources, each of which either transfers labeled information from relevant domains (supervised clas-sification), or derives grouping constraints from the unla-beled target objects (unsupervised clustering). Multiple hy-potheses can be drawn from these information sources, and each of them can help derive the target concept. However, since the individual hypotheses are diversified and heteroge-neous, their predictions could be at odds. Meanwhile, the strength of one usually complements the weakness of the other, and thus maximizing the agreement among them can significantly boost the performance. Therefore, in this pa-per, we study the problem of consolidating multiple super-vised and unsupervised information sources by negotiating their predictions to form a final superior classification solu-tion. We first illustrate how multiple information sources provide  X  X omplementary X  expertise and why their consensus produces more accurate results through a real example.
DBLP 1 provides bibliographic information on major com-puter science journals and proceedings, containing 654,628 authors and 4,940 conferences/journals. It is relatively easy to identify the fields of conferences/journals by their names, but much harder to label all the authors with their research interests. Therefore, we can use the conference labels, as well http://www.informatik.uni-trier.de/  X  ley/db/ as, the connections among authors in the DBLP network to classify authors into different research fields. Specifically, the following information sources are available: 1) We can infer the area of a researcher from the con-ferences and journals s/he publishes in. For example, the researchers in the database community would mostly pub-lish in the database conferences. However, this information source is not 100% reliable because it is not rare that people in the data mining field publish in database, machine learn-ing, information retrieval or even networking, bioinformatics and software engineering conferences. 2) The co-authorship network provides useful information about the research fields of the researchers as well. If two authors often write papers together, they are more likely to share the same research field. But for researchers who have many cross-discipline cooperations, we may not be able to easily predict their research areas from this network. 3) When the information network fails to predict correctly, the textual content of the researchers X  publications can play a role in the classification task. A researcher who concen-trates on frequent pattern mining, graph mining and feature selection can be easily categorized into data mining commu-nity. However, some research fields may share the same keywords. For example, in papers of both database and information retrieval areas, we can find the words  X  X uery optimization X ,  X  X ndexing X  and  X  X etrieve X , and thus a simple text classifier cannot distinguish among such areas.
For such problems, a simple concatenation of all the in-formation sources may not be possible due to incompati-ble schemes, or fail because the structural information of some data sources can be lost. Furthermore, for applications in distributed computing, privacy preserving or knowledge reuse, we may only have access to the labeling or grouping results but not the raw data. Formally, for a classification task on a set of instances T , we may have r v models that rely on outside knowledge (e.g., learn the areas of researchers from the areas of conferences), and r u unsupervised mod-els that group instances according to their similarity (e.g, the co-authorship network). We are aiming at consolidating predictions from all the models to boost the classification accuracy as well as improve the robustness and stability.
An example drawn from DBLP data is shown in Figure 1 where r v supervised models predict the class labels of seven authors from information management area to be one of { Databases, Data Mining, Information Retrieval } , whereas the other r u unsupervised models cluster the authors into cohesive groups. The objective is to find the global optimal labeling for the seven authors so that it agrees with the base models X  outputs as much as possible. Existing approaches only pick the most likely label for each instance among su-pervised models without negotiation with the unsupervised sources, or combine the unsupervised grouping results ignor-ing the useful outside knowledge. As discussed above, all the information sources, no matter supervised or unsupervised, are important and only a global consolidation provides the optimal label assignments. To the best of our knowledge, this problem has not been studied before.

In this paper, we first formulate consensus learning into an optimization problem and argue that it is NP-hard. So we propose to solve the problem using an effective two-step heuristic method involving global decision propagation and local negotiation. In global propagation (Section 3.1), we first collapse test instances into groups based on the pre-dictions of each model. We construct a belief graph where nodes represent the groups, and edge weights denote the percentage of common members they share, and start with the initial label assignments obtained from the supervised models. Each node iteratively propagates its prediction to its neighbors, and when it stabilizes, the groups that con-tain approximately the same set of instances would share the same predictions. In the second step (Section 3.2), to predict the class label of an instance, we make adjustments by negotiating among models locally with model weights re-flecting the degree of consistency with others. We evaluated the proposed framework on four real learning tasks includ-ing 20 newsgroups categorization, Cora research paper clas-sification, DBLP authors research areas categorization and Yahoo! movie-rating user grouping, where various learning models or information sources are available 2 . Experimental results show that the proposed method improves the classifi-cation accuracy as well as the clustering quality measure by up to 10% compared with the best base models. Moreover, both analysis and experimental results demonstrate that the running time of the solution is linear in the number of test instances, and thus it can be easily scaled to very large data sets without running into combinatorial explosions. We have a set T = { x 1 , . . . , x n } where x is the object ID and each object is represented in different information sources. We wish to predict the label of each example x in T : y  X  Y = { 1 , . . . , c } , where c is the number of classes. Sup-pose we have r v classification models trained on the labeled sources, and r u clustering methods relying on the internal structure of the test set, which can be obtained from differ-ent sources or using different algorithms. Let r = r v + r then we have r models:  X  = {  X  1 , . . . ,  X  r v ,  X  r v +1 , . . . ,  X  where the first r v of them are supervised and the remaining ones are unsupervised. A supervised model  X  a (1  X  a  X  r v maps an instance x to a specific category  X  a ( x )  X  Y , whereas an unsupervised model maps it to a cluster and cluster ID does not directly carry any category information. In this paper, we focus on  X  X ard X  classification and clustering, i.e., x is predicted to be in exactly one class or cluster. Our aim is to use each model in  X  to find a  X  X onsolidated X  solution  X   X  on T , which maps x  X  T to one of the classes. It should agree with both the supervised and the unsupervised models as much as possible. Note that the true labels of examples
At http://ews.uiuc.edu/  X  jinggao3/kdd09clsu.htm, there are experimental details, codes, data sets and additional ex-periment results. in T are unknown, and thus the defined consensus learning problem is  X  X nsupervised X . The final predictions are derived based on the assumption that  X  X onsensus is the best X , which proves to be valid in the experimental study in Section 4.
First, we favor the solution which maximizes the consen-sus. To define consensus, we need to first define the similar-ity or distance between two models X  predictions on T . For the sake of simplicity, we use the following simple distance function. Consider two points x i and x t in T , and we define the disagreement between models  X  a and  X  b regarding their predictions on the two points as: d If  X  a and  X  b agree on x i and x t  X  X  cluster or class assignment, the distance is set to 0, otherwise to 1. Then we define the distance between  X  a and  X  b on T as the number of object pairs on which the two models disagree: Therefore, one of our objectives is to minimize the disagree-ment with all the models: min  X 
Secondly, the consolidated solution should be consistent with the predictions made by the supervised models. In other words, we need to minimize the difference between the consolidated solution and {  X  1 , . . . ,  X  r v } on each x  X  X  la-bel. Therefore, we add a penalizing term to the objective function and the consolidated solution  X   X  satisfies:  X   X  = arg min where 0  X   X  &lt;  X  is the parameter to tune the contribu-between the predictions made by  X  and  X  a on x i .
It can be seen that clustering consensus is a special case of the proposed framework with  X  = 0 . Clustering consensus is shown to be NP-complete [8] based on the results of median partition problem [1]. We assume that there is at least one classifier and one clustering algorithm, and  X  is a finite num-ber. So if the problem proposed in Eq. (1) can be solved in polynomial time, the clustering consensus problem will also be solved in polynomial time, which leads to contradiction. Hence the proposed optimization in this paper is NP-hard. Because we are tackling classification problems, the search space would be c n , so an exhaustive search is formidable, and a greedy search would still have exponential time com-plexity and result in poor local maximum. For example, we would have to search 3 7 possibilities for the simple example shown in Figure 1 with 7 objects and 3 classes. Due to NP-completeness, we propose an effective heuristic in Section 3 to predict the class labels of examples in T with a linear scan of T . The solution represents the negotiation results among all the supervised and unsupervised models.
We solve the problem through two steps: Based on results of the two steps, we compute P ( y | x, E ) representing the consensus as: The predicted label for x goes to  X  y which minimizes the risk:  X  y = arg min y the cost incurred when the true class label is y 0 but the prediction goes to y . With the most commonly used zero-one loss function,  X  y = arg max y P ( y | x, E ).
We hope that the final prediction P ( y | x, E ) is close enough to the true but unknown P ( y | x ), which we assume can be reached by consolidating the base model predictions. The challenges include: 1) When  X  a is an unsupervised model, it simply assigns x to one of the clusters but does not pre-dict the category of x , so P ( y | x,  X  a ) cannot be directly ob-tained. On the other hand, when  X  a is a classifier, we can set P ( y | x,  X  a ) = 1 when  X  a ( x ) = y and 0 for all other y . However, this estimation is quite biased and we may want to modify it based on the negotiation with other models. 2) We expect that the weighting scheme can help reach the best consensus among all models. So ideally, P (  X  a | x ) should reflect the consistency of  X  a with other models on predicting x  X  X  label. We develop the following two heuristics that can solve the above problems effectively.
Each model  X  a partitions T into c a groups, and in the  X  X ard X  scenarios, an example x is a member of exactly one group if one of the supervised or unsupervised models  X  a applied on T . Then P ( g a h | x,  X  a ) = 1 if x belongs to group g h and 0 for all the other groups in  X  a . Therefore,
P ( y | x,  X  a ) = Initially, there is a one-to-one mapping between each group g h from a classifier  X  a (1  X  a  X  r v , 1  X  h  X  c a ) and each class label y  X  Y , i.e., if  X  a predicts the label of the examples in group g a h to be y (  X  a ( x  X  g a h ) = y ), then  X  We treat the label information of groups from supervised models as initial labeling and estimate P ( y | x, g a h ) for all the groups from all the models ( 1  X  a  X  r , 1  X  h  X  c a ). Altogether we have s = Each group g can be represented by a length-n binary vector: { v i } n i =1 where v i is x i  X  X  membership indicator [26]. For x T , its membership with respect to group g is 1 if x  X  g and 0 otherwise. The problem shown in Figure 1 is illustrated in the first four columns in Table 1 with two classifiers and
Figure 2: Illustration of the Label Propagation two clustering methods, and { DB, DM, IR } is mapped to { 1, 2, 3 } . The binary membership vector of each group is shown in one of the 12 columns at the right side of Table 1. For example, the second group in  X  1 contains two examples x 4 and x 7 , so the corresponding entries in g 2 are 1. Now we can measure the similarity between any two groups g k and g . One commonly used measure is Jaccard coefficient: where n k and n j are the number of examples in g k and g respectively, and n k,j is the number of examples in both g k and g j . For example, in Table 1, J ( g 4 , g 8 ) = 2 / 3 and J ( g 4 , g 9 ) = 1 / 5. Clearly, g 4 and g 8 are more similar with each other since they share more of the common members.
Now we can show the groups and their similarities using a belief graph G = ( V, E ). Each node in V is a group g k from a model  X  a . Each edge in E connecting two nodes g k and g is weighted by the similarity between these two groups. If a group g is from a classifier  X  a (1  X  a  X  r v ), it has its initial class label predicted by  X  a . The labels of the groups from clustering models  X  a ( r v +1  X  a  X  r ) are not assigned. The graph constructed for the example in Table 1 is shown in Figure 2. For the sake of simplicity, we did not connect all the edges. Each of the six groups from  X  1 and  X  2 has its initial label (the black nodes). For example, g 1 is mapped to class 1 since the examples in g 1 are all predicted to be in class 1. So  X  P ( y = 1 | x, g 1 ) = 1 , whereas  X  P ( y = 2 | x, g  X  P ( y = 3 | x, g 1 ) are both 0. On the other hand, the groups from  X  3 and  X  4 are unlabeled (the white nodes), and we set their conditional distribution to [0 0 0] at first. Through this graph, we can propagate the conditional probability in-formation from the groups in  X  1 and  X  2 (labeled nodes) to the groups in  X  3 and  X  4 (unlabeled nodes). The unlabeled nodes in turn change the predictions of their neighbors, la-beled or unlabeled. When the propagation becomes stable, the groups with similar members would share similar  X  P ( y | x ).
We now introduce the propagation method and analyze its optimality. Let Q s  X  c be the matrix of conditional prob-ability estimates we are aiming for, with each entry Q kz  X  P ( y = z | x, g k ). We set all the entries in Q to be zero ini-tially. We define another s  X  c matrix F corresponding to the initial labeling from supervised models, where F kz = 1 if  X 
P ( y = z | x, g k ) = 1 and 0 otherwise. In other words, if g is from a supervised model, it will have 1 at the entry cor-responding to its class label z . For unsupervised models, all the corresponding entries in F are 0. We construct the sim-ilarity matrix W s  X  s with each entry W kj equal to J ( g as defined in Eq. (3), and compute a diagonal matrix D with its ( k, k )-element equal to the sum of the k -th row of W . Let H = D  X  1 / 2 WD  X  1 / 2 which normalizes W . Then we iterate Q =  X HQ +(1  X   X  ) F until convergence, where  X  is a param-eter controlling the importance of the initial labeling. After the propagation stabilizes, we normalize Q so that each row of Q sums up to 1.

In fact, Q obtained from the propagation represents the minimum of the following objective function: 1 2 In this objective function, we hope that the difference be-tween the labels of two groups, g k and g j , would be as close as possible if their similarity W kj is high. The second term penalizes the deviation from the initial label assignments for the groups from supervised models. We define the normal-ized graph laplacian as L = D  X  1 / 2 ( D  X  W ) D  X  1 / 2 Due to the properties of graph laplacians, the above objec-tive function equals to: Differentiating the objective function in Eq. (4) with respect to Q to derive the optimal solution, we can get: Q  X   X  HQ  X  ( Q  X   X  F ) = 0 . By defining  X  = 1 1+  X  , we have Q  X  = (1  X   X  )( I  X   X H )  X  1 F . To avoid computing a matrix inverse, we compute Q in an iterative way where Q = 1 1+  X  ( HQ +  X F ) =  X HQ + (1  X   X  ) F . It converges to Q  X  , which is consistent with the initial labeling, and smoothes over the belief graph with nearby nodes sharing similar predictions.

The essence of this propagation method is that at each it-eration, the conditional probability of each group g from the supervised model is the average of its close neighbors X  prob-ability estimates and the initial labeling. For example, the third group in  X  1  X  X  initial conditional probability [0 0 1] is smoothed to [0 0.06 0.94] because it is affected by the neigh-boring node with conditional probability [0 1 0]. On the other hand, the conditional probability of a group from the unsupervised models is only the weighted average of those from its neighbors since its initial label assignment is [0 0 0]. The propagation continues until all the nodes X  predictions are stable, and then  X  P ( y | x, g k ) represents the results of ne-gotiation among all the models with prior knowledge from the supervised models.
As introduced in the above section, we compute the con-ditional probability estimate at the group level, and we wish to adjust it using a local weight P (  X  a | x ) in the final solution. The optimal P (  X  a | x ) should reflect the prediction ability of  X  a on x where  X  a gets a higher weight if its prediction on x is closer to the true P ( y | x ). The challenge is that the true P ( y | x ) is not known a priori, so the optimal weights cannot be obtained. In this work, we only have access to predictions made by multiple models, but no groundtruth labels for the examples. So traditional cross-validation approaches cannot be used to compute the weights. From the  X  X onsensus X  as-sumption, we know that the model that is more consistent with others on x  X  X  label tends to generate a more accurate prediction for x .

Therefore, we characterize the consistencies among models to approximate the model weight: S (  X  a ,  X  b | x ) denotes the pair-wise similarity between  X   X  b regarding x  X  X  label prediction, which represents the de-gree of consistencies between the two models. In other words, on x when we assume that  X  1 , . . . ,  X  a  X  1 ,  X  a +1 , . . . ,  X  correct model respectively. For models  X  a and  X  b , we rely on the local neighborhood structures around x with respect to the two models to compute S (  X  a ,  X  b | x ). Suppose the sets of the examples that are in the same group with x in  X  a and  X  b are X a and X b respectively. If many examples are common among X a and X b , it is quite probable that  X  a and  X  b agree with each other on x  X  X  label. Hence the measure for the pair-wise local consistency can be defined as: In this computation, it doesn X  X  matter whether  X  a and  X  b are supervised or unsupervised. We infer the label consis-tencies from x  X  X  neighbors according to the grouping results of  X  a and  X  b . As an example, let X  X  compute S (  X  1 ,  X  S (  X  1 ,  X  4 | x 1 ) for the problem in Table 1. The neighbors of x 1 in  X  1 ,  X  2 and  X  4 are { x 2 , x 3 } , { x 2 , x 7 } and { x respectively. So S (  X  1 ,  X  2 | x 1 )  X  1 / 3 and S (  X  1 which indicates that  X  1 agrees with  X  2 better on x . P (  X  is then calculated as the average pairwise similarity between  X  1 and the other three models on x . According to the defi-nition of P (  X  a | x ), it is obvious that when  X  a is more consis-tent with most of the other models on classifying x , its local weight is higher.

However, making the best local selection not always leads to global consensus. For certain examples, the most accurate label prediction may be attributed to the minority predic-tions. Therefore, we add a smoothing term to the model weight definition: Here, we assume that the model weight P (  X  a | x ) follows a mixture model of two components, where the  X  X onsensus model selector X  values the local consensus among models, but the  X  X andom model selector X  shows no preference to any model so that the majority and minority predictions have equal chances.  X  reflects our belief in this random selec-tor compared with the consensus model selector. Finally, from Eq. (6) and the constraints that we can calculate the local weight of each model indicating its prediction power on x . In this part, we examine the method X  X  time complexity. In the first step, the conditional probability of each group is learnt through propagation over the belief graph. The total number of groups is s , and each group can be repre-sented using a binary vector, so the time to construct and normalize the similarity matrix W is simply O ( s 2 ). Sup-pose we have f iterations, then the propagation time is O ( fcs 2 ) where c is the number of classes. The normaliza-tion on the prediction results takes O ( s ). The time of the second step is mainly attributed to the computation of pair-wise local consistency S (  X  a ,  X  b | x ), which can be computed in an efficient way. From Eq. (5), it can be derived that in the same groups according to  X  a , and according to  X  b as well. We have s ( s  X  1) 2 pairs of groups and we only need to calculate S (  X  a ,  X  b | x  X  g k , g j ) for these pairs of groups, so the time is O ( s 2 ). Note that till now, we work at the level of  X  X roups X  instead of  X  X xamples X , and usually both the number of models and the number of classes or clusters are quite small (e.g., less than 10), so the computation can be very fast and the running time is independent of the num-ber of examples. After that, we only need to check r models for each example to calculate the model weights and the weighted label prediction (combining results of step 1 and step 2). On the test set T with n examples, the complexity of this procedure is O ( rn ), usually n  X  r . So the method runs in linear time with respect to the number of examples, and can scale well to large data sets.
The proposed algorithm is summarized in Figure 3. As discussed, the two steps together help reach a consensus among r models. P ( y | x,  X  a ) is calculated at a coarser level, whereas we further negotiate among models using P (  X  a | x ) targeted to each example. On the other hand, P ( y | x,  X  computed globally by propagating the labeled information among all the groups. In the computation of P (  X  a | x ), only the local structure around x plays a role. So by combin-ing different granularity of information and conducting the consolidation both globally and locally, we effectively solve the consensus learning problem. We conducted an exhaus-tive search among 3 7 possibilities for the problem in Table 1 and found the optimal solution is { DB, DB, DB, DM, DM, IR, DM } . The proposed method can successfully output the same solution only by one scan of the 7 instances.
We show that the proposed method is scalable, and can generate more accurate predictions compared with the base-lines. Also, we can obtain conditional probability estimates for the examples even if the base models make  X  X ard X  deci-sions. The outputs can be used to summarize the character-istics of the underlying groups in the data.
Data Sets . We present results on four real-world applica-tions. 1) 20 Newsgroups categorization: The 20 newsgroups data set 3 contains approximately 20,000 newsgroup doc-uments, partitioned across 20 different newsgroups nearly evenly. We used the version where the newsgroup messages are sorted by date, and separated into training (60%) and test sets (40%). From the data sets, we construct 6 learning problems, each of which has documents from 4 different top-ics to distinguish. 2) Cora research paper classification [21]: The data set contains around 37,000 research papers that are classified into a topic hierarchy with 73 leaves. The cita-tions among papers are around 75,000 entries. We conduct experiments on two top-level and two second-level classifi-cation problems where papers are from three to five differ-ent research fields. 3) DBLP network: We extracted two data sets from the DBLP network. The smaller one con-tains authors and conferences of four closely related areas in information management domain, whereas the larger one has seven broader areas. 4) Yahoo! Movies: The dataset is from the Yahoo! Alliance WebScope program 4 and it con-tains around 10,000 users, 14,000 movies and 220,000 rat-ings based on data generated by Yahoo! Movies on or before November 2003. We sampled two subsets from this data set where movies are from three different genres. We utilize the movie descriptive information (genre and synopsis) and movie ratings to derive the user type (which kinds of movies the user favors). Users are anonymous but the demographic information (birth-year and gender) of most users are avail-able. More details about the above classification problems can be found in Table 2, where | T | is the number of objects.
Baseline Methods . First, the proposed method is based on multiple single supervised and unsupervised models. We determine the base models according to each data set X  X  char-acteristics. Since 20 Newsgroups data set only has text in-formation, the base models are two classification (logistic re-gression and SVM, implemented in [11] and [4], denoted as SC1 and SC2 ) and two clustering algorithms (K-means and min-cut, implemented in [15], denoted as UC1 and UC2 ). In Cora, DBLP and Yahoo! Movies data sets, both the la-beled set and the unlabeled target set T can be represented in two ways, which correspond to text and link informa-tion. On each of them, we train a logistic regression classi-fier on the labeled set and predict on T , as well as apply the K-means clustering algorithm on T . The two classification models are denoted as SC1 (link) and SC2 (text), and the two clustering models are UC1 (link) and UC2 (text). The two representations of these three data sets are as follows. Cora has paper abstracts as the text information. The link information is conveyed by the citation network where two papers are connected if one cites the other. So we can use the class labels of the neighboring nodes in the network as http://people.csail.mit.edu/jrennie/20Newsgroups/ http://research.yahoo.com the link features of each paper. For the DBLP data set, we pool the titles of publications in a conference or by an author as their text features. On the other hand, we regard each conference as one dimension, and the number of papers an author published in the conference is the link feature value. In the Yahoo! Movies data sets, the movie ratings of the users act as the link information, and we collect the syn-opses of the movies a user rates greater than 3 out of 5 as the text features. In 20 Newsgroups and Cora, for the su-pervised models, outside knowledge comes from the domain the set T belongs to, whereas in DBLP and Movie data sets, supervised models are trained on a different domain (e.g., conferences vs. authors, movies vs. users).

Besides the single models, we compare the proposed method with the following ensemble methods. Note that we assume the raw data are inaccessible, and the proposed algorithm only takes outputs from multiple models as input. Therefore the baseline ensemble methods should also combine multi-ple models X  outputs without referring to the original fea-ture values. The output of each model is  X  X ard X , i.e., it only gives the predicted class label or cluster ID. We first map the clusters generated in one clustering model to match with those from the other model with the help of hungarian method 5 . Then, we learn two majority-voting based en-sembles from the set of supervised and unsupervised base models separately, where ties are broken randomly. We denote them as Supervised Models Ensemble ( SME ) and Unsupervised Models Ensemble ( UME ) respectively. Also, we may ignore the class labels in the supervised models, regard all the base models as unsupervised clustering and try a clustering ensemble method to integrate all the parti-tionings. We use the Meta-Clustering Algorithm ( MCLA ) introduced in [26], where the final clustering solution is in-duced from the meta-clusters formed in a hyper-graph. Note that both UME and MCLA only perform clustering, and do not predict the class labels for the examples in T . We de-note the proposed method which consolidates all the mod-http://www.cs.umu.se/  X  niclas/matlab/assignprob/ els as Consensus Learning on Supervised and Unsupervised Models ( CLSU ). In the experiments, default parameters are used in the base packages and we set  X  = 0 . 4,  X  around 0.5, and the number of iterations in the first step to be 20.
Measures . The instances in 20 Newsgroups and Cora data sets have their class labels. Moreover, we manually la-bel the research fields of the authors in the first task of DBLP data set. For the purpose of evaluation, we restrict the num-ber of clusters from the clustering algorithms to be the same as the number of classes. On these data sets, we evaluate the proposed method and the baselines from the following two perspectives: 1) We map the outputs of all the clustering algorithms to the best possible class predictions using hun-garian method where cluster ids are matched with the class labels. Now all the methods have class label predictions for the examples in T , and thus we can evaluate their classifi-cation accuracy. Actually, this procedure on the clustering methods is  X  X heating X  since the true class labels are used to do the mapping, and thus it should be able to generate the best accuracy from these unsupervised models. 2) All the methods, no matter classification or clustering, can group the test instances into c groups, so we can evaluate the clus-tering quality using the external measure X  X ormalized mu-tual information (NMI) [26], averaged by the test set size. We construct a  X  X rue X  model from the groundtruth labels, and compute the amount of information shared by the al-gorithms and the true model. A higher NMI indicates that the algorithm performs better on the data set. Due to the scale of the second DBLP data set and the anonymity of the Yahoo! Movies users, we cannot label these two test sets but simply show some examples or statistics of each group.
In this section, we assess the performances of the proposed method in terms of accuracy and scalability.

Prediction Accuracy . Table 3 presents the experimen-tal results on the 20 Newsgroups, Cora and DBLP data sets using accuracy or NMI as the performance measure. For the baseline ensemble methods (SME, UME, MCLA), the ties are randomly broken so we obtain their performance mea-sures by averaging 50 runs. From the comparisons, we ob-serve that: 1) On different data sets, the best single model, with respect to accuracy and NMI, can be different, which indicates that there exists large variability in the single mod-els X  predictions. 2) If only part of the information sources are used to construct an ensemble (SME, UME, MCLA), the performance may not always be improved due to the information loss. 3) The proposed CLSU method always outperforms all the base models and the baseline ensemble methods with a large margin in terms of both classification accuracy and clustering quality. We can see the consistent and often dramatic increase in performance measures on the 20 Newsgroups and Cora data sets (baselines X  accuracy is mostly around 85%, but CLSU increases it to above 90%), as well as the DBLP data sets (the single models X  accuracy is from 79% to 93%, and CLSU improves it to over 95%). It demonstrates the generalization accuracy and robustness of the proposed method. The success is attributed to the proposed method X  X  wise negotiation among multiple infor-mation sources, which jointly make the accurate predictions.
Conditional Probability Estimates . The proposed con-sensus learning algorithm is also able to transform  X  X ard X  predictions of the base models to estimates of conditional probabilities. We selected some authors randomly sampled from the two DBLP tasks, and presented the probability of each author doing research in different areas in Table 4. The results are conducted on the subset of authors who publish in selected top conferences. The details about the research areas in the two tasks are shown in Table 2. These examples reveal that many authors are conducting research in multiple areas. It is very likely that the base models make different predictions about their areas, and thus the probability es-timates generated by the proposed method are distributed among the areas they contribute to. For example, Jeffrey D. Ullman contributes to both databases and theory com-munities, and Andrew W. Moore X  X  research is devoted to both data mining and machine learning areas. On the other hand, there are authors who mainly focus on one area, and the proposed method will assign a high probability to the area on which most of the models agree. Examples include Donald F. Towsley in networking and Michael Stonebraker in databases.

Group Summarization . In this experiment, we pre-dict the distribution of a user X  X  interests among the different movie genres in Yahoo! Movies data sets. Both the synopses and the ratings of the movies a user has watched provide use-ful information for this task, and we build both supervised and unsupervised models over the two types of information and consolidate their predictions. We compute the proba-bility of a user belonging to a movie genre group (such as Comedy). After that, we divide the users according to their demographic information: female or male, and age &lt; 20, age between 20 and 40, and age &gt; 40, and average the condi-tional probability estimates over the users of the same gen-der or age. Figure 4 reveals some interesting patterns we find in the user interest distributions. We can see that females love Drama and Comedy, whereas males X  main interests are on Action movies. When people grow older, their interests gradually shift from Comedy to Drama. Regarding the dis-tributions among Kids/Family, Science Fiction and Musical, females like the Musical movies much better than males, and people at ages 20 to 40 fall for Science Fiction movies the best, whereas teenagers have to watch Kids/Family movies a lot. Therefore, the proposed method can be applied to grouping of users for many of such services.

Scalability . As discussed in Section 3.3, the time com-plexity of the proposed method is quadratic in terms of the number of clusters and models, but linear with respect to the test set size. Since we usually deal with large-scale data sets which can be categorized into small groups, the run-ning time is mostly determined by the number of instances, and thus the proposed method scales well to large data sets. We select four learning tasks, and randomly sample a subset from each set containing  X  of the original instances (  X   X  { 20% , . . . , 100% } ). The results are averaged over 50 runs and demonstrated in Figure 5. As most curves are lin-ear especially when  X  is greater than 60%, we can conclude that the results are consistent with our analysis that the proposed method has linear time complexity.
Many studies have shown that ensembles of multiple clas-sifiers can usually reduce variance and achieve higher ac-curacy than individual classifiers [5, 13, 2]. These stud-ies usually focus on deriving weak classifiers from data and boosting their performance by model combination. Their problem setting is different from what we discussed in this paper because they usually assume the availability of raw data. In unsupervised learning, study of clustering ensem-ble [26, 7, 12, 24, 18, 6] has been an active research area, whose objective is to produce a single clustering that agrees with multiple clusterings. The success of combining multiple models has been recognized when ensemble is shown to ben-efit from individual models as well as improve the accuracy and robustness. In fact, our method shares the same spirit as all the ensemble methods, but we extend the scope of base models to both supervised and unsupervised fields and try to find the best solution by negotiating their differences.
In recent years, an extensive body of work has crossed the boundary of supervised and unsupervised information sources. Semi-supervised or transductive learning [14, 27, 29] explores the use of unlabeled information to achieve bet-ter generalization on the unlabeled set. Particularly, label propagation [27] is used in our approach to propagate infor-mation over the belief graph. Link-based classification (i.e., collective inference, relational learning) [20, 22, 25] utilizes the link structure to classify a set of related instances si-multaneously. These studies reveal that the unlabeled in-formation, when used together with labeled instances, can produce considerable improvement in classification accuracy. However, they only take one unlabeled information source into account (e.g., manifold structure or link structure in unlabeled data set), but ignore the other possible unlabeled sources. People have investigated the problem of learning from two complementary views (co-training) [3] or multiple views (multiple view learning) [9]. Our proposed framework is more general than these studies in the sense that we do not require the labeled and unlabeled sources to be symmet-ric. Furthermore, we do not require access to raw data, but instead use prediction results from multiple models as input.
Some other types of information combination have also drawn researchers X  attention, such as transfer learning en-semble [10, 19], webpages classification based on content and link information [28], label inference from two unlabeled data sources [17], and ensemble of relational classifiers [23]. However, all these methods only consider combining mod-els in some specific formats. In a world with information explosions, we need a general framework that can take ad-vantage of heterogeneous information sources. Li et al. [16] demonstrate that knowledge from the word space can be transformed to the document space to improve document clustering, however, the only information source used is the word co-occurrence matrix. In this paper, we show that for the task of knowledge transfer among variables of different types, information sources can be of many folds and a seam-less consolidation of all the sources can outperforms ad-hoc combinations of part of the information sources.
In many applications, the class label of the same object can be inferred from multiple sources in different formats, such as graphs, text documents, user ratings, and click through rates. These heterogeneous information sources could be ei-ther supervised that contains labeled information of inter-est, or unsupervised that only contains structural similarity. In this work, we take advantage of different but comple-mentary predictive powers of these heterogeneous sources to derive consolidated labels for a set of examples. This work extends the applicability of ensemble-based techniques to cross the boundary between labeled and unlabeled in-formation by reaching and negotiating a consensus among them. This is different from traditional approaches of ma-jority voting or model-averaging such that a minority label from supervised models or labels not even predicted by some supervised models could be the consolidated prediction. We presented a two-step heuristic method, which first uses a be-lief graph to propagate labeled information between super-vised and unsupervised models for groups of examples with similar properties until stable predictions are reached. The final prediction is determined by negotiating among multiple models according to each example X  X  neighborhood structure, and weighting models based on their consistencies with other models. On four data sets including 20 Newsgroups, Cora research papers, DBLP network and Yahoo! Movies, we have improved the best base model accuracy by 10%.
