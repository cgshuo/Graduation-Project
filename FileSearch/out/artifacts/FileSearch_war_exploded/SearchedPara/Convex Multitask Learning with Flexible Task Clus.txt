 Leon Wenliang Zhong WZHONG @ CSE . UST . HK James T. Kwok JAMESK @ CSE . UST . HK Many real-world problems involve the learning of a number of tasks. Instead of learning them individually, it is now well-known that better generalization performance can be obtained by harnessing the intrinsic task relationships and allowing tasks to borrow strength from each other. In recent years, a number of techniques have been developed under this multitask learning (MTL) framework.
 Traditional MTL methods assume that all the tasks are re-lated (Evgeniou &amp; Pontil, 2004; Evgeniou et al., 2005). However, when this assumption does not hold, the perfor-mance can be even worse than single-task learning. If it is known that the tasks are clustered, a simple remedy is to constrain task sharing to be just within the same clus-ter (Argyriou et al., 2008; Evgeniou et al., 2005). This can be further extended to the case where task relationships are represented in the form of a network (Kato et al., 2007). However, in practice, such an explicit knowledge of task clusters/network is rarely available.
 Recently, by adopting different modeling assumptions, a number of approaches have been proposed that identify task relationships simultaneously with parameter learning. For example, some assume that the task parameters share a common prior in a Bayesian model (Yu et al., 2005; Zhang &amp; Schneider, 2010; Zhang &amp; Yeung, 2010); that the data follows a dirty model (Jalali et al., 2010); that most of the tasks lie in a low-dimensional subspace (Ando &amp; Zhang, 2005; Chen et al., 2010), or that outlier tasks are present (Chen et al., 2011a). In this paper, we will mainly be in-terested in techniques that assume the tasks are clustered (Argyriou et al., 2008; Evgeniou et al., 2005), and then in-fer the clustering structure automatically during learning (Jacob et al., 2008; Kang et al., 2011). Interestingly, it is recently shown that this clustered MTL approach is equiv-alent to alternating structure optimization (Ando &amp; Zhang, 2005) that assumes the tasks share a low-dimensional struc-ture (Zhou et al., 2011).
 However, all the existing methods model the task relation-ships at the task level, and the features are assumed to al-ways observe the same set of task clusters or covariance structure (Figure 1(a)). This may be restrictive in some real-world applications. For example, in recommender systems, each customer corresponds to a task and each feature a movie attribute. Suppose that we have a rela-tively coherent group of customers, such as Jackie Chan fans who are interested in action comedy movies (Fig-ure 1(b)). On the  X  X anguage X  attribute, however, some of them may prefer English, some prefer standard Chinese (Putonghua/Mandarin), some prefer Cantonese or even a combination of these. Hence, the clustering structure as seen by this feature is very different from those of the oth-ers. Another example is when the features are obtained by some feature extraction algorithm (such as PCA) and so have different discrimination abilities. While the less dis-criminating features may be used in a similar manner by all the tasks, highly discriminating features may be very class-specific and are used differently by different tasks (Figure 1(c)). Hence, again, these features may observe different task relationships. This phenomenon will also be demonstrated in the experiments in Section 3. In this paper, we extend clustered MTL such that the task cluster structure can vary from feature to feature. This is thus more fine-grained than existing MTL methods that on-ly capture task-level (but not feature-level) relationships. Moreover, a key difference with (Jacob et al., 2008) is that we do not require the number of clusters to be pre-specified. Indeed, depending on the complexity of the tasks and use-fulness of each feature, different numbers of clusters can be formed for different features.
 Computationally, the optimization problem is often chal-lenging in clustered MTL algorithms. For example, in (Kang et al., 2011), it leads to a mixed integer program, which has to be relaxed as a nonlinear optimization prob-lem and then solved by gradient descent. This suffers from the local minimum problem and potentially slow con-vergence. On the other hand, the proposed approach di-rectly leads to a (strongly) convex optimization problem, which can then be efficiently solved by accelerated proxi-mal methods (Nesterov, 2007) after some transformations. Notation : Vector/matrix transpose is denoted by the super-script 0 , k A k F = p trace ( A 0 A ) is the Frobenius norm of matrix A , A i  X  is its i th row and A  X  j its j th column. Suppose that there are T tasks. The t th task has n t training R puts together to form matrices X ( t ) = [ x ( t ) 1 ,..., x y to learn each task. Let the weight associated with task t be w . The predictions on the n t samples are stored in the 2.1. Simultaneous Clustering of Task Parameters We decompose each w t into u t + v t , where u t tries to cap-ture the shared clustering structure among task parameters, and v t captures variations specific to each task. Learning of w t  X  X  is performed jointly with the clustering of u t the following regularized risk minimization problem: where U = [ u 1 ,..., u T ] and V = [ v 1 ,..., v T ] , and  X  , X  2 , X  3 are regularization parameters. The first term in (1) is the empirical (squared) loss on the training data, and k U k clus is the sum of pairwise differences for elements in each row of U , For each feature d and each ( u i , u j ) pair, the pairwise penalty in k U k clus encourages U di ,U dj to be close togeth-er, leading to feature-specific task clusters. It can also be shown that k U k clus is a convex relaxation of k -means clus-tering on each feature. Note that this is different from the fused lasso regularizer (Tibshirani et al., 2005), which is used for clustering features in single-task learning while k U k clus is for clustering tasks in MTL. It is also different from the graph-guided fused lasso (GFlasso) (Chen et al., 2011b), which does not decompose w t as u t + v t , and subsequently cannot cluster the tasks due to the use of s-moothing. The regularizer k V k 2 F = P T t =1 k v t k 2 penal-izes the deviations of each w t from u t , and k U k 2 F is the usual ridge regularizer penalizing U  X  X  complexity. Since k U k 2 F , k V k 2 F are strongly convex and the other terms in (1) are convex, (1) is a strongly convex optimization prob-lem.
 Some MTL papers also decompose w t as u t + v t , but the formulations and goals are different from ours. In (Evge-niou et al., 2005), u t is the (single) cluster center of all the tasks; in (Ando &amp; Zhang, 2005; Chen et al., 2010; 2011a), u comes from a low-dimensional linear subspace, which is extended to a nonlinear manifold in (Agarwal et al., 2010); in (Jalali et al., 2010), u t is the component that uses fea-tures shared by other tasks.
 Moreover, model (1) encompasses a number of interesting special cases: (i)  X  1  X   X  : 1 For each d , all U dt  X  X  become the same. Thus, w t reduces to  X  u + v t for some  X  X ean weight X   X  u , and (1) reduces to the model in (Evgeniou et al., 2005). (ii)  X  1 = 0 : The following Proposition shows that (1) reduces to independent ridge regression on each task. Proposition 1. When  X  1 = 0 , model (1) reduces to (iii)  X  2 6 = 0 , X  3 = 0 : Since u t is penalized while v not, u t will become zero at optimality, irrespective of the value of  X  1 . Thus, (1) reduces to independent least squares regression on each task: min w t k y ( t )  X  X ( t ) w t k 2 . Obvi-ously, this is the same as setting  X  1 =  X  2 =  X  3 = 0 . 2.2. Properties Denote the optimal solution in (1) by ( U  X  , V  X  ) , and let W  X   X  U  X  + V  X  . The following Proposition shows that if tasks i and j have similar weights on feature d , the cor-responding U  X  entries are clustered together. On the other hand, for an outlier task t , its u t component is separated from the main group.
 Proposition 2. If | W  X  di  X  W  X  dj | &lt;  X  1  X  For simplicity, all T tasks are assumed to have the same number of training instances n . Assume that the data for task t is generated as y ( t ) = X ( t )  X w t + , where  X  N ( 0 , X  2 I ) is the i.i.d. Gaussian noise, and k X ( t ) The following Theorem shows that, with high probabili-ty, W  X  is close to the ground truth  X  W = [  X w 1 ,...,  X w w.r.t. the elementwise `  X  -error k  X  W  X  W  X  k  X  ,  X  = all the tasks are identical, the shared clustering component U  X  is close to  X  W ; and V  X  , the deviation from the cluster center, goes to zero.
 Theorem 1. 1. k  X  W  X  W  X  k  X  ,  X   X  c 1 holds with probability at least 1  X  2 exp(  X  c 2 log( DT )) for any c 1 &gt; p (1 + c 2 ) log( DT ) and c 2 &gt; 0 . Here, tive definite matrix, and  X  1 , X  2 , X  3 = O (  X  arbitrary high probability. 2. When all tasks are identical (i.e.,  X w 1 =  X  X  X  =  X w and k V k  X  ,  X   X  0 hold with probability at least 1  X   X  c 2 &gt; 0 . Here, value of  X   X  =  X  and  X  2 = O ( arbitrary high probability.
 Moreover, the following Corollary shows that the underly-ing clustering structure can be exactly recovered when n is sufficiently large.
 Corollary 1. Suppose that for any feature d ,  X  W di  X  W dj if i,j are in the same cluster; and |  X  W dj |  X   X  otherwise. Assume that 1 2  X  ( t )  X  W  X  t  X  C h k  X  have U  X  di = U  X  dj if i,j are in the same cluster; and U di 6 = U 2 exp(  X  c 2 log( DT )) for any c 1 &gt; p (1 + c 2 ) log( DT ) and c 2 &gt; 0 . 2.3. Optimization via Accelerated Proximal Method In recent years, accelerated proximal methods (Nesterov, 2007) have been popularly used by the machine learning community (Bach et al., 2011) for convex problems of the form min  X  f (  X  ) + r (  X  ) , where f (  X  ) is convex and smooth, and r (  X  ) is convex but nonsmooth. The convergence rate is optimal for the class of first-order methods. Together with their algorithmic and implementation simplicities, they can be used on large smooth/nonsmooth convex problems. In this paper, we use the well-known method of FISTA (Fast Iterative Shrinkage-Thresholding Algorithm) (Beck &amp; Teboulle, 2009). Extending to other accelerated proxi-mal methods is straightforward. Each FISTA iteration per-forms the following proximal step min where  X   X  k is the current iterate, and L k is a scalar often determined by line search. Since (3) is required in every FISTA iteration, it needs to be solved very efficiently. For problem (1), let  X  = [ U 0 , V 0 ] 0 . Define Step (3) can be rewritten as min  X  k  X   X   X   X  k 2 F + 2 L where  X   X  = [  X  U 0 ,  X  V 0 ] 0 =  X   X  k  X  1 L 2011a). Expressing back in terms of U and V , (3) becomes  X 
U =  X  U k  X  As f (  X  ) in (4) is simply the squared loss, the t th column-step is only required to be convex and smooth, many other commonly used loss functions can be used in (1) instead. As U and V are now decoupled, they can be optimized independently as will be shown in the sequel. The whole algorithm for solving (1) is shown in Algorithm 1. Algorithm 1 Algorithm for solving (1). 1: Initialize:  X  U 1 ,  X  V 1 , X  1  X  1 . 2: for k = 1 , 2 ,...,N  X  1 do 3: Compute  X  U and  X  V in (6). 4: U k  X  arg min U k U  X   X  U k 2 F +  X   X  1 k U k clus + 8: end for 9: Output U N . For fixed U , the subproblem in (5) related to V is min V k V  X   X  V k 2 F +  X   X  3 k V k 2 F . On setting the gradient of the objective w.r.t. V to zero, we obtain V = h  X  v ij For fixed V , the subproblem in (5) related to U is min U k U  X   X  U k 2 F +  X   X  1 k U k clus +  X   X  2 k U k 2 F O ( T 2 ) number of terms in k U k 2 F , this is more challeng-ing than the computing of V in Section 2.3.1. However, as the rows of U are independent, U can be optimized row by row. For the d th row, we have can be rewritten as the optimization problem considered in (Zhong &amp; Kwok, 2011), and hence can be solved efficiently using the algorithm proposed there. Computing the gradients  X  U f (  X   X  k ) and  X  V f (  X  O ( nDT ) time. Computing V k takes O ( DT ) time. Com-puting one row of U k using the algorithm in (Zhong &amp; K-wok, 2011) takes O ( T log T ) time, and thus O ( DT log T ) time for the whole U k . Hence, the total complexity for Al-gorithm 1 is only O ( TDn + DT log T ) . Moreover, FISTA converges as O (1 /N 2 ) (Beck &amp; Teboulle, 2009), where N is the number of iterations. This is much faster than tra-ditional gradient methods, which converges as O (1 / It is also faster than GFlasso (Chen et al., 2011b), which solves a similar problem as (1), but converges as O (1 /N ) and has a per-iteration complexity of O ( T 2 ) .
 Though (7) is similar to the optimization problems of the pairwise fused lasso in (Petry et al., 2011; She, 2010), us-ing the optimization procedures there are much more ex-pensive. Specifically, the procedure in (Petry et al., 2011) takes O ( T 6 ) time, as it involves a QP with T 2 additional optimization variables; while (She, 2010) relies on anneal-ing, which is even more complicated and expensive. 2.4. Adaptive Clustering As in the adaptive lasso (Zou, 2006), weights can be added where  X  d,  X  i  X  j is the weight associated with the i th and j th with the unweighted k U k clus to obtain W , and then set  X  U i ,U d  X  j will be strongly encouraged to be clustered togeth-er, and vice verse. Moreover, the optimization procedure in Algorithm 1 can still be used. In this section, we perform experiments on a number of synthetic and real-world data sets. All the data sets are s-tandardized such that the features have zero mean and unit variance for each task. The output of each task is also stan-dardized to have mean zero.
 3.1. Synthetic Data Sets In this experiment, the input has dimensionality D = 30 and is generated from the multivariate normal distribution x  X  N ( 0 , I ) . We use T = 10 tasks, with the output of the t th task generated as y t  X  x 0  X w t + N (0 , 400) . All tasks have 30 training samples and 100 test samples. The task parameters are designed in the following manner to mimic various real-world scenarios: (C1) All tasks are independent:  X w t  X  X  ( 0 , 25 I ) for all t . (C2) All tasks are from the same cluster:  X w t = w m + (C3) All tasks are from the same cluster as in C2 , but (C4) A main task cluster plus a few outlier tasks:  X w t  X  (C5) Tasks in overlapping groups: We have two groups (C6) This is used to simulate the recommender systems The proposed model will be called FlexTClus ( Flexible Task-Clustered MTL ). It is compared with a variety of single-task and state-of-the-art MTL algorithms, includ-ing: 1) Independent ridge regression on each task; 2) Pool-ing all the training data together to learn a single model: This assumes that all the tasks are identical; 3) Regular-ized MTL: This assumes that all the tasks come from a s-ingle cluster (Evgeniou &amp; Pontil, 2004); 4) The dirty mod-el in (Jalali et al., 2010); 5) Low-rank-based robust MTL (Chen et al., 2011a); 6) Sparse-LowRank MTL (Chen et al., 2010), which learns sparse and low-rank patterns from the tasks; 7) Clustered MTL (Jacob et al., 2008) 2 and 8) Multi-task relationship learning (MTRL) (Zhang &amp; Yeung, 2010). Regularization parameters for all the methods are tuned by a validation set of size 100. To reduce statistical variabil-ity, results are averaged over 10 repetitions. In each rep-etition, w m is generated from N ( 0 , 25 I ) ; whereas in C5 , w (1)  X  N ( 0 , 25 I ) and w (2)  X  N ( 0 , 100 I ) . The normal-ized mean squared error (NMSE), which is defined as the MSE divided by the variance of the ground truth, is used for performance evaluation.
 Results are shown in Table 1. We have the following ob-servations.  X  C1 : Since the tasks are independent, so as expected,  X  In C2 , all tasks are from the same group, and hence  X  C4 is a common MTL setup. As expected, almost all  X  C5 and C6 are the most challenging. FlexTClus (and Figure 2 compares the ground truth clustering structures of the task parameters with those obtained by adaptive FlexTClus. As can be seen, FlexTClus can well capture the underlying structure. 3.2. Examination Score Prediction In this section, experiment is performed on the school data set (Bakker &amp; Heskes, 2003). As in (Chen et al., 2011a), we use 10% , 20% and 30% of the data for training, another 45% for testing, and the remaining for validation. To re-duce statistical variability, results are averaged over 5 rep-etitions.
 Results are shown in Table 2. Note that though the school data has been popularly used as a MTL benchmark, it has been pointed out previously that all the tasks are indeed the same (Bakker &amp; Heskes, 2003; Evgeniou et al., 2005). Hence, the trend in Table 2 is similar to that of C2 in Ta-ble 1. As can be seen, both versions of FlexTClus are very competitive in this single-cluster case, and are better than the other MTL methods. Figure 3 shows the task cluster-ing structure obtained by adaptive FlexTClus. Clearly, it indicates that there is only one underlying task cluster. 3.3. Handwritten Digit Recognition In this section, we perform experiments on two popu-lar handwritten digits data sets, USPS and MNIST. As in (Kang et al., 2011), PCA is used to reduce the feature dimensionality to 64 for USPS and 87 for MNIST. For each digit, we randomly choose 10 , 30 , 50 samples for training, 500 samples for validation and another 500 samples for testing. The 10-class classification problem is decomposed into 10 one-vs-rest binary problems, each of which is treat-ed as a task.
 Results averaged over 5 repetitions are shown in Table 3. We do not compare with pooling, which assumes that all the tasks are identical and is clearly invalid in this one-vs-rest setting. As can be seen, FlexTClus and its adaptive version are consistently among the best, while many other MTL methods suffer from negative transfer and are only comparable or even worse than ridge regression. Fig. 4 shows the task clustering structures obtained. As expected, many trailing PCA features are not useful for discrimina-tion and the corresponding weights are zero. In contrast, the leading PCA features are more discriminative and are used by the different tasks in different manners, leading to more varied cluster structures. 3.4. Rating of Products In this section, we use the computer survey data in (Argyri-ou et al., 2008). This contains the ratings of 201 students on 20 different personal computers, each described by 13 attributes. After removing the invalid ratings and students with more than 8 zero ratings, we are left with 172 students (tasks). For each task, we randomly split the 20 instances into training, validation and test sets of sizes 8,8, and 4, respectively.
 Table 4 shows the root mean squared error (RMSE) aver-aged over 10 random splits. Again, FlexTClus and its adap-tive variant outperform the other models. Figure 5 shows the task clustering structure obtained in a typical run. Note that the first 12 features are about the PC X  X  performance (such as memory and CPU speed). As can be seen, there is one main cluster, indicating that most students in this survey have similar preference on these attributes. On the other hand, the last feature is price, and the result indicates that there are lots of varied opinions on this attribute. While existing MTL methods can only model task rela-tionships at the task level, we introduced in this paper a novel MTL formulation that captures task relationship-s at the feature-level. Depending on the myriad relation-ships among tasks and features, the proposed method can cluster tasks in a flexible feature-by-feature manner, with-out even the need of pre-specifying the number of cluster-s. Moreover, the proposed formulation is (strongly) con-vex, and can be solved by accelerated proximal method-s with an efficient and scalable proximal step. Experi-ments on a number of synthetic and real-world data sets show that the proposed method is accurate. The obtained feature-specific task clustering structure also agrees with the known/plausible clustering structure of the tasks. This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant 614311).
 Agarwal, A., Daum  X  e III, H., and Gerber, S. Learning mul-tiple tasks using manifold regularization. In Advances in
Neural Information Processing Systems 23 , pp. 46 X 54. 2010.
 Ando, R.K. and Zhang, T. A framework for learning pre-dictive structures from multiple tasks and unlabeled data.
Journal of Machine Learning Research , 6:1817 X 1853, 2005.
 Argyriou, A., Evgeniou, T., and Pontil, M. Convex multi-task feature learning. Machine Learning , 73(3):243 X  272, 2008.
 Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. Con-vex optimization with sparsity-inducing norms. In Opti-mization for Machine Learning , pp. 19 X 53. MIT, 2011. Bakker, B. and Heskes, T. Task clustering and gating for
Bayesian multitask learning. Journal of Machine Learn-ing Research , 4:83 X 99, 2003.
 Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences , 2(1):183 X 202, 2009. Chen, J., Liu, J., and Ye, J. Learning incoherent sparse and low-rank patterns from multiple tasks. In Proceedings of the 16th International Conference on Knowledge Dis-covery and Data Mining , pp. 1179 X 1188, Washington D.C., USA, 2010.
 Chen, J., Zhou, J., and Ye, J. Integrating low-rank and group-sparse structures for robust multi-task tearning. In Proceedings of the 17th International Conference on Knowledge Discovery and Data Mining , pp. 42 X 50, San Diego, CA, USA, 2011a.
 Chen, X., Lin, Q., Kim, S., Carbonell, J. G., and Xing,
E. P. Smoothing proximal gradient method for general structured sparse learning. In Proceedings of the 27th
Conference on Uncertainty in Artificial Intelligence , pp. 105 X 114, Barcelona, Spain, 2011b.
 Evgeniou, T. and Pontil, M. Regularized multi-task learn-ing. In Proceedings of the 10th International Conference on Knowledge Discovery and Data Mining , pp. 109 X  117, Seattle, WA, USA, 2004.
 Evgeniou, T., Micchelli, C. A., and Pontil, M. Learning multiple tasks with kernel methods. Journal of Machine Learning Research , 6:615 X 637, 2005.
 Jacob, L., Bach, F., and Vert, J. Clustered multi-task learn-ing: A convex formulation. In Advances in Neural Infor-mation Processing Systems 21 , pp. 745 X 752. 2008.
 Jalali, A., Ravikumar, P., Sanghavi, S., and Ruan, C. A dirty model for multi-task learning. In Advances in Neu-ral Information Processing Systems 23 , pp. 964 X 972. Vancouver, 2010.
 Kang, Z., Grauman, K., and Sha, K. Learning with whom to share in multi-task feature learning. In Proceedings of the 28th International Conference on Machine Learning , pp. 521 X 528, Bellevue, WA, USA, June 2011.
 Kato, T., Kashima, H., Sugiyama, M., and Asai, K. Multi-task learning via conic programming. In Advances in
Neural Information Processing Systems 20 , pp. 737 X  744. 2007.
 Nesterov, Y. Gradient methods for minimizing composite objective function. Technical Report 76, Catholic Uni-versity of Louvain, 2007.
 Petry, S., Flexeder, C., and Tutz, G. Pairwise fused lasso.
Technical Report 102, Department of Statistics, Univer-sity of Munich, 2011.
 She, Y. Sparse regression with exact clustering. Electronic Journal of Statistics , 4:1055 X 1096, 2010.
 Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and
Knight, K. Sparsity and smoothness via the fused las-so. Journal of the Royal Statistical Society: Series B , 67 (1):91 X 108, 2005.
 Yu, K., Tresp, V., and Schwaighofer, A. Learning Gaus-sian processes from multiple tasks. In Proceedings of the 22nd International Conference on Machine Learn-ing , Bonn, Germany, August 2005.
 Zhang, Y. and Schneider, J. Learning multiple tasks with a sparse matrix-normal penalty. In Advances in Neural In-formation Processing Systems 23 , pp. 2550 X 2558. 2010. Zhang, Y. and Yeung, D.-Y. A convex formulation for learning task relationships in multi-task learning. In Pro-ceedings of the 24th Conference on Uncertainty in Ar-tificial Intelligence , pp. 733 X 742, Catalina Island, CA, USA, 2010.
 Zhong, L. W. and Kwok, J. T. Efficient sparse modeling with automatic feature grouping. In Proceedings of the 28th International Conference on Machine Learning , pp. 9 X 16, Bellevue, WA, USA, 2011.
 Zhou, J., Chen, J., and Ye, J. Clustered multi-task learning via alternating structure optimization. In Advances in Neural Information Processing Systems 25 . 2011.
 Zou, H. The adaptive lasso and its oracle properties. Jour-nal of the American Statistical Association , 101(476):
