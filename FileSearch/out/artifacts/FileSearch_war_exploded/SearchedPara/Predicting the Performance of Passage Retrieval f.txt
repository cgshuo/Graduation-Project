 We present a novel approach to predicting the performance of passage retrieval for question answering. That is, es-timating the effectiveness, for answer extraction, of a list of passages retrieved in response to a question when rele-vance judgments are not available. Our prediction model integrates two types of estimates. The first estimates the probability that the information need expressed by the ques-tion is satisfied by the passages. This estimate is devised by adapting query-performance predictors developed for the document retrieval task. The second type estimates the probability that the passages contain the answers. This es-timate relies on the occurrences of named entities that are likely to answer the question. Empirical evaluation demon-strates the merits of our prediction approach. For example, the prediction quality is much better than that of the only previous prediction method devised for the task at hand.
The retrieval process of typical QA systems is based on two main phases [10, 12, 9, 8]. The first is retrieving pas-sages from the collection that presumably contain the right answers. The second phase is extracting the answers from these passages. The performance of the second phase, and that of the entire system, is known to significantly depend on the performance of the first [5, 12, 8].

Thus, it is important for a QA system to be able to auto-matically determine if the first phase failed and the retrieved passages cannot be effectively used for the subsequent an-swer extraction phase. Accordingly, the challenge we focus on is predicting the effectiveness of the first phase.
We present a novel performance prediction method for passage retrieval for QA that predicts the  X  X uality X  of a re-trieved passage list; i.e., the presumed inclusion of many rel-evant passages that contain the correct answers. Our model integrates two types of estimates. The first is for the prob-ability that the information need expressed by the question is satisfied by the passage list; i.e., that the list provides question-related context. For this estimate, we adapt state-of-the-art query-performance predictors that were devised for the document retrieval task. The second type of esti-mate is for the probability that the passage list contains pertaining answers. Since the answers to many questions are named entities, we base the estimate on the presence of named entities, which are of the same type as that of the question X  X  answer, within the question X  X  context (i.e., in proximity to the question terms) in the passages.
Evaluation performed with TREC benchmarks shows that our approach substantially outperforms the only prediction method previously employed for the task we address here [7]. Furthermore, we show that applying our entity-based esti-mate benefits retrieval and prediction both independently and when applied simultaneously for these tasks.
The Clarity measure [6], which was originally designed for predicting document-retrieval effectiveness, was applied on passages to predict their effectiveness for the answer extrac-tion phase [7]  X  the task we address here. Although there is abundance of work on predicting query performance for doc-ument retrieval [4], to the best of our knowledge, there was no further work on devising prediction methods for passage retrieval for QA. We show that our approach substantially outperforms Clarity [7].

Our prediction method uses occurrence statistics of named entities in the retrieved passages. There is much work on us-ing named entities for improving the passage retrieval perfor-mance for QA [10, 3, 8, 1]. As noted above, our entity-based estimate is effective for both prediction and retrieval.
The challenge we address is performance prediction for the passage retrieval phase of the question answering (QA) task. Specifically, the goal is to predict the effectiveness (for answer extraction purposes) of a ranked list of passages, G , that was retrieved in response to question q by some retrieval method; S ( g ; q ) denotes the retrieval score assigned to passage g (in G ). In Section 4.1 we discuss the passage retrieval method used for evaluation.
The prediction task can be formally stated as estimating the probability p ( A q | G )thattheanswer A q to the question q can be found in (or extracted from) G .

A key observation that we make is that a requirement for the passage list G to be effective for answer extraction is that the answers appear within a context that is relevant to the information need, I q , expressed by q . For example, for the question q def =  X  X hat is the most densely populated city in France? X , the correct answer, A q ,is X  X aris X ;theinforma-tion need, I q , is  X  X ensely populated cities in France X . Now, if the retrieved passages contain the correct answer,  X  X aris X , but do not discuss aspects related to population density but rather topics such as geography, history, or even celebri-ties (e.g., Paris Hilton), then the answer extraction phase is likely to fail. Indeed, the guidelines for TREC X  X  QA tracks [14] specify that a correct answer appearing in a document that does not allow for identification or verification of the answer X  X  correctness is not considered correct.

Accordingly, the prediction task is devising an estimate where  X  p ( I q | G ) is an estimate for the probability that I satisfied by G ; and,  X  p ( A q | I q ,G ) is an estimate for the prob-ability that the correct answer A q can be extracted from G (i.e., is found within G in the right context) given that I satisfied. In what follows we present estimates that can be used to instantiate specific predictors from Eq. 1.
We first devise an estimate  X  p ( I q | G ) for the probability that the information need is satisfied by passages in G .To that end, we adapt previously proposed predictors, denoted P ( I q | G ). These predictors were devised and shown to be ef-fective for estimating document retrieval effectiveness with respect to information need expressed by a query. Similarly to the document retrieval case [4], here we employ the pre-dictors on G [ k ] ,the k highest ranked passages in G .The focus on highly ranked passages serves to better align the prediction with the passage-retrieval performance metric of concern. Specifically, this is the average precision (AP) mea-sure that will be used in Section 4 as the goal of prediction, and which was used in previous work for evaluating passage retrieval effectiveness for QA [9]. We now turn to describe the post-retrieval predictors adapted for passage retrieval. Clarity. The Clarity predictor [6] estimates the focus of G with respect to a corpus of documents D ,bymeasuring the KL divergence between their induced language models; passages in G are parts of documents in D .(SeeSection 4.1 for details regarding the passage retrieval methods em-ployed.) The higher the divergence, the more focused G is considered to be, and consequently, more likely to satisfy the underlying information need. Specifically, p ( w | x ) is the probability assigned to term w by a language model induced from x . (We describe the language-model induction techniques in Section 4.1.)
WIG . The WIG predictor [15] is based on the premise that high retrieval scores in the list imply to its effective-ness in satisfying the information need. Indeed, WIG was shown to be a highly effective query-performance predictor for document retrieval, and is formally defined here by:
NQC . While WIG is based on the average retrieval score of top-retrieved passages, the NQC predictor measures the standard deviation [11]. High deviation of document re-trieval scores was argued to imply reduced  X  X uery drift X , and thereby improved effectiveness for document retrieval. Accordingly, we define where  X  = 1 k g  X  G [ k ] S ( g ; q ) is the average score in G Originally, both WIG [15] and NQC [11] were normalized using the corpus retrieval score. We do not employ this nor-malization as the passage retrieval scores are already nor-malized (see Section 4.1) and further corpus-based normal-ization yielded degraded prediction quality.
We next devise an estimate  X  p ( A q | I q ,G )(seeEq. 1)forthe probability that a correct answer can be found in (extracted from) G given that the information need is satisfied. We refer to this estimate as a  X  X udge X  [8].

For many questions the answer is a named entity [3]. This fact can be exploited for devising a judge. In what follows, we focus on four types of entities that were found to be effective for the question answering task [3]; namely, Per-son, Organization, Location and Date. We use Misc to re-fer to the type of an answer that is not an entity or that is an entity of a type not among the four just specified (e.g.,  X  X hat is the distance of the moon from earth? X ). We assume that for each question the answer type t (  X  {
P erson, Organization, Location, Date, Misc } )canbeiden-tified [8]; q t denotes a question q with answer type t .
We first consider a judge for questions with answers that are entities with types in { Person, Organization, Location, Date } . Then, we present a generalized judge that also ac-counts for questions with the Misc answer type.

The NEQ judge that we consider accounts only for en-tities that occur in a question-related context. Specifically, an entity E t of type t is considered highly related to q and all q t  X  X  terms appear together in a short window of text within the passage. The statement ( E t ,q t )  X  win g  X  is used below to specify that such co-occurrence holds in a window, win g  X  , that is composed of  X  consecutive terms in passage g ;  X  is a free parameter.
 We apply log transformation to moderate the effect of large counts and employ add-2 smoothing to avoid zero multipli-cation in Eq. 1. Thus, NEQ considers G as likely to contain the answer if G contains many entities of the same type as that of the question X  X  answer type and that co-occur together with the question terms.

If the answer to q is of type Misc, then NEQ cannot be applied. More generally, it could be that NEQ should be more  X  X rusted X  for certain types of answers than for others. For example, if entities of a certain type are more likely, in general, to have high occurrence in texts than entities of another type, then the resultant NEQ estimate can be unjustifiably biased across answer types. Since one of our goals is to compare prediction quality across questions of varying answer types, we study the NEQT judge: P NEQT controls, on a per answer-type basis, the reliance on the NEQ judge;  X  t is a free parameter that depends on t . That is, for a small  X  t value, NEQT backs off from using NEQ to a constant-based prediction value, which amounts to more heavily relying on the information-need satisfaction aspect of prediction. (See Eq. 1.). As NEQ cannot be applied to Misc questions, we set  X  Misc to 0.
As is common in work on QA [3, 12, 9], the first step of our passage retrieval approach is retrieving from the corpus a document list, D q , based on document-question similarities. The sentences in these documents, capped at 64 terms, serve for passages; D q contains 500 documents as this results in highly effective passage retrieval performance.

To measure the similarity between the question q and text x , a passage or a document, we use a language-model-based estimate: Sim ( q,x ) def = q i  X  q p ( q i | x ) , where p ( w probability assigned to term w by a Dirichlet-smoothed un-igram language model induced from x with the smoothing parameter set to 2000.

Passage g is scored with respect to q by a common passage retrieval approach [2], henceforth referred to as PDQ : S
PDQ ( g ; q ) d g is the document to which g belongs;  X  is a free parame-ter set to 0 . 9 as this yields (near)-optimal passage retrieval performance (with respect to values in { 0 , 0 . 1 ,..., 1 all experimental settings. The 1000 most highly ranked pas-sages serve for the passage list; estimating the effectiveness of this list is the prediction goal.

As discussed in Section 3, we use the average precision measure (AP at cutoff 1000) to evaluate the effectiveness of the passage list for QA. We measure the AP of a list of passages using TREC X  X  relevance judgments and the answer patterns associated with the questions [13]. A passage is considered relevant if it (i) contains at least one of the answer patterns, and (ii) is extracted from a relevant document [13].
Pearson X  X  correlation between predicted (AP) performance and actual (AP) performance is the prediction quality mea-sure as in work on query-performance prediction for doc-ument retrieval [4]. Statistical significance of prediction val-ues is determined using Pearson X  X  correlation significance test at a confidence level of 95%.

The prediction methods we study incorporate free param-eters. The values of these parameters, listed below, were se-lected based on the prediction quality attained for the var-ious experimental settings after extensive experimentation with wide ranges. For Clarity (Eq. 2), k , the number of top-ranked passages used for inducing the (relevance) language model, which uses 100 terms, was set to 100. For WIG (Eq. 3) and NQC (Eq. 4), k was set to 5 and 25, respectively. For theNEQjudge(Eq. 5), k was set to 5. For  X  t (Eq. 6), which controls the reliance on NEQ for the varying question types, 0 was used for the Misc type, and a fixed positive value was effective for the other question types; specifically, the values 0 . 2, 0 . 7, and 0 . 8wereusedwhenNEQwasintegratedwith Clarity, WIG, and NQC respectively. The window size,  X , in Eq. 5 was set to 50.

We used Stanford X  X  NER 1 , a named entity extraction ap-plication, for annotating Person, Organization and Location entities within the passages. For Date entities, we used a simple date pattern detection method. Although some clas-sification schemes were successf ully employed for categoriz-ing questions [3, 7, 1], we manually categorized the test ques-tions to achieve maximal classification accuracy; the classi-fication problem is out of the scope of this paper. Experiments were conducted using TREC QA datasets:
We applied tokenization and Porter stemming to all data using the Lemur/Indri toolkit. We did not remove stop-words from the documents nor from the questions. However, for the NEQ judge, which counts only entities occurring to-gether with all question terms in a small text window, we removed stopwords from the questions (using the INQUERY stopword list plus single letter terms) as it significantly im-proved its performance. Main result. Table 1 presents the prediction quality num-bers. The second row presents the prediction quality of NEQT when used alone. The following rows present the prediction quality of the post-retrieval predictors when used alone and when integrated with NEQT based on Eq. 1.
NEQT posts low prediction quality when used alone. A possible explanation is that the passage list may contain many entities having the question X  X  answer type and hence highly estimated by NEQT. However, whereas the list does not pertain to the information need, we deem it ineffective. In contrast, once NEQT is integrated with any of the post-retrieval predictors, all prediction quality numbers are sta-tistically significant. Furthermore, we get an increased pre-diction quality in nearly all cases with respect to that of not using NEQT. These findings support the merits of our pre-diction approach that integrates a prediction for information need satisfaction with an estimate, based on named entities, for the probability that an answer can be extracted.
A comparison of the predictors, with or without NEQT, reveals that WIG  X  NEQT and NQC  X  NEQT yield the highest http://www-nlp.stanford.edu/software/CRF-NER.shtml Table 1: Main result. (Prediction quality measured using Pearson X  X  correlation.) Boldface: the best re-sult in a column. Statistically significant correla-tions are italicized. prediction quality over all collections. In fact, the Clarity predictor, the only baseline for predicting the effectiveness of passage retrieval for QA in previous work [7], is inferior in all cases to all other predictors when used alone (as in [7]), andwhenintegratedwithNEQT.
 strated above the effectiveness of our approach, which uses an entity-based judge, for predicting the performance of the PDQ retrieval method (Eq. 7) that does not use entity-based information. We now explore whether our predic-tion approach is also effective for predicting the performance of a passage retrieval method that uses entity-based infor-mation. To that end, we use the PDQ  X  NEQ method, which scores passage g by: S PDQ ( g ; q ) P NEQ ( A q t | fer back to Eq. 5 and 7). To alleviate the computation effort, PDQ  X  NEQ re-ranks the 50 initially highest ranked passages; lower ranked passages retain their original ranks. Furthermore, passages retriev ed for Misc type questions are not re-ranked as they have no target entity, and hence, re-ranking (based on NE occurrences) has no impact.

As it turns out, PDQ  X  NEQ outperforms PDQ in a sub-stantial and statistically significant manner (two tailed paired t-test, p =0 . 05). Specifically, PDQ X  X  MAP performance is . 31, . 24, . 21, and . 18 for TREC X 99, TREC X 00, TREC X 01, and TREC X 02, respectively, while that of PDQ  X  NEQ is . 33, . 26, . 23, and . 21. This finding, which echoes those from work on using named entities for improving passage retrieval perfor-mance for QA [3, 5, 1], attests to the effectiveness, in terms of retrieval performance, of our entity-based judge. In Table 2 we present the prediction quality of NQC and NQC  X  NEQT over all the question types studied (including Misc) when applied over G PDQ (the passage list used inso-far) and G PDQ  X  NEQ , the passage list created using PDQ (Using WIG yields similar prediction quality patterns.) We can see in Table 2 that the prediction quality for G PDQ  X  NEQ is often superior to the prediction quality for G
PDQ . Thus, our prediction approach is effective whether the passage retrieval method uses entity-based information or not. Furthermore, these findings allow for maximal flex-ibility in terms of resource allocation between the retrieval and prediction tasks for QA, because the entity-based judge can be used simultaneously for improving retrieval and for predicting retrieval performance.
We presented a novel approach to predicting the perfor-mance of passage retrieval for question answering and em-Table 2: Prediction quality for different passage lists. Boldface: the best result in a column. All correlations are statistically significant. pirically demonstrated its merits. The approach integrates predictors adapted from work on performance prediction for document retrieval with named-entity-based estimates. Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by the Israel Science Fo undation under grant no. 557/09, by IBM X  X  SUR award, and by Google X  X  and Yahoo! X  X  faculty research awards. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
