 1. Introduction
The compressed pattern matching problem which was first introduced by Amir and Benson (1992) is of searching for a pattern directly in the compressed text without decompressing it. It is a variant of the clas-sical pattern matching problem, in which one is given a pattern P and a text T , and the problem is to locate the first or all occurrences of P in T . In the compressed version of this problem, the text is supposed to be stored in some compressed form. More formally, given a pattern P and text T , and complementary encod-ing and decoding functions E and D , respectively, our goal is to search for the encoded pattern, E  X  P  X  , in the encoded text E  X  T  X  , rather than searching for the pattern, P , in the decompressed text, D  X  E  X  T  X  X  . Searching for the encoded pattern E  X  P  X  assumes that the encoded pattern is compressed in the same way throughout the text. This assumption is not always true, especially with dynamic compressions, such as Lempel-Ziv variants, where the compression changes as one proceeds. In these cases the encoding of the pattern is also dynamic, and is computed at each point of the file ( Amir, Benson, &amp; Farach, 1996; Klein &amp; Shapira, 2000 ).
Here we deal with static Huffman files ( Huffman, 1952 ), and search for E  X  P  X  in it using a modified KMP algorithm. In order to speed-up the search we use the skeleton trees introduced in the work of Klein (2000) and the Huffman implementation of Moffat and Turpin (1997) .

Searching for encoded patterns in encoded texts raises the problem of false matches , i.e., finding an occurrence of the encoded pattern in the encoded text which does not correspond to an occurrence of the pattern in the original text, due to crossing codeword boundaries. Consider for example the Huffman code {00,01,100,101,110,1110,1111} for the characters t , a , g , d , b , o and c , respectively. The binary string 101-1110-100 is the encoding of the string dog . Suppose, however, that we are searching for the pattern cat : we could find E  X  cat  X  starting at the third bit and extending to the end of E  X  dog  X  . The problem is thus one of verifying that the occurrence detected by the pattern matching algorithm is aligned on codeword boundaries. False matches can be avoided by using a code where no codeword is a prefix or suffix of any other codeword. This type of code is called Affix or a Fixfree code , and is extremely rare ( Fraenkel &amp; Klein, 1990 ). In this particular example, the code is not an affix code, since the codeword for t is a suffix of the codeword for g .

One approach for performing direct pattern matching in encoded texts is to generate a compression method which is especially suitable for compressed pattern matching. Manber (1997) , for example, presents a static compression technique which is based on packing pairs of frequent characters in a single byte. An-other work was done by Klein and Shapira (2000) , which modifies the LZSS algorithm by moving the pointers backwards in the file, so that pointers point forwards to the reoccurring string, rather than back-wards to strings that have already occurred. The dynamic nature of this compression requires a more pow-erful pattern matching algorithm.

Compressed pattern matching in Huffman encoded text has already been studied. Klein and Shapira (2001) present a probabilistic algorithm for searching Huffman encoded texts. They use the tendency of the Huffman code to re-synchronize quickly after errors. After an occurrence of the compressed pattern in the compressed text has been detected, the search continues by jumping back in the compressed text by a fixed number of bits, and starting decompressing from there up to the point of the occurrence. If the decoding synchronizes with the occurrence, a match with high probability is announced.
Although the probability of finding wrong matches is low, in this work we present a deterministic algorithm.

Turpin and Moffat (1997) present an algorithm to directly search texts which were compressed using word-based Huffman codes, allowing only one word patterns. They construct an index of all words that occur in all files in a given directory, and apply a known pattern matching technique (such as agrep by
Wu &amp; Manber, 1992 ) on the index, in order to search for a pattern in this set of files. The index, which lists all words and their occurrences, is stored as part of the compressed file. We adapt the way the Huffman file is decoded and extend this work by allowing more than a single word pattern to be searched. Using their implementation, and our modified KMP algorithm, the file is searched directly, and more than a single bit can be handled at a time.
 Another word-based Huffman method is studied in the work of deMoura, Navarro, Ziviani, and Baeza-
Yates (2000) . They present a compression and decompression technique where arbitrary portions of the compressed text can be decompressed, without the need for decompressing the entire file. Moreover, both exact and approximate pattern matching can be done directly on the compressed text. Their compression uses a word-based model and is based on byte oriented Huffman coding rather than bit oriented. Instead of using the original binary Huffman coding, their tree s degree is either 128 in what they call a tagged Huffman code , or 256 in their plain Huffman code . Thus the atomic unit of each codeword is a byte, and traditional byte oriented algorithms can be employed for searching through the compressed text. Although, they achieve fast searching times, the disadvantage of this approach is not only the need to re-compress Huffman encoded files in order to apply their pattern matching algorithm on the encoded file, but also the compres-sion effectiveness using this method as opposed to binary Huffman coding.

The work of Takeda et al. (2002) suggests to perform compressed pattern matching over multi-byte char-acter texts using the Aho-Corasick pattern matching machine. This technique processes each bit of the en-coded file exactly once and is applicable to any prefix code including Huffman encoded texts, and is extended to cope with XML documents, too. In this paper we use the same idea of Takeda et al. which merges the pattern matching and synchronization tasks into one. However, the algorithm we present here is able to process more than a single bit in one machine operation. Moreover, the processing time for build-ing the Aho-Corasick machine takes O( m  X  j R j ), while the KMP preprocess stage takes only O( m ) processing time.
 The remainder of this paper is organized as follows. Section 2 presents our modified KMP algorithm.
Section 3 shows how we can combine the modified KMP algorithm with Moffat and Turpin s (1997) decoding technique or Klein s skeleton trees (2000) , in order to perform compressed pattern matching.
Section 4 presents experiments that compare both processing time and compression performance of our implementations against the traditional  X  X  X ecompress then search X  X , and cgrep of deMoura et al. (2000) . 2. Modifying the KMP algorithm
While searching for a given pattern in the compressed text, we first compress the pattern with the same canonical code that was used for generating the compressed text. We then use a modified Knuth X  X orris X 
Pratt algorithm (1977) to search for the compressed pattern E  X  P  X  directly in the compressed text E  X  T  X  .Note that the Boyer-Moore algorithm which searches the pattern from its right end, is not suitable for our pur-pose since one can not determine the codeword boundaries in the compressed text unless the text is decoded from left to right. Moreover, Boyer-Moore s algorithm, even with its sub-linear performance, is suitable for large alphabets rather than a binary alphabet as in Huffman encoded texts.

The basic idea behind the original KMP algorithm is that each time a mismatch is detected, we know exactly how far to back-up the pointer in the pattern since this relies only on the characters in the pattern and not in the text. Consequently, the pointer in the text is never decremented. To accomplish this, the pat-tern is preprocessed to obtain a table that gives the index in the pattern of the character to be used for the next comparison with the character that caused the mismatch in the text. We use this idea for searching for the encoded pattern in the encoded text by preprocessing the encoded pattern.

For a given pattern P = p [0.. m 1] of length m , during the preprocessing stage of the original KMP algorithm, a next [0.. m 1] table is used to determine how far to back up when a mismatch is detected.
The original preprocessing algorithm slides a copy of the first i characters of the pattern over itself, from left to right, starting with the first character of the copy over the second character of the pattern, and stopping when all overlapping characters match. These overlapping characters define the next possible position the pattern could match, if a mismatch is detected at p [ i ]. The distance to back up in the pattern ( next [ i ]) is exactly the number of overlapping characters. Conventionally, next [0] = 1, which means that there is no overlap, and one must slide the pattern all the way to its beginning. The formal definition of the original next [] table is as follows:
For our purposes we cannot apply the same algorithm to generate the KMP next table for individual bits of the compressed pattern, as the table might instruct us to perform comparisons in between codewords boundaries. These comparisons must be omitted since they would result in a false match.

This situation is illustrated in the following example and presented in Fig. 1 . Let a = 00, b = 01, c = 100, d = 111 and e = 110, and let P = bace , and assume that the encoded pattern is aligned on some codeword boundary. The original KMP table assigns next [8] = 5, i.e., if a mismatch at the bit indexed 8 is detected, the next bit to be processed is the one indexed five. Even though there might be a match of E  X  P  X  at the shifted position of E  X  T  X  , this shifting will result in a mismatch, since the first bit of E  X  P  X  will be aligned on the second bit of the character a and thus not aligned on the first codeword boundary of E  X  T  X  .
Another problematic solution is using the next table generated by the original KMP on the characters of the uncompressed pattern, and applying it to the underlying bits. The following example illustrates the drawback of this solution. Assume the same codewords as in the previous example, and let P = bacbaebad . Then E  X  P  X  X  01-00-100-01-00-110-01-00-111. Suppose there is a mismatch when we are located on the last bit of E  X  P  X  . The original KMP table generated from the characters, will direct us to the character c indexed 2, while we already know that no occurrence occurs at the current position, since the last three bits read from E  X  T  X  are 110 and the codeword of c is 100. In this example one can slide the pattern all the way so that the current position is aligned on the first bit of E  X  T  X  . Note that shifting the pattern as instructed in the original KMP table, will force us to recheck bits.

The KMP next table for the encoded pattern, next _ bit  X  0.. j E  X  P  X j 1 , therefore must be such that each bit of the encoded text is processed exactly once while at the same time keeping codeword boundaries aligned.
Preprocessing the compressed pattern can then be done in the following way. For each bit in the com-pressed pattern look for the longest prefix that matches the current suffix, so that they are both aligned on the first codeword boundary (and therefore on all codeword boundaries). Also, since we are dealing with the binary alphabet, we can improve on this algorithm by taking into account the bit that caused the mis-match. If the pattern s bit that caused the mismatch is a 1, one can use this information and slide it to a point where the corresponding bit in the encoded pattern is a 0.

Given an encoded pattern E  X  P  X  of length m and an encoded text E  X  T  X  of length n , we consider an attempt to match the encoded pattern with the encoded text at position i , that is, the encoded pattern E  X  P  X  is aligned with the sub-pattern E  X  T  X  X  i .. i  X  m 1 of the encoded text. Assume that the first mismatch occurs between bits E  X  P  X  X  j and E  X  T  X  X  i  X  j for some 0 6 j &lt; j E  X  P  X j , as shown in Fig. 2 . Then E  X  P  X  X  0.. j 1  X  some suffix u of E  X  P  X  X  0.. j 1 , and they are both aligned on codeword boundaries. In addition, the bit fol-lowing v is equal to a  X  1 a , where a is the bit following u .

We define a mapping I between indices of the characters of the pattern and indices of the bits of the en-corresponding the i th codeword of E  X  P  X  . More formally, I  X  i  X  X  the codewords and pattern of the last example, and let us refer to the character c with index 2 (starting from 0), then I (2) = 6, since the index of the last bit of c is 6.

In order to compute the next _ bit table, we match the pattern against itself, as in the original KMP algo-rithm, but at the same time we also handle the codeword boundaries. The algorithm is presented in Fig. 3 .
All entries of the first codeword are set to 1, since a mismatch in the first codeword enforces a mismatch of the entire pattern. In order to slide the pattern against itself, we use two indices i and j .If j is equal to 1 and i is not pointing to the last bit in a codeword, we advance i to the last bit of the current codeword, while setting each entry to 1. Otherwise, we are pointing on a codeword boundary, and we can use the original logic of the KMP algorithm to compute the entries for the next _ bit table, based on the already computed ones.

The algorithm for searching Huffman encoded texts is given in Fig. 4 . Each bit in the encoded file is pro-cessed only once, using the modified KMP algorithm. Unlike the original KMP algorithm which uses a while statement in the following code: here an if statement suffices, since one can take advantage of the fact that the alphabet is binary, i.e., if a mismatch is detected, and one skips to some location pointed by the next_bit table, then the bits in the new location match. Note that using the original KMP algorithm on the characters (symbols of the uncom-pressed text), effectively decompresses the file, which is something we wanted to avoid. 3. Fast pattern matching on Huffman texts
The number of Huffman trees for a given probability distribution is quite large. Many applications pre-fer to use the data structure defined by Schwartz and Kallick (1964) known as a canonical tree . A tree is called canonical if when scanning its leaves from left to right, they appear in non-decreasing order of their depth. An equivalent way for defining it is that when the codewords are sorted by the frequency of their corresponding symbols, they are ordered lexicographically. When using canonical trees, decoding can be done in a more efficient manner, requiring less memory space and fewer bitwise operations. Moffat and
Turpin (1997) and Klein (2000) use the canonical codes for fast decoding of Huffman texts, so that more than a single bit can be processed in one machine operation. In this section we show how to combine the modified KMP algorithm with these methods.
 3.1. KMP with skeleton trees
In the KMP-search algorithm, Fig. 4 , we are processing each bit of the encoded text in order to locate the encoded pattern. We present the sk-KMP algorithm for searching Huffman texts, which improves it by pro-cessing more than a single bit per machine operation, using skeleton trees (2000). This data structure rep-resents canonical Huffman codes in a space efficient way and speeds up the decoding by handling more than a single bit at a time. A skeleton tree is a binary tree which is induced by the underlying Huffman tree, where all full subtrees of depth at least 1 have been pruned. That is, the nodes of the Huffman tree that remain in the skeleton tree are those up to the depth necessary to identify the length of the codeword with the prefix corresponding to the path from the root to that node. The leaves v of the skeleton tree then con-tain the length of the corresponding codewords  X  ( v ). To search for the encoded pattern in the Huffman en-coded text using a skeleton tree, we use a pointer to point to the root of the skeleton tree, a pointer to point to the first bit of the encoded text, and another pointer to point to the current index in our pattern (and, therefore, in the next _ bit table). The encoded text is scanned, while simultaneously traversing the skeleton tree and keeping track of the position j in the next _ bit table. After having read a 0 from the encoded text, we proceed to the left child of the current node in the skeleton tree, otherwise we proceed to the right. When we reach a leaf of the skeleton tree, the length of the current codeword is identified, and the  X  ( v ) remaining bits are fetched in order to complete reading the current codeword, in addition to updating the position j of the next _ bit table at the end of the current codeword.

The algorithm in Fig. 5 shows how to combine the skeleton tree with the modified KMP algorithm. We use v to point to the nodes in the skeleton tree, and j to point to bits of the encoded pattern. Each leaf, v ,of the skeleton tree, that is also on a path of a codeword of E  X  P  X  , has a leaf _ next table, with 2 the position in E  X  P  X  after reading into y , the remaining  X  bits of the codeword. The sk-KMP algorithm per-forms a single operation per node in the skeleton tree, and not per node in the Huffman tree, saving in prac-tice, about 50% of bit operations (2000).
 4. Experiments
The data files considered for the experiments were all natural language texts; these are as follows: world192.txt  X  X IA 1992 World Fact-book, bible.txt  X  X ing James Bible, books.txt  X  X andom selection of texts from the Gutenberg Archives which is an archive of over 1000 documents in the English language in computer text format ( Gutenberg Archives, 2004 ), 95-03-erp.txt  X  X S Economic Reports 2003. All experiments were performed on an Intel PC with a 900 MHz AMD Athlon CPU with 256 KB cache memory and 256 MB of main memory running RedHat Linux-7.3.

The compression parsing model used on the input files uses a word-based alphabet together with the spaceless words method presented in the work of deMoura et al. (2000) . Every word is assumed to be fol-lowed by a space, in which case only the word is encoded, otherwise if the next symbol corresponds to a separator, in which case the separator must also be encoded. For compressing the files, we used Moffat and Katajainen s algorithm (1995) to first compute the lengths of the codewords and then proceed with the canonical code construction.

To speed-up the KMP algorithm we use Moffat and Turpin s implementation of Huffman decoding (1997) who use the structure of canonical codes, so that all codewords of a given length are consecutive binary integers. Only the first codeword of each length is stored, which also gives a sorted list of integers.
A window, at least as long as the longest codeword, slides through the compressed stream, and its numer-ical value is compared against each one of these integers. The length of the next codeword is then deter-mined, and the translation of the symbols number to the output string is done by a table look-up. We combine this with our modified KMP algorithm and call it win-KMP .

We compare our algorithms, sk-KMP and win-KMP , against cgrep of deMoura et al. (2000) ,and agrep of Wu and Manber (1992) which searches the uncompressed text for patterns with or without errors. The alphabet of the cgrep , includes all words and separators of the text, and each element is assigned a sequence of bytes using only the 7 lower order bits of each byte. The most significant bit (MSB) in the first byte of each codeword is used as a separator between codewords by setting it to 1, while all other byte s MSB is set to 0. This is done so that they can make sure that a reported match is not a false match. An implementation of cgrep can be downloaded from http://roquefort.di.unipi.it/~ferrax/CompressedSearch ( Ferragina,
Tommasi, &amp; Manzini, 2004 ). This particular implementation searches for single words with or without errors.

Table 1 compares the compression performance of the original Huffman compression ( Huff ) against the compression achieved by cgrep and gzip . The compression ratio presented in this table is the size of the compressed text as a percentage of the uncompressed text.

Table 2 compares the processing time of pattern matching of these algorithms. The figures are given in seconds. The  X  X  X ecompress then search X  X  methods, first decode using skeleton trees ( sk-d ) or Moffat and Tur-pin s sliding window algorithm ( win-d ) and then perform the search using agrep .

As can be seen from these tables, using our modified KMP algorithm implemented with either skeleton trees or Moffat and Turpin s decoding, is faster than the  X  X  X ecompress then search X  X  algorithms using the corresponding methods. When comparing the processing times of the KMP variants to cgrep , cgrep is fas-ter, but its compression is less effective than Huffman. Not only should files be re-compress in order to use cgrep , but it also harms the compression. Moreover, compressed files that can fit into the main memory, might exceed the memory space using the cgrep compression. Therefore, pattern matching that could have been performed in main memory would now have to include the time spent to transfer the file from second-ary storage into main memory. 5. Conclusion
We have modified the Knuth X  X orris X  X ratt algorithm to perform compressed pattern matching in Huff-man encoded texts. Our bitwise algorithm processes each bit of the encoded text exactly once. By combin-ing it with the skeleton trees defined by Klein, or the Huffman decoding implementation presented in
Moffat and Turpin we are able to handle more than a single bit per machine operation. The processing times are better than the  X  X  X ecompress then search X  X  method and slower than cgrep . However, when com-pression performance is important or when one does not want to re-compress Huffman encoded files in order to use cgrep , the proposed algorithms are the better choice.
 References
