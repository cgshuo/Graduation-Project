 A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valu-able applications such as search, summarization, and con-tent browsing. In this paper we propose an algorithm for recursively constructing a hierarchy of topics from a collec-tion of content-representative documents. We characterize each topic in the hierarchy by an integrated ranked list of mixed-length phrases. Our mining framework is based on a phrase-centric view for clustering, extracting, and ranking topical phrases. Experiments with datasets from different domains illustrate our ability to generate hierarchies of high quality topics represented by meaningful phrases.
 I.7 [ Computing Methodologies ]: Document and Text Processing; H.2.8 [ Database Applications ]: Data Mining Topic Modeling, Ontology Learning, Network Analysis, Keyphrase Extraction, Keyphrase Ranking
A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valu-able applications in the areas of summarization, search and browsing. A student could familiarize herself with a new domain by perusing such a hierarchy and quickly learning the domain X  X  topics. Or, a researcher could discover which terminology phrases are representative of his topic of in-terest, assisting his search for relevant work done by other colleagues and potentially discovering subtopics to focus on. We are therefore motivated to create a robust framework for constructing high quality topical hierarchies from texts in different domains.

For this task, we work with datasets of short texts -in particular, content-representative documents. A document is content-representative if it may serve as a concise descrip-tion of its accompanying full document. For example, the title of a scientific paper is usually a content-representative document, because it is a good representation of the topics found in the paper itself. However, the same is rarely true of e.g. fiction books. The terms in a content-representative document (the title) can therefore be thought of as proba-bilistic priors for which terms are the most likely to generate phrases representative of the full document (the paper). Our goal is to represent the topics of a collection, so content-representative documents are cleaner, simpler to use, and more likely to be available than full documents, while yield-ing the desired result.

Our framework therefore aims to construct a hierarchy where each topic is represented by a ranked list of topical phrases, such that a child topic is a subset of its parent topic. For example, the topic of query processing and optimization may be described by the phrases {  X  X uery processing X ,  X  X uery optimization X , ... } , while its parent topic of general prob-lems in databases may be described by {  X  X uery processing X ,  X  X atabase systems X ,  X  X oncurrency control X , ... }
Our goal has several challenges. Topical phrases that would be regarded as high quality by human users are likely to vary in length (e.g.,  X  X upport vector machines X  and  X  X ea-ture selection X  would both be good phrases for a topic about machine learning). Existing phrase extraction and rank-ing approaches are term-centric and cannot directly com-pare such mixed-length phrases, highlighting the need for a phrase-centric approach. Globally frequent phrases are not assured to be good representations for individual topics, demonstrating the need to infer the frequency of phrases in each topic. Finally, we must be able to recursively estimate each phrase X  X  topical frequency for subtopics, in order to construct a topical hierarchy.
 In this work we present CATHY (Constructing A Topical HierarchY), a phrase-centric framework for topical hierarchy generation via recursive clustering and ranking. The main features of our framework are as follows:  X  Phrase-centric approach : We employ topic analysis and frequent pattern mining to estimate the topical frequency for phrases. By using a phrase-centric topical frequency mea-sure instead of a unigram-centric measure, we are able to mine and rank higher quality phrases for each topic.  X  Ranking of topical phrases : We define a topical keyphrase ranking function which implements the four criteria that in-tuitively represent high quality topical phrases: coverage, purity, phraseness, and completeness. Because the input to our ranking function is phrases and their topical frequencies, instead of unigrams that must somehow be combined, we can directly compare phrases of different lengths and yield an integrated ranking of mixed-length phrases.  X  Recursive clustering for hierarchy construction : Our topic inference is based on term co-occurrence network clustering. For any topic, we can extract its representative subnetwork, and recursively apply CATHY to discover subtopics.
Traditionally, a phrase is defined as a consecutive sequence of terms, or unigrams. However, as discussed in [16] this def-inition can be quite limiting as it is too sensitive to natural variations in the term order, or the morphological struc-ture of a phrase. For instance, consider that two computer science paper titles, one containing  X  X ining frequent pat-terns X  and the other containing  X  X requent pattern mining, X  are clearly discussing the same topic, and should be treated as such. A phrase may also be separated by other terms:  X  mining top-k frequent closed patterns  X  also belongs to the topic of frequent pattern mining, in addition to incor-porating secondary topics of top-k frequent patterns, and closed patterns. Therefore, we define a phrase to be an order-free set of terms appearing in the same document. Our framework can work with alternative definition of phrases as well, such as traditonally defined consecutive ngrams.
Definition 1 (Phrase). A phrase P with length n is an unordered set of n terms: P = { w x 1 ,...,w x n | w W } , where W is the set of all unique terms in a content-representative document collection. The frequency f ( P ) of a phrase is the number of documents in the collection that contain all of the n terms.

We use phrases as the basic units for constructing a topical hierarchy.

Definition 2 (Topical Hierarchy). A topical hier-archy is defined as a tree T in which each node is a topic. The root topic is denoted as o . Every non-root topic t with parent topic par ( t ) is represented by a ranked list of phrases {P t ,r t ( P t ) } , where P t is the set of phrases for topic t , and r ( P t ) is the ranking score for the phrases in topic t . For every non-leaf topic t in the tree, all of its subtopics com-prise its children set C t = { z  X  T ,par ( z ) = t } . A phrase can appear in multiple topics, though it will have a different ranking score in each topic.

To construct a topical hierarchy, we must soft cluster phrases into a hierarchy and find representative phrases for each topic. As an example, consider the task of judging what constitutes high quality phrases for various topics in computer science. There are four criteria for judging the quality of a phrase:  X  Coverage: A representative phrase for a topic should cover many documents within that topic. Example:  X  X n-formation retrieval X  has better coverage than  X  X ross-language information retrieval X  in the Information Retrieval topic.  X  Purity: A phrase is pure in a topic if it is only frequent in documents belonging to that topic and not frequent in documents within other topics. Example:  X  X uery processing X  is more pure than  X  X uery X  in the Databases topic.  X  Phraseness: A group of terms should be combined to-gether as a phrase if they co-occur significantly more of-ten than the expected chance co-occurrence frequency, given that each term in the phrase occurs independently. Example:  X  X ctive learning X  is a better phrase than  X  X earning classifica-tion X  in the Machine Learning topic.  X  Completeness: A phrase is not complete if it is a subset of a longer phrase, in the sense that it rarely occurs in a document without the presence of the longer phrase. Exam-ple:  X  X upport vector machines X  is a complete phrase, whereas  X  X ector machines X  is not because  X  X ector machines X  is almost always accompanied by  X  X upport X  in documents.

The measures which represent these criteria can all be characterized by an important concept: topical frequency.
Definition 3 (Topical Frequency). The topical fre-quency f t ( P ) of a phrase is the count of the number of times the phrase is attributed to topic t . For the root node o , f ( P ) = f ( P ) . For each topic node in the hierarchy, with quency is equal to the sum of the sub-topical frequencies.
Table 1 illustrates an example of estimating topical fre-quency for phrases in a computer science topic that has 4 subtopics. The phrase  X  X upport vector machines X  is es-timated to belong entirely to the Machine Learning (ML) topic with high frequency, and therefore is a candidate for a high quality phrase. However,  X  X ocial networks X  is fairly evenly distributed among three topics, and is thus less likely to be a high quality phrase. Section 3.3 discusses how such candidate phrases are actually ranked, using measures based on estimated topical frequency.
 Table 1: Example of estimating topical frequency. The topics
In order to estimate the topical frequency for each phrase, we need to infer the dataset X  X  topics. We perform topic inference and estimate topical frequency by analyzing our dataset X  X  term co-occurrence network.

Formally, every topic node t in the topical hierarchy is associated with a term co-occurrence network G t . The root node o is associated with the term co-occurrence network G o constructed from the collection of content-representative documents. G o consists of a set of nodes W and a set of links E . A node w i  X  W represents a term, and a link ( w i ,w j ) between two nodes represents a co-occurrence of the two terms in a document. The number of links e ij  X  E between two nodes w i and w j is equal to the number of documents containing both terms. For every non-root node t 6 = o , we construct a subnetwork G t by clustering the term co-occurrence network of its parent par ( t ). G t has all of the nodes from G par ( t ) , but only those links belonging to the particular subtopic t .

We chose to use term co-occurrence network for topic anal-ysis instead of document-term topic modeling because the it naturally supports recursive mining: the clustering result for one topic can be used as the input when further parti-tioning the topic into subtopics. The CATHY framework generates a topical hierarchy in a top-down, recursive way:
Step 1 . Construct the term co-occurrence network G o = ( W,E ) from the document collection. Set t = o .

Step 2 . For a topic t , cluster the term co-occurrence net-work G t into subtopic subnetworks G z ,z  X  C t , and estimate the subtopical frequency for its subtopical phrases using a generative model.

Step 3 . For each topic z  X  C t , extract candidate phrases based on estimated topical frequency.

Step 4 . For each topic z  X  C t , rank the topical phrases using a unified ranking function based on topical frequency. Phrases of different lengths are directly compared, yielding an integrated ranking.

Step 5 . Recursively apply Steps 2 -5 to each subtopic z  X  C t to construct the hierarchy in a top-down fashion.
We first introduce the process of clustering for one topic t . We assume C t contains k child topics, denoted by z = 1 ...k . The value of k can be either specified by users or chosen using a model selection criterion such as the Bayesian Information Criterion [27].

In the term co-occurrence network G t , we assume every co-occurrence of two terms w i and w j is attributed to a topic z  X  C t = { 1 ,...,k } . We represent the total link frequency e i,j between w i and w j as a summation of the number of links between w i and w j in each of the k topics: e ij P which is unlike most network analysis approaches.

We develop a generative model of the term co-occurrence network, and use it to estimate topical frequency f z ,z  X  C
To generate a topic-z link, we first generate one end node w i following a multinomial distribution p ( w i | z ) =  X  then generate the other end node w j with the same multino-mial distribution p ( w j | z ) =  X  z j . The probability of generat-ing a topic-z link ( w i ,w j ) is therefore p ( w i | z ) p ( w
With this generative assumption for each individual link, we can derive the distribution of topical frequency for any two terms ( w i ,w j ). If we repeat the generation of topic-z links for  X  z iterations, then the chance of generating a particular topic-z link between w i and w j can be modeled as a Bernoulli trial with success probability  X  z i  X  z is large, the total number of successes e z ij approximately follows a Poisson distribution Pois (  X  z  X  z i  X  z j ).
Now we can write down the generative model for random variables e z ij with parameters  X  z , X  z . The constraints guarantee a probabilistic interpretation. Ac-cording to the expectation property of the Poisson distri-property of expectations, In other words,  X  z is the total expected number of links in topic z .

One important implication due to the additive property of Poisson distribution is that So given the model parameters, the probability of all ob-served links is
In this model, the observed information is the total num-ber of links between every pair of nodes, including zero links and self-links. The parameters which must be learned are the role of each node in each topic  X  z i ,w i  X  W,z = 1 ,...,k , and the expected number of links in each topic  X  z total number of free parameters to learn is therefore k | W | . We learn the parameters by the Maximum Likelihood (ML) principle: find the parameter values that maximize the likeli-hood in Eq. (4). We use an Expectation-Maximization (EM) algorithm that can iteratively infer the model parameters:
Intuitively, the E-step calculates the expected number of links  X  e z ij in each topic z between the terms w i and w ratio of  X  e z ij to e ij is proportional to its Poisson parameter  X   X  z i  X  z j . The M-step calculates the ML parameter estimates:  X  i is the ratio of the total number of links in topic z where one end node is w i and  X  z , which is the sum of the total expected number of links in topic z .

We update  X  e z ij , X  z i , X  z in each iteration. Note that if e E , we do not need to calculate  X  e z ij because it equals 0. Therefore, the time complexity for each iteration is O (( | E | + | V | ) k ) = O ( | E | k ). Like other EM algorithms, the solution converges to a local maximum and the result may vary with different initializations. The EM algorithm may be run mul-tiple times with random initializations to find the solution with the best likelihood. We empirically find that the EM algorithm generally requires hundreds of iterations to con-verge, although we can improve the efficiency with some acceleration tricks. For example, we do not need to update a parameter in each iteration if it converges before the whole model converges. Similar tricks are used in other generative models such as [2], and we omit the details here.

It is important to note that our method naturally sup-ports top-down hierarchical clustering. To further discover subtopics of a topic, we can extract the subnetwork where E z = {  X  e z ij |  X  e z ij  X  1 } (expected number of links attributed to that topic, ignoring values less than 1) and then apply the same generative model on the subnetwork. This pro-cess can be recursively repeated until the desired hierarchy is constructed.
Using the learned model parameters, we can estimate the topical frequency for a phrase P = { w x 1 ...w x n } : This estimation is based on two assumptions: i) when gen-erating a topic-z phrase of length n , each of the n terms is generated with the multinomial distribution  X  z , and ii) the total number of topic-z phrases of length n is proportional to  X  z . It is easy to see that when n = 2, f z ( { w i ,w to  X  e z ij .
Since we define phrases to be sets of frequent terms, we de-velop an algorithm to mine frequent topical patterns. The goal is to extract patterns with topical frequency larger than some threshold minsup for every topic z . In contrast to tra-ditional frequent pattern mining problem, the topical fre-quency of each pattern is unknown and must be estimated. The results from the clustering step in Section 3.1 are nec-essary for our estimation.

To extract topical frequent patterns, one can first mine all frequent patterns with a traditional pattern mining algo-rithm such as Apriori [1] or FP-growth [15], and then filter them using the topical frequency estimation using Eq. (8). The following two properties of topical frequency can be fur-ther exploited to speed up this step:
Property 1. A phrase X  X  topic-z frequency has an upper bound of the topic-z frequency of any of its subphrases.
Property 2. A phrase X  X  topic-z frequency has an upper any subphrase of P .

Note that for only the top level topics z  X  C o , the parent topical frequency f par ( z ) ( P ) is equal to f ( P ) and must be counted from the text. However, for all lower levels, the parent topical frequency f par ( z ) ( P ) was already calculated when the parent topic was generated, and therefore never needs to be counted.

One problem with the extracted frequent term sets is that every subset of a frequent phrase is also a frequent phrase. However, some of these subphrases should be removed ac-cording to the completeness criterion described in Section 2 ( e.g. ,  X  X ector machines X ). To remove incomplete phrases, we adapt the notions of  X  X losed patterns X  and  X  X aximal pat-terns X  [14]. A maximal pattern has no frequent supersets, and a closed pattern has no supersets with the same fre-quency. For our task, retaining only maximal phrases is too aggressive since a long frequent phrase will override all of its subphrases. However, retaining all closed phrases only removes a phrase if it has a superphrase with exactly the same frequency, which is rare.

We therefore try to find a middle ground by unifying the definitions of maximal patterns and closed patterns together with a tunable parameter. For each topic, we remove a phrase P if there exists a frequent phrase P 0 , such that P  X  P 0 ,f z ( P 0 )  X   X f z ( P ). The remaining patterns are re-ferred to as  X  -maximal patterns (0  X   X   X  1). When  X  = 1, this is equivalent to a closed pattern, and when  X  = 0, this is a maximal pattern. We empirically set  X   X  0 . 5, which re-moves a phrase if its topical frequency is no more than twice of some superphrase. In other words, if a phrase co-occurs with a superphrase more than half the time, we consider that it is subsumed by the superphrase, and should be re-moved. According to Property 1, pruning can be performed by comparing the frequency of a length-n phrase with all of its length-( n + 1) superphrases. The collection of all of the  X  -maximal phrases of a topic z forms the candidate phrase set P z .
As discussed in Section 2, topical phrases in P z are ranked according to four criteria: coverage, purity, phraseness, and completeness. The last criterion is already employed as a filter for the phrase extraction step, parameterized by  X  . So we now combine the remaining three criteria into a ranking function using a probabilistic modeling approach.

The ranking function should be able to directly compare keyphrases of mixed lengths, which we refer to as having the comparability property . For example, the keyphrases  X  X lassification, X   X  X ecision trees, X  and  X  X upport vector machines X  should all be ranked highly in the integrated list of keyphrases for the Machine Learning topic, in spite of varying in length. Traditional probabilistic modeling approaches, such as lan-guage models or topic models do not exhibit the compara-bility property. These approaches simply find that longer n-grams have a much lower probability than shorter ones, because the probabilities of seeing every possible unigram sum to 1, and so do the probabilities of seeing every possible bigram, trigram, etc. However, the total number of possi-ble n-grams grows following a power law ( O ( | V | n )). While previous work has used various heuristics to correct for this bias during post-processing steps by, for example, using a penalization term with respect to the phrase length [29, 34], our approach is cleaner and more principled.

We propose a different ranking model that exhibits the comparability property. The key idea is to consider the oc-currence probability of  X  X eeing a phrase p in a random docu-ment with topic t . X  With this definition, the events of seeing n-grams of various lengths in a document are no longer mu-tually exclusive, and therefore the probabilities no longer need to sum to 1.

We construct estimations for occurrence probability and two contrastive probabilities that will be used to compare against the occurrence probablity. We use m z to denote the number of documents that contain at least one frequent topic-z phrase. Similarly, we use m Z to denote the num-ber of documents that contain at least one frequent topic-z phrase for some topic z  X  Z . We can then calculate the occurrence probability of a phrase P conditioned on topic z : The independent contrastive probability is the probability of independently seeing every term in phrase P = { w x 1 ,...w conditioned on topic z : and the mixture contrastive probability is the probability of a phrase P conditioned on a mixture of multiple sibling topics
We can now define the three remaining ranking crite-ria: coverage, purity, and phraseness. The coverage of a phrase is directly quantified by p ( P | z ). The phraseness can be measured by the log ratio of the occurrence probabil-ity to the independent contrastive probability log p ( P | z ) The purity can be measured by the log ratio of the occur-rence probability and the mixture contrastive probability tering the makeup of the topic mixture Z . For example, using the mixture of all the sibling topics C par ( z ) topic mixture results in a weaker purity criterion. However, deliberately choosing the subset Z so that the contrastive probability p ( P | Z ) is maximized, results in a stronger pu-rity criterion.

The three criteria are unified by the ranking function: r z ( P ) = p ( P | z ) log p ( P | z ) where  X  controls the importance of the phraseness criterion. This formulation of the ranking function has several desir-able characteristics:  X  The coverage measure p ( P | z ) is the most influential, since the other two criteria are represented by log ratios of p ( P | z ) and a contrastive probability, and the effect of contrastive probability on the ranking score is smaller than the influence of p ( P | z ). This is a desirable property because when a phrase P has low support, the estimates of purity and phraseness are unreliable; but their effect is small since the value of p ( P | z ) would be correspondingly low. Therefore, a phrase with low coverage would inevitably be ranked low, as should be the case for representative phrases.  X  The relative importance of the purity and phraseness measures is controlled by  X  . Both measures are log ra-tios on comparable scales, and can thus be balanced by weighted summation. As  X  increases, we expect more topic-independent but common phrases to be ranked higher. We therefore restrict  X   X  [0 , 1] because our task requires topic-related phrases to be highly ranked.  X  The ranking function can also be nicely represented as a pointwise Kullback-Leibler (KL) divergence in an infor-mation theoretic framework. Pointwise KL divergence is a distance measure between two probabilities. It is more ro-bust than pointwise mutual information because the former also considers absolute probability. In pointwise KL diver-gence, the relative difference between probabilities must be supported by a sufficiently high absolute probability. The divergence between the probabilities of p ( P | z ) and p ( P | Z ). KL divergence between the probabilities of p ( P | z ) under dif-ferent independence assumptions. Therefore, Eq. (12) can also be interpreted as a weighted summation of two point-wise KL divergence metrics.
With respect to the goal of our framework, our work is broadly related to ontology learning. Topical hierarchies, concept hierarchies, ontologies, etc. , provide a hierarchical organization of data at different levels of granularity, and have many important applications, e.g. , in web search and browsing [11]. There has been a substantial amount of re-search on ontology learning from text, though it remains a challenging problem (see [32] for a recent survey). The techniques can be broadly categorized as statistics-based or linguistic-based. Most studies aim to mine subsumption ( X  X s-a X ) relationships [17], either by using lexico-syntactic patterns ( e.g. ,  X  X  is a y X ) [28, 25] or statistics-based ap-proaches [33, 9]. Our definition of a topical hierarchy is clearly distinct from a subsumption hierarchy. Chuang and Chien [7] and Liu et al. [20] generate taxonomy of given key-word phrases by hierarchical clustering techniques, with the help of knowledge bases and search engine. If our work were to be broadly viewed as an ontology-learning approach, we use statistics-based techniques, without resorting to external knowledge resources such as WordNet or Wikipedia.
The nature of our technique is related to topical keyphrase extraction and ranking. Keyphrases are traditionally ex-tracted as ngrams using statistical modeling [31], or as noun phrases using natural language processing techniques [3]. We mainly review the related work in extracting topical keyphrases from document collections rather than keyphrase extraction from single documents. The state-of-the-art ap-proaches to unsupervised keyphrase extraction have gen-erally been graph-based, unigram-centric ranking methods, which first extract unigrams and rank them for each topic, and finally combine them into keyphrases [21, 34]. Some previous methods have used clustering techniques on word graphs with the help of external knowledge bases such as Wikipedia for keyphrase extraction [13]. Tomokiyo and Hurst [29] require a document collection with known topics as in-put and train a language model to define their ranking cri-teria. Mei et al. [22] use keyphrase extraction techniques to discover labels for topics.

In contrast to all of these studies, we relax the restriction that a phrase must be a consecutive n-gram, and instead use document colocation -which is effective due to the na-ture of the short, content-representative document collec-tions which our framework expects. We also do not employ any NLP techniques to parse the text of our datasets.
Our study is also related to topic modeling. Topic mod-eling techniques such as Latent Dirichlet Allocation [5] take documents as input, model them as mixtures of different topics, and output word distributions for each topic. Some extensions have been developed to discover topical phrases comprised of consecutive words [30, 31, 19, 4]. They can-not find hierarchical topics, and their definition of phrases is more restrictive. Several other extensions can model the hi-erarchical dependency of unigram-based topics [12, 18, 24]. It is challenging to apply these techniques to our scenario be-cause: i) since our text is sparse, the distribution estimates are quite brittle [10] when calculating multiple topic levels, and ii) these methods compute the entire hierarchy simulta-neously and do not support recursive discovery of subtopics from a topic.
In this section we first introduce the datasets and meth-ods we used for comparison. We then describe our 3-part evaluation: i) we conduct a user study with  X  X ntruder detec-tion X  tasks to evaluate hierarchy quality; ii) we use category-labeled data to evaluate the mutual information between phrase-represented topics and known topical divisions; and iii) we present several case studies.
We analyze our performance on two datasets: 1  X  DBLP . We collected a set of titles of recently published computer science papers in the areas related to Databases, Data Mining, Information Retrieval, Machine Learning, and Natural Language Processing. These titles come from DBLP 2 a bibliography website for computer science publications. We minimally pre-processed the dataset by removing all stopwords from the titles, resulting in a collection of 33,313 titles consisting of 18,598 unique terms.  X  Library. We obtain titles of books from the University of Illinois Library catalogue database in 6 general categories: Architecture, Literature, Mass Media, Motion Pictures, Mu-sic, and Theater. We pre-processed the titles by removing all stopwords and terms with frequency &lt; 5 in the dataset. We also remove titles over 10 words in length, and titles not in English. The resulting dataset contains 33,372 titles con-sisting of 3,556 unique terms.
As the topical hierarchy construction problem setting that we study is new, there are no directly comparable algo-rithms. We implement several methods:
SpecClus : As one baseline, we implement a common framework of clustering-based ontology construction, which first extracts all concepts from the text and then hierarchi-cally clusters them. We adapt this to our setting by first mining all frequent phrases using FP-growth [15], a typi-cal pattern mining algorithm. We then implement spectral clustering [26] for the clustering step, where the similarity metric between two phrases is their co-occurrence count in the dataset. This approach uses K-means to perform hard clustering after computing a spectral embedding of the sim-ilarity graph. Finally, we rank phrases in each cluster based on their distance from the cluster center. In order to go down in the hierarchy we recursively perform the same clustering and ranking on each cluster of phrases. hPAM : As a second baseline, we use a state-of-the-art hi-erarchical topic modeling approach: the hierarchical Pachinko Allocation Model [24]. hPAM takes documents as input and outputs a specified number of supertopics and subtopics, as well as the associations between them. However, it builds a hierarchy for 3 levels simultaneously, not recursively, so we only generate a hierarchy with 3 levels. 3 hPAM rr : For each topic, hPAM outputs a multinomial distribution over unigrams. These distributions can be used to calculate the coverage and purity measures in our ranking function (phraseness and completeness do not matter when all candidate phrases are unigrams). We therefore also im-plement a method that reranks the unigrams in each topic generated by hPAM, with our ranking function adopting the distribution learned by hPAM. We refer to the result as hPAM rerank , or hPAM rr . Note that we cannot rerank Spec-Clus because it does not generate a probability distribution that can be input into our ranking function.

CATHY cp : In this version of CATHY the ranking func-tion only considers the coverage and purity criteria, and not phraseness or completeness (  X  =1,  X  =0). This allows us to more closely compare the performance of our clustering and mining step with hPAM, using hPAM rr .

CATHY : For evaluation we set minsup =5,  X  =  X  =0 . 5 for phraseness and completeness criteria, and we use the strong definition of purity, as discussed in Section 3.3 (Refer to [8] for more thorough studies of the effects of the ranking function X  X  parameters.)
Our first evaluation assesses the ability of our method to construct topical phrases that appear to be high quality to human judges, via a user study. We construct hierarchies with 4 levels from the DBLP dataset. For simplicity we set the number of subtopics for the root node to be 5, for all other non-leaves to be 4, for all of the methods. Since hPAM and hPAM rr only construct 3 levels of the hierarchy, we compare the 3-level hierarchies across all methods, and the full hierarchies for the 3 methods which constructed them.
In the following subsections, we present a sample of the hierarchies actually generated by these methods and encoun-tered by participants in the user study. We then explain the details of our user study, and present quantitative results. Figure 1 shows a subset of hierarchies constructed by CATHY and the two baselines, SpecClus and hPAM. In gen-eral, CATHY constructs high quality phrases, representing the areas and subareas on both levels. hPAM outputs un-igrams that are fair at conveying the top-level topics when considered jointly, but independently are topic-ambiguous (e.g.,  X  X ervices X  for IR). hPAM X  X  second level subtopics are generally more difficult to interpret, and some parent-child relationships are not clearly observed. SpecClus tends to generate phrases with good purity but unsatisfactory cover-age and phraseness (e.g.,  X  X uerying spatial X  for DB).
To quantitatively measure topical phrase quality, we in-vited people to judge the topical phrases generated by the different methods. Since the DBLP dataset generates topics in computer science, we recruited 9 computer science grad-uate students -who could thus be considered to be very knowledgeable judges -for a user study. We first describe the two tasks administered in the user study, and then dis-cuss the obtained results.

In order to evaluate the quality of the generated topical phrases, we adapt two tasks from Chang et al. [6], who were the first to explore human evaluation of topic models. Our first task is Topic Intrusion, which tests the quality of the parent-child relationships in the generated hierarchies. Our second task is Phrase Intrusion, which evaluates how well the hierarchies are able to separate phrases in different topics. Both tasks are depicted in Figure 2.
 Figure 2: Examples of user study questions. In the Topic Intru-
Topic Intrusion Task : Participants are shown a parent topic t and T candidate child topics. T  X  1 of the child topics are actual children of t in the generated hierarchy, and the remaining child topic is not. Each topic is represented by its top 5 ranked phrases. Participants are asked to select the intruder child topic, or to indicate that they are unable to make a choice.
 Phrase Intrusion Task : Participants are shown T phrases. T  X  1 of the phrases come from the same topic and the re-maining phrase is from a sibling topic. Each phrase is a top-5 ranked phrase in the topic which it represents. Partic-ipants are asked to select the intruder phrase, or to indicate that they are unable to make a choice.

For the user study we set T = 4, and asked participants 80 Topic Intrusion questions and 130 Phrase Intrusion ques-tions. Questions are generated from the hierarchies con-structed by each of the methods. We sample questions from each hierarchy in a uniform way, drawing equally from all topics in each level.

We then calculate the agreement of the user choices with the actual hierarchical structure constructed by the various methods. We consider a higher match between a given hier-archy and user judgment to imply a higher quality hierarchy. For each method, we report the average percent of questions answered  X  X orrectly X  (matching the method), as well as the average percent of questions that users were able to answer.
Since the hPAM and hPAM rr hierarchies had one fewer level than other methods, we present two analyses. Table 2 presents the results from the full set of questions, except the questions generated by hPAM and hPAM rr (4 Levels), and the results from only those questions taken from the shared levels of every method X  X  hierarchies (3 Levels). Table 2: User study results, for 3 level and 4 level hierarchies.
For the Topic Intrusion task, CATHY outperforms all non-CATHY methods by a large margin. CATHY does slightly better than CATHY cp in the 3 level hierarchy, and is significantly better in the 4 level hierarchy, suggesting that participants found the phraseness and completeness crite-ria to be helpful. SpecClus slightly outperforms hPAM, because hPAM generates broad unigrams with good cov-erage, which makes the parent-child relationships difficult to identify, while SpecClus yields phrases with better purity which, when considered jointly, represent a topic more suc-cessfully. Participants answered more questions generated by the hPAM variations than by SpecClus, which, combined with the resulting accuracies, suggests that hPAM generated the least well-separated hierarchy (even with reranking).
CATHY and CATHY cp outperform other methods com-parably in the Phrase Intrusion task. As hPAM favors high coverage phrases which are often topic-ambiguous, it posted a low performance. hPAM rr considers phrase purity as well as topical coverage, and therefore performs much better on this task. SpecClus favors purity, and thus is more likely to generate seemingly unrelated high ranked phrases in the same topic, which is reflected here by its poor performance. Once again, participants were more likely to answer ques-tions generated by the CATHY variations than by any of the other three methods.
In this section, we work with the Library dataset. Since the book titles are labeled with their subjects, we examine how well a high quality topical phrase can predict its cat-egory, and vice versa. For this, we construct a hierarchy and measure the coverage-conscious mutual information at K ( CCMI K ) of the labels with the top level branches. Our evaluation is based on [24] but we modify their definition of mutual information to also depend on coverage because we represent topics with phrases.

As we saw in Section 5.3 that CATHY generally performs equal to or better than CATHY cp , and hPAM rr similarly outperforms hPAM, we simply compare the performances of CATHY, hPAM rr , and SpecClus, with 6 topics ( k = 6). For each method, we do multiple runs for various values of K (the number of top-ranked phrases per topic considered). To calculate CCMI K , we label each of the top K phrases per topic with the topic in which it is ranked highest. We then check if each title contains any of these top phrases. If so, we update the number of events  X  X eeing a topic t and category c X  for t = 1 ...k , with the averaged count for all those labeled phrases in the title; otherwise we update the number of events  X  X eeing a topic t and category c X  for t = 1 ...k uniformly, where c is the category label for the title. Finally, we compute coverage-conscious mutual information at K: Figure 3 shows CCMI K for each method, K  X  [1 , 100]. Since CCMI K considers the coverage of a phrase as well as its mutual information with a category, its value gener-ally grows with K . Both CATHY and SpecClus demonstrate this by slowly improving over time, although CATHY is con-sistently much better at differentiating the categories as K increases. hPAM rr prefers unigrams with high coverage, and thus hits an asymptote almost immediately because it is un-able to improve on the performance of the first few phrases. CCMI
Term co-occurrence can be defined in many ways: two term may be said to co-occur if they appear in the same sentence, same paragraph, or in a window of N unigrams of each other [23]. Because we worked with collections of short texts, we consistently defined term co-occurrence for our framework to mean co-occurring in the same document. However, most traditional methods of keyphrase extraction only consider phrases to be sequences of terms which ex-plicitly occur in the text. We ran a variation of CATHY which emulates this behavior by defining two terms to co-occur only if they are actually adjacent in the same title, and constructed a hierarchy on the DBLP dataset.

Using adjacency co-occurrence results in a sparser network and lowers the estimated phrase topical frequencies at every level. As can be seen in Figure 4, we observe lower quality phrases in the adjacency-based hierarchy (e.g., the topics which are supposed to be represented by the two rightmost children are very difficult to identify.)
In this work, we address the problem of constructing a topical hierarchy from short, content-representative texts, where topics are represented by ranked lists of phrases. We design a novel phrase mining framework to cluster, extract Figure 4: A level 3 topic and its level 4 subtopics, from hier-and rank phrases which recursively discovers specific topics from more general ones, thus constructing a top-down topi-cal hierarchy. A key aspect of our approach involves shifting from a unigram-centric to a phrase-centric view in order to consistently generate high caliber topics over multiple levels. By evaluating our approach on two datasets from different domains, we validate our ability to generate high quality, human-interpretable topic hierarchies.

We would like to extend our framework to incorporate su-pervised knowledge, either from user guidance or external knowledge bases. We would also like to explore integrat-ing advanced text mining and natural language processing techniques, which would help us work with longer texts.
This work was supported in part by the U.S. National Sci-ence Foundation grants IIS X 0905215, U.S. Army Research Laboratory under Cooperative Agreement No. W911NF X  09 X 2 X 0053 (NS-CTA). Chi Wang was supported by a Mi-crosoft Research PhD Fellowship. Marina Danilevsky was supported by a National Science Foundation Graduate Re-search Fellowship grant NSF DGE 07-15088. The authors wish to acknowledge the University of Illinois at Urbana-Champaign Library ( http://www.library.illinois. edu ), which provided support for this research.
