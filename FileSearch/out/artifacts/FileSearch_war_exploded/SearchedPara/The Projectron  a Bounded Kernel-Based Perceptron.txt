 Francesco Orabona forabona@idiap.ch Joseph Keshet jkeshet@idiap.ch Barbara Caputo bcaputo@idiap.ch One of the most important aspects of online learning methods is their ability to work in an open-ended fash-ion. Autonomous agents, for example, need to learn continuously from their surroundings, to adapt to the environment and maintain satisfactory performances. A recent stream of work on artificial cognitive systems have signaled the need for life-long learning methods and the promise of discriminative classifiers for this task (Orabona et al., 2007, and references therein). Kernel-based discriminative online algorithms have been shown to perform very well on binary classifica-tion problems (see for example (Kivinen et al., 2004; Crammer et al., 2006)). Most of them can be seen as belonging to the Perceptron algorithm family. They construct their classification function incrementally, keeping a subset of the instances called support set . Each time an instance is misclassified it is added to the support set, and the classification function is de-fined as a kernel combination of the observations in this set. It is clear that if the problem is not linearly separable, they will never stop updating the classifi-cation function. This leads eventually to a memory explosion, and it concretely limits the usage of these methods for all those applications where data must be acquired continuously in time.
 Several authors tried in the past to address this prob-lem, mainly by bounding a priori the memory require-ments. The first algorithm to overcome the unlimited growth of the support set was proposed by Crammer et al. (2003). The algorithm was then refined by Weston et al. (2005). The idea of the algorithm was to discard a vector of the solution, once the maximum dimension has been reached. The strategy was purely heuristic and no mistake bounds were given. A similar strategy has been used also in NORMA (Kivinen et al., 2004) and SILK (Cheng et al., 2007). The very first online algorithm to have a fixed memory  X  X udget X  and at the same time to have a relative mistake bound has been the Forgetron (Dekel et al., 2007). A stochastic algo-rithm that on average achieves similar performances, and with a similar mistake bound has been proposed by Cesa-Bianchi et al. (2006).
 In this paper we take a different route. We modify the Perceptron algorithm so that the number of stored samples is always bounded. Instead of fixing a priori the maximum dimension of the solution, we introduce a parameter that can be tuned by the user, to trade accuracy for sparseness, depending on the needs of the task at hand. We call the algorithm, that constitutes the first contribution of this paper, Projectron . The Projectron is an online, Perceptron-like method that is bounded in space and in time complexity. We derive for it a mistake bound, and we show experimentally that it outperforms consistently the Forgetron algo-rithm. The second contribution of this paper is the derivation of a second algorithm, that we call Projec-tron++ . It achieves better performances than the Per-ceptron, retaining all the advantage of the Projectron listed above. Note that this is opposite to previous budget online learning algorithms, delivering perfor-mances at most as good as the original Perceptron. The rest of the paper is organized as follows: in Section 2 we state the problem and we introduce the necessary background theory. Section 3 introduces the Projec-tron, Section 4 derives its properties and Section 4.1 derives the Projectron++. We report experiments in Section 5, and we conclude the paper with an overall discussion. The basis of our study is the well known Perceptron algorithm (Rosenblatt, 1958). The Perceptron algo-rithm learns the mapping f : X  X  R based on a set of examples S = { ( x 1 , y 1 ) , . . . , ( x T , y T ) } , where x is called an instance and y t  X  X  X  1 , +1 } is called a label . We denote the prediction of Perceptron as sign( f ( x )) and we interpret | f ( x ) | as the confidence in the predic-tion. We call the output f of the Perceptron algorithm a hypothesis , and we denote the set of all attainable hypotheses by H . In this paper we assume that H is a Reproducing Kernel Hilbert Space (RKHS) with a positive definite kernel function k : X  X X  X  R im-plementing the inner product h , i . The inner product is defined so that it satisfies the reproducing property, h k ( x , ) , f ( ) i = f ( x ) The Perceptron algorithm is an online algorithm, in which the learning takes place in rounds. At each round a new hypothesis function is estimated, based on the previous one. We denote the hypothesis esti-mated after the t -th round by f t . The algorithm starts with the zero hypothesis f 0 = 0 . On each round t , an instance x t  X  X is presented to the algorithm. The algorithm predicts a label  X  y t  X  X  X  1 , +1 } by using the current function,  X  y t = sign( f t ( x t )). Then, the cor-rect label y t is revealed. If the prediction  X  y t differs from the correct label y t , it updates the hypothesis f = f t  X  1 + y t k ( x t , ), otherwise the hypothesis is left intact, f t = f t  X  1 . Practically, the hypothesis f t can be written as a kernel expansion (Sch  X olkopf et al., 2000), where  X  i = y i and S t is defined to be the set of instance indices for which an update of the hypothesis occurred, support set. The Perceptron algorithm is summarized in Algorithm 1.
 Algorithm 1 Perceptron Algorithm
Initialize: S 0 =  X  , f 0 = 0 for t = 1 , 2 , . . . , T do end for Although the Perceptron is a very simple algorithm, it is considered to produce very good results. Our goal is to derive and analyze a new algorithm which attains the same results as the Perceptron but with a minimal size of support set. In the next section we present our Projectron algorithm. Let us first consider a finite dimensional RKHS H in-duced by a kernel such as the polynomial kernel. Since H is finite dimensional, there is a finite number of lin-early independent hypotheses in this space. Hence, any hypothesis in this space can be expressed using a finite number of examples. We can modify the Per-ceptron algorithm to use only one set of independent instances as follows. On each round the algorithm re-ceives an instance and predicts its label. On a pre-diction mistake, if the instance can be spanned by the to the support set. Instead, the coefficients {  X  i } in the expansion Eq. (1) are not merely y i , i  X  X  t  X  1 , but they are changed to reflect the addition of this instance to the hypothesis, that is,  X  i = y i + y t d i , 1  X  i  X  t  X  1. If the instance and the support set are linearly indepen-dent, the instance is added to the set with  X  t = y t as before. This technique reduces the size of the support set without changing the hypothesis in any way, and was used by Downs at al. (2001) to simplify Support Vector Machine solutions.
 Let us consider now the more elaborate case of an in-finite dimensional RKHS H induced by kernels such as the Gaussian kernel. In this case, it is not pos-sible to find a finite number of linearly independent vectors which span the whole space, and hence there is no guarantee that the hypothesis can be expressed by a finite number of instances. However, we can ap-proximate the concept of linear independence with a Algorithm 2 Projectron Algorithm
Initialize: S 0 =  X  , f 0 = 0 for t = 1 , 2 , . . . , T do end for finite number of vectors (Csat  X o &amp; Opper, 2001; En-gel et al., 2002; Orabona et al., 2007). In particular assume that at round t of the algorithm there is a pre-diction mistake and the mistaken instance x t should be added to the support set. Before adding the in-stance to the support, we construct two hypotheses: a temporal hypothesis f  X  t using the function k ( x t , ), esis f  X  X  t , which is the projection of f  X  t onto the space spanned by S t  X  1 . That is, the projected hypothesis is a hypothesis from the support set S t  X  1 which is the closest to the temporal hypothesis. Denote by  X  t the distance between the hypotheses  X  t = f  X  X  t  X  f  X  t . If the norm of distance k  X  t k is below some threshold  X  , we use the projected hypothesis as our next hypothesis, i.e., f t = f  X  X  t , otherwise we use the temporal hypothesis as our next hypothesis, i.e., f t = f  X  t . As we show in the next section, this strategy assures that the maximum size of the support set is always finite, regardless of the dimension of the RKHS H . Guided by these consider-ations we can design a new Perceptron-like algorithm that projects the solution onto the space spanned by the previous support vectors whenever possible. We call this algorithm Projectron. The algorithm is given in Algorithm 2.
 In our algorithm the parameter  X  plays an important role. If  X  is equal to zero, we obtain exactly the same solution of the Perceptron algorithm. In this case, however, the Projectron solution can still be sparser when some of the instances are linearly dependent or when the kernel induces a finite dimensional RKHS H . In case  X  is greater than zero we trade precision for sparseness. Moreover, as shown in the next sec-tion, this implies a bounded algorithmic complexity, namely, the memory and time requirements for each step are bounded. We will also derive mistake bounds to analyze the effect of  X  on the classification accuracy. We now consider the problem of deriving the projected hypothesis f  X  X  t in a Hilbert space H , induced by a kernel function k ( , ). Denote by P t  X  1 f t the projection of f  X  H onto the subspace H t  X  1  X  H spanned by the set S t  X  1 . The projected hypothesis f  X  X  t is defined as f The projection is an idempotent ( P 2 t  X  1 = P t  X  1 ) and linear operator, hence, and f  X  t we have Recall that the projection of f  X  t  X  H onto a sub-space H t  X  1  X  H is the hypothesis in H t  X  1 closest to f . Hence, let P j  X  X  H closest hypothesis is the one for which k  X  t k 2 = min Expanding Eq. (5) we get k  X  t k 2 = min Define K t  X  1 to be the matrix generated by the in-stances in the support set S t  X  1 , that is, { K t  X  1 } k ( x i , x j ) for every i, j  X  S t  X  1 . Define k t to be the vector whose i -th element is k t k  X  t k 2 = min applying the extremum conditions with respect to d , we obtain and, by substituting Eq. (8) into Eq. (7), Furthermore, substituting Eq. (8) into Eq. (3) we get We have shown how to calculate both the distance  X  t and the projected hypothesis f  X  X  t . In summary, one needs to compute d  X  according to Eq. (8), plug the result either into Eq. (9) and obtain  X  t or into Eq. (10) and obtain the projected hypothesis.
 In order to make the computation more tractable, we introduce an efficient method to calculate the matrix inversion K  X  1 t iteratively. This method was first in-troduced in (Cauwenberghs &amp; Poggio, 2000), and we give it here only for completeness. We would like to note in passing that the matrix K t  X  1 can be safely in-verted since, by incremental construction, it is always full-rank. After the addition of a new sample, K  X  1 t becomes  X   X   X   X   X  where d  X  and k  X  t k 2 are already evaluated during the previous steps of the algorithm. Thanks to this incre-mental evaluation, the time complexity of the linear independence check is O ( |S t  X  1 | 2 ), as one can easily see from Eq. (8). In this section we analyze the performance of the Pro-jectron algorithm in the usual framework of online learning with a competitor. First, we present a the-orem which states that the size of the support set is bounded.
 Theorem 1. Let k : X X X  X  R a continuous Mercer kernel, with X a compact subset of a Banach space. Then, for any training sequence ( x i , y i ) , i = 1 , ,  X  and for any  X  &gt; 0 , the size of the support set of the Projectron algorithm is finite.
 The proof of this theorem goes along the same lines as the proof of Theorem 3.1 in (Engel et al., 2002), and we omit it for brevity. Note that this theorem guarantees that the size of the support set is bounded, however it does not state that the size of the support set is fixed or can be estimated before training. The next theorem provides a mistake bound. The main idea is to bound the maximum number of mis-takes of the algorithm, relatively to the best hypothesis g  X  X  chosen in hindsight. Let us define D 1 as tion g on the example ( x t , y t ), that is, max { 0 , 1  X  y g ( x t ) } . With these definitions we can state the fol-lowing bound for the Projectron Algorithm.
 Theorem 2. Let ( x 1 , y 1 ) , , ( x T , y T ) be a sequence of instance-label pairs where x t  X  X , y t  X  { X  1 , +1 } , and k ( x t , x t )  X  R for all t . Let g be an arbitrary func-tion in H . Assume that the Projectron algorithm is run with 0  X   X  &lt; 2  X  R 2 2 k g k . Then the number of predic-tion mistakes the Projectron makes on the sequence is at most The proof of this theorem is based on the following lemma.
 Lemma 1. Let ( x , y ) be an example, with x  X  X  and y  X  { +1 ,  X  1 } . Denote by f an hypothesis in H , such Then the following bound holds for any  X   X  0 : Proof. = 2  X  y ( g ( x )  X  f ( x ))  X   X  2 k q ( ) k 2  X   X  2  X  ( f ( x ) , y )  X  2  X  ( g ( x ) , y )  X   X  k q ( ) k With this bound we are ready to prove Thm. 2. Proof. Define the relative progress in each round as  X  t = k f t  X  1  X  g k 2  X  X  f t  X  g k 2 . We bound the progress from above and below. On rounds in which there is no mistake  X  t is 0. On rounds in which there is a mistake there are two possible updates: either f t = f following we bound the progress from below, when the update is of the former type (the same bound can be obtained for the latter type as well, but the derivation is omitted). In particular we set q ( ) = P t  X  1 k ( x t Lemma 1 and use  X  t = y t P t  X  1 k ( x t , )  X  y t k ( x Eq. (4)  X  Note that h f t  X  1 ,  X  t i = 0, because f t  X  1 belongs to the space spanned by the functions indexed by S t  X  1 . Moreover, on every projection update k  X  t k  X   X  and using the theorem assumption k P t  X  1 k ( x t , ) k X  R , we then have  X  t  X   X  t 2 (  X  ( f t  X  1 ( x t ) , y t )  X   X  ( g ( x t ) , y We can further bound  X  t by noting that on every pre-diction mistake  X  ( f t  X  1 ( x t ) , y t )  X  1. Overall we have We sum over t both sides. Let  X  t be an indicator func-tion for a mistake on the t -th round, that is,  X  t is 1 if there is a mistake on round t and 0 otherwise, hence it can be upper bounded by 1. The left hand side of the equation is a telescopic sum, hence it collapses to k f 0  X  g k 2  X  X  f T  X  g k 2 , which can be upper bounded by k g k 2 , using the fact that f 0 = 0 and that k f T  X  g k 2 is non-negative. Finally, we have where M is the number of mistakes.
 To compare with other similar algorithms it can be useful to change the formulation of the algorithm in order to use the maximum norm of g as parameter instead of  X  . Hence we can fix an upper bound, U , on k g k and then we set  X  to have a positive progress. Specifically, on each round we set  X  to be 2 U The next corollary, based on Thm. 2, provides a mis-take bounds in terms of U rather than  X  .
 Corollary 1. Let ( x 1 , y 1 ) , , ( x T , y T ) be a sequence of instance-label pairs where x t  X  X , y t  X  { X  1 , +1 } , and k ( x t , x t )  X  1 for all t . Let g be an arbitrary func-tion in H , whose norm k g k is bounded by U . Assume that the Projectron algorithm is run with a parameter  X  , which is set in each round according to Eq. (13). Then, the number of prediction mistakes the Projec-tron makes on the sequence is at most Notice that the bound in Corollary 1 is similar to Thm. 5.1 in (Dekel et al., 2007) of the Forgetron algorithm. The difference is in the assumptions made: in the For-getron, the size of the support set is guaranteed to be less than a fixed size B that depends on U , while in the Projectron we choose the value of  X  or, equivalently, U , and there is no guarantee on the exact size of the sup-port set. However, the experimental results suggest that, with the same assumptions used in the deriva-tion of the Forgetron bound, the Projectron needs a smaller support set and produces less mistakes. It is also possible to give yet another bound by slightly changing the proof of Thm. 2. This theorem is a worst-case mistake bound for the Projectron algorithm. We state it here without the proof, leaving it for a long version of this paper.
 Theorem 3. Let ( x 1 , y 1 ) , , ( x T , y T ) be a sequence of instance-label pairs where x t  X  X , y t  X  { X  1 , +1 } , and k ( x t , x t )  X  R for all t . Let g an arbitrary function in H . Assume that the Projectron algorithm is run with 0  X   X  &lt; 1 k g k . Then, M , the number of prediction mistakes the Projectron makes on the sequence is at most The last theorem suggests that the performance of the Projectron are slightly worse than the Percep-tron (Shalev-Shwartz &amp; Singer, 2005). Specifically the degradation in the performance of Projectron com-pared to the Perceptron are related to 1 / (1  X   X  k g k ) the next subsection we present a variant to the Projec-tron algorithm, which attains even better performance. 4.1. Going Beyond the Perceptron The proof of Thm. 2 and Corollary 1 direct us how to improve the Projectron algorithm to go beyond the performance of the Perceptron algorithm, while main-taining a bounded support set.
 Let us start from the algorithm in Corollary 1. We change it so an update takes place not only if there is a prediction mistake, but also when the prediction is correct with a low confidence. We indicate this latter case as a margin error , that is, 0 &lt; y t f t  X  1 ( x t This strategy improves the classification rate but also increases the size of the support set (Crammer et al., 2006). A possible solution to this obstacle is not to update every round a margin error occurs, but also when the new instance can be projected onto the sup-port set. Hence, the update on margin error rounds would be in the general form with 0 &lt;  X  t  X  1. The last constraint comes from proofs of Thm. 2 and Corollary 1 in which we upper bound  X  by 1. Note that setting  X  t to 0 is equivalent to leave the hypothesis unchanged. The bound in Corollary 1 becomes where  X  t bounds the progress made on margin error round t . In particular it is easy to see from Lemma 1 that  X  t is  X  for 0 &lt;  X  t  X  1, and is 0 when there is no up-date. Whenever  X  t is non-negative the worst-case number of mistakes in Eq. (15) decreases, hopefully along with the classification error rate of the algo-rithm. Hence, we determine the optimal  X  t which maximizes  X  t . In particular, the expression of  X  t in Eq. (16) is quadratic in  X  t , and is maximized for  X  to be less than or equal to 1, we have 1  X  t = min {  X  ( f t  X  1 ( x t ) , y t ) / k P t  X  1 k ( x In summary, at every round t with margin error we calculate  X  t according to Eq. (17), and check that  X  t is non-negative. If so we update the hypothesis using Eq. (14), otherwise we leave it untouched.
 With this modification we expect better performance, that is, fewer mistakes, but without any increase of the support set size. We can even expect solutions with a smaller support set, since new instances can be added to the support set only if misclassified, hence having less mistakes should result in a smaller support set. We name this variant Projectron++, and in the next section we compare it to the original version. In this section we present experimental results that demonstrate the effectiveness of the Projectron and the Projectron++. We compare both algorithms to the Perceptron and to the budget algorithms For-getron (Dekel et al., 2007) and Randomized Budget Perceptron (RBP) (Cesa-Bianchi et al., 2006). For the Forgetron, we choose the state-of-the-art  X  X elf-tuned X  variant, which outperforms its other variants. We also use two other baseline algorithms: the first one is a Perceptron algorithm which stops updating the solu-tion once the support size has reached some limit, and it is used to verify that the Projectron is better than just stop learning. We name it Stoptron . The second baseline algorithm is the PA-I variant of the Passive-Aggressive learning algorithm (Crammer et al., 2006), which gives an upper bound to the classification per-formance that Projectron++ can reach.
 We tested the algorithms with two standard machine learning datasets: Adults9 and Vehicle 2 and a syn-thetic dataset, all of them with more than 10000 sam-ples. The synthetic dataset is built in the same way as in (Dekel et al., 2007). It is composed with sam-ples taken from two separate bi-dimensional Gaussian distributions. The means of the positive and nega-tive samples are (1 , 1) and (  X  1 ,  X  1), respectively, while the covariance matrix for both is diagonal matrix with (0 . 2 , 2) as its diagonal. Then the labels are flipped with a probability of 0 . 1 to introduce noise. All the experiments were performed over 5 different permutations of the training set. All algorithms used a Gaussian kernel with  X  2 equals 25, 4, and 0.5 for Adults9 , Vehicle , and the synthetic datasets, respec-tively. The C parameter of the PA-I was set to 1, to have an update similar to the Perceptron and Pro-jectron. Due to the different nature of our algorithm compared to the budget ones, we cannot select the sup-port set size in hindsight. Hence, we compared them using the proper conditions to obtain the same bounds. That is, we selected the maximum support size B for the Forgetron algorithm, which implies a maximum value U , the norm of g , for its bound to hold. In par-ticular U is equal to 1 / 4 p ( B + 1) / log ( B + 1) (Dekel et al., 2007), where B is the budget parameter that sets the maximum size of the support set. We then se-lected the parameter  X  in the Projectron in each round according to Eq. (13). Hence the final size of the Pro-jectron solution will depend on U and on the particular classification problem at hand. We have set B on each dataset roughly to 1 / 2 and 1 / 4 of the size of the Per-ceptron support set, for a total of 6 experiments. Note that Projectron can also be used without taking into account the norm of the competitor and considering  X  just as a parameter. In particular  X  should be set to trade accuracy for sparseness.
 In Tables 1 X 3 we summarize the results of our experi-ments. The cumulative number of mistakes as percent-age of the training size (mean  X  std) and the size of the support set are reported. In all the experiments both the Projectron and the Projectron++ outperform the Forgetron and the RBP with a smaller support size. Moreover, the Projectron++ always outperforms the Projectron and has smaller support set. Due to its theoretically derived formulation, it achieves better re-sults even if being bounded, and it has better perfor-mance than the Perceptron. In particular it gets closer to the classification rate of the PA-I, without paying the price of a large support set. It is interesting to note the performances of the Stoptron: it has an accuracy close to the other bounded algorithms in average, but with much bigger variance. This indicates that all the examined strategies for bounded learning are always better than the simple procedure to stop learning, at least to have stable performances.
 Last, we show the behavior of the algorithms over time. In Fig. 1 we show the average online error rate, that is, the total numbers of errors on the examples seen as a function of the number of samples for all algo-rithms on the Adult9 dataset with B = 1500. Note how the Projectron algorithm closely tracks the Per-ceptron. On the other hand the Forgetron and the RBP stop improving after reaching the support set size B , around 7500 samples. The growth of the support set as a function of the number of samples is depicted in Fig. 2. While for PA-I and Perceptron the growth is clearly linear, it is sub-linear for Projectron and for the Projectron++ and they will reach a maximum size and then they will stop growing (as stated in Thm. 1). In Fig. 3 we show the average online error rate as a function of the size of the support set. It is clear that the Projectron and the Projectron++ outperform the Perceptron with smaller support set. This paper presented two different versions of a bounded online learning algorithm. The algorithms depend on a parameter that allows to trade accuracy for sparseness of the solution. The size of the solution is always guaranteed to be bounded, therefore it solves the memory explosion problem of the Perceptron and similar algorithms. Although the size of the support set is guaranteed to be bounded, the actual size of the support set cannot be determined in advance, like in the Forgetron algorithm, and it is not fixed. Prac-tically, the size of the support set of the Projectron algorithms is much smaller than that of the budget algorithms.
 Compared to budget algorithms it has the advantage of a bounded support set size without removing or scal-ing instances in the set. This keeps performance high. We call this algorithm Projectron. Its second variant, the Projectron++, always outperforms the standard Perceptron algorithm, while assuring a bounded so-lution. Another advantage over budget algorithms is the possibility to obtain bounded batch solutions us-ing standard online-to-batch conversion. In fact using the averaging conversion (Cesa-Bianchi et al., 2004) we get a bounded solution. This is not true for bud-get algorithms, where more sophisticated techniques have to be used (Dekel &amp; Singer, 2005). A similar ap-proach has been used in (Csat  X o &amp; Opper, 2001) in the framework of the Gaussian Processes. However in that paper no mistake bounds were derived and the use of the hinge loss allows us to have sparser solution. Acknowledgments. This work was supported by EU project DIRAC (FP6-0027787).

