 Automatic image annotation is the task of automatically as-signing words to an image that describ e the con ten t of the image. Mac hine learning approac hes have been explored to mo del the asso ciation between words and images from an annotated set of images and generate annotations for a test image. The pap er prop oses metho ds to use a hierarc hy de ned on the annotation words deriv ed from a text ontol-ogy to impro ve automatic image annotation and retriev al. Speci cally , the hierarc hy is used in the con text of gener-ating a visual vocabulary for represen ting images and as a framew ork for the prop osed hierarc hical classi cation ap-proac h for automatic image annotation. The e ect of using the hierarc hy in generating the visual vocabulary is demon-strated by impro vemen ts in the annotation performance of translation mo dels. In addition to performance impro ve-men ts, hierarc hical classi cation approac hes yield well to constructing multimedia ontologies.
 H.3.3 [ Information storage and retriev al ]: Information searc h and retriev al| Retrieval models ; I.4.8 [ Image Pro-cessing and Computer Vision ]: Scene Analysis| Obje ct Recognition Algorithms, Measuremen t, Exp erimen tation Automatic image annotation, image retriev al, ontologies, translation mo dels, hierarc hical classi cation mo dels
With signi can t impro vemen ts in text searc h and natu-ral language question answ ering, there is gro wing interest in multimedia information retriev al and question answ ering. Early approac hes to video and image retriev al were based on attributes like the color and texture of images or key frames of video. User queries, however, are better expressed in nat-ural language than image attributes. It is easier to say find images with aircraft in an hangar than to generate the corresp onding color, texture or sketch query . Automatic im-age annotation { the task of asso ciating text to the seman tic con ten t of images { has been used as an intermediate step to image retriev al.

Di eren t mac hine learning metho ds for image annotation mo del the asso ciation between words and images or image regions. These include translation mo dels [7], classi cation approac hes [14, 5] and relev ance mo dels [10, 13]. While most mo dels have used the co-o ccurrence of image regions and words, few have explored the dep endence of annota-tion words on image regions [3]. This pap er prop oses to exploit ontological relationships between annotation words and demonstrate their e ect on automatic image annotation and retriev al.

The hierarc hical asp ect cluster mo del [2] for image annota-tion adapts a generativ e mo del that induces an hierarc hical structure from co-o ccurrence data [9]. The top ology of the hierarc hy is externally de ned and image regions and anno-tation words are attac hed to nodes of the hierarc hy based on clustering of images from a training set. The hierarc hy is intended to represen t the various levels of generalit y of the concepts expressed in image regions and words. Based on their seman tics, however, the annotation words themselv es can be placed in an hierarc hy of concepts. For example, the Corel data set pro vided by [7] includes images of cougar , leopard , lion , lynx , tiger and cats . Lexical and ontolog-ical resource like WordNet organizes di eren t animals in an hierarc hy and can place cougar , leopard , etc. under cat . In this pap er, suc h hierarc hical dep endencies between annota-tion words are used to generate impro ved visual lexicons for the translation-based approac hes. The inten t is to use the hierarc hies to capture the visual similarities among di eren t cats.

The hierarc hy induced by WordNet is the framew ork for the prop osed hier archic al classi c ation appr oach to image annotation . Giv en a visual vocabulary of blobs, eac h node in the hierarc hy is asso ciated with a statistical language mo del de ned on blobs characterizing the concept repre-sen ted by that node. Blob-lik eliho od probabilities for di er-ent concepts are estimated by com bining the di eren t lan-guage mo dels of the nodes in an hierarc hy using shrink age. While hierarc hical classi cation yields well to the image an-notation problem, the hierarc hical organization of concepts and the asso ciated language mo dels de ned on the blobs pro vides a represen tation for multimedia ontologies.
The next section motiv ates the relev ance of the hierar-chy induced on the annotation words for automatic image annotation and the need for suc h an organization to sup-port multimedia information retriev al and natural language question answ ering.
Text ontologies have been sho wn to impro ve information retriev al and natural language question answ ering. Di eren t TREC exp erimen ts [20] have explored the use of kno wledge bases for documen t retriev al. In the con text of imager re-triev al based on image captions, Smeaton and Quigley [19] use Hierarc hical Concept Graphs (HCG) deriv ed from Word-Net to estimate the seman tic distance between caption words for similarit y computation between query terms and image captions. In natural language question answ ering, Moldo van and Novichi [16] use the various relationships de ned among WordNet concepts and their gloss to disco ver and weigh t lexical chains between concepts, and demonstrate the use of these lexical chains in question-answ ering.

To supp ort questions addressed to multimedia data where the answ er is represen ted in multiple mo dalities, multime-dia ontologies must be able to express concepts in multiple media formats and pro vide cross-mo dal relationships to sup-port reasoning. Consider the question, What is the jersey color of Brazil's national soccer team? . Giv en that the picture and caption in Figure 1 is in the image collection, cross-mo dal reasoning is required to iden tify that Ronaldo is a Brazilian player and the color of his jersey is yello w. Figure 1: Image with the answ er to the question on the color of the Jersey of Brazil. The asso ciated text is Ronaldo seals Brazil's plac e in the last eight with a shot through Geert de Vlie ger's legs late on to eliminate Belgium . The color of Ronaldo's jersey will not sho w up in blac k and white versions of this picture.

An ontology should include an organization of concepts of interest and the relationships among them. In WordNet, eac h concept node is asso ciated with (1) the di eren t surface forms in whic h the concept occurs in text, (2) its sense and (3) a gloss of sen tences that sho w the use of the concept in that particular sense. While iden tifying the occurrence of a concept in a documen t is easily accomplished by searc hing for the occurrence of one of its surface forms, detecting con-cept occurrence in image or video (or any multimedia data) is not trivial. For example, it is easier to perform string matc h and detect the occurrence of tiger in text than rec-ognizing an orange patc h with blac k strip es as a tiger in an image. The mo dels used for image annotation de ne the mec hanism for detecting concept occurrence in images or video key frames. Asso ciating a mo del and its parameters with a concept node extends a text ontology to a multime-dia ontology . This can be done by asso ciating the visual tok ens for a concept with its node in the ontology . An im-age of tiger on grass con tains image regions that iden tify tiger and grass. The image regions that characterize tiger can be placed under the concept of tiger in the multimedia ontology . The image can be made part of the gloss for that concept.

The use of text ontologies as a basis for de ning visual vocabulary or as a framew ork for automatic image annota-tion increases the num ber of concepts an image annotation system can recognize for a given image. Based on the hy-pern ym y (ISA hierarc hy) of annotated words, it is possible to annotate images with words that do not have explicit ex-amples in the training set. Image annotation system using the Corel data set can annotate an image as an animal even when no image in the training set that is so annotated. Re-gions corresp onding to images of tiger , cougar , cat , horse , cow , etc., can be placed under the concept node of animal and be used to learn the characteristics of animal allo wing its prediction as an annotation.
 We restrict ourselv es to using lexical resources like Word-Net to induce hierarc hies in the annotation words. Addi-tional asp ects of WordNet, like the gloss, that can be used in disam biguating the sense of the annotation words are left for future work.

The rest of the pap er is organized as follo ws: the next section presen ts related work in automatic image annota-tion. In the con text of translation mo dels, Section 4 dis-cusses the use of an hierarc hy induced on the annotation words to generate visual vocabulary used to represen t image regions. Section 5 presen ts the hierarc hical classi cation ap-proac h to image annotation. The exp erimen ts rep orted here are based on the annotated images from the Corel data set by Dugulu et al. [7]. Eac h image is asso ciated with up to 5 keyw ords. They pro vide limited linguistic con text to use natural language pro cessing techniques to iden tify the sense of an annotation word. Section 6 discusses an approac h adopted to iden tify the sense of a word for the purp oses of generating the annotation word hierarc hy. Section 7 rep orts on the exp erimen ts comparing the translation and classi ca-tion metho ds for image annotation under di eren t settings of the visual vocabulary used for represen ting images. Sec-tion 8 concludes the pap er and describ es future work.
A num ber of mac hine learning approac hes have been ex-plored for the automatic image annotation problem. With an annotated training set of images, mo dels prop osed for im-age annotation learn from the co-o ccurrence of words and images or image regions. Image regions can be generated us-ing image segmen tation techniques like N-cuts [18] or from grids. In their co-o ccurrence mo del, Mori et al. [17] used a grid-based segmen tation metho d to iden tify image regions and used the co-o ccurrence of words and image regions to predict image annotations.

Duygulu et al. [7] mo deled image annotation as translat-ing visual represen tation of concepts in an image to their tex-tual represen tation. The visual vocabulary is generated by clustering the image regions iden ti ed using N-cuts segmen-tation algorithm. IBM Mo del 2 [4] was used in their image annotation exp erimen ts. Observing the skewness in the esti-mation of translation probabilities of uncommon words, Jin et al. [12] prop osed regularization of the translation mo del based on Bro wn's Mo del 1. While the translation mo dels capture the correlation between blobs and words, the dep en-dencies between blobs or words are not captured. Dep en-dence between annotation words in the training set, based on an hierarc hy deriv ed from WordNet, is used in this pap er for selecting the visual vocabulary .
 Motiv ated by the statistical clustering mo dels prop osed by Ho mann and Puzic ha [9] for co-o ccurrence data, Barnard and Forsyth [2] adapted the hierarc hical asp ect cluster mo del for image annotation. The hierarc hy of mo dels for generat-ing word and image elemen ts is deriv ed by clustering images in the training set. The clusters capture con textual similar-ities while the nodes capture generalit y of concepts. Words and blobs are then represen ted as a distribution over the nodes of the hierarc hy, thus words and blobs with similar distributions can be considered correlated. While the hier-archies induced by image clusters pro vide some seman tic in-terpretation for the mo dels, the structure of the hierarc hy is man ually speci ed. The prop osed hierarc hical classi cation approac h for image annotation uses an hierarc hy deriv ed from a kno wledge source that is based on concept organiza-tion in natural language. It is exp ected that remo val of this arbitrariness in the structure of the hierarc hy will pro vide impro ved results.

By viewing eac h annotated word as an indep endence class, text classi cation approac hes have been adapted for the image annotation task. These mo dels are de ned on im-age attributes that can be generated to predict the like-liho od of an annotation word for a given image. Classi-cation approac hes for image annotation and retriev al in-clude linguistic indexing of images [14] and Supp ort Vector Mac hines (SVM) [5]. Motiv ated by the hierarc hical clas-si cation approac h using shrink age prop osed by McCallum et al. [15], the prop osed hierarc hical classi cation approac h exploits the hierarc hy in the annotation words deriv ed from WordNet for image annotation.

Di eren t probabilistic mo dels have been prop osed for au-tomatic image annotation. Blei and Jordon [3] have pro-posed graphical mo dels of increasing sophistication of cap-turing the dep endence between words and image regions: Gaussian mixture mo dels, Guassian-Multinomial LD A and corresp ondence Laten t Diric hlet Allo cation (LD A) for the image annotation problem. Recen tly, Relev ance mo dels [10, 13, 8] for image annotation have sho wn signi can t perfor-mance impro vemen ts. The mo del learns the join t proba-bilit y of asso ciating words to image features from training set and uses it to generate the probabilit y of asso ciating a word to a given query image. While the above mo dels predicted the probabilit y, P ( w j I ) of an annotation word w given an image, I , one is interested in generating a the set of annotation words, f w g . While annotation of certain length can be selected by ranking the probabilities P ( w j I ), Jin et al. [11] prop osed the Coheren t Language Mo del that pre-dicts the annotation words, f w g , by relaxing the estimation of P ( f w gj I ) to estimating the probabilit y, P ( w j I ) of a lan-guage mo del w to generate the annotation words for the image I .

Except for the corresp ondence LD A mo del [3] that cap-tures the dep endence between image regions and word an-notations, and Coheren t Language Mo del [11] that captures the correlation between annotated words of an image , all the above mo dels assume indep endence of word and image elemen t events in their generativ e mo dels for image annota-tion. The image annotation and retriev al approac hes pro-posed here use the dep endencies between annotation words represen ted by the hierarc hy deriv ed from an text ontology .
The ARD A VACE program studies the use of ontologies in the visual domain extensiv ely, but the ontologies used there are mainly related to event detection and the pro cess of represen ting complex events as com binations of simple events. Using ontologies in the con text of annotations and object recognition would aid in the iden ti cation of the en-tities involved in an event.
Annotation words of an image describ e the seman tics of the image in text. Concepts expressed in text can be orga-nized in hierarc hies deriv ed from a kno wledge base, in our case, a lexical resource (W ordNet). This section prop oses to use the hierarc hy induced in the annotation words by WordNet in the translation mo del for image annotation.
In the translation mo del, images are segmen ted to images regions. Feature vectors capturing the image attributes of a region are clustered to generate the visual vocabulary or the lexicon for image represen tation. Eac h image region is mapp ed to a elemen t of this lexicon referred to as a blob. We use the IBM translation mo del 2 [4] that includes assignmen t probabilities of translating a blob in an image to a word. Giv en a training set, T , of N images, with eac h image J represen ted by w i ; b i with blobs b = f b i 1 ; b i 2 ; : : : ; b annotated words w = f w i 1 ; w i 2 ; : : : ; w im i g , the translation probabilit y P ( w j b ) is estimated from the likeliho od where P ( a ijk ) corresp onds to the probabilit y of assigning the blob b k to annotation word w j in the image J i . The Exp ectation-Maximization (EM) [6] is used to estimate the translation probabilities.
 Ontology-induced Visual Vocabulary The hierarc hy induced on the annotation words is used to de ne the visual vocabulary for images. In their translation mo del exp erimen ts, Duygulu et al. [7] use K-means clus-tering to cluster the images regions to generate the visual vocabulary . The e ectiv eness of the clusters generated by K-means clustering dep ends on the selection of the exem-plars assigned as cluster cen ters. Instead of a random selec-tion of initial cluster cen ters for the K-means clustering, the hierarc hy in the annotation words are used to perform the selection.

Giv en the hierarc hy of annotation words that includes other concepts related to the annotation words, images re-gions are group ed under eac h node in the hierarc hy. An image region r in image I is placed under a concept w if ei-ther (1) w is an annotation word for the image I , or, (2) one of its hyponyms (descendan ts in the hierarc hy) annotates the image, I . While image regions placed under a concept may characterize asp ects of the image not necessarily related to the concept, averaging the feature vectors of the images regions in this set pro duces a seman tically-motiv ated initial cluster cen ter for the K-means clustering. This initial clus-tering of image regions is used in the K-means clustering algorithm to generate the blobs.
 Weigh ted K-means clustering In eac h iteration of the K-means clustering algorithm, clus-ter cen ters are computed by averaging the feature vectors of the image regions placed in the cluster. Based on the asso ciation between between image regions and annotation words, weigh ts can be assigned to image regions quan tifying their con tribution to the particular cluster. Let W r be the set of words from the set of annotation words, W , that is asso ciated with a region r . The weigh t wt ( r; c ) is computed as where n ( w; c ) is the num ber of image regions in cluster c asso ciated with the word w and n ( c ) is the num ber of regions in the cluster c . The cluster cen ter for a cluster c with regions R c assigned to it is then given by where f ( r ) is the feature vector for region r . The weigh t corresp onds to the relev ance of a region to the cluster based on the words asso ciated with the cluster.

The results of translation mo dels for image annotation based on the above selection criteria for visual vocabulary are presen ted in Section 7.
Motiv ated by the statistical language mo deling approac h based on shrink age for hierarc hical classi cation [15], we pro-pose a hierarc hical classi cation approac h for image anno-tation based on the hierarc hy induced on annotation words deriv ed from WordNet. Viewing eac h annotation word as a class lab el, mo dels de ned on blobs are generated for pre-diction the the likeliho od probabilit y, P ( w j I ), of assigning the class lab el w given an image, I .

Adapting the notation of McCallum et al. [15] we describ e to approac h to image annotation as a classi cation prob-lem. Assuming that the image data is generated using a mixture mo del, , with one-to-one corresp ondence between the mo del comp onen ts and annotation words, w k 2 W . The generativ e mo del for an image J i then corresp onds to select-ing an annotation word, w k based on the priors P ( w k j ) and using the corresp onding mixture comp onen t to generate the image J i according to P ( J i j w k ; ). The marginal probabilit y of generating an image J i is given by Eac h image can be view ed as a sequence of blobs or visual terms from a visual vocabulary , V . An image can be mo d-eled to have been dra wn from a multinomial distribution of blobs by making the naiv e Bayes assumption that eac h selec-tion of a blob for the image is indep end of its con text given the annotation word, and it is also indep enden t of the po-sition of the blob in the image. The image likeliho od given the annotation word, w k , is given by The parameters for this mo del includes the mixture comp o-nen ts k for eac h annotation word w k and the blob proba-bilities, kl = P ( b l j w k ; ) suc h that k kl = 1.
Giv en a annotated training set of images, T , smo othed estimates can be obtained for the parameters of the mo del. Let N ( b l ; J i ) be the num ber of times blob b l occurs in image J and de ne P ( w k j J i ) 2 f 0 ; 1 g (Bernoulli mo del for anno-tations as they do not rep eat for an image) as given by the class lab el. Then, kl = P ( b l j w k ; ) = The Laplace smo othing of the maxim um likeliho od estimate is used in the above estimate to ensure non-zero probabilities for all blobs; The prior probabilities for annotation words is given by
For a given test image, I , represen ted by its sequence of blobs, b 1 ; b 2 ; : : : ; b m , the probabilit y of a word w annotation is given by
In the above no assumption is made on the dep endencies, if any, between the annotation words; Eac h word is asso ci-ated with a mixture comp onen t in the mixture mo del. The dep endencies between the annotation words, as iden ti ed by the ISA hierarc hy in WordNet is used in the follo wing to generate an hierarc hy mo del for estimating P ( b l j w Let H be the hierarc hy induced on the annotation concepts. Using shrink age results in the follo wing equation As an example, assume the hierarc hy of 'tiger' ISA 'cat' ISA 'animal' ISA 'ROOT'. The probabilit y P ( b j 0 tiger given by
P ( b j 0 tiger 0 ) = ( 0 tiger 0 ; 0 tiger 0 ) P mle ( b j
Here 'ROOT' is the highest node in all hierarc hies that corresp onds to the uniform distribution for P ( b ).
To learn the 's we use held-out images, H . From the training set, T , maxim um likeliho od estimates are obtained for the concepts in the hierarc hy. For an annotation word, w , let f v 1 = w j ; v 2 ; : : : ; v m = 0 ROOT 0 g be the set of con-cepts from the leaf to the root. Note that some of the in-termediate nodes themselv es can occur as annotations for some images. The corresp onding ML estimates be given by j ; 2 j ; : : : ; m j g . The objectiv e is to estimate The weigh t ( v; w ) is estimated using a simple form of EM [15]. We compare the results of using at organization of annotation words (baseline classi cation mo del) with the hierarc hy induced from WordNet (hierarc hical classi cation mo del). represen t the seman tics of the objects in the images. Eac h image has at most 5 words describing its con ten t. There is limited linguistic con text to use a syn tactic parser to iden tify the sense in whic h the particular words are used in the an-notation. Deriving from WordNet, Barnard et al. [1] use the sense of a word that has the largest hypern ym in common with the neigh boring words in an image annotation. This based on the assumption that senses are shared, in this case with shared paren tage, for text asso ciated with pictures.
Since the words used as annotations in the Corel Data set are nouns, we made a coarser assumption that a particular word is used in only one sense in the the whole corpus. The sense for a word is selected based on the num ber of times its (1) hyponyms (in that sense) and (2) hypern yms (in that sense) app ear as annotations of an image in the collection. For example, 'tiger' has two senses in WordNet: Sense 1 -tiger: a erce or audacious person, and Sense 2 -tiger: large feline of forests; a big cat. Due to the corpus having a num-ber of animal pictures, the second sense is assigned for 'tiger' and accordingly placed in the hierarc hy under animals. The con text pro vided by other annotation words is imp ortan t to determine the sense of a word. In the settings of a multime-dia ontology , di eren t senses of a concept will have nodes of their own righ t and accordingly , image samples included in their gloss represen ting the correct sense of the words.
The exp erimen ts are performed on the Corel Data set pro-vided by [7]. The data set consists of 5000 annotated images split into a training set of 4500 images with the remaining 500 used for testing. Eac h image is segmen ted into multi-ple regions represen ted by 36 image features, suc h as color, position, texture and shap e [7]. A total of 371 words an-notate the data set with up to 5 annotation words for eac h image. The reference translation mo del results are based on the blobs generated by clustering the image regions from the training set into 500 clusters using K-means clustering algorithm. This set of blobs is referred to as KM-500 .
Based on the metho d describ ed in Section 6, an hierarc hy with 714 nodes is generated for the 371 unique annotation words. This hierarc hy is used in the exp erimen ts describ ed below. Assuming one blob per concept node in the hierarc hy, 714 clusters are generated by initializing the K-means clus-tering algorithm with the images regions asso ciated with the concept. This set of clusters is referred to as ONT-714 . To compare the results obtained using KM-500 with 500 clus-ters, we reduced the set of 714 blobs to 500 by iterativ ely com bining closest clusters. In eac h iteration, the two closest clusters are replaced by averaging their cluster cen ters. This set is referred to as ONT-500 . While assigning one blob for a concept node ensures that eac h concept should have at least one 'visual surface form' in the vocabulary , more sophisti-cated metho ds can be explored for the num ber of blobs per concept and the selection of image regions for generating the visual vocabulary . This is left for future work.

Weigh ted K-means clusters are generated based on the metho d presen ted in Section 4. This set of referred to as WKM-500 .

As with previous studies on automatic image annotation, the qualit y of annotation is evaluated by comparing the gen-erated annotations with actual image annotations in the test set. Eac h image is annotated with the top 5 words based on their likeliho od probabilit y. Some images do not have ve annotation words, so this corresp onds to the unnormalized score in [7]. For a given word, w , let N ( w ) be the num ber of images annotated by w of whic h let r ( w ) be the num ber of images that were correctly annotated. Let R ( w ) be the total num ber of with w as its true annotation. Precision and recall are de ned by Overall performance is measured by the precision and recall values averaged over all annotation words.

In addition to these measures, the num ber of predicted words and words with positiv e recall are included in the re-sults. The num ber of words predicted as an annotation at least once for any image in the test set di ers for di eren t annotation metho ds. To mak e fair comparison of the dif-feren t metho ds, the average precision and recall values are evaluated for the union of all predicted words. This includes all words that are predicted by at least one of the compared metho ds.
The results of using the di eren t visual vocabularies in translation-based approac h to image annotation is given in Table 1. The precision and recall values along with the num ber of words predicted and the num ber of suc h words with positiv e recall is presen ted. The reference K-Means vocabulary gives higher precision and recall over weigh ted K-Means and ontology-induced vocabularies. However, the num ber of words predicted is less compared to the ontology-induced vocabularies.

The impro vemen t obtained by using the ontology induced visual vocabulary is eviden t from the precision and recall values in the last two rows. These are averaged over the words that were predicted by at least one of the annota-tion metho ds. The ONT-714 vocabulary gives 19.5% im-pro vemen t in average precision and 13% increase in average precision over the KM-500 vocabulary . Reducing the ontol-ogy to 500 clusters (ONT-500) results in 12.6% and 3.6% Measures KM-500 WKM-500 ONT-714 ONT-500 Precision 0.3306 0.3177 0.3074 0.3159 Recall 0.3618 0.3926 0.3178 0.3180 Predicted 28 27 36 33 Positiv e
Recall All 42 words Precision 0.2204 0.2042 0.2634 0.2482 Recall 0.2412 0.2524 0.2724 0.2499 Table 1: Comparing the performance of Translation Mo dels using di eren t visual vocabularies Measures KM-500 WKM-500 ONT-714 ONT-500 Precision 0.1627 0.1867 0.1647 0.1643 Recall 0.2766 0.2831 0.2724 0.2697 Predicted 152 153 150 141 Positiv e
Recall Precision 0.1805 0.1882 0.1723 0.1754 Recall 0.3174 0.3135 0.2926 0.2903 Predicted 146 140 150 137 Positiv e
Recall Table 2: Comparing the performance of classi ca-tion metho ds using di eren t visual vocabularies impro vemen t in average precision and recall, resp ectiv ely. With the simple assumption of one blob per concept, the ontology-induced clusters impro ves image annotation using translation-mo del. The hierarc hy-based initial clusters seem to attac h better seman tics to the clusters generated by the K-means algorithm.
Di eren t visual vocabularies were used to evaluate the classi cation approac hes for image annotation. The base-line classi cation mo del views eac h annotation word as an indep enden t class and learns the blob-lik eliho od mo del. The hierarc hical classi cation mo del uses the hierarc hy induced in annotation words to learn an interp olated mo del for like-liho od probabilities. Table 2 summarizes the results of com-paring of the two classi cation mo dels.

The hierarc hical classi cation approac h performs better than the baseline classi cation metho d on di eren t visual vocabulary settings. Using the KM-500 vocabulary the av-erage precision increase of about 10% and 14% increase ob-serv ed in average recall. While the num ber of predicted words reduces, there is increase in the num ber of words with positiv e recall. Across di eren t vocabularies, WKM-500 pro vides the best precision and recall values.
To mak e fair comparison of the performance of the clas-si cation metho ds, Table 3 presen ts the precision and recall values for predicting the pooled set of predicted annotation words. The comparison is performed by rst xing the vi-
Measures KM-500 WKM-500 ONT-714 ONT-500 # Words 168 169 171 163 Precision 0.1481 0.1701 0.1455 0.1431 Recall 0.2519 0.2580 0.2405 0.2350 Precision 0.1580 0.1570 0.1521 0.1485 Recall 0.2777 0.2616 0.2584 0.2458 Table 3: Comparing the performance of classi -cation mo dels predicting pooled set of annotation words
Measures KM-500 WKM-500 ONT-714 ONT-500 # Words 168 169 171 163 Precision 0.0551 0.0508 0.0647 0.0640 Recall 0.0603 0.0627 0.0669 0.0644 Precision 0.1481 0.1701 0.1455 0.1431 Recall 0.2519 0.2580 0.2405 0.2350 Precision 0.1579 0.1570 0.1521 0.1485 Recall 0.2777 0.2616 0.2584 0.2458 Table 4: Comparing the performance of translation mo del and classi cation mo dels sual vocabulary . Eac h column corresp onds to the selected visual vocabulary . The second row iden ti es the total num-ber of predicted words for the two classi cation metho ds using the same visual vocabulary .

While the two classi cation metho ds have comparable num ber of predicated words (Ref. Predicted word coun t in Table 2), the pooled results indicate that signi can t im-pro vemen ts using the KM-500 and ONT-714 vocabulary . The best precision and recall values observ ed under WKM-500 in Table 2 are not observ ed in the pooled set. The pre-cision decreases for the hierarc hical classi cation metho d.
Based on the above results, the annotation word hierar-chy seems is e ectiv e as a framew ork for the hierarc hical classi cation mo del as well as a source for selecting the ini-tial clusters for the visual vocabulary . While the hierarc hi-cal classi cation approac h impro ved in precision and recall under all visual vocabulary settings except WKM-500, the ONT-714 is the only one that does not lose predictiv e abilit y in the hierarc hical classi cation. Same num ber of words are predicted with 5 more words with non-zero recall value.
Table 4 compares the performance of the translation and classi cation mo dels on the pooled set of predicted words. The classi cation algorithms sho w a three to four times im-pro vemen t over the translation mo del, mainly due to the abilit y to predict three or four times as man y words as the translation mo del. The KM-500 set performs the best over the union of predicted words.

The hierarc hy induced in annotation words pro vides over-all impro vemen ts in automatic image annotation both when used for generating the visual vocabulary for image represen-tation and as a framew ork for the hierarc hical classi cation metho d.
This pap er prop osed metho ds to use hierarc hies induced on annotation words to impro ve automatic image annota-tion. While hierarc hical clustering mo dels have been ex-plored for the image annotation problem, the hierarc hies were statistically deriv ed from image clusters. This pap er presen ts a metho d for generating visual vocabularies based on the seman tics of the annotation words and their hier-archical organization in an ontology like WordNet. The seman tically-motiv ated K-means clustering for generating the blobs impro ved the performance of the translation mo d-els for image annotation.

In the con text of classi cation approac hes to image an-notation, the hierarc hy deriv ed from WordNet is used as a framew ork to generate classi cation mo dels de ned on blobs. While both classi cation metho ds performed better than translation mo del in our exp erimen ts, the hierarc hical classi cation pro vides signi can t impro vemen ts under dif-feren t settings of the visual vocabulary . The represen tation used in the hierarc hical classi cation approac h also yields well to de ning multimedia ontologies by extending a text ontology like WordNet. Mo dels for detecting concept occur-rences in images enable the de nition 'visual surface forms' for a multimedia ontology , and visual glosses pro vide train-ing data for creating these mo dels.
 Our exp erimen ts and observ ations are based on the Corel Data set. Captions and automatic speech recognition tran-scripts pro vide more linguistic clues than the keyw ord anno-tation for images in the Corel Data set. We intend to verify the impro vemen ts obtained using the prop osed metho ds on a larger and more challenging data set like TREC Video dataset [21].

Image regions were iden ti ed using N-cuts algorithm in our exp erimen ts. Grid based image regions have been ex-plored as image surrogates for image annotation [8]. Either metho ds for segmen ting images capture di eren t asp ects of an image. Metho ds that com bine both of these represen ta-tions can be explored for image annotation. While we have explored only the hypern ym y relations in WordNet other relations between concepts can be explored to capture suc h correlations between blobs.

We have prop osed metho ds to use ontologies in the pre-pro cessing stage to de ne the visual vocabularies for images as well as generating hierarc hical mo dels for automatic an-notation. Ontologies can also be used to de ne a con textual mo del at the end of automatic annotation to disam biguate the annotations words assigned for a given image. [1] K. Barnard, P. Duygulu, and D. A. Forsyth. Mo deling [2] K. Barnard and D. A. Forsyth. Learning the Seman tics [3] D. Blei and M. Jordan. Mo deling Annotated Data. In [4] P. Bro wn, S. A. Della Pietra, V. J. Della Pietra, and [5] C. Cusano, G. Cio cca, and R. Schettini. Image [6] A. P. Dempster, N. M. Laird, and D. B. Rubin.
 [7] P. Duygulu, K. Barnard, J. F. G. de Freitas, and [8] S. L. Feng, R. Manmatha, and V. Lavrenk o. Multiple [9] T. Ho mann and J. Puzic ha. Statistical Mo dels for [10] J. Jeon, V. Lavrenk o, and R. Manmatha. Automatic [11] R. Jin, J. Y. Chai, and L. Si. E ectiv e Automatic [12] F. Kang, R. Jin, and J. Y. Chai. Regularizing [13] V. Lavrenk o, R. Manmatha, and J. Jeon. A Mo del for [14] J. Li and J. Z. Wang. Automatic Linguistic Indexing [15] A. K. McCallum, R. Rosenfeld, T. M. Mitc hell, and [16] D. Moldo van and A. Novisc hi. Lexical Chains for [17] Y. Mori, H. Takahashi, and R. Oka. Image-to-W ord [18] J. Shi and J. Malik. Normalized Cuts and Image [19] A. F. Smeaton and I. Quigley . Exp erimen ts on Using [20] Text retriev al conference. http://trec.nist.gov . [21] Trec video data set.
