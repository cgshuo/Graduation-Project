 One-class learning has been proposed to handle the case where only one class of target class . In some real-world applications, such as anomaly detection [5,21], it is easy to obtain one class of normal data, whereas collecting and labeling abnormal instances may be expensive or impossible. To date, one-class learning has been found in a large variety of applications from anomaly detection [5], automatic image annotation [11], to sensor data drift detection [18].
The previous one-class learning can be classified into two broad categories: (1) the methods for one-class learning with unlabeled data [13,12,25,4,24], in which they first extracts negative examples from the unlabeled data, and then constructs a binary classifier based on th e labeled target class and the extracted negative class. For example, the method in [25] first uses a 1-DNF technique to extract negative documents and utilize SVM to iteratively build a binary classifier. (2) the method for one-class learning without unlabeled data [17,14], in which one-class SVM first maps the target data into a feature space and then constructs a hyper-plane to separate the target class and the origin of the feature class or non-target class.

Despite much progress on the one-class learning, most of the previous work considers the one-class learning as a si ngle learning task. However, in many real-world applications, we expect to reduce the labeling effort of a new task (referred to as target task) by transferring knowledge from the related task (source task), which is called transfer learning [15]. For example, we may have plenty of user X  X  previously labeled documents, which indicate the users X  inter-est; as time goes on, user X  X  interest may gradually drift; however, we may not have too much user X  X  currently labeled documents, since labeling plenty of doc-uments timely may be impossible for the user. Therefore, we expect the user X  X  previously labeled documents can transfer knowledge to help build an one-class classifier for the target task. Another important observation is that, collected data in many real-world applications is uncertain in nature [2]. This is because data collection methodologies are only able to capture a certain level of infor-mation, making the extracted data incomplete or inaccurate [2]. For example, in environmental monitoring applications, sensor networks typically generate a large amount of uncertain data because o f instrument errors, limited accuracy or noise-prone wireless transmission [2]. Therefore, it is necessary to develop the one-class transfer learning method f or uncertain data, and build an accurate classifier by transferring knowledge from the source task to the target task for prediction.

This paper addresses the problem of one-class transfer learning with uncer-tain data. To build an one-class transfer learning classifier for uncertain data, we have two challenges. The first one is to formulate data of uncertainty and transfer learning into the one-class learning. The second is to solve the formu-lated optimization to build an one-class classifier for the target task. To handle the above challenge, we propose a novel approach, called uncertain one-class transfer learning with SVM (UOCT-SVM), which incorporates data uncertainty and knowledge transfer into one-class SVM and provides an efficient framework to build an one-class classifier for the target task. The contribution of our work can be summarized as follows. 1. We incorporate the transfer learning and uncertain data into the one-class 2. We propose the usage of an iterative framework to mitigate the effect of 3. We conduct extensive experiments to evaluate the performance of our UOCT-Section 2 discusses the related work. Section 3 introduces the preliminaries. Sec-tion 4 presents our proposed approach. Section 5 reports experimental results. Section 6 concludes the paper and future work. In this section, we briefly review previous work related to our study. 2.1 Mining Uncertain Data In data collection, some records in the data might be degraded due to noise, precision of equipment, and are considered uncertain in their representation [2]. We briefly review the previous work on uncertain data as follows.

For the clustering and classification methods with uncertain data, they de-velop on the clustering and classification methods. FOPTICS [9] introduces a fuzzy distance function to measure the similarity between uncertain data on top of the hierarchical density-based clustering algorithm. The method in [8] studies the problem of clustering uncertain obj ects whose locations are described by probability density functions to cluster uncertain data. In addition, binary SVM is extended to handle uncertain data [7] to provide a geometric algorithm. 2.2 Transfer Learning In transfer learning [15], the knowledge is expected to transfer from a source task into the learning of target task such that the transferred knowledge can benefit the learned classifier for the target task. We briefly review some of them as follows.
 The work in [10] assumes the distribution of target and source tasks fit the Gaussian process. However, it assumes the distribution of the data to be specified as a priori, which makes them inapplicable to many real-world applications. Other algorithms such as [16] assume that some instances or features can be used as a bridge for knowledge transfer.

Multi-task learning [20] is closely related to transfer learning. In multi-task learning, several tasks are learned simultaneously. In contrast to multi-task learn-ing, transfer learning focuses on transferring knowledge from the source task to the target task, rather than ensuring the performance of each task. 3.1 One-Class SVM one-class SVM, input data is mapped from the input space into a feature space and the inner product of two vectors  X  ( x )and  X  ( x i ) can be calculated by a hyperplane to separate the target class and the origin of the space: where w is vector, parameter C is used to tradeoff the sphere volume and the errors. After solve problem (1) and obtain w and  X  = w  X   X  ( x ). For a test sample x t ,if w the non-target class.

In this paper, we extend the standard one-class SVM for one-class transfer learning with data of uncertainty. 3.2 Uncertain Model For the labeled target class, we assume each input data x i is subject to an additive noise vector ' x i . In this case, the original uncorrupted input x s i is denoted x s i = x i + ' x i . We can assume ' x i follows a given distribution. The method of bounded and ellipsoidal uncertainties has been investigated in [6,14]. In this situation, we consider a simple bound score for each instance such that ' x
We then let x i + ' x i ( ' x i  X   X  i )denotethe reachability area of instance x i as illustrated in Figure 1. (A). We then have
In this way, x s i falls in the reachability area of x i . By using the bound score for each input sample, we can convert the uncertain one-class transfer learning into standard one-class learning with constraints. In this section, we put forward our one-class transfer learning to handle uncertain data. Suppose we have two tasks, that is, to train one-class classifier on S s for source task and on S t for target task. Let where w 1 and w 2 are parameters of the one-class SVM for source and target tasks, respectively. w o is a common parameter while  X  1 and  X  2 are specific parameters. Here, w o can be considered as a bridge to transfer knowledge from source task 1 , 2 and the extended version of one-clas s transfer learning for uncertain data can be written as follows. For the above optimization, we then have: 2 We utilize common parameter w o as a bridge to transfer knowledge from 4.1 Solution to Uncertain One-Class Transfer Learning Classifier As the above optimization problem (4) is far more complicated than the stan-dard one-class SVM, we will use an iterative approach to calculate  X  1 ,  X  2 , ' x i and ' x j such that we can obtain the one-class transfer learning classifier for uncertain data. The iterative steps ca n be summarized as follows. (a): fix each ' x i and ' x j to solve the problem (4) to obtain  X  1 and  X  2 ;(b):fixtheobtained  X  1 and  X  2 to calculate steps as follows. (We omit the detailed derivation of the Theorems in this section due to space limitation) Calculation of Classifier by Fixing x i and x j . First of all, we fix each ' x i and ' x j as small values such that ' x i &lt; X  i , ' x j &lt; X  j 1 . Based on this, the constrains ' x i &lt; X  i , ' x j &lt; X  j in problem (4) won X  X  have effect on the solution. Then, problem (4) is equivalent to We then have the following Theorem.
 Theorem 1 : By using Lagrangian function [22], the solution of the optimization problem (5) is to solve the following dual problem in which and  X  2 =( w 0 + v 2 )  X  x j .
 Calculation of x i and x j by Fixing the Classifier. After setting ' x i and ' x j as a small values which are less than  X  i and  X  j respectively, and solving optimization problem (6), we obtain  X  1 and  X  2 . The next step is to use the obtained  X  1 and  X  2 to calculate new ' x i and ' x j . We then have Theorem 2 as follows.
 Theorem 2 : If the hyperplanes for the source and target tasks are denoted as  X  1 =( w 0 + v 1 ) and x j are This Theorem indicates that, for a given  X  1 and  X  2 , the minimization of problem (4) over ' x i and ' x j is quite straightforward.

After that, we have one round of alternation and continue to update  X  1 ,  X  2 , ' x i , ' x 2 until the algorithm converges. Algorithm 1. Uncertain one-class transfer l earning with uncertain data
To utilize Theorem 1 and Theorem 2 iteratively to calculate  X  and ' x ,we have Theorem 3 as follows.
 solution of problem (4) is equivalent to optimization problem (6).
 ' x i  X   X  i in problem (4) won X  X  have any effect on problem (4). The same analysis Iterative Framework. So far, we have introduced the framework to update  X  ,  X  2 , ' x i and ' x j at a round, and we can use the above steps to obtain an uncertain one-class transfer learning classifier. By referring to the alternating optimization method in [6], we propose the usage of the iterative approach to solve problem (4) in Algorithm 1.

In Algorithm 1,  X  is a threshold. Since the value of F val ( t ) is nonnegative, than a threshold. Thus, Algorithm 1 can converge in finite steps.

After that, we obtain the uncertain one-class transfer learning classifiers for the target task. We then utilize the learned classifier for prediction.
Note :(1): For the determination of  X  i for the sample x i in S s ,wecalculate the average distance of x i between it and the its k  X  nearest neighbors. The same operation is utilized to the sample x j in S t . This setting is previously utilized in the previous work [14]. At the beginning of the framework, we initialize each ' x i =0, ' x j = 0 and update them base on (7) and (8). Then, we can have ' x i  X   X  i and ' x j  X   X  j . (2): Above, we present the formulation of uncertain one-class transfer learning in the input space; while for the kernel space, we can utilize K ( x , q )=  X  ( x )  X   X  ( q ) in the above formulation. 5.1 Baseline and Metrics In this section, we investigate the performance of our proposed UOCT-SVM method. In transfer learning, we expect the transferred knowledge from the source task to the target task can improve the performance of the classifier built on the target task. For comparison, another two methods are used as baselines. 1. The first method is the standard one-class SVM (OC-SVM), which deter-2. The second baseline is the uncertain on e-class SVM (UOC-SVM) [14], which The performance of classification syste ms is typically evaluated in terms of F-measure [23], we use it as metrics . The F measure trades off precision p and recall r : F =2 p  X  r/ ( p + r ). From the definition, we know only when both precision and recall are large, will the F-measure exhibit large value. 5.2 Dataset and Experiment Setting One-Class Learning Data. To evaluate the properties of our approach, we conduct experiments on 20 Newsgroups 2 and Reuters-21578 3 . Both data sets have hierarchical structures. The 20 Newsgroups corpus contains several top categories, and under the top categories, there are 20 sub-categories where each subcategory has 1000 samples. Similarly, Reuters-21578 contains Reuters news wire articles organized into five top categories, and each category includes dif-ferent sub-categories.

Following the previous work [17,19] for one-class learning, we reorganize the original data in a way for the one-class transfer learning problem as follows. For the 20 Newsgroups, we consider one sub-category as target class, and se-lect a number of example from other categories as non-target class. Specifically, we first choose a sub-category ( a 1 ) from a top category (A), and consider this sub-category ( a 1 ) as the target class and consid er the examples from other top categories, i.e., except for category (A) as non-target class. Based on the this, we generate target class and non-target class for the source task. For the target task, we choose a sub-category ( a 2 ) from the same top category (A) as that for the source task, and consider this sub-category ( a 2 ) as the target class; while take the examples from other top categories except for (A) as non-target class.
For the Reuters-21578, each top category has many sub-categories, for ex-ample,  X  X eople X  has 267 sub-categories and the size of each sub-category is not always large. We organize it as follows. For a top category (A), all of the subcat-egories are organized into two parts (denoted as a (1) and a (2)), and each part is approximately equal in size. We then regard a (1) and a (2)) as the target class of the source task and target task respect ively. We also consider the examples from the other category except for (A) as the non-target class for the source and target task respectively.

In the above operations, we generate target class from the same top category (A), that are a (1) and a (2), for the source task and target task, this is because we should guarantee the two tasks are related. Otherwise, the transfer learning may not, and may even hurt, the performance of a target task, which can be referred to as negative transfer [3].
 Uncertain Information Generation. We note that the above data are de-terministic, so we need to model and involve uncertainty to these data sets. Following the method in the previous work [1], we generate the uncertain data as follows.

For generate data, we first compute the standard deviation  X  0 i of the entire data along the i th dimension, and then obtain the standard deviation of the add noise from a random distribution with standard deviation  X  i .Thus,adata denotes the number of dimensions for a data example x j ,and  X  x j i , i =1 ,  X  X  X  r represents the noise added into the i th dimension of the data example. Fig. 1 (B) illustrates the basic idea of the method.
 is used in the experiment since it is the most common kernel function. The  X  in RBF kernel function is ranged from 2  X  10 to 2 10 .Inourmethod, C 1 and C 2 control the tradeoff between the source task and target task. Since we care is chosen from 1 to 1000. For the k -nearest neighbors to g enerate bound score, we set k equal to ten percent of the tr aining target class. We set  X  is set as 0.15 in the experiment. All the experiments a re on a laptop with a 2.8 GHz processor and 3GB DRAM.
 Performance Comparison. For the target data set, we randomly choose around 20% to form a training set while the remaining example are used for testing. This is because transfer learning always assumes we do not have suffi-cient training data for the target task. We also conduct 10-fold cross validation on the test set. For the source data, sinc e we are more concerned about the per-formance of the target task we incorporate around 80% them into training and the remaining are used for testing. To avoid a sampling bias, we repeat the above process 10 times, and report the average f-measure accuracy and the standard deviations in Table 1, in which we set the noise level at 0 . 4.

It can be seen that, our proposed UOCT-SVM method always provides a su-perior performance compared with UOC-SVM. Although both UOCT-SVM and UOC-SVM can handle data of uncertainty, our method can transfer knowledge from the source task to the target task such that we can develop an accurate classifier for the target task. In addition, both UOCT-SVM and UOC-SVM per-form much better than the standard OC-SVM, this occurs because UOCT-SVM and UOC-SVM reduce the effect of the noise on the decision boundary; as re-sults, they can deliver better one-class classifier compared with the standard OC-SVM. In addition, we find the standard deviation of our method is less than the UOC-SVM and OC-SVM for most data sets.
 Performance on Different Noise Levels. We investigate the performance sensitivity of three methods on different noise level from 0 . 4 to 1. In Fig. 2, we illustrate the variation in effectiveness with increasing noise error. On the x  X  axis, we illustrate the noise level. On the y  X  axis, we illustrate the average f-measure value. It is clear that in each case, the f-measure value reduces with the increasing noise level. This occurs becaus e when the level of noise increases, the target class potentially becomes less distinguishable from the non-target class. However, we can clearly see that, UOCT-SVM approach can still consistently yield higher f-measure value than OC-SVM and UOC-SVM. This indicates that, UOCL method can reduce the effect of no ise. In addition, UOC-SVM performs better than OC-SVM since UOC-SVM can reduce the effect of noise on the decision classifier.
 Average Running Time Comparison. So far, we have investigated the performance of the three methods, it is interesting to compare the running time of them. The average running time of OC-SVM, UOC-SVM and UOCT-SVM are 1553, 4763 and 6980 seconds respectively. We find that the standard one-class SVM performs much faster than UOC-SVM and UOCT-SVM since OC-SVM does not consider the data of uncertainty and transfer knowledge in the learning; as a result, it performs fa ster while has the lowest accuracy com-pared with UOC-SVM and UOCT-SVM. In addition, UOC-SVM proceeds faster than UOCT-SVM, since the latter one transfers knowledge from the source task to the target task to benefit the classifier for target task, which takes time to fulfill the knowledge transfer. This paper proposes a novel approach, called UOCT-SVM, for one-class transfer learning with uncertain data. Our proposed UOCT-SVM first formulates the uncertain data and transfer learning to the one-class SVM learning as an op-timization problem, and then puts forward an efficient framework to solve the optimization problem such that we can obtain an accurate classifier for the target task by transferring knowledge from the source task to the target task. Extensive experiment has investigated the per formance of our proposed UOCT-SVM.
In the future, we would like to investigate how to design better methods to generate bound scores based on the data characteristics in a given application domain.
 Acknowledgements. This work is supported in part by US NSF through grants IIS-0905215, CNS-1115234, IIS-0914934, DBI-0960443, and OISE-1129076, US Department of Army through grant W911NF-12-1-0066, Google Mobile 2014 Program and KAU grant, Na tural Science Foundation of China (61070033, 61203280, 61202270), Natural Scienc e Foundation of Guangdong province (9251009001000005, S2011040004187, S2012040007078), Specialized Research Fund for the Doctoral Program of Higher Education (20124420120004), Australian Research Council Discovery Grant (DP1096218, DP130102691) and ARC Linkage Grant (LP100200774, LP120100566).

