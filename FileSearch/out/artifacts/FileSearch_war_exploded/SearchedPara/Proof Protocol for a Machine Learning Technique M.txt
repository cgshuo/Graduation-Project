 We demonstrate a protocol for proving strongly that a black-box machine learning technique robustly predicts the future in dynamic, indefinite contexts. We propose n ecessary components of the proof protocol and demonstrate results visualizations to support evaluation of the proof components. Components include contemporaneously verifiable discrete predictions, deterministic computability of longitudinal predictions, imposition of realistic costs and domain constraints, exposure to diverse contexts, benchmarks and Monte Carlo trials, insignificant decay of excess benefits, pathology detection and an extended real-time trial  X  X n the wild. X  We apply the protocol to a big data machine learning technique deployed since 2011 that finds persistent, exploitable opportunities in many of 41 segments of US financial markets, the existence of which opportunities substantially contradict the Efficient Market Hypothesis. H.3.4 [Information Storage and Retrieval]: Systems and Software -Performance evaluation (efficiency and effectiveness ) I.2.6 [Artificial Intelligence]: -Learning -Concept learning I.6.4 [Simulation and Modeling]: Model validation and Analysis Experiment design, hypothesis testing, black box, big data, longitudinal study, dynamic context, simulation, pattern analysis, market forecast, prediction.  X  X e think it very interesting to teach machines to understand behaviors that humans cannot describe. But, how do we know they understood? X  How does one prove that a frequently erring method -applied in a constantly evolving environment  X  is authentic, robust and valuable? Especially where task of proof is aggravated because the method itself is evolving in response to the changing environment and lives hidden inside a  X  X roprietary black box X ? This question might apply to behaviors of a live anti-viral flu vaccine, to methods of a stock pri ces prediction engine, to robotic vehicles navigating in a city undergoing wartime destruction or to an ad placement  X  X ptimization X  system for a multi-national, multi-channel, competitive consumer prod uct. In a broader sense, how does one persuasively answer the skeptic X  X  question of the value of big data analytics in the real and changing world where tomorrow will be different in unknowable ways -and where humans are incapable of doing the data learning and analysis that is being conducted within the  X  X lack box X ? In 2010, we set out to prove that a machine learning technique which we were deploying could successfully, repeatedly find exploitable future stock market inefficiencies at scale. Our goal was to prove that technique X  X  existence and efficacy beyond a reasonable doubt. Part of our task was defining a strong proof protocol. The proof must be strong because the existence and efficacy of such a technique contradicts the Efficient Market Hypothesis 21 for which the Nobel Prize was awarded in 2013. The learning technique makes early morning recommendations to buy a small, fixed size set of stocks at a price equal to the opening price (whatever it may be) today and to sell those same stocks one week later at the opening price then. It uses input data as of market close the prior day. It thus must work with two great uncertainties (latencies): it does not consider events occurring overnight when making its morning recommendation, and it does not revise recommendations, once made, in view of events in the coming week that affect the market before resu lts of the recommendations (sales) are recorded. Our null hypothesis is that such a technique will produce nothing statistically significant in any market segment. This article focuses on proof protocol and associated big data behavior visualizations applicable to dynamic contexts, rather than substantive results, which results are presented in another (pending) journal article. Proof protocols and techniques for evaluating data mining validity in stationary contexts have been well developed. N-fold cross validation with holdout test sets, confusion matrices, f-measure weightings and Receiver Operating Curves (ROC) are widely applied and well understood [6]. Statistical dispersion measures for The author is also Senior Analytics Scientist at Teradata Corp. The work described here has no association whatsoever with Teradata Corp. or the author X  X  employment there.  X  X n the 1960s, Eugene Fama dem onstrated that stock price movements are impossible to pred ict in the short-term... X  http://www.nobelprize.org /nobel_prizes/economic-sciences/laureates/ 2013/fama-facts.html measurement data, and goodness of f it to parametric descriptions have a long history. Evaluating big data techniques in dynamic contexts where no situation repeats requires new approaches [3,14]. Data distributions and object interconnectivity change constantly, and assumptions that distributions are  X  X ormal X  und er any parametric description are infrequently valid. Big data analytics feels like navigating mid-ocean on a stormy night with unreliable instruments. Nonetheless, despite the constant changing contexts, we suggest that strict scientific method be applied to avoid squishy evaluation using  X  X leasing anecdotes. X  While longitudinal experimentation is not new [24,26,27] and is applied in a variety of domains [1,2 ,8,12,19], big data is causing re-examination of the methodologies [5,22,25]. Proving validity of prediction in longitudinal, constantly changing problem spaces suffers from three special challenges. i) Data available today about prior inputs and prior outcomes may have been cleansed (sometimes almost invisibly) by hindsight. An obvious example is survivor bias during  X  X ack-testing X  in finance [9,13]. A less obvious example we observed is that publis hed data such as stock trading data is silently revised by vendors in later publications when  X  X inal numbers X  are received. ii) More insidious is the hindsight technique of publishing only favorable results from a panel of  X  X qually well-conceived X  trials that had mixed re sults. This variety of hindsight filtering has been used intentionally in some stock scams, and sometimes naively in drug research where side-effects in some cohorts are dismissed [7,17]. iii) Longitudinal interim results can compound in dynamic feedback systems, whether as part of the contextual environment or as part of the prediction technique. What seems well behaved in early episodes may fluctuate or become brittle -diverging or decaying over time. A well-defined protocol for provi ng capability of machine learning in longitudinal, dynamic contexts supports user trust in today X  X  black-box, proprietary technologies [10,18,21,23]. It is also needed for valuation of such technologies for capital investment. We suggest it is critical to define how success will be measured in advance of an experiment with a learning system, and how to assure the experiment is authentic and sufficient [20,22]. A prediction must be specific. The set of cohorts for which predictions will be made must be defined in advance. The deadline or trigger for each prediction must be established. In order to validate these predictions, all should be consolidated, unalterably archived and time-stamped. In later evaluation of a learning technique, only those predictions meeting this criteria should be included, otherwise one risks  X  X evise as we go X  hindsight. For the ZZAlpha machine learning technique, we defined (in 2010) 41 market segments for which we would produce long and short recommendations before the market opened every day. We also defined 4 size sets for each segment. We aggregated these 320 daily predictions and submitted them to encryption and electronic time-stamping by Digistamp [4] early each morning beginning in late 2011. Digistamp is a trusted third party time stamp authority using an encryption algorithm that does not require deposit of documents with them. Rather a public certific ate is provided for unlocking and verifying the date-time of the document. A timestamp costs $0.40 and is commonly used for intellectual property protection. The learning engine makes predictions for recommendations that are discrete and actionable: buy (or sell) a set of stocks at prescribed prices today. In longitudinal studies with asynchronous triggering events, there is context given the resource impact of the sequence of prior decisions. In a stock investment context, that means determining how much cash is now available for purchases after closing prior investments. In an advertising campaign that might mean how to allocate remaining spend after detecting cohort responses to prior channel impressions. In a dynamic real-life context, acting on every prediction causes resource adjustment, and there must be ongoing resource accounting with hard numbers. For the machine learning evaluation, we created a strict cash flow trading performance evaluator (detailed below) based on a typical tax-free retirement account where margin (borrowing) is prohibited. Thus, a sale must be closed at a defined price with dollar proceeds added to the  X  X ot X  before a following purchase can be made. As part of the resource accounting identified above, there must be imposed realistic transaction costs and context constraints. For example, in a stock trading context, commissions are paid at the time of trade. Short trades cannot be made when brokerages do not have short shares to lend. Institu tional investors may be prohibited by self-imposed investment guidelines from trading low priced stocks. These costs and constraints need to be applied during the go-forward accounting of prediction results. Identification of these costs and constraints typically re quires consultation with industry subject matter experts and careful modeling. In an advertising campaign this might include lead times, placement premiums, and commission levels. For the evaluation here, we applied trading commissions, price, liquidity and slippage constraints. To be deemed robust, a longitudinal technique must survive or thrive when exposed to the unknown -where those unknowns may arise in the future. A general technique, for example a drug therapy, must apply successfully to diverse cohorts and cohort specific stressors, for example cohorts receiving other treatments. Cohorts must be defined in advance of the experiment. If future contextual events will be  X  X xcused X , those pe rmissible excuses must also be defined in the proof protocol. For the learning technique experiment here, we identified 41 cohorts spanning diverse market segments, and allowed any trading day as an acceptable context. Impact ful events such as the fall of gold prices, Chinese economy slowing, credit card frauds, oil fracking, and very low interest rates were some of the unexpected future unknowns to which the learning technique had to respond, for all cohorts. In experiment design, the comparison benchmarks must be designated in advance [11]. They need to be appropriate to the cohort, and need to be measurable over the duration of the experiment. Thus, a traditional  X  X tatus quo X  strawman is never available in a dynamic context. There are often hindsight incentives to change to lesser benchmarks so that benefits can be announced from the method being attempted. In the stock domain, Exchange Traded funds indexed to a sector often offer appropriate benchmarks. Stock indexes are less persuasive benchmarks because they are generally capitalization weighted and exclude the compounding benefits of dividends. Monte Carlo evaluation methods can apply in dynamic contexts, and can be applied to the varying cohorts [23]. Fundamentally each decision of the method being evaluated is replaced with a random selection from the decision set. Evaluation of Monte Carlo results provides both a distributional representation of outcomes, and a sense of whether the chosen dispersion method, such as standard deviation, is appropriate. Experiment design also requires advance specification of how improvement is to be measured and what value constitutes success. At ZZAlpha we selected an ETF or market index for comparison to each recommendation segment, and built a Monte Carlo trial option into the longitudinal performance evaluator. We chose two measures of success. With respect to a market benchmark, the machine learning method, after expenses, must exceed the annualized returns of the benchmark by 200 basis points. With respect to the Monte Carlo trials, the machine learning method must provide results more than 3 standard deviations (z-score) above the segment mean. In stationary contexts, over-fitting causes brittleness that reduces accuracy when applied to the test sample. In longitudinal dynamic contexts a parallel problem can arise when the method is too strongly tuned to the training episodes [16]. Although one expects a learning method will vary in th e success of its predictions as contexts change, the longer term results should not decay. Historic market prices fell 40% or more in 2008 in most stock market segments, but rose again. Some segments have lower returns for several years. The three years of certified recommendations here do not support evaluation of decay. The 10 year simulated recommendations using the machine learning technique in forward-testing mode are the best available data to respond to decay concerns. We use a requirement of positive regression slope and sp ecifically a more positive slope than that of benchmarks. Black box techniques in dynamic contexts can exhibit pathological behaviors. In the market price prediction domain we needed to detect and evaluate three: Does the technique repeat or cycle prior predictions? Do individual recommen dations within a daily set have high risk (large losses) or are there extended episodes of underperformance? Does the technique only detect opportunities of either price increase (long) or price decrease (short)? For evaluation of these possible pathologies, we used a quarterly periodicity visualization, a min-ma x timeline envelope, z-scores of both long and short recommendations, and descriptive statistics of the severity of below regression line risk. To support the elements of the proof protocol, we prepared over 25 visualizations for each market segm ent, some of which are included below. These were to confirm the expected and expose the unexpected, to compare to benchmarks, and to support cross-segment interpretation and pathology identification. Simulations can inadvertently omit costs and constraints that occur when participating in dynamic cont exts. We suggest it is critical to obtain some portion of ground truth to corroborate lab evaluation techniques, and to uncover e xperiment misconceptions. We used daily a few of the machine learning recommendations in the most widely analyzed segment -the stocks in the S&amp;P 500 list -to control actual trading over 15 months with a pot of about $500,000. This parallels a limited clinical trial in the pharmaceutical domain or  X  X ack-road X  testing of driverless vehicles. We originated a machine learning technique and tuned that technique in 2009-2010 using portions of 2008 data. We began operating the machine learning technique in 2010 to make daily stock recommendations. The technique continues in autonomous operation today, situated on a large cloud platform. It has no tuning knobs. To protect proprietary intellectual property, we hide the inner details of the technique. However, in overview, it ingests data, applies transforms, derives features, considers past market dynamics, applies common machine learning technique(s), and predicts stocks likely to rise or fall in price (categorized within 41 market segments Table. 1). 32 Inputs: The technique consumes 5 types of public data: i) end-of-stock fundamentals updated weekly , iii) Securities and Exchange Commission (SEC) filings on initial public offerings, iv) daily information on merger and acquis ition announcements and v) semi-annual data needed to include or exclude stocks in lists of the 41 sub-sector of Banks (defined by SIC). All of the data consumed is available on the internet for less than $200 a year in subscription fees. It is also available more quickly from commercial suppliers such as CapitalIQ. Outputs: The technique outputs daily newsletters on 320 recommendation portfolios, and most importantly a complete, consolidated recommendation list that is encrypted and contemporaneously certified by Digistamp before market open and then archived. The certifications began Nov. 2011. In 2011 we simulated the machine learning technique to produce 2005-2009 recommendations to confirm its robustness. That simulation involved feeding the machine learner the weekly 
This paper does not intend to emphasize the stock recommendation system, but rather to use such a system as an example to focus on the hard , ethically ch allenging and increasingly important question of  X  X ow would one decide X  if such a system, and similar systems which must contend with unknown dynamic contexts, would be likely robust and effective. fundamentals data as originally published at that historic week and providing historic information (including delisted or renamed stocks) of prices and volume (which was split and dividend corrected) as of 2011. The machine learner then issued daily recommendations for the 320 recommendation portfolios day-by-day in a consolidated daily document. This five year simulation took over 9 months of computation. The goal of the historic simulation (combined with the subsequent go-forward predictions) was to help evaluate possible decay of results over the longer term, and to evaluate the resiliency of the machine learning technique in th e extreme contexts of 2007-2008. Table 1. Segments (cohorts) spanning the US market for predictions . All portfolios are produced for Long and Short positions, and in daily recommendation sizes of 2, 5, 10 and 20 stocks (with a few exceptions). The evaluator is entirely separate from the machine learning technique. However, it is focused on evaluating the  X  X eekly rolling recommendations X  of the type made by the machine learning technique.
 The evaluator is cash based [Fig. 1]. It takes a list of k stocks to buy on a day and consumes the available portion of cash for that day to make equal dollar valued (1/k) purchases of the stocks. After n trading days it sells those stocks and contributes the proceeds back to cash. It then repeats. Cash can never be negative. All purchases and sales are made at a price equivalent to the opening price that day. Commissions (either fixed dollar or per share) are applied at each transaction. We add several constraints to the evaluator: The  X  X ot X  of cash is initialized, typically set to $1 million. The pot is initially divided into n portions. On each day, one of the portions is available for purchases. After each day the portions are rebalanced to remain approximately equal. A minimum pu rchase block size is set to 10. All transactions are  X  X imit order,  X  which prevents price slippage during trades. Some cash tracking details: If a stock appears in the new  X  X uy X  list does not contribute to the cash portion on the originally expected day of  X  X ale X . If for any reason a stock does not have an opening price on the day of purchase, the stock is not purchased and funds remain in cash. When a stock is not purchased, the cash allocated for that purchase remains unspen t in the current portion and becomes again available n days later. The evaluator can also track tax gains and payments. The evaluator also allows selection of a benchmark, such as the Dow Jones Industrial Index or an S&amp;P500 tracking index fund (for comparison of index fund long term tax treatment  X  X uy and hold X ). The evaluator determines results of long positions. It does not evaluate short positions because i) some stocks never or seldom impossible to determine whether on a particular day a particular stock has short shares available from a particular brokerage for loan to a particular investor, and iii) regulations sometimes only allow short on uptick. However the evaluator does support a computational concept of  X  X nti-long X . Given a list of stocks that are predicted to decline in price (typically used in selecting shorts), it will indicate what results would be if that list had inadvertently been treated as long recommendations. The recommendation performance evaluator operates in two modes: Recommendation mode applies the results of the certified ZZAlpha X  portfolio recommendations (for a specific portfolio in a market segment and of a specific size). Those results have been historically recorded about 3 days after sale of a position, showing the contemporaneous purchase price and the sale price and relative result (without assuming a quantity of shares purchased). The purchase price is adjusted for dividends and splits to keep it consistent with the sale price. On a few occasions the record is incomplete, typically because of a data provider omission, because the stock price dropped below $3 or trading volume fell below 80,000 shares. Any missing records are treated as  X  X ot bought X  with recommendation mode. Random selection mode operates exactly the same as recommendation mode with two changes. First, instead of a recommendation stock, a stock is selected from the same portfolio market segment at random (using the Java cryptographic random number generator). Second, the purchase and sale prices are obtained from current and archive listings of daily stock prices, adjusted for dividends and splits. On a few occasions the current price record is incomplete, typically because of a data provider omission, because the stock pr ice dropped below $3, because trading volume fell below 80,000 shares or because of delisting. Share price increases due to mergers and acquisitions are typically present in the records. Any missi ng records are treated as  X  X ot bought X  with no gain or loss. There are more missing records in random selection mode than in recommendation mode, but seldom more than 10 missing records in a year. Missing records are logged during the random selection mode. The effect of switching from recommendation mode to random selection mode is to substitute a random choice from the relevant market segment for a machine learning generated recommendation. Thus, operation of the performance evaluator over a period of time (three years in this report) in random selection mode is a Monte Carlo simulation trial, using exactly the same assumptions, constraints, costs and accounting as used in the recommendation mode evaluation. All Monte Carlo trials are repeated 1000 to generate reliable estimates. We applied the protocol to the machine learning technique's recommendations. We show visualizations for S1-Basic Industries, one of the market segments, for five recommendations each day. Contemporaneously verifiable discrete predictions -The daily consolidated recommendations for the 320 set are unalterably time stamped by Digistamp prior to market open. Each recommendation set lists e.g. five stocks to buy at the opening price, which are to be held one week and sold. Deterministic computability of repetitive longitudinal application of predictions -We use the recommendation Performance Evaluator described in Fig 1 to obtain deterministic results. Imposition of realistic costs and context constraints during evaluation -In addition to the constraints expressed in the Performance Evaluator, we apply three additional constraints to assure that the recommendations pertain to stocks which institutional investors can purchase under regulatory requirements: $3 recent minimum price, 80,000 daily share volume, and listed on the major US stock exchanges. Exposure to diverse contexts -The 41 market segments span the US equities market. Each segment suffers somewhat individual stressors, such as falling oil prices or semiconductor innovations. The three years of certified recommendation do not include an episode of price drop of over 10%, and thus lack that diversity. The ten year simulation attempts to overcome that by including the 2007-2008 episode of price drop. Fig. 2 shows the ten years of annual returns. Figure 2. Annual results and benchmark for ten years, showing regression lines used for decay evaluation. Because the recommendation slope of 0.79 is less than the benchmark slope, this suggests the quality of recommendation is declining slightly. Statistically significant excess benefits relative to a priori benchmarks and Monte Carlo trials -We use z-score to measure significance. Figures 3, 4 and 5 show results for one of the segments -Sector 1-Basic Materials. Both the long and anti-long results have z-scores greater than 3, indicating great significance. In another article we detail that of the 41 recommendation sets of daily size 5, 5 have z-scores greater than 3, 3 have z-scores between 2 and 3, and 9 have z-scores between 1 and 2. Anti-longs show greater significance. Of the 41 portfolios, 23 exceed both their segment means and their benchmarks by 200 basis points (annualized), and another 2 exceed their segment means and their benchmarks by 100 basis points (annualized). Figure 3. Results compared to benchmarks: 1000 Monte Carlo trials and ETF. Monte Carlo mean and +/-standard deviation shown in black dotted lines. Machine learning recommendation results for long shown in blue line on right, for anti-long in red line on left. Benchmark XLB results indicated by green dashed line. (optional). mode, number of trials). Begin: Divide cash pot into n portions (equal to number of hold days = 5). Purchases must exceed minimum block size. ASSERT: remaining cash is near zero in each of the n portions. Continue until last date: purchase price. On last date: If Monte Carlo mode and trials are not complete, reinitialize pot and Begin. Figure 1. Performance Evaluator pseudo code Figure 4. Comparison of results of each recommendation over 3 year test to benchmark ETF results (for all week-long periods). There is a long right tail on the portfolio recommendations in this Cauchy-Lorentz distribution. The figure shows greater dispersion of results for the portfolio, but more positive area under the curve (in the 3 years, 52% of the recommended trades had positive profit). Figure 5. Cumulative results from recommendations in dark blue and benchmark XLB in thin black. This common timeline graph helps quickly identify correlations and anomalies. Fig. 6 shows persistence of results (both high and low) when comparing average results of individual stocks in the current year with the average results of the same stock in the prior 2 years. The positive correlation and regression slope suggest that a favorable context for a specific stock may repeat. The machine learning technique may be detecting that subtle context within the chaos of the dynamic market. However, this persistence was absent in other segments. Figure 6. For year 2014 in this segment,, the average returns of a recommended stock in the prior two years are somewhat correlated (Pearson r=0.24) with current year returns. The Loess spline regression emphasizes variations in the regression and indicates discontinuance of positive regression in the middle range, which is not shown by ordinary linear regression line. Insignificant decay of excess benefits -We identified two varieties of decay. As the number of daily recommendations increase, the recommendation effectiveness decr eases: Annualized returns (after commissions) for sizes 2, 5, 10 and 20 declined from 23.8 for size 2 to 20.2, 11.8 and 5.8 in the S1-Basic Materials segment. By changing the commission structure constraint from a fixed $8 per trade to $0.05 per share, the returns for size 20 were increased to 8.6. This is a pattern we saw often across the 41 market segments. Second, looking at annual return rates over 10 years (7 years of simulation and 3 years of certified results), we saw decay in results as shown in Fig. 2. Controlled risk and absence of pathologies -A machine learning technique in dynamic contexts will likely change its own behaviors. Visualizations from different perspectives are useful. In addition to traditional risk and drawdown statis tics, we inspected three suspect behaviors. First, we evaluated the daily range of results to identify spikes or longer duration episodes of extreme results Fig. 7. Figure 7. Daily ranges of recommendation results (5 daily recommendations) compared to benchmark. Note that more extreme events were positive, which contributes to rapid compounding of the rolling recommendation results. We found that although greater mean returns occurred at higher capitalization ranges of the segment, Fig. 8, the greatest total profits derived from the lowest capitalizati on stocks Fig 9. In Fig. 10, we saw that variation in returns dropped slowly as capitalization increased. Fig 11 indicated a slow change in capitalizations of selected stocks. We inspected recommendations for abnormal periodicity, Fig 12 and found one possible pathology. A few recommendations recurred 4 times on the same day in the 12 quarters of the 3 years, but no significant overall pattern appears in S1-Basic Materials. To further i nvestigate the possible pathology, we looked at the frequencies of i ndividual stock recommendations in the 3 years, which ranged from 90 to one. The existence of several high frequencies suggested some instances of clustered repetitions would not be abnormal. We also maintain extensive operational logs should the need arise to investigate pathologies. We also investigated how the set of five daily recommendations fit within the rank of all segment members. We expected to see ranked recommendations correlated with ranked results. Use of rank correlation avoids difficulties of comparing absolute results from different contexts, where  X  X ood X  can mean significantly different absolute results. Fig 13 show almost no rank correlation of the machine learning recommendations to ranked results. However we investigated the extremes in the corners of Fig 13. The histogram in Fig 14 discloses a narrow spike of high result rank correlation to high recommendation rank. This exemplifies the  X  X eedle in haystack X  that could be lost using ordinary descriptive statistics or down sampling of big data. Figure 8. Mean returns by capitalization. Bin size = $1B. This example differs from many of other segments where positive returns were more commonly seen in the smaller cap stocks. Figure 9. Profit by capitalization. Low capitalization stocks heavily contribute (because they are more often recommended) to recommendation set returns for S1-Basic Materials. Figure 10. Variation in returns by capitalization. The generally declining variation is as expected. It is one of many  X  X anity checks X  on performance over the 3 years. Figure 11. The recommendations included more large cap elements over time. The pink largest cap cluster at the top may indicate a single stock that eith er issued more shares and/or saw large price increases. Figure 12. Periodic repetition or recommendations across quarters. Small dots indicate 2x in 12 quarters. Largest dots indicate 4x in 12 quarters. The sequence of large dots at the top middle suggest a possible autocorrelation pathology for examination. Figure 13. Rank correlation permits comparison across dynamic contexts. For each day, both predictions and results within a segment are ranked. An optimal correlation of result rank to recommendation rank would be a thin scatter running from lower left to upper right. Instead this figure shows almost no correlation, but concentrations in corners. This visualization applies in dynamic contexts where correlation of actual returns would be effectively unusable because a  X  X ood X  prediction in an up market might be 5% increase compared to a market-wide 2% increase and a  X  X ood X  prediction in a down market might be a 5% decrease compared to a market-wide 8% decrease. The horizontal and vertical gaps in the center of the figure are artifacts resulting from different sizes of the sample set on different days. To support inspection of the high ranks, both the right and top are aligned for the upper half of the samples. The regression line in the middle of the figure shows the very weak positive slope. The rank correlation patterns seen in the other 40 market segments can vary substantially from this visualization. The recommendation engine is performing differently here than in other segments. Extended duration real-time trial  X  X n the wild X -We used a pot of about $500,000 for about 15 months (Oct 7, 2013-Dec. 31, 2014) to act upon machine learning recommendations (and no others) for stocks in the SP500 list, using a standard internet discount brokerage account. The actual returns (cumulative), after commissions, exceeded the returns calculated by the Performance Evaluator for that segment and dates (34.1% vs 22.2%). This both provides evidence confirming the ground truth of the Performance Evaluator software, and the reality that the machine learning technique's recommendations were actionable trades in the SP500 segment of the dynamic market. The small trial did not confirm viability at institutional scale. Figure 14. This figure repeats the right-most column of Figure 13 as a histogram. It indicates that for the 5 highest ranked recommendations (the only ones included), there is an unusually high correlation with the highest actual ranks, but most of the actual ranks are spread evenly across lower ranks. An optimal correlation would have higher right-side bars descending to lower left-side bars. This figu re discloses an unexpected behavior that may indicate fr agility. The center gap is an irrelevant artifact of aligning the right half of the visualization to account for varying sample set sizes. We developed and applied this proof protocol to a black box machine learning technique that makes repeated longitudinal recommendations for stocks in the US market. We designed the protocol to support a thorough, trustworthy and persuasive evaluation of the machine learning technique, recognizing that the technique often received erroneous real-time input data, that post recommendation events would materially affect recorded results and that it would need to operate in and be evaluated with respect to unknowable future contexts. Effective application of a proof protocol requires subject matter expertise to assure appropriate context constraints and relevant benchmarks are applied. Because hindsight filtering is a primary experimental risk in longitudina l experiments, we suggest time-stamp authenticity certification of every evaluated decision is a key minimal requirement. Extensive visualizations of the black box outputs, from many viewpoints help understanding of the performance. Cross-comparison of visualizations among segments add important understanding of broader patterns. Two proof concerns remain with respect to the machine learning recommendations discussed here: Fi rst, the contexts that arose during the three year certified experiment did not include an extended episode of falling prices, or  X  X rash. X  Second, there is some evidence of learning decay which need s continuing years of study. decisioning/recommending system built for dynamic contexts because of unforeseen factors. Yet, in business, defense, and space exploration final decisions must be made about design, expenditures and testing before deployment non-the-less. Decision makers need the best proof possible. Finally, we hope insightful, skeptical critiques of the proof protocol will support its extension to other big data longitudinal prediction domains where contexts evolve in unknown ways. Note: The three year certified recommendations discussed here were deposited for public access with U.Cal.Irvine machine learning repository. [1] Brooks, J. S. et al. 2006. Testing Hypotheses for the Success of [2] Canadi, I., Barford, P. and Sommers, J. 2012. Revisiting [3] Changa, R. M., Kauffman, R. J. and Kwonc, Y. 2014. [4] https://www.digistamp.com/authority/evidence/ (Feb. 2015). [5] Driscoll, K. and Walker, S. 2014. Big Data, Big Questions [6] Han, J. and Kamber, M. 2011. Data Mining: Concepts and [7] Heid, I. M. et al. 2009. Meta-Analysis of the INSIG2 [8] Hine, D. 2007. Change in a dynamic climate: a single [9] Hirst, D. E., Koonce, L. and Miller, J. 1999. The Joint Effect [10] Hu, R., and Watt, S. M. 2014. An Agent-Based Financial [11] Kais, M. et al. 2006. A Multi-Sensor Acquisition Architecture [12] Knox, S. and Walker, D. 2010. Empirical developments in the [13] Krahnena, J. P. and Weber, M. 2001. Generally accepted rating [14] Lebiere, C., Gonzalez, C. and Warwick, W. 2009. A [15] Li, Z. and Li, Z. 2015. Optimal robust optimization [16] Lowenstein, R. 2000. When Genius Failed: the Rise and Fall [17] Magnusson, D. a nd Bergman, L. R. 1990. Data Quality in [18] Olden, J. D. and Jackson, D. A. 2002. Illuminating the  X  X lack [19] Terry-McElrath, Y.M. et al. 2011. Effects of tobacco-related [20] Menon, A., et al. 1999. Antecedents and consequences of [21] Pavlou, P. A. and Sawy , O. A. 2011. Understanding the [22] Redline, S., Dean, D., and Sanders, M. H. 2013. Entering the [23] Sen, K., Viswanathan, M. and Agha, G. 2004. Statistical Model [24] Sterman, J. D. 1987. Testin g Behavioral Simulation Models by [25] Varian, Hal R. 2014. Big Data : New Tricks for Econometrics. [26] Warbrick-Smith, J. et al. 2009. Three hundred and fifty [27] Weiss, R. E. 2005. Modeling Longitudinal Data: With 72 
