 This paper describes an entity ranking model for example-based person search in email. Evaluation by comparison to manually resolved named references in Enron email yield results that correspond to typically placing the correct entity in the first or second rank.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Experimentation, Measurement Entity retrieval, name resolution, email
The usual formulation of the person search task requires finding a unique page (e.g., the principal home page) for a named person, where the name is provided in isolation as the query [1,2], but other formulations are possible. In this pa-per we focus on resolving named mentions in context, a task that we refer to as example-based person search .Wemodel our task in a manner similar to the entity linking task in the Text Analysis Conference (TAC), 1 but with a specific men-tion in an email messages as the  X  X uery, X  and automatically constructed entity models as the items to be ranked. Specif-ically, given an email message d where a person X  X  name m is mentioned, and a collection of person entities E , find the entity e  X  E to which m refers. We report Mean Reciprocal Rank (MRR) rather than TAC X  X  modified B-Cubed score as our principal evaluation measure because our focus is on formative rather than summative evaluation (thus prefer-ring a ranked measure) and because in our present work we http://nlp.cs.qc.cuny.edu/kbp/2011/ test only with mentions that human annotators are able to resolve (obviating the need to score what TAC calls  X  X ILs X ).
Our task is to rank all e  X  E in decreasing order of the probability of e being the correct resolution given m and d : P ( e | m, d ). By applying the chain rule, this can be inferred as in Formula 1: P ( e | m, d )= P ( e, m, d ) where the prior probability of an email message P ( d )can be assumed to be uniform, and P ( e | d ) is the probability of entity e being mentioned in email message d .Thereare several types of complementary evidence that could help us to estimate P ( e | d ), suggesting a log-linear model: where, f k ( e, d )isoneof K ways of estimating P ( e | d )and  X  k is some weight for f k ( e, d ). In this paper we allocate uniform weights to all  X  k , but in general these weights can and should be learned from training data.

To compute P ( m | e, d ), we simplify it as P ( m | e )by assuming that the probability of seeing some form of mention for an entity will not be affected by its surrounding email. Of course, this may not be true. For example, when writing to a family member, people might refer to another family member using a nickname that they would not normally use to refer to that same person in an email sent to a business colleague. We leave accounting for that factor to future work.

We estimate P ( m | e ) from normalized occurrence counts in the entity model C ( l m ), which count the number of times each lexical form is observed in a header, salutation or sig-nature. WeusetheDicecoefficient s ( l m ,m )toaccountfor partial string matches between the actual mention m and canonical lexical forms l m :
We take the Enron email collection [5] hosted by CMU 2 as our email collection; this collection contains messages but no http://www.cs.cmu.edu/  X  enron/ attachments. We obtained the identity models built auto-matically for this collection by Elsayed and Oard [4], which includes 123,773 unique entities. Figure 1 shows the data structure for each entity. The bag-of-words representation is stored separately for words in the body of messages sent, received-as-to by the entity, and received-as-cc by the entity (with quoted text removed). The communication graph con-tains a list of all entities e with whom entity e exchanged at least one message in each direction (i.e., at least one message sent as  X  X o X  or  X  X c X  to e and at least one message received as  X  X o X  or  X  X c X  from e ).

As ground truth we use a publicly available set of 470 single-token mentions (in 285 unique messages) that have been manually resolved to email addresses. 3 The reported inter-annotator agreement is 81% and the median lexical ambiguity (i.e., the number of identity models with an exact string match) is 116 [3].

We tried four ways of estimating f k ( e, d ). Method ( B1 ), based on Balog X  X   X  X odel 1 X  [1], finds entities who typically use similar words, regardless of whether those words are used together in one email. The words in the body of d are taken as query terms (with no removal of quoted text), and Lucene X  X  standard analyzer and index searcher are used to score each entity. Method ( B2 ), based on Balog X  X   X  X odel 2 X  X 1] works similarly, but indexing individual messages rather than the concatenation of all messages sent or received by each entity. For the top-scoring 50 messages d r retrieved for d q ,eachentityinthe X  X rom X , X  X o X  X r X  X c X  X ieldof d r re-ceives an equal portion of d r  X  X  retrieval score and these par-tial scores are then summed over all 50 documents. Method ( QE )directlyuses d q  X  X   X  X rom X   X  X o X  and  X  X c X  fields, assign-ing each such entity e equal weight. These entities are later used in the communication graph to retrieve their one-hop neighbors e , with each valued by their normalized number of messages exchanged with e . Using this same communication graph, Method ( EE ) is constructed by projecting weights to entities that are directly connected by any ranked entity collected from the three previous methods. Each method X  X  results are first renormalized to sum to 1, and then combined as described in equation (3). Contributions to the same en-tity are summed, and the final results are renormalized to sum to 1.
As Figure 2 shows, combining Methods ( B2 )and( QE ) does rather well, achieving an MRR of 0.667. Since our ultimate goal is to use this as the first stage of an au-http://www.cs.umd.edu/  X  telsayed/research.htm tomated entity resolution system (which will also leverage intra-document consistency evidence in subsequent iterative steps), we interpret this as a promising result. Our process-ing requires an average of 0.78 seconds per query (one core, 2.66 GHz CPU, 4 GB RAM), which meets our efficiency requirements for a preprocessing stage. Elsayed, using dif-ferent techniques, similarly found the communication graph to be the best source of evidence (MRR=0.785), but at the cost of the substantially greater computational cost to simul-taneously resolve all 1.3 million named references to people in the Enron collection [3].

Comparing Methods ( B1 )and( B2 ), our results paral-lel those Balog et al report for other content types, with ( B2 ) markedly better than ( B1 ) [1]. Combining evidence from Methods ( B1 )and( B2 ) yields no improvement over Method ( B2 ) alone, suggesting that further work on com-bination methods is called for. Zaragoza et al found the entity graph to be useful for ranking relevant entities [6]; our failure to replicate that by Method ( EE ) suggests that we should exlore graph-based entity ranking measures and that learned weights are likely needed for a log-linear com-bination in this application. We plan to explore those ideas in future work. We appreciate the advice and suggestions of Tim Finin, Tim Oates, Dawn Lawrie, Paul McNamee, Jim Mayfield, Ves Stoyanov and William Webber. [1] K. Balog, L. Azzopardi, and M. de Rijke. A language [2] A. P. de Vries et al. Overview of the INEX 2007 entity [3] T. Elsayed. Identity resolution in email collections . [4] T. Elsayed and D. Oard. Modeling identity in archival [5] B. Klimt and Y. Yang. The Enron corpus: A new [6] H. Zaragoza, H. Rode, P. Mika, J. Atserias,
