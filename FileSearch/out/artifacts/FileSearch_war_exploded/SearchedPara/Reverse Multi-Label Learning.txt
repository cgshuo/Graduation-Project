 several tags [6].
 F -score, which is the harmonic mean of precision and recall, might be more appropriate. a variety of performance measures. This means that the objective function being optimised by the method is tailored to the performance measure on which we want to do well in our specific Finally, source code is made available by us.
 the best performing overall. 1.1 Related Work an ensemble method  X  Ensemble of Classifier Chains (ECC)  X  where several random chains are RAndom K-labELsets (RAKEL) [10] deals with that by proposing an ensemble of classifiers, each of each element in the power set of this subset. Other proposed ensemble methods are Ensemble classes in a discriminative framework.
 decompose over the labels. number of instances. An input label x is encoded as x  X  { 0 , 1 } N , s.t. ! if
N =5 the second label is denoted as x = [0 1 0 0 0] . An output instance y is encoded as if
V = 10 and only instances 1 and 3 are annotated with label 2 , then the y corresponding to x test instances. 2.1 Loss Functions case for, in particular, Macro-precision , Macro-recall , Macro-F  X  1 and Hamming loss [10]:
Macro-F  X  = where associated to the performance measure. We assume a known loss function  X  : Y  X  Y  X  R + which given prediction is 1  X  F  X  ( y,  X  y ) , which is the one we focus on in this paper, 2.2 Features and Parameterization score of the model parameter vector  X  , i.e., a prediction is given by  X  y such that 3 i.e.,  X  ( x, y )= ! V x parameter  X  to be learned. 2.3 Optimisation Problem risk minimisers [15]. 3.1 Convex Relaxation convex upper bound on the structured loss of (4). The resulting optimisation problem is It is easy to see that  X   X  that of (4) for the optimal solution). Here,  X  y n constraints (6) hold for all y , they also hold for  X  y n  X  number of constraints which provably approximates well the original problem. ' of the violation margin  X  n , 3.2 Constraint generation Using eq.(2) and  X  ( x, y )= ! V where and Algorithm 1 Reverse Multi-Label Learning 1: Input: training set { ( x n ,y n ) } N n =1 ,  X  ,  X  , Output:  X  2: Initialize i =1 ,  X  1 =0 , MAX =  X  X  X  3: repeat 4: for n =1 to N do 5: Compute y  X  n (Na  X   X ve: Algorithm 2. Improved: See Appendix) 6: end for 7: Compute gradient g i (equation (12)) and objective o i (equation (11)) 9: until converged (see [18]) 10: return  X  Algorithm 2 Na  X   X ve Constraint Generation 1: Input: ( x n ,y n ) ,  X  ,  X  ,  X  , V , Output: y  X  n 2: MAX =  X  X  X  3: for k =1 to V do 7: if CURRENT &gt; MAX then 8: MAX = CURRENT 10: end if 11: end for 12: return y  X  n Y V solve argmax thus the objective function from (5) becomes whose gradient (with respect to  X  ) is We need both expressions (11) and (12) in Algorithm 1. 3.3 Prediction at Test Time the loss, is not present). Since z n a constant vector, the solution y  X  time is very fast. Table 1: Evaluation scores and cor-responding losses score  X  ( y,  X  y ) macro-precision 1  X  y T  X  y macro-recall 1  X  y T  X  y 3.4 Other scores Up to now we have focused on optimising Macro-F  X  , which already gives us several scores, in then plug in eq.(4).
 as test-time prediction (see subsection 3.3). Datasets Model selection Implementation Our implementation is in C++, using the Bundle Methods for Risk Minimization (BMRM) of [18] Comparison to published results on Macro-F 1 to allow us to make a fair comparison.
 [11] (EPS) and Random K Label Subsets [10] (RAKEL).
 Comparison to published results on Hamming Loss methods for which we could find Hamming loss results on publicly available data. tion by [1]. As can be seen, our model has the best performance on both datasets. Results on F  X  in. In Macro-F  X  , for example,  X  is a trade-off between precision and recall : when  X   X  0 we recover precision , and when  X   X  X  X  we get recall . Unlike with other methods, given a desired multi-label datasets, to train three models: BM[9], RAKEL[10] and MLKNN[20]. BM was chosen package.
 MLKNN has two parameters: the number of neighbors k and a smoothing parameter s controlling t default for t and m (respectively 0.5 and 2  X  N ) and set k to N the library X  X  defaults.
 while ML-KNN and RAKEL give more emphasis to precision (left side). Our method, however, medical , enron and emotions  X  it practically dominates over the entire range of  X  . viewed from this perspective, many popular performance measures admit convex relaxations that several state-of-the-art methods and overall our performance is notably superior. left as subject for future research.
 improve the paper. NICTA is funded by the Australian Government as represented by the Depart-ment of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. in the Medical and Enron datasets.
 Dataset Ours CCA CC BM SM MS ECC EBM EPS RAKEL Yeast 0.440 0.346 0.346 0.326 0.327 0.331 0.362 0.364 0.420 0.413 Scene 0.671 0.374 0.696 0.685 0.666 0.694 0.742 0.729 0.763 0.750 Medical 0.420 -0.377 0.364 0.321 0.370 0.386 0.382 0.324 0.377 Enron 0.243 -0.198 0.197 0.144 0.198 0.201 0.201 0.155 0.206 Figure 1: Macro-F  X  results on five datasets, with  X  ranging from 10  X  2 to 10 2 (i.e., log Macro-precision (left side) and Macro-recall (right side). [1] Krzysztof Dembczynski, Weiwei Cheng, and Eyke H  X  ullermeier. Bayes Optimal Multilabel [2] Xinhua Zhang, T. Graepel, and Ralf Herbrich. Bayesian Online Learning for Multi-label and [3] Piyush Rai and Hal Daume. Multi-Label Prediction via Sparse Infinite CCA. In Y. Bengio, [4] Jesse Read, Bernhard Pfahringer, Geoffrey Holmes, and Eibe Frank. Classifier chains for [6] Matthieu Guillaumin, Thomas Mensink, Jakob Verbeek, and Cordelia Schmid. TagProp: Dis-[7] Douglas W. Oard and Jason R. Baron. Overview of the TREC 2008 Legal Track. [9] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P. Vlahavas. Mining Multi-label Data . [10] Grigorios Tsoumakas and Ioannis P. Vlahavas. Random k-labelsets: An ensemble method [13] Martin Jansche. Maximum expected F-measure training of logistic regression models. HLT , [15] V. Vapnik. Statistical Learning Theory . John Wiley and Sons, New York, 1998. [17] D. E. Knuth. The Art of Computer Programming: Fundamental Algorithms , volume 1. [18] Choon Hui Teo, S. V. N. Vishwanathan, Alex J. Smola, and Quoc V. Le. Bundle methods for [20] Min-Ling Zhang and Zhi-Hua Zhou. ML-KNN: A lazy learning approach to multi-label learn-
