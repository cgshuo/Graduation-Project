 Headlines are particularly important for online news out-lets where there are many similar news stories competing for users X  attention. Traditionally, journalists have followed rules-of-thumb and experience to master the art of crafting catchy headlines, but with the valuable resource of large-scale click-through data of online news articles, we can apply quantitative analysis and text mining techniques to acquire an in-depth understanding of headlines. In this paper, we conduct a large-scale analysis and modeling of 150K news ar-ticles published over a period of four months on the Yahoo home page. We define a simple method to measure click-value of individual words, and analyze how temporal trends and linguistic attributes affect click-through rate (CTR). We then propose a novel generative model, headline click-based topic model (HCTM), that extends latent Dirichlet alloca-tion (LDA) to reveal the effect of topical context on the click-value of words in headlines. HCTM leverages clicks in aggregate on previously published headlines to identify words for headlines that will generate more clicks in the future. We show that by jointly taking topics and clicks into account we can detect changes in user interests within topics. We evaluate HCTM in two different experimental settings and compare its performance with ALDA (adapted LDA), LDA, and TextRank. The first task, full headline , is to retrieve full headline used for a news article given the body of news article. The second task, good headline , is to specifically identify words in the headline that have high click values for current news audience. For full headline task, our model performs on par with ALDA, a state-of-the art web-page summarization method that utilizes click-through information. For good headline task, which is of more prac-tical importance to both individual journalists and online news outlets, our model significantly outperforms all other comparative methods.
 H.2.8 [ Database Management ]: Database Applications -Data mining; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Headline Prediction, Large-scale Analysis, Click-through Rate, Online News Analysis
In recent years, a fast decline in print readership, cou-pled with spectacular growth in online news consumption have resulted in new types of journalism, new distribution methods and sources, as well as new business models. The entire field of journalism is experiencing an unprecedented amount of change and competition, particularly from online media. As news sources have multiplied, so have the number of articles that describe the same news event, and this poses a great challenge for journalists in attracting audiences to their stories. This problem is readily visible, as searching for any newsworthy topic on any given day is likely to yield thousands of results. At the same time, the rise of online social media and their mobile apps adds to this problem by stories being quickly shared and reaching millions of users worldwide. From the individual user X  X  perspective, the num-ber of articles he is exposed to on a daily basis has increased significantly; users can visit multiple news media sites, and each site can potentially host a vast number of articles.
Increases in news production and changes in user behavior have generated significant competition for users X  attention, in a marketplace where different headlines  X  X ompete X  for a user X  X  click (both within a particular page and across me-dia platforms). In many ways this is not new, and Tabloids in particular have historically been the masters of grabbing readers X  attention with classic headlines such as  X  X ord to City: Drop Dead X  and  X  X eadless Body in Topless Bar X  (a headline which inspired a movie of the same name) 1 . Ar-guably, the  X  X rt of headline writing X  is a skill developed by journalists that requires creativity and use of some good ground rules. A good headline summarizes the news article, and at the same time entices the reader to want to read it. Guidelines include, for example, verbs and adverbs are pre-http://nymag.com/nymetro/news/anniversary/35th/n 8568/ ferred to nouns and adjectives, and verbs of active forms are more effective than verbs of passive form [17].

The combination of a surge in online news production and consumption, real-world datasets of user click behavior, and advanced machine learning techniques, presents a singular opportunity for large-scale data-driven analysis of this art. Good headlines have been historically important in attract-ing readers, but with online news, the difference between a good and a bad headline for a single article can have impor-tant revenue impact, affect the propagation of the story in social media, and result in either growth or decline of reader-ship. Despite the potential and significance of a systematic approach to headlines, there has not been much scientific research on this topic, and journalists still rely on intuition and hand crafted rules of thumb. One possible exception is the Huffington Post, which uses A/B testing to choose some headlines [18],
In this paper, we conduct a large-scale quantitative and machine learning-based study of the relationship between user clicks and words in news headlines. We first present an in-depth analysis of user click-through data on 150K news articles published on the Yahoo front page over a four-month period. The analysis reveals important facets about words in headlines. First, some words significantly induce more clicks than others, which illustrates the need for a new met-ric for click-through rates of each word. Second, certain classes of words, such as named entities and past tense verbs, attract more clicks than others. Finally, word-level click-through rates rapidly vary over time, as events and topics emerge and dissipate. These results highlight the impor-tance of considering context of news articles in formulating effective headlines, and thus we propose the Headline Click-based Topic Model (HCTM) to explicitly model the topical context of words with respect to clicks. HCTM extends the traditional Bayesian topic model, latent Dirichlet allocation (LDA) [6], by taking into account aggregate information of clicks on headlines from previously published news articles. The central idea behind HCTM is that it models  X  X nterest X  on a given topic/word by aggregating click information on articles recently published (we use the previous week as the time frame), and leverage that to suggest headline word to journalists that could attract more clicks. HCTM models the distribution of click-value of words for each topic, and the distribution of clicks for each view as conjugate distribu-tions ( Beta and Binomial distributions, respectively), and incorporate them into the framework of topic modeling ap-proach.

Our main contributions can be summarized as follows:
The rest of this paper is organized as follows. In Sec-tion 2 we discuss related work. In Section 3, we define a new metric for word-level click-through rate. We then per-form quantitative analysis of word-level click-through rate with large-scale news and user click data. In Section 4 we present our proposed model and its inference. In Section 5 we present and discuss our experiments on the headline pre-diction task. Finally, in Section 6 we conclude the paper.
We explain the contributions of our research within vari-ous domains. First we discuss the common practice of head-line writing in journalism and what we can learn from that process. Then we look at previous work on automatic head-line generation which is a well-studied field but has a sig-nificantly different focus than ours. We then discuss topic modeling, which has been widely used for analyzing unstruc-tured text, and we discuss how our model fits into the topic modeling literature. Lastly, we discuss previous research on predicting which online news articles will be widely read, and how our research question is related to those papers.
Traditional studies in journalism tend to focus on the news value of the newspaper as a whole [24] partly because each newspaper has an audience of stable readers [21], and an important way to increase the readership is to improve its reputation by having a good set of well written articles. Con-sequently, in journalism research, the wording of headlines has been studied from a stylistic, pragmatic and linguistic perspective (cf. [12, 20]). A methodological shortcoming of these studies is that they are small in scale, covering around 1,000 articles, and they rely on study participants X  answers on questionnaires rather than direct observation of readers X  behavior.

Research on online media and their headlines requires a different approach. One reason is that unlike newspapers, online news delivery offers a much more convenient way for readers to skim through several headlines and choose what to read from thousands of news outlets, free from any physical constraint of newspapers. Another reason is that user be-havior can be directly observed through click-through data. To best of our knowledge, not much research has been con-ducted for online news headlines. One recent report revealed that the Huffington Post performs A/B testing to choose headlines that get more clicks [18], but there is no scientific research for this heuristic. In this paper, we present a large-scale analysis and a predictive model to learn the patterns of clicks and words in headlines based on observations of actual user clicks.
Many previous studies have looked at automatically gen-erating headlines. For example, [4] presents a statistical model for learning the likelihood of a headline. [27] de-scribes using a Hidden Markov Models for generating in-formative headlines for English texts, and [25] uses singular value decomposition for a similar task. And [8] describes a system that creates a headline for a newspaper story using linguistically-motivated heuristics. More recently, an exter-nal corpus, such as Wikipedia, has been used in headline generation [26]. A widely used method in this line of re-search is TextRank [15], based on a graph where the nodes are the words of the article and links are weighted with the number of sentences where the connected words co-occur. The PageRank algorithm is then applied on the constructed graph to rank the keywords. We use this state-of-the art method as a comparative baseline in the experimental sec-tion.

Though we share the same motivation of finding better headlines, the goal of our work differs from the line of re-searches discussed here as our model does not generate the full headline phrase, but rather proposes specific words that is relevant to the news content and, if included in the head-line, increases the chance of the article being clicked.
Topic modeling techniques have acquired a lot of pop-ularity since the seminal work of Blei et al. [6]. These models have been extended or applied in various domains such as document summarization [11], recommendation [5] or community detection [14]. As a variant of latent semantic analysis (LSA), ALSA (adapted LSA) utilizes click-through information to improve the quality of web-page summariza-tion [22]. In forming the term vector of each page, weight of each word is boosted by the number of times users click on the page after making a query that contains the word. We apply this method to our experimental setting as illus-trated in Section 5.3.1, and build ALDA (adapted LDA) as a comparative method.
In the context of online news popularity prediction, the main goal consists of identifying interesting features that can forecast the future popularity of a news articles [3, 1, 23, 9, 13, 16]. State-of-the-art models extract different features using user generated content (such as the number of clicks, of views, the publication time, number of comments, named entities) to predict the popularity of the news, i.e. the CTR. While not directly related to news forecasting, our problem can be seen as the reverse problem. Indeed, in this paper we design a machine learning algorithm that is able to learn headline words that are likely to generate more clicks by using the CTR as input. In [3] the authors show that named entity is a strong signal for news forecasting. In the same sense, in Section 3.3 we show that the presence of celebrity names (i.e. a category of named entities) as well as linguistic features significantly affect how well headlines attract clicks, which gives a strong encouragement in taking into account of both news contents and click information in predicting attractive words for headlines. p oli ce bombing suspects planned more attacks 0 .06 97 0.0601 0.0740 0.0484 0.0531 0.0515 Table 1: Example of w CTR from headline,  X  X olice: Bomb-ing Suspects Planned More Attacks X . Words and their cor-responding wCTR values are shown. Some words (sus-pects, police) are more likely to generate clicks than others (planned, attacks). CTR of this headline is 0.0659
In this section, we present the details of the quantitative analysis of headlines, including a new metric for word-level click-through rate ( w CTR( w,t )).We also examine how its average value over time ( w CTR( w )) and daily variability ( X ( w )) could reveal the role that an individual word plays in a headline. We describe our dataset in Section 3.1, and define w CTR in Section 3.2. Then we examine the extent to which word-level click-value influences CTR of headlines in Section 3.3. Finally, we discuss how we use the aforemen-tioned metrics to discover the effect of linguistic attribute of a headline on its CTR in Section 3.4.
Our dataset consists of a large set of articles published on the Yahoo homepage (yahoo.com) and their correspond-ing CTR data. A user visiting the homepage might perform several actions including checking mail, browsing photos, or reading news. We only consider user sessions that contain at least one click on a news article. We take news arti-cles published over a period of four months, from March to June 2013, and extract the number of times each article is presented to users (views), and the number of times it is ac-tually clicked (clicks). We filter out articles viewed less than 100 times, and select a random sample of 150K articles.
In this section we investigate how individual words in a headline impact the CTR of that headline 2 . More precisely, we hypothesize that each word carries an intrinsic click-value depending on current trends and interest manifested by on-line users, which is mainly revealed in click information. A widely used method to measure click-value of a news article is CTR. It is defined as where views ( d ) is the number of times a news article d is shown to users, and clicks ( d ) is the number of times it is actually clicked.

We propose a new measure: word Click-Through Rate ( w CTR) that calculates click-value of individual words from a set of headlines and its associated click information. On a given day, a word w can appear in multiple headlines, and in multiple user sessions.
Note that there is a one-to-one correspondence between news articles and their headlines. Thus one could use the terms  X  X rticle X  and  X  X eadline X  interchangeably when dis-cussing clicks and views. Figure 1: Correlation analysis between CTR and predicted CTR of 150K news articles. Predicted CTR is calculated by averaging w CTR of individual words in each headline. Correlation Coefficient is 0.882
Formally, we define w CTR of word w on day t as where views ( w,t ) is the number of times headlines that con-tain word w are shown to users on day t , and clicks ( w,t ) is the number of times such headlines are clicked. With this definition, a word with high w CTR value tends to generate more clicks than others. Table 1 gives an actual example of CTR and w CTR for the headline Police: Bombing Sus-pects Planned More Attacks , and it illustrates that individual words in the headline have different w CTR values.
We verify the extent to which click-value of individual words in the headline could explain the variability of CTR of the article. To do so, we calculate predicted CTR value of a headline by averaging w CTR of each word in the head-line. Correlation analysis reveals that the predicted CTR is positively strongly correlated with CTR ( r = 0 . 882, Figure 1).

This expected result validates the assumption that in-dividual words carry certain click-value, and they have a strong influence on the popularity of the news article. This encourages us to develop an unsupervised statistical method that learns from recent news articles and click history, and models click-value of individual words based on the context they are used in.
We discover interesting groups of words by analyzing tem-poral patterns of w CTR value. At each day, w CTR of each word is computed from the news articles published on that day. Then, we compute the mean of w CTR for each word ( w CTR( w )) on the four month news data as well as its av-Table 2: Words with highest (left) and lowest (right) w CTR( w ). Words with high mean w CTR are related to celebrity names, and words with low mean w CTR are re-lated to economic issues. Table 3: Words in ascending order of daily variability of w CTR, and their respective mean value w CTR( w ). Words with low daily variability of w CTR value are function words. Their mean w CTR values are close to global average (0.0571). erage daily variability ( X ( w )) calculated as the following: w CTR( w ) = 1 where T is the number of total days, and w CTR( w,i ) is the w CTR of the term w computed exclusively on data pub-lished the day i .

By ranking words based on their mean and daily vari-ability of their w CTR value, we observe clusters of words with similar patterns (Table 2, 3). For example, celebrity-related words have high mean click value, whereas business-related words have low mean click value. This suggests that celebrity names attract more clicks when shown to users words related to economic issues. This finding on our dataset confirms recent study on news forecasting where the authors POS T ag information w CTR( w ) WP$ P ossessive Wh-pronoun 0.0597 WP Wh-pronoun 0.0513 PRP$ Possessive Pronoun 0.0495 VBD Verb, past tense 0.0474 VBN Verb, past participle 0.0468 PRP Personal Pronoun 0.0467 RBS Adverb, superlative 0.0454 RB Adverb 0.0448 JJS Adjective, superlative 0.0437 WRB Wh-Adverb 0.0430 DT Determiner 0.0428 RP Particle 0.0422 MD Modal 0.0419 NNP Proper Noun, singular 0.0415 JJ Adjective 0.0413 NN Noun, singular or mass 0.0411 VB Verb, base form 0.0411 NNPS Proper Noun, plural 0.0408 NNS Noun, plural 0.0403 FW Foreign word 0.0398 VBP Verb, non-3rd person singular present 0.0393 CD Cardinal number 0.0390 RBR Adverb, comparative 0.0382 JJR Adjective, comparative 0.0368 Table 4: w CTR( w ) value of different lexical categories com-puted on Yahoo news corpus across four months peroiod. showed that named entities help in predicting popular news articles [3]. Furthermore, interestingly, function words such as preposition, and determiner have very low  X ( w ) value, and their w CTR( w ) value is very close to the global aver-age (0.0571), which means that their click value does not change over time, and they have little impact on the head-line regardless of the time period or the context they are used in.

Afterwards, we analyze the click value of different lexical categories using part-of-speech tags. The result (Table 4) confirms conventional wisdom on journalism that verbs are more effective than nouns, and adverbs are more effective than adjectives [17]. Another discovery is that superlative adverbs, and adjectives are much more effective than com-parative ones in generating clicks.
In this section, we propose headline click-based topic model (HCTM), a novel generative model that extends latent Dirich-let allocation (LDA) [6]. The rationale for approaching this problem with a topic model is that a close analysis of the data (detailed in Section 3) reveals that each word has an intrinsic click-value (i.e., how likely users clicks on a head-line containing that word), and that the click-value is de-pendent on the context in which the word is used. For ex-ample, celebrity names such as  X  X ionel Messi X  or  X  X ristiano Figure 2: Graphical representation of Headline Click-based Topic Model (HCTM). HCTM is an extension of LDA with two significant changes: 1) an additional observable variable c is introduced to account for whether user click behavior, and 2) z (topic indicator) is split into z h for headlines and z for bodies of articles. This enables realistic modeling of user click behavior, where clicks are generated solely from words and topics of the headline.
 Ronaldo X  are certainly more important in a  X  X ports X  article than in a  X  X usiness X  or  X  X olitics X  article.
HCTM jointly models headline and contents of news ar-ticle as well as the click data. To analyze user clicks, we consider user views of headlines, where a view of a headline occurs when the user is presented with the headline on the Webpage, and a click occurs if the user actually clicks on the headline. More specifically, as Figure 2 shows, HCTM in-cludes an observable variable c for user clicks where c d if v th user who views headline d clicks on it, and c otherwise. The model also introduces a latent variable  X  for the topic-specific click-value of each word. Note that in our model the latent indicator variable for topics, typically a single set z , is separated into two, z h for headline of the news, and z b for its content (i.e., body of article). Only the former latent variable, z h , guides the generation of clicks.
HCTM models the distribution of click value,  X  , as a Beta distribution, and the distribution of clicks, c , as a Binomial distribution and take advantage of the Beta-Binomial con-jugacy in posterior inference. More precisely, the probabil-ity of each word w h to be clicked in a specific context z is modeled by a latent variable c representing a Bernoulli trial. Our prior belief on the probability of a success of this trial is given by a Beta distribution. This completes a full generative Bayesian model.

Figure 2 depicts the graphical model of HCTM, and Table 5 describes the notation of the variables used in the model. Table 5: List of variables used in the generative model.
A formal description of the generative process is as follows: 1. For each topic k  X  K , 2. For each topic-word pair ( z,w )  X  K  X  I , 3. For each document d  X  N ,
In this section, we propose a Markov Chain Monte Carlo algorithm for posterior sampling. More precisely, we use the collapsed Gibbs sampling approach introduced in [10].
The joint probability of the model can be written as the following p ( w b , w h , z , c , X , X , X  ) = where p ( w d v |  X  ) indicates the probability of a word from the headline to be associated with v  X  X h click. This process is discussed more in detail as we describe the estimation of  X  .
We use Dirichlet-Multinomial conjugacy to write out the conditional distribution of w b and w h .
Sampling z b The conditional distribution of z id b given word w id b is proportional to the number of times the topic is used in the document d multiplied by the conditional prob-ability of w id b given the topic. p ( z id b = z | rest )  X  ( n  X  id zd +  X  1 )  X  p ( w id b where n  X  id zd indicates the number of times topic z is assigned in document d without counting z id b .

Sampling z h The conditional distribution of z jd h given word w jd h and  X  is proportional to the multiplication of the number of times the topic is used in document d , the condi-tional probability of word w jd h given the topic and the like-lihood of clicks associated with w jd h . p ( z jd h = z | rest )  X  ( n  X  jd zd +  X  1 )  X  p ( w jd h where the last multiplication is taken over each click v asso-ciated with the word w jd h .

Estimating  X  The posterior sampling of z jd h involves es-timation of click value  X  . First we write out the probability distribution of click variable c d v . where w d v is the headline word associated with the click c and z d v is the topic assigned to w d v . We associate click vari-able with a word in headline at each iteration of sampling. For each c d v , we draw a word w d v from the headline words set w h with probability proportional to its click value  X  z d v
We use Beta-Binomial conjugacy to write out the condi-tional distribution of  X  given observations on clicks, headline words and their topics. where m 1 z,w is the number of times click variable c associated with topic z and word w is observed to be 1 (clicked), and m z,w is the number of times it is observed to be 0 (not clicked).
In this section, we present the results of our analysis at two different levels. On one hand, we show how HCTM can provide further insight on headline formulation by jointly modeling news contents, click information, and topic-specific as they reflect real-time interest of audience. click-values: we present topics and their respective high-click value words inferred using HCTM, and discuss previously unseen patterns (Section 5.2); Then we evaluate the perfor-mance of HCTM in generating headlines and compare it to other approaches (Section 5.3).
We use the same data set as described in 3.1. In build-ing bag-of-words of each news articles, we generate both unigrams and bigrams from headline and body of the ar-ticle and perform simple linguistic filtering based on word frequency. Words that occur in more than 10% of the arti-cles are removed as stop words, and words that occur less than five times are removed as noise. Note that bigrams are important in  X  X icking up X  important topics or entities that consist of two words (e.g.,  X  X oston Bombing X , celebrity names).
As with LDA, our model can also be used in unsupervised data analysis. The difference is that our model discovers which headline words attract user clicks (in terms of topic-specific click-value of words) as well as topics from the cor-pus. We identify and present three topics (  X  k ) related to technology, economy, and sports and their respective high click-value words (  X  k ) tracked during two consecutive weeks. The match between topics in consecutive time period is done by associating each topic of one week to the most similar one from the next week in terms of KL divergence. For topics, we illustrate the top ten words in terms of word likelihood given topic, p( w i |  X  k ). For click-value, we illustrate the top ten words in terms of topic-specific click value,  X  k,i (see Sec-tion 4.1).

The results show two interesting previously unseen pat-terns with topics and their respective click-values (Table 6). First, topics account for general terms that describe cer-tain thematic category such as sports, or technology whereas high click-value words refer to more specific details such as names of people, locations, or special events. Second, high click-value words change more rapidly than topic words. For instance, company names such as microsoft , apple , google al-ways appear as top words of the technology topic. However, its high click-value words vary significantly with no over-lapping words. This illustrates how quickly user interests change over time even within the same topical domain. Our model is capable of accounting for both thematic groups of words and temporal trends of user interest as topics and click-values, respectively.
In this section, we describe how we evaluate the perfor-mance of HCTM in generating headlines for given news ar-ticles. We compare HCTM with a wide range of methods discussed in Section 5.3.1. We report detailed results mea-sured in terms of area under the ROC Curve (AUC), and mean average precision (MAP).

Data for training and testing are prepared using the fol-lowing method. News articles and their click information of seven consecutive days are gathered as training data. After training, the model is tested on the data of the following day. Given a test set of news articles without their head-lines and click information, each model predicts the words of the headline for each news article. For instance, we train our model on the data from March 1 to March 7, and test on the data of March 8.
To evaluate the models, trained models are provided with the test data, the news articles without headlines (i.e. the body). Each model measures the headline score of words in the body, and produces a rank-ordered list of words for the headline. Below we describe how each model is trained, and produces the headline score for each word.
For HCTM, we used the same value for parameters as suggested for LDA,  X  1 =  X  2 = 0 . 1 and  X  1 =  X  2 = 50 /T . We also conducted experiments with various number of topics (between 5 and 100) getting similar results. For consistency, we stick to results from using T = 30. Headline score of each word is calculated as the posterior probability of the word given each test news article as in LDA. Formally, we compute the headline score for each word j in document d as
Each model produces a rank-ordered list of headline score of words for each test document. We evaluate its predictive power based on the following two measures. In summing up the result, we take macro average over daily average scores. Figure 3: Performance of w CTR, TextRank, LDA, ALDA, and HCTM on the headline prediction task in terms of MAP@10 and AUC. Evaluation is performed with two ex-perimental settings. The first task is to retrieve all headline words (All, left side). HCTM performs on par with the best comparative method, ALDA. The second task is to iden-tify high click-value words in the headline ( w CTR &gt; 0.1, right side). HCTM significantly outperforms all compara-tive methods.
The evaluation is performed with two different experimen-tal settings. In the first test, we measure how well each model predicts all words in the headline given only the con-tents of a news article. Figure 3 summarizes the perfor-mance of each method. For this experiment (All, Figure 3 This could potentially be used in online journalism. left side), performance of our model is on par with ALDA, a state-of-the-art summarization algorithm that is able to utilize click-through information.
Even within a single headline, some words are more eye-catching than others. Identifying headline words that have high click-value is of greater importance as they attract more clicks. Therefore, in the second test, the objective is to iden-tify headline words that have high w CTR value for current news audience. Specifically, we evaluate each model based on how well it predicts headline words whose w CTR value is higher than 0.1 (measured within unseen test data) which is approximately equivalent to top 10% of all vocabulary.
In this experiment ( w CTR &gt; 0.1, Figure 3 right side), our model significantly outperforms ALDA as well as all other comparative methods in terms of both MAP and AUC. This illustrates that our model is able to jointly model topics and click information of news articles in addition to identify topic-specific click-value of each word in the corpus. As a result, our model predicts headline words of a given news article that not only well represents thematic gist of the contents, but also triggers user clicks.
Towards a read-world application The work presented here is a nice example of how social community preferences can be automatically used to suggest better headlines. In practice, the proposed model will be used to suggest new words for a news article for which editors have already proposed an headline. In that scenario, we can suggest to the editor the top words not already in the headline ranked by their posterior probability given by the model as shown in Table 7. Editors will have better understanding of real-time interests of news audience, and learn click-inducing words that are contextually appropriate. Also, we can assess how good the words used by the editors are for the headline given the model trained on last week data, henceforth capturing the current trend. In this way, we believe that such a tool may be very useful in any editorial platform such as WordPress that integrates for instance an A/B testing package [19].
Using more user generated content The study conducted in this paper have been restricted to study the impact of words (unigrams and bigrams) on the CTR, and how the user implicit feedback on the news platform can be used to improve the headline. However, other related studies, on news popularity forecasting have shown that other signals, mostly extracted from user generated content, can be used as well [3, 1, 23]. As discussed in the related work section, the task of headline generation using the CTR, is closely related to predicting the CTR of news articles, and there-fore we strongly believe that enriching our model with input signals such as: comments, shares on Facebook, shares on twitter, could improve significantly the quality of the sug-gested headline. We leave this task as further improvement of our model.

Influence of personalization algorithms It is worth to no-tice that the user feedback information suffers from a person-alization bias . Indeed, on the Yahoo front page, a personal-ization algorithm is used to display the most relevant articles for the user. This ranking depends on user preferences and therefore different users may have different ranking which can lead to a position bias . However, the ranking is mostly influenced by the time of publication (i.e. recency ) and the popularity of the news article (CTR) which does not depend on the user. Furthermore, even if there is a personalization algorithm introducing a position bias, it remains that a click indicates a positive feedback. This is confirmed by our ex-periment where we show the superiority of using our model over LDA not exploiting the click information.
In this work, we introduced a novel generative click based topic model for headline generation (HCTM). It extends LDA by taking into account clicks generated by users when presented with a list of headlines on a online news portal. We conducted a large-scale study on a sample of 150K news articles published during four months, on which we showed that current articles X  CTR is positively strongly correlated (r = 882) with average click-value of individual words in the headline. We also found that various aspects of words such as named-entities, and part-of-speech play important roles in click-through rates, confirming traditional wisdom in jour-nalism, as well as finding novel patterns. We also observed that click value of words change rapidly even within the same domain of topics. By using HCTM, we showed that we can detect topics and their respective high click-value words. Finally, on a headline generation task, using HCTM, we ob-tain results as competitive as ALDA. More importantly, our model outperforms all other competing models (i.e., ALDA, LDA, and TextRank) in generating high click-value headline words for news articles. [1] M. Ahmed, S. Spagna, F. Huici, and S. Niccolini. A [2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [3] R. Bandari, S. Asur, and B. A. Huberman. The pulse [4] M. Banko, V. O. Mittal, and M. J. Witbrock.
 [5] N. Barbieri and G. Manco. An analysis of probabilistic [6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [7] C. Castillo, M. El-Haddad, J. Pfeffer, and [8] B. Dorr, D. Zajic, and R. Schwartz. Hedge trimmer: A [9] T. Graepel, J. Q. Candela, T. Borchert, and [10] T. L. Griffiths and M. Steyvers. Finding scientific [11] A. Haghighi and L. Vanderwende. Exploring content [12] E. Ifantidou. Newspaper headlines and relevance: Ad [13] A. C. K  X  onig, M. Gamon, and Q. Wu. Click-through [14] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link [15] R. Mihalcea and P. Tarau. Textrank: Bringing order [16] M. Richardson, E. Dominowska, and R. Ragno.
 [17] S. Saxena. Headline writing . Sage, 2006. [18] Z. M. Seward. How the huffington post uses real-time [19] Z. M. Seward. A/b testing for headlines: Now [20] J.-S. Shie. Metaphors and metonymies in new york [21] A. V. Stavros. Advances in Communications and [22] J.-T. Sun, D. Shen, H.-J. Zeng, Q. Yang, Y. Lu, and [23] G. Szabo and B. A. Huberman. Predicting the [24] T. A. Van Dijk. News as discourse. Lawrence Erlbaum [25] S. Wan, M. Dras, C. Paris, and R. Dale. Using [26] S. Xu, S. Yang, and F. C.-M. Lau. Keyword extraction [27] D. Zajic, B. Dorr, and R. Schwartz. Automatic
