 Consider the setting where we are given large amounts of unla beled data together with dual su-pervision in the form of a few labeled examples as well as a few labeled features , and the goal is to estimate an unknown classification function. This sett ing arises naturally in numerous appli-cations. Imagine, for example, the problem of inferring sen timent ( X  X ositive X  versus  X  X egative X ) associated with presidential candidates from online polit ical blog posts represented as word vectors, given the following: (a) a vast collection of blog posts easi ly downloadable from the web (unlabeled examples), (b) a few blog posts whose sentiment for a candida te is manually identified (labeled ex-amples), and (c) prior knowledge of words that reflect positi ve (e.g.,  X  X uperb X  ) and negative (e.g,  X  X wful X  ) sentiment (labeled features). Most existing semi-superv ised algorithms do not explicitly incorporate feature supervision. They typically implemen t the cluster assumption [3] by learning decision boundaries such that unlabeled points belonging t o the same cluster are given the same label, and empirical loss over labeled examples is concurre ntly minimized. In situations where the classes are predominantly supported on unknown subsets of s imilar features, it is clear that feature supervision can potentially illuminate the true cluster st ructure inherent in the unlabeled examples over which the cluster assumption ought to be enforced.
 Even when feature supervision is not available, there is amp le empirical evidence in numerous recent papers in the co-clustering literature (see e.g., [5, 1] and references therein), suggesting that the clustering of columns (features) of a data matrix can lead to massive improvements in the quality of row (examples) clustering. An intuitive explanation is t hat column clustering enforces a form of dimensional reduction or implicit regularization that is r esponsible for performance enhancements observed in many applications such as text clustering, micr oarray data analysis and video content mining [1]. In this paper, we utilize data-dependent co-clu stering regularizers for semi-supervised learning in the presence of partial dual supervision. Our starting point is the spectral bipartite graph partitio ning approach of [5] which we briefly review in Section 2.1. This approach effectively applies spectral clustering on a graph representation of the data matrix and is also intimately related to Singular Va lue Decomposition. In Section 2.2 we review an equivalence between this approach and a matrix app roximation objective function that is minimized under orthogonality constraints [6]. By dropp ing the orthogonality constraints but imposing non-negativity constraints, one is led to a large f amily of co-clustering algorithms that arise from the non-negative matrix factorization literatu re.
 Based on the algorithmic intuitions embodied in the algorit hms above, we develop two semi-supervised classification algorithms that extend the spect ral bipartite graph partitioning approach and the matrix approximation approach respectively. We sta rt with Reproducing Kernel Hilbert Spaces (RKHSs) defined over both row and column spaces . These RKHSs are then coupled through co-clustering regularizers. In the first algorithm, we dire ctly adopt graph Laplacian regularizers constructed from the bipartite graph of [5] and include it as a row and column smoothing term in the standard regularization objective function. The solut ion is obtained by solving a convex opti-mization problem. This approach may be viewed as a modificati on of the Manifold Regularization framework [2] where we now jointly learn row and column class ification functions. In the second algorithm proposed in this paper, we instead add a (non-conv ex) matrix approximation term to the objective function, which is then minimized using a block-c oordinate descent procedure. Unlike, their unsupervised counterparts, our methods supp ort dual supervison and naturally possess out-of-sample extension. In Section 4, we provide experime ntal results where we compare against various baseline approaches, and highlight the performanc e benefits of feature supervision. Let X denote the data matrix with n data points and d features. The methods that we discuss in  X  m r is the desired number of row clusters and m c is the desired number of column clusters. Below, by x i we mean the i th example (row) and by f j we mean the j th column (feature) in the data matrix. 2.1 Bipartite Graph Partitioning In the co-clustering technique introduced by [5], the data m atrix is modeled as a bipartite graph with examples (rows) as one set of nodes and features (column s) as another. An edge ( i, j ) exists if feature f j assumes a non-zero value in example x i , in which case the edge is given a weight of X ij . This bi-partite graph is undirected and there are no inter-example or inter-feature edges. The adjacency matrix, W , and the normalized Laplacian [4], M , of this graph are given by, where D is the diagonal degree matrix defined by D ii = P i W ij and I is the ( n + d )  X  ( n + d ) identity matrix. Guided by the premise that column clusteri ng induces row clustering while row clustering induces column clustering, [5] propose to find an optimal partitioning of the nodes of the bipartite graph. This method is retricted to obtaining co-c lusterings where m r = m c = m . The m -partitioning is obtained by minimizing the relaxation of th e normalized cut objective function using standard spectral clustering techniques. This reduces to fi rst constructing a spectral representation of rows and columns given by the smallest eigenvectors of M , and then performing standard k -means structure of Eqn. 1, it can be shown that the spectral represe ntation used in this algorithm is related to the singular vectors of a normalized version of X . 2.2 Matrix Approximation Formulation In [6] it is shown that the bipartite spectral graph partitio ning is closely related to solving the fol-lowing matrix approximation problem, ( F r  X  , F c  X  ) = argmin F where F r is an n  X  m matrix and F c is a d  X  m matrix. Once the minimization is performed,  X  the orthogonality constraints are dropped to make the optim ization easier while non-negativity con-straints F r , F c  X  0 are introduced with the goal of lending better interpretabi lity to the solutions. There are numerous multiplicative update algorithms for NM F which essentially have the flavor of alternating non-convex optimization. In our empirical com parisons in Section 4, we use the Alter-nating Constrained Least Squares (ACLS) approach of [12]. I n Section 3.2 we consider a 3-factor non-negative matrix approximation to incorporate unequal values of m r and m c , and to improve the quality of the approximation. See [7, 13] for more details on matrix tri-factorization based formula-tions for co-clustering. Let us consider examples x to be elements of R  X   X  d . We consider column values f for each feature to be a data point in C  X   X  n . Our goal is to learn partition functions defined over the ent ire For this purpose, let us introduce k r : R  X  R  X   X  to be the row kernel that defines an associated RKHS H r . Similarly, k c : C  X  C  X   X  denotes the column kernel whose associated RKHS is H c . Below, we define  X  r ,  X  c using these real valued function spaces.
 Consider a simultaneous assignment of rows into m r classes and columns into m c classes. For any class assignments where f j r  X  H r for all j . For the given n data points, denote F r to be the n  X  m r class assignment matrix. Correspondingly, F c ( f ) is defined for any feature f  X  C , and F c denotes the associated column class assignment matrix. Additional ly, we are given dual supervision in the example is labeled with class j (simlarly for the feature labels matrix Y c ). The associated row sum for a labeled point is 1 . Unlabeled points have all-zero rows, and the row sums are th erefore 0 . Let J ( J c ) denote a diagonal matrix of size n  X  n ( d  X  d ) whose diagonal entry is 1 for labeled examples (features) and 0 otherwise. By I s we will denote an identity matrix of size s  X  s . We use the notation tr ( A ) to mean the trace of the matrix A . 3.1 Manifold Regularization with Bipartite Graph Laplacian (MR) In this approach, we setup the following optimization probl em, The first two terms impose the usual RKHS norm on the class indi cator functions for rows and columns. The middle two terms measure squared loss on labele d data. The final term measure smoothness of the row and column indicator functions with re spect to the bipartite graph introduced in Section 2.1. This term also incorporates unlabeled examp les and features.  X  r ,  X  c ,  X  are real-valued parameters that tradeoff various regularization terms.
 Clearly, by Representer Theorem the solution is has the form , Let  X  ,  X  denote the corresponding optimal expansion coefficient mat rices. Then, plugging in Eqn. 3 and solving the optimization problem, the solution is easil y seen to be given by, where K r , K c are gram matrices over datapoints and features respectivel y. The partition functions are then defined by As in Section 2.1, we assume m r = m c = m . If the linear system above is solved by explicitly computing the matrix inverse, the computational cost is O (( n + d ) 3 + ( n + d ) 2 m ) . This approach is closely related to the Manifold Regularization framewor k of [2], and may be viewed as an mod-ification of the Laplacian Regularized Least Squares ( LAPRLS ) algorithm, which uses a euclidean nearest neighbor row similarity graph to capture the manifold structure in the data. Instead of using the squared loss, one can develop variants using the SVM Hing e loss or the logistic loss function. One can also use a large family of graph regularizers derived from the graph Laplacian [3]. In particular, we use the iterated Laplacian of the form M p where p is an integer. 3.2 Matrix Approximation under Dual Supervision (MA) We now consider an alternative objective function where ins tead of the graph Laplacian regularizer, we add a penalty term that measures how well the data matrix is approximated by a trifactorization F As before, the first two terms above enforce smoothness, the t hird and fourth terms measure squared loss over labels and the final term enforces co-clustering. T he classical Representer Theorem (Eqn. 3) can again be applied since the above objective funct ion only depends on point evalua-tions and RKHS norms of functions in H r , H c . The optimal expansion coefficient matrices,  X  ,  X  , in this case are obtained by solving, argmin This problem is not convex in  X  ,  X  , Q . We propose a block coordinate descent algorithm for the problem above. Keeping two variables fixed, the optimizatio n over the other is a convex problem with a unique solution. This guarantees monotonic decrease of the objective function and conver-gence to a stationary point. We get the simple update equatio ns given below, In Eqn 8, we assume that the appropriate matrix inverses exis t. Eqns 9 and 10 are generalized Sylvester matrix equations of the form AXB  X  + CXD  X  = E whose unique solution X under certain regularity conditions can be exactly obtained by an extended version of the classical Bartels-Stewart method [9] whose complexity is O( ( p + q ) 3 ) for p  X  q -sized matrix variable X . Alternatively, one can solve the linear system [10]: B  X   X  A + D  X   X  C vec ( X ) = vec ( E ) where  X  denotes Kronecker product and vec ( X ) vectorizes X in a column oriented way (it behaves as the matlab operator X (:) ). Thus, the solution to Eqns (9,10) are as follows, These linear systems are of size nm r  X  nm r and dm c  X  dm c respectively. It is computationally prohibitive to solve these systems by direct matrix inversi on. We use an iterative conjugate gradients (CG) technique instead, which can exploit hot-starts from t he previous solution, and the fact that the matrix vector products can be computed relatively efficient ly as follows, [ I m To optimize  X  (  X  ) given fixed Q and  X  (  X  ), we run CG with a stringent tolerance of 10  X  10 and maximum of 200 iterations starting from the  X  (  X  ) from the previous iteration. In an outer loop, we monitor the relative decrease in the objective function and terminate when the relative improvement falls below 0 . 0001 . We use a maximum of 40 outer iterations where each iteration performs one round of  X  ,  X  , Q optimization. Empirically, we find that the block coordinat e descent approach often converges surprisingly quickly (see Section 4.2). Th e final classification is given by Eqn. 5. In this section, we present an empirical study aimed at compa ring the proposed algorithms with sev-eral baselines: (i) Unsupervised co-clustering with spect ral bipartite graph partitioning ( BIPARTITE ) and non-negative matrix factorization ( NMF ), (ii) supervised performance of standard regularized least squares classification ( RLS ) that ignores unlabeled data, and (iii) one-sided semi-sup ervised performance obtained with Laplacian RLS ( LAPRLS ) which uses a euclidean nearest-neighbor row similarity graph. The goal is to observe whether dual supervision parti cularly along features can help improve classification performance, and whether joint RKHS regularization as formulated in our al-gorithms (abbreviated MR for the manifold regularization b ased method of Section 3.1 and MA for the matrix approximation method of Section 3.2) along both r ows and columns leads to good qual-ity out-of-sample prediction. In the experiments below, th e performance of RLS and LAPRLS is optimized for best performance on the unlabeled set over a gr id of hyperparameters. We use Gaussian kernels with width  X  r for rows and  X  c for columns. These were set to 2 k  X  0 r , 2 k  X  0 c respectively where  X  0 r ,  X  0 c are (1 /m ) -quantile of pairwise euclidean distances among rows and columns respectively for an m class problem, and k is tuned over { X  2 ,  X  1 , 0 , 1 , 2 } to optimize 3 -fold cross-validation performance of fully supervised RLS . The values  X  r ,  X  c ,  X  are loosely tuned for MA,MR with respect to a single random split of the data int o training and validation set; more careful hyperparameter tuning may further improve the resu lts presented below.
 We focus on performance in predicting row labels. To enable c omparison with the unsupervised co-clustering methods, we use the popularly used F-measure defi ned on pairs of examples as follows: 4.1 A Toy Dataset We generated a toy 2-class dataset with 200 examples per clas s and 100 features to demonstrate the main observations. The feature vector for a positive exampl e is of the form [2 u  X  0 . 1 2 u +0 . 1] , and for a negative example is of the form [2 u +0 . 1 2 u  X  0 . 1] , where u is a 50-dimensional random vector lap between the two classes. Given a column partitioning  X  c , consider the following transformation: a single feature whose value equals the mean of all features i n the same partition. For the correct action of T are shown in Figure 1 (left). It is clear that T renders the data to be almost separable. It is therefore natural to attempt to (effectively) learn T in a semi-supervised manner. In Figure 1 (right), we plot the learning curves of various algorithms with respe ct to increasing number of row and col-umn labels. On this dataset, co-clustering techniques ( BIPARTITE , NMF ) perform fairly well, and even significantly better than RLS , which has an optimized F-measure of 67% with 25 row labels. With increasing amounts of column labels, the learning curv es of MR and MA steadily lift eventu-ally outperforming the unsupervised techniques. The hyper parameters used in this experiment are:  X  r = 2 . 1 ,  X  c = 4 . 1 ,  X  r =  X  c = 0 . 001 ,  X  = 10 for MR and 0 . 001 for MA.
 Figure 1: left : Examples in the toy dataset under the transformation define d by the correct column partitioning. right : Performance comparison  X  the number of column labels used a re marked. 4.2 Text Categorization We performed experiments on document-word matrices drawn f rom the 20-newgroups dataset pre-processed as in [15]. The preprocessed data has been made pub licly available by the authors of [15] 1 . For each word w and class c , we computed a score as follows: score ( w, c ) =  X  P ( Y = c ) log P ( Y = c )  X  P ( W = w ) P ( Y = c | W = w ) log P ( Y = c | W = w )  X  P ( W 6 = w ) P ( Y = c | W 6 = w ) log P ( Y = c | W 6 = w ) , where P ( Y = c ) is the fraction of documents whose category is c , P ( W = w ) is the fraction of times word w is encountered, and P ( Y = c | W = w ) ( P ( Y = c | W 6 = w ) ) is the fraction of documents with class c when w is present (absent). It is easy to see that the mutual informati on between the indicator random vari-able for w and the class variable is P c score ( w, c ) . We simulated manual labeling of words by associating w with the class argmax c score ( w, c ) . Finally, we restricted attention to 631 words with highest overall mutual information and 2000 documents that belong to the following 5 classes: comp.graphics, rec.motorcycles, rec.sport.baseball, sc i.space, talk.politics.mideast . Since words of talk.politics.mideast accounted for more than half the vocabulary, we used the clas s normalization prescribed in [11] to handle the imbalance in the labeled dat a.
 Results presented in Table 1 are averaged over 10 runs. In eac h run, we randomly split the documents into training and test sets, in the ratio 1 : 3 . The training set is then further split into labeled and unlabeled sets by randomly selecting 75 labeled documen ts. We experimented with increasing number of randomly chosen word labels. The hyperparameters are as follows:  X  r = 0 . 43 ,  X  c = 0 . 69 ,  X  r =  X  c =  X  = 1 for MR and  X  r =  X  c = 0 . 0001 ,  X  = 0 . 01 for MA.
 We observe that even without any word supervision MR outperf orms all the baseline approaches: unsupervised co-clustering with BIPARTITE and NMF , standard RLS that only uses labeled doc-uments, and also LAPRLS which uses a graph Laplacian based on document similarity fo r semi-supervised learning. This validates the effectiveness of t he bipartite document and word graph regularizer. As the amount of word supervision increases, t he performance of both MR and MA im-proves gracefully. The out-of-sample extension to test dat a is of good quality, considering that our test sets are much larger than our training sets. We also o bserved that the mean number of (outer) iterations required for convergence of MA decrease s as labels are increased from 0 to 500 : for each class sorted by the real-valued prediction score as signed by MR (in one run trained with 100 labeled words). Intuitvely, the main words associated w ith the class are retrieved. 4.3 Project Categorization We also considered a problem that arises in a real business-i ntelligence setting. The dataset is composed of 1169 projects tracked by the Integrated Technol ogy Services division of IBM. These projects need to be categorized into 8 predefined product cat egories within IBM X  X  Server Services product line, with the eventual goal of performing various f ollow-up business analyses at the gran-ularity of categories. Each project is represented as a 112-dimensional vector specifying the dis-tribution of skills required for its delivery. Therefore, e ach feature is associated with a particular job role/skill set (JR/SS) combination, e.g.,  X  X ata-speci alist (oracle database) X . Domain experts val-idated project (row) labels and additionally provided cate gory labels for 25 features deemed to be important skills for delivering projects in the correspond ing category. By demonstrating our algo-rithms on this dataset, we are able to validate a general meth odology with which to approach project categorization across all service product lines (SPLs) on a regular basis. The amount of dual su-pervision available in other SPLs is indeed severely limite d as both the project categories and skill definitions are constantly evolving due to the highly dynami c business environment.
 Results presented in Table 2 are averaged over 10 runs. In eac h run, we randomly split the projects into training and test sets, in the ratio 3 : 1 . The training set is then further split into labeled and unlabeled sets by randomly selecting 30 labeled projects. W e experimented with increasing number of randomly chosen column labels, from none to all 25 availab le labels. The hyperparameters are as follows:  X  r =  X  c = 0 . 0001 ,  X  r = 0 . 69 ,  X  c = 0 . 27 chosen as described earlier. Results in Tables 2(c),2(d) are obtained with  X  = 10 for MR,  X  = 0 . 001 for MA.
 We observe that BIPARTITE performs significantly better than NMF on this dataset, and is competitve with supervised RLS performance that relies only on labeled data. By using LAPRLS , performance can be slightly boosted. We find that MR outperforms all appro aches significantly even with very few column labels. We conjecture that the comparatively low er mean and high variance in the per-formance of MA on this dataset is due to suboptimal local mini ma issues, which may be alleviated using annealing techniques or multiple random starts, comm only used for Transductive SVMs [3]. From Tables 2(c),2(d) we also observe that both methods give high quality out-of-sample extension on this problem. We have developed semi-supervised kernel methods that supp ort partial supervision along both di-mensions of the data. Empirical studies show promising resu lts and highlight the previously un-tapped benefits of feature supervision in semi-supervised s ettings. For an application of closely related algorithms to blog sentiment classification, we poi nt the reader to [14]. For recent work on text categorization with labeled features instead of label ed examples, see [8].
 [1] A. Banerjee, I. Dhillon, J. Ghosh, S.Merugu, and D.S. Mod ha. A generalized maximum en-[2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regular ization: A geometric framework for [3] O. Chapelle, B. Sch  X  olkopf, and A. Zien, editors. Semi-Supervised Learning . MIT Press, 2006. [4] F. Chung, editor. Spectral Graph Theory . AMS, 1997. [5] I. Dhillon. Co-clustering documents and words using bip artite spectral graph partitioning. In [6] C. Ding, X. He, and H.D. Simon. On the equivalence of nonne gative matrix factorization and [7] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegati ve matrix tri-factorizations for [8] G. Druck, G. Mann, and A. McCallum. Learning from labeled features using generalized [9] J. Gardiner, Laub A.J, Amato J.J, and Moler C.B. Solution of the Sylvester matrix equation [10] D. Harville. Matrix Algebra From a Statistician X  X  Perspective . Springer, New York, 1997. [11] T.M. Huang and V. Kecman. Semi-supervised learning fro m unbalanced labeled data an [12] A. Langville, C. Meyer, and R. Albright. Initializatio ns for the non-negative matrix factoriza-[13] T. Li and C. Ding. The relationships among various nonne gative matrix factorization methods [14] V. Sindhwani and P. Melville. Document-word co-regula rization for semi-supervised sentiment [15] N. Slonim and N. Tishby. Document clustering using word clusters via the information bottle-
