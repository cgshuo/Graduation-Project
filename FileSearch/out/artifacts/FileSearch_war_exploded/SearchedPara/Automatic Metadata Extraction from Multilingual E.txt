 Enterprises provide professionally authored content about their products/services in different la nguages for use in web sites and customer care. For customer care, personalization/personalized information delivery is becoming important since it re-encourages users to return to the service provider. Personalization usually requires both contextual and desc riptive metadata. But current metadata authored by content deve lopers is usually quite simple. In this paper, we introduce an automatic metadata extraction framework, which can extract multilingual metadata from the enterprise content, for a pers onalized information retrieval system. We introduce two new ont ologies for metadata creation and a novel semi-automatic topic vocabulary extraction algorithm. We demonstrate and evaluate our approach on the English and German Symantec Norton 360 technical content. Evaluations indicate that the propos ed approach produces rich and high quality metadata for a personalized information retrieval system. I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods  X  semantic networks ; I.7.5 [ Document and Text Processing ]: Document Capture  X  document analysis .
 Algorithms, Design and Experimentation. Metadata Generation, Semantic We b, Ontologies, Personalization. Enterprises offer highly technical and professionally authored information about their products a nd services for use in technical manuals, web site documents, help files and customer care. With the advancements of the Web, today X  X  users have different expectations from enterprises within a very competitive marketplace. Particularly users prefer advanced personalized customer support services in their preferred languages. Because of this, enterprises focus on custom er support. There is also a growing interest to personalization within enterprises, because of languages. In our case study, we use the English and German versions of the Symantec Norton 360 technical documentations. The content is formatted in XML and structured with DocBook DTD [5]. DocBook DTD is a common vocabulary for describing technical documentation using SGML or XML and widely used by many enterprise organizations, such as Sun Microsystems, Microsoft, Hewlett Packard, Red Ha t and Symantec. It is very broad and complex since it covers numerous variations and options about the domain. The objective is to generate a common vocabulary (ontology) for enterprise content and to extract rich multilingual metadata from DocBook documents in the form of RDF. The generated metadata then can be used for a personalized customer care. The next section explains new ontologies developed for this research. In order to extract structural me tadata about DocBook documents in RDF format, we created a new ontology from DocBook DTD, which we call DocBook Ontology. The DocBook Ontology is domain independent thus it does not have to be created every time an enterprise needs localization in different subject domains and is reusable across different domains of documents. The ontology is manually constructed in OWL Lite using the Prot X g X . The excerpt of the classes and properties of the DocBook Ontology are illustrated in Figure 1. The DocBook ontology can also serve as a common vocabulary for other enterpri ses since many enterprises use DocBook DTD for formatting their content. DocBook documents mainly have book , chapter and section elements. The book is the most co mmon top level element and has a number of sub-components such as preface , chapter , appendix , bibliography , etc. In our ontology, the book class have dc:hasPart relationship to its sub-component s as shown in Figure 1. The components generally contain bl ock elements. Therefore, we generated blockelements class and it is further divided into sub-classes, such as para , procedure , tables , etc. Each sub-class may have other sub-classes such as para , informaltable , step , etc. Components usually have dc:has Part relationship to these BlockElements sub-cla sses. In addition, co mponents may contain sections using dc:hasPart relationship. Sections can also contain block elements and are often recu rsive (e.g. nested); one section may use the content of another section. We have used subsection relationship to state this kind of relationships. In DocBook documents, information is usually re-used such as different books may share same ch apters. To enable sequential navigation of instances under a component, we introduce Sequence class. Every sequence instance has data about the parent instance (i.e. dc:hasPart relationships is us ed to determine parents) and the sequence number under this parent. In the DocBook ontology, we covered most of the elements a nd attributes defined by DocBook DTD version 5.0, which resulte d in a complex ontology since DocBook DTD itself is very broad. In our ontology, all elements and attributes are optional as in the DocBook DTD. In addition, in order to facilitate re-usability and interoperability, we used RDF bindings of DC metadata elements within the DocBook ontology to describe some of the DocBook DTD elements. For this purpose, we created metadata mappings between DocBook DTD and DC elements, such as DocBook author attribute is mapped to The algorithm initially uses a scri pt (Javascript) to extract primary and secondary terms from documents and combines them (primary+secondary). We combine two terms since the combined term is more in formative and unambiguous. The algorithm then states that the primary and primary+secondary term are instances of skos:Concept . Subsequently, it is declared that the primary term is the skos:broader of the combined term and the combined term is the skos:narrower of the primary term. The document is annotated with the combined term using dc:subject . Indexterms contain variati ons of the same topics in the case study and the algorithm generates syntactically different but semantically dupli cated terms. Manual cleaning is performed to remove duplicates using Prot X g X . Besides, terms do not have broader topics are analyzed and if possible manually replaced under a concept. We tested our algorithm on the English Norton 360 and generated a controlled vocabulary, which we call Topic Ontology. The Topic Ontology contains 1089 topics, 40 root topics and the longest depth in the hierarchy is four. This ontology can be used for user modelling; user X  X  knowledge and interest s can be linked to ontology concepts for personalization. The automatic metadata generation framework works as follows: First DocBook documents are crawled and document Data Mining to Generate Metadata for Personalization: In order to support personalization, ri ch and useful metadata about resources are required. In our cas e study, we generate metadata for personalization using Pro cess Ontology and LOM. By analyzing DocBook elements, metadata for processType can be created (Table 1). For example, if the document contains Step element, then processType is set to process:Task . In addition, for each processType, we estimate the covering percentage of each process type within the document by comparing size of the process type content to the size of the document (byte comparison). Moreover, we use fuzzy logic to create metadata values for difficulty, interactivity level and interactivity type. The details of this approach are not in the scope of this paper. To assess the metadata quality, we applied Ochoa and Duval X  X  metadata quality metrics: completeness, accuracy, provenance, conformance to expectations, l ogical consistency/coherence, timeliness and accessibility [4]. The formulas of metrics are summarized in [4]. Completeness is the degree to which a metadata record represents all the information needed to have an ideal representation. Metadata st andards and application profiles provide information about non-null metadata fields using mandatory element fields. In our case study, the ideal representation differs based on cla ss types. For section instances, the ideal metadata record has the title, subject, dc:hasPart (at least a link to block components), difficulty, interactivity type, interactivity level, process type, format, source, created, modified, publisher, creator and language. Weighted completeness not only counts non-null metadata fields but also weights each field according to relative importance to the application. Accuracy measures the correctness of meta data values, usually by the manually entered values by expert s. A user study is resumed but has not been completed. Provenance represents the origin of the metadata. We generate metadata about creation/modification date, creator, publisher and source that are useful for provenance. Timeliness represents the degree to which a metadata record remains current and useful over time. This could be calculated using the age of the record or frequency of usage. Accessibility metric measures the degree to which a metadata record is accessible both in terms of logical and physical accessibility. The logical accessibility measures readability and physical accessibility is how easy to find a metadata record in the repository (linkage). Linkage value is equal to number of other records that reference to it. Consistency measures the degree to which a metadata record matche s a standard definition. For example, LOM suggests that if an object X  X  interactivity type is active, then it should have high values of interactivity level. Conformance to expectations measures the degree to which the metadata record fulfills the requirements of a given community of use: vocabulary terms should be meaningful for users (we reuse DC and LOM, which are well esta blished vocabularies), metadata 
