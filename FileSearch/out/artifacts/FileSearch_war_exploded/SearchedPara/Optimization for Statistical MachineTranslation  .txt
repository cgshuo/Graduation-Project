 Graduate School of Information Science Nara Institute of Science and Technology Google Inc.
 models (Och and Ney 2002) and minimum error rate training (Och 2003), to the most recent optimization for MT. 1. Introduction
Machine translation (MT) has long been both one of the most promising applications of natural language processing technology and one of the most elusive. However, over approximately the past decade, huge gains in translation accuracy have been achieved these advances in the accuracy and coverage of MT, but among them two particularly statistical models from data, and massive increases in the amount of data available to learn SMT models. ematically model the translation process. The first was the pioneering work of Brown et al. (1993), who proposed the idea of SMT, and described methods for estimation of the parameters used in translation. In that work, the parameters of a word-based generative translation model were optimized to maximize the conditional likelihood of the training proposed by Och and Ney (2002) and Och (2003), who propose log-linear models for
MT, optimized to maximize either the probability of getting the correct sentence from linear model, and describing discriminative algorithms to optimize these parameters, it became possible to think of MT like many other structured prediction problems, such as POS tagging or parsing (Collins 2002).
 in many ways, and as a result requires a number of unique design decisions not neces-sary in other frameworks (as summarized in Table 1). The first is the search space that must be considered. The search space in MT is generally too large to expand exhaus-tively, so it is necessary to decide which subset of all the possible hypotheses should forward, with automatic evaluation measures for MT still being researched to this day.
From the optimization perspective, even once we have chosen an automatic evaluation necessary to incorporate it into a loss function to target. The loss function should be optimization algorithms. Finally, it is necessary to choose an optimization algorithm .
In many cases it is possible to choose a standard algorithm from other fields, but there MT.
 2 in a comprehensive and systematic fashion, covering a wide variety of topics, with a unified set of terminology. In Section 2, we first provide definitions of the problem of machine translation, describe briefly how models are built, how features are defined, translation optimization. In Section 4, we explain the selection of oracle translations, describe batch optimization algorithms, starting with the popular minimum error rate training, and continuing with other approaches using likelihood, margin, rank loss, or and then moving on to algorithms based on perceptron, margin, or likelihood-based systems up to large amounts of data through parallel computing, and in Section 8, we cover a number of other topics in MT optimization such as non-linear models, domain adaptation, and the relationship between MT evaluation and optimization. Finally, we conclude in Section 9, overviewing the methods described, making a brief note about which methods see the most use in actual systems, and outlining some of the unsolved problems in the optimization of MT systems. 2. Machine Translation Preliminaries and Definitions liminaries and definitions regarding MT in general and the MT optimization problem in particular. We focus mainly on the aspects of MT that are relevant to optimization, and readers may refer to Koehn (2010) or Lopez (2008) for more details about MT in general. 2.1 Machine Translation
Machine translation is the problem of automatically translating from one natural lan-guage to another. Formally, we define this problem by specifying F to be the collection
Machine translation systems perform this translation process by dividing the translation represented as hidden variables , which together form a derivation .
 variables will be the alignment between the phrases of the source and target sentences,
We will define D ( f ) to be the space of possible derivations that can be acquired from derivation uniquely determines the translation, but there can be multiple derivations all of these tuples.
 translations. In order to do so, in machine translation it is common to define a linear we first define an M -dimensional feature vector for each output and its derivation as following maximization problem where the dot product of the parameters and features is equivalent to the score assigned to a particular translation.
 must be considered, it is necessary to take advantage of the problem structure, making
MT optimization an instance of structured learning . 2.2 Model Construction
The first step of creating a machine translation system is model construction, in which translation models (TMs) are extracted from a large parallel corpus. The TM is usually multi-word phrase pairs or synchronous grammar rules (Koehn, Och, and Marcu 2003;
Chiang 2007), and scoring these rules according to several features explained in more that does not directly consider the optimization of translation accuracy, followed by an optimization step that explicitly considers the accuracy achieved by the system. survey, we focus on the optimization step, and thus do not cover elements of model construction that do not directly optimize an objective function related to translation accuracy, but interested readers can reference Koehn (2010) for more details. translation, only phrase pairs included in the TM will be expanded during the process of searching for the best translation (explained in Section 2.4).
 tant of which being that we must use separate data for training the TM and optimizing way that allows it to  X  X emorize X  long multi-word phrases included in the training data.
Using the same data to train the model parameters will result in overfitting , learning parameters that heavily favor using these memorized multi-word phrases, which will not be present in a separate test set. 4 perform optimization of parameters on a separate set of data consisting of around one thousand sentences, often called the development set . When learning the weights for common to perform cross-validation , holding out some larger portion of the training set for parameter optimization. It is also possible to perform leaving-one-out training, model before translating the sentence (Wuebker, Mauser, and Ney 2010). 2.3 Features for Machine Translation non-local, and dense vs. sparse.
 features have the potential to make search more difficult.
 highly informative feature functions, and sparse features , which define a large number of less informative feature functions. Dense features are generally easier to optimize, both from a computational point of view because the smaller number of features re-duces computational and memory requirements, and because the smaller number of translation accuracy. The remainder of this section describes some of the widely used features in more detail. present in nearly all translation hypotheses, are used in the majority of machine trans-lation systems. The most fundamental set of dense features are phrase/rule translation estimated using counts of each phrase derived from the training corpus as follows:
In addition, it is also common to use lexical weighting , which estimates parameters for each phrase pair or rule by further decomposing them into word-wise probabilities (Koehn, Och, and Marcu 2003). This helps more accurately estimate the reliability of can be calculated directly from the rules themselves, and are thus local features. fluency of translation e , and are usually modeled by n -grams
Note that the n -gram LM is computed over e regardless of the boundaries of phrase pairs or rules in the derivation, and is thus a non-local feature.
 the trade-off between longer or shorter derivations. There exist other features that are the distortion probabilities that are computed by the distance on the source side of target-adjacent phrase pairs. More refined lexicalized reordering models estimate the parameters from the training data based on the relative distance of two phrase pairs (Tillman 2004; Galley and Manning 2008). 2.3.2 Sparse features. Although dense features form the foundation of most SMT systems, other hand, large and sparse feature sets make the MT optimization problem signifi-cantly harder, and many of the optimization methods we will cover in the rest of this survey are aimed at optimizing rich feature sets.
 learn these features. Chiang, Knight, and Wang (2009) have noted that this problem can be alleviated by only selecting and optimizing the more frequent of the sparse features. rules, transforming a rule into a string simply indicating whether each word is a terminal (T) or non-terminal (N)
Count-based features can also be extended to cover other features of the translation, together (Simianer, Riezler, and Dyer 2012).
 6 similar to lexical weighting, focus on the correspondence between the individual words in the source language (Chiang, Knight, and Wang 2009; Xiao et al. 2011), fire features between every pair of words in the source or target sentences (Watanabe et al. 2007), or integrate bigrams on the target side (Watanabe et al. 2007). Of these, the former two bigram context and are thus non-local features.
 (Blunsom and Osborne 2008; Marton and Resnik 2008). In particular, phrase-based and sense) in the construction of the models, so introducing this information in the form of features has a potential for benefit. One way to introduce this information is to parse the input sentence before translation, and use the information in the parse tree in the translation rule matches, or partially matches (Marton and Resnik 2008), a span with a particular label, based on the assumption that rules that match a syntactic span are more likely to be syntactically reasonable. 2.3.3 Summary features. Although sparse features are useful, training of sparse features that has been widely demonstrated as being able to robustly estimate the parameters of sparse features, then condensing the sparse features into dense features and performing one more optimization pass (potentially with a different algorithm), has been widely used in a large number of research papers and systems (Dyer et al. 2009; He and Deng 2012; Flanigan, Dyer, and Carbonell 2013; Setiawan and Zhou 2013). A dense feature created from a large group of sparse features and their weights is generally called a summary feature , and can be expressed as follows groups, creating a dense feature for each group (Xiang and Ittycheriah 2011; Liu et al. 2013). 2.4 Decoding steps expressed by  X  and Manning 2014). For example, in phrase-based translation, for an n -gram language
Och, and Marcu 2003). The local feature functions, such as phrase translation probabili-ties in Section 2.3.1, require no context from partial derivations, and thus  X   X  right order on the target side while remembering the translated source word positions.
CYK+ algorithm (Chappelier and Rajman 1998) on the source side and generating of the search error caused by heuristic pruning, in which the best scoring hypothesis is not necessarily optimal in terms of given model parameters.
 (Koehn, Och, and Marcu 2003; Huang and Chiang 2007), and the space is succinctly represented by compact data structures, such as graphs (Ueffing, Och, and Ney 2002) (or lattices ) in phrase-based MT (Koehn, Och, and Marcu 2003) and hypergraphs (Klein
These data structures may be directly used as compact representations of all derivations for optimization.
 either from a lattice in Figure 1(b) or from a forest in Figure 1(c). It should be noted extracting a unique k -best list that maintains only the best scored derivation sharing by incorporating a penalty term when scoring derivations (Gimpel et al. 2013), or by performing Monte Carlo sampling to acquire a more diverse set of candidates (Blunsom and Osborne 2008). 8 based MT, this is implemented by adding additional features to reward hypotheses that match with the given target sentence (Liang, Zhang, and Zhao 2012; Yu et al. 2013). In
MT using synchronous grammars, it is carried out by biparsing over two languages, for instance, by a variant of the CYK algorithm (Wu 1997) or by a more efficient two-step algorithm (Dyer 2010b; Peitz et al. 2012). Even if we perform forced decoding, we are still not guaranteed that the decoder will be able to produce the reference translation (because of unknown words, reordering limits, or other factors). This problem can be possible to create a neighborhood of a forced decoding derivation by adding additional examples for discriminative learning algorithms (Xiao et al. 2011). 2.5 Evaluation
Once we have a machine translation system that can produce translations, we next must perform evaluation to judge how good the generated translations actually are. As the final consumer of machine translation output is usually a human, the most natural form of evaluation is manual evaluation by human annotators. However, because human evaluation is expensive and time-consuming, in recent years there has been a shift to automatic calculation of the quality of MT output.
 sentences F = n f ( i ) o was created by a human translator. The input F is automatically translated using a ma-chine translation system to acquire MT results  X  E =  X  e ( i ) it is deemed to be, according to automatic evaluation. In addition, as there are often many ways to translate a particular sentence, it is also possible to perform evaluation with multiple references created by different translators. There has also been some work on encoding a huge number of references in a lattice, created either by hand (Dreyer and Marcu 2012) or by automatic paraphrasing (Zhou, Lin, and Hovy 2006).
 taking statistics over the whole corpus, whereas sentence-level measures are calculated to corpus-level measures are applicable to sentence-level measures, but the opposite is not true, making this distinction important from the optimization point of view. in some of the methods that follow. Of course, there have been many other evaluation measures proposed since BLEU, with TER (Snover et al. 2006) and METEOR (Banerjee and Lavie 2005) being among the most widely used. The great majority of metrics other than BLEU are defined on the sentence level, and thus are conducive to optimization evaluation in MT optimization more completely in Section 8.3. 2.5.1 BLEU. BLEU is defined as the geometric mean of n -gram precisions (usually for n from 1 to 4), and a brevity penalty to prevent short sentences from receiving unfairly high evaluation scores. For a single reference sentence e and a corresponding system output  X  e , we can define c n (  X  e ) as the number of n -grams in  X  e , and m of n -grams in  X  e that match e once, and  X  is an operator for multisets that allows for consideration of multiple in-sets R = { e (1) , ... , e ( N ) } , where each sentence has M references e
BLEU score of the corresponding system outputs E = {  X  e as because evaluation of precision favors systems that output only the words and phrases that have high accuracy, and avoids outputting more difficult-to-translate content that 10 might not match the reference. The brevity penalty prevents this by discounting outputs that are shorter than the reference 2.5.2 BLEU+1. One thing to notice here is that BLEU is calculated by taking statistics preventing us from calculating BLEU on a single sentence, but in the single-sentence case it is common for the number of matches of higher order n -grams to become zero, resulting in a BLEU score of zero for the entire sentence. One common solution to this problem is the use of a smoothed version of BLEU, commonly referred to as BLEU+1 (Lin and Och 2004). In BLEU+1, we add one to the numerators and denominators of each n -gram of order greater than one We can then re-define a sentence-level BLEU using these smoothed counts evaluations heuristics it is possible to achieve a more accurate approximation (Nakov, Guzman, and
Vogel 2012). 2.6 The Optimization Setting
During the optimization process, we will assume that we have some data consisting of sources F = n f ( i ) o previous section, and that we would like to use these to optimize the parameters of the model. As mentioned in Section 2.5, it is also possible to use more than one reference that only one reference is used.
 in Section 2.4. To express whether this effect is a positive or negative one, we define how  X  X ad X  the translations generated when we use a particular w are. As the goal of optimization is to achieve better translations, we would like to choose parameters that reduce this loss. More formally, we can cast the problem as minimizing the expectation of ` (  X  ), or risk minimization : we are likely to be required to translate. However, in reality we will not know the true the loss on this data:
Because we are now optimizing on a single empirically derived set of training data, this framework is called empirical risk minimization .
 regularized empirical risk minimization , which will encompass most of the methods described in this survey, and is formalized as ization term, common choices for which include the L 2 regularizer  X  w &gt; w or the L optimization will be less aggressive in minimizing loss on the training data, reducing improve accuracy. 3. Defining a Loss Function the final performance of the optimized MT system, and also the possible choices for op-timization algorithms. This section describes several common choices for loss functions, and describes their various features. 12 3.1 Error
The first, and most straightforward, loss that we can attempt to optimize is error (Och 2003). We assume that by comparing the decoder X  X  translation result ence E , we are able to calculate a function error( E ,  X 
Section 2.5 as an evaluation measure for our system, it is natural to use 1  X  BLEU as an error function, so that as our evaluation improves, the error decreases. Converting this to a loss function that is dependent on the model parameters, we obtain the following loss expressing the error over the 1-best results obtained by decoding in Equation (1): translation performance, and these features make it perhaps the most commonly used are not continuously differentiable. This makes direct minimization of error a difficult optimization problem (particularly for larger feature sets), and thus a number of other, easier-to-optimize losses are used as well.
 methods we will introduce in the following sections, is zero X  X ne loss . Zero X  X ne loss focuses on whether an oracle translation is chosen as the system output. Oracle trans-lations can be vaguely defined as  X  X ood X  translations, such as the reference translation e we define the set of oracle translations for sentence i as o plugging the following zero X  X ne error function into Equation (17): where  X  e ( i ) is the one-best translation candidate, and  X  (  X  e of o ( i ) and zero otherwise. 3.2 Softmax Loss probability of candidates can be useful, however, for estimation of confidence measures or incorporation with downstream applications. Softmax loss is a loss that is similar to the zero X  X ne loss, but directly defines a probabilistic model and attempts to maximize and Ney 2002; Blunsom, Cohn, and Osborne 2008). we can define softmax loss ` softmax (  X  ) as follows: the softmax objective prefers parameter settings that assign high scores to the oracle translations, and lower scores to any other members of c ( i ) the entire list and calculating the numerators and denominators in Equation (19). It is also possible, but more involved, to calculate over lattices or forests by using dynamic programming algorithms such as the forward X  X ackward or inside X  X utside algorithms (Blunsom, Cohn, and Osborne 2008; Gimpel and Smith 2009). 3.3 Risk-Based Loss
In contrast to softmax loss, which can be viewed as a probabilistic version of zero X  X ne
Zens, Hasan, and Ney 2007; Li and Eisner 2009; He and Deng 2012). Specifically, risk is based on the expected error incurred by a probabilistic model parameterized by w .
This combines the advantages of the probabilistic model in softmax loss with the direct consideration of translation accuracy afforded by using error directly. In comparison to of each hypothesis X  X  probability
Given this probability, we then calculate the expected loss as follows: 14
In Equation (22), when  X  = 0 regardless of parameters w every hypothesis  X  e , d  X  will be assigned a uniform probability, and when  X  = 1 the probabilities are equivalent to error defined in Equation (17). This  X  can be adjusted in a way that allows for more effective search of the parameter space, as described in more detail in Section 5.5. 3.4 Margin-Based Loss paradigm of support vector machines (SVMs) (Joachims 1998), takes this a step further, finding parameters that explicitly maximize the distance, or margin, between correct and incorrect candidates. The main advantage of margin-based methods is that they are able to consider the error function, and often achieve high accuracy. These advantages make margin-based methods perhaps the second most popular loss used in current MT systems after direct minimization of error.
 ` margin ( F , E , C ; w ) = 1 N ( C ) where we define
In Equation (25), we first specify that for each pair of oracle candidates o oracle candidates c ( i ) \ o ( i ) , the margin w &gt; should be greater than the difference in the error  X  err(  X  ). but on the sentence level, and may not directly correspond to our corpus-level error error(  X  ).
 define  X   X  e ( i ) ,  X  d ( i )  X  X  X  c ( i ) \ o ( i ) as the 1-best translation candidate ` hinge ( F , E , C ; w ) = machine learning in general, is perceptron loss (Liang et al. 2006), which further re-receives a higher score than the oracle ing the relative margin (Eidelman, Marton, and Resnik 2013). To explain the relative margin, we first define the worst hypothesis as and then calculate the spread  X  err( e ( i ) ,  X  e ( i ) tween the oracle hypothesis  X   X  e ( i ) ,  X  d ( i )  X  and worst hypothesis  X   X  e term can then be added to the objective function to penalize parameter settings with misclassified. 3.5 Ranking Loss
Perceptron and margin losses attempted to distinguish between oracle and non-oracle framework (Herbrich, Graepel, and Obermayer 1999; Freund et al. 2003; Burges et al. 2005; Cao et al. 2007), where, for an arbitrary pair of translation candidates, a binary 16 particular pair of candidates in the training data  X  e k , d correct order, the following condition is satisfied:
This can be expressed as correct and incorrect answers.
 candidates with higher and lower error, which can cause problems when the ranking by error does not correlate well with the ranking measured by the model. The cross-entropy ranking loss solves this problem by softly fitting the model distribution to the distribution of ranking measured by errors (Green et al. 2014). 3.6 Mean Squared Error Loss
Finally, mean squared error loss is another method that does not make a hard zero X  one decision between the better and worse candidates, but instead attempts to directly by first finding the difference in errors between the two candidates  X  err( e defining the loss as the mean squared error of the difference between the inverse of the difference in the errors and the difference in the model scores ` mse ( F , E , C ; w ) = 1 N ( C ) 4. Choosing Oracles not a trivial task, and in this section we describe the details involved. 4.1 Bold vs. Local Updates
In other structured learning tasks such as part-of-speech tagging or parsing, it is com-to optimizing towards an actual human reference, which is called bold update (Liang et al. 2006). It should be noted that even if we know the reference e , we still need to Section 2.4) to obtain this derivation.
 not guaranteed that the decoder is able to actually produce the reference (for example, decoder, but only by using a derivation that would normally receive an extremely low probability.
 eses produced during the normal decoding process. The space of hypotheses used to output by the decoder as described in Section 2.4. Because of the previously mentioned difficulties with bold update, it has been empirically observed that local update tends to outperform bold update in online optimization (Liang et al. 2006). However, it also and we will describe this process in more detail in the following section. 4.2 Selecting Oracles and Approximating Corpus-Level Errors c that minimize the error function
One thing to note here is that error(  X  ) is a corpus-level error function. As mentioned decomposable on the sentence level, and those that are not. If this error function can be composed as the sum of sentence-level errors, such as BLEU+1, choosing the oracle independently sentence by sentence. 6 18 1: procedure O RACLE (  X  F , E , C  X  ) 4: repeat 5: for i  X  P ERMUTE ( { 1, ... , N } ) do . Random order 7: s  X  X  X  8: for k  X  X  1, ... , K } do 10: if s 0 &lt; s then . Update the oracle 13: else if s 0 = s then . Same error value 15: end if 16: end for 17: end for 18: until convergence . If O doesn X  X  change, converged 19: return O 20: end procedure sophisticated method, such as the greedy method of Venugopal and Vogel (2005). translation candidates in c ( i ) , try replacing the current oracle o and check the change in the error function (Line 9), and if the error decreases, replace the oracle with the tested candidate. This process is repeated until there is no change in O . 4.3 Selecting Oracles for Margin-Based Methods is not necessarily the case that the candidates with the maximum model score  X   X  e margin-based objectives, it is common to modify the criterion for selecting candidates to use in the update as follows (Chiang, Marton, and Resnik 2008; Chiang, Knight, and
Wang 2009):
Thus, we can replace  X   X  e ( i ) ,  X  d ( i )  X  and  X  e  X  ( i ) a margin of algorithm work harder to correctly classify very bad hypotheses than it does for hy-potheses that are only slightly worse than the oracle. Inference methods that consider framework. 5. Batch Methods batch learning, for every training example  X  f ( i ) and derivation in the respective sets E ( f ( i ) ) and D ( f data.
 follow the general procedure shown in Figure 3, performing iterations that alternate between decoding and optimization (Och and Ney 2002). In line 6, GEN( f n  X  e k , d that this subset is expressed using a k -best list kbest ( i ) or forests, as explained in Section 2.4.
 for the distribution over potential translations of f 20 1: procedure B ATCH L EARN (  X  F , E  X  = 4: for t  X  X  1 ... T } do 5: for i  X  X  1 ... N } do 8: end for 9: w ( t + 1)  X  arg min w  X  R M ` ( F , E , C ; w ) +  X   X  ( w ) 10: end for 12: end procedure errors in decoding means that we are not even guaranteed to find the highest-scoring hypotheses, this approximation is far from perfect. The effect of this approximation is this by adjusting the parameters to heavily favor very long hypotheses, far overshooting the actual optimal parameters. 7
 X  ( w ) regularized loss function ` (  X  ) and obtain new parameters w w do not change (Och 2003).
 describes methods for directly optimizing the error function. There are also methods for optimizing other losses such as those based on probabilistic models (Section 5.2), error margins (Section 5.3), ranking (Section 5.4), and risk (Section 5.5). 5.1 Error Minimization 5.1.1 Minimum Error Rate Training Overview. Minimum error rate training (MERT) (Och
Section 3.1. Because error is not continuously differentiable, MERT uses optimization methods that do not require the calculation of a gradient, such as iterative line search 1: procedure MERT( F , E , C ) 2:  X  w  X  X  X  3: for r  X  X  1 ... R } do 4: w (1)  X  R M . Initialize randomly 5: for t  X  X  1 ... T } do . Until convergence 6: for m  X  X  1 ... M } do . For each dimension 7:  X   X  m  X  arg min  X  ` error ( F , E , C ; w ( t ) +  X  b m 8: end for 11: end for 12: if ` error ( F , E , C ; w ( T + 1) ) &lt; ` error ( F , E , C ;  X  w ) then 14: end if 15: end for 16: return  X  w 17: end procedure inspired by Powell X  X  method (Och 2003; Press et al. 2007), or the Downhill-Simplex method ( Nelder-Mead method ) (Press et al. 2007; Zens, Hasan, and Ney 2007; Zhao and Chen 2009).
 w among the  X  for each of the M search dimensions, we perform an update using  X   X  that affords the largest reduction in error (lines 9 and 10). This algorithm can be deemed dimension.
 from the previous iteration, and the remaining R  X  1 have each element of w chosen randomly and uniformly from some interval, although it has also been shown that more 2008). forward, the line search in Line 7 of Figure 4 requires a bit more explanation. In this step, we would like to choose the  X  that results in the ordering of hypotheses in c achieves the lowest error. In order to do so, MERT uses an algorithm that allows for 22 exact enumeration of which of the K candidates in c ( i )  X  . Concretely, we define a ( f ( i ) , e , d )( = w ( j ) &gt; h ( f ( i ) , e , d )) and slope b ( f ter. Equation (40) is a function that returns the translation candidate with the highest scoring candidate as follows:
We can see that Equation (41) is a piecewise linear function (Papineni 1999; Och 2003), that particular  X  . In Figure 5, we show an example with the following four translation candidates:
If we set  X  to a very small value such as  X  X  X  , the candidate with the smallest slope, in of c 1 and c respect to  X  .
 finding a convex hull in computational geometry. A standard and efficient algorithm for finding a convex hull of multiple lines is the sweep line algorithm (Bentley and
Ottmann 1979; Macherey et al. 2008) (see Figure 7). Here, we assume L is a set of the lines corresponding to the K translation candidates in c ( i )  X  a ( l ), b ( l ),  X  ( l )  X  with intercept a ( l ) = a ( f we define  X  ( l ) as an intersection initialized to  X  X  X  . S lines in the order of their slope b ( l ), and if two lines l the one with the larger intercept a ( l k g (  X  ; f ) ` (  X  ) 24 1: procedure S WEEP L INE ( L = { X  a ( l ( k ) ), b ( l ( k ) 2: H  X  X  X  , j  X  1 4: for l  X  L 0 do 5: if j &gt; 1 then 6: while j &gt; 1 do 8: if  X  ( H j  X  1 ) &lt;  X  ( l ) then . l is in the envelope 9: break 10: end if 11: j  X  j  X  1 12: end while 13: end if 14: if j = 1 then . The leftmost line 15:  X  ( l )  X  X  X  X  16: end if 17: H j  X  l , j  X  j + 1 18: end for 20: end procedure with line under consideration at the highest point (lines 6 X 12), and update the envelope H . As L contains at most K lines, H  X  X  size is also at most K .
 in the envelope as  X  ( i ) 1 &lt;  X  X  X  &lt;  X  ( i ) j &lt;  X  X  X  &lt;  X  in the loss function that occurs when we move from one span [  X  [  X  j ,  X  spans in increasing order, keeping track of the difference  X  ` boundary, it is possible to efficiently calculate the loss curve over all spans of  X  . the procedure for a single sentence, by calculating the envelopes for each sentence in the data 1  X  i  X  N , and combining these envelopes into a single plane, it is relatively simple to perform this processing on the corpus level as well. It should be noted that for corpus-based evaluation measures such as BLEU, when performing corpus-level the sufficient statistics amount to n -gram counts c n , n -gram matches m sufficient statistics, and find a  X  that minimizes Equation (17) based on this curve. By repeating this line search for each parameter until we can no longer obtain a decrease, it is possible to find a local minimum in the loss function, even for non-convex or non-differential functions. 5.1.3 MERT X  X  Weaknesses and Extensions. Although MERT is widely used as the standard optimization procedure for MT, it also has a number of weaknesses, and a number of extensions to the MERT framework have been proposed to resolve these problems. each iteration of the training algorithm generally involves a number of random restarts, the results will generally change over multiple training runs, with the changes often being quite significant. Some research has shown that this randomness can be stabilized somewhat by improving the ability of the line-search algorithm to find a globally good solution by choosing random seeds more intelligently (Moore and Quirk 2008; Foster and Kuhn 2009) or by searching in directions that consider multiple features at once,
Manning 2008). Orthogonally to actual improvement of the results, Clark et al. (2011) mization algorithms for MT, it is better experimental practice to perform optimization multiple times, and report the resulting means and standard deviations over various optimization runs.
 particle swarm optimization , a distributed algorithm where many  X  X articles X  are each it moves towards the current local and global optima. Another alternative optimization algorithm is Galley and Quirk X  X  (2011) method for using linear programming to per-form search for optimal parameters over more than one dimension, or all dimensions spaces.
 space of possible translations. One way to improve this approximation is by performing
MERT over an exponentially large number of hypotheses encoded in a translation lattice (Macherey et al. 2008) or hypergraph (Kumar et al. 2009). It is possible to perform MERT used in MERT can be expressed as a semiring (Dyer 2010a; Sokolov and Yvon 2011), hypergraph using polynomial-time dynamic programming (the forward algorithm or k -best approximation by either sampling k -best candidates from the translation lattice (Chatterjee and Cancedda 2010), or performing forced decoding to find derivations that Zhao 2012).

Manning (2008) propose a method to incorporate regularization by not choosing the sidering the loss values for a few surrounding plateaus, helping to avoid points that have a low loss but are surrounded by plateaus with higher loss. It is also possible to incorporate regularization into MERT-style line search using an SVM-inspired margin-based objective (Hayashi et al. 2009) or by using scale-invariant regularization methods such as L 0 or a scaled version of L 2 (Galley et al. 2013). 26 to large numbers of features . When using only a standard set of 20 or so features, MERT time, required in Algorithm 4 scales linearly with the number of features. Thus train-training standard MERT on thousands or millions of features. It should be noted, how-intelligent search directions by calculating the gradient of expected BLEU, as explained in Section 5.5.2. 5.2 Gradient-Based Batch Optimization alent to the error function, which is not continuously differentiable and thus precludes the use of standard convex optimization algorithms used in other optimization prob-these algorithms for MT optimization (Smith and Eisner 2006; Blunsom and Osborne 2008).
 so we do not cover it in depth, but methods such as conjugate gradient (using first-order statistics) (Nocedal and Wright 2006) and the limited-memory Broyden-Fletcher-
Goldfarb-Shanno method (using second-order statistics) (Liu and Nocedal 1989) are when the loss is combined with a differentiable regularizer  X  ( w ), such as L larization. Using a non-differentiable regularizer such as L difficult, but can be handled by other algorithms such as orthant-wise limited-memory Quasi-Newton (Andrew and Gao 2007).
 guaranteed that these algorithms will not get stuck in local optima and instead they will generally contain multiple members. Thus, we cannot be entirely certain that we will reach a global optimum. 5.3 Margin-Based Optimization
Minimizing the margin-based loss described in Section 3.4, possibly with the addition
Methods to solve Equation (25) include sequential minimization optimization (Platt 1999), dual coordinate descent (Hsieh et al. 2008), as well as the quadratic program solvers used in standard SVMs (Joachims 1998).
 based online learning algorithms explained in Section 6.3, but in a batch setting where the whole training corpus is decoded before each iteration of optimization (Cherry and implement the optimization procedure within the decoder, whereas in a batch setting the implementation of the decoding and optimization algorithm can be performed separately. 5.4 Ranking and Linear Regression Optimization
The rank-based loss described in Section 3.5 is essentially the combination of multiple based or margin-based methods, and thus optimization itself can be solved with the in this setting is training time. At the worst, the number of pairwise comparisons for required for training.

Hopkins and May (2011) describe a method dubbed pairwise ranking optimization scores (Nakov, Guzm  X  an, and Vogel 2013), or performing Monte Carlo sampling (Roth et al. 2010; Haddow, Arun, and Koehn 2011) X  X re also possible and potentially increase accuracy. Recently, there has also been a method proposed that uses an efficient ranking ranking over all pairs (Dreyer and Dong 2015).

This loss can be minimized using standard techniques for solving least-squared-error linear regression (Press et al. 2007). 5.5 Risk Minimization non-convex, and thus some care must be taken to ensure that optimization does not which can be useful for optimization. Putting this formally, we first define the entropy of p  X  , w (  X  ) as
When  X  takes a small value this entropy will be high, indicating that the loss function is relatively smooth and less sensitive to local optima. Conversely, when  X   X  X  X  , the 28 optima. It has been noted that this fact can be used for effective optimization through deterministic annealing, the parameter  X  is not set as a hyperparameter, and instead the process (Smith and Eisner 2006; Li and Eisner 2009):
In Equation (45), T is the temperature , which can either be set as a hyperparameter, or gradually decreased from  X  to  X  X  X  (or 0) through a process of cooling (Smith and steps using a smoother function will allow us to approach the global optimum, and the later steps will allow us to approach the actual error function.

There have also been attempts to make the risk minimization framework appli-methods. 5.5.1 Linear BLEU. Linear BLEU (Tromble et al. 2008) provides an approximation for corpus-level BLEU that can be divided among sentences. Linear BLEU uses a Taylor tence will have on corpus-level BLEU. We define r as the total length of the reference translations, c as the total length of the candidates, and c reference respectively. Taking the equation for corpus-level BLEU (Papineni et al. 2002) and assuming that the n -gram counts are approximately equal for 1  X  n  X  4, we get the following approximation:
If we assume that when we add the sufficient statistics of a particular sentence e , the corpus-level statistics change to r 0 , c 0 , c 0 n , and m
BLEU in the logarithm domain as follows
If we make the assumption that there is no change in the brevity penalty,  X  log BLEU
Taylor expansion as follows:
As c 0  X  c is the length of e , and m 0 n  X  m n is the number of n -grams ( g the n -grams ( g ( i ) n ) in e ( i ) , the sentence-level error function err prior to optimization. approximations or formulations such as linear BLEU, the expectation of the sufficient a corpus-level error, in a manner similar to MERT (Pauls, Denero, and Klein 2009). matched n -gram counts. We define the k th translation candidate in c
Next, we define the expectation of the n -gram ( g n  X  e k length as r i , k . These values can be calculated as: to calculate statistics over lattices or forests using dynamic programming algorithms and tools such as the expectation semiring (Eisner 2002; Li and Eisner 2009). 30 we define our optimization problem as the maximization of xBLEU: derivation in Appendix A. 6. Online Methods are performed sequentially over the entire training data. In contrast, online learning online methods is that updates are performed on a much more fine-grained basis X  X t is often the case that online methods converge faster than batch methods, particularly on larger data sets. On the other hand, online methods have the disadvantage of being batch methods can be separate), and also generally being less stable (with sensitivity to the order in which the training data is processed or other factors).
 n 1: procedure O NLINE L EARN (  X  F , E  X  = 3: for t  X  X  1, ... , T } do 4:  X   X  F ( t ) ,  X  E ( t )  X  X  X  X  F , E  X  . Sample ( | X   X  5:  X  C ( t )  X  X   X  c ( j ) =  X  X  K j = 1 . k -best translation candidates 8: end for 10: end for 12: end procedure parallel data  X   X  F ( t ) ,  X  E ( t )  X  = n  X   X  f ( j )  X  f (line 9). In contrast to the batch learning algorithm in Figure 3, we do not merge the k -bests from previous iterations. In addition, optimization is performed not over the algorithms and objective functions that can be used.
 level evaluation measure such as BLEU, and it is necessary to define an error function set, we can perform parameter updates according to a number of different algorithms stochastic gradient descent (SGD) (Section 6.5). 6.1 Approximating the Error but with respect to a subset of data sampled from the corpus. This has consequences for the calculation of translation quality when using a corpus-level evaluation measure methods, the oracles chosen when considering the entire corpus will be different from the oracles chosen when considering a mini-batch. In general, the amount of difference between the corpus-level and mini-batch level oracles will vary depending on the size of a mini-batch, with larger mini-batches providing a better approximation (Tan et al. 2013; Watanabe 2012). Thus, when using smaller batches, especially single sentences, it is necessary to use methods to approximate the corpus-level error function as covered in the next two sections. 6.1.1 Approximation with a Pseudo-Corpus. The first method to approximate the corpus-level evaluation measure relies on creating a pseudo-corpus , and using it to augment given the training data  X  F , E  X  = n  X  f ( i ) , e ( i ) or the oracle calculated during the decoding step in line 7 of Figure 8. In the pseudo-corpus approximation, the sentence-level error for the translation candidate e by decoding the i th source sentence in the training data can be defined as the corpus-level error acquired when in  X  E , the i th sentence  X  e 6.1.2 Approximation with Decay. When approximating the error function using a pseudo-corpus. In addition, the size of differences in the sentence-level error becomes depen-dent on the number of other sentences in the corpus, making it necessary to perform 32 proposed that remembers a single set of sufficient statistics for the whole corpus, and upon every update forces these statistics to decay according to some criterion (Chiang, Marton, and Resnik 2008; Chiang, Knight, and Wang 2009; Chiang 2012).
 n -grams, and the reference length. When evaluating the error for a candidate e i th sentence in the training data, we first decay the sufficient statistics
For example, if we want to calculate BLEU using stat BLEU sufficient statistics, we can use the following equation: tics from the 1-best hypothesis found during decoding. When the training data is large, this function will place more emphasis on the recently generated examples, forgetting the older ones. 6.2 The Perceptron The most simple algorithm for online learning is the perceptron algorithm (shown in 1: procedure P ERCEPTRON (  X  F , E  X  = 3: for t  X  X  1 ... T } do 4:  X  f , e  X  X  X  X  F , E  X  . Assume K = 1 5:  X  c  X  GEN( f , w ( t ) ) . Decode with w ( t ) 6:  X   X  e ,  X  d  X  X  X   X  c . 1-best candidate 7:  X  e  X  , d  X   X  X  X   X  o  X   X  c . Oracle candidate 8: if  X  e  X  , d  X   X 6 =  X   X  e ,  X  d  X  then 10: end if 11: end for 13: end procedure
Equation (31) with respect to w is 1 K this algorithm can also be viewed as updating the parameters based on this gradient, taken translation  X   X  e ( i ) ,  X  d ( i )  X  weaker.
 straightforward approach here is to simply return the parameters resulting from the final iteration of the perceptron training, but in a popular variant called the averaged (Collins 2002). This averaging helps reduce overfitting of sentences that were viewed near the end of the training process, and is known to improve robustness to unknown data, resulting in higher translation accuracy (Liang et al. 2006). 6.3 MIRA
In line 9 of the perceptron algorithm in Figure 9, the parameters are updated using the simple and being guaranteed to converge when the data is linearly separable, but it is common for MT to handle feature sets that do not allow for linear separation of the 1-best and oracle hypotheses, resulting in instability in learning. The margin infused relaxed algorithm (MIRA) (Crammer and Singer 2003; Crammer et al. 2006) is another update in MIRA follows the same Equation (59), but also adds an additional term that prevents the parameters w ( t ) from varying largely from their previous values ` eters at the previous time step, in contrast to the losses in Section 3. are chosen to minimize Equation (60) (Watanabe et al. 2007; Chiang, Marton, and Resnik 34
MIRA objective contains an error-minimizing term similar to that of the perceptron, but also contains a regularization term with respect to the change in parameters compared formulated using Lagrange multipliers and solved using a quadratic programming update formula
The amount of update is proportional to the difference in loss between hypotheses, but of the parameter  X  MIRA is to control these over-aggressive updates. It should also be noted that when we set  X  ( t ) = 1, MIRA reduces to the perceptron algorithm. 6.4 AROW
One of the problems often pointed out with MIRA is that it is overagressive, mainly
Equation (60), even when the training example is an outlier or includes noise. One way to reduce the effect of noise is through the use of adaptive regularization of weights (AROW) (Crammer, Kulesza, and Dredze 2009; Chiang 2012). AROW is based on a updated more widely, and weights with lower variance being updated less widely. which we can express the AROW loss as follows: ` AROW (  X  F ,  X  E ,  X  C ; w ,  X  , w ( t ) ,  X  ( t ) ) = KL( N ( w ,  X  ) || N ( w where KL(  X  ) is the Kullback-Leibler divergence. The KL divergence term here plays a similar role to the parameter norm in MIRA, preventing large changes in the weight same loss term as MIRA. The main difference lies in the second term within the sum-hypotheses to be updated. This has the effect of decreasing the variance, and thus the learning rate, for frequently updated features, preventing large and aggressive moves of weights for features that already have been seen often and thus can be considered relatively reliable. In the case of K = 1 and where  X  MIRA minimizes this loss consists of the following updates of w and  X  (Crammer, Kulesza, and Dredze 2009): 6.5 Stochastic Gradient Descent
Stochastic gradient descent (SGD) is a gradient-based online algorithm for optimizing differentiable losses, possibly with the addition of L 2 regularization  X  gradient-based batch algorithms in Section 5.2. Compared with batch algorithms, SGD requires less memory and tends to converge faster, but requires more care (particularly answer.
 (Bottou 1998): gradually reduced according to a function update(  X  ) as learning progresses. One stan-dard method for updating  X  ( t ) according to the following formula rameters are updated and we obtain w ( t + 1) . Within this framework, in the perceptron algorithm  X  ( t ) is set to a fixed value, and in MIRA the amount of update changes for every mini-batch. SGD-style online gradient-based methods have been used in transla-tion for optimizing risk-based (Gao and He 2013), ranking-based (Watanabe 2012; Green et al. 2013), and other (Tillmann and Zhang 2006) objectives. When the regularization use forward-backward splitting (FOBOS) (Duchi and Singer 2009; Green et al. 2013) in which the optimization is performed in two steps: 36 First, we perform updates without considering the regularization term in Equation (71).
Second, the regularization term is applied in Equation (72), which balances regulariza-averaging , which keeps track of the average of previous gradients and optimizes for these along with the full regularization term (Xiao 2010).
 2011; Green et al. 2013) updates. The motivation behind AdaGrad is similar to that of
AROW (Section 6.4), using second-order covariance statistics  X  to adjust the learning rate of individual parameters based on their update frequency. If we define the SGD gradient as x =  X  ` (  X  F ( t ) ,  X  E ( t ) ,  X  C ( t ) rule for AdaGrad can be expressed as follows
Like AROW, it is common to use a diagonal covariance matrix, and each time an update learning rate for more commonly updated features. It should be noted that as the update decay  X  as is necessary in SGD. 7. Large-Scale Optimization
Up to this point, we have generally given a mathematical or algorithmic explanation of the various optimization methods, and placed a smaller emphasis on factors such only a small number of weights for dense features on a training set of around 1,000 sentences, efficiency is often less of a concern. However, when trying to move to larger larger training sets, parallelization of both the decoding process and the optimization process becomes essential. In this section, we outline the methods that can be used to in MT has seen wider use with respect to online learning methods, we will start with a description of online methods and touch briefly upon batch methods afterwards. 7.1 Large-Scale Online Optimization through parallelization (McDonald, Hall, and Mann 2010). An example of this is shown in Figure 10, where the training data  X  F , E  X  = n  X  f ( i ) 2), learning is performed locally over each shard  X  F s , E acquired through local learning are combined according to a function mix(  X  ), a process called parameter mixing (line 6). This can be considered an instance of the MapReduce (Dean and Ghemawat 2008) programming model, where each Map assigns shards to S
CPUs and performs training, and Reduce combines the resulting parameters w each shard, it is not necessarily guaranteed that the parameters are optimized for the 1: procedure P ARALLEL L EARN (  X  F , E  X  ) 2: Split  X  F , E  X  into S shards { X  F 1 , E 1  X  , ... ,  X  F 3: for s  X  X  1, ... , S } parallel do . Run shards in parallel 5: end for 6: w  X  mix ( { w 1 , ... , w S } ) . Parameter mixing 7: return w 8: end procedure contradictions between the parameters (McDonald, Hall, and Mann 2010). Because of this, when performing distributed online learning, it is common to perform parameter mixing several times throughout the training process, which allows the separate shards to share information and prevents contradiction between the learned parameters. Based on the timing of the update, these varieties of mixing are called synchronous update and asynchronous update . shown in Figure 11, learning is performed independently over each shard  X  F performed T 0 times, with each iteration initialized with the parameters w previous iteration (line 7).
 2007; McDonald, Hall, and Mann 2010; Watanabe 2012). In the mixing function it is most common to use the average of the parameters 2: Split  X  F , E  X  into S fragments { X  F 1 , E 1  X  , ... ,  X  F 3: w (1)  X  0 . Initialize parameters 4: for s  X  X  1, ... , S } parallel do . Run shards in parallel 5: for t  X  X  1, ... , T } do 6:  X  w ( t ) s  X  w ( t ) . Copy parameters to each shard 9: end for 10: end for 12: end procedure 38
Hall, and Mann 2010; Simianer, Riezler, and Dyer 2012). It should be noted that this relatively straightforward implementation using parallel processing infrastructure such as Hadoop (Eidelman 2012).
 that have been learned over all shards, and sets all the remaining parameters to zero, allowing for a simple sort of feature selection. In particular, we define a S  X  M matrix that combines the parameters  X  w ( t + 1) s at each shard as  X  w takes the L 2 norm of each matrix column, and averages the columns with high norm values while setting the rest to zero. anteed to converge (McDonald, Hall, and Mann 2010), parameter mixing only occurs this problem, asynchronous update sends information about parameter updates to each shard asynchronously, allowing the parameters to be updated more frequently, resulting in faster learning (Chiang, Marton, and Resnik 2008; Chiang, Knight, and Wang 2009). performing optimization on the mini-batch level (line 12). 2: Split  X  F , E  X  into S fragments { X  F 1 , E 1  X  , ... ,  X  F 3: w (1)  X  0 . Initialize parameters 4: for s  X  X  1, ... , S } parallel do . Run s shards in parallel 5: for t  X  X  1, ... , T } do 11: end for 14: end for 15: end for 17: end procedure operator mix async (  X  ) in line 13 sends the result of the mini-batch level update mensions M in the parameter vector is extremely large, and each mini-batch will only update a small fraction of these parameters, by only sending the parameters that have actually changed in each mini-batch update we can greatly increase the efficiency of the other shards and mixes them together to acquire the full update vector
It should be noted that at mixing time, there is no need to wait for the update vectors from all of the other shards; update can be performed with only the update vectors that parameters that reflect all the most recent updates performed on other shards. However, as updates are performed much more frequently than in synchronous update, it is easier to avoid local optima, and learning tends to converge faster. It can also be shown that (under some conditions) the amount of accuracy lost by this delay in update is bounded (Zinkevich, Langford, and Smola 2009; Recht et al. 2011). 7.2 Large-Scale Batch Optimization
Compared with online learning, within the batch optimization framework, paralleliza-tion is usually straightforward. Often, decoding takes the majority of time required for the optimization process, and because the parameters will be identical for each sentence of parallelizing optimization itself depends slightly on the optimization algorithm, but is generally possible to achieve in a number of ways.
 mization runs . The most obvious example of this is MERT, where random restarts are these on different nodes, and finally check which run achieved the best accuracy and use that result.
 the others. Each of these line searches can be performed in parallel, and the direction allowing for the greatest gain in accuracy is chosen when all threads have completed.
Section 7.1.1, we first split the data into shards  X  F example, in MERT these sufficient statistics would consist of the envelope for each of 40 consist of the gradient calculated with respect to only the data on the shard. Finally, statistics from each shard, either by merging the envelopes, or by adding the gradients. 8. Other Topics in MT Optimization and the interaction between evaluation measures (Section 8.3) or search (Section 8.4) and optimization. 8.1 Non-Linear Models
Note that up until this point, all models that we have considered calculate the scores for translation hypotheses according to a linear model, where the score is calculated according to the dot product of the features and weights shown in Equation (1). How-ever, linear models are obviously limited in their expressive power, and a number of combinations.
 cality assumption breaks down when moving beyond a simple linear scoring function, and overcoming this problem is the main obstacle to applying nonlinear models to MT (or structured learning in general). A number of countermeasures to this problem exist:
Reranking: The most simple and commonly used method for incorporating non-
Improved Search Techniques: Although there is no general-purpose solution to incor-examples of nonlinear functions are those utilizing kernels, and methods applied to MT include kernel-like functions over the feature space such as the Parzen window, binning, and Gaussian kernels (Nguyen, Mahajan, and He 2007), or the n -spectrum string kernel and Szedmak 2007). Neural networks are another popular method for modeling non-late new local features for MT (Liu et al. 2013). Methods such as boosting or stacking, which combine together multiple parameterizations of the translation model, have been incorporated through reranking (Duh and Kirchhoff 2008; Lagarda and Casacuberta 2008; Duan et al. 2009; Sokolov, Wisniewski, and Yvon 2012b), or ensemble decoding (Razmara and Sarkar 2013). Regression decision trees have also been introduced as a method for inducing nonlinear functions, incorporated through history-based search features local to the search state (Toutanova and Ahn 2013). 8.2 Domain-Dependent Optimization
One widely acknowledged feature of machine learning problems in general is that the parameters are sensitive to the domain of the data, and by optimizing the parameters with data from the target domain it is possible to achieve gains in accuracy. In machine translation, this is also very true, although much of the work on domain adaptation has focused on adapting the model learning process prior to explicit optimization towards an evaluation measure (Koehn and Schroeder 2007). However, there are a few works on optimization-based domain adaptation in MT, as we will summarize subsequently. optimization and model training is also helpful (Ueffing, Haffari, and Sarkar 2007; Li domain training data (Mathur, Mauro, and Federico 2013; Denkowski, Dyer, and Lavie 2014).
 domain data to achieve more robust parameter estimates. This is essentially equivalent of MT there have been methods proposed to perform Bayesian adaptation of proba-bilistic models (Sanchis-Trilles and Casacuberta 2010), and online update using ultra-conservative algorithms (Liu et al. 2012). This can be extended to cover multiple target domains using multi-task learning (Cui et al. 2013). 42 to make the distinction between in-domain translation (when the model training data matches the test domain) and cross-domain translation (when the model training data accordingly to account for this (Pecina, Toral, and van Genabith 2012). It has also been shown that building TMs for several domains and tuning the parameters to maximize translation accuracy can improve MT accuracy on the target domain (Haddow 2013).
Another option for making the distinction between in-domain and out-of-domain data ing for the learning of different weights for different domains (Clark, Lavie, and Dyer 2012). 8.3 Evaluation Measures and Optimization reduce MT error defined using an evaluation measure, generally BLEU. However, as as measured by human evaluators.
 optimization on human assessments of the generated translations (Cer, Manning, and
Jurafsky 2010; Callison-Burch et al. 2011). These studies showed the rather surprising result that despite the fact that other evaluation measures had proven superior to BLEU with regards to post facto correlation with human evaluation, a BLEU-optimized system proved superior to systems tuned using other metrics. Since this result, however, there
TESLA (Liu, Dahlmeier, and Ng 2011) and MEANT (Lo et al. 2013) achieve superior results to BLEU-optimized systems.
 evaluation measures of translation quality (Zaidan and Callison-Burch 2009). However, sentences.
 time. The easiest way to do so is to simply use the linear interpolation of two or more metrics as the error function (Dyer et al. 2009; He and Way 2009; Servan and Schwenk 2011): 2012), which achieve errors lower than all other hypotheses on at least one evaluation measure,
To incorporate this concept of Pareto optimality into optimization, the Pareto-optimal that the Pareto-optimal hypotheses achieve a higher score than those that are not Pareto optimal. This method has also been extended to take advantage of ensemble decoding,
Sarkar, and Duh 2013). 8.4 Search and Optimization
As mentioned in Section 2.4, because MT decoders perform approximate search, they may make search errors and not find the hypothesis that achieves the highest model score. There have been a few attempts to consider this fact in the optimization process. gence guarantees of the structured perceptron no longer hold when using approximate search. The first method that can be used to resolve this problem is the early updating ing strategy is a variety of bold updates, where the decoder output e equal to the reference e ( i ) . Decoding proceeds as normal, but the moment the correct
Fayong, and Guo 2012; Yu et al. 2013). In the max-violation perceptron, forced decoding is performed to acquire a derivation  X  e  X  ( i ) , d  X  ( i )  X   X  i ) ,  X  d ( i )  X  exceeds that of the partial hypothesis  X  e point of  X  X aximum violation X ).
 search errors using an arbitrary optimization method. It does so by defining an evalu-the search process, and optimizes parameters for k -best lists of partial derivations. and Galley 2012). Using this method, it is possible to adjust the beam width, distortion penalty, or other parameters that actually affect the size and shape of the derivation space, as opposed to simply rescoring hypotheses within it. 9. Conclusion expansions to large scale data, and a number of other topics. While these optimization algorithms have already led to large improvements in machine translation accuracy, the task of MT optimization is, as stated in the Introduction, an extremely hard one that is far from solved. 44 tives. The final accuracy achieved is, of course, one of the most important factors, but speed, scalability, ease of implementation, final resulting model size, and many other factors play an important role. We can assume that the algorithms being used outside of way. Although it is difficult to exactly discern exactly which algorithms are seeing the largest amount of use (industrial SMT systems rarely disclose this sort of information publicly), one proxy for this is to look at systems that performed well on shared tasks we show the percentage of WMT systems using each optimization algorithm for the past four years, both including all systems, and systems that achieved the highest level of human evaluation in the resource-constrained setting for at least one language pair. optimization algorithm. However, starting in WMT 2013, we can see a move to systems systems.

MERT when using up to 20 features, and MIRA when using a large number of features of features (tens of thousands, or millions) in actual competitive systems, with a few risk-based objective and gradient-based optimization algorithms, often combining the features into summary features and performing a final tuning pass with MERT. here:
Evaluation Measures for Optimization: Although many evaluation measures show
Better Training/Utilization of Nonlinear Scoring Functions: Nonlinear functions us-
Resolving these, and many other, open questions will likely be among the top priorities of MT optimization research in the years to come.
 Appendix A: Derivation for xBLEU Gradients
In this appendix, we explain in detail how to derive a gradient for the xBLEU objective in Equation (54), which has not been described completely in previous work. algorithms. To remedy this problem, Rosti et al. (2010) propose the use of the following differentiable approximation for the brevity penalty.
Using Equation (54) and the approximated brevity penalty in Equation (A.2), we can express xBLEU = exp( P )  X  B , leading to the following equation
Taking the derivative of xBLEU with respect to w , we get 46
Thus, and
Additionally, the following equation holds: gradient-based methods. However, like MR using sentence-level evaluation mentioned in Section 5.5, the evaluation measure is not convex, and the same precautions need to be taken to avoid falling into local optima.
 References 48 50 52
