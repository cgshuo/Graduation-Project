 Predicting ad click X  X hrough rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learn-ing based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates.

We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for as-sessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated manage-ment of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promis-ing results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical ad-vances and practical engineering in this industrial setting, and to show the depth of challenges that appear when ap-plying traditional machine learning methods in a complex dynamic system.
 I.5.4 [ Computing Methodologies ]: Pattern Recognition X  Applications online advertising, data mining, large-scale learning
Online advertising is a multi-billion dollar industry that has served as one of the great success stories for machine learning. Sponsored search advertising, contextual advertis-ing, display advertising, and real-time bidding auctions have all relied heavily on the ability of learned models to predict ad click X  X hrough rates accurately, quickly, and reliably [28, 15, 33, 1, 16]. This problem setting has also pushed the field to address issues of scale that even a decade ago would have been almost inconceivable. A typical industrial model may provide predictions on billions of events per day, using a correspondingly large feature space, and then learn from the resulting mass of data.

In this paper, we present a series of case studies drawn from recent experiments in the setting of the deployed sys-tem used at Google to predict ad click X  X hrough rates for sponsored search advertising. Because this problem setting is now well studied, we choose to focus on a series of topics that have received less attention but are equally important in a working system. Thus, we explore issues of memory savings, performance analysis, confidence in predictions, cal-ibration, and feature management with the same rigor that is traditionally given to the problem of designing an effec-tive learning algorithm. The goal of this paper is to give the reader a sense of the depth of challenges that arise in real industrial settings, as well as to share tricks and insights that may be applied to other large-scale problem areas.
When a user does a search q , an initial set of candidate ads is matched to the query q based on advertiser-chosen keywords. An auction mechanism then determines whether these ads are shown to the user, what order they are shown in, and what prices the advertisers pay if their ad is clicked. In addition to the advertiser bids, an important input to the auction is, for each ad a , an estimate of P (click | q , a ), the probability that the ad will be clicked if it is shown.
The features used in our system are drawn from a vari-ety of sources, including the query, the text of the ad cre-ative, and various ad-related metadata. Data tends to be extremely sparse, with typically only a tiny fraction of non-zero feature values per example.

Methods such as regularized logistic regression are a nat-ural fit for this problem setting. It is necessary to make predictions many billions of times per day and to quickly update the model as new clicks and non-clicks are observed. Of course, this data rate means that training data sets are enormous. Data is provided by a streaming service based on the Photon system  X  see [2] for a full discussion.

Because large-scale learning has been so well studied in recent years (see [3], for example) we do not devote signif-Figure 1: High-level system overview. Sparsification is covered in Section 3, probabilistic feature inclu-sion in Section 4, progressive validation in Section 5, and calibration methods in Section 7. icant space in this paper to describing our system archi-tecture in detail. We will note, however, that the training methods bear resemblance to the Downpour SGD method described by the Google Brain team [8], with the difference that we train a single-layer model rather than a deep net-work of many layers. This allows us to handle significantly larger data sets and larger models than have been reported elsewhere to our knowledge, with billions of coefficients. Be-cause trained models are replicated to many data centers for serving (see Figure 1), we are much more concerned with sparsification at serving time rather than during training.
For learning at massive scale, online algorithms for gen-eralized linear models (e.g., logistic regression) have many advantages. Although the feature vector x might have bil-lions of dimensions, typically each instance will have only hundreds of nonzero values. This enables efficient training on large data sets by streaming examples from disk or over the network [3], since each training example only needs to be considered once.

To present the algorithm precisely, we need to establish some notation. We denote vectors like g t  X  R d with bold-face type, where t indexes the current training instance; the i th entry in a vector g t is denoted g t,i . We also use the compressed summation notation g 1: t = P t s =1 g s .
If we wish to model our problem using logistic regression, we can use the following online framework. On round t , we are asked to predict on an instance described by feature vector x t  X  R d ; given model parameters w t , we predict p  X  ( w t  X  x t ), where  X  ( a ) = 1 / (1 + exp(  X  a )) is the sigmoid function. Then, we observe the label y t  X  X  0 , 1 } , and suffer the resulting LogLoss (logistic loss), given as the negative log-likelihood of y t given p . It is straightforward to show O ` t ( w ) = (  X  ( w  X  x t )  X  y t ) x t = ( p t gradient is all we will need for optimization purposes.
Online gradient descent 1 (OGD) has proved very effective for these kinds of problems, producing excellent prediction accuracy with a minimum of computing resources. How-ever, in practice another key consideration is the size of the final model; since models can be stored sparsely, the num-ber of non-zero coefficients in w is the determining factor of
OGD is essentially the same as stochastic gradient descent; the name online emphasizes we are not solving a batch prob-lem, but rather predicting on a sequence of examples that need not be IID.
 Algorithm 1 Per-Coordinate FTRL-Proximal with L 1 and L 2 Regularization for Logistic Regression memory usage.

Unfortunately, OGD is not particularly effective at pro-ducing sparse models. In fact, simply adding a subgradi-ent of the L 1 penalty to the gradient of the loss ( O w ` will essentially never produce coefficients that are exactly zero. More sophisticated approaches such as FOBOS and truncated gradient do succeed in introducing sparsity [11, 20]. The Regularized Dual Averaging (RDA) algorithm pro-duces even better accuracy vs sparsity tradeoffs than FO-BOS [32]. However, we have observed the gradient-descent style methods can produce better accuracy than RDA on our datasets [24]. The question, then, is can we get both the sparsity provided by RDA and the improved accuracy of OGD? The answer is yes, using the  X  X ollow The (Prox-imally) Regularized Leader X  algorithm, or FTRL-Proximal. Without regularization, this algorithm is identical to stan-dard online gradient descent, but because it uses an alter-native lazy representation of the model coefficients w , L regularization can be implemented much more effectively.
The FTRL-Proximal algorithm has previously been framed in a way that makes theoretical analysis convenient [24]. Here, we focus on describing a practical implementation. Given a sequence of gradients g t  X  R , OGD performs the update where  X  t is a non-increasing learning-rate schedule, e.g.,  X  . The FTRL-Proximal algorithm instead uses the update w where we define  X  s in terms of the learning-rate schedule different, but in fact when we take  X  1 = 0, they produce an identical sequence of coefficient vectors. However, the FTRL-Proximal update with  X  1 &gt; 0 does an excellent job of inducing sparsity (see experimental results below).
On quick inspection, one might think the FTRL-Proximal update is harder to implement than gradient descent, or requires storing all the past coefficients. In fact, however, only one number per coefficient needs to be stored, since we Table 1: FTRL results, showing the relative number of non-zero coefficient values and AucLoss (1  X  AUC ) ) for competing approaches (smaller numbers are bet-ter for both). Overall, FTRL gives better sparsity for the same or better accuracy (a detriment of 0.6% is significant for our application). RDA and FOBOS were compared to FTRL on a smaller prototyping dataset with millions of examples, while OGD-Count was compared to FTRL on a full-scale data set. can re-write the update as the argmin over w  X  R d of beginning of round t we update by letting z t = z t  X  1 + g ( coordinate bases by Thus, FTRL-Proximal stores z  X  R d in memory, whereas OGD stores w  X  R d . Algorithm 1 takes this approach, but also adds a per-coordinate learning rate schedule (discussed next), and supports L 2 regularization of strength  X  2 . Alter-natively, we could store  X   X  t z t instead of storing z t then, when  X  1 = 0, we are storing exactly the normal gradi-ent descent coefficient. Note that when  X  t is a constant value  X  and  X  1 = 0, it is easy to see the equivalence to online gra-dient descent, since we have w t +1 =  X   X  z t =  X   X  P t s =1 exactly the point played by gradient descent.

Experimental Results. In earlier experiments on smaller prototyping versions of our data, McMahan [24] showed that FTRL-Proximal with L 1 regularization significantly outper-formed both RDA and FOBOS in terms of the size-versus-accuracy tradeoffs produced; these previous results are sum-marized in Table 1, rows 2 and 3.

In many instances, a simple heuristic works almost as well as the more principled approach, but this is not one of those cases. Our straw-man algorithm, OGD-Count, simply main-tains a count of the number of times it has seen a feature; until that count passes a threshold k , the coefficient is fixed at zero, but after the count passes k , online gradient descent (without any L 1 regularization) proceeds as usual. To test FTRL-Proximal against this simpler heuristic we ran on a very large data set. We tuned k to produce equal accuracy to FTRL-Proximal; using a larger k leads to worse AucLoss. Results are given in Table 1, row 4.

Overall, these results show that FTRL-Proximal gives sig-nificantly improved sparsity with the same or better predic-tion accuracy.
The standard theory for online gradient descent suggests using a global learning rate schedule  X  t = 1  X  t that is com-mon for all coordinates [34]. A simple thought experiment shows that this may not be ideal: suppose we are estimat-ing Pr(heads | coin i ) for 10 coins using logistic regression. Each round t , a single coin i is flipped, and we see a feature vector x  X  R 10 with x i = 1 and x j = 0 for j 6 = i . Thus, we are essentially solving 10 independent logistic regression problems, packaged up into a single problem.

We could run 10 independent copies of online gradient descent, where the algorithm instance for problem i would use a learning rate like  X  t,i = 1  X  n of times coin i has been flipped so far. If coin i is flipped much more often than coin j , then the learning rate for coin i will decrease more quickly, reflecting the fact we have more data; the learning rate will stay high for coin j , since we have less confidence in our current estimate, and so need to react more quickly to new data.

On the other hand, if we look at this as a single learn-ing problem, the standard learning rate schedule  X  t is applied to all coordinates: that is, we decrease the learn-ing rate for coin i even when it is not being flipped. This is clearly not the optimal behavior. In fact, Streeter and McMahan [29] have shown a family of problems where the performance for the standard algorithm is asymptotically much worse than running independent copies. 2 Thus, at least for some problems, per-coordinate learning rates can offer a substantial advantage.

Recall that g s,i is the i th coordinate of the gradient g O ` s ( w s ). Then, a careful analysis shows the per-coordinate rate is near-optimal in a certain sense. 3 In practice, we use a learning rate where  X  and  X  are chosen to yield good per-formance under progressive validation (see Section 5.1). We have also experimented with using a power on the counter n t,i other than 0 . 5. The optimal value of  X  can vary a fair bit depending on the features and dataset, and  X  = 1 is usually good enough; this simply ensures that early learning rates are not too high.

As stated, this algorithm requires us to keep track of both the sum of the gradients and the sum of the squares of the gradients for each feature. Section 4.5 presents an al-ternative memory-saving formulation where the sum of the squares of the gradients is amortized over many models.
A relatively simple analysis of per-coordinate learning rates appears in [29], as well as experimental results on small Google datasets; this work builds directly on the approach of Zinkevich [34]. A more theoretical treatment for FTRL-Proximal appears in [26]. Duchi et al. [10] analyze RDA and mirror-descent versions, and also give numerous exper-imental results.

Experimental Results. We assessed the impact of per-coordinate learning rates by testing two identical models,
Formally, regret (see, e.g., [34]) is  X ( T 2 3 ) for standard gra-dient descent, while independent copies yields regret O ( T
For a fixed sequence of gradients, if we take  X  to be twice the maximum allowed magnitude for w i , and  X  = 0, we bound our regret within a factor of ble regret bound (not regret) for any non-increasing per-coordinate learning rate schedule [29]. Table 2: Effect Probabilistic Feature Inclusion. Both methods are effective, but the bloom filter-ing approach gives better tradeoffs between RAM savings and prediction accuracy. one using a single global learning rate and one using per-coordinate learning rates. The base parameter  X  was tuned separately for each model. We ran on a representative data set, and used AucLoss as our evaluation metric (see Sec-tion 5). The results showed that using a per-coordinate learning rates reduced AucLoss by 11.2% compared to the global-learning-rate baseline. To put this result in context, in our setting AucLoss reductions of 1% are considered large.
As described above, we use L 1 regularization to save mem-ory at prediction time. In this section, we describe addi-tional tricks for saving memory during training.
In many domains with high dimensional data, the vast majority of features are extremely rare. In fact, in some of our models, half the unique features occur only once in the entire training set of billions of examples. 4
It is expensive to track statistics for such rare features which can never be of any real use. Unfortunately, we do not know in advance which features will be rare. Pre-processing the data to remove rare features is problematic in an on-line setting: an extra read and then write of the data is very expensive, and if some features are dropped (say, because they occur fewer than k times), it is no longer possible to try models that use those feature to estimate the cost of the pre-processing in terms of accuracy.

One family of methods achieves sparsity in training via an implementation of L 1 regularization that doesn X  X  need to track any statistics for features with a coefficient of zero (e.g.,[20]). This allows less informative features to be re-moved as training progresses. However, we found that this style of sparsification leads to an unacceptable loss in accu-racy compared to methods (like FTRL-Proximal) that track more features in training and sparsify only for serving. An-other common solution to this problem, hashing with colli-sions, also did not give useful benefit (see Section 9.1).
Another family of methods we explored is probabilistic feature inclusion, in which new features are included in the model probabilistically as they first occur. This achieves the effect of pre-processing the data, but can be executed in an online setting.

We tested two methods for this approach.
Because we deal exclusively with extremely sparse data in this paper, we say a feature  X  X ccurs X  when it appears with a non-zero value in an example.
Experimental Results. The effect of these methods is seen in Table 2, and shows that both methods work well, but the Bloom filter approach gives a better set of tradeoffs for RAM savings against loss in predictive quality.
Naive implementations of OGD use 32 or 64 bit floating point encodings to store coefficient values. Floating point encodings are often attractive because of their large dynamic range and fine-grained precision; however, for the coefficients of our regularized logistic regression models this turns out to be overkill. Nearly all of the coefficient values lie within the range (  X  2 , +2). Analysis shows that the fine-grained precision is also not needed [14], motivating us to explore the use of a fixed-point q2.13 encoding rather than floating point.

In q2.13 encoding, we reserve two bits to the left of the binary decimal point, thirteen bits to the right of the binary decimal point, and a bit for the sign, for a total of 16 bits used per value.

This reduced precision could create a problem with accu-mulated roundoff error in an OGD setting, which requires the accumulation of a large number of tiny steps. (In fact, we have even seen serious roundoff problems using 32 bit floats rather than 64.) However, a simple randomized rounding strategy corrects for this at the cost of a small added regret term [14]. The key is that by explicitly rounding, we can ensure the discretization error has zero mean.

In particular, if we are storing the coefficient w , we set where R is a random deviate uniformly distributed between 0 and 1. g i, rounded is then stored in the q2.13 fixed point format; values outside the range [  X  4 , 4) are clipped. For FTRL-Proximal, we can store  X  t z t in this manner, which has similar magnitude to w t .

Experimental Results. In practice, we observe no mea-surable loss comparing results from a model using q2.13 en-coding instead of 64-bit floating point values, and we save 75% of the RAM for coefficient storage.
When testing changes to hyper-parameter settings or fea-tures, it is often useful to evaluate many slight variants of one form or another. This common use-case allows for ef-ficient training strategies. One interesting piece of work in this line is [19], which used a fixed model as a prior and al-lowed variations to be evaluated against residual error. This approach is very cheap, but does not easily allow the evalu-ation of feature removals or alternate learning settings.
Our main approach relies on the observation that each co-ordinate relies on some data that can be efficiently shared across model variants, while other data (such as the coeffi-cient value itself) is specific to each model variant and can-not be shared. If we store model coefficients in a hash table, we can use a single table for all of the variants, amortizing the cost of storing the key (either a string or a many-byte hash). In the next section (4.5), we show how the per-model learning-rate counters n i can be replaced by statistics shared by all the variants, which also decreases storage.

Any variants which do not have a particular feature will store the coefficient for that feature as 0, wasting a small amount of space. (We enforce that by setting the learning rate for those features to 0.) Since we train together only highly similar models, the memory savings from not repre-senting the key and the counts per model is much larger than the loss from features not in common.

When several models are trained together, the amortized cost is driven down for all per-coordinate metadata such as the counts needed for per-coordinate learning rates, and the incremental cost of additional models depends only on the additional coefficient values that need to be stored. This saves not only memory, but also network bandwidth (the values are communicated across the network in the same way, and we read the training data only once), CPU (only one hash table lookup rather than many, and features are generated from the training data only once rather than once per model), and disk space. This bundled architecture in-creases our training capacity significantly.
Sometimes we wish to evaluate very large sets of model variants together that differ only by the addition or removal of small groups of features. Here, we can employ an even more compressed data structure that is both lossy and ad hoc but in practice gives remarkably useful results. This Sin-gle Value Structure stores just one coefficient value for each coordinate which is shared by all model variants that include that feature, rather than storing separate coefficient values for each model variant. A bit-field is used to track which model variants include the given coordinate. Note that this is similar in spirit to the method of [19], but also allows the evaluation of feature removals as well as additions. The RAM cost grows much more slowly with additional model variants than the method of Section 4.3.

Learning proceeds as follows. For a given update in OGD, each model variant computes its prediction and loss using the subset of coordinates that it includes, drawing on the stored single value for each coefficient. For each feature i , each model that uses i computes a new desired value for the given coefficient. The resulting values are averaged and stored as a single value that will then be shared by all vari-ants on the next step.

We evaluated this heuristic by comparing large groups of model variants trained with the single value structure against the same variants trained exactly with the set up from Section 4.3. The results showed nearly identical rela-tive performance across variants, but the single value struc-ture saved an order of magnitude in RAM.
As presented in Section 3.1, we need to store for each feature both the sum of the gradients and the sum of the squares of the gradients. It is important that the gradi-ent calculation be correct, but gross approximations may be made for the learning rate calculation.

Suppose that all events containing a given feature have the same probability. (In general, this is a terrible approx-imation, but it works for this purpose.) Suppose further that the model has accurately learned the probability. If there are N negative events, and P positive events, then the probability is p = P/ ( N + P ). If we use logistic regression, the gradient for positive events is p  X  1 and the gradient for negative events is p , the sum of the gradients needed for the learning rate Eq. (2) is This ruthless approximation allows us to keep track of only the counts N and P , and dispense with storing P g 2 t,i . Em-pirically, learning rates calculated with this approximation work just as well for us as learning rates calculated with the full sum. Using the framework of Section 4.3, total stor-age costs are lower since all variant models have the same counts, so the storage cost for N and P is amortized. The counts can be stored with variable length bit encodings, and the vast majority of the features do not require many bits.
Typical CTRs are much lower than 50%, which means that positive examples (clicks) are relatively rare. Thus, simple statistical calculations indicate that clicks are rela-tively more valuable in learning CTR estimates. We can take advantage of this to significantly reduce the training data size with minimal impact on accuracy. We create sub-sampled training data by including in our sample: Sampling at the query level is desirable, since computing many features requires common processing on the query phrase. Of course, naively training on this subsampled data would lead to significantly biased predictions. This problem is easily addressed by assigning an importance weight  X  t each example, where Since we control the sampling distribution, we do not have to estimate the weights  X  as in general sample selection [7]. The importance weight simply scales up the loss on each event, Eq. (1), and hence also scales the gradients. To see that this has the intended effect, consider the expected con-tribution of a randomly chosen event t in the unsampled data to the sub-sampled objective function. Let s t be the of model performance. Best viewed in color. probability with which event t is sampled (either 1 or r ), and so by definition s t = 1  X 
E [ ` t ( w t )] = s t  X  t ` t ( w t ) + (1  X  s t )0 = s t 1 Linearity of expectation then implies the expected weighted objective on the subsampled training data equals the ob-jective function on the original data set. Experiments have verified that even fairly aggressive sub-sampling of unclicked queries has a very mild impact on accuracy, and that predic-tive performance is not especially impacted by the specific value of r .
Evaluating the quality of our models is done most cheaply through the use of logged historical data. (Evaluating mod-els on portions of live traffic is an important, but more ex-pensive, piece of evaluation; see, for example, [30].)
Because the different metrics respond in different ways to model changes, we find that it is generally useful to evaluate model changes across a plurality of possible performance metrics. We compute metrics such as AucLoss (that is, 1  X  AUC, where AUC is the standard area under the ROC curve metric [13]), LogLoss (see Eq. (1)), and SquaredError. For consistency, we also design our metrics so that smaller values are always better.
We generally use progressive validation (sometimes called online loss) [5] rather than cross-validation or evaluation on a held out dataset. Because computing a gradient for learn-ing requires computing a prediction anyway, we can cheaply stream those predictions out for subsequent analysis, aggre-gated hourly. We also compute these metrics on a variety of sub-slices of the data, such as breakdowns by country, query topic, and layout.

The online loss is a good proxy for our accuracy in serving queries, because it measures the performance only on the most recent data before we train on it X  X xactly analogous to what happens when the model serves queries. The online loss also has considerably better statistics than a held-out validation set, because we can use 100% of our data for both training and testing. This is important because small improvements can have meaningful impact at scale and need large amounts of data to be observed with high confidence.
Absolute metric values are often misleading. Even if pre-dictions are perfect, the LogLoss and other metrics vary de-pending on the difficulty of the problem (that is, the Bayes risk). If the click rate is closer to 50%, the best achievable LogLoss is much higher than if the click rate is closer to 2%. This is important because click rates vary from country to country and from query to query, and therefore the averages change over the course of a single day.

We therefore always look at relative changes, usually ex-pressed as a percent change in the metric relative to a base-line model. In our experience, relative changes are much more stable over time. We also take care only to compare metrics computed from exactly the same data; for exam-ple, loss metrics computed on a model over one time range are not comparable to the same loss metrics computed on another model over a different time range.
One potential pitfall in massive scale learning is that ag-gregate performance metrics may hide effects that are spe-cific to certain sub-populations of the data. For example, a small aggregate accuracy win on one metric may in fact be caused by a mix of positive and negative changes in dis-tinct countries, or for particular query topics. This makes it critical to provide performance metrics not only on the aggregate data, but also on various slicings of the data, such as a per-country basis or a per-topic basis.

Because there are hundreds of ways to slice the data mean-ingfully, it is essential that we be able to examine a visual summary of the data effectively. To this end, we have de-veloped a high-dimensional interactive visualization called GridViz to allow comprehensive understanding of model per-formance.

A screen-shot of one view from GridViz is shown in Fig-Figure 3: Visualizing Uncertainty Scores. Log-odds errors |  X   X  1 ( p t )  X   X   X  1 ( p  X  t ) | plotted versus the uncer-tainty score, a measure of confidence. The x-axis is normalized so the density of the individual esti-mates (gray points) is uniform across the domain. Lines give the estimated 25%, 50%, 75% error per-centiles. High uncertainties are well correlated with larger prediction errors. ure 2, showing a set of slicings by query topic for two models in comparison to a control model. Metric values are rep-resented by colored cells, with rows corresponding to the model name and the columns corresponding to each unique slicing of the data. The column width connotes the impor-tance of the slicing, and may be set to reflect quantities such as number of impressions or number of clicks. The color of the cell reflects the value of the metric compared to a chosen baseline, which enables fast scanning for outliers and areas of interest, as well as visual understanding of the overall per-formance. When the columns are wide enough the numeric value of the selected metrics are shown. Multiple metrics may be selected; these are shown together in each row. A detailed report for a given cell pops up when the user mouse overs over the cell.

Because there are hundreds of possible slicings, we have designed an interactive interface that allows the user to se-lect different slicing groups via a dropdown menu, or via a regular expression on the slicing name. Columns may be sorted and the dynamic range of the color scale modified to suite the data at hand. Overall, this tool has enabled us to dramatically increase the depth of our understanding for model performance on a wide variety of subsets of the data, and to identify high impact areas for improvement.
For many applications, it is important to not only esti-mate the CTR of the ad, but also to quantify the expected accuracy of the prediction. In particular, such estimates can be used to measure and control explore/exploit tradeoffs: in order to make accurate predictions, the system must some-times show ads for which it has little data, but this should be balanced against the benefit of showing ads which are known to be good [21, 22].

Confidence intervals capture the notion of uncertainty, but for both practical and statistical reasons, they are inap-propriate for our application. Standard methods would as-sess the confidence of predictions of a fully-converged batch model without regularization; our models are online, do not assume IID data (so convergence is not even well defined), and heavily regularized. Standard statistical methods (e.g., [18], Sec. 2.5) also require inverting a n  X  n matrix; when n is in the billions, this is a non-starter.

Further, it is essential that any confidence estimate can be computed extremely cheaply at prediction time  X  say in about as much time as making the prediction itself.
We propose a heuristic we call the uncertainty score , which is computationally tractable and empirically does a good job of quantifying prediction accuracy. The essential observa-tion is that the learning algorithm itself maintains a notion of uncertainty in the per-feature counters n t,i used for learn-ing rate control. Features for which n i is large get a smaller learning rate, precisely because we believe the current coef-ficient values are more likely to be accurate. The gradient of logistic loss with respect to the log-odds score is ( p and hence has absolute value bounded by 1. Thus, if we assume feature vectors are normalized so | x t,i |  X  1, we can bound the change in the log-odds prediction due to observ-ing a single training example ( x ,y ). For simplicity, consider  X  1 =  X  2 = 0, so FTRL-Proximal is equivalent to online gra-dient descent. Letting n t,i =  X  + P t s =1 g 2 s,i and following Eq. (2), we have where  X   X   X  to is the vector of learning rates. We define the uncertainty score to be the upper bound u ( x )  X   X  X   X   X   X  x ; it can be computed with a single sparse dot product, just like the prediction p =  X  ( w  X  x ).

Experimental Results. We validated this methodology as follows. First, we trained a  X  X round truth X  model on real data, but using slightly different features than usual. Then, we discarded the real click labels, and sampled new labels taking the predictions of the ground-truth model as the true CTRs. This is necessary, as assessing the validity of a confi-dence procedure requires knowing the true labels. We then ran FTRL-Proximal on the re-labeled data, recording pre-dictions p t , which allows us to compare the accuracy of the predictions in log-odds space, e t = |  X   X  1 ( p t )  X   X   X  1 p t was the true CTR (given by the ground truth model). Figure 3 plots the errors e t as a function of the uncertainty score u t = u ( x t ); there is a high degree of correlation.
Additional experiments showed the uncertainty scores per-formed comparably (under the above evaluation regime) to the much more expensive estimates obtained via a bootstrap of 32 models trained on random subsamples of data.
Accurate and well-calibrated predictions are not only es-sential to run the auction, they also allow for a loosely cou-pled overall system design separating concerns of optimiza-tions in the auction from the machine learning machinery.
Systematic bias (the difference between the average pre-dicted and observed CTR on some slice of data) can be caused by a variety of factors, e.g., inaccurate modeling as-sumptions, deficiencies in the learning algorithm, or hidden features not available at training and/or serving time. To address this, we can use a calibration layer to match pre-dicted CTRs to observed click X  X hrough rates.

Our predictions are calibrated on a slice of data d if on average when we predict p , the actual observed CTR was near p . We can improve calibration by applying correction functions  X  d ( p ) where p is the predicted CTR and d is an element of a partition of the training data. We define success as giving well calibrated predictions across a wide range of possible partitions of the data.

A simple way of modeling  X  is to fit a function  X  ( p ) =  X p to the data. We can learn  X  and  X  using Poisson regression on aggregated data. A slightly more general approach that is able to cope with more complicated shapes in bias curves is to use a piecewise linear or piecewise constant correction function. The only restriction is that the mapping function  X  should be isotonic (monotonically increasing). We can find such a mapping using isotonic regression, which com-putes a weighted least-squares fit to the input data subject to that constraint (see, e.g., [27, 23]). This piecewise-linear approach significantly reduced bias for predictions at both the high and low ends of the range, compared to the reason-able baseline method above.

It is worth noting that, without strong additional assump-tions, the inherent feedback loop in the system makes it im-possible to provide theoretical guarantees for the impact of calibration [25].
An important aspect of scalable machine learning is man-aging the scale of the installation , encompassing all of the configuration, developers, code, and computing resources that make up a machine learning system. An installation comprised of several teams modeling dozens of domain spe-cific problems requires some overhead. A particularly in-teresting case is the management of the feature space for machine learning.

We can characterize the feature space as a set of contex-tual and semantic signals , where each signal (e.g.,  X  X ords in the advertisement X ,  X  X ountry of origin X , etc.) can be trans-lated to a set of real-valued features for learning. In a large installation, many developers may work asynchronously on signal development. A signal may have many versions corre-sponding to configuration changes, improvements, and alter-native implementations. An engineering team may consume signals which they do not directly develop. Signals may be consumed on multiple distinct learning platforms and ap-plied to differing learning problems (e.g. predicting search vs. display ad CTR). To handle the combinatorial growth of use cases, we have deployed a metadata index for managing consumption of thousands of input signals by hundreds of active models.

Indexed signals are annotated both manually and auto-matically for a variety of concerns; examples include dep-recation, platform-specific availability, and domain-specific applicability. Signals consumed by new and active models are vetted by an automatic system of alerts. Different learn-ing platforms share a common interface for reporting signal consumption to a central index. When a signal is depre-cated (such as when a newer version is made available), we can quickly identify all consumers of the signal and track replacement efforts. When an improved version of a signal is made available, consumers can be alerted to experiment with the new version.

New signals can be vetted by automatic testing and white-listed for inclusion. White-lists can be used both for ensuring correctness of production systems, and for learning systems using automated feature selection. Old signals which are no longer consumed are automatically earmarked for code cleanup, and for deletion of any associated data.

Effective automated signal consumption management en-sures that more learning is done correctly the first time. This cuts down on wasted and duplicate engineering effort, saving many engineering hours. Validating configurations for cor-rectness before running learning algorithms eliminates many cases where an unusable model might result, saving signifi-cant potential resource waste.
In this final section, we report briefly on a few directions that (perhaps surprisingly) did not yield significant benefit.
In recent years, there has been a flurry of activity around the use of feature hashing to reduce RAM cost of large-scale learning. Notably, [31] report excellent results using the hashing trick to project a feature space capable of learning personalized spam filtering model down to a space of only 2 24 features, resulting in a model small enough to fit easily in RAM on one machine. Similarly, Chapelle reported using the hashing trick with 2 24 resultant features for modeling display-advertisement data [6].

We tested this approach but found that we were unable to project down lower than several billion features without observable loss. This did not provide significant savings for us, and we have preferred to maintain interpretable (non-hashed) feature vectors instead.
Recent work has placed interest around the novel tech-nique of randomized  X  X ropout X  in training, especially in the deep belief network community [17]. The main idea is to randomly remove features from input example vectors inde-pendently with probability p , and compensate for this by scaling the resulting weight vector by a factor of (1  X  p ) at test time. This is seen as a form of regularization that emulates bagging over possible feature subsets.

We have experimented with a range of dropout rates from 0.1 to 0.5, each with an accompanying grid search for learn-ing rate settings, including varying the number of passes over the data. In all cases, we have found that dropout training does not give a benefit in predictive accuracy metrics or gen-eralization ability, and most often produces detriment.
We believe the source of difference between these negative results and the promising results from the vision community lie in the differences in feature distribution. In vision tasks, input features are commonly dense, while in our task input features are sparse and labels are noisy. In the dense setting, dropout serves to separate effects from strongly correlated features, resulting in a more robust classifier. But in our sparse, noisy setting adding in dropout appears to simply reduce the amount of data available for learning.
Another training variant along the lines of dropout that we investigated was that of feature bagging, in which k mod-els are trained independently on k overlapping subsets of the feature space. The outputs of the models are averaged for a final prediction. This approach has been used extensively in the data mining community, most notably with ensembles of decision trees [9], offering a potentially useful way of man-aging the bias-variance tradeoff. We were also interested in this as a potentially useful way to further parallelize training. However, we found that feature bagging actually slightly re-duced predictive quality, by between 0.1% and 0.6% AucLoss depending on the bagging scheme.
In our models the number of non-zero features per event can vary significantly, causing different examples x to have different magnitudes k x k . We worried that this variability may slow convergence or impact prediction accuracy. We explored several flavors of normalizing by training with x with a variety of norms, with the goal of reducing the vari-ance in magnitude across example vectors. Despite some early results showing small accuracy gains we were unable to translate these into overall positive metrics. In fact, our experiments looked somewhat detrimental, possibly due to interaction with per-coordinate learning rates and regular-ization.
We gratefully acknowledge the contributions of the fol-lowing: Vinay Chaudhary, Jean-Francois Crespo, Jonathan Feinberg, Mike Hochberg, Philip Henderson, Sridhar Ra-maswamy, Ricky Shan, Sajid Siddiqi, and Matthew Streeter.
