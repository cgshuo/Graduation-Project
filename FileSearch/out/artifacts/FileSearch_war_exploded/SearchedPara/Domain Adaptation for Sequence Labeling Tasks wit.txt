 Min Xiao minxiao@temple.edu Yuhong Guo yuhong@temple.edu Domain adaptation aims to learn a prediction model for a label-scarce target domain by exploiting infor-mation in a label-rich source domain (Blitzer et al., 2006; Daum  X e III, 2007; Ben-David et al., 2007; Daum  X e III et al., 2010). Domain adaptation is prevailingly needed for various sequence labeling tasks in natu-ral language processing (NLP) area, such as syntactic chunking (Daum  X e III, 2007; Huang &amp; Yates, 2009), part-of-speech (POS) tagging (Blitzer et al., 2011; 2006), parsing (McClosky et al., 2010), semantic role labeling (Carreras &amp; M`arquez, 2005), and named en-tity recognition (NER) (Daum  X e III, 2007; Turian et al., 2010; Daum  X e III &amp; Marcu, 2006).
 In a practical domain adaptation learning scenario in NLP, the source domain and the target domain usually have very different vocabularies , which renders the lex-ical feature-based NLP systems to perform poorly on the target domain (Ben-David et al., 2007; 2010). For example, a statistical machine learning model for POS tagging systems based on lexical features trained on newswire data with frequent terms like  X  X EO X ,  X  X or-poration X  cannot correctly infer POS tags for biomed-ical text with frequent terms like  X  X etastases X ,  X  X e-quencing X  and  X  X enomic X . Moreover, the learning machine based on lexical features may produce in-consistent predictions across domains. For example, the word  X  X ignaling X  in  X  X ignaling that . . .  X  from the Wall Street Journal (WSJ) domain is a verb (VBG), but it is a noun (NN) in  X  X ignaling pathway X  from the MEDLINE domain (Huang &amp; Yates, 2010). Re-cently, much work has been proposed to cope with those problems in order to improve the prediction performance for out-of-domain NLP systems, includ-ing feature augmentation based supervised adaptation method (Daum  X e III, 2007), semi-supervised adapta-tion method (Daum  X e III et al., 2010), and representa-tion learning methods (Blitzer et al., 2006). In this paper, we propose to adapt sequence labeling systems in NLP from a source domain to a different but related target domain by inducing distributed rep-resentations using a log-bilinear language adaptation (LBLA) model. It combines the advantages of repre-sentation learning methods from (Blitzer et al., 2006), which employ generalizable features across domains to reduce domain divergence, and feature augmentation based (semi-)supervised adaptation learning methods from (Daum  X e III, 2007; Daum  X e III et al., 2010), which exploit both common and domain-specific features from both domains. Specifically, the LBLA model si-multaneously models the source distribution by learn-ing generalizable and source-specific word representa-tions and models the target distribution by learning domain-sharing and target-specific word representa-tions. We then use the learned representation em-bedding functions to map the original data into the induced representation space as augmenting features, which are incorporated into supervised sequence label-ing systems to enable cross domain adaptability. The proposed learning technique is empirically evaluated for cross domain POS tagging systems on sentences from WSJ and MEDLINE domains, cross domain syn-tactic chunking and named entity recognition systems on sentences from WSJ and Brown corpora, and is shown to outperform a number of related methods. Domain adaptation has been intensively studied for a variety of sequence labeling tasks in the natu-ral language processing area. Daum  X e III &amp; Marcu (2006) proposed to distinguish between general fea-tures and domain-specific features by training three separate maximum entropy classifiers. They empiri-cally showed the effectiveness of the proposed method on mention type classification, mention tagging and recapitalization systems. Jiang &amp; Zhai (2007) investi-gated instance weighting method for semi-supervised domain adaptation by assigning more weights to la-beled source and target data, removing misleading training instances in the source domain, and augment-ing target training instances with predicted labels. They empirical evaluated their method for cross do-main part-of-speech tagging and named entity recog-nition to justify its efficacy. Daum  X e III (2007) pro-posed an easy adaptation learning method (EA) by using feature replication, which is later extended into a semi-supervised version (EA++) by incorporating unlabeled data via co-regularization (Daum  X e III et al., 2010). These methods demonstrated good empirical performance on a variety of NLP tasks.
 Recently, representation learning methods have been proposed to induce generalizable features by exploit-ing large amount of unlabeled data from both domains, which are then used to augment original instances to improve cross domain prediction performance (Blitzer et al., 2006; Ando &amp; Zhang, 2005; Huang &amp; Yates, 2009; 2010). Blitzer et al. (2006) proposed to seek for common latent features by performing structural correspondence learning (SCL), which models the cor-relation between pivots (frequent lexical features) and non-pivot features. Huang &amp; Yates (2009) proposed to induce hidden states as latent features by training Hidden Markov Models (HMMs) on unlabeled sen-tences from two domains. They empirically demon-strated the efficacy of their approach on out-of-domain part-of-speech tagging and syntactic chunking tasks. Their learning technique is also further exploited in (Huang &amp; Yates, 2010), which aims to learn a multi-dimensional feature representation by simultaneously training multiple HMMs with different initializations. Turian et al. (2010) empirically demonstrated that em-ploying Collobert and Weston embeddings (Collobert &amp; Weston, 2008), Brown clusters, or HLBL embed-dings (Mnih &amp; Hinton, 2009) as extra word features can improve the performance of out-of-domain named entity recognition systems and in-domain syntactic chunking systems.
 Distributed representations are widely exploited in nat-ural language processing area. Bengio et al. (2000; 2003) introduced neural network language models and demonstrated how to combine neural network proba-bility predictions with distributed representations for words in order to outperform standard n -gram models. Blitzer et al. (2004) demonstrated that those learned distributed representations of symbols make sense lin-guistically. The effectiveness of distributed representa-tions has also been demonstrated on other NLP tasks, such as sentiment analysis (Maas &amp; Ng, 2010), syntac-tic chunking, named entity recognition(Turian et al., 2010), semantic role labelling (Collobert &amp; Weston, 2008), and parsing (Socher et al., 2011). Previous empirical results showed that latent gener-alizable features can increase the accuracy for out-of-domain prediction performance (Blitzer et al., 2006). It has also been justified by a recent theoretic study that a proper feature representation is crucial to do-main adaptation due to its contribution on bridg-ing domain divergence (Ben-David et al., 2007; 2010). In this work, we propose to learn generalizable dis-tributed representations of words from sentence struc-tures to address the problem of domain adaptation for sequence labeling tasks in NLP.
 Distributed representations, which are dense, low-dimensional, and continuous-valued, are called word embeddings (Turian et al., 2010). Each dimension of the word embedding stands for a latent feature of the word, hopefully capturing useful semantic and syntac-tic regularities. The basic idea to learn a distributed representation is to link each word with a real-valued feature vector, typically by using neural language mod-els. A sentence can thus be transformed into a se-quence of these learned feature vectors. The neural language model learns to map the sequence of feature vectors to a prediction of interest, such as the condi-tional probability distribution over the current word given its previous context, and pushes the learned word features to form grammatical and semantic sim-ilarities (Bengio et al., 2000; 2003). The advantage of this distributed representation method is that it allows the model to generalize well to sequences that do not appear in the training set, but are similar to training sequences with respect to their distributed representa-tions (Bengio et al., 2000). The simplest neural lan-guage model is the log-bilinear language (LBL) model developed in (Mnih &amp; Hinton, 2007), which performs linear predictions in the semantic word feature space. Despite its simplicity, the LBL model has been shown to outperform n -grams on a large dataset (Mnih &amp; Hinton, 2007; Mnih &amp; Teh, 2012). Based on this sim-ple language model, we present a log-bilinear language adaptation (LBLA) model below to learn adaptive dis-tributed word representations for domain adaptation over sequence labeling tasks. 3.1. Log-Bilinear Language Adaptation Model We consider the domain adaptation problem from a source domain S to a target domain T . In the source domain, we have l s labeled sentences { ( X s i , Y s i ) } l + u s , where X s i is the i th input sentence, i.e., a sequence of words, w 1 , w 2 , . . . , w T responding label sequence, e.g. the sequence of POS tags. Similarly, in the target domain, we have l t la-much smaller than l s . Though the two domains may have very different vocabularies, for simplicity we use a common word vocabulary V for both domains. We adapt the log-bilinear language model to learn dis-tributed representations across domains by simulta-neously modeling two different but related data dis-tributions in the source and target domains. The distributed representations are encoded as real-valued vectors for words in the vocabulary. We refer to the matrix with all word representation vectors as R and denote the representation vector for word w as R ( w ). Motivated by (Daum  X e III, 2007), we split the represen-tation vector into three parts to capture both domain-sharing and domain-specific properties of each word. Thus the representation vector for a word w can be expressed as where R s ( w ) represents source-specific latent features, R c ( w ) represents common latent features, and R t ( w ) represents target-specific latent features. Naturally we assume the source domain contains no target-specific features and the target domain contains no source-specific features. In practice, we define two mapping target words to cross domain word embeddings where 0 t is a zero vector in the same size of R t ( w ) and 0 s is a zero vector in the same size of R s ( w ). functions exploited in previous work on domain adap-tation (Daum  X e III, 2007), since we propose to learn the latent feature vectors using a log-bilinear language model while they perform simple feature replication. The common and domain-specific features learned for a word w can be different in our proposed model, while they use two identical copies for both parts. The three-part distributed representation learning framework can explicitly model the relationship be-tween two data sources through the common repre-sentation part, while still maintaining the unique se-mantic and syntactic information of each data source through the domain-specific parts. For example, a POS tagging task uses WSJ as the source domain and MEDLINE as the target domain. The word  X  X ignal-ing X  in a sentence  X  X ignaling that ... X  from the source domain is a verb (VBG), but it is a noun (NN) in a sentence  X  X ignaling pathway ... X  from the target do-main. This syntactic difference of the same word in two domains can be encoded in the domain-specific latent features in the distributed representation. Recall that we have two sets of training sentences sampled from two domains, S and T . The LBLA model thus includes a set of conditional distributions, P
D ( w | h ) of each word w given its previous ( n c  X  1) words (denoted as the context h ), for each domain D  X  {S , T } , Here E D ( w, h ;  X  ) is a log-bilinear energy function, for d  X  { s, t } correspondingly, and it quantifies the compatibility of word w with context h in domain D . b w is the bias parameter used to capture the popularity of word w across domains. We refer to the bias vector for all words as b . b  X  d ( w ) is the predicted representa-tion vector for the target word w given its context h in the domain indexed by d , which can be computed by linearly combining the feature vectors for the context words, such that where C s i , C c i , C t i are the position-dependent con-text weight matrices for source-specific, common and target-specific features respectively. Thus the negated log-bilinear energy function  X  E D ( w, h ;  X  ) measures the similarity between the current word feature vec-tor  X  d ( w ) and the predicted feature vector b  X  d ( w ). Overall, the proposed LBLA model simultaneously models two sets of conditional distributions, P S ( . ;  X  ) and P T ( . ;  X  ), respectively on the two domains based on distributed feature representations. These two sets of distributions reflect both domain-sharing properties of the data, parameterized with { R c , { C c i } , b w } , and domain-specific properties of the data, parameterized with { R s , { C s i }} and { R t , { C t i }} . 3.2. Training with Noise-Contrastive We propose to train the LBLA model using noise-contrastive estimation (NCE) (Gutmann &amp; Hyv  X arinen, 2010; 2012), which has been recently in-troduced for training unnormalized probabilistic mod-els, and is shown to be less expensive than maxi-mum likelihood learning and more stable than impor-tance sampling (Bengio &amp; Sen  X ecal, 2003) for training neural probabilistic language models (Mnih &amp; Teh, 2012). Assume that we have a source data distribution P
S ( w | h ) and a target data distribution P T ( w | h ), which are the distributions of words occurring after a partic-ular context h , on the source domain and the target domain respectively. We propose to distinguish the ob-served source data and the observed target data from noise samples, which are artificially generated by a uni-gram distribution. We denote the context-independent noise distribution as P n ( w ). We would like to fit the context-dependent parameter models P S ( w | h ;  X  ) and P T ( w | h ;  X  ) to P S ( w | h ) and P T ( w | h ) respectively. We assume that observed samples appear k times less frequently than noise samples, and data samples come from the mixture distribution on a domain D , for D  X  {S , T } . Since we are fit-ting P D ( w | h,  X  ) to P D ( w | h ), we will replace P with P D ( w | h ;  X  ). Then given a context h , the pos-terior probabilities that a sample word w comes from the observed source data distribution and the observed target data distribution are However, evaluating Eq. (9) and Eq. (10) is too ex-pensive due to the normalization computation for P
D ( w | h ;  X  ) (Eq. 4). To tackle this issue, instead of conducting explicit normalization, NCE treats the nor-malization constants as parameters and parameterizes the models with respect to learned normalization pa-rameters z s ( h ) , z t ( h ) and unnormalized distributions P
S ( . | h ;  X  0 ), P T ( . | h ;  X  0 ), such that of the unnormalized distributions.
 To fit the context-dependent model to the data, given a context h , we simply maximize an objective J D ( h ;  X  ) for each domain D  X  {S , T } . It is the expectation of log P D (D | w, h ;  X  ) under the mixture distribution of the noise and observed data samples,
J D ( h ;  X  ) = kE P The gradient of this objective function can be com-puted as Since the conditional distributions for different con-texts of both domains share parameters, these distri-butions can then be learned jointly by optimizing a global NCE objective, which is defined as the combi-nation of weighted per-context NCE objectives in the two domains, where P ( h D ) is the empirical context probability of h D in domain D .
 In practice, given an observation word w in context h from the domain D , we generate k noise data-points x 1 , x 2 , . . . , x k from the unigram noise distri-bution P n ( w ), and consider an approximate objective b J
D ( w, h ;  X  ) such that Its gradient can be computed as Based on this, we then use an empirical global NCE objective for gradient computation in each iteration of a gradient ascent training procedure, which can be expressed as a sum of the generated approximate ob-jectives for each context-word pair appeared in all sen-tences of the two domains, i.e., b J (  X  ) = Here w s ij denotes the j th word of the sentence X s i , and h s ij denotes the context of w s ij , i.e, its previous ( n and its context h t ij . The gradient of b J (  X  ) can be eas-ily obtained by summing over the gradients of each context-word pair objective, which can be computed following Equation (17). 3.3. Feature Augmentation with Distributed After training the LBLA model, we obtain two feature sentence in the source domain, w 1 , w 2 , . . . , w T , we use a feature vector as augmenting features. Similarly, we produce a augmenting feature vector for each word in the target sentences using the feature mapping func-both domains, represented using both the original fea-tures and the augmenting features, to train supervised NLP systems such as POS tagging, syntactic chunking and named entity recognition, and apply these systems into the target domain. We conducted experiments to evaluate the proposed LBLA model based domain adaptation technique on three NLP tasks: POS tagging, syntactic chunking and named entity recognition. In this section, we report the experimental results.
 For each task, we compared the proposed LBLA method with the following three baseline methods and four domain adaptation methods: (1) SRCONLY , a baseline that conducts training only on the labeled source data; (2) TGTONLY , a baseline that conducts training only on the labeled target data; (3) ALL , a baseline that conducts training on the labeled data from both domains; (4) SCL , the structural correspon-dence learning (SCL) domain adaptation technique de-veloped in (Blitzer et al., 2006); (5) LBL , the method that uses LBL model to produce distributed represen-tation features as augmenting features for NLP sys-tems; (6) EA , the feature augmentation based super-vised domain adaptation method developed in (Daum  X e III, 2007); and (7) EA++ , the feature augmentation based semi-supervised domain adaptation method de-veloped in (Daum  X e III et al., 2010). 4.1. Domain Adaptation for POS Tagging For POS tagging, we used the same experimental setting as given in (Daum  X e III, 2007; Blitzer et al., 2006). The source domain contains articles from Wall Street Journal (WSJ), with 39,832 manually tagged sentences from sections 02-21 and 100,000 unlabeled sentences from a 1988 subset. The target domain con-tains bio-medical articles from MEDLINE, with 1061 labeled sentences and about 100,000 unlabeled sen-tences. Among the 1061 labeled bio-medical sentences, we used 561 sentences as test data while keeping the rest 500 sentences as labeled training data from the target domain. 4.1.1. Distributed Representation Learning We built a vocabulary with all sentences from the source and target domain. In order to reduce the vo-cabulary size, we mapped lower frequency (0-2) words to a single unique identifier in our vocabulary and mapped sole-digit words into a single unique identifier. On all processed sentences except the 561 biomedical sentences which we will keep as test data, we applied the proposed LBLA model to perform distributed rep-resentation learning. There are a few hyperparame-ters to be set when applying LBLA model. We set the word-embedding sizes for source-specific features, target-specific features and domain-sharing (common) features equally as 100. Thus the total size of a word embedding is 300. We set the context size n c as 3, which means we only consider the previous two words for each target word. We set the k value for noise-contrastive estimation as 25. We randomly initial-ized the word embeddings R , the position-dependent context weight matrices C = { C d i : i  X  { 1 , 2 } , d  X  { c, s, t }} , and initialized the bias vector b , and the normalization parameters { z s ( h ), z t ( h ) } with all ze-ros. The same hyperparameters and initializations were used for LBL model as well. After augmenting each sentence with the learned representation features, standard supervised POS tagging was performed. 4.1.2. Experimental Results for POS Tagging For supervised POS tagging, we used the SEARN algo-rithm, which is used in (Daum  X e III, 2007) as well. We used 39,832 labeled newswire sentences from the WSJ domain and 500 labeled biomedical sentences from the MEDLINE domain as training data, while the test data contains 561 biomedical sentences with 14,554 tokens. Under this setting, the test results of the com-parison methods in term of error rate are reported in Table 1. We can see that the LBLA method appar-ently outperforms all the other comparison methods on cross-domain POS tagging.
 We then conducted further experiments to investigate the performance of each method by varying the num-ber of labeled training sentences from the target do-main from 50 to 500. The test results in term of ac-curacy are plotted in Figure 1. We can see that the LBLA method consistently outperforms all the other comparison methods across the range of different num-ber of training sentences from the target domain. To investigate the significance of the improvements the proposed LBLA method achieved over the other meth-ods, we conducted McNemar X  X  significance tests for labeling disagreements (Gillick &amp; Cox, 1989) between the LBLA method and the other comparison methods (except the two basic baselines SRCONLY and TG-TONLY), with p &lt; 0 . 05 being significant. We found all the test comparisons between LBLA method and the other methods are significant, as shown in Table 2. 4.2. Domain Adaptation for Syntactic For syntactic chunking, we used WSJ as the source do-main and Brown corpus data as the target domain. We used the same source domain data as we did in POS tagging experiments. The target domain contains 3 sections (ck01-ck03) of Brown corpus data, with 426 labeled  X  X eneral fiction X  sentences and about 57,000 unlabeled sentences. Labeled sentences from both do-mains are tagged with syntactic chunking tags in IOB2 format, which is a standard format widely used for syn-tactic chunking. In IOB2 format, each chunk tag has two parts. The first part denotes the position of the corresponding token in the chunk and the second part represents the chunk type. For example, the chunk tag B-VP is used for the first word of a verb phrase. 4.2.1. Distributed Representation Learning We built a vocabulary with all sentences in the con-structed source domain and target domain. We ap-plied the same processing procedures used in the POS tagging experiments. On all processed sentences ex-cept 226  X  X eneral fiction X  sentences from the target domain which we will use as test data, we applied the LBLA and LBL models separately to perform dis-tributed representation learning. We used the same hyperparameter setting and initializations as we did in POS tagging experiments. After augmenting each sen-tence with the learned representation features, stan-dard supervised syntactic chunking can be performed. 4.2.2. Experimental Results for Syntactic We used the same SEARN algorithm for supervised syntactic chunking. We used 40,000 labeled newswire sentences from the source domain and 200  X  X eneral fiction X  sentences from the target domain as training data, and used 226  X  X eneral fiction X  sentences from the target domain as test data. In addition to the tra-ditional features, we also extracted POS tag features as inputs. Under this setting, the test results in term of error rate are reported in Table 3, which show the proposed LBLA based cross domain syntactic chunk-ing outperforms all the other methods. We then con-ducted experiments to investigate the performance of each method by varying the number of labeled train-ing sentences from the target domain between 50 and 200. The test results in term of accuracy are plotted in Figure 2. The proposed method demonstrated con-sistent advantages over all the other methods. By us-ing McNemar X  X  paired significance tests with p &lt; 0 . 05 being significant, we verified that the proposed LBLA based method significantly outperforms other methods as shown in Table 4.
 4.3. Domain Adaptation for Named Entity For named entity recognition task, we used the same source data and target data as the syntactic chunk-ing experiments. The labeled data are tagged with named entity chunking tags in IOB2 format. The task is to label each word with one of the named entity tags, which represents the position of the word in the named entity chunk and the type of the named entity. For example, I-LOC is used for the remaining words of a phrase that represents a location and B-PER is used for the first word of a phrase that represents a person. Words located outside of named entity chunks receive the tag O , representing miscellaneous names. We also used the same procedure of distributed presentation learning as we employed in syntactic chunking experi-ments to produce augmentation features for supervised named entity recognition. 4.3.1. Experimental Results for Named For supervised named entity recognition task, again we used 40,000 labeled newswire sentences from the source domain and 200 labeled  X  X eneral fiction X  sen-tences from the target domain as training data, and used 226  X  X eneral fiction X  sentences from the target domain as test data. We used the same SEARN al-gorithm to perform named entity recognition. In ad-dition to previous feature set, we also extracted syn-tactic chunking tags as phrase chunking features. The experimental results in term of error rate are reported in Table 5. We can see that the proposed LBLA repre-sentation learning based method outperforms all other methods. We then investigated how the number of labeled training sentences from the target domain af-fects the performance of each comparison method on named entity recognition. The results in term of ac-curacy are plotted in Figure 3, which show the pro-posed method clearly outperforms all other methods across the range of experiments. By using McNemar X  X  paired significance test, we verified the improvements achieved by the proposed method over the other meth-ods are mostly significant, as shown in Table 6. In this paper, we proposed to tackle domain adapta-tion problems for sequence labeling tasks in NLP by developing a log-bilinear language adaptation (LBLA) model. The LBLA model learns a distributed repre-sentation of the words across domains which encodes both generalizable features and domain-specific fea-tures. The distributed representation vector for each word can be then used as augmenting features for su-pervised natural language processing systems. We em-pirically evaluated the proposed LBLA based domain adaptation method on WSJ and MEDLINE domains for POS tagging systems, and on WSJ and Brown cor-pora for syntactic chunking and named entity recog-nition tasks. The results show that LBLA method consistently outperforms all other comparison meth-ods for cross domain sequence labeling tasks. Ando, R. and Zhang, T. A framework for learning predictive structures from multiple tasks and unla-beled data. Journal of Machine Learning Research (JMLR) , 6:1817 X 1853, 2005.
 Ben-David, S., Blitzer, J., Crammer, K., and Pereira,
F. Analysis of representations for domain adapta-tion. In Advances in Neural Information Processing Systems (NIPS) , 2007.
 Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A.,
Pereira, F., and Vaughan, J. A theory of learning from different domains. Machine Learning , 79:151 X  175, 2010.
 Bengio, Y. and Sen  X ecal, J. Quick training of probabilis-tic neural nets by importance sampling. In Proc. of the conference on Artificial Intelligence and Statis-tics (AISTATS) , 2003.
 Bengio, Y., Ducharme, R., and Vincent, P. A neural probabilistic language model. In Advances in Neural Information Processing Systems (NIPS) , 2000. Bengio, Y., Ducharme, R., Vincent, P., and Janvin,
C. A neural probabilistic language model. J. of Ma-chine Learn. Research (JMLR) , 3:1137 X 1155, 2003. Blitzer, J., Weinberger, K., Saul, L., and Pereira, F.
Hierarchical distributed representations for statisti-cal language modeling. In Advances in Neural In-formation Processing Systems(NIPS) , 2004.
 Blitzer, J., McDonald, R., and Pereira, F. Domain adaptation with structural correspondence learning. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2006.
 Blitzer, J., Foster, D., and Kakade, S. Domain adap-tation with coupled subspaces. In Proc. of the In-ternational Conference on Artificial Intelligence and Statistics (AISTATS) , 2011.
 Carreras, X. and M`arquez, L. Introduction to the conll-2005 shared task: semantic role labeling. In Proc. of the Conference on Computational Natural Language Learning (CoNLL) , 2005.
 Collobert, R. and Weston, J. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proc. of the Interna-tional Conf. on Machine Learning (ICML) , 2008. Daum  X e III, H. Frustratingly easy domain adaptation.
In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL) , 2007.
 Daum  X e III, H. and Marcu, D. Domain adaptation for statistical classifiers. Journal of Artificial Intelli-gence Research , 26:101 X 126, 2006.
 Daum  X e III, H., Kumar, A., and Saha, A. Co-regularization based semi-supervised domain adap-tation. In Advances in Neural Information Process-ing Systems (NIPS) , 2010.
 Gillick, L. and Cox, S. Some statistical issues in the comparison of speech recognition algorithms. In Proc. of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) , 1989. Gutmann, M. and Hyv  X arinen, A. Noise-contrastive es-timation: A new estimation principle for unnormal-ized statistical models. In Proc. of the International
Conference on Artificial Intelligence and Statistics (AISTATS) , 2010.
 Gutmann, M. U. and Hyv  X arinen, A. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. J. of Ma-chine Learning Research (JMLR) , 13:307 X 361, 2012. Huang, F. and Yates, A. Distributional representa-tions for handling sparsity in supervised sequence labeling. In Proc. of the Annual Meeting of the Asso-ciation for Computational Linguistics (ACL) , 2009. Huang, F. and Yates, A. Exploring representation-learning approaches to domain adaptation. In Proc. of the Annual Meeting of the Association for Com-putational Linguistics (ACL) , 2010.
 Jiang, J. and Zhai, C. Instance weighting for domain adaptation in nlp. In Proc. of the Annual Meet-ing of the Association of Computational Linguistics (ACL) , 2007.
 Maas, A. and Ng, A. A probabilistic model for seman-tic word vectors. In Advances in Neural Information Processing Systems Wordshop on Deep Learning and Unsupervised Feature Learning , 2010.
 McClosky, D., Charniak, E., and Johnson, M. Au-tomatic domain adaptation for parsing. In Proc. of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL) , 2010.
 Mnih, A. and Hinton, G. Three new graphical mod-els for statistical language modelling. In Proc. of the International Conference on Machine learning (ICML) , 2007.
 Mnih, A. and Hinton, G. A scalable hierarchical dis-tributed language model. In Advances in Neural In-formation Processing Systems (NIPS) , 2009.
 Mnih, A. and Teh, Y. A fast and simple algorithm for training neural probabilistic language models. In Proc. of the International Conference on Machine Learning (ICML) , 2012.
 Socher, R., Lin, C., Ng, A., and Manning, C. Parsing natural scenes and natural language with recursive neural networks. In Proc. of the International Con-ference on Machine Learning(ICML) , 2011.
 Turian, J., Ratinov, L., and Bengio, Y. Word repre-sentations: a simple and general method for semi-supervised learning. In Proc. of the Annual Meet-ing of the Association for Computational Linguistics
