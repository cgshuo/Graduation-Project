 To accomplish a search task and satisfy a single information need, users usually submit a series of que ries to web search engines. It is useful for web search engines to detect the task boundaries in a series of successive queries. Tr aditional task boundary detection methods are based on time gap and lexical comparisons, which topically related queries may not share any common words. In this paper we learn hidden topi cs from query log and leverage them to resolve the vocabulary gap problem. Unlike other external knowledge resources, such as WordNet and Wikipedia, the hidden topics discovered from query log cover long tail queries, which is useful to det ect task boundaries. Experimental results on dataset from real world query log demonstrate that the proposed method achieves significant quality enhancement. H.3.3 [ Information Storage and Retrieval ]: Query formulation, Search process Algorithms, Experimentation query task session, task boundary detection, search task In web search, users usually submit a series of queries to satisfy a single information need [16]. If we are able to accurately identify sets of queries with the same information need, we can evaluate the performance of a web search e ngine from the user X  X  point of view and suggest more related queri es to users [10]. Because it is useful for web search, task boundary detection has attracted many research and industry attentions [1, 3, 4, 7, 8, 9]. 
Traditionally, two clues have been exploited to detect task boundary of queries: one is the tim e gap between two queries, and the other is the lexical content of queries. Due to its simplicity, there have been many time gap based methods for task boundary detection [2, 3, 8, 14]. These methods determine whether two queries straddle a task boundary by learning a time gap threshold: the two queries belong to the same session if the time gap between them is smaller than the given threshold. Although time gap based methods are simple and fast, they usually cannot achieve a high accuracy because they ignore the lexical content of queries [10]. In order to improve the detection accuracy, recent methods start to incorporate the lexical content of queries. Spink et al . [16] and He et al . [9] proposed several lexical patterns between two queries to identify the relationship between them. Other methods leverage string si milarities such as Levenshtein distance [10] and Jaccard coefficient [11] to detect task boundaries. By taking the lexical content of queries into consideration, the lexical co mparison based methods often achieve better performance than the time gap based methods. 
The lexical comparison based methods, however, often suffer from the vocabulary gap problem. For example, the lexical comparison based methods cannot identify the queries Apple products and iphone belong to the same search task, because they share no common words. To resolve the vocabulary gap problem, there have been several studies tr ying to enrich short queries with external information, in order to get longer representation of the underlying information need. These external information include the search results [14, 15], the Wikipedia [11], etc. The problem of these information enriching me thods is that their performance depend on the quality and coverage of the external resources. For example, the Wikipedia only cover popular concepts, so it cannot resolve the long tail queries such as gardenweb and CIKM 2013 . Furthermore, these methods are usually too time consuming for real-world usage [7]. 
In this paper, we propose a task boundary detection method, which resolve the vocabulary gap problem by leveraging the hidden topics discovered from query log. Compared with Wikipedia, the hidden topics di scovered from query log cover a lot of words that do not exist in Wikipedia. This is extremely useful to deal with web queries . For example, Wikipedia cannot capture the semantic relationship between queries gardenweb and daylily flower because the word gardenweb does not appear in Wikipedia. Compared with sear ch results, the hidden topics discovered from query log are pre-processed and stored, and the number of sampling iterations for query topic inference is small (20 in this paper). So incorporating hidden topics is not too expensive in the sense of time cost. Experimental results on dataset from real world query l og demonstrate that our method achieves significant accuracy enhancement by leveraging the hidden topics. 
This paper is organized as fo llows. Section 2 describes the proposed method for task boundary de tection. Section 3 shows the experiment results. And Secti on 4 concludes this paper. In this section we present the details of the proposed method for task boundary detection. Formally , the task boundary detection is considered as follows: where q i and q j are two queries submitted by the same user. 1 denotes that q i and q j belong to the same search task and -1 denotes they belong to different search tasks. We built a two-step classifier to detect task boundaries. In the first step, we use lexical patterns to decide whether a query pair straddles a task boundary. In the second step, we focus on resolving the vocabulary gap problem by leveraging the hidde n topics discovered from query log. Given two queries q i and q j , according to [9], we first identify the lexical patterns between them. Specifically, the lexical patterns can be categorized into the following five types:  X  Generalization : the second query q j is the subset of the first  X  Reformulation : queries q i and q j have some common terms but  X  New : the relationship between queries q i and q Specialization and the Reformulation lexical patterns are strong indicators that two queries belong to the same search task. Due to the vocabulary gap problem, however, the New pattern does not always implies a task boundary. Hence, our classifier based on lexical patterns works as follows: if two queries are allocated to any of the first four patterns, we assign them to the same search task, and if two queries are allocated to the New pattern, we deal with them in the next step. As we described above, due to the vocabulary gap problem (e.g. conference on information and knowledge management and CIKM ), we need to collect more information to find out whether the New pattern actually implies a task boundary. Specifically, we collect the following information as features for dealing with the New pattern.  X  Time gap. The value of this feature is computed as follows: where t ( q i ) is the time query q i was issued by the user and time_limit is a pre-defined threshold which is used to normalize the time gap between q i and q j .  X  Normalized Levenshtein edit distance.  X  Clicked URLs similarity. We re present queries as  X  X ags of  X  Character n-grams similarity. We represent queries as  X  X ags  X  ESA similarity. We compute the semantic similarity between  X  Hidden topic similarity. We infer the hidden topics within 
Given the above features, we build a Support Vector Machine classifier using LIBSVM V3.12 1 . Because queries are very short and noisy, directly applying topic models (e.g. LDA and PLSA) on que ries may not work well. So we use Phan X  X  method [13] to infer the hidden topics within queries. The main idea of Phan X  X  method is that learning hidden topics from large external resources to enrich the representation of short texts. corresponding clicked URLs. If a user submits a query q and relevant to q in one way or another. Consequently, q can be used as an annotation of URL u . In this way, we can view each URL as a  X  X ocument X , which consists of the corresponding query annotations. Then we use LDA model with Gibbs Sampling to learn hidden topics from the UR Ls  X  X ocuments X . Using such URLs  X  X ocuments X  as external resources has two advantages. First, URLs  X  X ocuments X  cover a lot of terms that appear in queries. This is useful for task boundary detection. For example, the term gardenweb exists in some URLs  X  X ocuments X  while it does not appear in Wikipedia. If a user submits a query gardenweb in future, then the model learned from URLs  X  X ocuments X  can infer the topics of the query appropriately while the model based on Wikipedia ca nnot. Second, the hidden topics discovered from URLs  X  X ocum ents X  reflect the underlying example, the term Apple usually means Apple Inc. in query log. But, if we learn hidden topics from an external resource where query Apple belongs to the topic about fruits with a high probability. Then we cannot find out the semantic similarity between the query Apple and iphone . We can overcome this problem by learning hidden topics from URLs  X  X ocuments X  since the nature of these  X  X ocuments  X  is consistent with queries. http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
Given the vectors of all words and their topic assignment of the whole URLs  X  X ocuments X , we pe rform Gibbs Sampling to infer the hidden topics for a query. According to [13], the number of sampling iterations is set to 20. Let words and their topic assignment of the whole URLs  X  X ocuments X . qw and assignment of a query q . Then, the topic assignment for a given term t in where () n t k is the number of times the term t is assigned to topic k ; within query q except the current assignment; () i n  X  of terms in query q assigned to topic k except the current assignment; V is the total number of terms; K is the total number of hidden topics; and
After performing topic sampling, the topic distribution of query where () n k q is the number of terms in query q assigned to topic k . 
Given the topic distributions the topic similarity between q i and q j is computed as follows: where (|| ) In order to evaluate the perform ance of our method, we manually build a query-pair corpus labeled by three annotators, and each pair is labeled whether the two que ries belong to the same search task. We first sample 7,710 queries of 178 users from the 2006 AOL query log [12]. If we consider all pairs of queries from a user, the number of query pairs will be huge. And manually labeling so many query pair s is too time consuming and expensive. Hence, we only consid er the query pairs which consist of two consecutive queries from a user. In this way, we get 7,532 query pairs. We conduct 5-fold cr oss-validation: the query pairs are randomly split into five folds, where four folds are selected for training and the fifth fold is selected for test. Because we employ LDA topic model to infer topic distributions of queries, we need to set LDA hyperparameters ( topic number (K). In this paper, we fix K=100, 1,124,839 URLs  X  X ocuments X  from the 2006 AOL query log [12] as the external resource. Some sample topics discovered from URLs  X  X ocuments X  by the LDA m odel are shown in Figure 1. We use LIBSVM V3.12 with its defau lt parameters as the classifier for the New pattern. The evaluation metric is accuracy : correct N correct is the number of query pairs which a classifier predicts correctly and N total is the total number of query pairs. 
In our experiments, we use the following baselines:  X  Time gap based method (TG): th is method uses a given time  X  Lexical comparison based method (LC): this method uses  X  Geometric method (GM) [4]: th is method uses a geometric  X  SVM with all features (SVMA): this is our method which uses  X  SVM-topicsim: this method uses all features except topic  X  SVM-ESAsim: this method uses all features except ESA  X  SVM-LC: this method does not use lexical patterns We conduct experiments using a ll baselines and our methods. Table 1 shows the accuracy scores of all methods. From Table 1, we can see that: 1) Our method SVMA achieves the best performance with a 2) The method LC achieves an accuracy score 0.904, which In this paper we propose a two-step method for task boundary detection: in the first step, lexi cal pattern comparison is used to detect task boundaries; and in the second step, we learn hidden topics from query log and leverage them to resolve the vocabulary gap problem. Experimental results on dataset from real world query log demonstrate that the proposed method achieves significant quality enhancement. For future work, we plan to explore ways to optimi ze the parameters of LDA models to examine the effect of different parameters. The work is supported by the National Natural Science Foundation of China under Grants no. 90920010 and 61100152. Moreover, we sincerely thank the reviewers for their valuable comments. [1] Paolo Boldi, Francesco Bonchi , Carlos Castillo, et al. 2008. [2] Nikolai Buzikashvili and Bernard J. Jansen. 2006. Limits of [3] Doug Downey, Susan Dumais, and Eric Horvitz. 2007. [4] Daniel Gayo-Avello. 2009. A survey on session detection [5] Evgeniy Gabrilovich and Shaul Markovitch. 2007. [6] Thomas L Griffiths and Mark Steyvers. 2004. Finding [7] Matthias Hagen, Benno Stein, and Tino R  X  b. 2011. Query [8] Daqing He and Ayse G  X  ker. 2000. Detecting session [9] Daqing He, Ayse G X ker, and David J. Harper. 2002. [10] Rosie Jones and Kristina L.Klinkner. 2008. Beyond the [11] Claudio Lucchese, Salvatore Orlando, Raffaele Perego, [12] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. . 2006. [13] Xuan-Hieu Phan, Cam-Tu Nguyen, Dieu-Thu Le, Le-Minh [14] Filip Radlinski and Thorsten Joachims. 2005. Query chains: [15] Xuehua Shen, Bin Tan, and Chengxiang. Zhai. 2005. [16] Amanda Spink, Bernard J. Ja nsen, and H. C.  X zmutlu. 2000. 
