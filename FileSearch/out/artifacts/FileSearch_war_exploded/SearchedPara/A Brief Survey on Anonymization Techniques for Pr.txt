 Nowadays, partly driven by many Web 2.0 applications, more and more social network data has been made publicly available and analyzed in one way or another. Privacy pre-serving publishing of social network data becomes a more and more important concern. In this paper, we present a brief yet systematic review of the existing anonymization techniques for privacy preserving publishing of social net-work data. We identify the new challenges in privacy pre-serving publishing of social network data comparing to the extensively studied relational case, and examine the pos-sible problem formulation in three important dimensions: privacy, background knowledge, and data utility. We sur-vey the existing anonymization methods for privacy preser-vation in two categories: clustering-based approaches and graph modification approaches. Recently, social networks [24; 31] have received dramatic in-terest in research and development, partly due to more and more social networks are built online and the fast develop-ment of Web 2.0 applications. Social networks model social relationships by graph structures using vertices and edges. Vertices model individual social actors in a network, while edges model relationships between social actors. Many dif-ferent kinds of social networks present in our lives such as friendship networks, telephone call networks, and academia co-authorship networks.
 Due to the rapidly increasing popularity of social networking sites on the Web, more and more people participate in social networks. According to a poll by TNS Canadian Facts 1 , a Canadian marketing and social research firm, teens and young adults are the heaviest users of social networking sites.  X 
The research was supported in part by an NSERC Dis-covery grant and an NSERC Discovery Accelerator Supple-ments grant. All opinions, findings, conclusions and recom-mendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies. We sincerely thank the anonymous reviewers for their con-structive and informative advice which helps to improve the quality of this paper. Particularly, we thank them for their suggestions on some related work and the description of some critical properties of some methods. http://www.privcom.gc.ca/information/social/ index_e.asp Specifically, 83% of 13-17 years old people and 74% of 18-29 years old people visited at least one social networking site. 60% of people in their 30s and 45% of those in their 40s visited at least one social networking site.
 Social networks connect social actors. The connections are often beneficial to entrepreneurs and commercial companies. For example, they can use the connections to expand their customer bases. In many cases, those social networks can serve as a customer relationship management tool for com-panies selling products and services. Companies can also use social networks to identify potential customers or recruit candidate employees. For example, according to the statis-tics published in Time Magazine 2 , 12% of employers in the United States use popular social networking sites such as MySpace and Facebook to investigate potential employees. With the rapid growth of social networks, social network analysis [8; 24; 29; 25] has emerged as a key technique in modern sociology, geography, economics, and information science. The goal of social network analysis is to uncover hidden social patterns. The power of social network analysis has been shown much stronger than that of traditional meth-ods which focus on analyzing the attributes of individual so-cial actors. In social network analysis, the relationships and ties between social actors in a network are often regarded more important and informative than the attributes of in-dividual social actors. Social network analysis approaches have been shown very useful in capturing and explaining many real-world phenomena such as the well-known  X  X mall world phenomenon X  [30]. As more and more rich social media, popular online social networking sites, and various kinds of social network ana-lyzing and mining techniques are available, privacy in social networks becomes a serious concern [14; 25; 3], particularly when social network data is published.
 An adversary may intrude privacy of some victims using the published social network data and some background knowl-edge. Importantly, many of the richest emerging sources of social network data come from settings such as e-mails, instant messages or telephone communication. Users have strong expectations of privacy on such data. When social network data is made public in one way or another, it is far from sufficient to protect privacy by simply replacing the
August 20, 2007, which is available at http://www.time. com/time/magazine/article/0,9171,1651513,00.html . identifying attributes such as name and SSN of individuals by meaningless unique identifiers.
 Example 1 (Background knowledge-based attacks).
 Backstrom et al. [3] identified a family of attacks such that even from a single anonymized copy of a social network hiding the identifying attributes, it is possible for an adversary to learn whether edges exist or not between some specific target pairs of vertices. The attacks are based on the uniqueness of some small random subgraphs embedded in an arbitrary network, using ideas related to those found in arguments from Ramsey theory [2].
 Two categories of attacks are addressed in [3]. The first category is active attacks. Before releasing the anonymized network G of ( n  X  k ) vertices, the attackers can choose a set of b target users, randomly create a subgraph H containing k vertices, and then attach H to the target vertices. After the anonymized network is released, if the attackers can find the subgraph H in the released graph G , then the attackers can follow edges from H to locate the b target vertices and their locations in G , and determine all edges among those b vertices.
 To implement the attacks, the random graph H should satisfy the following requirements: 1. H must be uniquely and efficiently identifiable regard-2. there is no other subgraph S in G such that S and H 3. H has no automorphism.
 Backstrom et al. [3] provide two methods to construct sub-graphs satisfying the above requirements.
 The second category is passive attacks, which are based on the fact that most vertices in social networks usually belong to a small uniquely identifiable subgraph. Thus, an attacker can collude with other ( k  X  1) friends to identify additional vertices connected to the distinct subset of the coalition. The attacks are possible under the following assumptions: 1. all colluders should know edges among themselves, that 2. all colluders should know the name of their neighbors 3. there does not exist a Hamiltonian path linking The experiments on a real social network with 4 . 4 million vertices and 77 million edges show that the creation of 7 ver-tices by an attacker can reveal on average 70 target vertices and compromise the privacy of approximately 2 , 400 edges between them.
 Example 1 indicates that privacy issues in social networks are real. Several initiative methods on privacy preservation in social network data publishing have been proposed, which will be surveyed in the rest of the paper. Generally, privacy preservation methods against background knowledge-based attacks often adopt data anonymization approaches, which are the focus of this survey. Privacy preservation on relational data has been studied ex-tensively. A major category of privacy attacks on relational data is to re-identify individuals by joining a published table containing sensitive information with some external tables modeling background knowledge of attackers.
 To battle the re-identification attacks, the mechanism of k -anonymity was proposed [23; 26]. Specifically, a data set is said to be k -anonymous ( k  X  1) if, on the quasi-identifier attributes (that is, the maximal set of join attributes to re-identify individual records), each record is indistinguishable from at least ( k  X  1) other records. The larger the value of k , the better the privacy is protected.
 Although k -anonymity has been well adopted, Machanava-jjhala et al. [20] showed that a k -anonymous table may still have some subtle but severe privacy problems due to the lack of diversity in the sensitive attributes. In particular, they showed that, the degree of privacy protection does not really depend on the size of the equivalence classes on quasi-identifier attributes which contain tuples that are identi-cal on those attributes. Instead, it is determined by the number and distribution of distinct sensitive values associ-ated with each equivalence class. To overcome the weakness in k -anonymity, they propose the notion of l -diversity [20]. Xiao and Tao [33] prove that l -diversity always guarantees stronger privacy preservation than k -anonymity.
 Though several important models and many efficient algo-rithms have been proposed to preserve privacy in relational data, most of the existing studies can deal with relational data only. Those methods cannot be applied to social net-work data straightforwardly. Anonymizing social network data is much more challenging than anonymizing relational data [39].
 First, it is much more challenging to model background knowledge of adversaries and attacks about social network data than that about relational data. On relational data, it is often assumed that a set of attributes serving as a quasi-identifier is used to associate data from multiple tables, and attacks mainly come from identifying individuals from the quasi-identifier. However, in a social network, many pieces of information can be used to identify individuals, such as labels of vertices and edges, neighborhood graphs, induced subgraphs, and their combinations. It is much more com-plicated and much more difficult than the relational case. Second, it is much more challenging to measure the infor-mation loss in anonymizing social network data than that in anonymizing relational data. Typically, the information loss in an anonymized table can be measured using the sum of information loss in individual tuples. Given one tuple in the original table and the corresponding anonymized tuple in the released table, we can calculate the distance between the two tuples to measure the information loss at the tuple level. However, a social network consists of a set of vertices and a set of edges. It is hard to compare two social networks by comparing the vertices and edges individually. Two so-cial networks having the same number of vertices and the same number of edges may have very different network-wise properties such as connectivity, betweenness, and diameter. Thus, there can be many different ways to assess information loss and anonymization quality.
 Last but not least, it is much more challenging to devise anonymization methods for social network data than for re-lational data. Divide-and-conquer methods are extensively applied to anonymization of relational data due to the fact that tuples in a relational table are separable in anonymiza-tion. In other words, anonymizing a group of tuples does not affect other tuples in the table. However, anonymizing a social network is much more difficult since changing labels of vertices and edges may affect the neighborhoods of other vertices, and removing or adding vertices and edges may af-fect other vertices and edges as well as the properties of the network. Privacy preserving data publishing and analyzing techniques on relational data have been well developed. Recently, there have been a few studies on privacy preservation in social network data. However, the research and development of privacy preservation techniques on social network data are still in their infancy. This survey provides a timely overview of the recent studies on this direction. Particularly, we make the following two contributions. The rest of the paper is organized as follows. In Section 2, we analyze the privacy models in social networks. We categorize the existing anonymization methods in Section 3. We survey the clustering-based approaches and the graph modification approaches in Sections 4 and 5, respectively. We conclude the paper in Section 6. To battle privacy attacks and develop protection techniques in social networks, we need to model three aspects. First, we need to identify the privacy information which may be under attack. Second, we need to model the background knowledge that an adversary may use to attack the privacy of target individuals. Last, we need to specify the usage of the published social network data so that an anonymization method can try to retain the utility of the data as much as possible while the privacy information is fully preserved. In this section, we systematically analyze the privacy models in social networks.
 Generally, we model a social network as a simple graph G = ( V, E, L, L V , L E ), where V is a set of vertices, E  X  V  X  V is a set of edges, L is a set of labels, and a labeling function L
V : V  X  L assigns each vertex a label and a labeling function L E : E  X  L assigns each edge a label. For a graph G , V ( G ), E ( G ), L ( G ), L V ( G ), and L E ( G ) are the set of vertices, the set of edges, the set of labels, the vertex labeling function in G , and the edge labeling function in G , respectively. In privacy preservation on relational data, the attributes in a table are divided into two groups: non-sensitive attributes and sensitive attributes. The values in sensitive attributes are considered to be private for individuals. However, in social network data, much more pieces of information can be considered as privacy of individuals. We list some of them below as examples. Modeling privacy is important which sets up the goal of privacy preservation in social networks. Different privacy concerns may lead to different problem definitions and ac-cordingly different privacy preservation methods. Liu et al. [17] and Zheleva and Getoor [37] proposed a cat-egorization schema different from ours in this paper. They classified privacy in social networks into identity disclosure (that is, the identity of an individual who is associated with a vertex is revealed), link disclosure (that is, the sensitive re-lationship between two individuals is disclosed), and content disclosure (that is, the sensitive data associated with each vertex is compromised, for example, the email messages sent and/or received by the individuals in an email communica-tion network). The categorization presented here is more extensive than the three categories in [17; 37]. In relational data, a major type of privacy attacks is to re-identify individuals by joining the published table with some external tables modeling the background knowledge of users. Specifically, the adversaries are assumed knowing the values on the quasi-identifier attributes of the target victims. In privacy preservation in publishing social networks, due to the complex structures of graph data, the background knowledge of adversaries may be modeled in various ways. An important aspect of anonymizing social network data is how the anonymized networks are expected to be used. Different applications may use anonymized data in differ-ent ways. For example, in some situations, anonymized net-works may be used to analyze the global network structures. In some other situations, anonymized networks may be used to analyze the micro-structures. Clearly, different usage in-tentions may lead to different anonymization schemes. So far, two types of utility as follows have been considered. In summary, complex network structures introduce more di-mensions and consequently more challenges in modeling pri-vacy preservation problems in social network data. Gener-ally, many pieces of information in social network data can be used to model privacy, background knowledge, and utility of anonymized data. Different combinations of those factors may lead to different problem settings. Accordingly, differ-ent anonymization methods should be developed. Table 1 shows the existing studies in a space of 3 dimensions: the privacy concerns, the background knowledge, and the data utility. As can be seen, the research and development of privacy preserving methods in social network data is still in its infancy. Many problems still have not been touched. In privacy preserving data publishing, in order to prevent privacy attacks, data should be anonymized properly before it is released. Anonymization methods should take into ac-count the privacy models of the data and the utility of the data.
 Generalization and perturbation are the two popular anonymization approaches for relational data. Although privacy preservation in social network data is a relatively new problem, several privacy preserving methods have been developed. Similar to privacy preservation methods in rela-tional data, specific anonymization methods are developed for specific privacy models of social networks and specific utility goals of anonymized data.
 We categorize the state-of-the-art anonymization methods on social network data into two categorizes as follows. In the rest of this paper, we will focus on the clustering-based approaches in Section 4. Section 5 is dedicated to the graph modification approaches. Generalization is a popular way to anonymize relation data. Essentially, generalization can be regarded as clustering ver-tices and edges into groups and generalize all members in a group to the same.
 Depending on the subjects of clustering, the clustering-based approaches can be further divided into four subcate-gories: vertex clustering methods, edge clustering methods, vertex and edge clustering methods, and vertex-attribute mapping clustering methods. Hay et al. [12] considered a simple graph model, in which vertices and edges are unlabeled. They addressed ver-tex identifier attacks, and proposed a vertex clustering ap-proach. Three models of external information were consid-ered as the possible background knowledge of an adversary. These models represent a range of structural information that may be available to an adversary, including complete and partial descriptions of vertex neighborhoods, and con-nections to hubs in the network. The authors formalized the structural indistinguishability of a vertex with respect to an adversary with external information about the local neighborhood of the vertex. Specifically, background knowl-edge of adversaries are modeled using the following types of queries. Several graph properties are considered to be the utility of the network, including the degree distribution of vertices, the distribution of shortest path lengths of 500 randomly sampled pairs of vertices in the network, the distribution of clustering coefficients which are the proportion of all possi-ble neighbor pairs of a vertex that are connected, network resilience which is the number of vertices in the largest con-nected component of the graph when vertices are removed in degree decreasing order, and infectiousness which is the proportion of vertices infected by a hypothetical disease and simulated by first infecting a randomly chosen vertex and then transmitting the disease to each neighbor with the spec-ified infection rate.
 Hay et al. [12] proposed a scheme of anonymity through structural similarity. Vertices that look structurally similar may be indistinguishable to an adversary. A strong form of structural similarity between vertices is automorphism equivalence.
 The anonymization technique proposed in [12] is a vertex clustering approach. It generalizes an input network by grouping vertices into partitions and publishing the num-ber of vertices in each partition along with the densities of edges within and across partitions. Data analysts can still use the anonymized graphes to study macro-properties of the original graph.
 The partitioning of vertices is chosen such that the general-ized graph satisfies the privacy preservation goals and max-imizes the data utility. To ensure anonymity, we need to make sure that any adversary has at least a minimum level of uncertainty about the re-identification of any target ver-tex. Hay et al. [12] proposed to use the size of a partition to provide a basic guarantee against re-identification attacks, which mimics k -anonymity in relational data. Specifically, for any partition of vertices, the size should be at least k . To retain utility as much as possible, the partitions should best fit the input graph. The proposed method estimates fit-ness via a maximum likelihood approach. A local search is adopted to explore the exponential number of possible parti-tionings. To find the partitioning that maximizes the likeli-hood function, the algorithm uses simulated annealing [22]. In general, a social network can have different types of ver-tices and different types of edges. Zheleva and Getoor [37] focused on the case where there are multiple types of edges but only one type of vertices. Among all types of edges, one type is assumed sensitive and should be protected against link re-identification attacks. The privacy breach is mea-sured by counting the number of sensitive edges that can be inferred from the anonymized data.
 To model the background knowledge of adversaries, the au-thors considered predicting sensitive edges based on the other observed non-sensitive edges. To address the worst case, the authors assumed that an adversary has an accu-rate probabilistic model which can predict the existence of a sensitive edge e s ij (that is, an edge between two vertices v and v j carrying a sensitive label s ) based on a set of ob-servations O : P ( e s ij | O ), where each observation is an edge. A simple noisy-or model [21] for the existence of the sensi-tive edge is adopted. The noisy-or model can capture the scenario where each observed edge contributes to the prob-ability of the existence of a sensitive edge.
 The authors assumed that each observed edge e k has a noise parameter  X  k , which models the independent influence of e on the existence of a sensitive edge. In addition, they as-sumed that there exists a leak parameter  X  0 which models the probability of the existence of a sensitive edge due to some other hidden factors. According to the noisy-or model, the probability of the existence of a sensitive edge is calcu-lated as An adversary succeeds when she/he can correctly figure out whether a sensitive edge exists between two vertices. In order to model the data utility, the authors proposed to count the number of observations which have to be deleted during the anonymization process. The smaller the number of removed observations, the higher the utility.
 In order to protect sensitive relationships, several graph anonymization strategies are proposed. The first edge anonymization strategy is to only remove the sensitive edges, leaving all other observed edges intact.
 Another anonymization strategy is to remove some observed edges. Generally, a particular type of observations which sig-nificantly contributes to the overall likelihood of a sensitive relationship, or a certain percentage of observations that meet some pre-specified criteria (for example, at random, connecting high-degree vertices, etc.) can be removed. The most conservative anonymization strategy is to remove all edges in the network. Obviously, in the above approaches, the utility of an anonymized network is low.
 The authors assumed that the vertices are divided into equivalence classes and each class is anonymized properly using some existing relational data anonymization method. Then, a more effective approach to anonymize the social net-work is to collapse all vertices in an equivalence class into a single vertex, and consider which edges to be included in the collapsed graph. One feasible way is to publish for each edge type the number of edges of the type between two equivalence class vertices. This approach is called cluster-edge anonymization.
 The difference between the cluster-edge anonymization ap-proach and the approach in [12] is that the cluster-edge anonymization method aggregates edges on type to prevent the disclosure of sensitive relationships, while [12] clusters vertices to protect vertex identities. Campan and Truta [4] modeled a social network as a simple undirected graph. Moreover, vertices in the network are as-sociated with some attributes. Following the previous mod-els in relation data, the attributes associated with vertices can be classified into three categories, identifier attributes such as name and SSN which should be removed in publishing, quasi-identifier attributes such as zipcode and sex which may be used by an adversary in re-identification attacks, and sensitive attributes such as diagnosis and income which are assumed to be privacy information. Furthermore, in [4], edges are not labeled.
 To model data utility, Campan and Truta [4] consider the information loss due to generalization and the changes of structural properties. Information loss occurs when vertex labels are generalized. The changes of structural properties quantify the probability of error when one tries to recon-struct the structure of the original social network from the masked version.
 To protect privacy in social network data, Campan and Truta [4] advocates the k -anonymity model. Every vertex should be indistinguishable with at least other ( k  X  1) ver-tices in terms of both the attributes and the associated struc-tural information such as neighborhood of vertices. The anonymization method disturbs as little as possible the so-cial network data, both the attribute data associated to the vertices and the structural information.
 The method for anonymizing vertex attribute data uses gen-eralization, which has been well studied in relational data. For structure anonymization, the proposed method is called edge generalization, which is similar to the one described in [37] to some extent. The critical difference is that the method in [4] takes into account both the generalization in-formation loss and the structural information loss during the clustering procedure. This process can be tuned by users to achieve a desirable tradeoff between preserving more struc-tural information of the network and preserving more vertex attribute information.
 Similar to [37], in [4], vertices are partitioned into clusters in anonymization. To anonymize edges, vertices in the same cluster are collapsed into one single vertex, labeled with the number of vertices and edges in the cluster. The edges be-tween two clusters are collapsed into a single edge, labeled by the number of edges between them. In some applications, entities and their relationships can be modeled as a bipartite graph, such as customers and medical products used. The edges in such a bipartite graph may be considered as privacy. Cormode et al. [5] focused on the problem of anonymizing bipartite graphs.
 Generally, a bipartite graph G = ( V, W, E ) consists of | V | vertices of one type and | W | vertices of the other type, and a set of | E | edges E  X  V  X  W . When a bipartite graph is published, the graph structure is retained. The vertices are clustered into groups and the mapping between groups in the original graph and groups in the published graph is released. For example, the mapping table may state that vertices { x 1 , x 2 , x 3 } in the original graph are mapped to { a 20 , a 31 , a 206 } in the published graph. By devising the map-ping properly, privacy of entities such as whether a customer consumes a specific product can be preserved.
 To model the background knowledge of adversaries, Cor-mode et al. [5] consider both static attacks and learned link attacks. If a group of vertices X  X  V only connect to a group of vertices Y  X  W , a static attack can immediately obtain the vertices that those in X connect to. Generally, if very few edges exist between vertices in X and vertices not in Y , then a learned link attack can obtain the vertices that those in X connect to with a high confidence.
 The data utility is measured by the accuracy of answering aggregate queries, such as the average number of products purchased per user. Attributes of vertices in V (or W ), or both can be used to compose predicates in aggregate queries, such as the average number of products purchased by cus-tomers in California, and the average number of vitamin products by customers in California.
 Cormode et al. [5] proposed the safe grouping mechanism to protect privacy. A safe grouping of a bipartite graph partitions vertices into groups such that two vertices in the same group of V have no common neighbors in W and vice versa. To control the anonymization granularity, a ( k, l )-safe grouping ensures that each group on V contains at least k vertices and each group on W contains at least l vertices. A greedy algorithm is developed, which may or may not find a safe grouping. The vertices are processed one by one. For each vertex, the algorithm checks whether it can be put into an existing group without breaking the safety. If yes, it is added into a group. Otherwise, a new group is created. After all vertices are processed, there may be some groups with fewer than k vertices. Those vertices are collected and the algorithm continues to run on the collection with a larger group size threshold, say ( k +1). The iteration continues un-til either a safe grouping is found or the group size threshold exceeds the number of vertices in the collection of vertices to be partitioned. In the latter case, the algorithm fails. The clustering-based approaches reduce a cluster of vertices and edges into a super-vertex. Thus, the graph may be shrunk considerably after anonymization, which may not be desirable for analyzing local structures. To preserve the scale and the local structures of the original graph, graph modification approaches try to locally modify the graph structure to achieve the privacy preservation requirement. Liu and Terzi [18] studied the k -degree anonymization prob-lem on social networks without any vertex and edge labels. They considered the identity disclosure scenario where the identities of individuals associated with vertices are revealed. To model the background knowledge of an adversary, the authors considered possible re-identification attacks against individuals by an adversary using the prior knowledge of the degree of a target vertex. An adversary is assumed to know the degree of a target victim. By searching the degrees of vertices in the published network, the adversary may be able to identify the individual, even when the identities of the vertices are removed before the network data is published. Several graph properties are considered as utility of the net-works, including clustering coefficient, average path length, and edge intersection (i.e., the percentage of edges in the degree-anonymous graphs that are also in the original graph).
 In order to battle degree attacks, Liu and Terzi [18] pro-posed the notion of graph k -degree anonymity, which mim-ics k -anonymity in relational data. Specifically, a graph is said to be k -degree anonymous if for every vertex v in the graph, there exist at least ( k  X  1) other vertices in the graph with the same degree as v . An adversary with degree back-ground knowledge can identify some target individuals with For a graph G ( V, E ), the degree sequence of G , denoted by d , is a sequence of vertices in the degree descending order. A degree sequence is k -degree-anonymous if, for each vertex, there are at least other ( k  X  1) vertices carrying the same degree. By providing a privacy parameter k , the anonymiza-tion method proceeds in two steps.
 In the first step, starting from the original degree sequence d , Liu and Terzi [18] developed a dynamic programming method to construct a new degree sequence  X  d that is k -degree-anonymous and minimizes the degree anonymization cost D A (  X  d  X  d ) = L 1 (  X  d  X  d ). Therefore, their method is optimal in terms of the resulting degree sequence. In the second step, they constructed a graph  X  G ( V,  X  E ) such that  X  d is the degree sequence of  X  G and  X  E  X  E = E (or  X  E  X  E  X  E in a relaxed version). The graph construction problem is related to the problem of realizing degree se-quence with constraints, which has been studied extensively in graph theory [6; 11]. Generally speaking, the method of graph construction follows a randomized scheme. To achieve the desired degree sequence, a randomized edge swap trans-formation strategy was adopted in [18]. Randomized approaches have been widely used in privacy preservation in relation data [7; 1; 27]. The approach has also been adopted for anonymizing social network data. Hay et al. [13] tackled the same problem as [12] (reviewed in Section 4.1) except that hub fingerprint queries are not considered. They developed a randomized edge construction method.
 The method constructs an anonymized graph G 0 from the original graph G through a sequence of m edge deletions followed by m edge insertions. Edges deleted are chosen uniformly at random from the set of all existent edges in G , while edges inserted are chosen uniformly at random from the set of all non-existent edges of the interim graph. The vertices are not changed. The process of perturbation and the perturbation parameter m are assumed to be publicly known.
 An adversary may attempt to re-identify individuals using external information, such as vertex refinement queries and subgraph queries as discussed in Section 4.1. The perturba-tion of the graph ensures that the adversary cannot simply exclude from the candidate set of vertices (that is, a set of vertices matching the adversary X  X  background knowledge about the target vertex) those do not match the structural properties of the target. The adversary must consider the set of possible worlds implied by the anonymized graph G 0 and m random edge insertions and m deletions. The set of possible worlds consist all graphs that can result in G 0 m edge perturbations.
 The candidate set of a target vertex v includes all vertices u  X  G 0 such that u is a candidate in some possible world. Any vertex that may be a candidate for v will also be a can-didate under graph perturbation, since G is a possible world of G 0 . The candidate set may become very large with an in-creased number of perturbation operations. Consequently, the privacy of target individuals can be well protected. Ying and Wu [36] tackled the same problem as [13] by considering randomly adding/deleting edges or randomly switching edges. Instead of designing specific randomiza-tion algorithms, Ying and Wu [36] analyzed the effect of randomization in protecting attacks.
 The spectrum of a graph is defined as the set of eigenvalues of the adjacency matrix of the graph. The eigenvalues of a network are connected to important topological properties such as diameter, presence of cohesive clusters, long paths and bottlenecks, and randomness of the graph. Ying and Wu [36] showed that the spectrum property has close rela-tion with many graph characteristics and can provide global measures for some network properties. Furthermore, Ying and Wu [36] investigated the spectrum of networks. A natural idea for graph anonymization is to consider whether a graph can be perturbed without significantly changing one or some particular eigenvalues. If so, the ap-proach is probable to better preserve structural character-istics. By considering the change of spectrum in the ran-domization process, the proposed spectrum preserving ap-proach [36] can outperform the simple edge randomization methods. The algorithm can determine which edges should be added, removed or switched so that the change of the eigenvalues can be under control. In some applications, the social networks may be weighted. The weights of edges can reflect affinity between two ver-tices or record the communication cost between two individ-uals. For example, a social network about communication between friends may be weighted such that the weight of an edge is the communication frequency between two individ-uals, which may be considered the privacy for some people. Liu et al. [19] studied anonymization of graphs where edge weights are considered sensitive. Two kinds of data util-ity were considered. First, the authors considered how to approximate the lengths of shortest paths between vertices within an error bound. Second, they considered how to re-tain the exact shortest paths between vertices in a selected subset.
 Two methods were proposed. The first method uses Gaus-sian randomization multiplication. The authors showed that there does not exist a perturbation schema such that every edge weight is perturbed but the length of the shortest paths between every pair of vertices is preserved. Thus, they used a Gaussian noise matrix with mean 0 and standard devia-tion  X  to perturb the weights of edges so that the shortest path lengths are approximated with a quality guarantee. The second method is a greedy perturbation algorithm, which not only can keep exactly the same shortest paths for certain selected paths and approximate the shortest path lengths for the others, but also can maximize the weight pri-vacy preservation. Generally, any edge e i,j can be in one of the three categories: a non-betweenness edge (that is, it is not on any shortest path in the graph), an all-betweenness edge (that is, all shortest paths in a set of pre-selected vertices pass through e i,j ), and a partial-betweenness edge (that is, only some of the shortest paths pass through this edge). On different categorizes of edges special weight mod-ifications can be applied. The anonymization method per-turbs the weights greedily until the privacy preservation re-quirement is achieved. Greedy approaches have been widely used in privacy preser-vation in relational data [34; 15; 10], and have been shown effective. They also can be used in anonymizing social net-work data.
 Zhou and Pei [39] considered anonymization in social net-works where each vertex is associated with non-sensitive attributes. An attacker may have background knowledge about the neighborhoods of victims. The privacy preser-vation goal is to protect neighborhood attacks which use neighborhood matching to re-identify vertices.
 Consider a social network G = ( V, E, L, L ) and the anonymization G 0 = ( V 0 , E 0 , L 0 , L 0 ) for publishing. Zhou and Pei [39] assumed that no fake vertices should be added to the anonymized graph. This assumption is often desir-able in applications since introducing fake vertices may often change the global structure of a social network. Moreover, they assumed that the original connections between vertices in G are retained in the anonymization. To model the util-ity, they focused on using anonymized social networks to answer aggregate network queries.
 To battle neighborhood attacks, Zhou and Pei [39] extended the k -anonymity model in relational data to social network data. For a social network G , suppose an adversary knows the neighborhood structure of a vertex u  X  V ( G ), denoted by Neighbor G ( u ). If Neighbor G ( u ) has at least k isomor-phic copies in G 0 where G 0 is an anonymization of G , then u can be re-identified in G 0 with a confidence of at most 1 Zhou and Pei [39] introduced a practical greedy method to anonymize a social network to satisfy the k -anonymity re-quirement in two steps.
 In the first step, the algorithm extracts the neighborhoods of all vertices in the network. To facilitate the isomorphism tests among neighborhoods of different vertices which will be conducted frequently in anonymization, a simple yet ef-fective neighborhood component coding technique based on minimal DFS code [35] was proposed which represents neigh-borhoods in a concise way.
 In the second step, the algorithm greedily organizes vertices into groups and anonymizes the neighborhoods of vertices in a group to the same, until the graph satisfies k -anonymity. Due to the well recognized power law distribution of the degrees of vertices in large social networks, a heuristic of starting with those vertices of high degrees is adopted. The intuition is that in real social networks, those vertices with large degrees are the ones vulnerable to neighborhood at-tacks.
 Zhou and Pei [38] extended [39] and introduced l -diversity into social network anonymization. In this case, each vertex is associated with some non-sensitive attributes and some sensitive attributes. If an adversary can re-identify the sen-sitive attribute values of one target individual with a high confidence, the privacy of that individual is breached. An l -diverse graph makes sure that the adversary cannot infer Zhou and Pei [38] extend the k -anonymity method devel-oped in [39] to tackle the l -diversity problem. The diversity partitioning strategy is similar to that in [32]. In this paper, we surveyed a few recent studies on anonymization techniques for privacy preserving publish-ing of social network data. Although privacy preserving data publishing and analysis techniques in relational data have been well explored, the research and development of anonymization techniques on social network data is still in its infancy, as illustrated in Table 1.
 We discussed the new challenges in privacy preservation in social network data comparing to the extensively stud-ied relational case, and examined the possible problem for-mulation in three important dimensions: privacy, back-ground knowledge, and data utility. We reviewed the anonymization methods for privacy preservation in two cat-egories: clustering-based approaches and graph modification approaches.
 More extensively, there are also some other studies related to privacy preservation in social network data. For exam-ple, Frikken and Golle [9] studied the problem of construct-ing a graph from individuals who are vertices in the graph without intruding the privacy of the individuals. Wang et al. [28] proposed using description logic as a knowledge rep-resentation in social network data publishing. Leskovec and Faloutsos [16] proposed a method to generate a graph fitting the graph properties of a give graph. The graph generated can be used as a (perturbed) anonymization of the original graph.
 As social network data is much more complicated than re-lational data, privacy preserving in social networks is much more challenging and needs many serious efforts in the near future. Particularly, modeling adversarial attacks and de-veloping corresponding privacy preservation strategies are critical. [1] R. Agrawal, R. Srikant, and D. Thomas. Privacy pre-[2] N. Alon and J. Spencer. The Probabilistic Method . John [3] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore [4] A. Campan and T. M. Truta. A clustering approach [5] G. Cormode, D. Srivastava, T. Yu, and Q. Zhang. [6] R. Diestel. Graph Theory (3rd Edition) , volume 173. [7] A. Evfimievski, J. Gehrke, and R. Srikant. Lim-[8] L. C. Freeman, D. R. White, and A. K. Romney. Re-[9] K. B. Frikken and P. Golle. Private social network anal-[10] B. C. M. Fung, K. Wang, and P. S. Yu. Anonymiz-[11] J. Gross and J. Yellen. Graph theory and its applica-[12] M. Hay, G. Miklau, D. Jensen, and D. Towsley. Resist-[13] M. Hay, G. Miklau, D. Jensen, P. Weis, and S. Srivas-[14] J. M. Kleinberg. Challenges in mining social network [15] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Mon-[16] J. Leskovec and C. Faloutsos. Scalable modeling of real [17] K. Liu, K. Das, T. Grandison, and H. Kargupta. [18] K. Liu and E. Terzi. Towards identity anonymization on [19] L. Liu, J. Wang, J. Liu, and J. Zhang. Privacy preserv-[20] A. Machanavajjhala, J. Gehrke, D. Kifer, and [21] J. Pearl. Probabilistic reasoning in intelligent systems: [22] S. J. Russell and P. Norvig. Artificial Intelligence: A [23] P. Samarati and L. Sweeney. Generalizing data [24] J. Scott. Social Network Analysis Handbook . Sage Pub-[25] J. Srivastava, M. A. Ahmad, N. Pathak, and D. K.-W. [26] L. Sweeney. K-anonymity: a model for protecting pri-[27] Y. Tao, X. Xiao, J. Li, and D. Zhang. On anti-[28] D.-W. Wang, C.-J. Liau, and T. sheng Hsu. Privacy [29] S. Wasserman and K. Faust. Social network analysis: [30] D. J. Watts and S. H. Strogatz. Collective dynamics [31] B. Wellman. For a social network analysis of computer [32] X. Xiao and Y. Tao. Anatomy: Simple and effective [33] X. Xiao and Y. Tao. Personalized privacy preserva-[34] J. Xu, W. Wang, J. Pei, X. Wang, B. Shi, and A. W.-C. [35] X. Yan and J. Han. gspan: Graph-based substructure [36] X. Ying and X. Wu. Randomizing social networks: a [37] E. Zheleva and L. Getoor. Preserving the privacy of [38] B. Zhou and J. Pei. The k -anonymity and l -diversity [39] B. Zhou and J. Pei. Preserving privacy in social net-
