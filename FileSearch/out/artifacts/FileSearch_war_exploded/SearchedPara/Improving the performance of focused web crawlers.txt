 1. Introduction
Crawlers (also known as Robots or Spiders) are tools for assembling web content locally [1]. Focused crawlers in partic-tain subject-specific web portals or web document collections locally or for addressing complex information needs (for which a web search would yield no satisfactory results). Applications of focused crawlers also include guiding intelligent quality and up-to-date results, while minimizing the amount of resources (time, space and network bandwidth) to carry-out the search task. Focused crawlers try to download as many pages relevant to the subject as they can, while keeping the amount of not relevant pages downloaded to a minimum [2].

Crawlers are given a starting set of web pages (seed pages) as their input, extract outgoing links appearing in the seed pages and determine what links to visit next based on certain criteria. Web pages pointed to by these links are downloaded, desired number of pages have been downloaded or until local resources (such as storage) are exhausted.
Crawlers used by general purpose search engines retrieve massive numbers of web pages regardless of their topic. Fo-cused crawlers work by combining both the content of the retrieved Web pages and the link structure of the Web for assign-ing higher visiting priority to pages with higher probability of being relevant to a given topic. Focused crawlers can be categorized as follows: guide the search towards pages of interest. They incorporate criteria for assigning higher download priorities to links based on their likelihood to lead to pages on the topic of query. Pages pointed to by links with higher priority are down-loaded first. The crawler proceeds recursively on the links contained in the downloaded pages. Typically, download pri-orities are computed based on the similarity between the topic and the anchor text of a page link or between the topic is computed using an information similarity model such as the Boolean or the Vector Space Model (VSM) [4].
Semantic crawlers [5] are a variation of classic focused crawlers. Download priorities are assigned to pages by applying semantic similarity criteria for computing page-to-topic relevance: a page and the topic can be relevant if they share con-7].
 cess. They are characterized by the way relevant web pages or paths through web links for reaching relevant pages are
Web pages which is used to train the learning crawler [8,9]. Higher visit priority is assigned to links extracted from web pages classified as relevant to the topic. Methods based on context graphs [10] and Hidden Markov Models (HMM) [11] take into account not only the page content and the corresponding classification of web pages as relevant or not relevant to the topic but also the link structure of the Web and the probability that a given page (which may be not relevant to the topic) will lead to a relevant page within a small number of steps (hops). Hybrid methods [12] suggest combining ideas from learning crawlers with ideas of classic focused crawlers.

In this paper, state-of-the-art approaches for building topic driven focused crawlers are considered and evaluated, with particular emphasis on learning crawlers. The approach to learning crawlers presented in this work is inspired by previous work on Hidden Markov Model (HMM) crawlers [11] for learning paths leading to relevant pages, and from the most suc-ing download priorities to candidate pages. The crawlers mentioned above (and several variants of them) have all been implemented and their performance has been compared using several example topics.

The contributions of this work are the following: (b) Several novel variants of classic, semantic and learning crawlers are proposed and evaluated. Classic focused crawlers (c) A new approach to learning paths reaching web pages relevant to a given topic is proposed inspired by a recent con-
The rest of this paper is structured as follows: Section 2 reviews work on focused crawlers emphasizing on state-of-the-art approaches to Best-First, semantic and learning crawling. Issues related to the design and implementation of crawlers, in Section 5. 2. Related work and background
Commercial search engines use generic topic-independent crawlers for building their index and Web page repository for responding to user queries. GoogleBot 1 , Slurp 2 , MSNBot for implementing search engine crawlers include Breadth-First search, and page importance (using back-link counts or Page-
Rank [14]). 2.1. Focused crawlers
Topic oriented crawlers attempt to focus the crawling process on pages relevant to the topic. They keep the overall num-ber of downloaded web pages for processing [33,34] to a minimum, while maximizing the percentage of relevant pages. pages as input to a crawler or, alternatively, seed pages are selected among the best answers returned by a Web search en-good seed page can be the publications page of an author (laboratory or department). Alternatively, a good seed page can be the personal web page of the author (the home page of a laboratory or department respectively); although the last may con-tain no publications at all, it is known to lead to pages containing publications.

The Fish-search approach [17] assigns binary priority values (1 for relevant, 0 for not relevant) to pages candidate for downloading by means of simple keyword matching. Therefore, all relevant pages are assigned the same priority value.
The Shark-search method [13] suggests using Vector Space Model (VSM) [4] for assigning non binary priority values to can-didate pages. The priority values are computed by taking into account page content, anchor text, text surrounding the links and the priority value of parent pages (pages pointing to the page containing the links). Additional approaches to focused crawling include InfoSpiders and Best-First crawler [3]. InfoSpiders use Neural Networks, while Best-First crawlers assign priority values to candidate pages by computing their text similarity with the topic by applying VSM [4]. Shark-Search can be seen as a variant of Best-First crawler with a more complicated priority assignment function.
Best-First crawlers use only term frequency ( tf ) vectors for computing topic relevance. The use of inverse document fre-too small. Best-First crawlers have been shown to outperform InfoSpiders, and Shark-search as well as other non-focused ing due to its simplicity and efficiency.
 candidate pages. This results into a highly effective crawling algorithm that learns to crawl without direct user training. 2.2. Semantic crawlers
Best-First crawlers estimate page to topic relevance as document similarity. Computing similarity by classical information they share common terms. However, the lack of common terms in two documents does not necessarily mean that the doc-uments are unrelated. For example, two terms can be semantically similar (e.g., can be synonyms or have related meaning) ontologies conceptually similar terms are related by virtue of IS-A (or other types of) links.

All terms conceptually similar to the terms of the topic are retrieved from the taxonomy or the ontology and are used for enhancing the description of the topic (e.g. by adding synonyms and other conceptually similar terms to the topic). Docu-ment similarity can be computed by VSM or by specialized models such as the Semantic Similarity Retrieval Model (SSRM) [6], or the model suggested by Corley and Mihalcea [19] which have been shown to outperform VSM [7]. In this work, Best-
First crawlers are implemented based on this category of similarity metrics, forming the so called semantic crawler meth-ods. Along these lines, topic oriented ontologies have been suggested for finding pages relevant to the topic of interest [5,20] . 2.3. Learning crawlers set of pages and specifies which of them are relevant to the topic of interest. Training may also involve learning the path leading to relevant pages. The training set may consist either of relevant pages only or of both relevant and not relevant and Support Vector Machines [8].In [22], Support Vector Machines are applied to both page content and link context, and their combination is shown to outperform methods using page content or link context alone.
 duce the concept of  X  X  X ontext graphs X ; first back links to relevant pages are followed to recover pages leading to relevant pages. These pages along with their path information form the context graph. The original context graph method builds clas-graph method is the Hidden Markov Model (HMM) crawler [11,30] : The user browses the Web looking for relevant pages and indicates if a downloaded page is relevant to the topic or not. The visited sequence is recorded and is used to train the crawler to identify paths leading to relevant pages.

Chakrabarti et al. [24] proposed a two classifier approach. The open directory (DMOZ troids of training page sets) as topic descriptors.

Hybrid crawlers [12] combine ideas from learning and classic focused crawlers. In the work by Chen [12], the crawler to target pages) or as classic crawler. 3. Crawler design
Issues related to crawler design are discussed next: (b) Page downloading : The links in downloaded pages are extracted and placed in a queue. A non focused crawler uses (c) Content processing : Downloaded pages are lexically analyzed and reduced into term vectors (all terms are reduced to (d) Priority assignment : Extracted URLs from downloaded pages are placed in a priority queue where priorities are deter-3.1. Best-First crawlers
A Best-First crawler assigns download priorities to web pages by computing the document ( d ) similarity between the to VSM [4]: term weights for a document are computed using the following formula: where d i is the weight of term i in document d , tf i is the term frequency of term i in document d , idf frequency of term i , f i is the frequency of term i into d , max f ber of documents and N i is the number of documents containing term i .

There are several variants of the Best-First Crawling strategy, depending on how links in the same page are prioritized: (1) All links in the page receive the same download priority by applying Eq. (1) on the topic and page content (2) Priorities are assigned to pages by computing the similarity between the anchor text of the link pointing to the page irrelevant one. On the other hand, anchor text may be regarded as a summary of the content of the page that the link points bines page content and anchor text as Eq. (3) suggests. 3.2. Semantic crawlers
In semantic crawlers, all terms conceptually similar to topic terms are retrieved from the ontology. They are used for enhancing the description of the topic and for computing the semantic similarity between the topic and the candidate pages. archies. WordNet provides broad coverage of the English vocabulary and can be used for focused crawling on almost every general-interest topic making our implementation the first general purpose semantic crawler.

The similarity between the topic and a candidate page is computed as a function of semantic (conceptual) similarities between the terms they contain. Computing semantic similarity involves computing the similarity between related concepts which are not lexicographically similar [7]. The definition of document similarity depends on the choice of semantic simi-larity function. In this work, the priority of a page p is computed as follows: where k and l are terms in the topic and candidate page (or anchor text of the link) respectively, w similarity between terms is defined, based on WordNet as the underlying taxonomy, as follows: (a) Synonym Set Similarity: sim(k,l) is 1 if l belongs to the synonym set of term k into the WordNet taxonomy and 0 (b) Synonym, Hypernym/Hyponym Similarity: sim(k,l )is1if l belongs to the synonym set of term k , 0.5 if l is a hypernym
The download priority of a link is defined as the average of the similarity of the topic description with anchor text and page content respectively, both computed using Eq. (4). 3.3. Learning crawlers
The Hidden Markov Model (HMM) crawler [11] works by establishing a relation between page content and the path lead-and accumulating a sequence of pages, which he labels as relevant or not. Relevant pages form a cluster ( C evant pages are clustered by applying K-Means [27] ( K is user defined) forming clusters C target page.

A pictorial representation of an HMM training set is shown in Fig. 1 . L (1 link distance from target pages), L 2 are level 2 (2 links away from target pages), and L ferent levels and (conversely) pages at the same level may belong to different clusters.

The following summarizes the parameters and notation used by an HMM crawler [11]: Web pages are characterized by their level or hidden state L i , (where i is the level) and by the cluster C states and observations is modeled by a Hidden Markov Model [28].
I Initial Probability Matrix : p ={ P ( L 0 , ... , L states 1
Components p , A and B are computed prior to crawling based on the training set. The crawler downloads pages, extracts their vector representations (according to VSM) and assigns each page to a cluster using the K-Nearest Neighbors algorithm parameters. This probability corresponds to the visit priority of a link contained in that page. Using the HMM model param-culate the prediction value, each visited page is associated with values a ( L that the crawler downloads a page with hidden state L j at time t . Given values a ( L computed using the following recursion: where a ij is the transition probability from state L i to L a ( L j , t ) the probability that in the next time step the selected page will be in state L
The probability of being at state L 0 (relevant page) in the next step is the priority assigned to pages by the HMM crawler applying Eqs. (5) and (6)).

Notice that the work in [11] suggests pages associated with the same cluster sequence in the path leading to them are puted by the HMM crawler [11] and by the similarity of the centroid vector representing the cluster of relevant (positive) pages with the term vector representation of the page. Two variations of the proposed HMM crawler are proposed, using page content alone, or using both page content and anchor text. Fig. 2 summarizes the operation stages of the proposed crawler.

The operation of the focused crawler is illustrated in Fig. 3 . Pages P downloading. Both clusters C 1 and C 2 have identical probability of leading to cluster C both clusters may lead to target pages into two link steps. The crawler in [11] would assign higher probability to page P (page P 2 may also reach C 0 in two link steps the same as non-target pages in C crawler ( Fig. 2 ) would rather prefer P 2 instead, because of its proximity with the centroid C rect intuitively).
 4. Experimental results 4.1. Experiment setup
All crawlers are implemented in Java 8 . The downloaded pages must be in text/html format and their content size must not exceed 100 KB. Restrictions are also imposed on connection timeout and downloading times for performance reasons. These restrictions apply to all implemented crawlers. The crawling process is repeated until the predefined number of pages rithms X ,  X  X  X ptical nerve X ,  X  X  X hampions league X  and  X  X  X lympic games X .

Crawler performance is typically measured by the percentage of downloaded pages that are relevant to the topic (i.e. highly relevant to the topic. In the following, for clarity we present average numbers (over all topics) of relevant pages.
Each topic is issued as query to Google and the results are inspected by the user. Pages considered relevant by the user topic are compared with the ground truth: for each page returned by the crawler, its document similarity (using VSM) with of a crawler is computed as the average number of positive results over all topics.

The following crawlers are compared: (1) Non Focused Crawlers: (2) Classic Focused Crawlers: (3) Semantic Crawlers: (4) Learning Crawlers:
The crawlers were evaluated using the same 10 topics referred to above. Table 2 in appendix illustrates these topics along with their corresponding seed pages.

All crawlers where initialized using the same set of seed pages. Each crawler downloaded 10,000 pages on each topic
Experiments were conducted between 20/8/2008 and 22/10/2008. The results for each method are represented by a plot show-ing the average number of relevant pages returned by the method as a function of the total number of downloaded pages. Each the topic) as input. Instead, learning crawlers accept a set of target Web pages describing the topic. 4.2. Crawler evaluation We first present a comparative evaluation of the classic crawlers.

The comparison in Fig. 4 demonstrates the poor performance of Breadth-First crawler, as expected for a non focused crawler. Best-First crawler using anchor text outperforms the crawler using page content at the initial stages of crawling, indicating the value of anchor text for computing page to topic relevance. The crawler combining page and anchor text dem-onstrated superior performance. This result indicates that Web content relevance cannot be determined using page or an-chor text alone. Instead, the combination of page content and anchor text forms a more reliable page content descriptor.
The second experiment compares the performance of semantic crawlers using the topics of Table 2 (as in the previous experiment) against the Best-First-page content and anchor text crawler, which is the best classic crawler, as determined earlier.
 The semantic crawler with Synonym Set Similarity proposed in this work achieves almost identical performance with the downloaded. Notice the inferior performance of the method using topic expansion by hyponyms and hypernyms in Wordnet vide terms conceptually similar to the topic. WordNet is a general taxonomy with many thousands of English terms; not all ontologies on several diverse topics were not available to us for these experiments.
 Finally the performance of the HMM crawler [11] is compared with the performance of the proposed learning crawlers. with the centroid of the relevant cluster in the training set. This is computed as document (term vector) similarity using
VSM. The HMM crawler with page content and anchor text-centroid similarity, combines anchor text and page content sim-ilarities with the centroid of the target cluster.

Both implementations of the proposed learning crawler outperform the state-of-the-art HMM crawler [11] as they al-lowed for pages associated with the same cluster sequence in the path leading to them (even for links within a page when using anchor text) to be assigned different priorities depending on their content. 4.3. Overall comparison
As Figs. 4 and 6 indicate, the best average performance is achieved by the Best-First crawler using page content and an-chor text. However, average performance is not fully representative of the strengths and weaknesses of the evaluated crawl-ers since the results obtained differ considerably among various topics. A closer examination of the results indicate that topics can be divided into two categories: the first category is composed of topics unambiguously defined by a list of key-additional relevant pages too. The results obtained in these topics are shown in Fig. 7 .

The Best-First crawler using page content and anchor text clearly achieves the best overall performance. The superior per-keywords and relevant pages can easily be retrieved using content criteria alone.
The second category of topics consists of ambiguous (e.g.  X  X  X hampions league X ,  X  X  X irst aid X ) topics which are not clearly defined by keywords, topics containing terms that appear very frequently in non relevant pages (such as the term games in  X  X  X lympic games X ), or very specialized topics (e.g.  X  X  X engue fever X  and  X  X  X raph algorithms X ). Fig. 8 shows the performance of classic and learning crawlers for ambiguous or specialized topics (i.e., topics numbered 6 X 10 in Table 2 ).

In this set of experiments, the proposed learning crawlers clearly outperform Best-First (classic) crawlers. Methods using combination of Web page content and anchor text are (in the case of learning crawlers) most effective than methods using follows: user search preferences or topic descriptions can more effectively be modeled using a set of example pages, than a few keywords. At the same time, the number of relevant pages can be very small, and learning access paths can result in faster downloading of relevant pages than methods using mere content criteria as all classic crawlers do.
Table 1 illustrates average running times (in minutes) for crawling 10,000 pages along with average (over all topics for 10,000 pages) harvest rate for the methods considered in this work. Running times are only indicative of the time perfor-mance of each method and are presented only for comparison.

Running time statistics are affected by several factors, such as network load, average downloaded file size, amount of par-simpler crawlers such as Breadth First are faster than more involved ones such as learning crawlers. When high performance systems are used (rather than low-end PCs as in this work) we expect that computational time will be minimized, and that network load will be the major factor affecting running time performance. 5. Conclusions and future work
In this work, several variants of focused crawlers were implemented and evaluated using several topics and based on well emphasis is given to learning crawlers based on the Hidden Markov Model (HMM) capable of learning not only the content of target pages (as classic focused crawlers do) but also paths leading to target pages.

Building upon previous work [11] a new HMM crawler is proposed. The strength of this crawler over [11] relies on its ability to distinguish between pages associated with the same cluster sequence in the path leading to them (in [11] such pages are all treated equally and are assigned the same priority value). This is achieved by comparing the content of each candidate page with the centroid of pages matching the query topic (positive examples) in the training set. All crawlers achieve best performance when a combination of page content and link anchor text is taken as the description of the can-didate page.

The addition of semantic relations (suggested by semantic crawlers) did not improve performance over classic crawlers, ontologies such as WordNet.

Learning crawlers take as input user selected pages (rather than a query or topic described by keywords as other focused not always be modeled by such link patterns). The experimental results indicate that the performance of HMM crawlers is promising overall (especially when the search topic is vague or very specific) and may lead to even more successful imple-mentations of learning crawlers in the future. The present work can be regarded as a contribution towards that direction. Appendix See Table 2 .

References
