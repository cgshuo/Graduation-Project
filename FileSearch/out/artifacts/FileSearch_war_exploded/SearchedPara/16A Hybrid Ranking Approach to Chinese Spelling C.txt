 XIAODONG LIU, FEI CHENG, KEVIN DUH and YUJI MATSUMOTO , Nara Institute Spelling check, which is an automatic algorithm to detect and correct human spelling errors in every written language, has been an active research area in natural language processing (NLP) [Sun et al. 2010; Liu et al. 2013; Chen et al. 2011]. The study of such human spelling errors can help both native speakers and second language learners [Lin et al. 2002], as well as language processing systems, such as web search engines [Sun et al. 2010].

Unlike Indo-European languages, such as English, where each word contains sev-eral alphabetic characters, Chinese has a nonalphabetic writing system. Its graphic units, that is, characters, do not represent phonemes, but rather morphemic syllables. Furthermore, the Chinese writing system does not have explicit word boundary mark-ers, such as the word spaces, and the average length of a Chinese word is very short: usually one to four characters. Because of such differences, Chinese spelling check is a different task compared to, for instance, English spelling check, and requires more research.

Current research shows that 97% of Chinese spelling errors are due to phonological and visual similarity between the correct and incorrect character [Chen et al. 2011; Liu et al. 2011]. Phonologically similar Chinese characters have similar pronuncia-tion, which involves the nucleus and the tone. For instance, the four phonologically similar Chinese characters of  X  (cuo4) are: (1)  X  (cuo4), with the same nucleus and the same tone, (2)  X  (cuo1), with the same nucleus and different tone, (3) a different nucleus and the same tone and (4)  X  (chuo1), with a different nucleus and a different tone. On the other hand, the visually similar Chinese characters share the same partial component, such as the Chinese character  X  mentioned before is similar to the Chinese character  X  by sharing the part  X  . Based on this evidence, we can observe that in the Chinese Spelling Check, reducing the phonological and visual errors are the main factors that can help improve Chinese Spelling Check systems.
The SIGHAN 7 shared task provides a common platform to test and evaluate different systems for Chinese Spelling Check. The evaluation includes two subtasks: (1) error detection and (2) error correction.

In this article, we propose a novel framework to deal with Chinese spelling errors and we perform our experiments within the settings of the SIGHAN 7 shared task. Our framework includes two main parts: candidate generation and candidate ranking . For the candidate generation step, our effort is to generate as many as possible cor-rection candidates in the confusion set provided by the shared task. In the ranking step, we select the most possible characters to correct the errors in the given sentence. Additionally, to address the scarceness of resources, we further generate around 2 million artificial training sentences by using the Chinese character confusion sets.
The article is organized as follows. First, we briefly introduce the related work in Section 2 and overview of our framework in Section 3, followed by Section 3.1, Section 3.2 and Section 3.3, which describe the key components of our system. In Section 4, we discuss the experiment setting and experiment results. Chinese Spelling Check is a surging research topic, the goal being to help both lan-guage learners and other Chinese language processing systems [Liu et al. 2013; Chiu et al. 2013; Yeh et al. 2013]. This article is an extension of our previous work [Liu et al. 2013]; in particular we provide additional experiments and analyses of the hybrid system of Liu et al. [2013].

One research line is how to generate the confusion set, which is a collection of candi-dates for the spelling error. In the early work of Chang [1995], the confusion sets were manually edited from 4 viewpoints, that is, shape, pronunciation, meaning, and input keystroke sequence. Then by substituting each character in the input sentence with the characters in the corresponding confusion set, they used a language model to gen-erate a plausibility score to evaluate each possible substituted sentence. Because of the importance of confusion sets, some researchers attempted to automatically extend con-fusion sets by using different Chinese input methods. Intuitively, the characters with similar input key sequences are similar in shape. Zhang [2000] proposed a method to automatically generate confusion sets based on the Wubi method by replacing one key in the input key sequences of a certain character. Lin et al. [2002] used the Cangjie input method to extend confusion sets automatically.

Another approach is the Statistical Machine Translation (SMT) model [Liu et al. 2013; Chiu et al. 2013; Wu et al. 2010]. Given a sentence with or without errors, a SMT system is trained to find the best correction sentence. Since training a better transla-tion model requires large bitexts, which do not exit in the Chinese Spelling Check task, some researchers only employ a language model (LM), which is a model to eval-uate the correctness of a sentence [Liu et al. 2013; Yeh et al. 2013; Chen et al. 2013]. Some rule-based methods also have been proposed. Huang et al. [2007] proposed a method which uses a word segmentation tool to detect Chinese spelling errors. They used the CKIP word segmentation toolkit to generate correction candidates [Hung and Wu 1999]. By incorporating a dictionary and confusion sets, the system can detect whether a segmented word contains errors or not. Hung and Wu [2008] proposed a system which was based on manually edited error templates (short phrases contain-ing one error each). For the cost of editing error templates manually, Chen et al. [2009] proposed an automatic error template generation system. The basic assumption is that the frequency of a correct phrase is higher than the corresponding error template.
Our framework differs from previous research in that we use both a SMT model and a LM as components of our framework for generating the correction candidates; to improve the accuracy, we further employ an SVM classifier to rank the candidates generated by the SMT model and the LM. Our system includes two key components, as shown in Figure 1. Given a sentence with or without error characters, our procedure contains two steps.  X  Candidate Generation . We simultaneously generate the correction character can-didates using the word segmentation-based language model and the statistical machine translation model.  X  Candidate Ranking . We rank the candidates generated in the first step and give the most probable sentence.
 Each component in our system is described in detail in Section 3.1, Section 3.2, and Section 3.3. The confusion set, which lists likely phonological and visual character confusions, is a very valuable resource in generating likely correction candidates. However, it does not include context information, so it may over-generate candidates if applied blindly to a sentence. Therefore we propose a method that combines the confusion sets with language models (LMs), which effectively handle context, to efficiently generate correction candidates.

The LM-based candidate generation includes three steps: (1) word segmentation, which breaks the Chinese sentence into character chunks, called words; (2) candidate lattice construction based on the confusion sets, which are a set of Chinese characters with corresponding similar shape and similar pronunciation characters; and (3) gen-eration of the k -best most likely candidates by using the forward algorithm. Next, we will describe the three steps in detail.

Chinese word segmentation, which breaks the Chinese sentence into words, is one of the fundamental parts of Chinese language processing. In this study, we use the character-based Conditional Random Field (CRF) model for Chinese word segmen-tation [Xue et al. 2003] by using the open source CRFsuite well in out-of-vocabulary recall. The model is trained on the Academia Sinica cor-pus, released under the Chinese word segmentation bake-off 2005, templates are the same as in Sun [2011].

Given a Chinese sentence with spelling errors, the word segmentation results near the spelling error character are divided into two categories. (1) Spelling error character is in a multiple characters word. For instance, in the CRF (2) Spelling error character is in a single character word. For instance, in the
Based on those observations, which are also reported by Wu et al. [2010] and Yeh et al. [2013], we construct the candidate lattice by the following rules.  X  If a word only contains a single Chinese character, add all the candidates in the  X  If a word contains more than one Chinese character and it is not in the dictionary,  X  If a word contains more than one Chinese character and it is in the dictionary, do
The previous subsequence of Chinese sentences is built as shown in Figure 2. For instance, the Chinese word  X   X  is not in the Chinese dictionary, however, by replacing the candidates from confusion sets, we find that the word Chinese dictionary, and we add it into the lattice. Since for a single character word, there is no way to reduce the candidates in the confusion sets, we add all the candidates in the confusion sets as nodes in the lattice. For instance, the single character word (very, quite), we add all the candidates in the confusion sets, like into the lattice.

Finally, the forward algorithm [Rabiner 1989] is used to find the k -best sentences, where the score for each sentence is computed by where, X 1 X 2 ... X N denotes a sequence of Chinese characters (a.k.a. a sentence), N is length of the sentence and n is order of language model. We estimated the conditional probabilities of the language model on Chinese Gigaword by using the open source SRILM 3 package. From our experiments on both the training data and the dry run data provided by SIGHAN 7, when the order of language model is larger than 5, the accuracy does not change any more. Therefore, the parameter, the order of the language model, is set to 5 in all our experiments. As an alternative, we also employ a statistical machine translation model [Brown et al. 1993] as a way to detect and correct character errors [Wu et al. 2010].

Given a sentence with errors treated as a source language, our goal is to find the best correction sentence. Formally, given a sentence S which might contain error characters in it as a source sentence, the output is the sentence  X  C in the target language with the highest probability of different replacement C . Symbolically, it is represent by
Using the Bayes Rule, we can rewrite Formula (2) as
Here, p ( S | C ) is called an error model , which is the chance that a Chinese character is wrongly written; while p ( C ) is the language model , which evaluates the quality of the corrected Chinese sentence. Traditionally, p ( S | C ) can be estimated by using the word alignment models [Brown et al. 1993; Och and Ney 2003] from the  X  X rror-correct X  sentence pairs.

Unlike the language model-based candidate generation model, the SMT models de-tect and correct the spelling errors by incorporating both the error model and language model. If we have a large training corpus to estimate a better error model, which is treated as how likely a Chinese character is wrongly written, we can obtain better results. However, due to the scarcity of training data for the error model, it is diffi-cult to estimate the true parameters of the error model. To deal with this problem, we generate 2 million artificial training data by replacing each character in the provided 700 sentences in the training set with candidates in the confusion sets, as shown in Figure 3. These generated training data are very important for training error model SMT, because most candidates, not observed in training data, have zero probabilities. Empirically, we conduct a set of experiments and observe that without generated data, SMT cannot generate any candidates. One of the important assumptions under this algorithm is that the probabilities of error candidates in the confusion sets tend to be same, in other words, it is a uniform distribution, and by observing more spelling er-rors, the probabilities of such candidates increase while other candidates, which could not be observed, decrease. Therefore, compared to a LM model, the SMT model can generate more candidates.

To train our error model, we adopt the IBM 4 model with default iteration num-ber (1 5 3 3 4 3 ) by using the GIZA++ toolkit, which is an open source package alignment. For the language model, we use the SRILM package, mentioned before, with Kneser-Ney smoothing algorithm. The order of LM is set to 5, same as the LM generation method.
 In the decoding step, we employ the Moses toolkit 5 to find the best translations. Since there is no character reordering in Chinese Spelling check, we disable the dis-tortion, setting it to 0, in all the experiments. For parameter optimization, we tried Minimum Error Rate training (MERT) in the Moses toolkit, however, we do not find any improvement. Therefore, we only tune the language model factor, denoted as  X  ,of SMT model in the following experiments. Although any ranking algorithm can be a ranking component in our framework, for simplicity yet good performance, we adopt the Support Vector Machines (SVMs) in our system, which are supervised learning models used for classification and regression analysis [Burges 1998]. The goal of the Chinese spelling error detection task is to detect whether there are any errors in a given sentence, which we can treat as a series of binary classification problems: 6 if the current character is a spelling error, the label is 0, otherwise the label is 1.

Pitler et al. [2010] use the SVM-derived confidence score between two words in English noun phrases. It inspired us to use such confidence score to determine how likely the current character is a spelling error. By merging the origi-nal input character and the error candidates of the previous models, the system creates a candidate list for each character in the input text. And then candidates in the list will be ranked based on the confidence score computed by the SVM classifier. Finally the character with the highest confidence score will be treated as the correct character of our system. Besides improving precision, this approach also allows us to perform error detection task and the correction task simultaneously. In Figure 4, we give an example to show how our ranking component works. For instance, for the fourth character the input sentence  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X   X , the SMT 1-best model predicts a candidate  X  . Then we merge the input character and the predicted candidate and cre-ate a candidate list containing two characters:  X  and  X  . Finally, we rank them based on the confidence score computed by the SVM classifier and select the candidate with the highest score as output.

The features for our SVM are defined as follows: we denote a character token c a context sequence: ... c  X  2 c  X  1 c 0 c + 1 c + 2 ... and c the position s and ends at position e . Our system extracts the following features for each candidate:  X  character features (Base): c  X  3 , c  X  2 , c  X  1 , c 0 , c  X  pointwise mutual information (PMI) [Bouma 2009] between two characters,  X  identity of the character sequence if it exists in the dictionary (DICT) and the
We adopt the LIBLINEAR toolkit, an open library for SVMs, classifier with the L2-loss function. We tune the penalty parameter of the error term, C , by using the 5-fold cross-validation on the training data and the dry run data as well.

Further, we additionally applied a downscaling factor to the confidence scores of the generated candidates in order favor the original input character and prevent excessive correction. Empirically on the dry run data, we found that multiplying the confidence scores of the generated candidates (but not the original input character) by a factor of 0.625 gave good precision and recall.

To obtain more training data for the classifier, we generated 100-best artificial sentences using the LM procedure described in Section 3.1. Correct candidates are labeled as positive and incorrect candidate characters are labeled as negative. In our dataset, the number of training sentences is small and the average number of spelling errors in each sentence is one, and we found that such a large k-best is helpful for ensuring that the SVM observes the corrections. To train our models, we use the following datasets. (1) Confusion Set . Confusion sets are sets of Chinese characters including 5,401 (2) Dictionary . CC-CEDICT, a free traditional Chinese dictionary released by Creative (3) Bakeoff 7 Data . The statistics of the dataset, including the training, dry run and (4) Generated Artificial Data . From the Table I, we can find that the size of the training (5) Chinese Segmentation Data . Academia Sinica corpus in Bake-off 2005 for tra-(6) Chinese Gigaword . We use the whole traditional Chinese documents, acquired from In the Chinese Spelling Check shared task, there are two sets of evaluation metrics for the Error Detection task (sub1) and the Error Correction task (sub2) [Wu et al. 2013], which are introduced in detail as follows.

For the error detection subtask, the shared task adopts sentence level metrics for performance evaluation, defined as
For the error correction subtask, the shared task adopts similar sentence-level evaluation metrics, which is defined as follows:
Additionally, to evaluate the performance in detail, we also employ new metrics to evaluate the accuracy of Chinese Spelling Check in the character level. The additional evaluation metrics are
In the next sections, we use the character-level metrics to evaluate the candidate generation step and the candidate ranking step for analyzing the true performance of systems. Then we report our final results by using the shared task metrics for a fair comparison with the other systems in the shared task. In this section, we investigate the performance of our candidate generation component, which includes the language model-based approach and statistical machine transla-tion method. This step is very important, because a missed error would not be recov-erable. We are interested in knowing how many k -best candidates we need to generate to achieve good recall. Note that in our article, we only concatenate all the candidates from different systems together. Although, there are different ways to make LM and SVM work together, such as intersection the candidates from both systems, it may loss the true candidates, which cannot be recovered in the ranking part. Our strategy is ob-taining a higher recall in candidates generation step and obtaining a higher precision in ranking part. In the following experiments, we perform them on the dry run dataset to tune and to select the parameters.

Table II shows the character level recall on error detection task, also known as the subtask 1. We show the recall when picking the best possible (oracle) candidate in the k -best list. Note that k refers to the number of candidates generated, which is added to the original input sequence; So, the column k =1 refers to the oracle recall of a list of 2 sentences, the column k =2 refers to the oracle recall of a list of 3 sentences, and so on. Comparing the SMT model with the default language model factor parameter (  X  = 0.5), the LM obtained higher recall. Furthermore, with the number of k -best increasing, the LM outperforms the SMT at all the different settings. We hypothesize this is because (1) the word segmentation module is very effective at reducing the number of incorrect candidates, and (2) even with the artificial training data, the SMT is not able to generate sufficiently diverse k -best when k is relatively small.
Nevertheless, the advantage of a hybrid model can be observed, by concatenating the candidate of both the LM and SMT. It outperforms either the single language model or SMT because the candidates generated by the LM and SMT are different. In other words, the candidates generated by the LM and SMT are complementary to each other. The same observation can be obtained from Table III, which shows the results for subtask 2 (Error correction). Also, note that the recall on the error detection task (subtask 1), which is 1 in the 20 best list, is much better than the recall on the error correction task (subtask 2), which is 0.7838 in the 20 best list. We hypothesize that the reason is the inconsistency of the error distribution and the error rate, which is shown in Table I.

Figure 5 shows an example of how we catch the oracle candidates, given a real in-stance in the test data. First, our LM and SMT models generate the 2-best candidates, respectively. Note that these two k -best candidate lists usually contain different error candidates. Last, we merge those all the candidates for ranking. To evaluate the importance of the SVM ranking, we perform a set of experiments and tune parameters on the dry run dataset. The evaluation metrics are the character level metrics, introduced in Section 4.2.

Feature selection plays a crucial role in machine learning community and natural language processing [Yu and Liu 2004; Liu and Yu 2005; Manning and Sch  X  utze 1999]. We are interested in which kind of features should be used in our ranking system to obtain a good precision.

As shown in Tables IV and V, comparing the SVM-base model, which only uses the local character feature, the model which uses all types of feature including the local character feature, the dictionary feature, the n-gram language model feature and the pointwise information feature, significantly improved the F-score in both subtasks. Comparing to the original LM output, our SVM model with all feature types obtains significant improvement in the precision from 0.18 to 0.75 without dropping the recall on the error detection task. In the error correction task, although the recall dropped, the SVM with all features obtains a competitive F-score, which is a trade-off between the precision and recall. One interesting observation is that the LM model obtains a higher F-score compared to our model with the SVM ranker. The reason is mainly because of the extremely unbalanced error distribution in the training and dry run data as shown in Table I. On the other hand, it implies that we can choose a larger number of k -best in the candidate generation step to improve the recall, which will be reported in the next section.

Table VI shows how the accuracy is affected by the ranking component on the dry run dataset for the error detection task (subtask 1). One observation is that compar-ing the systems without ranking, our proposed approach with ranking reduces wrongly generated candidates and improves the precision score with a small sacrifice on the re-call, however, the F-score is improved. For example, in the LM model of 1-best setting, the precision increased to 0.75 from 0.18 by using the SVM ranker. One more interest-ing observation is that the LM outperforms the SMT due to the lack of training data to estimate a better error model, which is introduced in Section 3.2. Similar observations can be obtained from the Table VII, which is shown the results of subtask 2. All these evidences demonstrate the importance of our ranking component. To avoiding the bias on the small dry run data, we also conduct a serial experiments on training data by using 5-fold validations and can have similar observation as on the dry run data. In the final test, we use the standard test datasets provided by the shared task. Note that we use the best setting, which is empirically suggested by Section 4.4. These datasets contain 1000 sentences for each subtask: the error detection subtask and error correction subtask.

As shown in Table VIII, our system outperformed the Sinica&amp;NTU1, which used similar resources as ours, and achieved lower false alarm rate, as well as higher de-tection F-score and error location F-score. Furthermore, our system even exceeded the Sinica&amp;NTU2 and Sinica&amp;NTU3 systems in performance, which used additional in-formation from a web search engine (Baidu) [Chen et al. 2013]. However, our system was beaten by the two best systems reported in the shared task. Comparing our sys-tem, these two best systems in shared task used more resources, such as the POS tagging information, a considerable larger dictionary and an large idiom dictionary. We strongly believe that such resources have a great contribution to improve the Chinese Spelling Check System, and they can be flexibly incorporated into our pro-posed system if available.
 We can obtain similar results in the error correction task (sub2) shown in Table IX. Our system also outperformed all the three systems of Sinica&amp;NTU, which used the same resource as ours. However, the two best systems, reported in the shared task, also obtained the best performance in the error correction task. To recap, our system significantly outperformed the three systems of Sinica&amp;NTU, which is the best system that used similar resources in the shared task. Even comparing with the state-of-the-art systems, which used more resources than ours, our results are still competitive. Chinese spelling check is a hard problem because the error detection must be done within a context. The situation is that the context information is also ambiguous. We found that our system is quite effective if the local context provides sufficient infor-mation, but fails otherwise. The main error types produced by our system are the following. (1) Semantic error . In the sentence with id  X 0003 X  in the error detection task (subtask (2) Pronoun agreement error . In the sentence with id  X 0008 X  in the error detection
From such observations, we believe that the long-distance information or other se-mantic information may help to improve our Chinese Spelling Check system, if the corresponding resources are available. We proposed a simple and effective framework for Chinese Spelling Check which in-cludes two key components: candidate generation and candidate ranking. First, we generated the candidates by using the LM and SMT to achieve large recall. Then, to improve the precision, we employed an SVM classifier to rank the generated candi-dates and give the most likely correction. In this article, we examined in depth issues such as what type/number of candidates are most effective in improving recall, and what ranking features are best for improving precision. We also proposed a simple approach to improve the SMT model by replacing the characters in the training data with all the candidates in the confusion set, to generate many artificial samples. Our final test results reveal that our framework outperforms other systems, which adopted the same or similar resources as ours in the SIGHAN 7 shared task. Even compared with state-of-the-art systems, which used more resources, such as a considerable larger dictionary, an idiom dictionary, and other semantic information, our framework still obtains competitive results.

