 The relationship between constraint-based mining and con-straint programming is explored by showing how the typical constraints used in pattern mining can be formulated for use in constraint programming environments. The resulting framework is surprisingly flexible and allows us to combine a wide range of mining constraints in different ways. We implement this approach in off-the-shelf constraint progra m-ming systems and evaluate it empirically. The results show that the approach is not only very expressive, but also works well on complex benchmark problems.
 H.2.8 [ Database Management ]: Database applications X  Data Mining ; F.4.1 [ Mathematical Logic and Formal Languages ]: Mathematical Logic X  Logic and Constraint Programming Algorithms, Theory Itemset Mining, Constraint Programming
For quite some time, the data mining community has been interested in constraint-based mining , that is, the use of con-straints to specify the desired properties of the patterns t o be mined [1, 4, 5, 6, 8, 11, 12, 15, 16, 10]. The task of the data mining system is then to generate all patterns satisfy-ing the constraints. A wide variety of constraints for local pattern mining exist and have been implemented in an even wider range of specific data mining systems.

On the other hand, the artificial intelligence community has studied several types of constraint-satisfaction prob lems and contributed many general purpose algorithms and sys-tems for solving them. These approaches are now gathered in the area of constraint programming [2, 13]. In constraint programming, the user specifies the model, that is, the set of constraints to be satisfied, and the constraint solver gen -erates solutions. Thus, the goals of constraint programmin g and constraint based mining are similar (not to say iden-tical); it is only that constraint programming targets any type of constraint satisfaction problem, whereas constrai nt-based mining specifically targets data mining applications. Therefore, it is surprising that despite the similarities b e-tween these two endeavours, the two fields have evolved in-dependently of one another, and also, that  X  to the best of the authors X  knowledge  X  constraint programming tools and techniques have not yet been applied to pattern mining, and, vice versa, that ideas and challenges from constraint-base d mining have not yet been taken up by the constraint pro-gramming community.

In this paper, we bridge the gap between these two fields by investigating how standard constraint-programming tec h-niques can be applied to a wide range of pattern mining problems. To this aim, we first formalize most well-known constraint-based mining problems in terms of constraint pr o-gramming terminology. This includes constraints such as frequency, closedness and maximality, and constraints tha t are monotonic, anti-monotonic and convertible, as well as variations of these constraints, such as  X  -closedness. We then incorporate them in off-the-shelf and state-of-the-ar t constraint programming tools, such as Gecode 1 [14] and ECLiPSe 2 [2], and run experiments. The results are surpris-ing in that 1) using the constraint programming approach, it is natural to combine complex constraints in a flexible man-ner (for instance,  X   X  closedness in combination with mono-tonic and anti-monotonic constraints); unlike in the exist -ing constraint-based mining systems, this does not require modifications to the underlying solvers; 2) the search strat -egy of constraint programming systems turns out to parallel the search strategy of existing, specialized constraint-b ased mining approaches among which Eclat [16], LCM [15] and DualMiner [5]; 3) even though the constraint programming methods were not meant to cope with the specifics of data mining (such as coping with large data sets, having 10 000s of constraints to solve), and even though the focus of this study is not on the development of efficient algorithms, it turns out that existing constraint programming systems al-ready perform quite well as compared to dedicated data min-ing solvers in that on a number of benchmark problems their performance is similar, and in some cases even better com-http://www.gecode.org/ http://eclipse.crosscoreop.com/ pared to state-of-the-art itemset miners [10]. At the same time, it should be clear that  X  in principle  X  the resulting constraint programming methods can be further optimized towards data mining.

This paper is organized as follows: in Section 2 we intro-duce a wide variety of constraints for itemset mining prob-lems; in Section 3 we introduce the main principles of con-straint programming systems; Section 4 then shows how the contraint-based mining problems can be formulated using constraint programming principles; Section 5 then compare s the operation of constraint programming systems with those of dedicated itemset mining algorithms. Section 6 reports o n an experimental evaluation comparing an off-the-shelf con-straint programming system with state-of-the-art itemset mining implementations, and finally, Section 7 concludes.
Let I = { 1 ,...,m } be a set of items, and T = { 1 ,...,n } a set of transactions. Then an itemset database D is a binary matrix of size n  X  m . Furthermore,  X  : 2 I  X  2 T is a function that maps an itemset I to the set of transactions from T in which all its items occur, that is, Dually,  X  : 2 T  X  2 I is a function that maps a transaction-set T to the set of all items from I shared by all transactions in T , that is, It is well-known that  X  and  X  define a Galois connection between the lattices ( T ,  X  ) and ( I ,  X  ). This means that the following properties are satisfied: In the remainder of this section, we will introduce several well-known constraints in itemset mining by making use of these operators. We will show that many itemset mining problems can be formulated as a search for pairs ( I,T ), where I is an itemset and T a transaction set.
 Frequent Itemsets. Our first example are the traditional frequent itemsets [1]. The search for these itemsets can be seen as searching for solution pairs ( I,T ), such that where  X  is a frequency threshold. The first constraint spec-ifies that T must equal the set of transactions in which I occurs; the next constraint is the well-known minimum fre-quency requirement: the absolute number of transactions in which I occurs must be at least  X  . The properties of the  X  operator imply that minimum frequency is an anti-monotonic constraint: every subset of an itemset that satis-fies the frequency constraint, also satisfies the constraint . Anti-Monotonic Constraints. Other examples of anti-monotonic constraints are maximum itemset size and maximum total itemset cost [5, 4]. Assume that every item has a cost c i (in the case of a size constraint, c i = 1). Then we satisfy a maximum cost (size) constraint, for c ( I ) = P Monotonic Constraints. The duals of anti-monotonic constraints are monotonic constraints. Maximum frequency , minimum size and minimum cost are examples of monotonic constraints [5, 4]. Maximum frequency can be formulated similar to (2) as: while minimum cost is expressed similar to (3) by These constraints are called monotonic as every superset of an itemset that satisfies the constraints, also satisfies the se constraints.
 Convertible Anti-Monotonic Constraints. Some con-straints are neither monotonic nor anti-monotonic, but sti ll have properties that can be exploited in mining algorithms. One such class of constraints are the convertible (anti-)mo no-tonic constraints [12]. Let us illustrate these by the mini-mum average cost constraint, which can be specified as This constraint is called convertible anti-monotone as we c an compute an order on the items in I such that for any itemset I  X  X  , every prefix I  X  of the items in I sorted in this order, also satisfies the constraint. In this case, we can order the items decreasing in cost: the average cost can only go up if we remove the item with the lowest cost.
 Closed Itemsets. Closed Itemsets are a popular condensed representation for the set of all frequent itemsets and thei r frequencies [15]. Itemsets are called closed when they sati sfy in addition to constraint (1); alternatively this can be for -mulated as I =  X  (  X  ( I )). A generalization are the  X  -closed itemsets [7], which are itemsets that satisfy the traditional closed itemsets are a special case with  X  = 0. Maximal Itemsets. Maximal frequent itemsets are another condensed representation for the set of frequent itemsets [ 6]. In addition to the frequent itemset constraints (2) and (1) these itemsets also satisfy all itemsets that are a superset of a maximal itemset are infrequent, while all itemsets that are subsets are frequen t. Maximal frequent itemsets constitute a border between item-sets that are frequent and not frequent.
 Emerging Patterns. If two databases are given, one can be interested in finding itemsets that distinguish these two databases. In other words, one is interested in finding tripl es ( I,T (1) ,T (2) ), containing an itemset I and two transaction sets T (1) and T (2) , such that some distinguishing property Figure 1: There are two combinations of closedness and maximum size between T (1) and T (2) holds. Among the many ways to score a pattern X  X  ability to distinguish two datasets is the following constraint: for a given threshold  X  ; we assume that for every database we have separate  X  and  X  operators. An itemset that satisfies these constraints is called emerging [8].
 Combining Constraints. As pointed out in the intro-duction, it can also be interesting to combine constraints. Defining combinations is not always straightforward when working with constraints such as maximality and closedness [3]. For example, assume we want to mine for  X   X  closed fre-quent itemsets that have a size lower than a threshold. Two interpretations are possible. We can define closedness with respect to the set of all frequent itemsets, which means we combine (1), (2), (3) and (8) into: The other interpretation is that we mine for the itemsets that are  X   X  closed within the set of small itemsets: The difference between these two settings is illustrated in Figure 1 for  X  = 0 and  X  = 1. Itemsets closed according to constraint (8) are dashed in this figure. In the first setting, only the itemset { 3 } satisfies the constraints; in the second setting, the itemsets { 1 } and { 2 } are also closed considering the maximum size constraint.

Similarly, combinations of maximality and anti-monotonic constraints also have 2 interpretations. Further combina-tions can be obtained by combining them with emerging patterns. The challenge that we address in this paper, is how to solve such a broad range of queries and their combi-nations in a unified framework.
Constraint programming is a declarative programming par-adigm: instead of specifying how to solve a problem, the user only has to specify the problem itself. The constraint Algorithm 1 Constraint-Search( D ) 1: D :=propagate( D ) 2: if D is a false domain then 3: return 4: end if 5: if  X  x  X  X  : | D ( x ) | &gt; 1 then 7: for all d  X  D ( x ) do 8: Constraint-Search( D  X  X  x 7 X  X  d }} ) 9: end for 10: else 11: Output solution 12: end if programming system is then responsible for solving it. Con-straint programming systems solve constraint satisfactio n problems (CSP). A CSP P = ( V ,D, C ) is specified by A constraint C ( x 1 ,...,x k )  X  C is a boolean function from variables { x 1 ,...,x k }  X  V . A constraint is called unary if it involves one variable and binary if it involves two. A domain D  X  is called stronger than the initial domain D if D ( x )  X  D ( x ) for all x  X  V . A domain is false if there exists an x  X  V such that D ( x ) =  X  ; a variable x  X  V is called fixed if | D ( x ) | = 1. A solution to a CSP is a domain D  X  that fixes all variables (  X  x  X  V : | D  X  ( x ) | = 1) and sat-isfies all constraints: abusing notation, we must have that  X  C ( x 1 ,...,x k )  X  C : C ( D  X  ( x 1 ) ,...,D  X  ( x k more D  X  must be stronger than D , which guarantees that every variable has a value from its initial domain D ( x ).
Example 1. Assume we have four people that we want to allocate to 2 offices, and that every person has a list of other people that he does not want to share an office with. Further-more, every person has identified rooms he does not want to occupy. We can represent an instance of this problem with four variables, which represent the persons, and inequalit y constraints, which encode the room-sharing constraints:
The simplest algorithm to solve CSPs enumerates all pos-sible fixed domains, and evaluates all constraints on each of these domains; clearly this approach is inefficient. The outline of a general, more efficient Constraint Programming (CP) system is given in Algorithm 1 above [14]. Essentially, a CP system performs a depth-first search; in each node of the search tree the algorithm branches by assigning values to a variable that is unfixed (line 7). It backtracks when a violation of constraints is found (line 2). The search is further optimized by carefully choosing the variable that i s fixed next (line 6); a function f ( x ) ranks variables, for in-stance, by determining which variable is involved in most constraints.

The main concept used to speed-up the search is con-straint propagation (line 1). Propagation reduces the do-mains of variables such that the domain remains locally con-sistent . One can formally define many types of local consis-tencies, but we skip these definitions here. In general, in a locally consistent problem a value d does not occur in the domain of a variable x if it can be determined that there is no solution D  X  in which D  X  ( x ) = { d } . The main motiva-tion for maintaining local consistencies is to ensure that t he backtracking search does not unnecessarily branch, thereb y significantly speeding up the search.

To maintain local consistencies propagators or propagation rules are used. A propagator takes as input a domain and outputs a stronger, locally consistent domain. The propa-gation rules are derived by the system from the user speci-fied constraints. A checking propagator is a propagator that produces a false domain once the original constraint is vi-olated. Most propagators are checking propagators. The repeated application of propagators can lead to increasing ly stronger domains. Propagation continues until a fixed point is reached in which the domain does not change any more (line 1). There are many different constraint programming systems, which differ in the type of constraints they support and the way they handle these constraints. Most systems assign priorities to constraints to ensure that propagator s of lower computational complexity are evaluated first. The main challenge is to manipulate the propagators such that propagation is as cheap as possible.

Example 2 (Example 1 continued). The initial do-main of this problem is not consistent: the constraint x 1 cannot be satisfied when D ( x 1 ) = { 2 } ; consequently 2 is removed from D ( x 1 ) . Subsequently, the binary constraint x 1 6 = x 2 cannot be satisfied while x 2 = 1 . Therefore value 1 is removed from the domain of x 2 . The propagator for the constraint x 1 6 = x 2 has the following form: After applying all propagators in our example, we obtain a fixed point in which D ( x 1 ) = { 1 } and D ( x 2 ) = { 2 } , which means persons one and two have been allocated to an office. Two rooms are possible for person 3. The search branches therefore. For each of these branches, the second inequal-ity constraint is propagated; a fixed point is then reached in which every variable is fixed, and a solution is found.
To formulate itemset mining problems as constraint pro-gramming models, we only use variables with binary do-mains, i.e. D ( x ) = { 0 , 1 } for all x  X  V . Furthermore, we make extensive use of two types of constraints. The first is a summation constraint, whose general form is as follows: In this constraint, V  X  V is a set of variables and w x is a weight for variable x and can be either positive or negative.
To make clear how this constraint can be propagated, we show a propagator here, such as implemented in most CP systems. Let us use the following notation: x max = { x  X  V | w x  X  0 } and V  X  = { x  X  V | w x &lt; 0 } .
At any point during the search the following constraint must be satisfied in order for Equation (10) to be satisfied: The correctness of this formula follows from the fact that the lefthand side of the equation denotes the highest value that the sum can still achieve.

A checking propagator derived from the constraint for a variable x  X   X  V + conceptually has the following effects: 3: D ( x  X  ) = { 1 } 4: end if 5: else 6: D ( x  X  ) =  X  7: end if Only in line 3 an effective domain reduction takes place. A similar propagator can be derived for a variable x  X   X  V  X 
Example 3. Let us illustrate the application of the prop-agator for the summation constraint on this problem: In this case, we know that at least one of x 2 and x 3 must have the value 1, but we cannot conclude that either one of these variables is certainly zero or one. The propagator doe s not change any domains. On the other hand, if the propagator determines that D ( x 2 ) = D ( x 3 ) = { 1 } .
Another special type of constraints that we will use are reified constraints : where C is a constraint and x is a boolean variable. A reified constraint binds the value of one variable to the evaluation of a constraint. An example of such a constraint is which states that if the weighted sum of certain variables V is higher than  X  , variable x  X  should be true, and vice-versa. We can decompose a reified constraint in two directions: We show the propagation that can be performed for both directions, in the special case of Equation (11). For the di-rection C  X  x , the propagation is: 2: if D ( x ) = { 0 } then apply propagators for  X  C For the reverse direction the propagation is: 2: if D ( x ) = { 1 } then apply propagators for C
It is important to note the difference between a summation constraint C and its reified version x  X  C . As we can see from the code above, the propagator for x  X  C is not as expensive to evaluate as the propagator for C as soon as 1 6 X  D ( x ).

Even though the C  X  x constraint can be expressed in both ECLiPSe and Gecode, we found that the reified im-plications C  X  x and C  X  x are not available by default; in our implementations (see Section 6) we use additional variables to express one direction of reified constraints.
We now introduce the models of itemset mining prob-lems that can be provided to constraint programming sys-tems. We choose the following representation of itemsets and transactions. For every transaction we introduce a vari -thus, we can conceive an itemset I as a vector of length m with binary variables; a transaction set T is a vector of length n .

Theorem 1 (Frequent Itemsets). Frequent itemset mining is expressed by the following constraints:
Proof. The first constraint (12) is a reformulation of the coverage constraint (1): The second constraint (13) is derived as follows. We can reformulate the frequency constraint as: Together with the coverage constraint, this constraint de-fines the frequent itemset mining problem. As argued in the previous section, however, reified constraints can sometim es be desirable. To this purpose, we rewrite the frequency con-straint further. First, we can observe that  X  i  X  I : | T | = valid solution Please note that reification increases the number of con-straints significantly; it will depend on the problem settin g if this increase is still beneficial in the end. In the next sec -tion we will study how a constraint programming system operates in practice on these constraints.

Many other anti-monotonic constraints can also be speci-fied in a straightforward way.

Theorem 2 (Anti-Monotonic Constraints). The maximum total cost constraint is expressed by:
Theorem 3 (Monotonic Constraints). A monotonic minimum cost constraint is specified by or, equivalently, reified as Proof. The reified version of the constraint exploits that I  X   X  ( { t } ) for every t  X   X  ( I ); starting from (5):
Theorem 4 (Convertible Constraints). The con-vertible minimum average cost constraint is specified as fol -lows: Equivalently, a reified version can be used similar to (15) . Proof. This follows from rewriting constraint (6): X
Theorem 5 (Closed Itemsets). The frequent closed itemset mining problem is specified by the conjunction of the coverage constraint (12) , the frequency constraint (13) and The more general  X   X  closed itemsets are specified by
Proof. Formulation (16) follows from the second Galois operator in (7) similar to the reformulation of (12) above. We skip the derivation for the  X  -closed itemsets due to lack of space.
 Theorem 6 (Maximal Frequent Itemset Mining).
 The maximal frequent itemset mining problem is specified by the coverage constraint (12) and constraint
Proof. The maximality constraint is reformulated as fol-lows: Together with the minimum frequency constraint (13) this becomes a two sided reified constraint. Figure 2: Search tree for the frequent itemset min-ing problem.

Theorem 7 (Emerging Patterns). The problem of find-ing frequent emerging patterns is specified by:  X  k  X  X  1 , 2 } :  X  t  X  X  ( k ) : T ( k ) t = 1  X  X  X  i  X  X  : I i = 1  X  X  X  i  X  X  : I i = 1  X  X
Proof. This follows from the reification of | T (1) | / | T (2) | X   X   X  X furthermore, given that two datasets are given, coverage is expressed for both datasets.
We now investigate the behavior of constraint program-ming systems applied to itemset mining problems and com-pare this to standard constraint-based mining techniques. Let us start with the frequent itemset mining problem. The search tree for an example database is illustrated in Figure 2. We use a minimum frequency threshold of  X  = 2. In the initial search node ( n 1 ) the propagator for frequency constraint (13) sums for each item the number of transac-tions having that item, and determines that item 4 is only covered by 1 transaction, and therefore sets D ( I 4 ) = { 0 } . The propagators for coverage constraint (12) determine tha t transaction 2 is covered by all items, and hence D ( T 2 ) = { 1 } . This leads to the domain in node n 2 , where we have to branch. One of these branches leads to node n 5 , setting D ( I 1 ) = { 1 } , thus including item 1 in the itemset. Cov-erage propagation sets transaction D ( T 3 ) = { 0 } ; frequency propagation determines that itemset { 1 , 3 } is not frequent and sets D ( I 3 ) = { 0 } . Coverage propagation determines that transaction 1 is covered by all remaining items and sets D ( T 1 ) = { 1 } . Propagation stops in node n 6 . Here both possibilities in D ( I 2 ) are considered, but no further propa-gation is possible and we find the two frequent itemsets { 1 } and { 1 , 2 } .

In this example, the reified constraint is responsible for setting an item D ( I i ) = { 0 } . Without reification the fre-quency constraint would never influence the domain of items. In our example, the system would continue branching below node n 7 to set D ( I 3 ) = { 1 } , and only then find out that the resulting domain is false. By using the reified constraints, the system remembers which items were infrequent earlier, and will not try to add these deeper down the search tree. This example illustrates a key point: by using reified con-straints, a CP system behaves very similar to other well-known depth-first itemset miners such as Eclat [16] and FP-Growth [11]. For a given itemset also these miners maintain which items can still be added to yield a frequent itemset (in FP-Growth, for instance, a projected database only contain s such items). The transaction variables store a transaction set in a similar way as Eclat does: the variables that still have 1  X  D ( T t ) represent transactions that are covered by an itemset during the search. A difference between well-known itemset miners and a CP system is that a CP system also maintains a set of transactions that are fixed to 1; further-more, the CP system explicitly sets item variables to zero, while other systems usually do this implicitly by skipping them.

Let us consider the maximal frequent itemset mining prob-lem next. Compared to the search tree of Figure 2, the search tree for maximal frequent itemset mining is differ-ent below node n 6 : applying the additional propagator for the maximality constraint (19), the CP system would now conclude that a sufficient number of transactions containing item 2 have been fixed to 1 and would remove 0 from the domain of item I 2 . As all variables are fixed, the search stops here and itemset { 1 , 2 } is found. This behavior is very similar to that of a well-known maximal frequent itemset miner: MAFIA [6] (and its generalization DualMiner [5]) uses the set of  X  X navoidable X  transactions (obtained by com -puting the supporting transactions of the Head-Union-Tail (HUT) itemset), and immediately adds all items if the HUT turns out to be frequent.

Likewise, we can consider closed frequent itemset mining : in node n 6 the CP system would conclude that all transac-tions with D ( T t ) = { 1 } support item 2, and would remove value 0 from D ( I 2 ) due to constraint (16); in general, this means that for every itemset, the items in its closure are computed; if an item is in the closure, but is already fixed to 0, the search backtracks, otherwise, the search continue s with all items added. The same search strategy is employed by the well-known itemset miner LCM [15].

If we look at a monotonic minimum cost constraint, where we assume c 1 = c 2 = c 4 = 1 and c 3 = 2, and the cost threshold is at  X  = 3, the search tree would differ already at node n 2 : if the reified constraint (15) is used, the items in transaction 1 are not expensive enough to exceed the cost threshold, and D ( T 1 ) = { 0 } is set. The effect is that item 1 does not have sufficient support any more, and is set to zero. This kind of pruning for monotonic constraints was called ExAnte pruning and was implemented in the ExaMiner [4].
Until now we did not specify how a CP system selects its next variable to fix (Algorithm 1, line 6). A careful choice can influence the efficiency of the search. This sit-uation occurs when dealing with convertible anti-monotonic constraints such as minimum average cost constraints. We can show that for a well-chosen order of variables, which re-flects the cost constraint, the CP system will never encounte r a false domain (similar to systems such as FP-Growth ex-tended with convertible constraints [12]).

Summarizing our observations, it turns out that the search strategy employed by many itemset miners parallels that of standard CP systems applied to constraint-based mining problems. Furthermore, the CP approach is able to deal with combinations of constraints in a straight-forward man-ner. A comparison is given in Table 1 3 .
In this section we study the practical benefits and draw-backs of using state-of-the-art CP systems. In the first sec-tion, we consider this issue from a modeling perspective: how involved is it to specify an itemset mining task in CP systems? In the second section, we study the performance perspective, answering the question: how efficient are CP systems compared to existing itemset mining systems?
To illustrate how easy it is to specify itemset mining prob-lems, we develop a model for standard frequent itemset min-ing in the ECLiPSe Constraint Programming System, a logic programming based solver with a declarative modeling lan-guage [2]. This model is given in Algorithm 2. It is almost identical to the formal notation.

This model generates all frequent itemsets, given a 2-dimensional array D , minimum frequency Freq , and pred-icate prodlist(In1,In2,Out) , which returns a list where each element is the multiplication of the corresponding element s in the input lists. Next to procedures for reading data all functionality is available by default in ECLiPSe.
Models for other constraints, and combinations of them, can be created in a similar way. For example, if we only want maximal itemsets, we replace line 10 by I #= (sum(PList) #&gt;= Freq) . If we only want itemsets of size at least 2, then we can add before line 6: sum(Items) #&gt;= 2 .

A similar model can be specified using the Gecode CP library [14]. Compared to the development of specialized algorithms, significantly less effort is needed to model the problem in CP systems
These results are based on the parameters of the most re-cent implementations of the original authors, or, if not pub -licly available, on the original paper of these authors. Algorithm 2 Frequent Itemset Mining in ECLiPSe
In these experiments we only use the Gecode constraint programming system [14]; we found that ECLiPSe was un-able to handle the amounts of constraints that are needed to cope with larger datasets. We implemented models for fre-quent itemset mining, closed itemset mining, maximal item-set mining, and combinations of frequency, cost and size constraints. We compare with the most recent implemen-tations of LCM and Mafia from the FIMI repository [10]; furthermore, we obtained the PATTERNIST system, which implements the ExAnte property [4]. All these systems are among the most efficient systems available. We use data from the UCI repository 4 . Properties of the data are listed in Table 2. To deal with missing values we preprocessed each dataset in the same way as [9]: we first eliminated all attributes having more than 10% of missing values and then removed all examples (transactions) for which the remain-ing attributes still had missing values. Numerical attribu tes were binarized by using unsupervised discretization with 4 bins. Where applicable, we generated costs per items ran-domly using a uniform distribution between 0 and 200. Ex-periments were run on PCs with Intel Core 2 Duo E6600 processors and 4GB of RAM, running Ubuntu Linux. The code of our implementation and the datasets used are avail-able on our website 5 .

The density of a dataset is calculated by dividing the total number of 1s in the binary matrix by the size of this matrix. We encountered scalability problems for the large datasets in the FIMI challenge, and decided to restrict ourselves to smaller, but dense UCI datasets. We restrict ourselves to dense datasets as these are usually more difficult to mine. The numbers of frequent itemsets in Table 2 for a support threshold of 1% are given as an indication, and were com-puted using LCM. All our experiments were timed out after 30 minutes. The experiments indicate that additional con-straints are needed to mine itemsets for low support values on the Segment data. http://archive.ics.uci.edu/ml/ http://www.cs.kuleuven.be/  X  dtai/CP4IM/ Figure 3: Runtimes of itemset miners on standard problems for different values of minimum support In our first experiment, we evaluate how the runtimes of our Gecode-based solver, denoted by FIM CP , compare with those of state-of-the-art itemset miners, for the problems of frequent, maximal and closed itemset mining.

Results for two datasets are given in Figure 3. The exper-iments show that in most of the standard settings, which re-quire the computation of large numbers of itemsets, FIM CP is at the moment not very competitive. On the other hand, the experiments also show that the CP solver propagates all constraints as expected; for instance, maximal itemset min -ing is more efficient than frequent itemset mining. The sys-tem behaves very similar to other (specialized) systems fro m this perspective. In particular, if we compare FIM CP with LCM, we see that FIM CP sometimes performs better than LCM as a maximal frequent itemset miner for low support thresholds; similarly FIM CP sometimes outperforms Mafia as closed itemset miner. This is an indication that further improvements in the implementations of CP solvers could lead to systems that perform satisfactory for most users. In this experiment we determine how FIM CP compares with other systems when additional constraints are employe d.
Results for two settings are given in Figure 4. In the first experiment we employed a (monotonic) minimum size con-straint in addition to a minimum frequency constraint; in the second a (convertible) maximum average cost constraint . The results are positive: even though for small minimum size constraints the brute force mining algorithms, such as LCM, outperform FIM CP, FIM CP does search very effec-tively when this constraint selects a small number of very Figure 4: Runtimes of itemset miners on Segment data under constraints large itemsets (30 items or more); in extreme cases FIM CP finishes within seconds while other algorithms do not fin-ish within our cut-off time of 30 minutes. PATTERNIST was unable to finish some of these experiments due to mem-ory problems. This indicates that FIM CP is a competitive solver when the constraints require the discovery of a small number of very large itemsets. The results for convertible constraint are particularly interesting, as we did not opti -mize the item order in any of our experiments.
 In our final experiments we explore the effects of combining several types of constraints.

Our problem setting is related to the problem of finding itemsets that are predictive for one partition (or class) in the data; one way to find such itemsets is to look for item-sets that have high support in this partition and have low support in the remaining examples. Furthermore, it is desir -able to condense the itemsets found; we investigate the use of  X  -closedness;  X  -closedness has not been studied in combi-nation with other constraints in the literature. Finally, t o reduce the complexity of the individual patterns found, we investigate the use of size constraints on the itemsets; we consider both minimum and maximum size constraints.
In our experiments, we divided the Segment dataset ran-domly in 2 partitions. As default parameters for the con-straints we chose: a minimum support of 0.5%, a maximum support of 0.5%, a minimum size of 14, a maximum size of 16 and  X  = 0 . 20. Results are summarized in Table 3.
The issues that we study in this table are the follow-ing. First, as discussed in Section 2 and illustrated in Fig-ure 1, there are two ways of combining maximum size and  X   X  closedness, referred to as Setting 1 and Setting 2. We are interested in the difference in size in the resulting sets of itemsets. In practice it turns out that Setting 1 yields significantly less itemsets than the second setting. Second , we study the influence of the minimum size constraint. The experiments show that the CP system achieves significantly lower runtimes when this additional constraint is applied, thus pushing the constraints effectively, while finding fewe r patterns. Third, we study the influence of the  X   X  parameter. We mined for several values of  X  , as can be seen in Figure 5, without size constraints. The results show for higher value s of  X  less patterns are returned and the algorithm runs faster.
Please note that the runtimes of the CP system are much lower than those obtained by LCM for the same support threshold; the CP approach is more efficient than running LCM and post-processing its results. Table 3: Applying size constraints on Segment data
We have reformulated itemset mining problems in terms of constraint programming. This has allowed us to imple-ment a novel mining system using standard constraint pro-gramming tools. This approach has several benefits. At a conceptual level, constraint programming offers a more uni-form, extendible and declarative framework for a wide range of itemset mining problems than state-of-the-art data min-ing systems. At an algorithmic level, the general purpose constraint programming methods often emulate well-known itemset mining systems. Finally, from an experimental poin t of view, the results of the constraint programming imple-mentation are encouraging for constraints that select many short itemsets, and are competitive or better for constrain ts that select few long itemsets. This despite the fact that constraint programming systems are general purpose solver s and were not developed with the large number of constraints needed for data mining in mind.

The main advantage of our method is its generality. The framework can be used to explore new constraints and com-binations of constraints much more easily than is currently possible. In our approach, it is no longer necessary to de-velop new algorithms from scratch to deal with new types of constraints.
 There are several open questions for further research. 1) How can constraint-programming algorithms be specialized and optimized for use in data mining? 2) Which other types of constraints can be used for mining with the constraint programming approach? and 3) Can the introduced frame-work be extended for supporting the mining of structured data, such as sequences, trees and graphs?
To summarize, we have contributed a first step towards bridging the gap between data mining and constraint pro-gramming, and have formulated a number of open questions for future research, both on the constraint programming and on the data mining side.
 [1] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and [2] K. R. Apt and M. Wallace. Constraint Logic [3] F. Bonchi and C. Lucchese. On closed constrained [4] F. Bonchi and C. Lucchese. Extending the [5] C. Bucila, J. Gehrke, D. Kifer, and W. M. White. [6] D. Burdick, M. Calimlim, J. Flannick, J. Gehrke, and [7] J. Cheng, Y. Ke, and W. Ng.  X  -tolerance closed [8] G. Dong and J. Li. Efficient mining of emerging [9] E. Frank and I. H. Witten. Using a permutation test [10] B. Goethals and M. J. Zaki. Advances in frequent [11] J. Han, J. Pei, and Y. Yin. Mining frequent patterns [12] J. Pei, J. Han, and L. V. S. Lakshmanan. Mining [13] F. Rossi, P. van Beek, and T. Walsh. Handbook of [14] C. Schulte and P. J. Stuckey. Efficient constraint [15] T. Uno, M. Kiyomi, and H. Arimura. Lcm ver.3: [16] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li.
