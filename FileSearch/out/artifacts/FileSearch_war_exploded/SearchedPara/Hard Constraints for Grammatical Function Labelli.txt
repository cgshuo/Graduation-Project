 Phrase or constituent structure is often regarded as an analysis step guiding semantic interpretation, while grammatical functions (i. e. subject, object, modifier etc.) provide important information rele-vant to determining predicate-argument structure. In languages with restricted word order (e. g. English), core grammatical functions can often be recovered from configurational information in constituent structure analyses. By contrast, sim-ple constituent structures are not sufficient for less configurational languages, which tend to encode grammatical functions by morphological means (Bresnan, 2001). Case features, for instance, can be important indicators of grammatical functions. Unfortunately, many of these languages (including German) exhibit strong syncretism where morpho-logical cues can be highly ambiguous with respect to functional information.

Statistical classifiers have been successfully used to label constituent structure parser output with grammatical function information (Blaheta and Charniak, 2000; Chrupa X a and Van Genabith, 2006). However, as these approaches tend to use only limited and local context information for learning and prediction, they often fail to en-force simple yet important global linguistic con-straints that exist for most languages, e. g. that there will be at most one subject (object) per sen-tence/clause. 1  X  X ard X  linguistic constraints, such as these, tend to affect mostly the  X  X ore grammatical func-tions X , i. e. the argument functions (rather than e. g. adjuncts) of a particular predicate. As these functions constitute the core meaning of a sen-tence (as in: who did what to whom), it is impor-tant to get them right. We present a system that adds grammatical function labels to constituent parser output for German in a postprocessing step. We combine a statistical classifier with an inte-ger linear program (ILP) to model non-violable global linguistic constraints, restricting the solu-tion space of the classifier to those labellings that comply with our set of global constraints. There are, of course, many other ways of including func-tional information into the output of a syntactic parser. Klein and Manning (2003) show that merg-ing some linguistically motivated function labels with specific syntactic categories can improve the performance of a PCFG model on Penn-II En-glish data. 2 Tsarfaty and Sim X  X an (2008) present a statistical model (Relational-Realizational Pars-ing) that alternates between functional and config-urational information for constituency tree pars-ing and Hebrew data. Dependency parsers like the MST parser (McDonald and Pereira, 2006) and Malt parser (Nivre et al., 2007) use function labels as core part of their underlying formalism. In this paper, we focus on phrase structure parsing with function labelling as a post-processing step.
Integer linear programs have already been suc-cessfully used in related fields including semantic role labelling (Punyakanok et al., 2004), relation and entity classification (Roth and Yih, 2004), sen-tence compression (Clarke and Lapata, 2008) and dependency parsing (Martins et al., 2009). Early work on function labelling for German (Brants et al., 1997) reports 94.2% accuracy on gold data (a very early version of the TiGer Treebank (Brants et al., 2002)) using Markov models. Klenner (2007) uses a system similar to  X  but more re-stricted than  X  ours to label syntactic chunks de-rived from the TiGer Treebank. His research fo-cusses on the correct selection of predefined sub-categorisation frames for a verb (see also Klenner (2005)). By contrast, our research does not involve subcategorisation frames as an external resource, instead opting for a less knowledge-intensive ap-proach. Klenner X  X  system was evaluated on gold treebank data and used a small set of 7 dependency labels. We show that an ILP-based approach can be scaled to a large and comprehensive set of 42 labels, achieving 97.99% label accuracy on gold standard trees. Furthermore, we apply the sys-tem to automatically parsed data using a state-of-the-art statistical phrase-structure parser with a la-bel accuracy of 94.10%. In both cases, the ILP-based approach improves the quality of argument function labelling when compared with a non-ILP-approach. Finally, we show that the approach substantially improves the quality and coverage (from 93.6% to 98.4%) of treebank-based Lexical-Functional Grammars for German over previous work in Rehbein and van Genabith (2009).

The paper is structured as follows: Section 2 presents basic data demonstrating the challenges presented by German word order and case syn-cretism for the function labeller. Section 3 de-scribes the labeller including the feature model of the classifier and the integer linear program used to pick the correct labelling. The evaluation part (Section 4) is split into an intrinsic evaluation mea-suring the quality of the labelling directly using the German TiGer Treebank (Brants et al., 2002), and an extrinsic evaluation where we test the im-pact of the constraint-based labelling on treebank-based automatic LFG grammar acquisition. Unlike English, German exhibits a relatively free word order, i. e. in main clauses, the verb occu-pies second position (the last position in subor-dinated clauses) and arguments and adjuncts can be placed (fairly) freely. The grammatical func-tion of a noun phrase is marked morphologically on its constituting parts. Determiners, pronouns, adjectives and nouns carry case markings and in order to be well-formed, all parts of a noun phrase have to agree on their case features. German uses a nominative X  X ccusative system to mark predicate arguments. Subjects are marked with nominative case, direct objects carry accusative case. Further-more, indirect objects are mostly marked with da-tive case and sometimes genitive case. (1) shows a sentence containing the ditransi-tive verb geben (to give) with its three arguments. Here, the subject is unambiguously marked with nominative case (NOM), the indirect object with dative case (DAT) and the direct object with ac-cusative case (ACC). (2) shows possible word or-ders for the arguments in this sentence. 3
Since all permutations of arguments are possi-ble, there is no chance for a statistical classifier to decide on the correct function of a noun phrase by its position alone. Introducing adjuncts to this ex-ample makes matters even worse.
Case information for a given noun phrase can give a classifier some clue about the correct ar-gument function, since functions are strongly re-lated to case values. Unfortunately, the German case system is complex (see Eisenberg (2006) for a thorough description) and exhibits a high degree of case syncretism. (3) shows a sentence where both argument NPs are ambiguous between nom-inative or accusative case. In such cases, addi-tional semantic or contextual information is re-quired for disambiguation. A statistical classifier (with access to local information only) runs a high risk of incorrectly classifying both NPs as sub-jects, or both as direct objects or even as nominal predicates (which are also required to carry nom-inative case). This would leave us with uninter-pretable results. Uninterpretability of this kind can be avoided if we are able to constrain the number of subjects and objects globally to one per clause. 4 Our function labeller was developed and tested on the TiGer Treebank (Brants et al., 2002). The TiGer Treebank is a phrase-structure and gram-matical function annotated treebank with 50,000 newspaper sentences from the Frankfurter Rund-schau (Release 2, July 2006). Its overall anno-tation scheme is quite flat to account for the rel-atively free word order of German and does not allow for unary branching. The annotations use non-projective trees modelling long distance de-pendencies directly by crossing branches. Words are lemmatised and part-of-speech tagged with the Stuttgart-T  X  ubingen Tag Set (STTS) (Schiller et al., 1999) and contain morphological annotations (Re-lease 2). TiGer uses 25 syntactic categories and a set of 42 function labels to annotate the grammat-ical function of a phrase.

The function labeller consists of two main com-ponents, a maximum entropy classifier and an in-teger linear program. This basic architecture was introduced by Punyakanok et al. (2004) for the task of semantic role labelling and since then has been applied to different NLP tasks without signif-icant changes. In our case, its input is a bare tree structure (as obtained by a standard phrase struc-ture parser) and it outputs a tree structure where every node is labelled with the grammatical rela-tion it bears to its mother node. For each possi-ble label and for each node, the classifier assigns a probability that this node is labelled by this la-bel. This results in a complete probability distri-bution over all labels for each node. An integer linear program then tries to find the optimal over-all tree labelling by picking for each node the label with the highest probability without violating any of its constraints. These constraints implement lin-guistic rules like the one-subject-per-sentence rule mentioned above. They can also be used to cap-ture treebank particulars, such as for example that punctuation marks never receive a label. 3.1 The Feature Model Maximum entropy classifiers have been used in a wide range of applications in NLP for a long time (Berger et al., 1996; Ratnaparkhi, 1998). They usually give good results while at the same time allowing for the inclusion of arbitrarily complex features. They also have the advantage that they directly output probability distributions over their set of labels (unlike e. g. SVMs).

The classifier uses the following features:  X  the lemma (if terminal node)  X  the category (the POS for terminal nodes)  X  the number of left/right sisters  X  the category of the two left/right sisters  X  the number of daughters  X  the number of terminals covered  X  the lemma of the left/right corner terminal  X  the category of the left/right corner terminal  X  the category of the mother node  X  the category of the mother X  X  head node  X  the lemma of the mother X  X  head node  X  the category of the grandmother node  X  the category of the grandmother X  X  head node  X  the lemma of the grandmother X  X  head node  X  the case features for noun phrases  X  the category for PP objects  X  the lemma for PP objects (if terminal node)
These features are also computed for the head of the phrase, determined using a set of head-finding rules in the style of Magerman (1995) adapted to TiGer. For lemmatisation, we use Tree-Tagger (Schmid, 1994) and case features of noun phrases are obtained from a full German morpho-logical analyser based on (Schiller, 1994). If a noun phrase consists of a single word (e. g. pro-nouns, but also bare common nouns and proper nouns), all case values output by the analyser are used to reflect the case syncretism. For multi-word noun phrases, the case feature is computed by tak-ing the intersection of all case-bearing words in-side the noun phrase, i. e. determiners, pronouns, adjectives, common nouns and proper nouns. If, for some reason (e.g., due to a bracketing error in phrase structure parsing), the intersection turns out to be empty, all four case values are assigned to the phrase. 5 3.2 Constrained Optimisation In the second step, a binary integer linear pro-gram is used to select those labels that optimise the whole tree labelling. A linear program consists of a linear objective function that is to be maximised (or minimised) and a set of constraints which im-pose conditions on the variables of the objective function (see (Clarke and Lapata, 2008) for a short but readable introduction). Although solving a lin-ear program has polynomial complexity, requiring the variables to be integral or binary makes find-ing a solution exponentially hard in the worst case. Fortunately, there are efficient algorithms which are capable of handling a large number of vari-ables and constraints in practical applications. 6
For the function labeller, we define the set of binary variables V = N  X  L to be the crossprod-uct of the set of nodes N and the set of labels L . Setting a variable x n,l to 1 means that node n is labelled by label l . Every variable is weighted by the probability w n,l = P ( l | f ( n )) which the clas-sifier has assigned to this node-label combination. The objective function that we seek to optimise is defined as the sum over all weighted variables:
Since we want every node to receive exactly one label, we add a constraint that for every node n , exactly one of its variables is set to 1.

Up to now, the whole system is doing exactly the same as an ordinary classifier that always takes the most probable label for each node. We will now add additional global and local linguistic con-straints. 7
The first and most important constraint restricts the number of each argument function (as opposed to modifier functions) to at most one per clause. Let D  X  N  X  N be the direct dominance rela-tion between the nodes of the current tree. For ev-ery node n with category S (sentence) or VP (verb phrase), at most one of its daughters is allowed to be labelled SB (subject). The single-subject-function condition is defined as: cat ( n )  X  X  S, V P } X  X  X  X Identical constraints are added for labels OA, OA2, DA, OG, OP, PD, OC, EP. 8
We add further constraints to capture the follow-ing linguistic restrictions:  X  Of all daughters of a phrase, only one is allowed to be labelled HD (head).
  X  If a noun phrase carries no case feature for nom-inative case, it cannot be labelled SB, PD or EP.  X  If a noun phrase carries no case feature for ac-cusative case, it cannot be labelled OA or OA2.  X  If a noun phrase carries no case feature for da-tive case, it cannot be labelled DA.  X  If a noun phrase carries no case feature for gen-itive case, it cannot be labelled OG or AG 9 .
Unlike Klenner (2007), we do not use prede-fined subcategorization frames, instead letting the statistical model choose arguments.

In TiGer, sentences whose main verbs are formed from auxiliary-participle combinations, are annotated by embedding the participle under an extra VP node and non-subject arguments are sisters to the participle. Therefore we add an ex-tension of the constraint in (6) to the constraint set in order to also include the daughters of an embed-ded VP node in such a case.

Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punctuation does not get a label in TiGer. We set the label for those nodes to  X  X  X  (no label). Other examples are:  X  If a node X  X  category is PTKVZ (separated verb particle), it is labeled SVP (separable verb par-ticle).  X  If a node X  X  category is APPR, APPRART,
APPO or APZR (prepositions), it is labeled AC (adpositional case marker).  X  All daughters of an MTA node (multi-token adjective) are labeled ADC (adjective compo-nent).

These constraints are conditioned on part-of-speech tags and require high POS-tagging accu-racy (when dealing with raw text).

Due to the constraints imposed on the classifi-cation, the function labeller can no longer assign two subjects to the same S node. Faced with two nodes whose most probable label is SB, it has to decide on one of them taking the next best label for the other. This way, it outputs the optimal solution with respect to the set of constraints. Note that this requires the feature model not only to rank the cor-rect label highest but also to provide a reasonable ranking of the other labels as well. We conducted a number of experiments using 1,866 sentences of the TiGer Dependency Bank (Forst et al., 2004) as our test set. The TiGerDB is a part of the TiGer Treebank semi-automatically converted into a dependency representation. We use the manually labelled TiGer trees correspond-ing to the sentences in the TiGerDB for assessing the labelling quality in the intrinsic evaluation, and the dependencies from TiGerDB for assessing the quality and coverage of the automatically acquired LFG resources in the extrinsic evaluation.

In order to test on real parser output, the test set was parsed with the Berkeley Parser (Petrov et al., 2006) trained on 48k sentences of the TiGer corpus (Table 1), excluding the test set. Since the Berkeley Parser assumes projective structures, the training data and test data were made projective by raising non-projective nodes in the tree (K  X  ubler, 2005).
The maximum entropy classifier of the func-tion labeller was trained on 46,473 sentences of the TiGer Treebank (excluding the test set) which yields about 1.2 million nodes as training samples. For training the Maximum Entropy Model, we used the BLMVM algorithm (Benson and More, 2001) with a width factor of 1.0 (Kazama and Tsu-jii, 2005) implemented in an open-source C++ li-brary from Tsujii Laboratory. 10 The integer linear program was solved with the simplex algorithm in combination with a branch-and-bound method us-ing the freely available GLPK. 11 4.1 Intrinsic Evaluation In the intrinsic evaluation, we measured the qual-ity of the labelling itself. We used the node span evaluation method of (Blaheta and Char-niak, 2000) which takes only those nodes into ac-count which have been recognised correctly by the parser, i.e. if there are two nodes in the parse and the reference treebank tree which cover the same word span. Unlike Blaheta and Charniak (2000) however, we do not require the two nodes to carry the same syntactic category label. 12
Table 2 shows the results of the node span eval-uation. The labeller achieves close to 98% label accuracy on gold treebank trees which shows that the feature model captures the differences between the individual labels well. Results on parser output are about 4 percentage points (absolute) lower as parsing errors can distort local context features for the classifier even if the node itself has been parsed correctly. The addition of the ILP constraints im-proves results only slightly since the constraints affect only (a small number of) argument labels while the evaluation considers all 40 labels occur-ring in the test set. Since the constraints restrict the selection of certain labels, a less probable label has to be picked by the labeller if the most probable is not available. If the classifier is ranking labels sensibly, the correct label should emerge. How-ever, with an incorrect ranking, the ILP constraints might also introduce new errors.
As the main target of the constraint set are argu-ment functions, we also tested the quality of argu-ment labels. Table 3 shows the node span evalua-tion in terms of precision, recall and f-score for ar-gument functions only, with clear statistically sig-nificant improvements.
For comparison and to establish a highly com-petitive baseline, we use the best-scoring system in (Chrupa X a and Van Genabith, 2006), trained and tested on exactly the same data sets. This purely statistical labeller achieves accuracy of 96.44% (gold) and 92.81% (parser) for all labels, and f-scores of 89.88% (gold) and 84.98% (parser) for argument labels. Tables 2 and 3 show that our sys-tem (with and even without ILP constraints) com-prehensively outperforms all corresponding base-line scores.

The node span evaluation defines a correct la-belling by taking only those nodes (in parser out-put) into account that have a corresponding node in the reference tree. However, as this restricts at-tention to correctly parsed nodes, the results are somewhat over-optimistic. Table 4 provides the results obtained from an evalb evaluation of the same data sets. 13 The gold standard scores are high confirming our previous findings about the performance of the function labeller. However, the results on parser output are much worse. The evaluation scores are now taking the parsing qual-ity into account (Table 1). The considerable drop in quality between gold trees and parser output clearly shows that a good parse tree is an impor-tant prerequisite for reasonable function labelling. This is in accordance with previous findings by Punyakanok et al. (2008) who emphasise the im-portance of syntactic parsing for the closely re-lated task of semantic role labelling. 4.1.1 Subcategorisation Frames Early on in the paper we mention that, unlike e. g. Klenner (2007), we did not include predefined subcategorisation frames into the constraint set, but rather let the joint statistical and ILP models decide on the correct type of arguments assigned to a verb. The assumption is that if one uses prede-fined subcategorisation frames which fix the num-ber and type of arguments for a verb, one runs the risk of excluding correct labellings due to missing subcat frames, unless a very comprehensive and high quality subcat lexicon resource is available.
In order to test this assumption, we run an addi-tional experiment with about 10,000 verb frames for 4,508 verbs, which were automatically ex-tracted from our training section. Following Klen-ner (2007), for each verb and for each subcat frame for this verb attested at least once in the training data, we introduce a new binary variable f n to the ILP model representing the n-th frame (for the verb) weighted by its frequency.

We add an ILP constraint requiring exactly one of the frames to be set to one (each verb has to have a subcat frame) and replace the ILP constraint in (6) by:
This constraint requires the number of subjects in a phrase to be equal to the number of selected 14 verb frames that require a subject. As each verb is constrained to  X  X elect X  exactly one subcat frame (see additional ILP constraint above), there is at most one subject per phrase, if the frame in ques-tion requires a subject. If the selected frame does not require a subject, then the constraint blocks the assignment of subjects for the entire phrase. The same was done for the other argument functions and as before we included an extension of this con-straint to cover embedded VPs. For unseen verbs (i.e. verbs not attested in the training set) we keep the original constraints as a back-off.
Table 5 shows the results of the test set node span evaluation when using the ILP system en-hanced with subcat frames. Compared to Tables 2 and 3, the results are clearly inferior, and particu-larly so for argument grammatical functions. This seems to confirm our assumption that, given our data, letting the joint statistical and ILP model de-cide argument functions is superior to an approach that involves subcat frames. However, and impor-tantly, our results do not rule out that a more com-prehensive subcat frame resource may in fact re-sult in improvements. 4.2 Extrinsic Evaluation Over the last number of years, treebank-based deep grammar acquisition has emerged as an attractive alternative to hand-crafting resources within the HPSG, CCG and LFG paradigms (Miyao et al., 2003; Clark and Hockenmaier, 2002; Cahill et al., 2004). While most of the ini-tial development work focussed on English, more recently efforts have branched to other languages. Below we concentrate on LFG.
Lexical-Functional Grammar (Bresnan, 2001) is a constraint-based theory of grammar with min-imally two levels of representation: c(onstituent)-structure and f(unctional)-structure. C-structure (CFG trees) captures language specific surface configurations such as word order and the hier-archical grouping of words into phrases, while f-structure represents more abstract (and some-what more language independent) grammatical re-lations (essentially bilexical labelled dependencies with some morphological and semantic informa-tion, approximating to basic predicate-argument structures) in the form of attribute-value struc-tures. F-structures are defined in terms of equa-tions annotated to nodes in c-structure trees (gram-mar rules). Treebank-based LFG acquisition was originally developed for English (Cahill, 2004; Cahill et al., 2008) and is based on an f-structure annotation algorithm that annotates c-structure trees (from a treebank or parser output) with f-structure equations, which are read off of the tree and passed on to a constraint solver producing an f-structure for the given sentence. The English annotation algorithm (for Penn-II treebank-style trees) relies heavily on configurational and catego-rial information, translating this into grammatical functional information (subject, object etc.) rep-resented at f-structure. LFG is  X  X unctional X  in the mathematical sense, in that argument grammatical functions have to be single valued (there cannot be two or more subjects etc. in the same clause). In fact, if two or more values are assigned to a single argument grammatical function in a local tree, the LFG constraint solver will produce a clash (i. e. it will fail to produce an f-structure) and the sen-tence will be considered ungrammatical (in other words, the corresponding c-structure tree will be uninterpretable).

Rehbein (2009) and Rehbein and van Genabith (2009) develop an f-structure annotation algorithm for German based on the TiGer treebank resource. Unlike the English annotation algorithm and be-cause of the language-particular properties of Ger-man (see Section 2), the German annotation al-gorithm cannot rely on c-structure configurational information, but instead heavily uses TiGer func-tion labels in the treebank. Learning function la-bels is therefore crucial to the German LFG an-notation algorithm, in particular when parsing raw text. Because of the strong case syncretism in Ger-man, traditional classification models using local information only run the risk of predicting mul-tiple occurences of the same function (subject, object etc.) at the same level, causing feature clashes in the constraint solver with no f-structure being produced. Rehbein (2009) and Rehbein and van Genabith (2009) identify this as a major problem resulting in a considerable loss in cov-erage of the German annotation algorithm com-pared to English, in particular for parsing raw text, where TiGer function labels have to be supplied by a machine-learning-based method and where the coverage of the LFG annotation algorithm drops to 93.62% with corresponding drops in recall and f-scores for the f-structure evaluations (Table 6).
Below we test whether the coverage problems caused by incorrect multiple assignments of gram-matical functions can be addressed using the com-bination of classifier with ILP constraints devel-oped in this paper. We report experiments where automatically parsed and labelled data are handed over to an LFG f-structure computation algorithm. The f-structures produced are converted into a dependency triple representation (Crouch et al., 2002) and evaluated against TiGerDB.
Table 6 shows the results of the f-structure evaluation against TiGerDB, with 84.07% f-score upper-bound results for the f-structure annotation algorithm on the original TiGer treebank trees with hand-annotated function labels. Using the function labeller without ILP constraints results in drastic drops in coverage (between 4.5% and 6.5% points absolute) and hence recall (6% and 12%) and f-score (3.5% and 9.5%) for both gold trees and parser output (compared to upper bounds). By contrast, with ILP constraints, the loss in cov-erage observed above almost completely disap-pears and recall and f-scores improve by between 4.4% and 5.5% (recall) and 3% (f-score) abso-lute (over without ILP constraints). For compar-ison, we repeated the experiment using the best-scoring method of Rehbein (2009). Rehbein trains the Berkeley Parser to learn an extended category set, merging TiGer function labels with syntactic categories, where the parser outputs fully-labelled trees. The results show that this approach suf-fers from the same drop in coverage as the classi-fier without ILP constraints, with recall about 7% and f-score about 4% (absolute) lower than for the classifier with ILP constraints.

Table 7 shows the dramatic effect of the ILP constraints on the number of sentences in the test set that have multiple argument functions of the same type within the same clause. With ILP con-straints, the problem disappears and therefore, less feature-clashes occur during f-structure computa-tion.
In order to assess whether ILP constraints help with coverage only or whether they affect the qual-ity of the f-structures as well, we repeat the experi-ment in Table 6, however this time evaluating only on those sentences that receive an f-structure, ig-noring the rest. Table 8 shows that the impact of ILP constraints on quality is much less dramatic than on coverage, with only very small variations in precison, recall and f-scores across the board, and small increases over Rehbein (2009).
Early work on automatic LFG acquisition and parsing for German is presented in Cahill et al. (2003) and Cahill (2004), adapting the English Annotation Algorithm to an earlier and smaller version of the TiGer treebank (without morpho-logical information) and training a parser to learn merged Tiger function-category labels, and report-ing 95.75% coverage and an f-score of 74.56% f-structure quality against 2,000 gold treebank trees automatically converted into f-structures. Rehbein (2009) uses the larger Release 2 of the treebank (with morphological information) report-ing 77.79% f-score and coverage of 93.62% (Ta-ble 8) against the dependencies in the TiGerDB test set. The only rule-based approach to German LFG-parsing we are aware of is the hand-crafted German grammar in the ParGram Project (Butt et al., 2002). Forst (2007) reports 83.01% de-pendency f-score evaluated against a set of 1,497 sentences of the TiGerDB. It is very difficult to compare results across the board, as individual pa-pers use (i) different versions of the treebank, (ii) different (sections of) gold-standards to evaluate against (gold TiGer trees in TigerDB, the depen-dency representations provided by TigerDB, auto-matically generated gold-standards etc.) and (iii) different label/grammatical function sets. Further-more, (iv) coverage differs drastically (with the hand-crafted LFG resources achieving about 80% full f-structures) and finally, (v) some of the gram-mars evaluated having been used in the generation of the gold standards, possibly introducing a bias towards these resources: the German hand-crafted LFG was used to produce TiGerDB (Forst et al., 2004). In order to put the results into some per-spective, Table 9 shows an evaluation of our re-sources against a set of automatically generated gold standard f-structures produced by using the f-structure annotation algorithm on the original hand-labelled TiGer gold trees in the section cor-responding to TiGerDB: without ILP constraints we achieve a dependency f-score of 84.35%, with ILP constraints 87.23% and 98.89% coverage. In this paper, we addressed the problem of assign-ing grammatical functions to constituent struc-tures. We have proposed an approach to grammat-ical function labelling that combines the flexibil-ity of a statistical classifier with linguistic expert knowledge in the form of hard constraints imple-mented by an integer linear program. These con-straints restrict the solution space of the classifier by blocking those solutions that cannot be correct. One of the strengths of an integer linear program is the unlimited context it can take into account by optimising over the entire structure, providing an elegant way of supporting classifiers with ex-plicit linguistic knowledge while at the same time keeping feature models small and comprehensi-ble. Most of the constraints are direct formaliza-tions of linguistic generalizations for German. Our approach should generalise to other languages for which linguistic expertise is available.

We evaluated our system on the TiGer corpus and the TiGerDB and gave results on gold stan-dard trees and parser output. We also applied the German f-structure annotation algorithm to the automatically labelled data and evaluated the system by measuring the quality of the resulting f-structures. We found that by using the con-straint set, the function labeller ensures the inter-pretability and thus the usefulness of the syntac-tic structure for a subsequently applied processing step. In our f-structure evaluation, that means, the f-structure computation algorithm is able to pro-duce an f-structure for almost all sentences. The first author would like to thank Gerlof Bouma for a lot of very helpful discussions. We would like to thank our anonymous reviewers for de-tailed and helpful comments. The research was supported by the Science Foundation Ireland SFI (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie) and by DFG (German Research Foundation) through SFB 632 Potsdam-Berlin and SFB 732 Stuttgart.
