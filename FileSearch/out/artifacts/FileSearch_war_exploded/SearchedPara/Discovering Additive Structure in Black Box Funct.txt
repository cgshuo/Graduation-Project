
Many automated learning procedures lack interpretabil-ity, operating effectively as a black box: providing a predic-tion tool but no explanation of the underlying dynamics that drive it. A common approach to interpretation is to plot the dependence of a learned function on one or two predictors. We present a method that seeks not to display the behav-ior of a function, but to evaluate the importance of non-additive interactions within any set of variables. Should the function be close to a sum of low dimensional components, these components can be viewed and even modeled paramet-rically. Alternatively, the work here provides an indication of where intrinsically high-dimensional behavior takes place.
The calculations used in this paper correspond closely with the functional ANOVA decomposition; a well-developed construction in Statistics. In particular, the proposed score of interaction importance measures the loss associated with the projection of the prediction function onto a space of ad-ditive models. The algorithm runs in linear time and we present displays of the output as a graphical model of the function for interpretation purposes.

G.3 [ Probability and Statistics ]: Nonparametric statis-tics; I.5.2 [ Pattern Recognition ]: Design Methodology X  Feature Evaluation and Selection ; I.6.5 [ Simulation and Modeling ]: Model Development X  Modeling Methodologies Algorithms, Measurement, Design, Verification
Visualization, Diagnostics, Functional ANOVA, Additive models, Graphical models, Interpretation, Feature Selection
Many procedures in Data Mining produce prediction func-tions that act effectively as a  X  X lack box X : predicting a cer-tain quantity given particular inputs without providing in-sight into the underlying dynamics that would allow a better understanding of the system. This is particularly the case for neural networks, support vector machines and other ker-nel methods, and also for many ensemble methods such as boosting and bagging. Indeed, trees  X  the most obviously interpretable Machine Learning models  X  have proved to be highly unstable (c.f. [2]) and using them in ensemble meth-ods destroys that interpretability.

This paper initiates a computationally feasible investiga-tion into the structure of such black boxes. In particular, we are interested in measuring the importance of variables in determining the output of the function and in finding underlying additive, or approximately additive, interactions between subsets of variables. Doing this will allow us to decompose a response function into additive parts 1 . If each of these components is low dimensional, then they may be individually viewed, interpreted, and possibly even mod-eled parametrically. Additionally, this allows us to produce graphical models of function interactions to better see the important components in the structure of a potential func-tional ANOVA decomposition.

The diagnostic tools presented here also have a limit-ing function. It is common, when attempting to interpret black box predictions, to produce a matrix of bivariate plots, each describing aggregate behavior on two of the predictors. These are often complemented with plots of conditional vari-ance: how much functional variation is not being accounted for at each value of those two predictors. [13] and [9] ad-vocate this approach, and an example is given in Figure 1. If the function in question is additive up to first order interactions, then such a plot-matrix exactly captures its dynamics: one only needs to sum up the values of the plots, a relatively simple cognitive procedure. Plots of bivariate conditional variances may indicate where something intrin-sically higher-dimensional is going on, but they do not indi-cate what variables to look for as additional interactions, or indeed how many variables might be included in this inter-action. The work that we present here solves this problem.
Most of the approaches mentioned above also involve an evaluation of the function based on a uniform distribution.
It is important to emphasize the distinction here between additivity of functional effects and independence of the un-derlying predictors. That we can write F ( x, y ) = f ( x )+ g ( y ) does not make x and y independent. This introduces the problem of extrapolation. Prediction functions are rarely learned on data that is even close to uniform, so that Monte Carlo evaluation on uniformly sam-pled data will inevitably lead to evaluating the function at points of extrapolation. Indeed, such a uniform sample may contain more points of extrapolation than points close to the learning sample. The approach presented below miti-gates this problem.
Visualization procedures for the dynamics of a function or model often rely on plotting univariate or bivariate effects. A standard definition of these is found in the functional ANOVA decomposition, given for example in [11]. This con-struction has a long history going back to 1948 in [7]. This section will provide an introductory summary.

Let F ( x ) : R k  X  R be square integrable, with For u  X  { 1 , . . . , k } we denote by x u the subset of variables whose indexes are in u . Similarly x  X  u will indicate the vari-ables with indexes not in u . We can write f ( x ) uniquely as with f u depending only on x u and defined by In more concrete terms, F is represented as a constant ( u =  X  ), plus terms in one variable ( u = { i } ), plus terms in two variables and so forth. Each term is calculated as the pro-jection of F onto a particular subset of the predictors, tak-ing out the lower-order effects which have already been ac-counted for.

It can be shown that the f u are orthogonal and that the functional variance  X  2 ( f ) = R f 2 dx may be decomposed as Note that this definition can be generalized trivially to any product measure without compromising orthogonality. The Functional ANOVA decomposition was used in [9] and [13] as the basis for a representation of functional behavior using bivariate plots. Throughout the following, we will assume that F is scaled to give  X  2 ( F ) = 1.
The interest in this work is to understand functions learned on data. Since most of the data sets are far from uniform, or any product measure, creating a Monte Carlo sample from one of these will distort our picture of the dynamics of the function; placing greater emphasis on areas of low proba-bility mass. [5] defines the partial dependence of a function F ( x ) on x l to be where p  X  u ( x  X  u ) is the marginal distribution of x  X  u notes that taking the marginal distribution rather than a Figure 1: A typical matrix of partial dependence plots given for a neural network trained on the Boston Housing Data. The partial dependence on  X  X ge X  (top left), and that on  X  X tratio X  (bottom right). A contour plot of the partial dependence on the pair (top right) and a filled contour plot of the variance of the function given  X  X ge X  and  X  X tratio X  (bottom left). conditional distribution given x u preserves additive struc-ture in F in the sense that the partial dependence recovers the components of an additive function up to a constant. This comes at the expense of distorting the underlying prob-ability space. This distortion, however, is less severe than using a uniform distribution, or than taking the product of marginal densities in all dimensions in the sense that the Kullback-Leibler distance of the underlying distribution with respect to the original data distribution is smaller. [5] provides an efficient algorithm for producing plots of these functions for ensembles of trees. A data-driven ap-proximation can be produced for any black box function F by calculating where { x i } N i =1 is the sample used to learn F .
As an example, we trained a 13-26-1 neural network on the Boston Housing Data [6] using the default settings in the R package, nnet [12]. A typical display might look some-thing like Figure 1: a matrix of plots containing the partial dependence of a prediction function on individual variables on the diagonal. The upper diagonal elements then contain contours of the partial dependence of the function on pairs of variables. Below the diagonal is the variance of the func-tion at each value of a pair of variables. We have presented the two variables  X  X ge X  and  X  X tratio X , but more would typ-ically be given. A constant variance would indicate that the function has no interaction between the pair of variables specified and the other predictors.
Throughout this section we will assume that the predictor variables are drawn from a product distribution in order to retain the interpretational properties of the Functional ANOVA. In  X  5 we will give a data-driven approximation that is similar to partial dependence plots (2).

We say that a function f has ANOVA structure described by a collection U of subsets of { 1 , . . . , k } if f u ( x ) = 0 for all u that are proper supersets of some element of U . Ex-pressing the subsets of { 1 , . . . , k } as a lattice space ordered by the subset operator, this is equivalent to U being a least upper bound on the set of elements u of the lattice with non-zero  X  2 u ( f u ). In concrete terms, a function of the form f 1 ( x 1 ) + f 2 ( x 2 , x 3 ) would be said to have structure {{ 1 } , { 2 , 3 }} , describing the terms that are necessary to re-cover the function.
 The L 2 projection of F onto the set of functions with ANOVA structure described by U is given by: where | U | is the cardinality of U ,  X  i U represents the col-lection of i -way intersections among the elements of U , and E  X  v F ( x ) represents expectation conditioned on x v . The natural measure of goodness of fit for this projection is there-fore: with the expectation taken over the underlying product mea-sure.

Consider a 3 dimensional function, f ( x 1 , x 2 , x 3 ), with un-derlying uniform measure. Let us project this function onto the set of functions with ANOVA structure {{ 1 } , { 2 }} : ad-ditive in x 1 and x 2 and constant in x 3 . The projection is given by: E ( f | x 1 ) + E ( F | x 2 )  X  E ( f ) = the corresponding two first effects plus the mean. In general (3) is equal to the sum of the effects indexed by subsets of the elements of U .

Generally, we are interested in the significance of an in-teraction u , indexed by a subset of { 1 , . . . , k } . Formally, we are asking:  X  v  X  U : u  X  v : Is there a non-zero func-tional ANOVA component v that contains u ? To measure with no interaction in u . The equivalent ANOVA structure set of ANOVA structures not containing u . In this case (3) simplifies to: Here the quantity of interest, E ( F ( x )  X  G u ( x )) 2 in functional ANOVA terms, to the measure In the three dimensional example above,  X   X  2 1 measures the er-ror associated with approximating f ( x 1 , x 2 , x 3 ) with a func-tion of the form g ( x 2 , x 3 ), leaving out x 1 . We could test the interaction { x 2 , x 3 } by projecting onto the set of functions of the form g 1 ( x 1 , x 2 ) + g 2 ( x 1 , x 3 ).
This measure is of interest in the statistical quadrature literature [10] although it differs from the measures of sub-set importance given by [14]. It can be viewed as the L 2 cost of excluding the interaction u from the model and will alternatively be labeled the L 2 Cost of Exclusion or L 2
In order to estimate the quantities (5) empirically we will perform a simple Monte Carlo integration using our training data, in the vein of  X  3. Here, is the empirical partial dependence function of F on v . We use this to construct the empirical projection  X  G U ( x ) from (3) and measure Although we assumed that the data are drawn from a prod-uct distribution in  X  4, this is not strictly necessary. The inequalities below will still hold using partial dependence operators. Particularly, a function which exactly fits a given ANOVA structure will be exactly recoverable with these es-timates. However, we are no longer producing the L 2 pro-jection of the prediction function under a known measure onto a space of functions defined by a given ANOVA struc-ture. In this sense, the exact interpretation of the empirical L CoE is less clear.

Note that there is a variance associated with the estimate of both the G u ( x i ) and the outer sum of (6). The com-putational cost of (6) is O ( N 2 ). However, if for each i we estimate  X  G U ( x i ) using a randomly drawn subsample of size N , we can express where i has mean zero and variance 1 N This gives E 1 N with a variance of O 1 N + 1 NN to provide an O ( N ) estimation scheme without sacrificing variance. At this point, the estimate of  X   X  2 u becomes  X   X   X  where r ( i ) is an integer randomly chosen in { 1 , . . . , N } . Some algebra shows that if  X   X  2 u = 0 (the function does not have an interaction in u ) then the estimate also returns zero. This estimate is very similar to the Monte Carlo estimates employed by [10]; although that paper only examines a uni-form distribution. Figure 2: An example of the difference in the VIN graph for functions of the form f ( x 1 , x 2 , x 3 ) and g ( x 4 , x 5 ) + g 2 ( x 4 , x 6 ) + g 3 ( x 5 , x 6 ) .
In order to make use of the cost measures developed above as a diagnostic tool, it is helpful to have a graphical represen-tation of the ANOVA structure of a function. In a learned-function context, very rarely will the importance score for any particular interaction be identically zero. Therefore, a representation that conveys which interactions are deemed significant, as well as the overall importance of interactions, is needed.

For first-order interactions we can represent the set of sig-nificant interactions as edges in a graph that uses predictor variables as nodes. In this case, the graph has a similar inter-pretation to a Bayesian Network. That is: F is additive in x and x j given the collection of variables x u if any path from x to x j crosses x u . In fact, a Bayesian Network represents exactly this structure for the log density of the variables. An equivalent functional interpretation is that F is additive in x i and x j if for any fixed x u , F ( x i , x j ; x u for some functions f and g . This implies the statement We have labeled such a representation a Variable Interaction Network (VIN).

The inverse of the L 2 CoE measure can be taken as a dis-tance between nodes, so that the graph representation may be incorporated into a multi-dimensional scaling routine. The package XGvis [3] has been used to produce the graphs in Figures 2, 3, and 4, although the nodes have been posi-tioned by hand. We have found that while dynamic views of the scaled graphs  X  having a plot rotate with a three di-mensional representation  X  are informative about interaction strengths, static two-dimensional plots have not provided a good representation of interaction strengths.

A representation limited to edges in a graph with vari-ables as nodes still does not distinguish higher-order inter-actions, except in so far as they must appear as cliques in the graph: a fact that will appear below. We will there-fore represent these higher interactions by a  X  X artwheel X  to distinguish them from a set of additive first order terms. The VIN network we now produce can be interpreted as a hypergraph, with cartwheels representing multi-dimensional edges. We demonstrate this difference in Figure 2. As an example, we consider the function Figure 3: Variable Interaction Network for the func-tion (7) showing non-additive structure.
 The true VIN components for F are which induces the plot in Figure 3. Here edges { 1 , 2 } , { 1 , 3 } and { 2 , 3 } would normally form a clique if only first-order interactions were considered. A similar clique occurs for { x 7 , x 8 , x 9 , x 10 } .

Note that these cartwheels do not alter the graphical in-terpretation if we allow as paths any route through an  X  X n-tersection X .

These plots may also be thought of as a graphical version of the representation for hierarchical log-linear models given in [4]. There, a log-linear model is specified on categorical data with the highest order interaction terms listed as in (8). In that case, the absence of an interaction represents inde-pendence between categorical variables instead of additivity of a response in either categorical or continuous predictors.
While the evaluation of the strength of a particular inter-action can be made in O ( N ) functional evaluations, there are still 2 k interactions to be evaluated. Denoting by | u | the size of an interaction, the complexity of evaluation also scales as 2 | u | . This becomes prohibitive as k and | u | increases. How-ever, for many functions, the strength of interactions drops off quickly with their size, enabling a very aggressive search strategy.

We will make use of the following monotonicity property: That this holds is clear from equation (5). This allows us to observe that a d -way interaction can only be considered significant if all its ( d  X  1)-way interactions are significant Thus we can begin by considering main effects -removing one variable -giving a measure of variable importance. We then proceed to first order interactions whose components are all in the significant list. Second order interactions are only considered if all the first order interactions that they contain are included, and so forth. The algorithm here bears strong resemblance to the Apriori algorithm for association rules [1]. Both of these rely on a monotonicity property to dramatically reduce the search space as we increase the com-plexity of interactions (or item-sets) that we are considering.
Suppose we have a threshold  X  0, and we wish to find u :  X   X  2 u  X  . The following algorithm provides a least upper bound for this set.
We will define significance by whether its L 2 CoE exceeds some threshold . i = 1 U =  X  Loop: K = { u  X  1 , . . . , n : | u | = i,  X  v  X  u, v  X  U , | v | = i  X  1 } For Each u  X  K :
End For i  X  i + 1 End Loop ( K =  X  or i &gt; D ) So long as the function in question does not exhibit very high-dimensional behavior, this algorithm will examine only a very small subset of interactions, which themselves will be of low order. We believe that it is unlikely that we will find functions that are intrinsically high dimensional. [11] explores many apparently high dimensional functions and finds that many are very close to additive. In the event of interactions exceeding some given order (say 6), the algo-rithm can be curtailed as an indication that interpretation will become very difficult.
In  X  7, we produced a search algorithm designed to find u :  X   X  2 u  X  . The natural measure to concern ourselves with is the overall error resulting from a choice of interaction terms. We would like to find a minimal U to give for G U defined in 3: a representation that explains almost all of the function. Here, minimality is taken to be a least upper bound on a lattice. Instead of the set of elements with non-zero score  X  2 u ( f u ) given in  X  4, we are interested in an upper bound U that gives 3 Unfortunately, this problem is ill-defined and many such U may exist.
In order to specify the problem, importance can be given either to having low-order interactions, or a small number of variables. For the former, a breadth-first search can be per-formed, including all low-order interactions until the fitting requirement (9) is met. In this case we successively include the term with highest L 2 CoE among those candidates of lowest order. In doing this we maintain the hierarchical re-quirement that possible candidates must already have all of their subsets included.
 This algorithm is given formally in the pseudo-code below. S = 0 U =  X  Loop: K 1 = { u  X  1 , . . . , n :  X  v  X  u, | v | = i } K 2 = { u  X  K 1 : | u | = min v  X  K 1 | v |}
For Each u  X  K 2 : u  X  U is here taken to indicate  X  v  X  U : u  X  v .
 v  X  argmax u  X  K P F ( u ) U  X  U  X  v S  X  S +  X  2 ( v ) End Loop S &gt; 1  X  We can estimate The effect of this is to place a penalty on the maximum size of an interaction, only including higher-order terms after all the lower-order have been entered. Here we are looking for a graph that minimizes the size of the greatest  X  X artwheel X  in favor of many lower-order edges.

The alternative is a depth-first search: including the term with highest L 2 CoE among those candidates of highest or-der. The pseudo-code is identical to that above, replacing the definition of K 2 with This will do the exact converse to the breadth-first search and include a new predictor variable only after all interac-tions in the current set of predictors have been allowed. The graph from this is expected to have large  X  X artwheels X  but a smaller number of nodes.
Given that the problem (9) is poorly specified, we believe that the algorithm as originally stated provides a reasonable set of interactions. The interactions resulting from it can be interpreted as a greatest lower bound on the sets U that satisfy (9).

Theorem 8.1. The collection V = { u :  X   X  2 u  X  } repre-sents a lattice greatest lower bound on collections U that satisfy (9).

Proof. Suppose that  X   X  2 u &lt; , then the collection U = { v : ( v 6 X  u ) } has Conversely, for  X   X  2 u  X  , any U with u 6 X  U has This is satisfying in providing a collection of subsets that must be included to provide a good fit. Further, it aids interpretation significantly by providing a smaller collection of interactions.

There remains the question of choosing . In  X  9 we have employed a criteria of explaining 99% of the variance of the function on the empirical distribution of the training data. An alternative is to consider an ordered plot of scores for main effects -these typically die off exponentially -and choose a cutoff manually using a natural break in the scores. This cutoff could be employed throughout, or rechosen at every interaction size. It may well be advantageous to be more or less aggressive in choosing how many terms to in-clude when the relative scores are seen.
We examine the ANOVA structure of the 13-16-1 neural network from  X  3. Here we picked = 0 . 7 which corresponds to 1% of the over-all variance of the function, calculated from the predictions given at the original data. At this cut-off, we include all but three variables:  X  X has, X   X  X ndus X  and  X  X n. X  The remaining variables all have interactions with  X  X stat X  apart from  X  X . X  These are very similar findings to those of [13]. There is only one second-order interaction, in  X  X ge X - X  X ax X - X  X  X . This gives us the final VIN plot in Figure 4. An additive model fit using these interactions and cubic splines improved test error by 12% indicating that this structure is already sufficiently flexible to model the data well.
From a diagnostic point of view, Figure 4 indicates that this prediction function can be well represented by a sum of low dimensional terms. This means that, plots of bivariate representations of the function do provide us with a close-to-comprehensive account of functional behavior. However, care should be taken when examining interactions between the  X  X ge X ,  X  X ax X  and  X  X  X  predictors.
 Figure 4: Variable Interaction Network for a Neural Network trained on the Boston Housing Data, with cutoff = 0 . 7 .
The work presented here can be viewed as a lattice search for a greatest lower bound on the set of hierarchical func-tional ANOVA components that can be used to represent a learned predictor function. This allows us to represent the complexity of a function in terms of the size of its non-additive interaction components, providing not only an in-dication of the intrinsic dimensionality of the system, but which predictors interact in a non-additive manner.
This algorithm employs the monotonicity of the L 2 CoE measure to provide an efficient search through this lattice. Additional sampling theory allows us to do this in O ( N ) function evaluations. We have developed a graphical dis-play to make the results more accessible to the user and demonstrated that it has good interpretational properties.
The empirical estimators of which we used are tied di-rectly to the estimation of partial dependence plots. These have been chosen as providing a data-driven method which distorts the distribution of predictor variables less than the uniform distribution normally associated with the functional ANOVA. Results in [8] suggest that it is possible to estimate L
CoE measures without requiring any predictor variables to be independent. Where the underlying distribution is not an issue or is unknown, estimates based on a uniform distribution can be employed. The algorithms presented in this paper are compatible with any measure of interaction importance for which monotonicity holds.
 The author would like to thank Jerome Friedman and Art Owen for helpful discussion. [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] L. Breiman. Bagging predictors. Mach. Learn. , [3] A. Buja, D. F. Swayne, M. L. Littman, N. Dean, and [4] S. E. Feinberg. The Analysis of Cross-Classified [5] J. H. Friedman. Greedy function approximation: A [6] D. Harrison and D. L. Rubinfeld. Hedonic prices and [7] W. Hoeffding. A class of statistics with asymptotically [8] G. Hooker. Black box diagnostics and the problem of [9] T. Jiang and A. B. Owen. Quasi-regression with [10] R. Liu and A. B. Owen. Estimating mean [11] A. B. Owen. The dimension distribution and [12] R-project. http://www.r-project.org/. [13] C. Roosen. Visualization and Exploration of [14] I. M. Sobol. Global sensitivity indices for nonlinear
