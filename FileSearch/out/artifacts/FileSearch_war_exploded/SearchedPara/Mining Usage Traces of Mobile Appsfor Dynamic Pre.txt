 As mobile devices become more and more popular, a tremendous amount of mobile applications (abbreviated as Apps) are designed for varied functions and purposes. Users can download and execute Apps in their mobile devices to sat-isfy their needs and affinities. For App providers, to understand users X  prefer-ences is quite important to recommend new Apps, and to decide their marketing strategies for selling Apps [1 X 3]. Although users can rate the Apps they have experienced, only a small p ercentage of users rate their Apps. For example, the famous App, Angry Birds, only received 4% ratings from 1.3 million of down-loads [4]. Besides, users may not be willing to consistently rate the Apps when they change their preferences. On the other hand, although [5] states an Apps recommendation problem, it does not show the dynamic preferences of users. By contrast, through the dynamic pref erences, we can not only recommend Apps but investigate more tasks on Apps.

In this paper, we aim to predict users X  dynamic preferences of each App and further quantize the preferences to real numbers such that we can compare the preferences among different users. As use rs repeatedly invoke these Apps, their preferences are dynamic over time base d on what they have experienced. Here, we claim that a user X  X  dynamic preference is related to the usage trace (i.e., series of usage counts). For example, Fig. 1 shows three usage traces of Calender, Browser, and Messenger, for a certain us er. As can be seen in Fig. 1, the number of usages on  X  X essenger X  apparently drops down after 14 days (two weeks). Therefore, we can infer that the user dec line his/her preference on  X  X essenger X  in 14 days by either implicit or explicit reasons.

Nevertheless, usage counts of Apps are not directly related to the preferences of Apps. For example, in Fig. 1, although the usage count of Messenger is higher than the other two Apps, the preference of Messenger is not necessarily higher than the other two Apps. Probably, Messenger is a communication tool, which is designed to be used frequently. As for Calendar, users will not frequently check their calendar all the time. In our experimental results, we also show that the usage-based algorithm cannot predict anything, where its accuracy is often close to zero. To correctly predic t preferences of Apps from the usage traces, we propose two methods, Mode-based Prediction (abbreviated as MBP) and Reference-based Prediction (abbreviated as RBP). Both methods utilize different strategies to avoid the impact of the inherent magnitude bias of the the usage counts. MBP adopts only the usage mode of Apps: active mode for using the App while inactive mode for not using the App. RBP refers to the previous usage counts of each App as a reference history and thus, the usage count becomes a relative value of the reference history.

Both MBP and RBP consist of two phases: the trend detection phase and the change estimation phase. The first phase determines whether the preference is decreasing or increasing. The second phase estimates the absolute value on the preference change. For MBP, we increa se the preferences of those Apps which are used at current time unit, but decreases the preferences of others. Then, we propose a utility model: when a user uses more Apps at the same time unit, each App would receive less preference increment. According to the utility model, we can calculate the increment and decrement of each App. For RBP, it calculates an expected number of us age for each App at current time unit by solving an optimization problem wher e the expected number of usage can keep the trend of preference change staying s tatic. If the actual number of usage is larger (smaller, respectively) than the expectation, the preference will increase (decrease, respectively). Then, RBP uses a probabilistic model to estimate the change of preferences.

The contributions of this study are: 1. We explore usage traces of Apps for dynamically predicting the perferences 2. We analyze the characteristics of Apps, and propose two algorithms, MBP 3. In the MBP method, we derive the dynamic preferences according to only 4. In the RBP method, by solving an optimization problem, the expected num-5. We conduct a comprehensive performance evaluation. The experimental re-To the best of our knowledge, this paper is the first work discussing dynamic preferences prediction pr oblem. Although there are many research works dis-cussing the problem of predicting users X  preference, they only focused on a static environment. In a static c ircumstance, such as renting movies and purchasing books, users generally only act on them once and the preference remain static. Therefore, they can use the existing u ser preferences to predict the unknown preferences through the attributes of items [6]. The attributes could be the meta-data, such as artist, genre, etc., or the ratings the item already had. Although [6] focused on predicting the ratings of musics, they still treated the music ratings as static preferences. This is because their focus is on purchasing songs or CDs, not on the preference to listening to a song from a user collection at a particular moment. Only the authors in [7 X 9] recognized the temporal dynamics of users X  preferences. Nevertheless, [7] still need to obtain at least a portion of static rat-ings as training data. [8, 9] only consider the evolution of users X  behavior, instead of quantize their preferences. For pred icting preferences of Apps, users can use Apps repeatedly; therefore, their pref erence changes over time, and even be im-pacted by new Apps [10]. Consequently, the traditional preference prediction methods cannot be adopted for the dynamic preference problem, because 1) the traditional methods all need to obtain at least a portion of static preferences as training data, and 2) the static preferences are out-of-date when we perform the prediction in a dynamic environment. In this section, we first describe the symbols used in this paper. Explicitly, we use r min and r max to represent the minimum and maximum value of users X  pref-which represents the preference of user i on App j . To facilitate the presentation of this paper, U is the set of users and I is the set of Apps. A dynamic prefer-ence matrix is used to repr esent the preferences of A pps at a certain time unit. Here, we divide time space into time units, and use l , an application dependent parameter, to represent the length of a time unit. The formal definition is below: Definition 1. (Dynamic Preference Matrix) A dynamic preference matrix A usage count matrix constructed from users X  traces is defined in Definition 2 Definition 2. (Usage Count Matrix) Ausagecountmatrixattimeunit t , C ( t ) ,isa | U | X | I | matrix, where each element c ( t ) user i used App j at time unit t .
 We use a change matrix to record the preference change of each user-App pair. When the value is positive (negative, resp ectively), the prefer ence is increasing (decreasing, respectively). Definition 3 shows the detail of change matrix. In this paper, the change matrix is derived from both usage count matrix and dynamic preference matrix.
 Definition 3. (Change Matrix) A change matrix at time unit t , denoted as  X  ( t ) ,isa | U | X | I | matrix, where the value of each element  X  ( t ) We claim that the preference of an App would not change dramatically. Even when users do not use an App for a long ti me, the preference of it would decay smoothly over time. Therefore, we derive users X  preferences according to the previous preferences and current usage behavior as described in Definition 4. Definition 4. (Dynamic Preferences Prediction Problem) Let R ( t  X  1) be the dynamic preference matrix at time unit t  X  1 ,and C ( t ) be the usage count matrix at time unit t , the dynamic preference prediction problem is 1) calculating the change matrix,  X  ( t ) , and 2) deriving R ( t ) according to Eq. 1. For example, suppose we have two users and three Apps, and the system pa-rameters are r min =0and r max =5.Let R ( t  X  1) = preference matrix de rived at time unit t  X  1, and C ( t ) = 100 2 30 2 300 40 be the usage count matrix. First, we calculate the change matrix according to C ( t ) and R ( t  X  1) , such that, in this example, the values of the change matrix are related to the usage counts and will be in the defined range to avoid the values in R ( t ) being out of range. Assume that we can obtain  X  ( t ) = new dynamic preference could be derived as R ( t ) = As described in Definition 4, to obtain the dynamic preference matrix, R ( t ) ,we need to know the change matrix,  X  ( t ) , in advance. Here, we use  X  t ij to represent the elements in  X  ( t ) . Empirically, we can calculate  X  t ij by Eq. 2 which consists of two parts: 1) m  X  X  0 , 1 } which indicates whether  X  t ij is positive ( m =0)or negative ( m =1),and2) v ( t ) ij &gt; 0 which is the absolute value of  X  t ij . Through this equation, we can calculate the change matrix,  X  ( t ) , by finding a proper pair In this paper, we design a two-phase framework: the trend detection phase for the value of m and the change estimation phase to calculate v ( t ) ij . In order to smooth the preference change, the value of v ( t ) ij depends on not only the current usage is increasing (respectivel y, decreasing), the value of v ( t ) ij is in the range of [0, r Eq. 3, where u ( t ) ij is a utility parameter determined by user X  X  preference change. be larger.

In order to address the challenge related to the number of usages of Apps, we propose two algorithms based on different points of view. The first one is Mode-based Prediction (MBP) which takes into account of the binary usage mode of active and inactive. The second one is ca lled Reference-based Prediction (RBP) which adopts the previous usage counts as a reference history to examine the  X  4.1 Mode-BasedPrediction(MBP) The Mode-based Prediction (MBP) ignores the magnitude of usage counts by only considering two usage mode: one is active mode for using the App and another is inactive mode for not using the App. Then, a utility model is proposed to measure the usage change of a user, and the  X  ( t ) matrix could be estimated through this model.
 The trend detection phase. In this phase, we decide the value of m in Eq. 2. If user i executed App j at time unit t (i.e. c ( t ) ij &gt; 0), we would set m as 0 (increase the preference). By contrast, if c ( t ) ij =0,weset m to 1 (decrease the preference). The change estimation phase. The second phase is to estimate the absolute value of the preference change. In other words, we need to derive the value of v ( t ) ij according to utility parameter, u ( t ) ij . Since we only have the information of usage mode of each App, we propose a utility model to derive the utility parameter based only on the usage mode. Intuitively, when a user spends more time on some Apps, (s)he should spend less time on others. Thus, we claim that the overall usage change among Apps should be equal to 0. Eq. 4 formulates the utility model, where P (respectively, N ) is the set of Apps with active (respectively, inactive) mode. Suppose that the importance of each App is the same, the utility in Eq. 6. 4.2 Reference-Based Prediction (RBP) Although MBP successfully avoids the magnitude of usage counts by adopting the usage mode and the utility model, ignoring the magnitude of the usage counts makes the estimated preferences not be a ble to reflect users X  actual preferences. For example, in Fig. 2, the preference of Messenger predicted by MBP becomes higher and higher over time, since MBP increases the user  X  X  preference once the user invokes the Apps. However, we believe that the curve representing the preference of Messenger should be like the Ideal one. To obtain the ideal result, we propose a Reference-based Prediction (RBP) algorithm which compares the usage counts within an App instead of with other Apps.

RBP uses the previous usage counts of each App as a reference history, and derives a reference value from the reference history. In this paper, the size of reference history is decided by a tunable parameter, h , which means how many historical data points are included into the reference history. The concept is that only when the actual usage count of an A pp is higher than the reference value, its preference is increasing. Similarly, the preference decreases only when the number of usage is less than the reference value. The trend detection phase. In this phase, we decide whether the value of  X  ( t ) ij is negative or positive. Here, we use the previous usage counts as the reference history and derive an expected number of usage as the reference value from the reference history. We adopt the linear r egression to model the trend of reference history, and thus, the expected number of usage count should make the slope of the regression line be zero. Since the slope of a regression line represents the trend of the data points, the expected number of usage count which makes the regression line stay horizontal means that it makes the preference stay static. Then, if the actual number of usage is la rger (smaller, respectively) than the expected number of usage, the preference is considered as increa sing (decreasing, respectively). We use Fig. 3(a) to illustrate the concept of obtaining the expected number of usage by linear regression model. In Fig. 3(a), the three black points are the reference history (i.e. h = 3) and the reference value is the expected usage at time unit 10 (marked as a star point) which makes the regression line, L , be horizontal. Therefore, the goal of this phase is to find the value of star point by satisfying Eq. 7 which can be simplified into Eq. 8.
Since we only consider the slope of the regression line, we can shift the regres-sion line left such that x-axis of the k -th point of reference history and thus, x h +1 is the shifted x-axis of the star point. As shown in Fig. 3(b), we can shift the regression line to the shifted x-axis positions. Now, we can simplify Eq. 8 into Eq. 10, where the index of time units of c ( k ) ij is also shifted to ( k + t  X  h  X  1) for k =1 , 2 ,...,h +1. Therefore, we can extract c ( t ) ij from Eq. 10, and it is the expected number of usage EXP ( c ( t ) ij ), which could be derived from Eq. 11. For example, the value of the star point in Fig. 3(a) is EXP ( c (10) ij )=[(3+2)(12+11+5)  X  2(1  X  12 + 2  X  11 + 3  X  5)] / 3=42 / 3 = 14.
 The change estimation phase. As we have EXP ( c ( t ) ij ) to be the reference value, we need to formulate the utility parameter, u ( t ) ij , by calculating the dis-is far from EXP ( c ( t ) ij ), it means that the user is considered more likely to change his/her preference. Since we need a distance measure between 0 and 1, directly evaluating how many possible cases are between EXP ( c t ij )and c ( t ) ij . Therefore, when the preference is increasing ( m =0),weuse p ( EXP ( c ( t ) ij )  X  x  X  c t ij )rep-resenting the probability of obtaining a number of usage in-between EXP ( c ( t ) ij ) and c ( t ) ij ,where x is a random variable. On the other hand, when the preference of using an App j by c ( t ) ij times in a given time duration l (a parameter for the length of each time unit) by assuming a Poisson distribution shown in Eq. 12, in Eq. 13 and the absolute amount of preference change, v ( t ) ij ,asinEq.14.We also list algorithm 1 to describe the flow of RBP in detail. In the first itera-preference will stay the same when the actual number of usage equals to the expected number of usage. To evaluate the accuracy of the derived dynamic preferences, we examine the accuracy by testing the performance of u sing those derived preferences to make Algorithm 1. Algorithm of Reference-based Prediction recommendation. We adopt the All-But-One evaluation methods [11] which, for each user, we iteratively skip one App from a user X  X  preference list, and then make recommendation for this user. If the skipped App is recommended, we treat it as a hit. The hit ratio of user u at time unit t is calculated by Eq. 15, where k is the number of recommended Apps, I (  X  ) is an indicator function defined in Eq. 16, App k ( u,t ) is the top-k Apps with highest preference score for user u at time unit t ,and R k ( u,t ) is the list of k recommended Apps for user u at time unit t . The length of one time unit, l ,is1dayforbothApptraces,and7days for the Last.fm dataset. Eventually, the overall accuracy is the average hit ratio of every user at every time unit, which is shown in Eq. 17.
 5.1 Environment The range of users X  preferences is set to [0,5]. The adopted recommendation algorithm is Collaborative Filtering (CF) provided by Apache project, Mahout, with its similarity function as Pearson correlation function.
 Dataset description. We have three real-world datasets: two are App usage traces and one is music listening log from Last.fm [12]. For the two traces of App usage, one is a smaller trace which consists of 30 users and 226 Apps, while the other one has 80 users and 650 Apps. Through the two different scales of datasets, we can ensure whether our methods are scalable or not. For the music listening dataset, we have a relatively huge amount of users in the Last.fm 1K-users dataset. The music listening dataset consists of 1000 users and 48,361 music albums which is a very sparse dataset we have to deal with. The total time duration for two App traces is half a year and for the music listening log is one and half years. Compared methods. To compare the accuracy of our proposed algorithms, we adopt a usage-based method as the baseline. The usage-based method calculates the users X  preferences only by the usage count. The item with largest number of usage will be assigned the preference of r max , while the one with smallest number of usage will be assigned the preference of r min . Besides, the preferences of other items are calculated by an interpolation method shown in Eq. 18. 5.2 Performance Evaluation In this study, we evaluate the accuracy under various number of recommended Apps, k , and different length of a time unit, l over two proposed algorithms and one baseline method. Then, we focus on th e proposed Reference-based Prediction (RBP) algorithm to see the accuracy when changing the parameter h which is used to control how many historical data are used.
 Accuracy changed by k. Since k would affect the hit ratio, we calculate the hit ratio by different k from 5 to 25. However, although larger k could derive a better performance on hit ratio, fewer recommended items is more meaningful for users. Figs. 4(a) and 4(b) show the results of two App traces under different numbers of recommended items, k . Obviously, the accu racy increases as k grows up. Specifically, when k = 5, both RBP and MBP can achieve the accuracy of more than 80%. we note that in Fig. 4(b), the baseline remains close to zero even for k = 25, while in Fig. 4(a), the baseline achieves relatively low accuracy compared with RBP for k = 5. This is because the App-large dataset consists of more Apps and makes the dataset become sparser than App-small. Fig 4(c) depicts the results of Last.fm dataset. Since the music listening dataset is much sparser than the App traces, the performance on accuracy is not as good as the accuracy of the App traces. However, R BP is always the best method, while the baseline is close to zero. Here, for the two App traces, the length of one time unit is one day and the size of reference history, h , of RBP is set to 4 time units; for music listening dataset, the length of time unit is 1 week and the parameter h of RBP is 6 time units.
 Impact of parameter l . Here, we evaluate the accuracy change of various length of a time unit. As can be seen in Figs. 5(a) and 5(b), both RBP and MBP slightly decrease their accuracy when the amount of training data increases. The best length of a time unit is one day which matches the human behavior. By contrast, the baseline method incre ases the accuracy when the number of training data becomes larger. Because the baseline method does not consider the temporal information, more training data could provide more information to overcome this drawba ck. However, when d&gt; 6, the accuracy of baseline method also declines. On the other hand, as shown in Fig. 5(c), the best length of a time unit for Last.fm dataset is 7 days (one week) since music listening behavior is sparse and users may repeat the song s they listened in one week. Here, the reference history parameter, h , is set to 6 time units.
 Impact of parameter h for RBP. Since the amount of reference history is a critical parameter for RBP algorithm, we evaluate the accuracy of recommenda-tions under various reference histories. Figs. 6(a) and 6(b) depict the results of the App-small and App-large traces, and they reach the best accuracy on h =4 and h = 5 respectively. Furthermore, the results of h  X  2 are much better than the result of h = 1, because when h = 1, the number of reference points is too few to reflect the trend of users X  usage. In addition, Fig. 6(c) shows the results of the Last.fm dataset, and the best accuracy falls on h =6and l = 7. Because the Last.fm is a sparse dataset, RBP algorithm needs more reference points and training data to achieve a better performance. Empirically, the setting of h does highly depend on different applications. In this paper, we suggest choosing a proper h larger than 4, since the regression line constructed in the first phase of RBP is more meaningful to reflect the trend of users X  usage. We proposed a novel dynamic preference prediction problem which is to dy-namically quantize a user X  X  preferences on Apps they have used from their usage traces. Two effective algorithms are desi gned to solve this problem. One is named Mode-base Prediction (MBP) which adopts a user X  X  binary usage mode (active and inactive) and a proposed utility model to predict the preference value on an App. The other one is named Reference-ba se Prediction (RBP) which discovers a reference value by solving an optimiza tion problem in a linear regression model and constructs a probabilistic model to check if the current behavior satisfies the reference model. RBP estimates the user s X  preferences by measuring the differ-ence between actual usage and the derive d reference value. In the experiments section, we evaluate the derived dynamic preferences by applying Collaborative Filtering. When the deriv ed preferences can provide more accurate recommen-dation, the preferences are considered closer to users X  actual affinities. As the experimental results show, the derived preferences of both MBP and RBP are effective. In addition, the RBP method can reach the accuracy of more than 80% for App traces. We suggest that the propo sed dynamic preferences are valuable for many applications, such as providing recommendation of mobile applications, predicting and analysing users behavior, and make marketing decision.
 Acknowledgement. Wen-Chih Peng was supported in part by the National Science Council, Project No.100-2218-E-009-016-MY3 and 100-2218-E-009-013-MY3, by Taiwan MoE ATU Program, by HTC and by Delta.
 difficult to identify the influential aspects of papers quickly and effectively. the main content of the paper, but it only presents what the authors think to be the important contribution but not necessarily the actual impact of the paper. Actually, the impact of a paper should be judged by the consent of research community instead of the author himself. Moreover, the impact of a paper may dynamically change due to the progress of research. For example, a paper published before may be no longer the state of the art, but the research problem it addressed or the method it proposed will still attract peer attention. 
Therefore, we argue that only the abstract part representing the author X  X  point of view is not enough, and how other papers cite and describe the target paper needs to be comprehensively investigated to generate an impact summary, which can not only mining applications such as research trend prediction, and survey generation, etc. 
Actually, given a scientific paper, different citation sentences often focus on different to summarize its impact [1]. Although some research has been done based on citation sentences, to the best of our knowledge, simultaneous consideration of the impact from hybrid citation context associated with each citation sentence has not been investigated. Therefore, we propose a novel approach by incorporating the impact of hybrid citation relationships among papers and authors are first leveraged to jointly infer the impact of a sentence language smoothing model to measure citation sentence relationships more significance of each citation sentence by taking advantage of the relationships between citation sentences. The remainder of this paper is organized as follows. Section 2 reviews related work. results in Section 4. Finally, we present our conclusion and future work in Section 5. Automatic creation of scientific summaries has been studied for many years [2-4], but most previous work considers only the local features of the scientific paper, while other contextual information has been mostly ignored. 
Recently, researchers have begun to make use of contextual information to aid news and webpage summarization [5-10]. Likewise, in order to summarize a paper, differentiating and utilizing citations from context have received increasing interests. Nakov et al. used sentences surrounding citations to create train ing and testing data for scientific paper s ummarization [11]. Nan ba and Okumura classified different citation sentences into three categories and explored how to use them to aid survey generation [12]. Schwartz and Hearst utilized the citation sentences to summarize the status analysis to reveal the scientific attribution of a paper, in which each citation Contrast [14]. Kan et al. used annotated bi bliographies to cover certain summarization aspects [15]. Elkiss et al. performed a large-scale study on the PubMed repository and confirmed the importance of citation sentences in understanding what a p aper contributes [16]. They also concluded that the citation sentences contain more focused information that generally does not appear in the abstract part of a paper. 
Recently, Mei and Zhai proposed a language model based approach t o impact summarization [17]. Qazvinian and Radev presented two different methods for the task produce an impact summary. Another method first extracted a number of key phrases from the citation sentences, and then used these phrases to build the impact summary. How to produce more readable summaries based on citation sentences have also been investigated in [20] 
As far as we know, none of the previous studies has investigated the impact from author context), and has used the citation context in the same way as we did. In this study, we propose a context-aware approach to simultaneously consider the impact citation context and its impact into a sentence language smoothing model to measure the citation sentence relationships beyond sentence level. Our approach incorporates hybrid citation context into the impact summarization, estimation of citation sentence language model, and impact summary generation. 3.1 Inferring the Impact of Hybrid Citation Context authors with better authority expertise will contribute more than those written by less expertise of citing authors) should be inferred first. Our approach operates over a bi bliographic network G. G=(V, E)=(V P  X  V A , E E ) is an undirected graph representing the co-authorship relationships between G A and represents authorship associations between papers and authors. and keywords from the target paper, an d then the initial scores () 0 likelihood estimation of the term t in the paper p i , and n(t, q) is the number of times authored by a i , the initial score () 0
Inspired by [22] and based on the assumption that similar papers will have similar making use of the paper citation graph G P and the initial topical relevance of papers. paper p i cites another paper p j (i  X  j), then we set the corresponding element W matrix with (i,i)-element equal to the sum of the i-th row of W P . 
Next, inspired by [21], a regularization framework is developed by regularizing the smoothness of relevance over the graph and the cost function associated with it is defined as follows: Where the first tem defines the global consistency of the refined relevance over the minimizing 
Similarly, based on the assumption that if two authors co-authored many papers similar, we can refine the authority expertise of citing authors by making use of the author co-authorship graph G A and the initial authority expertise of authors. element matrix with (i,i)-element equal to the sum of the i-th row of W A . 
Next, a regularization framework is developed by regularizing the smoothness of expertise over the graph and the cost functio n associated with it is defined as follows [21]. By minimizing In addition to graph G PA by considering the authorship relations between papers and authors. The intuition behind with that of the relevant papers he published. To define the cost function G corresponding element normalized as S PA such as the sum of each row of the matrix equal to one. Next, a hybrid cost function  X  that combines unified regularization framework. We can minimize the hybrid cost function  X  using the standard conjugate gradient method, and a closed-form optimal solution can be deri ved. However, for a large-scale dataset, an iterative X  X orm computation strategy would be more effective. So in iterative computation strategy, which details are omitted due to space limit, and you can find it in [21]. 
Finally, the converged solutions R P * and R A * correspond to the topical relevance of citing papers and the authority expertise of citing authors respectively. 3.2 Estimation of Citation Sentence Language Model After inferring the impact of hybrid citation context, the next step is to make use of the contextual information to evaluate the relationships between citation sentences. 
From the language model perspective, it can be assumed that a citation sentence s is generated from a sentence language model often adopted to estimate Where |s| is the length of s, c(w, s) is the count of term w in s, p(w|B) is usually estimated by sentence smoothing parameter which is set as 1000 as in [17]. 
In this study, we propose a citation sentence language smoothing model inspired by [6] to estimate (|) be defined as follows. Where  X  ,  X  , and  X  belong to [0, 1] and  X  +  X  +  X  =1. citation sentence s belongs to and () a is the i-th citing author of the citing paper s p . expertise of author
Based on the estimated citation sentence language model, the distance (, ) average KL divergence as follows. the following formula. 3.3 Impact Summary Generation sentences with highest significant scores will be selected into the impact summary. 
In most of the methods for impact summarization, all citation sentences are treated uniformly. However, different citation sentences from different citation contexts context should receive higher significant scor e. Therefore, it is more reasonable to assign unequal weights to different citation sentences in accordance with the impact of different citation contexts which they belong to. undirected graph to reflect the relationships between citation sentences in S. Here V S is the set of citation sentences. E S is the set of edges and each edge formula 10. Two sentences are connected if their similarity is larger than 0 and we let Sim(s i , s i )=0 to avoid self transition. We use the affinity matrix Then Based on  X  deduced from those sentences linked with it, which can be formulated in a recursive form as follows: Where  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. For Usually the convergence of the iteration algorithm is achieved when the difference between the scores computed at two successive iterations for any citation sentences fall below a given threshold (0.0001 in this study). 
After evaluating the significance of each citation sentence, we select a f ew representative sentences with highest significant scores to g enerate the impact summary. Recall that in the proposed approach, we incorporate diverse relationships on G P , G
A , and G PA into a unified regularization framework to infer the impact of hybrid citation context, and then rank citation sentences on G S by leveraging both the impact of hybrid citation context and the relationships between citation sentences, which can be intuitively represented by Figure 1. 4.1 Data Collection We evaluate the proposed approach on the dataset 1 , which contains 25 highly cited papers from computational linguistics domain. Each paper has a set of manually selected terms representing the most importa nt impacts of that paper and shared by multiple evaluators who has read all the citation sentences of that paper. 
Considering that hybrid citation context may improve the performance of impact summarization, we extend the dataset by adding a number of papers with similar topic and related authors from the ACL Anthology Network 2 , which is a large collection of more than 18,000 p apers from computational linguistics domain. Table 1 s hows general statistics about the extended dataset. 
We deem that a good impact summary should cover more important impacts of the target paper. If an impact fact occurs in more citation sentences, it should be regarded as more important and be assigned higher weight. Under the condition, the citation sentence including more impact facts with higher weight will become a g ood candidate for impact summary. Accordingly, we construct a reference summary for each of the 25 highly cited papers by making use of the manually selected impact terms. We pick citation sentences that cover new and highly weighted impact terms into the reference summary until the defined summary length is reached. By this way, we expect a good system generated summary to be closer to the reference summary. 4.2 Evaluation Metrics In the study, the ROUGE toolkit [24] is adopted, which was officially adopted by DUC for automatic summarization evaluation. ROUGE metrics measure a summary X  X  content quality by counting overlapping units such as n-gram, word sequences, and word pairs between the automatically generated summary and the reference summary. The higher the ROUGE scores, the similar the two summaries are. A few recall-oriented ROUGE metrics have been employed including ROUGE-1, ROUGE-2, and ROUGE-SU4, etc. Among the different ROUGE scores, ROUGE-1 has been shown to agree with human judgment most [24]. Therefore, we only report ROUGE-1 in the following experiments since other metrics gives very similar results. 4.3 Experimental Results We compared our proposed approach with several baselines as follows. All the approaches for comparison are required to extract a few representative citation sentences between our approach and other baselines is that we leverage the hybrid citation context associated with each citation sentence while other baselines do not. citation sentences and added to the impact summary. 
OTS [25] : It integrates shallow NLP techniques with statistical word frequency analysis to rank and select citation sentences. sentences based on eigenvector centrality. 
C-LexRank [18] : This is another state-of-the-art impact summarizer in which the citation sentences are firstly clustered, and then the sentences within each cluster are ranked via LexRank algorithm. We show the evaluation results of different methods in Tables 2, and the highest ROUGE-1 scores are shown in bold type. 
In the experiments, the best result of our approach is achieved when the weight adjusting parameters in the formula 6 are set as f ollows:  X  =0.4,  X  =0.3, and  X  =0.3. These parameters give different weights to the citation sentence, the citation paper context, and the citation author context respectively.

Seen from Table 2, ou r proposed approach using the hybrid citation context achieves the best performance compared to that of the baseline approaches (i.e. C-LexRank, LexRank, OTS, and Random), which demonstrates that both citation paper impact summarization. 
C-LexRank and LexRank perform better than those of OTS and Random. This is mainly because both C-LexRank and LexRank make use of the inter-relationships local features. 
C-LexRank outperforms LexRank in our experiments, which indicates the use of sentences alone. 
Note that all these baselines generate the impact summary based only on the citation Our proposed approach shows significantly better performance on ROUGE scores, and the result difference between our approach and other baselines is significant at the 95% statistical confidence level. These observations again demonstrate the effectiveness of our approach by exploiting hybrid citation context to aid impact summarization. In the following, we will explore the effect of different parameters in our approach. The key parameters we want to investigated are  X  ,  X  , and  X  . 
Figure 2 t o 4 dem onstrate the influence of these parameters in the proposed approach when we tune a parameter from 0 to 1 with the step length 0.1 and vary the other two for the best performance to achieve. 
From Figure 2 to 4, it can be found that the citation sentence information performance. Both citation paper context and citation author context can help improve the performance, but excessive dependence on any one of them will impair the performance to a certain extent. This paper pr oposes a con text-aware approach to impact summarization. In the integrated in a sentence language smoothing model to m easure citation sentence relationships more effectively. 
In future work, it would be interesting to investigate the performance of the proposed approach on larger bibliographic datasets such as DBLP, ArnetMiner, etc. Besides, we will explore machine learning based methods to determine the parameters of our approach in an adaptive way. Acknowledgments. This work was supported by the National Natural Science Foundation of China (No. 61133012, 61173062, 61070082) and the Major Projects of Chinese National Social Science Foundation (No.11&amp;ZD189). 
