 Identifying interpretable discriminative high-order feature interactions given limited training data in high dimensions is challenging in both machine learning and data mining. In this paper, we propose a factorization based sparse learn-ing framework termed FHIM for identifying high-order fea-ture interactions in linear and logistic regression models, and study several optimization methods for solving them. Unlike previous sparse learning methods, our model FHIM recovers both the main effects and the interaction terms ac-curately without imposing tree-structured hierarchical con-straints. Furthermore, we show that FHIM has oracle prop-erties when extended to generalized linear regression models with pairwise interactions. Experiments on simulated data show that FHIM outperforms the state-of-the-art sparse lear-ning techniques. Further experiments on our experimen-tally generated data from patient blood samples using a novel SOMAmer (Slow Off-rate Modified Aptamer) technol-ogy show that, FHIM performs blood-based cancer diagno-sis and bio-marker discovery for Renal Cell Carcinoma much better than other competing methods, and it identifies inter-pretable block-wise high-order gene interactions predictive of cancer stages of samples. A literature survey shows that the interactions identified by FHIM play important roles in cancer development.
 J.3 [ Computer Applications ]: Life and Medical Sciences;
Co-first author
Co-first author, corresponding author To whom data request should be sent I.2.6 [ Computing Methodologies ]: Artificial Intelligence X  sparse learning, feature selection Sparse Learning; High-order interactions; Biomaker Discov-ery; Blood-based Cancer Diagnosis
Identifying interpretable high-order feature interactions is an important problem in machine learning, data mining, and biomedical informatics, because feature interactions often help reveal some hidden domain knowledge and the struc-tures of problems under consideration. For example, genes and proteins seldom perform their functions independently, so many human diseases are often manifested as the dysfunc-tion of some pathways or functional gene modules, and the disrupted patterns due to diseases are often more obvious at a pathway or module level. Identifying these disrupted gene interactions for different diseases such as cancer will help us understand the underlying mechanisms of the diseases and develop effective drugs to cure them. However, identi-fying reliable discriminative high-order gene/protein or SNP interactions for accurate disease diagnosis such as early can-cer diagnosis directly based on patient blood samples is still a challenging problem, because we often have very limited patient samples but a huge number of complex feature in-teractions to consider.

In this paper, we propose a sparse learning framework based on weight matrix factorizations and ` 1 regularizations for identifying discriminative high-order feature interactions in linear and logistic regression models, and we study sev-eral optimization methods for solving them. Experimen-tal results on synthetic and real-world datasets show that our method outperforms the state-of-the-art sparse learning techniques, and it provides  X  X nterpretable X  blockwise high-order interactions for disease status prediction. Our pro-posed sparse learning framework is general, and can be used to identify any discriminative complex system input inter-actions that are predictive of system outputs given limited high-dimensional training data.

Our contributions are as follows: (1) We propose a method capable of simultaneously identifying both informative single discriminative features and discriminative block-wise high-order interactions in a sparse learning framework, which can be easily extended to handle arbitrarily high-order feature interactions; (2) Our method works on high-dimensional in-put feature spaces and ill-posed problems with much more features than data points, which is typical for biomedical applications such as biomarker discovery and cancer diag-nosis; (3) Our method has interesting theoretical properties for generalized linear regression models; (4) The interactions identified by our method lead to biomedical insight into un-derstanding blood-based cancer diagnosis.
Variable selection has been a well studied topic in statis-tics, machine learning, and data mining literature. Gener-ally, variable selection approaches focus on identifying dis-criminative features using regularization techniques. Most recent methods focus on identifying discriminative features or groups of discriminative features based on Lasso penalty [18], Group Lasso [21], Trace-norm [6], Dirty model [8] and Support Vector Machines (SVMs) [16]. A recent approach [20] heuristically adds some possible high-order interactions into the input feature set in a greedy way based on lasso pe-nalized logistic regression. Some recent approaches [2],[3] en-force strong and/or weak heredity constraints to recover the pairwise interactions in linear regression models. In strong heredity, an interaction term can be included in the model only if the corresponding main terms are also included in the model, while in weak heredity, an interaction term is included when either of the main terms are included in the model. However, recent studies in bioinformatics has shown that feature interactions need not follow heredity constraints for manifestation of the diseases, and thus the above ap-proaches [2],[3] have limited chance of recovering relevant interactions. Kernel methods such as Gaussian Process [4] and Multiple Kernel Learning [10] can be used to model high-order feature interactions, but they can only tell which orders are important. Thus, all these previous approaches either failed to identify specific high-order interactions for prediction or identified sporadic pairwise interactions in a greedy way, which is very unlikely to recover the  X  X nter-pretable X  blockwise high-order interactions among features in different sub-components (for example: pathways or gene functional modules) of systems. Recently, [14] proposed an efficient way to identify combinatorial interactions among interactive genes in complex diseases by using overlapping group lasso and screening. However, they use prior infor-mation such as gene ontology in their approach, which is generally not available or difficult to collect for some ma-chine learning problems. Thus, there is a need to develop new efficient techniques to automatically capture the impor-tant  X  X lockwise X  high-order feature interactions in regression models, which is the focus of this paper.

The remainder of the paper is organized as follows: in sec-tion 3 we discuss our problem formulation and relevant no-tations used in the paper. In section 4, we discuss the main idea of our approach, and in section 5 we give a overview of theoretical properties associated with our method. In sec-tion 6, we present the optimization methods which we use to solve our optimization problem. In section 7, we discuss our experimental setup and present our results on synthetic and real datasets. Finally, in section 8 we conclude the paper with discussions and future research directions.
Consider a regression setup with a training set of n sam-ples and p features, { ( X ( i ) ,y ( i ) ) } , where X ( i ) instance (column) of the design matrix X ( p  X  n ), y ( i ) is the i th instance of response variable y ( n  X  1), and i = 1 ,...,n . To model the response in terms of the predictors, we can set up a linear regression model or a logistic regression model where  X   X  R p is the weight vector associated with single features (also called main effects),  X  R n is a noise vector, and  X  0  X  R is the bias term. In many practical fields such as bioinformatics and medical informatics, the main terms (the terms only involving single features) are not enough to capture complex relationship between the response and the predictors, and thus high-order interactions are necessary. In this paper, we consider regression models with both main effects and high-order interaction terms. Equation 3 shows a linear regression model with pairwise interaction terms. where W ( p  X  p ) is the weight matrix associated with the pairwise feature interactions. The corresponding loss func-tion (the sum of squared errors) is as follows (we center the data to avoid an additional bias term),
L We can similarly write the logistic regression model with pairwise interactions as follows, and the corresponding loss function (the sum of the negative log-likelihood of the training data) is,
In this section, we propose an optimization-driven sparse learning framework to identify discriminative single features and groups of high-order interactions among input features for output prediction in the setting of limited training data. When the number of input features is huge (e.g. biomed-ical applications), it is practically impossible to explicitly consider quadratic or even higher-order interactions among all the input features based on simple lasso penalized linear regression or logistic regression. To solve this problem, we propose to factorize the weight matrix W associated with high-order interactions between input features to be a sum of K rank-one matrices for pairwise interactions or a sum of low-rank high-order tensors for higher-order interactions. Each rank-one matrix for pairwise feature interactions is rep-resented by an outer product of two identical vectors, and each m -order ( m &gt; 2) tensor is represented by the outer product of m identical vectors. Besides minimizing the loss function of linear regression or logistic regression, we penal-ize the ` 1 norm of both the weights associated with single input features and the weights associated with high-order feature interactions. Mathematically, we solve the optimiza-tion problem to identify the discriminative single and pair-wise interaction features as follows, where W = P K k =1 a k a k , represents the tensor prod-uct/outer product, and  X   X  ,  X  a k represent the estimated pa-rameters of our model and let Q represent objective function of (7) . For logistic regression, we replace L sqerr (  X  , W ) in (7) by L logistic (  X  , W , X  0 ). We call our model Factorization-based High-order Interaction Model (FHIM).

Proposition 4.1. The optimization problem in Equation 7 is convex in  X  and non-convex in a k .
 Because of the non-convexity property of our optimization problem, it is difficult to propose optimization algorithms which guarantee convergence to global optima. Here, we adopt a greedy alternating optimization methods to find the local optima for our problem. In the case of pairwise inter-actions, fixing other weights, we solve each rank-one weight matrix each time. Please note that our symmetric posi-tive definite factorization of W makes this sub-optimization problem very easy. Moreover, for a particular rank-one weight matrix a k a k , the nonzero entries of the correspond-ing vector a k can be interpreted as the block-wise interaction feature indices of a densely interacting feature group. In the case of higher-order interactions, the optimization procedure is similar to the one for the pairwise interactions except that we have more rounds of alternating optimization. The pa-rameter K of W is generally unknown in real datasets, thus, we greedily estimate K during the alternating optimization algorithm. In fact, the combination of our factorization for-mulation and the greedy algorithm is effective for estimating the interaction weight matrix W .  X  is re-estimated when K is greedily added during the alternating optimization as shown in algorithm 1.
 Algorithm 1 Greedy Alternating Optimization 1: Initialize  X  to 0 , K = 1 and a K = 1 2: While (K==1) || ( a K  X  1 6 = 0 for K &gt; 1) 3: Repeat until convergence 6: End Repeat 7: K = K + 1; a K = 1 8: End While 9: Remove a K and a K  X  1 from a .
In this section, we study the asymptotic behavior of FHIM for the likelihood-based generalized linear regression models. The lemmas and theorems proved here are similar to the ones shown in the paper [3]. However, in their paper the au-thors make an assumption on the strong heredity (i.e. inter-action term coefficients are dependent on the main effects), which is not assumed in our model since we are interested in identifying all high-order interactions irrespective of hered-ity constraints. Here, we discuss the asymptotic properties w.r.t to the main effects and factorized co-efficients.
Problem Setup: Assume that the data V i = ( X i ,y i ) ,i = 1 ,...n are collected independently and Y i has a density of f ( g ( X i ) ,y i ) conditioned on X i , where g is a known regres-sion function with main effects and all possible pairwise interactions. Let  X   X  j and a  X  k,j denote the underlying true parameters satisfying block-wise properties implied by our factorization. Let Q n (  X  ) denote the objective with negative log-likelihood and  X   X  = (  X   X  T ,  X   X  T ) T , where  X   X  = ( a 1 ,...,K . We consider the estimates for FHIM as  X   X  n :  X   X  n = arg min  X  Q n (  X  ) (8) where L ( g ( X i ) ,y i ) is the loss function of generalized linear regression models with pairwise interactions. In the case of linear regression, g (  X  ) takes the form of Equation (3) without the noise term and L (  X  ) takes the form of Equation (4). Now, let us define where A 1 contains the indices of the main terms which corre-spond to the nonzero true coefficients, and similarly A 2 tains the indices of the factorized interaction terms whose true co-efficients are non-zero. Let us define Now, we show that our model possesses the oracle properties for (i) n  X   X  with fixed p and (ii) p n  X   X  as n  X   X  under some regularity conditions. Please refer to Appendix for proofs of the lemmas &amp; theorems of sections 5.1 and 5.2.
The asymptotic properties when sample size increases and the number of predictors is fixed are described in the follow-ing lemmas and theorems. FHIM possesses oracle properties [3] under certain regularity conditions (C1)-(C3) shown be-low. Let  X  denote the parameter space for  X  . (C1) The observations V i : i = 1 ,...,n are independent and identically distributed with a probability density f ( V ,  X  ), which has a common support. We assume the density f sat-isfies the following equations: and (C2) The Fisher Information Matrix is finite and positive definite at  X  =  X   X  . (C3) There exists an open set  X  of  X  that contains the true parameter point  X   X  such that for almost all V the den- X  X  k  X  X  l ) for all  X   X   X  and any j,k,l = 1 ,...,p ( K + 1). Fur-thermore, there exist functions M jkl such that where m jkl = E  X   X  [ M jkl ( V )] &lt;  X  . These regularity condi-tions are the existence of common support and first, second derivatives for f ( V ,  X  ); Fisher Information matrix being fi-nite and positive definite; and existence of bounded third derivative for f ( V ,  X  ). These regularity conditions guaran-tee asymptotic normality of the ordinary maximum likeli-hood estimates [11].

Lemma 5.1. Assume a n = o (1) as n  X   X  . Then under regularity conditions (C1)-(C3), there exists a local mini-mizer  X   X  n of Q n (  X  ) such that ||  X   X  n  X   X   X  || = O P
Theorem 5.2. Assume  X   X  n given in lemma 5.1 satisfies ||  X   X  n  X   X   X  || = O P Then under regularity conditions (C1)-(C3), we have Lemma 5.1 implies that when the tuning parameters associ-ated with the non-zero coefficients of main effects and pair-wise interactions tend to 0 at a rate faster than n  X  1 / 2 there exists a local minimizer of Q n (  X  ), which is tent (the sampling error is O p ( n  X  1 / 2 )). Theorem 5.2 shows that our model removes noise consistently with high proba-bility (  X  1). If and theorem 5.2 imply that the satisfies P (  X   X  A c = 0)  X  1.

Theorem 5.3. Assume under the regularity conditions (C1)-(C3), the component  X   X 
A of the local minimizer  X   X  n (given in lemma 5.1) satisfies where I (  X   X  A ) is the Fisher information matrix of  X  A  X  A assuming that  X   X  A c = 0 is known in advance.
 Theorem 5.3 shows that our model estimates the non-zero coefficients of the true model with the same asymptotic dis-tribution as if the zero coefficients were known in advance. Based on theorems 5.2 and 5.3, we can say that our model has the oracle property [3], [5], when the tuning param-eters satisfy the conditions To satisfy these conditions, we have to consider adaptive weights w  X  j ,w  X  k l [23] for our tuning parameters  X   X  appendix for more details). Thus, our tuning parameters are:
In this section, we consider the asymptotic behavior of our model when the number of predictors p n grows to infinity along with the sample size n . If certain regularity conditions (C4)-(C6) (shown below) hold, then we can show that our model possesses the oracle property.

We denote the total number of predictors by p n . We denote all the quantities that change with sample size by adding n as their subscript. A 1 , A 2 , A are defined as in sec-tion 5 and let s n = |A n | . The asymptotic properties of our model when the number of predictors increases along with the sample size are described in the following lemma and theorem. The regularity conditions (C4)-(C6) are given below: Let  X  n denote the parameter space for  X  n . (C4) The observations V ni : i = 1 ,...,n are independent and identically distributed with a probability density f n  X  ), which has a common support. We assume the density f n satisfies the following equations: and fies 0 &lt; C 1 &lt;  X  min I n (  X  n )  X   X  max I n (  X  n ) &lt; C n , where  X  min ( . ) and  X  max ( . ) represent the smallest and largest eigenvalues of a matrix respectively. Moreover, for any j,k = 1 ,...,p n , and (C6) There exists a large open set  X  n  X   X  n  X  R p n which contains the true parameters  X   X  n such that for almost all V the density admits all third derivatives  X  3 f n ( V ni ,  X   X  X  nk  X  X  nl for all  X  n  X   X  n . Furthermore, there are functions M for all  X  n  X   X  n and for all p n ,n, and j,k,l .

Lemma 5.4. Assume that the density f n ( V n , X   X  n ) satisfies some regularity conditions (C4)-(C6). If p /n  X  0 as n  X  X  X  , then there exists a local minimizer  X   X  n of Q n (  X  ) such that ||  X   X  n  X   X   X  n || = O P (
Theorem 5.5. Suppose that the density f n ( V n , X   X  n isfies some regularity conditions (C4)-(C6). If p n/p n b n  X   X  and p 5 n /n  X  0 as n  X   X  , then with prob-ability tending to 1, the p n/p n -consistent local minimizer  X   X  n in Lemma 5.4 satisfies the following: where A n is an arbitrary m  X  s n matrix with finite m such that A n A T n  X  G and G is a m  X  m nonnegative symmetric  X  as sample size n  X   X  , we could consider arbitrary linear combination A n  X   X  n A n for the asymptotic normality of our model X  X  estimates. Similar to section 5.1, to satisfy oracle property, we have to consider an adaptive weights w  X  nj ,w [23] for our tuning parameters  X   X  , X   X  k as:
In this section, we outline three optimization methods that we employ to solve our objective function (7), which corresponds to Line 4 and 5 in Algorithm 1. [15] provides a good survey on several optimization approaches for solv-ing ` 1 -regularized regression problems. In this paper, we use the sub-gradient and co-ordinate wise soft-thresholding based optimization methods since they work well and are easy to implement. We compare these methods in the ex-perimental results in section 7.
Sub-gradient based strategies treat the non-differentiable objective as a non-smooth optimization problem and use sub-gradients of the objective function at the non-differentiable points. For our model, the optimality conditions w.r.t pa-rameter vectors  X  and a k can be written out separately based on the objective function (7). Optimality conditions w.r.t a k is: where L ( a k ) is the loss function of our linear regression model or logistic regression model in Equation (7) w.r.t a Similarly, optimality conditions can be written for  X  . The sub-gradient  X  s j f ( a k ) for each a kj is given by where for our linear regression model. The negation of the sub-gradient represents the steepest descent direction. Similarly the sub-gradients for  X  (  X  s j f (  X  ) ) can be calculated. Differ-ential of the loss function of the linear regression in Equation (7) w.r.t  X  is given by  X  j L (  X  ) = 1
Andrew and Gao [1] proposed an effective strategy for solving large-scale ` 1 -regularized regression problems based on choosing an appropriate steepest descent direction for the objective function and taking a step like a Newton it-eration in this direction (with an L-BFGS Hessian approxi-mation [12]). The orthant-wise learning descent method for our model takes the following form where P O and P S are two projection operators and H  X  is the positive definite approximation of Hessian of quadratic ap-proximation of objective function f (  X  ), and  X   X  and  X  a step sizes. P S projects the Newton-like direction to guaran-tee that it is in the descent direction. P O projects the step onto the orthant containing  X  or a k and ensures that line search does not cross points of non-differentiability.
Schmidt [15] proposed optimization methods called Pro-jected Scaled Sub-Gradient methods where the iterations can be written as the projection of a scaling of a sub-gradient of the objective. Please refer to [1] and [15] for more details on OWD and PSS methods.
Soft-thresholding based co-ordinate descent optimization method can be used to find  X  , a k updates in the alternating optimization algorithm for our FHIM model. The  X  updates are  X   X  j and are given by where W = P k a k a k , and S is the soft-thresholding operator [7]. Similarly, the updates for a k are  X  a kj by where W  X  j is W with j th column and j th row elements are all zero.
In this section, we use synthetic and real datasets to demon-strate the efficacy of our model (FHIM), and compare its performance with LASSO [18], All-Pairs Lasso [2], Hierar-chical LASSO [2], Group Lasso [21], Trace-norm [9], Dirty model [8] and QUIRE [14]. For all these models, we perform 5 runs of 5-fold cross-validation on training dataset (80 %) to find the optimal parameters and evaluate prediction error on a test dataset (20 %). We search tuning parameters for all methods using grid search and for our model the param-eters  X   X  and  X  a k are searched in the range of [0 . 01 , 10]. We also discuss the support recovery of  X  and W for our model.
We use synthetic datasets and a real dataset for classifi-cation and support recovery experiments. We give detailed description of these datasets below.
We generate the predictors of the design matrix X using a normal distribution with mean zero and variance one. The weight matrix W was generated as a sum of K rank one matrices i.e. W = P K k =1 a k a T k .  X  , a k were generated as a sparse vector from a normal distribution with mean 0 and variance 1, while noise vector is generated from a normal distribution with mean 0 and variance 0 . 1. Finally, the re-sponse vectors y of the linear and logistic regression models with pairwise interactions were generated using Equations (3) and (5) respectively. We generated several synthetic datasets by varying number of instances ( n ), number of vari-ables/predictors ( p ), rank of W i.e. K and sparsity level of  X  , a k . We denote the combined total predictors (that is main effects predictors + predictors for interaction terms) by q , here q = p ( p + 1) / 2. Sparsity level (non-zeros) was chosen as 2  X  4% for large p ( &gt; 100), and 5  X  10% for small p ( &lt; 100) for both  X  , a k . In this paper, we show results for synthetic data in these settings: Case (1) n &gt; p and q &gt; n (high-dimensional setting w.r.t combined predictors) and, Case (2) p &gt; n (high-dimensional w.r.t original predictors).
To predict cancer progression status directly from blood samples, we generated our own dataset. All samples and clinical information were collected under Health Insurance Portability and Accountability Act compliance from study participants after obtaining written informed consent under clinical research protocols approved by the institutional re-view boards for each site. Blood was processed within 2 hours of collection according to established standard oper-ating procedures. To predict RCC status, serum samples were collected at a single study site from patients diagnosed with RCC or benign renal mass prior to treatment. Defini-tive pathology diagnosis of RCC and cancer stage was made after resection. Outcome data was obtained through follow-up from 3 months to 5 years after initial treatment. Our RCC dataset contains 212 RCC samples from benign and 4 different stages of tumor. Expression levels of 1092 proteins based on a high-throughput SOMAmer protein quantifica-tion technology are collected. The number of Benign, Stage 1, Stage 2, Stage 3 and Stage 4 tumor samples are 40; 101; 17; 24 and 31 respectively.
We use linear regression models (Equation 3) for all the following experiments and we only use logistic regression models (Equation 5) for synthetic data experiments shown in table 2. We evaluate the performance of our method (FHIM) by the following experiments: 1. Prediction error and support recovery experiments on 2. Classification experiments using RCC samples: We
We evaluate the performance of our model (FHIM) on synthetic dataset by the following experiments: (i) Com-parison of optimization methods presented in section 6, (ii) Prediction error on the test data for q &gt; n and p &gt; n (high-dimensional settings), (iii) Support recovery accuracy of  X  , W and (iv) Prediction of rank of W using greedy approach.
Table 3 shows the prediction error on test data when different optimization methods (discussed in section 6) are used for our model (FHIM). From table 3, we see that both OWD and PSS methods perform nearly similar (OWD is marginally better), and are better than the soft-thresholding method. This is because, in soft-thresholding, co-ordinate updates of variables might not be accurate in high dimen-sional settings (i.e. the solution is affected by the path taken during updates). We observed that soft-thresholding in gen-eral is slower than OWD and PSS methods. For all the other experiments discussed in this paper, we choose OWD as the optimization method for FHIM. Table 1 and Table 2 shows Figure 1: Support Recovery of  X  (90 % sparse) and W (99 % sparse) for synthetic data Case 1: n &gt; p and q &gt; n where n = 1000 ,p = 50 ,q = 1275 .
 Figure 2: Support Recovery of W (99.5 % sparse) for synthetic data Case 2: p &gt; n where p = 500, n = 100. Online supplementary materials contain high-quality images for this figure. the performance comparison (in terms of prediction error on test dataset) of FHIM for linear and logistic regression models with respect to the state-of-the-art approaches such as Lasso, Fused Lasso, Trace-Norm and Hierarchical Lasso (HLasso is a general version of SHIM [3]). From tables 1 and 2, we see that FHIM generally outperforms all the state-of-the-art approaches for both linear and logistic pairwise re-gression models. For q &gt; n , we see that test data prediction hence we don X  X  show it X  X  results here. error for FHIM is consistently lower compared to all other approaches. For p &gt; n , FHIM performs slightly better than other approaches, however, the prediction error for all the approaches is high since it X  X  hard to accurately recover the coefficients of main effects and pairwise interactions in very high-dimensional settings.

From figure 1 and table 4, we see that our model performs very well ( F 1 score close to 1) in the support recovery of  X  and W for the q &gt; n setting. From figure 2 and table 5, we see that our model performs fairly well in the support recovery of W for p &gt; n setting. We observe that when the tuning parameters are correctly chosen, support recovery of W works very well when W is low-rank (see table 4 and 5), and the F 1 score for the support recovery of W decreases with increase in rank of W . Table 5 shows that for q &gt; n the greedy strategy of FHIM accurately recovers the rank K of W , while for p &gt; n , the greedy strategy might not correctly recover K . This is because the tensor factorization is not unique and slightly correlated variables can enter our model during optimization. Figure 3: Comparison of the classification per-formances of different feature selection approaches with our model in identifying the different stages of RCC. We perform five fold cross validation five times and average AUC score is reported.
In this section, we report systematic experimental results on classification of samples from different stages of RCC. The predictive performance of the markers and pairwise in-teractions selected by our model (FHIM) is compared against the markers selected by Lasso, All-Pairs Lasso [2], Group Lasso, Dirty model [8] and QUIRE. We use SLEP [13], MAL-SAR [22] and QUIRE packages for the implementation of these models. The overall performance of the algorithms are shown in Figure 3. In this figure, we report average AUC score for five runs of 5-fold cross validation experi-ments for cancer stage prediction in RCC. In 5-fold cross validation experiments, we train our model on the four folds to identify the main effects and pairwise interactions and we use the remaining one fold for testing prediction. The average AUC achieved by features selected with our model are 0.68, 0.89 and 0.84 respectively for the three cases dis-cussed in section 7.1.3. We performed pairwise t-tests for the comparisons of our method vs. the other methods, and all p-values are below 0.0075. From figure 3, it is clear that our model outperforms all the other algorithms that do not use prior feature group information for all the three clas-sification cases of RCC prediction. In addition, our model has similar performance to the state-of-the-art technique -QUIRE [14], which uses Gene Ontology based functional an-notation for grouping and clustering of genes to identify high order interactions.
 Figure 4: Examples of functional modules for RCC Case 3, induced by markers and interactions discov-ered by our model and enriched in pathways and functions associated with RCC
An investigation of the pairwise interactions identified by our model on RCC dataset reveals that many of these inter-actions are indeed relevant to the prediction of cancer. Fig-ure 4 shows some of the interactions associated with higher weighted pairwise co-efficients for Case 3 of RCC classifica-tion experiment. The interactions include CX3CL1-CD97 , CHEK2-IL5RA which are known to be related to proteins in blood. CX3CL1 was recently found to promote breast can-cer [17], while CD97 was found to promote colorectal cancer [19]. We believe these protein interactions might lead to re-nal cell cancer. Further investigations of the interactions identified by our model might reveal novel protein interac-tions associated with renal cell cancer and thus leading to testable hypothesis.
FHIM has O ( np ) time complexity for algorithm 1. In gen-eral, FHIM takes more time than the Lasso approach since we do alternating optimization of  X ,a k . For q  X  n setting with n = 1000, q = 1275, our OWD learning optimiza-tion method on Matlab takes around  X  1 minute for 5-fold cross-validation, while for p &gt; n with p = 2000, n = 100, our FHIM model took around 2 hours for 5-fold cross-validation. Our experiments were run on intel i3 dual-core 2.9 GHz CPU with 8 GB RAM. Table 3: Comparison of optimization methods for our FHIM model based on test data prediction error
In this paper, we proposed a factorization based sparse learning framework called FHIM for identifying high-order feature interactions in linear and logistic regression mod-els, and studied several optimization methods for our model. Empirical experiments on synthetic and real datasets showed that our model outperforms several well-known techniques such as Lasso, Trace-norm, GroupLasso and achieves com-parable performance to the current state-of-the-art method -QUIRE, while not assuming any prior knowledge about the data. Our model gives  X  X nterpretable X  results for high-order feature interactions on RCC dataset which can be used for biomarker discovery for disease diagnosis.
 In the future, we will consider the following directions: (i) We will consider factorization of the weight matrix W as W = P k a k b T k and higher-order feature interactions, which is more general, but the optimization problem is non-convex; (ii) We will extend our optimization methods from Single-Task Learning to Multi-Task Learning; (iii) We will con-sider groupings of features for both Single Task Learning and Multi-Task Learning.
 [1] G. Andrew and J. Gao. Scalable training of [2] J. Bien, J. Taylor, and R. Tibshirani. A lasso for [3] N. H. Choi, W. Li, and J. Zhu. Variable selection with [4] D. K. Duvenaud, H. Nickisch, and C. E. Rasmussen. [5] J. Fan and R. Li. Variable selection via nonconcave [6] R. Foygel, N. Srebro, and R. Salakhutdinov. Matrix [7] J. Friedman, T. Hastie, H. H  X  ofling, and R. Tibshirani. [8] A. Jalali, P. Ravikumar, and S. Sanghavi. A dirty [9] S. Ji and J. Ye. An accelerated gradient method for [10] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. [11] E. L. Lehmann and G. Casella. Theory of point [12] D. C. Liu and J. Nocedal. On the limited memory [13] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with [14] R. Min, S. Chowdhury, Y. Qi, A. Stewart, and [15] M. Schmidt. Graphical model structure learning with [16] J. A. Suykens and J. Vandewalle. Least squares [17] M. Tardaguila, E. Mira, M. A. Garc  X  X a-Cabezas, A. M. [18] R. Tibshirani. Regression shrinkage and selection via [19] M. Wobus, O. Huber, J. Hamann, and G. Aust. Cd97 [20] T. T. Wu, Y. F. Chen, T. Hastie, E. Sobel, and [21] M. Yuan and Y. Lin. Model selection and estimation [22] J. Zhou, J. Chen, and J. Ye. MALSAR: Multi-tAsk [23] H. Zou. The adaptive lasso and its oracle properties.
Proof of Lemma 5.1:. Let  X  n = n  X  1 / 2 + a n and {  X   X  +  X   X  : ||  X  || X  d } be the ball around  X   X  , where  X  = ( u 1 v 11 ,....v Kp ) T = ( u T , v T ) T . Define Where Q n (  X   X  ) is defined in equation (8). For  X  that satisfies ||  X  || = d , we have We used Taylor X  X  expansion in above step. We split the above into three parts and we get: Thus,
We see that K 2 dominates the rest of the terms and is positive since I (  X  ) is positive definite at  X  =  X   X  from regu-larity condition (C2). Therefore, for any given &gt; 0 there exists a large enough constant d such that This implies that with probability at-least 1  X  , there exists a local minimizer in the ball {  X   X  +  X  n  X  : ||  X  || X  d } . Thus, there exists a local minimizer of Q n (  X  ) such that ||  X  O (  X  n ).

Proof of Theorem 5.2:. Let us first consider P (  X   X  A c 2 0)  X  1. It is sufficient to show that for any ( k,l )  X  X  with probability tending to 1, where n = Cn  X  1 / 2 and C &gt; 0 is any constant. To show (12), notice where  X   X  lies between  X   X  n and  X   X  . By regularity conditions (C1)-(C3) and the Lemma 5.1, we have As (11) has identical proof as above. Also, P (  X   X  A c can be proved similarly since in our model  X  and  X  are independent of each other.

Proof of Theorem 5.3. Let Q n (  X  A ) denote the objec-tive function Q n only on the A X  component of  X  , that is Q n (  X  ) with  X  A c . Based on Lemma 5.1 and Theorem 5.2, we have P (  X   X  A c = 0)  X  1 . Thus, Thus,  X   X  A should satisfy with probability tending to 1. Let L n (  X  A ) and P  X  (  X  note the log-likelihood function of  X  A and the penalty func-tion of  X  A respectively so that we have From (13), we have with probability tending to 1.
 Now, consider by Taylor expansion of first term and second terms at  X  A =  X   X  A , we get the following: since
Thus, we get, Therefore, from central limit theorem,
The proofs for lemma and theorem of section 5.2 are along the same lines as above. Please refer to Appendix section C for more details.

Here, we explain how the adaptive weights w  X  j ,w  X  k l be calculated for tuning parameters  X   X  , X   X  k in Theoretical properties (Theorems 5.3 &amp; 5.5) of Section 5. Let q be the to-tal number of predictors, let n be total number of instances. When n &gt; q , we can compute the adaptive weights w  X  j for tuning parameters  X   X  j , X   X  k l using ordinary least squares (OLS) estimates of the training observations. where When q &gt; n , the OLS estimates are not available and so we compute the weights using the ridge regression estimates, that is, replacing all the above OLS estimates with the ridge regression estimates. The tuning parameter for ridge re-gression can be selected using cross-validation. Note, we find  X   X  k OLS j by taking least squares w.r.t to each  X  k  X  [0 ,K ] for some K  X  trueK . Without loss of generality we can assume K = trueK for proving the Theoretical prop-erties in section 5. Even if K  X  trueK , it does not affect the Theoretical properties since the cardinality of A 2 ( |A 2 not affect the root-n consistency (see, proof of lemma 5.1). In practice, K is greedily chosen by algo. 1 in our paper.
For interested readers, we provide online supplementary materials at http://www.cs.toronto.edu/~cuty/FHIM_Supp. pdf with detailed proofs for Lemma 5.4 and Theorem 5.5. These proofs do not affect the understanding of this paper. We also provide high quality images for figure 1 in the sup-plementary materials. We provide more details about the experimental settings for state-of-the-art techniques used in this paper.
