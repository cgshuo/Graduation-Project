 Learning to rank method has been proposed for practical ap plication in the field of information retrieval. When em-ploying it in microblog retrie val, the significant interactions of various involved features are rarely considered. In this pa-per, we propose a Ranking Factorization Machine (Ranking FM) model, which applies Factorization Machine model to microblog ranking on ba sis of pairwise classification. In this way, our proposed model combines the generality of learn-ing to rank framew ork with the advantag es of factorization models in estim ating interactions between featu res, leading to better retrie val performa nce. Moreover, three group s of features ( content relevance features , semantic expansion fea-tures and quality features ) and their interactions are utilized in the Ranking FM model with the methods of stochastic gradient descent and adaptive regularization for optimiza-tion. Experime ntal results demo nstrate its superiori ty over several baseline syste ms on a real Twit ter dataset in term s of P@30 and MAP metri cs. Furthermore, it outperform s the best performing resul ts in the TREC X  12 Real-Time Search Task.
 H.3.3 [ Information Searc h and Retrieval ]: Information Search and Retrieval X  Retrieval models Learning to Rank; Microblog Retri eval; Ranking FM; Opti-mization Method
Microblog, a popular form of social medi a servi ce, pro-vides people a convenient way to post and share their ac-tivities, emot ions and statuses [6]. In Twitt er, people can  X 
First two authors contribute to this work equa lly.  X  Corres ponding author.
 post short messages limited within 140 characters. Besides, shortened URL s and some common signs (e.g.  X  X  X ,  X # X  and  X  X T X ) can b e embedded in a tweet for further content rep-resen tation or user interaction. To explore information re-trieval (IR) in micro blog ging environment such as Twit ter, TREC firstly introduced a Real-Time Search Task in 2011 [11], which doesn X  X  mean that the searc h results are simp ly ranked chronologically. It means that an information need arrives at a specific time and concerns some thing happening right now [15].

IR for microblog is a non-trivial problem which involves various kinds of factors, such as query expansion, link-based document expansion and social network analysis. However, a single retrieval model can hard ly utilize these factors perfec tly at the same time. Recen tly, learning to rank has become one of the most active resea rch topics in IR [9] and a great nu mber of features have been proved useful in IR for microblog [2]. However, there still exists much space for improvemen t in feature utilization of prior work. First, some features with the potential of enhancing the search effici ency are not fully exploited. For instance, as a crucial factor in IR, the link feature is usually treat ed as a binary one by which we can in fact obtain more seman tic information from the linked page. Second, featu res are considered independent when applied to most learning to rank approaches while it cann ot be neglected that some features are closely related to each other. For instance, RT and @ symbols occur in the same tweet frequen tly, revealing their intrinsic relationship. However, exist ing models which consider nested feature interactions are inefficient and not quite flexible such as the non-linear SVM model with polynomial kernel.

To remed y the above drawbacks, we propose employing an optimization for Ranking FM to improve the perform ance of system for Real-Time Search Task, which applies Factor-ization Machines (FM) model [13] to microblog ranking on basis of pairwise classification. The main contribu tions in this paper are conclud ed as follows: (1) we empl oy Rank-ing FM, which adopts FM as the ranking function to model interactions between features, and apply it to pairwise learn-ing to rank approach; (2) we utilize several effective features which are neglected in exist ing work to boost the microblog retrieval perform ance; (3) we optimize the Ranking FM by two meth ods of stochastic gradient descent and adaptive reg-ularization. The proposed approach is analyzed empirically on the Tweet11 corpus used in TREC X  11 and TREC X  12 Mi-croblog Track. Experimental results indicate that Ranking FM outperforms several baseline systems as well as the best perform ed system in the TREC  X 12 Real-Time Search Task.
The rema inder of this paper is organized as follows. We give an overview of the relat ed work in Section 2. In Section 3, we first introduce the Factorization Machines [13], and then describe our Ranking FM approach to deal with the micro blog retrie val problem, followed by two optimization meth ods to estimate Ranking FM model parameters. Section 4 presents three categories of features used in our meth od. In Section 5, experimen tal results and comparison are presented in detail. Finally, we conclud e the paper in Section 6.
Rece ntly, more and more machine learning techn ologies have been used to train the ranking model, and learn ing to rank has becom e one of the most active resea rch topics in IR [9]. Joachims et al. [7] applied Ranking SVM to optimize the retriev al quality of search engines with users X  clic k-through data. Besides, Cao et al. [1] adapted Ranking SVM to document retrie val by modifying the loss function.
Factorization Machines, a new model class combining the advantages of SVM with factorization models, has been proposed to model all nested variable interactions [12]. Factorization Machines are able to work with any real-value feature vector, and it also can mimi c most state-of-the-art factorization models by feature engineering. As a general predictor, Factorization Machines can b e applied to a variety of prediction tasks including regre ssion, binary classification and ranking.

Many attempts have been made in exploiting IR in the microblogosph ere. Massoud i et al. [10] prop osed incorporating query expa nsion and quality indicators in microblog retrie val. In their work, quality indicators such as emoticons, post length, shouting, capitalization, hyperlinks, reposts, followers and recen cy are taken into consideration to build the retrieval model. An integrated retriev al model und er languag e model framew ork is prese nted in [8]. The proposed approach described a two-stag e pseudo-relevance feedback query expansion to estimate the query languag e model and proposed two ways to expand document with shortened URLs in micro blog . In [2], Duan et al. employed learning to rank algorithms to determine the best set of features. Han et al. [3] adopted query expansion, document expansion and learning to rank techn ique to fuse the scores of the tweet text and the linked URL to improve retrieval performa nce. Specifically, they used a logistic regress ion model to learn a pairwise ranking for twitter retrieval. However, neither of Duan and Han took the pair interactions between features into consideration.
In this section, we first briefly introduce Factorization Ma-chines. Then we prese nt our Ranking FM framew ork which incorporates learning to rank approach with Factorization Machines. At last, two optimization methods are condu cted: stochastic gradient descent and adaptive regularization.
Factorization Machine (FM), proposed by Rendle in [12], models all nested interactions up to order d among n inpu t variables in x using factorized interaction parameters. The FM model of order d = 2 is defined as: where the model parameters that have to be estimated are: And h , i is the dot product of two vectors of size k : A row vector v i of V represents the i -t h variable with k factors. k  X  N + 0 is a hyper-parameter that defines the factorization dimensionality. A 2-way FM captures all single and pairwise interactions between variables with w as the global bias. The strength of the i -th variable is measured by w i and  X  w i,j := h v i , v j i models the interaction between the i -t h and j -th variables. Instead of using an own model parameter w i,j  X  R for interaction estimation, FM models the interaction b y factorizing it, which allo ws high quality parameter estimates of high-order interactions und er sparsity.

In [12], it proves that FM (eq.(1)) can be compu ted efficiently with the computational complexity of O ( k n ) as it is equivalent to:  X  y ( x ) := w 0 +
FM can be applied to a variety of prediction problems with variables whose interactions are hard for estimation. We can realize a Ranking FM approach b y incorporating the learning to rank approach with FM as follows. Assume that there exists an inpu t space X  X  R n , wher e n is the nu mber of features. Meanwhile there is an ou tput space of ranks/categories repres ented by labels Y = { r 1 , r 2 , , r with the nu mber of ranks q . They k eep a fixed order as r q  X  r q  X  1  X   X  r 1 , where  X  represen ts a prefe rence relation. A family of ranking functions f  X  F exist and each of the cand idate function can determ ine the preferen ce relations between instances: Supp ose that we are given a set of ranked instances S = n with prefere nce. The task here is to selec t the best function f  X   X  F that minimizes the loss function for the given ranked instances. This problem is form alized as learn ing for classification on pa irs of instances by Herb rich et al. [4]. To incorporate FM into pairwise learn ing to rank approach, we assume that f is repres ented with FM function ( d = 2): f  X  ( x ) := w 0 + Next, we take any instance pair and their relation to create a new instance with a new label. Let p and q denote the first and second instances in the pair instance, and y p and y denote their ranks, then we have: In this way, from a given training data set S , we create instances. Next , we can calculate the empirical Hinge Loss by the t -th instance pair of S  X  , where subscript X + X  X ndicates the positive part: intuitively with the computational complexity of O ( k n ) by app lying eq.(3). f Now, we define a global loss function over all training data where  X   X  is a regularization (hyper-)p arameter for the model parameter  X  . Theo retically, the regularization term can be chosen individ ually for each model parameter. However, in practical cases, it makes sense to use the same regularization parameters for similar model parameters (i.e.  X  w for w i  X 
From the frame work of Ranking FM, we can see that: (1) it is capable of modeling all nested variable interactions which are neglected in linear ranking model like Ranking SVM with linear kernel. (2) compared with other non-linear ranking models, its computational complexity is reduced to O ( k n ). Considering that k  X  n satisfies in most scenarios, we can estimate Ranking FM effici ently in linear time.
In this section, we present t wo methods to optimize the loss function in eq.(9). The two learning algorithms are stochastic gradient descent and adaptive regularization, both of which have been used to optimize FM [13, 14] and achieved good p erformance.
By differentiating eq.(9 ) with respect to parameter  X  , we can obtain: where of i and thus they can be precompu ted.

One of the most popular algo rithms for gradient descent is stochastic gradient descent ( SG D ), in which for each triple (( p , q ) , z )  X  S  X  , an update is perform ed as:
In eq.(9), we utilize L2 regularization to overcome the overfitting prob lem. The effect iveness of such regularization approach depends largely on the choice of the regularization parameter  X  . However, the grid search on validation set for fin ding the best  X  is time-consuming. Thus, to further enhance the effici ency of Ranking FM, we follow the work of [14], and modify the adaptive regularization method to estimate the Ranking FM model parameters.

The Adaptive Regularization ( A R ) method can b e sum-marized as a nested optimization task [14]. In Step 1 , on the training set S T , the future model parameters  X  ( t +1) optimized for the regularized loss objective with a current regularization constant  X  ( t ) ; In Step 2 , on the validation set S V , the objective is to determine the best future regu-larization values  X  ( t +1) with the updated model  X  ( t +1) minimizes the loss. In summary, with adaptive regulariza-tion, the optimization meth od is simp le and only requires a little extension to the stand ard SGD algorithm.
Several features have been proved effective in the prior work [2]. However, these features are not fully utili zed to further improve the performa nce of learning to rank approach in the micro blog osph ere. In this section, we describe the features used in Ranking FM in detail. We classify all the features into three group s as content relevance features , semantic expansion features and quality features .
Content relev ance features measure the relev ance between a tweet and a specific query by analyzing the content of tweets. Thes e features are Quer yTOT weet (the term overlap between a query and a tweet), Quer yBM25Tweet (the stan-dard BM25 weighting function is adop ted to measure the content relev ance between query Q and tweet T ), QueryT-FIDFT weet (the cosine similarity distance between a query and a tweet in the Vector Space Model with the TFIDF weighting method), QueryLMTweet (the KL-d ivergence lan-guag e model based retrieval meth od is utilized to meas ure the relev ance between query language model  X   X  Q and tweet language model  X   X  T ).
As microblog retriev al suffers severely from the vocabulary-mismatch prob lem (i.e. term overlap b etween query and tweet is relat ively small), different seman tic expansion tech niques can b e leveraged to improve the retrieval effect iveness. However, these expansion resou rces are not fully explored to obtain more informative evidence in the prior work (e.g. [2]). In this section, we introduce several novel seman tic expansion features on basis of query expansion and document expansion.

To extra ct features related to query expa nsion, we first fol-low the work [8] and employ a two-stag e pseudo-relev ance feedback query expansion approach to obtain a group of query-expansion (QE) terms . Then we treat the QE terms as a  X  X ew query  X  and calculate the sema ntic similarity between QE term s and the original tweet content using the aforemen-tioned similarity measures. Thus we obtain the following features: QETOT weet , QEB M25Tweet , QET FIDFTwe et and QEL MTweet .

Shortened URLs within tweets can enrich the repre sen-tation of the original tweets by leading users to an infor-mative, topic-related web page. Many learning to rank app roaches only leverage this resource as a binary feature or the coun t of the URL s. Inspired by the work [8], we expand the shortened URLs by extracting the content of the &lt; TITLE &gt; tag from the raw HTM L markup, and we name this content as Topic Information (i.e. TopicInfo). With the help of Topic Information, we extract the fol-lowing document-expansion related features by replacing the original tweet with TopicInfo: QueryTOTopicInfo , QE-TOT opicInfo , QueryBM25Top icInfo , QEB M25Top icInfo , QueryTFIDFT op icInfo , QET FIDFT opicInfo , QueryLM-Top icInfo and QELM Top icInfo .
Unlike content relev ance and sema ntic expansion features which are query-biased, quality features are tended to estimate the quality of a tweet. Some specific features of social network servic e can b e used to meas ure the quality and p otential popularity in the entire social network. Based on the assumption that users prefer those tweets that are related with their query or popular in the social network, we can conclud e the following featu res: mention coun t, retweet coun t, hashtag coun t, shortened URL coun t and length of tweet (after stopword remo val).

Previ ous work [2, 5] also demonstrates the usefulness of user-specific features in the microblogo sph ere, such as user activeness defin ed as tweet freque ncy and registration time, follower nu mber, popularity score [2] computed by Pag eRank, etc. However, these features are not availa ble in our experiment database (i.e. Twit ter11 Corp us), and we will explore them in our future work.
Several experiments are condu cted to measure the perfor-mance of Ranking FM for microblog retriev al. In this sec-tion, we firstly describe the experimen tal setup. Second ly, we evaluate the performa nce of Ranking FM and compare it with several baseline methods. Lastly, we condu ct analysis on (1) the importance of each feature group b y a feature ab-lation study, (2) the influence of hyper-parameter k which defines the factorization dimensionality and (3) the perfor-mance comparison b etween two optimization methods.
The Tweet11 corpus was obtained using a donation of the un ique identifiers of a samp le of tweets from Twit ter [11]. The Tweet11 has a samp le of abou t 16 million tweets. In addition, we also crawled all the shortened URLs contained in Tweet11 corpus to enrich the repres entation of original tweets, and extra cted each piece of topic information from the corresp onding URL to generate the TopicInfo corpus. For both corpora, we discarded the non-English tweets by using a languag e detector with infinity-gram, named ldig Also, we remo ved the simp le retweeted tweets beginning with  X  X T X  based on the assumption that such tweets have no extra inform ation b eyond the original ones. Moreover, each tweet was stem med using the Porter algo rithm and stop words were remo ved using the INQUERY words stoplist.
In TREC X  11 Microblog Track, NIST created 50 topics and provided the corresponding asses sments condu cted by NIST assessors. Tweets were judged on the basis of the defined information need using a three-point scale [11]: Not Relevant , Minimally Relevant and Highly Relevant . We took advantages of these topics along with assessments as training set in learn ing to rank approaches. We used another 60 topics, which are the official queries in the TREC  X 12 Microblog Track, as test queri es to meas ure the perform ance of Ranking FM. The main evaluation metrics in our experimen t are precision at N (i.e. P@N) and MAP, which are wide ly used in IR. In TRE C X 12, tweets are also judged based on three grades as in TREC X  11 Microblog Track. The evaluation measures are P@30 and MAP with respect to highrel (i.e. tweet set judged as highly relev ant) and P@30 is the official main metri c for the real-time searc h task in 2011 and 2012 .
To demonstrate the performa nce of our approach, we com-pare our system with three baseline meth ods. The first base-line meth od is a real-time ranking model und er language model framew ork, proposed by Liang et al. [8]. We real-ize this unsup ervised model and denote it as KL2SFBLoc . The second baseline meth od is a state-of-the-art ranking SVM model (denoted as RSVM Full ) with all the intro-duced features in Section 4. Specifically, we set Diric hlet smoothing parameter  X  = 100 when computing the Lan-gua ge model score and use default parameters in Lemur 2 to extra ct BM25 and TFIDF Model Score. A toolkit named SVM rank 3 implemented b y Thorsten Joachim is used to train this model ( C is tuned using five-fold cross-validation with training data). The best perform ed model in TREC X  2012 Real-Time Search task (i.e. hitURLrun3 proposed b y Han et al. [3]) is also reported for comparison.
As for the proposed method, we adopt all the features in-troduced in Section 4 and implement both stochastic gra-dient descent and adaptive regularization learning meth -ods to optimize Ranking FM with k = 3. The corre-sponding two methods are labeled as RFM FullSGD and RFM FullAR , respectively. In practice, when training Ranking FM with stochastic gradient descent, we use the same regularization parameters  X  w an d  X  v for W and V . In addition, we condu ct five-fold cross validation to searc h the regularization parameters. The best regularization pa-rameters are chosen based on the MAP score on validation set. https:// github .com/shuyo/ ldig http:// lemurproject.org/lemur/ http:// www. cs.cornell.edu/people/tj/svm light/svm rank.html (with p value &lt; 0 . 05 ) respectively.

Table 1 shows the perform ance comparison for afore-mentioned approaches. It can be clearly observed that both RFM FullSGD an d RFM FullAR outper-form KL2SFBLoc and RSV M Full rema rkably. More specifically, RFM FullSGD improves the P@30 over KL2SFBLoc and RSV M Full by 15.0 3% and 7.34%, re-spectively. More over, RFM FullAR is comparable with hitURLrun3 an d increases P@30 by 3.96%.
In this section, we empirically evaluate the effectiveness of each feature group with a featu re ablation study. As shown in Section 4, we classify all the features into three groups as content relev ance features, semantic expansion features and quality features. Sema ntic expansion features can b e furth er classified into query expa nsion featu res and document expa nsion features. In this experimen t, we first train the Ranking FM of k = 3 with all the features by stochastic gradient descent, and then remo ve one group of features each time. Notice that remo ving query expansion features means exclud ing the featu res with QE prefix (e.g. QEL MTopicInfo ) while remo ving document expansion features means exclud ing the features with TopicInfo suffix (e.g. Quer yLMTop icInfo ). Fig ure 1: P@N performan ce with different feature groups.  X  X ul l X  mean s all the features, while  X - X  means removing a specific group from the full feature set
We can observe from Figure 1 that when remo ving all the quality features, P@N drops substantially (p-value &lt; 0.01 by t-test). This indicates the significance of quality features in our ranking FM model. Remo ving query expa nsion features would also lead to a dramatic decrease in precision (p-value &lt; 0.01), which reveals the effectiveness of query expa nsion techn iques in short-text retriev al. Document expa nsion and content relev ance features, though not as important as query expansion features, also play an important role in gaining a goo d p erform ance. On the other hand , when only employing content relev ance features, the perform ance is much worse than that of the model with the full feature set (p-value &lt; 0.01).
The hyper-pa rameter k in eq.(3) defin es the factorization dimensionality. In this section, we analyze the influ ence of k in Ranking FM. When adapting learning to rank approaches to microblog retrieval, we consider feature interactions very significant in improving the retrie val perfor mance and the presentation of interactions is also important. To verify this assumption, we trained Ranking FM model with different hyper-parameter k with all features involved and stochastic gradient descent.

It can b e observed from Figure 2 that the Ranking FM model can achieve optimal P@30 and MAP values with k = 3. In addition, the improvements over Ranking FM with k = 0 (i.e. a 1-way Ranking FM neglecting pairwise feature interactions) are significant, which also reveals that the 2-way Ranking FM considering interactions between features is more effective in microblog retrieval. In fact, FM model with k = 0 is identical to a linear SVM model with can also explain the superiority of our 2-way Ranking FM over the state-of-the-art Ranking SVM with linear kernel. Figure 2: Rank ing FM performan ce with different hyper-parame ter k
On the other han d, when k is large and close to the feature size, the perform ance of 2-way Ranking FM drops while it is still better than that of Ranking FM with k = 0. This reveals the importance of selec ting an ap propriate k . Unlike the Ranking SVM with polynomial kernel which models each pair of interaction parameters indepen dently, the interaction parameters of a 2-way FM are factorized and v . This factorization also makes the 2-way Ranking FM more flexible by adjusting the nu mber of factors k according to specific applications.
We study different behaviors of two optimization meth ods, i.e. SGD and AR. Table 2 lists the retrie val results of Ranking FM of k = 3 with different optimization methods in Table 2: Effec tivenes s comp arison b etwee n SGD and AR for Rank ing FM terms of P@5, P@10, P@30 and MAP . We can observe that both methods perform very well in the Twitt er11 Corp us. Then , we experimen tally analyze the efficie ncy of Ranking FM with different optimization meth ods. We train our model with full feature set and set hyper-parameter k = 3. For SGD, the training time inclu des the model selec tion time with five-fold cross validation (  X  w and  X  v are both chosen with the selec ted best  X  w and  X  v ; when it comes to AR, training and validation are proceeding at the same time. Note that learning Ranking FM with AR does not require any predefined regularization values which are crucial for SGD and cost much time for determination. Fig ure 3: Efficiency comparison betwe en SGD and AR for Rank ing FM The efficiency comparison b etween SGD and AR for Ranking FM is shown in Figure 3. The resul ts indicate that AR is much more efficient than SGD though they both have an approximately linear time cost with respect to k . Instead of searching a grid of cand idate regularization values, AR has an unrestricted search space to choose the appropriate regularization pa rameters, thus it can significantly enhance the effici ency. Furthermore, as described in Section 3.3, SGD only uses two regularization values due to the exponential complexity of grid search, while AR can up date k + 1 regularization values at the same time without costing expensive search time.
In this study, Ranking FM model is employed which in-corporates pairwise learning to rank approach with Factor-ization Machines for microblog retrieval. Theoretically, we use Factorization Machines as ranking function an d utilize the hinge loss function to infer the Ranking FM model. Be-sides, we suggest t wo optimization methods, namely stochas-tic gradient descent and adaptive regularization, to estimate the model parameters. In addition, several effective fea-tures, includ ing content relevance features , semantic expan-sion features and quality features are utilized to boost the retrieval perform ance. Experimen tal resul ts on Tweet11 cor-pus demo nstrate the effectiveness of Ranking FM approach as it achieves significant improvemen ts compared with sev-eral baseline methods. Furthermore, Ranking FM which leverages all featu res outperforms the best perform ing runs of TREC X  12 Real-Time Search Task. The feature ablation study indicates the effectiveness of each feature group and the analysis of hyper-parameter k shows the importance of expressing the pairwise interactions between features in learning to rank approaches for microblog retriev al. Besides, the proposed Ranking FM is very flexible as k can b e ad-justed for different applications. Moreover, we evaluate the different behaviors of two optimization meth ods in term s of effectiveness and effici ency.
The work reported in this paper was supported by the National Natural science Found ation of China Gran t 6087503 3. [1] Y. Cao, J. Xu, T.-Y . Liu, H. Li, Y. Huang, and H.-W. [2] Y. Duan, L. Jiang, T. Qin, M. Zhou, and H.-Y . Shu m. [3] Z. Han, X. Li, M. Yang, H. Qi, S. Li, and T. Zhao. Hit [4] R. Herb rich, T. Graep el, and K. Oberma yer. Larg e [5] M. Huang, Y. Yang, and X. Zhu. Quality-biased [6] A. Java, X. Song, T. Finin, and B. Tsen g. Why we [7] T. Joachims. Optimizing search engines using [8] F. Liang, R. Qian g, and J. Yang. Exploiting real-time [9] T.-Y. Liu. Learning to rank for information retrieval . [10] K. Massoudi, M. Tsag kias, M. de Rijke, and [11] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. [12] S. Rend le. Factorization machines. In Procee dings of [13] S. Rend le. Factorization machines with libfm. ACM [14] S. Rend le. Learning recommender systems with [15] I. Soboroff, I. Ounis, and J. Lin. Overview of the
