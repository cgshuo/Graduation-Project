 This paper presents a novel approach to textual question answering (QA) which identifies answers to natural language questions by leveraging large collections of question-answer pairs extracted from web sources or generated automatically from text.

Instead of using the traditional retrieve-and-rerank ap-proach common to most previous approaches to factoid ques-tion answering, we introduce a new model of answer au-thority which allows question-answering systems to estimate the quality of answers not just in isolation  X  but in the larger context of the information contained in the corpus as a whole.

Our approach in this paper hinges on the creation of a new representation of the information stored in a document col-lection, known as a Question-Answer Database (QUAB). We assume that a QUAB represents a weighted directed graph consisting of the set of factoid question-answer pairs (QAP) that can be asked  X  and answered  X  given the content of a corpus. Once a set of QAP have been generated, we use in-ferential relationships identified by a system for recognizing textual entailment in order to construct the link structure necessary to compute the authority of each interconnected question or answer.

We have found access to the QUAB graph not only im-proves the accuracy of current answer retrieval techniques, but also allows for the determination of when no valid an-swer can be found for a question in a text corpus. Experi-mental results show that the authority derived from a graph of question-answer pairs can increase the performance of a factoid QA system by nearly 30%.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Performance, Experimentation Question Answering, Textual Reasoning, Information Re-trieval
The process of finding answers to a natural language ques-tion has traditionally been assumed to involve three separate stages: (1) question processing , the recognition of the infor-mation need expressed by a question; (2) passage retrieval , the retrieval of relevant text spans from specific keywords and phrases extracted from the text of a question, and (3) answer processing , the recognition and extraction of the text span corresponding to a question X  X  exact answer.

We argue that this is not the only valid paradigm for factoid question answering (QA). Instead of extracting an-swers from sets of retrieved passages, we introduce a novel paradigm which finds accurate answers to factoid questions by leveraging a network of question-answer pairs which rep-resent all of the questions  X  and answers  X  that can be de-rived from a particular text collection.

We hypothesize that by providing access to all of the avail-able questions that have viable answers in a document col-lection (as well as all of the answers that could be returned from the document collection in response to some question) we can develop a new model of answer authority which esti-mates the quality of candidate answers not just with respect to a set of features extracted from a question  X  but in the larger context of the information contained in the corpus as a whole.

Our approach hinges on the creation of a new represen-tation of the information stored in a document collection, known as a Question-Answer Database (henceforth, QUAB ). Once the set of all possible factoid questions have been gen-erated from a corpus, we show that it is possible to use the inferential relationships output by a textual entailment sys-tem [13] in order to construct a relevance-flow graph which can be used to the compute the likelihood that a given an-swer represents a valid answer to a question. In our current work, we use the HITS algorithm [18] in order to compute an estimate of each answer X  X  expected authority based on the distribution (and weight) of the entailment relationships rec-ognized between each of the question and answer nodes in-cluded in the QUAB. Given a factoid question q  X  , we assume that the best candidate answers retrievable from a QUAB are those which (1) have a high authority score and (2) are of the same semantic type as the expected answer type of q .

We anticipate that this new paradigm can also be used to determine when a question cannot be answered, given the available information in the text collection. Since a QUAB, by definition, is expected to enumerate (exhaustively) all of the potential candidate answers mentioned in a document and the questions that could be asked to retrieve those an-swers, we anticipate that questions that have no answers may only be weakly associable with the set of authoritative answers included in the graph. In order to capitalize on this intuition, we introduce a set of graph-based conditions which facilitate the identification of situations in which no text passage can sufficiently provide the information sought by a question.

Finally, we provide experimental results which shows that this framework enables a new model of relevance which pro-vides significantly increased performance on a factoid QA task. Experimental results show that the authority derived from a graph of question-answer pairs can increase the per-formance of a factoid QA system by nearly 30%.

The remainder of this paper is organized in the follow-ing way. Section 2 describes our model for computing the authority of answers contained in a QUAB graph. Sec-tion 3 describes our approach to automatically generating question-answer pairs from collections of unstructured text, while Section 4 presents the lightweight statistical frame-work for recognizing textual entailment that we use to esti-mate the authoritativeness of question-answer pairs included in a QUAB. In Section 5, we explore how authority scores derived from a QUAB can be used to answer questions sub-mitted by a user, while Section 6 presents experimental re-sults, and Section 7 presents our conclusions.
In this section, we introduce a new architecture for QA which leverages a novel representation of the information stored in a document collection in order to retrieve the best answer (or answers) in response to a submitted question.
The Internet has always been a good place to look for answers to questions. While collections of  X  X requently-asked questions X  (FAQs) have been a popular way for content pro-viders to share information with interested users, community-based question-answering websites (such as Yahoo! Answers 1 , gone one step further, enlisting the help of communities of users in order to create large, open-domain FAQs which can be searched or browsed on-line.

While these sites have provided a new way for users to gather information related to an area of interest, they face many of the same limitations encountered by traditional search applications. First, as with any information retrieval application, we anticipate that the performance of community-based question-answering sites is ultimately limited by the (1) number and (2) quality of the question-answer pairs (QAPs) contained in a particular database. Second, we ex-pect that estimating whether a valid (exact) answer to a user X  X  question is contained in a corpus will continue to re-main a challenge. In many cases, systems will continue to retrieve answers as long as there is some QAP which con-tains features from the original submitted question.
Imagine, for a moment, that a FAQ could be created which contained all of the questions that could be answered di-rectly from all of the (linguistic) content available on the Internet. While this imaginary corpus would be ridiculously large(bordering on several times the size of the linguistic content contained in the Internet itself), it would necessar-ily include all of the possible mappings between questions that individuals could ask and their (exact) answers. Like-wise, since the corpus would only contain connections be-tween questions and their answers, we could assume that questions for no valid answer could be found would remain unconnected to any answer passage.

We anticipate that access to a collection of interconnected question-answer pairs of this size could significantly change how question-answering systems retrieve answers to ques-tions: instead of extracting answers from sets of retrieved documents or passages, systems would be able to find ex-act answers by identifying the one (or more) pairs from this collection which come closest to expressing the information need expressed by the user X  X  submitted question.

Work in automatic question-answering [17, 11] has already begun to capitalize on this intuition. Work by [17] used a measure of syntactic similarity in order to retrieve candi-date answers from a collection of QAPs compiled from FAQs downloaded from the web, while [12] leveraged measures of conceptual similarity in order to rank question-answer pairs generated from passages retrieve in response to a query.
While these types of approaches have shown promise in improving the overall  X  X elevance X  of answers returned by a QA system, they do not guarantee than an exact answer will be returned in response to a user X  X  question, as answer pas-sages can be structurally or semantically similar to a ques-tion without containing an exact answer string.

In order to counter this limitation, we argue that QA sys-tems also need to be able to identify all of the possible ways that that a question (or an answer) could be expressed in order to identify the most appropriate QAPs for a factoid question. By knowing all (or at least some ) of the possi-ble ways that a question could be asked (or answered), we expect that exact answers can be found for any question by moving from one semantically-equivalent question to an-other until a question can be found which is linked to a valid candidate answer.

More formally, given the set of all questions { Q C } that have a correct answer in a document collection C , we assume that when a new question ( q  X  ) is asked, it can be answered correctly if either (1) the question q  X   X  Q C or (2) there exists some set of questions { q  X  i , ..., q  X  n } which lead to the inference of q  X  .

We expect that these types of equivalence relationships can be approximated using statistical techniques for recog-nizing forms of textual inference, such as textual entailment . Taken together with the observed connections between ques-tions and answers, we assume that these inferential relation-ships can provide a more informed mechanism for estimating the likelihood that any text passage corresponds to an exact answer to any question.

The rest of this section presents a formal description of the QA model which we have developed.
We define a QUAB as a weighted directed graph G = ( V, E ) consisting of:
We assume that a question node ( q i ) corresponds to any well-formed, natural language question for which there ex-ists at least one word, entity, or phrase which represents a complete and satisfactory answer to q i . We define an an-swer node ( a i ) as a pair ( r i , s i ), where r i represents the word, entity, or phrase which satisfies some factoid question q and s i corresponds to the sentence which both contains a mention of r i and provides the context necessary to support the recognition of r i as a valid answer to q i . (We assume that the same sentence s i can give rise to n valid question-answer pairs ( { ( q i 1 , a i 1 ) , ..., ( q i n , a i n the number of candidate answers r  X  s .)
We use two types of information to infer edges between pairs of nodes in V . First, we assume that a directed edge extends from a question q i to its corresponding answer node a ( q i  X  a i ) for every question-answer pair contained in Q .
Second, we assume that directed edges exist between any pair of nodes ( x, y ) which express the same propositional content. We use a system for recognizing textual entail-ment [6, 2, 8] in order to estimate the likelihood that the content of a node y can be inferred from the content of any other node x . (Our methodology for recognizing textual en-tailment (RTE) is presented in Section 4.) Following [6], we assume that x textually entails y (henceforth, | = te ) iff y can be considered to be true in every context in which x is also considered to be true. Since textual entailment (TE) an edge ( x  X  y ) exists if x | = te y . we assume that an edge ( x  X  y ) exists between two nodes ( x, y ) iff the probability that x textually entails y ( p TE ( yes, ( x, y ))) is above a pre-defined threshold  X  TE . ( N is used as a normalization factor to ensure that all of the edge probabilities sum to 1.) w ( x  X  y ) = Figure 1 provides an example of a small portion of a QUAB graph; edges derived from question-answer pairs (QAPs) are represented with a thin dashed line, while rela-tions derived from TE are represented with a thicker line. 5 As with logical entailment, we presume that if a | = te is not necessarily the case that b | = te a .

We assume that the relevance of any node included in a QUAB can be computed from measures which are sensi-tive to the interconnectedness of the graph. Following [18, 19], we consider that a QUAB represents an instance of a relevance-flow graph in which the weight assigned to an edge e  X  E extending from a node x to a node y (henceforth, w ( x  X  y )) is weighted based on a measure of the expected relevance of y given the evidence associated with x .
Given the link structure we have defined for a QUAB graph, we anticipate that the HITS algorithm [18] can be applied in order to compute weights for each of the edges in E . As with HITS , we assume that edge weights can be computed based on two recursively-defined measures: (1) a hub score (defined in (2)), corresponding to the quality of the nodes linked to by a given node and (2) an authority score (defined in (3)), corresponding to the relevance of a node based on the quality of the nodes which link to it.
Following [18], we assume that each node u  X  G can be assigned a non-negative authority score ( x u ) and a corre-sponding non-negative hub score ( y u ) based on the number  X  and the weight  X  of the incoming and outgoing edges that u participates in.
In order to find the optimal set of weights for a node u , we compute auth ( y ) and hub ( x ) in an alternating fashion un-til convergence or until a maximum number of t iterations is reached. 6 After each step, values for x u and y u are nor-malized such that the sum of their squares sum to 1 (i.e. P ( x u ) 2 = 1, P ( y u ) 2 = 1).

The HITS algorithm has been shown to converge to a pair of score functions, namely hubs  X  and auth  X  , for di-rected weighted graphs derived from a number of different sources, including: (1) hyperlinked documents [18] and (2) links derived from output of document clustering [19]. Since the QUAB relevance-flow graph we propose is structurally equivalent to these other graphs, we anticipate that the same algorithm can be applied to the graphs constructed from question-answer pairs.

We note that nodes in a QUAB graph may not necessarily be partition-able into distinct sets of hubs and authorities . While we anticipate that the X  X est X  X nswers should necessar-ily receive high authority scores (as they contain information 6 In our work, we set t =12. which either (1) serves as the answer to multiple questions or (2) is inferrable from the content of other answers in the network), it is important to note that the correct answer to a question may not always serve as a canonical authority in the QUAB graph. We anticipate that answers may also function as hubs if other answers can be inferred from their content. For example, while answer A 2 in Figure 1 repre-sents a prototypical instance of an authority (as it is linked to by 5 nodes), answer A 3 serves as a hub , as it links to multiple answers. While we expect most question nodes will be  X  X ub-like X , other questions will resemble authorities ; for example, Q 2 , which is linked to by three other nodes.
We anticipate that the hub and authority scores assigned to question and answer nodes in G can be used to retrieve the most likely answers for each new question q  X  submitted to a QA system.

When q  X  is submitted to the system, we assume that it is added to G as a new question node. Rather than select the subset of nodes in G that q  X  is connected to a priori , we assume that there is an unweighted, outgoing edge extending from q  X  to every other question and answer node in G .
We assume that the edge weight associated with any edge extending from q  X  to any other node in G depends on two factors: (1) the weights assigned to the other edges that q participates in and (2) a measure of the relevance of q  X  any other node u in the G (i.e. rel ( q  X   X  u )).
In order to estimate the weights associated with edges em-anating from q  X  , we use a pair of recursively-defined func-tions in order to estimate the edge weights for the set of edges extending between q  X  and the set of questions in G (i.e. w ( q  X   X  q i )) and the set of edges which hold between q  X  and the answer nodes in G ( w ( q  X   X  a i )). (These func-tions are defined in (4) and (5).)
We define rel ( x  X  y ) based on the hub and authority scores computed for the nodes and edges in G as in (6): where argmax w w ( x  X  y ) is defined as largest sum of edge weights leading from x to y in G .

As with HITS , we learn the optimal set of weights for each of the edges emanating from q  X  by computing w ( q  X  a ) and w ( q  X   X  a ) in an alternating fashion. After each round of computation, we consider all edges ( q  X   X  i ) above a threshold to be valid edges and recompute the hub and authority scores based on these new edges for all the nodes in G . This process continues until convergence, or until a maximum number of iterations has been reached.

Once the estimation of these edge weights is complete, we estimate the likelihood that an answer node a i connected to q  X  is selected as the exact answer ( p ans ) based on three factors: (1) the edge weight associated with the edge ( q a ), (2) the authority score of a e ( auth ( a e )), and (3) the likelihood that the candidate answer r e associated with a is of the same expected answer type as q  X  (i.e. p eat ( r
In this section, we describe our methodology for automati-cally generating collections of question-answer pairs (QAPs) from collections of unstructured texts. While we expect a QUAB can be created from any large collection of question-answer pairs (such as the QAPs collected by community-based question answering sites), we anticipate that gener-ating question-answer pairs from text can allow for the cre-ation of a QUAB which more closely represents the informa-tion content of a corpus. Relying on collections of automati-cally-generated QAPs may also alleviate some of the issues of data sparsity and incompleteness that are inherent to any hand-created collection of question-answer pairs.
Under our approach, each sentence s in a document col-lection C is indexed with the set of question-answer pairs ure 2 provides an example of some of the factoid ques-tions that can be generated from a single sentence.) We generate question-answer pairs using a statistical language model-based approach to generation in conjunction with a lightweight paraphrase acquisition system [9] in order to as-sociate each candidate answer mentioned in a corpus with as many well-formed questions as possible.
 We use a four-step process in order to generate QAPs. First, we use a trio of semantic parsers  X  based on Prop-Bank [25], NomBank [23], and FrameNet [1] annotations  X  in order to convert each sentence s in the corpus into a set { D } of multiple typed dependency graphs. Nodes in each graph d  X  { D } correspond to the predicates and the semantic ar-guments identified by each parser, while edges correspond to the role labels that the parser assign to each individual argument. (For example, given the example in Figure 2, our parsers create two dependency graphs, corresponding to the semantic dependencies associated with the predicates score and lift .)
Second, arguments identified by the semantic parsers are sent to an expected answer type detection module, which uses a Maximum Entropy classifier (akin to those in [20]) in con-junction with output from Language Computer Corpora-tion X  X  CiceroLite named entity recognition (NER) system in order to associate each argument { a i , ..., a n }  X  d with at least one of the expected answer types (EAT) found in the the answer type hierarchy used in [14].Heuristics were then used to map each EAT to a WH-phrase (e.g. Who, What, Where ) which could later be used to replace the argument in a generated question. (In our example in Figure 2, we associate the candidate answer Preston North End (labeled by the NER system as an instance of a sports team and by the semantic parser as an object argument of the verb lift ) with the WH-phrase Whom .)
Third, each dependency graph was then provided as input to a syntactic form generation module which leveraged a sta-tistical language model-based approach to generate a gram-matical question from the underlying dependency graph rep-resentation of each s .

We first manually created a small  X  X nalysis X  grammar of approximately 35 context-free and 70 context-sensitive rules designed to account for the syntactic properties of English factoid questions. (Examples of some of the context-free rules we employed are presented in Figure 2.) This X  X nalysis X  grammar was then used to train a lexicalized PCFG [5] by applying the grammar to a training corpus of 10,000 factoid questions; weights for each rule were based on the observed relative frequency of each rule when applied to the training set.

Questions were then generated for each candidate answer c in each graph d using a  X  X eneration X  grammar which con-sisted of only the top-k most probable rules from the  X  X nal-ysis X  grammar. Rules from the  X  X eneration X  grammar were applied following a head-driven node expansion [26] which allowed each recently-expanded non-terminal to be selected as the site of the next expansion until a terminal node is generated.

Questions were generated by first associating the WH-word selected for each candidate answer c as the root node ( q 0 ) of a syntactic tree Q . We then applied rules from the  X  X eneration X  grammar in order to generate the set of sub-structures { K i , ...K n } which provided a grammatical order-ing for all of the lexical items associated with nodes in g . Each possible substructure predicted by the  X  X eneration X  grammar was assigned a weight based on the probability of the rule language model.

In order to expand the set of questions that were asso-ciated with each candidate answer, we used a lightweight paraphrasing technique to generate paraphrases of each gen-erated question. (The algorithm we used is presented in Algorithm 1.)
We followed a three-step process to generate paraphrases of each sentence. First, pairs of arguments 7 ( h a 0 , a selected from each d ; paraphrases were then generated using sets of sentences which contained both a 0 and a i . Second, the top 1000 sentences containing each pair of arguments dex corresponding to the thematic role that they are associ-ated with. For example, a 0 is generally associated with the thematic role agent (or other agent -like roles), while a is generally associated with thematic roles such as patient or experiencer . For full details of these role assignments, please see [25].
 Algorithm 1 Paraphrase Generation 1: Input: Pairs of arguments, h a 0 , a i i , where 2: Output: Sets of paraphrased sentences, { s cl i 3: for all pairs h a 0 , a i i do 4: Retrieve 1000 s containing h a 0 , a i i from WWW 5: Compute token distance between a 0 , a i ( span ( a 0 6: Filter each s with 2  X  span ( a 0 , a i )  X  8 7: Complete-link cluster { s } into clusters { cl } 8: Filter cl with size ( size ( cl ))  X  10 9: return All sentences { s cl i 10: if size( cl )  X  25 then 11: Identify centroid ( s centroid ) from cl 13: end if 14: end for (separated by no more than 10 tokens) were retrieved from the WWW. When less than 1000 sentences containing the two arguments were available, we retrieved sentences which included instances of the named entity types associated with each argument. Third, we used a complete-link clustering algorithm in order to group sentences into sets that were presumed to be likely paraphrases of each other. Finally, we computed the likelihood that a word w p from a paraphrase was a valid paraphrase of a word from an original commit-ment w c as in (8), where p ( w p ) and p ( w o ) computed from the relative frequency of the occurrence of w p and w o in the set of clusters generated for c , and p ( k ) was computed from the frequency of each  X  X verlapping X  term found in the paraphrase and the original c .
The top 3 paraphrases generated for each c were then used to generate new versions of each question. (Examples of paraphrases generated from two different argument pairs are presented in Figure 3.)
In this section, we describe how a lightweight, statistical learning-based framework for recognizing textual entailment (RTE) [6, 2, 8] relationships can be leveraged in order to identify the set of edges which exist between the nodes con-tained in a QUAB.

In our work, we use our own implementation of the light-pute the likelihood that an edge ( x  X  y ) holds between a pair of nodes ( x, y ).
 Algorithm 2 Extracting Discourse Commitments 1: Input: An underlying argument structure S 2: Output: A syntactic tree Q corresponding to a well-3: for all Candidate answer c identified in { a i , ..., a 4: Adjoin ( c , q 0 ) 5: for all Non-terminal nodes { q 1 , ..., q n } in Q do 6: for all Grammar rules { r i , ..., r m } which apply to 7: Generate structure K : { q  X  i , ..., q  X  n } described by 8: Determine p ( K ) 9: for all Grammar rules { r  X  i , ..., r  X  m } which apply 10: Generate the structure K  X  : { q  X  X  i , ..., q  X  X  n } de-11: Determine p ( K  X  ) 12: end for 13: if p ( K )  X  p ( K  X  ) then 14: Discard ( K ) 15: end if 16: end for 17: Adjoin ( K , Q ) which argmax p ( K ) 18: end for 19: end for
First described in [6], the task of recognizing textual en-tailment (TE) requires systems to determine whether the meaning of a short text passage can be conventionally in-ferred from the meaning of some longer text passage. While the recognition of forms textual inference (such as TE) has traditionally been addressed using formal reasoning methods (such as automatic theorem proving [28], model checking, or model building [3]), a considerable amount of recent work conducted as part of the PASCAL Recognizing Textual En-tailment (RTE) Challenges [6, 2, 8] have demonstrated the viability of relatively X  X hallow X , statistical learning-based ap-proaches to RTE.

We presume that by recognizing the available textual in-ference relationships that occur between sets of question-answer pairs generated from text, we can organize QUABs in a manner which will enable the optimal retrieval of in-2007 PASCAL RTE evaluation, correctly recognizing TE in more than 80% of test cases. formation most relevant to a question X  X  information need. Since the goal of our current work is to investigate tech-niques which could lead to the better retrieval and identi-fication of candidate answers for QA applications, we will focus the remainder of our discussion on how textual entail-ment (TE) can be used in order to estimate the authority of a QUAB question-answer pair, given a submitted ques-tion. (The architecture of our system for recognizing TE is presented in Figure 5.)
We follow [13] in using adopting an approach to RTE which explicitly considers the set of discourse commitments that are derivable from the textual content of a pair of texts. In our work, we used the probabilistic finite-state transducer (FST)-based information extraction framework described in [7] to extract commitments from four differ-ent types of syntactic constructions, including: (1) supple-mental expressions [16] (such as as-clauses, non-restrictive relative clauses, nominal appositives, parenthetical adverbs, and epithets), (2) sentence-and phrase-level coordination, (3) subordination, and (4) possessive noun phrases. (See Figure 4 for a subset of the commitments that can be ex-tracted from the sentence in Figure 2.) Commitments were extracted using a series of weighted regular expressions R that were created by hand; weights were learned for each reg-ular expression r  X  R using our implementation of [7]. Af-ter each candidate commitment was processed by the FST, each commitment was then resubmitted to the FST for ad-ditional round(s) of extraction until no additional commit-ments could be extracted from the input string.
We then expand the total number of commitments avail-able from each sentence using the lightweight approach to paraphrasing described in Section 2. As with our approach to question generation, paraphrases are generated by first selecting a pair of arguments from each commitment; these arguments are then used to retrieve a set of sentences con-taining both arguments and supplied to a clustering algo-rithm in order to identify the constructions which represent the best possible paraphrases of the commitment.

Semantic dependencies identified by a PropBank-based se-mantic parser were then used to convert each commitment into a dependency graph, where nodes were associated with individual tokens (or phrases identified by a tokenizer) and edges were associated with the set of syntactic or seman-tic dependencies that link pairs of nodes. We then used a variant of the maximum weighted matching approach in-troduced in [27] in order to identify the commitments in a { C u } which represent the best possible alignment for each commitment in { C v } .

We used the reciprocal best-hit method [24] in order to select pairs of aligned commitments from { C } to be consid-ered in the recognition of textual entailment. Under this method, a pair of commitments ( c u , c v ) was considered iff c was found in the top-n alignments identified c v and iff c was also found in the top-n alignments identified for c u
Once the best set of commitment alignments ( { ( c u 1 , c use a decision tree classifier in order to estimate the like-lihood (expressed as the confidence of classifying a pair of commitments as either a positive or negative instance of tex-tual entailment) that a commitment from u textually entails a commitment derived from v . We learn this classifier using a set of linguistic features analogous to those described in several previous approaches, including [13, 22, 21, 29]. In our current model, we assume that the content of a node u textually entails a node v iff there exists at least some c  X  C u such that c | = te v .
In this section, we present a comparison of four different approaches to QA which leverage the structure of a QUAB graph in order to retrieve answers to factoid questions.
In order to provide a baseline against the  X  X uthoritative X  methods of answer retrieval we introduce in this paper, we explored two different classes of QA methodologies which did not use the authority scores computed for nodes in the QUAB. (We refer to these approaches collectively as  X  X on-authoritative X  methods.)
First, we compared the performance of proposed QA meth-ods against the performance of a traditional factoid QA system designed originally for the TREC QA evaluations. This system, described in [15, 14] leverages a traditional QA pipeline of (1) question processing, (2) answer type detec-tion, (3) document/passage retrieval, (4) answer extraction, and (5) answer validation modules to return a ranked list of answers in response to factoid questions.
In a second approach, we followed [17, 11] in using mea-sures of syntactic or semantic similarity to select the top-n question nodes from the QUAB which were most likely to be associated with an answer to a submitted question q . In this paper, we focus on four different similarity mea-sures based on: (1) term overlap, (2) WordNet distance, (3) translation-based similarity, and (4) textual entailment.
Term Overlap: First, we used a similarity measure based on term overlap in order to compute the similarity between a submitted question q  X  and the question node associated 9 Values for n were selected by optimizing the performance of the entailment classifier on our training set. with each QAP in G . We defined sim ( q  X  , q i ) as the ratio of overlapping content words w  X  q to all content words ( | A | ) as in (9). where s im ( w q  X  , w i = 1) iff w q  X  = w i .

WordNet Distance: Second, following [11], we esti-mated similarity between q  X  and q i using an idf -weighted measure of WordNet distance defined as in (10). where s im ( w q  X  , w q i ) equal to the WordNet distance between lemmatized forms of w q  X  and w q i .

Translation-based Similarity: Third, we used the trans-lation-based retrieval model introduced in [17] in order to identify questions based on the likelihood that a question node q i represented a valid translation of q  X  . Under this approach, similarity between q  X  and q i was computed as where  X  was a smoothing parameter, T ( w | t ) was equal to the probability that a word t from q i was translated as w from q , and p ( t | G ) and p ( w | G ) were equal to the probability that a word t or w appeared as part of the in the QUAB G .
Textual Entailment-based Re-ranking: Finally, fol-lowing [10], we used the output of the RTE system de-scribed in Section 4 in order to rank question nodes in G based on the likelihood that they were entailed by q  X  (i.e. p ( yes, ( q  X  , q i ))).
We compared the performance of the  X  X on-authoritative X  methods against the performance of a second set of QA approaches which make use of the authority scores com-puted from a QUAB: (1) the probabilistic retrieval model introduced in Section 2.4, (2) a traditional re-ranking model which used authority scores to rank retrieved answers, (3) four similarity-based ranking approaches (based on the ap-proaches introduced in Section 5.1.2), and (4) an approach based on a pair of random walks over G .
As described in Section 2.4, we use the hub and authority scores assigned to question and answer nodes in G to retrieve the most authoritative answer nodes which correspond to the expected answer type of q  X  . First, under a traditional retrieve-and-rerank approach, a QUAB graph G  X  is generated from the top-n ranked sen-tences retrieved by a question answering system in response to a factoid question q  X  . Answer nodes in G  X  were then ranked based on the authority scores computed from the QUAB graph; the top-ranked answer that corresponds to the expected answer type of the question q is then selected as the exact answer.
Second, QAPs from G were retrieved independently us-ing each of the four similarity metrics in Section 5.1.2. Af-ter question nodes were retrieved, answer nodes adjacent to each question node were extracted from G ; answers were ranked then based on their authority scores and the top-ranked answer which satisfied the expected answer type of the question was returned as the exact answer.
In a third approach, we use a random walk over a QUAB graph G in order to identify the answer which is most likely to contain an answer to a submitted q  X  .
 We assume that a random walk can be defined as a Markov Chain (MC) cast over a QUAB graph G consisting of V nodes and E edges. The walk is specified by transition prob-abilities stored in a | V |  X  | V | transition matrix A , where the u, v th entry of A is equal to the normalized probability of a transition between a node u and a node v (i.e. p ( u  X  v )). ( A is defined as row stochastic: that is, the values assigned to all of the elements in each row in A are assumed to sum to 1.) The random walk begins with the selection of an initial node u i ( i = 0) and continues to any connected node v i + 1 with probability  X  . (We assume that the initial node u 0 selected uniformly from all of the available nodes in V with probability p ( u 0 ) = 1 / | V | .) The probability that the walk stops after k steps is computed as (1  X   X  )  X  k C k u  X  v C u  X  v is equal to the normalized value p ( u  X  v ) stored in A .

As with other random walk-based approaches to retrieval (most notably [4]), we then estimate the likelihood of walk-ing from u  X  v as a function of the weights assigned to edges as in (12).
In order to ensure that answers are selected which corre-spond to the expected answer type of q , we include a second weighting term ( p eat ( v | q )) which corresponds to the likeli-hood that either the exact answer (of an answer node) or the expected answer type (of a question node) corresponds to the expected answer type assigned to q by the QA system X  X  Answer Type Detection (ATD) module. We set p eat ( v | q ) to the classification confidence output by the ATD module for either the exact answer string ( r v ) or the expected answer type ( eat v ) of v , given the eat q . If more than one EAT is associated with q , we set p eat ( v | q ) to the maximum value output by the ATD module for any of identified EATs.
We expect that the authoritative random walk-based ap-proach described in Section 5.2.3 can be used to estimate the likelihood that there exists an answer node a i  X  G that can serve as a valid answer to a submitted question. In-stead of walking to the nodes with the highest likelihood of representing an answer to q  X  , we perform this walk by se-lecting the edge u  X  v which minimizes the likelihood that a selected node v contains information which is more rel-evant to q than u . Cast in this way, we expect the walk will traverse nodes which are increasingly more peripheral to q  X  ; if the walk does not converge after k steps, we assume that the nodes in G contain sufficient evidence to warrant a second search for an exact answer to q . In a similar fash-ion, if the walk does converge before k steps are reached, we expect that the walk has ended in one of multiple possible irrelevant X  X inks X  X nd has not encountered sufficient relevant information to suggest that an exact answer to q would be contained in G .

For this walk, we estimate the probability of walking from u  X  v as in (13):
In this section, we discuss results from experiments con-ducted using a QUAB graph constructed from the AQUAINT-
QAPs were generated automatically using the methodol-ogy described in Section 3 from a sample of 100,000 doc-uments taken from AQUAINT-2. (Documents were con-sidered if they were among the top 100 documents retrieved by [14] X  X  QA system in response to any of the questions from the 2007 TREC QA test set. In order to evaluate the im-pact of pairs generated from different underlying representa-tions, we created 4 different QUABs using the QAPs derived from (1) PropBank (PB) dependencies only, (2) PropBank and NomBank (NB) dependencies only, (3) PropBank and FrameNet (FN) dependencies only, and (4) a combination of all three semantic parsers. Duplicate questions were re-moved from consideration, even if their associated answers were different.

We then used the lightweight paraphrasing algorithm de-scribed in Section 3 to generate up to 3 additional versions of each these QUABs. (We refer to QUABs that contain the top paraphrases 11 of each question as +Para ; QUABs that contain only generated questions are referred to as -Para .) Table 1 presents a breakdown of the number of question-answer pairs included in each QUAB.
Our approach to QAP generation generated between 62 and 116 QAPs per document, depending on the choice of at http://trec.nist.gov/ threshold (  X  para ) were included;  X  para was set at 0.75 in our work. semantic parser. We found that paraphrasing increased the number of QAPs by approximately 27%; we expect that this number could be increased or decreased by setting a different value for  X  para .
We evaluated each of the QA systems described in Section 5 using a set of 200 factoid questions selected at random from the test set used as in the Text Retrieval Conference (TREC) 2007 QA evaluation. Questions were limited to those which could be assigned at least one expected answer type by [14] X  X  answer type detection module. Table 2 provides a detailed description of the accuracy of each QA system when evalu-ated on the test set. (Accuracy is defined as the percentage of questions that the system returns the correct answer for in the top position.)
In our experiments, we found that  X  X uthoritative X  QA strategies significantly improved ( p &lt; 0 . 05) performance over  X  X on-authoritative X  strategies: using  X  X uthority X  im-proved performance by an average of 4.3% across the four similarity-based QA strategies (term overlap, WordNet dis-tance, translation, and TE) and by nearly 26% when author-ity measures computed in G were used to rank candidate answers retrieved by a  X  X raditional X  QA system. While the random walk-based approach performed competitively with the  X  X uthoritative X  traditional QA strategy, the  X  X uthorita-tive X  QA methodology introduced in Section 2.4 answered nearly 65% of questions correctly, more than 24% percent-age points higher than the  X  X on-authoritative X  QA strategy.
Performance correlated weakly (R=0.42) with the num-ber of QAPs available to the system; a one-way ANOVA, however, indicated that the type of QUAB used (whether PB, PB+NB, etc.) was a significant factor impacting per-formance of all of the strategies we examined. This re-sult underscores the importance of using multiple different dependency structures when generating QAPs from text. While PropBank-style semantic parsing allowed us to gener-ate QAPs which included answers for approximately 60% of the test set, we found a substantial number of valid candi-date answers could only be extracted using our NomBank or FrameNet-based semantic parsers. We believe that a multi-strategy approach to semantic parsing is a key part of our approach to QA, as it allows us to better capture the dif-ferent ways answers are expressed in text. We expect that multiple parsers will also be important when porting our existing algorithm to new types of textual data, including  X  X aw X  webpages, blogs, or chat logs.
In this paper, we have demonstrated how a novel ques-tion answering  X  X ipeline X  which leverages large collections of question -answer pairs can be used to provide precise answers to both factoid and complex natural language ques-tions submitted by users. In our model we propose, col-lections of question-answer pairs  X  known as QUABs  X  are organized as graphs, in which relations between pairs are validated using forms of textual reasoning. These graphs enable the generation of a new relevance model which both improves the quality of answer retrieval and answer extrac-tion  X  but also allows for the determination of when no valid answer can be found for a question in a text corpus. Experi-mental results show that the authority derived from a graph of question-answer pairs can increase the performance of a factoid QA system by nearly 30% over a competitive base-line.
This material is based upon work funded in whole or in part by the U.S. Government and any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the U.S. Government. [1] C. Baker, C. Fillmore, and J. Lowe. The Berkeley [2] R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, [3] P. Blackburn and J. Bos. Representation and Inference [4] S. Brin and L. Page. The anatomy of a large-scale [5] M. Collins. Head-Driven Statistical Models for Natural [6] I. Dagan, O. Glickman, and B. Magnini. The [7] J. Eisner. Parameter estimation for probabilistic [8] D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan. [9] O. Glickman and I. Dagan. Acquiring lexical [10] S. Harabagiu and A. Hickl. Methods for Using Textual [11] S. Harabagiu, A. Hickl, J. Lehmann, and [12] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, [13] A. Hickl and J. Bensley. A discourse [14] A. Hickl, K. Roberts, B. Rink, J. Bensley, T. Jungen, [15] A. Hickl, J. Williams, J. Bensley, K. Roberts, Y. Shi, [16] R. Huddleston and G. K. Pullum. The Cambridge [17] J. Jeon, W. Croft, and J. Lee. Finding Similar [18] J. Kleinberg. Authoritative sources in a hyperlinked [19] O. Kurland and L. Lee. Respect my authority! HITS [20] X. Li and D. Roth. Learning question classifiers: The [21] B. MacCartney, T. Grenager, M.-C. de Marneffe, [22] E. Marsi, E. Krahmer, and W. Bosma.
 [23] A. Meyers, R. Reeves, C. Macleod, R. Szekely, [24] A. R. Mushegian and E. V. Koonin. A minimal gene [25] M. Palmer, D. Gildea, and P. Kingsbury. The [26] S. Shieber, G. van Noord, F. Pereira, and R. Moore. [27] B. Taskar, V. Chatalbashev, D. Koller, and [28] M. Tatu, B. Iles, J. Slavick, A. Novischi, and [29] F. Zanzotto, A. Moschitti, M. Pennacchiotti, and
