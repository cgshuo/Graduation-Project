 Click-through logs over query-document pairs provide rich and valuable information for multiple tasks in information retrieval. This paper proposes a vector propagation algo-rithm on the click graph to learn vector representations for both queries and documents in the same semantic space. The proposed approach incorporates both click and con-tent information, and the produced vector representations can directly improve ranking performance for queries and documents that have been observed in the click log. For new queries and documents that are not in the click log, we propose a two-step framework to generate the vector repre-sentation, which significantly improves the coverage of our vectors while maintaining the high quality. Experiments on Web-scale search logs from a major commercial search en-gine demonstrate the effectiveness and scalability of the pro-posed method. Evaluation results show that NDCG scores are significantly improved against multiple baselines by us-ing the proposed method both as a ranking model and as a feature in a learning-to-rank framework.
 [ Information retrieval ]: Information retrieval query pro-cessing X  Query log analysis ; [ Information retrieval ]: Re-trieval models and ranking X  Similarity measures Click-through bipartite graph, vector propagation, vector generation, Web search, query-document relevance
Incorporating user feedback is one of the most effective ways to improve a search engine. For a commercial search engine with a large audience, this can be done by logging user actions  X  the queries they issued, the results they saw, the clicks they made, and so on. This behavioral data can be used to create a training target for a machine learned ranking function, or turned into features to improve the performance of the ranking function. In fact, click-based features are used as one of the primary sources to improve the ranking quality for popular queries.

However, the click information is sometimes noisy , and the coverage of clicks is quite limited compared to all possible relevant query-document pairs, which produces sparsity for the click-based features [9]. The sparsity and noise problems impact the overall quality of click-based features, especially for less popular queries.

To deal with these problems, an effective way is to learn a vector representation for both queries and documents in the same space [12, 25, 27]. Traditional methods typically learn low-rank vectors in a latent space. Though the low rank embedding of queries and documents has its own ad-vantages, it also has some weaknesses. For example, it hurts interpretability and debuggability of the ranking function because individual dimensions in the latent space are hard to interpret. In comparison, using word features gives a more interpretable representation. However, direct text match-ing methods, e.g. BM25 [22] and the language models [28], suffer from the lexical gap between queries and documents which is shown to exist by previous study [20]. 1 Thus, how to represent both queries and documents in the same seman-tic space and explore their relevance based on the click logs, remains a challenge.

Moreover, we need an approach that can be generalized to represent the queries and documents that have never been observed in the search logs. This is especially important in real applications because new queries and documents are emerging every day. However, purely click-based features are limited by the range of previous logs, and improving their coverage is a challenge.

Finally, though a Web-scale click graph provides us with unprecedentedly rich information, it also brings in great challenge of how to consume the entire click log efficiently rather than focus on a small sample. As a result, we need
Lexical gap means that the query terms are different from the corresponding terms in relevant documents in terms of string matching. an efficient and scalable approach that can be easily applied to large scale click logs.

To solve all these challenges in a unified framework, this paper proposes a propagation approach to learn vector rep-resentation based on both content and click information (Sec-tion 2). The vector representation learned by our algorithm can directly improve the ranking performance for queries and documents that have been observed in the click log. For queries and documents that are not in the click graph, which are referred as click-absent queries and documents, we further propose a two-step vector estimation algorithm which generates a representation based on their association with the vectors that are already created by propagation on the click graph (Section 3). This generalization significantly improves the coverage of our vectors, which is particularly important for long-tail queries in web search.

This whole framework is very efficient and scalable, espe-cially compared to matrix factorization-based methods [12, 25, 27]. Since a search engine click log can grow by many millions of new entries in a single day, this scalability is essential. In the experiments (Section 4), we collect click-through log data from a major commercial search engine, which contains 25 billion query-document pairs in total, and demonstrate the effectiveness of our proposed approach used as an individual ranking model and a feature in the learn-to-rank framework. More analysis and case study about why and how the proposed approach improves ranking are dis-cussed in Section 5.

To summarize, our main contributions include:
Click-through data provides a soft indicator of relevance between queries and documents, and click-based features are one type of the most important and effective features in ranking task. However, purely click-based features highly rely on the users X  behavior, and it sometimes suffers from sparsity and noise in the click data, especially for less popu-lar queries. On the other hand, content-based features cap-ture the intrinsic relevance embedded in the text content, and they are independent of other resources such as click logs. However, the lexical gap between queries and doc-uments and the fact of missing users X  feedback make the content-based features less effective in real ranking prob-lems.

This paper bridges the two information resources and pro-poses a vector propagation algorithm which makes use of both click and content information to learn the vector pre-sentation of queries and documents. This algorithm not only introduces new words from the relevant queries and docu-ments through the propagation, but also smoothes the term weights based on the click information. Besides, the sparsity and noise are also reduced during this process thus better quality of the vector representations is guaranteed. Once q : yahoo finance
Figure 1: An example of click-through bipartite graph. we obtain the vectors for queries and documents, similarity functions such as cosine similarity can be applied to compute the query-document relevance.
Click-through data consists of queries and documents with their co-click information 2 . Let D oc be the set of documents and Q uery be the set of queries. We construct the click graph G with the node set V = D oc  X  X  uery . For a document d  X  D oc and a query q  X  Q uery , if there is at least one co-click between them, i.e., at least one user clicked on d when he or she searched for query q , an edge is built between them. The weight of the edge is set to be the number of co-clicks. The set of edges in the graph is denoted as E . An example of the click graph is shown in Figure 1. Documents are denoted as open circles and labeled by the corresponding URL, while queries are represented as solid circles.
The adjacency matrix of the graph is denoted as C where the entry in the i -th row and j -th column ( C i,j ) equals to the weight of the edge between query q i and document d j If there is no edge between them, C i,j is 0.

We denote Q as the matrix where i -th row Q i is the vector representation of query q i . Q ( n ) is the query vector matrix at the n -th iteration. If the vocabulary size is V , then the size of Q is |Q uery |  X  V . Similarly we denote D as the document vector matrix and its size is |D oc | X  V .
The goal of this vector propagation algorithm is to learn the representation of queries and documents in the same se-mantic space (either the query space or the document space). It starts with the content information initialized as vectors on one side of the click graph, and propagates the vectors to the connected nodes on the other side of the click graph. Co-clicks between queries and documents are used to weight the vectors so that the important terms stand out while less informative terms are gradually filtered out. More details are introduced as follows.

We first choose one side of the click graph (either side) and initialize the vectors based on the content information, e.g., the query words on the query side, or the titles and ab-stracts on the document side. This initialization keeps the word-level information in the vectors. Words can be consid-ered as human-designed features and past studies have re-peatedly shown that keeping the word-level representation is very helpful. In traditional content-based models such as VSM [23], vocabulary from query and document is merged together. Though it makes the vector representation easier and more natural, it may also bring in the lexical gap be-tween queries and documents [20]. Our method starts with
If a document is clicked by a user for a proposed query, it is referred as a co-clicked query-document pair in this paper. one vocabulary space, and learns the vectors for queries and documents based on one consistent vocabulary space (either query vocabulary or the document vocabulary). Thus the lexical gap is reduced.

In the next step, vectors are propagated through the click graph. Our basic idea is to represent queries based on their relevant documents and vice versa. Queries that are co-clicked with a lot of common documents should have simi-lar representations, and documents sharing many co-clicked queries should also be close in the vector space. Moreover, the click information is a soft indicator of relevance and the confidence of relevance is positively correlated to the num-ber of clicks. Thus, co-clicked pairs that have a higher fre-quency should dominate the vectors. The propagation starts from the side where vectors are initialized, then the initial-ized vectors are weighted by the clicks and propagated to the connected nodes on the other side of the click graph. For example, if we start from the query side, the document vectors are calculated by aggregating all query vectors that are co-clicked with them. In the next iteration, vectors are propagated in the opposite direction, and this process is con-ducted repeatedly until convergence.

We take starting from the query side as an example. In the initialization, each query is represented by its own words. Words are weighted in proportion to their frequency and normalized by the L 2 norm. The initialized vector matrix is denote as Q (0) . In the n -th iteration ( n  X  1), we first compute document vectors D ( n ) by a weighted summation of their co-clicked queries where the weights depend on the number of clicks. Formally, given the j -th document d j vector representation D ( n ) j is calculated as: where Q ( n  X  1) i is the vector representation for the i -th query in the ( n -1)-th iteration. D ( n ) j is also normalized by L 2 norm. Then the query vectors in Q ( n ) are updated based on their co-clicked document vectors:
Through this propagation process, we learn the vectors for both queries and documents using the query vocabulary. Without the lexical gap, better query-document relevance can be achieved. Similarly, if we start from the document side, document vectors (denoted as D (0) ) are initialized by using their content words, and the vector propagation starts from the document side to the query side in each iteration:
Take the click graph in Figure 1 as an example. If we start from the query side, Q 1 , Q 2 and Q 3 are initialized as and { yahoo: 1  X  2 , finance:0, mail: 1  X  2 } respectively. Then document vectors are derived from the co-clicked queries. For instance, the vector representation for d 1 ( D 1 culated as ( 3 8 Q 1 + 5 8 Q 2 ) / || 3 8 Q 1 + 5 8 Q 2 || the document side, we first initialize the document vectors by their titles:  X  X ahoo Finance -Business Finance, Stock Market, Quotes, News X  and  X  X ahoo X  for d 1 and d 2 respec-tively. 3 Then the document vectors are propagated to the query side and the query vectors are generated in a similar way. Because of the lexical gap between queries and docu-ments, the learned vectors starting from different sides are indeed different, and in fact they complement each other in the real ranking system. More analysis is in Section 5.
The proposed vector propagation is similar to the impor-tance propagation in HITS algorithm [14]. In HITS algo-rithm, mutual reinforcement bonds the authority and hub scores through the Web link structure, while our propaga-tion algorithm aims at mutual improvement of query and document vector representations. Instead of propagating importance scores, we propagate the feature vectors which are derived from the text content. In this way, both content and click information are encoded in the learned vectors.
In practice, we represent each vector in a sparse way (only keeping the terms with non-zero weights), which helps to in-crease the computational speed. However, a potential prob-lem in this propagation algorithm is the rapid growth of the non-zero entries, which leads to a considerable increase in time and space consumption in a Web-scale scenario. How-ever, if we rank the terms in a vector according to their weights, we can clearly see that the vectors have long tails: the weights drop significantly after the first few terms, and the remaining terms in the vector have very small weights (examples can be found in Section 5.1). Thus we can sim-ply keep the top K terms with the highest weights for both query and document vectors in each iteration to further re-duce both time and space consumption.
Though the vector propagation algorithm proposed in Sec-tion 2 enables us to get the relevance feature for any query-document pair in the click graph, it still cannot handle queries or documents that have no click information. As a matter of fact, the ability to deal with click-absent queries and documents is of great importance in real applications, since we can always see new queries and documents that are not included in previous logs.

An intuitive way is to use bag-of-words model to generate their vector representations, e.g., use the query words to rep-resent the queries, and use the title words to represent the documents. However, it introduces the lexical gap between queries and documents that we try to avoid in this paper, and it is limited to the content words instead of expand-ing the vectors with additional related terms. Besides, it also breaks the indirect connections between the click-absent queries/documents and the documents/queries in the click graph, thus reweighting cannot be learned from the clicks.
To better estimate the vectors for click-absent queries and documents, this section proposes a novel and efficient two-step framework. We first break down queries or docu-ment titles (depending on from which side we start from in the vector propagation algorithm) into different units (e.g.,
Titles are extracted from the HTML sources. ngrams), and learn a vector representation for each unit based on the vectors we have already learned from the click graph. We then learn a weight for each unit by a regression model, and finally estimate the vectors for the click-absent queries and documents by a linear combination of the unit vectors. This framework connects the click-absent queries and documents to the click graph through the unit vectors and gets high-quality representations for them, thus enlarges the coverage of the vectors.
Vectors learned by the propagation algorithm keep word-level features, which provides a potential bridge to connect the click-absent queries and documents with those in the click graph. Even though we cannot exactly match a new query or document with an existing one in the search logs, the key words or phrases of these new queries and documents may be already contained in the observed queries and doc-uments. These key terms (such as unigrams, bigrams and trigrams) can be treated as basic message units in both click-absent and existing queries and documents, and we can get these ngram units from the text content. More specifically, if the propagation starts from the query side, all the queries are broken into unigrams, bigrams and trigrams. Similarly, document contents such as titles or abstracts are decom-posed to ngrams if we use document vocabulary. The set of units is denoted as U .

The next step is to learn a vector representation for each unit. In the vector propagation algorithm (Section 2), query vectors are learned by aggregating document vectors accord-ing to their co-clicking relationship, and we can learn the unit vectors in a similar way. Let X  X  assume the vector prop-agation starts from the query side and first build up the unit vectors in the query vocabulary space. For each unit u i U , we find all the queries containing the unit u note the set as O u i . We also find all the documents that are connected to any of these queries, and denote the document vector set as K u i . Next, we connect u i with vectors in K based on the click information: for the k -th vector in O and the j -th vector in K u i (denoted as O u i ( k ) and K respectively), if there is a click between their corresponding query and document, we say that there is a pseudo click be-tween u i and K u i ( j ) through O u i ( k ), denoted as P value P i,k,j equals to the query-document co-click number C k,j . If we aggregate all the pseudo clicks between u i and K u i ( j ) over all queries in O u i , we get the pseudo click be-tween u i and K u i ( j ) and denote it as P i,j : As the example shown in Figure 2, if u i is  X  X ahoo X  and the propagation starts from the query side, O u i is the set of queries that contains the unit  X  X ahoo X . In this example, we have three queries in O u i , which are q 1 , q 2 spectively. d 1 and d 2 are found to be co-clicked documents for these three queries, so their vectors constitute K u i pseudo clicks are built between them and u i with aggregated weights. If the unit is a bigram or trigram, we only consider the queries that contain the unit as a bigram or trigram. For example, if the unit is  X  X ahoo finance X , then queries contain-ing  X  X ahoo X  and  X  X inance X  as separate unigrams (e.g.,  X  X ahoo news in finance X ) are not included.

Finally, the vector representation of u i (denoted as U i calculated by a weighted sum of vectors in K u i , normalized by L 2 norm:
Similarly, if the vector propagation starts from the docu-ment side, we extract all the units from the document con-tent. Then for each unit u i , we find all the documents con-taining the unit u i and denote the set as O u i , then find all the queries that are connected to any of these documents and denote their vectors as K u i . We then can follow the same process to build up the connections between units and the query vectors to estimate the unit vectors in the document vocabulary space.

As we can see from this process, these unit vectors are generated from the related query or document vectors that are learned by the propagation algorithm in the click graph, so click information is also embedded in them.
In the next step, we learn a weight per unit to capture the importance for each unit globally, which is later used for generation of vectors for click-absent queries and docu-ments. Here we learn the weights through a linear regression model, and the vectors where the units originate from are used as training examples (named as target vectors for con-venience). The basic idea is to use the linear combination of unit vectors to approximate the target vectors, and then minimize the difference between the target vector and the approximated vector which is measured by the square of Euclidean distance: where T is the set of target vectors and T i is the i -th vector in T . If the units originates from queries, T consists of all the query vectors learned by the propagation algorithm; otherwise it contains all the document vectors. U all T i of all the units (unigrams, bigrams and trigrams, excluding itself) contained in the corresponding query or document content of T i . W is the weight vector with the j -th entry W j representing the weight for u j in the unit set U .
In Section 3.1 and Section 3.2, we introduce how to gener-ate the unit vectors and weights respectively. Given a new query or document without any click information, we can represent it by the weighted combination of unit vectors.
For a click-absent query q , we first decompose it into all possible units. If a unigram is contained in a bigram, or a bigram is contained a trigram, it is removed to avoid over-lapping information. For example, given a new query,  X  X al-mart credit card X , assume the set of unigrams, bigrams and trigrams contained in unit vocabulary includes {  X  X almart X ,  X  X redit X ,  X  X ard X ,  X  X redit card X  } , then we only keep  X  X almart X  and  X  X redit card X  in the unit set. The final unit set which is decomposed from q by the above rule is denoted as U q . Then the query vector q v is calculated as a linear combination of the unit vectors in U q :
For a click-absent document d , we can estimate its vec-tor representation in a similar way. Any possible content of documents can be used, and here we use document titles, which are concise yet provide key information. We decom-pose the title of d to a unit set U d according to the same rule as mentioned above, and the vector d v is calculated as:
Note that this generation algorithm estimates the vectors either in query vocabulary space or in the document vocab-ulary space, depending on how the unit vectors are built. Based on the vector propagation algorithm and the above vector generation algorithm, we now represent all queries and documents in the same semantic space. Relevance be-tween any query-document pair can be measured by similar-ity functions such as cosine similarity, which is further used as a feature for ranking.
We apply the proposed method to a Web-scale click graph from a major commercial search engine. The learned rele-vance score can be either used directly to rank documents for a given query or employed as one of the features in a learning-to-rank framework. We show that the proposed method helps to improve ranking results in both cases.
We construct the click-through bipartite graph from a ma-jor commercial search engine X  X  search log. There are about 25 billion co-clicked query-document pairs, containing about 8 billion unique queries and 3 billion unique documents.
To investigate whether the relevance score learned by our algorithm can help to improve ranking in a learning-to-rank framework, we use another dataset that includes 63k queries and 775k query-document pairs as training examples and 16k queries with 243k query-document pairs as test set. The relevance score of each pair ( X  X erfect X ,  X  X xcellent X ,  X  X ood X ,  X  X air X  or  X  X ad X ) is annotated by human annotators. 92.5% of these testing queries can be found in the search log and the others do not have click information. 78.9% of the testing documents are in the search log. For the 21.1% documents missing in the search log (about 51k documents), 91.7% of them have titles.
To give more insight into how well the proposed method captures the relevance between queries and documents, we use the relevance scores to rank documents and calculate NDCG scores to evaluate the performance. The trimming parameter K in vector propagation algorithm is set as 20. We compare our methods against multiple baseline methods:
As shown in Table 1, our methods VPCG and VPCG&amp;VG in general achieve the best performance. VPCG QUERY performs even better than VPCG DOC, most likely because titles sometimes contain noisy words or are not descriptive of page content. However, VPCG QUERY and VPCG DOC capture the query-document similarity from different view-points and complement each other, which is further dis-cussed in Section 5. The generalized version VPCG&amp;VG QUERY
Multiple experiments are done to evaluate the sensitivity of the selection of K, but it turns to be not very sensitive, thus we use K = 20 in this paper.
The Google News word vectors are available in https:// code.google.com/p/word2vec/ Table 1: Performance as an individual ranking model. BM25SD MULT 0.6132 0.6346 0.6668 0.7464 VPCG QUERY 0.6268 0.6498 0.6797 0.7509 VPCG&amp;VG QUERY 0.6344 * 0.6618 * 0.6948 * 0.7687 * VPCG&amp;VG DOC 0.5668 0.6268 0.6717 0.7509 and VPCG&amp;VG DOC further improve the results of VPCG QUERY and VPCG DOC respectively, because the feature coverage is improved. The CTR feature used here is a very accurate predictor of relevance, but it is only available for query-document pairs that have been observed in the log, so it does not perform well on the click-absent ones.
A typical industrial learning-to-rank setting builds pow-erful nonlinear models based on a large set of high quality features [7]. Indeed, these models can often combine many weak features to outperform a single strong feature [26]. It is therefore challenging to create a new feature that signifi-cantly improves the overall model performance. Here we use a variant of the gradient boosting machine proposed in [11] to implement a learning-to-rank framework with over 2,400 features. 6
We denote all the 2,400+ basic features as  X  X ase X . This basic feature pool contains an immense variety of features, including CTR, BM25SD and their variations, etc. We then extend the feature pool by adding new features, and the performances of the learning-to-rank model trained based on different feature pools are shown in Table 2.  X  X ase0 X  is a subset of the standard feature pool which excludes CTR and BM25SD MULT.  X  X ase + WMD X  means the standard fea-ture pool plus WMD feature.  X  X ase + VPCG X  extends the feature pool by adding VPCG QUERY and VPCG DOC, and  X  X ase + VPCG&amp;VG X  means adding VPCG&amp;VG QUERY and VPCG&amp;VG DOC to the feature pool.

As shown in Table 2, removing the powerful features CTR and BM25SD MULT does not change the model perfor-mance much, since the remaining features in the feature pool have already contributed similar inputs to the ranking sys-tem. The result is just slightly increased by adding WMD. This is likely because WMD introduces a notion of semantic similarity between different words that is not present in most text matching features. In general, the differences between the performance of  X  X ase0 X ,  X  X ase X  and  X  X ase + WMD X  are all very marginal.

By contrast, we do observe a significant improvement in overall model performance after adding our features to the pool. Our features make good use of both click and con-tent information, and bring in new information which is not presented in the existing feature set. It is interesting that the performance difference between VPCG&amp;VG and VPCG
For both space and confidentially reasons, we cannot here define the complete set of features.
 Table 2: Performance as a feature in a learning-to-ranking framework.
 Base + VPCG 0.7068 0.7207 0.7470 0.8080 Base + VPCG&amp;VG 0.7096 0.7256 * 0.7506 * 0.8108 *
Impo rtance score is narrowed in the learning-to-rank framework compared to when using them as individual ranking models (Table 1). This is because the learning-to-rank framework is able to rely on other features when the  X  X PCG X  feature is missing. Another reason is that only a small portion (about 7.5%) of our test queries are click-absent, limiting the impact of the VG method on this data set. However, we do find click-absent queries where the ranking is significantly improved by using the VG method. More detailed discussion is provided in Section 5.

To further investigate the impact of different features, we calculate the relative importance score for all the fea-tures [11], where the most influential feature is assigned a score of 100 and the estimated scores for others are scaled ac-cordingly. The distribution of the relative importance scores for the top 15 features are shown in Figure 3. We are not able to provide details about the features for confidential-ity reasons, but we can still conclude that our features in-deed play an important role in the retrieval system, since VPCG&amp;VG DOC and VPCG&amp;VG QUERY rank as the top two best features. CTR and BM25SD MULT rank as the fourth and fifth respectively. The importance scores drop dramatically between the second and the third one, while the difference between other pairs are much smaller, indicat-ing the superiority of our features. This is further evidence that our features significantly improve the performance both as a single feature and in the learning-to-rank models. Feature Value
In Section 4.2 and Section 4.3, we show significant im-provement on NDCG scores by our methods in all exper-iment settings. In this section, we look into more details about how our features work to distinguish different docu-ments according to their relevance to a given query.
As mentioned in Section 4.1, each document is labeled with one of the five degrees of relevance, namely  X  X erfect X ,  X  X xcellent X ,  X  X ood X ,  X  X air X  and  X  X ad X . To better evaluate the features, we measure the  X  X eparability X  of the features at these five labels: we randomly sample 500 tuples (  X  query, url, feature value  X  ) for each of the five labels from the test dataset, sort the tuples by their feature values, and plot them in Figure 4. Each point (x,y) in this figure corre-sponds to a tuple where X -coordinate is its ranking and Y -coordinate is its feature value. For the convenience of illustration, BM25SD and BM25SD MULT are normalized by its maximal value so that it is scaled between 0 and 1. Note that the normalization does not impact the ranking. We can see that CTR separates  X  X erfect X  from others, and WMD distinguishes  X  X ad X  from others. BM25SD is good at identifying  X  X xcellent X , but its ability of distinguishing oth-ers are not as good as the enhanced version BM25SD MULT. Compared to the baseline methods, both VPCG&amp;VG QUERY and VPCG&amp;VG DOC have a much stronger separability over all the five labels. Especially, VPCG&amp;VG QUERY is better at distinguishing  X  X erfect X ,  X  X xcellent X ,  X  X ood X  and  X  X air X  from  X  X ad X , while VPCG&amp;VG DOC is better at seper-ating  X  X erfect X ,  X  X xcellent X  and  X  X ood X  from  X  X air X  and  X  X ad X ; and VPCG&amp;VG QUERY has a slightly better seperabil-ity among  X  X erfect X ,  X  X xcellent X  and  X  X ood X , which explains why VPCG&amp;VG QUERY performs better as a single fea-ture while the learning-to-rank framework benefits from the combination of them.
To get a better understanding about the vectors gener-ated by our method, we study some query vectors in details and analyze how they influence the ranking result. Besides, we also further evaluate the vector generation algorithm for click-absent queries and documents, and illustrate how the generated vectors help to improve the ranking quality.
The main change of query and document vectors made by our propagation method lies in two aspects: the introduc-tion of latently related words and the re-weighting of vector terms. Introducing related words helps to explain the query or document better and re-weighting makes the key terms stand out, which facilitates tasks like disambiguation and query intent understanding.

Table 3 shows several examples of the learned query vec-tors (vector propagation starts from query side). We can see that keywords such as  X  X agles X ,  X  X lbum X  and  X  X yrics X  can help us to distinguish the query relevant to the song named  X  X otel california X  from an alike query  X  X otel california palm springs hotel X  which is about a real hotel. In the third example, the meaning of  X  X td amc X  is hard to figure out even for a human user who is not familiar with the background knowledge. Actually  X  X td X  means  X  X owntown Disney X  and the query is asking about AMC theaters in Downtown Disney. In this ex-ample, informative keywords such as  X  X isney X ,  X  X owntown X ,  X  X heater X  and  X  X ovie X  are introduced in the learned vector. As shown in the table, ranking model trained without fea-ture generated by the vector propagation algorithm (Base) performs very badly on this query (two of the top three are  X  X ad X ). When adding our feature, the result gets much bet-ter: both the top two retrieved documents are  X  X ood X  and the third one is  X  X air X .
Our proposed vector propagation algorithm is very flex-ible to start from either query or document side, denoted as VPCG QUERY and VPCG DOC respectively. To study the difference between the two features and how they com-plement each other, we take the query  X  X hoise fm X  as an ex-ample (Table 4).  X  X hoise fm X  is an urban music radio station owned by Global Radio and anchored by an FM operation in London, and it is now changed to  X  X apital xtra X . Be-cause this query is the old name of the music radio station, if we start from the query side, our algorithm propagates the old information  X  X hoise fm X  to the document side and learns larger weights for  X  X hoice X  and  X  X m X . Though the re-lationship between  X  X hoise fm X  and  X  X apital xtra X  is captured in the vector, it has much smaller weights for  X  X apital X  and  X  X tra X . Thus the top three ranking results are all about  X  X hoise fm X , and it fails to rank the perfect URL  X  X ttp: //www.capitalxtra.com/ X  in top 3. However, if we start from the document side, the document title includes the new name  X  X apital xtra X  and our algorithm propagates such information to the query side  X  X hoise fm X . Thus the query vector from VPCG DOC not only captures the relationship between  X  X hoise fm X  and  X  X aptial xtra X , and learns more ac-curate weights for  X  X apital X  and  X  X tra X . As a result, with the help from both VPCG QUERY and VPCG DOC, our model is able to rank the perfect URL  X  X ttp://www.capitalxtra. com/ X  as the top 1 result. Similarly, VPCG QUERY may capture more accurate query intent than VPCG DOC for some queries, and due to space limit, we do not list more ex-amples here. In general, VPCG QUERY and VPCG DOC complement each other and together improve the ranking results as shown in Section 4.
The goal of the vector generation method proposed in Sec-tion 3 is to get a good vector estimation for queries and documents that have no click information. In Section 4.2 and Section 4.3, we have already shown that those gener-ated vectors are indeed helpful for ranking. Here we further look into these generated vector representations to see how well they approximate the target vectors.

We use the query vectors generated by the vector propaga-tion algorithm starting from the query side as an example for the evaluation. The set is divided into two subsets, namely training set and test set. The training set is used to learn the unit vectors and their corresponding weights, based on which vectors for the test set are generated by the vector generation algorithm. The vectors learned by the vector propagation algorithm are used as ground truth. The evalu-ation metrics is the average of cosine similarity between the groundtruth vector and the vector generated by methods that estimate the vectors in other alternative ways.
We compare our method  X  X G X  against several baselines, and the results are shown in Table 5. Here  X  X OW X  is the bag-of-words model.  X  X nigram vector + equal weight X  uses a linear combination of the vectors for each unigram with equal weights. For example, the query vector for  X  X ahoo finance X  is calculated as Q ( X  X ahoo X ) + Q ( X  X inance X ) where Q ( X  X ahoo X ) and Q ( X  X inance X ) are vectors for query  X  X ahoo X  and  X  X inance X  which are learned by the vector propagation algorithm.  X  X nit vector + equal weight X  uses the unit vec-tors we introduce in Section 3. It decomposes queries into a set of units in the same way as  X  X G X  does, but assigns equal weight to each unit instead.

From Table 5, we can see that methods making use of existing query vector information ( X  X nigram vector + equal weight X ,  X  X nit vector + equal weight X  and  X  X G X ) outperform bag-of-words model. Using unit vectors works better than using original vectors for unigram.  X  X G X  brings in further improvement compared to assigning equal weights to units. In general, our proposed generalization algorithm provides the most accurate approximation of the ground truth vector.
In previous sections, we have shown the quantitative anal-ysis of the vector generation algorithm, including its perfor-mance in ranking and the quality of the estimated vectors. In this section, we continue our discussion about how it helps to improve ranking by illustrating a query example  X  X ow long is into the storm X , which is about the length of a movie  X  X nto the Storm X . This query is not in the search log so we can only use the vector generation method.

To generate the query vector,  X  X ow long is into the storm X  is decomposed into three units, namely  X  X ow long is X ,  X  X s into the X  and  X  X nto the storm X , whose vectors are given in Table 6. The keyword  X  X ovie X  is added to the unit vector for  X  X nto the storm X  with a high weight, which increases the relevance scores of movie-related documents. Besides, our regression model successfully captures that  X  X nto the storm X  is the most influential unit in this query and gives the high-est weight to it. In this way, the generated query vector is more likely to be connected with documents about the corresponding movie. In our experiment, the learning-to-rank framework trained without VG (Base + VPCG) ranks a science news from NASA which is not relevant at all as a top result (number three). With the help of VG (Base + VPCG&amp;VG), we get more reasonable results, where all the top three documents are about the movie (Table 6).
The most straightforward way to exploit the click log from a search engine is to aggregate statistics such as click prob-ability and dwell time by ( query, document ) pairs and use them as features in a learning-to-rank framework, as de-scribed by Agichtein et al. in [1].

To deal with the noise and sparsity problem in these fea-tures, there are two basic approaches. One solution is to cast it as a smoothing problem, as in [9], [13], and [29]. An alternative way is to learn a semantic representation of queries and documents, either in vocabulary space or in a latent space, as in [12], [25], and [27]. One advantage of the semantic representation approach is its flexibility. It is eas-ier to incorporate other useful information in the learning of the semantic representation. Besides, the relevance between queries and documents only depends on the representation, so similar documents always get close relevance scores, which reduces the bias caused by the raw click-through data. The semantic approach is the one adopted in this paper.
It is common to represent a click log as a graph with edges representing clicks, an early example being Baeza-Yates and Tiberi X  X  work [3]. Our approach is most similar to the label propagation technique of [16]. However, instead of requiring extrinsic labels or features, we propagate parts of the content information (i.e., query or document title) itself.
The semantic click models that we are aware of either make use of modeling techniques that can be difficult to scale to billions of samples, such as statistical machine trans-lation [12], and SVD [27], or else have very many hyperpa-rameters that are difficult to tune [12, 25]. By contrast, our method is strikingly simple and exceedingly scalable. It also has the advantage of producing a representation  X  a small set of weighted terms from queries or documents  X  that is amenable to human inspection.

Besides ranking, click-through data can be utilized for var-ious tasks in information retrieval. Click-through history records user behavior from which user X  X  information need can be discovered [24]. It also provides clues to discover potential relations between queries, and can be used to fa-cilitate tasks such as query clustering [4] and query sugges-tion [2, 10, 17, 18]. Click-through data can also be explored for document representation and organization. In [21], the authors represent documents in the query vocabulary space, where TFIDF is used for weighting. Compared to their method, ours improves both the query and document vector representation by involving more iteractions between them.
This paper presents a unified approach to learn query-document relevance by utilizing both click and content in-formation. For queries and documents in the click graph, we propagate the content information from either side on the click-through bipartite graph and represent both queries and documents as vectors in the same semantic space. For queries and documents without click information, their vec-tor representations are generated by connecting them with the vectors learned from the click graph based on the con-tent information. This whole process expands the vector representations with relevant new words through the prop-agation, and smoothes the term weights based on the click information. Besides, the sparsity and noise are also reduced during this process thus better quality and larger coverage of the vectors is guaranteed. The proposed approach works efficiently on a Web-scale click graph. The experimental results show the effectiveness of the proposed method in a ranking task. It stands out from thousands of features in a learning-to-rank framework, which is a nontrivial achieve-ment in real practice.

Besides ranking, one potential application of our method is query and document expansion. The learned vectors usu-ally consist of related terms propagated from their neighbors in the click graph, which can be further applied to improve automatic expansion of queries and documents in the search task. [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [2] R. Baeza-Yates, C. Hurtado, and M. Mendoza. Query [3] R. Baeza-Yates and A. Tiberi. Extracting semantic [4] D. Beeferman and A. Berger. Agglomerative clustering [5] M. Bendersky, D. Metzler, and W. B. Croft. Effective [6] A. Broder, E. Gabrilovich, V. Josifovski, [7] O. Chapelle and Y. Chang. Yahoo! learning to rank [8] K. Collins-Thompson and J. Callan. Query expansion [9] N. Craswell and M. Szummer. Random walks on the [10] H. Deng, M. R. Lyu, and I. King. A generalized co-hits [11] J. H. Friedman. Greedy function approximation: A [12] J. Gao, X. He, and J. Nie. Clickthrough-based [13] J. Gao, W. Yuan, X. Li, K. Deng, and J. Nie.
 [14] J. M. Kleinberg. Authoritative sources in a [15] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. [16] X. Li, Y.-Y. Wang, and A. Acero. Learning query [17] H. Ma, H. Yang, I. King, and M. R. Lyu. Learning [18] Q. Mei, D. Zhou, and K. Church. Query suggestion [19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [20] C. M  X  uller and I. Gurevych. A study on the semantic [21] B. Poblete and R. Baeza-Yates. Query-sets: using [22] S. E. Robertson, S. Walker, S. Jones, [23] G. Salton, A. Wong, and C.-S. Yang. A vector space [24] X. Shen, B. Tan, and C. Zhai. Context-sensitive [25] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. [26] K. M. Svore and C. J. Burges. A machine learning [27] W. Wu, H. Li, and J. Xu. Learning query and [28] C. Zhai and J. Lafferty. A study of smoothing [29] M. Zhukovskiy and T. Khatkevich. An optimization
