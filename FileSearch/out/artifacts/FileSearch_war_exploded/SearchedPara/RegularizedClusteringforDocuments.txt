 In recen t years, documen t clustering has been receiving more and more atten tions as an importan t and fundamen tal tech-nique for unsup ervised documen t organization, automatic topic extraction, and fast information retriev al or  X ltering. In this paper, we prop ose a novel metho d for clustering doc-umen ts using regularization. Unlik e traditional globally reg-ularized clustering metho ds, our metho d  X rst construct a local regularized linear label predictor for each documen t vector, and then combine all those local regularizers with a global smoothness regularizer. So we call our algorithm Clustering with Local and Global Regularization ( CLGR ). We will show that the cluster mem berships of the docu-ments can be achieved by eigen value decomp osition of a sparse symmetric matrix, which can be e X cien tly solved by iterativ e metho ds. Finally our experimen tal evaluations on several datasets are presen ted to show the superiorities of CLGR over traditional documen t clustering metho ds. H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al| Clustering ; I.2.6 [ Arti X cial Intelli-gence ]: Learning| Conc ept Learning Algorithms Documen t clustering, Regularization  X 
The work of Fei Wang, Changsh ui Zhang is supp orted by the China Natural Science Foundation No. 60675009. The work of Tao Li is partially supp orted by NSF IIS-0546280 and NIH/NIGMS S06 GM008205.
 Cop yright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00.
Documen t clustering has been receiving more and more atten tions as an importan t and fundamen tal technique for unsup ervised documen t organization, automatic topic ex-traction, and fast information retriev al or  X ltering. A good documen t clustering approac h can assist the computers to automatically organize the documen t corpus into a mean-ingful cluster hierarc hy for e X cien t browsing and navigation, which is very valuable for complemen ting the de X ciencies of traditional information retriev al technologies. As pointed out by [8], the information retriev al needs can be expressed by a spectrum ranged from narro w keyword-matc hing based search to broad information browsing such as what are the major international events in recen t months. Traditional documen t retriev al engines tend to  X t well with the search end of the spectrum, i.e. they usually provide speci X ed searc h for documen ts matc hing the user's query , however, it is hard for them to meet the needs from the rest of the spectrum in which a rather broad or vague information is needed. In such cases, e X cien t browsing through a good cluster hierarc hy will be de X nitely helpful.

Generally , documen t clustering metho ds can be mainly categorized into two classes: hierarc hical metho ds and par-titioning metho ds. The hierarc hical metho ds group the data points into a hierarc hical tree structure using bottom-up or top-do wn approac hes. For example, hierarc hical agglomera-tive clustering ( HAC ) [13] is a typical bottom-up hierarc hi-cal clustering metho d. It takes each data point as a single cluster to start o X  with and then builds bigger and bigger clusters by grouping similar data points together until the entire dataset is encapsulated into one  X nal cluster. On the other hand, partitioning metho ds decomp ose the dataset into a number of disjoin t clusters which are usually opti-mal in terms of some prede X ned criterion functions. For in-stance, K-me ans [13] is a typical partitioning metho d which aims to minimize the sum of the squared distance between the data points and their corresp onding cluster centers. In this paper, we will focus on the partitioning metho ds.
As we know that there are two main problems existing in partitioning metho ds (like Kmeans and Gaussian Mix-ture Model ( GMM ) [16]): (1) the prede X ned criterion is usu-ally non-con vex which causes many local optimal solutions; (2) the iterativ e procedure ( e.g. the Expectation Maximiza-tion ( EM ) algorithm) for optimizing the criterions usually makes the  X nal solutions heavily depend on the initializa-tions. In the last decades, many metho ds have been pro-posed to overcome the above problems of the partitioning metho ds [19][28].

Recen tly, another type of partitioning metho ds based on clustering on data graphs have aroused considerable inter-ests in the machine learning and data mining comm unity. The basic idea behind these metho ds is to  X rst model the whole dataset as a weighted graph, in which the graph nodes represen t the data points, and the weights on the edges cor-respond to the similarities between pairwise points. Then the cluster assignmen ts of the dataset can be achieved by optimizing some criterions de X ned on the graph. For exam-ple Spectral Clustering is one kind of the most represen tativ e graph-based clustering approac hes, it generally aims to opti-mize some cut value ( e.g. Normalize d Cut [22], Ratio Cut [7], Min-Max Cut [11]) de X ned on an undirected graph. After some relaxations, these criterions can usually be optimized via eigen-decomp ositions, which is guaran teed to be global optimal. In this way, spectral clustering e X cien tly avoids the problems of the traditional partitioning metho ds as we introduced in last paragraph.

In this paper, we prop ose a novel documen t clustering al-gorithm that inherits the superiorit y of spectral clustering, i.e. the  X nal cluster results can also be obtained by exploit the eigen-structure of a symmetric matrix. However, unlik e spectral clustering, which just enforces a smoothness con-strain t on the data labels over the whole data manifold [2], our metho d  X rst construct a regularized linear label predic-tor for each data point from its neigh borhood as in [25], and then combine the results of all these local label predic-tors with a global label smoothness regularizer. So we call our metho d Clustering with Local and Global Regularization ( CLGR ). The idea of incorp orating both local and global information into label prediction is inspired by the recen t works on semi-sup ervised learning [31], and our experimen-tal evaluations on several real documen t datasets show that CLGR performs better than many state-of-the-art clustering metho ds.

The rest of this paper is organized as follows: in section 2 we will introduce our CLGR algorithm in detail. The exper-imen tal results on several datasets are presen ted in section 3, followed by the conclusions and discussions in section 4.
In this section, we will introduce our Clustering with Local and Global Regularization ( CLGR ) algorithm in detail. First let's see the how the documen ts are represen ted throughout this paper.
In our work, all the documen ts are represen ted by the weighted term-frequency vectors. Let W = f w 1 ; w 2 ;  X  X  X  ; w be the complete vocabulary set of the documen t corpus (whic h is prepro cessed by the stopwords remo val and words stemming operations). The term-frequency vector x i of doc-umen t d i is de X ned as where t ik is the term frequency of w k 2 W , n is the size of the documen t corpus, idf k is the number of documen ts that contain word w k . In this way, x i is also called the TF-IDF represen tation of documen t d i . Furthermore, we also normalize each x i (1 6 i 6 n ) to have a unit length, so that each documen t is represen ted by a normalize d TF-IDF vector.
As its name suggests, CLGR is comp osed of two parts: local regularization and global regularization . In this subsec-tion we will introduce the local regularization part in detail.
As we know that clustering is one type of learning tech-niques, it aims to organize the dataset in a reasonable way. Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classi X ca-tion function that will assign labels to the training dataset and even the unseen testing dataset with some cost mini-mized [24]. For example, in the two-class classi X cation sce-nario 1 (in which we exactly know the label of each docu-ment), a linear classi X er with least square  X t aims to learn a column vector w such that the squared cost is minimized, where y i 2 f +1 ;  X  1 g is the label of x i taking @ J =@ w = 0, we get the solution which can further be written in its matrix form as where X = [ x 1 ; x 2 ;  X  X  X  ; x n ] is an m  X  n document matrix , y = [ y 1 ; y 2 ;  X  X  X  ; y n ] T is the label vector . Then for a test documen t t , we can determine its label by where sign (  X  ) is the sign function.

A natural problem in Eq.(3) is that the matrix XX T may be singular and thus not invertable ( e.g. when m  X  n ). To avoid such a problem, we can add a regularization term and minimize the following criterion where  X  is a regularization parameter. Then the optimal solution that minimize J 0 is given by where I is an m  X  m identity matrix. It has been reported that the regularized linear classi X er can achieve very good results on text classi X cation problems [29].

However, despite its empirical success, the regularized lin-ear classi X er is on earth a global classi X er, i.e. w  X  is esti-mated using the whole training set. According to [24], this may not be a smart idea, since a unique w  X  may not be good enough for predicting the labels of the whole input space. In order to get better predictions, [6] prop osed to train classi- X ers locally and use them to classify the testing points. For example, a testing point will be classi X ed by the local classi- X er trained using the training points located in the vicinit y
In the following discussions we all assume that the docu-ments coming from only two classes. The generalizations of our metho d to multi-class cases will be discussed in section 2.5.
 of it. Although this metho d seems slow and stupid, it is reported that it can get better performances than using a unique global classi X er on certain tasks [6].
Inspired by their success, we prop osed to apply the local learning algorithms for clustering. The basic idea is that, for each documen t vector x i (1 6 i 6 n ), we train a local label predictor based on its k -nearest neigh borhood N i , and then use it to predict the label of x i . Finally we will combine all those local predictors by minimizing the sum of their prediction errors. In this subsection we will introduce how to construct those local predictors.

Due to the simplicit y and e X ectiv eness of the regularized linear classi X er that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each documen t x i , the following criterion is minimized where n i = jN i j is the cardinalit y of N i , and q j is the clus-ter mem bership of x j . Then using Eq.(6), we can get the optimal solution is where X i = [ x i 1 ; x i 2 ;  X  X  X  ; x in i ], and we use x k -th nearest neigh bor of x i . q i = [ q i 1 ; q i 2 ;  X  X  X  ; q q ik represen ting the cluster assignmen t of x ik . The problem here is that X i X T i is an m  X  m matrix with m  X  n i , i.e. we should compute the inverse of an m  X  m matrix for ev-ery documen t vector, which is computationally prohibited. Fortunately , we have the following theorem: Theorem 1 . w  X  i in Eq.(8) can be rewritten as where I i is an n i  X  n i identity matrix.
 Proof. Since then Let then Therefore
Using theorem 1, we only need to compute the inverse of an n i  X  n i matrix for every documen t to train a local label predictor. Moreo ver, for a new testing point u that falls into N , we can classify it by the sign of This is an attractiv e expression since we can determine the cluster assignmen t of u by using the inner-pro ducts between the points in f u [ N i g , which suggests that such a local regularizer can easily be kernelize d [21] as long as we de X ne a prop er kernel function.
After all the local predictors having been constructed, we will combine them together by minimizing which stands for the sum of the prediction errors for all the local predictors. Com bining Eq.(10) with Eq.(6), we can get where q = [ q 1 ; q 2 ;  X  X  X  ; q n ] T , and the P is an n  X  n matrix constructing in the following way. Let then where P ij is the ( i; j )-th entry of P , and  X  i j represen ts the j -th entry of  X  i .

Till now we can write the criterion of clustering by com-bining locally regularized linear label predictors J l in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques. However, the results may not be good enough since we only exploit the local informations of the dataset. In the next subsection, we will introduce a global regularization criterion and combine it with J l , which aims to  X nd a good clustering result in a local-global way.
In data clustering, we usually require that the cluster as-signmen ts of the data points should be su X cien tly smooth with respect to the underlying data manifold, which implies (1) the nearb y points tend to have the same cluster assign-ments; (2) the points on the same structure ( e.g. subman-ifold or cluster) tend to have the same cluster assignmen ts [31].

Without the loss of generalit y, we assume that the data points reside (roughly) on a low-dimensional manifold M 2 and q is the cluster assignmen t function de X ned on M , i.e.
We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for 8 x 2M , q ( x ) returns the cluster mem bership of x . The smoothness of q over M can be calculated by the following Dirichlet integral [2] where the gradien t r q is a vector in the tangen t space TM and the integral is taken with respect to the standard mea-sure on M . If we restrict the scale of q by h q; q i M = 1 (where h X  ;  X i M is the inner product induced on M ), then it turns out that  X nding the smoothest function minimiz-ing D [ q ] reduces to  X nding the eigenfunctions of the Laplac e Beltr ami operator L , which is de X ned as where div is the divergence of a vector  X eld.

Generally , the graph can be viewed as the discretized form of manifold. We can model the dataset as an weighted undi-rected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represen t the similarities between pairwise points. Then it can be shown that minimizing Eq.(13) corresp onds to min-imizing Laplacian with its ( i; j )-th entry where d i = P j w ij is the degree of x i , w ij is the similarit y between x i and x j . If x i and x j are adjacen t 3 , w ij computed in the following way where  X  is a dataset dependen t parameter. It is proved that under certain conditions, such a form of w ij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].
In summary , using Eq.(15) with exponen tial weights can e X ectiv ely measure the smoothness of the data assignmen ts with respect to the intrinsic data manifold. Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignmen ts.
Com bining the contents we have introduced in section 2.2 and section 2.3 we can deriv e the clustering criterion is where P is de X ned as in Eq.(12), and  X  is a regularization parameter to trade o X  J l and J g . However, the discrete  X ll in the whole high-dimensional sample space. And it has been shown that the manifold based metho ds can achieve good results on text classi X cation tasks [31].
In this paper, we de X ne x i and x j to be adjacen t if x i N ( x j ) or x j 2N ( x i ). constrain t of p i makes the problem an NP hard integer pro-gramming problem. A natural way for making the problem solvable is to remo ve the constrain t and relax q i to be contin-uous, then the objectiv e that we aims to minimize becomes and we further add a constrain t q T q = 1 to restrict the scale of q . Then our objectiv e becomes Using the Lagrangian metho d, we can deriv e that the op-timal solution q corresp onds to the smallest eigen vector of the matrix M = ( P  X  I ) T ( P  X  I ) +  X  L , and the cluster as-signmen t of x i can be determined by the sign of q i , i.e. x will be classi X ed as class one if q i &gt; 0, otherwise it will be classi X ed as class 2. In the above we have introduced the basic framew ork of Clustering with Local and Global Regularization ( CLGR ) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.

First we assume that all the documen ts belong to C classes indexed by L = f 1 ; 2 ;  X  X  X  ; C g . q c is the classi X cation func-tion for class c (1 6 c 6 C ), such that q c ( x i ) returns the con X denc e that x i belongs to class c . Our goal is to obtain the value of q c ( x i ) (1 6 c 6 C; 1 6 i 6 n ), and the cluster assignmen t of x i can be determined by f q c ( x i ) g C some prop er discretization metho ds that we will introduce later.

Therefore, in this multi-class case, for each documen t x i 6 n ), we will construct C locally linear regularized label predictors whose normal vectors are bor of x i , and q c i = [ q c i 1 ; q c i 2 ;  X  X  X  ; q c in Then ( w c  X  i ) T x i returns the predicted con X dence of x longing to class c . Hence the local prediction error for class c can be de X ned as And the total local prediction error becomes As in Eq.(11), we can de X ne an n  X  n matrix P (see Eq.(12)) and rewrite J l as Similarly we can de X ne the global smoothness regularizer in multi-class case as Then the criterion to be minimized for CLGR in multi-class case becomes returns the trace of a matrix. The same as in Eq.(20), we also add the constrain t that Q T Q = I to restrict the scale of Q . Then our optimization problem becomes From the Ky Fan theorem [28], we know the optimal solution of the above problem is where q  X  k (1 6 k 6 C ) is the eigen vector corresp onds to the k -th smallest eigen value of matrix ( P  X  I ) T ( P  X  I ) +  X  L , and R is an arbitrary C  X  C matrix. Since the values of the entries in Q  X  is continuous, we need to further discretize Q to get the cluster assignmen ts of all the data points. There are mainly two approac hes to achieve this goal: 1. As in [20], we can treat the i -th row of Q as the embed-2. Since the optimal Q  X  is not unique (because of the The detailed algorithm procedure for CLGR is summarized in table 1.
In this section, experimen ts are conducted to empirically compare the clustering results of CLGR with other 8 rep-resen titive documen t clustering algorithms on 5 datasets. First we will introduce the basic informations of those datasets.
We use a variety of datasets, most of which are frequen tly used in the information retriev al researc h. Table 2 summa-rizes the characteristics of the datasets.
Here an indic ation matrix T is a n  X  c matrix with its ( i; j )-th entry T ij 2 f 0 ; 1 g such that for each row of Q  X  there is only one 1. Then the x i can be assigned to the j -th cluster such that j = arg j Q  X  ij = 1.
 Table 1: Clustering with Local and Global Regular-ization ( CLGR )
Input : Output :
Procedure : Table 2: Descriptions of the documen t datasets WebKB4 4199 4 WebACE 2340 20 Newsgroup4 3970 4
CSTR . This is the dataset of the abstracts of technical reports published in the Departmen t of Computer Science at a university. The dataset contained 476 abstracts, which were divided into four researc h areas: Natural Language Processing(NLP), Robotics/Vision, Systems, and Theory .
WebKB . The WebKB dataset contains webpages gath-ered from university computer science departmen ts. There are about 8280 documen ts and they are divided into 7 cate-gories: studen t, facult y, sta X , course, project, departmen t and other. The raw text is about 27MB. Among these 7 categories, studen t, facult y, course and project are four most populous entity-represen ting categories. The associ-ated subset is typically called WebKB4.

Reuters . The Reuters-21578 Text Categorization Test collection contains documen ts collected from the Reuters newswire in 1987. It is a standard text categorization bench-mark and contains 135 categories. In our experimen ts, we use a subset of the data collection which includes the 10 most frequen t categories among the 135 topics and we call it Reuters-top 10.

WebACE . The WebACE dataset was from WebACE project and has been used for documen t clustering [17][5]. The We-bACE dataset contains 2340 documen ts consisting news ar-ticles from Reuters new service via the Web in Octob er 1997. These documen ts are divided into 20 classes.

News4 . The News4 dataset used in our experimen ts are selected from the famous 20-newsgroups dataset 5 . The topic rec containing autos , motor cycles , baseball and hockey was selected from the version 20news-18828. The News4 dataset contains 3970 documen t vectors. http://p eople.csail.mit.edu/jrennie/20Newsgroups/
To pre-pro cess the datasets, we remo ve the stop words using a standard stop list, all HTML tags are skipp ed and all header  X elds except subject and organization of the posted articles are ignored. In all our experimen ts, we  X rst select the top 1000 words by mutual information with class labels.
In the experimen ts, we set the number of clusters equal to the true number of classes C for all the clustering al-gorithms. To evaluate their performance, we compare the clusters generated by these algorithms with the true classes by computing the following two performance measures.
Clustering Accuracy ( Acc ). The  X rst performance mea-sure is the Clustering Accuracy , which disco vers the one-to-one relationship between clusters and classes and measures the exten t to which each cluster contained data points from the corresp onding class. It sums up the whole matc hing de-gree between all pair class-clusters. Clustering accuracy can be computed as: where C k denotes the k -th cluster in the  X nal results, and L is the true m -th class. T ( C k ; L m ) is the number of entities which belong to class m are assigned to cluster k . Accu-racy computes the maxim um sum of T ( C k ; L m ) for all pairs of clusters and classes, and these pairs have no overlaps. The greater clustering accuracy means the better clustering performance.

Normalized Mutual Information ( NMI ). Another eval-uation metric we adopt here is the Normalize d Mutual In-formation NMI [23], which is widely used for determining the qualit y of clusters. For two random variable X and Y , the NMI is de X ned as: where I ( X ; Y ) is the mutual information between X and Y , while H ( X ) and H ( Y ) are the entropies of X and Y respectiv ely. One can see that N M I ( X ; X ) = 1, which is the maximal possible value of NMI . Given a clustering result, the NMI in Eq.(30) is estimated as where n k denotes the number of data contained in the cluster C k (1 6 k 6 C ), ^ n m is the number of data belonging to the m -th class (1 6 m 6 C ), and n k;m denotes the number of data that are in the intersection between the cluster C k the m -th class. The value calculated in Eq.(31) is used as a performance measure for the given clustering result. The larger this value, the better the clustering performance.
We have conducted comprehensiv e performance evalua-tions by testing our metho d and comparing it with 8 other represen tativ e data clustering metho ds using the same data corpora. The algorithms that we evaluated are listed below. 1. Traditional k-means (KM). 2. Spherical k-means (SKM). The implemen tation is based 3. Gaussian Mixture Model (GMM). The implemen tation 4. Spectral Clustering with Normalized Cuts (Ncut). The 5. Clustering using Pure Local Regularization (CPLR). 6. Adaptiv e Subspace Iteration (ASI). The implemen ta-7. Nonnegativ e Matrix Factorization (NMF). The imple-8. Tri-Factorization Nonnegativ e Matrix Factorization For computational e X ciency , in the implemen tation of CPLR and our CLGR algorithm, we have set all the local regularization parameters f  X  i g n i =1 to be identical, which is neigh borhoods is set by grid searc h from f 20 ; 40 ; 80 g . For the CLGR metho d, its global regularization parameter is set by grid searc h from f 0 : 1 ; 1 ; 10 g . When constructing the global regularizer, we have adopted the local scaling metho d [30] to construct the Laplacian matrix. The  X nal discretiza-tion metho d adopted in these two metho ds is the same as in [26], since our experimen ts show that using such metho d can achieve better results than using kmeans based metho ds as in [20].
The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4. From the two tables we mainly observ e that: 1. Our CLGR metho d outp erforms all other documen t 2. For documen t clustering, the Spheric al k-means metho d 3. The results achieved from the k-means and GMM type 4. The experimen tal comparisons empirically verify the Table 3: Clustering accuracies of the various meth-ods KM 0.4256 0.3888 0.4448 0.4001 0.3527 SKM 0.4690 0.4318 0.5025 0.4458 0.3912 GMM 0.4487 0.4271 0.4897 0.4521 0.3844 NMF 0.5713 0.4418 0.4947 0.4761 0.4213 Ncut 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 0.5974 0.5020 0.4832 0.5213 0.4890 CLGR 0.6235 0.5228 0.5341 0.5376 0.5102 Table 4: Normalized mutual information results of the various metho ds KM 0.3675 0.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 GMM 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 Ncut 0.4833 0.4497 0.4392 0.4289 0.4231 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690
CLGR 0.6012 0.5434 0.4935 0.5390 0.4908 5. The co-clustering based metho ds ( TNMF and ASI ) 6. The results achieved from CPLR are usually better
Besides the above comparison experimen ts, we also test the parameter sensibilit y of our metho d. There are mainly two sets of parameters in our CLGR algorithm, the local and global regularization parameters ( f  X  i g n i =1 and  X  , as we have said in section 3.3, we have set all  X  i 's to be identical to  X   X  in our experimen ts), and the size of the neigh borhoods. Therefore we have also done two sets of experimen ts: 1. Fixing the size of the neigh borhoods, and testing the 2. Fixing the local and global regularization parameters, Figure 1: Parameter sensibilit y testing results on the WebACE dataset with the neigh borhood size  X xed to 20, and the x-axis and y-axis represen ts the log 2 value of  X   X  and  X  .
Therefore, we can see that our CLGR algorithm (1) can achieve satisfactory results and (2) is not very sensitiv e to the choice of parameters, which makes it practical in real world applications.
In this paper, we deriv ed a new clustering algorithm called clustering with local and global regularization . Our metho d preserv es the merit of local learning algorithms and spectral clustering . Our experimen ts show that the prop osed algo-rithm outp erforms most of the state of the art algorithms on many benchmark datasets. In the future, we will focus on the parameter selection and acceleration issues of the CLGR algorithm. [1] L. Baker and A. McCallum. Distributional Clustering [2] M. Belkin and P. Niyogi. Laplacian Eigenmaps for [3] M. Belkin and P. Niyogi. Towards a Theoretical Figure 2: Parameter sensibilit y testing results on the WebACE dataset with the regularization param-eters being  X xed to 0.1, and the neigh borhood size varing from 10 to 100. [4] M. Belkin, P. Niyogi and V. Sindh wani. Manifold [5] D. Boley . Principal Direction Divisiv e Partitioning. [6] L. Bottou and V. Vapnik. Local learning algorithms. [7] P. K. Chan, D. F. Schlag and J. Y. Zien. Spectral [8] D. R. Cutting, D. R. Karger, J. O. Pederson and J. [9] I. S. Dhillon and D. S. Modha. Concept [10] C. Ding, X. He, and H. Simon. On the equiv alence of [11] C. Ding, X. He, H. Zha, M. Gu, and H. D. Simon. A [12] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal [13] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [14] T. Li, S. Ma, and M. Ogihara. Documen t Clustering [15] T. Li and C. Ding. The Relationships Among Various [16] X. Liu and Y. Gong. Documen t Clustering with [17] E. Han, D. Boley , M. Gini, R. Gross, K. Hastings, G. [18] M. Hein, J. Y. Audib ert, and U. von Luxburg. From [19] J. He, M. Lan, C.-L. Tan, S.-Y. Sung, and H.-B. Low. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. On Spectral [21] B. Sch X olkopf and A. Smola. Learning with Kernels . [22] J. Shi and J. Malik. Normalized Cuts and Image [23] A. Strehl and J. Ghosh. Cluster Ensem bles -A [24] V. N. Vapnik. The Natur e of Statistic al Learning [25] Wu, M. and Sch X olkopf, B. A Local Learning Approac h [26] S. X. Yu, J. Shi. Multiclass Spectral Clustering. In [27] W. Xu, X. Liu and Y. Gong. Documen t Clustering [28] H. Zha, X. He, C. Ding, M. Gu and H. Simon. Spectral [29] T. Zhang and F. J. Oles. Text Categorization Based [30] L. Zelnik-Manor and P. Perona. Self-T uning Spectral [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston and B.
