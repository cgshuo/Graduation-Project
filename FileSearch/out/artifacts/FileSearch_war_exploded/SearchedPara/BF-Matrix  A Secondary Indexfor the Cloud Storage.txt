 The wave of big data is leading a profound change to information infrastructure and is reshaping the landscape of storage systems. As an important complement of traditional database, cloud data store provides a possibility to cope with the ever-growing data volume and adapt to the real-time variable storage demand. For those people who are willing to maximize value from big data, it is appealing to deploy their applications on the cloud. However, while some of the Key-Value stores such as BigTable[1], Cassandra[2] and Dynamo[3] have already served as the back-end for many large scale applications, the lack of built-in support for secondary index restrains more users from migrating to the cloud storage. Even though MapReduce has acted as some systems X  execution engine, like Hive[4], it is not able to provide low latency response for on-line interactive applications.

Faced with such a short slab, the traditional index techniques could neither be directly applied to the massive data nor be simply migrated to the cloud platform[11]. Although some distributed indexes have been proposed for the cloud storage, the following challenges have not been well addressed: (1) In the face of the massive data, the current solutions did not explicitly discuss the scalability and the elasticity of their indexes, which are the major reasons for the successful and widespread adoption of cloud data store.
With the increase of data volume, the cloud storage may provide continuously growing storage space and computing power by means of scale-out. As a critical component in the cloud software stack, the index itself is also needed to be scalable. It is not only required that the index structure is capable of being scaled, but also need the index could make the most of distributed resources and keep the overhead of retrieval under control.

Furthermore, elasticity is critical to minimize operating cost while ensuring good performance during high loads. In the presence of fluctuant workloads, users may adjust available resources to match the current demands. This may result in the change of cluster scale, and also lead to frequent index expansion and contraction. Accordingly, the index should be flexible enough to adapt to the dynamic environment of cloud storage. (2) As NoSQL databases used to achieve high availability and low latency at the expense of strong consistency, none of the existing works explored the index consistency in the absence of transaction guarantee.

Be different from the relational database, cloud data store usually does not provide cross-row, cross-table ACID-comp liant transactions. In consideration of the availability and access delay, most of them opt for eventually consistency. Atomic access is supported only at the gr anularity of single keys. Even if some systems make effort to provide  X  X ocal X  transaction guarantee, the transaction semantics are still confined to a limited range of data set. As for the job of index update, we need to write the basic table and the index table at once. This goes beyond the semantics of single key access. So the difficulty is how to ensure the index consistency.

With above observations, our motivation of this work is to give a scal-able and elastic secondary index. Without the support of distributed transactions, it should still be consistent in the eyes of applications.
In this paper, to shorten the length of access path and make the best of scattered resources, we resort to the hierarchical structure to organize our index. It is a loose coupling solution. For each data node, a B+ tree is created as the local index. At the master node, a Bloom Filter Matrix(BF-Matrix) is used as the global index. In order to make our solution scalable and elastic, a vector of counting bloom filter is used to replace the native one. Moreover, we also describe the algorithms for data retrieval and index update. By presented two rules, they can avoid the problem of false negative and ensure the index table  X  X ook like consistent X . While the B+ tree index local to a datanode obey ACID semantics, the global index have looser consistency.

The rest of this paper is structured as follows. Section 2 discusses the re-lated work. Section 3 describes the problem and presents the topology of our distributed index. In Section 4, we proposed the BF-Matrix solution. In Section 5, we give the algorithms for retrieval and update. Experimental evaluation of our method is presented in Section 6 . We conclude the paper in Section 7. Since the topology of distributed index usually depend on the underlying net-work architecture, to discuss the existing solutions, we incline to classify them according to the organization of data nodes. (1)In a peer to peer configuration, the key-value stores of this kind, such as Dynamo[3], Cassandra[2] and Riak 1 , represent the cluster as a circle space o r ring, and rely on the consistent hash-ing to locate the correct node. while the indexes of P2P structure benefit from high scalability, they are not compatible with the MapReduce paradigm and are difficult to support richer query semantics. (2)For the Master/Slave mode, a minority of nodes play the role of coordinators. As Fig. 1 illustrated, theoret-ically, there are three methods to construct an index for a huge mass of data.  X 
The first way is to create a centralized index on the ma ster. However, it intro-duces a performance bottleneck and suffe rs from the single point of failure. In reality, people choose to distribute the whole index on multiple master nodes. BigTable[1], HBase 2 , Scalable distribut ed B-Tree[5], ITHBase 3 are of this type.  X 
The second approach turn to t he decentralized autono mous solution. By this way, each data node or each data split maintains an separate index by itself, and the master node is only responsible for forwarding messages to them. This kind of index can be used for scan operation and selective MapReduce jobs but fail to support random probe. HAIL[6], hindex 4 ,TrojanIndex[7],IHBase 5 be-long to this type.  X  The last scheme gives consider ation to both centralized and decentralized interests. Like the data c hunks, the index segmentations are dis-seminated in the cluster. Each data node builds some local index for its data, and one or more master nodes collectively maintain a global index. So, it is actually a two-layered distributed index. While the global index indicate which datanode kept the user required data, the local index concretely point to the position in the file. Examples include CG-index[8], RT-CAN[9], EMINC[10]. To design a secondary index, as Fig. 2 illustrated, we resort to a two-layered architecture to organize our distributed index. It is a hierarchical solution, but the global index and the local index are independent and loosely coupled. On the upper layer, we use a Bloom Filter Matrix to save the knowledge of data distribution. Each row of the matrix represents a bloom filter. It corresponds to a datanode and indicates whether a data object is stored on that node. At the same time, a B+ tree is built on each datanode, which only takes charge of the local storage. There are as many rows in the matrix as there are datanodes in the cluster. All the bloom filters are homogeneous and share the same group of hash functions.

With above solution, the process of non-key attribute queries could be divided into two steps: (1)locate the relevant datanodes; (2)look up the local index.
Since the second subproblem is not different from the traditional index tech-niques and has been well studied by the database community, the real difficulties lie in how to construct a scalable and elastic global index, which supports the membership query, and propose effective a lgorithms to defend the consistency of it. For our solution, to create a secondary index, a Standard Bloom Filter [12][13][14](SBF) is built for each datanode.

As illustrated in Fig. 3, supposing the universal set contains d possible data objects, and a total of h objects are stored on one of the nodes. According to the value of the indexed column, the data subset is mapped to an attribute-value subset of n elements. A bloom filter for representing the data subset is described by a vector of m bits, initially all set to 0. It uses k independent hash functions h ,h 2 ,...,h k to map the attribute value of a data object to a random number uniform over the range { 1 ,...,m } .
 Generally, given n and f ,theoptimal k and m can be calculated as k opt = m n ln 2, and m opt =  X  n  X  lnf ( ln 2) 2 , which minimizes the false positive rate[12]. 4.1 Deletable Index Although SBF offers a compact probabilistic way to represent the data subset of a datanode, it does not support the removal of elements.

To fix the weakness, we borrow the idea from Counting Bloom Filter[15](CBF), as shown in Fig. 4, and use a small counter to replace the single bit of each entry in the bloom filter. In contrast with the original m bits string, a counter vector of length m is used to represent each datanode.

To avoid counter overflow, we need to choose sufficiently large counters. As the work of CBF[15] has revealed, 4 bit s per counter would be amply sufficient for most applications. 4.2 Scalable and Elastic Index For a shared-nothing distributed system, if a datanode join or depart from the cluster, what we need to do is just to add a row into or delete a row from our BF-Matrix. In this case, the cost is small and the impact is limited. But if it is the data volume continues to grow, neither the SBF nor the CBF could expand with the increasing data set.

Without knowledge of the upper bound on the size of data set, a target false positive probability threshold cannot be guaranteed unless the bloom filter is rebuilt from scratch each time the set cardinality changes[16][17]. In our solu-tion, we employ a variable number of counting bloom filters to represent a dy-namic data set, named Dynamic Counting Bloom Filter(DCBF). As illustrated in Fig. 5, a DCBF is made up of one or more CBFs. There are two states of a CBF: active and full. A CBF is called active only if its volume does not reach a designed upper bound, and the DCBF only inserts items of a set into the active CBFs. At the outset, an empty CBF is allocated for each datanode and the ini-tial CBF is active; as more and more data objects are stored on the datanode, when the existing CBFs get full due to the limit on the fill ratio, a new one is added. On the contrary, if an item is removed from a datanode, a full filter may also become active. Querying is mad e by testing for the presence in each filter, if one of the CBFs return true, the item is assumed to be stored on the corresponding datanode. While a DCBF could scale up along with the growth of data volume, it can also constrict as the set cardinality decreases through merge operations, which is defined by the union of two bloom filters. Given two sets S 1 and S 2 , suppose we have bloom filters B 1 and B 2 to represent them respectively. Assuming their length and the related hash functions are all of the same, and the sum of the volume of the two filters is not larger than the capacity of one filter. The two active CBFs could be replaced by the union of them. Then a filter B that represents the union S = S 1  X  S 2 is created by taking the counter-wise addition of the original CBFs B = B 1 B 2 . So, the merged filter B will report any element belonging to S 1 or S 2 as belonging to set S .

Apart from the volume growth, a DCBF could also bring the change of false positive rate. As discussed above, a DCBF with $ n/c % CBFs can represent a dynamic set S with n items. When we use the DCBF instead of S to answer a membership query, the false match probability of the DCBF can be calculated in a straightforward way.
So, the false positive rate will grows linearly with the number of CBFs con-tained in a DCBF. For example, suppose the false ratio of a CBF f =0 . 01, after the DCBF expand to 10 times its original size n c = 10, then f d  X  0 . 1. This is also acceptable and meaningful for our index circumstance. 4.3 False Positive and False Negative To test whether an element is a member of a set, SBF may introduce false positive but not false negative. For the case of our distributed index, when a filter report that an element stored on a datanode by mistake, the cost is to forward a redundant query message to the datanode, and trigger a local retrieval via B+Tree index on that node. This will not destroy the correctness of search behavior or place a real burden on the system.

Although SBF never yield a false negative by itself, its variant DCBF may bring the mistake. A false negative in the index scenario indicates that even a data object is stored on a datanode, it is still possible that the filter return false. This is not allowed for cloud storage.

In order to avoid the risk of false negative, we present the first rule that restrict the behavior of index operations: To delete an index entry, first read back the data object so as to ensure the existence of data item; To insert an index entry, save the filter label with the data object so as to identify the correct filter that record the index entry. Given the structure of BF-Matrix, in this section, we present the algorithms for data retrieval and index maintenance, and discuss how to ensure the consistency of our index. 5.1 Data Retrieval It is clear that a BF-Matrix consisted of multiple DCBFs, and each DCBF is composed of several CBFs. All of the CBFs have the same length and share the same number of hash functions. To answer a membership query based on the DCBF instead of the set S , the first step is to calculate k hash values. The query operation iterates the set of DCBFs, and for each DCBF traverse over the set of CBFs. If all the hash counters of a CBF are set to a nonzero value, it returns true. An item is a member of a DCBF, if one of its CBFs contain the element. An item is not a member of a DCBF, if it is not found in all CBFs. After obtained the subset of DCBFs that may contained the element, we forward the query message to the corresponding datanodes, and collect result from them. Algorithm 1. Single Attribute Query 5.2 Index Maintenance (1)Index Consistency When it comes to the consistency of our hierarchical distributed index, the B+ tree index is treated as separate indexes for each datanode and is co-located with the primary table, it is not difficult to ensure the strong consistency of the local index. But the same problem for the global index is nontrivial.
However, although we could not keep the global index consistent with the pri-mary table at any time, we may make the global index  X  X ook like consistent X  even through sometimes it actually deviate from the user table. In other words, if an index only improve the query efficiency but not affect the results that a user read from the primary table, then we say the index table  X  X ook like consistent X . It is a weaker level than the strong consistency and may lead to inconsistency between the two tables, but will not change the correct behavior of the storage system.
If we wrap the index update operations in a transaction, to implement  X  X ook like consistent X , the following three conditions must be satisfied:  X  after a data object has been inserted into a primary table, the users must be able to find it along the index;  X  an index update transaction is idempo-tent;  X  partial index transactions can be detected at read time, so the atomicity can be guaranteed.
 Since most of the cloud storages employ MVCC(Multi-Version Concurrency Control) to increase concurrency, we also assume that there are multiple versions of a data object, and each version is marked by a timestamp. Thus the idempo-tent property of index transaction is satisfied. Fig. 6 shows the execution order of index operations. For example, Upon select , by choosing the latest version of a data object, we can filter the redundant index entries at read time, and do not need to clean them. They must be from a partial index update -either from a concurrent update that is still in progress, or from a partially failed update. This is how we tolerate partial index update and assure atomicity at read time. In conclusion, the intuition behind above operations is that, it is allowable to have an extra index entry, but not to have an entry in the main table and not in the index table. In this way, we meet the three conditions and could achieve a  X  X ook like consistent X  global index. At last, we introduce the second rule that restrict the behavior of index operations: Before delete the index entry, the data item should first be removed; Before insert a data item, the index entry should first be inserted. (2)Insert Operation Given above two rules, Algorithm 2 contains the details regarding the process of insert operation. Due to the space limitation, we omit the description of the whole process. (3)Delete Operation Similarly, the pseudo-code of delete operation is given in Algorithm 3 without further explanation. (4)Update Operation With regard to the update operation, as illustrated in Fig. 6, it involves an index insertion and an index deletion successively. So, the job can be done by call the Algorithm 2 and Algorithm 3, and we no longer present a separate one for it. (5)Data Migration For the purpose of load balance and fault tolerance, the data objects in cloud storage need to be moved from one node to another node from time to time[18], which will also trigger the index update. Similar to the update operation, it can be solved with above algorithms as well. In this section, we evaluate BF-Matrix, and compare it with ITHBase, hindex and parallel full table scan(MR-FTS) from two aspects: retrieval performance Algorithm 2. Insert and scalability. Furthermore, we also study the overhead of index maintenance. The experiments are implemented on Hadoop 0 . 20 . 2andHBase0 . 90 . 4. 6.1 Experimental Setup We perform the tests on an in-house cluster of 20 nodes connected by a 1 Gbit Ethernet switch. Each node has 4 cores, 16 GB of main memory, and 1 . 4 TB SATA disks. For the data set, a manmade table is generated which is composed of 11 columns. There are totally 500 million records and per record size is 270 bytes , so the whole table size is 125 . 8 GB . As shown in the shading part of the table schema, we create index on five columns and declare a 32  X  bits integer to save the  X  X ile number X , which acted as the RowKey. ( file number ,name,gender,age,occupation,nativeplace, identity card number , email address , microblog username , skype number , drive license number ) 6.2 Results and Discussion For the first experiment, we keep the size of a 7 nodes cluster unchanged and increase the data volume from 2 million to 500 million. As illustrated in Fig. 7, the auxiliary indexes indeed greatly improve the response time of non-key at-tribute queries. With the growth of data volume, the MapReduce based MR-FTS confront with increased delay. This is b ecause the fixed cluster scale lead to un-changed computing power and network bandwidth. Since hindex is 100% server side implementation with Coprocessors and the index table regions are collo-cated with primary table regions, when the data size is less than 15 million, the decentralized hindex outperform the cen tralized ITHBase. But as the data size continue to increase, the datanodes will spend more time to scan the region wise index data, the distributed method also encounter synchronous enlarged delay. By contrast, our hierarchical soluti on all along keeps stable performance. Algorithm 3. Delete
In the second experiment shown in Fig. 8 , we choose to scale out the cluster under a constant data set. In the literature, more nodes could share the over-head of computing and storage, MR-FTS evidently benefit from the enhanced cluster capability. At the same time, as more nodes join the cluster, the index segmentation and the data partition of the same record are more likely to be scattered on different datanodes. This int roduces more cross-node interactions, which amplify the access latency. Since hindex could make the most of cluster resources, the average latency at first d ecreases. However, for each query, the decentralized method has to interact wit h all the datanodes and wait for their responses. After the number of nodes is over 11, the wait time increase. Com-pared with the previous methods, BF-Matrix could always restrict the query processing in a limited scope, and has a good scalability.

To explore the maximum throughput, we try to simultaneously increase the cluster size and data volume, and use another 5 client server, each with 100 threads, to perform a stress test. As shown in Fig. 9, the curve of the BF-Matrix is on top of the other ones. Meanwhile, the slope of the BF-Matrix curve is almost equal to MR-FTS, and is steeper than hindex and ITHBase. They mean our solution not only could achieve higher throughput but also display a higher growth rate. This can be explained by the fact that while the indirect twice probe of ITHBase may prolong t he access path, the flooding forwarded messages of hindex also put a heavy burden on the storage system.

Apart from the retrieval performance, we also examine the cost of index main-tenance. To illustrate the average extra run time, we compare the latency of different operations on non-indexed table with the latency on indexed table, and show the differences in Fig. 10. Since ITHB ase implement transactions support to guarantee that all secondary index updates are consistent, it triggers the biggest latency in the three methods. On the contrary, hindex bring an almost constant delay across different operations. For the BF-Matrix, the delete operation actu-ally contains a read to confirm the correct datanode. So its cost is larger than the insert. With regard to the update operation, the overhead roughly equate to an insert plus a delete. In this paper, we try to address the problem of non-key attribute queries on the huge mass of data. The goal is to offer a secondary index in the cloud data store. The main contributions of our work include: ditional B+ tree to construct a hierarchical index. It not only adapts to the continuous growth of data set, but also is able to cope with the dynamic change of cluster size.
 of false negative, which is unacceptabl e for the storage system. By giving the first behavior rule, we eliminate the possibility of false negative. the strong consistency and give the second behavior rule. It makes the global index  X  X ook like consistent X  so as to assures the correct behavior of applications.
