 HERMANN MOISL University of Newcastle, UK 1. INTRODUCTION The advent of large collections of electronic text in academic, commercial, and governmental spheres has made cluster analysis an indispensable tool for the exploration of data abstracted from such collections and for generation of hypotheses about them. Thabet [2005] applied hierarchical cluster analysis to lexical frequency data abstracted fro m the Qur X  X n to see if this would yield a classification of the (suras) which could be useful in understanding its thematic structure. The results were interesting, but were compromised by a problem caused by the large variation in the length of the , which range from fewer than ten words to several thousand. The present aim is to resolve that problem.
 Thabet X  X  methodology and results, the second section explains why length variation was a problem for Thabet X  s analysis, the third section proposes a solution to the problem, and the fourth section applies the proposed solution to a reanalysis of the Qur X  X n.
 by the Western transliteration in brackets to make the discussion accessible to non-Arabic speakers. The sole exceptions are the singular and plural forms of  X  X ura X  (sg. unnecessary. 2. OVERVIEW OF EXISTING RESULTS To the naive reader, there is no obvious systematicity in the standard arrange-ment of the 114 of the Qur X  X n [Robinson 1996]. They are not, for exam-ple, sequenced in a way that reflects the order of revelation or the chronology of the Prophet X  X  life, nor is there any clear thematic arrangement. In fact, it has long been recognized that, apart from the first al-Fatiha), which is invocatory and v ery short, they are ordered roughly by length; Figure 1 shows this by plotting their order of occurrence in the Qur X  X n on the horizontal axis against their size in Kb on the vertical one. but the reason for the arrangement is not the focus of the present discussion. Rather, the focus is a particular consequence of the length-sequencing: that the revelation on any particular topic may be and in many cases demonstrably is spread across two or more noncontiguous , and that full understanding of the Qur X  X n X  X  teaching on that topic requires the reader to assemble the relevant passages from disparate parts of the text.
 to the electronic text of the Qur X  X n could be useful in such arrangement by classifying the thematically. The basis for her investigation was provided by a foundational principle in Information Retrieval (IR): that lexical types in documents represent concepts [Manning et al. 2008, Robertson 2004], and that documents in a collection can consequently be classified by their concep-tual content using the pattern of lexica l type occurrence across the collection X  documents in which words such as  X  X ield, X   X  X rops, X  and  X  X ield X  occur with more than chance probability and constitute a class because their lexical semantics indicate that they are about the same kind of thing. This class is distinct from another containing, say,  X  X omputer, X   X  X eyboard, X  and  X  X ouse. X  The Qur X  X n was regarded as a collection in this IR sen se consisting of 114 documents, the . An electronic text of the Qur X  X n 1 transliterated into Western alphabetic script was preprocessed in the standard way by removing all function words and stemming the remainder to coalesce morphological variants; for a full discus-sion of the stemming procedure see Thabet [2004]. A list of all lexical types that occurred in this preprocessed text was then compiled, and for each type the frequency of token occurrence across the 114 was determined. This data was stored in a matrix Q in which each row represented a different each column represented a differe nt lexical type, and the value at Q ij repre-sented the number of times lexical type j occurred in vector constituted the lexical frequency profile of the associated then transformed in two ways prior to cluster analysis:  X  X atrix columns with a lexical token frequency of 1 were removed on the grounds that these cannot contribute to determination of relationship among  X  X he row vectors of Q were normalized to compensate for the variation in length among the so that their lexical frequency profiles could be mean-where freq  X ( F ij ) is the length-adjusted frequency of variable j in freq ( F ij ) is the original frequency,  X  is the mean number of words per across all 114 ,and l is the number of words in procedure [Singhal et al. 1996] becomes increasingly unreliable as decreases, because there is a commensurate decrease in the probability that any given lexical type will occur, and so the shorter the corresponding frequency profile. Recognizing the potential effect of this on the reliability of a cluster analysis that included all the ,sheselectedthe24 longest ones for analysis, noting that the selection was ad hoc and that  X  X ny legitimate analysis of the Qur X  X n using this methodology will have to face the problem of which , if any, to exclude on length grounds in a principled way. X  Cluster analysis of the 24 selected partitioned them into two main groups corresponding to those which established Qur X  X nic scholarship assigns to rev-elation in Mecca and those revealed in Medina. Thabet went on to interpret this clustering thematically.
 problem. 3. THE PROBLEM OF VARIATION IN  X  X  LENGTH ing algorithms: single link, complete link, average link, and Ward X  X  Method [Everitt et al. 2001; Manning et al. 2008]; the Ward X  X  Method tree is given in Figure 2 as an example and for reference in the discussion that follows. of the tree are traditional scholarship [Robinson 1996], the place of revelation are disputed are marked with an asterisk.  X  X luster C contains mainly assigned to revelation in Medina between the (hijra) in 622 CE and the Prophet X  X  death in 632 CE.  X  X lusters A and B contain mainly that were revealed in Mecca between 610 CE and 622 CE.  X  X here is overlap in the Mecca/Medina classification in that A and B contain Medinan and C contains Meccan ones.
 clustering methods showed that there was a broad consensus among them on the cluster structure of Q, but also that there was considerable variation in that structure: a core of just over half the were consistently assigned to disjoint Meccan and Medinan clusters, but the remainder were volatile in that they were sometimes assigned to the Meccan cluster and sometimes to the Medinan without any obvious pattern.
 tering volatility is a consequence of the relative shortness of many of the . Given a population E of n events, the empirical interpretation of probability [Devore 2008; Walpole et al. 2007]. A sample of E can be used to estimate p ( e i ), and the Law of Large Numbers says that as sample size increases so does the likelihood that the sample estimate of an event X  X  population probability is accurate; a small sample might give an accurate estimate but is less likely to do so than a larger one, and for this reason larger samples are preferred. Applying these observations to the present case, each of the is taken to be a sample of Arabic contemporary with the Prophet. The longer the the more likely it is that its estimate of the population probabilities of lexical types in the Qur X  X n will be accurate, and, conversely, the shorter the less likely this will be. The Qur X  X n contains some very short ,andmostof the volatile ones are very short: a reasonable hypothesis is that these give very inaccurate lexical population probability estimates, and that this inaccuracy underlies their volatility.
 ability estimates of the lexical types in Q as Q was transformed in the following sequence. (2) The matrix was length-normalized using Thabet X  X  Equation (1). (3) The column vectors of Q were sorted by descending frequency so that the (4) The lexical frequency values in Q were transformed into probabilities by ble to observe how probabilities behave with increasing shows, for example, the from 1-114 on the horizontal axis and probability on the vertical axis for (yawm), the sixth most frequent lexical type. population value, as the Law of Large Numbers leads one to expect; the fluctu-ations on the left are caused by frequency values that are too large or too small relative to the length of the text sample to estimate the population probability accurately.
 ization. The result of normalization is merely a conjecture about what the frequencies of the lexical type variables would have been if all the document samples were of equal length. As we have just seen, however, these conjectures are not necessarily accurate, particularly for the shortest documents, and the normalization procedure can and does generate some extreme values. On the one hand, many of the frequencies in the shortest documents are zero sim-ply because they are too short for all but the highest-probability lexical types to have a reasonable chance of occurring even once. These zero values remain unchanged with normalization because zero times anything remains resolutely zero. If the documents in question had been longer, the lower-probability letter pairs would have begun to appear at least once, and these non-zero frequen-cies would then have been amenable to normalization. On the other hand, the pattern of occurrence of words that do occur in short documents does not necessarily reflect their population probabilities very well. If a short text in some arbitrary collection happens to mention a very rare word X  X  X ebra X  in an English-language collection, for example X  X ven once, normalization will ele-vate its frequency and thereby its conjectured probability out of all proportion to its actual population probability in English.
 values in the corresponding rows of Q can be unreliable estimators of popula-tion probabilities, which in turn affects their classification in cluster analysis. 4. PROPOSED SOLUTION The obvious solution is to determine which are too short to provide reasonably reliable estimates of population probabilities, and to eliminate the corresponding rows from the data matrix. But how short is too short? One possibility is implicit in Figure 3. Because the that are too short have large vertical fluctuations, the point on the horizontal axis where the fluctua-tions settle down is the required length threshold. One would, of course, want to look at scatter plots for other lexical types in the Qur X  X n, but the principle would remain the same. Though graphical representations of data character-istics are useful intuitive guides, interpretation of them can be imprecise and inconsistent. Where exactly in Figure 3 do the fluctuations settle down suf-ficiently for a length threshold to be drawn? Should the shortest 40 be eliminated, or the shortest 60, or the shortest 80? If too few are eliminated the cluster analysis will be unreliable, and if too many are eliminated then which could have been classified are excluded, diminishing the usefulness of the analysis. What is required is a more reliable method for determining the minimum length threshold.
 cepts used in this section are covered in any relevant textbook, for example [Devore and Peck 2005; Devore 2008; Hays 1994; Walpole et al. 2007]. Given a population containing m objects, a sample is a selection of n of these objects, where n &lt; m . Much of statistics is concerned with estimating characteristics or  X  X arameters X  of populations from samples which typically are not only smaller but much smaller than the populations from which they are drawn. A funda-mental question in such estimation is:  X  X ow large does a sample have to be to estimate, with some specified degre e of accuracy, the value of a population parameter of interest? X  In the present case the parameter of interest is lexical probability.
 required sample size function, and the second part applies it to establishing a minimum length threshold for the . 4.1 The Sample Size Function The sample size function for estimation of population probability is based on the properties of the sampling distribution of binomial variables. This section first outlines the nature of this distribution and then derives the sample size function from it.
 variable [Devore and Peck 2005, 719 X 25; Devore 2008, 108-13; Hays 1994, chs. 3 and 5] takes as its value the number of times some characteristic of interest x occurs in the sample: the number of males in a sample of 1,000 people, for example. The ratio x / n is an estimate of the population probability of x .It is, however, typically the case in statistical sampling that different samples of any fixed size n drawn from the same population yield x -values and thus prob-ability estimates which differ to greater or lesser degrees. Given only a single estimate based on a single sample X  X  so-called point estimate X  X here is no way of knowing how accurate it is, that is, how close it is to the population parame-ter. The sampling distribution [Devore and Peck 2005, ch. 8; Hays 1994, ch. 5; Walpole et al. 2007, 243 X 51] is a way of gaining insight into the accuracy of n -sized samples as estimators of population probability. A sampling distribu-tion for a population is generated by taking all possible n -sized samples from it and deriving the parameter estimate from each sample; the resulting prob-ability distribution describes the sampling variability of parameter estimates. probability of a variable, the corresponding sampling distribution has the fol-lowing properties [Devore and Peck 2005, 355]: (1) where the number of all possible n -sized samples is k ,themeanofthe k (2) the standard deviation  X  is (3) the larger the value of n , the more closely the sampling distribution ap-points  X   X  e and  X  + e on either side of the mean. The ratio of the area under the curve bounded by these two points to the total area under the curve is the proportion of probability values in the distribution that falls between the two points. If, for example, the ratio between the shaded area in Figure 4 to the total area under the curve was found to be 0.5, then 50% of the probability val-ues in the distribution would be within e of the distribution mean. Put another way, there would be a 50% chance that the population probability estimate de-rived from any given sample of size n would fall between the bounds  X   X  e and  X  + e .

In the statistical literature this 50% chance is called the confidence level and the corresponding  X   X  e ...  X  + e interval is called the confidence inter-val because one can be 50% confident that the population probability estimate from any n -sample will fall in the interval bounded on either side of the mean by e .
 ified and, using the known shape of the distribution provided by properties (1)  X  (3), it calculates the confidence le vel corresponding to that interval. The obverse is also possible; given a confidence level, what is the corresponding confidence interval? This is done by specifying z not explicitly but in terms of the number of standard deviations on either side of the mean, as in Figure 5. where e is as above,  X  is the sampling distribution standard deviation, and z is the confidence level expressed in terms of the number of standard deviations, as in Figure 5. Equation (2) provides a definition of sampling distribution standard deviation, so that Equation (3) can be rewritten as culates the confidence interval bound e if the confidence level z , the population probability  X  ,andthesamplesize n are known. But if e is known and n is not, then Equation (4) can be rewritten by algebraic rearrangement as estimate the population probability  X  of variable x so that with confidence level z , the estimate falls within an interval +/-e on either side of the mean. For derivation of the sample size function see [Cochran 1977, ch. 4; Devore and Peck 2005, 368 X 78; Devore 2008, ch. 7; Hays 1994, 256 X 7].
 4.2 Application and Results The first step in the application of the sample size Equation (5) to the prob-lem of determining a length threshold for the is to state the problem in sampling-theoretic terms.  X  X ach  X  X ach lexical variable in Q is a binomial variable, since, for any given variable j , each successive lexical token that occurs in sample i either is or is not  X  X or any given sample i and variable j , the ratio of the total number of token occurrences of variable j to the sample length n is an estimate of the population probability  X  of j .  X  X fter conversion of the frequency values in Q to probabilities, each of its column vectors is a sampling distribution of population probability esti-mates: all the samples are of the same size after normalization by mean doc-ument length at the data preprocessing stage, and the values in any given column vector are population probability estimates for the associated lexical variable based on a set of equal-size samples.
 nomial lexical variables in principle provides the term  X  in the sample size formula. By property (1) of sampling distributions given above, the mean of any given column vector of Q should be a good estimate of the population prob-ability  X  of the associated lexical variable. In practice, however, the column vectors of Q do not in general give good probability estimates. The validity of the sample size function is posited on the distribution being at least approxi-mately normal. It is a characteristic of binomial distributions that, for a given sample size n , the closer the population probability is to 0.5 the more normal the distribution is. The smaller the population probability, the larger n must be for the distribution to be at least approximately normal [Devore 2008]. In the present case, the largest probability for any lexical variable is 0.1016, which is very far from 0.5, and most are far smaller than that. The sampling distribu-tions of Q are commensurately far from normal, as the sample plots in Figure 6 show.
 tions for the application of the sample size formula. Specifically, in a normally shaped but positively skewed distribution the values are concentrated in the lower end of its numerical range proportional to the degree of skew, and the mean of those values is consequently numerically smaller than it would be if the values were equally distributed on either side of the mean. For any posi-tively skewed column vector of Q, therefore, the mean of the sampling distri-bution of probabilities is smaller than it would have been if the column had been normally distributed, and it consequently underestimates the popula-tion probability. By the same reasoning, a negatively skewed column vector will overestimate the population parameter. This in turn affects the results from the sample size function. Relative to a normal distribution with a given standard deviation, a positively-skewed distribution underestimates the sam-ple size and a negatively-skewed one overestimates.
 vector that exactly reverses the distribution using the function where v is a sampling distribution column vector from Q, j indexes the column vectors of Q in the range 1. . . 3672, i indexes the components of v j in the range 1. . . 114, and max / min are the maximum and minimu m values respectively in v . The relationship between the distributions of, for example, column vector 1 of Q and its mirror vector is shown in Figure 7.

Since the distributions are symmetrical, a sample size calculation based on the mirror vector will overestimate to the same extent that a calculation based on the original vector will underestimate. The mean of the two estimates is then the estimate of the population probability  X  used in the sample size calculation.
 able, it remains only to define values for the confidence interval bound e and for the confidence level z for that variable. As the confidence bound and level in Equation (4) are made more stringent in the sense that the value of e is made smaller and the value of z larger, n will grow. It is therefore crucial to choose values that on the one hand give a reasonable level of confidence that the threshold eliminates which are genuinely too short to be reliably clustered, and on the other does not eliminate too many , thereby com-promising the usefulness of the analysis. A principled choice for e ,usedhere, is the standard deviation, since it measures mean deviation from the popula-tion probability estimate. Reference to the distribution of probability values in Figure 3 and to corresponding plots for other sampling distributions in Q indicate that the standard deviation is a reasonable choice. Regarding z ,the usual value in the statistical literature is 95% confidence, corresponding to a value of 1.96 standard deviations, but there is nothing magical about this, and other confidence levels are possible. In what follows, an 80% confidence level corresponding to z =1 . 282 is used, but the results can always be tightened up with a higher level if necessary.
 to be calculated for each of the 3,672 lexical variables in Q, but in practice it is unnecessary to consider anything near that many. Sample sizes were calculated for the 1,000 highest-probability variables and, as we shall see, even this is considerably in excess of what is required. A vector of sample sizes for these 1,000 variables was generated and plotted in Figure 8.
 on the left to the lowest-probability one on the right, and sample size is shown on the vertical axis. It is clear that the sampling distributions for different lexical variables generate different sample sizes, and that as lexical probability decreases the sample size tends to increase, as one would intuitively expect, though the increase is not monotonic.
 variables complicates the selection of a sample size values in Figure 8, which should be selected? The answer is that the researcher must select a threshold which balances the number of that can be clustered against the number of variables available for the clustering in the light of his or her research aims. This is exemplified using Figure 9. The two columns of Figure 9a show of lexical tokens each contains in ascending order, and the two columns of Figure 9b show the sample sizes for the lexical variables on the horizontal axis of Figure 8. Only the leftmost 100 are shown to save space, but that is sufficient for present purposes.
 is, without encountering the problem of poor lexical probability estimation which has motivated the discussion thus far. This is impossible. Reference to Figure 9b shows that the shortest possible length threshold is 56, but since the 27 from have to be eliminated. The maximum number of suras that can be reliably clustered is 87.
 ably clustered, but the clustering can only be based on a single variable (1) in Figure 9b; all the other variables require able threshold must be selected from Figure 9b. Assume that the threshold is 300. This allows reliable clustering based on nine variables 1, 2, 3, 4, 5, 6, 10, 19, and 32, but there is a cost in terms of how many can be clustered. A threshold of 300 requires 67 from to be eliminated.
 on which clustering can be based, and the smaller the number of that can be clustered. The limiting case is a threshold of 2524, the length of the penultimate used, but only two can be clustered, the point of which is difficult to see. And, as one moves to the right in Figure 8, the required length soon exceeds the maximum length of any for clustering.
 lexical variables 1 47 x 9 matrix was analyzed using the same range of hierarchical clustering algorithms as in Section 2, and the result was that the analyses were stable. That is, the intercluster volatility characteristic of the analyses based on the full 114 x 3672 version of Q was gone. Figure 10 gives the Ward X  X  Method version of this analysis.
 thus a different 5. CONCLUSION This discussion addressed Thabet X  X  identification of poor estimation of lexi-cal population probabilities by short as a fundamental problem in cluster analysis of the Qur X  X n, and proposed a resolution of the problem which involves calculation of a minimum cal sampling theory followed by selection of and lexical variables based on that threshold. The key difference between an analysis which takes account of the implications of variation in not is in their relative reliability: the former provides a well-defined criterion for deciding which can be reliably clustered on which lexical variables, whereas the latter simply hopes for the best by attempting to cluster which may or may not be long enough on the basis of lexical variables for which the population probability estimates may or may not be sufficiently accurate, and thereby generates results that may well mislead.
 analysis more generally will be of interest to Qur X  X nic scholars. It is, moreover, easy to see that length variation can be a problem which extends to cluster analysis of document collections of any type and in any language, and that the solution proposed here is generally applicable.
 I would like to thank my student A. Omar for providing the Arabic ortho-graphic forms used in the discussion.

