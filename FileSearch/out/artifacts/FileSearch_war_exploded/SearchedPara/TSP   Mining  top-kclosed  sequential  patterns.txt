
Sequential pattern mining is an important data-mining task that has been studied extensively (Agrawal and Srikant 1995; Mannila et al. 1995; Guha et al. 1999; Pei including analysis of custom er purchase patterns, web-access patterns, discovery of motifs and tandem repeats in DNA sequences, analysis of various sequencing or time-related processes, such as scientific experiment s, disease treatments, natural disasters, and many more.
 kant (1994): Given a set of sequences, where each sequence consists of a list of elements and each element c onsists of a set of items, and given a user-specified sequences whose occurrence frequency is no less than min_support.
 to use a min_support threshold to generate the frequent sequential patterns, based on the popular Apriori property (Agrawal and Srikant 1994): every subpattern of a frequent pattern must be frequent (also called the downward closure property ). This framework leads to the following two problems that may hinder its popular use. terns, which is unavoidable when the database consists of long, frequent sequences.
The similar fact is observed at mining itemset and graph patterns when the sizes of the patterns are large. For example, a database containing a frequent sequence ( a 1 )( a 2 )...( a 64 ) (  X  i = j , a i = a j ) will generate at least 2 quences. It is very likely some subsequences share the same support with this long sequence, and they are essentially redundant patterns.
 needs to have prior knowledge about the mining query and the task-specific data and be able to estimate, without mining, how many patterns will be generated with a particular threshold. Setting min _ support is a subtle task: a too small value may lead to the generation of thousands of patterns, whereas a too big one may lead to no answers found .
 al. 2003). CloSpan can mine closed sequential patterns, where a sequential pattern s is closed if there exists no superpattern of s with the same support in the database.
Mining closed patterns may significantly reduce the number of patterns generated and is information lossless because it can be used to derive the complete set of sequential patterns.
 ing. Han et al. (2002) changes the task of mining frequent patterns to mining top-k frequent closed patterns of minimal length min _ ,where k is the number of closed patterns to be mined, top-k refers to the k most frequent patterns, and min _ is the minimal length of the closed patterns. This setting is also desirable in the context of sequential pattern mining. We will show a real application case of top-k sequential pattern mining in Sect. 2. Unfortunately, most of the techniques developed in Han et al. (2002) cannot be directly applied in s equence mining. This is because subse-quence testing requires order matching, which is more difficult than subset testing.
Moreover, the search space of sequences is much larger than that of itemsets. How-ever, some ideas developed in Han et al. (2002) are still influential in our algorithm design.
 finds the most frequent patterns early in the mining process and allows dynamic rais-ing of the min _ support threshold, which is then used to prune unpromising branches in the search space. Also, we propose an efficient closed pattern verification method desired closed sequential patterns. The efficiency of our mining algorithm is further improved by applying the minimum-length constraint in the mining and by employ-ing the early-termination conditions developed in CloSpan (Yan et al. 2003). parable or better performance than CloSpan , currently the most efficient algorithm for mining closed sequential patterns, even when CloSpan is running with the best tuned min _ support .

The rest of the paper is organized as fo llows. In Sect. 2, some basic concepts of sequential pattern mining are introduced and the problem of mining the top-k frequent sequential patterns without minimum support is formally defined. Section 3 presents the algorithm for mining top-k frequent closed sequential patterns. A per-formance study is reported in Sect. 4. Section 5 gives an overview of the related work on sequential pattern mining and top-k frequent pattern mining. We also dis-cuss extensions of our method and suggestions for future research in this section.
Section 6 concludes this study.
In this section, we define the basic concepts in sequential pattern mining and in-troduce the problem of mining the top-k frequent sequential patterns. The notations used here are similar to Yan et al. (2003).
 generality, we assume that the items in each itemset are sorted in a certain order in the sequence, i.e, | s |= m . The length, l ( s ) sequence, i.e., l ( s ) = m i = 1 | t i | . A sequence of another sequence  X  = b 1 , b 2 ,... , b n , denoted as  X   X  ), if and only if  X  i a  X  b contains  X  .If  X  contains  X  and their supports are the same, we say
A sequence database, D ={ s 1 , s 2 ,... , s n } , is a set of sequences. Each sequence is associated with an ID . For simplicity, say the ID of s number of sequences in the database D . The (absolute) support of a sequence a sequence database D is the number of sequences in D that contain ( X ) =|{ s | s  X  Dand  X  s }| .

Definition 2.1. (top-k closed sequential pattern) A sequence s is a sequential pat-tern in a sequence database D if its support (i.e., occurrence frequency) in D is no exists no sequential pattern s such that (1) s s ,and(2) support
A closed sequential pattern s is a top-k closed sequential pattern of minimal length min _ if there exist 1 no more than ( k  X  1 ) closed sequential patterns for which the length is at least min _ and for which support is higher than that of s .
Our task is to mine the top-k closed sequential patterns of minimal length min _ efficiently in a sequence database.
 Example 1. Table 1 shows a sample sequence database. We refer to this dataset as the top 2 closed sequential patterns with min _ = 2in D . The output should be ( to 3: ( ac ) : 3 , ( c )( e ) : 3, they are not in the result s et because they are not closed and both of them are absorbed by ( ac )( e ) : 3.

Application scenario. Although top-k sequential pattern mining has its applications in customer shopping sequence mining, it is interesting to note that it can be applied to improve the performance of computer storage systems (Li et al. 2003). Li et al. (2003) applies CloSpan to find block correlations in disk access sequences. A disk access sequence is a sequence of blocks like b 35 , b 100 block on the disk. Suppose an access to b 35 is repeatedly followed by an access to b 9039 , it may improve the I/O performance if we arrange these two blocks adja-cent or fetch them together. When we mine c losed sequential patterns in disk access sequences, the number of sequential patterns returned may vary a lot based on differ-ent support thresholds. In practice, it is difficult for users to provide an appropriate support threshold. However, the users may have an estimation about the number of most cases, it is sufficient to achieve good performance by optimizing the top thou-sands of correlated blocks. Thus, top-k sequential pattern mining paves the way for this kind of application.
This section presents our method, TSP , for mining top-k closed sequential patterns without a given minimum support threshold. First, we introduce the concept of pre-fix projection-based sequential pattern mining and the PrefixSpan algorithm (Pei et al. 2001), which provides the background for the development of our method. Next, we present a novel multipass search space traversal algorithm for mining the most frequent patterns and an efficient method for closed pattern verification and the min-imum support raising during the mining pro cess. Finally, we propose two additional optimization techniques that further improve the efficiency of the algorithm.
Here we briefly introduce PrefixSpan (Pei et al. 2001) and CloSpan (Yan et al. 2003), and then focus on the design of TSP .

Definition 3.1. Given a sequence s = t 1 ,... , t m andanitem concatenates with  X  . s  X  can be an I-step extension (Ayres et al. 2002), s t ,... , t s
For example, ( ae ) is an I-step extension of ( a ) , whereas extension of ( a ) . Yan et al. (2003) extends the definition of item extension to sequence extension.

Definition 3.2. Given two sequences, s = t 1 ,... , t m and p means s concatenates with p . It can be itemset extension, s t ,... , t t ,... , t
For example, ( ac ) is a prefix of ( ac )( d )( e ) and
Definition 3.3. An s -projected database is defined as D such that r is the minimum prefix (of s ) containing s (i.e., s rand r Notice that p can be empty.

For Table 1, D ( ac ) ={ ( d )( e ) , ( _ f )( e ) , ( e item c in ( ac ) come from the same itemset. For each suffix sequence p in D type of extension, i.e., whether s is an itemset extension or a sequence extension of s , is recorded. The type of extension helps correctly grow s using the projected database.
 Assume that there exists a lexicographic o rder in the set of all items in a database.
Set lexicographic order is a linear order defined as follows: Let t t ={ j iff either of the following is true: 1. for some h ,1 h min { k , l } ,wehave i r = j r for r 2. k &lt; l ,and i 1 = j 1 , i 2 = j 2 ,... , i k = j k .

For example, ( a , f )&lt;( b , f ) , ( a , b )&lt;( a ,
Based on this set lexicographic order, sequence lexicographic order is given as follows: (i) if s = s p ,then s &lt; s ; (ii) if s =  X  i p and s the order relation between p and p is, s &lt; s ; (iii) if s p &lt; p indicates s &lt; s ; and (iv) if s =  X  s &lt; s .

For example, ( ab ) &lt; ( ab )( a ) (i.e., a sequence is greater than its prefix); ( ab ) &lt; ( a )( a ) (i.e., a sequence-extended seque nce is greater than an itemset-extended sequence if both of them share the same prefix).

We construct a lexicographi c sequence tree as follows: 1. Each node in the tree corresponds to a sequence, and the root is a null sequence. tension of s or a sequence extension of s . 3. The left sibling is less than the right sibling in sequence lexicographic order.
Figure 1 shows a lexicographic sequence tree for mining the sample database in Table 1 with min _ support = 2. The numbers in the figure represent the support of each frequent sequence. We define the level of a node by the number of edges from the root to this node. If we do preorder transversal in the tree, we can build an operational picture of lexicographic seque nce tree (Fig. 2). It shows that the process extends a sequence by adding an I-step item or an S-step item.

Algorithm 3.1 from PrefixSpan (Pei et al. 2001) provides a general framework its projected database D s , it performs I-step extension (line 6) and S-step extension Algorithm 3.1 PrefixSpan covered. Line 3 shows the termination condition: when the number of sequences in the s -projected database is less than min _ support , it is unnecessary to extend s any more.
Because our task is to mine top-k closed sequential patterns without min _ support threshold, the mining process should start with min _ support sively during the mining process, and then use the raised min _ support to prune the search space. This can be done as follows: as soon as at least k closed sequential patterns with length no less than min _ are found, min _ support can be set to the support of the least frequent pattern, and this min _ support -raising process continues throughout the mining process. This min _ support -raising technique is simple and can lead to efficient mining.
However, there are two major problems that need to be addressed. The first is how to verify whether a newly found pattern is clos ed. This will be discussed in Subsect. 3.3.
The second is how to raise min _ support as quickly as possible. When min _ support is initiated or is very low, the search space will be huge and it is likely to find many patterns with pretty low support. This will lead to the slow raise of min _ support .As when enough patterns with higher support are found. Moreover, because a user is only interested in patterns with length at least min _ , many of the projected databases built at levels above min _ may not produce any frequent patterns at level min _ and below. Therefore, a na X ve mining al gorithm that traverses the search space in lexicographic order will make the mining of the top-k closed sequential patterns very slow. Breadth-first search also does not wo rk. Because short sequences may not have high frequency, one has to access lots of use less low-support short sequences before finding any high-support long sequences.

In this section, we propose a heuristic search space traversal algorithm that in quential patterns mining algorithm, even when the latter is tuned with the most ap-propriate min _ support threshold.
First, let us define what we mean by optim al traversal of the search space for top-k mining. Assuming that we have found the k most frequent closed sequential patterns for a given database, we call the support of the least frequent pattern final _ support .
This is the maximum min _ support that one can raise during the mining process. In example 1, final _ support = 3.

For the purpose of top-k mining, the optimal traversal of the search space (i.e., database) that has support less than final _ support , i.e., if the final _ support is given,
PrefixSpan will traverse the search space optimally in terms of our top-k mining problem. Figure 3 shows a prefix search tree constructed during an optimal traversal of the search space for example 1. It is important to note that optimality is defined here only for the basic search space traversal algorithm that will be used in our top-k mining method. The search space can be pruned further using other techniques such as the min _ constraint and the early-termination conditions discussed in the next subsections. This subsection develops the base algorithm that can traverse the search additional optimization techniques on it.

Algorithm 3.2 is a hypothetical algorithm that traverses the first min _ levels of search space greedily without raising min _ support . The patterns of length less min _ support . Algorithm 3.2 runs in the way that it always picks the most promising branch in the prefix search tree and does a depth-first search. A promising branch means there are lots of closed sequences of length longer than min _ in this branch and their support is very high. We can set different criteria to measure which branch may be promising. Here, we select the pattern that is the most frequent one among all patterns having the same length. After the algorithm reaches the level min _ node, the algorithm calls PrefixSpan to mine the descendant nodes completely. At the same time, it raises min _ support using the method described above. Algorithm 3.2 GreedyTraversal to set min _ support = 1 at the beginning. That is, one has to mine patterns with min _ support = 1 and build project databases for them before finding the first k closed sequences of length min _ . This is inefficient. Moreover, it is difficult to im-plement the criteria shown in line 5. In the next subsection, we propose a multipass, heuristic-based mining algorithm, which mitigates these problems.
Our goal is to develop an algorithm that builds as few prefix-projected databases with support less than final _ support as possible. Actually, we can first search the most promising branches in the prefix search tree in Fig. 2 and use the raised min _ support to search the remaining branches. The algorithm is outlined as follows: (1) initially (during the first pass), bu ild a small, limited number of projected databases for each limitation on the number of projected databases that are built and (3) repeat the mining again. Each time we reach a projected database D s we mine D s completely and use the mined sequences to raise min _ support . The stop condition for this multipass mining process is when all projected databases at level min _ with support greater than min _ support are mined completely. We limit the number of projected databases constructed at each level by setting different support thresholds for different levels. The reasoning behind this is that, if we set a support threshold that is passed by a small number of projected databases at some higher lower levels and vice versa. Algorithm 3.3 TopSequencesTraversal
Algorithm 3.3 performs a single pass of TSP . In order to find the complete result
The limit on the number of projected databases that are built during each pass is enforced by function GetLevelTopSupportFromHistogam, which uses histograms of the supports of the sequences found earlier in the same pass or in the previous passes and the factor  X  , which is set in the beginning of each pass. Figure 4 illustrates the multipass mining on the problem setting from example 1; the bolded lines show the branches traversed in each pass. In this ex ample, the mining is completed after the are no unvisited branches with support greater than or equal to 3.

In our current implementation, the factor  X  is a percentile in the histograms and the function GetLevelTopSupportFromHistogam returns the value of the support at the  X  th percentile in the histogram. The initial value of ginning of the mining process using the following formula: where N Items is the number of distinct items in th e database. In each of the following passes, the value of  X  is doubled. Our experiments show that the performance of the top-k mining algorithm does not change significantly for different initial values of  X  as long as they are small enough to divide the mining process in several passes. we use a tree structure that stores the projected databases built in the previous passes.
We call this structure Projected Database Tree or PDB-tree. The PDB-tree is a mem-ory representation of the prefix search tree and stores information about partially mined projected databases during the mu ltipass mining proce ss. Because the PDB-tree consists of partially mined projected databases, once a projected database is completely mined, it can be removed fro m the PDB-tree. Because of this property, versed during the mining process. The maximum depth of the PDB-tree is always less than min _ because TSP mines all projected databases at level min _ and below completely. In order to further reduce the memory required to store the PDB-tree, we use pseudoprojected databases at the nodes of the PDB-tree, i.e., we only store lists of pointers to the actual sequences in the original sequence database. Figure 5 shows an example of a PDB-tree, where each sear ched node is associated with a projected database.
Now we come back to the question raised earlier in this section: How can we guar-antee that at least k closed patterns are found so that min _ support can be raised in mining? Currently, CloSpan mines closed sequential patterns. CloSpan stores candi-dates for closed patterns during the mining process and, in its last step, it finds and removes the nonclosed ones. This approach is infeasible in top-k mining because it needs to know which pattern is closed and accumulates at least k closed patterns before it starts to raise the minimum support. Thus, closed pattern verification cannot be delayed to the final stage.
 that there exists no pattern in the datab ase that can absorb more than one pattern in the current result set. Otherwise, if such a pattern exists, it may reduce the number of patterns in the result set down to below k and make the final result incomplete or incorrect. For example, assume k = 2 , min _ = 2, and the patterns found so far are { ( a result can be jeopardized because, durin g some part of the mining, one might have used an invalid support threshold.
 Here we present a technique that handles this problem efficiently.

Definition 3.4. Given a sequence s , s  X  D , the set of the sequence id sofall sequences in the database D that contain s is called the sequence ID list, denoted by SIDList ( s ) .Thesumof SIDList ( s ) is called the sequence ID sum, denoted by SIDSum ( s ) .

If the sequences in the original database D do not have numeric identification numbers, we can assign such numbe rs when we scan the database.

Remark 3.1. Given sequences s and s ,if s s and support then SIDList ( s ) = SIDList ( s ) and SIDSum ( s ) = SIDSum
Rationale. Because s is a subsequence of s , s is contained in all sequences in the database that contain s . Also, s cannot be contained in any sequences that do not contain s because s and s have the same support. Therefore, SIDList SIDList ( s ) and SIDSum ( s ) = SIDSum ( s ) .

Lemma 3.1. Given sequences s and s ,if support ( s ) = support = SIDList ( s ) , then neither s is a subpattern of s nor is s a subpattern of s . Rationale. This lemma can be easily proven by contradiction using Remark 3.1.
Assume s s . Given that support ( s ) = support ( s ) ,wehave SIDList
SIDList ( s ) from Remark 3.1, which is a contradiction. In the same way, we can prove that s s is not possible either.

Remark 3.2. If there exists a frequent item  X ,  X   X  D support ( s ) or support ( s i  X ) = support ( s ) ,then s should not be added to the current top-k result set because there exists a superpattern of s with the same support.
Rationale. Because support ( s s  X ) = support ( s ) and s pattern and should not be added to the current result set. Similarly, we can prove it for the case of itemset extension.

Based on Remarks 3.1 and 3.2 and Lemma 3.1, we developed an efficient veri-fication mechanism to determine whether a pattern should be added to the top-k set and whether it should be used to raise the support threshold.

A prefix tree, called TopK _ Tree , is developed to store the current top-k result set in memory. Also, in order to improve the efficiency of the closed pattern verification, a hash table, called SIDSum _ Hash , is maintained that maps sequence ID sums to the nodes in TopK _ Tree .

In our top-k mining algorithm, when a new pattern is found, the algorithm takes one of the following three actions: (1) add_and_raise : the pattern is added to the top-k result set and is used to raise the support threshold, (2) add_but_no_ raise :the pattern is added to the top-k result set but is not used to raise the support threshold and (3) no_add : the pattern is not added to the top-k result set.
 Algorithm 3.4 Closed pattern verification the support threshold min _ support . This eliminates the problem mentioned earlier: lead to less than k patterns in the result set. In summary, our strategy is to maintain new pattern.
Now we discuss how to reduce the search space using the minimum length constraint min _ .

Remark 3.3. (Minimum length constraint) For any sequence s l ( s )&lt; min _ , the sequence s will not contribute to a frequent sequential pattern of minimum length min _ , and it can be removed from the projected database D each projected sequence to see whether it is shorter than min _ it to the projected database.
 a projected database D s only when l ( s )&lt; min _  X  longer than min _  X  2, the program does not need to check the length of the projected sequences.
Early termination by equivalence is a search space reduction technique developed in CloSpan (Yan et al. 2003). Let ( D ) represent the total number of items in D , defined as
We call ( D ) the size of the database. For the sample dataset in Table 1,
The property of early termination by equivalence shows, if two sequences s s and (
D of s in the lexicographical sequence tree must not be closed. Furthermore, the de-scendants of s and s are exactly the same. CloSpan uses this property to quickly prune the search space of s .

To facilitate early termination by equivalence in the top-k mining, we explore both the partially mined projected database tree, PDB _ Tree , and the result set tree,
TopK _ Tree . Two hash tables are maintained: one, called PDB _ Hash , mapping data-base sizes to nodes in PDB _ Tree and the other, called TopK _ Hash , mapping database sizes to nodes in TopK _ Tree .

For each new projected database D s that is built, we search the two hash tables using ( D s ) as a key and check the following conditions:  X 
If there exists a sequence s , s  X  PDB _ Tree , such that s s , then stop the search of the branch of s .  X 
If there exists a sequence s , s  X  PDB _ Tree , such that s s , then remove s from PDB _ Tree and continue the mining of the branch of s .  X  If there exists a sequence s , s  X  TopK _ Tree and s  X 
With this adoption of early termination in TSP , the performance of TSP is im-proved significantly.
This section reports the performance testing of TSP in large data sets. In particu-lar, we compare the performance of TSP with CloSpan . The comparison is based on assigning the optimal min _ support to CloSpan so that it generates the same set of top-k closed patterns as TSP for specified values of k and min _ . The optimal min _ support is found by first running TSP under each experimental condition. Be-cause this optimal min _ support is hard to speculate without mining, even if TSP achieves similar performance as CloSpan , TSP is still more valu able because it is much easier for a user to work out a k value for top-k patterns than a specific min _ support value.

The datasets used in this study are generated by a synthetic data generator pro-vided by IBM. It can be obtained at http://www.almaden.ibm.com/cs/quest. Table 2 shows the major parameters that can be speci fied in this data generator; more details are available in Agrawal and Srikant (1995).

All experiments were performed on a 1.8-GHz Intel Pentium-4 PC with 512 MB main memory, running Windows XP Professional. Both algorithms are written in C++ using STL and compiled with Visual Studio .Net 2002.

The performance of the two algorith ms has been compared by varying min _ and k .When k is fixed, its value is set to either 50 or 500, which covers the range of typical values for this parameter. Figures 6 and 7 show performance results for dataset D100C5T2.5N10S4I2.5. This dataset consists of relatively short sequences, each sequence contains 5 itemsets on ave rage and the itemsets have 2.5 items on average. The experimental results show that TSP mines this dataset very efficiently and, in most cases, runs several times faster than CloSpan . The difference between the running time of the two algorithms is more significant when longer patterns are mined (larger min _ ). There are two major reasons for the better performance of
TSP in this dataset. First, it uses the min _ constraint to prune short sequences during the mining process, which in some cases significantly reduces the search space and improves the performance. Second, TSP has a more efficient closed pattern verification scheme and stores a result set that contains only a small number of closed patterns, while CloSpan keeps a larger number of candidate patterns that could not be closed and removes the nonclosed ones at the end of the mining processes.
Figures 8 and 9 show the experiments on dataset D100C10T10N10S4I5, which consists of longer patterns compared with the previous one. The average number the two algorithms have comp arable performance when min _ is small. The reasons for the similar performance of the two algorithms are that the benefit of applying the min _ constraint is smaller because the seque nces in the dataset are relatively longer. However, as indicated by Fig. 9, when min _ increases, TSP runs faster than CloSpan again.

As we can see, min _ plays an important role in improving the performance of TSP . If we ignore the performance gain caused by min _ , TSP can achieve the competitive performance with well-tuned CloSpan . We may wonder why minimum support raising cannot boost the performance like min _ does. The rule of thumb is that the support of upper level nodes should be greater than lower level nodes (the support of short sequences should be greater than that of long sequences). Then, few nodes in the upper level can be pruned by the minimum support. Because we cannot access the long patterns without accessing the short patterns, we have to search database of the upper level nodes is very big and expensive to compute. Thus, if we cannot reduce checking the projected databases of the upper level nodes, it is unlikely that we can benefit from a support-raising technique a lot. However, the support-raising technique can free us from setting m inimum support without sacrificing the performance.
 namic raising of min _ support avoids the construction of a large number of unnec-essary projected databases with support less than final _ support . Even though the top-k algorithm is not given any minimum support threshold, it achieves a similar or even better performance in comparison with CloSpan running with the optimal min _ support threshold.
In this section, we discuss the related work and the directions for further study. We first show the relationships and differences of the TSP algorithm with (1) the
Apriori-based algorithms: AprioriAll, GSP and SPADE, (2) the pattern growth-based algorithms: FreeSpan, PrefixSpan and CloSpan and (3) the top-k frequent pattern-mining algorithm: TFP.
Agrawal and Srikant (1995) introduced the sequential pattern-mining problem and three algorithms to solve it. Among those algorithms, AprioriAll was the only one to mine the complete set of frequent sequential patterns. Later, in Srikant and Agrawal (1996), they proposed the GSP (generalized sequential patterns) algorithm for min-ing sequential patterns. All of these algorithms are based on the Apriori property proposed in association rule mining (Agrawal and Srikant 1994) and a candidate generation-and-test approach. The Aprior i property states that any superpattern of a nonfrequent pattern cannot be frequent. Using this heuristic, AprioriAll and GSP narrow down the search space for frequent sequential patterns drastically. To mine frequent sequences with length ( l + 1 ) , the Apriori-based algorithm needs to find all the candidate length-( l + 1 ) sequences from their previously derived length-l fre-quent sequences, scan the database one more time to collect their counts, which makes them inefficient for mining long patterns.
Zaki in Zaki (2001) proposed a new approach for mining frequent sequential patterns, called SPADE (sequential pattern discovery using equivalence classes). This algorithm uses vertical ID -list database format, i.e., for each item an ID list of the identifiers of the sequences in which it appears and their corresponding time stamps are created.
The frequent sequential patterns are mine d by performing temporal join operations on these ID lists. SPADE decomposes the original problem into smaller subproblems, which can be independently solved in main memory, using lattice search techniques.
SPADE outperforms GSP by a factor of two and by an order of magnitude with some preprocessed data. This method can mine the complete set of frequent sequences in only three database scans. FreeSpan (Frequent pattern-projected sequential pattern mining) was introduced by
Han et al. in Han et al. (2000). It uses frequent items to recursively project sequence databases into a set of smaller projected databases. The subsequent mining is limited to each of these smaller projected databases. FreeSpan is significantly more efficient than the Apriori-based GSP . The problem of FreeSpan is that the same sequence can be repeated in many projected databases. For example, if a sequential pattern appears in each sequence in the database, its projected database will have the same size as the original database, except for the infrequent items that will be removed.
In a later work (Pei et al. 2001) Pei et al. introduced PrefixSpan (prefix-projected sequential pattern mining). Its general idea is to examine only the prefix subse-quences and project only their corresponding suffix subsequences into projected databases. In each projected database, sequential patterns are grown by exploring only local frequent patterns. PrefixSpan runs considerably faster than both GSP and
FreeSpan , especially when longer sequential patterns are mined.
CloSpan (Yan et al. 2003) (closed sequential patterns mining) is a recently proposed closed sequence mining algorithm. It uses depth-first search and a prefix-projected database method to enumerate t he frequent sequential patterns. CloSpan developed a novel technique, called early termination by equivalence, which can efficiently de-termine whether there are new closed pa tterns in search subspaces and terminate the search of subspaces that do not contain such patterns. CloSpan outperforms
PrefixSpan by more than one order of magnitude and is capable of mining longer frequent sequences in large databases with low minimum support. An alternative to closed sequence mining is maximal sequence mining (Chen et al. 1996). A max-imal sequence is a frequent sequence that i s not contained in any other frequent sequence. One may lose support information on frequent sequences when mining maximal sequences. 5.1.5. TFP : Mining top-k frequent closed patterns without minimum support
The algorithms reviewed in the last four subsections mine frequent sequential patterns in sequence databases using user-specified min _ support threshold. Our algorithm has a frequent pattern-mining counterpart, TFP (Han et al. 2002), that mines frequent closed itemsets in transaction databases. Even though TFP does not mine sequential patterns, it is closely related to TSP because TFP is the first study on mining top-k frequent closed patterns with minimum length constraint.

TFP is an FP tree (Han et al. 2000) -based frequent pattern-mining algorithm for finding the top-k frequent closed patterns without a predefined min _ support thresh-old. TFP starts the mining process with min _ support threshold equal to 1 and raises the support threshold during both the FP-tree construction and the mining of the
FP-tree. TFP explores top-down and bottom-up combined FP-tree mining processes to first mine the most promising tree branch es. Also, an efficient closure verification scheme is developed to determine whether the newly discovered patterns are closed.
TFP in most cases achieves better performan ce than two of the most efficient frequent closed pattern-mining algorithms, CLOSET (Pei et al. 2000) and CHARM (Zaki and
Hsiao 2002), even when they are running with the best tuned min _ support thresh-old. TFP concludes that mining the top-k frequent patterns without min _ support can be efficient and should be more pre ferable than the traditional min _ support -based mining.
 and provides an efficient solution to this problem in the more challenging setting of mining frequent closed sequential patterns in sequence databases.
There are several issues related to our algorithm for mining top-k frequent closed sequential patterns that should be studied further. For example, more sophisticated methods for determining the parameter  X  that controls the level support thresholds appropriate values of  X  for the subsequent passes. Another potential optimization is it reaches level min _ . This can avoid traversal of long branches in the beginning of the mining and will raise min _ support faster. However, the breath-first traversal also has its disadvantages: it requires more memory and limits the usage of early termination by equivalence. Thus, a comprehensive study on a variety of datasets needs to be done to evaluate in what situations the algorithm should use breath-first search instead of depth-first search.
 experiments on real datasets need to be done. Other directions for future research include incorporation of user-specified constraints (Garofalakis et al. 1999; Pei et al. 2002) into the mining of top-k closed sequential patterns and extension of the method to mining other complicated structured patterns, such as closed graph patterns (Yan and Han 2003).
In this paper, we have studied the problem of mining top-k (frequent) closed se-quential patterns with length no less than min _ and proposed an efficient mining algorithm TSP , with the following distinct featur es: (1) it adopts a novel, multipass search space traversal strategy that allows mining of the most frequent patterns early in the mining process and fast raising of the minimal support threshold min _ support dynamically, which is then used to prune the search space; (2) it performs efficient closed pattern verification during the mining process tha t ensures accurate raising of min _ support and derives correct and complete results and (3) it develops several ad-ditional optimization techniques, including applying the minimum length constraint, min _ , and incorporating the early termination testing proposed in CloSpan . performance and, in ma ny cases, outperforms CloSpan , currently the most efficient algorithm for (closed) sequential pattern mining, even when CloSpan is running with the best tuned min _ support . Through this study, we conclude that mining top-k closed sequential patterns without min _ support is practical and in many cases more preferable than the traditional min _ support threshold-based sequential pattern mining.
