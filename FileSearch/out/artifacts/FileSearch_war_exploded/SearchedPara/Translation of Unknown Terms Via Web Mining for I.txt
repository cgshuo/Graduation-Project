 For cross-language information retrieval (CLIR), one of the major hindrances to achieving retrieval performance at the level of monolingual information retrieval is the translation of terms in queries, which are not found in a bilingual dictionary. Such terms are also called  X  X ut of vocabulary X  words (OOV) or unknown words [12]. In traditional CLIR approaches, bilingual dictionaries are used to translate queries. New words and phrases are constantly introduced and it would be hard to keep updating a dictionary to include them all. Approaches such as what we propose in this paper can be used to acquire translation knowledge about these terms. 
Mixed use of English and native languages presents a classical problem of vocabulary mismatch in monolingual information retrieval (MIR). The problem is significant especially in Asian language because words in the native language are often mixed with English words. Although English terms and their equivalences in a local language refer to the same concept, they are erroneously treated as independent units in traditional MIR. Such separation of semantically identical words in different languages may limit retrieval performance. For instance, Google search engine indexed 2,220,000 Web pages that contain  X   X  but not its English counterpart  X  X amsung X , about 46,000,000 pages that contain  X  X amsung X  but not  X   X  and about 183,000 for both of them. A user would expect that a query with either  X  X amsung X  or  X   X  would retrieve information from all of these three groups of Korean Web pages. Otherwise some potentially useful information will be ignored. There should be a semantic equivalence relation between them. Furthermore, one English term may have several corresponding terms in a different language. For instance, words  X   X ,  X   X , and  X   X  are found in Korean texts, which all correspond to the English word  X  X igital X  but are in different forms because of different phonetic interpretations. Establishing an equivalence class among the three Korean words and the English counterpart is indispensable. By doing so, although the query is  X   X , the Web pages containing  X   X ,  X   X  or  X  X igital X  can be all retrieved. Observing the fact that many specific phrases sometimes appear together with their English translation in Web texts, for example,  X ... (Roh, MooHyun).... X , it is possible to mine the bilingual search-result pages obtained from Web search engines to extract proper translations for specific terms which are treated as queries. Nagata [10] firstly suggested using the search-result pages for translation of specific Japanese terms which had not been registered in the bilingual lexicon yet. He queried a search engine with Japanese terms to be translated, and downloaded the top 100 Web documents returned from the search engine to find the proper English translations. from the search-result pages returned by a search engine. Instead of downloading the documents listed in the sear ch-result pages, Cheng only employed the snippets (including titles and page descriptions) of those documents in th e search-result pages. Therefore, it greatly reduced the complexity of mining process with a satisfied result. 
While mining the Web to translate words has been studied extensively in the statistical methods on Chinese-English pair. In this paper, we are interested in translating Korean specific phrases into English. More specifically, due to the Altai language nature of Korean, we integrate the phoneme and semanteme instead of statistical information alone to pick out the right translation from the search-result pages. Although the technique we developed has values in their own right and can be applied for other language engineering fields, we intend to understand to what extent monolingual IR case when relevant terms in different language are treated as one unit in index and the contribution of specific term translation for Korean-English CLIR. The procedure to translate OOV (out-of-vocabulary)-containing phrases by mining the search engine with an OOV-containing phrase to retrieve the search-result pages, a list of snippets of Web pages containi ng the phrase. Then, extract candidates as the potential English translations from the search-result pages. Finally, select one or more candidates as right translations for the OOV by statistical, phonetic, and semantic models. While the main thrust of the procedure can be applied to other languages like Chinese and Japanese, we are only focusing on Korean language in this paper and deal with Chinese in [8] where similar techniques are applied to resolve concept unification for IR. We query the search engine, Google, with an OOV term to retrieve the snippets of of these Web pages contain both the term and its translation as in the example in Introduction Section; only 183,000 Web pages contain both  X   X  and its counterpart  X  X amsung X  whereas 2,220,000 pages contain  X   X  only. In order to obtain Web pages with both the OOV query term and its English translation at high ranks, we need would co-occur with the target translation. Hint words can be obtained by searching the engine with the OOV query term first, selecting the top-n topic-relevant words using simple statistics like tf idf  X  , and translating them into English. 
Our method differs from Zhang X  X  method [12] that expands the query with one hint word at one time and collects all the snippets from several query sessions, in that we query the search engine with all the hint words included with an OR operator. This strategy not only saves the searching time, but also helps avoiding translation ambiguity of individual hint words because the Web pages containing multiple hint words are likely to be semantically more relevant than those containing only one. For example, an OOV term  X   X  (Faust) can be used as the query to the search engine to retrieve the top-n topic-relevant words. The top-three candidates are  X   X  (Goethe),  X   X  (introduction), and  X   X  (literature). Thus the query with the hint words to collect the snippets including translation of  X   X  becomes: { X   X + &lt; X  X oethe X  or  X  X ntroduction X  or  X  X iterature X &gt;}, instead of three different queries in Zhang X  X  approach. 
In this example,  X   X  can be translated into  X  X ntroduction X ,  X  X ecommendation X ,  X  X resentation X ,  X  X ispersal X ,  X  X ispersion X ,  X  X issemination X  by Korean-English WordNet. While we can choose one based on a sense disambiguation method using pseudo-disambiguation would occur with the other hint words while searching is carried out. The pages retrieved by  X  X oe the X  and  X  X iterature X  are likely to contain  X  X ntroduction X  rather than  X  X ispersion X . In other words, multiple hint words in a query would mutually disambiguate themselves and hence the retrieved pages are likely to contain the OOV term X  X  translation. 4.1 Extraction of Candidates for Selection After querying the search engine with the OOV term and its hint words, the next step is to extract English candidates within a window of a limited size, which includes the OOV term, in the snippets of Web texts in the returned search-result pages. Because the alignment types of translation pairs are diverse, (e.g. a Korean word  X   X  is aligned with three English words  X  X ad cow disease X ), we should combine English words into proper units as candidates, instead of making each single word as candidates. There are two typical ways, one is to group the words based on the co-sequential combination of the words as the candidates [13]. Although the first reduces the number of candidates, it risks losing the right combination of words as candidates. discontinuous word sequences as candidates. For example, in the phrase  X  X resident George W Bush X , the best 2-word entity is  X  X resident Bush X , or perhaps  X  X eorge Bush X , but certainly none of the adjacent combinations ( X  X resident George X ,  X  X eorge W X ,  X  X  Bush X ). Therefore, we use not only all adjacent combinations but also the discontinuous sequential combinations with skipped words. The maximum number of skipped words is 2. The final candidates for the previous example are shown in Table1. 4.2 Selection of Candidates OOV term. We present a method that considers the statistical, phonetic and semantic features of the English candidates for selection. 4.3 Statistical Model There are several statistical models to rank the candidates [10][12][13]. In our statistical model, we consider the frequency, length and location of candidates term; and the longer the candidates X  length, the higher the chance to be the right translation. The formula to calculate the ranking score is c , the word distance is one. If there is one word between them, it is counted as two dqc among all the candidates. 4.4 Phonetic and Semantic Model 4.4.1 Phonetic and Semantic Match There has been some related work on extracting term translation based on the transliteration model [3][5]. Different from Korean-English transliteration that approach is a kind a match problem since we already have English candidates and aim at selecting the right candidates. 
While the transliteration method is partially successful, it suffers from the problem that transliteration rules are not applied consistently. Korean phrase for which we are looking for the translation sometimes contains several words that may appear in a dictionary as an independent unit. Therefore, it can only be partially matched based on the phonetic similarity, and the rest part may be matched by the semantic similarity in such situation. 
Returning to the above example,  X   X  is matched with  X  X lone X  by phonetic similarity.  X   X  and  X   X  are matched with  X  X f X  and  X  X ttack X  respectively by semantic similarity. The objective is to find a set of mappings between the English word(s) in candidates and the Korean word(s) in a phrase, which maximize the sum of the semantic and phonetic mapping weights. We call the sum as SSP (Score of semanteme and phoneme). The higher SSP value is, the higher the probability of the candidate to be the right translation. 
The solution for a maximization problem can be found using an exhaustive search method. If the data we process get really big, however, performance degrades to an astonishing extent. As shown in Figure 1, the problem can be represented as a bipartite weighted graph matching problem. Let the English phrase, E, be represented as a sequence of tokens 1 ,..., m ew ew &lt;&gt; , and the Korean phrase, K, be represented as a sequence of tokens 1 ,..., n kw kw &lt;&gt; . Each English and Korean token is represented as a the average of normalized semantic and phonetic values, whose calculation details are explained in the following sections. In order to balance the number of vertices on both sides, we add the virtual vertex (vertices) with zero weight on the side with less number of vertices. The SSP is calculated: where  X  is a permutation of {1, 2, 3, ..., n}. It can be solved by the Kuhn-Munkres algorithm (also known as Hungarian algorithm) with polynomial time complexity [9]. 4.4.2 Phonetic and Semantic Weights Phonetic weight is the transliteration probability between Korean and English candidates. We adopt our previous method in [3] with some adjustments. In essence, we compute the probabilities of particular Korean specific phrase, KW, given a candidate in English, EW. where 11 (| ) jj j j Pkk g g ++ is computed from the training corpus as the ratio between the frequency of 1 jj kk + in the candidates, which were originated from 1 jj gg + in English substituted with a space mark. 
The semantic weight is calculated from the bilingual dictionary. The current dictionary we employed for the local languages are Korean-English WordNet and LDC Chinese-English dictionary with additional entries inserted manually. The weight relies on the degree of overlaps between an English translation and the candidate 
For example, given the Korean phrase  X   X  (Inha University) and its candidate  X  X nha University X ,  X  X niversity X  is translated into  X   X , therefore, the semantic weight between  X  X niversity X  and  X   X  is about 0.33 because only one third of the full translation is available in the candidate. 
Due to the range difference between phonetic and semantic weights, we normalized them by dividing the maximum phonetic and semantic weights in each pair of the Korean phrase and a candidate if the maximum is larger than zero. aspects from the others. If the SSP values of all candidates are less than the threshold, the top one obtained by statistical model is selected as the final translation. Otherwise, we re-rank the candidates according to the SSP value. Then we look down through the new rank list and draw a  X  X irtual X  line if there is a big jump of SSP value. If there is no big jump, the  X  X irtual X  line is drawn at the bottom of the new rank list. Instead of the top-1 candidate, the candidates above the  X  X irtual X  line are all selected as the final translations. It is because that a Korean phrase may have more than one correct translation. For instance, the Korean phrase  X   X  corresponds to two English translations  X  X acau X  and  X  X acou X . The candidate list based on the statistical information is  X  X acau, china, macou ... X . We then calculate the SSP value of these candidates and re-rank the candidates whose SSP values are larger than the threshold which we set to 0.3. Since the SSP value of  X  X acau (0.892) X  and  X  X acou (0.875) X  are both larger than the threshold and there is no big jump, both of them are selected as the final translation. We conducted extensive experiments for phrase translation, query translation in CLIR, and indexing for Monolingual IR. We first compared the phrase translation approach against other systems. We then examined the impact of our approach in CLIR. Finally, we wanted to know the impact of phrase translation on monolingual IR, with which terms in different languages are treated as one unit in the index. 
We selected 200 phrases from NTCIR-3 Korean corpus and manually found the translations for these phrases as the evaluation data. Table 2 shows the results in terms of the top 1, 3, 5 and 10 inclusion rates.  X  X iveTrans X  and  X  X oogle X  represent the systems against which we compared the translation ability. Google provides a machine translation function to translate text such as Web pages. Although it works contextual information is available for translation. LiveTrans [1] provided by the WKD lab in Academia Sinica is the first unknown word translation system based on higher precision is based on  X  X ontext-vector X  method (CV) and  X  X hi-square X  method and statistic model plus phonetic and semantic model, respectively. individual. For instance,  X  X ordan X  is the English translation of Korean term  X   X , sometimes misguides the selection. However, in our two-step selection approach, the final selection would not be diverted by the false statistic information. In addition, in order to examine the contribution of distance information in the statistical method, we ran our experiments based on statistical method (ST) with two different conditions. In of all candidates is ignored. In the second case, (, ) ki dqc is calculated based on the real textual distance of the candidates. As in table 2, the later case shows better performance. It also can be observed that  X  X T+PS X  shows the best performance, then followed by  X  X iveTrans (smart) X ,  X  X T X ,  X  X iveTrans (fast) X , and  X  X oogle X . The statistical methods seem to give a rough estimate for potential translations without giving high precision. Considering the contextual words surrounding the candidates and the query phrase can further improve the precision but still less than the improvement by phonetic and semantic information in our approach. 
The Korean-English CLIR experiments are based on the NTCIR-3 English corpus narrative and keywords relevant to the topic. In our experiment, only the title part was used as queries to retrieve the documents from the English document collection because the average length of the titles, about 3 terms, is closer to the real situation of user queries. The bilingual dictionary we used is Korean-English WordNet with additional entries inserted manually. Using this dictionary, 9 out of 32 Korean queries can not find English translation for at least one phrase. Since the Korean terms in the query may have many senses, some of the translated terms are not related to the meaning of the original query. We used the mutual information [4] to alleviate this translation ambiguity problem. Finally, we searched for the documents relevant to a translated Korean query as well as an original English query using the Okapi BM25 [11]. The main metric is interpolated 11-point average precision and mean average precision (MAP). Four runs are compared to investigate the contribution of lexeme translation in CLIR. Since not all of these 32 queries contain phrase(s) containing OOV, we report used. In the other case, only 9 queries containing phrase(s) with OOV are used. 1. RUN1 (Google): to provide a baseline for our test, we used Google Translation 2. RUN2 (Without OOV): Korean queries were translated using a dictionary look-up 3. RUN3 (With OOV): Korean queries were translated using a dictionary look-up 4. RUN4 (Monolingual): to provide a co mparison with the  X  X deal X  case, we retrieve 
NTCIR has two different criteria to evaluate the retrieval performance. The first is called  X  X igid Relevance X  where  X  X ighly relevant X  and  X  X elevant X  documents are regarded as relevant documents. The second is called  X  X elaxed Relevant X  where  X  X ighly relevant X ,  X  X elevant X  and  X  X artially relevant X  documents are all treated as these two criteria. The precision-recall performance on all queries is shown in Figure 2 and 3. Regardless of which criteria are used between  X  X elaxed Relevance X  or  X  X igid Relevance X , RUN3 (With OOV) shows better performance than RUN1 (Google) and RUN2 (Without OOV), but still performs le ss than RUN4 (Monolingual). We also observed that commercial translation product provided by Google performed almost the same as the RUN2 (Without OOV), where the lexemes containing OOV are ignored during the word-to-word translation. 
Since not all of these 32 queries contain the specific lexemes, we further carried our experiments only with 9 queries containing OOV lexemes. As shown in Figure 4 and 5, translating the OOV lexemes using our approach in CLIR (RUN3) shows quite similar performance to Monolingual case (RUN4). To our surprise, our approach outperformed the monolingual case, especially for high-ranked documents, when we used the  X  X igid Relevance X  criterion. 
In order to understand this situation, we calculated the mean average precision (MAP) per query. As shown in Figure 6 and 7, most queries except query 13, 20, and 27 gave similar retrieval performance between RUN3(With OOV) and RUN4 (Monolingual). Query 13 ( ,  X  X oomsday thought X ) was translated into  X  X schatology X  by our approach. Although it is different from the correct translation  X  X oomsday thought X  provided by NTCIR, it is still an acceptable translation. Unfortunately, only a few documents contain  X  X oomsday thought X  and no documents contain  X  X schatology X , resulting in retrieval failure. For query 20 ( Renault X ), although it performed more poorly in RUN3 (With OOV) based on  X  X elaxed Relevance X  standard in Figure 6, it shows better retrieval accuracy in RUN3 (With OOV) than RUN4 (Monolingual) based on the  X  X igid Relevance X  standard in Figure 7. Since the  X  X igid Relevance X  has a more strict rule to select  X  X elevant X  documents for test judgment, our approach shows a good ability for accurate information retrieval. In addition, the performance enhancement before 5-point is much higher than the 6~10-point area, that is, our approach performs well, especially for high-ranked documents. For query 27 (  X  X acau returns X ), it shows better performance in RUN3 (With OOV) than RUN4 (Monolingual) on both two relevance standards. The corresponding English query of query 27 ( ) provided by NTCIR is  X  X acau returns X , but the translation returned by our approach is  X  X acau macou return X . Since  X  X acau X  and  X  X acou X  are both the right translation of the  X   X , this query gave a better result. By applying our approach, the MAP of these 9 queries increased about 115% from 0.1484 in RUN2 to 0.3186 with  X  X igid Relevance X  judgment criterion and about 105% from 0.1586 to 0.3228 with  X  X elaxed Relevance X  judgment criterion. We ran experiments to examine the impact of our OOV handling method on IR. The retrieval system is based on the vector space model with our own indexing scheme to which the OOV handling part was added. We employed the standard tf idf  X  scheme for index term weighting and idf for query term weighting. Our experiment is based on KT-SET test collection [6], which has 30 queries together with relevance judgments for them. 
We tested two indexing methods. One was to extract the phrase with OOV word(s), which were recognized as index units. The other one was using our approach to translate these phrases into their English equivalents if possible, so the phrases and their corresponding counterparts were treated as the same index units. The baseline against which we compared our approach applied a relatively simple indexing technique. It uses a bilingual dictionary to identify index terms. The effectiveness of the baseline scheme is comparable with other indexing methods [7]. While there is a possibility that an indexing method with a full morphological analysis may perform better than our rather simple method, it would also suffer from the same problem associated with OOV words, which ca n be alleviated by our OOV translation approach. 
We obtained 9.4% improvement based on mean average 11-pt precision when we simply separated phrases with OOV word(s) and used them as index units. Furthermore, when we applied our translation approach to the entire phrases with OOV words so that a matching occurred between English counterparts, the improvement was 14.9% based on mean average 11-pt precision. In this paper, we presented our approach to the problem of automatically translating Korean phrases containing OOV words into English via Web mining. While our current work focused on Korean text, the main thrust of the method is applicable to other languages like Chinese and Japanese. We also studied its contribution to monolingual and cross-lingual information retrieval. As witnessed by previous research as well as in our experiments, translating the OOV words in the query is necessary for an effective cross-lingual information retrieval. Due to the wide use of English terms in Korean texts, it is useful to treat these English words and their corresponding Korean words as a single unit for monolingual IR. Treating terms in different languages as one index unit is not only meaningful to English-Korean pairs but also to others like English-Chinese or even triplets English-Chinese-Korean. This is along the line of work where researchers attempt to index documents with concepts rather than words. We would extend our work along this road in the future. 
