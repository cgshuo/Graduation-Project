 Expertise retrieval has been the subject of intense research over the past decade, particularly with the public availability of benchmark test collections for expertise retrieval in enter-prises. Another domain which has seen comparatively less research on expertise retrieval is academic search. In this paper, we describe the Lattes Expertise Retrieval (LExR) test collection for research on academic expertise retrieval. LExR has been designed to provide a large-scale benchmark for two complementary expertise retrieval tasks, namely, ex-pert profiling and expert finding . Unlike currently available test collections, which fully support only one of these tasks, LExR provides graded relevance judgments performed by expert judges separately for each task. In addition, LExR is both cross-organization and cross-area, encompassing can-didate experts from all areas of knowledge working in re-search institutions all over Brazil. As a result, it constitutes a valuable resource for fostering new research directions on expertise retrieval in an academic setting.
 Academic search; expertise retrieval
The need for expertise emerges whenever proficient knowl-edge is required about a particular topic. Two information retrieval tasks have been proposed to address complemen-tary sides of this need: expert profiling and expert finding [3]. Expert profiling can be seen as a summarization task, aimed at identifying the topics of expertise of a given person. In turn, expert finding (often referred to as expert search) can be seen as a ranking task, aimed at identifying expert per-sons given a topic of interest. These two complementary tasks have been the subject of substantial research over the past decade, which contributed to approaches for summa-rizing, ranking, and evaluating expertise.

One of the key driving forces for the progress of research on expertise retrieval has been the availability of public benchmark test collections. The TREC Enterprise track contributed two such test collections suitable for expert find-ing in an enterprise setting. In particular, the World Wide Web Consortium (W3C) test collection [5] includes W3C working group members as candidate experts, whereas the CSIRO Enterprise Research Collection (CERC) [1] includes CSIRO employees as candidate experts. Test collections de-veloped for expert finding in an academic setting have also been made publicly available. For instance, the UvT test collection [2] and its extended version TU [4] include em-ployees of Tilburg University, the Netherlands, as candidate experts. In turn, the ArnetMiner test collection [10] includes researchers with a DBLP page as candidate experts. Other non-public test collections for expertise retrieval have also been developed [3, see Section 4.2.4]. Table 1 describes core properties and salient statistics of these test collections.
As shown in the top half of Table 1, most of the exist-ing expertise retrieval test collections encompass candidate experts from a single organization. The only exception is the ArnetMiner test collection, which includes computer sci-ence researchers from multiple organizations as candidate experts. On the other hand, both the ArnetMiner as well as the W3C test collections encompass candidate experts from a single knowledge area, namely, computer science and web standards, respectively. In addition, none of these test collections fully supports both the expert profiling as well as the expert finding tasks. Indeed, W3C, CERC, and Ar-netMiner focus on expert finding, with only CERC includ-ing relevance judgments performed by expert judges. In contrast, UvT and TU primarily focus on expert profiling, with expert finding supported via pseudo relevance judg-ments adapted from expert profiling judgments. Moreover, except for TU X  X  support of expert profiling, none of these test collections provides graded relevance judgments, which could enable more discriminative ranking comparisons [9].
In this paper, we introduce the Lattes Expertise Retrieval (LExR) test collection, a new test collection for expertise retrieval in academia. As described in the bottom half of Table 1, LExR addresses the strengths and weaknesses of previous test collections in order to provide a comprehen-sive, large-scale benchmark for evaluating expertise retrieval approaches. In particular, it is both cross-organization and cross-area, encompassing candidate experts from all knowl-edge areas working in research institutions all over Brazil. In addition, it includes graded relevance judgments per-formed by the experts themselves as opposed to external judges. Moreover, additional cross-expert judgments are included for expert finding, which provide improved cov-erage of relevant experts for a given topic as well as bet-ter consensus on the relevance grades assigned to each ex-pert by multiple judges. As a result, LExR coherently sup-ports both expert profiling and expert finding, enabling the investigation of a variety of research questions related to expertise retrieval in academia. In the remainder of this paper, we discuss the methodology behind the construc-tion of the LExR test collection and its salient character-istics. The full test collection is available for download from http://www.lbd.dcc.ufmg.br/lbd/collections.
The LExR test collection is based upon the Lattes plat-information about science, technology, and innovation for individual researchers and research institutions in Brazil [6]. LExR was developed to provide a unified yet coherent bench-mark for research on expert profiling and expert finding in academia. In the remainder of this section, we describe the methodology behind the construction of LExR. In partic-ular, Section 2.1 describes the acquisition of documentary evidence of expertise for a large set of academic experts, whereas Section 2.2 describes the process of obtaining rele-vance judgments for expert profiling and finding.
The Lattes platform currently stores the academic cur-ricula of nearly 4.5 million individuals involved in research activities in Brazil. Each curriculum comprises information about an individual X  X  academic life, including his or her cur-rent and past affiliations, earned degrees, academic services, student supervisions, research projects, and scientific publi-cations. To start off with a large set of promising experts, we obtained the curricula of 206,697 individuals with a doc-torate degree and at least one publication. Figure 1 shows the distribution of candidate experts per broad area of in-terest, as informed in their curriculum (for now, ignore the red bars). As shown in the figure, exact and earth sciences contribute the majority of candidate experts in the collec-tion (36%), followed by biology (27%) and health sciences (16%). The least represented area is linguistics, languages, and arts , which account for less than 1% of all candidates. Figure 1: Candidate experts per area of interest.
I n order to simulate a general-purpose academic search scenario, we retained only metadata records associated with scientific publications, which are typically available in online repositories such as digital libraries and academic social net-works. The resulting 11,942,014 documents comprise multi-ple types of publication, including books and book chapters (8%), journal articles (20%), conference papers (46%), and other published material (26%). Most documents are writ-ten in Portuguese (61%), followed by English (22%), Spanish (9%), and other languages (8%). Among relevant candidates in our ground-truth, English documents are prevalent (51%), followed by Portuguese (37%), Spanish (6%), and other lan-guages (5%). For each document with a digital object iden-tifier (DOI), we further attempted to recover its abstract by scraping the landing page associated with its DOI. Given the enormous amount of publishers in our corpus (precisely, 2,836), we handcrafted scrapers for the 22 most prolific ones, which account for 80% of all documents with a DOI. Factor-ing in download failures, we were able to recover the abstract for a total of 413,356 documents. Another 69,866 abstracts the distribution of profile sizes in terms of the number of documents as well as the number of tokens in each profile. I n both cases, we can observe a long-tail distribution with most candidates having relatively small profiles and a few candidates having very large profiles.
For a test collection to provide a realistic benchmark for evaluation, a set of queries should be selected that are rep-resentative of real user information needs [9]. Although we do not have direct access to expert finding users in our sce-nario, we do have access to expert profiling users, namely, the experts themselves. In this section, we describe the pro-cess of producing queries and relevance judgments for both the expert profiling and the expert finding tasks.
Early test collections for expert finding such as the W3C test collection [1] lacked knowledgeable judges to assess the relevance of candidate experts [3]. For expert profiling, the most knowledgeable judge to assess an expert X  X  profile is ar-guably the expert him or herself. Indeed, even when exper-tise profiles are meant to be used by others, as is the case in academic and professional online social networks, they must be typically approved by the profiled expert before-hand. With this in mind, we invited a subset of 5,355 of the most prominent researchers from our set of candidates to participate in our expert profiling judgments. These in-cluded researchers from 123 research consortia awarded by the Brazilian government with a five-year grant (starting from 2008) to foster networks of collaborative research in several areas considered strategic for the country.
In contrast to previous expert profiling test collections, such as UvT [2] and TU [4], we did not impose a prede-fined set of topics of expertise for each expert to choose from. Instead, each of the 5,355 invited experts was pre-sented with a list of suggested topics obtained by pooling nine standard content-based tag recommenders operating on different fields of the expert X  X  publications, namely, ti-tles, abstracts, and keywords [8]. In particular, each expert was presented with up to 60 suggested topics in no particu-lar order, selected in a round-robin fashion from the top 50 tags returned by each recommender. Of the 5,355 invited ex-perts, 1,450 (27%) responded to our invitation and provided relevance judgments on the suggested topics of their exper-tise, with the option to inform us of further topics not among the suggested ones. Relevance judgments were performed on a graded scale, with 0, 1, and 2 indicating an irrelevant, relevant, and highly relevant topic for the expert X  X  profile, respectively. Altogether, these 1,450 experts (queries) iden-tified 50,802 somewhat relevant topics (qrels) that make our benchmark for expert profiling. Table 2 shows a breakdown of the average number of qrels of various relevance grades per query. On average, each expert in our benchmark has a total of 27 relevant topics of expertise in his or her profile. Table 2: Average number of relevants of various g rades per expert profiling and expert finding query.
While relevance judgments for expert profiling could be trivially converted into judgments for expert finding (and vice versa), the resulting judgments would likely lack in coverage and coherence. To illustrate this situation, Fig-ure 3 depicts a direct conversion of expert profiling judg-ments (represented by directed edges running from experts to topics, weighted by the relevance grade assigned in each judgment) into expert finding judgments (represented by transposed edges running from topics to experts). Figure 3: Asymmetric conversion between expert p rofiling and expert finding relevance judgments.
From Figure 3, a coverage problem may arise in the re-sulting expert finding judgments to the extent that further relevant experts on topics t 1 and t 2 will be missing, unless they have been previously included in the expert profiling judgments. Likewise, in a typical situation where expert pro-filing judgments are performed by different judges (ideally, each expert would judge his or her own profile), relevance grades assigned to the same topic will not be comparable to one another, thus resulting incoherent expert finding judg-ments. As an illustrative example, a boastful yet unknowl-edgeable candidate e 1 may assign him or herself a relevance grade 3 for topic t 1 , whereas a modest yet proficient can-didate e 2 may assign him or herself a grade 2 for the same topic. When converted into expert finding judgments, these assignments may convey the wrong impression about these candidates X  relative expertise on topic t 1 .
To overcome such an asymmetry, we conducted a second r ound of judgments aimed specifically at the expert finding task. To this end, we invited a subset of the researchers that had contributed judgments on their own expertise profiles to judge the expertise of others. To avoid overly specific topics, we discarded 102 of the 1,450 original respondents with a single topic of expertise shared by at most one other researcher. As a result, for each topic of his or her exper-tise, each of the invited 1,348 researchers (acting as a judge) was presented with a list of at least two other candidates that had also declared themselves somewhat experts on the topic. A link to the full curriculum of each candidate was also provided to aid in the judgment. The judge was then asked to determine the expertise of each candidate on the topic following a graded scale, with 0, 1, 2, and 3 indicating an unknowledgeable, somewhat knowledgeable, very knowl-edgeable, and expert person on the topic, respectively. Of the 1,348 invited researchers, 513 (38%) agreed to act as judges for this second round of relevance judgments.
To enforce a minimum level of support and an improved consensus on the judged experts, we retained 235 topics that had at least three self-declared experts (in the expert pro-filing judgments) subsequently considered as experts by at least two judges (in the expert finding judgments). The final relevance grade for an expert was computed as the rounded mean of the grades received from all judges. Analogously, we estimated the disagreement about an expert as the stan-dard deviation around the expert X  X  unrounded mean grade. Per-topic disagreement values were then obtained by aver-aging the disagreements observed for all experts associated with each topic. Figure 4 shows the distributions of dis-agreement considering all topics, as well as subsets of topics with experts from an increasing number of areas of interest.
As shown in the  X  X ll X  group in Figure 4, disagreement values vary considerably across topics, with an average of 0 : 29  X  0 : 02. Interestingly, the more knowledge areas covered by a topic, the higher the disagreement observed for this topic, with topics covering five or more areas showing the highest average disagreement (0 : 37  X  0 : 07). In total, the 235 obtained topics (queries) were associated with 1,635 experts (qrels) in our benchmark for expert finding. As shown in Table 2, this makes an average of over five experts per query. In addition, these experts are more evenly distributed across knowledge areas, as highlighted by the red bars in Figure 1. Disagreement Figure 4: Judging disagreement distributions for all topics and for subsets of topics with experts from an increasing number of knowledge areas.
The public availability of benchmark test collections has fostered intense research on expertise retrieval over the past decade, particularly in the enterprise domain. In this paper, we described a new test collection for expertise retrieval in the academic domain, which has not seen as much research progress in comparison. In particular, the Lattes Exper-tise Retrieval (LExR) test collection provides a large-scale benchmark for the evaluation of two complementary exper-tise retrieval tasks, namely, expert profiling and expert find-tion that fully supports both tasks, with dedicated graded relevance judgments performed by real experts for each task separately. In addition, it is both cross-organization and cross-area, encompassing candidate experts from all areas of knowledge working in research institutions spread all over Brazil. Together, these features make LExR a unique and valuable resource for the research community.
 This work was partially funded by projects InWeb (MCT/ CNPq 573871/2008-6) and MASWeb (FAPEMIG/PRONEX APQ-01400-14), and by the authors X  individual grants from CNPq and FAPEMIG. [1] P. Bailey, N. Craswell, I. Soboroff, and A. P. de Vries. [2] K. Balog, T. Bogers, L. Azzopardi, M. de Rijke, and [3] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and [4] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and [5] N. Craswell, A. P. de Vries, and I. Soboroff. Overview [6] J. Lane. Let X  X  make science metrics more scientific. [7] V. Mangaravite and R. L. T. Santos. On [8] I. S. Ribeiro, R. L. T. Santos, M. A. Gon  X calves, and [9] M. Sanderson. Test collection based evaluation of [10] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
