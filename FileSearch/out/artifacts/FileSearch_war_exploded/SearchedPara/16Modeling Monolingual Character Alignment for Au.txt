 MAOXI LI, MINGWEN WANG, HANXI LI, and FAN XU , Jiangxi Normal University Since the introduction of statistical approaches (i.e., the IBM translation model) to machine translation [Brown et al. 1990, 1993], a variety of different paradigms for Statistical Machine Translation (SMT) have been developed, including phrase-based SMT [Koehn et al. 2003], syntax-based SMT [Ding and Palmer 2005; Galley et al. 2006; Liu et al. 2006; Xiong et al. 2006; Xiao and Zhu 2013; Zhai et al. 2013], and semantic-based SMT [Jones et al. 2012; Xiong and Zhang 2014; Zhang et al. 2014]. Each approach has its own strengths and weaknesses. It is important to assess the translation quality of different translation approaches in order to better understand and compare them.

There are two methods for machine translation evaluation. One is to manually judge the translation quality. In this approach, a human annotator applies an absolute multi-point scale to score translation fluency and adequacy or ranks translation outputs pro-duced by different approaches or paired-compares translation outputs [Callison-Burch et al. 2012; Bojar et al. 2013; Bojar et al. 2014]. Another approach is to automatically evaluate translation quality. Here, as a measure of translation quality, a computer pro-gram calculates a similarity score between a translation output and human references based on heuristic rules or machine-learning methods. Because of the high manual as-sessment costs and the time required, translation quality is rarely assessed by humans except in machine translation evaluation campaigns that employ human annotators to score participating translation systems. The human judgment collection released by an evaluation campaign is often defined as the golden standard for validating automatic evaluation metrics.

In recent years, many state-of-the-art automatic evaluation metrics, such as BLEU [Papineni et al. 2002], NIST [Doddington 2002], Meteor [Banerjee and Lavie 2005], TER [Snover et al. 2006], MAXSIM [Chan and Ng 2008], TESLA [Liu et al. 2010], and PORT [Chen et al. 2012], have been proposed as automated understudies to human assessment. All these metrics follow the intuition underlying BLEU:  X  X he closer a machine translation is to a professional human translation, the better it is. X  The BLEU and NIST metrics are based on n -gram precision between machine translation and human references, whereas the METEOR, MAXSIM, TESLA, and PORT metrics are based on n -gram precision and recall. The TER metric is based on error rate, which measures the minimum number of edits when modifying the human translation to exactly match the human reference.

Most of these automatic metrics focus on evaluating translation outputs in Euro-pean languages, in particular, English. When Chinese translations are evaluated with the same metrics, the correlation between automatic evaluation results and human judgments becomes much weaker. There are two important differences between En-glish (as well as other European languages) and Chinese with respect to the automatic evaluation of machine translation. First, English translations are already segmented into word sequences with white spaces, and the word is adopted as the atomic unit in the subsequent processing work. In Chinese, tokens are not originally demarcated in the translation outputs. Thus, the first step is to segment a Chinese translation into a sequence of appropriate tokens, such as words or characters. Second, computerized representation differs between the English word and the Chinese character. An English word consists of several letters. It is quite easy to determine if two different words have some similarity in form (e.g., using cognates). This similarity is indeed incorporated into some existing metrics when letter n -grams are used. In Chinese, the computer code for a character does not encode any semantic information. It is impossible to determine a semantic similarity from the codes of characters. For example, the characters and 0xE4BDA0 and 0xE682A8, which have nothing in common. The existing metrics using character n -grams cannot cope with the hidden semantic similarity between Chinese characters. This presents an additional challenge for automatic evaluation of Chinese translation.

In this article, we introduce a novel approach to evaluate Chinese translation out-put. The Chinese character is adopted as the atomic unit for automatic evaluation of Chinese translation for two reasons: (i) Chinese word segmentation often produces inconsistent results, leading to mismatches between two words that should match; and (ii), in analyzing the internal structure of Chinese synonyms, we notice that many of them share some common characters. A character-based evaluation can capture part of the similarity between these words. To match the Chinese characters with the same meaning, we employ the Indirect Hidden Markov Model (IHMM) [He et al. 2008; Fishel et al. 2011] to align the characters in the translation outputs and the human refer-ences. In addition, a shift penalty is integrated to yield a better correlation with human judgments; this penalizes inconsistent word order between machine translation and human reference.

The remainder of this article is organized as follows: Section 2 reviews related re-search. In Section 3, we explain why the character is adopted as the atomic unit in the evaluation. In Section 4, we describe in detail the use of IHMM to align Chinese trans-lation output and human references at the character level. The experimental results are presented in Section 5. Section 6 provides the analysis. Finally, our conclusion can be found in Section 7. Only a few researchers have investigated the problem of automatic evaluation of Chinese translation [Li et al. 2011; Liu and Ng 2012]. Although several automatic metrics, such as BLEU, Meteor, and TER, are designed to be language independent, they rely on exact word or character matches. This means that a translation is deemed correct only if it shares the same words or characters with the references. Translations by synonyms are not considered correct. Although this strategy may raise some problems in European languages, the problem is much more serious when it is used on Chinese translations due to the representation problem mentioned earlier.
A few studies have addressed specifically the evaluation of Chinese translations. Li et al. [2011] conducted an empirical study on automatic evaluation of Chinese transla-tion. They performed a comparison between character-level versus word-level metrics (including BLEU, NIST, Meteor, GTM, and TER) on submitted Chinese translation out-puts in the IWSLT X 08 English-to-Chinese ASR challenge task (IWSLT X 08 CT-EC) and the NIST X 08 English-to-Chinese translation task (NIST X 08 EC). They found a stronger correlation with human assessments when the evaluation is performed at the character rather than at the word level. However, the study by Li et al. does not investigate how to integrate language-specific knowledge or language-specific resources into automatic evaluation of Chinese translations. Thus, the problem of synonym matching remains unresolved.

Intuitively, it should be possible to use a synonym dictionary to cope with Chinese synonyms, which acts as WordNet for English. However, Chinese synonym dictionaries only contain synonym words, not synonym characters. Thus, it is difficult to match characters with the same meaning. In an effort to integrate synonym relations in translation evaluation, Liu and Ng [2012] present a new automatic metric, TESLA-CELAB, which integrates word-level linguistic knowledge into a character-level metric. They use Cilin ( ) as their word synonym dictionary. Characters with the same meaning are matched if they have the same appearance (i.e., are identical) or if they are defined as synonyms by Cilin. The study only addresses synonym-matching in which a synonym contains multiple characters.

To effectively use language-specific resources, Denkowski and Lavie [2014] extended the Meteor metric. Their new metric, Meteor Universal, incorporates language-specific resources to any target language, such as paraphrase tables and function word lists extracted automatically from a bilingual training corpus. The basic idea is that if an automatic translation uses a paraphrase or function word equivalent to that used in the reference translation, it is considered to be a correct translation. In the workshop for the statistical machine translation evaluation task, Meteor Universal was found to significantly outperform the baseline BLEU metric in the evaluation of Russian and Hindi translations. However, the study did not attempt to evaluate Chinese translation.
Inspired by the study of Denkowski and Lavie, we investigate a new evaluation metric that considers character matching extracted from language-specific resources X  namely a bilingual training corpus X  X nstead of language-specific knowledge. We use the IHMM framework to derive character-level similarity from a bilingual corpus. This approach results in more precise character matching than using language-specific knowledge. In addition, we introduce a penalty for shifting character sequences to slightly penalize disordered phrases in our approach. Compared to an evaluation metric based on Chinese words, a metric based on Chinese characters offers the advantage of more easily capturing synonym matches. This can be explained from two perspectives.

First, the Chinese character is a smaller unit than a Chinese word. Thus, it can match synonyms that share common characters. Table I present examples of Chinese words that share common characters in IWSLT X 08 CT-EC. Both the human references and the Chinese translation outputs are segmented into word sequences by the Chi-nese word-segmenter tool Urheen [Wang et al. 2010]. The Chinese words that share common characters appear in bold, and the relevant word meaning in English is in-cluded in brackets. For example, the word is a synonym for the word ,and both mean thank in English. If words are used for matching, the word in the Chinese translation will not match the word in the human reference despite the fact that they are equivalent. However, if characters are used, the word will partly match in the human reference because of the shared character . This example shows that adopting the character as the matching unit can better capture synonym matches. The sharing of characters between synonym words in Chinese is very common. Very often, if two Chinese words share some common character(s), they are synonyms or semantically related words. According to the semantic relationships between words that share common characters, we can classify the words into three types: exact match (i.e., the meanings of the words are identical), partial match (their meanings partially match), and no match. The statistics on the translation outputs from a participating SMT system nlpr [He et al. 2008] in IWSLT X 08 CT-EC (see examples in Table I) show that  X  X xact match X  accounts for nearly 70%,  X  X artial match X  accounts for 20%, and  X  X o match X  only accounts for 10%. These statistics mean that words sharing common characters are synonyms in most cases. Therefore, matching automatic and manual translations at character level can naturally capture part of their semantic similarity.

Another reason to use characters instead of words in evaluation is due to the inconsis-tency in Chinese word segmentation: Words in a Chinese translation may be segmented in an way that is inconsistent with a human reference. For example, one may have a correct Chinese translation segmented as ( Ibuyit ), whereas the correspond-ing human reference is segmented as ( I want to buy it ). The sequence of characters , meaning buy it , is segmented differently, thus making the translation incomparable to the human reference at the word level. If the translation is compared to the human reference at character level, however, the problem is avoided.
In conclusion, adopting the Chinese character as the atomic unit in Chinese transla-tion evaluation can not only capture more synonym matches but also avoid the incon-sistency problem in Chinese word segmentation. Synonymy relations in Chinese are not restricted to words sharing common characters. In many cases, words with different characters can also express the same meaning, especially if there is a strong semantic relation between the characters. For example, both and mean you in English and should be matched. The problem is that no resource has been created to show the semantic similarity between characters. Therefore, we extract such relations between characters from a bilingual corpus. The idea is that if two Chinese characters are aligned strongly with the same English word in the parallel corpus, they should be similar.

We use IHMM to align characters in an automatic translation and a human refer-ence. IHMM is a type of Hidden Markov Model (HMM) in which the emission probabil-ity and the transition probability are estimated by heuristic rules instead of training via maximum-likelihood estimation. IHMM has been successfully applied to align the translation hypothesis with the backbone translation hypothesis to construct a confu-sion network for the system combination of machine translation [He et al. 2008] and to build monolingual word alignment in order to detect lexical errors and order errors in English-to-Czech translations [Fishel et al. 2011]. In our work, we use IHMM to match the Chinese characters with the same meanings, which could be identical characters or different characters. Given a Chinese translation that consists of m characters h m 1 = ( h 1 ,..., h m )anda human reference that consists of n characters r n 1 = ( r 1 ,..., r n ), suppose that each character in the human reference is an HMM state, the characters in the Chinese translation are the HMM observation sequence, and the alignment a n 1 = ( a 1 ,..., a n )that defines the position of the character in the human reference aligned to each character in the Chinese translation is the hidden variable. The conditional probability that the Chinese translation is generated by the human reference is:
The emission probability of the HMM, p ( h j | r a which defines the similarity between a character in the human reference and a charac-ter in the Chinese translation. In addition, the similarity model is a linear interpolation of the semantic similarity and surface similarity between two characters:
For Chinese characters, the surface similarity between two characters is set at 1 if they are identical; otherwise, it is set at 0. The semantic similarity is estimated by using the source sentence f k 1 = ( f 1 ,..., f k ) as a hidden layer: bilities that can be learned from the bilingual training data by GIZA ++ [Och and Ney 2002]. Note that this approach is a powerful method to estimate the similarity model when characters with the same meaning are not identical. For example, in an evalua-tion on IWSLT  X 08 CT-EC test set, the source sentence  X  X hat is your favorite place in Kyoto? X  has a human reference  X  ?  X , and a machine translation  X  ?  X . Note that the translations of your are, respectively, and . The surface similarity between the two Chinese characters is 0 because they are not identical. However, because they are both mapped to the word your in the source sentence, we can estimate their semantic similarity as 1 :
Although the semantic similarity value is low, it is substantially larger than the other possible similarity value, such as p semantic ( | ), p semantic ( | ).
The transition probabilities of the HMM in Equation (1), p ( a j = i | a j  X  1 = i , I ), are estimated by the distortion model, which depends only on the jump width ( i  X  i ) [Vogel et al. 1996]. The transition probability is defined as: where c ( i  X  i ) is a function of the jump width ( i  X  i ). To favor monotonic alignment and penalize non-monotonic alignment, c ( i  X  i ) is heuristically defined: After the estimation of the emission probabilities and the transition probabilities, we use the Viterbi algorithm to seek the optimal alignment to maximize the conditional probability that the Chinese translation is generated by the human reference. The algorithm is decomposed into two steps: a down-top recursive procedure and a for-ward backtracking procedure. The recursive procedure is used to obtain the maximum probability and save the path to the maximum probability, whereas the backtracking procedure is used to obtain the alignment between the reference translation and the machine translation. The alignment position of the last character in the human refer-ence is found first in the latter process. Then, the procedure backtracks to the forward character until all alignment positions in the human reference are found. Using IHMM, the alignment is established between the characters in the human ref-erence and the characters in the Chinese translation. However, the alignment cannot be directly used to match characters. There are three special cases that must be con-sidered. First, the alignment may contain a small number of 1 -N mappings between the human reference and the Chinese translation. Where 1 -1 mappings are required to match characters, we only retain the alignment with the highest similarity model probability as a match in this case. Second, if a character in the human reference is aligned to a null point in the Chinese translation or vice versa, the character is unlikely to have any corresponding matching character. Finally, to avoid characters aligned with higher distortion probability and lower similarity model probability being considered as a match, we stipulate that the similarity model probability between the characters aligned to one another should be larger than a threshold value, which is empirically set at 0.06.

Figure 1(a) provides an alignment example generated by IHMM for the human reference  X  ?  X ( Where in Kyoto do you like the most? ) and the machine translation  X  ?  X . Although the character are not viewed as a match because of the low similarity model probability. However, the character in the human reference and the character in the Chinese translation are perceived as a match because the similarity model probability between them is larger than the threshold value.

Figure 1(b) provides the alignment generated by the Meteor Universal package 2 for the same human reference and Chinese translation. Compared with the alignment generated by IHMM, the alignment produced by Meteor Universal fails to align char-acters that are different but have the same meaning; namely, between and .By examining the implementation details in the Meteor Universal package, we found that a greedy algorithm is used to match characters in two steps: exact match and para-phrase match. This approach might produce suboptimal solutions because the greedy algorithm finds the local optima rather than global optima. However, the IHMM ap-proach uses the Viterbi algorithm to find a solution that achieves the optimal value, although the solution is not unique. Thus, the IHMM approach can match characters more precisely. After the character matching between the Chinese translation and the human refer-ence, we define the score of the Chinese translation quality in terms of the F measure. The precision P is the ratio of the number of matched characters in the Chinese trans-lation to the length of the Chinese translation, whereas the recall R is the ratio of the number of matched characters in the Chinese translation to the length of the hu-man reference. The fragmentation penalty is introduced to favor long matches, and the shift penalty is introduced to penalize shifts of character sequences. Finally, we set the objective function as: where the parameters  X  ,  X  ,  X  ,and  X  attribute a relative importance to different factors. The objective function resembles that of the Meteor metric except that we add a shift penalty:
The reason we apply the shift penalty is because if the character order of the Chinese translation is not consistent with that of the human reference, the translation quality of the Chinese translation may be degraded. As shown in Figure 1, the character sequences and in the human reference are shifted to another position in the Chinese translation. The human reference  X  ?  X  means  X  what is your favorite place in Kyoto? X  However, the Chinese translation is  X  meaning of the Chinese translation is inconsistent with that of the human reference. Thus, the translation quality is degraded, and we should penalize this case. The shift penalty is defined as the ratio of the shifting times of the character sequences in the Chinese translation to the number of matched characters.

Algorithm 1 describes how to calculate the shifting times according to the character alignment between the human reference and the Chinese translation. If the alignment point in the Chinese translation of the current character in the human reference is in front of the alignment point of the preceding character, then we define this case as a shift.

Similar to the Meteor metrics, when there are multiple human references, we cal-culate a score for each of them and adopt the best score as the segment-level score. ALGORITHM 1: Shifting Times Calculation Algorithm The system-level score is computed based on aggregate statistics accumulated over the entire test set. To test the performance of the IHMM approach in the automatic evaluation of an Chinese translation, we conducted experiments on two English-to-Chinese translation datasets. One dataset was in the spoken language translation domain. The other was in the newswire translation domain. The IWSLT  X 08 CT-EC task evaluated the translation quality of seven machine transla-tion systems [Paul 2008]. The test set contains 300 segments with human assessment of the system translation quality. Each segment came with seven human reference translations. Human assessment of translation quality was performed on the fluency and adequacy of the translations. Additionally, a rank was assigned to the output of each system. For the rank judgment, human graders were requested to  X  X ank each whole sentence translation from best to worst relative to the other choices X  [Paul 2008]. Because of the substantial time investment of manual assessment, the fluency and ad-equacy assessment was limited to the output of four submitted systems, whereas the human rank assessment was applied to all seven submitted systems. The evaluation based on ranking is reported in this article. The experimental results for fluency and adequacy judgment agree with the results of the human rank assessment.

To estimate the reliability of the human assessment, the kappa coefficient is in-troduced to measure the interannotator agreement among human graders, which is defined as: where P ( A ) is the proportion of times that the annotators agree and P ( E ) is the propor-tion of time that they would agree by chance. The interannotator agreement is 0.41 for fluency assessment, 0.40 for adequacy assessment, and 0.57 for ranking assessment. Kappa values between 0.4 and 0.6 are considered to represent moderate agreement. Thus, the agreement rates are more or less consistent with other comparable experi-ments.

The NIST X 08 EC dataset consists of 127 documents with 1,830 segments. Each seg-ment has 4 reference translations and the translation outputs of 11 machine trans-lation systems (all released in the corpus LDC2010T01). Because no manual eval-uation is available for the dataset, Liu and Ng [2012] provided adequacy and flu-ency judgment for the first 30 documents (document ID AFP_ENG_20070701.0026 to AFP_ENG_20070731.0115) (355 segments). In addition, each translation output was judged by three human annotators. The average interannotator agreement between the human annotators was 0.43. Thus, the kappa values are considered to represent moderate agreement. These agreement rates are consistent with other comparable experiments. To facilitate the comparison of different automatic machine translation metrics, we transform the human adequacy and fluency judgment into rankings. The automatic machine translation metrics are compared in terms of segment-level consistency and system-level correlation with human judgments.

Kendall X  X  tau rank correlation coefficient is used to compute the segment-level consis-tency between the human judgments and the automatic metrics. We calculate segment-level consistency as: where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons with which a given metric disagrees. Ties between human judgments are excluded in the pair-wise comparison. However, ties between the automatic metrics are added to the denominator [Machacek and Bojar 2014]. The possible value for  X  ranges between 1 and  X  1. The higher the value of  X  , the more closely the automatic metric correlates with the human judgments.

Pearson X  X  correlation coefficient is used to measure the system-level correlation be-tween human judgments and automatic metrics. The formula for Pearson X  X  correlation coefficient is: where H i and M i are the system-level human score and automatic metric score for the i -th machine translation system, respectively, and n is the number of machine translation systems.

Because the human rank assessment occurs at the segment level, we have to find a way to use the collected rankings of the overall test set to produce a system-level human score for a system. Three strategies can be adopted to calculate the system-level human score. The first is to calculate the average number of times each system was judged better than other systems [Koehn 2012]. The second is the graphical model formulation introduced by Hopkins and May [2013], which makes the idea of underlying system ability even more explicit. The last is to calculate the system-level human score using the TrueSkill TM algorithm, which is an online approach for modeling the relative skills of players in ongoing competitions, such as Microsoft X  X  Xbox Live [Sakaguchi et al. 2014]. We used the TrueSkill approach to compute the system-level human score in the experiment, which has been adopted as the official system ranking method for the WMT X 14 machine translation task [Bojar et al. 2014].
 BLEU is the standard metric for automatic evaluation of machine translation perfor-mance, and it combines the geometric average of the clipped n -gram precisions and the brevity penalty to score the machine translation quality [Papineni et al. 2002]. The BLEU metric only uses the information of word appearance to match words or n -grams in machine translation and human references, and it does not use any linguis-tic knowledge. Thus, it is language-independent. Before the BLEU metric was applied to score the translation, we divided the Chinese translation into character sequences. The BLEU metric accumulative total was set to 4 -gram. Because a Chinese word may contain several characters, we also tried a higher order n -gram ( n &gt; 4) BLEU metric and found no significant improvement. The segment-level consistency and system-level correlation of the BLEU metric and the human judgments are shown in the first rows of Tables II and III for the IWSLT X 08 CT-EC dataset and the NIST X 08 EC dataset, respectively.

Meteor is a widely used automatic machine translation evaluation metric that com-bines the F measure of unigram and the fragmentation penalty [Banerjee and Lavie 2005]. For English translation, the monolingual word alignments are incrementally constructed in a series of stages using word appearance (exact), word stem, synonym knowledge, and paraphrase knowledge, respectively. At each stage, one of the preced-ing alignments identifies all possible word matches between a machine translation and human reference using words not aligned in previous stages. The final alignment is defined as the union of all stage alignments. For Chinese translation, only the ex-act alignments of characters can be used. In the experiment, the Meteor open source package version 1.5 is used, and the parameters are set as default. The segment-level consistency and system-level correlation are reported in the second rows of Tables II and III. Compared with the BLEU metric, the Meteor metric achieves significant improvements ( p &lt; 0 . 01).

The Meteor Universal metric extends the Meteor metric to previously unsupported target languages. Because of linguistic knowledge, such as word stemming, synonyms are limited to one or a few languages; thus, to match synonyms, the Meteor Universal metric uses the same bilingual training corpus used to construct statistical translation systems to learn paraphrases [Denkowski and Lavie 2014]. For Chinese, the metric extracts many paraphrases, such as and ( in case )or and ( one week ), to align characters in the paraphrase stage. Thus, the Meteor Universal metric incrementally uses two stages (i.e., exact and paraphrase) to align the charac-ters of a machine translation with a human reference. In the experiment, the default parameters of the Meteor Universal metric are used. As shown in Tables II and III, the segment-level consistency of the Meteor Universal metric is slightly better than that of the Meteor metric.

The TESLA-CELAB metric is designed for automatic evaluation of machine transla-tion in languages with ambiguous word boundaries; in particular, for Chinese transla-tion. The metric adopts the character as the atomic unit for matching and uses linear programming to combine the advantages of character-and word-level metrics [Liu and Ng 2012]. The segment-level consistency and system-level correlation between the TESLA-CELAB metric and the human judgments are shown in the fourth rows of Tables II and III. The scores indicate that the TESLA-CELAB metric outperforms the BLEU metric but has no clear advantage over the Meteor metric and the Meteor Universal metric. These results agree with those of Liu and Ng [2012]. We used the IHMM approach to evaluate Chinese translation by incorporating dif-ferent resources. First, the bilingual training corpus was not used, and the objective function degraded into that of the Meteor metric. If the bilingual training corpus was unavailable, the similarity model of IHMM became: which means that only the information of word appearance was used to estimate the emission probability. Additionally, the objective function changed from Equation (6) to Equation (12): This model is highly similar to that of the Meteor metric. The only change is that we use the distortion model to align characters and Viterbi algorithm to search for the optimal match. The parameters  X  ,  X  ,and  X  are set in the same way as for the Meteor metric, and this approach is denoted as IHMM-EXACT in Tables II and III. Compared with the Meteor metric, we found that this approach performs slightly better. The results proved the effectiveness of the distortion model and the algorithm we used.
Subsequently, we used the bilingual training corpus to train the bidirectional lex-ical probabilities and added semantic similarity to the similarity model (IHMM-SEMANITY in Tables II and III). Compared with the Meteor Universal metric, we noted that the alignment is performed in one instead of two stages. Compared with the performance of the Meteor Universal metric, the results also indicate that the IHMM-SEMANITY approach consistently improved in the segment-level consistency and the system-level correlation with human judgments.
 Finally, we added the shift penalty to the objective function (IHMM-SHIFT in Tables II and III). IHMM-SHIFT not only performs better than IHMM-SEMANITY, but also outperforms the baseline metrics (i.e., Meteor Universal and TESLA-CELAB). The improvement in the segment-level consistency is statistically significant at the p &lt; 0 . 05 level. Therefore, the difference of segment-level consistency and system-level correlation among IHMM-SHIFT, Meteor Universal, and TESLA-CELAB highlights the contribution of the monolingual character alignment approach, IHMM, and the shift penalty.

As far as the parameters in the objection function are concerned, it would be possible to tune them so that the best translation could obtain the highest score when we have a large set of training data. Because such a large training set is unavailable to Chinese translations, we set the parameters in the same way as in the Meteor metric version 1.5: namely,  X  = 0 . 85,  X  = 0 . 2,  X  = 0 . 6, while  X  is empirically set as 0.1. This setting may be suboptimal for our score function, but it has the advantage that the score can be directly compared with Meteor. At the same time, we also try the parameters used by Meteor Universal metric: namely,  X  = 0 . 7,  X  = 1 . 4,  X  = 0 . 3, which were learned from human judgments of translation quality from WMT  X 12 on CZ-EN, DE-EN, ES-EN, FR-EN, EN-CZ, EN-DE, EN-ES, and EN-FR translation directions. The segment-level consistency is 0.5420 and the system-level correlation is 0.9673 in the IWSLT X 08 CT-EC task, whereas the segment-level consistency is 0.3190 and the system-level correlation is 0.9271 in the NIST X 08 EC task. The latter parameters provide better performance than the former parameters in the NIST X 08 EC task, but provide worse performance in the IWSLT X 08 CT-EC task. It shows that the parameters used by Meteor Universal metric are a better fit for the newswire translation domain (long sentences) than for the spoken language translation domain (short sentences). Although the monolingual character alignment approach is designed for Chinese trans-lation, it is easily transferrable to other languages for word matching. For example, with a few adjustments, it can be used to match English words in machine transla-tion and human references. If English words are adopted as a sequence of letters, the surface similarity can be heuristically estimated by the length of the Longest Matched Prefix (LMP) as follows [He et al. 2008]:
To test the performance in the automatic evaluation of English translation, we also conducted an experiment on TIDES X 2003 Chinese-to-English machine transla-tion tasks. The outputs of the submitted system and the human judgments were included in the LDC2006T04 corpus. Table IV shows the segment-level consistency and system-level correlation between the automatic metrics (i.e., BLEU, Meteor, and IHMM-SHIFT) and human judgments. Here, the Meteor metric aligned words accord-ing to exact, stem, and synonym matchers. The experimental results also demonstrated that the IHMM approach consistently outperformed the Meteor and BLEU metrics ( p &lt; 0 . 05). When IHMM is used to align characters between a human reference and a Chinese translation, not only are synonyms that share common characters partially matched, but also synonyms having different appearances can be matched. Table V presents monolingual character alignment examples produced by IHMM. The positions of char-acters start from 1. The alignments represent the positions of the characters aligned with one another in the Chinese translation and the human reference. The relevant matches between different characters and the corresponding alignment points appear in bold.

Because the character is taken as the atomic unit for matching and surface similarity is used, the IHMM approach can recognize the same characters in synonyms that share common characters, such as ( Japan )and ( Japanese )or ( pink )and
In addition, because semantic similarity based on bidirectional lexical probabilities is used, the IHMM approach can recognize synonyms with different character appear-ances, such as ( or so )and ( about )or ( recommend )and ( suggest ).
 These synonyms cannot be identified by the BLEU and Meteor metrics, which use an exact match to recognize character matches.

The IHMM approach can distinguish the same character with different meanings in the Chinese translation and the human reference by the combination effect of the distortion model and the similarity model. For example, in the Chinese translation  X  ?  X  and in the human reference  X  ?  X  the same character appears. This character expresses the meaning take in the Chinese translation. However, in the human reference, it combines with the character Chinese translation is aligned with the same character in the human reference, they will have larger similarity model probabilities but smaller distortion model probabil-ities as a result of jumping two characters. However, if this character is aligned with the character ( take ) in the human reference, the two characters will have larger distortion model probabilities because the jumping width is 0 but smaller similarity model probabilities because the character appearances are not identical. The IHMM approach aligns the character with by using a combination between the simi-larity model and the distortion model, which represents a more reasonable choice. In contrast, the Meteor Universal metric and the TESLA-CELAB metric will choose to match the same characters because their appearances are identical. To compare the IHMM approach with the Meteor universal metric, we conducted a detailed comparison between the monolingual alignments generated by the IHMM approach and those by Meteor Universal.

Since no gold standard exists for the monolingual Chinese character alignment, we manually annotated the alignments between the Chinese translation from the participating system nlpr [He et al. 2008] and the human reference that produced the best score. The translation outputs are composed of 2,564 characters, in which about 1,720 characters matched with the characters having the same appearance in the references. The annotated data also contain, respectively, 131 and 126 alignment points with characters having a different appearance for the Meteor Universal metric and for the IHMM approach. The number of the alignment point is slightly different because the Meteor Universal metric and the IHMM approach take a different human reference as the best reference.

Because characters having the same appearance can be matched with almost perfect accuracy by the Meteor Universal metric and the IHMM approach, we focus on those matched characters that have a different appearance, which account for 5.11% and 4.91% of the matches in the IHMM approach and the Meteor Universal metric, respec-tively. Table VI shows the precision, recall, and F1 measure for the alignment points for this group of characters generated by the IHMM approach and Meteor Universal. The results indicate that the alignment generated by the IHMM approach is better than those by the Meteor Universal metric both on precision and recall. However, the proportion of the matched characters having a different appearance is small among all matched characters, making the improvements on the whole set of alignments small. Nevertheless, our results showed that such matching of different characters is neces-sary, especially for Chinese translation. Automatic machine translation evaluation relies on the matching of items between a machine translation and a human reference. A good evaluation metric should match items of the same meaning even if they look different. In European languages, similar words can be matched to some extent using stems and letter n -grams. In Chinese translations, similar approaches do not apply. One can only match identical characters. Such strict character matching will result in a suboptimal quality measure because different words and different characters can bear the same meaning. A crucial problem is to create possible matching relations between different words and characters.
Instead of using a synonym dictionary, which only exists for words, we extract match-ing relations between characters from a parallel corpus. This approach is inspired by the study of Denkowski and Lavie [2014], which extracted paraphrases and equivalent function words from a parallel corpus, but we apply it at the character level in Chinese. We use IHMM to extract such matching relations between characters. The extracted matching characters are used to align characters between the machine translation and the human reference, and this results in a new evaluation metric.

We evaluated the new metric on two well-known English-to-Chinese machine trans-lation evaluation datasets, measuring the correlation between our metric and a human rating. Our experimental results showed that the new metric has a stronger correlation with human rating than the state-of-the-art evaluation metrics BLEU, Meteor Univer-sal, and TESLA-CELAB. The experimental results demonstrate the effectiveness of our evaluation metric, which incorporates monolingual character alignment.
The study described in this article is a first step toward coping with different elements of the same meaning, which is a crucial problem in automatic machine translation evaluation. Ideally, any items or structures that bear the same meaning should be matched. This work can be extended in the future to match items beyond characters in Chinese. For example, it may be useful to extract equivalent words or paraphrases. We will extend our investigation to such elements in the future.

