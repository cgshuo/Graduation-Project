
In this paper we address the problem of document re-trieval with semantically structured queries -queries where each term has a tagged field label. We intr oduce Dirichlet Aspect Weighting model which integrates terms from exter-nal databases into the query language model in a bayesian learning framework. For this model, the dirichlet prior dis-tribution is governed by parameters which depend on the number of fields in the external databases. This model needs additional examples to be augmented to the seman-tically structured query. These examples are obtained using pseudo relevance feedback. We formulate a loglikelihood function for the Dirichlet Aspect Weighting model and max-imize it using a novel Generalized EM algorithm. Compar-ison of the results of Dirichlet Aspect Weighting model on TREC 2005 Genomics Track dataset with baseline methods using pseudo relevance feedback, while incorporating terms from external databases shows an improvement.
Most retrieval models assume that a query is just a se-quence of keywords. Indeed, such short keyword queries are quite common when people search on the Web. How-ever, in many application domains, queries may be semanti-cally structured (i.e., consisting of several fields, each being a sequence of keywords).
 For example, consider the following query topic from TREC 2005 Genomics Task [6]: &lt;gene&gt; hypocretin receptor2 &lt;/gene&gt; &lt;impact&gt; role in narcolepsy &lt;/impact&gt; &lt;ordinary&gt; Provide information about mutations and its their &lt;/ordinary&gt; The original natural language query is  X  X rovide information about Mutations of hypocretin receptor 2 and its/their role in narcolepsy. X  In general, field labels like gene, impact and ordinary 1 can be obtained either manually or automatically by using techniques like named entity recognition.
In this case, the query consists of three aspects  X  the gene, its impact, and ordinary information. Matching a word in some field (e.g., impact) may be more important than others (e.g., ordinary).

Similar queries may also occur when we use search re-sults from relational databases to generate a query to search over text collections.

The above example shows that we often have a query that consists of multiple text fields, where each field corre-sponds to a different aspect of a user X  X  information need. We refer to such a query as semantically structured query . Since most retrieval models assume that a query is a bag of terms, semantically structured queries have not been well studied. While we may ignore the semantic structure and treat such queries as a bag of terms, considering the seman-tic structure is clearly beneficial and can potentially improve retrieval accuracy of unstructured documents. An unstruc-tured document means considering the words in a document to be a bag of words. The idea of modeling unstructured documents using aspects has been explored in [7, 3].
The advantage of a semantically structured query is that there is scope for integrating semantically similar external data fields to the fields of the query in order to improve retrieval performance. In general we refer to the different fields of an external database from which we extract terms to be added to the semantically structured query as external data fields.

In this paper we use a Bayesian learning strategy to learn a model which integrates terms from external databases with the semantically structured query. We need relevant examples for learning a model, by relevant examples we mean those documents that have preference for the same aspects as specified in the semantically structured query. We automatically generate these examples by using a few top ranking documents from an initial retrieval run using the orginal query. In general we refer to the semantically structured query as the original query. This technique of us-ing top ranking documents from an initial retrieval run as a feedback to estimate a more accurate query model is called as pseudo relevance feedback. We combine the semanti-cally structured query with these automatically generated examples to form an augmented query.
 In our Bayesian learning strategy we introduce the Dirichlet Aspect Weighting model, where we generate an augmented query using an aspect model with a dirichlet prior , whose parameters are obtained by weighting terms extracted from external databases. For estimation of pa-rameters of this model, we introduce a novel Generalized EM algorithm which involves the use of gradient projection method.

In [6], the use of pseudo relevance feedback, and addition of query related terms extracted from external databases related to Genomics task leads to improved re-sults when compared to retrieval with the original query. Hence we compare the performance of Dirichlet Aspect Weighting model with pseudo relevance feedback tech-niques that integrate terms from external databases with the original query.
 The organization of the rest of the paper is as follows. In Section 2, we describe some r elated work. In Section 3 we describe the basic retrieval model. In Section 4, we describe query augmentation and prior induction using subfields. In Section 5, we explain the generative process for the Dirich-let Aspect Weighting model. In Section 6, we explain the parameter learning and give a proof of convergence for the iterative technique described in this Section. The results are shown in Section 7, and Section 8 concludes the paper.
There has been some work on modeling queries with structures. In INEX evaluation [5], the structure in the query corresponds to the structure in XML documents. In these queries the content in the query fields needs to be matched with the content in structured document fields. The retrieval of XML documents, which is a structured document re-trieval problem is very different from the problem of un-structured document retrieval using queries with a seman-tic structure that we address in this paper. A major differ-ence being that we are capturing semantic structure of un-structured documents based on the aspects specified by the query. In what follows, we refer to an unstructured docu-ment (bag of words) as just a document.
Our basic retrieval method is the Kullback-Leibler (KL) divergence retrieval model [9, 17]. This retrieval model rep-resents the query and document using a language modeling approach[11, 18, 4]. According to this retrieval method, given an unstructured query q and a document d , we esti-mate a unigram query language model  X  q and a document language model  X  d , and then score the document using the KL-divergence of  X  q and  X  d ,definedas where V is the vocabulary set.

Clearly, in using this method, our main tasks are to es-timate  X  q and  X  d . We estimate  X  d using Dirichlet prior smoothing [18]. In this smoothing method  X  d is estimated as where  X  is the dirichlet smoothing parameter. p ( w |C ) is a background language model for the entire collection of documents C ,and c ( w, d ) is the frequency of word w in document d .

In what follows when we refer to an unigram query lan-guage model, it means that it is estimated assuming the query has no structure.

The main research question we address is how to esti-mate  X  q for a semantically structured query, in order to use the KL divergence retrieval method for semantically struc-tured queries. Our general idea is to model each query field with a unigram language model and define the overall query language model as a mixture model with each field model as a component. Thus, we have K unigram language mod-els for a query, which we denote as  X  =(  X  1 ,... X  K ) , with  X  j modeling the j th field. The query language model for the overall information need of the user can then be defined as a combination of these field language models: where  X  j is the weight on field model  X  j . The remaining questions are how to estimate each unigram language model  X  j and the mixing weights  X  j  X  X . It is expected that the es-timated values of  X  j  X  X  and  X  j  X  X  will reflect the relative pro-portions of the aspects and their distributions in the relevant documents.
In order to integrate external datafields we need to relate them to fields in the semantically structured query. Those fields of the external databases that belong to the same se-mantic category as a particular field in the original query form the subfields of that field.

We first describe the procedure to augment a query and later we describe the procedure to induce subfield based prior on the augmented query.
We need data that reflects the properties of the relevant documents in addition to the semantically structured query in order to estimate an accurate language model for the query. We augment the original query by adding R top rank-ing documents from an initial retrieval run. The initial re-trieval run uses only the original query. The retrieval model for initial run is KL divergence using a unigram query lan-guage model. These R documents are considered as un-structured query examples that have preference for the same aspects as specified in the semantically structured query. Let q u  X  X  = { q u 1 , ..., q u R } denote an unstructured query example. Thus the augmented query is {F ,q s } .
We can obtain terms that are semantically related to the aspects of semantically structured query from external databases. These terms can be used to form prior on the query term distribution. We want to use external databases to induce field dependent priors on augmented query. To do this, we use an external database, where one can search under categories. These categories become subfields of a semantically structured query. We thus search this external database with the terms in each field of our semantically structured query to add terms to the subfields. The terms that are added to subfields are denoted as prior terms.
Examples of subfields asso ciated with the field label gene are gene function and gene symbol. The prior terms in the subfield gene function can be added by searching for the functions of the gene. The prior terms in the subfield gene symbol can be added by searching for all symbols of the gene.

For the example of semantically structured query de-scribed in Section 1, subfield added query is depicted in Figure 1. It can be seen that field 1 has two subfields as-sociated with it. In this Figure, we just show the first term in each subfield. The term orexi n which is indicated in the first subfield is a peptide related to the gene function. The term hcrtr2 indicated in the second subfield is a symbol for query terms in this field. It can be seen that field 2 has two subfields associated with it. The term paroxysmal which is indicated in field 1 refers to the sudden or frequently oc-curring symptoms. The term amnesia which is indicated in the last subfield is a condition of memory loss. Field 3 is marked as the ordinary field, which means that the terms in this field are less relevant. In general prior terms are not added to the ordinary field. Hence there are no subfields associated with this field.

We have M j subfields associated with the j th field. Let e mj denote the set of prior terms added to the m th sub-field of the j th field of the query and e j denote all the terms added to the j th field. We estimate a probability distribution for each subfield. Let  X  mjw denote the prob-ability of word w in the m th subfield of the j th field,  X  in e mj else  X  ( w, e mj )=0 .Let  X  denote the set of all prior term probabilities. Let E = { e 1 ,e 2 ,...,e K } denote the set of all prior terms added to all subfields.

The idea behind inducing prior terms through subfields is that we would like to associate a different weight with each subfield. For example it may be possible that the terms added to gene function subfield are also related to some disease. These terms can bias the query to retrieve docu-ments of a related disease, though the query is unrelated to this disease. This bias can decrease the retrieval perfor-mance. Thus by weighting the prior terms in this subfield with lesser weight, we can overcome this problem. The advantage of weighting based on subfield rather having a different weight on each prior term is that it leads to a re-duction in the number of parameters affecting the prior dis-tribution. Let  X  mj denote the mixing weight for the prob-ability distribution of the m th subfield in the j th field and  X  =(  X  1 ,..., X  K ) denotes weights of all fields, where each  X  j denotes the vector of mixing weights for M j subfields.
In the Dirichlet Aspect Weighting(DAW) Model, an ob-servation tuple contains a word w occuring in a given query q . The query field label z may be hidden or belong to the observation tuple depending on whether w occurs in an un-structured query example or a structured query. The gen-erative process for this model is shown in Figure 2, and it follows the steps below. 1. Initially unigram language model  X  for all fields is sampled from a drichlet distri bution parameterized by sub-field mixing weights  X  , and prior term probabilities  X  . 2. A query q is chosen with uniform probability p ( q ) . 3. Given q , field label z is sampled from the distribution p ( z | q ) . 4. A word w is sampled according to p ( w | z,  X  ) .
The joint distribution can thus be written as
Next, we derive expressions for probability of variables in the generative process. Let  X  j be sampled from the dirichlet distribution D (  X  j 1 , X  j 2 ,..., X  jV ) ,where The weights  X  satisfy bility of the dirichlet prior on  X  j is thus, p (  X  j |  X  j , X  j )=
 X (
In the case of an unstructured query example the obser-vation tuple is ( q u ,w ) and z is a hidden variable. Here we assume that the number of aspects[7] is equal to the number of fields in the semantically structured query. In addition to the K aspects we also introduce a fixed background com-ponent, denoted as B . The background component is intro-duced based on our belief that in every unstructured query example there is a fixed proportion of noise words. The fixed mixing weight for the background component is inde-pendent of q u . Thus, z  X  X  1 ,...,K } X  X  B } . Marginalizing equation (3) over z and  X  , and considering the Background component, we have p ( q u ,w )= p ( q u ) (1  X  p ( z = B )) Note that p ( q u ) is a uniform probability distribution. Let O u denote the set of all observation tuples from F . Figure 2. A graphical model representation of
DAW model. The variable z is hidden for a un-structured query example ( q = q u ) and it is an observed variable in the case of a seman-tically structured query ( q = q s )
Let q s =( q s 1 ,...,q s K ) denote the semantically struc-tured query, where q s j is the set of words contained in the j th query field. For a semantically structured query, the field label z is also observed, and hence the observation tu-ple is ( q s ,w,z ) . It follows from equation (3) that,
Let c ( w, q s j ) denote the number of times word w occurs in q s with field tag z = j .Let O s denote the set of all observation tuples in an semantically structured query, then
Let  X  B = p ( z = B ) ,  X  jw = p ( w | z = j ) and  X  Bw = p ( w | z = B ) . Note that  X  B is a fixed external parameter, es-timated during training (see Section 7.2). The fixed Back-ground language model  X  Bw is estimated from the collec-tion of all documents in the dataset, denoted as C . where c ( w, d ) is the number of times w occurs in docu-ment d  X  X  .Let  X  jq u = p ( z = j | q u ) .
 (  X 
For every query, we need to estimate the parameters  X  , where  X =(  X ,  X  u , X  s , X  ) given the observations { O s ,O Let us further assume that the parameters  X  q , X  u , X  have uniform prior. Then, from the above discussion, we can write the posterior likelihood of the parameters given the observations as Let
In general when the value of R is large, the estimated values of  X  j  X  X  will get dominated by the properties of the examples added by feedback. This is something not de-sirable, especially when the value of R is large. It is de-sirable that the  X  j  X  X  are more biased towards the seman-tically structured query rather than the examples added through feedback. Thus to bias the estimated  X  j  X  X  towards q s we inflate the count of words in the semantically struc-tured query. Let c ( w, q s j ) be the inflated count, c ( w, q s  X   X  avdl  X  c ( w, q s length i.e., the average length of a document in the dataset and  X  is one of the external parameters of the model, set during training (see section 7.2). Note that  X  controls the amount of bias in the estimated model.

The Maximum a posteriori estimate of  X  is thus the so-lution to the following problem:
Maximize L ( X ) = subject to
We denote the constraint set in equation (5) as S 1 ,the constraint set in equation (6) as S 2 and the constraint set in equation (7) as S 3 .

We use Generalized Expect ation Maximization [10] (GEM) algorithm to maximize L ( X ) . The need for GEM arises since standard EM cannot be used. This is be-of  X  do not have a closed form solution. The expression for p (  X  j |  X  j , X  j ) is given in equation (4). Let us suppose that the estimate of the parameters in the ( n  X  1) th iter-ation is  X  ( n  X  1) . It can be easily seen that the paramters  X  s =(  X  1 q s ,..., X  Kq s ) have a closed form solution given by equation (9), which does not change with the iterations of GEM algorithm.
Next, we describe the E and M steps of the n th iteration of the GEM algorithm. The condition that these steps con-stitute a GEM algorithm i.e., equation (10) is later proved in Theorem 6.3. (a) Estep We compute, where the expectation is taken over the posteriors on hidden variable z . We denote the posteriors updated in this iteration as: Hence, P The posterior updates are, (b) Mstep
Maximization of Q ( X ;  X  ( n  X  1) ) does not have a closed form solution for all optimiza tion variables. This is because are a function of optimization variables. In the M step of GEM we need to find  X  ( n ) such that,
Since p (  X  j |  X  j , X  j ) does not contain the variables previ-ously denoted as  X  u , the updated values of these variables optimization problem, Note that  X  u is updated only once within the M step . The closed form solution for the optimization problem in equation (11) is
In order to find  X  ( n ) which satisfies equation (10) we adopt a strategy of alternating maximization between a set of variables previously denoted as  X  and another set of variables previously denoted as  X  . While maximizing the Q ( X ;  X  ( n  X  1) ) function for one set of variables, the other is assumed to be constant. The first set of variables denoted as  X  belong to the constraint set S 1 .

The variables in the second set, denoted as  X  belong to the constraint set S 2 .

The k th iteration of alternating maximization procedure within the M-step of the n th iteration of EM algorithm can be explained by equations (13) and (14).

In equation (13) we maximize Q ( X ;  X  ( n  X  1) ) w.r.t  X  , while using the optimal value of  X  from the previous itera-tion of alternating maximization within the M step, denoted as  X  ( n ) , ( k  X  1) . In equation (14) we maximize Q ( X ;  X  w.r.t  X  , while using the optimal value of  X  from the current iteration of alternating maximization within the M step, de-Alternating Maximization
There exists a closed form solution for  X  satisfying the maximization problem in equation (13) and it is given by equation (15). On the contrary there is no closed form so-lution for equation (14), but its solution is obtained for each  X  j by maximizing Hence we find a solution for equation (14) using an itera-tive technique called as gradient projection method [2]. We alternate between the closed form solution of equation (13) and the solution of the gradient projection method until con-vergence.

Let  X  ( n  X  1) j denote the converged value of  X  j in the alternating maximization procedure within ( n  X  1) th itera-tion of GEM algorithm, then the alternating maximization procedure within M step of the n th iteration of GEM algorithm is as follows, Alternate between: Closed Form Update of  X  and Gradient Projection Stage until convergence 2. Closed Form Update of  X  :  X  3. Gradient Projection Stage:
The updates below implement one iteration of the gradi-ent projection method and give the updated value of  X  j  X  The updates in equation (16) are continued until they con-verge. We denote the converged value as  X  ( n ) , ( k ) j tional details about equation (16) are provided in Section 6.2.
The additional details in equation (16) are explained below [ . ] + denotes a projection on the convex constraint set The value of  X   X  is chosen by maximization over [0 , 1] , i.e.,
We denote the gradient vector of f (  X  j ) as,  X  f (  X  where,  X  f m (  X  j )= X (
After estimating the parameters  X  , using the GEM algo-rithm described in Section 6.1 we find the query field mix-ing weights, The mixing weights {  X  j } K j =1 and the estimated {  X  jw used in the query language model in equation (2). This query language model is used in the KL divergence retrieval model.
In Lemma 6.1 we prove that the values estimated in equations (12) and (15) satisfy equations (13) and (14) re-spectively. In Theorem 6.3 we use Lemmas 6.1 and 6.2 to prove the convergence of E and M step iterations described in Section 6.1. the optimization problem in equation (18). Then this opti-mization problem achieves a global maximum for  X  given by the closed form solution in equation (15) tion problem in equation (19). Then this optimization prob-lem achieves a global maximum for  X  u given by the closed form solution in equation (12).
 Proof (a) The lagrangian for the optimization problem in equation (18) is,
By using the necessary condition for a local maxima i.e.,  X   X  L (  X ,  X  )=0 , we obtain the closed form solution for  X  in equation (15). Since the local maxima of a concave func-tion maximzed over a convex set is also the global maxi-mum the proof of this lemma is completed if we prove that
From the expression provided for Q ( X ;  X  ( n  X  1) ) in E step of Section 6.1, it can be seen that for is a concave function of  X  ,where C jw &gt; 0 . equation (19) is,
By using the necessary condition for a local maxima i.e.,  X   X  u L (  X  u , X  )=0 , we obtain the closed form solution for  X  u in equation (12). The proof of this lemma is completed if in  X  u .

From the expression provided for Q ( X ;  X  ( n  X  1) ) in E step of Section 6.1, it can be seen that for is a concave function of  X  u ,where C jq u &gt; 0 . Lemma 6.2 Let Q (  X  ( n ) , ( k ) , X  ( n ) u , X  s , X  ; X  ( global maximum at  X  =  X  ( n ) , ( k ) when (  X  ( n ) , ( k are constants. Then the updates in equation (16) converge Proof Since the updates in equation (16) implement the gradient projection method in [2], by Prop. 2.3.1 [Ch. 2] in [2] these updates converge to a stationary point in S 2 over  X  , then there is only one stationary point in the inte-rior of S 2 which corresponds to the global maximum, i.e.,  X  completes the proof. From equation (4) and the expression for Q ( X ;  X  ( n  X  1) ) in E step of Section 6.1, we have
From Theorem 1.2.5 and Corollary 1.2.6 [Ch. 1] in [1] it follows that log  X ( function of  X  .
 Hence,  X  log  X ( tion of  X  . Since a linear function is also a concave func-tion, it follows that, function of  X  .
 Since sum of concave functions is also concave.
 Q (  X  ( k ) , X  ( k ) u , X  s , X  ) is a concave function of  X  . Theorem 6.3 Let  X  ( n ) , ( k ) be the value of the parameter set  X  in the k th iteration of the alternating maximization procedure within the n th iteration of E and M steps described in Section 6.1. Then, (a) The sequence produced by alternating maximiza-tion procedure of equations (13), (14) converges to point (  X  (b) The iterations of E and M steps described in Section 6.1 constitute a Generalized Expectation Maximization ( GEM ) algorithm [10], and hence convergent.
 Proof (a) In the above Theorem statement  X  gence of alternating minimization procedures was proved using Zangwill X  X  Convergence Theorem A [Ch. 4] in [16]. Corollary 15 in [15] deals with the convergence of alternating projection algorithm. It can be inferred from this Corollary, that an algorithm maximizing a function by alternatingly finding global maxima on two compact sets generates a sequence of iterates which converges in norm. Here, the constraint sets S 1 , S 2 are clearly compact; and equations (13) and (14) find the global maxima on S 1 , S 2 respectively. Thus by applying the Corollary, the sequence of iterates {  X  ( n ) , ( k ) , X  ( converges, i.e., lim k  X  X  X   X  ( n ) , ( k ) = X  ( n ) , where  X  (  X  (b) From equations (11), (13) and (14), it follows that
Initial value for the alternating maximization procedure equation (20) recursively for k =1 , 2 ,...,  X  , it follows
Hence, it is proved that the E and M step updates in Sec-tion 6.1 constitute a GEM algorithm
We evaluate the performance of DAW query language model using the dataset of TREC 2005 Genomics Track [6], adhoc retrieval task. The dataset consists of 4.5 million doc-uments, which are a subset of the MEDLINE bibliographic database. This dataset consists of 50 query topics, of which we use the first 15 query topics to form the training set and the rest 35 query topics are used for evaluating the perfor-mance of different retrieval methods.

We add prior terms to the query using the following ex-ternal databases, Medical Subject Headigs(MesH) and En-treze Gene. MesH is the National Library of Medicine X  X  controlled vocabulary thesaurus. It consists of sets of terms naming descriptors in a hierachical structure that permits searching at various levels of specificity. Entreze Gene is a database of National Center for Biotechnology Informa-tion for gene specific information. It provides information about genomes that have been completely sequenced. The terms obtained from this database for each field in the orig-inal query are added to the related subfield. The number of subfields for which we can add p rior terms varies between 0 and 4. If a particular field has query terms for which we do not obtain any prior terms from the external databases, then it has zero subfields.
The parameter  X  B is the background component mixing weight. Since most of the words in the document collection are noisy, we believe that if we cluster the word frequencies into two clusters, the larger of the two should correspond to noisy words, and their corresponding proportion to  X  B .We thus use a Gaussian mixture model with two Gaussian com-ponents to cluster the word frequencies, C is the document collection. After estimating the parame-ters of this two component Gaussian mixture model we set  X 
B equal to the mixing weight with a higher value. Thus the value of  X  B =0 . 8768 . For different values of number of feedback documents i.e., R = { 5 , 10 , 30 , 50 } we obtain optimal value of  X  by varying it in [0,1] to maximize mean average precision(MAP) [8] over query topics in the train-ing set.

For example when R =5 the value of  X  =0 . 5 .The smoothing parameter  X  in equation (1) for the document language model is set by maximizing the MAP value while using the original queries in the training set. Thus the op-timized value of  X  = 2000 , and for all experiments in this paper we use this value of  X  only. In equation (17) we set s =0 . 1 , as any positive value of s is sufficient. In order to understand the retrieval performance of the DAW model, we compare its performance with some base-line methods that do not use any feedback documents and also with baseline methods that use feedback documents. 7.3.1 No feedback The following baseline methods do not use any feedback. 1. Original Unigram : The retrieval model used in this method is same as that described in Section 3. This method is called as Original Unigram because we only use the orig-inal query while estimating the unigram language model. 2. Original TFIDF+Okapi : In this retrieval method we use the TFIDF model [14] with Okapi TF weighting [12]. This method is called as Original TFIDF+Okapi because the query used in this retrieval method consists of only terms in the original query. 3. Concatenate Unigram : In this method the query is formed by concatenating the terms from the external databases to the original query i.e., the concatenated query is { q s ,E } . The retrieval model used in this method is same that described in Section 3. Hence this method is called as Concatenate Unigram. 4. Concatenate TFIDF+Okapi : In this method the query is formed by concatenating the terms from the ex-ternal databases to the original query i.e., the concatenated query is { q s ,E } . In this retrieval method we use the TFIDF model [14] with Okapi TF weighting [12]. Hence this method is called as Concatenate TFIDF+Okapi. 7.3.2 With feedback The DAW model makes use of pseudo relevance feedback in augmenting unstructured query examples to the struc-tured query example. Hence we use minor modifications of two well known pseudo relevance feedback techniques in order to integrate the terms added from the external databases optimally with the original query. The baseline method which is a minor modification of the mixture model feedback technique in [17] is called as Interpolated Query Mixture Feedback Model and is denoted as IQM .The baseline method which is a minor modification of Rocchio feedback[13], using TFIDF model with Okapi TF weight-ing is called as Scaled Query Rocchio Feedback Model and is denoted as SQR . 1. Interpolated Query Mixture Feedback Model(IQM) : In this method the query language model is given by Where  X  is the query interpolation weight. This weighting is mean X  X  to ensure that the content in the original query is not lost due to the terms from the external databases.
After estimating  X  F from the mixture model feedback technique in [17], we need to interpolate the estimated model with the original query model  X  q , the new model is  X  q =(1 polation weight. The values of the parameters  X ,  X  are set by varying them in the range [0, 1] so as to maximize MAP value on the training set queries for different feedback num-ber of documents. 2. Scaled Query Rocchio Feedback Model(SQR) :Inthis baseline method we scale the frequency of all terms in the original query by  X  and add the terms in all the subfields to it. We call  X  as the original query scaling factor. The scaled query q scaled has a term frequency, c ( w, q scaled  X c ( w, q s )+ c ( w, E ) . We then use this scaled query in the TFIDF model [14] with Okapi TF weighting [12], while us-ing Rocchio feedback[13]. The value of  X  is set by varying it in the range [1,100] so as to maximize MAP value on the training set queries for different feedback number of docu-ments.
We use mean average precision(MAP) to evaluate re-trieval performance. Mean average precision [8] is a stan-dard measure used in Text Retrieval Conference(TREC) Evaluation. It was used as a measure for judging perfor-mance of various retrieval schemes in TREC Genomics Track 2005 [6]. As stated earlier in this Section we only use the 35 test set query topics for our evaluation. Table 1 shows the highest values of MAP for various retrieval methods without feedback and with feedback. The MAP for the Original Unigram method using KL divergence retrieval model is 0.2420. The MAP for Original TFIDF+Okapi method is 0.2314. Thus we are assured that the initial run of results generated by the KL divergence retrieval model is better than a traditional term weighting retrieval method like the Okapi weighting schem e. The MAP for Concatenate Unigram is 0.1528. It is to be noted that there is decrease of 36.86% in the MAP value for the unigram model by sim-ply concatenating the terms from the external databases to the original query. In the case of TFIDF+Okapi method the MAP value decreases to 0.1381 after concatenating the terms from the external databases. It is to be noted that there is a decrease of 40.32% in the MAP value of the TFIDF+Okapi method by simply concatenating the terms from the external databases to the original query. 7.4.1 Effect of weighting external database terms It can be seen from Table 1 that the MAP for IQM and SQR methods which use some form of weighting be-tween the original query terms and the terms from external databases performs substantially better than Concatenate Unigram and Concatenate TFIDF+Okapi methods, which simply concatenate the external database terms to the orig-inal query. As described earlier the values of the query interpolation weight  X  and the original query scaling fac-tor  X  are set by maximizing MAP value on the training set queries. Thus the optimal value of  X  =0 . 9 and  X  =7 .The Concatenate Unigram method is same as the IQM method with  X  =0 . 5 . The query in Concatenate TFIDF+Okapi method is same as the scaled query q scaled , with  X  =1 . Hence, it is evident that by giving a relatively higher weight to the original query as compared to the terms from the ex-ternal databases we can get a substantial improvement in re-trieval performance. It is to be noted that MAP of IQM im-proves over Concact unigram by 66.03%, and that of SQR based methods. DAW model has the best retrieval performance improves over Concact TFIDF+Okapi method by 68.07%. Thus by using simple weighting between the original query terms and external databse terms we get significant improv-ment in results. It is to be seen as to how DAW model per-forms in comparison with IQM and SQR methods. 7.4.2 DAW vs all other methods Table 1 also shows the highest MAP values for the feedback based methods DAW, IQM and SQR. For all the feedback based methods we vary the number of feedback documents i.e., R = { 5 , 10 , 30 , 50 } and report the highest MAP value. In case of DAW method and IQM method we get the high-est MAP for R =5 . The SQR method achieves the highest MAP for R =10 . The DAW model has value of MAP = 0.2656, the other external database term weighted methods IQM and SQR have a MAP values of 0.2537 and 0.2321. Thus it is evident that of all methods that integrate exter-nal database terms with the orginal query, DAW model per-forms the best.
It can be seen from Figure 3 that MAP of DAW model for R =5 gradually rises from its value corresponding to  X  less than 0.1 to its peak value corresponding to  X  =0 . 5 and gradually decreases there after.

The MAP has a relatively low value of 0.2392 at  X  = 0 . 05 , then it increases to 0.2436 at  X  =0 . 1 , further in-creases to 0.2535 at  X  =0 . 25 and finally peaks at  X  =0 . 5 with a value of 0.2656. For values of  X  greater than 0.5 the MAP slowly decreases and reaches as low as 0.2427 for  X  =1 .

This trend can be understood from the  X  parameter up-date equation (15) of GEM algorithm. Since c ( w, q s j )=  X   X  avdl  X  c ( w, q s in the feedback documents to bias the estimation of  X  to-wards them, thus introducing noise into the query language model. This explains for a low value of MAP at  X  =0 . 05 .
A high value like  X  =1 biases the estimation of  X  to-wards the terms in the original query, resulting in a query language model similar to that with original query terms only. Hence the MAP at  X  =1 is almost equal to the MAP of Original Unigram method. It can be seen from Figure 4 that for the DAW model the MAP value decreases as we increase the number of feed-back documents( R ) from 5 to 50. The DAW model per-forms better than the IQM and SQR models for R = 5, 10. For R = 30, 50 the IQM method has a higher MAP over DAW model. The SQR model has a retrieval performance much below DAW and IQM method for R = 5, 10, 30. For R = 50 the SQR model has a slightly higher MAP over DAW model. Thus it is evident that the DAW model per-forms better than baseline methods when R = 5, 10, but as the number of feedback documents are increased i.e., R = 30, 50, its retrieval performance decreases.
Figure 3. A plot of MAP vs  X  for the DAW model with R =5
Figure 4. A comparative plot of MAP values for DAW, IQM and SQR methods while vary-ing the number of feedback documents ( R ).
 DAW model performs better than IQM and
SQR methods for R =5,10
We introduce the Dirichlet Aspect Weighting(DAW) model which facilitates the integration of terms from ex-ternal database with a semantically structured query as a prior in a bayesian learning framework. The advantage of this model is that rather than weighting each prior term, we weight the subfield to which it belongs to, thus leading to a major decrease in the number of parameters governing the Dirichlet prior. For estimation of parameters of the model, we introduce a novel Generalized EM algorithm, using gra-dient projection method. Comparison with other techniques which are minor modifications pseudo relevance feedback shows that DAW model has better performance than base-line methods.
We thank Himanshu Arora and ChengXiang Zhai for their helpful comments on this work. This work was sup-ported in part by National Science Foundation Grant CCF 04-26627.

