 We present a multicriteria clustering approach that has been developed to address a problem known as attack attribution in the realm of investigative data mining. Our method can be applied to a broad range of security data sets in order to get a better understanding of the root causes of the underly-ing phenomena that may have produced the observed data. A key feature of this approach is the combination of cluster analysis with a component for multi-criteria decision analy-sis. As a result, multiple criteria of interest (or attack fea-tures) can be aggregated using different techniques, allowing one to unveil complex relationships resulting from phenom-ena with eventually dynamic behaviors. To illustrate the method, we provide some empirical results obtained from a data set made of attack traces collected in the Internet by a set of honeypots during two years. Thanks to the ap-plication of our attribution method, we are able to identify several large-scale phenomena composed of IP sources that are linked to the same root cause, which constitute a type of phenomenon that we have called Misbehaving cloud (MC). An in-depth analysis of two instances of such clouds demon-strates the utility and meaningfulness of the approach, as well as the kind of insights we can get into the behaviors of malicious sources involved in these clouds.
 Investigative data mining, attack attribution, threat analy-sis. There is no real consensus on the definition of  X  X ttack attri-bution X  in the cyber domain. If one looks at a general def-inition of the term attribution in a dictionary, he will find something similar to:  X  X o explain by indicating a cause X  [Merriam-Webster]. However, we observe that most previ-ous work related to that field tend to use the term  X  X ttri-bution X  as a synonym for traceback , which consists in  X  X e-termining the identity or location of an attacker or an at-tacker X  X  intermediary X  [20]. In the context of a cyber-attack, the obtained identity can refer to a person X  X  name, an ac-count, an alias, or similar information associated with a per-son or an organisation. The location may include physical (geographic) location, or any virtual address such as an IP address or Ethernet address. The rationale for developing such attribution techniques is mainly due to the untrusted nature of the IP protocol, in which the source IP address is not authenticated and can thus be easily falsified. An ex-tensive survey of attack attribution techniques used in the context of IP traceback can be found in [20].
 In this paper, we refer to  X  X ttack attribution X  as something quite different from what is described here above, both in terms of techniques and objectives. Although tracing back to an ordinary, isolated hacker is an important issue, we are primarily concerned by larger scale attacks that could be mounted by criminal or underground organizations. For this purpose, we present an analytical method that can help security analysts in determining the plausible root causes of attack phenomena, and in deriving their modus operandi . The method presented hereafter can be applied to a broad range of security data sets, or more generally, to many prob-lems related to investigative data mining. This paper illus-trates the application of this method through an empirical analysis of some attacks collected during two years by a set of low interaction honeypots deployed all over the world by the Leurr  X e.com project [11]. Regarding this specific dataset, some typical phenomena that we want to identify vary from worm or malware families propagating through code injec-tion attacks [10], to established botnets controlled by the same people and targeting machines in the IP space. As showed in the experimental results, all malicious sources in-volved in the same root phenomenon seem to form what we call a Misbehaving Cloud (MC).
 The remainder of this paper is structured as follows: in Sec-tion 2, we formally describe the main components of our multi-criteria clustering method, i.e., a graph-based clus-tering process followed by a multi-criteria decision analysis. Then, Section 3 describes how we have applied this method to a specific dataset made of network attack traces. In Sec-tion 4, we detail some experimental results, and we provide a more in-depth study of two instances of misbehaving clouds. Section 5 concludes the paper. In investigative data mining, an analyst must usually syn-thesize different pieces of evidence to eventually identify the potential root causes of attack phenomena. The goal is to determine how to  X  X onnect the dots X , i.e., how to discover important patterns and how to combine them meaningfully, so as to expose the  X  X ig picture X  [19]. However, the amount of data of today X  X  systems used for gathering data far ex-Figure 1: Overview of the multi-criteria clustering method. F ceeds our capacity to analyze it manually. For this reason, we aim at developing a multi-criteria clustering approach that can systematically discover, extract and combine un-known patterns from a security dataset, according to a set of potentially useful features.
 As illustrated in Fig. 1, our approach is based on three com-ponents: 1. Feature selection : we determine which features we 2. Graph-based clustering : an undirected edge-weighted 3. Multi-criteria aggregation : the different graphs are The approach is mostly unsupervised, i.e., it does not rely on a preliminary training phase to classify objects to larger scale phenomena. Instead, we have only a data set of un-labeled observations, and we need to learn what patterns are present in the data in function of different characteris-tics that can hopefully bring some light on the underlying phenomenon. In cluster analysis, one of the very first steps consists in se-lecting some key characteristics from the dataset, i.e., salient features that may reveal meaningful patterns [7]. The se-lection of these features may optionally be completed by a feature extraction process, i.e., one or more transformations of the input to produce features that are more suited to subsequent processing. Pattern representation refers to the number of categories, classes, or variables available for each feature to be used by the clustering algorithm.
 More formally, we have thus a dataset D composed of N data objects. From D , we can select and/or extract n dif-ferent features so as to create our feature set F = { F k 1 ,...,n . The purpose of this first component consists in creating one set of feature vectors X k for each F k , i.e.: where a row X k ( i, 1 : p ) = x ( k ) i represents a feature vector for the i th object of D obtained for the k th feature F k dimensionality of the feature vectors is p , the number of variables (or categories) that have been extracted or defined. We now turn to the description of the second component of our method, which implements a pairwise clustering. We formulate the problem of clustering objects from D using a graph-based approach. That is, for each feature F k , we can construct a graph G k in which the vertices (or nodes) are mapped to the feature vectors x ( k ) i , and the edges (or links) express some degree of similarity between objects regard-ing the considered feature. As customary, we can represent the undirected edge-weighted graph (with no self-loops) ob-tained for a given feature F k by G k = ( V k ,E k , X  k ), where V the edge set and represents the relationships between each pair of vertices, and  X  k : E k  X &lt; + is a positive weight func-tion associated with each edge of E k .
 In practice, we represent each graph G k with its correspond-ing weighted adjacency matrix (or dissimilarity matrix), which is the N  X  N symmetric matrix A k ( i,j ) defined as: Note that the weight function  X  k ( i,j ) must be defined with a similarity metric that is appropriate to the nature of the feature vectors. In fact, there is a wide range of possibil-ities for chosing a distance, from rather simple ones (e.g., Euclidean, sample correlation, Mahalanobis, Minkowski) to more elaborate functions such as statistical distances (e.g., Kullback-Leibler, Bhattacharyya, etc). We further detail this aspect in the application of the method (Section 3). Finally, the graph-clustering can be performed by extracting strongly connected components for each graph G k . To do this, we use an algorithm based on maximal cliques , which are induced subgraphs in which the vertices are fully con-nected (i.e., complete subgraphs). In practice, this means we identify for each feature F k which subgroups of objects are highly similar and thus share the same pattern regard-ing F k . The Maximal Clique Problem (MCP) is a classical combinatorial problem that can be solved using different algorithms. In our method, we take advantage of the domi-nant sets approach of Pavan et al. [14], which generalizes the MCP to the edge-weighted case. Dominant sets (DM) are equivalent to maximum weighted cliques, hence to very co-herent clusters. However, finding dominant sets is attractive } ,k = 1 ,  X  X  X  ,n ) from a computational viewpoint, since it can be done with a continuous optimization technique that applies replicator dynamics (from evolutionary game theory). As a result, we can find dominant sets by simply making a particular tem-poral expression converge. More precisely, consider the fol-lowing dynamical system represented with its discrete time equation, and A k being the adjacency matrix of G k : with i = 1 ,...,N . Starting from an arbitrary initial state, this dynamical system will eventually be attracted by the nearest asymptotically stable point. As it has been showed in [14], this corresponds to a dominant set, hence to a max-imum weighted clique. Then, the DM algorithm will try to find a new dominant set with the remaining vertices of the graph until a stopping criterion is met (e.g., when the sum of the remaining edge weights is less than 0.05). Aggregation functions are used in many prototypical situa-tions where we have several criteria of concern, with respect to which we assess different options. The objective con-sists in calculating a combined score for each option, and this combined output forms then a basis from which deci-sions can be made. For example, aggregation functions are largely used in problems of multi criteria decision analysis (MCDA), in which an alternative has to be chosen based on several, sometimes conflicting criteria. Usually, the alter-natives are evaluated from different attributes (or features) that are expressed with numerical values representing a de-gree of preference, or a degree of membership.
 Definition (Aggregation function) . An aggregation func-tion is formally defined as a function of n arguments ( n &gt; 1) that maps the (n-dimensional) unit cube onto the unit inter-val: F : [0 , 1] n  X  X  X  [0 , 1], with the following properties [1]: (i) F (0 , 0 ,..., 0 (ii) x i  X  y i for all i  X  { 1 ,...,n } implies F ( x 1 ,...,x In our multi criteria method, we have n different attack features, according to the F k  X  X , and thus a vector of criteria z ij  X  [0 , 1] n can be constructed from the similarity weights, such that: with A k the weighted adjacency matrix of graph G k corre-sponding to attack feature F k , and ( i,j ) is a pair of objects from the data set D . Our approach consists in combining the n values of each criteria vector z ij (which reflects the set of all relationships between a pair of data objects), in order to build an aggregated graph G  X  = P G k .
 A rather simplistic approach consists in combining the cri-teria using a simple arithmetic mean, eventually with differ-ent weights assigned to each criteria (i.e., a weighted mean). However, this does not allow us to model more complex re-lationships, such as  X  X ost of X , or  X  X t least two X  criteria to be satisfied in the overall decision function, and this without having to know which set of criteria is more relevant for a given pair of objects. In other words, what we need is an aggregation function in which the combination of criteria of interest (and the associated weights) is not predetermined in a static way.
 Ordered Weighted Averaging.
 Yager has introduced in [21] a type of operator called Or-dered Weighted Averaging (OWA), which allows to include certain relationships between criteria in the aggregation pro-cess. An OWA operator differs from a classical weighted means in that the weights are not associated with particular inputs, but rather with their magnitude . As a result, OWA can emphasize the largest, smallest or mid-range values. It has become very popular in the research community work-ing on fuzzy sets.
 Definition (OWA) . For a given weighting vector w , w i  X  0, P w i = 1, the OWA aggregation function is defined by: where we use the notation z &amp; to represent the vector ob-tained from z by arranging its components in decreasing It is easy to see that for any weighting vector w , the result of OWA lies between the classical and and or operators, which are in fact the two extreme cases when w = (0 , 0 ,..., 1) (then OWA w ( z ) = min ( z )) or when z = (1 , 0 ,..., 0) (then OWA w ( z ) = max ( z )). Another special case is when all weights w i = 1 n , which results in the classical arithmetic mean.
 To define the weights w i to be used in OWA, Yager sug-gests two possible approaches: either to use some learning mechanism with sample data and a regression model (i.e., fitting weights by using training data and minimizing the least-square residual error), or to give some semantics, or meaning to the w i  X  X  by asking a decision-maker to provide directly those values, based on domain knowledge. In many attribution cases, we shall rely on the latter since the process is mostly unsupervised, i.e., we have no training samples for the phenomena we aim to identify.
 Combined graph G  X  .
 The multicriteria aggregation leads finally to a combined graph G  X  , represented by its adjacency matrix A  X  , which can be obtained through following operation: Finally, we can extract the connected components from G  X  in order to identify all subgraphs in which any two vertices are connected to each other by a certain path: which gives us our final set of subgraphs P , where SG x  X  G , and  X  ( i,j )  X  SG x : OWA w ( z ij )  X  t , with t  X  ]0 , 1]. There exist several algorithms to extract connected compo-nents from a graph (depth-first search, breadth-first search, etc). We use here the Dulmage-Mendelsohn decomposition of A  X  , which is a lightweight and efficient operation on ma-trices [6].
 Each subgraph can now help the analyst to figure out which root phenomenon could have created the observations within SG x . As we show with a practical application in Section 4, by analyzing and visualizing the resulting phenomena through the clustering results of their respective features, we can get a global picture of all important relationships among the observations of a same subgraph, and hence we get a better insight into the root cause and the behavior of the underly-ing phenomenon.
 Note that we can optionally apply a thresholding function on A  X  in order to eliminate combined edges that could re-sult from an unfortunate linkage between two objects having some weak correlation for a number of features, which means they would otherwise end up in the same subgraph while not related to the same root phenomenon. To illustrate our multi-criteria attribution method, we present a specific application of this method to a set of attack traces collected by honeypots in the Internet. Our dataset is made of network attack traces collected from a distributed set of sensors (e.g., server honeypots), which are deployed in the context of the Leurr  X e.com Project [9; 11]. Since honeypots are systems deployed for the sole purpose of being probed or compromised, any network connection that Figure 2: Illustration of a M -event, composed of 3  X  -events that they establish with a remote IP can be considered as mali-cious, or at least suspicious. In Leurr  X e.com, each IP source observed on a sensor is assigned to a so-called attack clus-ter [16] according to its network characteristics, such as the number of IP addresses targeted on the sensor, the number of packets and bytes sent to each honeypot, the attack du-ration, the average inter-arrival time between packets, the associated port sequence being probed, and the packet pay-load. Therefore, all IP sources belonging to a given attack cluster have left very similar network traces on a given sen-sor and consequently, they are considered as having the same attack profile . This leads us then to the concept of micro at-tack event: Definition (  X  -event). A micro attack event (or  X  -event) refers to a set of IP sources having the same attack profile on a given sensor, and whose coordinated activity has been observed within a specific time window.
 Fig. 2 illustrates this notion by representing the time series (i.e., the number of sources per day) of three coordinated  X  -events observed on two different sensors in the same time interval, and targeting three different ports. By extension, a macro event (or M -event) refers to the set of all  X  -events observed over the same time period, and during which the time series are strongly correlated (e.g., the three  X  -events in Fig. 2 belong to the same M -event). How to identify such events from the spurious, nonproductive traffic collected by honeypots are issues that have been explained in [15]. For the purpose of this study, our dataset D comprises 2,454  X  -events that have been observed on 40 different platforms located in 22 different countries, on a period spanning from Sep 2006 until November 2008. Our set of  X  -events accounts for a total of 2,538,922 malicious sources, which have been assigned to 320 distinct attack profiles.
 In the rest of the paper, we show how we have applied our multi-criteria method to this set of attack events in order to establish connections between them. All  X  -events that share enough features constitute a phenomenon that we call a Misbehaving Cloud (MC). We hypothesize that mali-cious sources involved in a MC have a common root cause. By identifying them and studying their global behavior, we hope to get a better insight into the modus operandi and the strategies of those responsible for them. The first key features used hereafter deal with the spatial distributions of malicious sources involved in  X  -events, in terms of originating countries and IP blocks. Looking at these statistical characteristics may reveal attack activities having a specific distribution of originating countries or IP networks, which can help for instance to confirm the exis-tence of  X  X nclean networks X  [3]. Concretely speaking, for each  X  -event i , we create a feature vector x geo i ing the distribution of countries of origin (as a result of the IP to geolocation mapping), and a vector x sub i represent-ing the distribution of IP addresses (grouped by their Class A-prefix, to limit the vector X  X  size to 256 categories). We have also selected an attack characteristic related to the targeted platforms . Looking at which specific platform has observed a  X  -event is certainly a pertinent feature. More-over, we combine this information with the M -event identi-fication, since M -events are composed of  X  -events that are strongly correlated in time (which indicates a certain degree of coordination among attackers). This leads to the creation of a feature vector x targ i for each  X  -event.
 Besides the origins and the targets, the type of activity per-formed by the attackers seems also relevant. In fact, ma-licious software (e.g., worm or bot) is often crafted with a certain number of available exploits targeting a given set of TCP or UDP ports. So, it makes sense to take advantage of similarities between the sequences of ports that have been probed or exploited by malicious sources. This gives us a feature vector x ps i made of several categories representing the targeted ports for each  X  -event.
 Finally, we have computed, for each pair of  X  -events, the ra-tio of common IP addresses. We are aware of the fact that, as time passes, some machines of a given botnet (or misbe-having cloud) might be cured while others may get infected (and thus join the cloud). Additionally, due to the dynamic IP allocation of ISP X  X , certain infected machines can have different IP addresses when we observe them at different moments. Nevertheless, considering the huge size of the IP space, it is still reasonable to expect that two  X  -events are probably related to the same root phenomenon when they have a high percentage of IP addresses in common.
 To summarize, for each  X  -event we define our set of features F as follows: where: 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; : Recall that in the second step, we create an undirected weighted graph for each attack feature separately, on which we apply a graph-theroretical clustering algorithm in order to extract maximal cliques. To create the dissimilarity ma-trix A k for each graph G k , quite obviously, we need an ap-propriate distance function. When we have to deal with ob-servations that are in the form of probability distributions (or frequencies), like in the case of features F geo and F then statistical distances seem more appropriate. One such technique (which is commonly used in information theory) is the Kullback-Leibler divergence [8]. Let p 1 and p 2 be for instance two probability distributions over a discrete space X , then the K-L divergence of p 2 from p 1 is defined as: which is also called the information divergence (or relative entropy ). Because D KL is not considered as a true met-ric, it is usually better to use instead the Jensen-Shannon divergence (JSD) [12], defined as: where  X  p = ( p 1 + p 2 ) / 2. In other words, the Jensen-Shannon divergence is the average of the KL-divergences to the aver-age distribution .
 Now, to transform pairwise distances d ij to similarity weights sim ij , we still have to define a mapping function. Previ-ous studies found that the similarity between stimuli decay exponentially with some power of the perceptual measure distance [17]. As customary, we can thus use the following functional form to do this transformation: where  X  is a positive real number that affects the decreasing rate of sim ij .
 Measuring pairwise similarities for the other considered fea-tures ( F targ ,F ps ,F cip ) can be done using simpler distance functions, such as the Jaccard similarity coefficient . Let s and s 2 be two sample sets (for instance with F ps , s 1 and s are sets of ports that have been probed by sources of two  X  -events), then the Jaccard coefficent is defined as the size of the intersection divided by the size of the union of the sample sets, i.e.: Extracting cliques of attackers.
 We applied the unsupervised graph-theoretic clustering al-gorithm on each dissimilarity matrix A k , as described in Section 2.3. The goal consists in discovering interesting pat-terns by extracting all groups of highly similar events. The obtained knowledge will be used to characterize the behavior of global phenomena identified by the multi-criteria compo-nent.
 Globally, for each feature F k , the clique algorithm could find on average about 85 clusters accounting for approximatively 70% of the data set, which seems to indicate that many strong relationships exist among  X  -events. Note that the dominant set framework is quite attractive, since it does not require a number of clusters as input, and the algorithm will naturally extract the most significant groups in the first stages of the algorithm.
 To illustrate this step, we have mapped on Fig. 3 some ge-ographical clusters found for F geo . We have used a dimen-sionality reduction technique called t-Distributed Stochastic Neighbor Embedding (t-SNE), which aims at converting a high-dimensional dataset into a low-dimensional representa-tion that can be displayed, for example, in a scatter plot. rectangle on the left.
 The aim of dimensionality reduction is to preserve as much of the significant structure of the high-dimensional data as possible in the low-dimensional map. This can be helpful to visualize a certain dimension of the data set, but also to assess the consistency of the clustering results. t-SNE [18] is a variation of Stochastic Neighbour Embedding ; it pro-duces significantly better visualizations than other Multi-dimensional Scaling techniques (such as Sammon mapping, Isomaps or Laplacian Eigenmaps) by reducing the tendency to crowd points together in the centre of the map.
 Figure 3 (Left) shows the resulting two-dimensional plot ob-tained by mapping the top 20 geographical clusters (encom-passing a total of 720  X  -events) on a 2D map using t-SNE. Each datapoint on this map represents the geographical distribution of a  X  -event. The coloring refers to the clus-ter membership of each event, as obtained by applying the clique algorithm, while the text labels indicate the cluster centro  X  X d. We could easily verify that two adjacent events on the map have highly similar geographical distributions, while two distant events have clearly nothing in common in terms of originating countries.
 However, as showed in Figure 3 (Right), we observe that several clusters of  X  -events originate from some large or pop-ular countries (e.g., US, China, Korea, France, Italy, etc). Although the clique algorithm did perform well, several clus-ters are overlapping with each other. Not surprisingly, the same kind of issue appeared for other dimensions as well (for instance, some well-known Windows ports are more heavily targeted than others). As a result, it leads to the natu-ral intuition that multiple features need to be aggregated in a consistent manner so as to leverage the results of this knowledge discovery process. This last step aims at combining all similarity values for each pair of  X  -events ( i,j ), by applying the OWA w operator to each criteria vector z ij constructed from all graphs G with k  X  X  geo,sub,targ,ps,cip } . However, we still need to define which values of the weighting vector w are the most appropriate to model the phenomena under scrutiny. In this case, we hypothesize that attack phenomena such as misbe-having clouds (e.g., worms, botnets) may perfectly evolve over time. That is, two consecutive  X  -events of the same MC must not necessarily have all their attributes in com-mon. For example, the composition of a botnet may evolve over time because of the cleaning of infected machines or the recruitment of new bots (which leads to a shift in the IP subnet distribution of subsequent events related to this botnet). Or, a botnet may be instructed to scan several consecutive IP subnets in a short interval of time, which leads to the observation of different  X  -events having highly similar distributions for the origins, but those events target completely different sensors, and may eventually use differ-ent exploits (hence, targeting different ports).
 Based on this domain knowledge, we have thus defined a terpreted as: at least three criteria must be satisfied, but the first criteria is of less importance (because the first corre-lated feature between two  X  -events might be due to chance only). These weights must be carefully chosen in order to avoid an unfortunate linkage between  X  -events when, for ex-ample, two  X  -events involve IP sources originating from pop-ular countries (typ. US, China, Korea, Germany, etc), and are targeting common (Windows) ports in the same inter-val of time; but in reality, those  X  -events are not necessarily linked to the same phenomenon. By considering different worst-case scenarios, we verified that this weighting vector w minimizes the final output value in such undesirable cases. We now turn to the description of the experimental results obtained from the application of the multi-criteria method on a 2-year dataset of honeypot traces, as described in pre-vious Section. Starting from the 2,454  X  -events, the method has identified a total of 83 Misbehaving Clouds ( MCs ), which correspond to 1,607  X  -events, and 506,835 attack-ing sources. Singletons and MC X  X  containing less than 3  X  -events have been discarded for further analysis. The phe-nomena involve almost all common services such as NetBios (ports 139/TCP, 445/TCP), Windows DCOM Service (port 135/TCP), Virtual Network Computing (port 5900/TCP), Microsoft SQL Server (port 1433/TCP), Windows Messen-ger Service (ports 1025-1028/UDP), Symantec Agent (port 2967/TCP), and some others. Figure 4 shows the cumula-tive distribution of  X  -events per MC. As we can see, in most cases, the MCs contain rather few  X  -events and sources. However, around 10% of MCs contain at least 20 thousand observable 1 sources, and some even contain up to 200 thou-sand sources (spread over 300  X  -events or more). Regarding the lifetime of these MC X  X  (i.e., the time interval, in days, between the very first and the very last attack event), about 67% of MCs exist during less than 50 days but around 22% of them last for more than 200 days. Another global char-acteristic (not represented on the Fig.) is that, in 94% of the cases, the MCs are seen on less than 10 platforms. These various characteristics suggest that the root causes behind the existence of these MCs are fairly stable, lo-calised attack processes. In other words, different places of the world do observe different kind of attackers but their modus operandi remain stable over a long period of time. We are, apparently, not that good at stopping them from misbehaving.
 Regarding the origins of MC X  X , we observe some very persis-tent groups of IP subnets and countries of origin. On Fig. 5, we have represented the CDF of the IP addresses involved in the ten largest MC X  X , where the x-axis represents the first byte of the IPv4 address space. Clearly, malicious sources in-volved in those phenomena are highly unevenly distributed, and form a relatively small number of tight clusters that are responsible for a large deal of the observed malicious activities. This is consistent with other prior work on moni-toring global malicious activities, in particular with previous studies related to measurements of Internet background ra-diation [2; 13; 22]. However, we can show here that there are still some notable differences in the spatial distributions of those misbehaving clouds, even though there is a large overlap between  X  X ombie-friendly X  IP subnets. Moreover,
It is important to note that the sizes of the phenomena given here only reflect the number of sources we could ob-serve on our sensors; the actual sizes of those armies are most probably much larger, even though some churn effects (DHCP, NAT) could also affect these numbers. Figure 5: CDF of IP addresses involved in the ten largest MC X  X . because of the dynamics of this kind of phenomenon, we can even observe different spatial distributions within the same cloud at different moments of its lifetime. This is an advantage of our analysis method that is more precise and enables us to distinguish individual phenomena, instead of global trends, and even to observe their dynamic behavior. Another interesting observation on Fig. 5 is the CDF of MC3 (uniformly distributed in the IPv4 space, which means ran-domly chosen source addresses) and MC20 (a constant dis-tribution coming exclusively from the subnet 24.0.0.0/8). A likely explanation is that those MC X  X  have used spoofed ad-dresses to send UDP spam messages to the Windows Mes-senger service. So, this indicates that IP spoofing is still possible under the current state of filtering policies imple-mented by certain ISP X  X  on the Internet. We refer the in-terested reader to [5] in which we provide a more in-depth analysis of this interesting UDP spam phenomenon. Finally, we further detail two case studies from Table 2 to illustrate some typical behaviors we could observe among the misbehaving clouds identified so far, e.g.: i) a move (or drift) in the origins of certain MC X  X  (both ii) a scan sweep by the same cloud, targeting several con-iii) within the same cloud, multiple changes in the port iv) a higher-level coordination among attackers of the same Botnet Cloud.
 An more in-depth analysis of MC28, in particular the shape of the time series of  X  -events and the arrival rate of the sources, has led us to conjecture that MC28 is quite likely due to a botnet phenomenon. What is of interest here is that it has showed the behaviors i) and iv) . On Fig. 6, we can see this cloud had four main waves of activity during which it was randomly scanning 5 different subnets (note the perfect coordination among the time series). When inspecting the subnet distributions of the different attack waves, we could clearly observe a drift in the origins of those of phenomenon, based on the results of the attack attribution method. sources, probably as certain machines were infected by (resp. cleaned from) the bot software. Then, we found also that a smaller subset of  X  -events in this MC were involving ma-chines that directly attacked the Windows ports (without scanning them). However, this group of attacking zombies had quite different origins from those of the scanners, and seem to be ordered to attack only specific IP addresses on our sensors (i.e., the Windows honeypots). We conjecture that the attackers probably took advantage of the results given by the larger set of scanners. In Annex 1, we provide a graph visualization of this misbehaving cloud to further illustrate its behavior (i.e., the separation of duties between scanners and attackers, and the drift in the origins of the sources that leads to multiple geographical clusters). Worm-behaving Cloud.
 MC2 is an interesting case in which we can observe the be-haviors ii) and iii) . It consists of 122  X  -attack events that have a shape which is fairly similar to the one left by a net-work worm: its trace exists for several days, it has a small amplitude at the beginning but grows quickly, exhibits im-portant drops that can correspond to subnets being cured or blacklisted, and it eventually dies slowly [15]. The lifetime of this MC was fairly long (about 741 days!). It is composed of  X  -events that have targeted a number of dis-tinct services, including 1025T, 135T, 139T, 1433T, 2967T and 5900T. The results of the multi-criteria fusion algorithm indicate that those  X  -events have been grouped together mainly because of the following three features: geographical location, targeted platform, and ports sequence. Moreover, a detailed analysis reveals that an important amount of IP addresses is shared by many  X  -events composing this MC . A node-link graph is provided in Annex 1 to visualize the rather complex network structure formed by the  X  -events of this worm-behaving cloud, in which we can observe its highly dynamic behavior. The cloud has been scanning (at least) four consecutive class A-subnets during its lifetime, while probing at the same time several ports on these sub-networks. It is not excluded that all these attacks could be due to two or three distinct worms (and thus, different groups of people). However, this result indicates that the same core piece of code has probably been reused, from a very similar starting point to launch a number of distinct at-tacks, which is an important piece of information for those who are in charge of identifying those misbehaving groups and their modus operandi. We have presented a generic and systematic method to ad-dress the complex problem related to  X  X ttack attribution X . Our approach relies on a novel combination of a graph-based clustering analysis and a multi-criteria aggregation process. We have applied our technique to 2 years of attack traces collected by 40 honeypots located all over the world, which has delivered some interesting results showing the utility and the meaningfulness of this approach. It is worth noting that the method could as easily be applied on completely different threats-related events. In fact, the interim Syman-tec report published mid October 2009 on the analysis of Rogue Security Software [4] offers results of the application of this very same method to the problem of understanding the modus operandi of malicious users setting up Rogue AV campaigns.
 It is our hope that people will be interested in trying to understand the rationales behind the Misbehaving Clouds we have identified. We are eager to share as much information as possible with such interested parties. Similarly, we are looking forward in having other opportunities to apply this method to other security datasets that future partners would be willing to share with us.

