 Jason Weston  X  jasonw@nec-labs.com Fr  X ed  X eric Ratle  X  frederic.ratle@gmail.com Ronan Collobert  X  collober@nec-labs.com Embedding data into a lower dimensional space or the related task of clustering are unsupervised dimension-ality reduction techniques that have been intensively studied. Most algorithms are developed with the moti-vation of producing a useful analysis and visualization tool.
 Recently, the field of semi-supervised learning (Chapelle et al., 2006), which has the goal of improv-ing generalization on supervised tasks using unlabeled data, has made use of many of these techniques. For example, researchers have used nonlinear embedding or cluster representations as features for a supervised classifier, with improved results.
 Most of these architectures are disjoint and shallow , by which we mean the unsupervised dimensionality reduction algorithm is trained on unlabeled data sep-arately as a first step, and then its results are fed to a supervised classifier which has a shallow archi-tecture such as a (kernelized) linear model. For ex-ample, several methods learn a clustering or a dis-tance measure based on a nonlinear manifold embed-ding as a first step (Chapelle et al., 2003; Chapelle &amp; Zien, 2005). Transductive Support Vector Machines (TSVMs) (Vapnik, 1998) (which employs a kind of clustering) and LapSVM (Belkin et al., 2006) (which employs a kind of embedding) are examples of meth-ods that are joint in their use of unlabeled data and labeled data, but their architecture is still shallow . Deep architectures seem a natural choice in hard AI tasks which involve several sub-tasks which can be coded into the layers of the architecture. As argued by several researchers (Hinton et al., 2006; Bengio et al., 2007) semi-supervised learning is also natural in such a setting as otherwise one is not likely to ever have enough labeled data to perform well.
 Several authors have recently proposed methods for using unlabeled data in deep neural network-based ar-chitectures. These methods either perform a greedy layer-wise pre-training of weights using unlabeled data alone followed by supervised fine-tuning (which can be compared to the disjoint shallow techniques for semi-supervised learning described before), or learn unsu-pervised encodings at multiple levels of the architec-ture jointly with a supervised signal. Only considering the latter, the basic setup we advocate is simple: 1. Choose an unsupervised learning algorithm. 2. Choose a model with a deep architecture. 3. The unsupervised learning is plugged into any (or 4. Train supervised and unsupervised tasks using the The aim is that the unsupervised method will improve accuracy on the task at hand. However, the unsu-pervised methods so far proposed for deep architec-tures are in our opinion somewhat complicated and restricted. They include a particular kind of genera-tive model (a restricted Boltzmann machine) (Hinton et al., 2006), autoassociators (Bengio et al., 2007), and a method of sparse encoding (Ranzato et al., 2007). Moreover, in all cases these methods are not compared with, and appear on the surface to be completely dif-ferent to, algorithms developed by researchers in the field of semi-supervised learning.
 In this article we advocate simpler ways of perform-ing deep learning by leveraging existing ideas from semi-supervised algorithms so far developed in shal-low architectures. In particular, we focus on the idea of combining an embedding -based regularizer with a supervised learner to perform semi-supervised learn-ing, such as is used in Laplacian SVMs (Belkin et al., 2006). We show that this method can be: (i) general-ized to multi-layer networks and trained by stochastic gradient descent; and (ii) is valid in the deep learning framework given above.
 Our experimental evaluation is then split into three parts: (i) stochastic training of semi-supervised multi-layered architectures is compared with existing semi-supervised approaches on several benchmarks, with positive results; (ii) a demonstration of how to use semi-supervised regularizers in deep architectures by plugging them into any layer of the architecture is shown on the well-known MNIST dataset; and (iii) a case-study is presented using these techniques for deep-learning of semantic role labeling of English sen-tences.
 The rest of the article is as follows. In Section 2 we describe existing techniques for semi-supervised em-bedding. In Section 3 we describe how to generalize these techniques to the task of deep learning . Section 4 reviews existing techniques for deep learning, Section 5 gives an experimental comparison between all these approaches, and Section 6 concludes. A key assumption in many semi-supervised algorithms is the structure assumption 1 : points within the same structure (such as a cluster or a manifold) are likely to have the same label. Given this assumption, the aim is to use unlabeled data to uncover this structure. In order to do this many algorithms such as cluster kernels (Chapelle et al., 2003), LDS (Chapelle &amp; Zien, 2005), label propagation (Zhu &amp; Ghahramani, 2002) and LapSVM (Belkin et al., 2006), to name a few, make use of regularizers that are directly related to unsupervised embedding algorithms. To understand these methods we will first review some relevant ap-proaches to linear and nonlinear embedding. 2.1. Embedding Algorithms We will focus on a rather general class of embedding al-gorithms that can be described by the following type of optimization problem: given the data x 1 , . . . , x U find an embedding f ( x i ) of each point x i by minimizing w.r.t.  X  , subject to This type of optimization problem has the following main ingredients:  X  f ( x )  X  R n is the embedding one is trying to learn  X  L is a loss function between pairs of examples.  X  The matrix W of weights W ij specifying the sim- X  A balancing constraint is often required for cer-Many well known algorithms fit into this framework. Multidimensional scaling (MDS) is a classical al-gorithm that attempts to preserve the distance be-tween points, whilst embedding them in a lower di-mensional space, e.g. by using the loss function MDS is equivalent to PCA if the metric is Euclidean (Williams, 2001).
 ISOMAP (Tenenbaum et al., 2000) is a nonlinear embedding technique that attempts to capture mani-fold structure in the original data. It works by defin-ing a similarity metric that measures distances along the manifold, e.g. W ij is defined by the shortest path on the neighborhood graph. One then uses those dis-tances to embed using conventional MDS.
 Laplacian Eigenmaps (Belkin &amp; Niyogi, 2003) learn manifold structure by emphasizing the preserva-tion of local distances . One defines the distance metric between the examples by encoding them in the Lapla-cian L = W  X  D , where D ii = P j W ij is diagonal. Then, the following optimization is used:
X subject to the balancing constraint: Siamese Networks (Bromley et al., 1993) are also a classical method for nonlinear embedding. Neural networks researchers think of such models as a network with two identical copies of the same function, with the same weights, fed into a  X  X istance measuring X  layer to compute whether the two examples are similar or not, given labeled data. In fact, this is exactly the same as the formulation given at the beginning of this Section. Several loss functions have been proposed for siamese networks , here we describe a margin-based loss pro-posed by the authors of (Hadsell et al., 2006): L ( f i , f j , W ij ) = which encourages similar examples to be close, and dis-similar ones to have a distance of at least m from each other. Note that no balancing constraint is needed with such a choice of loss as the margin constraint inhibits a trivial solution. Compared to using con-straints like (2) this is much easier to optimize by gra-dient descent. 2.2. Semi-Supervised Algorithms Several semi-supervised classification algorithms have been proposed which take advantage of the algorithms described in the last section. Here we assume the set-ting where one is given L + U examples x i , but only the first L have a known label y i .
 Label Propagation (Zhu &amp; Ghahramani, 2002) adds a Laplacian Eigenmap type regularization to a nearest-neighbor type classifier: The algorithm tries to give two examples with large weighted edge W ij the same label. The neighbors of neighbors tend to also get the same label as each other by transitivity, hence the name label propagation . LapSVM (Belkin et al., 2006) uses the Laplacian Eigenmaps type regularizer with an SVM: minimize || w || 2 +  X  where H ( x ) = max(0 , 1  X  x ) is the hinge loss. Other Methods In (Chapelle &amp; Zien, 2005) a method called graph is suggested which combines a modified version of ISOMAP with an SVM. The au-thors also suggest to combine modified ISOMAP with TSVMs rather than SVMs, and call it Low Density Separation (LDS). We would like to use the ideas developed in semi-supervised learning for deep learning . Deep learning consists of learning a model with several layers of non-linear mapping. In this article we will consider multi-layer networks with M layers of hidden units that give a C -dimensional output vector: where w O are the weights for the output layer, and typically the k th layer is defined as and S is a non-linear squashing function such as tanh. Here, we describe a standard fully connected multi-layer network but prior knowledge about a particular problem could lead one to other network designs. For example in sequence and image recognition time delay and convolutional networks (TDNNs and CNNs) (Le-Cun et al., 1998) have been very successful. In those approaches one introduces layers that apply convolu-tions on their input which take into account locality information in the data, i.e. they learn features from image patches or windows within a sequence.
 The general method we propose for semi-supervised deep learning is to add a semi-supervised regularizer in deep architectures in one of three different modes, as shown in Figure 1: (a) Add a semi-supervised loss (regularizer) to the su-(b) Regularize the k th hidden layer (7) directly: (c) Create an auxiliary network which shares the first In our experiments we use the loss function (3) for embedding, and the hinge loss Algorithm 1 Embed NN
Input: labeled data ( x i , y i ), i = 1 , . . . , L , unlabeled data x i , i = L + 1 , . . . , U , set of functions f (  X  ), and embedding functions g k (  X  ), see Figure 1 and equa-tions (9), (10) and (11). repeat until stopping criteria is met. for labeled examples, where y ( c ) = 1 if y = c and -1 otherwise. For neighboring points, this is the same regularizer as used in LapSVM and Laplacian Eigen-maps. For non-neighbors, where W ij = 0, this loss  X  X ulls X  points apart, thus inhibiting trivial solutions without requiring difficult constraints such as (2). To achieve an embedding without labeled data the latter is necessary or all examples would collapse to a sin-gle point in the embedding space. We therefore prefer this regularizer to using (1) alone. Pseudocode of our approach is given in Algorithm 1.
 Labeling unlabeled data as neighbors Training neural networks online using stochastic gradient de-scent is fast and can scale to millions of examples. A possible bottleneck with our approach is computation of the matrix W , that is, computing which unlabeled examples are neighbors and have value W ij = 1. Em-bedding algorithms often use k -nearest neighbor for this task, and although many methods for its fast com-putation do exist, this could still be slower than we would like. One possibility is to approximate it with sampling techniques.
 However, there are also many other ways of collecting neighboring unlabeled data, notably if one is given se-quence data such as in audio, video or text problems. For example, one can take images from two consecutive frames of video as a neighboring pair with W ij = 1. Such pairs are likely to have the same label, and are collected cheaply. In Section 5 we apply this kind of idea to text and train a semi-supervised semantic role labeler using an unlabeled set of 631 million words. When do we expect this approach to work? One can see our approach as an instance of multi-task learning (Caruana, 1997) using unsupervised auxiliary tasks. In common with other semi-supervised learn-ing approaches, and indeed other deep learning ap-proaches, we only expect this to work if p ( x ) is useful for the supervised task p ( y | x ), i.e. if the structure as-sumption is true. We believe many natural tasks have this property.
 We note that an alternative multi-task learning scheme is presented in (Ando &amp; Zhang, 2005) and applied to neural networks in (Ahmed et al., 2008) which instead constructs auxiliary supervised tasks from unlabeled data by constructing tasks with labels y  X  . This is use-ful when p ( y  X  | x ) is correlated to p ( y | x ), however an expert must engineer a useful target y  X  . Hinton and coworkers (2006) proposed the Deep Be-lief Net (DBN) which is a multi-layered network first trained as a generative model with unlabeled data be-fore being subsequently trained in supervised mode. It is based around iteratively training Restricted Boltz-mann machines (RBMs) for each layer. An RBM is a two-layer network in which visible, binary stochas-tic pixels v are connected to hidden binary stochastic feature detectors h . The probability assigned to an example x is: E ( x, h ) =  X  X The idea is to obtain large values for the training ex-amples, and small values elsewhere just as in any maxi-mum likelihood density estimator. This is trained with a procedure called contrastive divergence whereby one pushes up the energy on training data and pushes down the energy on samples generated by the model. The authors used this method to pretrain a deep neigh-borhood component analysis model (DBN-NCA) and a regularized version that simultaneously trains an autoencoder (DBN-rNCA) (Salakhutdinov &amp; Hinton, 2007).
 The authors of (Bengio et al., 2007) suggested a sim-pler scheme: define an autoencoder that given an in-put x tries to encode it in a low dimensional space z = f enc ( x ), and then decode it again to reproduce it as well as possible, e.g. so that is small. (Actually you can also view RBMs in this way, see (Ranzato et al., 2007).) The idea is to use an autoencoder as a regularizer which is trained on unlabeled data. If the autoencoder is linear it corre-sponds to PCA (Japkowicz et al., 2000) and hence also MDS, making a clear link to the embedding algorithms we discussed in Section 2.1. The authors claim that autoassociators have the advantage  X  X hat almost any parametrizations of the layers are possible, as long as the training criterion is continuous in the parameters [...] the class of probabilistic models for which [DBNs] can be applied is currently more limited. X  Finally, recently the authors of (Ranzato et al., 2007) introduced another method of deep learning which also amounts to a kind of encoder/decoder architecture, called SESM. In this case they choose to learn large, sparse codes as they believe these are good for classifi-cation. They choose an encoder f enc ( x ) = w &gt; x + b and a decoder with shared weights f dec ( z ) = wS ( z ) + b dec . They then optimize the following loss:  X  || z  X  f enc ( x ) || 2 2 + || x  X  f dec ( z ) || 2 2 +  X  where the first term makes the output of the encoder close to the code z (which is also learnt), the second term makes the decoder try to reproduce the input, and the third and fourth terms sparsify the codes z and the weights of the encoder and decoder w .  X  e ,  X  s and  X  r are all hyperparameters. The training requires an online coordinate descent scheme because both z and w are being optimized.
 We believe all of the methods just described are sig-nificantly more complicated than our approach. Our embedding approach can also be seen as an encoder f enc ( x ) that embeds data into a low dimensional space. However we do not need to decode during training (or indeed at all). Further, if the data is high dimensional and sparse there is a significant speedup from not hav-ing to decode.
 Finally, existing approaches advocate greedy layer-wise training, followed by a  X  X ine-tuning X  step using the supervised signal. The intention is that the un-supervised learning provides a better initialization for supervised learning, and hence a better final local min-imum. Our approach does not use a pre-training step, but instead directly optimizes our new objective func-tion. We advocate that it is the new choice of objective that can provide improved results. We test our approach on several datasets summarized in Table 1.
 Small-scale experiments g50c, Text and Uspst are small-scale datasets often used for semi-supervised learning experiments (Chapelle &amp; Zien, 2005; Sind-hwani et al., 2005; Collobert et al., 2006). We fol-lowed the same experimental setup, averaging results of ten splits of 50 labeled examples where the rest of the data is unlabeled. In these experiments we test the embedding regularizer on the output of a neural net-work (see equation (9) and Figure 1(a)). We define a two-layer neural network (NN) with hu hidden units. We define W so that the 10 nearest neighbors of i have W ij = 1, and W ij = 0 otherwise. We train for 50 epochs of stochastic gradient descent and fixed  X  = 1, but for the first 5 we optimized the supervised tar-get alone (without the embedding regularizer). This gives two free hyperparameters: the number of hidden units hu = { 0 , 5 , 10 , 20 , 30 , 40 , 50 } and the learning We report the optimum choices of these values opti-mized by 5-fold cross validation and by optimizing on the test set in Table 2. Note the datasets are very small, so cross validation is unreliable. Several meth-ods from the literature optimized their hyperparam-eters using the test set (those that are not marked with ( cv )). Our Embed NN is competitive with state-of-the-art semi-supervised methods based on SVMs, even outperforming them in some cases.
 MNIST experiments We compare our method in all three different modes (Figure 1) with conventional semi-supervised learning (TSVM) using the same data split and validation set as in (Collobert et al., 2006). We also compare to several deep learning methods: RBMs, SESM and DBN-NCA and DBN-rNCA (how-ever, they are trained on a different data split). In these experiments we consider 2-layer networks (NN) and 6-layer convolutional neural nets (CNN) for em-bedding. We optimize the parameters of NN ( hu = { 50 , 100 , 150 , 200 , 400 } hidden units and learning rates as before) on the validation set. The CNN architecture is fixed: 5 layers of image patch-type convolutions, fol-lowed by a linear layer of 50 hidden units, similar to (LeCun et al., 1998). The results given in Table 3 show the effectiveness of embedding in all three modes, with both NNs and CNNs.
 Deeper MNIST experiments We then conducted a similar set of experiments but with very deep archi-tectures  X  up to 15 layers, where each hidden layer has 50 hidden units. Using Mnist1h, we first compare conventional NNs to Embed ALL NN where we learn an auxiliary nonlinear embedding (50 hidden units and a 10 dimensional embedding space) on each layer, as well as Embed O NN where we only embed the outputs. Results are given in Table 4. When we increase the number of layers, NNs trained with conventional back-propagation overfit and yield steadily worse test er-ror (although they are easily capable of achieving zero training error). In contrast, Embed ALL NN improves with increasing depth due to the semi-supervised  X  X eg-ularization X . Embedding on all layers of the network has made deep learning possible. Embed O NN (embed-ding on the outputs) also helps, but not as much. We also conducted some experiments using the full MNIST dataset, Mnist60k. Again using deep networks of up to 15 layers using either 50 or 100 hidden units Embed ALL NN outperforms standard NN. Results are given in Table 5. Increasing the number of hidden units is likely to improve these results further, e.g. us-ing 4 layers and 500 hidden units on each layer, one obtains 1.27% using Embed ALL NN.
 Semantic Role Labeling The goal of semantic role labeling (SRL) is, given a sentence and a relation of interest, to label each word with one of 16 tags that indicate that word X  X  semantic role with respect to the action of the relation. For example the sentence  X  X he cat eats the fish in the pond X  is labeled in the following in ARG0 and ARG1 effectively indicate the subject and object of the relation  X  X ats X  and ARGM-LOC indi-cates a locational modifier. The PropBank dataset includes around 1 million labeled words from the Wall Street Journal. We follow the experimental setup of (Collobert &amp; Weston, 2007) and train a 5-layer con-volutional neural network for this task, where the first layer represents the input sentence words as 50-dimensional vectors. Unlike (Collobert &amp; Weston, 2007), we do not give any prior knowledge to our classi-fier. In that work words were stemmed and clustered using their parts-of-speech. Our classifier is trained using only the original input words.
 We attempt to improve this system by, as before, learning an auxiliary embedding task. Our embedding is learnt using unlabeled sentences from the Wikipedia web site, consisting of 631 million words in total using the scheme described in Section 3. The same lookup table of word vectors as in the supervised task is used as input to an 11 word window around a given word, yielding 550 features. Then a linear layer projects these features into a 100 dimensional embedding space. All windows of text from Wikipedia are considered neighbors, and non-neighbors are constructed by re-placing the middle word in a sentence window with a random word. Our lookup table indexes the most frequently used 30,000 words, and all other words are assigned index 30,001.
 The results in Table 6 indicate a clear improvement when learning an auxiliary embedding. ASSERT (Pradhan et al., 2004) is an SVM parser-based sys-tem with many hand-coded features, and SENNA is a NN which uses part-of-speech information to build its word vectors. In contrast, our system is the only state-of-the-art method that does not use prior knowledge in the form of features derived from parts-of-speech or parse tree data. This application will be described in more detail in a forthcoming paper. In this work, we showed how one can improve su-pervised learning for deep architectures if one jointly learns an embedding task using unlabeled data. Our results both confirm previous findings and generalize them. Researchers using shallow architectures already showed two ways of using embedding to improve gen-eralization: (i) embedding unlabeled data as a sepa-rate pre-processing step (i.e., first layer training) and; (ii) using embedding as a regularizer (i.e., at the out-put layer). More importantly, we generalized these ap-proaches to the case where we train a semi-supervised embedding jointly with a supervised deep multi-layer architecture on any (or all) layers of the network, and showed this can bring real benefits in complex tasks.
