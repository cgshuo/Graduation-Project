 Implicit user feedback, including click-through and subse quent brows-ing behavior, is crucial for evaluating and improving the qu ality of results returned by search engines. Several recent studies [1, 2, 3, 13, 25] have used post-result browsing behavior including t he sites visited, the number of clicks, and the dwell time on site in or der to improve the ranking of search results. In this paper, we first study user behavior on sponsored search results (i.e., the advert isements displayed by search engines next to the organic results), an d compare this behavior to that of organic results. Second, to exploit post-result user behavior for better ranking of sponsored results, we fo cus on identifying patterns in user behavior and predict expected on-site ac-tions in future instances. In particular, we show how post-r esult be-havior depends on various properties of the queries, advert isement, sites, and users, and build a classifier using properties suc h as these to predict certain aspects of the user behavior. Additional ly, we de-velop a generative model to mimic trends in observed user act iv-ity using a mixture of pareto distributions. We conduct expe riments based on billions of real navigation trails collected by a ma jor search engine X  X  browser toolbar.
 Categories and Subject Descriptors: H.4.m [Information Systems Applications]: General General Terms: Experimentation, Measurement Keywords: implicit feedback, user behavior, sponsored search
Recently, a great deal of effort in the research community ha s fo-cused on improving user experience in web search through the incor-poration of implicit user feedback [2, 13]. This feedback in cludes click-through behavior, dwell times on sites visited from q uery re-sults, and other navigational behavior by search engine use rs. In general, implicit feedback provides valuable information about user satisfaction on web search engines.

While most prior work on implicit feedback has centered on or -ganic search, we focus here on user navigation behavior asso ciated with sponsored search. To our knowledge, this is the first wor k to focus on user behavior within sponsored search advertiseme nts  X  ads which are displayed by search engines next to convention al or-ganic search results. Revenue from these sponsored results provide much of the economic foundation of modern web search engines . However, sponsored search differs significantly from organ ic search in several important ways: First, while organic search is fo cused on satisfying users by addressing search queries, sponsored s earch has to optimize for ad revenue while accounting for user satisfa ction and the constraints and objectives of advertisers. Second, the ranking of ads in sponsored search differs from that of organic search. Specif-ically, sponsored search ranking relies heavily on feature s such as predicted click-through rates and advertisement bid amoun ts, and less on hyperlink and anchortext information, which are fre quently unavailable for short-lived ads. Third, click-through rat es in spon-sored search tend to be lower than for organic results, sugge sting users might interact differently with these results than wi th organic results [8].

Our experiments rely on user browsing behavior collected fr om a navigational toolbar plug-in issued by a major search engin e, com-prising several million users who opted to share their brows ing data. From this data, we use anonymized information about the quer ies submitted to search engines, the organic and sponsored resu lts clicked on by users, and their subsequent behavior (including URLs a nd associated dwell times) on the visited sites. Analysis of th is data leads to interesting observations. Perhaps the most compel ling one is that in expectation, the CTR of ads does not have a strong co r-relation with what happens afterwards, in terms of click-ac tivity or time spent. One possibly explanation for this counter-intu itive re-sult is the proliferation of deceptive textual ad-snippets designed to entice users to visit sites, resulting in a high CTR but faili ng to ef-fectively engage users once they visit. This hypothesis sug gests that optimizing ad placement for high CTR does not necessarily im ply the best user experience as is often assumed in the sponsored search community.

Above findings motivate the incorporation of implicit feedb ack, e.g., number of subsequent clicks on the site, and dwell time , into the ranking of advertisements. While many techniques have b een proposed recently on how to best utilize implicit feedback i n search [1, 2, 3, 5, 18, 19, 23, 25], the effectiveness of such proposa ls is often constrained by the limited availability of feedback d ata. For small search engines which do not have toolbar-like product s, it is nearly impossible to gather a large amount of user behaviora l infor-mation. Even major search engines can gather only a fraction of the data associated with a small subset of users. To address t his is-sue, we demonstrate how user visit behavior on result sites d epends on various properties of the queries, ads, sites, and users a ssociated with that visit. We then go on to use such properties to predic t cer-tain aspects of the user behavior. Formally, given a query, r esult, and user, we are interested in modeling and predicting the post-result user behavior, in terms of the number of additional clicks ma de and the amount of time spent by users on the result. This predicti on of seemingly complex user behavior is made even more difficult w hen one considers the proliferation of missing data that likely results in such a real-world classification task. We develop a robust me thod for predicting user behavior, which we evaluate thoroughly on d ifferent scenarios of information availability.

Furthermore, we present a generative model based on mixture of pareto distributions to describe user behavior. There are a large n um-ber of variables influencing the navigation of a user on query results. Different queries induce different probabilistic constra ints on user navigation based on that query X  X  intent. For instance, broa d infor-mational queries (e.g., camera shopping) require more brow sing by users relative to more focused queries (e.g., finding a speci fic publi-cation). By accounting for the varieties of behavior presen t in user activity, our model provides a better fit to observed data tha n the pre-vious model in [14]. Overall, this paper makes the following contri-butions: The rest of the paper is organized as follows. Section 2 discu sses relevant previous research. Section 3 describes our data an d exper-imental setup. Section 4 analyses user behavior on sites and com-pares behavior on organic and sponsored results. Section 5 e xplains our generative model for user behavior, and Section 6 provid es a closer analysis of user behavior by controlling for several important factors. Our results on predicting of user behavior are pres ented in Section 7, and Section 8 provides some concluding remarks.
Sponsored search: There has recently been a large amount of re-search on sponsored search, i.e., how to best select ads to di splay next to search results, which is an important part of the emer ging area of Computational Advertising [9]. While click-through behav-ior (and in particular CTR) is known to be an important factor in ranking sponsored search ads, we are not aware of any detaile d stud-ies of post-click behavior , knowledge useful for both advertiser and user satisfaction.

Using Implicit Feedback in Search: Several recent studies have focused on how implicit measures can be utilized to improve W eb search [2, 13, 16, 22]. In [2] it was found that implicit feedb ack can improve the accuracy of a competitive search ranking algori thm by almost 31%. Various methods have been proposed for how to inc or-porate implicit measures into ranking. For instance, there is work on how to interpret click-through data accurately [15, 16], id entify rel-evant websites using past user activity [1, 3, 5, 24], and ran k pages based on user feedback [18, 19, 21]. Our work differs from the above in that we focus on sponsored search. Additionally, we not on ly an-alyze implicit feedback, but detail a prediction mechanism for fore-casting user behavior in previously unseen scenarios. Whil e [8] pro-vides some notion of user behavior in the context of sponsore d query results, it is with the intention of expanding textual simil arity, a task sufficiently different from that tackled here.

Modeling User Behavior: Another line of work has focused on modeling user behavior [4, 10, 14]. We show that while our dat a conforms to a power law as in [14], the exponent of the distrib ution best fitting our data is substantially different from that pr edicted in prior work. While our study is based on search-induced behav ior, [14] studied trails created from more undirected browsing. The work presented here adopts a novel pareto mixture-model based on query information need that is able to accurately fit observed user behavior.
In the past several years, browser enhancing plug-ins have s een wide-spread acceptance. These plug-ins are third party pro grams which modify the browser software to provide additional fun ction-ality when navigating the web. One particularly popular typ e of browser plug-in are search toolbars , which embed a search inter-face into the web browser. These toolbars typically send bac k to the engine various information about the user X  X  navigation al behav-ior, given the user X  X  consent, and this information is used b y the en-gines to constantly improve the quality of their search serv ices, and in some cases also to personalize the results for the particu lar user.
The data used in our experiment was collected between Januar y and July 2008, and represents a sample of users of a major sear ch en-gine X  X  search toolbar who opted into sharing their data. Thi s sample contains roughly 4 million anonymous users, as identified by their associated browser cookies, and billions of individual pag e requests. Following the technique described by White and Drucker [25] , we segment user navigation into post-query trails, i.e., the s equence of pages viewed as the direct consequence of following a query r esult. When creating these post-query trails, we introduce an addi tional criterion which terminates a trail upon navigation to a site other than that of the clicked query result as in [14]. Thus, we focu s on the query results themselves and the implicit feedback that can be gleaned from user behavior on the corresponding sites only.
By monitoring the trails induced by post-result user naviga tion, we are able to compile a detailed understanding of how users i nter-act with the sites that are offered to them. The focus of this s ection is to present trends in user browsing and information seekin g behav-ior, and to offer some intuitive explanations of our findings when possible.

Many attributes could be collected to provide a quantitativ e sum-mary of user behavior on query results. Following prior work in relevance feedback and user navigation, for each trail (ori ginating from a search result page) we focus on: (a) the number of click s the user makes on the trail ( trail length ), and (b) the total time spent in the trail ( trail duration ). These two numbers provide a useful synopsis of user navigation behavior. Additionally, to des cribe how individual sites are navigated in aggregate, we use the Shan non en-tropy describing the various trails that users take in the si te (more details in Section 4.2). Using these simple features, we can obtain useful insights into the ways users interact with query resu lts. While we will consider both organic and sponsored search results, we are particularly interested in the case of sponsored search res ults, which has been studied much less by previous work.

In Sections 4.1 and 4.2, we present and discuss the observed d is-tributions for trail length, trail duration, and entropy ta ken from the accumulated organic and sponsored results. Section 4.3 stu dies the interdependence between these observed variables.
After partitioning our data into query trails, we investiga te the dis-tributions of the number of clicks made in a trail after landi ng on a query result (trail length), and the total time spent in the t rail after landing on the result (trail duration). Figure 1(a) present s the dis-tribution of trail lengths; not surprisingly most trails ar e very short, and in fact more than 70% of trails involve no additional click after the click-through on the search result. Excluding the initi al click-through, the average number of clicks per trail is 0 . 39 for spon-sored results and 0 . 25 for organic results. Thus, while sponsored results tend to have lower click-through rates than organic results (not shown here), once users clicks on sponsored results the y are, on average, more active.

Figure 1(b) displays the distribution of trail durations. W hile spon-sored search results tend to lead to more time spent on result s than organic results  X  82 s versus 75 s in expectation, we note that spon-sored results are also much more likely to result in visits of &lt; 20 s than organic results; i.e., users frequently click sponsor ed results and then leave almost immediately. This may be an indicator of th e occa-sionally deceptive nature of the textual snippets designed by adver-tisers to be presented to users for sponsored search results . Adver-tisers realize that increased traffic to their site maximize s the num-ber of potential customers and have developed expertise at e ngineer-ing snippets to optimize click-through rate, possibly dece iving some users into thinking the resulting site will suit their needs . (In con-trast, snippets for organic results are created not by the si te owner, but automatically by the engine.)
We note that it is difficult to directly compare the expected b e-havior of users on organic and sponsored search results. Any given cause for initiating a web search may result in clicks on orga nic re-sults, while only a subset of these scenarios tend to find user s pur-suing sponsored results. Thus, a query  X  X ar insurance X  has a rea-sonable chance of a sponsored result being clicked, however , for a query such as, e.g.,  X  X ean of beta distribution X , it is much l ess likely that a sponsored result would be clicked or even offered. Con sid-ering that contrasting these data sets is difficult, we obser ve that the overall trend seems to be of similar overall shape; it is like ly that sim-ilar processes underlie both organic and sponsored behavio r, perhaps with different parameters or initial conditions. In the fol lowing, we focus primarily on the latter of these two, sponsored search , a subset of user behavior largely overlooked by prior work on user beh avior.
The number of clicks alone is a rather course-grained featur e for describing user behavior and engagement. Within any given s ite, there are likely many trails of a given click-length along wh ich a user can navigate, with associated meanings for user and site own er. Five clicks on a trail describing how to file a complaint can be inte rpreted very differently than five clicks browsing and purchasing pr oducts on a retail site. The infinite possibilities of the web makes a la rge scale analysis of fine-grained user surfing behavior across many di fferent sites difficult. However, we still seek to understand in a bro ad sense how users navigate on a query result site. One possible metri c for this is the Shannon entropy H of the navigational history on a site trail in S , and p  X  is the observed probability of a user taking that trail. This roughly translates to the number of bits needed t o describe which trail a user has taken. Sites with a large entropy tend t o see a wide variety of trails taken by users, while those sites wit h a very low H have most users take one of a few different trials, either by choice or due to site structure.

For both ads and non-ad results, we collect all sites receivi ng at least 50 visits and calculate the entropy as above. Figure 2 presents the distribution of relative site frequency of observed ent ropy values for organic and sponsored results. We notice that sites resu lting from sponsored search results tend to have higher entropy than or ganic result sites. This is especially significant since there is a far greater number of organic observations, and entropy is dependent on the number of observations. Alternately, the longer expected t rail length of sponsored results may contribute to the difference in ent ropies.
From the observed difference in entropy, one could conjectu re dif-ferences in site function and associated query type (naviga tional, in-formational, transactional) between sponsored and organi c results. For example, many advertisements are placed by large retail sites, where users can browse for many different products related t o their interests. On the other hand, many organic sites are clicked in re-sponse to a simple, direct question, where only a single clic k is needed to satisfy the user.

The number of clicks made and the time spent on a trail are high ly correlated, as would be expected; it takes a certain amount o f time for users to make successive clicks. More interestingly, we observe that the probability of a user making an additional click in t he trail seems to be dependent on the time spent on the current page. Fi gure 3 shows the strong correspondence between the likelihood of making another click on the site ( P ( nextclick ) ) and the dwell time ( t ) on the current page. We note that as the dwell time t increases, the probability of making the next click P ( nextclick ) also increases, for both sponsored and organic results.
 Figure 3: Probability of Another Click vs. Time Spent on Page
To study this further, we look at how P ( nextclick | time  X  y ) depends on the number of prior clicks made in the trail, i.e., P ( nextclick | time  X  y, totclick = x ) . In Figure 4, we plot the Figure 4: Probability of Subsequent Clicks Depending on Num -ber of Previous Clicks and Time Spent on Current Page for Sponsored Results probability of a user making successive clicks conditioned on the number of prior clicks as well as the time spent on the current page. Note that the probability of making another click increases with both the number of previous clicks on the site and the time spent on the current page. One possible explanation is that of increasing user engagement , i.e., the probability of making additional clicks depends on some measure of the user X  X  engagement with the site. Incre ased on-site activity in the form of clicks or time spent may indic ate a greater chance of the user becoming engaged. Visits on query results constitute a diverse set of navigati onal trails. We observe that the distribution or trail lengths closely re sembles a power law, i.e., the probability of observing a trail of leng th x , P ( x ) , is proportional to x  X   X  where  X  is the scaling exponent of the power law. This observation is in accordance with the study by Hube rman et al. in [14]. In Figure 5 we plot the observed distribution o n the log-log scale. In this scale, it is evident that the distribu tion adheres to a straight line (whose slope is roughly equal to 3 ).

While both our study and [14] obtain a power law, we note that t he exponent of our power law differs significantly from the expo nent of 1 . 5 that was observed and theoretically derived in [14]. Figure 5 shows the power law with exponent 1 . 5 along with the observed dis-tribution. Also shown in the figure is the distribution predi cted by our model (discussed below), which provides a very good fit to the observed distribution.

Before delving into the details of our proposed model, we not e the reasons for which we believe our observed power law diffe rs from the one observed and predicted in [14]. First, the natur e of the web has changed dramatically since the study in [14]. Sec ond, our analysis focuses solely on trails originating from quer y result pages, while Huberman et al. focus on trails obtained from ge n-eral browsing. Search-induced trails are likely to be short er than random-surfing trails for two reasons: (i) typically, searc hers seek specific information and when they find what they are looking f or, they quickly end their trails, moving on to the next task in ha nd, while in [14], the assumption is that users continue browsin g until the benefit (enjoyment) of the pages encountered becomes les s than the  X  X ost X  of browsing, and (ii) in the case when users do not fi nd the desired information following a search result, it is lik ely they go back to a different search result or reformulate the query to start a new trail.
A cursory look at our data reveals that fitting one power law ov er the surfing behavior aggregated over millions of queries is i nade-quate, as it greatly oversimplifies complex human behavior. Intu-itively, different queries have different information nee ds, thereby inducing very different types of click behavior. For instan ce, queries related to shopping entail much more browsing on behalf of us ers than queries on more focused tasks, e.g., finding a specific bo ok or publication. To account for this diversity, we now propose a mixture model of user behavior based on queries 1 .

Instead of one underlying power law, we assume that there is a mixture of power law distributions generating user behavio r. In par-ticular, our model consists of C clusters of queries. Each cluster, c , has its own discrete power law distribution with unknown pa ram-eter  X  c . A particular c models a set of queries that have a certain information need and possess a characteristic click behavi or best fit by  X  c . Under the discrete power law, a user makes x clicks fol-lowing a search result with probability P ( x |  X  c ) = f ( x,  X  imply long user trails (e.g., broad queries requiring some a mount of browsing), while small values of  X  c imply short trails (e.g., more fo-cused queries). The prior probability of a cluster to contai n a query is  X  c , where P C c =1  X  c = 1 . We face the problem of estimating these unknown parameters of our model, denoted by  X  = {  X  c ,  X 
From the data we construct a vector for each query q where q ( x ) denotes the number of trails of length x originated from q . Given this query vector, we can assign query q to the above mentioned clusters. We denote the probability that query q belongs cluster c by  X  Assuming that all visits to a query are drawn i.i.d. in accord ance to that query X  X  parameters, we can calculate  X  q,c as:
Applying the law of total probability, we normalize  X  q,c P c =1  X  q,c = 1 . Thus, the log-likelihood of the entire query data Q , given unknown  X  , is: ln ` P ( Q |  X  )  X  = X
To optimize log-likelihood over the unknown parameters (  X   X  X ,  X   X  X ), we use the Expectation-Maximization (EM) algorithm [12]. I n this paradigm, we iteratively improve our estimates on  X  and  X  , as shown in Algorithm 1.
The model can be generalized to accomodate the influence of us ers and pages as well. Algorithm 1 Exp. Maximization for Power Law Mixture Model while convergence condition not met do end while Figure 6: Trail distributions of the 5 clusters derived by EM .
We ran the EM alogorithm with C = 5 clusters on 2 million trails from the sponsored search data set. Figure 6 shows the trail l ength distribution associated with each cluster. (Each cluster c ontains at least 5% of the queries.) From the scaling exponents, it is evident that these clusters differ significantly from each other. Th is validates the hypothesis that different queries have different infor mation intent and lead to vastly different user behavior. (The overall fit o f our mixture model to the observed power law is shown in Figure 5.)
In the previous sections, we showed general patterns of user be-havior in terms of number of clicks and time spent, and discus sed possible models for this behavior. Of course, query topic, q uery intent, time of day, specialization, domain knowledge, the user X  X  dis-position, and countless other ingredients can contribute t o how a user behaves after clicking on a query result. While considering each of these facets is impossible, in this section we perform a more detailed analysis by separately controlling for a few of the more inte resting of these factors. In particular, we look at click-through ra te, query topic, and several other properties of queries.
In the sponsored search community it is widely assumed that t he ads with the highest CTR are the  X  X est X  ads  X  the high proporti on of clicks has been interpreted as a testament to the site X  X  qu ality and relevance to the user X  X  needs [20]. It is unknown, however, i f an in-creased click-through rate for a particular ad or site trans lates to more on-site activity per visit. This is an important considerat ion for ad-vertisers and search engines; the results that users tend to click most frequently may not actually be the most useful results to the user, or the result leading to the most on-site activity or even purch ases. Figure 7: Expected On-Site Clicks Vs. Site Click-Through Ra te To investigate the presence of such a correlation, we compar e the CTR of sites with the expected trail length and duration of vi sits to those sites, for sponsored and organic results. We compute t he CTR of a site by dividing the number of times the site was clicked b y the number of impressions for the site. 2 Figure 7 shows the mean trail lengths, depending on the site X  X  CTR 3 . As shown in Figure 7, there is no obvious relation between the level of click-base d activ-ity and the CTR; web surfers do not seem to browse more on sites with a higher CTR. This appears to be true for both sponsored a nd organic results, with sponsored results resulting in sligh tly more ac-tivity across the range of CTRs. Measurements of query durat ion as influence by CTR appear very similar to Figure 7, showing very little if any noticeable correlation between CTR and trail duratio n. This is an interesting and somewhat unexpected result: While opt imizing result placement based on CTR may optimize the payments to th e search engine, from our data it seems that a higher CTR does no t always lead to more activity on the site per visit. This pheno menon could possibly be explained by the proliferation of decepti ve textual ad-snippets designed to entice users to visit sites, result ing in a high CTR but ineffective engagement of users once they visit.
Previously, we have speculated that query topic likely influ ences the behavior of users on the site visited from the result page . In order to show the impact of query topic on site activity, we to ok the entire set of query trails culminating from sponsored searc h results and identified a query topic according to a proprietary, ad-c entric topical taxonomy, using a specialized classifier. Then, wit hin each topic, we calculated the average trail length and trail dura tion after landing on the result site. The results are shown in Figure 8 p resents this comparison. (Note that each topic had a significant numb er of instances, and that overall data is roughly balanced across topics.)
As conjectured, the amount of activity does indeed vary acro ss the topics we have considered, both for trail length and duratio n. This has possible implications for approaches that exploit user behavior to improve search results, in that one great care must be exer cised when making comparisons across topic boundaries.

Second, and perhaps more interestingly, increased click ac tivity in a topic is not necessarily associated with increased time spent. Note that queries in the Travel category tend to lead to significant on-site activity in terms of both number of clicks and time sp ent, while for Finance we have fairly small trail lengths but very long trail durations.
While the clickability and thus CTR of an ad/site depends on m any factors, including position on the result page, here we look at this simple definition of CTR, leaving more detailed analysis for future work.
Here we present relative CTRs to preserve proprietary infor mation. Figure 8: Distribution of Trail Lengths and Trail Durations Ac-cording to Query Topic
So far, we have considered the influence of CTR and query topic on on-site activity. Next, we look at the impact of other fact ors, that, in the past, have often been associated with user brows ing ac-tivity or page quality: (i) The ordering of the results click ed for the query. (ii) The number of query terms. (iii) The navigationa l vs. informational nature of the query. (iv) The PageRank of the c licked search results [6]. (v) The overall frequency of the query in our data set. These features were selected due to their intuitive infl uence on a user X  X  browsing behavior or their use in prior work.

To demonstrate the impact that these facets may have on user browsing, we segregate data into sets according to the value of that facet. In order to maximize the illustrative ability permis sible by such an arrangement, we partition data according to the boun daries made by a decision tree attempting to predict onsite activit y based solely on the feature in question. After partitioning the da ta, we cal-culate the trail length for each bin. Due to lack of space, we d o not prevent similar results for trail duration. The result of th ese experi-ments can be seen in Figure 9. The y-axis is the average trail l ength per bin (  X  ). Below we explain these features and associated obser-vations in detail.
Ordering of Results Clicked for a Query: Examining Figure 9, we notice that users tend to be more active on the first search r esult visited during a multi-result query session. In other words , trails originating from the first result that is clicked tend to be lo nger. This might be due to a loss of patience as the session carries on, or due to the fact that users first click on higher ranked and thus pos sibly better results. Additionally, sessions with many results c licked may simply not have any good results, therefore the need for many results to be visited. We note in this context that there is a known bia s for users to click on the first result on a search result page, even if that result is worse than a lower-ranked result on the page; it is p ossible that this bias carries over to subsequent clicks on the site.
Number of Query Terms: Contrary to our initial assumptions, the amount of on-site surfing decreases as the number of terms in the query grows. Our expectation was that longer queries ten d to be more specific than their shorter counterparts thereby lea ding to greater interest once a result had been chosen. There are sev eral pos-sible explanations. First, longer queries may lead more dir ectly to the page that the user really wants, making additional click s unnec-essary. Second, long queries are often difficult to answer fo r search engines and may thus give worse results.

Navigational vs. Informational Nature of the Query: Broder [7] proposed a taxonomy of query goals into three main catego ries: navigational, informational, and transactional queries. We expect the query goal would influence the manner in which users navigate the results presented to them. Rather than follow the detailed t echniques described in prior work (e.g., [17]) to automatically ident ify the cat-egory of a query, we use the simpler idea (also described in [1 7]) that clicks on navigational queries typically focus on one or onl y a few re-sults (e.g., most clicks for the query  X  X yspace X  go to myspac e.com), while the other two categories of queries may see a variety of results clicked by different users. Thus, we use the variance in the s elected results for a query as a proxy for how navigational a query may be; queries with low variance see one of a few results chosen in mo st cases, and are therefore more likely to be navigational.

Looking at the result in Figure 9, we see that non-navigation al queries tend to lead to more click activity than navigationa l que-ries. Non-navigational queries are often more exploratory in nature; a user may not know exactly what is wanted or may require some orientation when searching for the correct information [23 ]. Non-navigational results that are product-oriented may also le ad to brows-ing to facilitate comparison shopping: a user searching for  X  X ard drive X  is likely to examine similar products before making a final decision.

Pagerank of Clicked Search Results : PageRank is an enormously influential static ranking of page importance based on the hy perlink structure of the web, and has occasionally been considered a proxy for page quality [6]. To study the impact of Pagerank on site a ctiv-ity, we computed Pagerank values for a site-based web graph. As we see, Pagerank seems to have fairly little impact on the sur fing behavior of users when visiting a site via query results. Fig ure 9 shows the weak correlation between PageRank and click-acti vity. In fact, search results with the lowest values of PageRank seem to have slightly more activity than results with moderate PageRank . Results with low PageRank may be sufficiently specialized to prevent the wide-spread attention necessary for a high PageRank, howev er, this specialization may attract a lot of interest from users visi ting that site.

Query Frequency : On the other hand, query frequency seems to have a substantial impact on the amount of on-site activity, with the most frequent queries resulting in many more clicks than inf requent queries. We conjecture that more common queries lead to bett er results, which in turn result in more activity [1].
In the previous sections, we examined toolbar logs to study u ser behavior, characterizing user behavior in terms of post-re sult click activity, time spent, and entropy. Metrics such as these cou ld be leveraged to improve ranking of sponsored and organic resul ts, as illustrated in a rich set of prior work in IR [2, 5, 18, 19, 23]. Fur-thermore, our study revealed that these metrics may be even m ore important given the fact that neither CTR nor PageRank provi de a strong signal for how users behave after a result is clicked.
However, a major hurdle in leveraging user behavior for rank ing is its limited availability. Given the vast size of the inter net, the data available to any one party is likely to cover only a small frac tion of the trail activity that takes place on the Web. To address thi s sparsity, next we look at the problem of predicting user behavior: give n a user, query, and result, we want to predict the activity on th e trail originating from this result, conditioned on the fact that t he result is clicked 4 .

Predictions about on-site user activity are made using stan dard sta-tistical tools, incorporating features extracted from pas t user brows-ing behavior, the various query features from Section 6, and data about how users navigate on individual landing pages and the ir as-sociated sites. We show that some measures of behavior can be pre-dicted reliably when all three entities involved in the give n triplet (i.e., user, query, result) have been previously seen in the training data. Also, we show that the classification can be made robust to the case where some of these entities are new, i.e., unseen in the train-ing data. These results have two implications: While predic tion on known entities shows that user behavior tends to be consiste nt (i.e., past behavior predicts future behavior), prediction on new entities demonstrates generalizability of the prediction task.

Since our emphasis is on understanding user engagement in te rms of the number of clicks and amount of time spent on-site, we no w demonstrate that we can predict with sufficient reliability whether or not a user will spend more than a certain amount (in our case , 60 seconds) on a result and whether that user will make one or two additional clicks on that site. We believe the ability to pre dict these simple metrics implies that more general forecasts can be ma de, a task which we leave to future work. In the remainder of this se ction we present the set of features used when making these predict ions, and discuss issues associated with their collection. We the n present our results for predicting user activity. Finally, in order to simulate a more realistic setting and to evaluate the influence of certa in features, we perform predictions with intentionally excluded featur e sets. The features in our experiments are divided into four entiti es: User Features: Observations compiled on a specific user X  X  query Query Features: How clicked results are distributed, and how re-Site Features: Navigational features aggregated across all landing Note that the probability of a result to get clicked is its CTR value. CTR prediction has been studied before, and we focus on the or thog-onal problem of predicting activity after the result is clic ked.
Future work could follow previous work in [5] and use individ ual query terms (rather than complete queries) to alleviate spa rseness in the data.
 Page Features: Features pertaining to individual landing pages. While The specific nature of each feature is described below. Some o f these features are accumulated for all four of the above entities, while oth-ers are specific to one or two. This list of features is by no mea ns exhaustive, and additional features may lead to further gai ns in pre-diction.
 Click Probabilities: The probability of making i  X  [1 , 5] clicks, Distribution of Times: The amount of time spent on the page re-Navigational Shannon Entropy: The number of bits needed to en-Query Intent: Rather than perform more advanced computation in User Activity: The number of queries issued by a user, the aver-Activity on Queries: The frequency, click rate, ad click rate, and Query Topic: The topic of a particular query, as determined by our Click-Through Rate A simple estimation of click-through rate for
Our data consists of more than 2 million instances, where eac h in-stance consists of a triple &lt; user, query, result URL &gt; and the result-ing click trail on the site. Since many features listed above require aggregations over many instances (e.g., click probabiliti es), we need to be careful during the feature extraction process to ensur e that the information from the test set is not  X  X eaked X  to the classifie r. More specifically, we perform the aggregation and agglomeration of fea-tures as follows: our instances are partitioned into two equ al sized sets. One of these two sets is used to compile features which r equire Table 1: Predictive Performance on the Various Data Sets Use d for Ad and Non-Ad Data. cross-instance aggregation. We call this set the training set and the other set the test set .

From the training set, a classifier is trained. While evaluat ing on the test set, given a test instance we probe into our training set to check whichever features are available for the entities inv olved in the test instance. For instance, if the user involved in the t est in-stance is entirely new, then we may not get any user-centric f eatures for him/her from our training set. The same is not true for que -ries though, since features like query topic or query intent can be extracted even if the query is new. The classifer then takes t hese fea-tures to predict user behavior on the test instance. This eva luation process ensures that no information from the test set is disc losed to the classifier.

While it is impossible to make meaningful predictions in a co m-plete absence of features, we show that even a few features ar e suffi-cient to provide reasonable classifier performance. In orde r to eval-uate the predictive response to missing features, two data s ets are compiled for both organic and sponsored search results. (Bo th data sets consist of training and test data, in order to ensure fai r evalua-tions, as described above). All classification tasks in this section were performed usin g cost-sensitive two-class logistic regression with ridge regula rization. The vector, w T is optimized using Newton X  X  Method. Parameters and weights used for each classification sub-problem were hand t uned by compiling a large parameter set, performing model traini ng and test evaluation on each parameter. The configuration offeri ng the best test performance is retained. To account for imbalance s in the size of positive to negative instances in each of our experim ents, we use the area under the receiver operating characteristic cu rve (AUC) as our metric for evaluating predictive ability.
Initial Results: Classifier performance on each of these data sets is presented in Table 1. The first experiment we evaluate is th e bi-nary classification task deciding whether or not a user will m ake one additional click (column titled  X  X lick AUC X ), or if a user wi ll spend at least one minute ( X  X ime AUC X  column), on a particular resu lt site given that the result was clicked. Both data sets are used to s how how well we can do with the Top 100 k data set as well as a more natural setting of the Random 100 k data. We conduct these experiments on both organic and sponsored search data.

From Table 1, we see that in both the ad and non-ad cases, the To p 100 k data set offers an improvement in classification performanc e over the Random 100 k data set. This result is unsurprising since the former data set consists of instances where the most informa tion is available, while the latter follows the natural, often spar se distribu-tion of feature availability. We note that while Random 100 k offers degraded classification performance, we believe the result s are still acceptable. With more optimized feature extraction user be havior can be conjectured with sufficient reliability in the wild.
Of interest is the improved predictive ability on the ad data set in comparison to the non-ad data. This is surprising, since o ur data set contained more non-ad data, by an order of magnitude, imp lying denser feature availability. We believe this can be attribu ted to dif-ferent user expectations when clicking on ads as opposed to n on-ads: There are many reasons why a user may click on an organic resul t presented by a search engine, but only a small subset of these tend to lead to clicks on sponsored search results.

Predicting Two Clicks: While predicting a single click on a query result is a challenging task, we seek to discover whether fur ther ac-tivity can be forecasted. As a simple test, we take query resu lts from the ad and non-ad Top 100 k data sets, and predict if a user will go to make two clicks on a particular result. Table 2 presents th e results of this experiment. We see that performance is comparable to that achieved in predicting a single click. This may indicate tha t most of the uncertainty involved in predicting user behavior occ urs at the first step. For future work, we would like to perform a much mor e detailed prediction of user activity.

Ablative Feature Experiments: Missing some features is com-mon in our data. In order to understand the reduction in class ifier performance from missing a particular feature set, we evalu ate the performance of our classifier in the presence of all but a given en-tity of features. This ablative feature classification is pe rformed by removing certain features from the training and test data, a nd using logistic regression as before. Top 100 k data sets are used to ensure that the feature removed was present in sufficient numbers in the ini-tial data set, and while only results from our sponsored sear ch data set are used due to space constraints, the results are simila r for non-ad data. The results of this study are presented in Table 3. Table 3: Accuracy Results In an Ablative Feature Comparison for Ads on the Top 100 k Set
Single Feature Experiments: Table 3 reveals the dependency of our classifier on site and URL specific information; missing e ither of these feature sets severely restricts the ability of our c lassifier, while user and query can be removed with little consequence o n the output. At this point, it is unclear if users or queries are re ally use-less in determining on-site behavior, or if this informatio n is largely subsumed once site and URL information is known. We conduct a single feature classification experiment on our ad Top 100 k data in order to understand exactly how much information related to each feature set helps in prediction. This is done by filtering all but a par-ticular feature entity, then classifying as above; the resu lts are shown in Table 4: Table 4: Accuracy Results Using Individual Feature Sets for Ads on the Top 100 k Set
From Table 4, we see that user features provide very little in forma-tion as to user navigation behavior in our model. This has the upside that data can be collected in a way that maximizes privacy. Qu eries prove to be a better discriminator then users by a wide margin ; how-ever, the performance is still significantly below that achi eved when all features are present. This could be because result quali ty often varies significantly for a particular query, or that query sy nonymy or ambiguity confounds predictions based on query features alone. Site and URL features seem to offer the most classification in forma-tion  X  some sites or pages are predictably more prone to elici t clicks or browsing time from users. It is interesting to note that si te fea-tures tend to outperform URL features, even though a URL prov ides information on a finer resolution then a site alone as one site can contain many URLs. One possible explanation is that there ex ists much more site information in our data set, enabling better f eature estimates. Regardless of the cause, this observation is pro mising since site level information is the most frequently availab le data in our logs.
In this paper, we have performed the first detailed study of po st-click through user behavior on sponsored results, and compa red it to the case of organic search. We also presented a generative model based on a mixture of power laws, and showed how to predict use r behavior on result sites using a set of user, query, site, and page features.

Overall, our results show that user behavior in sponsored se arch has a number of similarities, but also some differences from that in organic search. This observation is interesting since sp onsored search results originate from a distinct and specialized me chanism, ruled more by bid prices than relevance to users, who only ind irectly affected ad selection via click-through rate. However, it i s becoming increasingly clear that to be successful, a sponsored searc h platform has to balance the interests of advertisers, searchers, and search en-gines, and this requires use of a richer set of features inclu ding those gleaned through implicit feedback.

Our work here takes a first step towards using such features in sponsored search, but leaves open a number of questions. Fir st, prediction results could be improved greatly by using more s ophis-ticated methods and by incorporating additional features, some of which such as the position of the clicked ad among sponsored r esults were not available at the time of this study. It would also be i nterest-ing to relate post click-through behavior to actual converg ence (e.g., a purchase, or as defined by the advertiser), and to explore th e long-term changes in user behavior due to engagement with ad sites (e.g., do people return to a site after engaging in more clicks on a pr evious visit).
