 Michail G. Lagoudakis MGL @ CS . DUKE . EDU Department of Computer Science, Duke University, Durham, NC 27708 USA Reinforcement learning provides an intuitively appealing framework for addressing a wide variety of planning and control problems(Sutton &amp; Barto, 1998). Compelling con-vergence results exist for small state spaces(Jaakkola et al., 1994) and there has been some success in tackling large state spaces through the use of value function approxima-tion and/or search through a space of parameterized poli-cies (Williams, 1992).
 Despite the successes of reinforcement learning, some frus-tration remains about the extent and difficulty of feature engineering required to achieve success. This is true both of value function methods, which require a rich set of fea-tures to represent accurately the value of a policy, and pol-icy search methods, which require a parameterized policy function that is both rich enough to express interesting poli-cies, yet smooth enough to ensure that good policies can be discovered. The validity of such criticisms is debatable (since nearly all practical applications of machine learning methods require some feature engineering), but there can be no question that recent advances in classifier learning have raised the bar. For example, it is not uncommon to hear anecdotal reports of the naive application of support vector machines matching or exceeding the performance of classical approaches with careful feature engineering. We are not the first to note the potential benefits of modern classification methods to reinforcement learning. For ex-ample, Yoon et al.(2002) use inductive learning techniques, including bagging, to generalize across similar problems. Dietterich and Wang (2001) also use a kernel-based ap-proximation method to generalize across similar problems. The novelty in our approach is its orientation towards the application of modern classification methods within asin-gle, noisy problem at the inner loop of a policy iteration algorithm. By using rollouts and a classifier to represent policies, we avoid the sometimes problematic step of value function approximation. Thus we aim to address the cri-tiques of value function methods raised by the proponents of direct policy search, while avoiding the confines of a pa-rameterized policy space.
 We note that in recent work, Fern, Yoon and Givan(2003) also examine policy iteration with rollouts and an induc-tive learner at the inner loop. However, their emphasis is different. They focus on policy space bias as a means of searching a rich space of policies, while we emphasize modern classifiers as a method of recovering high dimen-sional structure in policies. In this section we review the basic definitions for Markov Decision Processes (MDPs), policy iteration, approximate policy iteration and rollouts. This is intended primarily as a review and to familiarize the reader with our notation. More extensive discussions of these topics are widely avail-able (Bertsekas &amp; Tsitsiklis, 1996).
 2.1. MDPs An MDP is defined as a 6-tuple ( S ; A ;P;R; X ;D ) where: S is the state space of the process; A is a finite set of ac-tions; P is a Markovian transition model, where P ( s;a;s is the probability of making a transition to state s 0 when taking action a in state s ; R is a reward (or cost) function, such that R ( s;a ) is the expected reward for taking action a in state s ;  X  2 [0 ; 1) is the discount factor for future re-wards; and, D is the initial state distribution from which states are drawn when the process is initialized. In reinforcement learning, it is assumed that the learner can observe the state of the process and the immediate reward at every step, however P and R are completely unknown. In this paper, we also make the assumption that our learning algorithm has access to a generative model of the process which is a black box that takes a state s and an action a as inputs and outputs a next state s 0 drawn from P and a reward r . Note that this is not the same as having the model ( P and R ) itself.
 A deterministic policy for an MDP is a mapping : S7! A from states to actions, where ( s ) is the action the agent takes at state s .Thevalue V ( s ) of a state s under a policy is the expected, total, discounted reward when the pro-cess begins in state s and all decisions at all steps are made according to policy : The goal of the decision maker is to find an optimal policy from the initial state distribution D : It is known that for every MDP, there exists at least one optimal deterministic policy. 2.2. Policy Iteration Policy iteration is a method of discovering such a policy by iterating through a sequence 1 , 2 , ..., k of improving policies. The iteration terminates when there is no change in the policy ( k = k  X  1 )and k is the optimal policy. This is typically achieved by computing V done iteratively or by solving a system of linear equations and then determining a set of Q-values: and the improved policy as In practice, policy iteration terminates in a surprisingly small number of steps. However, it relies on the availabil-ity of the full model of the MDP, exact evaluation of each policy, and exact representation of each policy. 2.3. Approximate methods Approximate methods are frequently used when the state space of the underlying MDP is extremely large and exact (solution or learning) methods fail. A general framework of using approximation within policy iteration is known as ap-proximate policy iteration (API). In its most general form, API assumes a policy b i at each step, which may not nec-essarily be an exact implementation of the improved pol-icy from the previous time step. Typically, the error in-troduced is bounded by the maximum norm ( L 1 ) error in what is presumed to be an approximate b Q to generate i +1 (Bertsekas &amp; Tsitsiklis, 1996). API can-not guarantee monotonic improvement and convergence to the optimal policy. However, in practice it often finds very good policies in a few iterations, since it normally makes big steps in the space of possible policies. This is in con-trast to policy gradient methods which, despite acceleration methods, are often forced to take very small steps. LSPI (Lagoudakis &amp; Parr, 2001) is an example of an API algorithm. It uses temporal differences of sample transitions between state-action pairs to approximate each Q . It is also possible to use a technique closer to pure Monte Carlo evaluation called rollouts . Rollouts estimate Q ( s;a ) by executing action a in state s and following pol-icy thereafter, while recording the total discounted re-ward obtained during the run. This processes is repeated several times and it requires a simulator with resets to ar-bitrary states (or a generative model) since a large num-ber of trajectories is required to obtain an accurate esti-mate of Q ( s;a ) . Rollouts were first used by Tesauro and Galperin (1997) to improve online performance of a backgammon player. (A supercomputer did rollouts before selecting each move.) However, they can also be used to specify a set of target values for a function approximator used within API to estimate Q . The rollout algorithm is summarized in Figure 1. The dependence of typical API algorithms on value func-tions places continuous function approximation methods at the inner loop of most approximate policy iteration algo-rithms. These methods typically minimize L 2 error, which is a poor match with the L 1 bounds for API. This prob-lem is not just theoretical. Efforts to improve performance such as adding new features to a neural network or new basis functions to LSPI don X  X  always have the expected ef-fect. Increasing expressive power can sometimes lead to surprisingly worse performance, which can make feature engineering a somewhat tedious and counterintuitive task. If a model is available and the structure of the value func-tion approximation is compatible with the model, value function approximation minimizing L 1 error is possi-ble (Guestrin et al., 2001). There are arguments that such approximations are better suited to MDPs, suggesting the use of something like support vector regression, which tries to limit the worst case approximation error. Unfortunately, the approach does not generalize easily to reinforcement learning in the presence of noise. To see this, consider that in minimizing L 2 error, we serve two objectives: averag-ing within states and smoothing across states. Thus, the mean state value (based upon trajectories or temporal dif-ferences) is also the one that minimizes L 2 error at that state. In contrast, minimizing L 1 error is ill suited to raw data of the type encountered in reinforcement learn-ing since the mean state value can differ sharply from the L 1 error minimizing estimate. (Suppose we draw 100 next states from state s ,ofwhich 99 have value 1 : 0 and 1 has value 0 : 0 . The mean value for V ( s ) is 0 : 99 , which is also the value that minimizes the L 2 error in the temporal dif-ferences. However, V ( s )=0 : 5 minimizes the L 1 error.) An important observation, also noted by Fern, Yoon and Givan (2003), is that rollouts can be used within API to avoid the problematic value function approximation step entirely. We choose some representative set of states S and assume that we can perform enough rollouts to de-termine which action maximizes Q ( s;a ) for the current policy. Rather than fitting a function approximator to the values obtained by the rollouts, we instead train a classi-fication learner where the maximizing action is the label for the state. This approximate policy iteration algorithm is described in Figure 2.
 We use the notation ( s;a ) + to indicate a positive train-ing example, and ( s;a )  X  to indicate a negative example. Learn is a supervised learning algorithm that trains a clas-sifier given a set of labeled training data. The structure of the policy iteration algorithm naturally suggests a batch im-plementation since the policy is updated in distinct phases. The termination condition is left somewhat open ended. It can be when the performance of the current policy does not exceed that of the previous one, when two subsequent poli-cies are similar (the notion of similarity will depend upon the learner used), or when a cycle of policies is detected (also learner dependent). If we assume a fortuitous choice of
S and a sufficiently powerful learner that can correctly generalize from S to the entire state space, the i th itera-tion of this algorithm will learn the improved policy of i effectively implementing a full policy iteration algorithm, and terminating with the optimal policy. For large-scale problems, choosing S and dealing with imperfect classi-fiers will pose some challenges. For the choice of S , we have a number of alternatives. The simplest is to try to dense, uniform covering of the state space. For low-dimensional state spaces, this will be practical, but it scales poorly. A similar option would be to randomly select S from some uniform distribution over the state space. This is again problematic due to poor cov-erage for high-dimensional spaces.
 A natural choice of S would be the distribution of states induced by the current policy, i . While intuitively appeal-ing, this distribution may differ dramatically from the dis-tribution of the subsequent policy, i +1 , for which we must train our classifier. (To see this, consider a i that directs the system towards one  X  X ide X  of the state space and a i +1 that directs the system towards another side. If we train our classifier on states drawn from i , when we try to use our classifier to execute i +1 , it may be asked to classify states that are disjoint from the ones it has been trained on.) This mismatch between training and testing can be dealt with by using a step size , 0 &lt; 1 , to keep the policy for the next iteration sufficiently close to the current policy so that performance does not degrade: Note that b i +1 is now a stochastic policy that chooses from the improved policy with probability and from the old policy with probability (1  X  ) . A positive that im-proves performance is guaranteed to exist (Jaakkola et al., 1995). Recently, Kakade and Langford (2002) demon-strated a method for picking near optimally, although the largest  X  X afe X  value may be quite small.
 Fortunately, our assumption of a generative model gives us the luxury of drawing states from the policy we wish to learn before we have completely discovered or learned it. While this may sound paradoxical at first, it is actu-ally quite simple thanks to an observation by Fern (personal communication). If we begin in some state s 0 , we can use rollouts to determine i +1 for s 0 . We can then sample s by executing action i +1 ( s 0 ) in s 0 . We continue by using rollouts to determine i +1 ( s 1 ) and executing this action to obtain s 2 . We continue in this fashion until we have sam-pled states and actions along an entire trajectory of i +1 starting from s 0 . Trajectories produced during rollouts are discarded, the only training kept are from i +1 . Fern X  X  observation mostly solves the S problem, but it leaves open the question of how the initial state s 0 is se-lected. The initial distribution D may seem like a natural distribution from which to draw s 0 . In practice, however, this can cause API to get stuck in local optima: Suppose visits only a small region of the state space. To improve upon i , rollouts must discover better alternatives at the fringe of the states reachable by i . However, our clas-sifier for i was never trained on states that aren X  X  reach-able by i , making it unlikely that rollouts from the fron-tier of i will produce a better alternative to staying within the region normally circumscribed by i . The choice of a  X  X estart distribution X  which differs from the problems nat-ural starting distribution is also explored by Kakade and Langford(2002), who show that a poor choice of a restart distribution can lead to arbitrarily bad performance (policy loss that grows with the size of the state space). Of course, the ideal restart distribution would be that of the optimal policy, but this begs the question.
 A related practical problem is what to do in states where rollouts cannot provide sufficient information to select i +1 . We discuss this issue in Section 6. Suppose that our learner fails to learn i ( s ) perfectly when presented with ~ i ( s ) for all s in S . To quantify the extent of this failure, we must first define the test distribution. Fol-lowing Kakade and Langford (2002), we express the natu-ral test distribution for this problem as the set of states en-countered when starting from states drawn from the initial distribution D , and following b i . The probability of reach-ing future states is discounted by  X  and the infinite sum is normalized by (1  X   X  ) , resulting in: where the s subscript indicates that we are selecting com-ponent s of the matrix-vector product.
 If we have an apriori guarantee that our learner will choose the wrong action with probability at most on states drawn from this distribution, then we can bound the expected shortfall from following i instead of b i as follows: where R max is the maximum reward value. This pes-simistic bound arises from the assumption that all mistakes are made in the initial states, which occur with probability (1  X   X  ) , incurring penalty R max = (1  X   X  ) . Since we cannot guarantee that such learners exist, this is not meant to be a serious bound, but reassurance that in principle good learn-ers can produce good policies. In practice, the actual loss is best measured empirically by Monte Carlo evaluation, or estimated by the error rate on the training set. From a prac-tical standpoint, high observed errors (or low performance) will suggest a change in representation, or a change in the learning mechanism, such as a change of kernel. The main contribution of our paper is a particular embod-iment of the approximate policy iteration algorithm de-scribed in Section 3. Training examples can be formed for any given state s 2 S assuming some underlying policy b . The estimated values e Q b ( s;a ) are computed by rollouts for all possible actions in state s .Ifthevalues e Q b ( s;a ) were exact, then the maximizing action a would yield one positive example ( s;a ) + and the rest of the actions would yield a number of negative examples ( s;a )  X  for all a 6 Unfortunately, the estimates e Q b ( s;a ) are noisy and could yield incorrect examples if treated as exact. Thus, we used a simple two-sample t -test to compare rollout values. To generate examples in any state s using the rollout values e Q ( s;a ) , we did the following: 1. Use a fixed budget of k samples to determine e Q in 2. Generate a positive example ( s;a ) + if the value of 3. Generate a negative example ( s;a )  X  for each action A positive example is generated only if there is a clearly best action in which case all other actions generate nega-tive examples. If there is no best action, negative examples can still be generated for the actions that are clearly inferior. Notice that in this case the remaining actions appear to be equally good and, by not generating a positive example, the classifier is essentially given the freedom to choose any of them. The only case where no training examples are gen-erated is when all actions appear to be equally good. We expect this approach would benefit from more sophisticated approaches to managing the number of samples used (Kael-bling, 1993; Kearns et al., 1999).
 One peculiarity of rollout based policy iteration is that if the current policy is very good, i.e. able to recover from small mistakes, there will be no statistically significant dif-ferences between many of the actions. This can make it difficult to acquire sufficient training data for the next pol-icy. We mitigate this problem by treating the demonstrably bad actions as negative training examples even if we can-not determine a single, clearly superior action. Note that randomly selecting an action among the equivalent ones and marking it as positive will create a lot of noise for our learner since subsequent visits to the same state may pol-lute the training set with multiple  X  X ptimal X  actions for the same state. A simple lexicographic ordering can also have unexpected side effects at execution time by introducing strong preferences for particular actions and heavily bias-ing the training data with examples of just one class. The most significant contribution of effort is that it opens reinforcement learning to the full array of modern classi-fication methods through the learn function. SVMs are a particularly appealing choice to the reinforcement learning practitioner vexed by the feature selection problem. We offer a brief sketch of how SVMs work to justify this ap-peal: With the kernel trick, SVMs are able to implicitly and automatically consider classifiers with very complex feature spaces. Nevertheless, the optimization performed by SVMs can be interpreted as a search through a space of classifiers to find one that is both a good fit and has low VC dimension. In the most optimistic interpretation, this dodges the feature selection problem while simultaneously demonstrating resistance to overfitting. In practice there are, of course, complications but if SVMs come close to this dramatic and optimistic description, we should be able to feed the raw state variables used by our simulators into our SVM classifier with little regard for the feature engi-neering required to obtain success in these problems using value function methods.
 While SVMs are a particularly appealing choice for learn , they are not the only option and may not be the most de-sirable option in many cases. The theoretical motivations for using SVMs are not as crisp for multiclass problems. For problems with many actions, other classification meth-ods may be more natural: neural nets, Bayes nets, decision trees, etc. For these reasons, and for the sake of compar-ison, we also implemented learn using a neural network. We designed the neural network with a number of outputs equals to the number of actions and trained the network to activate output i (and not others) output on positive exam-advantage of negative examples. We implemented the SVM version of our API algorithm using SVMTorch (Collobert &amp; Bengio, 2001), a publicly available implementation of support vector machines. The SVMTorch package provides a simple multiclass capability (one versus all), but is not necessarily representative of the best that can be done on multiclass problems using SVM technology. We also implemented a version of our algo-rithm using a simple feedforward, multi-layer neural net-work as the multiclass classifier. In this section, we present experimental results on the inverted pendulum problem and the bicycle balancing and riding problem. Our goal in these preliminary experiments is not necessarily to demon-strate the superiority of our rollout approach in terms of CPU cycles or sample complexity, but rather its viability as an alternate approach to the reinforcement learning control problem.
 In our experiments we ran approximate policy iteration un-til the observed performance of the policy, as measured with experiments with the simulator, decreased. Since ap-proximate policy iteration does not ensure monotonically improving policies, it is possible that continuing to run pol-icy iteration beyond an initial setback could still result in better policies, but we did not explore this possibility. 7.1. Inverted pendulum In the inverted pendulum domain, the task is to balance a pendulum of unknown length and mass at the upright posi-tion by applying forces to the cart to which it is attached. Three actions are allowed: left force LF (  X  50 Newtons), right force RF ( +50 Newtons), or no force (NF) at all ( 0 Newtons). All three actions are noisy; uniform noise in [  X  10 ; 10] is added to the chosen action. The state space of the problem is continuous and consists of the vertical angle and the angular velocity _ of the pendulum. The transitions are governed by the nonlinear dynamics of the system (Wang &amp; Griffin, 1996). and depend on the current state and the current (noisy) control u :  X  = g sin( ) where g is the gravity constant ( g =9 : 8 m=s 2 ), m is the mass of the pendulum ( m =2 : 0 kg), M is the mass of the cart ( M =8 : 0 kg), l is the length of the pendulum ( l =0 : 5 m), and =1 = ( m + M ) . A reward of 1 is given as long as the angle of the pendulum does not exceed = 2 in absolute value (the pendulum is above the horizontal line). An angle greater than = 2 signals the end of the episode and a reward (penalty) of 0 . The discount factor of the process is set to 0 : 95 .
 Using about 200 rollout states, the algorithm consistently learns excellent balancing policies in one or two iterations with both neural nets and SVMs, starting with an initial policy that selects actions randomly with uniform proba-bility. Such  X  X xcellent X  policies balance the pendulum for more then 3 simulated minutes (in practice, we found that such policies could balance essentially indefinitely). The choice of the sampling distribution did not affect the re-sults significantly. For illustration, we used uniform sam-pling for rollout states. Figure 3 shows the training data obtained for the LF action. A positive example indicates a state where LF was found to be the best action and a neg-ative example is a state where LF was found to be a bad choice. It is easy to see that positive and negative examples are easily separated. The same figure also shows the result-ing support vectors for the LF classifier using a polynomial kernel of degree 2.
 Figure 4 shows the entire learned policies (blue/dark-gray for LF, red/medium-gray for RF, and green/light-gray for NF) for all three classifiers: SVM with a polynomial ker-nel, SVM with a Gaussian kernel, and a neural network classifier with 5 hidden units. Interestingly, in the case of the polynomial kernel, the policy does not use the NF ac-tion at all, whereas the other policies do. This is due to the limited abilities of the polynomial degree-2 kernel. All policies are excellent in the sense that they can all balance the pendulum for a long time, perhaps indefinitely. In all cases, the input to the SVM or the neural network was just the 2-dimensional state description. For SVMs, the number of support vectors was normally smaller than the number of rollout states. The constant C , the trade-off between train-ing error and margin, was set to 1 .
 We note that pendulum balancing is a relatively simple problem. The classes are nearly linearly separable, so good classification performance here should not be surprising to those familiar with modern classification methods. Note-worthy features from the reinforcement learning perspec-tive are the small number of iterations of policy iteration required and the non-parametric representation of the pol-icy. Figure 3 shows the ability of the SVM to adapt the rep-resentation to match the training data since only the support vectors are used to represent the policy. 7.2. Bicycle riding In the bicycle balancing and riding problem (Randl X v &amp; Alstr X m, 1998) the goal is to learn to balance and ride a bicycle to a target position located 1 km away from the starting location. Initially, the bicycle X  X  orientation is at an angle of 90 to the goal. The state description is a six-dimensional real-valued vector ( ; _ ;!; _ !;  X  !; ) ,where is the angle of the handlebar, ! is the vertical angle of the bicycle, and is the angle of the bicycle to the goal. The actions are the torque applied to the handlebar (dis-cretized to f X  2 ; 0 ; +2 g ) and the displacement of the rider (discretized to f X  0 : 02 ; 0 ; +0 : 02 g ). In our experiments, actions are restricted so that either =0 or =0 giving a total of 5 actions. The noise in the system is a uniformly distributed term in [  X  0 : 02 ; +0 : 02] added to the displace-ment component of the action. The dynamics of the bicycle are based on the model of Randlov and Alstrom (1998) and the time step of the simulation is set to 0 : 02 seconds. As is typical with this problem, we used a shaping reward (Ng et al., 1999).
 Our experiments with the bicycle did show some sensitivity to the parameters of the problem as well as the parameters of our learner. This made it difficult for us to find parame-ters that consistently produced good performance. Some of this may simply be reflective of our inexperience in tuning the parameters of SVMs. It is also possible that we did not consider enough samples.
 For our SVM experiments, we used a shaping reward of r t given at each time step, where r t =( d t  X  1  X   X d t ) as long as j ! j &lt;= 15 ,and r =0 otherwise. d t is the distance of the back wheel of the bicycle to the goal position at time The discount factor was set to 0 : 95 .
 In our preliminary experiments with this domain, we were able to solve the problem with uniform sampling and poly-nomial kernels of low degree. However it required a large number of rollout states (about 5 ; 000 ). With sampling from the distribution of the next policy, we were able to solve the problem with fewer rollout states and both RBF and polynomial kernels. However we did not find kernels that consistently produced good policies with reasonable sample sizes. (The balancing problem is solved easily us-ing any of the classification methods, but riding to the goal proved more difficult.) Figure 5 shows a sample trajectory from the final policy of one of our better policy iteration runs using SVMs. The bi-cycle moves in the 2-dimensional plane from the initial po-sition (0 ; 0) (left side) to the goal position (1000 ; 0) side). This policy was produced with a polynomial ker-nel of degree 3 and 4000 rollout states. In the final policy, the bicycle rides to the goal, then turns around toward the goal in a very tight radius. This policy was obtained in just two API iterations, starting with a uniformly random ac-tion selection policy. Similarly to the pendulum, the input to the SVM was the raw 6-dimensional state description and C =1 .
 For our neural network experiments, we used a shaping re-ward of r t given at each time step, where r t =1+( d t  X  1  X d t ) as long as j ! j &lt;= 15 ,and r =0 otherwise. d t is the distance of the back wheel of the bicycle to the goal posi-tion at time t . The discount factor was set to 0 : 99 . Since our neural network learner only uses positive examples and not all states successfully produce positive training instances, we used 8000 rollout states.
 Figure 5 shows sample trajectories of one of our better neu-ral network policy iteration runs using 30 hidden units. Af-ter the first iteration, the learned policy can only balance the bicycle for a few steps and it crashes. The policy at the second iteration reaches the goal, but fails to return to it. Finally, the policy at the third iteration, reaches the goal faster and stays there. The best neural network policy is not as good as the best SVM policy, but it is more illustrative of the progress of policy iteration because it takes an extra iteration. We have presented a case for an approach to RL that com-bines policy iteration and pure classification learning with rollouts. The emphasis of the approach in this paper is the ability to use of modern classification techniques such as SVMs to alleviate some of the burden of feature engineer-ing from the practitioner of reinforcement learning. How-ever, our empirical results also suggest that more traditional methods such as neural networks can be used successfully. We believe that these initial successes will help open the door to greater exploitation of modern classification meth-ods on more challenging reinforcement learning domains. Of course, many questions remain. More thorough inves-tigation of issues relating to sample complexity and restart distributions are important areas for future work. This research was supported in part by NSF grant 0209088. We also thank Alan Fern, Bob Givan, Carlos Guestrin and Ryan Deering for helpful discussions.

