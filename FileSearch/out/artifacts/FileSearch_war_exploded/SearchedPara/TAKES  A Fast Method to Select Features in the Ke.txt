  X  Feature selection is an effective tool to deal with the  X  X urse of di-mensionality X . To cope with the non-separable problem, feature selection in the kernel space has been investigated. However, pre-vious study cannot adequately estimate the intrinsic dimensionality of the kernel space. Thus, it is difficult to accurately preserve the sketch of the kernel space using the learned basis, and the feature selection performance is affected. Moreover, the computing load of the algorithm reaches at least cubic with the number of training data. In this paper, we propose a fast framework to conduct feature selection in the kernel space. By designing a fast kernel subspace learning method, we automatically learn the intrinsic dimension-ality and construct an orthogonal basis set of kernel space. The learned basis can accurately preserve the sketch of kernel space. Then backed by the constructed basis, we directly select features in kernel space. The whole proposed framework has a quadratic complexity with the number of training data, which is faster than existing kernel methods for feature selection. We evaluate our work under several typical datasets and find it not only preserves the sketch of the kernel space more accurately but also achieves bet-ter classification performance compared with many state-of-the-art methods.
 I.2.6 [ Learning ]: Induction; I.5.4 [ Pattern Recognition ]: Appli-cation Theory, Algorithm, Performance Feature Selection, Kernel Space, Orthogonal Subspace Learning  X 
This paper was done when Ye Xu was affiliated with Nanjing Uni-versity.

The  X  X urse of dimensionality X  is a serious problem for classifi-cation tasks involving high-dimensional data. Reducing the dimen-sionality is an important tool to address this problem. By reducing the dimensionality, we obtain a reduced representation of input data that is much smaller in volume, and produce the same (or almost the same) analytical result.

Generally, techniques for reducing dimensionality can be cate-gorized into two classes, i.e., dimensionality reduction and feature selection. Some typical dimensionality reduction methods, such as Principal Component Analysis (PCA) [9], Linear Discriminant Analysis (LDA) [21], Maximum Margin Criterion (MMC) [17], Locally Linear Embedding (LLE) [24], ISOMAP[31] and their in-variants [8, 38, 37, 19, 23, 39, 5], try to use transformations to con-vert original high-dimensional data space into lower-dimensional feature space, which have been proved to be effective. However, the high dimensionality of data sets often fails many such algo-rithms due to their high computational cost.

In this paper we focus on feature selection techniques. Feature selection algorithms (e.g., [15, 40, 32, 33, 29, 14, 36]) directly remove non-discriminative features in order to build robust learning models based on some certain criterions, which attract a variety of attentions [16, 28, 12, 25].

Traditional methods for reducing dimensionality may work well in linear separable problems. However, for nonlinear problems, the performance may be affected. Therefore, kernel technique is proposed to tackle this issue. Some traditional dimensionality re-duction methods have been extended into kernel version [22, 30, 1] and achieve sweet results.

For feature selection methods, some works [20, 18] have been done to employ kernels to help select features in original space, which improve classification performance to some extent. How-ever, such algorithms still select features in the original space, which can not guarantee a good performance for the nonlinear problems. Because the dimensionality of the kernel space is implicit, it is a big challenge to directly select features in the kernel space. It is no wonder that few works can handle this thorny issue.

In this paper, we propose a feature selection method in the kernel space named as Two-stage fAst Kernel fEature Selection (TAKES). In the first stage, we design a fast kernel subspace learning method to learn the intrinsic dimensionality of the kernel space, and con-struct an orthogonal basis. Then in the second stage, via the learned orthogonal basis, we directly select discriminative features in the kernel space using the kernel trick. In summary, the first contribu-tion of this paper is that, we design a framework to directly select discriminative features in the kernel space. The classification per-formance under the selected features is better. The second contribu-tion of our work is designing a kernel subspace learning algorithm to automatically estimate the intrinsic dimensionality of the kernel space and construct an orthogonal basis. The learned dimensional-ity of the kernel space is completely data-dependent and a small re-construction cost [24] is guaranteed. Here, the reconstruction cost is a measure to evaluate whether the learned basis can accurately preserve the sketch of data [10] in the kernel space. The third con-tribution is the small computing load compared with other kernel methods for reducing dimensionality.

The rest of this paper is organized as follows. In section 2, we briefly discuss some related works. Our TAKES method is pro-posed in section 3. In section 4, we report on experimental results. Finally in Section 5, we conclude the paper.
Feature selection algorithms have been investigated for many years. Because obtaining an optimum set of features has been demonstrated as an NP-hard problem [35], many algorithms handle feature selection tasks by introducing weights on dimensionality to obtain approximate solutions. Such methods can be divided into several categories. Based on the assumption that the discriminative power of each feature is independent of each other, Information Gain (IG) [33] and  X  2 -test [40] measure the amount of information obtained by each feature for classification and then discard those features whose discriminative powers are small. But these meth-ods might fail when features are not independent of each other. Relief [15, 29] is another category feature selection algorithm. It estimates features according to how well their values distinguish among instances that are near each other, where smart heuristic is used. Boosting methods [32] have been used for feature selection, in which discriminative features are selected and combined into an ensemble classifier. The generalization error of boosting family al-gorithms [14] has been proven to be controlled by a bound in terms of a margin measure [34]. Therefore, such algorithms have better performance and wide applications [16, 28]. To improve the time complexity of boosting methods, Forward Feature Selection (FFS) [36] has been proposed. Unlike many other boosting algorithms, FFS trains weak classifiers only once, and thus is able to obtain lower time complexity (O( dN )) compared with most boosting fam-ily methods (O( dNlogN )). Here, N is the size of the training set, and d is the number of selected features.

Our work also relates to kernel methods for feature selection [20, 18]. Most of such methods still select features in the original space, which cannot change the non-separability nature of the data. A bet-ter solution is directly selecting features in kernel space. To the best of our knowledge, [6] is the only work that selects features in the kernel space. In this work, by learning a basis for the kernel space, Relief [15] is employed to select features. However, the ma-jor shortcoming of [6] is that, the intrinsic dimensionality of the kernel space in their work is constantly set up as the size of train-ing set. It is not accurate unless all mapped training data in the kernel space are independent of each other. However, they are not necessarily independent. Therefore, this heuristically determined dimensionality undermines the orthogonality of learned basis. In this case, the basis set cannot accurately preserve the sketch of the kernel space; the feature selection performance is influenced. An-other limitation of [6] is the computing load. By employing a ker-nel subspace learning method such as Kernel Gram-Schmidt (KGS) or Kernel Principal Component Analysis (KPCA) [26] to construct the basis, the time complexity is at least O ( N 3 ) .
 In our work, we aim at selecting features in the kernel space. First we design a fast kernel subspace learning method to learn the intrinsic dimensionality of the kernel space, and construct an or-thogonal basis to accurately preserve the sketch of the mapped data in the kernel space. Then with the aid of the basis, discrimina-tive features in the kernel space are directly selected with a smaller computing load.
Traditional feature selection methods may not work well in lin-early non-separable problems. To overcome the shortcoming, in this paper, we select discriminative features directly from the ker-nel space instead of from the original data space.

Suppose  X  maps input data x  X  R D to a H -dimensional kernel space: In order to directly select discriminative features in the kernel space, we need to know the value of  X  i ( x ) , i.e., the i th feature of  X  ( x ) . However, in the kernel space,  X  ( x ) and the number of features H are implicit. Thus it is difficult for us to achieve  X  i ( x ) explicitly.
To address this problem, we construct an orthogonal basis set { w i } = { P N j =1  X   X  i,j  X  ( x j ) } for the kernel space, where { x are training data,  X   X  i are the coefficient vectors, and  X   X  component of  X   X  i . Then  X  i ( x ) can be calculated by using the kernel trick: Therefore, a key issue for feature selection in the kernel space is to properly estimate the intrinsic dimensionality and find an orthogo-nal basis of the kernel space, so that the mapped data in the kernel space can be accurately represented. In other words, the  X  computed by (2) can be accurate only if the sketch of the kernel space is accurately preserved by the learned basis .

The dimensionality of basis set for kernel space is the rank of mapped data set {  X  ( x j ) } N j =1 . This rank equals to the numerical rank of kernel matrix K , whose entries are defined by: Computing the rank of a matrix (i.e. the intrinsic dimensionality of kernel space) is a difficult issue, which has a time complexity of O ( N 3 ) [13]. It is, however, not an ideal choice to predetermine the target dimensionality as a constant, either. If it is too small, the learned basis will have a large distinction from the kernel space. On the contrary, a too large predetermined dimensionality, as adopted by [6], renders it difficult in learning orthogonal base vectors and aggravates computing load. Take the Hilbert Matrix [7] for exam-ple: The n  X  n Hilbert matrix H ( n ) is a positive definite symmetric matrix. It means that, theoretically speaking, the dimensionality of the space that is spanned by H ( n )  X  X  column vectors should be n . However, in a case study, we validate that there are only about 18 linear independent base vectors in the space spanned by H (100)  X  X  100 column vectors. If we set up the dimensionality of the space spanned by H (100)  X  X  column vectors as 100 , many learned base vectors of this space are numerically non-orthogonal and the too large predetermined dimensionality increases the computing load of the algorithm. Therefore, under both of the two conditions, the learned basis cannot accurately preserve the sketch of the kernel space and the performance of feature selection is undermined.
To tackle this problem, we design a fast kernel subspace learning method  X  Kernel Orthogonal Basis Learner (KOBL). KOBL auto-matically learns the intrinsic dimensionality of the kernel space, and constructs an orthogonal basis set with a small reconstruction cost [24]. Here, reconstruction cost is employed to measure the quality of the learned basis set. A small value of reconstruction cost means the learned basis set can accurately preserve the sketch of the data that are used to learn the basis, i.e., the quality of learned basis is high. By designing a stop criterion, KOBL has a time complexity of O ( dN 2 ) , which outperforms many other kernel subspace learn-ing methods such as KGS and KPCA (at least O ( N 3 ) ). Backed by a high quality basis learned by KOBL, our feature selection work achieves better performance. In the rest of Section 3, we give the detailed TAKES.
In this part, we propose the KOBL algorithm to learn the intrinsic dimensionality of feature space and construct an orthogonal basis from the mapped training data.

At first, we initialize the data matrix A (0) = [  X  ( x 1 ..., X  ( x N ) (0) ] , basis M of the kernel space to empty basis and its dimensionality k = Dim ( M ) = 0 . Therein, {  X  ( x i ) (0) mapped data in the kernel space. (In this paper, we use  X  ( x and  X  ( x i ) interchangeably.)
In every iteration, we first find the ( k + 1) th pivot column as fol-lows (Suppose the former k iterations have finished and the former k base vectors have been obtained.): Here, j is the column that has the largest column norm from the ( k + 1) th column to the N th column.

Then we interchange the pivot column j with column k + 1 , where k is the current number of the learned base vectors. After the process of column pivoting and exchange, we calculate the norm of the ( k + 1) th column vector r k +1 ,k +1 by: And we obtain the normalized potential base vector w k + 1 Here, the potential base vector means the learned base vectors that may be accepted or rejected determined by a threshold policy that is discussed later. If w k + 1 is accepted, the data matrix A updated using orthogonal transformation as follows, Therein, r k +1 ,j can be computed by
On the contrary, if the threshold condition does not satisfy, we will not accept w k + 1 .

Note that the kernel map  X  is unseen, and the form of basis { w is implicit. Therefore, using the kernel trick described in (2), the norm of mapped data in (5) can be computed by the following equa-tion, Therein,  X   X  1 ,...,  X   X  k are the k coefficient vectors achieved in the for-mer k iterations.

Because the norm of w k + 1 is we implicitly achieve the ( k + 1) th normalized base vector w by setting the ( k + 1) th coefficient vectors 1 ,
Similarly, (8) is fulfilled via updating the coefficient vectors as follows,
It deserves mentioning that most kernel subspace learning meth-ods such as KGS, KPCA and their invariants [27] ceaselessly learn the basis until all training data are used, which suffers from heavy computing time. In KOBL, a stop criterion is designed to over-come this shortcoming. In the rest of this part, we analyze KOBL algorithm in detail.
To learn an orthogonal basis, KOBL takes the independence be-tween the learned base vectors and the mapped data in the kernel space into consideration. Suppose we have learned basis w ..., w k sequentially by orthogonal transformations. If a mapped data  X  ( x k +1 ) is dependent on the learned basis, the corresponding computed vector w k + 1 is unsuitable to be accepted. It is because that if a base vector is transformed from a pattern that has strong dependence on the learned basis, the computation error would arise and affect the numerical orthogonality of the learned basis [13]. Therefore, it is necessary to consider the independence between the learned basis and the mapped data.

Here, based on the linear dependence theorem [13], we use the projection distance of a mapped data  X  ( x k +1 ) onto the current learned basis space span { w 1 , w 2 ,..., w k } to measure the numer-ical independence between  X  ( x k +1 ) and the learned basis for the kernel space: (Suppose we have already learned the base vectors w 1 , w 2 ,..., and w k . The potential base vector w k + 1 computed by (7).)
At the beginning of the algorithm, the coefficient vectors are sim-ply initialized as the unit vectors:  X  (0) j = e j , for j = 1 : N . Therein, e j is the vector whose j th component is 1 and the rest components are 0. The distance in (14) can be rewritten in the following form: where  X a = ( a 1 ,a 2 ,...,a k ) &gt; is a coordinate vector.
We define matrix W k = [ w 1 , w 2 ,..., w k ] , thus (14) can be rewritten in the form:
The column vectors of W k are orthogonal. Thereby, we can construct an orthogonal matrix Q 2 in the following form: Q umn vectors of Q 1 and W k are orthogonal.

Since orthogonal transformation never changes the 2-norm of a vector, (16) is equal to the following expression: k W k  X a  X   X  ( x k +1 ) k 2 = k Q 2 &gt; ( W k  X a  X   X  ( x
We let thereby, the value of (14) is equal to k Q 1 &gt;  X  ( x independence between the learned base vectors and  X  ( x k +1 be measured by the value of k Q 1 &gt;  X  ( x k +1 ) k 2 .
According to (6) and (8), it is straightforward to validate that r k +1 ,k +1 is the projection distance of initial mapped data  X  ( x onto the orthogonal complement space span { w 1 , w 2 ,..., w whether to accept the computed vectors w k + 1 into current basis M or not according to the value of r k +1 ,k +1 . As illustrated in Fig.1, If r k +1 ,k +1 is small, it means that the mapped data  X  ( x nearly or even completely in the space spanned by the base vec-tors span { w 1 , w 2 ,..., w k } . In this case,  X  ( x dependence on span { w 1 , w 2 ,..., w k } , and it is not reasonable to add the potential base vector w k + 1 into the basis M since w is transformed from  X  ( x k +1 ) . On the contrary, when r large,  X  ( x k +1 ) is orthogonal to span { w 1 , w 2 ,..., w that  X  ( x k +1 ) is independent of span { w 1 , w 2 ,..., w is an ideal base vector.
 Figure 1:  X  ( x k +1 ) is projected onto two orthogonal spaces: span { w 1 , w 2 ,..., w k } and span { w 1 , w 2 ,..., w jection distance onto the space span { w 1 , w 2 ,..., w r base vector. Otherwise we have to discard it.
As discussed above, to keep the independence of learned ba-sis and mapped data, we make the decision of whether to accept the computed vectors w k + 1 into basis according to the value of r k +1 ,k +1 . In KOBL, we use a threshold T to make the decision: if r k +1 ,k +1  X  T , we accept the potential vector w k + 1 versa. The value of r 11 is not smaller than r ii for all i &gt; 1 . (It will be demonstrated in Theorem 2.) Therefore, for the purpose of convenient comparison, instead of r k +1 ,k +1 , we use r make the final decision.

The learned dimensionality for the kernel space equals to the number of accepted base vectors. Therefore, the value of threshold T plays an important role in estimating the intrinsic dimensionality of kernel space and affects the orthogonality of the learned basis. If the estimated dimensionality is too small, the learned basis will have a large distinction from the kernel space. On the contrary, a too large estimated dimensionality will render it difficult in learning orthogonal base vectors. Take the Hilbert Matrix [7] for example. If we simply set up the dimensionality of the space spanned by H (100)  X  X  column vectors as 100 , many base vectors learned are not orthogonal. Under both of the two cases, using (2) to compute  X  ( x ) is not accurate, so that the performance of feature selection would be affected.

Many current subspace learning methods (e.g, KPCA and KGS) need a user to predetermine the intrinsic dimensionality for the ker-nel space. Although for KPCA and some KPCA based methods such as Kernel Feature Analysis [27], the intrinsic dimensionality for the kernel space can be determined based on the accumulation ratio whose lower bound comes from the tolerable approximation error, this lower bound has to be predetermined by users as param-eters, which may still lead to an improper dimensionality for the kernel space. On the other hand, there are some reported schemes such as Relevant Dimension Estimation(RDE) [3, 4] that can pro-vide an estimation for relevant dimensionality in the kernel space with respect to a supervised learning task. However, such schemes themselves suffer from heavy computational cost ( O ( N 3 is impractical.

In KOBL, instead of employing an user defined threshold, we aim at proposing an adaptive threshold scheme to learn the intrinsic dimensionality of kernel space without adding much extra comput-ing burden.

Besides ensuring the orthogonality of the basis, we hope the adaptive threshold scheme to guarantee that the learned basis can preserve the sketch of the kernel space, namely, the reconstruction cost E ( M ) defined as follows is small.
 Therein, w 1 , w 2 ,..., w d are all learned base vectors in basis set M . Moreover, we hope that the threshold T is confined between between 0 and 1 .)
To satisfy the above constraints, we propose a self-adaptive thresh-old: we accept the potential base vector w k + 1 if Therein, Dim ( M ) is the currently learned dimensionality of the kernel space, and f ( t ) is a monotonically increasing function and 0  X  f ( t )  X  1 when 0  X  t  X  1 .

Now, in the following theorem, we show how the threshold pol-icy defined by (20) can ensure a small reconstruction cost.
T HEOREM 1. The value of E ( M ) for OCA has an upper bound
P ROOF . Suppose finally we obtain d base vectors for the kernel space: w 1 , w 2 ,..., w d .

Therefore, for those mapped data  X  ( x j ) ( j  X  d ), we have
As for those mapped data  X  ( x j ) ( j &gt; d ), due to the non-increasing monotonicity of r jj , (which is demonstrated in Theorem 2,) we achieve the following equations,
As a result, E ( M )  X  1 N ( N  X  d ) r d +1 ,d +1 . From (20), we know that, which completes the proof.
 Since f ( t )  X  1 is a monotonically increasing function, and Dim ( M ) &lt;&lt; N , the value of r 11 f ( Dim ( M ) N It means the adaptive threshold technique ensures that the basis ob-tained by KOBL can accurately preserve the sketch of the kernel space, which meets our demand.

N + 1 vectors of N dimensionality must be linearly dependent and non-orthogonal. The number of base vectors is always smaller than the dimensionality of input data, i.e., Dim ( M )  X  N . There-fore, the value of T is confined between 0 and 1 .
 We should notice that all elements in this threshold policy, namely matically updated during the learning process. It means that the threshold scheme is self-adaptive, and costs little extra computing load.

The very threshold policy helps KOBL learn the intrinsic dimen-sionality of the kernel space and construct an orthogonal basis with small reconstruction cost.
Most subspace learning methods use all the data to learn the ba-sis, which suffers from heavy computing load when the data vol-ume is large. In KOBL, to address this problem, we propose a stop criterion based on the following theorem.
 T HEOREM 2. If j  X  k then r jj  X  r kk .

P ROOF . We denote the original data matrix A (0) = [  X  ( x  X  ( x 2 ) (0) ,..., X  ( x N ) (0) ] , where N is the number of data. In ev-ery iteration of KOBL, the data matrix is transformed. Suppose we have learned k base vectors w 1 , w 2 ,..., w k through k times iterations, then In the ( k + 1) th iteration of KOBL, we obtain the following equa-tion:
For all j &gt; k + 1 , we have, following item:
Due to the column pivoting scheme, we have for all j &gt; k + 1 , and proof.
 According to the above theorem, we develop a stop criterion in KOBL: we stop the algorithm if we arrive at the first computed vector w d satisfying the following inequality: Suppose w d is the first computed vector that does not satisfy the threshold condition, i.e., r dd r is monotone nondecreasing, and r dd r during training, as indicated in Theorem 2. Therefore, if the integer d does not satisfy r dd r threshold condition, either. It means even if we use the remaining training data to learn the basis, the potential base vectors learned later ( { w i } N i = d +1 ) do not meet the threshold condition and will be rejected. Thus, we don X  X  need to continue learning the basis any more.

Based on the stop criterion (21), KOBL is able to realize a low computing complexity. Without this stop criterion, KOBL would have to process all training data; the time complexity would reach O ( N 3 ) . By the stop criterion, the time complexity reduces to O ( N 2 d ) . Here, N is the size of training set, and d is number of base vectors. Usually d &lt;&lt; N , thus the stop criterion saves a lot of computing time, and helps KOBL outperform those typical ker-nel subspace learning methods such as KGS ( O ( N 3 ) ) and KPCA ( O ( N 3 + N 2 d ) ).

According to the above discussion, we give the detailed KOBL in Algorithm 1.
Backed by the orthogonal basis set constructed by KOBL, we can select discriminative features in the kernel space. During past few years, many methods [33, 15, 32, 36] have been proposed for feature selection in the original space. In our work, we extend For-ward Feature Selection (FFS) to select features in the kernel space due to its smaller computing load. In this part, we show how to adjust FFS, a method that was proposed to select features in the original space [36], to directly select features in the kernel space with the assistance of the orthogonal basis set learned by KOBL.
The key issue of FFS is to train weak classifiers and store the their classification results. Here, each weak classifier is trained 10: Update basis dimensionality k  X  k + 1 . 11: Goto step 4 to continue the learning process. 12: else 13: Stop the algorithm. 14: end if 15: return Basis M and Dim ( M ) .
 based on one feature. By the aid of the learned orthogonal basis set { w i } d i =1 of the kernel space, we obtain the i th mapped data  X  ( x j ) when training the weak classifiers:
In Adaboost and its variants, many supervised methods can be used to train the weak classifiers. The most commonly used method is Nearest Neighbor Classifier, which has a time complexity of O ( N 2 ) . Thus its kernel version (Kernel Nearest Neighbor Classi-fier) is O ( N 3 ) . In our work, Kernel Least Mean Square algorithm is employed because it only costs O ( N 2 d ) computing time.
The detailed KFFS is given in Algorithm 2. 10: i = the error rate of ensemble classifier under feature 11: end for 13: v = v + V ( k, :) . 15: Computed the weight of the selected feature by equation 16: end for 17: return The weight of each feature {  X  i } d i =1 .

The Kernel Forward Feature Selection (KFFS) operates as fol-lows: first, weak classifiers are trained (step 2-step 5 of Algorithm 2). Then we select features that make the ensemble classifier have smallest error on the training set (step 6 -step 13). After the fea-tures in an ensemble are selected, we update the weights of features according to the smallest error rate k (step 14-step 15): The discriminative ability of each feature is reflected by the com-puted weight. The time complexity of KFFS is O ( N 2 d ) .
In this section, we give the detailed TAKES in Algorithm 3. In the first stage, we learn an orthogonal basis for the kernel space by KOBL. Note that the intrinsic dimensionality of the feature space can be estimated by KOBL in the first stage. Then in the second stage, with the aid the basis learned in the first stage, the discrimi-native features are weighted directly in the kernel space.
As discussed above, the time complexity of constructing basis set is O ( N 2 d ) , and KFFS is O ( N 2 d ) . Thus, the total time complex-ity of the proposed TAKES is O ( N 2 d ) . Therein, N is the number of training data, and d is the dimensionality of the original data. To compare with KOBL, we analyze the time complexity of some other kernel based feature selection such as Feature Selection via KPCA (FSKPCA) and Feature Selection via KGS (FSKGS)[6], and some dimensionality reduction methods such as Kernel Discrim-inant Analysis (KDA) [22] and Generalized Discriminant Analy-sis (GDA)[1]. The results are listed in Table 1.

Table 1 indicates that the proposed TAKES outperforms the only existing kernel feature selection methods FSKPCA and FSKGS [6] as far as we know. Compared with some kernel based dimensional-ity reduction methods, such as KDA and GDA, TAKES is encour-aging to the aspects of time complexity.
To validate the effectiveness and efficiency of the proposed meth-ods, we conduct experiments under some widely used real-world databases from [2] and [11]: Australian, Diabetes, Ionosphere, Heart, Sonar, and Thyroid. Some typical kernel methods for reducing di-mensionality are compared with our TAKES. As discussed above, as far as we know, the only existing work that selects features in the kernel space is [6]. Thus, we compare our work with FSKGS and FSKPCA [6]. Kernel Discriminant Analysis (KDA), a typical kernel dimensionality reduction method, is also used to compare with our work.
In our work, we estimate the intrinsic dimensionality for the ker-nel space and construct an orthogonal basis by the proposed KOBL to conduct feature selection in the kernel space. In this part, we employ the reconstruction cost E ( M ) [24] to testify whether the learned basis can accurately preserve the sketch of the mapped data in the kernel space. d ) O ( N 3 ) O ( N 3 ) O ( N 3 + N 2 d ) Dataset KOBL KGS KPCA KDA Australian 0.044 36 0.063 345 0.412 345 0.523 36 Diabetes 0.062 58 0.082 468 0.392 468 0.496 58 Ionosphere 0.111 69 0.098 281 0.383 281 0.915 69 Thyroid 0.043 20 0.083 140 0.407 140 0.700 20
To compare with the quality of basis set constructed by our work, we use some typical kernel subspace learners such as KGS, KPCA, and KDA to compare with KOBL under some widely used databases [2, 11]. Without loss of generality, we use RBF kernel for KOBL, KGS, KPCA, and KDA respectively. It deserves noting KOBL automatically estimates the intrinsic dimensionality of the kernel space, which is listed in Table 2. But for KGS and KPCA, the di-mensionality of the kernel space needs users to predetermine. For KGS and KPCA, we take the policy adopted by [6]: the target di-mensionality is predetermined as the size of training set. For KDA, the target dimensionality needs users to set up as well. For the pur-pose of fair comparison, in KDA we set up the dimensionality for the kernel space the same as it learned by KOBL.

The results of E ( M ) listed in Table 2 indicate that under most conditions, KOBL achieves the smallest reconstruction cost com-pared with other methods. It is because for KOBL, the proper learned intrinsic dimensionality of the kernel space and the orthog-onality of constructed basis guarantee that the sketch of the kernel space is accurately preserved. For KGS, the improper predeter-mined dimensionality for the kernel space undermines the orthogo-nality of basis set. KPCA achieves higher reconstruction cost than KOBL under all datasets. It demonstrates again that setting up the number of mapped data as the kernel space dimensionality adopted by [6] is unreasonable because the too large predetermined dimen-sionality of the kernel space renders it difficult to obtain orthogonal basis, and preserve the sketch of the data accurately. The basis set constructed by KDA is not necessary orthogonal, thus it is no won-der that its reconstruction cost is poor.

From the experiments, we can conclude that, compared with other methods, KOBL constructs the basis set with the smallest reconstruction cost. It means the learned basis set by our work can preserve the sketch of data in the kernel space accurately.
In this section, we conduct experiments to validate the classifica-tion power of the proposed method. Similar to section 4.1, the six datasets, i.e., Australian, Diabetes, Ionosphere, Heart, Sonar, and Thyroid, are also used in this experiment.
In our algorithm, we design the KOBL to learn the intrinsic di-mensionality and the orthogonal basis of the kernel space (i.e., the first stage), and then adopt KFFS to directly select features in the kernel space (i.e., the second stage). In this subsection, to further validate the effectiveness of the basis constructed by KOBL, we compare the classification accuracy of TAKES with another case that the feature selection stage is the same but the basis learners are different. Namely, in this case, we use KGS to learn the basis (in the first stage), and then apply KFFS to select features in the kernel space (in the second stage).

We execute the above two methods to obtain the selected fea-tures. Then we simply use 1  X  Kernel Nearest Neighbor Classifier (KNNC) to classify data in test sets under the selected features. We depict the classification results of each method under differ-ent databases in Fig.2. In the figures, the abscissa stands for the number of selected features and the vertical ordinate stands for the corresponding classification accuracy. It deserves noting that for the proposed TAKES, the dimensionality of the kernel space is au-tomatically learned. In the stage of selection, we employ KFFS to weight all features according to their respective discriminative abilities. Therefore, the number of selected features for TAKES is data-dependent, and we only need to portray a point instead of a curve line to reflect the classification performance of TAKES.
Fig.2 dedicates that the classification accuracy of TAKES is bet-ter than the KGS+KFFS case under most databases. The feature selection method (in the second stage) used in the case study is the same, thus we can conclude the basis learned by KOBL is the best. It is because different from KGS, KOBL can estimate the intrinsic dimensionality for the kernel space, which ensures the sketch of the kernel space is accurately preserved by the learned orthogonal ba-sis and the classification accuracy is good. As for the KGS+KFFS case, the heuristically predetermined target dimensionality under-mines the orthogonality of the learned basis set, and leads to poorer classification performance.
In this part, we compare the classification accurcy of TAKES with the only existing feature selection works in the kernel space [6] as far as we know: FSKGS and FSKPCA. KDA is also used to compare with TAKES.

We execute each algorithm to obtain the selected features. Then we use 1  X  KNNC for classification under the selected features. Meanwhile, we compute the classification accuracy that is obtained merely by KNNC without dimensionality reduction or feature se-lection scheme, which reflects the classification performance of all features in the kernel space. We depict the classification results in Fig.3.

Fig. 3 indicates that TAKES can obtain sound classification per-formance compared with other typical kernel methods for reduc-ing dimensionality. Compared with KNNC, TAKES obtains bet-ter classification results under all datasets. It indicates that after feature selection, the selected features have better discriminative power than all features in the kernel space.
 The classification accuracy of the proposed TAKES outperforms FSKGS and FSKPCA under almost all the datasets. It is because that TAKES can properly estimate the intrinsic dimensionality of the kernel space and obtain orthogonal basis. Another point worth mentioning is that, compared with the classification accuracy of the classification accuracy of TAKES.
 FSKGS, KGS+KFFS (shown in the last subsection) is a little bet-ter under most datasets. (KGS+KFFS is better under the Diabetes, Heart, Ionosphere, Sonar, and Thyroid.) It demonstrates that the Kernel Forward Feature Selection (KFFS) adopted by the proposed TAKES is effective to detect discriminant features in the kernel space.

Compared to KDA, which is a kernel dimensionality reduction method that aims at obtaining a transformation matrix to convert original data into data in feature space, TAKES works better under all conditions. Although KDA improves the classification accu-racy by maximizing the between-class scatter and minimizing the within-class scatter, TAKES, which directly selects discriminative features from the kernel space, is proven to be more effective.
Then considering the fact that the time complexity of TAKES is better than many typical kernel methods for reducing dimensional-ity, our method is promising.
In this paper, we propose a framework for feature selection in the kernel space. In the first stage, we design a subspace learning method KOBL to estimate the intrinsic dimensionality and con-struct an orthogonal basis for the kernel space. The designed sub-space learning algorithm automatically learns the intrinsic dimen-curve line to show the classification accuracy of TAKES. sionality of the kernel space guaranteeing that the sketch of the kernel space is accurately preserved. In the second stage, with the aid of the learned basis set, we select features directly in the ker-nel space. The whole procedure enjoys a smaller time complexity compared with other typical kernel methods.

In the experiments, we compare the proposed TAKES frame-work with FSKGS and FSKPCA (the only existing feature selec-tion methods as far as we know), and KDA (a famous kernel di-mensionality reduction method) under several typical datasets. The results dictate that the reconstruction cost and the classification per-formance of TAKES are the best.
We would like to thank Shaohan Hu from UIUC and the anony-mous reviewers for their invaluable inputs. This work was sup-ported in part by the Fund of the National Natural Science Founda-tion of China (Grant # 60975047, 60723003, 60721002), 973 Pro-gram (2010CB327903), and Jiangsu NSF grant ( # BK2009080). [1] G. Baudat and F. Anouar. Generalized discriminant analysis [2] C. L. Blake and C. J. Merz. UCI repository of machine [3] M. L. Braun and J. M. Buhmann and K.-L. Muller.
 [4] M. L. Braun and J. M. Buhmann and K.-L. Muller. On [5] D. Cai and X. He. Orthogonal locality preserving indexing. [6] B. Cao, D. Shen, J. Sun, Q. Yang, and Z. Chen. Feature [7] M. D. Choi,  X  X ricks or treats with the hilbert matrix, X  [8] I. Dagher and R. Nachar. Face recognition using IPCA-ICA [9] P. A. Devijver and J. Kittler. Pattern Recognition: A [10] W. Dong, M. Charikar, and K. Li. Asymmetric distance [11] M. Duarte and Y. H. Hu. Vehicle classification in distributed [12] X. Geng and T.-Y. Liu and T. Qin and H. Li. Feature [13] G. H. Golub and C. F. V. Loan. Matrix Computations, 3rd [14] H. Grabner and H. Bischof. On-line boosting and vision. In [15] K. Kira and L. A. Rendell. A practical approach to feature [16] A. Kolcz, X. Sun, and J. Kalita. Efficient handling of [17] H. Li, T. Jiang, and K. Zhang. Efficient and robust feature [18] Z. Liang and T. Zhao. Feature selection for linear support [19] Y. Ma, S. Lao, E. Takikawa, and M. Kawade. Discriminant [20] S. Martin, M. Kirby, and R. Miranda. Kernel/feature [21] A. M. Martinez and A. C. Kak. PCA versus LDA. IEEE [22] S. Mika, G. Ratsch, and K. R. Muller. A mathematical [23] W. Ping, Y. Xu, K. Ren, C.-H. Chi, and F. Shen. Non-I.I.D. [24] S. T. Roweis and L. K. Saul. Nonlinear dimensionality [25] Y. Saeys and I. Inza and P. Larranaga. A review of feature [26] B. Scholkopf, A. Smola, and K. R. Muller. Nonlinear [27] A. J. Smola and O. L. Mangasarian and B. Scholkopf. parse [28] Y. Song, D. Zhou, J. Huang, I. G. Councill, H. Zha, and C. L. [29] Y. Sun. Iterative relief for feature weighting: Algorithms, [30] X. Tao, J. Ye, Q. Li, R. Janardan, and V. Cherkassky. [31] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global [32] P. Viola and M. Jones. Rapid object detection using a [33] G. Wang, F. H. Lochovsky, and Q. Yang. Feature selection [34] L. Wang, M. Sugiyama, Z. H. Zhou, and J. Feng. On the [35] J. Weston, A. Elisseeff, and B. Scholkopf. Use of zero-norm [36] J. Wu, S. C. Brubaker, M. D. Mullin, and J. M. Rehg. Fast [37] Y. Xu. Orthogonal component analysis for dimensionality [38] Y. Xu, S. Furao, J. Zhao, and O. Hasegawa. To obtain [39] J. Yan, B. Zhang, S. Yan, Q. Yang, H. Li, and Z. Chen. [40] Y. Yang and J. O. Pedersen. A comparative study on feature
