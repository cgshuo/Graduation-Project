
The notion of relations is extremely important in math-ematics. In this paper, we use relations to describe the embedding problem and propose a novel stochastic rela-tional model for nonlinear embedding. Given some relation among points in a high-dimensional space, we start from preserving the same relation in a low embedded space and model the relation as probabilistic distributions over these two spaces, respectively. We illustrate that the stochastic neighbor embedding and the Gaussian process latent vari-able model can be derived from our relational model. More-over we devise a new stochastic embedding model and refer to it as Bernoulli relational embedding (BRE). BRE X  X  ability in nonlinear dimensionality reduction is illustrated on a set of synthetic data and collections of bitmaps of handwritten digits and face images.
Dimensionality reduction is an important issue in ma-chine learning and pattern recognition. Principal compo-nent analysis (PCA) and multidimensional scaling (MDS) are two important linear techniques for it. Recently, non-linear dimensionality reduction techniques have become in-creasingly popular for visualization and other applications. Most of the nonlinear dimensionality reduction techniques typically begin with an affinity matrix between pairwise in-stances or variates and are followed by an eigendecompo-sition process. Thus, they are also referred to as spectral embedding methods [3, 4]. Along this line, Tenenbaum et al. [15] presented ISOMAP by using the geodesic distance on manifolds. Roweis and Saul [13] proposed locally lin-ear embedding (LLE) by preserving local metric informa-tion in the original instances. Other representative work includes kernel-based techniques, such as the kernel PCA [14], Laplacian eigenmaps [1] and kernel eigenmaps [2].
Recently, Hinton and Roweis [7] proposed a probabilis-tic model called stochastic neighbor embedding (SNE), in which a probabilistic distribution over all the potential neighbors of points in a high-dimensional space is defined under Gaussian assumptions and this distribution is then approximated in a low-dimensional space. A generaliza-tion of SNE, called multiple relational embedding (MRE) [10], has been developed by incorporating multiple differ-ent types of similarity relations. Alternatively, a genera-tive model, called Gaussian process latent variable model (GPLVM), has been proposed by Lawrence [9]. GPLVM uses a Gaussian process prior as a mapping from a latent space to a feature space induced by a kernel. Thus, it is a kernelized probabilistic model.

In this paper, we propose a new stochastic framework for nonlinear embedding. We use the  X  X elation X , an extremely important notion in mathematics, to describe the embed-ding problem and then model the relation as a distribution. Given some relation among points in a high-dimensional space, we seek to obtain corresponding coordinates in a low-dimensional space by preserving relational identities before and after embedding. Further, we introduce a set of latent variables to characterize the relation. The latent variables are defined as two sets of distributions over the high and low dimensional spaces, respectively. Minimizing the Kullback-Leibler (KL) divergence between the two sets of distributions is used as an embedding criterion. Conse-quently, the low-dimensional embeddings can be obtained via minimizing the KL divergence with a numerical opti-mization method, such as the scaled conjugate gradient al-gorithm. More interestingly, if multinomial distributions and Gaussian distributions are used as the priors of the la-tent variables, we obtain respectively SNE of Hinton and Roweis [7] and GPLVM, also called  X  X win kernel PCA X , of Lawrence [9]. In this paper, we employ Bernoulli dis-tribution priors and induce a new stochastic nonlinear em-bedding model, which is referred to as Bernoulli relational embedding (BRE).

It is worthy to note that  X  X elation X  [8] represents an ab-stract mathematical concept. For practical applications in machine learning and pattern recognition, however, we are usually concerned with several concrete  X  X elations X , such as adjacency in a graph, neighbor of a position, similarity within a class, etc. Indeed, the neighborhood relation and the adjacency relation have widely been used in the existing unsupervised embedding methods [13, 15, 1, 2, 7], while the class relation has also been used in supervised methods [5, 6]. Interestingly, if we concretely consider the neighbor relation in BRE and resort to a first-order numerical opti-mization method for embedding, BRE can be regarded as a probabilistic version of LLE.

The rest of this paper is organized as follows. Section 2 outlines a generic stochastic model for nonlinear embed-ding and accordingly re-derives SNE and the twin kernel PCA. Section 3 presents our BRE model and its implemen-tations. The experiments on the synthetic and real dataset are then presented in Section 4. Finally, some concluding remarks are given in Section 5.
Given a set of k high-dimensional points X = { x 1 , x 2 ,..., x k } X  R n , our objective is to embed them into a low-dimensional space R m where m&lt;n .Let Y = { y 1 , y 2 ,..., y k } be the set of embedded coordinates corresponding to X . Consider a binary relation R [8] be-tween elements of X .If R X  X  X X and ( x i , x j )  X  X  , we say x i is in relation R with x j , and denoted x i  X  x for convenience. If x i is not in relation R with x j , we de-note this as x i x j . Although, relations are an abstract concept in mathematics, for our purpose we are especially concerned with some concrete types such as adjacency re-lation , neighborhood relation or class relation . That is,
R a = { ( x i , x j ) | x i is adjacent with x j inagraph
R n = { ( x i , x j ) | x i is a neighbor of x j in position
R c = { ( x i , x j ) | x i and x j belong to the same class All pairwise relations between elements over X are rep-resented by the affinity matrix R k  X  k =[ r ij ] , and over are represented by the affinity matrix S k  X  k =[ s ij ] , where r ( s ij ) encodes the degree of such a relation from x i ( to Our first step is to define the same relation on X and Y . Moreover, we assume that x i  X  x j if and only if y i  X  y In other words, we seek to preserve the relation R before and after embedding. From now on, we denote i  X  j when either x i  X  x j or y i  X  y j . Our second step is to introduce a set of latent variables l to represent the relation R for each pair of points i and j . l can be either continuous or discrete. Each possible value of the latent variables encodes some re-lation among the elements. We model the latent variables l as distributions over X and Y , respectively. Based on dif-ferent assumptions, the value ranges and the distributions of the latent variables will be different. The distribution of l over X depends on R , and the distribution of l over Y depends on S . Since the data in high dimensional space is fixed, the distribution p ( l | R ) is already known. By tuning the coordinates of elements in Y , the affinity matrix S is changed correspondingly. We use the KL divergence min-imization between two distributions over X and Y as an embedding criterion to reduce the difference of the affinity relations between R and S . When the affinity matrices R and S are the same, all pairwise relations over X are the same as those over Y , and therefore the divergence is zero. Consequently, the low dimensional embedding can be ob-tained via minimizing the KL divergence with a numerical optimization method. The structure of our stochastic model is clarified in Figure 1.

Figure 1. The structure of the stochastic model. The distribution of the latent variable p is over X , and the distribution of the latent variable q is over Y . The KL divergence be-tween p and q is to be minimized as the crite-rion for embedding.

We now illustrate that both SNE [7] and the twin ker-nel PCA [9] can be derived from our stochastic relational model in terms of the different prior distributions of l . First, we assume that the latent variable l i =( l i 1 ,...,l ik ( k  X  1) vector (other than l ii ) for the point i , and l i ues all zero except a 1 in one entry. We model l i as multino-mial distributions on X and Y , respectively. Then we have p ( and s il are, respectively, defined over X and Y subject to divergence between p ( l i | r i ) and q ( l i | s i ) as Since l i is defined only for point i , the overall latent vari-ally independent, we derive SNE, which obtains y i through minimizing The latent variable l i has an explicit physical meaning. Namely, if l ij =1 , the point i will pick j as its neighbor. Figure 2 shows the structure of the SNE model for the point i .
Figure 2. The structure of the SNE model for the point i . The latent variable l i isa( k  X  1 )-vector. r i and s i characterize the relations from point i to the rest of the points. There are There are k such figures for all points.

On the other hand, we can assume that the latent variable l is a k dimensional continuous vector and follows Gaus-sian distributions on both X or Y . Specifically, p ( l | R N ( 0 , R ) and q ( l | S )= N ( 0 , S ) , where R and S are positive definite matrices (also called kernel matrices) over X and Y , respectively. In this case,
KL q ( l | S ) p ( l | R ) = 1 This leads us to the twin kernel PCA [9]. The structure of the GPLVM model is illustrated in Figure 3.

When gradient descent methods, such as the scaled con-jugate gradient (SCG) algorithm, [11] are used, the twin kernel PCA has higher computational costs than SNE be-cause the former requires inverting a kernel matrix during each iteration, although it can be sped up through sparsifi-cation techniques (see [9] for more details). Recall that the multinomial latent variable l i has an explicit physical mean-ing. However, since l i has only one element with value 1, this implies that point i is related to one and only one point for one latent value. From this perspective, the constraint Figure 3. The structure of the GPLVM model.

The latent variable l is a k vector and its distri-butions depend on R or S , which are symmet-ric positive matrices. The bi-directional line indicates the relation from i to j is the same as the relation from j to i of SNE is a little strong. For example, consider the adja-cent relation in a graph, where each vertex may have sev-eral adjacent vertices. In the next section, we shall relax this constraint and develop a new embedding model.
As we stated, in the SNE model, l ij =1 implies i  X  j , while l ij =0 implies i j . This motivates us to directly model l ij  X  X  ( i = j ) as independent Bernoulli distributions . Accordingly, we form a set of independent Bernoulli distri-butions, { l ij | i, j =1 , 2 ,...,k and i = j } . Specifically, we define with r ij  X  [0 , 1] is a relational coefficient between x x j on X , while with s ij  X  [0 , 1] is a relational coefficient between y y j on Y . We refer to this embedding model as Bernoulli relational embedding (BRE), where r ij is pre-specified as afunctionof x i and x j , and s ij is a link function between unknown y i and y j . It is easy to obtain the KL divergence between p ( l ij | r ij ) and q ( l ij | s ij ) as KL ( p ( l ij | r ij ) q ( l ij | s ij )) = r ij log Figure 4 represents the Bernoulli relational model, which encodes the relation from point i to point j .
Figure 4. The structure of the Bernoulli re-lational model for the pairwise relation from point i to j . l ij is a binary variable. It is permis-sible that r ij = r ji and s ij = s ji , hence there is a uni-directional line from i to j . r ij  X  [0 , 1] and s ij  X  [0 , 1] . Since there are k  X  ( k  X  1) pairwise relations from one point to another, the overall Bernoulli relational model con-tains k  X  ( k  X  1) such figures.

Furthermore, we assume the r ij and s ij share some com-mon attributes. That is, Assumption (1) shows that the self relation exist in BRE. The goal is to make BRE consistent with other models. In fact, we do not need to use the self relation. The goal of Assumption (1) makes BRE consistent. Assumption (2) is necessary because it ensures the KL is well-defined due to 0log 0 0 =0 . Note that it is permissible that r ij = r ji independent observations { l ij | i, j =1 , 2 ,...,k and i j } from the Bernoulli distributions with the parameters r  X  X , we seek to find the coordinates of the y i  X  X  through minimizing the following loss function
L ( { y i } )= 1
Let R =[ r ij ] and S =[ s ij ] be affinity matrices. The latent variable in BRE and SNE is a matrix, where each el-ement l ij represents the relation R from point i to point In BRE, l ij ( i, j =1 ,...,k and i = j ) is a binary variable, and the l ij s are independent of each other. BRE minimizes the similarity between corresponding elements of R and S in the case that their elements are in [0 , 1] .InSNE,the ( k  X  1) dimensional latent vector l i encodes the relations from point i to the rest of the ( k  X  1 ) points. Since there are constraints that the row summation of the affinity ma-trices R and S are 1, the ( k  X  1 ) relations from point i coupled together. SNE minimizes the similarity between corresponding rows of R and S in the case that their rows are normalized. The latent variable in GPLVM is a contin-uous k dimensional vector. GPLVM directly minimizes the similarity between R and S in the case that they are positive definite.
The most direct approach is to compute each entry of the affinity matrix by each pairwise distance. We employ the Gaussian kernel to define r ij ( i = j ), e.g., where  X &gt; 0 is a width parameter. Similarly, we can define s where  X &gt; 0 is also a width parameter. It is clear that s ) is translation-invariant in x (or y ). Moreover, the def-initions of r ij and s ij are consistent for the above assump-tions. This affinity definition tries to map nearby points in the original space to nearby points in the embedding and faraway points to faraway points. Therefore, it tends to give a more faithful representation of the data X  X  global and local structure in the embedding. With the r ij and s ij , our cur-rent problem becomes finding the coordinates of the y i  X  X  via of
L ( { y i } ) w.r.t. { y i } is To update y i , we only use relevant relations r ij ( s ij 1 , ..., k ) from point i . Hence, compared with the gradient in SNE [7] where one update needs to compute all pairwise relations, BRE costs less time.
 We can resort to the SCG algorithm to obtain the y i  X  X . Specifically, we treat the minimization of L ( { y i } ) as a multi-parameter optimization problem and employ a condi-tional SCG algorithm with parallel-update scheme. That is, L ( y 1 ( t ) ,..., y i  X  1 ( t ) , y i , y i +1 ( t ) ,..., y can be adaptively estimated by using SCG, we let  X  =1 in the following experiments for computational simplicity .
In many cases, the high dimensional data may have its own intrinsic structure on a low dimensional manifold. The two points, which are closed in the original space, may be faraway on the intrinsic manifold. As such, embedding needs to be optimized only to preserve the local configura-tions of nearest neighbors like LLE. Similarly, other relation information could be used to guide the distance measure-ment such as class label, side information [16], etc. Since BRE seeks to propagate some relation among x i  X  X  in R n to y i  X  X  in R m , it is natural to utilize this relation to define r . Here we present several effective methods to choose r . One candidate is The affinity matrix R =[ r ij ] defined by (5) possesses some nice discriminant properties. Firstly, its induced distance is metric 1 . In addition, the affinity between any two points with the same relation (e.g., in the same neighborhood) will never be larger than that between any two points with different relations (e.g., in different neighborhoods). The larger the Euclidean distance between points, the smaller the within-relation affinity while the larger the between-relation affinity.
 Another candidate comes from a truncated version of the Gaussian kernel To match Assumption (3), we define s ij as The affinity matrix R =[ r ij ] defined by (6) is a positive semi-definite matrix, so its induced distance is Euclidean. If we consider the neighborhood relation, this definition shows that the affinity between any two points x i and x j belonging to different neighborhoods is zero. Furthermore, the affinity between y i and y j corresponding to x i and also zero. Thus, BRE can preserve the local neighborhood relation before and after embedding like LLE [13, 12]. In addition, the conditional SCG algorithm that we use works in a first-order gradient iterative setting. The updating equa-tion in the gradient method is y ( t +1) = y i ( t )  X   X  This equation illustrates that a low-dimensional embedding, e.g., y i , is a linear combination of other low-dimensional embeddings, y j  X  X , within the same neighborhood with y i Thus, BRE can be regarded as a probabilistic version of LLE.
In this section, we illustrate BRE X  X  ability in modeling local neighborhood relations by producing two dimensional visualization of data. We firstly generate 2000 points on each of surfaces of the S-curve and the twin peaks in three dimensional space. In these two manifolds, two nearby points in the original space may be faraway on the manifold surface. Hence, the obtained embedding should not pre-serve a faithful representation for every pairwise distance. Instead, each point in the embedding tries to keep the same neighbors as those in the original space. Therefore, we han-dle the relation by the neighborhood concept and use equa-tions (6) and (7) to define the affinity matrices R and S The embedding results are shown in Figure 5. PCA is used to initialize the coordinates of the iteration. Since PCA is a kind of linear mapping, it generally projects the points of the different area of the manifold onto the same region. It is not a good dimensionality reduction mapping method as illustrated in the figure. Although points are arranged as in PCA at the beginning of the iteration, the BRE model can coordinate the points to keep the original neighborhood re-lations as shown in Figure 5.

We also apply BRE to a collection of bitmaps of hand-written digits (USPS) and a set of face images from a video sequence, all of which have been used in [12, 7, 9]. These two high-dimensional datasets are likely to have in-trinsic discriminant structures in lower dimensional spaces. Here we explore the neighborhood relation between pair-wise points. The low-dimensional embeddings are obtained via SCG with a maximum number of 200 iterations and ini-tializations based on the standard linear PCA.

We follow the settings in [7] to randomly choose a subset of 3000 of the digits 0-4 (600 for each digit) from a 16  X  grayscale version of the USPS database. We adopt (5) with  X  = 500 for r ij and (4) with  X  =1 for s ij . Moreover, we define i  X  j if x i  X  x j 2  X  2  X  , and i j otherwise. Figure 6 illustrates a 2-D visualization on this dataset after having implemented BRE.

We also apply BRE to Brendan X  X  face dataset, which consists of 1965 grayscale images at 28  X  20 resolution. Each image contains one face with a certain pose, expres-sion and lightness. For this dataset, we adopt (6) with  X  = 200 for r ij and (7) with  X  =1 for s ij . In addition, we determine i  X  j if and only if x i belongs to K nearest neighborhoods of x j . In the experiment, we set K = 200 . Figure 7 shows our results where the faces are embedded into a two dimensional space. As we can see, some at-tributes of the images change smoothly across this embed-ded space, e.g., the images in the lower part are brighter than those in the upper part. Moreover, BRE seems to clus-ter these faces in terms of the features of pose, expression and lightness. We enlarge four small regions in Figure 7, each of which depicts the images in a local area of the em-bedded space for better visualization. As can be seen, the images in the same local region possess similar attributes (see the illustrations in Figure 7).
In this paper, we have proposed a new method for nonlin-ear embedding problems. This method is motivated by the need to maintain some relational identities between a high-dimensional space and a low-dimensional embedded space and introduces stochastic latent variables to describe the re-lation over these two spaces, respectively. With different prior distributions of the latent variables, we can construct different embedding methods. For example, the multino-mial, Gaussian and Bernoulli distributions lead us to SNE, the twin kernel PCA and BRE, respectively. Naturally, this motivates us to develop new embedding methods in the fu-ture along this line.

BRE not only has an explicit physical meaning, but has a low computational cost as well, especially when the num-ber of points, k , is very large. Since like SNE, BRE uses an iterative optimization process, it avoids either spectral de-composition on a k  X  k matrix required by ISOMAP and KPCA, or inverting a k  X  k matrix required by GPLVM. We have applied BRE to the embedding of the USPS digit dataset and Brendan X  X  face dataset, in which we explored the  X  X eighborhood relation X  between pairwise points. Ex-perimental results show that BRE is efficient for nonlinear embedding. It is readily applicable to the multiclass learn-ing problem [5, 6] by considering the  X  X lass relation X . In-spired by MRE of Memisevic and Hinton [10], we can also use multiple types of relations in BRE according to the na-ture of the problem at hand.
 (c) Images in the center sub-block.
 Brendan smiles. (e) Images in the right sub-block. Bren-dan turns his face to the right.

