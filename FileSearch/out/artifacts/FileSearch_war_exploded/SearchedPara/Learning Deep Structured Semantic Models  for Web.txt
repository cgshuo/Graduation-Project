 {xiaohe, jfgao, deng, alexac, lheck}@microsoft.com Latent semantic models, such as LS A, intend to map a query to its relevant documents at the sema ntic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance be tween them. The proposed deep structured semantic models ar e discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthr ough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model signi ficantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation Deep Learning, Semantic Model, Clickthrough Data, Web Search Modern search engines retrie ve Web documents mainly by matching keywords in documents with those in search queries. However, lexical matching can be inaccurate due to the fact that a concept is often expressed us ing different vocabularies and language styles in doc uments and queries. (LSA) are able to map a query to its relevant documents at the semantic level where lexical matching often fails (e.g., [6][15][2][8][21]). These latent semantic models address the language discrepancy between Web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. Thus, a query and a document, represented as two vectors in the lower-dimensional semantic space, can still have a high similarity score even if they do not share any term. Extending from LSA, probabilistic topic models such as probabilistic LSA (PLSA) and Latent Dirichlet Allocation (LDA) have also been proposed for semant ic matching [15][2]. However, these models are often trained in an unsupervised manner using an objective function that is only loos ely coupled with the evaluation metric for the retrieval task. Thus the performance of these models on Web search tasks is not as good as originally expected. the aforementioned latent semantic models, which will be briefly reviewed below. their clicked documents, is exploite d for semantic m odeling so as to bridge the language discrepa ncy between search queries and Web documents [9][10]. For exampl e, Gao et al. [10] propose the use of Bi-Lingual Topic Models (BLTMs) and linear Discriminative Projection Models (DPMs) for query-document matching at the sema ntic level. These models are trained on clickthrough data using objectives that tailor to the document ranking task. More specifically, BLTM is a generative model which requires that a query and its clicked documents not only share the same distribution over topics, but also contain similar factions of words assigned to each topic. In contrast, the DPM is learned using the S2Net algorithm [26] that follows the pairwise learning-to-rank paradigm outlined in [3]. After projecting term vectors of queries and documents into concept vectors in a low-dimensional semant ic space, the concept vectors of the query and its clicked documents have a smaller distance than that of the query and its unclicked documents. Ga o et al. [10] report that both BLTM and DPM outperform significa ntly the unsupervised latent semantic models, including LS A and PLSA, in the document ranking task. However, the training of BLTM, though using clickthrough data, is to maximize a log-likelihood criterion which is sub-optimal for the evaluation metric for document ranking. On the other hand, the training of DPM involves larg e-scale matrix multiplications. The sizes of these matrices often grow quickly with the vocabulary size, which could be of an order of millions in Web search tasks. In order to make the training time tolerable, the vocabulary was pruned aggressive ly. Although a small vocabulary makes the models trainable, it leads to suboptimal performance. extended the semantic modeling using deep auto-encoders [22]. They demonstrated that hierarchical semantic structure embedded in the query and the document can be extracted via deep learning. Superior performance to the conventional LSA is reported [22]. However, the deep learning approach they used still adopts an unsupervised learning method where the model parameters are optimized for the reconstruction of the documents rather than for differentiating the relevant docume nts from the irrelevant ones for a given query. As a result, th e deep learning models do not significantly outperform the base line retrieval models based on keyword matching. Moreover, the semantic hashing model also faces the scalability challenge regarding large-scale matrix multiplication. We will show in this paper that the capability of learning semantic models with large vocabularies is crucial to obtain good results in real-world Web search tasks. above, we propose a series of Deep Structured Semantic Models (DSSM) for Web search. More specifically, our best model uses a deep neural network (DNN) to ra nk a set of documents for a given query as follows. First, a non-linear projection is performed to map the query and the documents to a common semantic space. Then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. The neural netw ork models are discriminatively trained using the clickthrough data such that the conditional likelihood of the clicked document given the query is maximized. Different from the previous late nt semantic models that are learned in an unsupervised fash ion, our models are optimized directly for Web document ranki ng, and thus give superior performance, as we will show shor tly. Furthermore, to deal with large vocabularies, we propose the so-called word hashing method, through which the high-dimensional term vectors of queries or documents are projec ted to low-dimensional letter based n -gram vectors with little information loss. In our experiments, we show that, by adding this extra layer of representation in semantic models , word hashing enables us to learn discriminatively the semantic models with large vocabularies, which are essential for Web search. We evaluated the proposed DSSMs on a Web document ranking task using a real-world data set. The resu lts show that our best model outperforms all the competing met hods with a significant margin of 2.5-4.3% in NDCG@1. Section 3 describes our DSSM for Web search. Section 4 presents the experiments, and Secti on 5 concludes the paper. Our work is based on two recent ex tensions to the latent semantic models for IR. The first is the exploration of the clickthrough data for learning latent semantic mode ls in a supervised fashion [10]. The second is the introduction of deep learning methods for semantic modeling [22]. The use of latent semantic mode ls for query-document matching is a long-standing research topi c in the IR community. Popular models can be grouped into two categories, linear projection models and generative topic models, which we will review in turn. [6]. By using the singular va lue decomposition (SVD) of a document-term matrix, a document (or a query) can be mapped to a low-dimensional concept vector  X   X   X  X   X   X  , where the  X  is the projection matrix. In document search, the relevance score between a query and a document, represented respectively by term vectors  X  and  X  , is assumed to be proportional to their cosine similarity score of the corresponding concept vectors  X  according to the projection matrix  X  trained on clicked query-document pairs provide an alternative approach to semantic matching [9]. Unlike latent semantic models, the translation-based approach learns translation relationships directly between a term in a do cument and a term in a query. Recent studies show that given large amounts of clickthrough data for training, this approach can be very effective [9][10]. We will also compare our approach with translation models experimentally as re ported in Section 4. Recently, deep learning methods have been successfully applied to a variety of language and information retrieva l applications architectures, deep learning techniques are able to discover from training data the hidden structures and features at different levels of abstractions useful for the tasks. In [22] Salakhutdinov and Hinton extended the LSA model by using a deep network (auto-encoder) to discover the hierarchic al semantic structure embedded in the query and the document. They proposed a semantic hashing (SH) method which uses bottleneck features learned from the deep auto-encoder for information retrieval. These deep models are learned in two stages. First, a stack of generative models (i.e., the restricted Boltzmann machine) are learned to map a term vector representation of a document layer-by-layer to a low-dimensional semantic concep t vector. Second, the model parameters are fine-tuned so as to minimize the cross entropy error between the original term vector of the document and the reconstructed term vector. The intermediate layer activations are used as features (i.e., bottleneck) for document ranking. Their evaluation shows that the SH approach achieves a superior document retrieval performance to the LSA. However, SH suffers from two problems, and cannot out perform the standard lexical matching based retrieval model (e.g ., cosine similarity using TF-IDF term weighting). The first problem is that the model parameters are optimized for the reconstruction of the document term vectors rather than for diffe rentiating the relevant documents from the irrelevant ones for a gi ven query. Second, in order to make the computational cost manageable, the term vectors of documents consist of only the most-frequent 2000 words. In the next section, we will show our solutions to these two problems. The typical DNN architecture we have developed for mapping the raw text features into the features in a semantic space is shown in Fig. 1. The input (raw text features) to the DNN is a high-dimensional term vector, e.g., raw counts of terms in a query or a document without normalization, and the output of the DNN is a concept vector in a low-dimensiona l semantic feature space. This DNN model is used for Web documen t ranking as follows: 1) to map term vectors to their corresponding semantic concept vectors; 2) to compute the relevance score between a document and a query as cosine similarity of their corresponding semantic concept vectors; rf. Eq. (3) to (5). the output vector,  X   X   X 1 X  X 1,..., X  , , as the intermediate hidden we have where we use the  X  X  X  X  as the activation function at the output layer and the hidden layers  X   X   X 1 X  X 2,..., X , : The semantic relevance score between a query  X  and a document  X  is then measured as: where  X   X  and  X   X  are the concept vectors of the query and the document, respectively. In Web search, given the query, the documents are sorted by their semantic relevance scores. viewed as the raw bag-of-words features in IR, is identical to that of the vocabulary that is used for indexing the Web document collection. The vocabulary size is usually very large in real-world Web search tasks. Therefore, when using term vector as the input, the size of the input layer of the neural network would be unmanageable for inference and model training. To address this problem, we have developed a me thod called  X  X ord hashing X  for the first layer of the DNN, as i ndicated in the lower portion of Figure 1. This layer consists of only linear hidden units in which the weight matrix of a very la rge size is not learned. In the following section, we describe th e word hashing method in detail. The word hashing method descri bed here aims to reduce the dimensionality of the bag-of-words term vectors. It is based on letter n -gram, and is a new method developed especially for our task. Given a word (e.g. good ), we first add word starting and ending marks to the word (e.g. #good# ). Then, we break the word into letter n -grams (e.g. letter trigrams: #go , goo , ood , od# ). Finally, the word is represen ted using a vector of letter n -grams. words could have the same letter n -gram vector representation. Table 1 shows some statistics of word hashing on two vocabularies. Compared with the original size of the one-hot vector, word hashing allows us to represent a query or a document using a vector with much lowe r dimensionality. Take the 40K-word vocabulary as an example. Each word can be represented by a 10,306-dimentional vector using letter trigrams, giving a four-fold dimensionality reduction with few collisions. The reduction of dimensionality is even more si gnificant when the technique is applied to a larger vocabulary. As shown in Table 1, each word in the 500K-word vocabulary can be represented by a 30,621 dimensional vector usin g letter trigrams, a reduction of 16-fold in dimensionality with a negligib le collision rate of 0.0044% (22/500,000). number of letter n -grams in English (or other similar languages) is often limited. Moreover, word hashing is able to map the morphological variations of the same word to the points that are close to each other in the letter n -gram space. More importantly, while a word unseen in the training set always causes difficulties in word-based representations, it is not the case where the letter n -gram based representation is us ed. The only risk is the minor representation collision as quantifie d in Table 1. Thus, letter n-gram based word hashing is robust to the out-of-vocabulary problem, allowing us to scale up the DNN solution to the Web search tasks where extremely large vocabularies are desirable. We will demonstrate the benefit of the technique in Section 4. can be viewed as a fixed (i.e., non-adaptive) linear transformation, through which an term vector in the input layer is projected to a letter n -gram vector in the next layer higher up, as shown in Figure 1. Since the letter n -gram vector is of a much lower dimensionality, DNN learning can be carried out effectively. Word Size 40k 1107 18 10306 2 500k 1607 1192 30621 22 Table 1: Word hashing token size and collision numbers as a function of the vocabulary size and th e type of letter ngrams. The clickthrough logs consist of a list of queries and their clicked documents. We assume that a query is relevant, at least partially, to the documents that are clicked on for that query. Inspired by the discriminative training approaches in speech and language processing, we thus propose a supe rvised training method to learn our model parameters, i.e., the weight matrices  X  vectors  X   X  in our neural network as the essential part of the DSSM, so as to maximize the co nditional likelihood of the clicked documents given the queries. given a query from the semantic relevance score between them through a softmax function where  X  is a smoothing factor in the softmax function, which is set empirically on a held-out data set in our experiment.  X  denotes the set of candidate docume nts to be ranked. Ideally,  X  should contain all possible documents. In practice, for each (query, clicked-document) pair, denoted by  X , X  X   X   X  where  X  is a query and  X   X  is the clicked document, we approximate D by including  X   X  and four randomly selected un clicked documents, denote by  X  X   X   X 1,...,4 X  X ; . In our pilot study, we do not observe any significant difference when diff erent sampling strategies were used to select the unclicked documents. the likelihood of the clicked docu ments given the queries across the training set. Equivalently, we need to minimize the following loss function where  X  denotes the parameter set of the neural networks  X  X  Since  X  X  X  X  X  is differentiable w.r.t. to  X  , the model is trained readily using gradient-based numerical optimization algorithms. The detailed derivation is omitted due to the space limitation. To determine the training parameters and to avoid over-fitting, we divided the clickthrough data into two sets that do not overlap, called training and validation datasets, respectively. In our experiments, the models are tr ained on the training set and the training parameters are optimized on the validation dataset. For the DNN experiments, we used the architecture with three hidden layers as shown in Figure 1. The first hidden layer is the word hashing layer containing about 30k nodes (e.g., the size of the letter-trigrams as shown in Tabl e 1). The next two hidden layers have 300 hidden nodes each, and the output layer has 128 nodes. Word hashing is based on a fixed projection matrix. The similarity measure is based on the output la yer with the dimensionality of 128. Following [20], we initialize the network weights with between  X   X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X /6 and  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X /6 where  X  X  X  X  X  and  X  X  X  X  X  X  are the number of input and output units, respectively. Empirically, we have not observed better performance by doing layer-wise pre-training. In the training stage, we optimize the model usi ng mini-batch based stochastic gradient descent (SGD). Each mini-batch consists of 1024 training samples. We observed that the DNN training usually converges within 20 epochs (passes) over the entire training data. We evaluated the DSSM, proposed in Section 3, on the Web document ranking task using a real-world data set. In this section, we first describe the data set on which the models are evaluated. Then, we compare the performances of our best model against other state of the art ranking models. We also investigate the break-down impact of the techni ques proposed in Section 3. We have evaluated the retrieval models on a large-scale real world data set, called the evaluation data set henceforth. The evaluation data set contains 16,510 Englis h queries sampled from one-year query log files of a commercial search engine. On average, each query is associated with 15 Web documents (URLs). Each query-title pair has a relevance label. The label is human generated and is on a 5-level relevance scale, 0 to 4, where level 4 means that the document is the most relevant to query  X  and 0 means  X  is not relevant to  X  . All the queries and documents are preprocessed such that the text is white-space tokenized and lowercased, numbers are retained, and no stemming/inflection is performed. models, and linear projection m odels) contain many free hyper-parameters that must be estimated empirically. In all experiments, we have used a 2-fold cross validation: A set of results on one half of the data is obtained using the parameter settings optimized on the other half, and the global retrieval results are combined from the two sets. been measured by mean Normalized Discounted Cumulative Gain (NDCG) [17], and we will report NDCG scores at truncation levels 1, 3, and 10 in this section. We have also performed a significance test using the paired t-test. Differences are considered statistically significant when the p -value is less than 0.05. titles of the documents clicked on for that query. We extracted large amounts of the query-title pairs for model training from one year query log files using a procedure similar to [11]. Some previous studies, e.g., [24][11], s howed that the query click field, when it is valid, is the most effective piece of information for Web search, seconded by the ti tle field. However, click information is unavailable for many URLs, especially new URLs and tail URLs, leaving their click fields invalid (i.e., the field is either empty or unreliable because of sparseness). In this study, we assume that each document contained in the evaluation data set is either a new URL or a tail URL, thus has no click information (i.e., its click field is invalid). Our research goal is to investigate how to learn the latent semantic models from the popular URLs that have rich click information, and apply the models to improve the retrieval of those tail or new URLs. To this end, in our experiments only the title fields of the Web documents are used for ranking. For training latent semantic models, we use a randomly sampled subset of approximately 100 million pairs whose documents are popular and have rich click information. We then test trained models in ranking the documents in the evaluation data set containing no click information. The query-title pairs are pre-processed in the same way as the evaluation data to ensure uniformity. The main results of our experime nts are summarized in Table 2, where we compared our best version of the DSSM (Row 12) with three sets of baseline models. The first set of baselines includes a couple of widely used lexical matching methods such as TF-IDF (Row 1) and BM25 (Row 2). The second is a word translation model (WTM in Row 3) which is intended to directly address the query-document language discrepa ncy problem by learning a lexical mapping between query words and document words [9][10]. The third includes a set of state-of-the-art latent semantic models which are learned eith er on documents only in an unsupervised manner (LSA, PLSA, DAE as in Rows 4 to 6) or on clickthrough data in a supervis ed way (BLTM-PR, DPM, as in Rows 7 and 8). In order to make the results comparable, we re-implement these models following th e descriptions in [10], e.g., models of LSA and DPM are trai ned using a 40k-word vocabulary due to the model complexity cons traint, and the other models are trained using a 500K-word vocabulary. Details are elaborated in the following paragraphs. and queries are represented as term vectors with TF-IDF term weighting. The documents are ranked by the cosine similarity between the query and document vectors. We also use BM25 (Row 2) ranking model as one of our baselines. Both TF-IDF and BM25 are state-of-the-art documen t ranking models based on term matching. They have been widely used as baselines in related studies. model described in [9], listed he re for comparison. We see that WTM outperforms both baselines (TF-IDF and BM25) significantly, confirming the conclusion reached in [9]. LSA (Row 4) is our implementation of latent semantic analysis model. We used PCA instead of SVD to compute the linear projection matrix. Queries and titles are treated as separate documents, the pair information from the clickthrough data was not used in this model. PLSA (Rows 5) is our impl ementation of the model proposed in [15], and was trained on documents only (i.e., the title side of the query-title pairs). Di fferent from [15], our version of PLSA was learned using MAP estimation as in [10]. DAE (Row 6) is our implementation of the deep auto-encoder based semantic hashing model proposed by Salakhutdinov and Hinton in [22]. Due to the model training complexity, the input term vector is based on a 40k-word vocabulary. The DAE architecture contains four hidden layers, each of which has 300 nodes, and a bottleneck layer in the middle which has 128 nodes. The model is trained on documents only in an unsupervis ed manner. In the fine-tuning stage, we used cross-entropy error as training criteria. The central layer activations are used as features for the computation of cosine similarity between query and docu ment. Our results are consistent with previous results reported in [22]. The DNN based latent semantic model outperforms the li near projection model (e.g., LSA). However, both LSA a nd DAE are trained in an unsupervised fashion on document collection only, thus cannot outperform the state-of-the-art lexical matching ranking models. versions of the bilingual topic m odels described in [10]. BLTM with posterior regularization (BLT M-PR) is trained on query-title pairs using the EM algorithm with a constraint enforcing the paired query and title to have sa me fractions of terms assigned to each hidden topic. DPM (Row 8) is the linear discriminative projection model proposed in [10], where the projection matrix is discriminatively learned usin g the S2Net algorithm [26] on relevant and irrelevant pairs of queries and titles. Similar to that BLTM is an extension to PLSA, DPM can also be viewed as an extension of LSA, where the linear projection matrix is learned in a supervised manner using clickthrough data, optimized for document ranking. We see that usi ng clickthrough data for model training leads to some signifi cant improvement. Both BLTM-PR and DPM outperform the baseline models (TF-IDF and BM25). DSSM. DNN ( Row 9 ) is a DSSM without using word hashing. It uses the same structure as DAE ( Row 6 ), but is trained in a supervised fashion on the clickthr ough data. The input term vector is based on a 40k-word vocabulary, as used by DAE. L-WH linear ( Row 10 ) is the model built using letter trigram based word hashing and supervised training. It differs from the L-WH non-linear model ( Row 11 ) in that we do not apply any nonlinear activation function, such as tanh , to its output layer. L-WH DNN (Row 12) is our best DNN-based sema ntic model, which uses three hidden layers, including the layer with the Letter-trigram-based Word Hashing (L-WH), and an output layer, and is discriminatively trained on quer y-title pairs, as described in Section 3. Although the letter n -gram based word hashing method perform a fair comparison with other competing methods, the model uses a 500K-word vocabulary. model is the best performer, beating other methods by a statistically significant margin in NDCG and demonstrating the empirical effectiveness of usin g DNNs for seman tic matching. learning on clickthrough data, coupled with an IR-centric optimization criterion tailoring to ranking, is essential for obtaining superior document rank ing performance. For example, both DNN and DAE (Row 9 and 6) use the same 40k-word vocabulary and adopt the same deep architecture. The former outperforms the latter by 3.2 points in NDCG@1. modeling. For instance, the mode ls in Rows 12, which use a 500k-word vocabulary (with word hashing), significantly outperform the model in Row 9, which uses a 40k-word vocabulary, although the former has slightly fewer free parameters than the later since the word hashing layer containing about only 30k nodes. versus a shallow one in modeling semantic information embedded in a query and a document. Results in Table 2 show that DAE (Row 3) is better than LSA (Row 2), while both LSA and DAE are unsupervised models. We also have observed similar results when comparing the shallow vs. deep architecture in the case of supervised models. Comparin g models in Rows 11 and 12 respectively, we observe that increasing the number of nonlinear layers from one to three raises the NDCG scores by 0.4-0.5 point which are statistically significant, while there is no significant difference between linear and non-linear models if both are one-layer shallow models (Row 10 vs. Row 11). # Models NDCG@1 NDCG@3 NDCG@10 1 TF-IDF 0.319 0.382 0.462 2 BM25 0.308 0.373 0.455 3 WTM 0.332 0.400 0.478 4 LSA 0.298 0.372 0.455 5 PLSA 0.295 0.371 0.456 6 DAE 0.310 0.377 0.459 7 BLTM-PR 0.337 0.403 0.480 8 DPM 0.329 0.401 0.479 12 L-WH DNN 0.362 0.425 0.498 Table 2: Comparative results with the previous state of the art approaches and various settings of DSSM. We present and evaluate a series of new latent semantic models, notably those with deep architectures which we call the DSSM. The main contribution lies in our significant extension of the previous latent semantic models (e.g., LSA) in three key aspects. First, we make use of the c lickthrough data to optimize the parameters of all versions of the models by directly targeting the goal of document ranking. Second, inspired by the deep learning framework recently shown to be highly successful in speech recognition [5][13][14][16][18], we extend the linear semantic models to their nonlinear counterparts using multiple hidden-representation layers. The deep architectures adopted have further enhanced the modeling capacity so that more sophisticated semantic structures in queries and documents can be captured and represented. Third, we use a letter n-gram based word hashing technique that proves instrumental in scaling up the training of the deep models so that very large vocabularies can be used in realistic web search. In our experiments, we show that the new techniques pertaining to each of the above three aspects lead to significant performance improveme nt on the document ranking task. A combination of all three se ts of new techniques has led to a new state-of-the-art semantic m odel that beats all the previously developed competing models with a significant margin. [1] Bengio, Y., 2009.  X  X earning deep architectures for AI. X  [2] Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003.  X  X atent [3] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., [4] Collobert, R., Weston, J., Bottou, L., Karlen, M., [5] Dahl, G., Yu, D., Deng, L., and Acero, A., 2012.  X  X ontext-[6] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T., [7] Deng, L., He, X., and Gao, J., 2013. "Deep stacking [8] Dumais, S. T., Letsche, T. A., Littman, M. L., and Landauer, [9] Gao, J., He, X., and Nie, J-Y. 2010.  X  X lickthrough-based [10] Gao, J., Toutanova, K., Yih., W-T. 2011.  X  X lickthrough-[11] Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 2009. [12] He, X., Deng, L., and Chou, W., 2008.  X  X iscriminative [13] Heck, L., Konig, Y., Sonmez, M. K., and Weintraub, M. [14] Hinton, G., Deng, L., Yu, D., Da hl, G., Mohamed, A., Jaitly, [15] Hofmann, T. 1999.  X  X robabilistic latent semantic indexing. X  [16] Hutchinson, B., Deng, L., and Yu, D., 2013.  X  X ensor deep [17] Jarvelin, K. and Kekalainen, J. 2000.  X  X R evaluation methods [18] Konig, Y., Heck, L., Weintra ub, M., and Sonmez, M. K. [19] Mesnil, G., He, X., Deng, L., and Bengio, Y., 2013. [20] Montavon, G., Orr, G., M X ller , K., 2012. Neural Networks: [21] Platt, J., Toutanova, K., an d Yih, W. 2010.  X  X ranslingual [22] Salakhutdinov R., and Hinton, G., 2007  X  X emantic hashing. X  [23] Socher, R., Huval, B., Manning, C., Ng, A., 2012.  X  X emantic [24] Svore, K., and Burges, C. 2009.  X  X  machine learning [25] Tur, G., Deng, L., Hakkani-T ur, D., and He, X., 2012. [26] Yih, W., Toutanova, K., Plat t, J., and Meek, C. 2011. 
