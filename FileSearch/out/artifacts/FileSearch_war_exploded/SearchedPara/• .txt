
Similarity measure of time series is an important subroutine in many KDD applications. Previous similarity models mainly focus on the prominent series behaviors by considering the whole information of time series. In this paper, we address the problem: which portion of information is more suitable for similarity measure for the data collected from a certain field. We propose a model for the retrieval and representation of the partial information in time series data, and a methodology for evaluating the similarity measurements based on partial information. The methodology is to retrieve various portions of information from the raw data and represent it in a concise form, then cluster the time series using the partial information and evaluate the similarity measurements through comparing the results with a standard classification. 
Experiments on data set from stock market give some interesting observations and justify the usefulness of our approach. 
H.2.8 [Database Management]: Database Applications -data mining; 1.5.3 [Pattern Recognition]: Clustering -similarity measures 
Time series, Similarity measure, Partial information 
Time series constitute a large part of data stored in many information systems, e.g. stock price, telecommunication data, weather data, astronomical data [15], medical data [4], audio data, etc. Recently, there has been a lot of interest in mining time series data. In many data mining problems, such as similarity queries, cluster, classification, etc., similarity measure is an important subroutine. 
There have been several efforts to develop effective and efficient similarity models. Most previous approaches consider the whole information of a time series, or partial information that is derived by ignoring minor series behaviors for the sake of efficiency. Such models focus on the prominent series behaviors and evaluate the 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that requires prior specific permission and/or a fee. SIGKDD "02, July 23-26, 2002, Edmonton, Alberta, Canada. Copyright 2002 ACM 1-58113-56%X/02/0007...$5.00. associations between the overall shapes of time series. However, in many real applications, a "good" similarity measure means what consists best with not the shape of the time series but the human feeling. On such occasions, a common observation is: using normalized data could provide more accurate results than using the raw data [2]. :Such normalized data, representing minor series behaviors, contain only a portion of all the information of the raw data. We could also decompose time series by other criterion, whereupon derive other portion of information. Then can we find another potion of information that is more meaningful for similarity measure? We believe it is true for certain classes of applications. 
Example 1: If remove the overall series behaviors from the time series in Figure 1, the remainder series movements would like that in Figure 2. Such partial information could reveal another kind of relations about these time series, furthermore, about the systems that generated these time series. Therefore, in some applications, it's more meaningful to consider ($2, $3, $4) similar than consider ($1, $2) similar as what is done using the common approach. 
Another promotion of our research is to facilitate dimension reduction. Since the real data sets tend to be very large, much similarity measure related work focuses on retrieving partial information that centralizes the most energy [1][6][10][13] for similarity measuring, indexing, querying, etc. However, there isn't any detailed and accurate evaluation of similarity measurements using different portions of information. The evaluation is of crucial impotence when the goal is to measure the similarity based on subjective feeling, because it could enable us go beyond the criterion of energy centralization and choose the most efficient partial information. 
Accurate evaluations on the meaningfulness of various portion of information could help designing a more effective and more efficient similarity measurement. To our knowledge, this problem has not been weU considered in the KDD field. Even the model that can retrieve and represent the partial information in time series has not been well defined. To fill this gap, we propose such a model in this paper, which includes time series decomposition, 
A time series X=(X(1),...,X(N)) is a sequence of real numbers, each number represents a value at a time point. In this paper, all time series are of the same length. 
X',, i.e. X~(X'I ..... X'r), so that X(n)=Zk X'~(n). Then X'k is the k-th F-based component of X. 
Definition 2: Segment X into a set of sub-series XF(X(Jr-r+l),...,)((jr)) through sliding a time window of width r. Let X~k be the k-th F-based component of sub-series Xj. Use an one-to-one series-value mapping rule T to map each X)k to a value RkQ), i.e. 
X)~R~'). Then the sequence Rk=(Rk(1) ..... Rk(N/r)) is the k-th representing sequence. Definition 3: Given the components (X',) of time series X, 
K=(K1,...,Kw) that is the orders of all the representing sequences of interest, and Ak=(Ab...,Ar) where A, is the degree of user's interest to the n-th component, the sequence ~n (Ax, X'xn) is the portion of partial information of interest. Definition 4: Given the k-th representing sequence Rk of time series 
X, K=(Kb...,Kw) that is the orders of all the representing sequences of interest, and Ak=(Ab...,Ar) where A, is the degree of user's interest to the n-th component, The sequence is the full representing sequence (FRS) of the partial information ~. (AK.X'K.). We denote it as: FRS(X)=R. 
Here MOD(m,K) denotes the remainder ofm/K and ['a'] denotes the smallest integer value which is larger than a. In Definition 3 and 
Definition 4, vector K represents which components are evolved in the partial information, and A. represents the degree of user's interest on the n-th component. 
Definition 5: Given two time series X, Y. The micro distance (MD) between Xand Y is the distance between FRS(X) and FRS(Y). We denote it as MD(X,Y)=D(FRS(X), FRS(Y)). 
D(X,Y) in Definition 5 could be any distance measurement currently in use. For example, we could use Euclidean distance: D(X, Y)=LE(X, Y). Then MD(X,Y)=L2(FRS(X), FRS(Y)). 
To sum up, a representing model for the partial information can be summarized by &lt;Decomposition method: F, Representation method: T, Distance measurement: D&gt;. The first one F is for retrieving the partial information and storing the partial information in a set of components. The second one T is for the compact storage and the efficient process of the partial information. 
The storage expense for FRS that represents the partial information is NW/r. It accomplishes significant storage compression comparing with storing all the components directly when the storage expense is NW. Finally, D is for measuring the distance of the partial information. MD is the similarity measurement based on partial information. By a careful selection of T and D, the FRS can be distance-preserved with the original partial information, i.e. 
Only in this occasion, the micro distance is exactly equal to the distance of the partial information, which means we could calculate the distance by using less data. 
Example 2: Consider the time series in Example 1, let the window width to be 8 which is correspond to the period of the local components, of which one is the local fluctuating movement S'l, and another is the global movement S'2. Then S' 1 is what is shown in Figure 2, and S'2 is the remainder. Since each local fluctuating movement is of the same shape, we use T that is: map S',~ to the same value a if there is such fluctuating movement and 0 otherwise; anddo not map S'~2. Then the 1st representing sequence 
Suppose we are only interested in the local fluctuating movement, so let K=(l) and AN(l,0). Then the full representing sequence of the partial information of interest is: FRS(X)=Ri. Finally we use L2 as the distance measure D, i.e. MD(X,F)=L2(FRS(X), FRS(Y)). It's largest singular value of a single fluctuating sub-series. In addition, the length of the FRS(X) is 200/8. So we need process only 1/8 data when accessing the representing sequence FRS(X), comparing with accessing the portion of partial information S'l directly. 
Above arc general definitions on the representing model. Further investigation of effective &lt;F T D&gt; is needed to fill the gap in application. We propose using orthonormal discrete transform as F and T, and Euclidean distance as D. window. Then the j-th sub series of X is XF(X(/r-r+I),...,X(/r)) , 
N/r~l&gt;_l. Let H be the transform matrix of a given orthonormal discrete transform. The results of the r point transform of sub-series Xj is sequence Tfik), given by TFH.X j. We denote the results of discrete transform of time series X and Y by DT(Xj)=3f/~, DT(Yj)ffiYT~. Then the k-th component of Xis: where The k-th representing sequence is: FRS(X) can be calculated as: After FRSs of the time series have been derived, we use the Euclidean distance to calculate MD. 
Here, we would like a transform 1) that can preserve the distance; 2) that is easy to compute; 3) that can represent interesting pattern easily and efficiently. The distance-preservation requirement is met by orthonormal transform and Euclidean distance [7]. By using 
Parseval's theorem, it is easy to prove that the above method is distance preserved [3], i.e. 
This enable us c~dculate the Euclidean distance between FRS (X) and FRS (Y), which is compact, to measure the distance between the partial information in Xand that in Y. 
Widely used orthonormal transforms form two classes: 1) the data-dependent ones, like the Karhunen-Loeve (K-L) transform and 2) the data-independent ones, like discrete Foruie transform (DFT), discrete Cosine transform (DCT), discrete Walsh transform, and discrete wavelet transform (DWT). For data-dependent transforms, if the data set is changed, a recomputation of the transform matrix is required to avoid performance degradation, requiring expensive data reorganization. So we prefer data-independent transforms. At the same time, the response time of our method will improve with the ability of the transform to concentrate the information of interest. The fewer the resulting coefficients that contain the interested information are, the more efficient the method for accessing the partial information will be. 
Note that our approach can be applied with any transform that fits our requirements. Among them, we used DCT in our experiments because it is widely used in many areas, its code is readily available and it does a good job of meeting all of our requirements. 4. SYSTEM SETUP Based on Partial Information 
We used a common strategy to evaluate the performances of similarity measurements based on partial information. First, cluster the data set using the given partial information. Then evaluate the measurement by comparing the resulting clusters to a standard classification of the data set. similar objects. Commonly, the clustering process is based on the distance of objects. Cluster by partial information is: use partial information to measure the similarity of time series, i.e. calculate 
MD; then based on the similarity, each time series is grouped into a cluster. The problem includes three sub-problems: 1) get the partial information and represent it by a compact version, 2) compute the distance based on partial information, and 3) use a distance based cluster method to cluster the time series. The first sub-problem could be solved by time series decomposition and representation that is presented in section 3. Consider the second sub-problem, our methodology to deal with partial information enables us use the Euclidean distance between FRS (X) and FRS (Y) as the distance between the partial information in X and that in Y. Finally, We could solve the third sub-problem by clustering the 
FRS sequences instead of the original time series or the components. 
Our strategy has no constraint on the clustering algorithm, any common distance based clustering method could be used, e.g. recursive k-means, agglomerative and divisive hierarchical clustering, BIRCH, CUBE, Chameleon, etc. [8], and neither the clustering algorithm nor the similarity measurement need modifying. We used hierarchical agglomerative clustering (HAC) because the nmaaber of resulting cluster can be pre-defined and it can be easily implemented. HAC starts by placing each object in its own cluster and then merges atomic clusters with minimum inter-cluster distance into larger and larger clusters, until certain termination conditions are satisfied. We specified the desired number of clusters as the termination condition. There are several roles for agglomeration, such as minimum distance, maximum 546 maximum distance was used in our experimental system. Alter the FRSs have been clustered, the following modified FI model was used to compare the resulting clusters with a standard classification: given clusters C = C1 ... Ck and standard classification S=Si...St, their similarity is calculated as follows: The similarity measure will return 0 if the two clusters are completely different and 1 if they are the same. And this measure is not symmetric. We used the Standard and Poor 500 index (S&amp;P) historical stock data at http://kumo.swcp.com/stocks/, in which daily price movements of approximately 500 stocks had been collected daily over the time period of one year. Among them, 449 stocks, of which the data of all trading days were available, were used. Data of each stock are a set of time series, which indicate the opening price, closing price, trading volume, etc. We only considered the S&amp;P classification which groups the stocks into total 109 groups based on their primary business focus, e.g. We first combined the 109 member clusters into 73 super clusters This information was then used as the standard classification in our experiments. There is a broad consensus that similarity measurement with proper preprocessing could give better results. For example, time series can have different baselines and scaling factors [5][12]. From human interpretation, if two time series are with similar behaviors running at different levels or with different scale, they should be treated as similar. In order to compare our approach with the previous measurements accurately, we first preprocessed the data using methods that were proved optimal in [2]. The preprocessing stage consists of two steps, which are mapping and normalization. In the first step, the original time series is mapped as 
Y(n)=X(n+l)-X(n). In the second step, the normalization is done as follows: split each sequence into windows, and then, divide the vector of each window by its L2 norm. Our overall experimental method is as follows: For each time series, DCT was applied to decompose the time series and to represent the partial information. We used a binary sequence E=(Et ..... Er) to represent the chosen portion of information, where r is the window width. Each E, could be either 1 or 0, which means we used or did not use the (n+l)-th component in similarity measure. Note that we did not use the mean value, which is the first component. Then generated all possible portions of partial information by varying E. Since the first component was not involved, the total number ore is 2rl-1. Each time, a generated E was used to calculate K. Then together with a user-specified d, 
FRSs of each time series were generated. FRSs were sequentially used in calculating MD, and furthermore were used in clustering as we proposed in section 4.1 (see section 3 for the definition of K and A). Finally, the similarities between each resulting cluster and the official S&amp;P classification were calculated, including Sim (S,C), Sim (C,S), and average similarity. 
In our experiments, the window width in preprocessing stage and partial information retrieving stage was both set to be 10, A was set to be (1,1,1,1,1,1,1,1,1). 
Results are shown in Table 1, 2, 3 and 4. Among them, Table 1 shows the results using various amounts of data. Note that, result the whole information of the normalized data. The performance of this similarity measurement is the same as that of the common method currently in use, which uses the optimal preprocessing setup. Table 2 lists the first 20 results with the largest average lists the first 20 results with the smallest average similarity in all the results, i.e. the first 20 "worst" results. Table 4 shows the results when the partial information with the same data amount (10% comparing with the original data set) was used. We also show the avg. Sim of each individual portion of information in Figure 3, together with the relative amount of data that was used. We could get several interesting observations form the results: 
First of all, similarity measure on different components gave different results. This is apparent in practice and is widely known in many research areas. The best result (ID: 711) was obtained when only a portion of the information is used. Furthermore, 
Different portion of information contributed differently to similarity measure. This could be explained by the criterions of generating the standard classification. Different components of the price movements implicate different characters of the stocks. And the subjective classification is only based on some of the characters. 
We can get more interesting observations by comparing the frequency of each used components. In Table 2, the frequencies, i.e. the times of being used, of the components are (11 15 14 10 19 10 14 17 14), which means the first component was used for 11 times, the second component was used for 15 times, etc. This indicates the 5th component was more frequently used when "good" results which show the 5th component was used for only 2 times when the "worst" clusters were produced. This observation gives an actually useful knowledge, which is using partial information that includes the 5th components are apt to produce good results. More research work is needed in order to fully understand this observation. But we believe acquiring such knowledge will help constructing more effective similarity measurement. For example, we could modify the measurement by increasing the weight of the 5th component. 
Another observation is about the relationship between the amounts of used components, i.e. amount of data accessed, and the performance of the corresponding similarity measurement. There is an assertion in the areas of time series indexing and querying that more information involved could give better results. However, 
Figure 3 indicates this turn out to be only a general rule. By analyzing the results, we could observe that how many components 548 quantitative. We could balance the two performance factors of accuracy and efficiency by browsing the results, and finally choose one for developing further similarity model. For example, if the goal is to reduce the dimensionality, then we could browse all results, which are produced using fewer data, to find the partial information that has been used to produce the best result. Similarly, when the goal is to develop an accurate measure strategy, we browse all the good results and find one portion of information concerning the fewest data. 6. Conclusion 
In this paper, we propose a model for retrieving and representing the partial information in time series data, and a methodology for evaluating the similarity measurements based on partial information. 
The experimental results give some interesting observations. First, similarity measurement that use different portion of information gives different results, and different portion, of information contributes differently to similarity measure. Second, how many components are considered does not have the decisive effect on the similarity measure. Finally, using the first few components is not the best choose for similarity measure. These observations could help designing a more effective and more efficient similarity measurement. Furthermore, the results of experiments on two data sets justified the usefulness of our approach. Our methodology gives the accurate and quantitative evaluation on the similarity measurements based on partial information. This enable us balance the two performance factors including accuracy and efficiency, and finally find the optimal portion of information for further KDD applications. 7. ACKNOWLEDGMENTS 
The research has been supported in part of Chinese national key fundamental research program (No, G1998030414) and Chinese national fund of natural science (No. 79990580) 8. REFERENCE [1] Rakesh A, Christos F, Efficient similarity search in [2] M. Gavrilov, D. Angnelov, P. Indyk, R. Motwani. [3] X. Jin, Y. Lu, C Shi, Micro similarity query in time [4] Juan P. Caraca-Valente, Ignacio Lopez-Chavarrias, [5] R. Agrawal, K.I. Lin, H. S. Sawhney, K. Shim, Fast [6] D. Hull. Improving text retrieval for the routing [7] K. Fukunaga, Introduction to statistical pattern [8] J. Han, M. Kambr. Data mining, concept and [9] Hagit S, Stanley B. Z. Approximate queries and [10]K. Chan, A. Fu. Efficient time series matching by [I1]E.J. Keogh and M.J. Pazzani, An enhanced [12]Dennis D, Mining multivariate time-series sensor data [13]D. Rafiei and A.O. Mendelzon, Similarity-based [14]Jagadish H. V., Alberto O. M, Tova M, Similarity-[15]M.K. Ng, Z. Huang. Data-mining massive time series 
