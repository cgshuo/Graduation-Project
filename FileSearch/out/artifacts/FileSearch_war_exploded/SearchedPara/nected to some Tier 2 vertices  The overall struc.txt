 nected to some Tier 2 vertices. The overall structure is a core-periphery, which can be succinctly summarised by the image diagram in Figure 1b, which shows the roles as ver-tices, and the inter role interactions as edges (the amount of connectivity between roles is expressed by the thickness of the edges in the image diagram). As can be seen, blockmod-els summarise the structure of a graph succinctly, allowing us to understand and characterise the underlying structure (e.g., it is a core-periphery), discover the important roles (e.g., the Core role is highly connected to all other roles and hence routes most of the traffic) and compare with other graphs.

A community decomposition seeks to find roles that are densely connected within themselves and sparsely connected to vertices of other roles. Such a type of decomposition is not able to find the structure in Figure 1b. For example, the connections between the Core vertices to the Tier 1 vertices are more dense than among the Tier 1 vertices, so it is likely that the Core and Tier 1 vertices are incorrectly merged into one role. Blockmodelling, on the other hand, can find community decompositions, and hence, it can be considered as a generalisation of community finding for graphs.
The vertex equivalence used in the previous example is called structural equivalence ; there are several others, but we concentrate on this type of equivalence in this paper as it is the most common and easiest to interpret. Equivalent ver-tices have similar in and out neighbours. For example, the vertices labelled 2 and 3 in Figure 2a are structurally equiv-alent because they have the same out neighbours (1,2,3,4) and the same in neighbours (1,2,3,4).

As the underlying graphs are not static, it is essential to fit and track blockmodels across time. For example, in the dynamic graph representing the email communications in Enron, it is known that during the crisis, communication patterns and intensity among different employee roles expe-rienced large shifts [8]. Hence if we used a single blockmodel to describe the whole sequence we will not detect or see these changes.

There are several open challenges in fitting dynamic block-models. Unlike the case of static blockmodelling where there is a strong formalism between the vertex equivalences and what type of blockmodels will satisfy those equivalences [16], there are no equivalence definitions for dynamic blockmod-els. These formalisms are important, because they provide the theoretical underpinnings between (intuitive) definitions of vertex equivalences (i.e., what defines a group of vertices playing the same role) and how they relate to structure seen in the adjacency matrix. In this paper, we introduce two for-mal definitions of evolving structural equivalence and pro-vide lemmas on how this is reflected in the block structure of a blockmodel decomposition.

In addition, dynamic blockmodels have only been recently studied [17]. In these works, each snapshot in the dynamic graph is modelled by a blockmodel, with smoothing applied on the membership of the roles (for the rest of the paper, we will use roles and positions, which is the term from social sciences, interchangeably). However, this might not be the optimal model to represent a dynamic graph. Consider the following scenarios, using the Enron email network as an ex-ample. If we used a single blockmodel to represent the whole graph sequence, it will be the simplest blockmodel, but it will be unlikely to model the communication changes inside Enron. On the other hand, if we represented each snapshot (a) T1 Snapshot. (b) T2 Snapshot. (c) T3 Snapshot. (d) T1 Adj matrix. (e) T2 Adj matrix. (f) T3 Adj matrix. Figure 2: A dynamic graph example of 3 snapshots. The top and bottom rows illustrate the snapshots and corresponding adjacency matrices respectively. with its own blockmodel, this will accurately model the un-derlying dynamic graph, but at the expense of overfitting. Hence, it is desirable to find a balance between accuracy and generality. During periods where the edges within the graph undergo minor noisy fluctuations, it is better to use one blockmodel to represent these periods. On the other hand, when the underlying graph is going through a funda-mental shift, then using multiple blockmodels to represent this period might be preferable. This shows the importance of having a) an approach to quantify and find a tradeoff be-tween model complexity and model accuracy; b) a method to fit blockmodels over subsequences of snapshots and c) a way to find where best to segment a graph sequence.
In this paper, we present an information theoretic ap-proach to address these challenges. Using the minimum description length principle [14] to quantify the tradeoff be-tween model complexity and accuracy, we propose four dif-ferent encoding schemes that are used as objectives for find-ing blockmodels over subsequences, for our two proposed definitions of evolving structural equivalence. In addition, we propose a new, fast, change point detection based ap-proach and a new blockmodel comparison measure to seg-ment the graph sequence. Using generated and three real datasets, we show that our approach can accurately de-termine good locations to segment the dynamic graph se-quence and find interesting dynamic blockmodels. We call our framework SeqiBloc ( S ubs eq uence I nformation Theo-retic Bloc kmodelling).

We present the following contributions in this paper:
In this section, we describe related work in static and dy-namic blockmodelling and finding dynamic communities.
Blockmodelling was initially introduced in sociology to model social networks [16]. Later, stochastic blockmodels [2] were proposed to relax the presumption of exact vertex equivalences. Edges, the position of vertices and other vari-ables are modelled as random variables. The assumptions made about the variables modelled, the dependencies be-tween random variables and the parametric distributions of the probabilities lead to different formulations and proba-bilistic models. For example, Airolid et al. [2] introduced amodelthatallowedverticestobelongtomultipleposi-tions In [17], Xing et al. extended the mixed membership model of [2] to dynamic networks, where there is one block-model per snapshot. The positions of vertices are allowed to change with time and be smoothed between adjacent snap-shots. This is similar to one of the dynamic equivalence definitions we propose. To improve the fitting complexity, Yang et al. [18] performed the fitting using Bayesian infer-ence.

Statistical approaches can infer insightful models, but their fitting process can be computationally demanding. More im-portantly, the dynamic blockmodel techniques fit one block-model per snapshot, which is likely overfit.

There are a variety of approaches proposed for finding and tracking communities across time. Initially, static methods were applied to each individual snapshot and the amount of overlap between the communities of adjacent snapshots were used to categorise community events such as growth, decay and stability [11]. Subsequent approaches used the idea of smoothing from evolutionary clustering [4] to find communities that are good for the current snapshot but also close to the communities found from the previous snapshot. Work in this area includes [5], which extended static spectral community finding to this framework.

Dinh et al. [9] proposed an incremental algorithm to update existing communities when a new snapshot arrives. Similarly, [1] introduced efficient ways to maintain a set of communities in massive graph streams. The emphasis is on incrementally maintaining the best current set of communi-ties, not on finding communities across a subsequence.
Algorithms for finding dynamic communities generally do not work with dynamic blockmodels, as finding communities is only one type of blockmodel and there are many other interesting types, like the core-periphery structure of the BGP example.

To the best of our knowledge, only [15] represents a graph sequence as a series of models. Graphscope [15] uses a mini-mum description length coding formulation to find evolving co-clusters in bi-partite graphs. The sequence of graphs is partitioned into segments where a single co-clustering holds. This is similar to our approach, and as a baseline we have adapted Graphscope to finding unipartite blockmodels. How-ever, the original Graphscope can only find one type of dynamic equivalence and its greedy lookahead segmenting approach misses many segmenting points in synthetic and real datasets. In addition, the blockmodels it found for real datasets have an unreasonably high number of positions. In contrast, we propose four different encodings to find our pro-posed dynamic equivalences, and our change point detection approach can accurately segment the graph sequences and find blockmodels that have a reasonable number of positions. Table 1: Description of symbols used in this paper.
In this section, we formally introduce the ideas of block-modelling and structural equivalence, their dynamic vari-ants, and the notation we use in this paper (see Table 1).
A graph G consists of a set of vertices V ,andaset of edges E , E : V  X  V . In this paper, the graphs we study are uniquely labelled and the set of vertices do not change (we leave this to future work). A graph can also be modelled by its adjacency matrix A .A dynamic graph is represented as a sequence of snapshots &lt;G 1 ,T &gt; = &lt; G ,...,G t ,...,G T &gt; ,1 &lt;t&lt;T ( T can be  X  ).
We start by describing the traditional, static definition of blockmodelling. Recall that a blockmodel partitions the vertices of a graph into a set of positions, based on notions of vertex equivalence. The positions in turn divide the ad-jacency matrix of the graph into a set of blocks. Each block defines the edge interactions between two positions.
Let the set of positions be denoted by C = { C 1 ,C 2 ,...,C e.g., C 1 in Figure 2d is { 1 , 2 , 3 , 4 } . A block A ( C submatrix of the adjacency matrix A , and represents the edges from the vertices of position C i to the vertices of po-sition C j . An example block is A ( C 1 , C 1 ) from Figure 2d, which occupies the top left of the adjacency matrix.
A blockmodel is defined as B ( A , C ), where the rows and columns of A are rearranged into blocks according to the set of positions C , such that the vertices in the same position are (structurally) equivalent.
As explained in Section 1, evolving structural equivalence has not been explored in the literature hence it is not clear what it means to have dynamic structural equivalence. There-fore, in this section, we propose and define two definitions of dynamic structural equivalence.

Themoststrictdefinition, block preserving structural equivalence , asserts that two vertices are equivalent over a span of time if they have the same neighbours over that span of time.

Definition 1. Two vertices v i and v j are block preserv-ing structurally equivalent over the continuous time span of [ ts, te ], where ts  X  te ,iffor t and t , where ts  X  t te, ts  X  t  X  te , and for v k ,v l  X  V :
For example, in Figure 2, vertices 2 and 3 are block pre-serving equivalent over T1 and T2, but not T3, because their neighbourhood in T3 is different from T1 and T2. Next, we relate this definition of equivalence with the density of the blocks of the corresponding blockmodel.

Lemma 1. Let a block consisting of all 1s be denoted by 1 , and all 0s be denoted by 0 .Let V be partitioned into block preserving structurally equivalent positions, C , over the span [ ts, te ] . Then for all t, t , ts  X  t, t  X  te ,and  X  C x 1. A t ( C x , C y )= 1 or 0 ; 2. A t ( C x , C y )= A t ( C x , C y ) .
 The inverse direction holds true also.
 Proof. Consider the vertices v i and v j  X  C x and v k  X  C ,  X  C x ,C y  X  X  .Since v i and v j are equivalent, then they must either both have an edge, or both have no edge to v k over all t . This means condition 1, and since it is over all t , condition 2 of the lemma are both true. To show the inverse direction, consider a block A t ( C x , C y ). If it is equal to 1 , then there is an edge from all vertices v i  X  C x to all vertices v  X  C y . As the blocks are equal across all t , then the edge e i,j must exist over all t , satisfying first part of condition 1 of Definition 1. Similar reasoning can be used to show the second part of condition 1 and condition 2 of Definition 1 ( A t ( C x , C y ) = 0 case) holds also.

The second definition, position preserving structural equivalence , asserts that two vertices are equivalent over a span of time if for every time instance in the span, the two vertices are structurally equivalent. Note that the set of neighbours of equivalent vertices can be different at differ-ent time instances, but must be the same at the same time instance. For example, vertices 2 and 3 are position preserv-ing equivalent over T1 to T3 in Figure 2. This equivalence is useful for finding dynamic equivalences that might not maintain the same connectivity throughout a span of time.
For example, we could represent the replying between a group of experts and newbies in a Q&amp;A forum as a dynamic graph with one snapshot per hour. During the day, the ex-perts are active, but they go to sleep at night. Using a position preserving structural equivalence definition, the ex-perts are considered equivalent throughout a 24 hour period because they are structurally equivalent for each snapshot. They are still regarded as experts when they go to sleep. However, applying a block preserving definition, their be-haviour means they are equivalent in two subsequences -one for the day, when they are highly active, and one for night, when they are inactive and likely to have the same replying behaviour as newbies (and be equivalent to them). This equivalence asserts that the experts are only considered experts when they are actively answering questions.
Definition 2. Two vertices v i and v j are position pre-serving structurally equivalent over the continuous time span of [ ts, te ], where ts  X  te ,if  X  t, ts  X  t  X  te , and for v ,v l  X  V :
Lemma 2. Let V be partitioned into position preserving structurally equivalent positions, C , over the span [ ts, te ] . Then for all t , ts  X  t  X  te and  X  C x ,C y  X  X  , 1. A t ( C x , C y )= 1 or 0 .
 The inverse direction holds true also.

Proof. The proof is similar to Lemma 1 and due to space limitations, we omit it.
In this section, we describe our information-theoretic frame-work to evaluate how well blockmodels conform to our pro-posed equivalence definitions.

Given data and a model, the idea is to evaluate a model by the amount of compression it can achieve on the data against how complex the model is. The more compression that is achieved, the lower the cost to encode the model, and the more accurate the model is. This is balanced against the complexity of the model (the more complexity, the higher the encoding costs). Using the minimum description length principle, we can combine the two ideas as C (graph, position) = C (position) + C (graph | position). A blockmodel (de-scribed by its set of positions) is the best tradeoff between accuracy and complexity if it minimises this expression.
In this work, we assume an unweighted, directed graph representation. Note that we can extend our representation to weighted graphs; we describe this in Section 7. Next, we describe the different encoding schemes.
As a baseline and for our change point detection approach, we first present two encodings that encode each snapshot of a graph sequence with a blockmodel.
Let C t denotes the set of positions (blockmodel) for snap-shot G t . Then the total encoding cost is: C
Ind ( &lt;G where C ( n ) is the cost to send the number of vertices (needed to decode the sizes of the positions), C ( C t )isthe cost to encode the positions, and C ( G t |C t )isthecostto encode the snapshot G t using C t .

Next, we describe the position description cost C ( C t ). Let the random variable  X ( C ) represent the distribution of the position memberships of C ;i.e., p ( X ( C )= i )= | C i | n C i  X  X  . Let H(X) denote the entropy of random variable X. From information theory [7], we know we can design lossless codes that uses H ( X ( C t )) bits, on average, to encode the po-sition of a vertex, and n  X  H ( X ( C t )) to encode the positions of all vertices. In order to be able to recover the member-ships fully, we also need the number of positions, which can beencodedasaninteger[7]withcost C ( |C t | ). Then the cost to send the position information is : C ( C t )= C ( |C t | H( X ( C t )).

Next, we explain how to compute C ( G t |C t ). Given a blockmodel, the snapshot can be described as a sequence of its blocks 1 .
In row or column order -we use row order
We can treat each timesliced block as a vector, and encode it using single symbol encoding. Let m 1 ( A t ( C i , C j thenumberof1 X  X intheblock A t ( C i , C j ). Let the random variable  X ( A t ( C i , C j )) represent the distribution of 0 X  X  and 1 X  X  in the block A t ( C i , C j ); i.e, p ( X ( A t ( C i , C C ( A t ( C i , C j )) =
Again, we need the number of edges (1 X  X ) in the block, sent with cost C ( m 1 ( A t ( C i , C j ))), in order to fully reconstruct the edges in the block.
This model is based on ideas from evolutionary cluster-ing, to smooth out position changes between two consecu-tive snapshots. Similar to the individual snapshot encoding, we can write the overall encoding cost as:
The only term that is different from the individual snap-shot encoding is C ( C t  X  1 |C t ), which is the encoding cost of describing the changes in position between C t and C t  X  1 This can be encoded using a membership difference vector of length n ,wherewecanencodeeachsymbolwithanaverage cost of H( X ( C t ) |  X ( C t  X  1 )) bits:
These encodings are used to describe blockmodels that span more than one snapshot and measure how close they are to being block preserving and position preserving equiv-alent. The graph sequence is represented as a series of con-tinuous subsequences, where a single blockmodel holds for each subsequence.
This encoding scheme is designed to find block preserving equivalent blockmodels. Let the subsequence be delimited by the timing indices t 1 ,t 2 ,...,t L , where t 1 &lt;t (with t 1 =1and t L = T )and L is the number of sub-sequences up to time index T . Then the graph sequence &lt;G 1 ,T &gt; is divided into a series of subsequences, &lt;G  X  positions for the subsequence &lt;G t s ,t s +1 &gt; ,1  X  The the overall encoding cost is:
C ( &lt;G 1 ,T &gt;, {C 1 ,... C L } )= C ( n )
Most of the cost terms are the same as the individual en-codings, apart from C ( t s +1  X  t s ), which is the cost to encode the multi-time slices of the adjacencies of the snapshots G to G t e , between the positions C i and C j . Then the graph description cost is:
Each multi-time sliced block is encoded as one string. An useful way to think of this is that we unroll each block of each snapshot into a string. The strings are concatenated and then encoded as a single string. The cost is: sliced blocks must be all 0s or all 1s. This means the two conditions of Lemma 1 are satisfied, which means the de-composition is block preserving equivalent. Therefore this encoding is a measure of block preserving equivalence.
This encoding implements the position preserving equiv-alence. The overall encoding cost formulation is similar to the block preserving encoding, except for how the multi-time block slices are encoded: The blocks are encoded per snapshot. Again, which occurs when the position preserving condition of Lemma 2 is true, hence this encoding is a measure of position preserving equivalence.
In this section, we describe our common approach to opti-mising the encodings and finding optimal blockmodels. We use a similar approach to [15] to optimise our encodings over subsequences, but we introduce a change point detec-tion approach to segmenting the graph sequence, which we will show is more accurate and faster than the one lookahead approach of Graphscope.

Given a subsequence segment, the greedy algorithm ap-proach used to optimise the positions will iteratively try to merge positions, split them and move vertices between po-sitions to minimise the encoding cost.
Graphscope determines a change point by comparing the encoding costs of extending the existing, single model against the costs of two models, the existing model up to the change point and a new model for describing the rest of the sequence (the lookahead subsequence). Graphscope uses a lookahead of 1 snapshot, and as our results show, it can sometimes miss change points. Once a change point is missed, the ob-jective generally favours extending the current subsequence, Algorithm 1 Main procedure in SeqiBloc. 1: Input: New snapshot G t , Current subsequence 2: Output: C ts 3: // Compute partitioning on single snapshot 4: C t =updatePos( G t ) 5: deltaCost = M ( C t , C t  X  1 ) 6: if isChangePoint(deltaCost)) then 7: C ts = updatePos( &lt;G ts,t &gt; ) 8: end if 9: updateTimeSeries(deltaCost) because the lookahead is too short. Hence, a greater looka-head is needed, but there is no principled and inexpensive way to determine the appropriate lookahead, and the opti-mal lookhead is likely to change across time.

Instead, we propose a simple change point detection ap-proach (see Algorithm 1 for an overview). First, note that a new blockmodel should only be induced when there is a change point and the existing one is no longer accurate for describing the new snapshots. This means that the opti-mal blockmodel over the new snapshots varies significantly from the existing blockmodel. Conversely, when there is no change point and only (uniformly) random noise, then the likelihood of dense or sparse sub-blocks appearing by chance is low and therefore the blockmodel over the new snapshots is likely to be similar to the existing blockmodel. Therefore, if we found the best blockmodels for each snapshot (using one of the individual encodings), and monitored the differ-ences between consecutive blockmodels (see next section), then the actual change points are likely to occur when there are significant spikes in the difference, which can be detected using traditional change point detection methods.
In this subsection, we describe the two measures we used to determine the change points in the graph streams.
To detect the change points of position preserving block-models, we monitor for differences in the positions of two time-adjacent blockmodels. Existing measures from exter-nal clustering validation can be used to measure this position difference. We found that the Variation of Information (VI) measure [13] performed well for this task. We denote this change point approach with the VI measure as pos-cp .
To detect change points in block preserving blockmodels, we know from Lemma 1 that we need to monitor for changes in the positions and the block densities. There are no mea-sures that satisfy our requirements; the closest are spatially aware cluster comparison techniques [6], but these assume the entities are points embedded in a Euclidean space, which is generally non-trivial to map from graphs.

Hence, we propose a measure, called BMDD that com-pares the differences in densities across the blocks of the two blockmodels. We cannot directly compare blocks, be-cause the positions and blocks between two models might not align. Instead, we compute the expected difference in densities across the blocks of the two models. Recall that  X (
C ) is the random variable of the position memberships and event space of C .Let d () denote a distance between densities (we will elaborate shortly) and the probability p ( X ( C t i,  X ( C t )= j,  X ( C t  X  1 )= x,  X ( C t  X  1 )= y ) be simplified to p ( i, j, x, y ). Then BMDD is defined as where p ( r 1 ,r 2) = | C r 1  X  C r 2 | n and p ( c 1 ,c 2) = effectively weighs the difference between block densities based on the amount of overlap the two blocks have.

For the density distance dd, we used the absolute dif-ference of their densities d ( A t ( C r1 , C c1 ) , A t  X  1 |
A t ( C r1 , C c1 )  X  A t  X  1 ( C r2 , C c2 ) | , which we found to work well. We denote this change point approach with the BMDD measure as pos-cp . The change point we seek is a large, significant spike. There are many different change point measures we can use (our framework can accommodate any that can detect spikes), but we found the basic control chart measure [3] worked well. The control chat measure tracks the mean and standard deviation of our blockmodel difference measures, and flags a change point when the standard deviation is sig-nificantly ( X   X   X  , where  X  is the alarm threshold) above the mean. Because we do not make any assumptions on the underlying graph stream, we cannot use techniques like gen-eralised likelihood tests to quantify significance. Hence, the alarm threshold is the only parameter in our approach. We found  X  = 0.5 works well for many datasets, and this can be used as the default value.
In this section, we will show that our change point detec-tion based algorithms, pos-cp and pos-cp , are more accurate and efficient than the corresponding Graphscope algorithms at detecting change points and learning new blockmodels. In addition, we demonstrate the difference between the block and position preserving formulations. We use a combination of synthetic data and three real datasets.

We evaluate against two baseline Graphscope algorithms, block-gs and pos-gs . block-gs and pos-gs use the one-lookahead approach of Graphscope to optimise the block preserving and position preserving encodings respectively.
To construct the synthetic graphs, we use a static stochas-tic blockmodel algorithm [2] to generate initial static block-models of 250 vertices, 10 uniformly sized positions, 30% of blocks having 75% density and rest having 5% density. We then introduced change points at 10 uniformly random locations, where we change the underlying blockmodel by moving vertices between positions, flipping the densities of some blocks (e.g., 0.75 to 0.25) or adding or deleting a posi-tion. Each subsequence (of 100 snapshots) is then generated from a static blockmodel, with some uniformly distributed noise (flipping of edges) added to the snapshots between the change points. For each parameter setting, we generated three blockmodels, and from each, three subsequences, and ran each of the algorithms three times, for a total of 27 runs peralgorithmforeachdataset 2 setting.
Available at http://eng.unimelb.edu.au/jeffreyc/data Table 2: Segmenting results for generated datasets. Results are reported as average (std. deviation).
To evaluate the ability of the algorithms to detect the in-troduced change points, we use the precision and recall mea-sures. Let I true and I act denote the set of true and detected change points respectively. Then precision = | I true  X  I tected and generated blockmodels are by comparing their sets of positions across the snapshots and averaging the re-sults. We use the average variation of information (VI) per snapshot as our comparison measure. Lower VI means more similar sets of positions. Although the mean of the total ob-jective values for the change point algorithms are 20 X 30% lower than the corresponding Graphscope variants, we do not report these values because we found they vary greatly between the datasets (large variance) and hence are incon-clusive for our analysis.
In this subsection, we evaluate the segmenting accuracy of SeqiBloc. We first evaluate its ability to locate the intro-duced change points, varying the amount of noise and the size of the blockmodel shift at the change points.
We first changed the amount of noise introduced from 1% of edges to 10% of edges. We found that the algorithms were mostly invariant to the amount of noise introduced, hence we only show one of the results (5% of edges changed) in Table 2a. The results indicate that when pos-cp detects a change, it is always correct in these tests (100% precision), but it can sometimes miss some change points (73% recall). On the other hand, pos-cp is able to detect all the introduced changed points (100% recall), but is sometimes oversensitive and makes a false detection (62.6% precision). block-gs has less precision and recall than both our algorithms, indicating it is overly sensitive due to its greedy nature. pos-gs cannot detect any change points at all. The variation of information values indicate that both pos-cp and pos-cp obtained block-models that are significantly closer to the true blockmodels than the Graphscope variants.

Next, we show the results of experiments where we only change the block densities (Table 2b) and the position mem-berships (Table 2c). The first test should favour pos-cp ,as pos-cp and its position-preserving formulation generally can-not detect change points with block densities changes only. The results in Table 2b confirm this. The second test should favour pos-cp , as it only involves position change and our measure BMDD monitors for a change in position and den-sity. Again, the results confirm this. Note that for both tests, pos-gs could not detect any change points, and when comparing the block-preserving algorithms, block-gs is al-ways less accurate than the equivalent pos-cp .

The synthetic results show that the change point algo-rithms are more accurate than their Graphscope counter-parts.
In this subsection, we demonstrate the blockmodels and segmenting produced by the different formulations for the MIT reality mining proximity graphs [10], the Enron email graph [8] and the BGP Internet routing network [12]. Their statistics are described in Table 3.

The Reality Mining graph measures the proximity of users in a laboratory environment over a period of a few months. Each vertex in the graph represents a user, and an undi-rected edge represents proximity. The Enron email graph tracks the email communications among Enron employees 3 . Each vertex in the Enron graph represents a person, and a directed edge represents sending at least one email from the person represented by the source vertex to the person repre-senting the target vertex. Finally, the BGP Internet routing graph represents the connectivity among organisations in the Internet. A vertex represents an organisation, and an undirected edge represents connectivity between them. The time spanned by each snapshot of the Reality Mining, En-ron and BGP graphs are one day, one week and two days respectively. The datasets span a variety of data types, with the MIT and Enron data being unstable and noisy in general and the BGP data being larger and relatively stable.
To provide an illustration of the blockmodels found, we first show a 10 day graph sequence of the proximity net-work, with each snapshot spanning a day. We show the results from the pos-cp , pos-cp and block-gs algorithms (Fig-ures 3a to 3c). Each figure shows the adjacency matrix of each snapshot, with the red dotted lines as delimiters of the positions/blocks. Time runs top to bottom, left to right. We used a basic matching algorithm to align the blocks and ver-tices as best we can, but vertices of different snapshots that
We use the version of dataset that only tracks the internal communications within Enron. delimit blockmodel decompositions of each snapshot. have the same labels in the figures might not correspond to thesameactualvertex.
 As the Figure 3c shows, block-gs fragments very easily. Once the number of positions increases, the greedy algo-rithm is unable to recover and all subsequent parts of the graph sequence are lumped into one blockmodel.

Now consider the blockmodels obtained from pos-cp and pos-cp (Figures 3a and 3b respectively). They clearly reflect the weekday (5 days of high levels of proximity) and week-ends (almost no proxmity detected). Because the position preserving equivalence is easier to satisfy, pos-cp is generally able to find blockmodels of longer lengths (e.g. d6 X  X 9 in Fig-ure 3b vs. individual blockmodels over the same period in Figure 3a). In contrast, the block preserving equivalence is less likely to fragment (compare the subsequence d1 X  X 8). Both equivalences are able to produce blockmodels that ap-pear to be visually reasonable.

To analyse the amount of fragmentation and the length of segments obtained, we analysed the number of segments, their average length and the number of positions found over time. The results are in Table 4a and Figure 4a, which shows the number of positions found across time.

The results in Table 4a and Figure 4a confirm that block-gs cannot segment the proximity graph sequence and only man-aged two fragmented segments. pos-cp , being more strict than pos-cp , had more segments, but the larger standard deviation of segment lengths show that they vary more in length than ones obtained from pos-cp . The reason for this is that pos-cp tends to be more sensitive to changes (recall the synthetic results), hence more likely to segment during Table 4: Segmenting results for Reality Mining and Enron. Segment length results are reported as av-erage (standard deviation). fluctuating periods than pos-cp . In addition, both change point models used much less positions to describe the data than block-gs , even after we capped the maximum number of positions to 51. The lower objective value of pos-cp over block-gs suggest that the evolving blockmodels found by pos-cp are a better fit. The running time of block-gs is much slower than the other two algorithms, because the optimis-ing algorithm X  X  complexity scales at least quadratically with the number of positions.
We repeat the previous analysis to evaluate the segment-ing quality on the Enron data. Consider Table 4b and Fig-ure 4b. They show that block-gs is unable to segment the stream at all, producing one segment for the whole stream. Figure 4: Plots of the number of partitions vs. time for the Reality Mining and Enron datasets.
 This caused the single blockmodel to fragment into many positions and causing the run time to grow to 700 seconds. In contrast, both pos-cp and pos-cp were able to segment the sequence into a number of segments, with pos-cp producing more segments again. Again, it shows block-gs fragmenting and not being able to produce meaningful blockmodel de-compositions, pos-cp having more positions in general while pos-cp being the most stable of the three, in terms of number of positions.
The BGP data is largely stable, and for the period we analysed over November 2011, there was no known out-ages. For both pos-cp and pos-cp we correctly found one segment, with the set of positions found illustrated in Fig-ure 1. The running time was several hours, which is about 10-100 times more scalable than state of the art blockmod-elling algorithms [17].

In summary, both pos-cp and pos-cp can mine blockmod-els that are a balance between accuracy (one blockmodel per snapshot) and simplicity (one blockmodel for the whole se-quence). We found weekday/weekend structure in the Real-ity Mining data, a stable hierarchical structure for the BGP graph, and showed that the change point formulations in SeqiBloc performed well in synthetic datasets and do not fragment like Graphscope does. In conclusion, we have presented a novel framework, Seqi-Bloc, to decompose a dynamic graph into a series of multi-snapshot blockmodels that summarises the evolving struc-tural patterns. In this framework, we have introduced two new definitions of dynamic structural equivalence and showed what this means in terms of adjacency matrix structure and blockmodelling. Based on these definitions, we have for-mulated four different information theoretic encodings that correspond to the new equivalence definitions and provide an intuitive tradeoff between the number of positions in a blockmodel, the time it spans and how well it fits the subse-quence spanned. We then introduced a change point detec-tion approach with a new blockmodel comparison measure, BMDD, to find the appropriate length of these blockmodels. Using synthetic and real datasets like the Reality Mining, Enron and BGP dynamic graphs, we showed our approach can find relevant segments and discover interesting and in-tuitive blockmodels across time.

For future work, it will be interesting to mine for frequent multi-snapshot blockmodels. For example, in the Reality Mining results, we saw there were clear weekday and week-end structural patterns. If we can group the similar weekend and weekday behaviours (in terms of blockmodels), then we can find normal patterns of interactions and detect outliers.
Another possible future direction is to extend the frame-work to weighted blockmodels. There are no standard defini-tions of weighted equivalences and blockmodels, hence there is a need to find intuitive definitions of weighted equiva-lences. [1] C. C. Aggarwal, Y. Zhao, and P. S. Yu. On Clustering [2] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. [3] B. Brodsky and B. Darkhovsky. Nonparametric [4] D.Chakrabarti,R.Kumar,andA.Tomkins.
 [5] Y. Chi, X. Song, D. Zhou, K. Hino, and B. L. Tseng. [6] M. Coen, H. Ansari, and N. Fillmore. Comparing [7] T. Cover and J. Thomas. Elements of Information [8] J.Diesner,T.Frantz,andK.Carley.Communication [9] T. N. Dinh, I. Shin, N. K. Thai, M. T. Thai, and [10] N. Eagle and A. (Sandy) Pentland. Reality mining: [11] D. Greene, D. Doyle, and P. Cunningham. Tracking [12] Y. Hyun, B. Huffaker, D. Andersen, E. Aben, [13] M. Meila. Comparing clusterings by the variation of [14] J. Rissanen. Modeling by shortest data description. [15] J. Sun, C. Faloutsos, S. Papadimitriou, and P. S. Yu. [16] S. Wasserman and K. Faust. Social Network Analysis: [17] E. P. Xing, W. Fu, and L. Song. A State-Space Mixed [18] T.Yang,Y.Chi,S.Zhu,Y.Gong,andR.Jin.

