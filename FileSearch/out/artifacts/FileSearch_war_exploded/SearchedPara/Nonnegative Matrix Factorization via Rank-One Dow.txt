 Michael Biggs mbiggs@alumni.uwaterloo.ca Ali Ghodsi aghodsib@uwaterloo.ca Stephen Vavasis vavasis@uwaterloo.ca University of Waterloo, Waterloo, ON N2L 3G1 Several problems in information retrieval can be posed as low-rank matrix approximation. The seminal pa-per by Deerwester et al. (1990) on latent semantic indexing (LSI) showed that approximating a term-document matrix describing a corpus of articles via the SVD led to powerful query and classification tech-niques. A drawback of LSI is that the low-rank fac-tors in general will have both positive and negative entries, and there is no obvious statistical interpreta-tion of the negative entries. This led Lee and Seung (1999) among others to propose nonnegative matrix factorization (NMF), that is, approximation of a ma-trix A  X  R m  X  n as a product of two factors W H T , where W  X  R m  X  k , H  X  R n  X  k , both have nonnegative entries, and k  X  min( m, n ). Lee and Seung showed intriguing results with a corpus of images. In a re-lated work, Hofmann (1999) showed the application of NMF to text retrieval. Nonnegative matrix fac-torization has its roots in work of Gregory and Pull-man (1983), Paatero and Tapper (1994) and Cohen and Rothblum (1993).
 Since the problem is NP-hard (Vavasis, 2007), it is not surprising that no algorithm is known to solve NMF to optimality. Heuristic algorithms proposed for NMF have generally been based on incrementally improving the objective k A  X  W H T k in some norm using local moves. A particularly sophisticated example of local search is due, e.g., to Kim and Park (2007). A draw-back of local search is that it is sensitive to initial-ization and it is also sometimes difficult to establish convergence.
 We propose an NMF method based on greedy rank-one downdating that we call R1D. R1D is partly motived by Jordan X  X  algorithm for computing the SVD, which is described in Section 2. Unlike local search methods, greedy methods do not require an initial guess. In Section 3, we compare our algorithm to Jordan X  X  SVD algorithm, which is the archetypal greedy downdat-ing procedure. Previous work on greedy downdating algorithms for NMF is the subject of Section 4. In Sec-tion 5, we present the main theoretical result of this paper, which states that in a certain model of text due to Papadimitriou et al. (2000), optimizing our objec-tive function means correctly identifying a topic in a text corpus; and Section 6 discusses the complexity of this problem. We then turn to computational exper-iments: in Section 7, we present results for R1D on image datasets, and in Section 8, we present results on text.
 Rank-one downdate (R1D) is based on the simple ob-servation that the leading singular vectors of a nonneg-ative matrix are nonnegative. This is a consequence of the Perron-Frobenius theorem (Golub &amp; Van Loan, 1996). Based on this observation, it is trivial to com-pute a rank-one NMF. This idea can be extended to approximate a higher order NMF. Suppose we com-pute the rank-one NMF and then subtract it from the original matrix. The original matrix will not be nonnegative any more but all negative entries can be forced to be zero or positive and the procedure can be repeated.
 An improvement on this idea takes only a submatrix of the original matrix and applies the Perron-Frobenius theorem. The point is that taking the whole matrix will in some sense average the features, whereas a sub-matrix can pick out particular features. A second rea-son to take a submatrix is that a correctly chosen sub-matrix may be very close to having a rank of one, so the step of forcing the residuals to be zero will not in-troduce significant inaccuracy (since they will already be close to zero).
 The outer loop of the R1D algorithm may be described as follows.
 Algorithm 1 R1D input A  X  R m  X  n , k &gt; 0 1: for  X  = 1 to k do 2: [ M, N, u , v ,  X  ] = ApproxRankOneSubmatrix ( A ) 3: W ( M,  X  ) = u ( M ) 4: H ( N,  X  ) =  X  v ( N ) 5: A ( M, N ) = 0 6: end for Here, M is a subset of { 1 , . . . , m } , N is a sub-set of { 1 , . . . , n } , u  X  R m , v  X  R n and  X   X  R , and u , v are both unit vectors. The function ApproxRankOneSubmatrix selects these five values so that the submatrix of A indexed by rows M and N is approximately rank one, and in particular, is ap-proximately equal to u ( M )  X  v T ( N ). We follow Mat-lab subscripting conventions, so that A ( M, N ) denotes this particular submatrix.
 This outer loop for R1D may be called  X  X reedy rank-one downdating X  since it greedily tries to fill the columns of W and H from left to right by finding good rank-one submatrices of A and subtracting them from A . The classical greedy rank-one downdating algo-rithm is Jordan X  X  algorithm for the SVD, described in Section 3. Related work on greedy rank-one downdat-ing for NMF is the topic of Section 4.
 The subroutine ApproxRankOneSubmatrix , presented later in this section, is a heuristic routine to maximize the following objective function: Here,  X  is a penalty parameter. The Frobenius norm of an m  X  n matrix B , denoted k B k F is defined to be p
B (1 , 1) 2 + B (1 , 2) 2 +  X  X  X  + B ( m, n ) 2 . The rationale for (1) is as follows: the first term in (1) expresses the objective that A ( M, N ) should be large, while the sec-ond term penalizes departure of A ( M, N ) from being a rank-one matrix.
 Since the optimal u ,  X , v come from the SVD (once M, N are fixed), the above objective function can be rewritten just in terms of M and N as f ( M, N ) = where p = min( | M | , | N | ). The penalty parameter  X  should be greater than 1 so that the presence of low-rank contributions is penalized rather than rewarded. We conjecture that maximizing (1) is NP-hard (see Section 6), so we instead propose a heuristic routine for optimizing it. The procedure alternates improving M , N , u ,  X  and v cyclically. First, observe that if M, N are already known, then the optimal choice of u ,  X  , v can be found with the SVD. For fixed ( v , N ), the objective function (1) is separable by rows of the matrix. In particular, the contribution of row i  X  M is where  X  i = u i  X  . Note that  X  i may be undefined if i /  X  M . Nonetheless, given v , the optimal  X  i (i.e., the choice that minimizes k A ( i, N )  X  u i v T k ) is easy to compute: it is A ( i, N ) v , the solution to a simple least-squares minimization. Thus, we conclude that putting column i into index set M is favorable for the overall objective function provided that f i &gt; 0, where The formula for f i can be simplified as follows: f i = A ( i, N ) A ( i, N ) T  X   X  ( A ( i, N ) If we rescale by  X   X  1 (which does not affect the accep-tance criterion), and we define new penalty parameters  X   X  :=  X / (  X   X  1), then we see that row i is accepted pro-vided that A similar analysis applies to the columns, and leads to the conclusion that, given values for M and u , column j should be accepted provided that The next issue is the choice of a starting guess for M, N, u ,  X , v . The algorithm should be initialized with a starting guess that has a positive score, or else the rules for discarding rows and columns could conceiv-ably discard all rows or columns. To put this more strongly, in order to improve the score of a converged solution, it seems sensible to select a starting guess with a high score. For this reason, R1D uses a single column of A as its starting guess, and in particular, the column of A with the greatest norm. (A single row may also be chosen.) It then chooses u to be the normaliza-tion of this column. This column is exactly rank one, so for the correct values of  X  and v the first penalty term of (1) is zero. We have derived the following al-gorithm for the subroutine ApproxRankOneSubmatrix occuring in statement  X  2  X  in R1D.
 Algorithm 2 ApproxRankOneSubmatrix input A  X  R m  X  n , parameter  X   X  &gt; 1 output M  X  X  1 , . . . , m } , N  X  X  1 , . . . , n } , 1: Select j 0  X  X  1 , . . . , n } to maximize k A (: , j 0 2: M = { 1 , . . . , m } 3: N = { j 0 } 4:  X  = k A (: , j 0 ) k 5: u = A (: , j 0 ) / X  6: repeat 7: Let  X  v = A ( M, :) T u ( M ) 8: N = { j :  X   X   X  v ( j ) 2  X  X  A ( M, j ) k 2 &gt; 0 } 9: v ( N ) =  X  v ( N ) / k  X  v ( N ) k 10: Let  X  u = A (: , N ) v ( N ) 11: M = { i :  X   X   X  u ( i ) 2  X  X  A ( i, N ) k 2 &gt; 0 } 12:  X  = k u ( M ) k 13: u ( M ) =  X  u ( M ) / X  14: until stagnation in M, N, u ,  X , v The  X  X epeat X  loop is guaranteed to make progress be-cause each iteration increases the value of the objective function. On the other hand, there does not seem to be any easy way to derive a useful prior upper bound on its number of iterations. In practice, it proceeds quite quickly, usually converging in 10 X 15 iterations. But to guarantee fast termination, monotonicity can be forced on M and N by requiring M to shrink and N to grow. In other words, statement  X  8  X  can be replaced by and statement  X  11  X  by Our experiments indicate that this change does not have a major impact on the performance of R1D. Another possible modification to the algorithm is as follows: we modify the objective function by adding a second penalty term  X   X  | M | X | N | to (1) where  X  &gt; 0 is a parameter. The purpose of this term is to penalize very low-norm rows or columns from being inserted into A ( M, N ) since they are probably noisy. For data with larger norm, the first term of (1) should dominate this penalty. Notice that this penalty term is also sep-arable so it is easy to implement: the formula in  X  8  X  is formula in  X  11  X  becomes  X   X   X  u ( i ) 2  X  X  A ( i, N ) k 0, where  X   X  =  X / (  X   X  1). A good value for  X   X  is to set it so that in the initial starting point, the third penalty term is a small fraction (say  X   X  = 1 / 20) of the other terms. This leads to the following definition for  X  : which may be computed immediately after  X  4  X  . Greedy rank-one downdating appears to be much faster than other NMF algorithms. Generating each column of W and H requires approximately 20 matrix-vector multiplications; these multiplications are always at least as sparse as the original data. There is no it-erative improvement phase. It can also be much faster than the SVD, especially for sparse data. The classical rank-one greedy downdating algorithm is Jordan X  X  algorithm for computing the singular value decomposition (SVD) (Stewart, 1993). Recall that the SVD takes as input an m  X  n matrix A and returns three factors U,  X  , V such that U  X  R m  X  k and U has orthonormal columns (i.e., U T U = I ),  X   X  R k  X  k and is diagonal with nonnegative diagonal entries, and V  X  R n  X  k also with orthonormal columns, such that U  X  V T is the optimal rank-k approximation to A in either the 2-norm or Frobenius norm. (Recall that the 2-norm of an m  X  n matrix B , denoted k B k 2 , is Algorithm 3 JordanSVD input A  X  R m  X  n and k  X  min( m, n ) output U,  X  , V as above. 1: for  X  = 1 to k do 2: Select a random nonzero  X  u  X  R m 3:  X  = k  X  u k 4: u =  X  u / X  5: repeat { power method } 6:  X  v = A T u 7: v =  X  v / k  X  v k 8:  X  u = A v 9:  X  = k  X  u k 10: u =  X  u / X  11: until stagnation in u ,  X , v 12: A = A  X  u  X  v T 13: U (: ,  X  ) = u 14: V (: ,  X  ) = v 15:  X (  X ,  X  ) =  X  16: end for defined to be p  X  max ( B T B ), where  X  max denotes the maximum eigenvalue.) Thus, we see that R1D is quite similar to the SVD. The principal difference is that R1D tries to find a subma-trix indexed by M  X  N at the same time that it tries to identify the optimal u and v . Hence, the formulas for u and v occurring in  X  9  X  and  X  13  X  of subroutine ApproxRankOneSubmatrix , which were presented ear-lier as solutions to a least-squares problem, may also be regarded as steps in a power method. In particular, this means that if M and N are fixed, then the inner repeat loop of this subroutine will indeed converge to the dominant singular triple of A ( M, N ).
 As mentioned earlier, a shortcoming of the SVD is that its factors contain both positive and negative numbers. It has another subtler shortcoming when used for clus-tering which is as follows: because the SVD always operates on the entire matrix, it can return a singular vector that averages the results from two nearly dis-joint topics in a corpus (see Biggs et al. (2008) for an example). R1D avoids this pitfall by seeking a subma-trix that is approximately rank-one as it applies the power method. As mentioned in the introduction, most algorithms proposed in the literature are based on forming an initial W and H and then improving them by local search on an objective function. The objective func-tion usually includes a term of the form k A  X  W H T k in some norm, and may include other terms.
 A few previous works follow an approach similar to ours, namely, greedy subtraction of rank-one matrices. This includes the work of Bergmann et al. (2003), who identify the rank-one matrix to subtract as the fixed point of an iterative process. Asgarian and Greiner (2006) find the dominant singular pair and then trun-cate it. Gillis (2006) finds a rank-one understimator and subtracts that. Boutsidis and Gallopoulos (2007) consider the use of a greedy algorithm for initializing other algorithm and make the following interesting ob-servation: The nonnegative part of a rank-one matrix has rank at most 2.
 The main innovation herein is the idea that the search for the rank-one submatrix should itself be an opti-mization subproblem. This observation allows us to compare one candidate submatrix to another. (Gillis also phrases his subproblem as optimization, although his optimization problem does not explicitly seek sub-matrices like ours.) A second innovation is our anal-ysis in Section 5 showing that if the subproblem were solved optimally, then R1D would be able to accu-rately find the topics in the model of -separable cor-pora (Papadimitriou et al., 2000). In this section, we establish the main theoretical result of the paper, namely, that the objective function given by (1) is able to correctly identify a topic in a nearly separable corpus. We define our text model as fol-lows. There is a universe of terms numbered 1 , . . . , m . There is also a set of topics numbered 1 , . . . , t . Topic k , for k = 1 , . . . , t , is a probability distribution over the terms. Let P ( i, k ) denote the probability of term i occurring in topic k . Thus, P is a singly stochastic ma-trix, i.e., it has nonnegative entries with column sums exactly 1. We assume also that there is a probability distribution over topics; say the probability of topic k by P and  X  1 , . . . ,  X  t . We use the Zipf distribution as the model of document length. In particular, there is a number L such that all documents have length less than L , and the probability that a document of length l occurs is We have checked that the Zipf model is a good fit for several common datasets.
 A document is generated from this text model as fol-lows. First, topic k is chosen at random according to the probability distribution {  X  1 , . . . ,  X  t } . Then, a length l is chosen at random from { 1 , . . . , L  X  1 } ac-cording to the Zipf distribution. Finally, the docu-ment itself is chosen at random by selecting l terms in-dependently according to the probability distribution P (: , k ). A corpus is a set of n documents chosen in-dependently using this text model. Its term-document matrix is the m  X  n matrix A such that A ( i, j ) is the frequency of term i in document j .
 We further assume that the text model is -separable , meaning that each topic k is associated with a set of disjoint, and that P ( i, k )  X  for i /  X  T k , i.e., the prob-ability that a document on topic k will use a term outside of T k is small. Let P min = min { P ( i, k ) : i  X  T , k = 1 , . . . , t } . Without loss of generality, P min since any row i  X  T k such that P ( i, k ) = 0 may be removed from T k without affecting the validity of the model. Parameter must satisfy an inequality men-tioned below. This corpus model is quite similar to that of Papadimitriou et al. (2000). One difference is in the the document length model. Our model also relaxes several assumptions of Papadimitriou et al. Our main theorem is that the objective function given by (1) correctly finds documents associated with a par-ticular topic in a corpus.
 Theorem 1. Let ( P, (  X  1 , . . . ,  X  t )) specify a text model, and let  X  &gt; 0 be chosen arbitrarily. Assume &gt; 0 is chosen smaller than a function ( P min , m, t,  X  ) (see Biggs et al. (2008) for this function). Suppose that the text-model is -separable with respect to T 1 , . . . , T t subsets of terms defining the topics. Let A be the term-document matrix of a corpus of n documents drawn from this model when the document-length parameter is L .
 Choose  X  = 4 in (1) . Then with probability tending to 1 as n  X   X  and L  X   X  , the optimizing pair ( M, N ) of (1) satisfies the following. Let D 1 , . . . , D t be the partitioning of the columns of A according to topics. There exists a topic k  X  X  1 , . . . , t } such that A ( M, N ) and A ( T k , D k ) are nearly coincident in the following sense.
 Here, X 4 Y denotes the set-theoretic symmetric dif-ference ( X  X  Y )  X  ( Y  X  X ). The proof of this theorem is lengthy and appears in Biggs et al. (2008). It re-lies on Chernoff-Hoeffding estimates and perturbation results for singular vectors such as Theorem 8.6.5 of Golub and Van Loan (1996). In this section, we observe that the problem of globally maximizing (2) is NP-hard at least in the case that  X  is treated as an input parameter. This observation explains why R1D settles for a heuristic maximization of (2) rather than exact maximization. First, observe that the maximum biclique (MBC) problem is NP-hard as proved by Peeters (2003). We show that the MBC problem can be transformed to an instance of (2).
 Let us recall the definition of the MBC problem. The input is a bipartite graph G . The problem is to find an ( m, n )-complete bipartite subgraph K (sometimes called a biclique ) of G such that mn is maximized, i.e., the number of edges of K is maximized.
 Suppose we are given G , an instance of the maximum biclique problem. Let A be the left-right adjacency matrix of G , that is, if G = ( U, V, E ) where U  X  V is the bipartition of the node set, then A has | U | rows and | V | columns, and A ( i, j ) = 1 if ( i, j )  X  E for i  X  U and j  X  V , else A ( i, j ) = 0.
 Consider maximizing (2) for this choice of A . We re-quire the following preliminary lemmas whose proofs are omitted.
 Lemma 2. Let A be a matrix that has either of the following as a submatrix: Then  X  2 ( A ) &gt; 0 . 618 .
 This lemma leads to the following lemma.
 Lemma 3. Suppose all entries of A  X  R m  X  n are ei-ther 0 or 1 , and suppose and at least one entry is 1. Suppose M, N are the optimal solution for maximizing f ( M, N ) given by (2) . Suppose also that the parameter  X  is chosen to be 2 . 7 mn + 1 or larger. Then the op-timal choice of M, N must yield a matrix A ( M, N ) of all 1  X  X , possibly augmented with some rows or columns that are entirely zeros.
 Now consider the main claim, namely, that optimize ( M, N ) of the objective function for this A corresponds to the max biclique. If A ( M, N ) includes a row or col-umn entirely of zeros, then this row or column may be dropped without affecting the value of the objective function (2). Hence it follows from the lemma that without loss of generality that the optimizer ( M, N ) of (2) indexes a matrix of all 1 X  X . In that case,  X  ( A ( M, N )) = p | M | X | N | while  X  2 ( A ( M, N )) =  X  X  X  =  X  p ( A ( M, N )) = 0 (where p = min( | M | , | N | )), and hence f ( M, N ) = | M | X | N | . Thus, the value of the objective function corresponds exactly to the number of edges in the biclique. This completes the proof that biclique is reducible in polynomial time to maximizing (2).
 We note that Gillis (2006) also uses the result of Peeters for a similar purpose, namely, to show that the subproblem arising in his NMF algorithm is also NP-hard.
 The NP-hardness result in this section requires that  X  be an input parameter. We conjecture that (2) is NP-hard even when  X  is fixed (say  X  = 4 as used herein). We first demonstrate the performance of R1D on a simple binary image dataset, depicted in Figure 1 ( a ). Each of the ten dataset images is composed of one or two  X  X asis X  triangles. The results of R1D (with pa-rameter  X   X  = 4) and LSI on this dataset are shown in Figure 1 ( b ) and ( c ), respectively, and the interpre-tation is as follows. The leftmost column illustrates the four leading columns of W , which are the learned features. For each of these, the images on the right are the dataset images with the largest entries in the corresponding column of H ; they should be closely as-sociated with the feature on the left.
 R1D discovered the four triangles as a basis, and to each it associated exactly the dataset images which contain the appropriate triangle. Alternatively, the LSI factorization is not as interpretable.
 We have also compared results against NMFDIV from nmfpack (Hoyer, 2000; Hoyer, 2004). NMFDIV re-quires k , the number of basis vectors to compute, as an input parameter which globally affects the factors W and H . If k is correctly set to 4, NMFDIV is able to compute the same correct result as R1D. Otherwise, some or all of the basis vectors will appear incorrect, including the first ones. R1D and LSI will each com-pute the same leading columns regardless of k , and on this dataset they will not compute more than 4 columns; all subsequent columns of W and H will be zero.
 Figure 2 conducts a similar experiment on the Frey face dataset, which consists of 1965 registered face im-ages of size 28  X  20. Again, the leading columns of W present the  X  X igenfaces X  or  X  X eatures X  discovered in the dataset, and the corresponding column of H selects dataset images that are classified as carrying the feature most prominently. R1D seems to be the most successful at finding features and classifying im-ages; in each case, the column of W shows a particular highlight that distinguishes some images in the dataset from others. NMFDIV appears to be slightly inferior to R1D, while LSI is noticeably worse.
 In this experiment, the algorithms computed 30 basis vectors of the NMF. NMFDIV was allowed 500 itera-tions which took 727 seconds; in contrast, LSI required 20 seconds and R1D took 47 seconds.
 Additionally, R1D is effective at finding a sparse fac-torization. Table 1 demonstrates the sparsity in the first few columns of W and H . The first column of W and H is fully dense, because the data matrix appears to be approximately rank-one; its first singular value is dominant. Apart from this, the other columns of the NMF are sparse, and the sparsity can be controlled by the  X   X  parameter (here we have used  X   X  = 2). Al-ternatively, both NMFDIV and LSI perform a dense factorization with very few values near zero in any col-umn. In Tables 2 and 3 we illustrate LSI versus R1D (with parameter  X   X  = 4) on the TDT Pilot Study (TDT Study, 1997). The columns of each table are the lead-ing columns of W , with the leading terms per column displayed. The LSI results show that the topics are not properly separated and terms from different top-ics recur or are mixed. The columns in the R1D table are clearly identifiable topics, and the terms in each columns are all correctly associated with the given top-ics.
 NMFDIV (and the other implementations of NMF in nmfpack) were not run on this dataset because they would exhaust all of the computer X  X  memory. As noted earlier, R1D on text datasets is able to efficiently work with sparse matrices throughout its operation. R1D was able to compute 80 basis vectors of the TDT cor-pus in 171 seconds, whereas LSI required 269 seconds. We have proposed an algorithm called R1D for non-negative matrix factorization. It is based on greedy rank-one downdating according to an objective func-tion, which is heuristically maximized. We have shown that the objective function is well suited for identifying topics in the -separable text model. Finally, we have shown that the algorithm performs well in practice. This work raises several interesting open questions. First, the -separable text model seems rather too sim-ple to describe real text, so it would be interesting to see if the results generalize to more realistic models. A second arising question asks whether a re-sult like Theorem 1 will hold for the R1D algo-rithm. In other words, if the heuristic subroutine ApproxRankOneSubmatrix is applied to an -separable corpus, does it successfully identify a topic? Here is an example of a difficulty. Suppose n  X   X  much faster than L . In this case, the document j with the highest norm will be the one in which l j is very close to L and in which one entry A ( i, j ) is very close to L while the rest are mostly zeros. This is because the maximizer of k x k 2 subject to the constraint that k x k 1 = C oc-curs when one entry of x is equal to C and the rest are zero. It is likely that at least one instance of such a document will occur regardless of the matrix P (  X  ,  X  ) if n is sufficiently large. This document will then act as the seed for expanding M and N , but it may not be similar to any topic. This scenario can perhaps be prevented by a more intelligent selection of a starting vector for ApproxRankOneSubmatrix.
 Asgarian, N., &amp; Greiner, R. (2006). Using rank-1 bi-clusters to classify microarray data. Department of
Computing Science, University of Alberta, Edmon-ton, AB, Canada.
 Bergmann, S., Ihmels, J., &amp; Barkai, N. (2003). Iter-ative signature algorithm for the analysis of large-scale gene expression data. Physical Review E , 67 , 031902.
 Biggs, M., Ghodsi, A., &amp; Vavasis, S. (2008). Nonnegative matrix factorization via rank-one downdate. Available online at http://www.arxiv.org/abs/0805.0120.
 Boutsidis, C., &amp; Gallopoulos, E. (2007). SVD based initialization: A head start for nonnegative matrix factorization. In press.
 Cohen, J., &amp; Rothblum, U. (1993). Nonnegative ranks, decompositions and factorizations of nonnegative matrices. Linear Algebra and its Applications , 190 , 149 X 168.
 Deerwester, S., Dumais, S., Furnas, G., Landauer, T., &amp; Harshman, R. (1990). Indexing by latent seman-tic analysis. Journal of the American Society for Information Science , 41 , 391 X 407.
 Gillis, N. (2006). Approximation et sous-approximation de matrices par factorisation positive: algorithmes, complexit  X e et applications. Master X  X  thesis, Universit  X e Catholique de Louvain, Louvain-la-Neuve, Belgium. In French.
 Golub, G. H., &amp; Van Loan, C. F. (1996). Matrix com-putations, 3rd edition . Baltimore: Johns Hopkins University Press.
 Gregory, D. A., &amp; Pullman, N. J. (1983). Semiring rank: Boolean rank and nonnegative matrix rank. J. Combin. Inform. System Sci , 3 , 223 X 233.
 Hofmann, T. (1999). Probabilistic latent semantic analysis. UAI  X 99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,
Stockholm, Sweden, July 30-August 1, 1999 (pp. 289 X 296). Morgan Kaufmann.
 Hoyer, P. (2000). nmfpack -matlab code for nmf. http://http://www.hiit.fi/node/70.
 Hoyer, P. (2004). Non-negative matrix factorization with sparseness constraints. Journal of Machine Learning Research , 5 , 1457 X 1469.
 Kim, H., &amp; Park, H. (2007). Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data anal-ysis. Bioinformatics (to appear).
 Lee, D., &amp; Seung, H. (1999). Learning the parts of ob-jects by non-negative matrix factorization. Nature , 401 , 788 X 791.
 Paatero, P., &amp; Tapper, U. (1994). Positive matrix fac-torization: A non-negative factor model with op-timal utilization of error estimates of data values. Environmetrics , 5 , 111 X 126.
 Papadimitriou, C., Raghavan, P., Tamaki, H., &amp; Vem-pala, S. (2000). Latent semantic indexing: A proba-bilistic analysis. J. Comput. Syst. Sci. , 61 , 217 X 235. Peeters, R. (2003). The maximum edge biclique prob-lem is NP-complete. Discrete Applied Mathematics , 131 , 651 X 654.
 Stewart, G. W. (1993). On the early history of the sin-gular value decomposition. SIAM Review , 35 , 551 X  566.
 TDT Study (1997). Topic detection and tracking pilot study. http://projects.ldc.upenn.edu/TDT/.
 Vavasis, S. (2007). On the complexity of nonnegative
