 under which various polynomial-time methods are either con sistent, or conversely inconsistent. exploited so as to increase the statistical efficiency of est imation procedures. In this paper, we study the high-dimensional scaling of bloc k ` regularization, as compared to simpler ` the following questions. fraction  X   X  [0 , 1] of their entries. For this problem, we prove that block ` phase transition in terms of the rescaled sample size In words, for any  X  &gt; 0 and for scalings of the quadruple ( n, p, s,  X  ) such that  X  (ordinary ` transition as a function of the rescaled sample size problem sequences ( n, p, s ) such that  X  is too small X  X ore precisely, if  X  &lt; 2 / 3  X  X hen block ` illustration by comparison to empirical simulations. block-regularized programs for estimating sparse vectors . 2.1 Multivariate regression be a regression vector, and consider the family of linear obs ervation models paper, we assume that each w i has a multivariate Gaussian N (0 ,  X  2 I more background). 2.2 Block-regularization schemes Given a parameter q  X  [1 ,  X  ] , we define the ` corresponding to applying the ` that all of these block norms are special cases of the CAP fami ly of penalties [12]. solving the block-` where  X  between the different regression problems is induced by the block-norm regularization. Lasso [11, 6]. Another important case [9, 8], and the focus of this paper, is block ` The motivation for using block ` regression matrix B . Geometrically, like the ` in the matrix B . Intuitively, taking the maximum encourages the elements (  X  1 one i  X  X  1 , . . . , r } , then there is no additional penalty to have  X  j 2.3 Estimation in ` For a given  X  convex, since the quadratic term is rank deficient and the blo ck ` the optimal solution b B is in fact unique.
 p and r , and the sparsity index s = max difference set overlap among the different supports.
 the signed quantities sign(  X  i use primal or dual information from an optimal solution. ` ` R p  X  r . For each row k = 1 , . . . , p , compute the set M k : = arg max dual optimal solution associated with the optimal primal so lution. 2.4 Notational conventions S `  X  n matrix.
 ` our final two results (Theorem 2 and 3) show that block ` sharpness with some simulation results. 3.1 Sufficient conditions for deterministic designs terms of the minimum eigenvalue C as well as an ` them to scale.
 We assume that the columns of each design matrix X i , i = 1 , . . . , r are normalized so that incoherence condition on the design matrix is satisified: there exists some  X   X  (0 , 1] such that and we also define the support minimum value B For a parameter  X  &gt; 1 (to be chosen by the user), we define the probability which specifies the precise rate with which the  X  X igh probabi lity X  statements in Theorem 1 hold. incoherence condition (14) . Suppose that we solve the block-regularized ` larization parameter  X  2 we are guaranteed that: J correct. Note that we are guaranteed that b  X  i such that b  X  i related to geometric properties of the block ` some j  X  X  1 , . . . , r } , then there is no further penalty to having b  X  i S estimator, it is always guaranteed to correctly recover the union of supports J . Theorem 2, doing so requires additional assumptions on the s ize of the gap |  X  i S (  X  i )  X  S (  X  j ) . 3.2 Sharp results for standard Gaussian ensembles scaling for the Lasso, or ordinary ` associated with using ` Gaussian ensemble [2, 4] X  X .e., with i.i.d. rows N (0 , I to make quantiative comparisons with the Lasso.
 p the sample size n .
 In order to state our main result, we define the order paramete r or rescaled sample size  X  c 3.2.1 Sufficient conditions We begin with a result that provides sufficient conditions fo r support recovery using block ` Gaussian entries, and consider problem sequences ( n, p, s,  X  ) for which  X   X  &gt; 0 . If we solve the block-regularized program (6) with  X  n =  X  greater than 1  X  c 3.2.2 Necessary conditions We now turn to the question of finding matching necessary cond itions for support recovery. Gaussian entries. It is important to note that c conditions of Theorem 3(a) and (b) are equivalent. However, note that if c method could fail to recover the correct support even if  X  ` the gap is too large, then correct joint support recovery is n ot possible. 3.3 Illustrative simulations and some consequences ports using dual support recovery (10) X  X amely, P [ S versus the order parameter  X  sharp threshold.
 ` this naive approach is governed by the order parameter meaning that for any  X  &gt; 0 , it succeeds for sequences such that  X  such that  X  implies that the naive method is more efficient.
 With this notation, we have the following: Corollary 1. The relative efficiency of the block ` [11] Kim Y., Kim J., and Y. Kim. Blockwise sparse regression. Statistica Sinica , 16(2), 2006.
