 This paper presents a novel prototype hierarchy based clus-tering (PHC) framework for the organization of web collec-tions. It solves simultaneously the problem of categorizing web collections and interpreting the clustering results for navigation. By utilizing prototype hierarchies and the un-derlying topic structures of the collections, PHC is modeled as a multi-criterion optimization problem based on mini-mizing the hierarchy evolution, maximizing category cohe-siveness and inter-hierarchy structural and semantic resem-blance. The flexible design of metrics enables PHC to be a general framework for applications in various domains. In the experiments on categorizing 4 collections of distinct domains, PHC achieves 30% improvement in  X F 1 over the state-of-the-art techniques. Further experiments provide in-sights on performance variations with abstract and concrete domains, completeness of the prototype hierarchy, and ef-fects of different combinations of optimization criteria. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  clustering Algorithms, Performance, Experimentation.
 Hierarchical Clustering, Prototype Hierarchy, Hierarchy In-duction, Criterion Function
With the flourishing of user contributed services like Ya-hoo! Answers, discovering the utility of user-generated-contents becomes a research topic of interest to many researchers. The utility of user-generated-contents comes in two major aspects, the quality and accessibility. Efforts have been put to distinguish the good and bad quality content [1]. To make contents more accessible, state-of-the-art retrieval models like translation based language model [18] and syntactic tree matching [15] have achieved promising performance. Orga-nizing the huge collections of data for information navigation is another important direction in exploring web collections. Categorization, especially hierarchical clustering with labels and descriptions of clusters, enables browsing style of infor-mation access. Users can navigate through the hierarchy driven by their information needs [11, 17].

Currently, web services rely on users to construct topic hierarchies and assign objects into their nodes. Open Di-rectory Project (ODP) and Wikipedia are both examples of hierarchically organized web collections formed by com-munity of editors. Yahoo! Answers (YA) is organized in a hierarchical tree containing 728 nodes with 26 top-level cate-gories, relying on users to select a category for their postings. Besides the reliance on manual assignment, a hierarchy as large as YA X  X  directory is too coarse to contain a category like IPod (it is in Music &amp; Music players ) whose subtopics might be of interest to many users. These suggest the ne-cessity of automatic fine-grained hierarchical categorization.
Toward automatic categorization of web collections into hierarchies, supervised techniques that require manually-labeled corpora are not appropriate for dynamic Web infor-mation services [9]. Existing unsupervised techniques gen-erally focus either on clustering the collections into smaller groups [5, 17], or extracting labels for clustered groups [4]. SnakeT [6] is a successful hierarchical clustering engine that performs sequential clustering and labeling on snippets re-turned by search engines. However, the resulting clusters and labels may not be consistent and systematic because of its data-driven nature. LiveClassifier [9] addresses the cat-egorization and navigation in one go by utilizing predefined topic hierarchies and searching the training instances to feed into a supervised learner. This approach, however, ignores the underlying topic structure of the target collection; and the result is confined to the predefined hierarchy which may not be a perfect match to the collection.
 In this paper, we propose an unsupervised approach called Prototype Hierarchy based Clustering (PHC) to tackle the problem of web collection categorization and navigation. PHC utilizes the world knowledge in the form of prototype hierarchies, while adapts to the underlying topic structures of the collections. By following the structure of the proto-type hierarchy, PHC eliminates the problem of determining the number of clusters and assigning initial clusters.
Moreover, the PHC results are interpretable, comprehen-Figure 1: Categorizing a Yahoo! Answers dataset using the Prototype Hierarchy of IPhone sive, and organized. Unlike in general clustering schemes where no label is provided for the resulting groups, or in conceptual clustering [12] where extra efforts are needed to extract labels, in PHC the labels or the descriptions of clus-ters are already provided by the prototype hierarchy used. Therefore the trouble of inventing cluster labels is elimi-nated. What X  X  more, the labels provided by the supervision hierarchy are logically and systematically arranged, while the labels extracted from unsupervised clustering might be inconsistent and disorganized.

PHC allows flexible forms of supervision: the prototype hierarchy can come in different level of granularity, in dif-ferent forms, and even tailored to any specific applications. Thanks to the diversity of web content, the hierarchy can even be automatically extracted. It is thus less rigid than the example-based learning and constraining.

In the rest of this paper, we first provide an overview of PHC in Section 3. Section 4 gives details of the PHC problem and algorithm. Section 5 presents experiments and results analysis. Related work is reviewed in Section 5, and with the conclusion in Section 6.
Generally, PHC takes in as input a prototype hierarchy and a target collection on the same topic, and produces as output a data hierarchy that contains all the data items from the collection. For ease of discussion, we first give some basic definitions and notations.
Preliminary 1. A Hierarchy ( H )isdefinedasatreethat consists of a set of uniquely labeled nodes V and a set of parent-child relations R between these nodes. A Concept Hierarchy ( CH ) is a hierarchy whose V represents a set of concepts ,witheach being used as a label for each V .
Definition 1. A Prototype Hierarchy ( PH ) is defined as a hierarchy whose nodes set V represents a set of &lt;, X &gt; tuples, with  X  a Prototype serving as a typical example, description, or standard for the concept .

Definition 2. Data Hierarchy ( DH ) is a hierarchy that organizes a collection of objects d .Eachnodeof DH rep-resents a category of objects CO . Non-leaf nodes subsumes their child nodes in a recursive manner. The root of DH consists of a single broadest category containing all objects, and the leaves correspond to the finest categories.
PH is the hierarchy that supervises the categorization process. It can be seen as a concept hierarchy CH with each labeled node embodied by a prototype  X  .With PH and DH , we define the problem of Prototype Hierarchy based Clustering (PHC) as follows. Given a collection D of objects on a topic  X  , PHC partitions and maps D into the categories that are predefined by a PH on  X  , such that the formed objects clusters CO 1 , CO 2 , ... , CO k are organized in a DH with similar structures. The output DH is readily labeled by the PH , and thus could be easily browsed by users to find information at different granularity.
Figure 1 illustrates how the prototype hierarchy based categorization works. Suppose the problem is to organize an archive of Yahoo! Answer questions on iPhone .Given the dataset of questions and the predefined prototype hi-erarchy as shown in Figure 1, PHC assigns each object (question) of the dataset into the leaf nodes of the hierar-chy. For instance, question 1 is categorized into MobileMe and naturally becomes a member of Online Services .Note that the category IPhone:Software:Interface does not have a single object and question 7 has no appropriate category to assign to. These are two typical cases to be handled in the PHC algorithm. With all the objects being assigned, a data hierarchy that has the same structure with the pro-totype hierarchy is formed. A user may thus easily browse the organized dataset by navigating the prototype hierar-chy and clicking on any node to view the questions from the corresponding node of the data hierarchy.

This example suggests that clustering of a dataset accord-ing to a supervision hierarchy is not trivial. We identify the following requirements in the study of this paper: 1. The data hierarchy, like a taxonomy or an ontology, 2. The data and prototype hierarchy are to be matched 3. The distance between objects are measured by appro-
These requirements suggest th e criteria in c onstructing a data hierarchy from a dataset. They form the basis of the PHC framework. In the following Section, we address the requirements in the first three subsections, and induce a multi-criterion optimization function in the last subsection. To characterize the structure of a hierarchy, we introduce Hierarchy Metric and Information Function inspired by re-search in automatic taxonomy induction [19] and ontology Figure 2: Illustration of Prototype Hierarchy (i-iii) and Data Hierarchy (iv). The prototype hierarchy (i) is a full match of (iv), (ii) an incomplete match of (iv), and (iii) an excess match of (iv). learning, due to the similar nature of hierarchy and taxon-omy.

We define a hierarchy metric as a function that operates on all the nodes in a hierarchy, similar to ontology metric [19] on an ontology. Formally, it is a function h : V  X  V  X  R +, where V is the set of nodes in H . h ( ., . ) is recursively defined. For an adjacent pair of nodes v p and v q , the hierarchy metric is defined as the edge weight w ( e v p v q ). For the other pairs, the hierarchy metric h ( ., . )on H with edge weights w for any node pair v i ,v j  X  V is the sum of all edge weights along the shortest path between the pair: where P ( v i ,v j ) is the set of edges defining the shortest path from nodes v i to v j . The quality of the structure of a hier-archy is measured by the amount of information carried in H , defined as the sum of all hierarchy metrics in H : where i&lt;j reduces duplicated entries of h ( ., . )sincethe hierarchy metric is a symmetric measure.

Figure 2 (i) gives an example of a 6-node hierarchy. We can calculate the hierarchy metric between A and F as h ( A, F )= h ( A, C )+ h ( C, F )=2 . 8, and the Information Function of the hierarchy as the sum of 15 pairs of nodes, resulting in Info ( H )=38 . 5.
Minimum Evolution( obj 1 ) is designed to monitor the structural evolution of the data hierarchy. The data hierar-chy is incrementally hosting more objects until the whole collection is categorized and allocated. We assume that DH ( n +1) with n + 1 nodes to be the one that introduces the least changes of information from its previous status DH (
DH ( n +1) = arg min DH || Info ( DH ( n ) )  X  Info ( DH )
Therefore the optimal DH organizes the whole collection so as to introduce the least information changes since the ini-tial data hierarchy DH (0) :  X  DH = arg min DH || Info ( DH Info ( DH ) || ,where Info ( DH (0) ) = 0 since the initial DH is empty. By plugging in Equation 1 and 2, the minimum evolution objective function becomes: obj 1 suggests that the optimal DH on a collection is the one that contains the least information. It makes intuitive sense that the DH that compactly  X  X ncodes X  the collection into topic categories is the best.
The hierarchy metric and information function discussed above are defined in  X  X ode space X . For PH , the nodes are represented by prototypes. For DH , we use centroid to rep-resent a node of objects CO . Note that in previous work [7] on classification, centroid and prototype are two equivalent and interchangeable concepts. In this paper, we distinguish the two concepts by emphasizing that prototype is knowl-edge oriented and centroid is data oriented.

The centroids for DH nodes are generated in an incre-mental manner. When the first object is categorized into a category, it acts as the initial centroid of the category (and its ancestor categories). With subsequent objects be-ing inserted into the same category, the centroid is updated incrementally upon its previous status.

Suppose that the centroids and the objects are represented by vectors on the term space as new object d is inserted into a node, its centroid is updated by taking the algorithmic average of all the existing objects and
The new object in a leaf node automatically becomes members of its ancestor nodes whose centroids are to be updated too. We consider that the magnitude of the change decreases with the levels from the leaf node. The updating formula for a data hierarchy centroid is defined as where t is the number of edges on the shortest path be-tween the updated node and its descendent leaf nodes, and g ( t ) is a monotonically decreasing updating coefficient. In the implementation, a heuristic function g ( t )=1  X  t/ | is utilized, | H | is the height of the data hierarchy H ,and g ( t )  X  (0 . 0 , 1 . 0]. For example,when v is a leaf node, t =0, and g ( t )=1 . 0.
Preliminary 2. A full match between two hierarchies H 1 and H 2 is defined such that nodes V 1 = V 2 and relations R 1 = R 2 .A partial match between H 1 and H 2 can be ei-ther an incomplete match or an excess match. When H 1 is an incomplete match of H 2 , V 1 + V in = V 2 , R 1 + R in and the incomplete rate is defined as | V in | / | V 2 | ;when H is an excess match of H 2 , V 1 = V 2 + V e , R 1 = R 2 + R and the excess rate is defined as | V e | / | V 2 | . In the case of a partial match, the matched nodes and relations constitute a sub-hierarchy called a common hierarchy .
 Figure 2(iv) shows a data hierarchy, and Figure 2(i-iii) show three prototype hierarchies tha t are respectively a full match, an incomplete match, and an excess match of the data hier-archy. The common hierarchies of Figure 2(iii) and (iv) are (A,B,C,D,E,F) and (a,b,c,d,e,f), and G is an excess node.
Prototype Centrality ( obj 2 ) We assume that a proto-type is located at the center of an object cluster in the object space. Formally, the prototype centrality is expressed as where | V | is the number of nodes in the hierarchy, v p i represented by its prototype  X  i ,and v o i is represented by its centroid. v o i is updated with incrementally added new objects. c ( ., . ) measures the similarity between a v p i a v o i that is not empty. For this study, c ( ., . )employsthe simple and effective cosine similarity function on the term vectors of v p i and v o i .

The maximization of the prototype centrality objective is actuallyequivalenttoaddingadataobjectintoanode,so that the updated centroids (including the parental node cen-troids), are most similar to their corresponding prototypes.
Prototype-Data Hierarchy Resemblance ( obj 3 ) con-siders the common part of the data hierarchy and the pro-totype hierarchy. Formally, the prototype-data hierarchy resemblance is defined as follows: minimize obj 3 = 1 | where | M | is the size of the common hierarchy, v p i ,v and v o i ,v o j  X  V o are the corresponding nodes of PH and DH respectively.

Since PH is predefined and static, it is usually a partial match of DH . To enable DH to flexibly adjust to the col-lection distribution by having more or less nodes, obj 2 and obj 3 are measured on the common hierarchy.
In the ideal setting, the prototype hierarchy is the one that fully represents the underlying topic structure in the target collection, i.e. , the resulting data hierarchy is a full match. However, it is not possible to define such an ideal prototype hierarchy. Therefore the implementation should consider the cases when the predefined PH is a partial match.

If the predefined PH is an incomplete match of the un-derlying DH of a collection, we expand PH by adding more nodes to accommodate more categories. This is solved by adding dummy child nodes to the existing nodes in PH . For instance, in the Figure 1 example, equestion 7 has no appropriate category to assign to. This question will thus be assigned to an unnamed (dummy) node as the child of category IPhone:Online Service: Itunes Store 1 .

The added nodes, however, do not have a specific concept label and prototype description. For such scenario, we adopt the existing label extraction algorithms [4] for these new categories. The disadvantage of post-categorizing labeled concepts is that they may be less consistent with the existing concept space.
 If the predefined PH is an excess match of the underlying DH of a collection, some of the nodes in PH may result in empty category in DH ,suchas IPhone:Software:Interface in the Figure 1 example. For such cases, the empty nodes will be labeled as empty or removed from the browsing interface.
This is a combined effect of all the objectives, if only obj 2 and obj 3 are considered, question 7 will be assigned to IPhone:Online Service: Itunes Store:audio category.
Under Data Hierarchy, Object Metric M ( d i ,d j ) is defined as the distance (similarity) between a pair of objects d i d j within a node. Theoretically, any metric that satisfies the non-negativity, symmetricity, and triangular inequality criteria is a valid metric. For text clustering, a desirable metric is the one that captures the lexical, syntactic, and semantic features of the texts. We explore two state-of-the-art text retrieval models for estimation the distance metrics, the translation-based language model and the syntactic tree kernel matching model.
Translation-based language model (TBLM) is originally proposed to solve the lexical gap problem in document re-trieval. The monolingual translation probabilities capture the lexical semantic relatedness between mismatched terms in the query and the documents. We employ TBLM to mea-sure the semantic similarity between two texts d 1 and d 2 The similarity score function is similar to the retrieval func-tion proposed by Xue et al [18]:
P mx ( w | d 2 )=(1  X   X  ) P ml ( w | d 2 )+  X  where P ( w | d 2 ), the probability that w is generated from document d 2 , is smoothed using P ml ( w | D ), the prior proba-bility that w is generated from the document collection D .  X  is the smoothing parameter. P mx ( w | d 2 ) is the interpolated probability of P ml ( w | d 2 ) and the sum of the probabilities that w is a translation of t , P ( w | t ), weighted by P P ml is computed using the maximum likelihood estimator.
Due to its asymmetricity, P TBLM ( d 1 | d 2 ) cannot be di-rectly applied as a metric between a pair of text d 1 and d We thus define a symmetric distance metric by taking the average of the two scores that switch the role of d 1 and d as the  X  X uery X  and  X  X ocument X :
The tree kernel function is one of the most effective ways to represent the syntactic structure of a sentence [15]. Syn-tactic Tree Kernel Matching Model(STKM) is designed based on the idea of counting the number of tree fragments (sub-trees) that are common to both parsing trees: where W 1 and W 2 are sets of nodes(terms) in two syntactic trees T 1 and T 2 ,and C ( w 1 ,w 2 ) is the number of common tree fragments rooted in nodes w 1 and w 2 .Wang et al [15] improved node matching function C ( w 1 ,w 2 ) by adapting the tree kernel function to take into account the syntactic as well as semantic variations.

STKM is originally designed to measure the similarity be-tween two sentences. We generalize it into a similarity met-ric between multiple-sentence texts as follows: Table 1: Statistics of Dataset ( indicates webpages,  X  indicates question answer pairs).
 where s i and s j are sentences from d 1 and d 2 respectively.
M TBLM and M STKM emphasize on semantic and syn-tactic similarity of text objects respectively, we propose an integrated object metrics by taking their interpolation: M With the above defined M ( ., . ), similar objects can be better clustered into the same category.
Category Cohesiveness ( obj 4 ) objective requires that the collection is categorized such that objects in the same category are similar to each other and those in different cat-egories are dissimilar to each other. More specifically, when the intra-category similarity is the highest, and the inter-category similarity is the lowest, the categorization achieves the highest cohesiveness. Formally, the cohesiveness of the data hierarchy is defined as: where c ( v i ,v j ) measures the cosine similarity between the centroids of v i and v j . Note that in the denominator, only sibling categories compared, rather than as a whole as in [4]. The assumption is that categories at different levels could be different in terms of abstractness and thus not comparable.
In Section 3.1-3.3, we have discussed the criteria on con-structing a data hierarchy, and induced four objective func-tions obj 1 , obj 2 , obj 3 ,and obj 4 . In our proposed prototype hierarchy based clustering framework, all the criteria are to be satisfied, therefore the four objectives are to be optimized simultaneously: minimize O m =  X  1 obj 1  X   X  2 obj 2 +  X  3 obj 3  X   X  4 obj where  X  1 ,  X  2 ,  X  3 ,and  X  4 are introduced to control the con-tribution of each objective within the range of 0 to 1.
The multi-criterion optimization function leads to a greedy optimization algorithm, which at each object insertion step, produces a new data hierarchy by adding the new object into an appropriate node, which minimizes O m .
To evaluate the proposed prototype hierarchy based clus-tering scheme, we apply the techniques developed to recon-struct the subdirectories of ODP, and to organize Yahoo! Answers questions according to prototype hierarchies from external knowledge source. Table 1 shows the statistics of the prototype hierarchies and the associated collections of the four datasets.

For the ODP datasets, the prototype hierarchies are con-structed by extracting the subcategories of two topics, Com-puter Science (CS) and Religion and Spirituality (RS). The subcategory descriptions are extracted as prototypes. Some subcategories for portals ( e.g. , classified, directory) or those labeled by alphabetic orders, which are uninformative for the purpose of categorization and navigation, are removed. The two collections contain websites belonging to the categories of the extracted prototype hierarchies; where homepages of these websites are the objects of the collections.
The datasets under the topics Dental and IPod are col-lected from YA. The prototype hierarchy for the Dental dataset is directly extracted from Wikipedia hierarchy un-der Dentistry , where the prototype for each subcategory is the first part (the definition) of the corresponding Wikipedia article. The prototype hierarchy for IPod is a manually con-structed hybrid hierarchy by combining Wikipedia IPod ar-ticle hierarchy, Wordnet IPod meronyms, and product spec-ification from IPod website). The corresponding prototypes are also the combined descriptions from these three sources. The objects of the two collections are questions downloaded using YA API from Dental and Music &amp; Music players .We asked two dentistry graduate students and two computer science graduate students to organize the two collections by reading through the downloaded archives. Inter-rater agree-ments in terms of Kappa statistics are 85% and 91% for Dental and IPod respectively. The differences between the two annotators are made consistent by discussion.
Each of the four collections are equally divided into two parts ( C 1 and C 2 ) for training/developing and testing. The results are presented by taking the average of the two suits of experiments, using either part as testing sets.
The four datasets are carefully constructed to represent different scenarios. CS is a topic with a deep hierarchy, while RS has a broad hierarchy. IPod represents a con-crete domain and RS an abstract domain. ODP hierarchies (CS and RS) are noisier than Wikipedia hierarchy (Dental); while the semi-manually constructed hierarchy (IPod) has better quality.
To evaluate the performance of the proposed PHC model, we compare the following systems: 1) proKmeans : a prototype hierarchy enhanced K-means divisive hierarchical clustering. We choose a divisive algo-rithm as our baseline, as divisive algorithms has been found to be better solutions than agglomerative algorithms [20]. K-means works well when K and the initial partitioning are properly set. At each step of the division, we set K to be the number of leaf nodes in the prototype hierarchy; and the associate prototypes as the initial centroids. In this way, an intuitive unsupervised method is enhanced to be a relatively strong baseline. 2) LiveClassifier [9]: a state-of-the-art hierarchical classi-fier. We employ the approach 3 and KNN as the learning algorithm as in [9]. We adopt Yahoo BOSS API 2 as the http://developer.yahoo.com/search/boss/ Table 2: Comparison of the proposed PHC, the two baselines, and a supervised method CFC in terms of  X F 1 and mF 1 .
 search engine to gather snippets as training examples. The number of pseudo nodes for leaf node is set to be 6. We set K=1 for result evaluation. This method is used as the second baseline. 3) PHC-BOW, PHC-TBLM, PHC-STKM ,and PHC-TB-ST : 4 variations of PHC using Bag-of-Word, TBLM, STKM, and the combined TBLM and STKM respectively as the object metric. For  X  1 ,  X  2 ,  X  3 ,and  X  4 in Equation 10, we perform an exhaustive grid search of step size 0 . 1on[0 , 1] to find parameters that produce the best  X F 1 on the de-veloping set. For TBLM,  X  is set to 0 . 8and  X  to 0 . 5; the translation probabilities are trained using the set C and tested on C 2 / C 1 . For STKM, the four parameters (the node/size/depth weighting factors, and the weight of the matching tree fragment) are tuned by using the set C 1 / C as the development set. For TB-ST,  X  is set to 0 . 5. 4) CFC Classifier [7]: a state-of-the-art supervised text categorization technique. It is included to test the effective-ness of semi-supervised PHC against a supervised method. Experimental results are averaged using either set C 1 / C the training set.

We use the average accuracy of categorizing the leaf cat-egories as the performance measure for each dataset. In particular, we use the micro-averaging F 1 (  X F 1 )andmacro-averaging F 1 ( mF 1 ) as the performance metrics. F 1 is a combined form for precision ( p )andrecall( r ), which is de-fined as F 1 =2 rp/ ( r + p ).
The results are evaluated on all the leaf nodes and dis-played in Table 2. By comparing the vertical entries by different methods, we draw the following observations: (1) Both (unsupervised) baselines achieve reasonably high  X F 1 of about 0 . 6. For LiveClassifier , the results are con-sistent with that reported in the original experiment in [9] which shown it to be comparable to a supervised approach. For proKmeans , it achieves better  X F 1 than that reported in a state-of-the-art K-means clustering algorithm [8] that employs the sophisticated semantic features. This suggests that by specifying a prototype hierarchy for a collection, even a simple method like divisive K-means can categorize the collection reasonably well. (2) PHC with TB-ST surpasses all the other unsupervised systems. All the four PHC systems perform significantly better than the two baselines. Even the simplest PHC de-sign with BOW-based metric achieves improvement of above 10% over both baselines. This indicates that PHC is supe-rior in terms of utilizing the prototype hierarchy. proKmeans makes use of the prototypes and the number of children un-der each node; whereas LiveClassifier makes use of the re-lations of nodes and node labels (concepts). Either baseline benefits from the prototype hierarchy but not as compre-hensively as the PHC X  X . (3) PHC achieves the best performance when the object metrics (TBLM and STKM) are used in combination (TB-ST). It is however difficult to compare the PHC with TBLM and PHC with STKM. Generally, syntactic tree kernel based method works better on the two ODP collections; and trans-lation based method works better on the two YA collections. We conjecture that the ODP collections are more standard than YA in term of English grammar and thus more suit-able for syntactic tree parsing; while the YA collections use parallel question-answer corpus which is more suitable for generating translation probabilities. The combination of TBLM and STKM yields significant performance improve-ment when compared to the individual model. This shows that semantic and syntactic features complement each other and contribute to the overall result. (4) PHC with TB-ST even achieves comparable result with CFC, a state-of-the-art supervised classification algo-rithm. CFC can be deemed as using the hand-labeled cor-pora, while PHC makes use of hand-built hierarchy. This implies that a prototype hierarchy created by experts or web community is enough to help create good categorization of a large web collection, instead of needing to manually or-ganize and label the large corpus. Moreover, PHC provides the additional benefit of facilitating navigation. (5) PHC introduces new nodes into predefined hierarchy. In PHC-TB-ST setting, the numbers of new nodes intro-duced are 7 for CS, 11 for RS, 5 for Dental, and 3 for IPod. Since we deem the prototype hierarchies and collections in Table 1 as fully matched, the objects in the newly added nodes are simply evaluated as incorrect for ease of experi-mentation. This indicates that the results in Table 2 may underestimate the actual performance of PHC.
By comparing the horizontal entries of Table 2, we draw the following observations of PHC performance on different domains and prototype hierarchies: (1) PHC works better on concrete domains than on ab-stract domains. This is evidenced by comparing PHC per-formances on the two ODP collections. Computer Science achieves higher  X F 1 scores than Religion and Spirituality for the last 3 settings of PHC. It can be explained by two reasons: (i) the concepts in concrete domains like CS are more specific, therefore the prototypes associated with the concepts are more precise, informative, and have poten-tially more word overlapping with the target objects; (ii) the subtopics of abstract domains like RS are less systemat-ically arranged in a hierarchical structure and the ancestor-descendent relations between the subtopics are less obvious; Table 3: Objectives analysis. % of change in  X F 1 when a single objective is removed. therefore PHC is harder to benefit from the  X  X oose X  hierar-chies of the abstract domains. (2) A better prototype hierarchy can potentially enhance the categorization performance. Table 2 shows that in the PHC settings, IPod collections (with semi-manually com-piled hierarchy) attains the best results, followed by Dental (with Wikipedia hierarchy), and the worst are the two ODP collections. This indicates that high quality hierarchy will lead to better results. Besides the quality of prototype hier-archy, the quality of the prototypes used may also influence the categorization performance. Prototypes from ODP cat-egory descriptions are of various qualities, depending on the devotion of the category editor. Prototypes from Wikipedia articles are mediated by a larger community and thus main-tain a high level of quality. We conjecture that when more high quality hierarchies are available in digital form, PHC can achieve even better performance and wider adaptability.
We do a leave-one-out study on the optimization objec-tives to analyze the effects of each objective on the catego-rization. In the implementation, we set one of the parame-ters (  X  1 ,  X  2 ,  X  3 ,and  X  4 ) in Equation 11 to 0, and optimize the rest using grid search.

Table 3 shows that removing an objective from the multi-criterion optimization generally results in degraded perfor-mance. Prototype Centrality ( obj 2 ) influences all the four collections greatly, followed by Category Cohesiveness ( obj Prototype-Data Hierarchy Resemblance ( obj 3 ) influences Den-tal and IPod more than CS and RS. obj 2 is a bit more complex. A prototype may not repre-sent the perfect center for a cluster. Two cases exist: (i) the prototype provided is roughly located at the center of a category X  X  object space; the objects that are closer to the true center but farther away from the prototype may be ad-versely influenced by the prototype centrality score. (ii) the target collection is not large enough to form typical cate-gories; even though the prototype provided might be good, the object center itself may shift from general knowledge.
An interesting observation is that removing obj 1 (minimum evolution) increases the  X F 1 measureonRSandDental. By examining the clustering output, we find that the data hierarchy varies less from the prototype hierarchy without the minimum evolution. For example, two websites about Youth for Human Rights are under ODP category Religion and Spirituality: Scientology: Church of Scientology: Vol-unteer and Community Activities ; the two websites are cor-rectly categorized when the minimum evolution objective is removed. Under the full-fledge multi-criterion setting, the output data hierarchy has a new node created to accom-modate the two websites under Religion and Spirituality: Scientology: Church of Scientology , as a sibling of Volunteer and Community Activities . Since there is no standard on Figure 3: The influence of partially matched proto-types on PHC performance. whether some objects should be assigned into a new node or an existing node, the phenomenon implies that minimum evolution objective leads to a self-contained data hierarchy.
To further study the tolerance of PHC with partially matched prototype hierarchies, we deliberately manipulate the pro-totype hierarchies and the collections to examine the two types of mismatching effects: incomplete and excess proto-type hierarchy.

We mimic an incomplete (insufficient) prototype hierar-chy by deleting nodes from th e completely matched proto-type hierarchy. Similarly, we mimic an excess(overfitted) prototype hierarchy by inserting dummy nodes into the cur-rent hierarchy. Alternatively, we remove objects belong to a certain node from the collection, such that the node in the prototype hierarchy becomes redundant. We adopt the second approach because lacking certain groups of objects is more common in practical applications.

In Figure 3, we plot  X F 1 of PHC results on the four col-lections with excess rate and incomplete rate ranging from 0 to 20%. The two rates are defined in Section 3.2. From Figure 3(i), we can see that the  X F 1 measures on all the four collections slight degrade over the 0  X  15% excess rate. By examining the output object hierarchy, we find that for most excess nodes, PHC produces empty clusters. It suggests that PHC X  X  is robust against overfitted prototype hierarchies.
From Figure 3(ii), we can see that incomplete match is well tackled at 0  X  5% range and degrades drastically from 10% onwards. In the experimental setting, at the lower in-complete rate, only some leaf nodes are removed; whereas at higher incomplete rate, even those sub-hierarchies are re-moved. This suggests that PHC has only limited ability to  X  X reate X  categories. We conjecture that it is because PHC is designed to add one level of child nodes to existing nodes in the prototype hierarchy without considering multiply lev-els. In other words, our system needs further improvement to tackle the high incomplete rate, or seek more complete match prototype hierarchies to avoid the problem.
Hierarchical clustering has long been recognized as a nat-ural way to organize and navigate text collections [10, 5, 17]. Existing algorithms for hierarchical clustering are gen-erally either agglomerative, divisive, or combined [20]. The automatically generated clusters are however less neatly or-ganized as a manually constructed hierarchical tree like the ODP and Wikipedia hierarchies. Another limitation is that the clusters do not have labels to indicate the topics con-tained. Further more hierarchical clustering, labeling is more complicated since an internal node in the hierarchy has to be distinguished from its siblings, parent, and children.
World knowledge has been found to be useful in enhanc-ing clustering and labeling. For clustering, metric based [16] and constraints based [14] approaches utilize knowledge in the form of a small amount of labeled samples. Bilenko et al [2] integrated the two approaches and obtained further improvement over either approach. Hu et al [8] proposed to enrich short texts representation for clustering with syn-tactic and semantic features from WordNet and Wikipedia. For cluster labeling, Carmel et al [4] successfully enhanced it by extracting candidate labels from Wikipedia, in addition to the important terms that are extracted directly from the text. In our work, world knowledge comes in the form of a topic hierarchy and pro totype descriptions.

Various criterion functions for document clustering have been studied in [20]. These functions represent some of the most widely used criteria for document clustering, but not cover the structural aspects of the hierarchies.

Hierarchy and taxonomy induction has long been studied on concepts [13], noun-phrases [19], and word-based top-ics [3]. For the first time it has been applied as a crite-rion on hierarchical clustering in this work. Moreover, the concept hierarchies [13] and taxonomies [19] could be used to automatically extract prototype hierarchies for the PHC framework.

Our work is similar in spirit to conceptual clustering [12] which is distinguished from ordinary data clustering by gen-erating a concept description for each generated class. An-other similar work is LiveClassifier [9]. It tries to tackle the clustering and labeling problem in a reverse order. Assuming a predefined topic hierarchy, it augments the hierarchy and uses search engines to automatically gather training corpus for classifying websites into the hierarchy. Our framework is similar to theirs in term of specifying a topic hierarchy, but exploits more on the structure and evolution of the hierarchy rather than searching for training data.
This paper proposed a prototype hierarchy based cluster-ing framework for web collection categorization and navi-gation. By minimizing the hierarchy evolution, maximizing category cohesiveness and inter-hierarchy structural and se-mantic resemblance, the hierarchical clustering task is mod-eled as a multi-criterion optimization problem. Empirical results on categorizing 4 web collections of various domains have shown that PHC is superior to the two strong unsuper-vised baseline methods and comparable to a state-of-the-art supervised method.

In future work, we plan to optimize the efficiency of the proposed PHC algorithm and explore its applicability to multimedia collections with domain specific hierarchy and object metrics. [1] R. Baeza-Yates. User generated content: how good is [2] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [3] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. [4] D. Carmel, H. Roitman, and N. Zwerdling. Enhancing [5] S. Dumais and H. Chen. Hierarchical classification of [6] P. Ferragina and A. Gulli. A personalized search [7] H. Guan, J. Zhou, and M. Guo. A [8] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploiting [9] C.-C. Huang, S.-L. Chuang, and L.-F. Chien.
 [10] S. Johnson. Hierarchical clustering schemes. [11] D. J. Lawrie and W. B. Croft. Generating hierarchical [12] R. Michalski and R. Stepp. Learning from observation: [13] M. Sanderson and B. Croft. Deriving concept [14] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. [15] K. Wang, Z. Ming, and T.-S. Chua. A syntactic tree [16] E. Xing, A. Ng, M. Jordan, and S. Russell. Distance [17] G.-R.Xue,D.Xing,Q.Yang,andY.Yu.Deep [18] X. Xue, J. Jeon, and W. B. Croft. Retrieval models [19] H. Yang and J. Callan. A metric-based framework for [20] Y. Zhao, G. Karypis, and U. Fayyad. Hierarchical
