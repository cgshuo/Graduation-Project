 JIAUL H. PAIK and SWAPAN K. PARUI , Indian Statistical Institute Stemming refers to the process of transforming variant word forms to their common root. For example the words  X  X ducate X ,  X  X ducation X , and  X  X ducating X  are transformed to their common root  X  X ducat X . In Information Retrieval this process improves the ability of a system to match query document vocabulary. For morphologically rich languages its purpose is two-fold. On one hand, it increases the system recall rate [Xu and Croft 1998]. On the other hand, for highly inflectional languages, it reduces the index size significantly, thereby saving the disk storage and facilitating the keyword search process during query processing.

Effectiveness of stemming for English in ad hoc retrieval has produced mixed re-sults [Harman 1991]. But the subsequent studies have shown a consistent improve-ment [Hull 1996; Krovetz 1993] compared to no stemming. For many other languages, such as Slovene [Popovic and Willett 1999], Dutch [Kraaij and Pohlmann 1996], and Arabic [de Roeck and Al-Fares 2000] stemming improves the retrieval performance. Stemming is usually considered to be a recall enhancing method, but sometimes it also promotes the relevant documents in higher ranks resulting in improved precision [Xu and Croft 1998].

For languages such as English a number of stemmers are available and their perfor-mance has been extensively examined [Harman 1991; Hull 1996]. Stemming for other European languages such as Spanish and Hungarian [Majumder et al. 2008; Savoy 2008] is also well studied in the literature. For language-specific rules of stemming additional resources (like dictionary) are also sometimes employed [Krovetz 1993] to group morphologically related words. These language-specific stemmers have found use in the retrieval tasks in these languages. But for many Indian languages such resources are either unavailable or lack a fair amount of coverage to be able to give an acceptable performance. We have, theref ore, taken a purely corpus-based unsuper-vised method for stemming which does not need any additional resources or human intervention to generate the equivalence classes for a given language corpus. The pur-pose of this corpus-based stemming is to see i f it can enhance performance in retrieval tasks.

Our main objective here is to propose an unsupervised stemming algorithm which (i) is very fast, (ii) enhances the retrieval performance, (iii) can handle a wide range of languages, and (iv) is reasonably robust in terms of selection of parameter values. We proceed as follows: we first collect the words from a corpus and then collect the potential suffixes based on their frequency of occurrence. The underlying rationale is that if a language employs certain suffixes to generate word form variations, given a large enough text corpus, they will appear quite frequently. We then group the words based on the common prefix of a given length and measure the strength of the com-mon prefix of the class by incorporating the potential suffix information. The strength measures the goodness of a string to be considered a root of the equivalence class. If the common prefix fails to become a good root then the class must contain other roots and we try to find those iteratively. We have evaluated the performance of our proposed algorithm on four languages, namely, Bengali, Marathi, English, and Hun-garian. Among these languages, English is the least inflectional while the other three languages are inflectional with varying degrees. The experimental results show that for Bengali and Marathi the proposed method gives significant performance improve-ment over no stemming and performs equally well as the clustering based stemmer like YASS, which was designed to handle concatenative morphology. But the main dif-ference of our approach is that it is very fast and robust in terms of parameter selection unlike YASS [Majumder et al. 2007].

The article is organized as follows: we give the related background work on cor-pus based stemming in Section 2. In Section 3 we present our method for stemming. Section 4 gives the experimental setup, a description about the corpora used for exper-iments, and a weighting formula for retrieval. Experimental results and comparisons with other methods are given in Section 5. Results on computation time are presented in Section 6. Finally, we conclude in Section 7. Statistical methods of stemming is a popular approach [Goldsmith 2001; Xu and Croft 1998] because of their natural ability to detect the equivalence classes of variant words without any knowledge of the language. In information retrieval they are equally ca-pable of improving performance, when compared with the rule-driven stemmers such as Porter [Majumder et al. 2007].

Majumder et al. [2007] developed a generic clustering-based unsupervised tech-nique (YASS) for word normalization capable of handling a family of languages primarily suffixing in nature. They defined a set of string distance measures between word pairs that reward the long matching prefixes and penalize an early mismatch. Complete linkage clustering algorithm is used to discover the equivalence classes. Since the number of clusters is undetermined a priori and also the cluster center for a set of strings carries no rational interpretation, it motivated them to choose the graph clustering techniques rather than the partitional clustering techniques. The number of clusters was then determined by deleting the graph edges having weights (that is, the distances between two words) above some predefined threshold value. Their results have shown that the approach is comparable with the rule-based stemmers such as Porter and Lovins when performed over TREC English collection and provided substantial improvement for Bengali and French collection compared against no stemming.

Oard et al. [2000] did suffix discovery statistically from a text collection and elimi-nated it from a word ending. They first counted the frequency of every one, two, three, and four character suffix that would result in a stem of three or more characters for the first 500,000 words of the collection. Then they subtracted the frequency of the most common subsuming suffix of the next longer length from each suffix (for exam-ple, frequency of  X -ing X  from the the frequency of  X -ng X ). The adjusted frequencies were then used to sort all n -gram suffixes in descending order. In the case of English, the count versus rank plot was found to be convex and so the rank at which the second derivative was maximum was chosen as the cutoff limit for the number of suffixes for each length. It is to be noted here that this method is computationally very simple.
Character n -gram tokenization [McNamee and Mayfield 2004] is another attractive option to handle morphology in an alphabetic language. It was reported that 4-gram, in particular, performs well for European languages. One major pitfall with the n -gram is the increase in the size of the inverted index. A passage of k characters contains k  X  n + 1 n-grams of length n , but only approximately ( k +1) / ( l +1)words,where l is the average word length for the language. As a consequence (of index size expansion), there is a hefty increase in query processing time when n -grams are used: retrieval with 4-gram is 10 times slower than plain word retrieval.

Effectiveness of corpus analysis for stemming was investigated by Xu and Croft [1998]. The basic assumption of their approach is that the word variants that should be conflated will co-occur in the same docume nts and more specifically in the same text window (they experimented on a 100 word text window). Based on this they devised a co-occurrence metric that measures the sign ificance of association between two words. Graph partitioning algorithms (connected component, optimal partitioning) were used to refine equivalence classes generated by a number of aggressive stemmer like Porter. They experimented with Spanish test collection and observed improvements in recall.
Recently some language-specific word normalization techniques on Indian lan-guages were reported. Ramanathan and Rao [2003] reported work on Hindi stem-ming. They made a handcrafted set of suffixes and removed them from word endings based on some rules. However, they did not report any experimental results on in-formation retrieval. Dolamic and Jacques [2008] reported retrieval experiments on the effectiveness of light stemming strategies on Bengali, Hindi, and Marathi. They remove the inflectional and some frequently occurring derivational suffixes attached to the nouns and adjectives. In the TIDES surprise language exercise, Larkey et al. [2003] used a light stemmer for Hindi. They used a set of 27 suffixes (indicative of gender, number, tense, and nominalization) identified by a native Hindi speaker and removed the longest suffix first from word endings. They observed that stemming im-proves retrieval performance on unexpended monolingual queries. In Almeida and Bhattacharyya [2008], the authors study the effects of lexical analysis on Marathi monolingual retrieval. Their results show that lemmatization improved the retrieval performance of Marathi significantly.

In the light of the above, the motivation in the present article is to devise a corpus-based stemmer that is sufficiently fast and enhances retrieval performance significantly. We first identify in the lexicon a set of all suffixes of length n ( n = 1, 2, ...) by grouping words that share the same suffix and the number of words in a group becomes the frequency of the corresponding suffix. A suffix is called a potential suffix if its fre-quency in the lexicon is larger than a certain cut-off threshold. The rationale is that variant word forms of a language are generated by adding suffixes taken from a finite set of suffixes specific to the language and given a large enough corpus they also oc-cur sufficiently frequently. So the frequency is a good indicator of the potentiality of a suffix. Suffixes selected purely based on th eir frequency of occurrence in a lexicon have high recall but very low precision. That is, although most of the valid suffixes for the language concerned belong to the selected set, there are a lot more invalid suffixes. Solely depending on these suffixes is not an efficient approach to the task at hand. So our primary consideration is that if a set of words { w 1 ,w 2 , ........, w n } is generated by a root word w then the suffixes of w 1 , w 2 ,......, w n induced by w belong to the potential suffix set of the specific language. Based on this consideration we generate the equiva-lence classes combining the common prefix and potential suffix information. Thus the common prefix and potential suffixes help to recognize the better equivalence classes through mutual agreement.
 titative measures.

Definition 1. character/symbol: In the Indian language context, the length of a word/prefix/suffix is defined in the following way. A basic character or a consonant conjunct (compound character) is called a gl yph. A glyph with or without a diacritic (dependent character) is called a symbol. It is to be noted here that Indian languages are not alphabetic like English. We define the length of a word/prefix/suffix for In-dian languages as the number of symbols present in it. Henceforth, we will be using the terms  X  X haracters X  and  X  X ymbols X  for the alphabetic and non-alphabetic languages respectively. For example, consider the Bengali words anAdI and nAn  X  dAnIk (the let-ters in capital mean a de pendent character,  X   X   X  sign is a placeholder for consonant conjunct), the length of the first word is 3 ( a, nA, dI ) and that of second is 4 ( nA, n  X  dA, nI, k ).

Definition 2. k -equivalence class: A set S = { w 1 ,w 2 , ....., w n } of words is said ger), where common-prefix-length( w i ,w j ) maps a pair of character/symbol strings to a natural number representing the number of characters/symbols present in their longest common prefix. For example, consider the set of Bengali words { ak  X  rAm, ak  X  rAmkE, ak  X  rAmEr, ak  X  rAmi, ak  X  rAmAn, ak  X  rAmAni, ak  X  rAmAnEr, ak  X  rAmAno, ak  X  rAmAnkArI } . The longest common prefix of the group is ak  X  rAm which is of length 3( a, k  X  rA, m ). Therefore, this class of words forms a 3-equivalence class.
Definition 3. generated-class: Let S = { w 1 ,w 2 , ....., w n } be a set of character strings and R be another character string. R is the generator of the class S if R is the longest prefix of w i for all w i S . We call such a class the generated-class (R) .Considerthe example in definition 2. Here the generator of the class is ak  X  rAm .

Definition 4. potential-class: Let S = { w 1 ,w 2 , ....., w m } be the largest subset of generated-class (R) with  X  1 , X  2 , .....,  X  m (atmostone  X  i may be null in which case induced by R such that for all i frequency of  X  i  X   X  for some predefined value  X  (that is,  X  i is a potential suffix). S is said to be potentially generated by R and is denoted by potential-class(R).  X  is called the cut-off threshold for suffix fre-quency. If the set of potential suffixes is { kE, Er, i, o, I, kArI } then the ex-ample in definition 2 contains two potential classes. One containing the words { ak  X  rAm, ak  X  rAmkE (ak  X  rAm+kE), ak  X  rAmEr (ak  X  rAm+Er), ak  X  rAmi (ak  X  rAm+i) } where the root is ak  X  rAm (a cricketer X  X  name) and the other contains the words { ak  X  rAmAn, ak  X  rAmAni (ak  X  rAmAn+i), ak  X  rAmAnEr (ak  X  rAmAn+Er), ak  X  rAmAno (ak  X  rAmAn+o), ak  X  rAmAnkArI (ak  X  rAmAn+kArI) } , where the root is ak  X  rAmAn (at-tack). It can be seen that a potential class is a subset of a generated class. They are the same when  X  is 1.

Definition 5. strength of a root: Let R be a character string. The strength of R is defined as: We now present our algorithmic steps: Here there are two positive integer parameters, namely, k 1 (initial prefix length) and k 2 (finishing prefix length) where k 1 meters will be discussed in results section. (1) From the lexicon generate the k 1 -equivalence classes. Let the classes be (2) Check the strength of each class: For i = 1, 2, ...., N (3) Class Refinement: [Common prefix fails to become a potential root of the class
Note that in the above steps (say, Phase 1), the potential roots of length k 1 or more were identified. However, there may be valid potential roots having length smaller than k 1 . In order to find these roots, in Phase 2, Steps 1 and 2 are performed separately results obtained above are not the same as those obtained by replacing k 1 by k 2 and performing only Phase 1.
 The general procedure for the experiments on each language (Bengali, Marathi, Hun-garian, English) is as follows. (1) Collect all unique words, remove the numbers and stopwords. (2) Collect the potential suffixes based on their frequency in the lexicon. (3) Generate the equivalence classes based on the proposed approach. (4) Compare the retrieval results with no stemming (baseline) and other stemmers
We used TERRIER 1 retrieval system to perform all o ur experiments. Retrieval was done using IFB2 model [Amati and Van Rijsbergen 2002]. This model is a mixture of term frequency and inverse term frequency with two-level normalization. In the first level, the risk factor is considered in a ccepting a term as a good document descrip-tor. The fundamental assumption behind the first normalization is that the gain is directly proportional to the amount of risk involved in accepting a term as a good doc-ument descriptor for potentially relevant documents. The second level resizes the term frequency in the light of the length of the document by hypothesizing that the density function of the term frequency is inversely proportional to the length of the document. The query is assumed to be a set of independent terms. Documents have been indexed following the unigram word model assumption. Query-document matching formula is given by the Equation 1. Table I gives the meaning of different symbols used in the weighting function.
 The retrieval experiments have been carried out on four languages, namely Bengali, Marathi, English, and Hungarian. The Bengali corpus is the newspaper articles collected from the Bengali daily Anandabazar Patrika over a period of four years (2004-2007). The English experiments use the WSJ corpus of TREC along with 200 topics. The Marathi corpus was obtained from two Marathi dailies, Maharashtra Times and Sakal and the articles span from 2004 to 2007. The Hungarian corpus is the Magyar Hirlap collection of the CLEF [Peters et al. 2006], and we used 98 topics (two topics have no relevant document) from 2006 and 2007 for the experimentation. Bengali, Marathi, and English corpora are supplemented by 75, 75, and 200 topics respectively. In all cases, retrieval has been performed using T (title only) and TD (title and de-scription) fields separately. For all experiments, stopwords have been removed from both queries and documents. The detailed statistics about all the corpora used for experimentation are given in Table II. To evaluate the performance of the proposed method we use mean average precision (MAP) [Manning et al. 2008] as our primary evaluation measure. Precision at 10 and R ( R -precision), where R is the number of relevant documents, have been provided to show an improvement in retrieval precision against no stemming. As stemming is generally considered as a recall enhancing device, the number of relevant documents retrieved has also been given. The quantity within the parenthesis in each table in-dicates the percentage of increase or decre ase (-sign) in retrieval performance when compared with no stemming. Retrieval results on four languages, namely, Bengali, Marathi, Hungarian, and Eng-lish, are presented in this section. For each language, the following results are provided.  X  A table showing the retrieval results using T and TD query fields.  X  A table showing the p -values for paired t -test for average precision values (query wise) obtained by the proposed method versus the other methods.

The quantity in the bold face in the first table (for each language) show the best performance and the same in the second table (for each language) indicate statistically better performance. The algorithm that we propose here depends on three parameters. They are k 1 ,  X  and  X  .Thevalueof k 2 is set to 2 for all languages. The nature of a language does have a role to play in selecting the initial class prefix length. In Indian languages, a character is either a basic character with or without a modifier or a compound character with or without a modifier. For example, in Bengali the word LONDON has 3 characters ( LO, NDO, N ). Thus, the value of k 1 will naturally be smaller for an Indian language than in an European language. We have experimented with several values of this parameter for Bengali and Marathi and found 3 to be a good choice. However, for English and Hungarian 4, 5, and 6 give comparable performances and we have selected 5 to be the parameter value. Determination of the value of the  X  is corpus based. For this we first arrange the suffixes based on the descending order of their frequency and choose the frequency where the frequency distributi on curve shows a local plateau. For actual determination we take two consecutive 10 length non-overlapping windows and take the ratio of the average frequency of the right window to the average frequency of the left window. Initially, this ratio will be much smaller than 1 since the graph will have a highly negative slope. When for the first time, this ratio becomes very close to 1, the average frequency of the left window is taken as the value of the suffix frequency cut. In the present study, we check when this ratio becomes greater than 0.99 for the first time, when we move from more frequent suffixes to less frequent suffixes. The value of third parameter,  X  (root strength) influences the retrieval performance to an extent. We experimented on four different values of  X  ranging from 0.5 to 0.8 with a step length 0.1. All languages show consistently good results at  X  =0 . 6and  X  =0 . 7. It is observed that the average precision goes down as the root strength value is increased from 0.8 or decreased from 0.5. Table III shows the influence of the root strength parameter (  X  ) on all four languages for T and TD runs. It is found that the column averages (in T and TD runs) are the maximum when the value of  X  is 0.6. We obtain the experimental results by setting the value of  X  at 0.6 and k 1 at 3 for Bengali and Marathi, and at 5 for English and Hungarian. Bengali experiments were performed using 75 queries and 123,047 documents. A stop-word list containing 363 words 2 was used. For Bengali experiments, we compared the performance of our method with YASS, 4-gram approach (unicode level), and a com-putationally simple corpus based approach proposed by Oard et al. [2000]. Retrieval results with no stemming are also reported. To cluster the lexicon using YASS an edge threshold cut is to be determined. We experimented with two different edge thresh-olds, namely, 0.55 and 1.5 as proposed by the authors, and found in this particular corpus YASS produced better results when the edge threshold chosen was 1.5. We use this threshold for our experiments. Table IV presents the retrieval results obtained by the methods discussed above. Out of 2610 relevant documents, the unstemmed, Oard, 4-gram; YASS; and the pro-posed methods retrieve 2186, 2309, 2283, 2325, and 2345 documents, respectively, for T field and 2315, 2406, 2391, 2465, and 2482, respectively, for TD field. The perfor-mance of the proposed method is statistically the same as that of YASS. This is clear from Table V in which the p values of the paired t -test are given. It is also clear from Table V that the proposed method (both in T and TD runs) performs statistically better than the unstemmed, Oard, and 4-gram methods. YASS was designed to handle the kind of morphology that the Marathi language pos-sesses. Therefore, we compared our method against no stemming and YASS along with 4-gram and Oard X  X  method. We used a stopword list of 99 words 3 to remove the insignificant words from index and query. Like Bengali we set the values of k 1 and  X  to 3 and 0.6, respectively.

The Marathi corpus contains 99,362 documents and on average each document has a length of 273 words. The lexicon consists of 854,324 unique words and is much larger than the other lexicons used in our experiments. Although the number of documents in Marathi corpus is less than those of Bengali and English, the lexicon size is much higher than the other languages which is indicative of the morphological complexity of Marathi.

The results in Table VI show that the proposed unsupervised approach provides a mean average precision improvement of 34.3% (for T) and 38.5% (for TD) compared to no stemming and an improvement of 24.5% and 18.9% compared to YASS in T and TD settings respectively. Four-gram is consistently better than all other methods in both the query fields. Four-gram performs almost 7% better than the proposed method in both the query field settings. Among 1,542 relevant documents, the proposed method retrieved 1,311 (for T) and 1,397 (for TD) documents which is an improvement of 17.5% and 14.4% with respect to the unstemmed counterpart which retrieved 1,116 and 1,221 documents respectively. Here 4-gram is the best-performing approach. Like Bengali, experimental results reveal that stemming (by all methods) on Marathi leads to improved precision and recall. Table VII summarizes the statistical significance test between the proposed approach and all other methods in both T and TD runs in terms of average precision values (query wise). From this it is clear that the proposed method performs statistically better than all the other methods except 4-gram. Also, the performance difference between the pro posed method and 4-gram is statistically insignificant. Hungarian language has an extensive inflectional and derivational morphology and is rich in compound words. Being motivated by the nature of the language, we chose to test our approach on it. For these experiments, we used Magyar Hirlap text col-lection of CLEF and 98 topics from the year 2006 and 2007. The lexicon contains 528,315 surface words, and 98 test queries were supplemented by 2,219 relevant doc-uments, on average 23 documents per query. We conducted six retrieval runs. The first does not use stemming, the second uses Oard X  X  approach, the third, fourth, fifth, and sixth use 4-gram, YASS (threshold chosen 1.5), Porter counterpart of Hungarian language (Snowball stemmer) implemente d with the TERRIER platform and the pro-posed method respectively.

Table VIII shows that the proposed method provides an improvement of 58.5% (T) and 43% (TD) in the mean average precision against no stemming and is better than its closest counterpart the Snowball stemmer (in T and TD). Among all the methods, it is numerically observed that the 4-gram and the proposed method remain almost equally competent at low recall levels, but at higher recall levels, the proposed method outper-forms the 4-gram approach, in both T and TD runs. Table IX shows that the difference in average precision value between the proposed method and Snowball stemmer is sta-tistically significant in the T run but not in the TD run. However, the proposed method is significantly (statistically) better than all other four runs (no stem, Oard, 4-gram, and YASS). The next set of experiments compares the performance of the proposed method for Eng-lish language on WSJ TREC collection with 200 queries (TREC query 1-200). The re-call base contains 20,791 relevant documents, an average of 104 documents per query. The lexicon contains 179,387 surface words and the collection has 173,252 documents.
Table X presents the results on WSJ collection for 200 TREC queries. For the T queries Porter and the proposed method perform almost equally (Porter has a very marginal edge over the proposed one) in terms of MAP values, whereas the proposed method performs better than the others. In the TD queries all methods except 4-gram provide very similar performance and are better compared to no stemming. Inter-estingly, in both the T and TD runs, the proposed method achieves higher precision at 10 and retrieved more relevant docume nt than Porter stemmer. In TD setting, Oard X  X  approach and the proposed method perform equally well in terms of MAP value. Table XI shows that the proposed method performs significantly better than no stemming, YASS, Oard and 4-gram for T queries and for TD queries it is significantly better than no stemming, YASS and 4-gram, while its performance is statistically the same as that of Oard method. Also, in both T and TD settings, the proposed method and Porter are statistically equal performers. We have measured the processing time taken by various steps of our algorithm on a Pentium 4 standalone Linux machine. First, the lexicon is created from the text corpus and the algorithm was run on it. On Bengali corpus of lexicon size 533,605, it takes about 2.5 minutes to collect the suffixes and about 0.5 minute to generate the equiva-lence classes. Collecting suffixes from Marathi lexicon (854,324 words) takes about 3 minutes and grouping them takes about 0.75 minute. It took 0.8 minute (0.6 minute on suffix collection and 0.2 minute to equivalence class formation) on Hungarian data and 0.3 minute on English data (0.2 minute to collect suffixes and 0.1 minute to group similar words). We compared the processi ng time taken by our algorithm with YASS and Oard X  X  approach both of which are corpus based. Table XII summarizes the time taken on four test corpus and the comparison with YASS and Oard X  X  method. In this section we analyze the performance of the proposed method for four languages. We particularly chose the queries where th e performance differences are large. Con-sider the query 164 ( Generic Drugs -Illegal Activities by Manufacturers ) of the TREC collection. In this query the proposed method achieved more than 10% better MAP than Porter stemmer. The difference lies i n the fact that Porter stemmer conflates the terms illegal and illegible to illeg , as a result the document frequency of the term illegal gets increased, that in turn lowers the importance of the term illegal which is important for this particular query. On the other hand, the proposed method puts them in two separate equivalence classes. Because of this the term illegal did not lose its im-portance (as a higher idf value is mo re discriminatory). In query 102 ( Laser Research Applicable to the U.S. X  X  Strategic Defense Initiative ) the Porter stemmer performed bet-ter than the proposed stemmer. By analyzing some of the top ranked documents we noticed that the term strategic itself is very contributory. The proposed method groups the terms strategy, strategies, strategic, strategical together, but the Porter stemmer keeps them in two separate groups. As a consequence the conflation by the proposed stemmer retrieved many documents at the top of the rank list that talk about general defense strategies since the terms defense and research dominate the importance of the term strategic , whose presence happens to be very important for the relevance of a document for this query. Similar observations are now given for the performance differences between YASS and the proposed method. Query 174 ( Hazardous Waste Cleanup ) is one such example where the proposed method performed significantly bet-ter than YASS. The key fact that made the difference is the proposed method X  X  ability to conflate the term hazardous to hazard . As a result it retrieved the documents which contain hazard also. On the other hand, hazardous and hazard were assigned to two different equivalence classes by YASS resulting in poorer retrieval performance. Com-ingtothequery70( Surrogate Motherhood ), we observed that the proposed method was poorer than YASS. YASS groups motherhood in the group of mother ,whichthe proposed method failed to achieve (as the suffix hood is very rare). Since the terms mother and motherhood are very related, this conflation boosted the performance of the query by YASS.

When analyzing the queries of the other languages, we observe the same phenom-enon as in English : the distribution of query terms and their variants is very often performance determining factor. However, there are other issues that we have no-ticed. One of the YASS X  X  ability is to handle the tail end spelling variation. For ex-ample, the FIRE topic 54 ( HIV and AIDS epidemic ) for Bengali collection contains the term mAhAmArII (means epidemic). For this query YASS performed fairly better than the proposed method because many relevant documents contain the term mAhAmArI with the spelling different (the last character is a short  X  X  X  instead of long  X  X  X ) from the one in query term. Since the proposed method is based on the frequent suffixes, it could not group the two words together although they are essentially the same. On the other hand, for the Bengali topic 39 ( Attacks on American soldiers in Iraq ), YASS could not group Iraqi (inhabitant of Iraq) and its variants with Iraq ,whichthepro-posed method was able to do. This enhances the performance of the proposed method for this query. In the case of Hungarian the proposed method outperformed YASS in query 315 ( Doppingol  X  as a sportban ). Here YASS failed to conflate sportban to sport although they are related terms whereas th e proposed method did it correctly. Such a conflation retrieved many relevant documents and eventually gave rise to better MAP. For Marathi the largest performance difference was observed with query no 41 ( New Labour Laws in France ). The key factor here that led to such a performance difference is the structure of the equivalence classes corresponding to the term kAyAdE generated by the two methods (YASS and the proposed). YASS could not group many variants of kAyAdE (a keyword in the Marathi query) with it and splits the equivalence class incorrectly. On the contrary the proposed m ethod was able to do it more correctly and as a result it improved the performance. One major pitfall of YASS is its inability to handle the large equivalence classes due to the addition of longer suffixes, which is very common in both Hungarian and Marathi. Since the complete linkage cluster-ing algorithm ensures that every pair of members of a cluster must have a distance less than a given value, it splits the large equivalence classes into several clusters. Furthermore, the clusters generated by the complete linkage algorithm is sensitive to the order in which the members are chosen and this in some occasions forms er-roneous classes. These are perhaps the most notable reason why YASS performed poorer than the proposed method in these two languages. On the other hand, the pro-posed method incorrectly splits a valid equivalence class since it ignores infrequent suffixes.

Overall, for Bengali, YASS and the proposed method outperform each other almost in equal number of queries, and in English, Hungarian, and Marathi the proposed method beats YASS in a large number of queries, but remains equally competent with the Porter stemmer. Table XIII summarizes the query-based performance comparison of the proposed method. The entries in the Table XIII are the number of queries for which a method is better while the entries within the parentheses indicate the number of queries for which a method provides at least a 10% better MAP value.
 One important observation about stemming in general is that it is query specific. That is, terms that should be grouped together depend on the nature of the query as well as the distribution of the query terms and their variants in the corpus. For example, the conflation of prevention to prevent reduces MAP drastically for the query 175 ( NRA Prevention of Gun Control Legislation ) of TREC collection, although they are morphologically related terms. This is because the term prevent is frequent in the corpus and very often is from the documents which convey different topics than the term prevention and the system retrieves many documents at the top which are not relevant.

Another query-by-query analysis shows that although a great many number of queries benefited from stemming, several queries were also hurt by it. The possible reason for this is that when generating an equivalence class based on lexicographic similarity (which we adopted) there might be a possibility that many semantically unrelated terms become part of an equivalence class thereby making the class more semantically heterogeneous. To actually understand the impact of the problem, we manually corrected the equivalence classes of 5 badly hurt Bengali queries due to stemming and observed that four queries improved the performance compared to stemmed run and among these, two queries even did better than no stemming. One query remains at the same level. Table XIV gives the MAP values of the five queries in three different circumstances. We proposed an unsupervised generic stemming algorithm suitable for languages that are inflectional in nature. The proposed algorithm is entirely corpus based and does not employ any language-specific rules. Retrieval experiments on Bengali, Marathi, Hungarian, and English data sets show that although it is unsupervised, it gives a comparable performance on English and Hungarian with respect to the linguistic rule-based stemmers. For all the four languages under consideration, it provides a sig-nificant performance improvement compared to no stemming. The proposed method performed equally well with the clustering based algorithm YASS on Bengali but sig-nificantly better than YASS on all the other three languages.

The main difference of the proposed method with YASS is its low computational overhead. For a moderate size lexicon, YASS takes hours to complete the job whereas the proposed method takes a few minutes. More specifically our conclusions are (1) Stemming improves recall as well as precision for Indian languages such as Bengali and Marathi and a European language such as Hungarian, (2) the performance of the proposed unsupervised method is comparable with the linguistic rule-based method such as Porter and Snowball algorithms, and (3) the computational cost is very low compared to the clustering-based method such as YASS.

Though Oard X  X  method is computationally simple, its retrieval performance is in-ferior to that achieved by the proposed method. Also, the proposed method performs significantly better than n -gram approach in all the four languages except Marathi.
