 The Ohio State University University of Edinburgh University of Edinburgh unit selection synthesis that takes the resulting prosodic structure into account. The system selects the most important options to mention and the attributes that are most relevant to from the information structure using Combinatory Categorial Grammar in a way that allows pitch accents and edge tones is shown to yield prosodic structures with significantly higher acceptability than baseline prosody prediction models in an expert evaluation. These prosodic structures are then shown to enable perceptibly more natural synthesis using a unit selection voice that aims to produce the target tunes, in comparison to two baseline synthetic voices. An its contribution to listeners X  ratings. 1. Introduction In an evaluation of nine spoken dialogue information systems, developed as part of the
DARPA Communicator program, the information presentation phase of the dialogues was found to be the primary contributor to dialogue duration (Walker, Passonneau, and Boland 2001). During this phase, the typical system sequentially presents the set of options that match the user X  X  constraints, as shown in Figure 1. The user can then navi-gate through these options and refine them by offering new constraints. When multiple options are returned, this process can be exacting, leading to reduced user satisfaction. tiallymakesithardfortheusertorememberinformationrelevanttomakingadecision. Toreduceusermemoryload,weneedalternative strategiesforsequentialpresentation.
In particular, we require better algorithms for: 1. selecting the most relevant subset of options to mention, as well as the 2. determining how to organize and express the descriptions of the selected system, reviewing and extending the description given in Moore et al. (2004). FLIGHTS follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore 2000; Walker et al. 2002) in applying decision-theoretic models of user preferences to the generation of tailored descriptions of the most relevant available options. Multi-attributedecisiontheoryprovidesadetailedaccountofhowmodelsofuserpreferences can be used in decision making (Edwards and Barron 1994). Such preference models have been shown to enable systems to present information in ways that are concise and 160 tailored to the user X  X  interests (Carenini and Moore 2001; Walker et al. 2004; Carenini and Moore 2006). Decision-theoretic models have also been commercially deployed in Web systems. 3 (with respect to the user model) is presented first, followed by the most compelling remaining options, in terms of trade-offs between attributes that are important to the user. (Multiple options are selected only when each offers a compelling trade-off.) An important property of this strategy is that it naturally lends itself to the determination of information structure, as well as the contents of referring expressions. Thus, to help make the trade-offs among the selected options clear to the user, FLIGHTS (1) groups attributesthatarepositivelyandnegativelyvaluedfortheuser,(2)choosesreferringex-pressionsthathighlightthesalientdistinguishingattributes,(3)determinesinformation structure and prosodic structure that express contrasts intelligibly, and (4) synthesizes utterances with a unit selection voice that takes the prosodic structure into account.
As such, FLIGHTS goes beyond previous systems in adapting its output according to user preferences at all levels of the generation process, not just at the levels of content selection and text planning.
 (1995) and Steedman (2000a) in using Combinatory Categorial Grammar (CCG) to convey the information structure of sentences via pitch accents and edge tones. To adaptPrevostandSteedman X  X approachtoFLIGHTS,weoperationalizetheinformation structural notion of theme to correspond to implicit questions that necessarily arise in presenting the trade-offs among options. We also refine the way in which prosodic structure is derived from information structure by allowing for a more flexible, one-to-many mapping between themes or rhemes and intonational phrases, where the final choice of the type and placement of edge tones is determined by n -gram models. To investigate the impact of information structural grammatical constraints in our hybrid rule-based, data-driven approach, we compare realizer outputs with those of baseline n -gram models, and show that the realizer yields target prosodic structures with signif-icantly higher acceptability than the baseline models in an expert evaluation. markup to the speech synthesizer. The synthesizer uses this prosodic markup in the text analysis phase of synthesis in place of the structures that it would otherwise have to predict from the text. The synthesizer then uses the context provided by the markup to enforce the selection of suitable units from the database. To verify that the prosodic markup yields improvements in the quality of synthetic speech, we present an exper-iment which shows that listeners perceive a unit selection voice that aims to produce the target prosodic structures as significantly more natural than either of two baseline unit selection voices that do not use the markup. We also present an expert evaluation andf0analysiswhichconfirmthesuperiorityofthegenerator-driven intonationandits contribution to listeners X  ratings.
 proach to natural language generation (NLG) in the information presentation phase of a FLIGHTS dialogue, including how multi-attribute decision models are used in content selection; how rhetorical and information structure are determined during dis-course planning; how lexical choice and referring expressions are handled in sentence planning; how prosodic structures are derived in surface realization; and how these prosodic structures compare to those predicted by baseline n -gram models in an expert evaluation.Section3describeshowtheprosodicstructuresareusedintheunitselection voice employed in the present study, and compares this voice to the baseline ones used in our perception experiment. Section 4 provides the methods and results of the perception experiment itself, along with the expert prosody evaluation and f0 analysis.
Section 5 compares our approach to related work. Finally, Section 6 concludes with a summary and discussion of remaining issues. 2. NLG in FLIGHTS 2.1 TailoringFlight Descriptions
To illustrate how decision-theoretic models of user preferences can be used to tailor descriptions of the available options at many points in the generation process, let us consider the following three hypothetical users of the FLIGHTS system: Student (S) A student who cares most about price, all else being equal.

Frequent Flyer (FF) A business traveler who prefers business class, but cares most
Business Class (BC) AnotherbusinesstravelerwhoprefersKLM,butwants,aboveall, certain day, and would like to arrive by five o X  X lock in the afternoon. FLIGHTS begins thedialoguebygatheringthedetailsnecessarytoquerythedatabaseforpossibleflights.
Next,itusesthepreferencesencodedintheusermodeltoselectthehighestrankedflight for each user, as well as those flights that offer interesting trade-offs. These flights are then described to the user, as shown in Figure 2. 4 expensive, direct flight that arrives near the desired time. The Ryanair flight is also 162 mentioned as a possibility, as it has the best price; it ends up ranked lower overall than the BMI flight though, because it requires a connection and arrives well in advance of the desired arrival time. For the KLM frequent flyer (FF), life is a bit more complicated:
A KLM flight with a good arrival time is offered as the top choice, even though it is a connecting flight with no availability in business class. As alternatives, the direct flight on BMI (with no business class availability) and the British Airways flight with seats available in business class (but requiring a connection) are described. Finally, for the must-have-business-class traveler (BC), the British Airways flight with business class available is presented first, despite its requiring a connection; the direct flight on BMI is offered as another possibility.
 lection and ordering, they also have more subtle effects on many aspects of how the selected content is organized and expressed, as explained subsequently.

Referring expressions: Ratherthanalwaysreferringtotheavailableflightsinthesame
Aggregation: For conciseness, multiple attributes may be given in a single sentence,
Scalar terms: Scalar modifiers like good ,asin good price ,and just ,asin just fifty pounds ,
Discourse cues: Attributes with negative values for the user are acknowledged using
Information structure and prosody: Compelling trade-offs are always indicated via 2.2Architecture
The architecture of the FLIGHTS generator appears in Figure 3. OAA (Martin, Cheyer, and Moran 1999) serves as a communications hub, with the following agents responsi-ble for specific tasks: DIPPER (Bos et al. 2003) for dialogue management; a Java agent thatimplementsanadditivemulti-attributevaluefunction(AMVF),adecision-theoretic model of the user X  X  preferences (Carenini and Moore 2000, 2006), for user modeling;
OPlan (Currie and Tate 1991) for content planning; Xalan XSLT 2004, 2006a, 2006b) for sentence planning and surface realization; and Festival (Taylor,
Black, and Caley 1998) for speech synthesis. The user modeling, content planning, sen-tence planning, and surface realization agents are described in the ensuing subsections.
The NLG subsystem takes as input an abstract communicative goal from the dialogue manager. In the information presentation phase of the dialogue, this goal is to describe the available flights that best meet the user X  X  constraints and preferences. Given a communicativegoal,thecontentplannerselectsandarrangestheinformationtoconvey by applying the plan operators that implement its presentation strategy. In so doing, it makes use of three further knowledge sources: the user model, the domain model, and the dialogue history. Next, the content plan is sent to the sentence planner, which uses XSLT templates to perform aggregation, lexicalization, and referring expression generation. The output of sentence planning is a sequence of logical forms (LFs). The use of LF templates represents a practical and flexible way to deal with the interaction of decisions made at the sentence planning level, and further blurs the traditional distinction between template-based and  X  X eal X  NLG that van Deemter, Krahmer, and
Theune (2005) have called into question. Each LF is realized as a sentence using a CCG lexico-grammar (Steedman 2000a, 2000b). Note that in contrast to the generation archi-tectures of, for example, Pan, McKeown, and Hirschberg (2002) and Walker, Rambow, and Rogati (2002), the prosodic structure of the sentence is determined as an integral part of surface realization, rather than in a separate prosody prediction component. The prosodic structure is passed to the Festival speech synthesizer using Affective Presentation Markup Language (de Carolis et al. 2004; Steedman 2004), or APML, an XML markup language for the annotation of affect, information structure, and prosody.
Festival uses the Tones and Break Indices (Silverman et al. 1992), or ToBI, accents and edge tones X  X pecified as APML annotations X  X n determining utterance phrasing and intonation, and employs a custom synthetic voice to produce the system utterances. 164 2.3 User Modeling
FLIGHTS uses an additive multi-attribute value function (AMVF) to represent the user X  X  preferences, as in the GEA real estate recommendation system (Carenini and
Moore 2000, 2006) and the MATCH restaurant recommendation system (Walker et al. 2004). Decision-theoretic models of this kind are based on the notion that, if anything is valued, it is valued for multiple reasons, where the relative importance of different reasons may vary among users.
 arrivalordeparturetime.Thefollowingattributescontributetothisobjective: arrival-time , departure-time , number-of-legs , total-travel-time , price , airline , fare-class ,and layover-airport . As in MATCH, these attributes are arranged into a one-level tree.
 maps from the features of a flight to a number between 0 and 1, representing the value of that flight for that attribute, where 0 is the worst and 1 is the best. For example, the function for total-travel-time computes the difference in minutes between the flight X  X  arrival and departure times, and then multiplies the result by a scaling factor to obtainanevaluationbetween0and1.Thefunctionsforthe airline , layover-airport , and fare-class attributes make use of user-specified preferred or dispreferred values for that attribute. In the current version of these functions, a preferred value is given a score of 0.8, a dispreferred value 0.2, and all other values 0.5. about flight selection. Situational features are incorporated in two ways. The requested origin and destination are used as a filter when selecting the set of available options by querying the database. In contrast, the requested arrival or departure time X  X f specified X  X s used in the corresponding attribute X  X  evaluation function to give a higher score to flights that are closer to the specified time. If an arrival or departure time is not specified, the corresponding attribute is disabled in the user model.
 weightedsumofitsevaluationoneachattribute.Thatis,if f representstheoptionbeing evaluated, N isthetotalnumberofattributes,and w i and v i andthevalueforattribute i ,thentheevaluation v ( f )ofoption f iscomputedasfollows: user must rank the attributes in order of importance, and he or she must also specify any preferred or dispreferred attribute values for the airline , layover-airport ,and fare-class attributes. In FLIGHTS, we also allow users to specify a partial ordering of the rankings, so that several attributes can be given equal importance when registering tousethesystem.Figure4showstheusermodelsforthestudent(S),frequent-flyer(FF), and business-class (BC) users discussed earlier; because no departure time is specified in the sample query, departure-time is not included in these examples.
Asinpreviouswork,weuseRankOrderCentroid(ROC)weights(EdwardsandBarron 1994). This allows weights to be assigned based on rankings, guaranteeing that the sum will be 1. The n th ROC weight w R n of N total weights is computed as follows: i ... j all have the same ranking, then the weight of each will be the mean of the relevant
ROC weights; that is,
As a concrete example, if there is a single highest-ranked attribute followed by a three-way tie for second, then w 1 = w R 1 ,and w 2 = w 3 = w 4 2.4 Content Planning 2.4.1 Content Selection. Once a specific user model has been created, the AMVF can be used to select a set of flights to describe for that user, and to determine the features of those flights that should be included in the descriptions. We use a novel strategy that combines features of the Compare and Recommend strategies of Walker et al. (2004), refining them with an enhanced method of selecting options to mention. In brief, the idea behind the strategy is to select the top-ranked flight, along with any other highly ranked flights that offer a compelling trade-off X  X hat is, a better value (for the user) on one of its attributes. As we shall see in Section 2.4.2, by guaranteeing that any option beyond the first one offers such a trade-off, our strategy lends itself naturally to the determination of information structure and the contents of referring expressions identifying the option. By contrast, Walker et al. X  X  Recommend strategy only presents a single option, and their Compare strategy does not present options in a ranked order that facilitates making trade-offs salient. In addition, we may observe that with our strategy, the user model need only contain a rough approximation of the user X  X  true 166 preferences in order for it to do its job of helping to identify good flights for the user to consider. 8 A similar observation underlies the candidate/critique model of Linden, Hanks, and Lesh X  X  (1997) Web-based system.

Selecting the Options to Describe. In determining whether an option is worth mention-ing, we make use of two measures. Firstly, we use the z -score of each option; this mea-sures how far the evaluation v ( f )ofanoption f is from the mean evaluation. Formally, it is defined using the mean (  X  V ) and standard deviation (  X  follows: (2000, 2006), who provide a formal definition. Informally, the compellingness of an attribute measures its strength in contributing to the overall difference between the evaluation of two options, all other things being equal. For options f , g ,andthreshold for f than for g , and for which the compellingness is above k ranked option. Next, for all of the other options whose z -score is above a threshold k we check whether there is an attribute of that option that offers a compelling trade-off over the already selected options; if so, we add that option to the set. presented in Figure 5.
 selects the top-ranked flight: a connecting flight on British Airways with availability in business class. The next-highest-ranked flight is a morning flight, which does not have any attributes that are compellingly better than those of the top choice, and is therefore skipped. However, the third option presents an interesting trade-off: even though business class is not available, it is a direct flight, so it is also included. None oftheotheroptionsabovethethresholdpresentanyinterestingtrade-offs,soonlythose two flights are included. mentioned in the discussion of Figure 2. For FF, the selected flights are a connecting, economy-class flight on the preferred airline; a direct, economy-class flight on a neutral airline; and a connecting, business-class flight on a neutral airline. For S, the top choices are a reasonably cheap direct flight that arrives near the desired time, and an even cheaper, connecting flight that arrives much earlier in the day.

Selecting the Attributes to Include. When selecting the attributes to include in the de-scription, we make use of an additional measure, s-compellingness . Informally, the s-compellingness of an attribute represents the contribution of that attribute to the evaluation of a single option; again, the formal definition is given by Carenini and
Moore (2000, 2006). Note that an attribute may be s-compelling in either a positive or a negative way. For an option f and threshold k c , we define the set s-comp ( f , k of attributes whose s-compellingness for f is greater than k pelling attributes of the top choice. Next, we add all attributes that represent a trade-off between any two of the selected options; that is, attributes that are compellingly better for one option than for another. The algorithm appears in Figure 6.
 time and fare-class ; the latter is also a compelling advantage of this flight over the second option. The advantage of the second option over the first is that it is direct, so number-of-legs is also included. A similar process on the other user models results in price , arrival-time ,and number-of-legs being selected for S, and arrival-time , fare-class , airline ,and number-of-legs for FF. 2.4.2 Planning Texts with Information Structure. Based on the information returned by the content selection process, together with further information from the user model and the current dialogue context, the content planning agent develops a plan for presenting the available options. A distinguishing feature of the resulting content plans is that they contain specifications of the information structure of sentences (Steedman 2000a), including sentence theme (roughly, the topic the sentence addresses) and sentence rheme (roughly, the new contribution on a topic).
 stating that a theme presupposes a rheme alternative set, in the sense of Rooth (1992), whilearhemerestrictsthisset.BecauseSteedmandoesnotfullyformalizethediscourse update semantics of sentence themes, we have chosen to operationalize themes in
FLIGHTS in one particular way, namely, as corresponding to implicit questions that arise in the context of describing the available options. Our reasoning is as follows.
First, wenotethatasetofalternative answers corresponds formally tothemeaningofa question.Next,weobservethatwheneveraflightoptionpresentsacompellingtrade-off 168 for a particular attribute, it at least partially addresses the question of whether there are any flights that have a desirable value for that attribute; moreover, whenever a flight is presented that has a less-than-optimal value for an attribute, its mention implicitly raises the question of whether any flights are available with a better value for that attribute. Finally, we conclude that by specifying and realizing content as thematic, the system can help the user understand why a flight option is being presented, since the theme (by virtue of its presupposition) identifies what implicit question X  X hat is, what trade-off X  X he option is addressing.
 operators. These operators present the selected flights as an ordered sequence of op-tions, starting with the best one. Each flight is suggested and then further described. As part of suggesting a flight, it is identified by its most compelling attribute, according to the user model. (Recall that any selected flight options beyond the highest ranked one must offer a compelling trade-off.) Flights are additionally identified by their airline , which we deemed sufficiently significant to warrant special treatment (otherwise the strategy is domain-independent).
 made to the preceding query. Each subsequent, alternative flight is presented with its most compelling attribute in the theme, and the remaining attributes in the rheme. For instance,considerthestudentexample(S)inFigure2.AfterpresentingtheBMIflight X  which has a good price, is direct, and arrives near the desired time X  X he question arises whether there are any cheaper alternatives. Because the second option has price as its mostcompellingattribute X  X ndgiventhatitistheleastexpensiveflightavailable X  X tis identified as the CHEAPEST flight , with this phrase forming the theme of the utterance. As another illustration, consider the business-class traveler example (BC) in Figure 2.
After presenting the British Airways flight, which has availability in business class but is not direct, the question arises whether there are any direct flights available. The presentation of the second option addresses this implicit question, introducing the BMI flight with the theme phrase Thereisa DIRECT flight .
 produced during planning. Figure 7 shows the resulting content plan for the student example. Note that, as part of suggesting the second flight option in the sequence ( f2 ), subgoals are introduced that identify the option by informing the user that the option has type flight and that the option has the attribute cheapest , where this attribute is the most compelling one for the user. Both subgoals are marked as part of the theme (rheme marking is the default); the subgoal for the option type is also marked for definiteness (indefinite is the default), as the cheapest attribute uniquely identifies the flight.Theremaininginformationforthesecondflightispresentedintermsofacontrast between its positive and negative attributes, as determined by the user model. tive flight suggestions to implicit questions is related to Prevost X  X  (1995) use of theme phrases to link system answers to immediately preceding user questions. It is also 170 related to Kruijff-Korbayov  X  a et al. X  X  (2003) use of theme phrases to link utterances with questionsunderdiscussion(Ginzburg1996;Roberts1996)inaninformation-statebased dialogue system. An interesting challenge that remains for future work is to determine to what extent our presentation strategy can be generalized to handle theme/rheme partitioning,bothforexplicitquestionsacrossturnsaswellforimplicitquestionswithin turns. 2.5 Sentence Planning
The sentence planning agent uses the Xalan XSLT processor to transform the output of the content planner into a sequence of LFs that can be realized by the OpenCCG agent.
It is intended to be a relatively straightforward component, as the content planner has been designed to implement the most important high-level generation choices. Its pri-maryresponsibilityistolexicalizethebasicspeechactsinthecontentplan X  X hichmay appearinreferringexpressions X  X longwiththerhetoricalspeechactsthatconnectthem together. When alternative lexicalizations are available, all possibilities are included in a packed structure (Foster and White 2004; White 2006a). The sentence planner is also responsible for adding discourse markers such as also and but , adding pronouns, and choosing sentence boundaries. It additionally implements a small number of rhetorical restructuring operations for enhanced fluency.
 transform content plans into logical forms. An example logical form that results from applyingthesetemplatestothecontentplanshowninFigure7appearsinFigure8(with alternative lexicalizations suppressed). As described further in Section 2.6.1, the logical forms produced by the sentence planner are semantic dependency structures, make use of an info feature to encode theme/rheme partitioning, and a kon feature to implementSteedman X  X (2006)notionof kontrast (Vallduv  X   X andVilkuna1998).Following
Steedman, kontrast is assigned to the interpretations of words which contribute to distinguishing the theme or rheme of the utterance from other alternatives that the context makes available.
 and either , the sentence planner compares the inform acts for consecutive flight options to see whether acts with the same type have the same value. also trigger de-accenting. For example, when two consecutive flights have no seats in businessclass,thesecondonecanbedescribedusingthephrase it has NO AVAILABILITY in business class either , where business class has been de-accented.
 to invoke OpenCCG to parse a target sentence and then use the resulting logical form as the basis of a template. (See Section 3 for a description of how the target sentences in the FLIGHTS voice script were developed.) Using LF templates, rather than templates at the string level, makes it simpler to uniformly handle discourse markers such as also and either , which have different preferred positions within a clause, depending in part on which verb they modify. LF templates also simplify the treatment of subject X  verb agreement. Additionally, by employing LF templates with a single theme/rheme partition, it becomes possible to underspecify whether the theme and rheme will be realized by one intonational phrase each or by multiple phrases (see Section 2.6.2). At the same time, certain aspects of the LF templates can be left alone, when there is no need for further analysis and generalization.
 betweentemplate-basedand X  X eal X  X LGthatvanDeemter,Krahmer,andTheune(2005) havecalledintoquestion.Inthecaseofreferringexpressionsespecially,LFtemplatesin
FLIGHTS represent a practical and flexible way to deal with the interaction of decisions made at the sentence planning level, as the speech acts identifying flight options are considered together with the other basic and rhetorical speech acts in the applicability conditionsforthetemplatesthatstructureclauses.Inthisway,optionscanbeidentified not only in definite NPs, such as the CHEAPEST flight ,butalsoin there -existentials and there X  X  a BMI flight that . . . .Wemay furtherobserve thatthetraditional approach togen-eratingreferringexpressions(ReiterandDale2000),whereadistinguishing description for an entity is constructed during sentence planning without regard to a user model, wouldnotfitinourarchitecture,wheretheusermodeldrivestheselectionofareferring expression X  X  content at the content planning level.
 handling sentence planning tasks that were not the focus of our research, it is not one that promotes reuse, and thus it is worth noting which aspects of our sentence planner would pose challenges for a more declarative and general treatment. The bulk of the templates concern domain-specific lexicalization and are straightforward; given the way these were developed from the results of OpenCCG parsing, it is conceiv-able that this process could be largely automated from example input X  X utput pairs.
The templates for adding pronouns and discourse markers require more expertise but remain reasonably straightforward; the templates for rhetorical restructuring and choosing sentence boundaries, in contrast, are fairly intricate. In principle, satisfactory results might be obtained using a more general set of options for handling pronouns, discourse markers, and sentence boundaries, together with an overgenerate-and-rank methodology; we leave this possibility as a topic for future research. 2.6 Surface Realization 2.6.1 Chart Realization with OpenCCG. Forsurfacerealization,weusetheOpenCCGopen source realizer (White 2004, 2006a, 2006b). A distinguishing feature of OpenCCG is that 172 it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and
Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer.
 word order and inflection. For example, the realizer determines that also should prefer-ablyfollowtheverbin There is also a very cheap flight on Air France ,whereasinothercases enforces subject X  X erb agreement, for example, between is and flight , or between are and seats . Less typically, in FLIGHTS and in the COMIC 12 system, the OpenCCG realizer additionally determines the prosodic structure, in terms of the type and placement of pitch accents and edge tones, based on the information structure of its input logical forms.AlthoughOpenCCG X  X algorithmicdetailshavebeendescribedintheworkscited above, details of how prosodic structure can be determined from information structure in OpenCCG appear for the first time in this article.
 written with the aim of achieving very high quality. However, to streamline grammar development, the grammar has been allowed to overgenerate in areas where rules are difficult to write and where n -gram models can be reliable; in particular, it does not sufficiently constrain modifier order, which in the case of adverb placement especially can lead to a large number of possible orderings. Additionally, it allows for a one-to-manymappingfromthemesorrhemestoedgetones,yieldingmanyvariantsthatdiffer only in boundary type or placement. As will be explained subsequently, we consider this more flexible, data-driven approach to phrasing to be better suited to the needs of generation than would be a more direct implementation of Steedman X  X  (2000a) theory, which would require all phrasing choices to be made at the sentence planning level. describedinSection3.1.Toenhancegeneralization,namedentitiesandscalaradjectives havebeenreplacedwiththenamesoftheirsemanticclasses(suchasTIME,DATE,CITY,
AIRLINE, etc.), as is often done in limited domain systems. Note that in the corpus and the model, pitch accents are treated as integral parts of words, whereas edge tones and punctuation marks appear as separate words. The language model is a 4-gram back-off model built with the SRI language modeling toolkit (Stolcke 2002), keeping all 1-counts andusingRistad X  X (1995)naturaldiscountinglawforsmoothing.OpenCCGhasitsown implementation for run-time scoring; in FLIGHTS, the model additionally incorporates an a/an -filter, which assigns a score of zero to sequences containing a followed by a vowel, or an followed by a consonant, subject to exceptions culled from bigram counts. 2.6.2 Deriving Prosody. CCG is a unification-based categorial framework that is both linguisticallyandcomputationallyattractive.WeprovidehereabriefoverviewofCCG; an extensive introduction appears in Steedman (2000b). which are (possibly complex) categories bearing standard feature information (such as verb form, agreement, etc.) and subcategorization information. CCG has a small set of rules which can be used to combine categories in derivations. The two most basic rules are forward ( &gt; ) and backward ( &lt; ) function application. These rules are illustrated in
Figure 9, which shows the derivation of a simple copular sentence (with no prosodic information). 13 In the figure, the noun and proper name receive atomic categories with labels n and np ,respectively.Theremainingwordsreceivefunctionalcategories,suchas \ np / ( s adj \ np ) for the verb is ; this category seeks a predicative adjective ( s right and an np to its left, and returns the category s dcl that dcl and adj are values for the form feature; other features, such as those for number and case, have been suppressed in the figure (as has the feature label, form ). and substitution ( S ) combinators of combinatory logic. These rules add an element of associativity to the grammar, making possible multiple derivations with the same semantics. They are crucial for building the  X  X on-standard X  constituents that are the hallmark of categorial grammars, and which are essential for CCG X  X  handling of coor-dination, extraction, intonation, and other phenomena. For example, Figure 10 shows how the category for there can be type-raised and composed with the category for is in order to derive the constituent there is a direct flight ( s the VP in traditional syntax. Because there is a direct flight corresponds to the theme phrase, and on BMI to the rheme phrase, in the first clause of the second sentence in 174 the business-class (BC) example in Figure 2, the derivation in Figure 10 also shows how
CCG X  X  flexible notion of constituency allows the intonation structure and information structure to coincide, where the intonation coincides with surface structure, and the information structure is built compositionally from the constituent analysis. of pitch accents and edge tones. The notation for pitch accents and edge tones is taken from Pierrehumbert (1980) and ToBI (Silverman et al. 1992). We have implemented a re-duced version of Steedman X  X  theory in OpenCCG, where theme tunes generally consist of one or more L+H* pitch accents followed by a L-H% compound edge tone at the end of the phrase, 14 and rheme tunes consist of one or more H* pitch accents followed by a L-or L-L% boundary at phrase end. Additionally, yes / no questions typically receive a H-H% final boundary, and L-H% boundaries are often used as continuation rises to mark the end of a non-final conjoined phrase.
 the syntactic categories to enforce information structural phrasing constraints X  X hat is, to ensure that the intonational phrases are consistent with the theme/rheme partition in the semantics. For example, if there is a direct flight corresponds to the theme and on
BMI therheme,thenthesyntacticfeaturesensurethattheintonationbracketstheclause is) (a direct flight on BMI) , etc. To illustrate, Figure 11 shows, again from the parsing perspective, how theme and rheme phrases are derived in OpenCCG for the subject
NP and VP of Figure 9, respectively. (To save space, pitch accents and edge tones will henceforth be written using subscripts and string elements, rather than appearing on a separate tonal tier.) In the figure, the category for cheapest has the info feature on each atomic category set to th(eme) ,and Ryanair has its info feature set to rh(eme) ,dueto the presence of the L+H* and H* accents, respectively. The remaining words have no accents, and thus their categories have a variable ( M ,for EME ) as the value of the info feature on each atomic category. 15 All the words also have a variable ( O ) as the value of the owner feature, discussed subsequently, on each atomic category. As words combine into phrases, the info variables serve to propagate the theme or rheme status of a phrase;thus,thephrase the cheapest L + H  X  flight hascategory np values, they cannot combine before being  X  X romoted X  to intonational phrases by their respective edge tones. In this way, the constraint that phrasing choices respect the theme/rheme partition is enforced.
 changing the value of the info feature to phr(ase) in the result category. the argument categories of the edge tones do not select for a particular value of the info feature; instead, L-H% has s h $ 1 as its argument category, whereas L-L% has s where h(earer) and s(peaker) aretherespectivevaluesofthe owner feature. with an edge tone unifies the owner features throughout the phrase. In a prototypical theme phrase, the hearer is the owner, whereas in a rheme phrase, the speaker is the owner.
 be built in parallel with the derivational process. Traditionally, the  X  -calculus has been used to express semantic interpretations, but OpenCCG instead makes use of a more flexible representational framework, Hybrid Logic Dependency Semantics (Baldridge and Kruijff 2002; Kruijff 2003), or HLDS. In HLDS, hybrid logic (Blackburn 2000) terms are used to describe semantic dependency graphs, such as the one seen earlier in
Figure 8. As discussed in White (2006b), HLDS is well suited to the realization task, in thatitenablesanapproachtosemanticconstructionthatensuressemanticmonotonicity, simplifies equality tests, and avoids copying in coordinate constructions. words are derived by a lexical rule which adds pitch accents and information structure features to the base forms. The format of the entries is lexeme category , where the categoryisitselfapairintheformat syntax : logical form .Thelogicalformisaconjunction of elementary predications (EPs), which come in three varieties: lexical predications, such as @ E be ; semantic features, such as @ E TENSE pres; and dependency relations, such as @ E A RG X . 176
In these entries, the indices (or nominals , in hybrid logic terms) in the logical forms, such as X , P ,and E , correspond to nodes in the semantic graph structure, and are linked to the syntactic categories via the index feature. Similarly, the syntactic features info and owner have associated INFO and OWNER features in the semantics. As discussed previously, in derivations the values of the info and owner features are propagated throughout an intonational phrase, which has the effect of propagating the values of the INFO and OWNER semantic features to every node in the dependency graph corresponding to the phrase. In this way, a distributed representation of the theme/rheme partition isencoded, inafashionreminiscent ofKruijff X  X (2003) approach to representing topic X  X ocus articulation using hybrid logic. By contrast, the KON fea-ture (cf. Section 2.5) is a purely local one, and thus appears only in the semantics. Note that because the edge tones do not add any elementary predications, one or more edge tones X  X nd thus one or more intonational phrases X  X ay be used to derive the same theme or rheme in the logical form.
 feature to subtrees in the logical form, set the OWNER feature to its prototypical value, and set the value of the KON feature to false. When applied to the logical form for the semantic dependency graph in Figure 8, the rules yield the HLDS term in Exam-ple (2). After this logical form is flattened to a conjunction of EPs, lexical instantiation looks up relevant lexical entries, such as those in Example (1), and instantiates the variables to match those in the EPs. The chart-based search for complete realizations proceeds from these instantiated lexical entries. we have chosen to employ hard constraints in the OpenCCG grammar on the choice of pitch accents and on the use of edge tones to separate theme and rheme phrases.
However, the grammar only partially constrains the type of edge tone (as explained previously), and allows the theme and rheme to be expressed by one or more intona-tional phrases each; consequently, the final choice of the type and placement of edge tones is determined by the n -gram model. To illustrate, consider Example (3), which shows how the frequent flyer sentence seen in Figure 2 is divided into four intonational phrases.Otherpossibilities(amongmany)allowedbythegrammarincludeleavingout theL-L%boundarybetween flight and arriving ,whichwouldyieldaphrasethat X  X likely to be perceived as too long, or adding a L-or L-L% boundary between there X  X  and a , which would yield two unnecessarily short phrases.
To allow for this flexible mapping between themes and rhemes and one or more into-national phrases, we take advantage of our distributed approach to representing the theme/rheme partition, where edge tones mark the ends of intonational phrases with-out introducing their own elementary predications. As an alternative, we could have associated a theme or rheme predication with the edge tones, which would be more in line with Steedman X  X  (2000a) approach. However, doing so would make it necessary to include one such predication per phrase in the logical forms, thereby anticipating the desired number of output theme and rheme phrases in the realizer X  X  input. Given that the naturalness of intonational phrasing decisions can depend on surface features like phrase length, we consider the distributed approach to representing theme/rheme status to be better suited to the needs of generation. 2.7 Comparison to Baseline Prosody Prediction Models
As noted in Section 2.2, spoken language dialogue systems often include a separate prosodypredictioncomponent(Pan,McKeown,andHirschberg2002;Walker,Rambow, and Rogati 2002), rather than determining prosodic structure as an integral part of surface realization, as we do here. Although it is beyond the scope of this article to compare our approach to a full-blown, machine learning X  X ased prosody prediction model, we do present in this section an expert evaluation that shows that our approach outperforms strong baseline n -gram models. In particular, we show that the informa-tion structural constraints in the grammar play an important role in producing target prosodic boundaries, and that these boundary choices are preferred to those made by an n -gram model in isolation.
 models can be surpringly good:  X  X he word itself also proves to be a good predictor for both accent and break index prediction. . . . Since the performance of this [word] model isthebestamongallthe[single-feature]accentpredictionmodelsinvestigated,itseems to suggest that for a CTS [Concept-to-Speech] application created for a specific domain, features like word can be quite effective in prosody prediction. X  Indeed, although their bestaccentpredictionmodelexceededtheword-basedone,thedifferencedidnotreach statistical significance (page 485). Additionally, word predictability, measured by the log probability of bigrams and trigrams, was found to significantly correlate with pitch accent decisions (pages 482 X 483), and contributed totheir best machine-learned models for accent presence and boundary presence. 18 andboundariesusingtheFLIGHTSspeechcorpus(Section3.1),thesamecorpususedto train the realizer X  X  language model. The baseline n -gram models are factored language models (Bilmes and Kirchhoff 2003), with words, accents, and boundaries as factors.
The accent models have accent as the child variable and 1 X 4 words as parent variables, startingwiththecurrentword,andincludinguptothreepreviouswords.Theboundary models are analogous, with boundary as the child variable. With these models, each maximum likelihood prediction of pitch accent or edge tone is independent of all other choices,sothereisnoneedtoperformabest-pathsearch.Themajoritybaselinepredicts no accent and no edge tone for each word. 178 synthesizethestimuliintheperceptionexperimentdescribedinSection4(seeFigure13 for an example mini-dialogue). None of the test sentences appear verbatim in the
FLIGHTS speech corpus. The test sentences contain target prosodic structures intended to be appropriate for the discourse context. Given these structures, we can quantify the accuracy with which the realizer is able to reproduce the pitch accent and edge tone choices in the target sentences, and compare it to the accuracy with which n -gram models predict these choices using maximum likelihood. Note that the target prosodic structuresmaynotrepresenttheonlynaturalchoices,motivatingtheneedfortheexpert evaluation described further subsequently.
 test sentence exactly, the test sentence (with its target prosody) is not always the top-ranked realization of the corresponding logical form, which may differ in word order or choice of function words. Thus, to compare the realizer X  X  choices against the target accentsandboundaries,wegenerated n -bestlistsofrealizationsthatincludedthetarget realization, and compared this realization to others in the n -best list with the same wordsinthesameorder(ignoringpitchaccentsandedgetones).Ineachcase,thetarget realization was ranked higher than all other realizations with the same word sequence, and so we may conclude that the realizer reproduces the target accent and boundary choices in the test sentences with 100% accuracy.
 is shown in Table 1. As the test sentences are very similar to those in the FLIGHTS speech corpus, the accent model performs remarkably well, with the bigram model reproducing the exact accent type (including no accent) in 97.4% of the cases, and agreeing on the choice of whether to accent the word at all in 98.6% of the cases. The boundary model also performs well, though substantially worse than the realizer, with the 4-gram model reproducing the boundary type (including no boundary) in 93.9% of the cases, and agreeing on boundary presence in 95.1% of the cases. asked an expert ToBI annotator, who was unfamiliar with the experimental hypotheses under investigation, to indicate for each test sentence the range of contextually appro-priate tunes by providing all the pitch accents and edge tones that would be acceptable for each word. 20 However, our annotator found this task to be too difficult, in part because of the difficulty of coming up with all possible acceptable tunes, and in part because of dependencies between the choices made for each word. For this reason, we instead chose to follow the approach taken with the Human Translation Error
Rate (HTER) post-edit metric in MT (Snover et al. 2006), and asked our annotator to indicate,forthetargettuneandthe n -grambaselinetune,whichaccentsandboundaries would need to change in order to yield a tune appropriate for the context. For the n -gram baseline tune, we used the choices of the bigram accent model and the 4-gram boundary model.
 baseline accent and boundary prediction models appear in Table 2, along with the corrections provided by our expert ToBI annotator. 21 In Table 2(a), the target tune, which contains the theme phrase the only direct L + H  X  flight L-H% , was considered fully acceptable. By contrast, with the tune of the n -gram models, the H* accent on direct was not considered acceptable for the context, and at least a minor phrase boundary was considered necessary to delimit the theme phrase. In Table 2(b), we have an example wherethetargettunewasnotconsideredfullyacceptable:Althoughthetargetconsisted of a single, all-rheme intonational phrase, with no intermediate phrases, our annotator indicated that at least a minor phrase break was necessary after British Airways .The n -gram models assigned a major phrase break at this point, which was also considered acceptable. Note also that the n -gram models had a L+H* accent on good , in contrast to the target tune X  X  H*, but both choices were considered acceptable.
 tences at different levels of specificity. Overall, there were just 10 accent or boundary corrections for the target tunes, versus 24 for those of the baseline models, out of 688 total accent and boundary choices, a significant difference (p = 0.01, Fisher X  X  Exact Test [FET], 1-tailed). With the accents, there were fewer corrections for the target tunes, but not many in either case, and the difference was not significant. Of course, with a 180 larger sample size, or with test sentences that are less similar to those in the corpus, a significant difference could arise. With the boundaries, out of 344 choices, the target tunes had six corrections, only two of which involved the presence of a boundary, and none of which involved a missing major boundary; by contrast, the n -gram baseline had 17, 13, and 9 such corrections, respectively, a significant difference in each case (p = 0.02, p = 0.003, p = 0.002, respectively, FET). In the subset of 12 sentences involving theme phrases, where the intonation is more marked than in the all-rheme sentences, the target tunes again had significantly fewer corrections overall (3 vs. 11 corrections out of 220 total choices; p = 0.03, FET), and the difference in boundary and boundary presence corrections approached significance (p = 0.06 in each case, FET). important role in producing target realizations with contextually appropriate prosodic structures X  X n particular, in making acceptable boundary choices, where the choice is not deterministically rule-governed X  X e now briefly demonstrate that the realizer X  X  n -grammodel(seeSection2.6.1)hasanimportantroletoplayaswell.Table4compares the realizer X  X  output on the test sentences against its output with the language model disabled, in which case an arbitrary choice is effectively made among those outputs allowed by the grammar. Not surprisingly, given that the grammar has been allowed to overgenerate, the realizer produces far more exact matches and far higher BLEU (Papineni et al. 2001) scores with its language model than without. Looking at the dif-ferences between the realizer X  X  highest scoring outputs and the target realizations, the differences largely appear to represent cases of acceptable variation. The most frequent difference concerns whether an auxiliary or copular verb is contracted or not, where eitherchoiceseemsreasonable.Mostoftheotherdifferencesrepresentminorvariations in word order, such as direct H  X  Air France H  X  flight vs. direct contrast, many of the outputs chosen with no language model scoring contain unde-sirable variation in word order or phrasing; for example: Air France instead of direct H  X  flight on Air France H  X  ,and you L-though would need L-to connect Amsterdam H  X  L-instead of you X  X  need to connect H  X  in Amsterdam 2.8 InterimSummary
In this section, we have introduced a presentation strategy for highlighting the most compelling trade-offs for the user that straightforwardly lends itself to the determina-tion of information structure, which then drives the choice of prosodic structure during surface realization with CCG. We have also shown how phrase boundaries can be determined in a flexible, data-driven fashion, while respecting the constraints imposed by the grammar. An expert evaluation demonstrated that the approach yields prosodic structures with significantly higher acceptability than strong n -gram baseline prosody predictionmodels.Inthenexttwosections,weshowhowthegenerator-drivenprosody can be used to produce perceptibly more natural synthetic speech. 3. Unit Selection Synthesis withProsodic Markup
In this section we describe the unit selection voices that we employed in our perception experiment (Section 4). Three voices in total were used in the evaluation: GEN, ALL, and APML. Each was a state-of-the-art unit selection voice using the Festival Multisyn speech synthesis engine (Clark, Richmond, and King 2007). The GEN voice, used as a baseline, is a general-purpose unit selection voice. The ALL voice is a voice built using the same data as the GEN voice but with the addition of the data from the
FLIGHTS speech corpus described in Section 3.1. The APML voice augments the in-domaindataintheALLvoice withprosodicmarkup, whichisthenusedatrun-timeby the synthesizer in conjunction with marked-up input to guide the selection of units. a suitable data set from the original speaker used for the GEN voice. We describe the process of constructing this speech corpus for FLIGHTS next, in Section 3.1. Then, in
Section3.2,wedescribetheunitselectionvoicesindetail,alongwithhowthein-domain data was used in building them. 3.1 The FLIGHTS Speech Corpus
The FLIGHTS speech corpus was intended to have a version of each word that needs to bespokenbythesystem,recordedinthecontextinwhichitwillbespoken.Thiscontext can be thought of as a three-word window centered around the given word, together with the word X  X  target pitch accent and edge tone, if any. This would theoretically provide more than sufficient speech data for a full limited domain voice, and a voice usingthisdatawouldbeguaranteedtohaveaunitsequenceforanysentencegenerated by FLIGHTS where each individual unit is a tight context match for the target unit. recording the voice data for FLIGHTS, we developed a recording script by combining the generated data that was available with additional utterances that we anticipated the finished system would generate. To do so, we assembled a set of around 50 utter-ance templates that describe flight availability X  X ith slots to be filled by times, dates, amounts, airlines, airports, cities, and flight details X  X o which we added approximately two hundred individual or single-slot utterances, which account for the introductions, 182 questions,confirmations,andotherresponsesthatthesystemmakes.Wethenmadeuse of an algorithm designed by Baker (2003) to iterate through the filler combinations for each utterance template to provide a suitable recording script. utterancesintherecordingscriptcreatedfromthistemplate.Theexampledemonstrates that having multiple slots in a template makes it possible to reduce the number of sen-tencesthatneedtoberecordedtocoverallpossiblecombinationsoffillers.With(b)and (c) recorded as illustrated to fill the template in (a), we would then have the recorded data to mix and match the slot fillers with two different values each to synthesize eight different sentences. It is also often possible to use the fillers for a slot in one utterance
A more complex case is shown in Example (5), involving adjacent slots. In this case, it is not sufficient to just record one example of each possible filler, as the context around that filler is changed by the presence of other slots; instead, combinations of fillers must be considered (Baker 2003). (4) a. It arrives at TIME H  X  L-H% and costs just AMOUNT H (5) a. There are NUM H  X  MOD H  X  flights L-L% from CITY H caps to indicate intended word-level emphasis and punctuation to partially indicate desired phrasing, as the speaker was not familiar with ToBI annotation. The intended dialogue context for the utterances was discussed with our speaker, but the recordings didnotconsistofcompletedialogues,asusingcompletedialogueswouldhavebeentoo time consuming. The recording took place during two afternoon sessions and yielded approximately two hours of recorded speech. The resulting speech database consisted of 1,237 utterances, the bulk of which was derived from the utterances that describe flight availability.
 which had a generated APML file with the pitch accents and edge tones predicted for the utterance. The prosodic structures were based on intuition, as there was no corpus ofhuman X  X uman dialoguessufficientlysimilartowhatthesystemproduces thatcould have been used to inform prosodic choice.
 ing to the audio and looking at the f0 contours to see if the pitch accents and edge tones matched the predicted ones. In the interest of consistency, changes were only made to the APML files when there were clear cases of the speaker diverging from the predicted phrasing or emphasis; nevertheless, the changes did involve mismatches in boththepresenceandthetypeoftheaccentsandboundaries.Asanexampleofthekind of changes that were made, in Example (4), we had predicted that the speaker would use a L-H% boundary (a continuation rise) prior to the word but , however she instead consistently used a low boundary tone in this location. Consequently, we changed all the APML files for sentences with this pattern to include a L-L% compound edge tone at that point. Further details of this data cleanup process are given in Rocha (2004). 3.2 The Synthetic Voices
Ideally, we would like to build a general purpose unit selection voice where we can fully specify the intonation in terms of ToBI accents and boundaries and then have the synthesizer generate this for us. This, however, is currently not a fully viable option for the reasons discussed subsequently, so instead we built a number of different voices using the unit selection technique to investigate how working towards a voice with full intonation control would function. There are a number of ways in which prosodic generation for unit selection can be approached (Aylett 2005; van Santen et al. 2005; Clark and King 2006), with most systems designed to produce best general prosody.
This contrasts with the framework chosen here, which is designed to make maximum use of in-domain data, in an attempt to produce a system that realizes prosody as well as is possible in that domain. This can be considered an upper bound for what more general systems could achieve under optimal conditions.
 components: the database of speech, which must be fully annotated for intonation; the predicted intonation for a target being synthesized; and a target cost that ensures suitable candidates are found to match the target. Each of these requirements prove to be difficult to achieve successfully.
 tion labels is both difficult and expensive, whereas automatic labeling techniques tend toproducemediocreresultsatbest.Producingaccentlabelingfortheflightinformation script is somewhat easier because of the limited number of templates that the script is based upon, andonce thetemplates are labeled, intonation forindividual sentences can be derived. To build a general purpose unit selection voice for the FLIGHTS domain, we would want to combine the FLIGHTS data with speech data designed to provide more general coverage. Providing accent labeling for the extra, non-FLIGHTS, data is the main problem here.
 only really be done well when the domain of the speech synthesis is constrained. For example, a number of statistical modeling techniques may be able to provide adequate accent predictionforasystemtoreadthenewsorforecast theweather,because thesen-tence structure is usually limited to a sequence of simple statements. The task becomes difficult when the sentence structure is more variable, for example in dialogue where a number of different question forms may exist along with contrastive statements, and so forth. In the FLIGHTS domain we can side-step the prediction problem by providing a specification for the intonation of a sentence as part of language generation. suitable intonation is not difficult, as a simple penalty for not matching the target intonation suffices. However, standard unit selection techniques only ever take into 184 account local effects, and no provision is made to ensure a suitable global intonation contour.
 automaticsegmentalignment,andthevoicebuildingprocess,prohibitusfrombuilding a general purpose unit selection voice where we can specify the required intonation.
Oneofthequestionsthatthisstudyisattemptingtoansweriswhetheritisworthtrying to resolve these issues to build such a system in the future. To address this question we present a number of voices designed to investigate the issues of producing natural speech synthesis in the FLIGHTS domain. 3.2.1 The GEN Voice. The GEN voice uses a database of approximately 2,000 sentences of read newspaper speech. The Festival Multisyn unit selection engine selects diphone units from the database by minimizing a combination of a target cost and a join cost.
The target cost scores the linguistic context of the diphone in terms of stress, position of the diphone in the current word, syllable and phrase, phonetic context, and part of speech. The join cost scores the continuity between selected units in terms of spectrum, energy, and f0. There is no explicit modeling of prosody in this system.
 domain associated with any part of this voice, and the quality of the resulting synthesis is comparable with a typical unit selection speech synthesiser. 3.2.2 The ALL voice. The ALL voice was created by augmenting the GEN voice with the speech data recorded as part of the FLIGHTS speech corpus described herein. The motivation here is to attempt to provide a system which would have the best possible quality that a general purpose unit selection synthesiser could have when working in this domain. The additional data increases the availability of units in the exact contexts thatwouldberequiredbytheFLIGHTSsystem.Havingexamplesofairportandairline names in the database, for example, increases the likelihood that when these words are synthesized there are appropriate, often consecutive, units available to synthesize them. Naturalness is improved both by the better context match and by the need for fewer joins in constructing these words and the utterances they are used in. 3.2.3 The APML voice. The APML voice is different from the ALL voice in that it is designed to take APML input from the FLIGHTS system rather than text input. The
APML voice comprises the same speech data as the ALL voice, but also includes the APML annotation for the FLIGHTS part of the corpora. The target cost for the synthesizer is augmented with a prosodic component which penalizes any mismatch betweensuppliedAPMLinputandtheAPMLspecificationassociatedwithaparticular unit. As the read newspaper component of this voice does not have accompanying
APML markup, and because the voice is required to work for text input (although this capability is not used here), the target cost penalizes (1) the synthesis of an APML-specified target with a unit that does not have accompanying APML markup; (2) the synthesis of a target without APML specification with APML-specified units, and (3) synthesis where there is an APML mismatch between the target and candidate. It is important that these prosodic mismatches are only discouraged, rather than forbidden, to allow the system to synthesize out of the original domain if required. speech corpus failed to anticipate all the possible outputs of the system. For example, although we included an utterance template for conveying price in a conjoined verb phrase, as in Example (4), we did not include a template for conveying price in a single independent clause, as in It costs just fifty pounds . Had the system been further along in its development when we recorded the speech data, we could have pursued a strategy of selecting utterances to record from generated outputs, in order to maximize some coverage criterion. 23 In either case, though, we consider it difficult to develop a recording script that will completely cover all three-word sequences that a system will ever generate, especially if further development is considered. For this reason, we believe it to be essential to have a strategy to handle the cases where generation needs go beyond the original plan. The use of an augmented general purpose unit selection system X  X ather than, for example, a strictly limited domain system X  X eans that there isnodifficultyinsynthesizingextramaterialasneeded,althoughthereistheriskthatit will not sound quite as good as that which is closer in context to the original in-domain recordings. ForanAPMLvoice wheretheinput is X  X ew X  X PML,ifthereare nosuitable units within the APML-annotated section of the corpus, units will be chosen from the main portion of the corpus. There will be a penalty in terms of target cost for doing so, but the best sequence will still be found. 4. Synthesis Evaluation
In this section, we describe a perceptual experiment which was carried out to deter-mine whether the prosodic structures generated by the FLlGHTS system actually result in improved naturalness in speech synthesis. We also describe an evaluation of the prosody in the synthesized speech that makes use of an expert annotator X  X  assessments of the acceptability of the perceived tunes for the given context, and an evaluation that examinesobjectivedifferencesinthef0contoursofthethemephrasesinthesynthesized speech. The three types of evaluation (subject ratings, expert annotation, and objective measures)allshowtheAPMLvoiceoutperformingtheALLvoice,bothonthecomplete setoftestutterancesaswellasonthesubsetcontainingthemephrases.Additionally,the three types of evaluation support each other in that differences in ratings of particular items correspond to differences in acceptability annotations and to differences in the objective measures, as will be explained in the following. 4.1 Perception Experiment 4.1.1 Methodology. Our experimental hypothesis was that listeners would prefer the APML voice, used with contextually appropriate intonation markup, over the ALL and
GEN control voices. We further hypothesized that the preference would be larger for the utterances containing theme phrases, where the intonation is more marked than it is in the all-rheme utterances.
 request in text and three versions of the system X  X  response, one for each of the three voices, as shown in Figure 12. System utterances were presented side-by-side, with each system turn comprising two to four utterances, and each voice labeled as A, B, or C. The label assignments were balanced across the mini-dialogues so that each voice appeared an equal number of times labeled as A, B, or C, and the mini-dialogues were presented in an individually randomized order. Subjects were asked to assign ratings to each version of each utterance on a 1 X 7 scale, with 7 corresponding to  X  X ompletely natural X  X nd1correspondingto X  X ompletelyunnatural. X  X atingsweregatheredon-line 186 using webexp2 . 24 Subjects were allowed to play the sound files any number of times in any order, but were required to assign ratings to all the utterances before proceeding to the next screen. In assigning their ratings, subjects were instructed to pay attention to thecontextgivenbythesummaryoftheuser X  X request,keepingthefollowingquestions in mind: dialogues were constructed so as to contain a representative range of theme phrases, with each mini-dialogue containing one utterance with a theme phrase. An example dialogue,withtargettunesfortheAPMLvoice,appearsinFigure13;thesecondsystem utterance contains a theme phrase. The complete set of stimuli, including sound files, is available on-line 25 from the first author X  X  web page.
 were all from the UK or USA and had no known hearing deficits. For participating in the study, subjects were entered into a prize drawing. 4.1.2 Results. As shown in Figure 14, the APML voice received higher ratings than the
ALL voice, and both the APML and ALL voices scored much higher than the GEN voice. Overall, the APML voice X  X  average rating surpassed that of the ALL voice by 0.77; its score of 5.83 was close to 6 on the rating scale, corresponding to  X  X ostly natural, X  while the ALL voice X  X  score was 5.06, just above 5 on the scale, corresponding to  X  X omewhat natural. X  The difference between the two voices was highly significant (paired t-test, t 433 = 10 . 20, p &lt; 0.001). On the theme utterances, the difference between the APML and ALL voices was even larger, at 0.91. With the APML voice, there was no significant difference between the average ratings of the theme utterances and those without theme phrases. In contrast, the ALL voice showed a trend towards the theme utterances scoring worse than the remaining ones (t-test, 1-tailed, t 0.08), with the average of the theme utterances 0.19 lower than that of the all-rheme utterances. The GEN voice did considerably worse (0.48) on the theme utterances (t-test, 1-tailed, t 432 =  X  3 . 06, p = 0.001). 4.2 Expert Prosody Evaluation 4.2.1 Methodology. To more directly examine whether the APML voice yielded more contextually appropriate prosody than the ALL voice, we asked an expert ToBI anno-tator, who was unfamiliar with the experimental hypotheses under investigation, to annotate the perceived tunes in the synthesized stimuli for these two voices. In cases of uncertainty, multiple annotations were given. The stimuli from the ALL voice were always presented first, so that listening to the tune from the APML item would not bias our annotator against the corresponding ALL item. Because there is not necessarily a unique tune that is acceptable for the context, we also asked our annotator to indicate the closest acceptable tune by indicating which accents and boundaries would need to 188 change in order to yield a tune appropriate for the context, in much the same fashion as in our prosody prediction evaluation (Section 2.7). 26 accent or boundary corrections at different levels of specificity.
 is the second one in Dialogue 05, where the user prefers to fly BMI; accordingly, the utterance contains the theme phrase (with target tune) the only BMI
Withthe APMLversion of the utterance, the annotator perceived a L+H* accent on BMI and a L-H% boundary on de-accented flight , as desired. Additionally, the annotator perceived a H* accent on only , which was not part of the target tune. The tune was nevertheless considered completely acceptable, as the Edits tier is identical to the Tones tier. By contrast, with the ALL version of the utterance, BMI was less prominent than only and flight , and accordingly it received no pitch accent, whereas only and flight received L+H* and H* accents, respectively; in addition, a H-boundary was annotated on only , and a boundary that was uncertain between L-and L-L% was annotated on flight . This tune for the theme phrase was not considered acceptable for the context: As the Edits tier shows, the lack of an accent on BMI , and the presence of an accent on flight , was corrected by our annotator, as was the H-minor phrase boundary on only .
This choice makes sense for the context, as BMI is what distinguishes this option from the one suggested in the first utterance, while flight is given information at this point in the dialogue. Indeed, it is difficult to come up with an interpretation of only with no accent on BMI , though this tune might make sense if the question of whether the flight was code-shared with another airline was a salient one in the context. 4.2.2 Results. TheresultsoftheexpertprosodyevaluationappearinTable5.Acrossall31 utterances, there were 49 accent or boundary corrections for the ALL voice, versus just 22fortheAPMLvoice,outof688totalaccentandboundarychoices,ahighlysignificant difference (p &lt; 0.001, Fisher X  X  Exact Test, 1-tailed). With the 12 theme utterances, there were 26 corrections for the ALL voice, versus 11 for the APML voice, out of 220 total choices (p = 0.008, FET). Looking at the pitch accents, the difference in the number of accent corrections was significant in each case (p &lt; 0.001, FET, and p = 0.004, FET, respectively), as was the number of corrections involving the presence or absence of a pitch accent (p = 0.004, FET, and p = 0.05, FET, respectively) and the number involving
L+H* accents (p = 0.02, FET, and p = 0.05, FET, respectively). aries, we may observe that with the ALL voice, the majority of corrections were with accents, whereas with the APML voice, the majority were with boundaries. Although the APML voice had fewer boundary corrections, the difference was not significant. did not always exactly match the target tunes, sometimes in ways that our expert annotator considered acceptable. More commonly, however, mismatches between the perceived and target tunes were not judged to be acceptable. In fact, of the 22 correc-tions for the APML voice, 19 would have been acceptable had the target tune been successfully achieved; that is, 19 of the 22 corrections changed a perceived accent or boundary to the one in the target tune. In 12 of these 19 cases, our annotator perceived an accent or boundary where the target tune had none. There were also five other cases 190 where the target was a L-L% boundary, but a rising boundary was perceived instead. The remaining cases involved accent type mismatches.
 expert ToBI annotator and the ratings in the perception experiment, we grouped the
APML and ALL utterances by the number of corrections and calculated the mean ratings for each group. The results appear in Figure 16, which shows the expected relationship between mean ratings and the number of corrections, with ratings go-ing down as the number of corrections go up. The pattern is less clear with two to four corrections, where there are fewer tokens. One-tailed t-tests show that utterances with zero corrections were rated significantly higher than utterances with one to four corrections (t 39 = 2 . 96, t 35 = 5 . 14, t 30 = 3 . 03, t case); utterances with one correction were also significantly higher than those with four corrections (t 17 = 2 . 30, p = 0.02).
 of errors noted by our expert annotator through a multiple regression analysis. The regressors were the number of accent or boundary presence errors and the number of remainingaccentorboundaryerrors,involvingonlyadifferenceintype.Theregression equation was Rating = 5 . 85  X  0 . 41  X  PresenceErrors  X  0 . 28 counted for 32% of the variance and was highly significant (F expected, ratings were negatively related to accent and boundary errors, with presence errorsshowingagreater impact thantype-onlyerrors. Boththeeffectofpresence errors (t significant. Because both kinds of errors had a significant impact on ratings, the results suggestthatitisworthwhiletomakeuseoffine-grainedToBIcategorieswherefeasible, as we discuss further in Section 6. 4.3 Objective Measures 4.3.1 Methodology. In addition to the expert prosody evaluation, we also measured the f0valuesanddurationsoftheadjectivesandnounsinthethemephrases,toseewhether they differed significantly between the APML and ALL voices. We also measured the drop in f0 between the adjective, which should receive a contrastive pitch accent, and the noun, which should be deaccented. Note that the f0 drop is a potentially more fine-grained measure than pitch accent corrections, because the accents could be considered acceptable even though the tune is less distinct in some cases.
 utterance of Dialogue 03, seen earlier in Figure 13, for the APML and ALL voices. Here, the APML version was annotated with the target accents, namely L+H* for Lufthansa and none for flight , whereas the ALL version was annotated with a L+H* !H* pattern, whichwasalsoconsideredacceptable.However,withtheAPMLversionthepitchdrops fromanf0valueof293Hzto177Hz,whereaswiththeALLversionthepitchonlydrops from 260 Hz to 209 Hz. As a result, the theme tune is much less distinctive in the ALL version, in a way that could impact the ease with which the theme phrase X  X  discourse function is identified. 4.3.2 Results. Figure 18 shows the mean f0 values across all 12 theme phrases for the adjective,whichshouldreceivecontrastiveemphasis,andtheheadnoun,whichshould be reduced. As the figure shows, the pattern seen in Figure 17 was borne out on the whole in the stimuli with theme phrases. In particular, the theme phrases for the APML voice had an f0 drop of 90 Hz on average from the adjective to the noun, whereas the
ALL voice only had an f0 drop of 50 Hz, a significant difference (t-test, 1-tailed, t 2 . 60, p = 0.008). The durations of the adjectives and nouns, however, did not show a significant difference. In addition to the f0 drop, a significant difference was also found with the f0 max on the adjective and the f0 min on the noun (t-tests, 1-tailed, t p=0.05andt 22 =  X  1 . 86,p=0.04,respectively).Finally,arelativelyhighcorrelation(r= 0.78) was also found between the difference in f0 drop and the difference in the ratings ofthethemeutterancesforthetwovoices(t 10 = 3 . 94,p=0.001),suggestingthattheless 192 distinctive theme tunes produced by the ALL voice were perceived as less natural than the ones produced by the APML voice. 4.4 Discussion The perception experiment confirmed our hypothesis that listeners would prefer the
APML voice, used with contextually appropriate intonation markup, over the ALL and GEN control voices. Both on the complete set of utterances, as well as the subset containing theme phrases, the APML voice was rated substantially higher on average than the ALL voice, and much higher than the GEN voice. Note that the ALL voice was also rated much higher on average than the GEN voice X  X hich is not surprising, given that it has access to the same limited domain data as the APML voice X  X howing that theALLvoiceservesasatoughbaselinetobeat.Theexpertprosodyevaluationandthe objective measures also confirmed the superiority of the APML voice, in particular on the theme utterances.
 voice would be larger for the utterances containing theme phrases X  X here the intona-tion is more marked than it is in the all-rheme utterances X  X s the difference between the mean ratings of the APML and ALL voices was larger on the theme utterances than on the remaining ones. Additionally, with the APML voice, there was no significant difference between the mean ratings of the theme utterances and those without theme phrases, whereas the ALL voice showed a trend towards the theme utterances scoring worse than the all-rheme ones, and the GEN voice clearly did considerably worse on the theme utterances. One small surprise was that with the ALL voice, the difference between the mean ratings of the theme and all-rheme utterances did not reach the standard level of statistical significance. Of course, it could be that with a larger sample size, a more significant difference would be found. However, it is undoubtedly the case that the ALL voice dropped off less on the theme utterances than did the GEN voice, and the reason is almost certainly that the limited domain data has good coverage of the theme phrases, and thus the ALL voice often does reasonably well on the theme utterances even without explicit prosodic control. What is perhaps more remarkable is that the ALL voice did not do better on the all-rheme utterances, as can be seen in the number of expert corrections listed in Table 5 for all the utterances, which go well beyond those in the theme utterance subset. That the ALL voice had more than double the number of corrections as the APML voice on both the complete set of utterances and the subset containing theme phrases shows that the prosodic specifications were important throughout.
 tions than did the ALL voice, the difference did not reach significance, suggesting that there is room for improvement in how boundaries are handled in the APML voice. In particular, this result suggests that edge tones, and intermediate phrase boundaries in particular, should affect the selection of units non-locally, as theoretically their effect on thepitchcontourspreadsbacktothelastpitchaccent.Infact,itmaywellbethatbecause the speech synthesis system only models prosodic effects locally, essentially at the syllable level, and does not take utterance-level structures such as tune into account, a ceilinghasbeenreachedforbothaccentandphrasingperformance.Representingglobal prosodic structures to ensure prosodic coherence will be one of the major challenges for future generations of speech synthesis systems. 5. Related Work
The FLIGHTS system combines and extends earlier approaches to user-tailored genera-tioninspokendialogue.AdistinguishingfeatureofFLIGHTSisthatitadaptsitsoutput according to user preferences at all levels of the generation process, from the selection of content to linguistic realization and the prosody targeted in speech synthesis. pler content planning strategies and does not explicitly point out the trade-offs among options. MATCH also uses simple templates for realization, and does not attempt to control intonation. Carenini and Moore X  X  (2000, 2006) system is also closely related, but it does not make comparisons, and generates text rather than speech. Carberry et al. X  X  (1999) system likewise employs additive decision models in recommending courses, thoughtheirfocusisondynamicallyacquiringamodelofthestudent X  X preferences,and thesystemislimitedtorecommending asingleoptionconsideredbetterthantheuser X  X  current one. In addition, the system only addresses the problem of selecting positive attributestojustifytherecommendation,anddoesnotplanandprosodicallyrealizethe positive and negative attributes of multiple suggested options.
 identify the attributes that justify their desirability, in order to present a summary, com-parison, or recommendation to the user. Evaluation showed that tailoring recommen-dations and comparisons to the user increased argument effectiveness and improved user satisfaction (Walker et al. 2004). Thus, the user-model (UM-) based approach is an appropriate strategy for spoken dialogue systems when there are a small number of options to present, either because the number of options is limited or because users can supply sufficient constraints to winnow down a large set before querying the database of options.
 for a number of reasons: (1) if there are many options that share attribute values, they will be very close in score when ranked using the UM-based approach; (2) users may not be able to provide constraints until they hear more information about the space of options; and (3) the UM-based approach does not give users an overview of the option space, and this may reduce their confidence that they have been told about the best option(s) (Demberg and Moore 2006).
 proach, in which the system structures a large number of options into a small number 194 ofclustersthatshareattributes.Thesystemthensummarizestheclustersbasedontheir attributes, implicitly prompting the user to provide additional constraints. The system produces summaries such as which help the user get an overview of the option space. Chung (2004) extended this approach by proposing a constraint relaxation strategy for coping with queries that are too restrictive to be satisfied by any option. Pon-Barry, Weng, and Varges (2006) foundthatfewerdialogueturnswerenecessarywhenthesystemproactivelysuggested refinements and relaxations.
 to the SR approach. First, many turns may be required during the refinement process.
Second, if there is no optimal solution, exploration of trade-offs is difficult. Finally, the attributes on which the data has been clustered may be irrelevant for the specific user.
Demberg and Moore (2006) subsequently developed the user-model-based summarize and refine approach (UMSR) to combine the benefits of the UM and SR approaches, by integratingusermodelingwithautomatedclustering.Whentherearemorethanasmall number of relevant options, the UMSR approach builds a cluster-based tree structure which orders the options to allow for stepwise refinement. The effectiveness of the tree structure, which directs the dialogue flow, is optimized by taking the user X  X  preferences into account. Trade-offs between alternative options are presented explicitly to give the user a better overview of the option space and lead the user to a more informed choice. To give the user confidence that they are being presented with all relevant options,abriefaccountoftheremaining(irrelevant)optionsisalsoprovided.Resultsof alaboratoryexperimentcomparingtheSRandUMSRapproachesdemonstratedthat(1) participantspreferredUMSR,(2)UMSRpresentationsareaseasytounderstandasthose ofSR,(3)UMSRincreasesoverallusersatisfaction,(4)UMSRsignificantlyimprovesthe user X  X  overview of the available options, and (5) UMSR increases users X  confidence in having heard about all relevant options. Although the UMSR approach has not been implemented in the FLIGHTS system, it could be used when there are a large number of available options to winnow them down to a handful of relevant ones, which would then be compared following the approach described in this article.
 generatorhasdirectlyinformedourapproachtoinformationstructureandprosody;his systemdoesnotmakeuseofquantitativeusermodelsthough,andonlydescribessingle options. Theune (2002) likewise follows Prevost X  X  approach in her system, refining the way contrast is determined in assigning pitch accents. Theune et al. (2001) show that a system employing syntactic templates and a rule-based prosody assignment algorithm leads to more natural synthesis (of Dutch); unlike FLIGHTS though, their D2S system does not employ a user preference model or a notion of theme phrase, and does not distinguish different types of pitch accents. Also closely related is Kruijff-Korbayov  X  a et al. X  X  (2003) information-state based dialogue system, in which the authors explore a similar approach to using information structure across dialogue turns; however, their system does not make use of a user model, and employs template-based realization with much simpler sentence structures. Kruijff-Korbayov  X  a et al. likewise present an evaluation indicating that the contextual appropriateness of spoken output (in Ger-man) improves when intonation is assigned on the basis of information structure. In comparisonwithourevaluation,theirsexaminesimprovementsoverageneralpurpose text-to-speech voice with default intonation, rather than a limited domain voice, which provides a much higher baseline in terms of the naturalness of the resulting intonation. prediction in text-to-speech (TTS) systems (Hirschberg 1993; Hirschberg and Prieto 1996; Taylor and Black 1998; Dusterhoff, Black, and Taylor 1999; Brenier et al. 2006) and concept-to-speech (CTS) systems (Hitzeman et al. 1998, 1999; Pan, McKeown, and
Hirschberg 2002). These approaches have typically aimed to develop generic models of prosody prediction, by training classifiers for accents and boundaries that make use of a considerable variety of features. For example, in predicting accent placement (but not type), Hitzeman et al. X  X  CTS system makes use of rhetorical relations such as list and contrast , along with the reference type of NPs and whether they represent first mentions of an entity in the discourse. In Pan et al. X  X  more comprehensive study, their system predicts accent placement (but not type), break indices, and edge tones based on features extracted from the SURGE realizer (Elhadad 1993), deep semantic and discourse features, including semantic type, semantic abnormality and given/new status,andsurfacefeatures,suchaspartofspeechandwordinformativeness.However, neither of these CTS approaches makes use of the theme/rheme distinction, or the notion of kontrast that stems from Rooth X  X  (1992) work on alternative sets, both of which are crucial to Steedman X  X  (2000a) theory of how information structure constrains prosodic choices. More recently, Brenier et al. have shown that the ratio of accented to unaccentedtokensofawordinspontaneousspeechisasurprisinglyeffectivefeaturein predicting pitch accents; they also argued that using information status and contrast is unlikely to improve upon prominence prediction based only on surface features, since thesemanuallylabeledfeaturesdidnotyieldsubstantialimprovementsintheirdecision tree models. Again, however, their approach does not make use of the theme/rheme distinction, and does not attempt topredict pitch accent type or edge tones; in addition, they report frequent errors on auxiliaries and negatives (e.g., no ), which we have found to be important for highlighting trade-offs prosodically.
 of sharply distinctive theme and rheme tunes in the limited domain of a dialogue system, using a hybrid rule-based and data-driven approach. In particular, whereas
Hitzeman et al. (1998, 1999) and Pan, McKeown, and Hirschberg (2002) make use of individual classifiers for prosodic realization decisions X  X ith no means of tying the decisions of these classifiers together X  X ur approach instead uses rules and constraints in the grammar to specify a space of possible realizations, through which the realizer searches to find a sequence of words, pitch accents, and edge tones that maximizes the probability assigned by an n -gram model for the domain.
 likewise aim to reproduce distinctive intonational patterns in a limited domain. How-ever, unlike our approach, theirs makes use of simple templates for generating para-phrases, as their focus is on how deferring the final choice of wording and prosodic realizationtotheirsynthesizerenablesthemtoachievemorenaturalsoundingsynthetic speech. Following on the work described in this article, Nakatsu and White (2006) present a discriminative approach to realization ranking based on predicted synthesis quality that is directly compatible with the FLIGHTS system.
 speech synthesis evaluation continues, with the Blizzard Challenge (Black and Tokuda 2005;FraserandKing2007)provingtobeausefulforumfordiscussingandperforming evaluation across different synthesis platforms. Mayo, Clark, and King (2005) have pro-posed to evaluate speech synthesis evaluation from a perceptual viewpoint to discover 196 exactly what subjects pay attention to, in order to ensure that evaluation actually is evaluating what we think it is. The authors found through the use of multidimensional scaling (MDS) techniques that, when asked to make judgments on the naturalness of syntheticspeech,subjectsmadejudgmentsrelatingtoanumberofdifferentdimensions, including both segmental quality and prosody. Subsequently, Clark et al. (2007) found correlations between results in MDS spaces and standard mean opinion score (MOS) tests, but as the MOS tests did not correspond to single dimensions in the MDS space, they suggested that it may be possible to design more informative evaluations by asking subjects to specifically rate each factor of interest (e.g., prosody), where each factor relates to one dimension in the MDS space. However, as no specific method is suggested to guarantee reliable prosodic judgments from naive listeners, we have left thisquestionforfutureresearch,optinginsteadtoaugmentthelistenerratingsgathered in our perception experiment with an expert prosody evaluation and an f0 analysis of the theme phrases. 6. Conclusions and Discussion
In this article, we have described an approach to presenting user-tailored information in spoken dialogues which for the first time brings together multi-attribute decision models,strategiccontentplanning,surfacerealizationwhichincorporatesprosodicfea-tures,andunitselectionsynthesisthattakestheresultingprosodicmarkupintoaccount.
Based on the user model, the system selects the most important subset of the available options to mention and the attributes that are most relevant to choosing between them. To convey these trade-offs, the system employs a novel presentation strategy which makes it straightforward to determine information structure and the contents of referringexpressions.DuringsurfacerealizationwithOpenCCG,theprosodicstructure is derived from the information structure in a way that allows phrase boundaries to be determinedinaflexible,data-drivenfashion,andwithsignificantlyhigheracceptability than baseline prosody prediction models in an expert evaluation. We hypothesize that the resulting descriptions are both memorable and easy for users to understand. As a step towards verifying this hypothesis, we have presented an experiment which shows that listeners perceive a unit selection voice that makes use of the prosodic markup as significantly more natural than either of two baseline synthetic voices. Through an expert evaluation and f0 analysis, we have also confirmed the superiority of the generator-drivenintonationanditscontributiontolisteners X  X atings.Infuturework,we intendtoexaminetheimpactofourgenerationandsynthesismethodsonmemorability or other task-oriented measures.
 of developing general purpose synthesizers that accept prosodic specifications in their input. The main reason that we did not use such a synthesizer in our evaluation is that inordertobuildageneralpurposeFestivalAPMLvoice,suitableAPMLmarkupwould berequiredforthe2,000 X 3,000 utterancesthatmakeupthecoreunitselectiondatabase.
As these utterances are outside of the FLIGHTS domain (and thus not generated by the NLG system), it would not be possible with current technology to provide accurate
APML markup for these utterances. Given the difficulty of automatically annotating general texts with APML, it may be worth considering a simplified version of the markup for the database. For example, a system which marks the location of pri-mary phrasal stress, other pitch accents, and a simple categorization of overall tune ( wh -question, yes / no -question, statement, etc.) could be used to annotate the speech database. This could be achieved with an accent detector and a very simple parser to determine tune type. The APML specification on the input could easily be mapped to information equivalent to the database annotation by simple rules. Reasonable quality synthesis could then be achieved without the database being fully parsed and anno-tated. Additionally, if there is a portion of in-domain data in the database where full annotation is available, it could be used directly when those units are searched. prediction models, along the lines of Hitzeman et al. (1999) and Pan, McKeown, and
Hirschberg(2002) X  X hichmakenouseofsuchinformationstructuralnotionsastheme, rheme, and kontrast (cf. Section 2.5) X  X ould be trained to produce tunes as distinctive as those we have targeted. We suspect that such models would have trouble doing so, given data sparsity issues and the fact that machine-learned classification models tend todiscovergeneraltrends,ratherthanaimingtoreproduceaspectsofspecificexamples, which may contain important but rare events. At the same time, it remains for us to investigatewhetherourhybridrule-basedanddata-drivenapproachcanbegeneralized tobeasflexibleandwidelyapplicableasthesemachine-learnedmodels,whileretaining its ability to express contrasts intelligibly. In so doing, we expect information structural constraints in the grammar to continue to play an important role.
 Acknowledgments References 198 200
