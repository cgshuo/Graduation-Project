 Venkata Jakkula Murugan Sankardadass Eric Cosatto Srimat Chakradhar Jakkula@nec-labs.com murugs@nec -labs.com cosatto@nec -labs.com chak@nec -labs.com Machine learning demands higher and higher compute -performance, but serial processors are not improving that much anymore -at least not a s quickly as they used to. Main stream processor development is moving to multi -core systems, using shared memory technol ogy to hide the parallel nature of the processors. But shared memory technology does not scale to hundreds or thousands of cores. In order to reach such levels of parallelization alternative approaches have to be developed. Massively parallel general -purpose computers had limited success so far, because of difficulties programming these machines, and they remain a niche market, mostly in high -performance computing. Yet processors specialized for certain application domains , such as successful mass products. They improve performance over general -purpose processors by focusing on a few key algorithmic elements, yet still maintain enough flexibility that they can be programmed for a variety of applications. We explore in this paper if a similar approach can lead to efficient machine learning processors. Several processors optimized for machine learning, in particular for neural networks, were developed during the 1980 X  X  and 90 X  X . E xample s are the Synaps e-1 architecture [1], or the Connectionist Network Supercomputer, CNS1 [ 2]. Recently there has been less activity in this field, but some accelerators are sold today for specific applications, such as the Axeon [ 3] processor for power train control of cars. Beside digital processors a large number of analog circuits were built, emulating neural network structures. Extremely high perfor mance with low implementations on FPGA have been demonstrated in recent years [6-8], yet reached only low compute -performances . All m achine learning processors had only limited success so far , indicating how difficult it is to find a good combination of performance, flexibility, price and ease of use. An important consideration is that many applications of machine learning, such as vide o analysis, data mining, or personalization of servic es, show the most promise in embedded systems. Embedded learning requires high compute performance while dissipating little power, a aim is to develop architectures that meet the requirements for embedded learning, but are programmable and therefore can be used in a wide range of applications . With the goal of analyzing different architectures we designed a development and testing environment where the parallel computation is mapped onto FPGA  X  X . Initially this system was intended only for experimentation, but its performance is so high that this platform is useful in its own right as accelerator for high -performance systems. While the experiments shown here emphasize high performance, the architecture has been designed from the start for low power main data flow local, low operating frequencies, and a modular design, so that unused parts can be powered down dynamically. All results shown here are from the test platform; migration to low -power FPGA or chip designs are done in a later stage. For a substantial improvement over a general purpose processor, the algorithms, the arithmetic units, as well as the architecture have to be optimized simultaneously. This is not just an exercise in hardware design, but algorithms and their software implementations have to be developed concurrently. M ost machine learning algorithms have not been developed with parallelization in bottlenecks , and then extract common computational patterns that can be mapped into accelerator hardware. 2 .1 Algorithms Characteristic for machine learning is that large amounts of data need to be processed, often with predictable data access patterns and no dependency between operations over large segments of the computation. This is why data -parallel ization can often provide good accelerations on multi -core chips, clusters of machines, or even on loosely coupled networks of machines. Using MapReduce, speedups linear with the number of processors have been reported in [9] for several machine processors in some cases. Many algorithms , such as KNN, K -means clustering, LVQ, and Neural Networks can b e reduced to forms where the computation is dominated by vector -matrix multiplications , which are easily parallelizable . For Convolutional Neural N etworks (CNN) the data flow can be complex, yet the core of the computation is a convolution, an operation which has been studied extensively for parallel implementations. For Support Vector Machines (SVM ), several parallel algorithms were described, but most saturate quickly for more than 16 processors . Scaling to larger numbers of processors ha s been demonstrated, applying MapReduce on a graphics processor with 128 cores [10]. Another implementation on a clu ster of 48 dual -core machines (with 384 MMX units ) [11] scales even super -linearly, and, according to simulations, scales to thousan ds of cores . vector dimensionalities and large numbers of vectors must be handled efficiently . Yet this alone is not sufficient since data access patterns vary greatl y between algorithms. W e analyze this here in more detail for SVM and CNN . Thes e algorithms were chosen , because they are widely used for industrial applications and cover a broad range of computation , I/O , and memory requirements. The characteristics of the SVM training are summarized in Table 1. We use an approach similar to kernel matrix dominates by far . This i s needed to update the gradients, and in the present around 100, operations 2 and 5 can become bottlenecks and should also be mapped onto the into the processor, leading to very high I /O requirements. W e consider here dimensions of 10 -10 4 to the processors at each iteration . Neural network algorithms are essentially sequences of vector -matrix multiplications, but networks with special connectivity patterns, such as convolutional networks have very different IO characteristics than fully connected networks. Table 2 shows the computation and IO requirements for scanning several convolution kernels over one input plane. A full network requires multiple of these operations for one layer, with nonlineari ties between layers . We map all operations onto the FPGA accelerator, since intermediate results are re -used right away. The m ost significant difference to between th e SVM and CNN is the Compute /IO ratio: SVM: ~ 1; CNN : ~ L*k 2 &gt; 100. Therefore the requirements for these two algorithms are very different, and handling both cases efficiently is quite a challenge for an architecture design.
 1 Load L kernels L* k 2 FPGA 2 Shift in new pixel n* m FPGA 3 Multiply kernels n * m * L * k 2 FPGA 4 Shift out result n *m FPGA 2.2 Arithmetic low resolution in most of the computation s. This has been investigated extensively for neural noisy process, because we see only a sparse sampling of the true probability distributions. A different type of n oise is introduced in gradient descent algorithms , whe n only a few training data are used at a time to move the optimization forward iteratively . This noise is particularly pronounced for stochastic gradient descent. There is no point in representing noisy variables with computation can be used.
 It is important, not to confuse this tolerance to low resolution with the resolu tion required to avoid numeric instabilities. Some of the computations have to be performed with a high resolution, in particular for variables that are updated incrementally. They maintain the state of the optimization and may change in very small steps . But usually by far the largest part of the computation can be executed at a low resolution. Key is that the hardware is flexible enough and can take advantage of reduced resolution while handling high resolution where necessary . We developed a simulator that allows running the training algorithms with various resolutions in each of the variables. A few examples for SVM training are show n in Table 3. Reducing the resolution of the kernel values from double or float to 16 bit fixed point representations does not accumulator needs sufficient resolution to avoid over/under flow (48 bit). Once the calculation of tolerable for the  X  values, but a high resolution is required for the gradients (double). For Neural Networks, including CNN, several studies have confirmed that states and gradients can be kept at low resolutions (&lt;16 bit), but the weights must be maintained at a high resolution (float) (see e.g. trained , for the classification low resolutions can be used for the weights as well (&lt;16 bit). 2.3 Architecture Based on the analysis above, it is clear that the architecture must be optimized for processing massive amounts of data with relatively low precision. Most of the time, data access patterns are predictable and data are processed in blocks that can be stored contiguously. This type of computation is well suited for vector processing, and simple vector processing elemen ts (VPE) with fixed -point arithmetic can handle the operations. Since typically large blocks of data are processed with the same operation, groups of VPE can work in SIMD (single instruction multiple data) mode. Algorithms must then be segmented to map t he high-volume, low precision parts onto the vector accelerators and parts requiring high precision arithmetic onto the CPU. The most important design decision i s the organization of the memory . Most memory access es are done in large blocks, so that the d ata can be streamed, mak ing complex caching unnecessary. This is fortunate, since the amounts of data to be loaded onto the processor are so large that conventional caching strategies would be overwhelmed anyway . Because the blocks tend to be large, a high data bandwidth is crucial , but latency for starting a block transfer is less critical. Therefore we can use regular DDR memories and still get high IO rates . This led to the design shown schematically in Figure 1 , where independent memory banks are connected via separate IO ports for each group of 32 VPE . By connecting multiple of the units shown in Figure 1 to a CPU, t his architecture scales to large r numbers of VPE. Parallel data IO and parallel memory access scale simultan eously with the number of parallel cores , and we therefore refer to this as the P 3 (P-cube) architecture. block . Avoiding movements of data over long distances is crucial for low power dissipation. How far this architecture can reasonably scale with one CPU depends on the algorithms, the amount of data and the vector dimensionality (see below). A few hundred VPE per CPU have provided good faster CPU -FPGA connections . This architecture fits surprisingly well onto some of the recent FPGA chips that are available with several hundred Digital Signal Processors ( DSP ) units and over 1,000 I O pins for data transfers . The board s used here contain each one Xilinx Virtex 5 LX330T -2 FPGA coupled to 4 independent DDR2 SDRAM with a total of 1GB, and 2 independent 4MB SSRAM memory banks (commercial board from AlphaData) . One FPGA chip contains 192 DSP with a maximum speed of 550MHz, which corresponds to a theoretical compute -performance of 105.6 GMACS (18 bit and 25 bit operands). There is a total of 14 Mbit of on-chip memory, can be used and the actual clock frequencies tend to be considerably lower than what is advertised for such chips (typically 230MHz or less for our designs) . Nevertheless, we obtain high performance s because we can use a large number of DSP units for executing the main computation. The main architecture features are: Figure 2 shows the configuration of the VPE s for vector dot product computation used for SVM training and classification . For training , the main computation is the calculation of one per cycle. But the speed is nevertheless IO bound. When several vectors can be stored on -chip, as is the case for classification, then the speed becomes compute-bound. The operation for SVM training on the FPGA corresponds to a vector -matrix multiplication and the one for classification to a matrix -matrix multiplication. Therefore the configuration of Figure 2 is useful for many other algorithms as well, where operations with large vectors and matrices are needed, such as Neural Networks. We implemented a specialized configuration for Convolutional Neural Networks, for more efficiency and lower power dissipation . The VPE are daisy -chained ratio (Table 2) to reduce the data transfers from memory. We evaluated SVM training and classification with the NORB and MNIST problems, the latter with up to 2 million training samples (data from [11]). Both are benchmarks with vectors of high dimensionality , representative for applications in image and video analysis. The computation is split between CPU and FPGA as indicated by Table 1. The DDR2 memory banks are clocked at 230MHz, providing double that rate for data trans fers. The data may be com pressed to save IO bandwidth. O n the FPGA they are decompressed first and distributed to the VPE . In our case , a 32 bit word contain s eight 4-bit vector components. Four 32 bit words are needed to feed all 32 VPE s VPE executes a multiplication plus add operation in one clock cycle, resulting in a theoretical maximum of 14.7 GMACS per chip. The sustained compute -rate is lower, about 9.4 GMACS , due to overhead (see Table 4 ). The computation on the host CPU overlaps with that on the FPGA , and clocked higher, at 2 30 MHz . B y using 4-bit operands we can execute 2 multiply -accumulates simultaneously on one DSP, resulting in speed that is more than four time s higher and a sustained 43.0 GMACS limited by the number and speed of the VPE . Adding a second FPGA card doubles saturation (see Fig. 3) . The compute speed in GMACS obtained for NORB is almost identical. # Iter ations t ime speed time speed time speed time speed 60k 8,000 754 s 0. 5 240 s 1. 5 7 40 s 9. 42 2 1 s 17.9 2M 266,900 ----531 , 534 s 1.5 8 88 , 589 s 9. 48 48,723 s 1 7 .2 Table 4: Training times and average compute speed for SVM training . Systems tested: CPU, Opteron, 2.2GHz ; CPU using MMX; CPU with one FPGA; CPU with two FPGA boards. 
Results are shown for training size s of 60k and 2M samples. Compute speed is in GMACS (just kernel computations) . Training algorithm: SMO with second order working set selection.
 Parallelization s of SVM training have been reported recently for a GPU [10] and for a cluster [11] , both using the MNIST data . In [10] different bounds for stopping were used than here and in [11] . Nevertheless, a comparison of the compute performance is possible, because b ased on the number of iterations we can compute the average GMACS for the kernel computation s. As can be seen in Table 5 a single FPGA is similar in speed to a GPU with 128 stream processors, despite a clock MMX unit s is about 6 times faster than one FPGA with 128 VPE , but dissipates about two orders of magnitude m ore electric power . For the FPGA this calculation includes only the computation of the rest of the calculations can be mapped on the FPGA as well and will increase the power dissipation only minimally.
 Table 5: Comparison of performances for SVM training ( MNIST data) . GPU: Nvidia 8800 GT X. Cluster: 48 dual core CPU (Athlon ), 384 MMX units . The GPU was training with 60k samples ([10], table 2, second order), the cluster trained with 2 million samples .

Figure 3: Acceleration of SVM training as a functio n of the number of VPE. MNIST n: 2,000,000, d=78 4; NORB: n=48,560, d=5,184. The points for 128 and 256 VPE are experimental, the h igher ones are simulations. Curves MNIST , NORB : Multiple FP GA are attached to one CPU. Curve MNIST C : Each FPGA is attached to a separate host CPU. Scaling of the acceleration with the number of VPE s is shown in Figure 3. The reference speed is that of one FPGA attached to a CPU. The evaluation has been done experimentally for 128 and dimensionality of the vectors, but to a much lesser extent on the number of training vectors (up to the limit of the memory on the FPGA card). MNIST s aturates for more than two FPGA s because the n the CPU and FPGA computation times become comparable. For the larger vectors of NORB ( d =5,1 84) this saturation starts to be noticeable for more than 4 FPGA. Alternatively, a system can be scaled by grouping multiple CPU, each wit h one attached FPGA accelerator . Then the scaling follows a linear or even super -linear acceleration (MNIST C) to s everal thousand VPE. If the CPU s are working in a cluster arrangement, the scaling is similar to the one described in [11]. VPE to operate as systolic array. In this way convolutions can be implemented with minimal data mov ements. In addition to the convolution, also sub-sampling and non-linear functions plus the logistics to handle multiple layers with arbitrary numbers of kernels in each layer are done on the FPGA. Four separate blocks of such convolvers are packed onto one FPGA, using 100 VPE. Clocked at 115MHz , this architecture provides a maximum of 11.5 GMACS. Including all the overhead the sustained speed is about 10 GMACS. By systematically exploiting characteristic properties of machine learning algorithms , we developed a new massively parallel processor architecture that is very efficient and can be scaled to thousands of processing elements . The implementation demonstrated here is more tha n an order of magnitude higher in performance than previous FPGA implementations of SVM or CNN. For the MNIST problem it is comparable to the fastest GPU implementations reported so far . The se results underline the importance of flexibility over raw compute -speed for massively parallel systems. The flexibility of the FPGA allows more efficient routing and packing of the data and the use of computations with the lowest resolution an algorithm permits. The results of Table 5 indicate the potential of this architecture for low -power operation in embedded applications. References
