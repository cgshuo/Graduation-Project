 User-generated content is becoming increasingly valuable to both individuals and businesses due to its usefulness and in-fluence in e-commerce markets. As consumers rely more on such information, posting deceptive opinions, which can be deliberately used for potential profit, is becoming more of an issue. Existing work on opinion spam detection focuses mainly on linguistic features such as n-grams, syntactic pat-terns, or LIWC. However, deep semantic analysis remains largely unstudied. In this paper, we propose a frame-based deep semantic analysis method for understanding rich char-acteristics of deceptive and truthful opinions written by var-ious types of individuals including crowdsourcing workers, employees who have expert-level domain knowledge about local businesses, and online users who post on Yelp and Tri-pAdvisor. Using our proposed semantic frame feature, we developed a classification model that outperforms the base-line model and achieves an accuracy of nearly 91%. Also, we performed qualitative analysis of deceptive and truthful review datasets and considered their semantic differences. Finally, we successfully found some interesting features that existing methods were unable to identify.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Algorithms, Experimentation  X  Corresponding author.
 c  X  Deceptive Opinion Spam, FrameNet, Semantic Analysis
With the rapid growth of social network services and in-ternet media, online review communities receive a significant amount of attention from both individuals and businesses. Online users increasingly rely on such reviews when making decisions. Recent surveys report that over 79% of users trust reviews written by others and consult them before making a purchase decision [1]. Naturally, there is also a growing con-cern about the potential for deceptive opinion spam or false reviews X  X ictitious opinions that are deliberately written to sound authentic to deceive readers for potential monetary gain [15]. Human readers usually cannot detect this kind of opinion spam because most opinion spam is carefully de-signed to avoid being identified by users or review content providers.

Since Jindal et al. introduced the first deceptive opinion spam problem to the research community [9], much signif-icant work has been done on identifying deceptive opinion spam contents [4, 11, 15], individual spammers [10, 12] and spamming groups [13]. Recently, the research community as well as businesses have been concerned about the opinion spam issue. Yelp 1 may be one of the few top local business review hosting sites in the U.S. that takes this opinion spam issue seriously. It has been working on its own spam filtering method for over 10 years, and its spam filtering algorithm is still trade secret. Mukherjee et al. investigated the char-acteristics of real-life dataset collected from Yelp [14]. Even though the simple linguistic feature-based (such as n-gram) classification model is highly accurate in identifying crowd-sourced deceptive opinion spam datasets [15], their study showed that it was no longer useful for the Yelp dataset. They used behavioral features of the users along with n-grams to classify Yelp X  X   X  X iltered X  and  X  X on-filtered X  reviews. The model that used behavioral features outperformed the baseline model that used only linguistic features. Mukher-jee et al. assume that Yelp uses primarily the behavioral patterns of users with many internal metrics that are un-available to the public. http://www.yelp.com
However, even though behavioral features can be used to identify deceptive opinions, they are ineffective at times. Suppose we used spammers X  activity patterns to success-fully find and block them from a site. If the spammers change their account name or make a new account (cold start), blocking spammers X  access is no longer a useful fil-tering method. This scenario illustrates that content level review analysis still needs to be considered with individu-als X  behavioral patterns. Hence, the scope of this work also focuses on content level review analysis.

The majority of existing work focuses on review (or con-tent) level classification tasks that are based on supervised learning approaches that use linguistic features. Linguistic features can be used as simple, yet powerful features in most classification tasks. However, we hardly know why specific features are highly weighted in trained models. Linguistic features do not convey semantic information that can help humans intuitively understand data. To gain insight into texts using linguistic features such as part-of-speech (POS) tags or n-grams, we need further analysis to interpret the characteristics of those features.

For example, Li et al. found that truthful reviews tend to include more nouns, adjectives, prepositions, and deter-miners (N, JJ, IN, and DT, respectively), while deceptive reviews tend to include more verbs, adverbs, and personal pronouns (V, RB, and PRP, respectively) [11]. The au-thors argue that domain experts tend to include more details about a local business using nouns, adjectives, preposition or determiners. However, the presence of particular linguistic features such as part-of-speech tags, LIWC categories, or n-gram tokens still cannot be solid evidence of truthful review contents. Also, Feng et al. showed that deep syntactic fea-tures driven from context free grammar (CFG) can improve opinion spam detection performance [4]. Their production rule is based on the CFG parse tree (e.g., NP X S  X  NN, NP X PP  X  DT NNP) which also does not carry meaningful semantic information.

In this paper, we view deceptive opinion analysis as a problem of understanding natural language. To overcome the limitations of the existing linguistic features mentioned above, we introduce a novel frame-based semantic feature based on FrameNet [5], a knowledge base in both human and machine-readable format. While there have been many studies on linguistic features such as n-gram, part-of-speech and syntactic structure, semantic analysis that uses frame features remains unstudied. Our contributions in this paper are four-fold:
We have found that truthful reviewers are more likely to focus on spatial details, which is consistent with previous studies [11, 15]. Moreover, truthful reviewers usually in-clude exact dates or dimensions of objects to describe their real experience. Another interesting observation is that even though employees had expert-level knowledge of their spe-cific business location and are able to include details in their review, the employees failed to mention details about their travel experience, itinerary, or travel activities. We also found different sentiment expression patterns in deceptive reviews and truthful reviews in terms of linguistic theory-based sentiment word categorization.

The rest of this paper is organized as follows. In Section 2, we briefly discuss previous opinion spam research trends. In Section 3, we describe our proposed Normalized Frame Rate (NFR) and Normalized Bi-frame Rate (NBFR) methods and their statistical validation. In Section 4, we qualitatively analyze semantic frame features. In Section 5, we report our own implementation of the baseline model in [15] and our classification results using semantic frame features with and without n-gram features. We conclude the paper and discuss future work in Section 6. Previous studies on spam detection. The opinion spam issue was first introduced by Jindal et al. [9] and was inves-tigated after studies on web or email spam were published [2, 3, 8, 17]. The authors categorized review spam into sev-eral different types (untruthful opinions, reviews on brands only, and non-reviews) by characteristics, and proposed de-tection techniques using review centric, reviewer centric, and product centric features. In [12], target and deviation based spamming behavioral patterns were used to detect spam-mers. To support human evaluators X  decision making, they also developed review spammer evaluation software that pro-vides a visualization interface for reviews. The interface contains information about review duplication, review rat-ings, other recent ratings, and multiple reviews on the same products. Wang et al. proposed a review graph-based ap-proach for finding fake reviewers [18]. They introduced three concepts regarding the trustiness of reviewers, the honesty of reviews, and the reliability of stores and their interre-lationships. The experimental results and human partici-pants confirmed that the proposed method can identify sub-tle spamming activities.

An in-depth study about deceptive opinion spam was con-ducted by Ott et al. [15]. They used Amazon Mechani-cal Turk 2 (AMT) to solicit fake reviews (which will be re-ferred to as AMT dataset in this paper) about hotels in the Chicago area. Their model achieved high accuracy (89.8%) using simple n-gram-based features. Feng et al. introduced syntactic structures of review sentences and thus further boosted classification accuracy [4]. Li et al. developed multi-domain deceptive datasets (which will be referred to as Em-ployee dataset) that include reviews by domain experts to establish general rules for identifying deceptive opinions in reviews [11]. Their Employee dataset compensates for the lack of experience of Turkers who also do not put much ef-fort into faking reviews. Li et al. found that a lack of spatial details may not be a universal cue for deception. They also proposed general linguistic cues of deceptive opinion spam as different usage of part-of-speech tags, LIWC categories [16], and first-person singular pronouns. https://www.mturk.com/mturk/welcome (http://demo.ark.cs.cmu.edu/parse)
Although some studies focused mainly on identifying opin-ion spam content, some existing studies have used consumers X  behavioral patterns to detect individual fake reviewers and fake review groups. For instance, Jindal et al. captured un-usual review patterns using domain-independent rules [10]. Mukherjee et al. proposed several spamming behavioral in-dicators of spamming activities [13].
 Semantic Frame Theory and FrameNet. FrameNet [5] is a lexical resource for English with semantic represen-tations in combination of word sense disambiguation and semantic role labeling and is based on frame semantics [6]. Frames are cognitively founded and formally explored de-vices that represent knowledge about objects and categories by means of their attributes and values [7]. A word (or a lexical unit (LU)) in a sentence evokes a frame of semantic knowledge and the frame describes prototypical situations spoken in natural language. The frame contains a set of se-mantic roles that correspond to the participants (Frame El-ements (FEs)) of a described event or situation. Many NLP researches use FrameNet especially for semantic role label-ing, word sense disambiguation and question-answering. In this work, we use FrameNet as our core analysis tool. In FrameNet, the machine processes all review content us-ing semantic frame units. Unlike linguistic features, output results that are based on frames are also in human-readable and understandable format, which helps us understand the data more efficiently. We use a simple example to explain frame-based analysis.

Suppose we have the following sentence:  X  X y girlfriend and I stayed 4 nights at the Talbott returning home on Sat-urday 9/29. X  3 The FrameNet parsing result of the above sentence is described in Figure 1. In this example, a total of seven frames are identified. The darker gray box represents a frame that was evoked by a lexical unit and the succes-sive lighter gray boxes represent their frame units (and frame range). Note that the verbs stayed and returning evoke their dedicated Residence and Arriving frames respectively, and the nouns girlfriend and home evoke Personal relationship and Foreign or domestic country frames, respectively. By analyzing these extracted frames, we can figure out that the reviewer would like to convey some sense of his or her rela-tionships, travel schedule, and duration of stay.

We hypothesize that the frame occurrence may be differ-ent because of the different mindsets or experiences of truth-ful users and spammers. On this basis, we propose metrics for measuring the prevalence of frames for each dataset in the following subsection.
This sentence is borrowed from the opinion spam dataset of [15].
In this section, we present our methodology of using se-mantic frames for analyzing user-generated content. We first define measures of the difference between frame proportions for each dataset. We then obtain frames that have high discriminative power and explore their interesting charac-teristics.
Although there are many, not every frame that appears in a sentence is useful. To select the most effective frames for an analysis, we propose Normalized Frame Rate (NFR) and Normalized Bi-frame Rate (NBFR), both of which quanti-tatively measure the discriminative power of a frame and frame pairs, respectively. We will explain each metric using examples in Table 1.
We define Normalized Frame Rate (NFR) to compute the frame distribution differences between truthful reviews and deceptive opinion spam. The NFR value of a specific frame indicates the number of times the frame occurred compared with the total number of occurrences of frames in a dataset. Before we count each frame occurrence from the dataset, we first investigate whether there is a statistically significant difference of frame proportion between the deceptive opinion spam dataset and the truthful review dataset using a two-proportion z-test. The test result successfully rejects the null hypothesis. ( H 0: sample proportions of each frame are equal on both datasets in significant level p &lt; 0.01)
Now, let us explain how to calculate the NFR value for each class. The Personal relationship frame (f 1 ) occurred twice in the deceptive class but did not occur in the truthful class. Since the total sum of frame occurrence of the de-ceptive class is 11, the NFR value of f 1 is 2/11 or 0.18 for the deceptive class and 0 for the truthful class. Similarly, the Calendric unit frame (f 4 ) occurred three times in the truthful class and twice in the deceptive class. The NFR value of f 4 for the truthful class is 0.33 and 0.18 for the deceptive class. Then we calculate the difference between two NFR values of a frame by subtracting the NFR value for the truthful class from the NFR value for the deceptive class. The difference between two NFR values for a specific frame f m is calculated as below:
For example, the  X  N F R f 4 value is -0.15 (0.18 -0.3); it is negative because the Calendric unit frame (f 4 ) is more likely to appear in the truthful class.
A bi-frame refers to a pair of successively occurring frames in a sentence. While bigram catches context of a text in n-gram-based approach, a bi-frame also conveys semantic context of a given sentence. Furthermore, we consider the frame sequence as the latent writing style of a reviewer.
We use Table 1 again to explain NBFR. In a sentence s , f 1 , f 2 , f 9 and f 7 frames are identified. We obtain bi-frame pairs by grouping successive frames into two such as (f in the exact same way as we calculate NFR, but now we count two frames as one counting unit. For example, the bi-frame Personal relationship (f 1 )  X  Residence (f 2 ) occurred twice in the deceptive class but did not occur in the truth-ful class. Therefore, the NBFR value of the bi-frame Per-sonal relationship (f 1 )  X  Residence (f 2 ) in the deceptive class is 2/7 (0.29). In general,  X  X BFR for frame f f is calculated as below: Hence,  X  N BF R f 1 f 2 is a positive value of 0.29 which means that this bi-frame is more likely to appear in the deceptive class.
This section presents a qualitative analysis of frame fea-tures. We first introduce the dataset that was used in this paper in Section 4.1. Then, we investigate the individual frame characteristics that appeared in each dataset and look into bi-frames in Section 4.2.
Table 2 shows the statistics of the datasets used in this paper. For the truthful opinion reviews, we used the Tri-pAdvisor dataset from Ott et al. [15]. Details about the de-ceptive opinion spam datasets solicited from crowdsource, employees, and real-life screened reviews are described in what follows.

Gathering quality and reliable gold-standard deceptive opin-ion datasets is a challenging task in opinion spam analysis research. Ott et al. employed a crowdsourcing framework provided by Amazon to generate gold-standard deceptive opinion spam datasets [15]. However, Li et al. pointed out that the AMT dataset is only representative of a specific type of deception that is generated by people who do not have particular experience with a target local business [11]. The reviewers X  lack of experience is complemented by hiring real employees and allowing them to write deceptive opinion reviews based on their higher-level domain knowledge.
For the experiment, we explore the gold-standard decep-tive opinion datasets from [15], [11] and our own real-life dataset collected from Yelp. Since the AMT dataset con-tains only hotel reviews, we also only use the hotel employee dataset in [11]. We will investigate the characteristics of each dataset in relation to semantic frames, and not just linguis-tic differences which were already studied in previous work [4, 11, 15].
We gathered 3,869  X  X ecommended X  reviews and 3,361  X  X ot currently recommended X  4 reviews for 265 hotels in the Chicago area from Yelp. We collected only 5-star reviews to con-stantly maintain the sentiment polarity. We mixed popular and unpopular hotels based on the number of reviews but we used only reviews of hotels whose overall ratings were over 3.5. The number of unique unigrams in the Yelp dataset was almost 4 times larger than that in the AMT dataset due to the different size of the dataset (  X  20,000 compared with  X  5,000).
The goal of our analysis is to understand how frames are distributed in different datasets. Figure 2 and Figure 3 il-lustrate the top 50 frames that were sorted by their  X  X FR value, which were differentially expressed in the deceptive class and the truthful class from each of the two datasets. The NFR value of each frame is ranged from -1.0 to 1.25 in the AMT dataset, and from -2.0 to 1.6 in the Employee dataset. By the definition of  X  X FR, if a frame X  X   X  X FR
Yelp recently changed its review filtering service name from  X  X iltered X  to  X  X ot currently recommended X . The word  X  X ur-rently X  would give the sense that its review recommendation policy continues to change, so that blocked reviews are tem-porary. value is negative, the frame is more likely to occur in the truthful dataset than in the deceptive class and vice-versa.
In this analysis, we found that the nature of the frames detected by our method is also largely consistent with pre-vious deceptive opinion spam studies.
 Spatial and business detail-related frames. We ob-served that frames such as Cardinal numbers, Building subparts, Calendric unit, Dimension, Expensiveness, which are used to described the spatial details about a business are con-spicuous in the truthful dataset. For example, the Cardi-nal numbers and Calendric unit frames express a specific date of travel or arrival, duration of stay, and so on. The Building subparts frame evoked by lexical units such as room, floor, bathroom , and lobby is frequently used in both truthful datasets showing strong negative  X  X FR value for describing spatial details. These building parts are explained using spe-cific measurements (Cardinal number and Dimension). Fur-thermore, we can see that the truthful reviewers frequently commented on the price of a hotel (negative  X  X FR value of Expensiveness frame in both datasets).
This observation is consistent with previous studies that truthful reviewers are more likely to include spatial details mentioned above [11, 13, 15]. On the other hand, frames that have positive  X  X FR values (e.g., Buildings, Residence, Travel, etc.) are rather general and do not contain much detailed information.
 Personal relationship-related frames. We found that the spammers deliberately emphasize relationship keywords to convince the reader that their review is based on real ex-perience. For example, the Personal relationship frame that is evoked by the lexical units wife, husband, friend is highly ranked in Table 4 which is also confirms the previous studies. In Ott et al., The highest weighted truthful and deceptive lexical features learned by the supervised model contain the word husband [15]. Similarly, the highly weighted LIWC feature list contains the word family in the study of Li et al. [11]. Unlike the previous studies, instead of capturing each individual word as a feature, our approach captures a semantic group of a word as a feature.
 Comparison of our top-ranked lexical units and top-weighted terms in previous work. We observed that some of our top-k lexical units in Table 4 were also reported in previous studies [11, 14, 15]. For example, in the study of Li et al., the top weighted LIWC feature list contains the category word number in the truthful class. This corre-sponds to our  X  X FR negative Cardinal numbers frame [11]. Furthermore, the lexical units small, hotel , and husband of the Dimension, Buildings, and Personal relationship frames, respectively, are in the top-weighted lexical features list in [15]. The verb felt as well as nouns, which were captured by Mukherjee et al., would correspond to the lexical unit felt like of the Desiring frame [14].
 Sentiment expression pattern analysis. Interestingly, we discovered that there are different sentiment expression patterns between deceptive Turkers and truthful reviewers. Table 3 shows that the Stimulus focus frame is used more in the deceptive reviews while the Desirability frame is used more in the truthful review dataset even though both frames are related to sentiment expression. According to the Frame-Net, the Stimulus evokes a particular emotion or experience in the Experiencer. For example, in the following sentence,  X  X he movie was quite fascinating. X  5 the stimulus the movie brings out positive emotions in the reviewer. On the other hand, the Desirability frame describes the event of judging the gradable attribute Evaluee for its quality. For the given sentence,  X  X he view was astonishing. X  6 the reviewer judges the goodness of the view aspect. We assume that Turkers tend to fabricate their emotions, and that truthful reviewers tend to evaluate a target using their real experience. Hu-man readers may not be able to differentiate two sentiment words in spoken language. However, when machines read and process sentences, they capture subtle differences using the linguistic knowledge provided by the FrameNet lexical database.
Table 5 and Table 6 are the lists of the top differentially expressed bi-frames from the AMT dataset and the Em-
Example sentence of Stimulus focus frame in FrameNet. Example sentence of Desirability frame in FrameNet. ployee dataset, respectively. Each table illustrates the top 15 and the bottom 15 bi-frames, all of which were ranked according to their  X  X BFR values. To investigate how bi-frame co-occurrence patterns of two datasets are different, we use their co-occurrence pattern similarity.
 Let L be the union of the top bi-frames in Table 5 and Table 6 where | L | =42. Then, we define n  X  n matrices M and M t for the deceptive and the truthful dataset, respec-tively, where n is the number of frames in set L . Each cell of matrix M represents the NBFR value of bi-frame. By comparing the co-occurrence matrices of the two datasets, we can see how spammers and non-spammers write reviews differently in terms of semantic flow. The similarity between the two matrices M d and M t is calculated as follows.
Table 7 shows the similarity values computed between dataset pairs. To compare the similarity values, we needed a threshold value to make a decision whether two datasets show different bi-frame co-occurrence patterns. So we ran-domly divided a single dataset into two subsets and calcu-lated their within-set similarity. We repeated this procedure 10 times and reported the averages in Table 7.

As we can see in Table 7, the similarity value of AMT (de-ceptive) and TripAdvisor (truthful) is less than the within-set similarity value (0.86 &lt; 0.92). This means that the bi-frame patterns of deceptive and truthful reviews are differ-ent. There are also differences between AMT and Employee even though they are in the deceptive category. Li et al. also mentioned these characteristics and reported that their two-class classifier achieves an accuracy over 0.76 in distin-guishing between Turker and Employee reviews [11]. Note that the similarity between Employee and TripAdvisor is al-most equal to that of the Employee-within set. This implies that employees put more effort into writing fake reviews to try to sound authentic.
In Table 5 and Table 6, we also observe a similar pat-tern in the top  X  X FR list. We see that in the truthful group (the negative value of  X  X BFR), the reviewers in-clude details about the business. For instance, a bi-frame Building subparts  X  Dimension appeared in both datasets and was most likely used to describe the interior of the ho-tel. The Self motion frame evoked by the lexical unit walk or hike explains that the Self mover 7 moves under its own di-rection along a path. Accordingly, the Self motion  X  Range bi-frame was categorized in the truthful group and thus can be interpreted as a situation where the reviewer describes his or her activity around the hotel.

However, bi-frames in the deceptive group are mostly not notable and unspecific, compared with those in the truth-ful group. The Businesses  X  Travel bi-frame in the AMT dataset may indicate the reason of travel that fake review-ers fabricate for their reviews. This may be because paid Turkers need to quickly fabricate plausible reasons to try to sound authentic and pick a general topic such as business. On the other hand, employees are more likely to comment on the quality of a food (Food  X  Desirability). This is a core frame element of Self motion frame in FrameNet.

Physical artworks  X  Building subparts well represent em-ployees X  detailed knowledge about the business.
 Personal relationship  X  Arriving is also a conspicuous bi-frame which is more like a clich  X e in deception. Note that the Cardinal numbers  X  Calendric unit bi-frame is top ranked in both Tables 5 and 6 and has lowest  X  X BFR values, which means this bi-frame is more likely to appear in truthful re-views. Although employees can write details about the busi-ness (such as Physical artwork), we can assume that they cannot fabricate details about an activity-based experience such as a specific date (Calendric unit, Self motion, etc.).
We perform machine learning-based classification tasks to distinguish truthful reviews from deceptive reviews. In Sec-tion 5.1, we explain the pre-processing procedure used in the experiment. In Section 5.2, we report the classification results of varying features on different classification models.
We set a default analysis unit as a single sentence for frame analysis. Since both datasets do not annotate sen-tence boundaries, we divide each review document into mul-tiple sentences using OpenNLP Sentence Detector 8 . Then, each sentence is analyzed for dependency parsing and n-gram tokenization using Stanford Parser 9 . The parsed re-sults are passed to SEMAPHORE V2.1, an automatic frame-semantic annotation 10 system for frame extraction.
Table 8 shows the statistics of the frame extraction re-sults. The truthful dataset contains more frames than the deceptive dataset, but both datasets contain almost same number of unique frames. Due to the difficulties of gathering datasets from the real employees, the size of the Employee dataset is smaller than that of the AMT dataset.
 Table 7: Bi-frame co-occurrence matrix similarity Table 8: Frame extraction statistics obtained from two datasets
For a reliable comparison, we also report our own imple-mentation 11 of [15] for the AMT dataset (Table 9). We use SVM ( SV M light , Joachim, 1999) and Naive Bayes classifiers as classification models. To find the separating hyperplane https://opennlp.apache.org/ http://nlp.stanford.edu/software/stanford-dependencies.shtml http://www.ark.cs.cmu.edu/SEMAFOR/
We were not able to obtain the original implementation from the authors.
 Table 10: Classification result using frame feature-only Table 11: Classification result varying the number of bi-frames with a fixed number of frame features for SVM, a linear kernel is used. Each model is validated based on a nested 5-fold cross validation scheme which was also done by Ott et al. [15].

Nested cross-validation works in the following way. There is an inner CV loop where we search for optimal parameters using the parameter search algorithm (e.g., grid search). At the end of this process, we end up with k models ( k being the number of folds in the outer loop) that gives the best accuracy within the inner CV. Note that the performance re-sults of our implementation are slightly different from those of the original results reported in [15]. It is due to the differ-ent experimental settings such as model parameter, n-gram tokenization, and so on, which were not provided in the orig-inal paper.
We report the classification result of using only frame fea-ture and investigate how frame features are effective for dif-ferentiating spam and non-spam. Frm-k , ( k = 1, 2 ... n ) in Table 10 represents the number of k frames from the top and bottom in the frame list sorted by  X  X FR values. First, we trained a SVM model using only frame features. Interestingly, we obtained a much higher classification accu-racy when we used only six frame features (Frm3 setting in Table 10) than random guess of 50% (0.660 and 0.824) for both the AMT dataset and the Employee dataset. Adding more frame features gradually increases the accuracy. As we can see in Table 10, the model that uses the Frm5 fea-ture yields almost the same accuracy as the model using the Frm full feature. This proves that the top-k frames can be used as discriminative features for training supervised clas-sification models. The top 5 frames from both datasets are listed in Table 3.
We also use bi-frames along with frame features. To com-pare the results of using only frame features reported in Table 10, we mixed the bi-frame feature onto the frame fea-ture. Instead of using every combination of frames, we take only the top 1000 and the bottom 1000 bi-frame features in the bi-frame list which were sorted by their  X  X BFR value.
Figure 4 shows that the classification accuracy is grad-ually improved by adding bi-frame features to the model compared to the model that used frame-only feature in both datasets. The semantic context delivered by bi-frames can be useful for detecting deceptive opinions. Next, we investi-gate how the number of bi-frames affects classification per-formance when the number of bi-frame features varies and when the number of frame features is fixed to  X  X ull X . In this setting, the classification result did not change significantly.
In the AMT vs. Truthful setting in Table 11, the classi-fication accuracy improved from 0.785 to 0.796 as a result of increasing the number of bi-frames. However, in the Em-ployee vs. Truthful setting, the performance improvement was not as large as that in the AMT vs. Truthful setting. The accuracy did not improve even though we increased the number of bi-frames. From this result, we can assume that the more frame and bi-frame features we used, the more likely the performance will improve.
 Figure 4: Classification accuracy using frame and bi-frame feature together
In this experiment, we train Naive Bayes and SVM clas-sifiers using individual frame and bi-frame features in com-bination with lexical n-grams and the result is reported in Table 12. To address the question about the power of lexical n-grams, we vary the number of n-grams. We have evalu-ated every combination of features with various numbers of n-grams. However, we report only the meaningful re-sults due to the limit of space. Likewise Frm-k , Uni-k indi-cates the top-k unigrams that were selected using the same  X  X FR method that we used to obtain the top-k frames. The superscript + of Bi + feature indicates that the bigram feature subsumes the unigram feature set.

In Table 12, the feature combination of Frm5 and Uni500 achieved an accuracy of 0.877 whereas the baseline result achieved an accuracy of 0.870 (using unigram-only features in our implementation of Ott et al. [15]). After various tries, we obtained the highest classification accuracy of 0.914 in the AMT dataset using the feature combination of Frm5, Bi-Frm20 and full Bi + with a Naive Bayes model (a 4.34% increase compared with our baseline implementation). In the Employee dataset, Frm5, Bi-Frm15 and Uni full features obtained an accuracy of 0.924 using an SVM model (a 0.87% increase compared with the baseline).
In Table 13, we also report an SVM classification result on the Yelp dataset. First, the classification experiments that use an n-gram feature on the Yelp data yielded an accu-racy of 0.625. This result is consistent with [14] that simple linguistic features are hardly effective for real-life datasets (an accuracy of 0.676 was obtained from their Yelp dataset). Next, we added frame and bi-frame features (Frm5 and Bi-Frm20) to n-grams, which improved the performance of the previous classification task. However, the semantic frame feature did not improve classification performance. We as-sume that Yelp X  X  review recommendation system does not consider the semantic features of review contents but uti-lizes other non-content features such as users X  behavioral patterns, which were investigated by Mukherjee et al. [14]. Table 13: SVM 5-fold CV result on our Yelp dataset
In this paper, we investigate deceptive opinion spam and truthful reviews using semantic frames. We also propose an analysis method for measuring the proportion of frame in a sentence. To the best of our knowledge, this is the first study to perform semantic frame analysis of customer reviews.
Our experimental results show that the classification model that used frame features outperformed the baseline model by 4.34% for the AMT dataset. Furthermore, using new se-mantic frame features, we successively captured some inter-pretable differences between fake and real reviews in terms of their unique semantic features, which cannot be done by other existing methods. For example, even though em-ployees who have expert-level domain knowledge attempt to write a seemingly truthful review, they cannot fabricate specific details about an activity-based experience. We also captured subtle differences of sentiment expression patterns in deceptive and truthful review contents using a linguis-tic theoretical method. Looking at other domains such as restaurants or medical services using our proposed semantic analysis method remains for the future work in this area.
This work was supported by the National Research Foun-dation of Korea(NRF) grant funded by the Korea govern-ment(MSIP) (NRF-2014R1A2A1A10051238). [1] 2013 study: 79% of consumers trust online reviews as [2] A. A. Benczur, K. Csalogany, T. Sarlos, and M. Uher. [3] C. Castillo, D. Donato, A. Gionis, V. Murdock, and [4] S. Feng, R. Banerjee, and Y. Choi. Syntactic [5] C. Fillmore, C. Johnson, and M. Petruck. Background [6] C. J. Fillmore. Frame semantics and the nature of [7] T. Gamerschlag, D. Gerland, R. Osswald, and [8] Z. Gy  X  ongyi, H. Garcia-Molina, and J. Pedersen. [9] N. Jindal and B. Liu. Opinion spam and analysis. In [10] N. Jindal, B. Liu, and E.-P. Lim. Finding unusual [11] J. Li, M. Ott, C. Cardie, and E. Hovy. Towards a [12] E.-P. Lim, V.-A. Nguyen, N. Jindal, B. Liu, and [13] A. Mukherjee, B. Liu, and N. S. Glance. Spotting fake [14] A. Mukherjee, V. Venkataraman, B. Liu, and N. S. [15] M. Ott, Y. Choi, C. Cardie, and J. T. Hancock. [16] J. W. Pennebaker, C. K. Chung, M. Ireland, [17] N. Spirin and J. Han. Survey on web spam detection: [18] G. Wang, S. Xie, B. Liu, and P. S. Yu. Review graph
