 The clustering challenge. Cluster analysis represents a fundamental and widely used method of knowledge discovery, for which many approaches and algorithms have been proposed over the years [9]. Among the mos t popular methods, we find partition-based clustering (e.g. k -means [10]), density based clustering (e.g. DBScan [6]), hierarchical methods (e.g. BIRCH [14]) and grid-based methods (e.g. STING [13]). The continu-ous stream of clustering algorithms proposed over the years underscores the fact that the logical and algorithmic complexities of this many-facet problem have yet to be tamed completely, and that along with the great progress achieved in the past, signif-icant progress should be expected in the future. In particular, it is well known that no clustering algorithm completely satisfies both accuracy and effici ency requirements, thus a good clustering algorithm has to be evaluated w.r.t. some external criteria that are independent from the metric being used to compute clusters. Indeed, in this paper we propose an algorithm that significantly improves the state of the art in clustering analysis, with respect to speed, repeatability, and accuracy whose performances have been evaluated using widely accepte d clustering validity metric.
 Current Solutions: Who is the best in terms of speed and accuracy? In order to com-pare our performances we preliminary tested existing clustering solutions. We chose algorithms that are widely used by data min ers due to their general purpose nature. More in detail, we evaluated algorithms that satisfy user needs in a wide variety of ap-plication scenarios. In terms of speed of computation, the standard of paragon is set by the k -means [10] algorithm that has as objective minimizing the average distance of the samples from their cluster centroids. Owning to its efficiency, simplicity, and the naturalness of its objective, k -means has become the most wid ely used clustering algo-rithm in practical applications. But k -means also suffers from serious shortcomings: in particular, the algorithm does not assure repeatability of results, which instead depends on the choice of the initial k points. In practice, therefore, the data mining analyst will have to select the value of k , and the initial k points, via some exploratory dry runs. k -means X  shortcomings have motivated much research work [11,2,4], seeking to reduce the variability of the results it produces and bring it closer to the  X  X nsupervised learning X  archetype. A second line of research has ins tead produced different mining algorithms to overcome k -means X  problem while keeping with its general objective of clustering the points around centroids. Approaches such as grid-based clustering work by parti-tioning the data space into cells arranged in a grid and then merging them to build clus-ters, while density based approaches search for fully-connected dense regions. Finally, hierarchical clustering me thods work by performing a hierarchical decomposition of data in either bottom-up (agglomerative) or top-down (divisive) way. In this respect hi-erarchical approaches offer good performan ces w.r.t. the accuracy of clustering. All the techniques mentioned here present advantages and weakness that will be discussed in detail in the related work section. For the goal of assessing the quality of our approach, it is worth noticing that the hierarchical clustering algorithm BIRCH [14] is stable (i.e. its results do not vary depending on some initial parameter setting), accurate , impervi-ous to noise and scalable to very large data sets . However, BIRCH is typically not as fast as k -means.
 Our Solution. The above thought-provoking discussion guided our search for a new clustering algorithm. Indeed, in this paper we propose a new hierarchical algorithm called CLUBS (for CLustering Using Binary Splitting) whose speed performances are better than k -means and whose accuracy overcomes previous hierarchical algorithms while operating in a completely unsupervised fashion. The first phase of the algorithm is divisive, as the original data set is split r ecursively into miniclusters through successive binary splits: the algorithm X  X  second phase is agglomerative since these miniclusters are recombined into the final result. Due t o its features our algorithm can be used also for refining other approaches performances . As an example it can be used to overcome k -means initial assignment problem since its low complexity will not affect the overall complexity while the accuracy of our results will guarantee an excellent initial assign-ment of cluster centroids. Further, our approach induces during execution a dynamic hierarchical grid that will better fit the dataset w.r.t. classical grid approaches that ex-ploit a fixed grid instead. Finally, the algorithm exploits the analytical properties of the Quadratic Sums of Squares (SSQ in the following) function to minimize the cost of merge and split operations, and indeed the approach results really fast. One may argue that many different measures could be used f or cluster computation but the accuracy of SSQ is as good as other cluster distance measures (e.g. Single Link, Complete Link, Average) for real case scenarios and its computation can be made faster than other measures. These properties are discussed in Section 2.
 Main Difference of CLUBS w.r.t. other approaches. CLUBS works in a completely unsupervised way and overcomes the main limitations that beset other algorithms. In particular, we have that (1) CLUBS is not tied to a fixed grid, (2) it can backtrack on pre-viously wrong calculation, and (3) it performs also well on non-globular clusters where clusters are not spherical in shape, this feature will be intuitively understood after the partitioning and recombination strategy will be detailed in Section 3 (BIRCH does not perform as well, because it uses the notion o f radius or diameter to control the boundary of a cluster, and the same drawback also affects k -means like algorithms). Moreover we have that (4) CLUBS can detect the natural cl usters present in data, while in Birch each node in the auxiliary tree exploited (called CF tree) can hold only a limited number of entries due to its size thus a CF tree node does not always correspond to what a user may consider a natural cluster. Finally, (5) density based algorithms like DBSCAN are very sensitive to clustering parameters like Minimum Neighborhood Points and they fail to identify clusters if density varies and if the data set is too sparse and different sampling affects density measures, however we co mpared CLUBS against OPTICS that allows to detect clusters with different densities instead. As will be clear by experimental eval-uation, CLUBS does not suffer these limitations due to unique features of SSQ and the two-phase algorithm. After recalling some basic notions used in our algorithm, we discuss binary partitioning and the cluster quality measures there used. Throughout the paper, for each dataset a d -dimensional data distribution D is assumed. D will be treated as a multi-dimensional array of integers with volume n d (without loss of generality, we assume that all dimen-sions of D have the same size). The number of non-zero elements of D will be denoted as N .A range  X  i on the i -th dimension of D is an interval [ l..u ] , such that 1  X  l  X  u  X  n . Boundaries l and u of  X  bound ), respectively. The size of  X  i will be denoted as size (  X  i )= ub (  X  i )  X  lb (  X  i )+1 . A block b (of D )isa d -tuple  X  1 ,..., X  d where  X  i is a range on the dimension i ,for each 1  X  i  X  d . Informally, a block represents a  X  X yper-rectangular X  region of D .A block b of D with all zero elements is said to be a null block . The volume of a block b =  X  1 ,..., X  d is given by size (  X  1 )  X  ...  X  size (  X  d ) and will be denoted as vol ( b ) . Given a point in the multidimensional space x = x 1 ,...,x d , we say that x belongs to the block b (written x  X  b )if lb (  X  i )  X  x i  X  ub (  X  i ) for each i  X  [1 ..d ] .
Given a block b =  X  1 ,..., X  d ,let x be a coordinate on the i -th dimension of b such b b along the dimension i at the position x ;dimension i and coordinate x are said to be the splitting dimension and the splitting position , respectively.

Informally, a binary partition can be obtained by performing a binary split on D (thus generating the two sub-blocks D low and D high ), and then recursively partitioning these two sub-blocks with the same binary hierarchical scheme.
 Definition 1. Given a d -dimensional data distribution D with volume n d ,a binary par-tition BP of D is a binary tree such that the root of BP is the block [1 ..n ] ,..., [1 ..n ] and for each internal node p of BP the pair of children of p is a binary-split of p . Clustering Computation Preliminaries. Given a dataset DS cluster analysis aims at producing a clustering C = { C 1 ,  X  X  X  ,C n } that is a subset of the set of all subsets of DS such that C contains disjoint (non-overlapping) subsets, covering the whole object set (we refer in this paper exclusively to har d clustering problem, where every data point belongs to one and only one cluster). Consequently, every point x  X  X S is contained in exactly one and only one set C i . These sets C i are called clusters.
 Definition 2. Let C s be a cluster (set) of Nd -dimensional points. Let S =( S 1 ,...,S d )= p  X  C C . The center of C s is C 0 s = S N .Let Q =( Q 1 ,...,Q d ) ,where Q i = p  X  C p 2 i ,be the vector whose i -th coordinate is the sum of the squared i -th coordinates of the points in S . The SSQ (Sum of Squares) of C s is defined as: we recall that N is the number of points in C and thus we obtain by substituting: finally by definition of Q i and S i we obtain: From the latter, it is clear that, in order to quickly compute the SSQ of a cluster, we need only to store Q , S ,and N . In the next section we will show how these information can be used effectively and efficiently to optimize the divisive and agglomerative steps of the CLUBS algorithm. In order to obtain a good tradeoff between accu racy and efficiency we exploit in this pa-per a new really fast hierarchical approach . Among hierarchical algorithms, bottom-up approaches tend to be more accurate but hav e a higher computationa l cost than the top-down approaches [9]. The higher cost is due to the higher number of candidate clusters to be taken into account. To overcome this limitation, in our approach, the agglomera-tive step is only used on mini-clusters generated by a first divisive process, this results in a remarkable efficiency increase. Top-down partitioning exploiting greedy algorithms has been widely used in the multidimensional data compression due to its efficiency. Here we use a similar divisive approach to minimize the SSQ among the data belong-ing to clusters, we recall again that in litera ture many measure have been proposed (e.g. EES) that works in a similar way as SSQ but we chose SSQ since it offers a really fast computation while maintain ing an high accuracy in cluster model evaluation. Thus, our clustering algorithm consists of two steps, where in the first step we use binary hierar-chical partitioning to produce a set of mini-cl usters and in the second step, we pairwise merge the mini-clusters so obtained in a bottom-up fashion. In both steps the clusters are defined by a hierarchical partition of the multi-dimensional space. The partition can be compactly represented by a binary tree ( BT in the following), where:1) each node is associated with a range of the multi-dimensional domain; 2) the root is associated with the whole data domain; 3)for each inner node n , its children are associated with a pair of ranges representing a (rectangular) partition of n .

Each node also maintains summary information about points inside its range, to ex-pedite the clustering computation. The top-down splitting works as follows. As aux-iliary structure, we maintain a priority queue of clusters whose elements are ordered on the basis of the SSQ of each cluster. At each iteration, the algorithm performs the following two steps: A) select the cluster C s that exhibits the highest SSQ (i.e. the one on top of the priority queue), and then B) partition this C s in such a way that the overall SSQ reduction, denoted  X SSQ , is maximized. For step B, we compute  X SSQ ( i,j ) for each dimension i and for each cutting position j ; then we choos e the position j that guarantees the maximum  X SSQ . This computation can be done very efficiently since we pre-compute Q and S , and therefore we need a single scan of the data. We repeat these two steps, A and B above, while  X SSQ is greater than the average SSQ. We recall that the partition (i.e., the cluster tr ee) is built by exploiting a greedy strategy. To this end, the tree is constructed top-down , by means of leaf-node splitting. At each step, the leaf with the largest SSQ is chosen, and it is split as to maximize  X SSQ .Being SSQ a measure of a range skewness, we perform splits as long as  X SSQ remains  X  X ig-nificant X . After the early splits that yield large SSQ reductions, the values of  X SSQ become smaller and smaller, until after n splits both SSQ and  X SSQ become 0 (since each point has become its own cluster). Thus, the average SSQ reduction per split is SSQ 0 /n , and we will compare this value against the current  X SSQ to decide when we should stop splitting, The rationale for thi s criterion is clearly illustrate by Fig. 1, where the typical  X SSQ slope is displayed against the average SSQ: there is no gain in splitting beyond th e turning point (marked with a solid circle) since the SSQ reduction is less than the average  X SSQ and thus imputable to random distributions rather than cluster-like ones.

The splitting process just described is tied to the grid partitioning and thus may cause a non-optimal sp litting of some cluster s. The successive phase overcomes this limitation since the merging is performed cons idering all the possi ble pairs of adjacent mini-clusters, and recombining those that offer best SSQ reduction. This agglomerative process offers significant advantages. One is that it merges clusters in different grid partitions, thus overcoming non optimal splits obtained in the first phase (see Fig. 3(b) and Fig. 3(c)). The second critical advantage is that the computational complexity of this bottom-up step is very low since the number of merging steps is related to the number clusters that is very low compared to usual dataset sizes. The final advantage is that this phase also halts automatically, producing an algorithm that does not require any seeding or other parameters from the user a really nice feature that is not shared by all clustering algorithms.
 The Clustering Algorithm. Fig. 2 provides a more formal description of the CLUBS algorithm. We use the initializeTree to load the dataset into the root of the auxiliary tree structure BT exploited for partitioning. Once the tree structure has been initialized the topdowns plitting step starts. In particular, the root of BT is added to a priority queue whose ordering criterion is based on the SSQ values of clusters stored in the queue. The initial cluster assignment performed by initializeClusters is composed by the root r of BT and the initial SSQ is the one computed on r . The function computeAver-ageDeltaSSQ averages the actual SSQ for all the points in the cluster. The function computeWeightedDeltaSSQ is applied to the cluster C s that is currently on top of the priority queue. The weighted X  SSQ is computed as the average gain of SSQ obtained by splitting C s as explained above for  X  SSQ , i.e. we pre-compute the marginal sums ( S and Q ) for a given splitting point (w.r.t the coordinates ordering) and reassigning the splitting point based on these partial sums. In order to improve the effectiveness of splits the value of  X  SSQ is raised to a power of p , p&lt; 1 , thus obtaining weighted X  SSQ value. If weighted X  SSQ is greater than avgDeltaSSQ computed by computeAver-ageDeltaSSQ then we proceed with the split, otherwise we do not. We use values of p that are less than 1 ,sincefor p  X  1 we would end up splitting clusters where the gain does not exceed the Average  X  SSQ associated with a random distribution. This would result in a large number of small clusters, where both intra-cluster and inter-cluster dis-tances small. We instead seek values of p that reduce the former while magnifying the latter. We determined experimentally that the best value is p =0 . 8 regardless the dataset feature thus the user is not required to set any parameter, due to space limitations we cannot report here the detailed discussi on of the experiments being conducted.
When no more top-down splits are possible, the topDownSplitting ends and we begin the bottomUpMerging . In order to obtain more compact clusters, we select (by running selectBestPair ) the pair of clusters that, if merged, yields the least SSQ increase (that is assigned to minInc by function computeSSQIncrease ). This merging step is repeated until minInc becomes larger than avgDeltaSSQ . Fig. 3 shows the algorithm in ac-tion. After three steps, the initial samples in 3(a) are partitioned according to the grid shown in Fig. 3(b). The algorithm takes se ven more splitting steps producing the parti-tion of Fig. 3(c). The merging phase produces the final five clusters that a human will instinctively recognize at a glance Fig. 3(d).

We point out that producing axis parallel c uts is not a limitation, we can still obtain, in our approach, non parallel cuts however this will not improve the performances of the algorithm. Furthermore, also grid based approaches are tied to parallel cuts since they allow more efficient computation without paying any accuracy loss.

In terms of computational complexity, we see that, in order to split, we have to com-pute the SSQ for each dimension and for each splitting point. Thus, each split has a complexity O ( n  X  d  X  l ) and we perform s splits. The bottom-up step contributes to the overall complexity with a term O ( k 2 ) where k is the number of clusters, since for each cluster we have to consider all the possibl y adjacent clusters for merging; but since k&lt;&lt;n we can disregard this term. Thus the complexity is as follows: Proposition 1. Algorithm CLUBS works in O ( n  X  d  X  l  X  s ) where n is the number of points, d is the number of dimensions, l is the number of splitting positions for each dimension and s is the number of splits. An extensive set of experiments was executed to evaluate the performance of CLUBS. In particular, we compared our method with BIRCH [14], K-means++ [2](we refer to it as KM++) and k*-means [4] (we refer to it as SMART) and OPTICS [1].

Our test suite encompasses a large numbe r of widely used benchmarks over a wide spectrum of different characteristics. Du e to space limitations we can present here a small subset of the results, so we choose the really interesting results we obtained on a severe test bench, i.e. microarray data. We used two publicly available dataset on Gene Expression Omnibus Database: a dataset provided by [8], Dataset 1 hereafter, and a dataset provided by [5], Dataset 2 hereafter.

As regards Dataset 1 , authors examined 42 patients by using Affymetrix HU133A (Affymetrix, Santa Clara CA) microarrays. Patients were subdivided in three groups. Women at usual breast cancer risk undergoing mammoplasty reduction (RM) , women with breast cancer undergoing surgery for either an ER+ or ER-breast tumor (HN), and high-risk patients, consisting of women undergoing prophylactic mastectomy (PM). Dataset providers selected 98 differentially expressed genes in HN w.r.t. RM and they built a matrix of these genes for all three groups. The resulting dataset was analyzed by clustering in order to catch the difference among three groups.

Dataset 2 comprises samples extracted from human breast cancer cells analyzed using the Affymetrix U133A 2.0 gene chips (Affymetrix, Santa Clara, CA). Dataset provider considered 4 group of cells treated with 20 lh/ml of actein at 6 and 24 hours, and cells treated with 40 lg/ml of actein at 6 and 24 hours in order to elucidate the effect of actein. The initial preprocessing was p erformed using the GCRMA method. The sta-tistical significance of differential expressi on with respect to the same reference value was calculated using the empirical Bayesian LIMMA (LI Model for MicroArrays). We started our analysis considering the se preprocessed datasets on which we used CLUBS and the other clustering algorithms for the sake of comparison. The obtained results are reported in Table 1 where values represent SSQ per dataset and milliseconds.
The results obtained are quite convinci ng both for the accuracy and the execution times where CLUBS offer best performances . In particular our clustering method cor-rectly detected the number of clusters in the data (3 clusters for Dataset 1 and 4 clusters for Dataset 2). Indeed, CLUBS showed a nice f eature when clustering Dataset 1: the HN group contains two subgroups ER+ and ER-, CL UBS during the splitting step identified these two subgroups that have been collapsed in a single cluster after the merging step. To asses, the validity of the approach we e xploited several method-independent quality measure that are reported in the following.
 Quality of Clustering Results. Here we will evaluate the quality of the results CLUBS produces and its reliability. The issue of findi ng method-independent measures for clus-tering results has been the source of much topical discussions, but over time sound measures have emerged that can be used reliably to compare the quality of the results produced by a wide range of clustering algorithms [3]. In particular the following three measures have sound theoretical and practical bases. The Variance Ratio measures the ratio between the average distance between points belonging to different clusters and the average distance between points within the same cluster [3]. The range of variance ratio is [0 ,  X  ) and larger values of variance ratio indicate better clustering quality. The Relative Margin reports the average of the Relative Point Margin defined as the ra-tio between the distance of a given point x to the center of the cluster it belongs to and the distance between x and the closest cluster center different from the cluster x belongs to [3]. The range of relative margin is [0 , 1) , and lower relative margin indi-cates a better clustering. The We a k e s t L i n k measure is defined as the maximal value of weakest link over all pairs of points belonging to the same cluster, divided by the shortest between-cluster distance [3]. The range of values of weakest link is [0 ,  X  ) . Lower values of weakest link represent better clusterings. The results obtained for the above mentioned quality measures are given in Table 2(a): they show that CLUBS out-performs other methods significantly, producing values for Relative Margin &amp; Weakest Link (resp. Variance Ratio) that are significantly lower (larger) than those other meth-ods, i.e. clusters of much better quality. These results also confirm that CLUBS finds the exact number of clusters and the quality of the found cluster is overwhelming w.r.t the other methods.
 Additional Quality Measures. SSQ is a natural and widely used norm of similarity, but a devil X  X  advocate can point out that other clustering algorithms might not measure their effectiveness in terms of SSQ or even the compactness of each cluster around its centroid. Thus, in this section we will m easure the quality of the clusters produced by CLUBS using very different criteria inspired by the nearest subclass classifiers that were previously used in a similar role in [12] and [7].
 A first relevant evaluation measure i n this approach is the error rate of a k -Nearest Neighbor classifier defined by the clustering results. This value provide relevant in-formation about the ability of the clustering method under e valuation to minimize the errors due to incorrect assignment of points to the proper cluster. Indeed, this informa-tion is crucial for biological data analys is. Thus, for each point, we can check whether the dominant class of the k closer elements allows to corr ectly predict the actual class of membership (there is no relationship between the value of k used here and that of k -means). Thus, the total number of points corr ectly classified measures the effectiveness of the clustering at hand. Formally, the error e k ( D ) of a k -NN classifier exploiting a the distance matrix among every pair of points. D can be defined as where N is the total number of points, and  X  k ( i ) is 0 if the predicted class of the i -th point ( x i ) coincides with its actual class, and 1 otherwise. Low values of the e k ( D ) index denote high-quality clusters.

Following [7], we can go deeper in our evaluation by measuring the average number of elements, in a range of k elements (we recall again that we use the expected clus-ter size value), having the same class as the point under consideration. Practically, we define q k as the average percentage of points in the k -neighborhood of a generic point belonging to the same class of that point. Formally: where Cl ( i ) represents the actual class associated with the i -th point in the dataset, n according to the distance used at hand. This va lue will provide a rea lly interesting in-formation, in fact it will measure the purity of the clusters since it take into account the number of points wrongly assigned to a cluster. In principle, a Nearest Neighbor classi-fier exhibits a good performance when q k is high. Furthermore, q k provides a measure of the stability of a Nearest-Neighbor: high values of q k make a k -NN classifier less sen-sitive to increasing values k of neighbors considered. The sensitivity of the clustering can also be measured by considering, for a given group of points x,y,z , the probability that x and y belong to the same class and z belongs to a different class, but z is more similar to x than y is. We denote this probability by  X  ( D ) , estimated as: where  X  D is 1 if D ( i,j ) &lt;D ( i,k ) , and 0 otherwise. This value gives information about the ambiguity in cluster assignments. Here too, low values of ( D ) denote a good performance of the clustering under consideration.

The results in Table 2(b) show that CLUBS produces better results than the other algorithms. Table 2(b) shows that CLUBS offers the best performance on all indices and in particular the really high values of q k (it is practically 1 since it detects exactly the number of clusters for each dataset and th e point assignment to cluster is correct) allow to asses that the clusters are well defined, and CLUBS outperforms both BIRCH and OPTICS. In measuring e k and q k , we used neighborhoods of size 10 (this value is the actual cluster size available by datasets p rovider). The overall structure of the clus-ters and the points distribution for Dataset 1 (results in Table 2(b)) produced superior performance for CLUBS on every index, with particularly low values of  X  . This result is confirmed also for Dataset 2 and suggests that CLUBS exhibits the highest effectiveness compared to the other a pproaches even when SSQ is not the chosen metric. The naturalness of the hierarchical appro ach for clustering objects is widely recog-nized, and also supported by psychological studies of children X  X  cognitive behaviors 1 . CLUBS is providing the analytical and algorithmic advances that have turned this in-tuitive approach into a data mining method of superior accuracy, robustness and speed. The speed achieved by our approach is largely due to CLUBS X  ability of exploiting the analytical properties of its quadratic distance functions to simplify the computation. We conjecture that similar benefits might be at hand for situations where the samples are in data streams or in secondary store. These situations were not studied in this paper, but represent a promising topic for future research.

