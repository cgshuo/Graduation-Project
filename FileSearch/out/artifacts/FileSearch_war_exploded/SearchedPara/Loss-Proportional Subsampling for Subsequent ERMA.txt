 Paul Mineiro pmineiro@microsoft.com Nikos Karampatziakis nikosk@microsoft.com Data volumes are growing at a faster rate than avail-able computing power, storage space, or network band-width. This has fueled interest in distributed ap-proaches to machine learning. However, the substan-tial jump in communication costs between a multicore and a multicomputer system currently confines many popular techniques to the single computer regime. Consequently, even those machine learning workflows that today originate with distributed data in a clus-tered environment often terminate with the learning problem being solved on a single machine. Further-more, because of computation, storage, or network limitations there is often a subsampling step between the data store and the single machine.
 Our concern here is to make the subsampling step as statistically efficient as possible, knowing that the data will subsequently be given to an empirical risk mini-mization (ERM) algorithm. What properties can we hope to have from a subsample of the original data? If we were to perform ERM on the original data, the excess risk would be O (1 / the original data set. If we uniformly subsampled the original data set to size m and performed ERM on this subsample, the excess risk would be O (1 / the perspective of ERM this compression is lossy. Ide-ally, we would like to reduce the data set size while still retaining excess risk O (1 / At first blush, this appears to be the active learning scenario. However, the subsampling strategy has ac-cess to all examples and labels but cannot assume knowledge of the hypothesis set ultimately used for ERM. This is because the types of hypotheses ulti-mately considered are presumed to be intractable on the full dataset. Instead we will only assume access to a subset of the (otherwise unknown) hypothesis set. As an example, suppose that the training on the subsam-ple will be via neural networks with an unknown ar-chitecture, but known to have direct connections from the input layer to the output layer. In this case, the set of linear predictors is a subset of the final hypoth-esis space, and linear learning is feasible at terafea-ture scale (Agarwal et al., 2011). Our results show we can compress the data set by encoding relative to the mistakes of a linear predictor, without distorting the subsequent ERM over neural networks; the amount of compression possible is limited by the quality of the linear predictor.
 In effect many current workflows look like 1) a sub-sampling step followed by 2) a model selection step. We propose to replace the first step by 1a) a simpler model selection step, followed by 1b) a subsampling step. Although our approach does apply recursively, in practice we believe much of the benefit would be captured by the introduction of a single simple model selection step prior to subsampling. 1.1. Relation to Prior Work This work bears strong resemblance to multiple threads of research, and the main contribution is inter-preting previous understanding and practices through the lens of empirical Bernstein bounds (Maurer &amp; Pon-til, 2009).
 Boosting algorithms have popularized the idea of se-quential model selection via importance weighting. Of particular relevance is FilterBoost (Bradley &amp; Schapire, 2008), which leverages the correspondence between importance-weighting and rejection sampling in the large data setting: the large data set is iter-atively subjected to a weak learner on a manageable subsample. The scheme herein is akin to a degenerate two-stage version of FilterBoost, but the differences are important. Theoretically, the second stage of our procedure is on an unknown superset of hypotheses. This mandates enforcing a minimum sampling proba-bility related to the quality of the initial model. Prac-tically, in typical workflows there really is a large fixed dataset from which a single subsample is extracted and subsequently used for (multiple!) model selection ex-periments. This commonplace scenario motivates the study of this particular two-stage procedure.
 The architecture of cascading classifiers with increas-ing computational complexity to achieve an efficient ensemble was popularized by Viola and Jones (Vi-ola &amp; Jones, 2001). More recently, the FCBoost al-gorithm (Saberian &amp; Vasconcelos, 2010) was intro-duced, which implements fully automatic cascade de-sign within a boosting framework. While similar ar-chitecturally, cascades focus on short-circuiting a se-quential chain of classifiers in order to minimize eval-uation time complexity, whereas the primary concern here is reduction of training set size. This is necessi-tated by the superlinear scaling in training time com-plexity of many popular methods such as decision trees and Gaussian processes. Nonetheless the procedure outlined herein can be considered a simple two-stage cascade, leading to the same important differences as highlighted in the previous paragraph.
 Active learning is concerned with achieving good gen-eralization while limiting the number of labels revealed to the learner, and our results here are clearly re-lated. In particular our subsampling rate is lower-bounded similarly to worst-case label complexity re-sults (Beygelzimer et al., 2008). It is tempting to conclude that more sophisticated active learning ap-proaches would achieve lower subsampling rates, but again the fact that the second stage of our procedure is on an unknown superset of hypotheses is impor-tant. In particular, since disagreement regions (Han-neke, 2007) only increase on the hypothesis superset, for our particular scenario an active learning algorithm which attempts to exploit what appears to be an iso-lated empirical minimizer is subject to poor worst-case behaviour.
 Interesting connections exist between this work and research directions in sample compression and Mini-mum Description Length (MDL). Sample compression algorithms (Floyd &amp; Warmuth, 1995) learn classifiers that can be described by only a small fraction of the training data. Algorithms based on the MDL principle (Gr  X unwald, 2007) treat training examples and models as data that need to be transmitted to a receiver and they operate by conceptually finding a code that min-imizes communication costs. However, both sample compression and MDL are aware of the subsequent steps in the protocol they are operating. In sample compression the reconstruction function has to match the compression function and in MDL the code used to communicate has to be known to the receiver. In our work, the subsampling step is oblivious to the sub-sequent ERM step which provides great flexibility in practical applications.
 Subsampling to mitigate computational constraints during training is an old idea and a common practice in the machine learning community. The exact setup considered here was investigated empirically almost 2 decades ago (Lewis &amp; Catlett, 1994), where a simpler computationally inexpensive hypothesis was used to select importance-weighted training data for a more expressive computationally intensive hypothesis. More recent work from neural language modeling (Bengio &amp; Senecal, 2008) indicates the issue of controlling worst-case subsample deviations remains open. In that work, the authors address the issue by adapting the com-pressing hypothesis to match the empirical minimizer (both of which are learned online). The approach is this paper is much simpler: we enforce a minimum sampling probability to upper-bound worst-case em-pirical variance. 1.2. Contributions We define an optimal subsample selection problem given a compressing hypothesis using empirical Bern-stein bounds. The solution is a simple strategy param-eterized by the overall subsample budget. We prove a bound on deviations between the subsample empiri-cal risk minimizer and the true risk minimizer which compares favorably to ERM on the original sample. We demonstrate the effectiveness by achieving compet-itive results on a large public dataset for which naive subsampling techniques are not an effective strategy. Our starting point is the old practitioner X  X  chestnut: when faced with a binary classification problem with a highly unbalanced label distribution, discard exam-ples associated with the more frequent label until the relative number of examples with each label is about even. Remaining examples, associated with the for-mally more frequent class label, must be importance weighted to retain an unbiased sample. Curiously, for logistic regression this can be done analytically by ad-justing the bias weight after training, and in language modeling this results in large training time speedups without significant degradation of generalization per-formance (Xu et al., 2011).
 Although this practice is widespread and intuitively reasonable, our goal is a satisfactory theoretical expla-nation of the approach which in turn suggests how to improve the technique. Note that since we can lever-age the labels of the entire dataset, our setup does not obviously correspond to active learning. We believe a thorough understanding requires a non-uniform view of the hypothesis space, and in particular, our results leverage empirical Bernstein bounds.
 A key observation is that subsampling the more fre-quent class exactly preserves the empirical 0-1 loss of the best constant hypothesis, because the discarded points have a loss of 0. In fact, if the only purpose of the subsample was to transmit the empirical risk of the best constant hypothesis, all instances associ-ated with the more frequent class could be discarded. However, the subsample will be used for empirical risk minimization. By definition, the empirical minimizer on the subsample will have empirical risk on the sub-sample at least as good as the best constant hypothe-sis. However, the deviation between the empirical risk on the subsample and the true risk might be large. Fortunately, retaining the instances associated with the more frequent class has the effect of bounding the worst-case empirical variance of the loss of the subsam-ple empirical risk minimizer. This, together with an application of empirical Bernstein bounds, indicates that the deviation between subsample risk and true risk is small. 2.1. A Compressing Hypothesis One way to generalize frequent label subsampling is to let the set of initial predictors considered be richer than the constant predictors. For instance, the class labels might be approximately balanced. Yet, when conditioned on a single feature, the class label distri-bution might be somewhat imbalanced. In general, we might be able to easily search at scale over sim-ple hypothesis spaces prior to subsampling for model selection: can we take advantage of this? Consider any hypothesis  X  h which is guaranteed to be in the set of hypotheses for the final ERM step. For now, let us assume the subsampling procedure does not distort the empirical risk of  X  h . Then, we can upper bound the subsample empirical risk of the subsample empirical risk minimizer, which in turn will allow us to bound deviations of the subsample minimizer from the true underlying distribution.
 Formally, consider that we have an i.i.d. empirical sample X = ( X 1 ,...,X n ) of size n . With a slight abuse of notation in what follows, we will consider the loss function fixed and we will not distinguish be-tween a hypothesis h and the induced loss function ` h . Therefore, we do not need to distinguish between fea-tures and labels. Empirical risk minimization on the original sample would be driven by the risk Given h , our subsampling strategy makes condition-ally independent decisions to sample each X i , where Q i  X  X  0 , 1 } is a random variable indicating whether or not X i is included in the subsample, and P i = E [ Q i | X ] is the sampling probability. The final ERM step min-imizes importance-weighted empirical risk on the re-sulting subsample: We want to limit the degradation introduced by sub-sampling, i.e., bound deviations between R X ( h ) and R Q , X ( h ). The empirical Bernstein bound (Maurer &amp; Pontil, 2009) suggests that deviations are driven by the subsample empirical variance It turns out the worst case scenario is when h has high loss on examples where P i is small. For any distribution D and any random variable Z  X  [0 ,w ] we have V D [ Z ]  X  w E D [ Z ]. For the subsample empirical distribution, in particular, V n ( h | Q , X )  X  1 / (min i P i ) R Q , X ( h ). Thus, we can bound the worst-case subsample empirical variance of R Q , X ( h ) by en-forcing a minimum sampling probability P min . If we knew that the subsample empirical minimizer  X  h had subsample empirical risk R Q , X (  X  h ), we could choose P tions introduced by subsampling are of the same order as deviations in the original sample. Unfortunately we cannot choose P min in this fashion as it involves circular reasoning.
 Instead we can leverage the compressing hypothesis  X  h to choose P min . In particular, if the subsampling procedure does not distort the empirical risk of  X  P of  X  h between sample and subsample is critical. For  X  h we can use Bennett X  X  inequality to bound the devia-tion introduced by subsampling via the variance of the subsampling procedure, V = E Q = The above considerations motivate the following for-mulation of optimal subsampling, The KKT conditions reveal P i = max { P min , X   X  h ( X i where  X  depends upon both the variance budget V and the minimum probability P min . Thus we will be sampling at a rate proportional to the instantaneous loss of compressing hypothesis, subject to a minimum sampling rate. 2.2. The Sampling Strategy The above considerations lead to the following. Definition 1 (Sampling Strategy) . Fix a sample X = ( X 1 ,...,X n )  X  X n , let  X  &gt; 0 , let P min &gt; 0 , and let  X  h : X  X  [0 , 1] be any hypothesis. The sampling strategy wrt (  X  h, X ,P min ) is a set of random variables Q = ( Q 1 ,...,Q n ) that defines a subsample of X , where the Q i  X  { 0 , 1 } have conditional independence Q For this strategy we can prove the following.
 Theorem 1. Let X be a random variable with values in set X with distribution D , let X = ( X 1 ,...,X n )  X  D n be an i.i.d. empirical sample of size n , let H be a finite set of hypotheses h : X  X  [0 , 1] , let  X  h  X  X  be any hypothesis with empirical mean R X (  X  h ) , and let Q = ( Q 1 ,...,Q n ) be a set of random variables according to the sampling strategy wrt (  X  h, X ,P min ) . Let h be any hypothesis with minimum true mean, and let  X  h  X  H be any hypothesis with minimum subsampled empirical mean R Q , X ( h ) . For  X  &gt; 0 , n  X  2 we have with probability at least 1  X  3  X  in Q and X , Proof. See the appendix.
 Analogous results are possible for infinite hypothesis classes whose complexity can be suitably controlled. From Theorem 1 it is clear that our scheme cannot subsample at a rate below the average loss of the com-pressing hypothesis without incurring increasing ex-cess risk; this is analogous to a lossless compression rate threshold. However if P min  X  R X (  X  h ) and  X   X  1, then excess risk is O (1 / the original data set size and m is a lower bound on the subsampled data set size.
 In practice P min and  X  are chosen according to the subsample budget, since the expected size of the sub-sample is upper bounded by ( P min +  X R X (  X  h )) n . Unfor-tunately there are two hyperparameters and the analy-sis presented here does not guide the choice except for suggesting the constraints P min  X  R X (  X  h ) and  X   X  1; this is a subject for future investigation.
 For binary classification 0-1 loss, using the best con-stant predictor as the compressing hypothesis, P min = R iar  X  X ubsample instances with the rarer class label in order to make a balanced data set. X  To demonstrate the technique we used the DNA dataset from the 2008 Pascal Large Scale Learning challenge (Sonnenburg, 2008). This dataset consists of 50 million instances of 200 base pair oligonucleotides with associated binary labels corresponding to whether or not the sequence contains a splice site. This is a highly imbalanced data set, with 144,823 positives and 49,855,177 negatives. This dataset is notable because it is a large public data set for which subsampling has not heretofore been an effective learning strategy (Son-nenburg &amp; Franc, 2010; Agarwal et al., 2011). The conventional evaluation metric for this data set is area under the precision-recall curve. AuPRCs of circa 0.2 are typical of  X  X ast X  methods for this dataset, although the best known technique for this dataset achieves an AuPRc of 0.586 on the validation set (Son-nenburg &amp; Franc, 2010). The labels for the validation set for this dataset are not published, and are accessi-ble only via a submission oracle. We took the original published training set and split it into training and test sets by reserving the first 1 million instances as test. Unless otherwise indicated, we utilize our train/test split and the reported metrics are not directly com-parable with other published results. To assess the sensitivity of our results to the exact test set, we use the bootstrap to estimate the dispersion in the AuPRc. We generate bootstrap samples of the test set and com-pute the AuPRc statistic on each bootstrap sample using the same predictor. In what follows, a 90% con-fidence interval refers to the 5 th and 95 th quantile of the distribution of AuPRc values obtained this way. 3.1. Trigram final model For our initial (compressing) model we used logistic regression as implemented in Vowpal Wabbit (Lang-ford, 2011), encoding the nucleotide at each position with a one-hot encoding. This model achieves 0.215 Subsample Training Set Test AuPRc (90% CI) constant 4,829,983 0.472 ([0.454, 0.489]) full data 49,000,000 0.494 ([0.475, 0.511]) test AuPRc.
 To generate a subsample, we used the initial model to subsample the original data set as per definition 1 with P min = R X (  X  h ) and for a range of  X  from 1 to 65536 exponentially spaced; we name this subsampling method linear . For the loss function we used logistic loss, normalized on the training set to be in the range [0 , 1]. We compared this to the well-known and ubiq-uitously applied strategy of taking all the positively labelled instances plus a uniform sample of the neg-atively labelled instances; we name this subsampling method constant .
 For our final model we again used logistic regression but included one-hot encodings of bigrams and tri-grams at each position. Figure 1 shows the results of training a trigram model on the subsample as a func-tion of subsample fraction and subsampling method. Only the training set is subsampled: the complete test set is used every time for evaluation. For the range of subsample fractions roughly between 1% and 10%, the subsample generated via linear results in better test performance; performance at other subsampling rates is essentially equivalent. At a 7% fraction, linear achieves a test AuPRc of 0.491 with 90% confidence interval [0 . 474 , 0 . 506]. This is equivalent to training a trigram logistic regression on the entire data set, which achieves test AuPRc of 0.494 with 90% confidence in-terval [0 . 475 , 0 . 511], as summarized in table 1. The confusion matrix for the initial linear model on the training set provides some intuition regarding the improved efficiency.
 Both linear and constant will have a positively labeled instance enriched subsample, the latter by explicit de-sign, and the former because most true positives have large logistic loss using the initial model. The con-stant model, however, will have a uniform subsample of negatively labeled instances. By contrast, linear
Method Size Test AuPRc (90% CI) constant 873,405 0.480 ([0.462, 0.495]) linear 840,118 0.524 ([0.501, 0.542]) trigram 834,131 0.567 ([0.545, 0.582]) will treat the 3412 false positives similarly to positively labeled instances, and furthermore negatively labelled instances that are near the classification boundary will be more likely to be incorporated into the subsample. This non-uniform view of the negatively labelled data helps prevent overfitting in the subsample. 3.2. GBM final model Next we experimented with the gbm decision tree pack-age (Ridgeway, 2005), with which using the complete dataset is not feasible on a current commodity desktop machine. We used the trigram feature encoding us-ing depth 3 trees, i.e., 3-way interactions between tri-grams. For the initial model we used constant and lin-ear as above, but additionally employed trigram which is the final model from the previous experiment trained on the entire data set. The results are in table 2. gbm utilizing the subsample defined by trigram achieves AuPRc of 0.567 ([0 . 545 , 0 . 582]), which is bet-ter than trigram model trained on the entire dataset. Hence, a more computationally demanding model se-lection step on a subsample can achieve better results than a simpler model selection step utilizing all the data. Furthermore, this is competitive with the best known solutions, despite gbm only having access to less than 2% of the data.
 The difference in training time between linear and tri-gram is quite modest: roughly 60 vs. 75 minutes for the entire data set on a single core of a commodity lap-top. On the same hardware gbm takes roughly 3 days to produce a 10,000 tree ensemble using 800,000 exam-ples. We speculate that for some domains there is a knee in the performance of classifiers relative to com-putational effort, such that reasonable performance can be achieved with modest effort, as with the tri-gram model above. In such cases, using a  X  X weet spot X  model as the compressing hypothesis for a more computationally demanding technique is a productive strategy. We have derived a general technique for subsampling prior to model selection which leverages a compressing hypothesis, proven a deviation bound for subsampled empirical risk minimization which compares favorably to empirical risk minimization on the original sample, and demonstrated the approach experimentally on a large public dataset.
 These results enable the beneficial use of effective but non-scalable learning algorithms on larger datasets. The next two Theorems are from (Maurer &amp; Pontil, 2009), slightly modified to range over [0 ,w ]. Theorem 2 (Bennett X  X  Inequality) . Let Z,Z 1 ,...Z n be i.i.d. random variables with values in [0 ,w ] and let  X  &gt; 0 . With probability at least 1  X   X  in the i.i.d. vector Z = ( Z 1 ,...Z n ) we have where V ( Z ) = E [( Z  X  E [ Z ]) 2 ] is the variance. Theorem 3 (Empirical Bernstein Inequality) . Let Z,Z 1 ,...Z n be i.i.d. random variables with values in [0 ,w ] and let  X  &gt; 0 . With probability at least 1  X   X  in the i.i.d. vector Z = ( Z 1 ,...Z n ) we have where V n ( Z ) = (1 / ( n  X  1)) P n i =1 ( Z i  X  (1 /n ) P is the empirical variance.
 Lemma 1. Fix a sample X = ( X 1 ,...,X n ) , let H be a finite set of hypotheses h : X  X  [0 , 1] , let  X  h  X  X  be any hypothesis with empirical mean R X (  X  h ) , and let Q = ( Q 1 ,...,Q n ) be a set of random variables according to the sampling strategy wrt (  X  h, X ,P min ) . For  X  &gt; 0 we have with probability at least 1  X   X  in Q , Proof. First we bound the variance due to sampling, Applying Bennett X  X  inequality using range [0 , 1 /P min ] yields the desired result.
 Lemma 2. Fix a sample X = ( X 1 ,...,X n ) , let H be a finite set of hypotheses h : X  X  [0 , 1] , let  X  h  X  H be any hypothesis with empirical mean R X (  X  h ) , and let Q = ( Q 1 ,...,Q n ) be a set of random variables ac-cording to the sampling strategy wrt (  X  h, X ,P min ) . Let  X  h  X  X  be any hypothesis with minimum subsample em-pirical mean R Q , X (  X  h ) . For  X  &gt; 0 , n  X  2 we have with probability at least 1  X  2  X  in Q , 1 n
X Proof. First we bound the empirical subsample vari-ance, V =  X   X   X  , where the first inequality is due to P i  X  P min , the second due to optimality of  X  h on the filtered sample, and the third due to the previous lemma. Applying empirical Bernstein and the concavity of square root yields The desired result follows from upper-bounding con-stants by 10 / 3.
 Lemma 3. Fix a sample X = ( X 1 ,...,X n ) , let H be a finite set of hypotheses h : X  X  [0 , 1] , let  X  h  X  X  be any hypothesis with empirical mean R X (  X  h ) , and let Q = ( Q 1 ,...,Q n ) be a set of random variables according to the sampling strategy wrt (  X  h, X ,P min ) . Let h  X   X  H be any hypothesis with minimum true mean. For  X  &gt; 0 , n  X  2 we have with probability at least 1  X  2  X  in Q , Proof. First we bound the variance due to sampling,
V Q ( h  X  | X )) where the first inequality is due to P i  X  P min and the second due to h  X  ( X i )  X  [0 , 1]. The true optimality of h  X  and Hoeffding X  X  inequality imply therefore V
Q ( h  X  | X )  X  Next applying Bennett X  X  inequality and the concavity of square root yields 1 n
X  X   X  Proof of Theorem 1. Combining the two previous lem-mas with the empirical filtered optimality of  X  h yields Applying Hoeffding X  X  inequality twice yields the de-sired result.
 Agarwal, Alekh, Chapelle, Olivier, Dud  X  X k, Miroslav, and Langford, John. A reliable effective terascale linear learning system. CoRR , abs/1110.4198, 2011. Bengio, Y. and Senecal, J. S. Adaptive importance sampling to accelerate training of a neural proba-bilistic language model. Trans. Neur. Netw. , 19(4): 713 X 722, April 2008. ISSN 1045-9227.
 Beygelzimer, Alina, Dasgupta, Sanjoy, and Langford,
John. Importance weighted active learning. CoRR , abs/0812.4952, 2008.
 Bradley, Joseph K and Schapire, Robert. Filterboost: Regression and classification on large datasets. In
Platt, J.C., Koller, D., Singer, Y., and Roweis, S. (eds.), Advances in Neural Information Processing Systems 20 , pp. 185 X 192. MIT Press, Cambridge, MA, 2008.
 Floyd, S. and Warmuth, M. Sample compression, learnability, and the Vapnik-Chervonenkis dimen-sion. Machine Learning , 21(3):269 X 304, 1995.
 Gr  X unwald, P.D. The minimum description length prin-ciple . MIT press, 2007.
 Hanneke, S. A bound on the label complexity of ag-nostic active learning. In Proceedings of the 24th international conference on Machine learning , pp. 353 X 360. ACM, 2007.
 Langford, John. Vowpal wabbit, 2011. URL https:// github.com/JohnLangford/vowpal_wabbit/wiki .
 Lewis, David D. and Catlett, Jason. Heterogeneous uncertainty sampling for supervised learning. In
In Proceedings of the Eleventh International Con-ference on Machine Learning , pp. 148 X 156. Morgan Kaufmann, 1994.
 Maurer, Andreas and Pontil, Massimiliano. Empirical Bernstein bounds and sample-variance penalization. In The 22nd Conference on Learning Theory , 2009. Ridgeway, Greg. Generalized boosted models: A guide to the gbm package, 2005.
 Saberian, Mohammad and Vasconcelos, Nuno. Boost-ing classifier cascades. In Lafferty, J., Williams, C. K. I., Shawe-Taylor, J., Zemel, R.S., and Culotta,
A. (eds.), Advances in Neural Information Process-ing Systems 23 , pp. 2047 X 2055. 2010.
 Sonnenburg, S  X oren. Pascal large scale learning challenge, 2008. URL http://largescale.ml. tu-berlin.de/about/ .
 Sonnenburg, S  X oren and Franc, Vojtech. Coffin : A computational framework for linear svms. In Proc. ICML 2010 , 2010.
 Viola, Paul and Jones, Michael. Rapid object de-tection using a boosted cascade of simple features, 2001.
 Xu, Puyang, Gunawardana, Asela, and Khudanpur,
Sanjeev. Efficient subsampling for training complex language models. In Proceedings of the Conference on Empirical Methods in Natural Language Process-ing , EMNLP  X 11, pp. 1128 X 1136, 2011. ISBN 978-1-
