 The paper describes a user oriented performance evaluation measure for text segmentation. Experiments show that the proposed measure differentiates well between error distribu-tions with varying user impact.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithms, Experimentation Story Segmentation
Presentation of results in search engines that index audio or video broadcast news requires that the broadcast be au-tomatically divided into short segments. Preferably, these short segments should be topically coherent and aligned with the user X  X  idea of news  X  X tories. X  Evaluating the accuracy of the text segmentation component thus requires a metric that reflects the manner in which the content is presented to the user. We emphasize that the user seeks the contents of the story, not the boundaries between stories. Story boundary errors prevent the user from accessing the content of the story, or expose the user to content from the wrong story.
The aim of work described in this paper is to find segmen-tation performance measure to be used in designing segmen-tation algorithms applied in an integrated system for pro-cessing of broadcast news. The desired properties of the measure include: reflecting end user behavior and interac-tion with the presented material in an information retrieval system, measuring the segmentation quality in user-oriented units (words or tokens,) and avoiding data-specific parame-ters.
 Figure 1: Misplaced story boundaries cause spurious content presented to a user.
 Most currently used story segmentation measures, such as P k [2], TDT C seg [3] and WindowDiff [4]arebasedoncount-ing the number of incorrectly proposed boundaries. The count is smoothed by a sliding window scheme to assign partial credit to system boundaries at incorrect positions that are close to the reference boundaries. There are sev-eral shortcomings to these approaches. First, the user is primarily interested in the content of the stories, not the boundaries between the stories. Miss and false alarm rates of boundaries do not clearly measure what content is missed by the user, or incorrectly presented to the user. Second, the width of the sliding window is an arbitrary parameter, and whether it should be a constant or a constant fraction of a typical story length is unclear. Thus results over long projects with evolving data sources are difficult to compare.
In the next section, we propose a segmentation metric that directly measures the content that, due to incorrect story boundaries, is missed by the user, or incorrectly presented to the user.
When a user is presented a story with incorrect bound-aries, there are two types of errors. First, the user misses some of the story because either the system-proposed start of the story was too late or the system-proposed story end came too soon. Second, the user is presented with some content from a different story ( false-alarms ) if the system-proposed story starts too soon or ends too late. Our mea-sure of segmenter performance is computed by averaging the number of words of missed content and the number of words of false alarm content over all word positions in the corpus: where the Miss ( w )and FA ( w ) are numbers of missed and false alarm words, respectively, at a given word position and N is the total number of word positions in the test set. R Miss and R FA are measured in words, which means that the values indicate an average number of missed and spurious words presented t oauserretrievingastory.
The simplest assumption in choosing this metric rather than one of several possible generalizations is that all word positions in the corpus are equally likely to be the target of the user, and that all word positions are equally valuable to the user. Depending upon the application, the metric could be generalized by 1) by assigning idf-like weights to individ-ual dictionary words in the dictionary, or by decreasing the weight of repeating words, 2) the per word (micro) averaging canbereplacedorcombinedwith per story length fraction (macro) averaging, 3) importance-based weights can be as-signed to leading/trailing missed/FA content. Also, recall and false alarm content could be measured by time, rather than number of words. In this paper we validate the simplest measure of this family rather than exploring all possibilities.
To show that the proposed measure is sensitive to degra-dation that directly impacts user experience, we measure the performance of a segmentation system with introduced noise. Our system is a maximum entropy based segmenter, trained to estimate the segmentation probability at utter-ance boundaries based on a variety of features, an approach similar to systems described in [1]. The training and test data are extracted from the PRI subset of the TDT-4 cor-pus [5]. The training set contains the chronologically first 44 shows (1157 stories); the test set contains the chronolog-ically last 18 shows (474 stories).

We introduce noise into the system by swapping the system-calculated probabilities of a story boundary at a word posi-tion at 200 pairs of utterance boundaries. Swapping proba-bilities ensures that the num ber of boundaries produced by a noisy system is identical to the number produced by the baseline (no noise) system even as a threshold is varied to probe the false alarm/miss tradeoff. We choose the swaps from two distributions. In the first case, the swap points are distributed uniformly in the corpus -thus stories are likely to be broken in the middle. In the second case, all the swaps points are located within ten utterances of a true story boundary, effectively moving story boundaries a small distance. The second case is believed to be less disruptive to the user experience.

The Miss/FA curves obtained using a window-based mea-sure as described in [3] and the proposed measure are shown on Figures 2 and 3, respectively. The square markers cor-respond to  X  X ractical X  operating points, where the average length of produced stories matches the test data. For com-parison purposes, a naive baseline of equally spaced seg-ments (of length matching the average story length) yields R
Miss = 548 and R FA = 92. We observe that near the  X  X ractical X  operating points, the window based measure does not clearly distinguish between the two types of noise; whereas the more user-disruptive uniform noise clearly impacts the proposed measure more than the near-boundary noise.
We have introduced a new segmentation measure that di-rectly measures whether appropriate content is presented to the user, and have shown that this measure is sensitive to particular types of errors th at directly impact the user. This work was partially supported by the Defense Advanced Research Projects Agency under contract No. HR0011-06-2-0001. The vie ws and finding s contained in this material are those of the authors and do not neces-sarily reflect the position or policy of the U.S. government and no official endorsement should be inferred. [1] J. Allan, editor. Topic Detection and Tracking: [2] D. Beeferman, A. Berger, and J. D. Lafferty. Statistical [3] G. Doddington. The topic detection and tracking phase [4] L. Pevzner and M. A. Hearst. A critique and [5] S. Strassel and M. Glenn. Creating the annotated
