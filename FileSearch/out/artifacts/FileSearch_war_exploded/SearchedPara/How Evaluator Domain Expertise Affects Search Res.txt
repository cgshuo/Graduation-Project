 Traditional search evaluation approaches have often relied on domain experts to evaluate results for each query. Un-fortunately, the range of topics present in any representa-tive sample of web queries makes it impractical to have ex-pert evaluators for every topic. In this paper, we investigate the effect of using  X  X eneralist X  evaluators instead of experts in the domain of queries being evaluated. Empirically, we find that for queries drawn from domains requiring high ex-pertise, (1) generalists tend to give shallow, inaccurate rat-ings as compared to experts. (2) Further experiments show that generalists disagree on the underlying meaning of these queries significantly more often than experts, and often ap-pear to  X  X ive up X  and fall back on surface features such as keyword matching. (3) Finally, by estimating the percentage of  X  X xpertise requiring X  queries in a web query sample, we estimate the impact of using generalists, versus the ideal of having domain experts for every X  X xpertise requiring X  X uery. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Relevancefeedback Experimentation, Human Factors, Measurement, Reliability search evaluation, domain expertise, information retrieval
IR systems are generally evaluated by gathering relevance judgments for the results of some sample of queries. To ob-tain a statistically meaningful measure of a system X  X  overall quality, it is useful to evaluate a representative sample of queries issued to the system. Since modern internet search engines process millions of queries a day, on every imagin-able topic, any representative sample will also contain an extremely wide range of topics. While past IR evaluations have often relied on domain experts to evaluate the relevance of results (e.g., [4, 5]), such an approach is simply not prac-tical for web search engines; the range of domain expertise required would be too large. A more practical approach is to ask general-purpose evaluators to evaluate the relevance of query results, even when they do not personally have deep domain expertise in each query X  X  topic.

In this paper, we explore the relationship between domain expertise and relevance judgments. That is, what is the im-pact of using evaluators who are not domain experts on the queries they are evaluating? Do  X  X xpert X  evaluators rate re-sults differently from general evaluators, and if so, are the differences systematic? Will using a general pool of evalua-tors give us a skewed measurement of the quality of a web search engine, and if so, how badly skewed?
To explore these questions, we report on a series of ex-periments comparing expert and general evaluators across two domains: computer-programming queries and biomed-ical queries. We use graded relevance judgments that can be combined using a summary quality metric such as the Discounted Cumulated Gain(DCG) [7].

Our experiments show that generalist evaluators X  ratings of search results are often inaccurate when compared with ratings from domain experts. Generalists sometimes rate documents as more relevant, and sometimes less relevant than experts. We explain the directionality by examin-ing the prevalence of keyword matches between queries and rated documents. We find that ge neralists X  rating errors are often caused by rating based on the presence or absence of keyword hits, rather than by how well the document matches the query X  X  underlying intent.

By separately asking evaluators to explicitly describe the intent of each query, we find that experts display a signifi-cantly higher level of cross-evaluator agreement about query intent than generalists. The implication is that (not surpris-ingly) the experts better understand the true meanings of the queries. Among generalists, we find a high incidence of simplistic or under-specified query intent statements  X  that is, generalists often appear to simply give up on trying to really understand the query.

Presumably, this lack of query understanding is a key cause of the relevance judgment differences we see between generalists and experts. That is, when generalists do not understand the true intent of a query, they fall back on more simplistic ways of judging relevance, such as keyword matches. To test this theory, we provided generalists with expert-written descriptions of each query X  X  intent along with the query. Their ratings improved significantly.

Finally, in order to understand the impact of using gen-eralist evaluators on overall quality measurements, we pro-vide a rough estimate of the percentage of a random web querystream that requires expert knowledge to be rated ac-curately. Combining this estimate and the rating deltas we saw in our domain-specific experiments, we generate a rough  X  X orst-case X  estimate of the potential impact on a simple nDCG-style summary metric.

The rest of the paper is organized as follows. Section 2 de-scribes previous work related to domain knowledge in search engine evaluation. In Section 3, we examine ratings given by expert and generalist raters for two domains, and explore the differences between the two groups X  ratings. Section 4 describes experiments in which experts and generalists were asked to write intent statements for queries, and generalists were given the experts X  intent statements in an attempt to improve their rating quality. Section 5 examines the impact of using generalist raters to evaluate a web search engine for a representative sample of queries. Section 6 discusses the overall findings and implications for web search evalua-tion, and Section 7 concludes and suggests areas for further research.
While the impact of domain knowledge on search engine evaluation per se has not been previously studied, several re-searchers have studied the impact of domain knowledge on search behavior and search effectiveness [1, 2, 3, 6, 8, 9, 10, 11]. Jenkens, Corritore, and Wiedenbeck [8] assigned two search tasks related to finding information about osteoporo-sis to a group of nurses categorized according to how much experience they have with using the internet and how much expertise they have in osteoporosis. They found that nurses with no osteoporosis expertise were more likely to be satis-fied by the first relevant documents they found, while nurses with osteoporosis expertise were more demanding when eval-uating the relevance of documents found in their searches.
H  X  olscher and Strube [6] also investigated the impact of internet expertise and domain expertise on web search be-havior. Subjects were given search tasks related to eco-nomics, and several variables such as task success rate and time data were recorded for each session. The study found that searchers with less domain knowledge are less flexible in search behavior and tend to backtrack through their browser history when unsuccessful instead of conducting a deeper search of web sites for relevant information.

Zhang, Anghelescu, and Yuan [11] compared the search behavior and search effectiveness of a group of engineering and science students. The students were asked to rate their familiarity with 200 thesaurus terms in order to measure their level of domain knowledge on the topic of thermo-dynamics. They found that as the level of domain knowl-edge increases, the participants were more likely to do more searches and use more terms in their queries. However, a link between search effectiveness (measured by running the par-ticipants X  top choices of relevant documents found in their searches) and domain knowledge was not found.
To explore differences between relevance judgments given by domain experts and generalists, we performed experi-ments in the computer-programming and biomedical do-mains. For each domain, a group of experts in that do-main was assembled, and a query set was sampled from anonymized query logs. The computer programming experts all held college degrees in computer science. The biomedical experts all held advanced degrees in some field related to the biosciences and/or medicine. The general group of raters all held college degrees, but were not screened for degrees in any particular field.

The computer programming queries were chosen by first randomly sampling queries from Google X  X  2007 English US query stream that contained programming terms such as  X  X ++ X  and  X  X ava X . We then excluded any queries seek-ing basic information such as [java tutorial]. The biomed-ical queries were hand-selected from a large list of random queries. Only queries that were specific to biomedical re-search were selected. The programming query set contained 110querie sandthebiomedica lquerysetcontained200queries.
We submitted the queries in the programming and biomed-ical query sets to Google, and the first five search results for each query were submitted to the evaluators for relevance rating. The evaluators were gi ven basic instructions on the relevance levels in the graduated scale in an effort to make their ratings more consistent. The rating task was not timed. The computer-programming results were evaluated by the computer programming experts and the biomedical results were evaluated by the biomedical experts. The generalist group evaluated both the computer-programming and the biomedical results. Five raters from each group rated each assigned query-result pair on a graduated scale; their ratings were averaged and normalized to a score between 0 and 1.
We define relevance of a query, Relevance[q] ,asasim-ple position-weighted mean (position meaning whether the result was 1st, 2nd, 3rd, 4th or 5th for the query), weighted by 1/position. Calling the relevance judgment for position This is a simple cumulative discounted gain measure [7]. We define the average relevance over a query set Q as:
Figure 1 and Figure 2 show the Relevance scores calcu-lated from the experts and the generalists for the program-ming and biomedical queries, respectively. The dashed line marks the points for which the expert and general Rele-vance scores would be equal. AR[programming set]=0.67 for the expert raters and 0.74 for the generalists. In con-trast, AR[biomedical set]=0.47 for the experts and 0.39 for the generalists. While the difference between the experts and generalists was statistically significant in both experi-ments, they are in opposite directions. We investigate the reason for this difference in the next section. Figure 1: Relevance scores for programming queries
Figure 2: Relevance scores for biomedical queries
In the programming query experiment, the generalists gave higher relevance ratings, but in the biomedical query experiment, the generalists gave lower relevance ratings. To understand why, consider some examples where generalists andexpertsdifferedthemost.

The query [javascript  X  X ontstyle X  underline] from the pro-gramming experiment returns a result that shows a question posted to a VB .NET forum. Figure 3 shows a screen shot of the document. The generalists gave this document an aver-age relevance rating of 0.5 out of a possible maximum score of 1.0. The experts gave the document a very low average rating of 0.15. In fact, the document is about changing the font size in VB, and has nothing to do with javascript. The generalists, not being familiar with programming, likely saw the  X  X ontStyle.Underline X  parameter in the forum question and concluded that the document must be relevant. Figure 3: Example result from programming query [javascript  X  X ontstyle X  underline]
Another example from the computer-programming exper-iment shows the same pattern. The query is [javascript tab key press] and the result is a page discussing the accessi-bility features of the page X  X  host. While this page is only slightly relevant to the query, there are some keyword hits (as seen in Figure 4). The generalists rated this page 0.45, while the experts rated it as only 0.15. These examples sug-gest that the generalists gave higher scores than the experts because they interpreted keyword hits as indicative of rele-vance. The experts had sufficient knowledge to be able to make a deeper, more informed judgment. Also note that in the first example, not all of the keywords were present. However, two of three were sufficient for the generalists to (erroneously) conclude that the document was relevant.
Now consider the query [reptilia respiratory -monophyletic bronchial branched] from the biomedical experiment. One of this query X  X  results shows an abstract and citation for pa-per on the evolution of respiratory systems across animal groups (Figure 5 shows a screenshot of the page). The ex-perts realized that this paper is closely related to the query and rated it 0.7. The generalists only rated it 0.2. Note that the document doesn X  X  contain many of the keywords in the query. One must understand what the query means in order to realize this document is relevant.

Another example from the biomedical experiment shows a similar phenomenon. The query is [the dominance of the haploid condition in fungal for adaptive advantage] and one of its results has the full text of a paper closely related to the query (Figure 6 shows a screenshot of the document). The experts rated this document 0.7 and the generalists rated it 0.3. In this case the connection is clearer, and some of the generalists realized that the document was relevant. How-ever, some of the keywords are buried far down on the page, and some are not present at all. Figure 4: Example result from programming query [javascript tab key press]
These examples suggest that the difference in the out-comes for the computer programming and biomedical exper-iments lies in the presence and/or prominence of query key-words in a document. Presumably, when generalists don X  X  understand the intent of the queries as well as the experts, they rely on keyword hits to assess relevance. Figure 7 presents a simple model of the two types of rating errors that result. We expect relevant documents without strong keyword ties to be underrated by generalists. Conversely, we expect irrelevant documents with strong, but superficial, keyword ties to be overrated by generalists.

In order to test this theory, we classified cases with large differences (greater than 0.25) between generalist and ex-pert ratings, according to w hether the query X  X  keywords were prominent in the document being rated. For the pro-gramming experiment, there were 14 such documents that the generalists overrated; 79% of them had prominent query keywords. There were 4 documents that the generalists un-derrated; only one (25%) had prominent query keywords.
In the biomedical experiment, 76 documents had an aver-age rating difference of greater than 0.25. There were only 5 documents in the biomedical experiment that generalists overrated by greater than 0.25. Of these, 80% had promi-nent query keywords. There were 71 documents the general-ists underrated; of these, we classified the top 20. 30% had prominent query keywords.

Figure 8 shows the summary re sults, pooled across both experiments. As predicted by our simple model, generalists were much more likely to overestimate the relevance of a document when the query keywords were prominent on the page. Conversely, generalists were likely to underestimate the relevance of a document when the query keywords were not prominently displayed.

Is this reliance on keywords due to generalists not under-standing the intent of the queries? In the next section we explore the extent to which generalists understand expert queries and whether providing generalists with additional information can improve their ratings.
 Figure 5: Example result from biomedical query [reptilia respiratory -monophyletic bronchial branched]
A likely explanation for generalists X  rating inaccuracies is that domain experts better understand queries in their domain. That is, they understand the true intent, or in-formation need, of each query, and can thus evaluate more accurately how well a given result meets that intent, as op-posed to simply looking at keyword matches. In this section we explore this hypothesis.

Unfortunately, a description of the intention of the origi-nal user issuing a query is not available for a sample drawn from search engine logs. This means that in order to ex-plore differences in understanding query intent between do-main expert and generalist evaluators, we can X  X  simply ask each evaluator what they think each query means, and then compute their accuracy against the  X  X rue X  original user X  X  in-tentions.

As a proxy for true accuracy, we looked at cross-evaluator agreement on query intent. That is, if multiple evaluators express essentially the same intent for a given query, we assume they must be  X  X ight X . Within each group of evalu-ators (domain experts and generalists), we take the degree of cross-evaluator agreement on query intents as a proxy for how accurately that group of evaluators understands the true intent of the queries.

We asked three expert and three generalist evaluators to write intent statements for each query in the computer pro-gramming queryset. A few examples of these statements are shown in Table 1. We then showed five additional evaluators the query and the three intent statements, and asked them to judge whether the statements express the same, similar, or different intents. Finally, we computed agreement rates as follows: If four or more evaluators rated the intents as the same or similar, that query is counted as an  X  X greement X ; if the evaluators split 3-2 or 2-3 as to whether the intents are similar or different, the query is  X  X nclear X ; otherwise, the query is counted as a  X  X isagreement X .

Agreement rates for the expert and generalist evaluator groups on the programming queryset are shown in Figure 9. While domain experts X  intent statements agreed for 58% of
Query Intent statement expert evaluators Figure 6: Example result from biomedical query [the dominance of the haploid condition in fungal for adaptive advantage] queries and disagreed on only 13%, generalists X  agreement rate was only 40%, with disagreement on 22%. These differ-ences are fairly significant even for this small query sample (agreement rate comparison, p-value = 0.008; disagreement rate comparison, p-value =0.068).

In order to better understand the type and severity of disagreements, we hand-coded the disagreements into sev-eral categories. Our top-level categorization split the dis-agreements into hard queries, and evaluator errors. Hard queries are cases in which we felt it was justifiably difficult for the evaluators to generate an accurate intent statement for the query. Evaluator error queries are cases where one or more evaluators clearly misin terpreted the query (including under-interpreting or  X  X iving up X  on finding an intent). 1
In a similar previous study using more general queries, we found additional disagreement types, such as truly ambigu-ous queries, overly vague queries, and queries for which eval-uators disagreed about whether the query intent was infor-mational vs. transactional. These types of disagreements didn X  X  occur for the computer science queryset; none of the queries are ambiguous or vague, and they are all essentially informational queries.
 Prominent Keywords Absent/Obscured Keywords Figure 7: A simple model of generalist rating be-haviors Prominent Keywords Absent/Obscured Keywords Figure 8: Queries with large rating differences be-tween experts and generalists, classified by query keyword prominence in the rated document. Figure 9: Generalist and domain expert evaluators X  query intent agreement rates for computer program-ming queries Figure 10: Categories of intent-disagreement queries for generalist and expert evaluators on computer sci-ence queries
Figure 10 shows the categorization of disagreement queries for generalists and domain experts. The generalists had a higher rate of evaluator errors , not surprisingly, and also disagreed on more hard-to-inte rpret queries than experts. We further sub-categorized the disagreements as shown in Figure 11. Both experts and generalists had disagreements on about 10% of queries due to one of the evaluator intent statements simply being a poor interpretation of the query. This may be a reflection of the complexity and breadth of these queries: they cover multiple programming languages and topics, so even our expert evaluators are likely not be as familiar with some areas.

Generalists exhibited the  X  X verly generic interpretation X  subcategory significantly more often than experts. Here, generalist evaluators seemed to essentially  X  X ive up X  on gen-erating an intention that actually matches the query, and in-stead provided a generic interpretation with no depth. A few examples are shown in Table 4. This tendency matches what was observed in the result evaluation experiments: when faced with a complex domain-specific query, some evalua-tors appear to give up on real understanding, and fall back on shallow, generic query intent, and keyword matches. Figure 11: Sub-categories of intent-disagreement queries on computer science queries
Query Intent statement javascript strings with quotes java format integer leading zeros python string escape find info/tutorial on spe- X  X write /etc/profile.d/java.sh X  Table 2: Generalist evaluators tend to provide overly generic query intent statements for complex queries.

Generalists also could not come to agreement on several more  X  X ard X  queries than experts. In these cases, experts were able to agree on the intent of the query based on their knowledge of the domain, even though there are seemingly no highly relevant results for the query found by search en-gines. Finally, one query appeared to be difficult to interpret for both groups of evaluators because of a misspelling.
Our results indicate that generalists X  relevance judgments often differ from experts because of generalists X  inability to understand the intent of complex queries. If that is cor-rect, it implies that we can potentially improve generalists X  performance by providing them with an accurate intent de-scription along with each query.

To test this idea, using the computer science queryset, we chose the best expert-generat ed intent statement for each query. We asked two groups of generalist evaluators to eval-uate each query X  X  top five results (averaging five evaluator X  X  scores for each result as before). One group was given the expert-generated intent statement along with each query, the other was not.

The results are shown in Figure 12, which plots expert ratings versus generalist ratings for each search result. The correlation between generalist a nd expert ratings increased when we provided generalists with query intent statements. The correlation was 0.69 without intent statements versus 0.74 with intent statements (p-value 0.015). Furthermore, the effect was more pronounced for lower relevance results; generalists gave lower scores for low-relevance results when given intent statements. That is, generalist evaluators are able to better judge the true relevance of lower-quality re-sults (even though they contain query keywords) when given intent statements.

However, generalists were still somewhat more conserva-tive than experts in giving high relevance scores to highly relevant pages. Perhaps this reflects a lack of confidence to  X  X oldly X  declare results highly relevant, given these evalua-tors X  lack of related domain knowledge.
 Figure 12: Generalists X  relevance ratings on the programming queryset, compared to experts, when given and not given query intent statements.
If our goal is to evaluate an internet search engine on a representative sample of queries, how much do these gen-eralist evaluator effects matter? While an exact answer to this question is difficult, we attempted to get an estimate by running the following simple (and admittedly biased) exper-iment. We asked a group of generalist evaluators to classify a random sample of US English queries by the level of do-main expertise required to accurately evaluate each query X  X  results. Seven evaluators classified each query into the fol-lowing buckets: The seven evaluations were averaged and bucketed as fol-lows: 1.5 was chosen as the threshold for  X  X ignificant expertise X  because it is the point at which at least half the evalua-tors indicated they would require hours of research, or al-ternatively, some evaluators indicated requiring pre-existing expertise to evaluate results accurately.

We ran this experiment on both a 500-query random sam-ple of U.S. English queries ( X  X ull sample X ), and a 496-query random sample of popular U.S. English queries ( X  X opular sample X ). Popular here was defined as queries appearing within the most frequent 15% of queries issued to Google over a one-year period. The bucketed results are shown in Figure 13. The overall mean on the full sample was 0.56, with a standard deviation of 0.38; the popular sample mean was 0.38 with a standard deviation of 0.33. Evaluators felt only five queries (1%) from the full sample required a high level of expertise to evaluate results accurately. No queries from the popular sample reached the high-expertise level. Figure 13: Evaluators X  rating of expertise required to accurately evaluate search results for two query samples.

Clearly, evaluators believe d that nearly all the queries in these samples could be well understood, and search re-sults accurately evaluated, with only a few minutes of back-ground research. Obviously, this experiment comes with manycaveats.Perhapsth eevaluatorsaresimplyover-confid ent, or are shallowly mis-judging the amount of expertise it would take to really understand and evaluate a query X  X  results. Still, it gives us an interesting rough estimate of what pro-portion of an internet search engine X  X  querystream is likely to be complex, expertise-requiring queries.

For comparison, we ran this same experiment using the computer programming queryset , with generalist evaluators. Not surprisingly, evaluators felt this queryset required much greater expertise than the random and popular query sam-ples. Of 111 queries, 43 (39%) were rated to require high expertise and 66 (60%) medium expertise. The overall mean was 1.40 (near the top of the  X  X edium X  expertise bucket) compared with 0.56 for the full querystream sample. Only two queries in the set were judged to require low expertise ([ X  X ++ program example X  X  and [internet explorer 7 enable java]).
We have seen that generalist evaluators sometimes do not understand queries that seek  X  X xpert X  information, and that this lack of understanding can result in shallow and inac-curate relevance ratings. Generalists X  rating errors appear to follow a query-keyword prominence model. We have also seen that generalist ratings improve when the evaluators are given  X  X ntent statements X  written by experts, and that the queries that require significant expertise represent a rela-tively small portion of the full querystream. These results have several important implications for search engine evalu-ation.

A common goal of search engine evaluation is to produce a summary relevance metric such as nDCG over a query set that is representative of a full querystream. Such a sum-mary metric can be used to track a search engine X  X  per-formance over time or compare two search engines to each other. Since expertise-requiring queries represent a small portion of a full querystream, the inaccuracy introduced by using generalist evaluators is probably acceptably low for this purpose. However, if the same query set is to be used for many evaluations, it may be worthwhile to classify the queries into various  X  X xpert X  categories and use expert eval-uators to write intent statements for generalist evaluators to use in future evaluations.

One may also be interested in comparing a search en-gine X  X  performance on various querystream  X  X lices X  such as queries containing more than 3 terms or  X  X erticals X  such as law-related queries. Our results suggest that particular cau-tion should be exercised in such cases. If the queries are more likely to require expertise (as the examples above likely would), then the use of generalist evaluators could introduce significant bias into the results. Use of special groups of ex-perts would be justifiable in such cases.
The nature of modern internet search engines requires that evaluation be done by generalist evaluators. We have found that generalists sometimes do not understand the in-tent of  X  X xpertise-requiring queries X , and as a result they rely on shallow keyword matches in order to assess relevance. However, we have also seen that expertise-requiring queries represent a relatively small portion of a full querystream, indicating that generalist evaluators are probably adequate for producing summary metrics o n query sets representative of a full querystream. We have also found that the rating accuracy of generalists can b e improved by providing  X  X n-tent statements X  written by experts. For investigations of queries more likely to require expertise, intent statements can be provided to generalists or, for more limited applica-tions, groups of expert raters can be used.
We thank Anne Aula and Daniel Russell for helpful com-ments and suggestions. [1] A. Aula and K. Nordhausen. Modeling successful [2] R. E. Downing, J. L. Moore, and S. W. Brown. The [3] G. B. Duggan and S. J. Payne. Knowledge in the head [4] D. Harman. Overview of the first trec conference. In [5] W.Hersh,C.Buckley,T.J.Leone,andD.Hickam.
 [6] C. Holscher and G. Strube. Web search behavior of [7] K. Jarvelin and J. Kekalainen. Cumulated gain-based [8] C.Jenkins,C.L.Corritore,andS.Wiedenbeck.
 [9] P. Vakkari, M. Pennanen, and S. Serola. Changes of [10] B. M. Wildemuth. The effects of domain knowledge on [11] X. Zhang, H. G. B. Anghelescu, and X. Yuan. Domain
