 Word representations are a critical component of many natural language processing systems. It is common to represent words as indices in a vocab-ulary, but this fails to capture the rich relational structure of the lexicon. Vector-based models do much better in this regard. They encode continu-ous similarities between words as distance or angle between word vectors in a high-dimensional space. The general approach has proven useful in tasks such as word sense disambiguation, named entity recognition, part of speech tagging, and document retrieval (Turney and Pantel, 2010; Collobert and Weston, 2008; Turian et al., 2010).

In this paper, we present a model to capture both semantic and sentiment similarities among words. The semantic component of our model learns word vectors via an unsupervised probabilistic model of documents. However, in keeping with linguistic and cognitive research arguing that expressive content and descriptive semantic content are distinct (Ka-plan, 1999; Jay, 2000; Potts, 2007), we find that this basic model misses crucial sentiment informa-tion. For example, while it learns that wonderful and amazing are semantically close, it doesn X  X  cap-ture the fact that these are both very strong positive sentiment words, at the opposite end of the spectrum from terrible and awful .

Thus, we extend the model with a supervised sentiment component that is capable of embracing many social and attitudinal aspects of meaning (Wil-son et al., 2004; Alm et al., 2005; Andreevskaia and Bergler, 2006; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007). This component of the model uses the vector represen-tation of words to predict the sentiment annotations on contexts in which the words appear. This causes words expressing similar sentiment to have similar vector representations. The full objective function of the model thus learns semantic vectors that are imbued with nuanced sentiment information. In our experiments, we show how the model can leverage document-level sentiment annotations of a sort that are abundant online in the form of consumer reviews for movies, products, etc. The technique is suffi-ciently general to work also with continuous and multi-dimensional notions of sentiment as well as non-sentiment annotations (e.g., political affiliation, speaker commitment).

After presenting the model in detail, we pro-vide illustrative examples of the vectors it learns, and then we systematically evaluate the approach on document-level and sentence-level classification tasks. Our experiments involve the small, widely used sentiment and subjectivity corpora of Pang and Lee (2004), which permits us to make comparisons with a number of related approaches and published results. We also show that this dataset contains many correlations between examples in the training and testing sets. This leads us to evaluate on, and make publicly available, a large dataset of informal movie reviews from the Internet Movie Database (IMDB). The model we present in the next section draws in-spiration from prior work on both probabilistic topic modeling and vector-spaced models for word mean-ings.

Latent Dirichlet Allocation (LDA; (Blei et al., 2003)) is a probabilistic document model that as-sumes each document is a mixture of latent top-ics. For each latent topic T , the model learns a conditional distribution p ( w | T ) for the probability that word w occurs in T . One can obtain a k -dimensional vector representation of words by first training a k -topic model and then filling the matrix with the p ( w | T ) values (normalized to unit length). The result is a word X  X opic matrix in which the rows are taken to represent word meanings. However, because the emphasis in LDA is on modeling top-ics, not word meanings, there is no guarantee that the row (word) vectors are sensible as points in a k -dimensional space. Indeed, we show in section 4 that using LDA in this way does not deliver ro-bust word vectors. The semantic component of our model shares its probabilistic foundation with LDA, but is factored in a manner designed to discover word vectors rather than latent topics. Some recent work introduces extensions of LDA to capture sen-timent in addition to topical information (Li et al., 2010; Lin and He, 2009; Boyd-Graber and Resnik, 2010). Like LDA, these methods focus on model-ing sentiment-imbued topics rather than embedding words in a vector space.

Vector space models (VSMs) seek to model words directly (Turney and Pantel, 2010). Latent Seman-tic Analysis (LSA), perhaps the best known VSM, explicitly learns semantic word vectors by apply-ing singular value decomposition (SVD) to factor a term X  X ocument co-occurrence matrix. It is typical to weight and normalize the matrix values prior to SVD. To obtain a k -dimensional representation for a given word, only the entries corresponding to the k largest singular values are taken from the word X  X  ba-sis in the factored matrix. Such matrix factorization-based approaches are extremely successful in prac-tice, but they force the researcher to make a number of design choices (weighting, normalization, dimen-sionality reduction algorithm) with little theoretical guidance to suggest which to prefer.

Using term frequency (tf) and inverse document frequency (idf) weighting to transform the values in a VSM often increases the performance of re-trieval and categorization systems. Delta idf weight-ing (Martineau and Finin, 2009) is a supervised vari-ant of idf weighting in which the idf calculation is done for each document class and then one value is subtracted from the other. Martineau and Finin present evidence that this weighting helps with sen-timent classification, and Paltoglou and Thelwall (2010) systematically explore a number of weight-ing schemes in the context of sentiment analysis. The success of delta idf weighting in previous work suggests that incorporating sentiment information into VSM values via supervised methods is help-ful for sentiment analysis. We adopt this insight, but we are able to incorporate it directly into our model X  X  objective function. (Section 4 compares our approach with a representative sample of such weighting schemes.) To capture semantic similarities among words, we derive a probabilistic model of documents which learns word representations. This component does not require labeled data, and shares its foundation with probabilistic topic models such as LDA. The sentiment component of our model uses sentiment annotations to constrain words expressing similar sentiment to have similar representations. We can efficiently learn parameters for the joint objective function using alternating maximization. 3.1 Capturing Semantic Similarities We build a probabilistic model of a document us-ing a continuous mixture distribution over words in-dexed by a multi-dimensional random variable  X  . We assume words in a document are conditionally independent given the mixture variable  X  . We assign a probability to a document d using a joint distribu-tion over the document and  X  . The model assumes each word w the other words given  X  . The probability of a docu-ment is thus p ( d ) =
Where N is the number of words in d and w the i th word in d . We use a Gaussian prior on  X  .
We define the conditional distribution p ( w ing a log-linear model with parameters R and b . The energy function uses a word representation ma-as a one-on vector) in the vocabulary V has a  X  -dimensional vector representation  X  sponding to that word X  X  column in R . The random variable  X  is also a  X  -dimensional vector,  X   X  R  X  which weights each of the  X  dimensions of words X  representation vectors. We additionally introduce a bias b all word frequencies. The energy assigned to a word w given these model parameters is To obtain the distribution p ( w |  X  ) we use a softmax, p ( w |  X  ; R,b ) = The number of terms in the denominator X  X  sum-mation grows linearly in | V | , making exact com-putation of the distribution possible. For a given  X  , a word w  X  X  occurrence probability is related to how closely its representation vector  X  scaling direction of  X  . This idea is similar to the word vector inner product used in the log-bilinear language model of Mnih and Hinton (2007).
 Equation 1 resembles the probabilistic model of LDA (Blei et al., 2003), which models documents as mixtures of latent topics. One could view the en-tries of a word vector  X  as that word X  X  association strength with respect to each latent topic dimension. The random variable  X  then defines a weighting over topics. However, our model does not attempt to model individual topics, but instead directly models word probabilities conditioned on the topic mixture variable  X  . Because of the log-linear formulation of the conditional distribution,  X  is a vector in R  X  and not restricted to the unit simplex as it is in LDA.
We now derive maximum likelihood learning for this model when given a set of unlabeled documents D . In maximum likelihood learning we maximize the probability of the observed data given the model parameters. We assume documents d samples. Thus the learning problem becomes max Using maximum a posteriori (MAP) estimates for  X  , we approximate this learning problem as where  X   X  We introduce a Frobenious norm regularization term for the word representation matrix R . The word bi-ases b are not regularized reflecting the fact that we want the biases to capture whatever overall word fre-quency statistics are present in the data. By taking the logarithm and simplifying we obtain the final ob-jective,  X  || R || 2 F + which is maximized with respect to R and b . The hyper-parameters in the model are the regularization weights (  X  and  X  ), and the word vector dimension-ality  X  . 3.2 Capturing Word Sentiment The model presented so far does not explicitly cap-ture sentiment information. Applying this algorithm to documents will produce representations where words that occur together in documents have sim-ilar representations. However, this unsupervised approach has no explicit way of capturing which words are predictive of sentiment as opposed to content-related. Much previous work in natural lan-guage processing achieves better representations by learning from multiple tasks (Collobert and Weston, 2008; Finkel and Manning, 2009). Following this theme we introduce a second task to utilize labeled documents to improve our model X  X  word representa-tions.

Sentiment is a complex, multi-dimensional con-cept. Depending on which aspects of sentiment we wish to capture, we can give some body of text a sentiment label s which can be categorical, continu-ous, or multi-dimensional. To leverage such labels, we introduce an objective that the word vectors of our model should predict the sentiment label using some appropriate predictor, Using an appropriate predictor function f ( x ) we map a word vector  X   X  s . We can then improve our word vector  X  predict the sentiment labels of contexts in which that word occurs.

For simplicity we consider the case where the sen-timent label s is a scalar continuous value repre-senting sentiment polarity of a document. This cap-tures the case of many online reviews where doc-uments are associated with a label on a star rating scale. We linearly map such star values to the inter-val s  X  [0 , 1] and treat them as a probability of pos-itive sentiment polarity. Using this formulation, we employ a logistic regression as our predictor f ( x ) . We use w  X  X  vector representation  X  weights  X  to express this as where  X  ( x ) is the logistic function and  X   X  R  X  is the logistic regression weight vector. We additionally introduce a scalar bias b
The logistic regression weights  X  and b a linear hyperplane in the word vector space where a word vector X  X  positive sentiment probability de-pends on where it lies with respect to this hyper-plane. Learning over a collection of documents re-sults in words residing different distances from this hyperplane based on the average polarity of docu-ments in which the words occur.

Given a set of labeled documents D where s the sentiment label for document d maximize the probability of document labels given the documents. We assume documents in the collec-tion and words within a document are i.i.d. samples. By maximizing the log-objective we obtain, The conditional probability p ( s easily obtained from equation 9. 3.3 Learning The full learning objective maximizes a sum of the two objectives presented. This produces a final ob-jective function of,  X  || R || 2 F + | S with the same rounded value of s and s combat the well-known imbalance in ratings present in review collections. This weighting prevents the overall distribution of document ratings from affect-ing the estimate of document ratings in which a par-ticular word occurs. The hyper-parameters of the model are the regularization weights (  X  and  X  ), and the word vector dimensionality  X  .
 Maximizing the objective function with respect to R , b ,  X  , and b alternating maximization, which first optimizes the word representations ( R , b ,  X  , and b ing the MAP estimates (  X   X  ) fixed. Then we find the new MAP estimate for each document while leav-ing the word representations fixed, and continue this process until convergence. The optimization algo-rithm quickly finds a global solution for each  X   X  cause we have a low-dimensional, convex problem in each  X   X  for different documents are independent, we can solve them on separate machines in parallel. This facilitates scaling the model to document collections with hundreds of thousands of documents. We evaluate our model with document-level and sentence-level categorization tasks in the domain of online movie reviews. For document categoriza-tion, we compare our method to previously pub-lished results on a standard dataset, and introduce a new dataset for the task. In both tasks we com-pare our model X  X  word representations with several bag of words weighting methods, and alternative ap-proaches to word vector induction. 4.1 Word Representation Learning We induce word representations with our model us-ing 25,000 movie reviews from IMDB. Because some movies receive substantially more reviews than others, we limited ourselves to including at most 30 reviews from any movie in the collection. We build a fixed dictionary of the 5,000 most fre-quent tokens, but ignore the 50 most frequent terms from the original full vocabulary. Traditional stop word removal was not used because certain stop words (e.g. negating words) are indicative of senti-ment. Stemming was not applied because the model learns similar representations for words of the same stem when the data suggests it. Additionally, be-cause certain non-word tokens (e.g.  X ! X  and  X :-) X  ) are indicative of sentiment, we allow them in our vo-cabulary. Ratings on IMDB are given as star values (  X  { 1 , 2 ,..., 10 } ) , which we linearly map to [0 , 1] use as document labels when training our model.
The semantic component of our model does not require document labels. We train a variant of our model which uses 50,000 unlabeled reviews in addi-tion to the labeled set of 25,000 reviews. The unla-beled set of reviews contains neutral reviews as well as those which are polarized as found in the labeled set. Training the model with additional unlabeled data captures a common scenario where the amount of labeled data is small relative to the amount of un-labeled data available. For all word vector models, we use 50-dimensional vectors.

As a qualitative assessment of word represen-tations, we visualize the words most similar to a query word using vector similarity of the learned representations. Given a query word w and an- X  larity of w with all other words w  X  , we can find the words deemed most similar by the model.

Table 1 shows the most similar words to given query words using our model X  X  word representations as well as those of LSA. All of these vectors cap-ture broad semantic similarities. However, both ver-sions of our model seem to do better than LSA in avoiding accidental distributional similarities (e.g., screwball and grant as similar to romantic ) A com-parison of the two versions of our model also begins to highlight the importance of adding sentiment in-formation. In general, words indicative of sentiment tend to have high similarity with words of the same sentiment polarity, so even the purely unsupervised model X  X  results look promising. However, they also show more genre and content effects. For exam-ple, the sentiment enriched vectors for ghastly are truly semantic alternatives to that word, whereas the vectors without sentiment also contain some content words that tend to have ghastly predicated of them. Of course, this is only an impressionistic analysis of a few cases, but it is helpful in understanding why the sentiment-enriched model proves superior at the sentiment classification results we report next. 4.2 Other Word Representations For comparison, we implemented several alternative vector space models that are conceptually similar to our own, as discussed in section 2: Latent Semantic Analysis (LSA; Deerwester et al., 1990) We apply truncated SVD to a tf.idf weighted, cosine normalized count matrix, which is a standard weighting and smoothing scheme for VSM induction (Turney and Pantel, 2010).
 Latent Dirichlet Allocation (LDA; Blei et al., 2003) We use the method described in sec-tion 2 for inducing word representations from the topic matrix. To train the 50-topic LDA model we use code released by Blei et al. (2003). We use the same 5,000 term vocabulary for LDA as is used for training word vector models. We leave the LDA hyperparameters at their default values, though some work suggests optimizing over priors for LDA is important (Wallach et al., 2009).
 Weighting Variants We evaluate both binary (b) term frequency weighting with smoothed delta idf (  X  t X ) and no idf (n) because these variants worked well in previous experiments in sentiment (Mar-tineau and Finin, 2009; Pang et al., 2002). In all cases, we use cosine normalization (c). Paltoglou and Thelwall (2010) perform an extensive analysis of such weighting variants for sentiment tasks. 4.3 Document Polarity Classification Our first evaluation task is document-level senti-ment polarity classification. A classifier must pre-dict whether a given review is positive or negative given the review text.

Given a document X  X  bag of words vector v , we obtain features from our model using a matrix-vector product Rv , where v can have arbitrary tf.idf weighting. We do not cosine normalize v , instead applying cosine normalization to the final feature vector Rv . This procedure is also used to obtain features from the LDA and LSA word vectors. In preliminary experiments, we found  X  X nn X  weighting to work best for v when generating document fea-tures via the product Rv . In all experiments, we use this weighting to get multi-word representations from word vectors. 4.3.1 Pang and Lee Movie Review Dataset The polarity dataset version 2.0 introduced by Pang where each is associated with a binary sentiment po-larity label. We report 10-fold cross validation re-sults using the authors X  published folds to make our results comparable with others in the literature. We use a linear support vector machine (SVM) classifier trained with LIBLINEAR (Fan et al., 2008), and set the SVM regularization parameter to the same value used by Pang and Lee (2004).

Table 2 shows the classification performance of our method, other VSMs we implemented, and pre-viously reported results from the literature. Bag of words vectors are denoted by their weighting nota-tion. Features from word vector learner are denoted by the learner name. As a control, we trained ver-sions of our model with only the unsupervised se-mantic component, and the full model (semantic and sentiment). We also include results for a version of our full model trained with 50,000 additional unla-beled examples. Finally, to test whether our mod-els X  representations complement a standard bag of words, we evaluate performance of the two feature representations concatenated.
Our method X  X  features clearly outperform those of other VSMs, and perform best when combined with the original bag of words representation. The vari-ant of our model trained with additional unlabeled data performed best, suggesting the model can effec-tively utilize large amounts of unlabeled data along with labeled examples. Our method performs com-petitively with previously reported results in spite of our restriction to a vocabulary of only 5,000 words.
We extracted the movie title associated with each review and found that 1,299 of the 2,000 reviews in the dataset have at least one other review of the same movie in the dataset. Of 406 movies with multiple reviews, 249 have the same polarity label for all of their reviews. Overall, these facts suggest that, rela-tive to the size of the dataset, there are highly corre-lated examples with correlated labels. This is a nat-ural and expected property of this kind of document collection, but it can have a substantial impact on performance in datasets of this scale. In the random folds distributed by the authors, approximately 50% of reviews in each validation fold X  X  test set have a review of the same movie with the same label in the training set. Because the dataset is small, a learner may perform well by memorizing the association be-tween label and words unique to a particular movie (e.g., character names or plot terms).

We introduce a substantially larger dataset, which uses disjoint sets of movies for training and testing. These steps minimize the ability of a learner to rely on idiosyncratic word X  X lass associations, thereby focusing attention on genuine sentiment features. 4.3.2 IMDB Review Dataset We constructed a collection of 50,000 reviews from IMDB, allowing no more than 30 reviews per movie. The constructed dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. Following previous work on polarity classification, we consider only highly po-larized reviews. A negative review has a score  X  4 out of 10, and a positive review has a score  X  7 out of 10. Neutral reviews are not included in the dataset. In the interest of providing a benchmark for future work in this area, we release this dataset to
We evenly divided the dataset into training and test sets. The training set is the same 25,000 la-beled reviews used to induce word vectors with our model. We evaluate classifier performance after cross-validating classifier parameters on the training set, again using a linear SVM in all cases. Table 2 shows classification performance on our subset of IMDB reviews. Our model showed superior per-formance to other approaches, and performed best when concatenated with bag of words representa-tion. Again the variant of our model which utilized extra unlabeled data during training performed best.
Differences in accuracy are small, but, because our test set contains 25,000 examples, the variance of the performance estimate is quite low. For ex-ample, an accuracy increase of 0.1% corresponds to correctly classifying an additional 25 reviews. 4.4 Subjectivity Detection As a second evaluation task, we performed sentence-level subjectivity classification. In this task, a clas-sifier is trained to decide whether a given sentence is subjective, expressing the writer X  X  opinions, or ob-jective, expressing purely facts. We used the dataset of Pang and Lee (2004), which contains subjective sentences from movie review summaries and objec-tive sentences from movie plot summaries. This task is substantially different from the review classifica-tion task because it uses sentences as opposed to en-tire documents and the target concept is subjectivity instead of opinion polarity. We randomly split the 10,000 examples into 10 folds and report 10-fold cross validation accuracy using the SVM training protocol of Pang and Lee (2004).

Table 2 shows classification accuracies from the sentence subjectivity experiment. Our model again provided superior features when compared against other VSMs. Improvement over the bag-of-words baseline is obtained by concatenating the two feature vectors. We presented a vector space model that learns word representations captuing semantic and sentiment in-formation. The model X  X  probabilistic foundation gives a theoretically justified technique for word vector induction as an alternative to the overwhelm-ing number of matrix factorization-based techniques commonly used. Our model is parametrized as a log-bilinear model following recent success in us-ing similar techniques for language models (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007), and it is related to probabilistic latent topic models (Blei et al., 2003; Steyvers and Grif-fiths, 2006). We parametrize the topical component of our model in a manner that aims to capture word representations instead of latent topics. In our ex-periments, our method performed better than LDA, which models latent topics directly.

We extended the unsupervised model to incor-porate sentiment information and showed how this extended model can leverage the abundance of sentiment-labeled texts available online to yield word representations that capture both sentiment and semantic relations. We demonstrated the util-ity of such representations on two tasks of senti-ment classification, using existing datasets as well as a larger one that we release for future research. These tasks involve relatively simple sentiment in-formation, but the model is highly flexible in this regard; it can be used to characterize a wide variety of annotations, and thus is broadly applicable in the growing areas of sentiment analysis and retrieval. This work is supported by the DARPA Deep Learn-ing program under contract number FA8650-10-C-7020, an NSF Graduate Fellowship awarded to AM, and ONR grant No. N00014-10-1-0109 to CP.

