 best action).
 to that of the action with the lowest cumulative loss.
 and the number of actions, Hedge can achieve a regret upper-b ounded by O (  X (  X  and the total loss of the worst action in the indicated interv al of measure  X  . completely adaptive algorithm for DTOL. Our algorithm is ca lled NormalHedge. NormalHedge followed by an updating of weights for all actions.
 that perform well is extremely reasonable  X  see, for example , Figure 1. of the learner and the  X   X N  X  -th element in the sorted list.
 We prove that for NormalHedge, the regret to the top  X  -quantile of actions is at most action is upper-bounded by O notion of regret, see Section 4).
 NormalHedge works by assigning each action i a potential; actions which have lower cumulative loss than the algorithm are assigned a potential exp( R 2 i and c the potential assigned by Hedge to action i is proportional to exp(  X R previous potential-based methods.
 algorithm will perform better than most of the actions and th us assign them zero probability. present some outlines of the proof. The proof details are in t he Supplementary Materials. 2.1 Setting p incurs the expected loss under this distribution: The learner X  X  instantaneous regret to an action i in round t is r regret to an action i in the first t rounds is We assume that the losses  X  loss does not matter). The goal of the learner is to minimize t his cumulative regret R i (in particular, the best action), for any value of t . 2.2 Normal-Hedge tribution, specifically where [ x ] differentiable, and twice-differentiable except at x = 0 .
 In addition to tracking the cumulative regrets R also maintains a scale parameter c actions i , evaluated at R We observe that since  X  ( x, c ) is convex in c &gt; 0 , we can determine c at Notice that the actions for which R We summarize the learning algorithm in Figure 2. mance of NormalHedge with two representative algorithms: a version of Hedge due to [7], and the the other hand, NormalHedge automatically adapts to the los s-sequences of the actions. as a function of n .
 Our example attempts to model a practical scenario where one often finds multiple actions with indicates that in this case, the performance of Hedge and the Polynomial Weights will depend on discretization.
 how much better the good actions are compared to the rest. Fin ally, T is the number of rounds. The instantaneous losses of the N actions are represented by a N  X  T matrix B  X ,k First, we construct a (preliminary) n  X  T matrix A horizontally T / 2 d times, and finally, (4) halving the first column. We show A If the rows of A eventually assign all actions the same weight. Now, let A  X ,k subtracted from each entry of the first k rows, e.g.
 A Now, when losses are given by A  X ,k we artificially replicate each action (each row) N/n times to yield the final loss matrix B  X ,k actions: We compare the performance of NormalHedge to two other repre sentative algorithms, which we due to [7] (roughly,  X  that of Hedge.
 number of actions N .
 no matter how many times the actions may be replicated, the pe rformance of NormalHedge stays not depend on the replication factor (the peformance would b e the same as the N/n = 1 case). setting their parameters. k is the (effective) number of good actions. Here, we fix n = 126 and  X  = 0 . 025 . There has been a large amount of literature on various aspect s of DTOL. The Hedge algorithm of been well-studied.
 of
O (  X  of  X  Section 3. Moreover, they do not consider our notion of regre t.
 O ( values of  X  uniformly, without any extra overhead. More recent work in [14, 7, 10] provide algorithms with signi ficantly improved bounds when the explicitly on N .
 tions N . The weight assigned to action i in round t is proportional to ([ R algorithm [16, 17] by scaling the perturbations by a paramet er that depends on both t and N . 5.1 Main results Our main result is the following theorem.
 at most In particular, with  X  = 1 /N , the regret to the best action is at most essentially goes away.
 Corollary 2. If Normal-Hedge has access to N actions, then, as t  X   X  , the regret of Normal-Hedge to the top  X  -quantile of actions approaches an upper bound of In particular, the regret of Normal-Hedge to the best action approaches an upper bound of of The proof of Theorem 1 follows from a combination of Lemmas 3, 4, and 5, and is presented in detail at the end of the current section. 5.2 Regret bounds from the potential equation The following lemma relates the performance of the algorith m at time t to the scale c Lemma 3. At any time t , the regret to the best action can be bounded as Moreover, for any 0  X   X   X  1 and any t , the regret to the top  X  -quantile of actions is at most Proof. We use E lemma follows from the fact that, for any action i  X  E which implies R For the second part of the lemma, let R R from which the second part of the lemma follows. 5.3 Bounds on the scale c In Lemmas 4 and 5, we bound the growth of the scale c The main outline of the proof of Theorem 1 is as follows. As c can divide the rounds t into two phases, t &lt; t for some fixed  X   X  (0 , 1 / 2) . We then show bounds on the growth of c Lemma 4 shows that c per-round growth of c we defer them to the supplementary appendix.
 Lemma 4. For any time t , Lemma 5. Suppose that at some time t Then, for any time t  X  t We now combine Lemmas 4 and 5 together with Lemma 3 to prove the main theorem. Proof of Theorem 1. Let t which is at most Combining these last two inequalities yields Now the theorem follows by applying Lemma 3. References
