 Kernel selection is one of the key issues both in recent re-search and application of kernel methods. This is usually done by minimizing either an estimate of generalization error or some other related performance measure. It is well known that a kernel matrix can be interpreted as an empirical ver-sion of a continuous integral operator, and its eigenvalues converge to the eigenvalues of integral operator. In this pa-per, we introduce new kernel selection criteria based on the eigenvalues perturbation of the integral operator. This per-turbation quantifies the difference between the eigenvalues of the kernel matrix and those of the integral operator. We establish the connection between eigenvalues perturbation and generalization error. By minimizing the derived gener-alization error bounds, we propose the kernel selection cri-teria. Therefore the kernel chosen by our proposed criteria can guarantee good generalization performance. To compute the values of our criteria, we present a method to obtain the eigenvalues of integral operator via the Fourier transform. Experiments on benchmark datasets demonstrate that our kernel selection criteria are sound and effective. I.2.6 [ Artificial Intelligence ]: Learning X  Parameter Learn-ing ; I.5.2 [ Pattern Recognition ]: Design Methodology X  Classifier Design and Evaluation ; H.2.8 [ Database Man-agement ]: Database Applications X  Data Mining Algorithms, Theory, Experimentation Kernel Selection, Eigenvalues Perturbation, Integral Opera-tor, Generalization Error.  X  c orresponding author
It is well known that the kernel matrix contains most of the information needed by the kernel methods, and its eigen-values play an important role in kernel matrix. Because the kernel matrix can be interpreted as an empirical version of a continuous integral operator, and its eigenvalues converge to the eigenvalues of integral operator [5, 20, 28], therefore we aim at presenting new kernel selection criteria based on the eigenvalues perturbation of integral operator in this pa-per. This perturbation quantifies the difference between the eigenvalues of kernel matrix and those of integral operator. Different from most of the existing complexity measures, we can compute the value of eigenvalues perturbation for any given kernel function from empirical data, which makes it usable for kernel selection. We first use the eigenvalues perturbation to derive generalization error bounds for kernel ridge regression (KRR) and Support Vector Machine (SVM). Then, by minimizing the derived generalization error bound-s, we propose two new kernel selection criteria: EPKRR (for KRR) and EPSVM (for SVM). To compute the values of our proposed criteria, we propose a method to compute the eigenvalues of integral operator based on the Fourier transform. For the popular Gaussian kernel and Laplacian kernel, the closed form of eigenvalues of their correspond-ing integral operators are given. Experimental results show that, for classification, the kernel chosen by EPSVM gives better results than those chosen by the popular classification criteria: CKTA, FSM and KCV, and for regression, EPKRR better than the popular regression criteria: KCV, LOO and GCV.

The rest of the paper is organized as follows. In Section 2, we introduce some elementary facts. In Section 3, we present the definition of eigenvalues perturbation, and use this def-inition to derive generalization error bounds for KRR and SVM. In Section 4, we propose the kernel selection criteria by minimizing the derived generalization error bounds, and present a method to compute the eigenvalues of integral op-erator. In Section 5 we empirically analyze the performance of our proposed kernel selection criteria compared with other popular criteria. We end in Section 6 with conclusion. Given a training set of size m drawn identically and independently distributed from a fixed, but unknown probability measure  X  on Z = X  X Y , where Y  X  R for regression, and Y  X  { +1 ,  X  1 } for classification.

Let K : X  X X  X  R be a kernel, that is, K is symmetric and for any finite set of points { x 1 , . . . , x m } X  X  , the kernel matrix is positive semidefinite. The reproducing kernel Hilbert s-pace (RKHS) H K associated with the kernel K is defined to be the completion of the linear span of the set of functions with the inner product denoted as  X  X  ,  X  X  K satisfying
Bec ause the kernel matrix contains most of the informa-tion needed by the regularized algorithms, and its eigenval-ues converge to the eigenvalues of integral operator. There-fore we introduce the notion of eigenvalues perturbation, which quantifies the difference between the eigenvalues of kernel matrix and those of integral operator.

Definition 1 (Eigenvalues Perturbation). The k-ernel function K is  X  eigenvalues perturbation if the follow-ing holds: where K is the kernel matrix, [ K ] i;j = 1 m K ( x i , x j ) , L K is the integral operator defined in (1),  X  i ( K ) and  X  i ( L K ) are the eigenvalues of K and L K , respectively.

The eigenvalues perturbation is defined on the kernel ma-trix and integral operator, therefore, if we obtain the eigen-values of integral operator (the eigenvalues of integral op-erator induced by the popular radial kernels are given in Theorem 5), we can estimate its value for any given kernel function from empirical data, which makes it able to be used for kernel selection. In the next, we will show that the eigen-values perturbation can yield upper bounds of generalization error for KRR and SVM.
KRR has successfully been applied to solve regression problems, which is a special case of the regularized algo-rithms when the loss function  X  ( f ( x ) , y ) = ( f ( x )  X  y ) 2 . For KRR, the generalization error R ( S ) = E z ( f ( x )  X  y ) 2 and the empirical error R emp ( S ) = 1 m
The orem 1. If the kernel function K is  X  eigenvalues perturbation, then for the KRR, with probability 1  X   X  , we have R ( S )  X  R emp ( S ) + The proof of this theorem is given in Appendix.A.

This theorem shows that small R emp ( S ) and  X  can guar-antee good generalization performance for KRR.

Next, we also give a better exponential generalization er-ror bound based on concentration inequalities.

Theorem 2. If the kernel function K is  X  spectral per-turbation stability, then for the KRR, with probability 1  X   X  , we have
R ( S )  X  R emp ( S ) + 4 M ( C X  + Q ) + 2 M ( C X  + Q ) 2 The proof of this theorem is given in Appendix.B.
SVM has successfully been applied to solve classification problems, its loss function is the hinge loss  X  ( f ( x ) , y ) = max(0 , 1  X  yf ( x )). For SVM, R ( S ) = E z max(0 , 1  X  yf ( x )) and R emp ( S ) = 1 m
In order to use these criteria for kernel selection, we should compute the eigenvalues of integral operator. In the follow-ing, we will present the method to compute the eigenvalues of integral operator induced by the popular radial kernels, such as Gaussian kernel and Laplacian kernel.

Theorem 5. Assuming the radial kernel K ( x  X  x  X  ) is de-fined on [  X  M/ 2 , M/ 2] d , M &gt; 0 , d is the dimension of the input data, then the eigenvalues of integral operator induced by the radial kernel K ( x  X  x  X  ) are where n = ( n 1 , . . . , n d ) , n i  X  N  X  X  0 } ,
Proof. We assume d = 1. A generalization to multidi-mensional kernels ( d &gt; 1) is straightforward. Since the radi-al kernel K ( x  X  x  X  ) is an even function defined on [  X  M/ 2 , M/ 2], therefore, by the Fourier transform, we have where a 0 = 1 M F (0), a n = 2 M F ( n ), n = 1 , 2 , . . . ,  X  . By the definition of integral operator (see (1)), we have Note that cos nw ( x  X  t ) = cos nwx cos nwt +sin nwx sin nwt , and is a standard orthogonal basis of the square integrable space L  X  M= 2 ;M= 2] . Thus, it is easy to verify that In the same way, we can obtain that Th erefore, F ( n ) is the eigenvalue of the integral operator L
K , its associate eigenfunction is n = 1 , . . . . F (0) is the eigenvalue of L K , its associate eigen-function is different partitions. M ethod EPSVM CKTA FSM 3-CV 5-CV 10-CV M ethod EPSVM CKTA FSM 3-CV 5-CV 10-CV M ethod EPSVM CKTA FSM 3-CV 5-CV 10-CV M ethod EPSVM CKTA FSM 3-CV 5-CV 10-CV M ethod EPSVM CKTA FSM 3-CV 5-CV 10-CV the test errors for the chosen parameters on testing set. the chosen parameters on the testing set. and evaluate the test errors for the chosen parameters on testing set. sets. In particular, for each  X  , EPSVM outperforms FSM on 6 (or more) out of 8 sets, and outperforms 3-CV, 5-CV and 10-CV on 5 (or more) out of 8 sets. Thus it implicates that choosing the kernel based on the eigenvalues perturba-tion can guarantee good generalization. (b) The test errors of EPSVM are more stable than these of other five criteria when changing the value of  X  , that is the fluctuation of the test errors of the EPSVM are smaller than those of other methods when changing the value of  X  . This property may bring some advantages in practical application.

In the next experiment, we will explore the effect of the regularization coefficient  X  for EPSVM. The average test er-rors with different  X  are given in Figure 1. We find that the optimal  X  belong to [2 5 , 2 10 ] on most data sets. Therefore, the range of  X  should be set between 2 5 and 2 10 .
We will compare EPKRR criterion with five popular re-gression criteria: 3-CV, 5-CV, 10-CV, generalized cross-validation (GCV) [19] and leave-one-out (LOO) 3 . The learn-ing algorithm we use here is the KRR.

The test mean square errors (TMSE) with standard de-viations are reported in Table 3. In this experiment, the parameter  X  = 100. The results in Table 3 can be sum-marized as follows: (a) EPKRR criterion is much better than GCV on the nearly all data sets. In particular, for  X   X  { 0 . 0001 , 0 . 001 , 0 . 01 } , EPKRR outperforms GCV on 6 out of 7 sets, and also gives the close result on the remain-ing 1 sets. (b) EPKRR criterion is comparable or better than 3-CV, 5-CV, 10-CV and LOO on most data sets. So it implicates that the EPKRR criterion is sound and effective. Finally, we will explore the effect of the parameter  X  for EPKRR. The TMSE with different  X  are given in Figure 2. It is interesting to note that the TMSE does not quite depend on  X  . Thus, we can set  X  to be a constant for simplicity in practice (In Figure 2, we can find that the  X  = 2 10 is a reasonable choice). w e only need to solve the KRR once to compute LOO [6].
Pr oof. Denote vectors k , k i , y and y i as k = ( K ( x , x 1 ) , . . . , K ( x , x m )) T , k y = ( y 1 , . . . , y m ) T , y = ( y 1 , . . . , y i  X  1 , y i +1 , . . . , y m ) T , respectively. Denote the ( m  X  1)  X  ( m  X  1) kernel matrix K i with respect to the training set S i as
Acc ording to [18], we know that the solutions of the KRR with respect to the training set S and S i can be respectively written as wh ere I and I i are the m  X  m and ( m  X  1)  X  ( m  X  1) identity matrices, respectively. For each i  X  { 1 , . . . , m } , denote the m  X  m i -th removed kernel matrix as K i with it is easy to verify that wh ere A i = diag(0 , . . . , 0 , 1 , 0 , . . . , 0) is a diagonal matrix, with the i -th diagonal element 1 , others 0. Therefore, we have S ince M  X  X  X  1  X  M  X  1 =  X  M  X  X  X  1 ( M  X   X  M ) M  X  1 is valid for any invertible matrices M , M  X  , so, we have Thus, we can obtain that wh ere  X  min ( K +  X  I ) is the smallest eigenvalue of K +  X  I and  X  min ( K i +  X  I ) the smallest eigenvalue of K i +  X  I . Thus, B y the definition of pointwise hypothesis stability (see Defi-nition 2), it is easy to verity that the KRR with  X  eigenvalues perturbation is pointwise hypothesis stability.

Since | y | X  M and f S ( x ) = 1 m k T ( K +  X  I )  X  1 y , we have No te that Thus, by using Theorem 7, this assertion can be proved. We first give a definition of uniform stability and a theorem introduced in [4].

Definition 3 (Uniform Stability). An algorithm A has uniform stability  X  with respect to the loss function  X  if the following holds:  X  S  X  X  m ,  X  i  X  X  1 , . . . , m } ,
Theorem 8 (Theorem 12 in [4]). Let A be an algo-rithm with uniform stability  X  with respect to a loss function  X  such that 0  X   X  ( f S , z )  X  L , for all z  X  Z and all sets S . Then, for any m  X  1 , and any  X   X  (0 , 1) , the following bounds hold (separately) with probability at least 1  X   X  over the random draw of the sample S ,
Pr oof of Theorem 2. By (4) and (5), we have |  X  ( f S , z )  X   X  ( f By the definition of uniform stability (see in Definition 3), it is easy to verity that the KRR with  X  eigenvalues pertur-bation is uniform stability. Thus, based on Theorem 8, we can prove this theorem.

Lemma 1 (Proposition 2 in [13]). Let h  X  denote the hypothesis returned by SVMs when using the approximate kernel matrix K  X  . Then, the following inequality holds for all x  X  X  : | h ( x )  X  h ( x ) | X  T o prove Theorem 3, we first give the following theorem: [1] F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel [2] P. L. Bartlett, S. Boucheron, and G. Lugosi. Model [3] P. L. Bartlett and S. Mendelson. Rademacher and [4] O. Bousquet and A. Elisseeff. Stability and [5] M. Braun. Accurate error bounds for the eigenvalues [6] G. C. Cawley and N. L. C. Talbot. Preventing [7] G. C. Cawley and N. L. C. Talbot. On over-fitting in [8] O. Chapelle and V. Vapnik. Model selection for [9] O. Chapelle, V. Vapnik, O. Bousquet, and [10] C. Corte and V. Vapnik. Support-vector networks. [11] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 [12] C. Cortes, M. Mohri, and A. Rostamizadeh.
 [13] C. Cortes, M. Mohri, and A. Talwalkar. On the [14] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. S. [15] M. Debruyne, M. Hubert, and J. A. Suykens. Model [16] T. Evgeniou, M. Pontil, and T. Poggio. Regularization [17] C. S. A. Gammerman and V. Vovk. Ridge regression
