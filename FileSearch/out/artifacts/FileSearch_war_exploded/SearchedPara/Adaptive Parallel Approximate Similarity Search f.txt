 This paper introduces Hypercurves, a flexible framework for pro-viding similarity search indexing to high throughput multimedia services. Hypercurves efficiently and effectively answers k-nearest neighbor searches on multigigabyte high-dimensional databases. It supports massively parallel processing and adapts at runtime its parallelization regimens to keep answer times optimal for ei-ther low and high demands. In order to achieve its goals, Hyper-curves introduces new techniques for selecting parallelism config-urations and allocating threads to computation cores, including hy-perthreaded cores. Its efficiency gains are throughly validated on a large database of multimedia descriptors, where it presented near linear speedups and superlinear scaleups. The adaptation reduces query response times in 43% and 74% for both platforms tested, when compared to the best static parallelism regimens.
 H.2.2 [ Database management ]: Physical design  X  access meth-ods; H.2.4 [ Database management ]: Systems  X  multimedia databases; H.3.1 [ Information storage and retrieval ]: Content analysis and indexing  X  indexing methods Algorithms, Measurement, Performance, Design, Experimentation. Descriptor indexing, Local descriptors, Information retrieval.
In the past decades, content-based multimedia retrieval has been proposed by the scientific community as a sophisticated alterna-tive to the often impractical keyword-based retrieval of multime-dia documents. Though the concretization of those goals in the marketplace has been much more elusive, in the past few years, a quick sequence of successful applications has brought content-based retrieval from the pages of scholarly articles to the reality of consumer service business. Those services include the obvious web image retrieval search engines, but also the less obvious image recognition for handheld devices, real-time song identification, and a myriad of assorted applications.

The alluring diversity of those services is somewhat deceptive, for behind the multitude of media formats, query modalities and database genera, content-based multimedia retrieval and classifica-tion is backed by the unifying principle of the descriptor. The de-scriptors give the multimedia documents a perceptually meaning-ful geometry, which bridges the so called  X  X emantic gap X , i.e., the meaninglessness of their low-level coding (the pixels in the image, the samples in the audio, etc.) and the complexity of the high-level task at hand. In that geometry, the documents may be seen as vec-tors in a space, which can be closer or further apart, according to their perceptual properties.

In terms of database systems, the pivotal operation is similar-ity search on the descriptors. From that point of view, looking for similar multimedia documents is equivalent to look for documents whose descriptors are similar. The complete multimedia query pro-cessing may actually be more complex than that, and consist of sev-eral phases. Nevertheless, similarity search on the descriptors will often be the first step and, surprisingly, one of the most expensive.
In this paper we are concerned with that core operation of sim-ilarity search. We introduce a flexible architecture, which we call Hypercurves, for providing similarity search to online multimedia services.

Hypercurves is built upon the synergy of two ideas: the sequen-tial multidimensional index Multicurves [35] and the concurrent execution environment Anthill [31]. It addresses many challenges, presenting several novel contributions: 1. In order to address large scale databases and heavy request 2. Hypercurves is intended for online services, with variable, 3. Hypercurves performance (almost linear speedup and super-It is opportune to highlight that the extensions proposed to Anthill, though tailored to Hypercurves, are useful to a large class of online services. The equivalence proof can also be applied in other contexts, since it is general to the problem of searching on distributed sorted lists.

All contributions are tested throughly in several experimental scenarios.
Classification and retrieval of multimedia documents is almost always mediated by descriptors, which embed multimedia docu-ments into a space, thus bridging the gap between the amorphous low level coding and the complex semantic relationships.

In what concerns visual documents (images and videos), the last decade witnessed the ascent of descriptors inspired by Computer Vision, especially the so-called local descriptors [21, 33], with the remarkable success of the SIFT local descriptors [18]. Meanwhile, the idea of using compact representations based on codebooks of those descriptors became popular and one of the main tools in the literature [4].

But regardless of the representation employed, a key operation in all systems is retrieving similar descriptors. This operation can be used either directly (many early CBIR systems were little more than a similarity search engine attached to a descriptor space [29]), either indirectly (the search may be part of a kNN classifier, it can retrieve a preliminary set of candidates to be refined by a more computationally intensive classifier, etc.). Nevertheless, it remains a critical component, if the system is to be used in real world, large-scale databases [17].

Efficient query-processing for multidimensional data has been pursued for at least four decades. Its applications are many, and go far beyond the matching of multimedia descriptors, including sat-isfying multicriteria searches and spatial and spatio-temporal con-straints [24, 19, 12, 9, 23]. An exhaustive review is daunting, and beyond the scope of this text: for a comprehensive introduction and state of the art on the subject the reader is referred to [26, 27].
In multimedia we are almost always interested in approximate techniques. That is not only a result of the technical challenge of treating high dimensionalities; the information coded by the de-scriptors is always intrinsically approximate, so insisting on exact retrieval makes no sense. What is expected is a good trade-off be-tween precision and speed. But if perfect accuracy can be excused, the efficiency requirements remain very challenging: the method should perform well for high-dimensional data (hundreds of dimen-sions) in very large databases (at least millions of records); it must adapt well to secondary-memory storage, which in practice means that few random accesses should be performed; it should be dy-namic, i.e., allow data insertion and deletion without performance degradation.

Despite the huge assortment of methods available, those of prac-tical interest in the context of content-based multimedia services are surprisingly few. Many methods assume that the implementa-tion uses shared main memory (with cheap random access), other methods have prohibitively high index building times (with a forced rebuilding if the index changes too much), and so on.

MEDRANK is an interesting method, which projects the data into several random straight lines. The one-dimensional position on the line is used to index the data [11]. The method has an interesting theoretical analysis, establishing that under certain hy-potheses, rank aggregation on straight line projections offers some (lax) bounds on approximation error. The techniques employed by the algorithm were extremely well succeeded in moderately-dimensional multicriteria databases, for which it is still feasible to search for exact solutions. In those cases many of its choices are provably optimal [12].

For high-dimensional multimedia information, however, the technique fails, mainly due to the lack of correlation between the distance in the straight lines and the distance in the high-dimensional space [34]. This phenomenon is known as  X  X urse of dimensionality X  and expresses the difficulty in partitioning the data or the space in efficient ways when dimensionality is high [3, 37]. Image descriptors with 30 dimensions are commonplace, and those at around 100 are not unusual. Dimensionalities of 400 and above are not unheard of. For spaces like those, kNN search is extremely challenging.

An interesting family of solutions employs the fractal space-filling curves. Like MEDRANK, those methods reduce multidi-mensional indexing to one-dimensional indexing, but using more sophisticated projections [16, 28]. The method upon which we build our work, Multicurves, is one of those methods and it is ex-plained in detail in the next section.
The remainder of the text is organized as follows. In Section 2 we describe Multicurves in detail. In Section 3, the Anthill execu-tion model is discussed. Section 4 is devoted to Hypercurves and its scheme of parallelization. In Section 5 we present an important result: the proof of the probabilitistic equivalence of the proposed parallel algorithm to the original sequential version. In Section 6 we present an experimental evaluation of the proposed scheme in many stress scenarios. With Section 7, we conclude the article.
Multicurves [35, 36] is an index for data immersed in Hilbert spaces, whose properties make it especially adapted for large-scale multimedia descriptor databases. Multicurves is based on space-filling curves that are fractal curves introduced by Peano and Hilbert [25], which provide a continuous mapping from the unit interval [0; 1] to any unit hypercube [0; 1] d . Most space-filling curves are constructed by recursive procedures, where, in the limit, the curve fills the entire space. Though these curves are fascinat-ing by themselves, here we are interested in their ability to induce a  X  X icinity-sensitive X  total order to the data: the curve produces a total order that, locally and with high-probability, preserves the neighborhood relations (putting near in the curve points also near in the space). The biggest problem when using those curves is the existence of boundary regions where locality-preserving prop-erties are violated, and points close in space are placed far appart in the curve. This issue worsens significantly as dimensionality grows [16, 28]. Algorithm 1 Multicurves index construction 1: subindexes []  X  new array with curves empty sorted lists 2: for all p  X  points do 3: for c  X  1 to curves do 4: proj []  X  new array with dims [ c ] empty elements 8: Insert &lt; key, p &gt; into subindex [ curve ] ; 9: return subindexes []
Multicurves proposes to alleviate this issue by making each curve responsible for only a subset of the dimensions. Because of the exponential nature of the  X  X urse", it is more efficient to process several low dimensional queries than a single high-dimensional one. As we will see in Section 4.2, this structure also opens the op-portunity for parallelism, since querying the sub-indexes is mostly independent.

Multicurves index construction is simple: the data elements are divided among a certain number of space-filling curves. Each curve projects the data on a corresponding subspace and then computes their extended-keys (the one-dimensional position in the curve). The pairs &lt; extended-key, data element &gt; are placed in lists sorted by extended-key, one list per curve. Each list is a subindex (Al-gorithm 1). For ease of understanding, it is described as a  X  X atch X  algorithm, but nothing prevents the index from being built incre-mentally, if the structure used to implement the sorted list allows so.
 Algorithm 2 Multicurves search phase 1: best  X  X  X  2: for c  X  1 to curves do 3: proj []  X  new array with dims [ c ] empty elements 7: candidates  X  set of probe  X  depth points whose keys are 8: kLocal  X  the k points nearest to query f rom candidates 9: best  X  best  X  kLocal 10: knn  X  the k points from best nearest to query 11: return knn
The search is conceptually similar: the query is decomposed into projections (whose subspaces must be the same used during the in-dex construction) and each projection has its extended-key com-puted. Then, from each subindex, we obtain a certain number of candidate elements ( probe-depth ), whose extended-keys are the nearest to the extended-key of the corresponding projection of the query. At the end, we compute the actual distance from those ele-ments to the query and keep the k nearest (Algorithm 2).

The extended-key computation depends on the space-filling curve employed. For the Hilbert curve, the most efficient algorithm is [6]. The extended-key on a Z-order curve, which we employ in Hypercurves, is simpler to compute: it suffices to intercalate the bits of the dimensions.
Anthill [31, 32] is a flexible runtime system based on the dataflow model. As such, applications are decomposed into pro-cessing stages, called filters , which communicate with each other using unidirectional streams . The application is, therefore, de-scribed as a multi-graph representing the logical interconnections of the filters. At runtime, Anthill spawns on nodes of the cluster instances of each filter, called transparent copies. Anthill automati-cally handles runtime communication and state partitioning among those copies. A typical Filter-Stream application is shown in Fig-ure 1, where we emphasize the copy mechanism and the possibility of creating loops among filters.

The filter programming abstraction provided by Anthill is event-oriented, and derives heavily from the message-oriented program-ming model [22, 38]. The programmer provides functions that match input buffers from the multiple streams to a set of depen-dencies, creating event queues. Anthill will then take charge of the needed non-blocking I/O streaming.

The programmer also provides handling functions for each of the events on a filter that are automatically invoked by the run-time. Those events, in the dataflow model, amount to asynchronous and independent tasks. Since the filters are multithreaded, multi-ple tasks can be spawned provided there are pending events and computational resources. This feature not only is essential to fully exploit the capability of current multicore architectures, but also, in heterogeneous platforms, is used to spawn tasks on multiple de-vices. To accomplish that, Anthill allows the user to provide multi-ple handlers for the same event, with hints indicating which should be employed on each specific device.

The assignment of events to processors is demand-driven. Thus, when events are queued, they are not immediately assigned to a processor. Rather, this occurs on-demand as devices become avail-able. In the current implementation, the demand-driven, first-come, first-served (DDFCFS) task assignment policy is used as default strategy of the event scheduler.
The Hypercurves parallel execution strategy partitions the database among nodes of the distributed environment, while the search is performed locally in the subindexes managed by each node, and a reduction stage merges the results of those local searches. The cost of the algorithm is dominated by the local searches, which are further dependent on the probe-depth used (the number of candidates to retrieve from each subindex). When us-ing the same probe-depth of the sequential algorithm for each local index of the distributed environment, the answer of Hypercurves is guaranteed at least as good as the sequential algorithm. However, this is an extremely pessimistic and costly choice for the local in-dexes probe-depth: it can be shown that the quality of Hypercurves is equivalent to that of Multicurves with very high probability by using a probe-depth a slightly higher than taking the original probe-depth and dividing it by the number of nodes (See Section 5).
Therefore, the parallelization we propose exploits this property to partition that subindexes among the nodes in a distributed envi-ronment, avoiding data replication and consequently improving the scalability of the solution. The user can further modify the probe-depth of the parallel algorithm accordingly using Equation 4 (Sec-tion 5) to guarantee that the quality of Hypercurves is equivalent to that of Multicurves with desired probability.
Hypercurves consists of four filters, organized in two parallel computation pipelines, as shown in Figure 2. The first pipeline is conceptually an index builder/updater. Its first filter,  X  X nput Reader X  (IRR), reads the points from the input database and partitions them among the copies of the second filter,  X  X ndex Holder/Local Searcher X  (IHLS). The default partition function divides the points equally among IHLS copies, by using a round-robin policy. (It is opportune to highlight, though, that other partition policies may be performed, for providing fault-tolerance or for balancing the load when computing nodes are heterogeneous. We expect to evaluate those scenarios in future works.) The IHLS filter adds the points received from IRR to its local index, according to Algorithm 1. In-dex building and database reading are performed concurrently, as ensured by Anthill X  X  execution model. After the input is exhausted, IRR interacts with IHLS in case of updates to the database.
The second computation pipeline, which corresponds conceptu-ally to the search phase, contains three filters: (i) Query receiver (QR); (ii) IHLS (shared with the first pipeline); and (iii) Aggrega-tor. QR is the entry point to the search server, receiving the queries and broadcasting them to all IHLS copies, since queries must be processed by all index partitions. That broadcast strategy has lit-tle impact on performance, because the search itself dominates the execution time. Thus the communication latency is offset by the speedups on computation.

For each query, IHLS instances independently perform the search on their local index partition, retrieving k nearest local data points, according to sequential Multicurves (Algorithm 2).
The final answer to the query is obtained by reducing the k points local to each IHLS copy into k global nearest points. This operation is performed by the Aggregator filter. We allow for multiple con-current copies of this filter, performing reductions from different queries. It is, therefore, crucial that all messages related to some query be sent to the same copy of the Aggregator filter. That is guaranteed by making full use of Anthill Labeled-Stream commu-nication policy, which provides an affordable way to map specific message tags to the same copy of the receiver filter in a stream. The Aggregator filter, itself, receives k nearest points from each IHLS instance, calculates the k global nearest values and emits the query final response. The operation performed between IHLS and Aggre-gator is very similar to a generalized parallel data reduction [40], except that it outputs a list of values for each output, and that an arbitrary number of reductions are executed in parallel.

Hypercurves scheme exploits all the four dimensions of paral-lelism: task, data, pipeline, and intra-filter. Task parallelism occurs as filters of different pipelines are executed in parallel, e.g., QR and IRR, which concurrently perform index updates and searches. Data parallelism is achieved as the input database is partitioned among copies of IHLS filters. Pipeline parallelism is a result of Anthill X  X  ability to execute in parallel the filters which compose a single computational pipeline, e.g., IRR and IHLS for updating the index. The fourth dimension of parallelism (next section), refers to a single filter instance being able to process events in parallel. That provides the ability of efficiently exploiting multiple comput-ing devices available in modern multicore computers.
Inter-filter parallelism basically instantiates multiple copies of a sequential event handler. As a strategy, it is very convenient for the programmer, and very effective for many batch applications. How-ever, as the number of independent events falls below the number of computing cores, that  X  X mbarrassingly parallel X  approach fails to fully utilize the resources. In on-line applications like Hyper-curves, there is the additional constraint of the demand being based on users requests, and thus varying during execution.

To fully exploit symmetric multiprocessing (SMP) machines when the number of ready events is lower than the available CC (computing cores), we propose in this work the concept of parallel event handlers  X  PEH . Those event handlers consists of parallel processing functions, capable, thus, of using multiple processing cores for computing a single event. Parallel event handlers intro-duce the opportunity of employing all available computing cores (even if the number of ready events is low), but at the cost of ad-ditional intrinsic overhead brought by the finer grained parallelism. Therefore, their use must be judicious in order to extract the maxi-mum overall performance.
With the addition of parallel handlers, two intra-filter degrees of parallelism exist: (i) the event level degree of parallelism, EP , which refers to the number of events that a filter instance executes in parallel; and, (ii) the degree of internal parallelism, HP , which is the number of cores or threads used by a handler when execut-ing a single event. A parallelism strategy adopted by a filter copy (e.g. one of the instances of IHLS) is described by a set of pairs ( EP, HP ) , each pair defining the number of events EP using a particular amount of threads HP in their PEH. For instance, a IHLS copy could be executing 3 events concurrently: two of them using 1 thread each, and the third using 6 threads at once; that strat-egy would correspond to the set { (2 , 1) , (1 , 6) } .

The crucial point we want to emphasize again is that the paral-lelism of PEH is achieved at the cost of higher overheads, and thus, likely to be less scalable than the strategy using only event-level parallelism ( EP, 1) . Therefore, the PEH should be em-ployed judiciously in order to maximize application performance. To increase the challenge, the number of events ready to execute varies during the execution, and thus, the optimal intra-filter paral-lelism strategy changes dynamically. That problem is detailed and addressed in Sections 4.2.2 and 4.3.

Although crucial to fully exploit SMP machines, the parallelism of PEH requires extra effort from the programmers. Facilitating the development of SMP-based parallel codes is beyond the scope of this paper. However, it is a highly investigated subject, and much progress has been made on automatic parallelizing compilers [39], and other tools for parallel programming [8, 14, 15].
We have implemented HyperCurves X  own parallel event handler using OpenMP [8]. IHLS is Hypercurves X  most computational in-tensive filter, and, thus, an ideal candidate for having a parallel event handler. Though the filter contains two handlers, we have parallelized the search phase that refers to retrieve the k nearest local points for each query, since it is the only handler in IHLS affected by users X  behavior.

For this handler, the most expensive operations are examining each subindex/curve in order to retrieve the candidate points, and to reduce these candidates into k nearest points in that curve. This phase is parallelized in our approach, as the subindices are concur-rently examined by multiple threads . This parallel section repre-sents the lines 2 to 9 of the search phase shown in Algorithm 2. A small sequential section is still necessary in order to pre-aggregate the candidates from each thread to produce the k local points, that are then forwarded to the Aggregator filter (Line 10 from Algo-rithm 2). However, the cost associated with those local sequential operations is usually very low.
As mentioned before, Hypercurves poses a very interesting chal-lenge with respect to intra-filter parallelism: the number of events ready to execute within a filter varies during the execution, ac-cording to the user request demand. Therefore, any intra-filter parallelism configuration chosen prior to execution may result in suboptimal response times (See Figure 4). The relation between load/demand (given by query rate ) and optimal parallelism strategy is affected by complex factors, including hardware architecture and application parameters. Optimizing response times under variable workloads is, therefore, beyond the capabilities of static tuning.
The problem of selecting the adequate values for each paral-lelism level corresponds to defining a set Z of pairs ( EP that minimizes the average response time for events created in a filter. To further constraint the optimization, we assume an 1:1 cor-respondence between the HP parallelism employed in Z and the available CC (computing cores). That corresponds to the following constraint:
When this constraint is too restrictive, omitting sets Z with better performance (e.g., unscalable PEH) the programmer can handcraft another value to be used in the place of CC in the equation above. We believe, however, that the 1:1 assumption to be reasonable when targeting SMP machines.

More importantly, the formulation assumes different optimal so-lutions throughout the execution as the load varies. The algo-rithm proposed, called Dynamic Selection of Parallelism Degrees  X  DSPD , is independently executed by each filter instance at run-time. Its rationale is to minimize query response time = event queued time + event processing time . Therefore, our choice is to execute the highest possible number of events in parallel, in-creasing EP and reducing HP  X  this also tends to minimize the queued time of events and improve throughput. However, when the number of ready events is not enough to use all available CC , HP is increased to use multiple devices for a single event. In that scenario, event processing time is reduced, and an even better re-sponse time is delivered. In all cases, the scheduler works to min-imize queuing time and response time (Algorithm 3). It runs dur-ing the execution every time that an I/O operation is performed (in background to the events processing), and the computed Z is used thereafter by that filter instance.
 Algorithm 3 Dynamic selection of parallelism degrees  X  DSPD 1: EP  X  CC, HP  X  1 2: while not EndOfWork do 3: if EP &lt; CC and GetNumberReadyEvents () &gt; 0 4: EP  X  Min ( CC, GetNumberReadyEvents ()) 5: else if GetNumberIdleThreads () &gt; 0 then 6: EP  X  Max (1 , EP  X  GetNumberIdleThreads ()) 7: HP  X  X  CC  X  EP c 8: if EP  X  HP = CC then 9: Z  X  X  ( EP, HP ) } 10: else 11: idleCores  X  CC  X  EP  X  HP 12: Z  X  X  ( idleCores, HP + 1) } 13: Z  X  Z  X  X  ( EP  X  idleCores, HP ) }
First step, DSPD checks whether there are events ready to exe-cute. The outer, event parallelism EP is then kept as large as pos-sible, in the limit of the number of waiting events and computing cores, in order to empty the queue as fast as possible (lines 3 X 5). If there are no events queued and available resources, a higher HP is employed to exploit them (lines 5 X 7). Finally, the parallelism strat-egy is set as one pair (if the product EP  X  HP fully occupies all cores) or two pairs (if a single  X  X ecipe X  would leave idle cores) of parallelism strategies. Thus, all CC are employed and a minimum number of threads is used within each PEH.

Defining the right parallelism granularity is a complex problem, which has been studied for a long time, and has many applica-tions, for instance, in the context of parallel nested loops [7]. Most works on this area, however, focus on applying loop transforma-tions according to the available resources in order to adjust the par-allelism granularity, aiming at gaining parallelism with reasonable synchronization overhead. However, the best parallelism config-uration may be beyond any static tuning, as discussed. That has motived several recent works, which focus on runtime transforma-tions [1, 2, 10]. Those interesting works aim at parallelism tuning for a lower level and are complementary to the strategy we propose. Though they do not address the needs of adjusting application-level parallelism in changing working loads, they could be applied inter-nally to a parallel event handler.
The advances in multicore systems have turned traditional SMP machines into hierarchical computing environments. As a result, current processors are built using several levels of cache, with dif-ferent interconnections among cores ( Non Uniform Memory Access  X  NUMA ), and Symmetric Multithreaded Processors (branded Hy-perThreading by Intel), are also available. To exploit those archi-tectures, a deep analysis is required when scheduling computations ( threads ).

In Filter-Stream, thread assignment refers to the problem of choosing the computation cores employed when executing a paral-lel event handler, i.e., the assignment of each pair ( EP to the available computing cores. The challenge is heightened as DSPD changes dynamically the value of Z , requiring the place-ment to be recomputed.

Events ready to execute are independent tasks in the program-ming model and, as such, can be dispatched for execution in any order, without assumptions about co-scheduling. There is no com-munication between events, but only among groups of threads HP processing the same event. Thus, the placement of those threads should minimize their communication cost. However, threads of the same group are likely to execute similar operations (e.g., I/O followed by processing, etc.) in the same sequence, and thus are expected to request the same resources concurrently. That makes them bad candidates to employ virtual cores mapped to the same physical core (i.e., HyperThreaded cores), which achieve maxi-mum performance for processes with different requirements [30]. In summary, the proposed solution is based on the following as-pects to optimize: (i) distance among cores used by a PEH, ac-cording to memory hierarchy (cache and NUMA interconnections); and, (ii) load imbalance among cores, and better use of SMP. Algorithm 4 Architecture-aware thread placement  X  TP 1: for all ( EP i , HP i )  X  Z  X  sorted by HP do 2: { For all handlers using HP i threads} 3: for x  X  1 to | EP i | do 4: coresSet x  X  FindClosestCores( HP i )
As mentioned above, this optimization is time critical, since it is performed at run time and repeated after every modification in Z . Therefore, we use a lightweight greedy algorithm presented in Figure 4. It first sorts the pairs ( EP i , HP i ) in descending order of HP i . It then places the sets in this descending order: the goal is to benefit handlers with higher number of threads and communi-cation/synchronization costs. The choice of the adequate cores for each HP i is further executed by the FindClosestCores , which first creates a tree based representation of the computing cores, ac-cording to the memory level shared among them, using hwloc [5]. Thereafter, the mapping is made by navigating through the tree in a preorder basis, and assigning the closest set of cores according to the memory hierarchy for each call. The cores are then marked as used, preventing the algorithm from revisiting this tree branch during the next search. Moreover, when HyperThreading is avail-able, we avoid employing the same physical core for an HP thread group, unless no other placement is available. By placing threads of different groups on the same physical core we intend to exploit tem-poral variability on resource demands among those threads. Our strategy differs from traditional approaches where processes that use different resources are executed in the same physical cores [30]. Instead, we exploit the temporal aspect to co-schedule similar com-putations with time varying demands. In the PEH used by IHLS the demand varies as it has an I/O intensive stage followed by a compute intensive phase. The first phase consists of consulting the subindexes, while the second refers to computing the distance of the candidates returned from the subindexes to the query. Hypercurves was created by adapting the original sequential Multicurves algorithm to the Filter-Stream programming model implemented by Anthill. As seen in the previous sections, that task is far from trivial and required (i) identifying, as filter candidates, sections of the code at once computational intensive, logically con-cise; (ii) refining the filters boundaries by trading off parallelism and communication costs; (iii) further trade-offs between paral-lelism opportunities and degree of precision. The first two steps are general to most dataflow languages and runtime systems [13]. The latter step is particular to the approximate nature of the prob-lem in hand.

Multicurves is based upon the ability of space-filling curves giv-ing a total order to data. This is exploited on each subindex (Al-gorithm 2), where a number of candidates is retrieved from each sorted list. Those candidates are, then, measured against the query and the k nearest are kept. In Hypercurves, the index is further frag-mented, and each IHLS filter instance has available only a subset of the database: a single filter cannot warrant the equivalent approxi-mate k nearest neighbors. That is exactly the role of the Aggregator filter: collecting multiple local best neighbors and returning a re-fined answer set. Note that in terms of equivalence between Multi-and Hypercurves it matters little how the candidates are distributed among the IHLS instances ( i.e., whether the "right" candidates end up in a single filter copy or whether they are dispersed). Because both the local aggregation at the end of IHLS and the final Aggre-gator filter consider all the candidates retrieved so far, they cannot result in a loss of precision.

The only way for both Multi-and Hypercurves to miss a correct answer in the kNN set is failing to retrieve it from the subindexes. In that sense Hypercurves can be made guaranteedly equivalent to Multicurves by employing on each filter copy a probe-depth equiv-alent to the one used in the sequential Multicurves. That way, no candidates are ever missed, but this is an extremely pessimistic choice.

Consider the same dataset, either in a Multicurves X  subindex with probe-depth = 2  X   X  , or partitioned among ` Hypercurves X  IHLS filter copies, each with probe-depth = 2  X   X  (even probe-depths make the analyses more symmetric, but the argument is essentially the same for odd values). Both on Multi-and Hypercurves each subindex is conceptually a sorted list. For any query, the candi-dates that would be in a single sorted list in Multicurves are now distributed among ` sorted lists in Hypercurves.
 The problem can be restated, in more general terms, as follows. We start with a single sorted list and retrieve the 2  X   X  elements closest to a query point. If we distribute randomly that single sorted list into ` sorted lists, how many elements must we retrieve from each of those new lists (i.e. which value for 2  X   X  must we employ) to ensure that none of the originally retrieved elements is missed.
Note that: (i) due to the sorted nature of the list, the elements before the query cannot exchange positions with the elements after the query; (ii) no element of the original list can be lost as long as all those 2  X  `  X  X alf-lists X  are shorter than  X  .
 Due to (i), we can analyze each half of the list independently. The distribution of the elements among the ` lists, is given by an Multinomial distribution with  X  trials and all probabilities equal to `  X  1 . The exact probability of no list being longer than  X  in-volves computing a truncated part of the distribution, but the exact formulas are exceedingly complex and little elucidative. We can, however, bound it from below [20] with:
Where List i is an arbitrary single component of the equiprob-able Multinomial, which, by construction has a Binomial distribu-tion for  X  trials and success rate of `  X  1 . Thus, the probability of any miss on any of the 2  X  ` half-lists is bounded from above by: 1  X  Max
Where I () is the regularized incomplete Beta function. This probability tends to zero for very reasonable values of  X  , still much lower than  X  . This is more easily seen if we make  X  = (1 +  X  ) d  X  /` e , i.e., if we "distribute" the probe-depth among the filters, adding a "slack factor" of  X  . For all reasonable scenarios, the probability tends to zero very fast, even for small  X  (Figure 3). Figure 3: Equivalence between sequential Multicurves with a probe-depth of 2  X   X  = 256 and parallel Hypercurves with distributed probe-depth of 2  X   X  , with  X  = (1 +  X  ) d  X  /` e and ` = the number of filter copies. The probability of missing any of the candidate points drop sharply to zero for very low values of  X  . For the large initial probe-depths actually employed in our experiments ( 2  X   X  = 6000 ), the convergence to zero is even faster. The experiments were performed using two setups of machines. The first setup was composed of 2 PCs connected with Gigabit Ethernet, each with two quad-core AMD Opteron 2.0 GHz pro-cessors and 16 GB of main memory. The second setup was an 8-node cluster connected through Gigabit Ethernet, each node be-ing a PC with two quad-core hyperthreaded Intel Xeon processors E5620 and 12 GB of main memory. All machines run Linux.

The main dataset used to evaluate our algorithm contained 233,852  X  X ackground X  images from Web, and 225  X  X oreground X  images from our personal collections  X  the foreground images were used to compute sets of descriptors that must be matched, while the background images were used to compute descriptors added to confound the method. The descriptors were computed us-ing SIFT [18], summing up to 130,463,526 local descriptors with 128 dimensions. Due to the number of evaluations performed, we have also employed smaller partitions of that main database, in or-der to achieve feasible experimentation run times. The experiments were executed at least three times with standard deviation smaller than 3%.

The experiments concentrate on issues of efficiency, since, as demonstrated in Section 5, Hypercurves, with very high probabil-ity, returns the same results of Multicurves. Thus, by construction, it inherits the good trade-off between precision and speed of Mul-ticurves [36].
In this section, we evaluate the intra-filter parallelism scalabil-ity, as a function of the number of threads. For both node setups, we compare the (1 , HP ) strategy (only one event at once, with an increasing number of threads employed internally by the parallel event handler) and the ( EP, 1) strategy (only one thread per event, with an increasing number of parallel events). The evaluation em-ployed a small subsample of 150,000 descriptors randomly selected from the main database and 1,000 queries. The small dataset is appropriate for this evaluation, since it provides shorter execution times, and, thus, highlights existing overheads. For those experi-ments, all queries are sent to the filter QR in the beginning of the execution, and only one copy of the filter IHLS is running in a ded-icated machine, while the other filters employ a second node. Table 1: Execution times of two extreme parallelism regimens: only internal parallelism (1,HP) and only event parallelism (EP,1). Note that the latter scales better.

Table 1 shows Hypercurves execution times as the number of threads increases. Both configurations achieve performance gains as the number of threads grows, but, as expected, the configuration with only event-level parallelism ( EP, 1) scales better, primarily due to the absence of thread communication / synchronization costs inside the event handler.

The experiments in Table 1 (b) also evaluate the impact of In-tel Hyperthreading (which is present from 8 cores/threads and on-wards) on both configurations. The configuration ( EP, 1) per-formed consistently better than (1 , HP ) , as expected, but the dif-ference becomes more visible for the hyperthreading scenario: ( EP, 1) is 14% faster than (1 , HP ) when using 16 hyperthreaded cores, and it has an speedup of 1 . 3  X  when compared to the config-uration with 8 non-hyperthreaded cores. The causes of the better gains of event-only parallelism in the hyperthreading scenario are (i) the lesser imbalance among threads; and, (ii) the independence of threads mapped to the same physical core (since they are pro-cessing different events). The fact the threads are likely to be in different phases of the computation, and thus employing distinct resources in time, is crucial to exploit hyperthreading.
Here, we have evaluated query response times according to the query rate submitted. We have used the same database and queries of the last section. The configurations of parallelism employed are those where all CC are used on a single ( EP, HP ) pair (i.e. there is a single configuration pair, with EP  X  HP = CC ). The query rate is varied in the interval of 10-120% of the maximum query rate that Hypercurves is able to answer. The maximum rate was calculated in previous executions, considering the configuration ( EP, 1) , and all queries sent to execution at the beginning (those values are 45 and 138, respectively, for the 1st and 2nd setups). Figure 4: Average query response times as a function of load (query rates) for different intra-filter parallelism configura-tions, on both machine setups. No static configuration is con-sistently optimal throughout the load variation.
 In Figure 4, we present the average query response times for Hypercurves in both setups. The performance of the DSPD algo-rithm is depicted in this figure, for comparison purposes, but we will discuss it on the next section. The results for the 1st setup, Figure 4(a), show that none of the static configuration delivers the minimum query response times as the query rate varies: for in-stance, the (8 , 1) configuration, which provides the best throughput with a heavy load  X  query rate, is about 5  X  slower than the con-figuration (1 , 8) , when the load is light. It is interesting to note that each static configuration has the best response times for at least one load. The same set of experiments, executed on the 2nd setup, is shown on Figure 4(b). Again, the best intra-filter parallelism con-figuration varies according to the query rate. Moreover, for those experiments, the configuration that maximizes throughput (16 , 1) has response times up to 7  X  worse than (2 , 8) , for query rates lower than 90% of the maximum.

The experiments on both setups show that there is no obvious tradeoff to define the best configuration value. In addition, the choice should depend not only on the query rate (as shown), but also on the input data characteristics and application parameters. Thus, any static tuning is likely to achieve suboptimal performance if those factors change. These aspects motivated the auto-tuning algorithm, as proposed in Section 4.3
This section evaluates the auto-tuning algorithm  X  DSPD. The first set of experiments, shown in Figure 4, present DSPD perfor-mance for static loads (each experimental point has a fixed query rate throughout execution). The initial DSPD configuration in all experiments is ( EP, 1) , where EP is the number of cores used. Table 2: Average query response times (in s ) for static and auto-tuned parallelism configuration under stochastic loads.

DSPD was able to adapt the parallelism, keeping the average response near to the best static configurations in nearly all cases. Moreover, it is able to surpass, on average, the best static configura-tions for all query rates (78% and 12% improvement, respectively, for 1st and 2nd setups).

We also contrast DSPD to the static parallelism configurations on runs whose query rate varies stochastically during the execution. In those experiments, the load follows a Poisson distribution. These results are presented in Table 2. Again, DSPD performed very well, delivering an equivalent or best performance on almost all cases. For both setups, a static configuration achieved better results when a high load is employed (Poisson average equals to maximum query rate), but the difference is only significant for the second setup. That phenomenon is mainly due to the flatter speedup curves of the intra-filter parallelism, caused by the non-linear speedup achieved by employing virtual cores available with the Hyperthreading mechanism, making the (8 , 2) configuration a good choice up to very heavy loads. DSPD, however, starts migrating to intermediary configurations from (8 , 2) to (1 , 16) , before the last becomes the best parallelism configuration. The thread placement algorithm is proposed as an extension to DSPD (DSPD + TP). We start comparing DSPD and DSPD+TP un-der fixed query rates (Figure 4). On those experiments, DSPD+TP performs very similarly or superpasses DSPD, for both machine se-tups. The average gains in query response times over DSPD were: 6.45% on the first setup, and 19.3% on the 2nd setup.

The improvements on the 1st setup (2-node AMD) are purely due to faster communication among threads placed with the DSPD+TP. The benefits observed on the 2nd setup (8-node Intel), however, are even more exciting. In this case, most of the gains are due to Hyperthreading-aware placement. When the same evaluation is performed without considering the existence of this mechanism the gains are reduced to about 7%.

For stochastic, time-varying loads, the results are shown on Ta-ble 2. Here, DSPD+TP achieves even better results, being equal or better than any other configuration for all runs, with average im-provement of 43% and 74%, respectively, for the 1st and 2nd setups when compared to the best static parallelism regimens.
The distributed memory analysis in this section has focused in evaluating Hypercurves speedup and scaleup. We consider the compromises between parallelism performance and quality preser-vation, as data is partitioned among the computing nodes. The query rate delivered by the algorithm considers two parameteriza-tion scenarios named Optimist and Pessimist (Table 3), which differ in their guarantees of equivalence (in terms of precision of the kNN search) to the sequential Multicurves algorithm.

The Optimist parameterization divides the probe-depth equally among the nodes, without any slack  X  it will only be equivalent to Multicurves in the unlikely case that all candidates of that query are equally distributed among the nodes. The Pessimist parameter-ization uses a slack such to guarantee that a point encountered in probe-depth of the sequential algorithm may not be in the probe-depth of the distributed version with probability smaller than 1% (the proof details are on Section 5). Note that this choice is still extremely conservative, because, in order to effectively affect the answer, the missed points from the candidate set have to be among the actual top k set, and k is much smaller than the probe-depth.
For those experiments, 1/8 of the database is used, such that it occupies 96% of the main memory in each machine. As presented in Table 3, the speedup of the Optimist case is nearly linear in the number of nodes employed. A maximum of 38,932 queries per minute are answered when the entire cluster of the 2nd setup is em-ployed. The Pessimist case also achieves a near to linear speedup in the number of nodes, but with a smaller factor as it performs extra computation due to the slack added to guarantee the quality.
The Optimist and Pessimist cases are interesting, since they may be considered extreme choices, and taken together, they reflect well the upper and lower bounds on speedup. They enclose a number of reasonable configurations, where the trade-off between perfor-mance and quality of the answer can be examined to define the appropriated  X  X lack X  in the choice of probe-depth.

We also investigate how the use of more machines allows Hy-percurves to process larger datasets. In Table 4, we present the number of queries answered per minute by Hypercurves during that scaleup evaluation. For this experiment we also added a slack to the probe-depth such that we guarantee that with a probabil-ity smaller than 1% a point in the probe-depth of the sequential execution will not be in probe-depth of the distributed execution. Those experiments started with 1 computing node, using the 16 vir-tual cores available, and 1/8 of the main database. The number of machines and the database fraction were increased proportionally up to the configuration with 8 nodes and the entire input database (130,463,526 descriptors). The performance of the Hypercurves in this scenario is even more exciting as it achieved superlinear scaleup . The performance of the algorithm is impressive in scaleup experiments because it is only affected by the number of points in the dataset when it is retrieving the probe-depth points from the curves or subindexes, and the cost of this phase grows logarithmic with the size of the database. The most costly phase refers to com-puting the distance of the query to the probe-depth points. How-ever, the probabilistic correctness (Section 5) allows the distance computation costs to be efficiently distributed among the nodes, as the probe-depth of the sequential executions is divided by the num-ber of nodes and increased by a small slack.
This work evaluated the similarity search problem on very large databases, focusing on the aspects that arise when it is employed in the context of online search. We have proposed a parallel self-adaptable approximate search algorithm  X  Hypercurves, which was implemented utilizing the Filter-Stream programming model and is based on space-filling curves concepts. Hypercurves has been designed in order to fully exploit massively parallel machines under variable load. Thus, it can self-adapt the types of parallelism employed within its computation stages, in response to variations on query request rate submitted, to minimize response times.
Also, as a future work, we intend to examine the performance of Hypercurves in heterogeneous clusters and grid environments. An important extension to this algorithm would be the use of ac-celerators (e.g. GPUs), in order to further improve its performance. The use of GPUs is very challenging with respect to the online de-mands of Hypercurves, as this type of processor is normally used for throughput oriented computations.
The authors would like to express their gratitude to the reviewers of our paper for their valuable comments. This work is partially supported by CNPq, Capes, Fapemig, InWeb -Brazilian INCT for the Web, and FAPESP (project 2009/05951-8).
