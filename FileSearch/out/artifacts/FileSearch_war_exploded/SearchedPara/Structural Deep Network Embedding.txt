 Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and pre-serve the network structure. Almost all the existing network em-bedding methods adopt shallow models. However, since the under-lying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet impor-tant problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE . More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to p-reserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network struc-ture. While the first-order proximity is used as the supervised infor-mation in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruc-t the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link pre-diction and visualization.
 Network Embedding, Deep Learning, Network Analysis
Nowadays, networks are ubiquitous and many real-world appli-cations need to mine the information within these networks. For example, recommendation system in Twitter aims to mine the pre-ferred tweets for users from the social network. Online advertise-ment targeting often needs to cluster the users into communities in the social network. Therefore, mining the information in the net-work is very important. One of the fundamental problems is how to learn useful network representations [5]. An effective way is to embed networks into a low-dimensional space, i.e. learn vec-tor representations for each vertex, with the goal of reconstructing the network in the learned embedding space. As a result, mining information in networks, such as information retrieval [34], classi-fication [15], and clustering [20], can be directly conducted in the low-dimensional space.

Learning network representations faces the following great chal-lenges: (1) High non-linearity : As [19] stated, the underlying structure of the network is highly non-linear. Therefore, how to design a model to capture the highly non-linear structure is rather difficult. (2) Structure-preserving : To support applications an-alyzing networks, network embedding is required to preserve the network structure. However, the underlying structure of the net-work is very complex [24]. The similarity of vertexes is dependent on both the local and global network structure. Therefore, how to simultaneously preserve the local and global structure is a tough problem. (3) Sparsity : Many real-world networks are often so s-parse that only utilizing the very limited observed links is not e-nough to reach a satisfactory performance [21].

In the past decades, many network embedding methods have been proposed, which adopted shallow models, such as IsoMAP [29], Laplacian Eigenmaps (LE) [1] and Line [26]. However, due to the limited representation ability of shallow models [2], it is d-ifficult for them to capture the highly nonlinear network structure [30]. Although some methods adopt kernel techniques [32], as [36] stated, kernel methods are also shallow models and cannot capture the highly non-linear structure well.

In order to capture the highly non-linear structure well, in this paper we propose a new deep model to learn vertex representations for networks. This is motivated by the recent success of deep learn-ing, which has been demonstrated to have a powerful representation ability to learn complex structures of the data [2] and has achieved substantial success in dealing with images [15], text [25] and audio [10] data. In particular, in our proposed model we design a multi-layer architecture which consists of multiple non-linear functions. The composition of multiple layers of non-linear functions can map the data into a highly non-linear latent space, thereby being able to capture the highly non-linear network structure.

In order to address the structure-preserving and sparsity prob-lems in the deep model, we further propose to exploit the first-order and second-order proximity [26] jointly into the learning process. The first-order proximity is the local pairwise similarity only be-tween the vertexes linked by edges, which characterizes the local network structure. However, due to the sparsity of the network, Figure 1: The number of pairs of vertexes which have first-order and second-order proximity in different datasets. many legitimate links are missing. As a result, the first-order prox-imity is not sufficient to represent the network structure. Therefore, we further propose the second-order proximity, which indicates the similarity of the vertexes X  neighborhood structures, to capture the global network structure. With the first-order and second-order proximity, we can well characterize the local and global network structure, respectively. To preserve both the local and global net-work structure in the deep model, we propose a semi-supervised architecture, in which the unsupervised component reconstructs the second-order proximity to preserve the global network structure while the supervised component exploits the first-order proximity as the supervised information to preserve the local network struc-ture. As a result, the learned representations can well preserve both the local and global network structure. In addition, as shown in Figure 1, the number of pairs of vertexes which have second-order proximity is much huger than those have first-order proximi-ty. Therefore, the import of second-order proximity is able to pro-vide much more information in term of characterizing the network structure. As a result, our method is robust to sparse networks.
Empirically, we conduct the experiments on five real-world net-worked datasets and four real-world applications. The results show that compared with baselines, the representations generated by our method can reconstruct the original networks significantly better and achieve substantial gains on various tasks and various network-s, including very sparse networks. It demonstrates that our repre-sentations learned in the highly non-linear space can preserve the network structure well and are robust to sparse networks.
In summary, the contributions of this paper are listed as follows:
Representation learning has long been an important problem of machine learning and many works aim at learning representations for samples [3, 35]. Recent advances in deep neural networks have witnessed that they have powerful representations abilities [12] and can generate very useful representations for many types of data. For example, [15] proposed a seven-layer convolutional neural network to generate image representations for classification. [33] proposed a multimodal deep model to learn image-text unified representa-tions to achieve cross-modality retrieval task.

However, to the best of our knowledge, there have been few deep learning works handling networks, especially learning network rep-resentations. In [9], Restricted Boltzmann Machines were adopted to do collaborative filtering. [30] adopted deep autoencoder to do graph clustering. [5] proposed a heterogeneous deep model to do heterogeneous data embedding. We differ from these works in t-wo aspects. Firstly, the goals are different. Our work focuses on learning low-dimensional structure-preserved network representa-tions which can be utilized among tasks. Secondly, we consider both the first-order and second-order proximity between vertexes to preserve the local and global network structure. But they only focus on one-order information.
Our work solves the problem of network embedding, which aims to learn representations for networks. Some earlier works like Lo-cal Linear Embedding (LLE) [22], IsoMAP [29] first constructed the affinity graph based on the feature vectors and then solved the leading eigenvectors as the network representations. More recent-ly, [26] designed two loss functions attempting to capture the local and global network structure respectively. Furthermore, [4] extend-ed the work to utilize high-order information. Despite the success of these network embedding approaches, they all adopt shallow models. As we have explained earlier, it is difficult for shallow models to effectively capture the highly non-linear structure in the underlying network. In addition, although some of them attempt to use first-order and high-order proximity to preserve the local and global network structure, they learn the representations for them separately and simply concatenate the representations. Obviously, it is sub-optimal than simultaneously modeling them in a unified architecture to capture both the local and global network structure.
DeepWalk [21] combined random walk and skip-gram to learn network representations. Although empirically effective, it lacks a clear objective function to articulate how to preserve the network structure. It is prone to preserving only the second-order proximity. However, our method designs an explicit objective function, which aims at simultaneously preserving the local and global structure by preserving both the first-order and second-order proximity.
In this section, we first define the problem. Then we introduce the proposed semi-supervised deep model of SDNE . At last we present some discussions and analysis on the model. We first give the definition of a Graph.

D EFINITION 1. (Graph) A graph is denoted as G = ( V,E ) , where V = { v 1 ,...,v n } represents n vertexes and E = { e represents the edges. Each edge e i,j is associated with a weight s i,j  X  0 1 . For v i and v j not linked by an edge, s i,j erwise, for unweighted graph s i,j = 1 and for weighted graph, s i,j &gt; 0 .

Network embedding aims to map the graph data into a low-dimensional latent space, where each vertex is represented as a low-dimensional vector and the network computing can be direct-ly realized. As we have explained, both local and global structure are essential to be preserved. Then we first define the first-order proximity, which characterizes the local network structure.
D EFINITION 2. (First-Order Proximity) The first-order prox-imity describes the pairwise proximity between vertexes. For any pair of vertexes, if s i,j &gt; 0 , there exists positive first-order proxim-ity between v i and v j . Otherwise, the first-order proximity between v and v j is 0 .

Naturally, it is necessary for network embedding to preserve the first-order proximity because it implies that two vertexes in real-world networks are always similar if they are linked by an observed edge. For example, if a paper cites another paper, they should con-tain some common topic. However, real-world datasets are often so sparse that the observed links only account for a small portion. There exist many vertexes which are similar with each other but not linked by any edges. Therefore, only capturing the first-order prox-imity is not sufficient. We introduce the second-order proximity to capture the global network structure.

D EFINITION 3. (Second-Order Proximity) The second-order prox-imity between a pair of vertexes describes the proximity of the pair X  X  neighborhood structure. Let N u = { s u, 1 ,...,s u, | V | first-order proximity between v u and other vertexes. Then, second-order proximity is determined by the similarity of N u and N
Intuitively, the second-order proximity assumes that if two ver-texes share many common neighbors, they tend to be similar. Such an assumption has been proved reasonable in many fields [6, 14]. For example, in linguistics words will be similar if they are always surrounded by similar contexts [6]. People will be friends if they have many common friends [14]. The second-order proximity has been demonstrated to be a good metric to define the similarity of a pair of vertexes, even if they are not linked by an edge [17], and thus can highly enrich the relationship of vertexes. Therefore, by introducing the second-order proximity, it is able to characterize the global network structure and alleviate the sparsity problem.
With the first-order and second-order proximity, we investigate the problem of how to integrate them simultaneously to preserve both the local and global structure when we perform network em-bedding. Such a problem is defined as follows:
D EFINITION 4. (Network Embedding) Given a graph denoted as G = ( V,E ) , network embedding aims to learn a mapping func-tion f : v i 7 X  X  X  y i  X  R d , where d | V | . The objective of the function is to make the similarity between y i and y j explicitly pre-serve the first-order and second-order proximity of v i and v
In this paper, we propose a semi-supervised deep model to per-form network embedding, whose framework is shown in Figure 2. In detail, to capture the highly non-linear network structure, we propose a deep architecture, which is composed of multiple non-linear mapping functions to map the input data to a highly non-linear latent space to capture the network structure. Furthermore, in
For signed network, negative links exist. But in this paper we only consider non-negative links. Figure 2: The framework of the semi-supervised deep model of SDNE order to address the structure-preserving and sparsity problems, we propose a semi-supervised model to exploit both the second-order and first-order proximity. For each vertex, we are able to obtain its neighborhood. Accordingly, we design the unsupervised com-ponent to preserve the second-order proximity, by reconstructing the neighborhood structure of each vertex. Meanwhile, for a small portion of pairs of nodes, we can obtain their pairwise similarities, i.e. the first-order proximities. Therefore, we design the super-vised component to exploit the first-order proximity as the super-vised information to refine the representations in the latent space. By jointly optimizing them in the proposed semi-supervised deep model, SDNE can preserve the highly-nonlinear local-global net-work structure well and is robust to sparse networks. In the follow-ing section, we will introduce how to realize the semi-supervised deep model in detail.
Before introducing the loss functions, we define some of the terms and notations in Table 1 which will be used later. Note that  X  above the parameters represents the parameters of the decoder.
Now we introduce the loss functions for the semi-supervised model. We first describe how the unsupervised component exploits the second-order proximity to preserve the global network struc-ture.

The second-order proximity refers to how similar the neighbor-hood structure of a pair of vertexes is. Thus, to model the second-order proximity, it is required to model the neighborhood of each vertex. Given a network G = ( V,E ) , we can obtain its adjacency matrix S , which contains n instances s 1 ,..., s n . For each instance s = { s i,j } n j =1 , s i,j &gt; 0 if and only if there exists a link between v and v j . Therefore, s i describes the neighborhood structure of the vertex v i and S provides the information of the neighborhood structure of each vertex. With S , we extend the traditional deep autoencoder [23] to preserve the second-order proximity.
For the consideration of being self-contained, we briefly review the key idea of deep autoencoder. It is an unsupervised model which is composed of two parts, i.e. the encoder and decoder. The encoder consists of multiple non-linear functions that map the in-put data to the representation space. The decoder also consists of multiple non-linear functions mapping the representations in rep-resentation space to reconstruction space. Then given the input x the hidden representations for each layer are shown as follows
After obtaining y ( K ) i , we can obtain the output  X  x i the calculation process of encoder. The goal of the autoencoder is to minimize the reconstruction error of the output and the input. The loss function is shown as follows:
As [23] proved, although minimizing the reconstruction loss does not explicitly preserve the similarity between samples, the recon-struction criterion can smoothly capture the data manifolds and thus preserve the similarity between samples. Then considering our case that if we use the adjacency matrix S as the input to the autoencoder, i.e. x i = s i , since each instance s i characterizes the neighborhood structure of the vertex v i , the reconstruction process will make the vertexes which have similar neighborhood structures have similar latent representations.

Nevertheless, such a reconstruction process cannot be directly applied to our problem because of some specific characteristics of networks. In the networks, we can observe some links but simulta-neously many legitimate links are not observed, which means that the links between vertexes do indicate their similarity but no links do not necessarily indicate their dissimilarity. Moreover, due to the sparsity of networks, the number of non-zero elements in S is far less than that of zero elements. Then if we directly use S as the input to the traditional autoencoder, it is more prone to reconstruct the zero elements in S . However, this is not what we want. To address this problem, we impose more penalty to the reconstruc-tion error of the non-zero elements than that of zero elements. The revised objective function is shown as follows: where means the Hadamard product, b i = { b i,j } n j =1 . If s 0 , b i,j = 1 , else b i,j =  X  &gt; 1 . Now by using the revised deep au-toencoder with the adjacency matrix S as input, the vertexes which have similar neighborhood structure will be mapped near in the representations space, guaranteed by the reconstruction criterion. In other words, the unsupervised component of our model can p-reserve the global network structure by reconstructing the second-order proximity between vertexes.

It is not only necessary to preserve the global network structure, but also essential to capture the local structure. We use the first-order proximity to denote the local network structure. The first-order proximity can be regarded as the supervised information to constrain the similarity of the latent representations of a pair of vertexes. Therefore, we design the supervised component to exploit
In this work, we use the sigmoid function  X  ( x ) = 1 1+ exp (  X  x ) the non-linear activation function the first-order proximity. The loss function for this goal is defined as follows 3 : The objective function of Eq. 4 borrows the idea of Laplacian Eigenmaps [1], which incurs a penalty when similar vertexes are mapped far away in the embedding space. Some works about social networks [13] also use the similar idea. We differentiate them in the aspect that we incorporate the idea in the deep model to make the vertexes linked by an edge be mapped near in the embedding space. As a result, the model preserves the first-order proximity.
To preserve the first-order and second-order proximity simulta-neously, we propose a semi-supervised model, which combines Eq. 4 and Eq. 3 and joint minimizes the following objective function: L where L reg is an L 2-norm regularizer term to prevent overfitting, which is defined as follows: To optimize the aforementioned model, the goal is to minimize L mix as a function of  X  . In detail, the key step is to calculate the partial derivative of  X  L mix / X   X  W ( k ) and  X  L mix / X  X  tailed mathematical form of the partial derivative is shown as fol-lows:
We first look at  X  L 2 nd / X   X  W ( K ) . It can be rephrased as follows:
For the first term, according to Eq. 3 we have:
The calculation of the second term  X   X  X/ X   X  W is easy since  X  (  X  on back-propagation, we can iteratively obtain  X  L 2 nd / X  1 ,...K  X  1 and  X  L 2 nd / X  X  ( k ) ,k = 1 ,...K . Now the calculation of the partial derivative of L 2 nd is finished.

Then we continue to calculate the partial derivative of  X  L The loss function of L 1 st can be rephrased as follows: For simplicity of notations, we denote network representations Y where L = D  X  S , D  X  R n  X  n is a diagonal matrix, D i,i P
Then we first focus on the calculation of  X  L 1 st / X  X  ( K )
Since Y =  X  ( Y ( K  X  1) W ( K ) + b ( K ) ) , the calculation of the sec-ond term  X  X / X  X  ( K ) is easy. For the first term of  X  L 1 st have:
Similarly, by using back-propagation we can finish the calcula-tion of partial derivative of L 1 st .
 Now we have obtained the partial derivatives of the parameters. With an initialization of the parameters, the proposed deep model can be optimized by using stochastic gradient descent. Note that due to the high nonlinearity of the model, it suffers from many local optimal in the parameter space. Therefore, in order to find a good region of parameter space, we use Deep Belief Network to pretrain the parameters at first [11], which has been demonstrated as an essential initialization of parameters for deep learning in literature [7]. The full algorithm is presented in Alg. 1.
 Algorithm 1 Training Algorithm for the semi-supervised deep model of SDNE Input: the network G = ( V,E ) with adjacency matrix S , the pa-Output: Network representations Y and updated Parameters:  X  1: Pretrain the model through deep belief network to obtain the 2: X = S 3: repeat 4: Based on X and  X  , apply Eq. 1 to obtain  X  X and Y = Y 5: L mix ( X ;  X  ) = k (  X  X  X  X ) B k 2 F +2  X tr ( Y T LY )+  X  L 6: Based on Eq. 6, use  X  X  mix / X  X  to back-propagate through 7: until converge 8: Obtain the network representations Y = Y ( K )
In this section, we present some analysis and discussions of the proposed semi-supervised deep model of SDNE .

New vertexes : A practical issue for network embedding is how to learn representations for newly arrived vertexes. For a new ver-tex v k , if its connections to the existing vertexes is known, we can obtain its adjacency vector x = { s 1 ,k ,...,s n,k } , where s cates the similarity between the existing v i and the new vertex v Then we can simply feed the x into our deep model and use the trained parameters  X  to get the representations for v k . The com-plexity for such a process is O (1) . If there exist no connections between v i and the existing vertexes in the network, our method and state-of-the-art network embedding methods cannot handle. To handle such case, we can resort to other side information, such as the content features of the new vertexes, which we leave as the fu-ture work.

Training Complexity : It is not difficult to see that the training complexity of our model is O ( ncdI ) , where n is the number of vertexes, d is the maximum dimension of the hidden layer, c is the average degree of the network and I is the number of iterations. Parameter d is usually related to the dimension of embedding vec-tors but not related to the number of vertexes. I is also independent with n . For c , it usually can be regarded as a constant in real-world applications. For example, in the social network, the maximum number of friends for a people is always bounded [30]. In top-k similarity graph, c = k . Therefore, cdI is independent with n and thus the overall training complexity is linear to the number of vertexes in the network.
In this section, we evaluate our proposed method on several real-world datasets and applications. The experimental results demon-strate significant improvements over baselines.
In order to comprehensively evaluate the effectiveness of the rep-resentations, we use five networked datasets, including three social networks, one citation network and one language network, for three real-world applications, i.e. multi-label classification, link predic-tion and visualization. Considering the characteristics of these dataset-s, for each application we use one or more datasets to evaluate the performances. The detailed descriptions are listed as follows.
To summarize, we conduct experiments on both weighted and unweighted, sparse and dense, small and large networks. There-fore, the datasets can comprehensively reflect the characteristics of the network embedding methods. The detailed statistics of the datasets can be summarized in Table 2.
 http://qwone.com/ jason/20Newsgroups/
We use the following five methods as the baselines. The first four are network embedding methods. Common Neighbor directly predicts the links over the networks, which has been demonstrated to be an effective method to perform link prediction [17].
In our experiment, we perform the task of reconstruction, link prediction, multi-label classification and visualization. For recon-struction and link prediction, we use precision@k and Mean Aver-age Precision (MAP) to evaluate the performance. Their definitions are listed as follows:
For the multi-label classification task, we adopt micro-F 1 and macro-F 1 as many other works do [27]. In detail, for a label A, we denote TP(A), FP(A) and FN(A) as the number of true posi-tives, false positives and false negatives in the instances which are predicted as A, respectively. Suppose C is the overall label set. Micro-F 1 and Macro-F 1 are defined as follows:
We propose a multi-layer deep structure in this paper and the number of layers varies with different datasets. The dimension of each layer is listed in Table 3. The neural networks have three lay-ers for B LOGCATALOG , A RXIV GR-QC and 20-N EWSGROUP and four layers for F LICKR and Y OUTUBE . If we use deeper model, the performance almost remains unchanged or even becomes worse.
For our method, the hyper-parameters of  X  ,  X  and  X  are tuned by using grid search on the validation set. The parameters for base-lines are tuned to be optimal. For LINE , the mini-batch size of the stochastic gradient descent is set to 1 . The learning rate of the start-ing value is 0 . 025 . The number of negative samples is set as 5 and the total number of samples is 10 billion. In addition, according to [26], LINE yields better results when concatenating both 1-step and 2-step representations to form the final embedding vectors and do L 2 normalization to the final embedding vectors. We follow their way to get the results of the LINE . For DeepWalk , we set window size as 10 , walk length as 40 , walks per vertex as 40 . For GraRep , we set maximum matrix transition step as 5 . In this section, we first evaluate the reconstruction performance. Then we report the results of the generalization of the network rep-resentations generated by different embedding methods on three classic data mining and machine learning applications, i.e. multi-label classification, link prediction and visualization.
Before proceeding to evaluate the generalization of the proposed method on real-world applications, we first provide a basic evalua-tion on different network embedding methods with respect to their capability of network reconstruction. The reason for this experi-ment is that a good network embedding method should ensure that the the learned embeddings can preserve the original network struc-ture. We use a language network A RXIV GR-QC and a social net-work B LOGCATALOG as representatives. Given a network, we use different network embedding methods to learn the network repre-sentations and then predict the links of the original networks. As the existing links in the original network are known and can serve as the ground-truth, we can evaluate the reconstruction performance, i.e. the training set error, of different methods. The precision@k and MAP are used as the evaluation metrics. The result on the pre-cision@k is presented in Figure 3. The result on MAP is shown in Table 4.

From the results, we have the following observations and analy-sis: LOGCATALOG on reconstruction task Figure 3: precision@ k on (a) A RXIV GR-QC and (b) B LOG CATALOG . The results show that our method achieves better reconstruction performance than that of baselines.
Classification is a so important task among many applications that the related algorithm and theories have been investigated by many works [18]. Therefore, we evaluate the effectiveness of d-ifferent network representations through a multilabel classification task in this experiment. The representations for the vertexes are generated from the network embedding methods and are used as features to classify each vertex into a set of labels. Specifically, we adopt the LIBLINEAR package [8] to train the classifiers. When training the classifier, we randomly sample a portion of the labeled nodes as the training data and the rest as the test. For B ALOG , we randomly sample 10 % to 90 % of the vertexes as the training samples and use the left vertexes to test the performance. For F LICKR and for Y OUTUBE , we randomly sample 1 % to 10 % of the vertexes as the training samples and use the left vertexes to test the performance. In addition, we remove the vertexes which are not labelled by any categories in Y OUTUBE . We repeat such a process 5 times and report the averaged Micro-F1 and Macro-F1 . The results are shown in Figure 4 and Figure 5, respectively. Figure 4: Micro-F1 and Macro-F1 on B LOGCATALOG . The results show that our method achieves better classification per-formance than that of baselines.

From the results, we have the following observations and analy-sis
In this section, we concentrate on the link prediction task and conduct two experiments. The first evaluates the overall perfor-mance and the second evaluates that how different sparsity of the networks affects the performance of different methods.

We use the dataset A RXIV GR-QC in this section. To conduct the link prediction task in a network, we randomly hide a portion of the existing links and use the left network to train the network embedding model. After the training, we can obtain the represen-tations for each vertex and then use the obtained representations Some results such as the observation that our method outperforms LINE have been listed and explained in Section 4.5.1. Therefore, we only list some particular observations of this experiment. Figure 5: Micro-F1 and Macro-F1 on (a) F LICKR and (b) Y OUTUBE . The results show that our method achieves the best classification performance among baselines.
 GR-QC for link prediction to predict the unobserved link. Unlike the reconstruction task, this task predicts the future links instead of reconstructing the existing links. Therefore, this task can show the performance of predictabil-ity of different network embedding methods. In addition, we add Common Neighbor in this task because it has been proved as an effective method to do link prediction [17].

For the first experiment, we randomly hide 15 precentage of ex-isting links (about 4000 links) and use the precision@k as the eval-uation metric of predicting the hidden links. We gradually increase the k from 2 to 10000 and report the result in Table 5. The best performance is highlighted in bold. Some of the observations and analysis on Table 5 are listed as follows:
In the second experiment, we change the sparsity of the networks by randomly removing a portion of links in the original network and then follow the aforementioned procedure to report the results of different network embedding methods. The result is shown in Figure 6.

The result shows that the margin between LE and SDNE or be-tween LE and LINE becomes larger when the network is sparser. It demonstrates that the import of second-order proximity is able to make the learned representations more robust to sparse networks. Moreover, when we remove 80 % of the links, our method still per-forms significantly better than baselines. It also demonstrates the power of SDNE in dealing with sparse networks. Figure 6: Performance of network embedding on networks of different sparsity. It shows that SDNE is more robust to the sparse network.
Another important application for network embedding is to gen-erate visualizations of a network on a two-dimensional space. There-fore, we visualize the learned representations of the 20-N network. We use the low-dimensional network representations learned by different network embedding methods as the input to the visu-alization tool t-SNE [31]. As a result, each newsgroup document is mapped as a two-dimensional vector. Then we can visualize each vector as a point on a two dimensional space. For documents which are labelled as different categories, we use different colors on the corresponded points. Therefore, a good visualization result is that the points of the same color are near from each other. The visual-ization figure is shown in Figure 7. Besides the visualization figure, similar to [4] we use the Kullback-Leibler divergence as a quanti-tative evaluation metric. The lower the KL divergence, the better the performance. The result is shown in Table 6.

From Figure 7, we can see that the results of LE and DeepWalk are not satisfactory because the points belonging to different cate-gories are mixed with each other. For LINE , the clusters of different categories are formed. However, in the center part the documents topic of talk.politics.guns precision Table 6: KL-divergence for the 20-N EWSGROUP dataset KL divergence 0.68** 0.73 0.87 2.6 2.95 of different categories are still mixed with each other. For GraRep , the result looks better because points of the same color form seg-mented groups. However, the boundaries of each group are not very clear. Obviously, the visualization of SDNE performs best in both the aspects of group separation and boundary aspects. The results shown in Table 6 also quantitatively demonstrate the superiority of our method in the visualization task.
We investigate the parameter sensitivity in this section. Specifi-cally, we evaluate how different numbers of the embedding dimen-sions and different values of hyper-parameter  X  and  X  can affect the results. We report precision @ k on the dataset of A RXIV Choose an appropriate number of embedding dimensions : We first show how the dimension of the embedding vectors affects the performance in Figure 8(a). We can see that initially the per-formance raises when the number of dimension increases. This is intuitive as more bits can encode more useful information in the increasing bits. However, when the number of dimensions continu-ously increases, the performance starts to drop slowly. The reason is that too large number of dimensions may introduce noises which will deteriorate the performance. Overall, it is always important to determine the number of dimensions for the latent embedding space, but our method is not very sensitive to this parameter.
Find a good balanced point between first-order and second-order proximity : Then we show how the value of  X  affects the per-formance in Figure 8(b). The parameter of  X  balances the weight of the first-order proximity and second-order proximity between ver-texes. When  X  = 0 , the performance is totally determined by the second-order proximity. And the larger the  X  , the more the model concentrates on the first-order proximity. From Figure 8(b), we can see that the performance of  X  = 0 . 1 and  X  = 0 . 2 are better than that of  X  = 0 . It demonstrates that both first-order and second-order proximity are essential for network embedding methods to characterize the network structure.

Concentrate more on the reconstruction error for the non-zero elements : At last, we show how the value of  X  affects the performance. The  X  controls the reconstruction weight of the non-zero elements in the training graph. The larger the  X  , the model will more prone to reconstruct the non-zero elements. The result is shown in Figure 8(c). We can see that when  X  = 1 , the result is not good. Because in this case, the model will equally reconstruct the non-zero and zero elements in the training network. As we have explained before, no link between two nodes does not indicate dis-similarity of these two nodes, but the existence of a link between two nodes does indicate similarity of these two nodes. Therefore, the reconstruction of zero elements will introduce noises and thus deteriorate the performance. However, when  X  is too large, the per-formance decreases too. The reason is that, in this case, the model almost ignores the zero-elements when perform reconstruction and is prone to maintain similarity between any pair of nodes. How-ever, many zero elements still indicate the dissimilarity between vertexes. Therefore, the performance drops. This experiment sug-gests that we should concentrate more on the reconstruction error of none-zero elements in the training networks but cannot totally omit the reconstruction error of zero elements when we perform network embedding.
In this paper, we propose a Structural Deep Network Embed-ding, namely SDNE , to perform network embedding. Specifical-ly, to capture the highly non-linear network structure, we design a semi-supervised deep model, which has multiple layers of non-linear functions. To further address the structure-preserving and sparsity problem, we jointly exploit the first-order proximity and second-order proximity to characterize the local and global net-work structure. By jointly optimizing them in the semi-supervised deep model, the learned representations are local-global structure-preserved and are robust to sparse networks. Empirically, we eval-uate the generated network representations in a variety of network datasets and applications. The results demonstrate substantial gains of our method compared with state-of-the-art.

Our future work will focus on how to learn representations for a new vertex which has no linkage to existing vertexes. This work was supported by National Program on Key Basic Research Project, No. 2015CB352300; National Natural Science Foundation of China, No. 61370022, No. 61531006, No. 61472444 and No. 61210008. Thanks for the research fund of Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology.
