 Automatically detecting controversy on the Web is a useful capability for a search engine to help users review web con-tent with a more balanced and critical view. The current state-of-the art approach is to find K-Nearest-Neighbors in Wikipedia to the document query, and to aggregate their controversy scores that are automatically computed from the Wikipedia edit-history features. In this paper, we discover two major weakness in the prior work and propose modifi-cations. First, the generated single query from document to find KNN Wikipages easily becomes ambiguous. Thus, we propose to generate multiple queries from smaller but more topically coherent paragraph of the document. Second, the automatically computed controversy scores of Wikipedia ar-ticles that depend on  X  X dit war X  features have a drawback that without an edit history, there can be no edit wars. To infer more reliable controversy scores for articles with little edit history, we smooth the original score from the scores of the neighbors with more established edit history. We show that the modified framework is improved by up to 5% for binary controversy classification in a publicly available dataset.
The Web is an excellent source for obtaining accurate and useful information for a huge number of topics, but it is also an excellent source for obtaining misguided, untrustworthy and biased information. To help users review webpage con-tents with a more balanced and critical view, alerting users that the topic of a webpage is controversial will be a useful feature for a search engine.

Dori-Hacohen and Allan [4] proposed a framework for mak-ing binary classification on general webpage, whether the webpage presents a perspective on a controversial topic or not. Their framework consists of four steps: 1. Matching k-NN Wikipages: When a webpage is 2. Computing Controversy Score on Wikipages: 3. Aggregate: They aggregated the three types of k 4. Vote and Classify: They apply a voting scheme to
While examining the performance of the current frame-work, we identified two major weaknesses. First, generating a single query from a document in Step 1 has issues. As documents almost always contain multiple sub-topics, the generated query contains an unknown mixture of different sub-topics. This makes the query X  X  intent less clear, as it targets many sub-topics at the same time and in unknown balance. It is also unlikely that all sub-topics are covered in the query  X  or covered appropriately  X  because keywords are extracted from a bag-of-words, which does not model the ex-istence of sub-topics as it is. As an alternative, we propose a text-segmentation based query generation approach named tilequery . We first segment the document into multiple tiles, where each tile contains fewer sub-topics than the doc-ument, ideally one sub-topic per tile. We generate a query from each tile and then aggregate the ranked lists obtained from the tiles X  queries. This can be viewed as a  X  X ivide-and-conquer X  approach for document query generation.

Another issue is that the Wikipedia controversy scores de-pend on  X  X dit-war X  features, evidence of multiple editors exchanging opposing opinions. However, the controversy level is naturally underestimated on specific and sub-topical Wikipages whose topical disputes have often been delegated to other Wikipages of the broader topic. In other words, not having the  X  X dit-war X  does not necessarily mean that there was no war in this topic, but that the war has been happen-ing somewhere else instead. This phenomenon causes the algorithm to easily make false negative errors (i.e., classify-ing  X  X ontroversial X  as  X  X on-controversial X ).

We next provide details of the modified query genera-tion approach and how we address the problem of missing or underestimated controversy scores using smoothing from neighbors. We then carry out an evaluation using 303 non-Wikipages as starting points and show the impact of our modifications to the framework on classification accuracy. We show that gains come from both changes but that cor-recting controversy scores has a greater impact, yielding 5% improvements in classification accuracy over the state-of-the art performance.
To the best of our knowledge, Dori-Hacohen and Allan were the first team to extend controversy detection to gen-eral open-domain web-pages [4]. However, many efforts have been made to understand and estimate controversy in Wiki-pedia. As Wikipedia contains manually tagged controver-sial articles by editors, machine-learning based methods ap-proaches were trained to learn them. To estimate the level of controversy in Wikipages, information extracted from the edit-history, such as revision count, number of unique edi-tors, number of reverts, the number of editors participating in the edit-war, and their reputations have been exploited [6, 10]. Sepehri Rad and Barbosa surveyed five established con-troversy detection algorithms on Wikipedia and compared their performances [8].

Outside of Wikipedia, controversy detection has also been studied within Twitter [7] or news corpus [1], focusing on po-litical domain. Wang and Cardie recently studied online dis-pute detection using sentiment-analysis based method trained from controversial corpus in Wikiepdia [9]. However, as in Wikipedia, this method requires evidence of explicit disputes between two people, which is not applicable to general web-pages.
Here we provide details of the two modifications that we propose to the existing framework.
Document Segmentation We first use the block com-parison algorithm described by the TextTiling technique [5]. The block comparison method defines as block with a few sentences, and computes a lexical similarity score for every gap between two blocks. When the similarity score dramat-ically changes at a gap, we assume that is where a sub-topic shift occurs and create a tile of blocks.

Query Generation Once we create tiles from a document using TextTiling, we generate a query from each tile. There are often some tiles that are hard to understand its meaning without the context of the full text. Therefore, adding the global context helps clarify the topic of each tile, anchor-ing the tile X  X  query to the containing document X  X  topic. We hence generate a query by using the g (global) most frequent terms from the document,and the l (local) most frequent terms from the tile. We empirically found that g = 3 , l = 7 gives the best performance when using 10 terms.

Aggregating the Ranked Lists Each tilequery returns a ranked list. We compute the relevance score for each re-trieved Wikipage w i by aggregating the reversed ranking order: where T is the set of tiles generated from the document. This scoring prioritizes Wikipages that appear high in some tile or at reasonable ranks in many tiles, or preferably both.
Wikipedia Controversy Scores Previous work studied algorithms for automatically computing scores that estimate the level of controversy. They use features available in Wiki-pages, meta-data, talk pages, and edit-history. We briefly explain the three scores that the previous framework adopted. C Score This is a regression-based method that estimates M Score Yasseri et al. investigated edit-wars based on sta-D Score This is a Boolean value indicating whether a Wiki-Unfortunately, these approaches are limited for the same reason that many Wikipages with controversial topics do not have sufficient edit-history or explicit edit-wars. There is a tendency that the heat of the edit-wars are focused on one Wikipage of a general and broad topic, leaving other related but sub-topical pages less attended. After all, there is simply no point of having the same  X  X ar X  on all similar Wikipages. Table 1 shows an example of a few  X  X bortion X  related top-ics and their M and C Score. While the  X  X bortion X  page received a lot of attention, other pages with more specific topics such as Abortion in certain countries and Abortion Act had virtually no edit-wars. Unless there is a specific is-sue or event specifically tied to the page, all general disputes on abortion have been delegated to the  X  X bortion X  page. Figure 1: An example of the constructed network for Abortion
Due to this phenomenon, even if we generate a better query to find more relevant k pages, the framework still would not be able to fully take advantage of that due to the underestimated scores. Hence, it is necessary to revise these scores to reflect controversy better. If the purpose of the M or C score was to measure the controversy level presented in the Wikipage per se , we need newly revised scores that accurately signify controversiality of the topic in general. To do this, we will construct a network that links topically re-lated neighbors within the Wikipedia. We then revise the controversy score by  X  X moothing X  using the scores of neigh-bors with more edit history, whose scores were computed with more confidence.

Constructing Wikipages X  Graph We construct a tree-structured graph to identify topically related neighbors of a Wikipage. Let G = ( V, E ) be a directed graph with nodes V (Wikipages) and edges E (sub-topical relations). An edge e ( u, v ) represents that node v is a sub-topic of u .
As a simple and straightforward method to construct the edges, we look at their titles. If a Wikipage u  X  X  title is used as a prefix of other v  X  X  title, we assume that v is sub-topic of u . While we use nodes X  titles to construct edges, we assume there is a mapping between a title and a node and will use them interchangeably.

Let a Wikititle T be a ordered list [ t 1 , t 2 , ..., t t is an i-th space-delimited token. The parent node set P(T) (i.e., Wikipages whose titles that have T as a child) is obtained by: where W T is a set of all Wikipedia titles. The graph also contains many noisy relations when the prefix is an ambigu-ous entity, or a simply too general word, such as  X  X merican X . To filter out the noisy relations, we remove the edges if two pages are not linked in any direction. Using this graph, we finally revise the controversy score using smoothing.
Network-based Smoothing When Wikipage is given as a query, we extract a sub-graph around the node from the constructed graph using one of the two methods:
Pair-based : A sub-graph around the query node including
Clique-based : Pair-based sub-graph + sibling nodes that
Once we obtain the sub-graph, we treat all nodes in the sub-graph as topically related neighbors of the query node. We want to fix the query node X  X  controversy score by smooth-ing from neighbors that have more reliable scores. For that we assume that the controversy score of a Wikipage with more revision history is more reliable. Hence we convert this graph into a weighted, directed network whose direction rep-resents which way influence should extend to (i.e., the one with higher revision count to the other with lower count), and whose edge weight represents the confidence of the in-fluence relation, which is the revision count of the source (Figure 2). From the graph, the new controversy score of Wikipage W i is computed as: Figure 2: A network constructed around the query  X  X ame-sex marriage in Sweden X . The scores are bor-rowed from three neighbors that have more revision counts than the query page by weighted smoothing.
 where r i is a revision count of W i .
 Table 1: An example of two controversy scores on several Wikipages on  X  X bortion X , before and after score smoothing
We summarize the aggregation and voting schemes intro-duced by previous work. Once the controversy scores are ob-tained for k Wikipages, we aggregate the k scores by taking average or max of them. Since we use three different scores, M, C, and D, three aggregated scores, M agg , C agg , and D are computed. We turn these scores into binary label indi-cating controversial (1) or non-controversial (0), using cor-responding thresholds. M label = 1 if M agg  X  T hreshold M and 0 otherwise. Using the three generated labels, we use a voting scheme to make a final decision. We test 6 voting schemes as parameters in our experiments.

The webpage is controversial if:  X  C/M/D : { C label , M label , D label } is 1, respectively.  X  Majority : the majority (i.e., at least two) of { C label M  X  Or/And : C label { X  /  X } M label { X  /  X } D label is 1.
We use the publicly available controversy dataset released by Dori-Hacohen et al. [3]. We used 303 clueweb documents whose controversy level was annotated with four scales: 1 - X  X learly controversial X , 2 - X  X ossibly controversial X , 3 - X  X os-sibly non-controversial X , and 4 - X  X learly non-controversial X . To convert the annotations to binary judgments, we treated the documents with average ratings among annotators of less than 2.5 as controversial, and otherwise non-controversial. Of 303 documents, 42% of them are controversial.

To test the effectiveness of the proposed query method, we consider two other query baselines. One is TF10 , the 10 most frequent terms, as in the prior work. As taking only k terms as in a query might miss information, we consider an-other baseline, all query that uses all terms in a document as a query to observe the extreme case of TF N .

We consider 9 settings from all possible combinations of three query methods and three scoring schemes (Table 2). Run 4 is the setting proposed in the prior work [4]. In each setting, we varied the four sets of parameters, the number of neighbors K (1, 5, 10, 15, 20), aggregation method (avg, max), voting methods (C, M, D, Majority, Or, And, D  X  (C  X  M)), and thresholds for C and M as tested in [4]. We found the best parameter setting for each run using 5-fold cross validation with the target metric accuracy. Thus, for 9 settings, there are 5 sets of parameters learned for each fold. We used McNemar X  X  Test 1 for statistical significance test. https://en.wikipedia.org/wiki/McNemar%27s test } { 40000,20000 } { M, Maj. } 0.72 0.50 } { 40000,20000 } { M, Maj. } 0.78 0.68 } { 40000,20000 } { M, Maj. } 0.73 0.53  X  2 { 20000, 40000, 84930 } { M, Maj. } 0.75 0.57  X  2 84930 Maj. 0.79 0.68  X  2 { 20000, 84930 } Maj. 0.75 0.57  X  2 { 40000,20000 } { M, Maj. } 0.75 0.59  X  2 { 40000,20000 } { M, Maj. } 0.75 0.61 Table 3: Improvements of accuracy and F1 Score between runs (bold: statistically significant)
Row # Runs | Acc 1 -Acc 2 || F1 1 -F1 2 | p value
Impact of Query Methods Our statistical significant tests suggest that the difference of accuracies between the three query methods in runs 1, 4, and 7 are not significant (Row 2 &amp; 5 in Table 3), which suggest that the three meth-ods mostly made similar classifications.

However, once we apply neighbor-based smoothing on con-troversy scores, query methods cause classification to work differently. The accuracy gain of TileQuery over TF10 was 1%, and 4% of F1-score with smoothing. Although the accuracy gain was small, the query set that each method performed well was different as the significance test implies (Row 6 &amp; 7 in Table 3).

Impact of Neighbor-based Smoothing In all settings, using controversy score smoothing significantly improved the classification accuracy and F1-score. As row 1, 4, and 8 in Table 3 show, the accuracy was improved by 4-6% and the F1-score was improved by 2-12% in all three query methods. Clique-based neighbor selection consistently outperformed pair-based selection.
We revisited the prior work for automatically detecting controversy from the general open-domain webpages. We identified two major weakness in the framework and pro-posed two modifications to fix the issues. The controversy score smoothing consistently improved the controversy clas-sification accuracies by 4-6% compared to those without smoothing. Overall, the run with our two modifications of tilequery and controversy score smoothing gave the best accuracy improving the previous framework by 5%. In the future, we plan to further investigate different scenar-ios when TF10 and TileQuery works well. As we were only able to find topically related neighbors for 5% of the Wikipages with prefix-relation, we will explore more sophis-ticated methods to increase this coverage.
 This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS-0910884, and in part by NSF grant #IIS-1217281. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. The authors thank Shiri Dori-Hacohen for providing valuable resources. [1] Y. Choi, Y. Jung, and S.-H. Myaeng. Identifying [2] S. Das, A. Lavoie, and M. Magdon-Ismail.
 [3] S. Dori-Hacohen and J. Allan. Detecting controversy [4] S. Dori-Hacohen and J. Allan. Automated controversy [5] M. A. Hearst. Texttiling: Segmenting text into [6] A. Kittur, B. Suh, B. A. Pendleton, and E. H. Chi. He [7] A.-M. Popescu and M. Pennacchiotti. Detecting [8] H. S. Rad and D. Barbosa. Identifying controversial [9] L. Wang and C. Cardie. A piece of my mind: A [10] T. Yasseri, R. Sumi, A. Rung, A. Kornai, and
