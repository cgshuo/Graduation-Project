 1. Introduction
Text mining refers to the detection of trends, patterns, or similarities in natural language text. Given a collection of text documents, often the need arises to classify the documents into groups or clusters based on similarity of content. For a relatively small collection, it may be possible to manually perform the par-titioning of documents into specific categories. But to partition large volumes of text, the process would be extremely time consuming. Moreover, automation also greatly reduces the time needed to perform the classification.
When the categories or topics for classification are predefined, the process of classification is considered ( Dunham, 2003 ). However, in absence of any information regarding the nature of the data, the problem of classification becomes much more difficult. For unsupervised classification of text data, only one valid assumption can be made, which is that the text collection is completely unstructured. The task then be-comes organizing the documents into a structure based solely on patterns learned from the collection itself.
This structure can be partitional or hierarchical ( Dunham, 2003 ). The hierarchical organization of docu-ments has a tree-like structure with the entire collection situated at the root level. In subsequent levels of the tree, the collection is partitioned into smaller groups and eventually each document is represented as a separate group at the bottom level.

If the text collection is given a partitional structure, then the documents in the collection are flatly par-titioned or clustered into groups that are nonoverlapping. The proposed nonnegative matrix factorization (NMF) method for text mining introduces a technique for partitional clustering that identifies semantic fea-tures in a document collection and groups the documents into clusters on the basis of shared semantic fea-tures. The factorization can be used to compute a low rank approximation of a large sparse matrix along with preservation of natural data nonnegativity.

In the vector space model of text data, documents are encoded as n -dimensional vectors where n is the number of terms in the dictionary, and each vector component reflects the importance of the correspond-ing term with respect to the semantics of a document ( Berry, Drmac  X  , &amp; Jessup, 1999 ). A collection of documents can, thus, be represented as a term-by-document matrix. Since each vector component is given a positive value (or weight) if the corresponding term is present in the document and a null or zero value otherwise, the resulting term-by-document matrix is always nonnegative. This inherent data nonnegativity is preserved by the NMF method as a result of constraints (placed on the factorization) that produce nonnegative lower rank factors that can be interpreted as semantic features or patterns in the text collec-tion. The vectors or documents in the original matrix can be reconstructed by combining these seman-tic features, and documents that have common features can be viewed as a cluster. As shown by Xu,
Liu, and Gong (2003) , NMF outperforms traditional vector space approaches to information retrieval (such as latent semantic indexing) for document clustering on a few topic detection benchmark collections. 2. Related work
Nonnegative matrix factorization differs from other rank reduction methods for vector space models in text mining, e.g., principal component analysis (PCA) or vector quantization (VQ), due to use of con-straints that produce nonnegative basis vectors, which make possible the concept of a parts -based represen-tation ( Lee &amp; Seung, 2001 ). Lee and Seung first introduced the notion of parts-based representations for problems in image analysis or text mining that occupy nonnegative subspaces in a vector-space model.
Techniques like PCA and VQ also generate basis vectors X  X arious additive and subtractive combinations of which can be used to reconstruct the original space. But the basis vectors for PCA and VQ contain neg-ative entries and cannot be directly related to the the original vector space to derive meaningful interpre-tations. In the case of NMF, the basis vectors contain no negative entries X  X his allows only additive combinations of the vectors to reproduce the original. So the perception of the whole, be it an image or a document in a collection, becomes a combination of its parts represented by these basis vectors. In text mining, the vectors represent or identify semantic features, i.e., a set of words denoting a particular concept or topic. If a document is viewed as a combination of basis vectors, then it can be categorized as belonging to the topic represented by its principal vector. Thus, NMF can be used to organize text collections into partitional structures or clusters directly derived from the nonnegative factors.
Recently Xu et al. (2003) have demonstrated that NMF outperforms methods such as singular value decomposition and is comparable to graph partitioning methods that are widely used in clustering text doc-uments. The tests were conducted on two different datasets: the Reuters data corpus both considered benchmark collections for topic detection. These two data corpora are also used in this study to observe the results of using nonnegative factorization for text mining or document clustering.
The algorithm used to derive the factorization introduces a new parameter to control the number of basis vectors used to reconstruct the document vectors, thereby providing a mechanism to balance the tradeoff between accuracy and computational cost (including storage). 3. Algorithm
With the standard vector space model, a set of documents S can be expressed as a m  X  n matrix V , where m is the number of terms in the dictionary and n is the number of documents in S . Each column V an encoding of a document in S and each entry v ij of vector V semantics of V j , where i ranges across the terms in the dictionary. The NMF problem is defined as finding a low rank approximation of V in terms of some metric (e.g., the norm) by factoring V into the product ( WH ) of two reduced-dimensional matrices W and H . Each column of W is a basis vector, i.e., it contains an encoding of a semantic space or concept from V and each column of H contains an encoding of the linear combination of the basis vectors that approximates the corresponding column of V . Dimensions of W and
H are m  X  k and k  X  n respectively, where k is the reduced rank or selected number of topics. Usually k is chosen to be much smaller than n , but more accurately, k min( m , n ). Finding the appropriate value of k 2002 ).

Common approaches to NMF obtain an approximation of V by computing a ( W , H ) pair to minimize the Frobenius norm of the difference V WH . The problem can be cast in the following way ( Pauca, Shah-naz, Berry, &amp; Plemmons, 2004 ) X  X et V 2 R m  X  n be a nonnegative matrix and W 2 R 0&lt; k min( m , n ). Then, the objective function or minimization problem can be stated as with W ij &gt; 0 and H ij &gt; 0 for each i and j .

The matrices W and H are not unique. Usually H is initialized to zero and W to a randomly generated the algorithm. In the following subsections some existing NMF techniques are discussed and a new algo-rithm is proposed. 3.1. Multiplicative method
The NMF method proposed by Lee and Seung is based on multiplicative update rules of W and H . This scheme is referred to as the multiplicative method (MM).

MM Algorithm (1) Initialize W and H with nonnegative values. (2) Iterate for each c , j , and i until convergence or after l iterations:
In steps 2(a) and (b), , a small positive parameter equal to 10 observed from the MM Algorithm, W and H remain nonnegative during the updates. Simultaneous updat-ing of W and H generally yield better results than updating each matrix factor fully before the other. In the algorithm, the columns of W or the basis vectors are normalized at each iteration; in case of W , the opti-mization is performed on a unit hypersphere with the columns of W effectively being mapped to the surface of the hypersphere by repeated normalization ( Pauca et al., 2004 ).

The computational complexity of MM can be shown to be O ( kmn ) operations (for a rank-k approxima-tion) per iteration ( Pauca et al., 2004 ). Once the term-by-document matrix V has been factored into W and H , if new data needs to be added, then the data can be a direct addition to W with a minor modification to the initial approximations. It is shown by Lee and Seung (2001) that under the MM-update rules, the objec-tive function (1) is monotonically nonincreasing and becomes constant if and only if W and H are at a sta-tionary point. This multiplicative method is related to expectation-maximization approaches used in image diagonally-scaled gradient descent method ( Guillamet &amp; Vitria, 2002 ). 3.2. Sparse encoding
A new nonnegative sparse encoding scheme, based on the study of neural networks has been sug-gested by Hoyer (2002) . This scheme is applicable to the decomposition of datasets into independent feature subspaces by Hyva  X  rinen and Hoyer (2000) . The method proposed by Hoyer (2002, 2004) has an important feature that enforces a statistical sparsity of the H matrix. As the sparsity of H increases, the basis vectors become more localized, i.e., the parts-based representation of the data in W becomes more and more enhanced. Mu, Plemmons, and Santago (2003) have put forth a regularization approach that achieves the same objective of enforcing statistical sparsity of H by using a point-count regulari-zation scheme that penalizes the number of nonzero entries rather than the sum of entries in H . 3.3. A hybrid method
The NMF algorithm used in this study ( Pauca et al., 2004 ) is a hybrid method that combines some of the better features of the methods discussed in the previous sections. In this approach, the multiplicative method, which is basically a version of the gradient descent optimization scheme, is used at each iterative step to approximate the basis vector matrix W . H is calculated using a constrained least squares (CLS) model as the metric. It serves to penalize the nonsmoothness and nonsparsity of H ; as a result of this penal-ization, the basis vectors or topics in W become more localized, thereby reducing the number of vectors needed to represent each document. The method for approximating H is similar to the methods described in Hoyer (2002) and Mu et al. (2003) and related to the least squares Tikhonov regularization technique commonly used in image restoration ( Prasad et al., 2003 ). This hybrid algorithm is denoted by GD-CLS (gradient descent with constrained least squares) in ( Pauca et al., 2004 ). Although there are several alter-of a best algorithm is generally considered to be problem-dependent. The proposed GD-CLS algorithm has performed quite well on the topic detection experiments discussed in Section 5.
GD-CLS Algorithm (1) Initialize W and H with nonnegative values, and scale the columns of W to unit norm. (2) Iterate until convergence or after l iterations: 4. Software implementation Two software packages, namely GTP and LAPACK, were used in the C-based implementation of GD-CLS algorithm. The General Text Parser (GTP) is a software environment developed at the University of
Tennessee by Giles, Wo, and Berry (2003) . One of the functions of GTP is to parse text documents and construct a sparse matrix data structure, i.e., a term-by-document matrix that defines the relationship be-tween the documents and the parsed terms ( Mironova, 2003 ). The GTP software can be used to parse single files or entire directories and is fitted with the capability to process both raw text and HTML files.
The user can also integrate external filters into the software to process other forms of tagged data. Cur-rently there are two versions of the software available X  X ne in C++ and another in Java X  X oth of which are designed to facilitate users with all ranges of expertise. For this study, the C++ version of GTP was used.

LAPACK has various routines, which can be individually downloaded from the LAPACK website solving different types of linear equations. For the C version of NMF, the dposv software routine of LAPACK is used to derive solutions (in double precision) to linear systems of the form AX = B , where
A is a symmetric positive definite matrix. 5. Experiments Originally written in MATLAB (see Pauca et al. (2004) and Fig. 1 ) the proposed NMF algorithm or
GD-CLS has been converted to C in this study for scalability. Performance evaluations are conducted using two different datasets X  X he Reuters Document Corpus and TDT2. This section describes the methodology used for evaluation, while the actual results 4 are discussed in Section 6. 5.1. Reuters
The Reuters data corpus, contains 21578 documents and 135 topics or document clusters created man-ually and each document in the corpus is been assigned one or more topics or category labels based on its content. The manually created cluster sizes, i.e., the number of documents assigned to the topics, range any-where from less than ten to nearly four thousand. The documents are in SGML format (see Shahnaz, 2004 ) with meta tags denoting title, topic(s), and beginning and end of content.

For this experiment, documents associated with only one topic are used and topics with cluster sizes the corpus and create an index of topics with associated cluster sizes, where a document is considered part of a cluster only if it has a single topic.

In order to observe the performance of the GD-CLS implementation of NMF as the complexity of the problem increases, i.e., as the number of clusters or the parameter k is incremented, seven different k values the predominant number of elements of these matrices are zero, the Harwell-Boeing (HB) column-com-pressed sparse matrix format ( Berry &amp; Browne, 1999 ) is used to access all nonzero elements. After the
HB matrices are generated, the NMF clustering algorithm is performed on all 21 matrices (7 values of k  X  3 document subsets each) to produce the W and H factors for each HB matrix. For any given HB ma-trix V , with k topics and n documents, matrix W has k columns or basis vectors that represent the k clusters, while matrix H has n columns that represent the n documents. A column vector in H has k components, each of which denotes the contribution of the corresponding basis vector to that column or document.
The classification or clustering of documents is then performed based on the index of the highest value of k for each document. So, for document i ( i =1, ... , n ), if the maximum value is the j th entry ( j =1, ... , k ), document i is assigned to cluster j . After the documents are clustered into k topics, the NMF generated k clusters are compared to the original k clusters using a mapping function. The mapping is performed using a Perl script that assigns the original cluster labels to the NMF clusters based on a sim-ilarity measure. The example below provides an explanation of the mapping process for k =2.
Once the relabeling is accomplished, the accuracy of the classification or clustering is assessed using the metric AC ( Xu et al., 2003 ) defined by otherwise, and n is the number of documents in the collection. So, for our two-cluster example In the GD-CLS implementation of NMF, the contribution of the k parameter with which the sparsity of calculated.
 Two-Cluster Example
Using GD-CLS on the HB matrix generated from D with topic set T yields WH , where W 2 R and H 2 R 2  X  5 . Assuming H has the value shown in Table 1 , the clustering based on the maximum column entry is
The values of the following mapping function are used to form a matrix S ( Table 2 ), where S larity(Cluster i ,Cluster X ) = number of documents in Cluster X ={ A , B }.

Each Cluster i is assigned the original cluster label to which it is the most similar. Cluster are assigned labels A and B respectively and the documents are reassigned to topics based on the new clus-tering. A comparison of the original clustering to the GD-CLS generated cluster labels is shown in
Table 3 . 5.2. TDT2 The second data corpus TDT2, obtained from the Language Data Consortium at The University of
Pennsylvania, contains transcripts from a total of six news sources several transcripts or documents. Although the corpus consists of about sixty-four thousand documents in
SGML format (see Shahnaz, 2004 ), some fourteen thousand of these are actually assigned a topic label and the rest are not classified. Among the preclassified documents, 7919 documents are single topic documents, i.e., these documents only have a single topic or category label. The SGML markup tags for each document denote a unique document ID or identification number and the beginning and end of text content. The doc-ument-topic relationships are described in a separate file that contains a line in it for each document with a category label. A line corresponding to a particular document consists of the document ID, topic label, and the name of the file containing that document.

In order to make the document collection from this corpus comparable to the Reuters dataset, some pre-processing with the use of Perl scripts is applied to the SGML files. First, the file containing the document-least five documents is created. Here also, as with the Reuters collection, documents containing multiple topic labels are not deemed relevant. Since the entire document corpus consists of 64000 documents and only 7919 are relevant to the experiments, another preprocessing step is taken to reduce the runtime of
GTP by traversing the entire collection once and writing the relevant documents to a single file. For all sub-sequent testing, only this file is then used in order to avoid traversing thousands of irrelevant documents for each test run. Once the topic file and the reduced set of 7919 documents are at hand, several subsets are created to monitor the decline of accuracy for the NMF algorithm as complexity or the k values increase.
As before, 7 different k values (2, 4, 6, 8, 10, 15, 20) are chosen with 10 different topic sets or document subsets each. After application of the GD-CLS algorithm and the accuracy metric, this selection of datasets yielded some of the results presented in the next section.
 6. Observations
The results from TDT2 and Reuters data corpora bring to attention trends such as the decline in accu-racy in relation to the increase in complexity or the value of k . Results from both document collections indicate that as more and more topics or document clusters are added to the dataset being clustered by
GD-CLS, the accuracy of the clustering decreases. For the Reuters collection, in case of k = 2, i.e., when dealing with only two topics, the algorithm performs with above 99% accuracy, but in case of k = 20, the accuracy drops down to just above 54% ( Table 4 ). However, in case of TDT2, the drop in accuracy is much less precipitous than for Reuters ( Table 5 ). For TDT2 with k = 20, accuracy is just above 80%, which seems like a significant improvement from 54% for Reuters. This disparity can be attributed to the differences in content of the two collections. Documents in the Reuters collection are categorized under
TDT2, the topic labels are much more specific (e.g.,  X  X  X he Asian economic crisis, X  X   X  X  X ornado in Florida, X  X  and  X  X  X prah lawsuit X  X ). The very specificity of the topics in the TDT2 guarantees a heterogeneity in the doc-ument collection that is not present in the Reuters collection. In the case of Reuters, while  X  X  X otato X  X  and  X  X  X inc X  X  may constitute very distinct clusters,  X  X  X nterest X  X  and  X  X  X oney-fixes X  X  do not. In fact, as noted by tributes to the much more rapid decline of accuracy in the case of Reuters than it does for TDT2.
Another notable trend that also points to the sensitivity of the GD-CLS algorithm (for NMF) to the contents of the document collections is the differences in accuracy for the different values of k . In the case of TDT2, the different k values for each k do not affect the performance by any noticeable amount. But for
Reuters, the drop in accuracy for increasing values of the k parameter suggests that text collections that are somewhat homogeneous in content, are more sensitive to the changes of the k parameter (or the sparsity of the H matrix). The primary reason for using a larger k value (or an increase in the sparsity of H )isto achieve faster computation times. Inspection of the results from Tables 4 and 5 suggests that that is indeed the case, especially in higher complexity problems ( Table 6 ).

Following Karvanen and Cichocki (2003) , we have measured the increase in sparsity of H =[ h the  X  p matrix norm 6 (for p &lt; 1) which is defined by
Figs. 2 and 3 illustrate the consistent reduction ( y -axis) in k H k for selected k = 10 and k = 20 document subsets of the Reuters collection as the regularization parameter k ( x -axis) increases from 10 6 to 1. Recall that k is the number of topics extracted for each subset and the number of feature vectors (column dimension of matrix W ) generated by the GD-CLS algorithm.

It can be inferred from Table 6 that an increase in the sparsity of H results in a significant increase in computational speed and this holds for both TDT2 and Reuters. As for accuracy, the k values do affect performance for Reuters but not for TDT2. However, when compared to the gain in computational time, the 2 X 3% decrease in accuracy can be considered a very reasonable tradeoff.

An aspect of GD-CLS that cannot be directly observed from the result tables is the change in perfor-mance of the factorization with regard to disparate cluster sizes. When creating document subsets for each value of k from the preclassified clusters of the Reuters or TDT2 corpus, attention is given to keep the clus-ter sizes within a reasonable bound of one another. This constraint, which is not imposed by Xu et al. in
The imbalance in the cluster sizes in dataset 1 has a definite effect on the performance of GD-CLS regard-less of the document corpus being used. In case of the original clusters from dataset cluster 2 is approximately 48:1, while the clusters produced by GD-CLS have a ratio of 3:1. This implies
GD-CLS performs much better on datasets that have balanced cluster sizes, such as dataset tering is performed with almost 100% accuracy. 7. Conclusions and future work
Clustering of documents in a database according to semantic features inherent to the data is a challeng-ing problem in text data mining. In this paper we present a new approach for unsupervised identification of semantics topics and associated document clustering in a heterogeneous text collection. This approach is based on the use of nonnegative matrix factorization algorithms for the factorization of term X  X ocument matrices. Additional statistical sparsity constraints are imposed. A new algorithm, GD-CLS, is presented for factoring a term X  X ocument matrix into a matrix W of basis vectors denoting semantic features and a matrix H of mixing coefficients specifying document clustering. The sparsity constraint is enforced via a parameter k used for controlling document clustering as well as to balance computational cost and accu-racy, as illustrated in our results. Other variations of GD-CLS have been explored by the authors in order to consider both accuracy and efficiency. For example, nonnegative least squares methods can be used to compute W and H at each iteration in order to speed up convergence, at the cost of a significant increase in computation per iteration. On the other hand, gradient descend type methods can be used for updating W and H efficiently at the cost of slow convergence. The GD-CLS algorithm attempts to strike a balance be-tween both of these extremes.

Updating semantic features and document clusters is often needed as documents are added to a text col-lection. In its current stage, the GD-CLS algorithm is not equipped to handle updating in an efficient man-ner. Once the document collection has been clustered, adding a small number of documents to the collection can be achieved by comparing each of the new documents (represented by a vector) to the basis vectors and associating the new document to the basis vector or topic to which it is most similar. But this updating technique is not scalable and would produce poor results if used to add a large number of doc-uments that cannot be associated with any of the basis vectors. Techniques for efficiently updating features and clusters as documents are added to a text collection are currently under investigation.
Although the primary function of NMF would be for classification as opposed to query-based informa-updating discussed above, a user query can be represented by a term vector, which is then used to compute a similarity measure (e.g., cosine measurement) between the query and the basis vectors. The basis vector or topic that yields the highest value is deemed the most relevant and documents belonging to that topic are provided to the user.

In general, NMF has mostly been applied to image analysis and text mining. Another field that could benefit from this technique is bioinformatics. Problems such as identifying motifs or significant features in protein sequences (partial strings of DNAs) are natural candidates for application of NMF. In such problems, protein sequences can be viewed as analogous to text documents and the basis vectors or topics enforcing adequate constraints, such as statistical sparsity, to these types of problems.
 Acknowledgements
The authors would like to thank Murray Browne for his technical assistance with the production of the original manuscript and the anonymous referees for their comments and suggestions for the subsequent revision.
 References
