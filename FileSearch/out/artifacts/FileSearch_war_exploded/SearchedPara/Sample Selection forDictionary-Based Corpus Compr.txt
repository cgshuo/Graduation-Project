 Compression of large text corpora has the potential to dras-tically reduce both storage requirements and per-document access costs. Adaptive methods used for general-purpose compression are ineffective for this application, and histori-cally the most successful methods have been based on word-based dictionaries, which allow use of global properties of the text. However, these are dependent on the text comply-ing with assumptions about content and lead to dictionaries of unpredictable size. In recent work we have described an LZ-like approach in which sampled blocks of a corpus are used as a dictionary against which the complete corpus is compressed, giving compression twice as effective than that of zlib . Here we explore how pre-processing can be used to eliminate redundancy in our sampled dictionary. Our ex-periments show that dictionary size can be reduced by 50% or more (less than 0.1% of the collection size) with no sig-nificant effect on compression or access speed.
 E.4 [ Data ]: Coding and Information Theory  X  data com-paction and compression ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search pro-cess Algorithms, Performance Dictionary Compression, Random Access, Document Re-trieval, Sampling
Corpus compression can dramatically improve the effi-ciency of an information retrieval system, first by reduc-ing storage and second by reducing the cost of disk seek and read operations. There are further benefits in improved bandwidth between levels of the memory hierarchy and more effective caching [6]. Key features of an effective algorithm are that the compression model needs to be determined glob-tremely simple approach in which we treated a collection a s a single string and took samples of lengths s (say 1024 bytes) at evenly-sized intervals across the collection (with m of say 0.5 Gb or 1.0 Gb). That is, we take m/s samples from n/ ( m/s ) locations, on the assumption (which our ex-periments confirmed) that any sufficiently frequent material in the corpus would be captured in this process.
However, although compression effectiveness was high, we observed that a significant percentage of the dictionaries was unused. There was a strong skew in the samples that were used, and, even amongst these, there was redundancy as some samples contained repeated material.

Such redundancy can be removed in a pre-processing phase, either to reduce memory footprint or to replace samples. For each encoded RLZ pair we incremented a counter corre-sponding to the sample the factor occurred in. Once the pairs have been processed we sorted the samples by fre-quency and generated a new dictionary comprised of the most frequently used entries, stopping when we reached a specific usage threshold or byte limit. Such an approach, although simple, gives explicit control of memory use and should maximize compression effectiveness for a given num-ber of samples.

Results are shown in the tables. Dictionaries were gener-ated from Clueweb09 256 Gb Wikipedia corpus. We gener-ated samples as discussed above, and then pruned them to give a fixed-size dictionary. As can be seen, halving dictio-nary size led to a small increase in encoding size.
Even at a ten-fold reduction in dictionary size, compres-sion and document access was still better than with zlib . We observed consistent access speeds of over 100 documents per second simulating document requests for a typical IR system. These requests were generated from ranked listings using the first 5000 queries of TREC X  X  MQ09 query-log. The top 20 ranked documents were used for each query. This is more than double the speed of access to uncompressed doc-uments, or to documents compressed with zlib and lzma in block groups of varied size. Sequential decompression runs at around 10,000 documents per second, a significant in-crease in access speed compared to our two baseline imple-mentations.

Experiments were conducted on a 3.0 GHz Intel Xeon pro-cessor with 4Gb RAM running a Linux Kernel 2.6.18 and compiled with GCC 4.1.2 using full optimizations. Caches were dropped between each run. All timing results were recorded as wall clock time. Compressed collections used for evaluation are significantly larger than internal memory, so our timings account for disk seek and read latency as they are the dominant cost in document retrieval.

Our earlier results showed that RLZ is the method of choice for compressing a large corpus. It gives excellent compression, fast retrieval, and can be easily modified for a dynamic environment. In this work we have shown that the dictionary size can be substantially reduced without loss of efficiency. [1] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.
