 William L Hamilton 1 WHAMIL 3@ CS . MCGILL . CA Probabilistic models with latent variables are a powerful and flexible tool for modelling complex probability dis-tributions over structured data. Unfortunately, this power comes at a price: learning and predicting with latent vari-able models is typically a computationally expensive task, to the point where obtaining exact solutions can become intractable. Designing efficient, scalable, and accurate ap-proximation algorithms for these tasks is an important chal-lenge that needs to be solved if we want to use these models for solving large-scale real world problems.
 A classic solution to the problem of learning latent vari-able models under the maximum likelihood criterion is the expectation X  X aximization (EM) algorithm (Dempster et al., 1977). However, this algorithm suffers from two fun-damental limitations: there is a high computational cost on large state spaces, and no statistical guarantees about the accuracy of the solutions obtained are available. A way to overcome the limitations of EM is by considering re-laxed versions of the maximum likelihood principle (Varin, 2008).
 A recent alternative line of work consists of designing learning algorithms for latent variable models exploiting an entirely different statistical principle: the so-called method of moments (Pearson, 1894). The key idea underlying this principle is that, since the low order moments of a distri-bution are typically easy to estimate, by writing a set of equations that relate the moments with the parameters of the distribution and solving these equations using estimated moments, one can obtain approximations to the parame-ters of the target distribution. In some cases solving these equations only involves spectral decompositions of matri-ces or tensors and basic linear algebra operations (Anand-kumar et al., 2012d;b). Since moment estimation can be performed in time linear in the number of examples, and the required algebraic operations involve dimensions inde-pendent of the number of training examples, this approach can provide extremely fast learning algorithms. In addition, statistical analyses show that these algorithms are robust to noise and can learn models satisfying some basic assump-tions from samples of size polynomial in the relevant pa-rameters (see Hsu et al., 2009; Anandkumar et al., 2012d; Balle &amp; Mohri, 2012; Hsu &amp; Kakade, 2013, and references therein).
 A witness of the generality of the method of moments is the wide and ever-growing class of models that can be learned with this approach. These include multiple probabilistic models over sequences, such as HMM (Hsu et al., 2009; Anandkumar et al., 2012d;b), weighted automata (Bailly et al., 2009; Balle et al., 2012), predictive states repre-sentations (Boots et al., 2011; Hamilton et al., 2013), and variants thereof (Siddiqi et al., 2010; Balle et al., 2011; Stratos et al., 2013; Bailly et al., 2013b). Building on top of these sequential models, algorithms for learning context-free formalisms used in natural language process-ing have been proposed (Bailly et al., 2010; 2013a; Cohen et al., 2012; 2013; Luque et al., 2012; Dhillon et al., 2012). Other classes of latent variable models known to be learn-able using this approach include: multiple classes of tree-structured graphical models (Parikh et al., 2011; Song et al., 2013); mixture, admixture, and topic models (Anandkumar et al., 2012c;a;b;d; Chaganty &amp; Liang, 2013); and commu-nity detection models (Anandkumar et al., 2013). Further-more, the method can be extended using features functions (Boots et al., 2011; Recasens &amp; Quattoni, 2013) and ker-nelized to deal with continuous observation spaces (Song et al., 2010; Boots et al., 2013).
 Despite the expectations and promises raised by this new class of methods, their use is far from widespread in applied domains. In particular, few works in this area report empir-ical results, and in most cases these address only fairly re-stricted tasks. Therefore, it is unclear at this point whether methods of moments can provide truly competitive learn-ing algorithms for wide ranges of problems and match the expectations set forward by their theoretical appeal. One explanation for this is that most of the papers describing the fundamentals of these methods provide very little or no implementation details  X  a notable exception being (Cohen et al., 2013). In addition, although competitive results have been obtained using methods of moments in NLP (Cohen et al., 2013; Balle et al., 2013) and robotics (Boots et al., 2011), naive implementations tend to perform much worse than expected, and significant amounts of tuning and opti-mization are needed to obtain fast and accurate algorithms. The goal of this paper is to present a unified review of three different methods of moments in the context of learn-ing hidden Markov models (HMM) and other probabilistic models over sequences. We consider the SVD-based al-gorithm from (Hsu et al., 2009), the convex optimization approach described in (Balle et al., 2012), and the symmet-ric tensor decomposition framework of (Anandkumar et al., 2012b). We choose these three methods because they are the easiest to present in our context, and in addition, almost every other algorithm based on the method of moments uses (variations of) these as a subroutine. Our compari-son touches upon several aspects of these algorithms that we believe have been neglected in previous works. First, by giving a unified presentation we are able to stress the some-times subtle differences between the three methods and dis-cuss in which settings is each of them statistically consis-tent. And using this unified theoretical framework we pro-vide a novel algebraic proof of consistency for one of the three methods (symmetric tensor decomposition). Then we present empirical experiments with synthetic and real data comparing between these algorithms and EM. By present-ing results using two different accuracy metrics (word er-ror rate and perplexity), our experiments unveil some inter-esting subtleties about the different methods. Finally, we provide a thorough description of implementation details required to make each method scalable and accurate. 2 2.1. Notation We use bold letters to denote vectors v  X  R d , matrices Given a matrix M we write k M k F for its Frobenius norm and k M k  X  for its trace/nuclear norm. We use M + to de-note the Moore X  X enrose pseudo-inverse of M . Sometimes we shall give names to the columns and rows of a matrix using ordered index sets I and J . In this case we will write M  X  R I X J to denote a matrix of size |I| X |J| with rows indexed by I and columns indexed by J .
 A matrix M  X  R d  X  d is symmetric if M = M &gt; . Simi-larly, a tensor T  X  R d  X  d  X  d is symmetric if for any per-mutation  X  of the set { 1 , 2 , 3 } we have T = T  X  , where T [ d ] . Given vectors v i  X  R d i for 1  X  i  X  3 , we can take ten-sor products to obtain matrices v 1  X  v 2 = v 1 v &gt; 2  X  we also write v  X  v  X  v = v  X  3 , which is a third order symmetric tensor.
 we consider the contraction operation that produces a ten-given by T 0 j we used Einstein X  X  summation convention.
 Let  X  be a finite alphabet. We use  X  ? to denote the set of all finite strings over  X  , and we write  X  for the empty string. Given two strings u,v  X   X  ? we write w = uv for their concatenation, in which case we say that u is a prefix of w , and v is a suffix of w . Given two sets of strings P , S  X   X  ? we write PS for the set obtained by taking every string of the form uv with u  X  P and v  X  S . When singletons are involved, we write u S instead of { u }S . If f :  X  ?  X  R is a function, we use f ( P ) to denote P Given strings u,v  X   X  ? , we denote by | v | u the number of occurrences of u as a substring of v . 2.2. Latent Variable Models A stochastic language is a probability distribution over  X  More formally, it is a function f :  X  ?  X  R such that f ( x )  X  0 for every x  X   X  ? and P x  X   X  ? f ( x ) = 1 . The main learning problem we consider in this paper is to infer a stochastic language  X  f from a sample S = ( x 1 ,...,x m of i.i.d. strings generated from some f . In order to give a succinct representation for  X  f we use hypothesis classes based on finite automata. In the following we present sev-eral types of automata that are used throughout the paper. A weighted automaton (WA) over  X  is a tuple A =  X   X  0 ,  X   X  , { A  X  }  X   X   X   X  , with  X  0 ,  X   X   X  R n and A  X  R n  X  n . The vectors  X  0 and  X   X  are called the initial and final weights, respectively. Matrices A  X  are transition operators. The size n of these objects is the number of states of A . A weighted automaton A computes a function f
A :  X  ?  X  R as follows: f A function f :  X  ?  X  R is realized by A if f A = f . If f is a stochastic language, then A is a stochastic automaton . A learning task one might consider in this setting is the following: assuming the target stochastic language can be realized by some WA, try to find a stochastic WA realizing approximately the same distribution (w.r.t. some metric). The methods given in (Hsu et al., 2009; Bailly et al., 2009)  X  which we review in Section 3  X  can be used to solve this problem, provided one is content with a WA A such that  X  f = f A approximates f but is not necessarily stochastic. It turns out that this is an essential limitation of using WA as a hypothesis class: in general, checking whether a WA A is stochastic is an undecidable problem (Denis &amp; Esposito, 2008). Thus, if one imperatively needs the hypothesis to be a probability distribution, it is necessary to consider meth-ods that produce a WA which is stochastic by construc-tion (e.g., a probabilistic automaton). Alternatively one can employ heuristics to approximate a probability distribution with a WA which is not stochastic (see Section 4.2). A probabilistic automaton (PA) is a WA A =  X   X  0 ,  X   X  , { A  X  } X  where the weights satisfy (1)  X  0  X  0 , with  X  &gt; 0 1 = 1 and (2)  X   X   X  0 , A  X   X  0 , with P  X  A  X  1 +  X   X  = 1 . These conditions say that  X  0 can be interpreted as probabilities of starting in each state and that A  X  and  X   X  define a collection of emission/transition and stopping probabilities. It is easy to check that PA are stochastic by construction; that is, when A is a PA the func-tion f A is a stochastic language. A deterministic PA (DPA) is a PA where  X  0 ( i ) = 1 for some i  X  [ n ] , and for every  X   X   X  and every i  X  [ n ] there exists a unique j  X  [ n ] such that A  X  ( i,j ) &gt; 0 .
 A factorized weighted automaton (FWA) is a tuple A =  X   X  0 ,  X   X  , T , { O  X  }  X   X   X   X  with initial and final weights  X  0 ,  X   X   X  R n , transition weights T  X  R n  X  n , and emis-sion weights O  X   X  R n  X  n , where the matrices O  X  are di-agonal. One can readily transform a FWA into a WA by taking B =  X   X  0 ,  X   X  , { B  X  } X  with  X  0 =  X  0 ,  X   X  =  X  and B  X  = O  X  T . A hidden Markov model (HMM) is a FWA where the weights satisfy (1)  X  0  X  0 with  X  &gt; 0 1 = 1 , (2) T  X  0 with T1 = 1 , and (3)  X   X   X  0 , O  X   X  0 , with P  X  O  X  1 +  X   X  = 1 . It can be easily checked that these conditions imply that the WA obtained from a HMM is a PA. For convenience, given a HMM we also define the observation matrix O  X  R  X   X  n with entries O (  X ,i ) = O  X  ( i,i ) .
 Note that unlike with WA, both PA and HMM readily de-fine probability distributions. But this comes at a price: there are stochastic WA realizing probability distributions that cannot be realized by any PA or HMM with a fi-nite number of states (Denis &amp; Esposito, 2008). In terms of representational power, both PA and HMM are equiv-alent when the number of states is unrestricted. How-ever, in general PA provide more compact representations: given a PA with n states one can always obtain an HMM with min { n 2 ,n |  X  |} states realizing the same distribution. Moreover, there are PA with n states such that every HMM realizing the same distribution needs more than n states (Dupont et al., 2005). These facts imply that different hy-pothesis classes for learning stochastic languages impose different limitations upon the learned representation. Given a stochastic language f that assigns probabilities to strings, there are two functions computing aggregate statis-tics that one can consider: f p for probabilities of prefixes, and f s for expected number of occurrences of substrings. In particular, we have f p ( x ) = f ( x  X  ? ) = P y  X   X  ? and f s ( x ) = E y  X  f [ | y | x ] = P y,z  X   X  ? f ( yxz ) . Note that given a sample S of size m generated from f , it is equally easy to estimate the empirical probabilities  X  f ( x ) = (1 /m ) P m i =1 I [ x i = x ] , as well as empirical prefix probabilities  X  f p S ( x ) = (1 /m ) P m i =1 I and empirical substring occurrence expectations  X  f s S ( x ) = (1 /m ) P m i =1 | x i | x . It is shown in (Balle et al., 2013) that when f is realized by a WA, PA, or HMM, then so are f p and f s . This result provides an explicit conversion that pre-serves the number of states and that can be easily reversed. Therefore, in terms of learning algorithms, one can work with any of these three representations indistinctively. 2.3. Learning Latent Variable Models with EM EM is an iterative algorithm for locally maximizing the non-convex log-likelihood function (Dempster et al., 1977). It alternates between two types of steps: an expec-tation (E) step, where the expected distribution of the hid-den variables are computed, and a maximization (M) step, where the parameters of the model are updated by max-imizing the joint likelihood of the observed data and the expected hidden variables X  distributions. To avoid getting stuck in local maxima, restarts and other heuristics are usu-ally necessary (Hulden, 2012). In practice, given enough time to explore the space of parameters these heuristics yield very competitive models (e.g., Verwer et al., 2012). The key idea behind method of moments algorithms is to derive equations relating the parameters of some stochastic automaton realizing the target distribution to statistics of the distribution. Then, using estimations of these statistics computed from observed data, one can solve these equa-tions for the parameters of a hypothesis model. In the case of rational stochastic languages, these equations involve mostly linear algebra operations that can be solved using several methods. In this section we describe three algo-rithms for solving these equations. Our selection is repre-sentative of the possible approaches to the method of mo-ments, all of which involve either singular value decompo-sitions, convex optimization, or symmetric tensor decom-positions. For ease of presentation, we assume that we have access to the target stochastic language f , which can be used to compute the probability f ( x ) of any string x  X   X  Very few modifications are needed when the algorithms are applied to empirical estimates  X  f S computed from a sam-ple S . We give detailed descriptions of these modifications wherever they are needed. 3.1. Singular Value Decomposition Method A key step underlying the method of moments algorithms for learning stochastic languages is the arrangement of a finite set of values of f into a matrices or tensors in a way such that spectral factorizations of these linear ob-jects reveal information about the operators of a WA, PA, or HMM realizing f . As a simple example, consider f = f A for some WA A =  X   X  0 ,  X   X  , { A  X  } X  with n states. Given two sets of strings P , S  X   X  ? which we call prefixes and suffixes, consider the matrix H  X  R P X S with entries given by H ( u,v ) = f ( uv ) . This is the Han-kel matrix 3 of f on prefixes P and suffixes S . Writing f ( u,v ) = (  X  &gt; 0 A u )( A v  X   X  ) we see that this Hankel ma-trix can be written as H = PS , where P  X  R P X  n with the u th row equal to  X  &gt; 0 A u , and S  X  R n  X S with v th col-umn equal to A v  X   X  . Then it is easy to see that the Hankel matrix H  X   X  R P X S with entries H  X  ( u,v ) = f ( u X v ) for some  X   X   X  can be written as H  X  = PA  X  S . Thus, a way to recover the operators A  X  of A is to obtain a factoriza-tion H = PS and use it to solve for A  X  in the expression of H  X  . In practice H is factorized via a singular value de-composition, hence the name spectral method.
 We now proceed to give the details of the algorithm, which is based on (Hsu et al., 2009; Bailly et al., 2009). The al-gorithm computes a minimal WA that approximates f but which, in general, is not stochastic. As input, the method requires sets of prefixes and suffixes P , S  X   X  ? , and the number of states n of the target automaton.
 The algorithm starts by computing the Hankel matrices H , H  X   X  R P X S for each  X   X   X  . It also computes vec-tors h  X , S  X  R S with h  X , S ( v ) = f ( v ) and h P , X   X  h
P , X  ( u ) = f ( u ) . Next, it computes the reduced SVD 4 composition H = UDV &gt; with U  X  R P X  n , V  X  R S X  n and diagonal D  X  R n  X  n . The algorithm then returns a WA A =  X   X  0 ,  X   X  , { A  X  } X  given by  X  &gt; 0 = h &gt;  X  3.2. Convex Optimization Method This method recovers the operators of a WA by solving an optimization problem involving the sum of a Frobenius norm loss and a trace norm regularizer. As with the SVD method, the learned WA is not stochastic by construction. To motivate the algorithm, recall from the previous sec-tion that if f is computed by a WA with n states, then a Hankel matrix of f admits a factorization of the form H = PS , P  X  R P X  n , S  X  R n  X S . Now suppose that P has rank n . Then, taking B  X  = PA  X  P +  X  R P X P we have B  X  H = H  X  and rank( B  X  )  X  n . Since the WA given by B =  X   X  0 ,  X   X  , { B  X  } X  with  X  &gt; 0 =  X  &gt;  X   X  = P  X   X  satisfies f A = f B , this motivates an algo-rithm that looks for a low-rank solution of MH = H  X  . An algorithm based on this principle is described in (Balle et al., 2012). As input, the method requires sets of prefixes P and suffixes S with  X   X  P  X  S , and a regularization parameter  X  &gt; 0 . The number of states of the WA produced by this algorithm is equal to the number of prefixes |P| . The algorithm starts by computing two Hankel matrices H  X  R P X S and H  X   X  R P  X   X S , where H is defined like before, and H  X  ( u X ,v ) = f ( u X v ) . Note that because we have  X   X  P  X  X  , now the vectors h P , X  and h  X , S are con-tained inside of H . The operators of the hypothesis are obtained by solving the optimization problem and then taking the submatrices A  X   X  R P X P given by A  X  ( u,u 0 ) = A  X  ( u X ,u 0 ) . The output automaton is ob-tained by taking A =  X   X  0 ,  X   X  , { A  X  } X  , with the operators recovered from A  X  ,  X  0 = e  X  the indicator vector corre-sponding to the empty prefix, and  X   X  = h P , X  . 3.3. Symmetric Tensor Decomposition Method The tensor decomposition method can be applied when the target distribution is generated by a HMM. The idea behind this approach is to observe that when f can be realized by a FWA, then the factorization of the Hankel matrix associ-ated with a symbol  X   X   X  becomes H  X  = PO  X  TS . Since P , S , and T appear in the decomposition for all  X  , and O is diagonal, this implies that under some assumptions on the ranks of these matrices, all the H  X  admit a joint diago-nalization. Stacking these matrices together yields a Han-kel tensor H P ,  X  , S  X  R P X   X   X S with a particular structure that can be exploited to recover first O , and then the tran-sition matrix T and the weight vectors  X  0 and  X   X  . The algorithm we describe in this section implements this idea by following the symmetrization and whitening approach of (Anandkumar et al., 2012b). Our presentation is a vari-ant of their method, which extends the method to work with arbitrary sets of prefixes P and suffixes S , and also is able to recover the set of stopping probabilities. We present a consistency analysis of this variant in the Supplementary Material.
 Again, the method needs as input sets of prefixes P and suffixes S with  X   X  P  X  S , and the number of states n of the target HMM, which must satisfy n  X  |  X  | . The algorithm proceeds in four stages. In its first stage, the algorithm computes a set of Hankel matrices and tensors. In particular, a third order tensor H P ,  X  , S R
P X   X   X S with entries H P ,  X  , S ( u, X ,v ) = f ( u X v ) , a Han-kel matrix H P , S  X  R P X S with entries H P , S ( u,v ) = f ( uv ) , and a Hankel matrix H p P ,  X   X  R P X   X  with en-ferent dimensions of the tensor H P ,  X  , S , the algorithm ob-tains three more matrices:  X  H  X  , S  X  R  X   X S with entries  X  H
 X  , S (  X ,v ) = P u f ( u X v ) ,  X  H
P , S ( u,v ) = P  X  f ( u X v ) , and  X  H P ,  X   X  R P X   X  with en-tries  X  H P ,  X  ( u, X  ) = P v f ( u X v ) .
 The goal of the second stage is to obtain an orthogonal decomposition of a tensor derived from H P ,  X  , S as fol-lows. Assuming  X  H P , S has rank at least n , the algorithm first finds matrices Q P  X  R n  X P and Q S  X  R n  X S such obtained as follows: X  X  =  X  H  X  , S N  X  H P ,  X  , and Y H X
 X  and Y  X  are symmetric. 5 Then, assuming X  X  is pos-itive definite of rank at least n , we can find W  X  R  X   X  n such that W &gt; X  X  W = I . This is used to whiten the ten-sor Y  X  by taking Z  X  = Y  X  ( W , W , W )  X  R n  X  n  X  n . Next we compute the robust orthogonal eigendecomposi-tion Z  X  = P i  X  [ n ]  X  i z  X  3 i using a power method for ten-sors similar to that used to compute eigendecompositions of matrices (Anandkumar et al., 2012b). Using these robust eigenpairs (  X  i , z i ) we build a matrix  X  O  X  R  X   X  n column is  X  i ( W &gt; ) + z i . After a normalization operation, this will be the observation matrix of the output model. The third stage recovers the rest of parameters (up to nor-malization) via a series of matrix manipulations. Let  X  O  X  H start by taking  X   X  &gt; 0 = e &gt;  X   X  O P and  X   X   X  = tively, the rows of  X  O P and  X  O S corresponding to  X  . Simi-larly, the algorithm computes  X  T =  X  O + P  X  H P , S H R In the last stage the model parameters are normalized as follows. Let  X  D  X  = diag(  X  2 1 ,..., X  2 n )  X  R  X  D first compute  X  =  X  D S  X  T +  X  D  X   X   X   X   X  R n and then let  X   X  ( i ) =  X  ( i ) / (1 +  X  ( i )) . The initial weights are ob-Finally, we let O =  X  O  X  D  X  and T =  X  D S  X  T  X  D + working with empirical approximations these matrices are not guaranteed to satisfy the requirements in the definition of a HMM. In this case, a last step is necessary to enforce the constraints by projecting the parameters into the sim-plex (Duchi et al., 2008).
 One important benefit of the symmetric tensor decomposi-tion approach is that, since it returns proper HMM param-eters, it can be used to initialize other optimization algo-rithms (e.g., maximum likelihood methods such as EM). 3.4. Statistical Guarantees Under some natural assumptions, these three methods are known to be consistent. Assuming the Hankel matrices and tensors are computed from a WA with n states, the SVD and convex optimization methods are consistent whenever H has rank n (Hsu et al., 2009; Balle et al., 2012). To guar-antee the consistency of the tensor decomposition method we require that f can be realized by some HMM with n  X  |  X  | states and  X  H P , S has rank n (see (Anandkumar et al., 2012b) and the proof in the Supplementary Mate-rial). We note that in most cases, whether these conditions are satisfied or not may depend on our choice of P and S . Finite sample bounds can also be obtained for some of these methods, which show that the error of method of mo-ments typically decreases at a parametric rate O p ( m  X  1 / 2 Moreover, explicit dependencies on several task-related pa-rameters can be obtained with these types of analyses. In the case of stochastic languages, one obtains bounds that depend polynomially on the number of states n , the al-phabet size |  X  | , the dimensions of the Hankel matrices p |P||S| , and the n th singular value of H (see Hsu et al., 2009; Anandkumar et al., 2012b; Hsu &amp; Kakade, 2013; Balle, 2013, for explicit bounds and further pointers) . We compare the empirical performance of the methods de-scribed above using two contrasting performance metrics on synthetic data and a real-world natural language pro-cessing (NLP) task. The goal of this analysis is to elucidate the performance and implementation tradeoffs between the different moment-based methods, while comparing them to a state-of-the-art EM baseline. 4.1. Methods Compared The following methods are compared: Spec-Str , the spec-tral method (3.1) applied to empirical estimates  X  f S ; Spec-Sub , the spectral method (3.1) applied to empirical esti-mates  X  f s S ; CO , the convex optimization method (3.2) ap-plied to empirical estimates  X  f s S and using the alternating direction method of multipliers (ADMM) (Boyd et al., 2011) optimization algorithm; Tensor , the symmetric ten-sor decomposition method using the tensor-power method (Anandkumar et al., 2012b) (3.3) applied to empirical esti-mates  X  f S ; EM , an optimized implementation 6 of the Baum-Welch algorithm (Dempster et al., 1977); EM-Tensor , the EM method initialized with the solution from the tensor method.
 Versions of the spectral method using both string (  X  f S substring (  X  f s S ) estimates are included in order to demon-strate a basic tradeoff: using  X  f s S maximizes the amount of information extracted from training data but leads to dense Hankel matrices while using  X  f S leads to sparse Hankel ma-trix estimates. The CO method uses  X  f s S as its complex-ity scales with the size of the Hankel matrix and the op-timization routines do not preserve sparsity. The tensor method uses  X  f S , as our implementation relies on exploit-ing the sparsity of the Hankel estimates.
 Table 1 summarizes important characteristic and imple-mentation details for the different moment-based methods. 4.2. Performance Metrics We examine two contrasting performance metrics: one based upon perplexity and the other on word-error-rate (WER). We define the perplexity of a model M on a test set the true probability of the string (estimated as its empiri-cal frequency in the test set if necessary) and p M ( x ) is the probability assigned to the string by the model. For the spectral methods and the CO method, the models are not guaranteed to return true probabilities, so thresholding to (0,1] is employed (see Cohen et al., 2013, for a discussion of other possible heuristics). In contrast, the WER met-ric measures the fraction of incorrectly predicted symbols when, for each prefix of strings in the test set, the most likely next symbol is predicted. 4.3. Hyper-parameter optimization To ensure a fair comparison, the only hyper-optimization performed for each domain was a search for the best model-size, or in the case of CO, a search for  X  . The search-schedule was fixed across all domains but was not uni-form across all methods. In particular, (1) EM is orders-of-magnitude slower than the other methods (see Figure 1), necessitating a more coarse-grained search, and (2) for tensor decomposition the space of model-sizes is upper-bounded by |  X  | , restricting the search. Further details can be found in the Supplementary Material. 4.4. Synthetic Experiments We tested the algorithms on 12 synthetic problems taken from the set of problems used in the PAutomaC competi-tion (Verwer et al., 2012). These domains were selected to represent a diverse sampling of possible target stochastic languages. The different classes of models used to gener-ate the datasets are HMM, PA, and DPA. We selected four models from each of these classes, such that within each class exactly half of the problems have the property that n &lt; |  X  | . Table A.1 in (Verwer et al., 2012) provides de-tailed descriptions of these different problems.
 Table 2 summarizes the performance of the different meth-ods in terms of WER and Table 3 the performance in terms of perplexity.
 Several conclusions can be drawn from these results. First, we note that Spec-Sub and EM are the top performers in terms of WER. And though EM outperforms Spec-Sub on a majority of domains, their performance is quite close (compared to the gap between those two and the other al-gorithms), while Spec-Sub is 40x faster. In terms of per-plexity EM and CO are the top-performers; EM performs best on a majority of domains but with a large penalty in terms of runtime. We also observe that the tensor decom-position initialized EM method usually outperformed the tensor method alone in terms of WER; in terms of perplex-ity its behavior was more erratic.
 Figure 1 summarizes a representative example of the run-time costs of the algorithms, distinguishing between ini-tialization and model-building phases. For the moment-methods, the Hankel matrices (and their spectral decom-positions) only need to be computed once prior to hyper-parameter optimization. This represents a major advantage of the moment-based methods compared to EM, where no computation results are reused. 4.5. Natural Language Processing Problem In addition to synthetic experiments, we compared the methods performance on a NLP task. In this task the methods were used to learn a model of the parts-of-speech (POS) tags from the Penn-Treebank Corpus (Marcus et al., 1993). There are 11 distinct POS tags in the task, so |  X  | = 11 .
 Table 4 summarizes the performance of the algorithms on the NLP data. Here, we again see that EM is the top per-former in terms of WER with the Spec-Sub method per-forming only slightly worse. In terms of perplexity the CO method outperforms all the other methods by a large mar-gin. Overall, the results on the NLP data reinforce the con-clusions reached via the synthetic experiments. In this work we provided a unified presentation of three methods of moments for learning probabilistic models over stochastic languages. Beyond providing a solid and unified foundation for cross-method comparisons, this presentation also extended the symmetric tensor decomposition method to work with arbitrary prefix and suffix bases.
 With this foundation in place, we discussed several con-crete instantiations of these approaches, highlighting im-plementation techniques that are necessary for scalabil-ity and performance, and we empirically compared these methods, along with an EM baseline, on both synthetic and real-world data. The synthetic experiments elucidated several important performance trends, which we hope will serve as aids for future research in this area and as impetus for the adoption of moment-based methods in applied set-tings. The NLP experiment demonstrates that these trends appear to carry over to noisy real-world settings. With respect to the WER metric, the experiments demon-strated that the Spec-Sub method produces WER results competitive with EM with a speed-up factor of 40 x. This makes Spec-Sub an attractive candidate for applications re-quiring low WERs, given its combination of speed, sim-plicity, and accuracy.
 With respect to model perplexity, the experiments demon-strated that the convex relaxation of the spectral method can produce highly accurate models. The good performance of CO in this setting is intriguing given its relatively poor WER performance and the fact that the algorithm is not guaranteed to return a PA (in contrast to the tensor and EM methods).
 It is should be noted, however, that EM was the top-performer on a majority of domains; though it is consider-ably more expensive in terms of runtime. This is an impor-tant finding as it demonstrates that an optimized implemen-tation of EM with random restarts is not significantly disad-vantaged by the issue of local-minima and that the primary drawback of EM compared to the moment-methods is its computational inefficiency, specially on large state spaces. Our experiments also elucidate other interesting properties of the methods. First, the results demonstrate that the ad-vantage of using  X  f s S estimates, which lead to dense Hankel estimates and extract more information from the training sample (compared to  X  f estimates), is quite pronounced in the case of the spectral method. In addition, these experi-ments highlight the unpredictable nature of initializing EM with a moment-based solution.
 Of course, for the sake of clarity of presentation and anal-ysis, this empirical comparison did exclude certain settings and methods. We did not examine the effect of large or continuous alphabets, as in those cases the performance of moment-methods are contingent upon the feature rep-resentation or kernel embedding employed (Song et al., 2010; Boots et al., 2013). And we did not examine differ-ent methods of choosing prefix and suffix bases. However, both these issues are largely orthogonal to the core learning problem, as we expect all three moment-methods to benefit equally from feature-representations and basis selections. That said, our analysis offers a clear picture of the cur-rent empirical state-of-the-art in moment-based methods for modelling stochastic languages. This work also rises a number of important directions and open questions for fu-ture work. Of particular interest are problems such as: (1) establishing theoretical justification for CO X  X  strong perfor-mance on the perplexity metric; (2) relax the theoretical constraints of tensor-based methods; and (3) developing al-gorithms in which moment-initialized maximum likelihood optimization is guaranteed to improve solutions.
 The authors are grateful to Animashree Anandkumar, Furong Huang, Percy Liang, and Andreu Mayo for sharing their implementations of some of the algorithms described in this paper. Financial support for this research was pro-vided by the NSERC Discovery and CGS-M programs, and the James McGill Research Fund.

