 In multivariate analysis, rank minimization emerges when a low-rank structure of matrices is desired as well as a small estimation error. Rank minimization is nonconvex and gen-erally NP-hard, imposing one major challenge. In this pa-per, we derive a general closed-form for a global minimizer of a nonconvex least squares problem, in lieu of the common practice that seeks a local solution or a surrogate solution based on nuclear-norm regularization. Computationally, we develop ecient algorithms to compute a global solution as well as an entire regularization solution path. Theoretically, we show that our method reconstructs the oracle estimator exactly from noisy data. As a result, it recovers the true rank optimally against any method and leads to sharper pa-rameter estimation over its counterpart. Finally, the utility of the proposed method is demonstrated by simulations and image reconstruction from noisy background.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms Nonconvex, global optimality, rank minimization
In multivariate analysis, estimation of lower-dimensional structures has received attention in statistics, signal process-ing and machine learning. One type of such structures is the low-rank of matrices [5, 22], where the rank measures the dimension of a multivariate response. Rank minimization approximates multivariate data with the smallest possible rank of matrices. It has many applications in, for instance, multi-task learning [6, 11], multi-class classi cation [2], ma-trix completion [8, 17], collaborative ltering [33, 1], clus-tering [20, 29], computer vision [35, 21, 18], among others. The central topic this article addresses is least squares rank minimization.

Consider multi-response linear regression in which a k -dimensional response vector z i follows where a i is a p -dimensional design vector, is a p k re-gression parameter matrix, and components of " i are inde-pendent. Model (1) reduces to the widely-used linear model in compressed sensing when k = 1 and becomes a multivari-ate autoregressive model with a i = z i 1 . Denote the rank of as r ( ) and rewrite (1) in the matrix form as follows: where Z = ( z 1 ; ; z n ) T 2 R n k , A = ( a 1 ; ; a n ) R n p and e = ( " 1 ; ; " n ) T 2 R n k are the data, design and error matrices. In (1), we estimate based on n pairs that r ( ) is relatively small in comparison to min( n; k; p ), where the number of unknown parameters k; p can greatly exceed the sample size n .

Least squares rank minimization, as described, solves where  X  X  X  F is the Frobenius-norm and s is an integer-valued tuning parameter taking values from [1 ; min( n; k; p )]. The general rank minimization is nonconvex and NP-hard [23], which is like the L 0 -minimization in univariate analysis. Therefore an exact global solution to (3) is not known as well as its statistical properties, due primarily to discrete-ness and non-convexity of the rank function.

Estimation under the restriction that r ( ) = r has been studied when n ! 1 with k and p held xed, see [3, 4, 15, 28, 26]. Two major computational approaches have been proposed for approximating the optimal solution of (3). The rst one involves regularization using surrogate func-tion, such as the nuclear-norm, which is a convex envelope of the rank function [13] and can be solved by ecient al-gorithms [8, 19, 34, 25, 16]. In some cases, the solution of this convex problem coincides with a global minimizer of (3) under certain isometry assumptions [27]. However, these as-su mptions can be strong and dicult to check. Recently, [7] obtained a global minimizer of a regularized version of (3).
The second approach attacks (3) by approximating the rank function iteratively through calculating the largest sin-gular vector using greedy search [30], or by singular value projection (SVP) through a local gradient method [17]. Un-der weaker isometry assumptions [27, 10, 9] than that of the nuclear-norm approach, these methods guarantee an ex-act solution of (3) but su er from the same diculties as the regularization method [30], although they have achieved promising results on both simulated and real-world data.
Theoretically, some loss error bounds of the rst regular-ization approach are obtained in [24] under Frobenius-norm, and rank selection consistency is established in [7]. Unfortu-nately, to our best knowledge, whether similar conclusions hold for our formulation (3) remains largely unknown.
In this paper, we have advanced on two fronts. Com-putationally, we derive a general closed-form for a global minimizer of (3) in Theorem 1, and give a condition un-der which (3) and its nonconvex regularized counterpart are equivalent with regard to global minimizers, although these two methods are not generally equivalent. Moreover, we develop an ecient algorithm for computing the entire regularization solution path at the cost of computing only one solution for a single regularization parameter. Theoret-ically, we establish optimality for a global minimizer of (3). More speci cally, the proposed method is optimal against any other ones in that it reconstructs the oracle estimator exactly, thus the true rank, under (1). It is important to note that this exact recovery result is a much stronger property than consistency, which is attributed to the discrete nature of the rank function as well as tuning parameter s . Such a result may not be shared by its regularized counterpart with a continuum tuning parameter. In addition, the method en-joys a higher degree of accuracy for parameter estimation than nuclear-norm rank estimation.

After the rst draft of this paper was completed, we were aware that [14] and [32] gave an expression of Theorem 1. However, neither paper considered computational and sta-tistical aspects of the solution. Inevitably, some partial over-laps exist between our Theorem 1 and theirs.

The rest of the paper is organized as follows. Section 2 presents a closed-form solution to (3). Section 3 gives an ecient path algorithm for a regularized version of (3). Sec-tion 4 is devoted to theoretical investigation, followed by Section 5 discussing methods for tuning. Section 6 presents proofs for all the theorems we develop and Section 7 presents results of empirical evaluations, where several rank mini-mization methods are compared. Section 8 concludes the paper.
 Notation: Table 1 summarizes the notations used in the rest of the paper.
This section derives a closed-form solution to (3). The strategy is to simplify (3) through the singular value de-composition (SVD) of matrix A and utilizing the properties of the rank function. Before proceeding, we present a moti-vating lemma, known as the Eckart-Young theorem [12]. Lemma 1. The best s -rank approximation, in terms of the Frobenius-norm, for a t -rank matrix Z with t s , i.e., a No tation Description glo bal minimizer of is given by = P s ( Z ) = U z D s V T z , where D s consists of the largest s singular values of Z given the SVD of Z = U
Intuitively P s ( Z ) may be viewed as a projection of Z onto a set of matrices whose ranks are no more than s . Note that (4) is a special case of (3) with matrix A being the identity matrix. This motivates us to solve (3) through the simpler problem (4).
 When A is nonsingular, clearly (3) has a global minimizer A 1 P s ( Z ) by rank preserveness of any nonsingular matrix in matrix multiplication. When A is singular, we rst as-sume that r ( A ) s . However, this assumption is by no means mandatory, which will be discussed later. Given the SVD of A = UDV T , with orthogonal matrices U 2 R n n and V 2 R p p and diagonal matrix D 2 R n p , we have  X  A Z  X  F =  X  U T ( A Z )  X  F =  X  DV T U T Z  X  F : This follows from the fact that the Frobenius-norm is invari-ant under any orthogonal transformation. Let Y = V T and W = U T Z , clearly we have r ( Y ) = r ( ). Now solv-ing (3) amounts to solving an equivalent form: Consequently, a global minimizer of (3) becomes V Y , where Y is a global minimizer of (5) and is given by the following theorem.

Theorem 1. Let D , Y , Z and s be as de ned above. If s r ( A ) , then a global minimizer of (5) is given by where D r ( A ) is a diagonal matrix consisting of all the nonzero singular value of A , a can be set as the zero matrix, and W r ( A ) consists of the rst r ( A ) rows of W .
 Here are some remarks regarding the above theorem:
Remark 1. The solution to problem (5) is generally not unique. Speci cally, the matrix a in (6) need not be xed at zero, as long as it does not change the rank of Y . However, if A is of full column rank, i.e., when r ( A ) = p , then a vanishes and Y can be uniquely determined. In this case, the optimal solution of (3) is also unique.
Re mark 2. The optimal Y can also be computed for the general matrix A with an arbitrary rank, i.e., when r ( A ) &lt; s . See the proof of Theorem 1 in the Section 6.

It is important to note that the value of a is irrelevant for prediction, but matters for parameter estimation. In other words, when r ( A ) &lt; p , a global minimizer is not unique, hence that parameter estimation is not identi able: see Sec-tion 4 for a discussion. For simplicity, we set a = 0 for Y subsequently.

In what follows, our estimator is de ned as ^ s , as well as an estimated rank ^ r . Algorithm 1 below summarizes the main steps for computing ^ s with regard to s min( n; k; p ), where the LSRM stands for Least Squares Rank Minimiza-tion.
 A lgorithm 1 Exact solution of (3) Inp ut: A , Z , s r ( A ) Output: A global minimizer of (3) Function: LSRM ( A , Z , s ) 1: if A is nonsingular then 2: = A 1 P s ( Z ) 3: else 4: Perform SVD on A : A = UDV T 5: Extract the first r rows of U T Z and denote it as 6: = V 7: end if 8: return 
The complexity of Algorithm 1 is determined mainly by the most expensive operations{matrix inversion and SVD, speci cally, at most one matrix inversion and two SVDs. Such operations roughly require a cubic time complexity 1
This section studies a regularized counterpart of (3): where &gt; 0 is a continuous regularization parameter corre-sponding to s in (3), and is a global minimizer of (7). The next theorem establishes an equivalence between (7) and (3) when is unique, occurring when r ( A ) = p . Such a result is not generally anticipated for a nonconvex prob-lem.

Theorem 2 (Equivalence). When p = r ( A ) , (7) has a unique global minimizer. Moreover, (7) and (3) are equiv-alent with respect to their solutions. For any with 0 , there exists 1 s = r ( ) such that = ^ s , and vice verse.

Next we develop an algorithm for computing an entire solution path for all values of with complexity comparable
Mo re speci cally, for a matrix of dimension n p , the SVD has a complexity of O (min f n 2 p; p 2 n g ), whereas the matrix inversion has a complexity of O ( r ( A ) 3 ), which can be im-proved to O ( r ( A ) 2 : 807 ) when the Strassen Algorithm is uti-lized. to that of solving (7) at a single -value. For motivation, rst consider a special case of the identity A in (7):
In (8), r ( ) decreases as increases from 0, where r ( ) goes through all integer values from r ( Z ) down to 0 when becomes suciently large. In addition, the value of g ( ) is nondecreasing as increases. The next theorem summarizes these results.
 Theorem 3 (Monotone property). Let r ( Z ) be r . Then the following properties hold: (1) There exists a solution path vector S of length r + 2 (2) Function g ( ) is nondecreasing and piecewise linear.
The monotone property leads to an ecient algorithm for calculating the pathwise solution of (8). Figure 1 displays the solution path by illustrating the function value g ( ) and r ( ) as a function of .
Through the monotone property, we compute the optimal solution of (8) at a particular by locating in the correct interval in the solution path vector S , which can be achieved eciently via a simple binary search. Algorithm 2 describes the main steps.
 Al gorithm 2 Pathwise solution of (8) Inpu t: , Z Output: Solution path vector S , pathwise solution Function: pathwise ( , Z ) 1: Initialize : S 0 = 0, 0 = Z , r = r ( Z ) 2: Perform SVD on Z : Z = U D V T 3: for i = r down to 1 do 6: end for 7: return S ,
Al gorithm 2 requires only one SVD operation, therefore its complexity is of the same order as that of Algorithm 1 at a single s -value. When Z is a low-rank matrix, existing soft-ware for SVD computation such as PROPACK is applicable to further improve computational eciency.
For a general design matrix A , note that  X  where W =  X 
W  X   X  2 F , we solve Note that D r is nonsingular, then the problem reduces to the simple case where ^ Y = D r Y r . The solution path can be obtained di-rectly from Algorithm 2.
This section is devoted to the theoretical investigation of least squares rank minimization, which remains largely unexplored, although nuclear-norm regularization has been studied. In particular, we will reveal what is the best perfor-mance for prediction as well as the optimal risk for parame-ter estimation. Moreover, we will establish the optimality of the proposed method. In fact, the proposed method recon-structs the oracle estimator, the optimal one as if the true rank were known in advance. Here the oracle estimator ^ 0 is de ned as a global minimizer of  X  A Z  X  2 F subject to r ( ) = r 0 , where 0 and r 0 = r ( 0 ) 1 denote the true parameter matrix and the true rank respectively. This leads to the exact rank recovery, in addition to the reconstruction of the optimal performance of the oracle estimator. In other words, the proposed method is optimal against any other methods such as the nuclear-norm rank regularization.
Given the design matrix A , we study the accuracy of rank recovery as well as prediction and parameter estimation. Let P and E be the true probability and expectation under 0 given A . For rank recovery, we use the metric P (^ r  X  = r For prediction and parameter estimation, we employ the risk E K ( ^ s ; 0 ) and E  X  ^ s 0  X  2 F respectively, where is the Kullback-Leibler loss and  X  X  X  2 is the L 2 -norm for a vec-tor. Note that the predictive risk equals to 2 2 E K ( ^ s and parameter estimation is considered when it is identi -able, i.e., r ( A ) = p .

Now we present the risk bounds under (1) without a Gaus-sian error assumption.

Theorem 4. Under (1) , the oracle estimator is exactly reconstructed by our method in that ^ r 0 = 0 under P , when r 0 min ( r ( A ) ; p ) . As a result, exact reconstruction of the optimal performance is achieved by our estimator ^ r In particular, and where j and min &gt; 0 are the j th largest and the smallest nonzero singular value of e  X  = U T r ( A ) e and n 1 = 2 tively, and U r ( A ) denotes the rst r ( A ) rows of U .
Remark: In general, E
Theorem 4 says that the optimal oracle estimator is ex-actly reconstructed by our method. Interestingly, the true rank is exactly recovered from noisy data, which is attributed to discreteness of the rank and is analogous to the maxi-mum likelihood estimation over a discrete parameter space. Concerning prediction and parameter estimation, the op-timal Kullback-Leibler risk is r 0 k n b ut the risk under the Frobenius-norm is r 0 k 2 degrees of freedom is in contrast to a rate r 0 kp n with out a rank restriction. This permits p to be much larger than n , or k; p  X  n . For es-timation, however, p enters the risk through 2 min , where p can not be larger than n , or max( k; p ) n .
As shown in Section 4, theoretically, exact rank recon-struction can be accomplished through tuning. Practically, we employ a predictive measure for rank selection. The predictive performance of ^ s is measured by wh ich is proportional to the risk R ( ^ s ), where the expecta-tion is taken with respect to ( Z ; A ).
T o estimate s over integer values in [0 ; ; min( n; p; k )], one may cross-validate through a tuning data set, which es-timates the MSE. Alternatively, one may use the generalized degrees of freedom [31] through data perturbation without a tuning set. [ GDF( ^ s ) = n 1  X  Z A ^ s  X  2 +2 n 1 the l th component of z i and the l th component of a T i the case that e i in (1) follows N ( 0 ; 2 I k k ), the method of data perturbation of [31] is applicable. Speci cally, sample e i from N ( 0 ; 2 I k k ) and let where Cov ( z il ; ( a i ^ s ) l ) is the sample covariance with the Monte Carlo size T . For the types of problems we consider, we xed T to be n .
In this section we present detailed proofs to the theorems developed in the previous sections. We rst present a tech-nical lemma to be used in the proof of Theorem 4. Lemma 2. Suppose A and B are two n 1 n 2 matrices. Then, and r ( A ) is the rank of A .
 Proof. Let the singular value decomposition of A and B be A = U 1 1 V T 1 and B = U 2 2 V T 2 where U i and V ; i = 1 ; 2, are orthogonal matrices, and 1 and 2 are diagonal matrices with their diagonal elements being the singular values of A and B , respectively. Then where U = U T 1 U 2 and V = V T 1 V 2 continue to be orthogo-nal. Let the ordered singular values of A be 1 r ( A ) and e B = ( ~ b ij ) = U 2 V T . By Cauchy-Schwarz's inequality, Similarly, let the ordered singular values of B be 1 r ( B ) . Then it suces to show that Assume, without loss of generality, that i = 0 if i &gt; r ( B ). Let n = min( n 1 ; n 2 ). By Cauchy-Schwarz's inequality, where the last step uses the fact that  X  tion of the above bounds lead to the desired results. This completes the proof. First partition D and W as follows: then
Evidently, only the rst r ( A ) rows of Y are involved in minimizing  X  DY W  X  2 2 , which amounts to computing the global minimizer Y r ( A ) of and Lemma 1 with s r ( A ). For s &gt; r ( A ), recall that, only the upper part of Y is relevant in minimizing (5). The result then follows. This completes the proof.
For any with &gt; 0, let s = r ( ). Next we prove by contradiction that = ^ s . Suppose  X  = ^ s . By uniqueness of ^ s given in Theorem 1 and the de nition of minimization,  X  A ^ s Z  X  2 F &lt;  X  A Z  X  2 F . This, together with r ( ^ s ) = r ( ), implies that This contradicts to the fact that is a minimizer of (7). The converse is revealed in the proof of Theorem 3.
We prove the rst conclusion by constructing such a solu-tion path vector S . Let S 0 = 0, S r +1 = + 1 . De ne S k 1 k r as the solution of equation:  X P I t follows that where j is the j th largest non-zero singular value of Z . By (12), S k is increasing. In addition, by de nition of S S k +1 , whenever falls into the interval [ S k ; S k +1 ), the rank of a global minimizer of (8) would be no more than r k and larger than r k 1. In other words, is always of rank r k and is given by P r k ( Z ). Therefore, the constructed solution path vector S satis es all the requirements in the theorem.

Moreover, when S k &lt; S k +1 , Since P r k ( Z ) is independent of , g ( ) is a nondecreasing linear function of in each interval [ S k ; S k +1 ). Combined with the de nition of the solution path vector S , we conclude that g ( ) is nondecreasing and piecewise linear with each element of S as a kink point, as shown in Figure 1. The proof is completed. The proof uses direct calculations. First we bound the Kullback-Leibler loss. By Theorem 1, where the Frobenius-norm is invariant under orthogonal transfor-mation, it follows that where e  X  = we can conclude that: which implies that, where the last inequality follows from Lemma 2. Thus, The risk bounds then follow.

Second we bound  X  ^ r 0 0  X  2 F , which is equal to  X  V =  X  =  X  D 1 last term vanishes. Thus
F inally, if r ( A ) r 0 , E  X  ^ r 0 0  X  2 F 4 2 r ( A ) = r 0 , then
This section examines e ectiveness of the proposed method and compares it with the nuclear-norm regularization as well as the SVP method [17]. One bene t of our method is that it can evaluate the approximation quality of the inexact ones. For the SVP, we choose a default initial value 0 for this local method since no other choices are guaranteed to deliver bet-ter performance. Our numerical experiment, which is not reported in here, suggests that the SVP method is indeed sensitive to the choice of the initial value. For nuclear-norm regularization, we select a suitable regularization parame-ter value giving the solution satisfying the rank constraint in (3).
Simulations are performed under (2). First, the n p de-sign matrix A is sampled, with each entry being iid N (10 ; 1). Second, the p k truth 0 is generated by multiplying a p r matrix and an r k matrix, each entry of which has a nor-mal distribution over N (10 ; 1). The data matrix Z is then sampled according to (2) with e following iid N (0 ; 2 ) with = 0 : 5.

For predictive performance and rank recovery, we compute the MSE  X  A ( 0 ^ ^ r )  X  2 F , the absolute di erence j ^ r r and record the training time for each method, averaged over 100 simulation replications on a test set of size 10 n , where ^ r is tuned over integers in [0 ; min( n; p )] by an independent tuning set of size 2 n . We consider three possible situations, i.e., when k = r 0 &lt; p , k &gt; p &gt; r 0 and p &gt; k &gt; r illustrate our theoretical conclusion in Theorem 4 that the prediction error bound does not depend on the value of p , we add one more case of p &gt; k &gt; r 0 with a larger value of p . The simulation results are summarized in Tables 2-4. method.
 T able 3: Rank recovery for the synthetic data: averaged values of method.

As suggested in Tables 2 and 3, our exact method is much more precise in prediction in all cases and rank recovery except one case of k = r 0 = 5 &lt; p = 99, than the other two methods. This is in agreement with the theoretical results in Theorem 4 that exact reconstruction of the oracle estimator is achieved through tuning. Note that the best MSE value does not necessarily yield best rank recovery, as in the case of k = r 0 = 5 &lt; p = 99, which is due to the bias/variance trade-o phenomenon. As indicated in Table 4, our method is, on average, 10-20 times faster than the other two.
Next we examine the three competing methods for recon-structing the MIT logo image, which was studied in [27, 17]. The original logo is displayed in Figure 2, where we use the gray image of size 44 85 and the image matrix has rank 7.
Our objective is to reconstruct this image from its noisy version and examine the quality of reconstruction. Towards this end, we sample the design matrix A with each entry being iid N (0 ; 1), where the sample size n ranges from 20 to 80. To generate a noisy version, we add random error sampled from N (0 ; 0 : 5 2 ) to each element of the sample data. The reconstruction results are displayed in Figure 3 for the three methods with the default initial value 0 for the SVP.
Visually, our method delivers highly competitive perfor-mance as compared to the other two methods, as displayed in Figure 3, and yields nearly perfect reconstruction when the size of the design matrix A becomes larger, say n = 60 and n = 80. For all the methods, better reconstruction can be reached as n increases, and comparable results are achieved when n is small, say n = 20 and n = 40. This conclusion is consistent with our theoretical analysis.
In practice, the exact rank of the matrix to be estimated is unknown but may reasonably be assumed to be small. In this sense, s needs to be tuned or estimated. Next we inves-tigate the e ect of estimation of s with regard to the recon-struction quality, measured by the MSE and the recovery rate. The latter is de ned as the ratio  X  ^ s 0  X  F =  X  0  X  commonly used to measure the quality of parameter estima-tion. In particular, we display both of them as a function of s in Figures 4 and 5, where s is de ned in (3).

For the relative recovery error, a clear transition of our solution occurs around s = 44, after which a perfect recovery is achieved, whereas no improvement occurs for the other two methods when s increases. In the case of an under-determined A , i.e., A is not of full column rank, all three methods produce similar recovery results. For the MSEs, our method yields the perfect result zero when s 7 and a reasonably small value when s = 3 ; 5, whereas the other two methods lead to elevated MSEs as s increases. This is in accordance with our theory which suggests that our method constructs the oracle estimator giving perfect image reconstruction when s 7.
This paper derives an exact closed-form global minimiza-tion for nonconvex least squares rank minimization. In ad-dition, an ecient pathwise algorithm is also developed for its regularized counterpart. Moreover, we establish the op-timality of the global solution, against any other ones. Ex-perimental results on synthetic and real data demonstrate the eciency and e ectiveness of the proposed algorithm.
An exact global solution seems rare for nonconvex min-imization. In future work, we plan to expand the present work to a general loss function, by solving a sequence of weighted least squares rank minimization problems itera-tively. We will pursue along this direction for a general rank minimization problem. (for each case of n ): SVP; nuclear-norm regularization; our method. This work was supported by NSF grants IIS-0953662 and DMS-0906616, NIH grants R01LM010730, 2R01GM081535-01, and R01HL105397. [1] J. Abernethy, F. Bach, T. Evgeniou, and J. Vert. [2] Y. Amit, M. Fink, N. Srebro, and S. Ullman.
 [3] T. Anderson. Estimating linear restrictions on [4] T. Anderson. Asymptotic distribution of the reduced [5] T. Andre, R. Nowak, and B. Van Veen. Low-rank [6] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [7] F. Bunea, Y. She, and M. Wegkamp. Optimal [8] J. Cai, E. Candes, and Z. Shen. A singular value [9] E. Candes and Y. Plan. Matrix completion with noise. [10] E. Candes and B. Recht. Exact matrix completion via [11] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse [12] C. Eckart and G. Young. The approximation of one [13] M. Fazel, H. Hindi, and S. Boyd. A rank minimization [14] S. Friedland and A. Torokhti. Generalized [15] A. Izenman. Reduced-rank regression for the [16] M. Jaggi and M. Sulovsky. A Simple Algorithm for [17] P. Jain, R. Meka, and I. Dhillon. Guaranteed rank [18] H. Ji, C. Liu, Z. Shen, and Y. Xu. Robust video rank constraints. (d) in which the MSEs are identical to zero. [19] S. Ji and J. Ye. An accelerated gradient method for [20] B. Kulis, A. Surendran, and J. Platt. Fast low-rank [21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. [22] X. Luo. High dimensional low rank and sparse [23] B. K. Natarajan. Sparse approximation solutions to [24] S. Negahban and M. J. Wainwright. Estimation of [25] T. Pong, P. Tseng, S. Ji, and J. Ye. Trace norm [26] C. Rao. Matrix approximations and reduction of [27] B. Recht, M. Fazel, and P. Parrilo. Guaranteed [28] G. Reinsel and R. Velu. Multivariate reduced-rank [29] B. Savas and I. Dhillon. Clustered low rank [30] S. Shalev-Shwartz, A. Gonen, and O. Shamir.
 [31] X. Shen and H. Huang. Optimal model assessment, [32] D. Sondermann. Best approximate solutions to matrix [33] N. Srebro, J. Rennie, and T. Jaakkola.
 [34] K. Toh and S. Yun. An accelerated proximal gradient [35] J. Wright, A. Ganesh, S. Rao, and Y. Ma. Robust
