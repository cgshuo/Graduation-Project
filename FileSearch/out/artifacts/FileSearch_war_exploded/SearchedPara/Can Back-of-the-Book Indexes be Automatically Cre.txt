 H.3.3 [ Information Search and Retrieval ]: Text Mining; H.4 [ Information Systems Applications ]: Miscellaneous Back-of-the-Book Index, Book Index, Term Informativeness
A back-of-the-book index is a collection of words or phrases, often alphabetically arranged to allow users to locate in-formation in a given book. Creating an index for a non-fiction book is the responsibility of the author, but most authors do not actually do it. Usually indexing is done by freelancers hired by authors, publishers or an independent business which manages the production of a book, namely professional indexers. An index entry is an index term with its locators as shown in Figure 1. The locators are usually depicted by single page numbers, ranges of page numbers or even the section numbers, where a bold number indicates a more important reference. Besides, a good back-of-the-book index may also contain semantic relationships among those index terms. Typical relationships include 1) subcategory represented by subheadings such as those under  X  X ixture model X ; 2) homographs or synonyms indicated by  X  X ee X , e.g.  X  X LP, see multiple perceptron X ; and 3) cross references indi-cated by  X  X ee X  and  X  X ee also X . A good index is more than just an alphabetical list of all proper nouns (which is properly called a concordance) and their locations; it is a way of re-organizing the knowledge in a book that can guide a reader to the locations they most need to consult. So a good book index should be regarded as a creative work of a professional indexer, which reflects his/her understanding and analyzing of the book content; thus the indexing industry believe the book index creation cannot be replaced by computers 1 .
Indexing softwares can help to sort and manipulate index; establish subheading sequences; restyle and amend entries; and keep track of what has been indexed where; but can-not really create indexes  X  X utomatically X , or even provide some facility to do content analysis and understanding. Re-cent development of keyword extraction should render new assistance to the book indexing. Surprisingly, very little attention has been drawn to book indexing by taking ad-vantage of keyword extraction techniques except [4], which explored the automatic back-of-the-book index generation problem as a keyword extraction problem using supervised learning. However, a set of keywords is still far away from the book index. What really matters for the index is a term with its proper locators that point to the text. For example, in the book Pattern Recognition and Machine Learning [1], the term  X  X idden Markov model X  appears in more than 20 pages while the actual index entry has only 2 pages as its locators. Keyword extraction only gives a list of keywords in a context-oblivious way, without pointing out from what context the keyword should be indexed. Back-of-the-book index generation needs to identify the exact contexts/pages of a term to ensure it X  X  informative or valuable enough for indexing. The new challenge is how to identify the proper locators given a keyword. http://www.backwordsindexing.com/Comp.html
Besides, whether the supervised learning approach is a good choice seems questionable. First, books from different domains vary a lot in vocabulary composition and structure style, requiring various indexing specialties. For example, there are different indexing guiding books for medicine [24] and law [9]. Second, book indexing is a highly subjective work and indexes of different books are always created by different professional indexers who have their own prefer-ences and background [5]. Third, the learning is on an ex-tremely unbalanced training set. As we found in our dataset, the index size is only 0.42% of the length of book in aver-age. These deficiencies might cause the problems of domain dependence and harm the efficiency. All these give us the motivation to move a further step on book indexing by ex-plore generating index terms aware of their locators through an efficient training-free and domain independent approach.
Admittedly, the intellectual and analytical work of index-ing is the task of human, but can we do more than just give a list of keywords? Inspired by how human indexers work on book indexing, we presents a domain-independent and training-free approach, relying on the book content and structure and the web knowledge base. Given a book, the index terms will be selected according to 1) the indexabil-ity score measuring a word X  X  quality of being an index term for its context, combined with, 2) the context weight that measures the importance of a term X  X  context based on the book structure information. We redefine the back-of-the-book generation as an optimization problem aiming to find a set of index terms with good  X  X ndexability X  in contexts with high context weight. We then develop efficient algorithms to solve the problem and evaluated our methods on hundreds of books in more than 10 domains, including computer sci-ence, art, history, Psychology, and so on. The experimental results show that our approach is effective and efficient in creating book indexes and outperforms the previous super-vised learning approaches. Besides the domain-independent and training-free, we argue our approach could be easily ap-plied to books of other languages. There are plenty of indexing softwares such as CINDEX, Macrex 2 , and IndDoc [14], which provide aid to human indexers. They can help to produce a concordance or a word list, locate the various occurrences of a word, sort the entries in alphabetical order and format the resulting index according to different index styles. However, they by no means give any aid to the key indexing process, i.e. selecting an index term based on understanding of book content.
Though more and more books are becoming available on-line in electronic format, the automatic back-of-the-book in-dex generation, as an influential application for the index-ing society and publishing industry and also a challenging AI task, has not receive enough attention. In 1998, Schutze proposed hypertext concordance as a hypertext back-of-the-book index, which mainly focus on the design of connecting index entries and their occurrence in a book while simply select index terms based on the likelihood ratio of occurring http://www.asindexing.org/i4a/pages/index.cfm?pageid=3319 in the target document collection and a reference collection [16]. Csomai and Mihalcea first evaluated the performance of different informativeness measurements such as TFIDF,  X  , and KL divergence for selecting book index terms [3], and then studied it in a supervised learning framework [4] based on syntactic features, linguistical features, encyclo-pedic features and so on. However, their work is closer to keyword extraction in long documents rather than build a real book index.
The closest work of rich literature to back-of-the-book indexing is the keyphrase extraction, which has been ex-tensively studied in different application contexts such as academical and digital library resources [6, 18, 8, 13], web pages [12, 25, 15, 19] and social media documents [11, 26, 20]. Presumably many of the keyphrase extraction methods can be applied to select candidate index terms for books. However, they are inherently context-oblivious, limiting its capability to find the appropriate references of index terms. What X  X  more, most of the keyphrase extraction approaches are either highly supervised or must be run in a large cor-pus environment. Our is not only unsupervised and domain independent, but also corpus-free, thus can be much more practical for back-of-the-book index generation.
Unlike the previous approach that simply hires a brunch of features and then throws them to supervised learning mod-els, ours is inspired by the way professional indexers create real book indexes. They read the text, select the terms that readers are likely to inquiry and relates them to the text segments, based on not only the structure and content of a book but also, more importantly, their own background knowledge that enables them to gain a comprehensive un-derstanding of the book. Similarly, our method goes through the whole book sequentially and identifies the index terms according to the indexability score and context weight mea-sured by both inner information within the book and exter-nal information from web knowledge. Definition A book is a sequence of contexts, i.e. B = [ C 0 ,...,C | B | X  1 ]. A context is a syntactic unit of discourse such as a paragraph or a sentence, composed of a sequence of terms, denoted by C i = [ t i 1 ,...,t i | C i | ], where a term t is represented by an n-gram. The whole set of terms in book B is denoted by T . An index item is paired by an index term and its context, denoted by it = ( t,id C ), where t = it [0]  X  T and id C = it [1]  X  X  0 ,..., | B | X  1 } . A back-of-the-book index, denoted by E = { it 0 ,...it s  X  1 } , is a collection of index items where s is the index size or total number of index items. Given a book and its index size s , our goal is to generate s index items that maximize the indexability scores as well as have good coverage to all the information conveyed in the whole book. Note that the total number of index terms  X  s since an index entry can have multiple contexts. Based on the definitions, the back-of-the-book index generation can be formulated as the following problem: s.t. where Since our method is corpus-free, relying only on a given book, we can omit the B in all related notations.
The indexability is encoded by two factors, context-aware informativeness and keyphraseness.
Most known approaches to measure term informativeness fall into two categories: frequency-based and semantic-based. The frequency-based methods are based on derivations from term frequency (TF) and document frequency (DF), while the semantic-based methods compute latent semantic anal-ysis (LSA) term vector length or the ratio of a term X  X  LSA vector length to its document frequency [10]. Those meth-ods, all corpus-based, which are effective to identify infor-mative words in document level, lose the power to capture the term informativeness in a particular context, since they are context-oblivious[22]. We could segment a book into chapters or paragraphs, and then apply the corpus-based methods, or even more sophisticated keyword extraction al-gorithms. However, we still will face the difficulty in de-termining proper locators of index terms. We could hire the context weighting metric, which will be given in next subsection, to choose the contexts. However, before lever-aging the structure information of a book, can we derive a context-aware term informativeness measurement for a given context or even a general textual snippet? Our intuition is that higher informative terms in a context tend to have more similar contexts. Considering the following two contexts of  X  X raph X  from chapter 8 of PRML, it is easy to demonstrate that the first one tends to have more similar contexts on the web by search  X  X raph X  using Wikipedia, Google, and Bing. This motivates our definition of context-aware term infor-mativeness. 1. A graph comprises nodes (also called vertices) con-2. We shall now go from this graph to the correspond-Definition Given a term t and its context C , suppose the featured context set of t is U f = { C i | i = 1 ,...,k } , the term informativeness of t in context C i is defined as where sim ( C,C i ) is a similarity distance between C and C and p ( C i ) is the prior importance of C i to t . Wikipedia is used as the knowledge base since it is cur-rently one of the largest and most readily available knowl-edge repositories. Given any keyword, the Wikipedia query API will return the ranked Wikipedia entries along with the contexts containing the keyword. The top k results are con-sidered as the featured context set and the discounted rank is used to estimate the prior p ( C i ).
We assume the keyphraseness of a term is qualitatively de-termined by two key properties: 1) it should be a meaning-ful phrase in the book; 2) it is highly possible that the term had been selected as a keyphrase in other similar resources. Pointwise Mutual Information (PMI) [2], was shown to be the best metric to measure word association [17]. Given an n-gram ( x 1 ..x n ), its PMI is defined as where p ( x ) is the probability of occurrence of word x and p ( x 1 ..x n ) the corresponding joint probability in a book.
In Wikipedia a term is marked as  X  X eyword X  in terms of title or anchor text in a link. The Wikipedia keyphraseness estimates the probability of a term t being a keyword in a new document by the ratio of number of documents where t was marked as a keyword to its DF [4].
 This metric is computational impractical due to the high cost for harvesting count ( D key ) online. It is even costly to precompute WK of all keywords in Wikipedia to provide local fast access. Thus we present a new lightweight version by leveraging the power of Wikipedia API.
 where Backlinks ( t ) lists pages that link to t by Backlinks API 3 and totalhits ( t ) returns number of hit pages by Search API 4 .
It is possible that human indexers may not be knowl-edgeable enough to gain an exact understanding of every book they work on, especially the professional and technical books. However, they can still create an acceptable index using their indexing techniques, most of which are based on the fact the structure and format information in a book gives useful clues for what should be indexed. For example, words in chapter titles should be more likely to be indexed than those in general paragraphs. To leverage the structure infor-mation, the context weight score is defined to measure the importance of a context w.r.t indexability based on solely on structure or format features, which have been previously shown useful for keyphrase extraction [7]. We define the context weight as a weighed sum of the three indicators: where http://www.mediawiki.org/wiki/API:Backlinks https://www.mediawiki.org/wiki/API:Search
Algorithm 1: Back-of-the-book index generation 1 Input : a book B 2 Output : a book index E sizes s 3 begin 4 initialize a temporary index  X  E ; 5 for C  X  B do 6 cands  X  X  X  getCandidates ( C ); 7  X  C  X  X  X  getContextWeight ( C,cands,B ); 8 if  X  C &lt;  X  0 then continue; 9 for t  X  cands do 10 S  X  getIndexability ( t,cands,C ); 11 if S &lt; S 0 then continue; 12 Score  X  X  X   X  C  X  S ; 13 add (( t,id C ) : Score ) to  X  E ; 14 move top s highest Score items from  X  E to E ; 15 while  X  it j s.t. it j [1]  X  it j  X  1 [1] &gt;  X  0 in E do 16 S 1  X  X  it  X   X  E | it j  X  1 [1]  X  it [1]  X  it j [1] } ; 17 move arg max 18 S 2  X  X  it k  X  E | it k +1 [1]  X  it k  X  1 [1]  X   X  0 } ; 19 remove arg min
An efficient algorithm for the back-of-the-book index gen-eration problem is presented in Algorithm 1. getCandidates returns the candidate index terms of a context based on POS patterns and stop-words. getContextWeight calculates the context weight score defined in Eq.(8). The default  X  1 ,  X  and  X  3 are set to 0.5, 0.3 and 0.2.  X  t ,  X  d , and  X  e initialized to zero. The effective length is calculated by the ratio of number of candidates to the total number of grams in context C , shown in line 5.  X  t and  X  d will be set to 1 if the context is a chapter or subchapter title. All the chapter and subchapter titles of a book are collected by detecting its table of content [23]. We then simply check whether C can find a fuzzy match in the title collection. If C is not a title, we then set  X  t to 0 and compute  X  t based on the distance of C deviated from its direct title. We find the title searching the nearest context before C from the title col-lection. Number of contexts under a title is the difference between id of a title and id of its next sibling title. getIndexabilityScore computes the indexability score of each candidate index term in its context. The default im-plementation of Sim for context-aware term informativeness score is the cosine similarity using candidate index terms as the feature vector. The similarity score is discounted by log( rank + 1) as an approximation of the prior probability p in Eq. (4). Finally, we use a hybrid score combining the context-aware informativeness with term frequency, PMI, and Wikipedia Keyphraseness. In the experiments, we use a simple linear combination with the same weight and each part normalized to [0,1]. We use TF instead of TFIDF because we found that TF is comparable to TFIDF for identifying index terms. Moreover, it is costly to have a huge word dictionary with DF that covers all candidate in-dex term for an arbitrary book while computing TF is super easy and fast. The PMI score is computed according to Eq. (5) while the probability of an n-gram is simply estimated using its TF in a book. The Wikipedia keyphraseness is based on Eq. (7).

The smaller  X  0 and S 0 are, the more items will be added to  X 
E . Their default values are set to 0 . 01. The final s items are tweaked to satisfy Eq. (3), i.e., making the distance of each two adjacent items no larger than  X  0 , delineated by line 15-19. In line 16-17, we select from  X  E the item of the highest Score between the two items whose distance is large than  X  0 and add it to E . In line 18-19, we remove from E the item of the lowest Score while the distance between its previous and next item is no larger than  X  0 . We set  X  0 the minimum number of contexts in a subchapter.
To evaluate our back-of-the-book index generation method, we conduct extensive experiments on books in various do-mains, from the Gutenberg dataset and the open book dataset described in Table 1. The first one was created by [4], con-taining 55 free books collected from Gutenburg. Every book in the dataset has the body text, the original version and three cleaned versions of its index. The cleaned versions are a list of keyphrases from the original index without their locators. However, there is no page number information in the body text. So on this dataset we cannot consider the location of index terms, but only serve the evaluation as a keyword extraction task. Books in the second dataset was collected from the Citeseer repository, most of which are in computer science and engineering [21]. We manually select 213 books with good quality back-of-the-book index. The original books are all in pdf format. We extracted the paged body text and the back index using Pdfbox. Having each in-dex term associated with its locators (page numbers), we can perform fairer evaluation for different methods, not solely in the keyword extraction style.
The POS patterns can be represented by regular expres-sion  X  X IJN] { 0,4 } N X . A single letter  X  X  X  represents a word whose tag starting with  X  X  X , belonging to a Noun.  X  X  X  indi-cates an Adjective.  X  X  X  denotes a Preposition.  X  X IJN] { 0,4 } N X  contains all the n-grams ( n  X  [1 , 5]) where the last word is a Noun and all the previous ones are Adjective, Prepo-sition or Noun. When tagging, we use the model  X  X sj-0-18-left3words-distsim.tagger X  from the Stanford POS tagger . We first examine whether our candidate index term gen-http://nlp.stanford.edu/software/tagger.shtml
Figure 2: Results for candidate index term generation eration method is capable to enumerate all potential index terms. Ideally, we hope it can cover 100% of the true index terms by generating a relative small number of candidates, since the number of items from a book index is much smaller than the number of words from the book text. We estimated the average ratio as 0.42% in our open book dataset, which is very close to 0.45% shown in the previous work[4]. The distribution of the ratio is shown in Figure 2a. Figure 2b shows the ratio of number of generated candidates to the to-tal number of words in each book. It covers more than 95% of the ground truth using only less than 25% of number of all the words in most books. In average, there are 104600 words in a book while number of candidates is 12055, in a ratio of 11.5%. Considering that those candidates are se-lected from unigram to five-gram, it actually rules out far more than 88.5% of the total grams.
To measure the effectiveness of our approach as well as the roles of  X  X ndexability X  and  X  X ontext weight X , we run ex-periments on the open book dataset using three methods, showing in Figure 3a. CTI is the context-aware term in-formativeness in Section 3.2.1. Indexability does not use context weight. CI-Indexer represents method shown in Al-gorithm 1. The results clearly demonstrate that TF , PMI , and WK can gain around 4% more recall over CTI while context weight can bring around 6% more recall over index-ability. Note that here the recall of CIT is lower than that in Section 4.3 because we take the locators of index terms into consideration. Only a index term with its context in the right page will be counted into recall.

We then compare our method with three baselines includ-ing TFIDF, KEA 6 , and SLD (supervised learning using de-cision tree in Csomai X  X  [4]). KEA builds a Naive Bayes model using features TFIDF, first occurrence, length of a phrase, and node degree (number of candidates that are semanti-cally related to this phrase). First occurrence is computed as the percentage of the document preceding the first occur-rence of the term in the document. We compute the node degree as the textrank[13] degree in a book by simply relat-ing two candidate terms with each other if they are in the same context. For SLD, we hire all the features presented in [4] except the discourse comprehension based features which are too complicate to implement. We choose decision tree because its training is much faster than other two models in [4] while its performance is close to the best. Since those three are context-oblivious, we set two strategies to make them context-aware. First, we select the page of a term X  X  http://www.nzdl.org/Kea/ first occurrence as its locator, denoted by  X +FirstOccurence X  in Figure 3. Second, we apply our context weight to them, denoted by  X +CW X . The results are shown in Figure 3b, 3c and 3d respectively. For all the three baselines, adding con-text weight gains better performance than using the simple first occurrence guess, especially for TFIDF. KEA benefits least from context weight, suggesting its first occurrence and node degree features play part of similar role as our context weight features. SLD outperforms TFIDF and KEA under both the two strategies probably because the new features of POS pattern and Wikipedia keyphraseness. SLD+CW performs closest to ours.
The context weight can be regarded as an  X  X ndexability X  measurement in context level. To further understand the importance of different ingredients in context weight, we run the CI-Indexer by ruling out one part of context weight and set the other two equally weighted. The experimen-tal results are shown in Figure 4a denoted by CI-Indexer -CW(title), CI-Indexer -CW(distance) and CI-Indexer -CW(length). Based on the decrement from CI-Indexer to each of them, we could conclude that  X  X itle X  is more impor-tant than  X  X istance X  while  X  X ength X  is the least important part. We also study the sensitivity of their weights  X  1 and  X  3 and recommend their setting following  X  1 &gt;  X  2 with  X  1  X  [0 . 4 , 0 . 6],  X  2  X  [0 . 2 , 0 . 4] and  X  3 Another important parameter is the number of retrieved snippets from Wikipedia when computing the context-aware term informativeness. We run the CTI in Section 4.3 by set-ting the number to 5, 10, 15 and 20. Results in Figure 4b shows the performance of CTI is not sensitive to this number but gain slight improvement as it increases under different output index size.

Time is not an issue for our approach since it is training free and corpus free. For a book of around 200,000 words, while our approach totally costs 5-8 minutes to generate an index sizing 5 times of the ground truth, the supervised learning approach costs 10-15 minutes apart from the much more offline training time. More than 60% time of our ap-proach is cost on the computation of CTI. In terms of scala-bility, the proposed method is inherently parallelizable, not only at the document level, but also a the context level, since computing CTI does not depend on any other context in the document. In addition, we do not need to issue the same query more than once. Our strategy is to locally cache the returned results of every seen query. For a new term seen in a previous query, we can directly access the local cached file. If we have built a large local pool, the queries will rarely go to a search engine or other source. Given a corpus size N (words in total), the number of actual issued queries will be at most the number of unique terms, which is far less than O(N). Of course, new terms never seen will have to be processed, but there will be fewer of these over time.
In this paper, we tackled the back-of-the-book index gen-eration problem in a novel context-aware approach, which is capable of giving index terms as well as their locators. We measure indexability of a term in context by informative-ness and keyphraseness . We then developed a new context-aware term informativeness measurement by leveraging web knowledge base. and a computationally practical Wikipedia keyphraseness. The structure information residing in book text are then encoded as context weight to weight the im-portance of contexts. We conducted extensive experiments to examine the performance of our approach.

In the future, we will make our back-of-the-book index generation system publicly available. Besides, the automatic construction of semantic relationships between index terms, which are not addressed in this work, including subheadings, homographs or synonyms, and cross references, could all be promising future research topics. [1] C. M. Bishop. Pattern Recognition and Machine [2] K. W. Church and P. Hanks. Word association norms, [3] A. Csomai and R. Mihalcea. Investigations in [4] A. Csomai and R. Mihalcea. Linguistically motivated [5] V. Diodato and G. Gandt. Back of book indexes and [6] I. H. W. E. Frank, G. W. Paynter and C. Gutwin. [7] K. Hofmann, M. Tsagkias, E. Meij, and M. de Rijke. [8] E. F. C. G. I. H. Witten, G. W. Paynter and C. G. [9] P. Kendrick and E. L. Zafran. Indexing Specialities: [10] K. Kireyev. Semantic-based estimation of term [11] Z. Li, D. Zhou, Y.-F. Juan, and J. Han. Keyword [12] R. Mihalcea and A. Csomai. Wikify!: linking [13] R. Mihalcea and P. Tarau. Textrank: Bringing order [14] A. Nazarenko and T. A. E. Mekki. Building [15] E. F. O. Medelyan and I. H. Witten.
 [16] H. Schutze. The hypertext concordance: a better [17] E. Terra and C. L. Clarke. Frequency estimates for [18] P. D. Turney. Learning algorithms for keyphrase [19] R. West, D. Precup, and J. Pineau. Automatically [20] W. Wu, B. Zhang, and M. Ostendorf. Automatic [21] Z. Wu, S. Das, Z. Li, P. Mitra, and C. L. Giles. [22] Z. Wu and C. L. Giles. Measuring term [23] Z. Wu, P. Mitra, and C. L. Giles. Table of contents [24] L. P. Wyman. Indexing Specialities: Medicine . [25] Y. Z. Z. Liu, W. Huang and M. Sun. Automatic [26] W. X. Zhao, J. Jiang, J. He, Y. Song,
