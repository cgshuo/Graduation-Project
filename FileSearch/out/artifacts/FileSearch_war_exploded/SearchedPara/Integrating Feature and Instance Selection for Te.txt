
Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data. Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances. So far, these two methods have mostly been considered in isolation. In this paper, we present a new algorithm, which we call FIS (Feature and Instance Selection) that targets both problems simultaneously in the context of text classification 
Our experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances. The accuracy of a range of classifiers including Naive Bayes, TAN and LB considerably improves when using the FIS preprocessed datasets, matching and exceeding that of Support Vector Machines, which is currently considered to be one of the best text classification methods. In all cases the results are much betier compared to Mutual Information based feature selection. The training and classification speed of all classifiers is also greatly improved. 1.5.2 [Pattern Recognition]: Design Methodology -Feature evaluation and selection. 1.5.4 [Pattern Recognition]: 
Applications -Text processing. 
Feature selection is a major research area within IR since the reduction of the features used for the representation of documents is an absolute requirement for the use of any but the simplest machine learning algorithms [3]. Feature selection methods reduce the dimensionality of datasets by removing features that are considered irrelevant for the classification. This transformation procedure has been shown to present a number of advantages such as smaller dataset size, less computational requirements for the classification algorithms (especially those that do not scale well with the feature set size), reduction of the search space and improved classification accuracy 
As Figure 1 illustrates, instance selection [3] works orthogonal to feature selection. The aim here is the reduction of the number of instances presented to the classifier during the training phase. The not made or distributed for profit or commercial advantage and that requires prior specific permission and/or a fee. SIGKDD '02, July 23-26, 2002, Edmonton, Alberta, Canada. 
ConvriQht 2002 ACM 1-58113-567-X/02/0007...$5.00. motives are similar to the ones considered in feature selection and include smaller dataset size, faster training and classification time for the classification algorithms (especially those that do not scale well with the dataset size) and improvement of the input quality by removing noise introduced by inconsistent examples and by examples that do not provide information useful for the classification. 
In this paper, we present our approach that deals with both problems simultaneously. Our algorithm, which we call FIS (Feature and Instance Selection), targets the feature and instance selection problem in the context of text classification. FIS works in two steps. In the first step, it sequentially selects features that have high precision in predicting the target class. All documents that do not contain at least one such feature are dropped from the training set. In the second step, FIS searches within this subset of the initial dataset for a set of features that tend to predict the complement of the target class and these features are also selected. The sum of the features selected during these two steps is the new feature set and the documents selected from the first step comprise the training set. 
Our experimental results with standard benchmark datasets show that FIS considerably reduces both the number of features and the number of instances. Equally important, the accuracy of a range of classifiers including Narve Bayes [4], TAN [6] and LB [12] is considerably increased when the resulting training sets are used instead of the original ones. In some cases, these classifiers prove to be as accurate as the Support Vector Machines [7], which is currently considered one of the best text classification methods. 
A very good survey on feature and instance selection as two independent problems in the context of machine learning is presented in [3]. In the context of information retrieval and text classification, several works have indicated that effective feature selection can enhance the performance of classifiers, In [5], [11] and [17] a few tens or hundreds of words maximize the performance of a range of classifiers. Similar results are reported in [9][13] as well. SVMs are a notable exception to this since they achieve the highest accuracy when almost all words are used, but they do not scale up in the dataset size 
John et. al [8] identify three types of features; Irrelevant features, which can be ignored without degradation in the classifier performance, strongly relevant features that contain useful information such that if removed the classification accuracy will degrade and weakly relevant features that contain information useful for the classification, but are unnecessary given that some other words are present in an instance. In the design of FIS, we took into account this definition of relevance, which is very intuitive. Some words may contribute to the distinguishing of the class, but they might be redundant, as they tend to co-occur with other words, which are also good predictors. Therefore, we can reduce the redundancies hoping that we will not reduce the amount of information in the dataset. 
There is not much research on Instance Selection for text classification. The issue is mostly addressed either with the traditional statistical approach of sampling [16] or by more elaborate, but sometimes heuristic, approaches. Most of the work refers to Instance-based or lazy algorithms [1]. In [15] the problem is addressed using a distance measure. In essence instances that are "closer" to each other tend to bear overlapping information; therefore, some of them can be discarded. Active 
Learning [10][14] is another approach to Instance Selection where the learner has access to a pool of unlabeled instances and can request the labels for some of them. 
In this section we describe in detail our algorithm for feature and instance selection, called FIS (Feature and Instance Selection), and state the objectives behind our choices for the various steps of the algorithm. 
Assume a document collection D ={dl,de,...dloL}, where each document db k in {1,2 ..... ID[}, contains one or more words from a vocabulary IV={IVi, IV2,... V6,4. Each word 1~ is associated with a binary variable w~ where w:l, if IV/is present one or more times in d, and we=O, if t'6 is not present in dk. In addition, each document dk is associated with a class label c, which indicates whether d, belongs to the target class C (c=l) or not (c=0) in which ease we will say that it belongs to class C'. For the sake of simplicity, we ignore the number of occurrences of a particular word IVi in a document dk as well as their relative position in the document. Instead, we use the so-called "Bag of words" [11] document representation. Additionally, we consider only the binary class problem; multi-class problems have to be split into a sequence of binary class problems. Although in some cases this is not very convenient, it is the case in a variety of problems where a document may belong to more than one possible class. 
The FIS algorithm operates in two steps. During the first step FIS searches for a subset Fe of the original vocabulary Ivthat contains the words of IV that are the best predictors of the given class C. 
We call such words positive features. As a convenient side effect of this selection, D is pruned and only the documents with non-empty projection on Fp are kept. The resulting dataset DFe contains the documents with at least one word from Fe. All other documents are ignored during training and assigned to C' during classification. Our goal is that the set DFe contains the majority of the C-labeled documents and only a small portion of the documents from C' while at the same time the documents outside 
DFe will mostly belong to C'. Taking this argument to the extreme, in the ideal case DFe would contain all documents of class C and only these documents. In reality this is not the case discovering words that are good predictors of C'. 
In the second step FIS searches within DFp for the set of negative feature.,; FN. This step is exactly symmetrical to the first one and the resulting set contains the words of tV that are the best predictors of class C' within DEe. The output of FIS is the feature set F = Fp u_F/ X  and the instance set T = DFe that contains the documents ill DFe represented using the features in F. time to discover Fp from the total set of documents D and the second time to discover FN from the documents in DFe  X  14. F=Fu{f} 502 
The procedure FIS0 starts with an empty set of relevant features F and an empty set of relevant documents DF. At each iteration it selects the best new feature f to be added to the set of relevant features, where "best" is measured with the function line 7 of Figure 2. This feature is added to F and all documents that contain it are added to DF if they are not already there. This procedure repeats until some stopping criteria are met. 
Figure 3 illustrates the sets involved in the operation of FIS0. At some point in its operation FIS0 will have created a set selected relevant documents. The aim of FIS0 is to ensure that De will contain most, and ideally all, C-labeled documents without containing many (and ideally containing none) C-labeled documents. In other words, the objective of FIS0 is to maximize [DF nDc[ while keeping [DF-Dcl as small as possible. Note that in the ideal case the set Dr would contain all and only the documents that belong to class C, i.e., the feature selection would have built a perfect classifier. 
Let us now examine the criteria for the selection of the next feature to be inserted in the set of relevant features F. For this, assume that the feature f is currently evaluated and 
Figure 3, is the set of all documents that containf The wordfwill be inserted into F if its score, measured in line 7 of FIS0, is the highest among all other candidate words. The sets Fi and F~, shown in Figure 3, are used to derive a value for the function 
Figure 3: The document sets involved in evaluating the The values of increase in the number of positive and negative examples in DF respectively, if all the documents that contain the word f are added to DF. By dividing nominator and denominator by we can show that score~f) essentially represents the ratio of the precision of the word f as a predictor of C in the set versus the precision of the wordfas a predictor of C' in the same set. The set Df -DF is the set of documents that contain wordf but do not contain any of the previously selected words. 
The set F generated by FIS0 contains words that also exist in negative examples and consequently D F contains negative examples. As new features are added to F, DF grows and along with DF, [DF nDc[ and [DF-Dc[ grow as well. As the of the new features inserted into F in later iterations decreases, the rate increases. This means that each newly added feature adds fewer positive and more negative documents in DF than its predecessors do. In the extreme case, if all features are added to F DF will contain all documents in D. The final size of DF, however, as well as the proportions of and 12 of FIS0: 1. Sf &gt; min_seore, where rain_score is a user defined constant. 
For example a value of min_score=l means that FIS0 will stop if all candidate words introduce more C-labeled than C-labeled documents to De. We fixed the second constant to a value of c_support=O.Ol. This requires the frequency of fin the dataset to be higher than a minimum threshold of 0.01.[D~ to overfitting. 
The FIS algorithm described in Figure 4 is a wrapper algorithm in D, and DEe contains all documents from D that have at least one word from Fp. DFe may be regarded as the union of two sets: DEe n Dc and DFe -D c . This means that it contains both documents that belong to C and documents that belong to C'. The relative size of DEe n Dc and DFe-De depends on the quality of the words in Fe and the threshold min_score_pos. obvious that the higher the precision of the words in Fe as predictors of C, the smaller the size OfDFp -De. FIS ( W, D,min score_pos,min_score neg, c_support, F, DF) rain score_neg, c_support F= FeuFN i.FIS0(C, W, D, min_score_pos, c support, Fp, Dpr). 2.FIS0(C',W, mpp ,min_score neg, c_support,F, ,DFM ). case, Dee c~Dc = Dc and DFp -Dc = {}. In the real world, however, DF e -DC usually contains many documents that might drive a classification system to make wrong decisions. This is the reason why FIS calls FIS0 again to perform a second step of feature extraction. 
In the second step FIS0 looks only within DFp for a set of words that may accurately discriminate documents that belong to C' from documents that belong to C. Our experimental results showed that if we represent the new dataset with the features of 
Fp alone and feed them to a classifier, the classifier will not be able to accurately learn class C. This is why the second iteration that returns F~v, is absolutely necessary. 
As a result of the feature selection, FIS produces the transformed data set DFe where documents are represented with the features from FpuF N . We set ad-hoc values for the parameters min_score_pos and min_scoreneg as 0.01 and 1, respectively. 
Intuitively min_score_pos should be minimal but non-zero to guarantee that even a word that rarely appears in positive documents will be included in Fp. A value of min_score_neg=l ensures that a word will not be characterized as negative unless it contributes more new negative cases than positive ones. In our experiments we show that this transformed dataset leads to superior results by a variety of classification algorithms. 
For our experiments, we used two standard benchmark document collections, the Reuters-21578 and the 20 Newsgroups [2] collection. The classification algorithms we used for the evaluation were NB, LB, TAN and SVM. In the following paragraph, we briefly present the algorithms used in the experiments. Then, we describe the datasets and the experimental setup. We next provide an overview of the experimental results and finally we drill down discussing the performance of the algorithms in more detail. 
Our experiments evaluated the competitiveness of FIS as a feature and instance selection method. We chose Mutual Information (MI) based feature selection as the alternative for the comparison with FIS. Note that within the Information Retrieval literature Mutual Information is sometimes confusingly called Information Gain. Here we use the standard Information Theory definition. 
The MI-based feature selection algorithm selects for each class C a local dictionary consisting of the K words wt with the highest average mutual information with C: 
The classification task requires that a document may be assigned in none, one or more than one categories. 
Feature selection methods influence the performance of classification algorithms in a different way. We used four algorithms: Naive Bayes (NB) [4], Tree augmented Naive Bayes 
Classifier (TAN) [6], Large (or Local) Bayes Classifier (LB) [12] and Support Vector Machines (SVM) [7]. The 20 Newsgroups contains about 20,000 newsgroup postings from 20 different UseNet groups. Each document belongs to one or more groups. The documents are evenly divided among the classes. We extracted word tokens from the data and removed words that occur only once in the whole collection. All headers from the postings (including the newsgroup header of course) were removed and only the body was used for training or testing. No stemming was used. To reduce the vocabulary size we kept only features that occurred in at least five documents and removed stop-words. The resulting vocabulary contained 12,357 words. For each class we created a training set consisting of the first 80% of the documents and a testing set containing the last 20% of the data set. The Reuters-21578 Distribution 1.0 [2] consists of 21,578 stories from the 1987 Reuters newswire, each one pre-assigned to one or more of a list of 135 topics. We used the 'ModApte" training-test split that contains 9,603 training and 3,299 test examples. All words were converted to lower case, punctuation marks were removed, numbers were ignored and stop-words were removed. Within Reuters the frequency of the classes is highly skewed. Following a practice popular in the literature, we only used the 10 most popular classes for our experiments. In both cases, the classification task requires that a document may be assigned in none, one or more than one categories. We followed standard practice and treated the problem as a series of binary classification problems. We evaluated performance using the standard Information Retrieval measures recall, precision and accuracy. To combine recall and precision with a single-value metric that can be used to derive a total order on the classifiers we use the Fi measure, defined as follows: FI (recall, precision) = The algorithms were optimized to yield maximum Ft scores using a validation test consisting of the last 25 per cent of the training stories. This is done with an additional pass over the data to adjust the decision thresholds of the algorithms to maximize F1. 4.2.2 Results discussion The positive influence of FIS in the performance of classification algorithms is summarized in Table 1 and Table 2 that list the FI-measure, accuracy and train/test time of the four classifiers described above in the Reuters (Table 1) and 20-newsgroups (Table 2) datasets. The two tables compare FIS as a feature and instance selection method with Mutual Information (MI) based feature selection. MI is a commonly used method for feature selection and it has been shown to yield very competitive results compared to other feature selection methods [17]. In both datasets, there is a significant improvement in the  X  classification quality of all four algorithms as measured by the FI measure when FIS is used as the feature selection method compared to MI-based feature selection. Interestingly, the performance improvement with FIS is higher for the methods that have the lowest performance when MI is used. We believe that this is attributed to the following two reasons: returned by FIS compared to the features returned by MI, and compared to the total dataset size. F1 57.5 69.52 62.6 67.98 59.2 58.63 64.99 56.23 Accuracy 95.5 196.98 96.4 96.89 95.5 i97.02 97.01 96.89 Time 2.5 [0.66 226.2 87.75 1.25 2,78 56 10.1 Test Time 1.36 10.36 2.74 0.71 4.9 0.54 1.65 C).62 
Table 3: Effect of FIS and MI on "20 Newsgroups" and 
Newsgroups is a much tougher domain. The set Fp contains an average of 118 features per class; more than four times larger than in Reuters. F1 NB TAN ~ LB I SVM Class MI FIS MI FISI MI FIS i MI FIS Earn 96.90 96.60 96.71 96.751 97.4 97.21 97.66 97.67 
Acq 87.33 92.01 90.99 91.93 89.66 91.94 91.47 92.67 money-fx157.45 73.25 63.70 69.36168.28 72.77165.06 75.86 Grain 75.08 92.31 84.21 92.2683.06 83.44 91.56 91.75 90.55 Crude 180.11 83.8282.89 82.48 81.84 80.87 82.04 Trade 54.98 65.44 57.73 66.67 61.33 66.37170.20 66.67 Interest 50.90 70.0861.03 69.32 62.91 67.91 62.50 68.33 Wheat 69.66 89.61 78.20 88.89~ 71.19 89.47 84.29 86.45 Ship 81.08 78.82 82.96 79.76 83.16 73.20[77.22 79.04 Corn 52.48 90.32 78 90.16 72.44 90.16i87.72.88.52 
Table 4: Per class classification quality for Reuters-21578 class [ FIS M! 
Avg, I ~ I (N) 118 159792 733 2525 20 1453 761 11110 
Avg. i i (R) j 27 62 719 716 1471 72 764 716 [ 8913 
Table 5: Average characteristics of the datasets generated by FIS 
Mutual Information has long been considered the core methodology for feature selection in automated text categorization. In this paper we introduced a new algorithm, named FIS, which combines feature and instance selection. The 
FIS algorithm is easy to implement, very fast, while it greatly decreases the number of features and training instances required to train accurate text classifiers. Training and testing times for text classification are also decreased, in some cases to an order of magnitude. Furthermore, the accuracy of the well-known Naive 
Bayes classifier increases to such a degree, that, in many cases, it proves to be equally or more accurate than Support Vector Machines, one of the most accurate classification systems today. 
A key task that remains to be done is to examine the performance of individual components of FIS; initial experiments have shown that if only the set of positive feature is used the classification performance suffers. We intend to measure the contribution of both the positive and the negative features to the classification accuracy. Similarly we intend to measure the contribution of instance pruning and of feature pruning to the classification accuracy. This will provide a deeper insight to the operation of 
FIS and may lead to enhancements of the algorithm. We also intend to extend FIS for dealing with multiclass problems and to apply it to structured data in addition to text. [1] D. W. Aha, Lazy Learning, (Reprinted from Artificial [2] S. D. Bay, The UCI KDD Archive [http://kdd.ics.uci.edu]. [3] L. Blum and P. Langley. "Selection of relevant features and [4] R. Duda, P. Hart, Pattern Classification and Scene Analysis, [5] S. Dumais, J. Platt, D. Heckerman and M. Sahami, [6] N. Friedman, D. Geiger, M. Goldszmidt, "Bayesian Network [7] T. Joachims, "Text Categorization with Support Vector [8] G. John, R. Kohavi and K. Pfleger, "Irrelevant Features and [9] D. Lewis, "Feature Selection and Feature Extraction for Text [10]D.Lewis and W.Gale. "A sequential algorithm for training [ll]A. McCallum and K. Nigam. "A Comparison of Event [12]D. Meretakis, D. Fragoudis, H. Lu and S. Likothanassis, [13] E. Riloff, "Little words can make a big difference for Text [14] G.Schohn and D. Cohn. "Less is more: Active learning with [15]D. R. Wilson and T. R. Martinez, "Instance Pruning [16] Yang, Y. "Sampling strategies and learning efficiency in text [17] Y. Yang and J. O. Pedersen, "A Comparative Study on 
