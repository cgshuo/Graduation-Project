 Social networks [9,11] are made of social entities (e.g., users) who are linked by some specific types of relationships (e.g., friendship, common interest, kinship). Facebook, Google+, LinkedIn, Twitter and Weibo [13,16] are some examples of social networks. Within these networks, a user f i usually can create a personal profile, add other users as friends, endorse their skills/expertise, exchange mes-sages among friends. These social networks may consist of thousands or millions of users, and each user f i can have different number of friends. Among them, some are more important or influential than others [2,6,7,15,17].

Over the past few years, several data mining techniques [5,8,12,14] have been developed to help users extract implicit, previously unknown, and potentially useful information about the important friends. Recent works on social network mining include the discovery of strong friends [3] and significant friends [10] based on the degree of one-to-one interactions (e.g., based on the number of postings to a friend X  X  wall).

However, in some situations, it is also important to discover users who (i) are influential in the social networks, (ii) have high level of expertise in some do-mains, and/or (iii) have diverse interest in multiple domains. In other words, users may want to find important friends based on their influence, prominence, and/or diversity. For instance, some users may be narrowly interested in one specific domain (e.g., com puters). Other users may be interested in a wide range of domains (e.g., computers, music, spor ts), but their expertise level may vary from one domain to another (e.g., a user f i may be a computer expert but only a beginner in music).

In this paper, one of our key contributions is an efficient tree-based algorithm called Div-growth for mining diverse friends from social networks. Div-growth takes into account multiple properties (e.g., influence, prominence, and/or diver-sity) of friends in the networks. Another key contribution is a prefix-tree based structure called Div-tree for capturing the social network data in a memory-efficient manner. Once the Div-tree is c onstructed, Div-g rowth computes the diversity of users based on both their influence and prominence to mine diverse groups of friends.

The remainder of this paper is organized as follows. We introduce the notion of diverse friends in the next section. S ection 3 introduces our Div-growth algo-rithm, which mines diverse friends from our Div-tree. Experimental results are reported in Section 4; conclusio ns are presented in Section 5. Consider a social network on three different domains (domains D 1 ,D 2 ,D 3 )and seven individuals ( Amy, Bob, Cathy, Don, Ed, Fred &amp; Greg ) with prominence values in each domain, as shown in Table 1(a). Each domain represents a sub-category (e.g., sports, arts, education) of interest. The prominence value of an individual reveals his level of expertis e (e.g., importance, weight, value, rep-utation, belief, position, status, or significance) in a domain. In other words, the prominence value indicates how important, valued, significant, or well-positioned the individual is in each domain. The prominence value can be measured by us-ing a common scale, which could be (i) specified by users or (ii) automatically calculated based on some user-centric para meters (e.g., connectivity, centrality, expertise in the domain, years of membership in the domain, degree of involve-ment in activities in the domain, numbers of involved activities in the domain). In this paper, the prominence value is normalized into the range (0, 1]. As the same individual may have different levels of expertise in different domains, his corre-sponding prominence value may vary from one domain to another. For example, prominence value Prom D 1 ( Amy )of Amy in domain D 1 is 0.45, which (i) is dif-ferent from Prom D 2 ( Amy )=0.60 and (ii) is higher than Prom D 1 ( Cathy )=0.20 (implying that Amy is more influential than Cathy in D 1 ).

Consistent with existing settings of a social network [3,5,10], let F = { f 1 ,f 2 , ...,f m } be a set of individuals/friends in a social network. An interest-group list L  X  F is a list of individuals who are connected as friends due to some common interests. Let G = { f 1 ,f 2 ,...,f k } X  F be a group of friends (i.e., friend group) with k friends. Then, Size ( G )= k , which represents the number of individuals in G .A friend network F SN = { L 1 ,L 2 ,...,L n } is the set of all n interest-group lists in the entire social network. These lists belong to some domains, and each domain contains at least one list. The set of lists in a particular domain D is called a domain database (denoted as F D ). Here, we assume that there exists an interest-group list in every domain. The projected list F G D of G in F D is the set of lists in F D that contains group G . The frequency Freq D ( G )of G in F D indicates the number of lists L j  X  X  in F G D , and the frequencies of G in multiple domains are represented as Freq D 1 Agroup G of friends in a social network F SN is considered diverse if its diversity value Div ( G )  X  the user-specified minimum threshold minDiv ,whichcanbe expressed as an absolute (non-negativ e real) number or a relative percentage (with respect to the size of F SN ). Given F SN and minDiv , the research problem of mining diverse friends from social networks is to find every group G of friends having Div ( G )  X  minDiv . When mining frequent patterns, the frequency/support measure [1,4] satisfies the downward closure property (i.e., all supersets of an infrequent patterns are infre-quent). This helps reduce the search/solution space by pruning infrequent pat-terns, which in turn speeds up the mining process. However, when mining diverse friends, diversity does not satisfy the downward closure property. Recall from Ex-ample 3, group G = { Ed } ) is not diverse but its super-group G = { Cathy,Ed } ) is diverse. As we cannot prune those groups that are not diverse, the mining of diverse friends can be challenging.

To handle this challenge, for each domain D , we identify the (global) max-imum prominence value GMProm D among all friends. Then, for each friend f i , we calculate an upper bound of the influence value Inf U D ( f i )bymul-tiplying GMProm D (instead of the actual Prom D ( f i )) with the corresponding frequency Freq D ( f i ). The upper bound of diversity value Div U ( f i )canthenbe computed by using Inf U D ( f i ).
 3.1 Phase 1: Constructing a Div-tree Structure Given F SN and minDiv , our proposed Div-growth algorithm constructs a Div-tree as follows. It first scans F SN to calculate Freq D each domain D j .Foreach f i , Div-growth then uses GMProm D to compute the upper bound of the diversity value Div U ( f i ), which is used to prune groups of friends who are not potentially divers e. Every potentially diverse friend f i ,along with its Freq D 1
Afterwards, Div-growth scans F SN the second time to capture the important information about potentially diverse friends in a user-defined order in the Div-tree. Each tree node consists of (i) a frie nd name and (ii) its frequency counters for all d domains in the respective path. The basic construction process of a Div-tree is similar to that of the FP-tree [4]. A key difference is that, rather than using only a single frequency counter capturing either the maximum or average frequency for all domains (which may lead to loss of information), we use d frequency counters capturing the frequency for all d domains. See Example 5. Example 5. To construct a Div-tree for F SN showninTable1(b)when minDiv =0.5, With this tree construction process, the size of the Div-tree for F SN with a given minDiv is observed to be bounded above by L 3.2 Phase 2: Mining Diverse Friend Groups OncetheDiv-treeisconstru cted, Div-growth recursiv ely mines diverse friend groups by building projected and conditional trees in a fashion similar to that of FP-growth [4].

Recall that Div ( G ) computed based on Prom D ( G )does not satisfy the down-ward closure property. To facilitate pruning, we use GMProm D ( f i )tocom-pute Div U ( f i ), which then satisfies the downward closure property. However, if Div U ( G ) was computed as an upper bound to super-group G of f i ,thenit may overestimate diversity of G and may lead to false positives. To reduce the number of false positives, Div-growth uses the local maximum prominence value LMProm D ( G ) =max f tional trees for G . See Lemma 2 and Example 6. Lemma 2. The diversity value of a friend group G computed based on LMProm D ( G ) Example 6. To mine potentially diverse friend groups from the Div-tree in Fig. 1(b) To evaluate the effectiveness of our proposed Div-growth algorithm and its as-sociated Div-tree structure, we co mpared them with a closely related weighted frequent pattern mining algorithm called Weight [18] (but it does not use differ-ent weights for individual items). As Weight was designed for frequent pattern mining (instead of social network mining), we apply those datasets commonly used in frequent pattern mining for a fair comparison: (i) IBM synthetic datasets (e.g., T10I4D100K ) and (ii) real datasets (e.g., mushroom , kosarak )fromtheFre-quent Itemset Mining Dataset Repository fimi.cs.helsinki.fi/data .SeeTa-ble 2 for more detail. Items in transactions in these datasets are mapped into friends in interest-group lists. To reflect the concept of domains , we subdivided the datasets into several batches. Moreover, a random number in the range (0, 1] is generated as a prominence value for each friend in every domain.

All programs were written in C++ and run on the Windows XP operating system with a 2.13 GHz CPU and 1 GB main memory. The runtime specified indicates the total execution time (i.e., CPU and I/Os). The reported results are based on the average of multiple runs for each case. We obtained consistent results for all of these datasets.
 Runtime. First, we compared the runtime of Div-growth (which includes the construction of the Div-tree, the mining of potentially diverse friend groups from the Div-tree, and the removal of false positives) with that of Weight . Fig. 3(a) shows the results for a dense dataset ( mushroom ), which were consistent with those for sparse datasets (e.g., T10I4D100K ). Due to page limitation, we omit the results for sparse datasets. Runtimes of both algorithms increased when mining larger datasets (social networks), more batches (domains), and/or with lower minDiv thresholds. Between the two algorithms, our tree-based Div-growth al-gorithm outperformed the Apriori-based Weight algorithm. Note that, although FP-growth [4] is also a tree-based algorithm, it was not design to capture weights. To avoid distraction, we omit experimental results on FP-growth and only show those on Weight (which captures weights).
 Compactness of the Div-tree. Next, we evaluated the memory consumption. Fig. 3(b) shows the amount of memory required by our Div-tree for capturing the content of social networks with the lowest minDiv threshold (i.e., without removing any friends who were not diverse). Although this simulated the worst-case scenario for our Div-tr ee, Div-tree was observed ( i) to consume a reasonable amount of memory and (ii) to require less memory than Weight (because our Div-tree is compact due to the prefix sharing).
 Scalability. Then, we tested the scalability of our Div-growth algorithm by varying the number of transactions (interest-group lists). We used the kosarak dataset as it is a huge sparse dataset with a large number of distinct items (individual users). We divided this dataset into five portions, and each portion is subdivided into multiple batches (domains). We set minDiv =5% of each portion. Fig. 3(c) shows that, when the size of the dataset increased, the runtime also increased proportionally implying that Div-growth is scalable. Additional Evaluation. So far, we have evaluated the efficiency (e.g., runtime, compactness or memory consumption, as well as scalability) of our Div-growth algorithm. Experimental results show that Div-growth is time-and space-efficient as well as scalable. As ongoing work, we plan to evaluate the quality (e.g., precision) of Div-growth in finding diverse friend groups. Moreover, for a fair comparison with Weight , we have used those datasets that are com-monly used in frequent pattern mining. As ongoing work, we plan to evaluate Div-growth using real-life social network datasets. In this paper, we (i) introduced a new notion of diverse friends for social net-works, (ii) proposed a compa ct tree structure called Div-tree to capture impor-tant information from social networks, and (iii) designed a tree-based mining algorithm called Div-growth to find diverse (groups of) friends from social net-works. Diversity of friends is measured based on their prominence, frequency and influence in different domains on the networks. Although diversity does not satisfy the downward closure property, we managed to address this issue by us-ing the global and local maximum prominence values of users as upper bounds. Experimental results showed that (i) ou r Div-tree is compact and space-effective and (ii) our Div-growth algorithm is fast and scalable for both sparse and dense datasets. As ongoing work, we conduct more extensive experimental evaluation to measure other aspects (e.g., precision) of our Div-growth algorithm in finding diverse friends. We also plan to (i) design a more sophisticated way to mea-sure influence and (ii) incorporate other computational metrics (e.g., popularity, significance, strength) with prominence into our discovery of useful information from social networks.
 Acknowledgements. This project is partially supported by NSERC (Canada) and University of Manitoba.

