 Hybrid recommender systems  X  systems using multiple algo-rithms together to improve recommendation quality  X  have been well-known for many years and have shown good per-formance in recent demonstrations such as the NetFlix Prize. Modern hybridization techniques, such as feature-weighted linear stacking, take advantage of the hypothesis that the relative performance of recommenders varies by circumstance and attempt to optimize each item score to maximize the strengths of the component recommenders. Less attention, however, has been paid to understanding what these strengths and failure modes are. Understanding what causes particular recommenders to fail will facilitate better selection of the component recommenders for future hybrid systems and a better understanding of how individual recommender per-sonalities can be harnessed to improve the recommender user experience. We present an analysis of the predictions made by several well-known recommender algorithms on the MovieLens 10M data set, showing that for many cases in which one algorithm fails, there is another that will correctly predict the rating.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Recommender systems, evaluation, hybrid recommenders
Hybrid recommender systems [1] are a well-known tech-nique for harnessing the strengths of multiple recommenders to produce results that are more accurate or useful than those achieved by individual constituent recommenders. They have  X 
The scripts to re-run the evaluations in this paper are available at http://www-users.cs.umn.edu/~ekstrand/ recsys2012/recsys-scripts.tgz . seek to demonstrate this more concretely and develop a transparent model for combining recommenders. Our goal is not to immediately produce a more accurate hybrid, but to gain insight into the relative performance of algorithms that can inform future recommender designs and deployments.
To address these questions, we analyzed prediction error of several algorithms using the LensKit recommender toolkit [3] on the MovieLens 10M data set in a cross-validation setup. While prediction accuracy is just one piece of the broader picture of recommender usefulness and suitability, it pro-vides a tractable way to inspect the differing errors made by recommender algoritms.
Burke [1] provides an overview and taxonomy of hybrid recommender systems, outlining a variety of methods (includ-ing switching and weighting ) that can be used to combine individual recommenders into a composite recommender. In this work, we focus primarily on switching hybrids, which pick which recommender to use in each situation and report its result alone; this framing provides the simplest way to study what causes individual algorithms to succeed or fail. More recent developments in hybridization include hierarchical methods like feature-weighted linear stacking [11].
McNee [7] argued for designing and evaluating recom-mender systems in the context of user needs, picking rec-ommenders that provide specific characteristics that sup-port user information needs. Later work [12] demonstrated that different algorithms that perform similarly on aggregate numeric measures of accuracy exhibit differing user-visible behaviors.
We used the MovieLens 10M data set 1 (ML10M) and the LensKit recommender toolkit [3] for our experiments. We used LensKit X  X  evaluation framework to partition the data set in a 5-fold cross-validation configuration. Users were partitioned into 5 sets; for each user in each partition, we randomly selected 20% of their ratings to be the test ratings for that data set, with the remaining ratings plus all ratings from users in the other partitions forming the training set.
We then ran five recommender algorithms on the data, and captured the predictions each algorithm made for each test rating. We used the following algorithms, choosing pa-rameters based on prior results in the research literature and experience tuning LensKit for the MovieLens data sets [3]: http://grouplens.org/node/73
Table 1 shows the cumulative  X  X ood X  predictions for the algorithms we tested. A prediction is considered  X  X ood X  if it is within 0.5 stars; the table is computed by first picking the algorithm that has the most good predictions. The re-maining algorithms are selected and computed by picking the algorithm which has the most good predictions that no prior algorithm has correctly made and adding it to the table. So ItemItem predicts 52% of ratings correctly, UserUser an additional 8%, and so on.

This result provides initial confirmation of H1: algorithms differ in which predictions they get right or wrong. ItemItem gets the most predictions right (52%), but the other algo-rithms correctly make various predictions until 69% of all test ratings are predicted correctly by at least one algorithm.
This result is robust to higher thresholds; using a threshold of 1.0 stars for good prediction scales the ItemItem hit count up and the other hit counts correspondingly down, but does not change the relative ordering of algorithms.

The existence of differences in the errors made by individual algorithms is further substantiated by the results shown for  X  X estPred X  in Figure 1. This is the RMSE achieved by switching hybrid recommender that uses an oracle to select the best predictor for each individual prediction; the left side of Figure 2 shows how often each algorithm provided the best prediction. This shows that, if we can perfectly predict the best predictor to use, there is room for substantial improvement in error. It therefore provides a lower bound on the error of a switching hybrid comprised of the algorithms in our experiment.

When selecting algorithms to deploy in an ensemble rec-ommender, it is not necessarily desirable just to pick the ones that perform the best. If two algorithms are highly corre-lated in the errors they make, failing in the same cases, then including both of them will likely not provide much benefit. In selecting algorithms, we look for the following criteria:
In general, we found FunkSVD and ItemItem to be highly correlated; the absolute error produced by each has a correla-tion of  X  = 0 . 888, and ignoring one accrued the greatest ben-efit to the other. This corroborates their showing in Table 1; if we regenerate the table using FunkSVD first, ItemItem is the last recommender to be picked before Mean. FunkSVD section shows the RMSE achieved by three different per-user algorithm switching strategies. UserBest uses the algorithm with the best RMSE for that user; it shows a lower bound on the error of per-user switching hybrids optimizing for user RMSE. It shows some improvement over all single algorithms and over the linear blend, particularly when looking at per-user error. It therefore seems that it should be possible to achieve accuracy improvements through intelligent selection of algorithms on a per-user basis.

TuneBest attempts to pick the best algorithm using the predictions for each user X  X  5 tuning ratings, then apply that algorithm to the remaining test ratings. Using this approach  X  which does not provide any insight into why the algorithm is selected  X  achieves an accuracy a bit worse than the best single algorithms (ItemItem and FunkSVD). Its per-user RMSE is slightly better than any single algorithm.
ModelBest uses the 5 probe ratings to train a logistic regression predicting whether either UserUser or Mean will be better than ItemItem for that user based on the log of the number of items the user has rated and an interaction term between that and the variance of their ratings. In turning the modeling from Table 2 around and using it to decide which recommender to use, we found these features to be most useful; variance on its own was not significant and did not make a noticeable contribution in addition to the item count and interaction terms, and mean was similarly unhelpful in improving the regression X  X  power. The regression is then thresholded to decide whether ItemItem or UserUser is used; the threshold was chosen to produce a ratio of UserUser to ItemItem choices that kept in line with the other two models. The resulting predictor beats UserUser alone, but does not improve upon ItemItem. It does, however, beat TuneBest ; we infer from this that user features are useful for hybridization, but there is still work to do to make this algorithm an actual improvement over the current state of the art.
In our experiment, we found that recommenders do indeed fail on different users and items, thus confirming H1. We have also identified user features predicting relative algorithm performance and achieved some success building a hybrid around them, providing preliminary confirmation of H2 and H3, but more work needs to be done to make the algorithm an improvement over the state of the art and to develop a deeper understanding of what makes the various recommenders succeed or fail.

Immediate future work involves continuing to look for fea-tures that will help us to select the appropriate recommender to use, as well as investigating blending approaches using hierarchical regressions (similar to FWLS), with the goal of understanding what it is that makes particular algorithms work well, where their individual weaknesses are, and how to combine them into an effective ensemble.

User studies and qualitative investigation of the items and users themselves will likely be helpful in further elucidating the specific behavior of each algorithm. So far, our work has focused only on generic statistics of users and items in the rating set; seeing what actual items are being mispredicted and collecting user feedback on erroneous predictions or bad recommendations will hopefully provide further insight into how the algorithms behave.

Systematic investigation of recommender failures has po-tential to improve both our understanding of the workings
