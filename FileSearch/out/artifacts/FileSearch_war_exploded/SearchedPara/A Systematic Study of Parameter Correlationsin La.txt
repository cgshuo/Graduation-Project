 Duplicate pages and mirrored web sites are phenomenal on the web. For exam-ple, it was reported that more than 250 sites mirrored the documents of Linux Document Project (LDP) 1 .Broder et al. clustered the duplicated and nearly-duplicated documents in 30 millions documents and got 3.6 millions clusters con-taining 12.1 millions documents [1]. Bharat and Broder reported that about 10% of hosts were mirrored to various extents in a study involving 238,000 hosts [2].
Because of the high duplication of Web documents, it is important to de-tect duplicated and nearly duplicated documents in many applications, such as crawling, ranking, clustering, archiving, and caching. On the other hand, the tremendous volume of web pages challenges the performance and scalability of DDD algorithms. As far as we know, Broder et al. for the first time proposed a DDD algorithm for large-scale documents sets in [1]. Many applications and following research, such as [2] [3] [4] [5] [6], later adopted this algorithm for its simplicity and efficiency.

While much work has been done on both DDD algorithms and their applica-tions, little has been explored about the factors affecting their performance and scalability. Meanwhile, because of the huge volume data, all prior work makes some kinds of tradeoffs in DDD. How do these tradeoffs affect accuracy? To our best knowledge, no previous work conducts any systematic analysis on correla-tions among different parameters of DDD, and none of them provides a formal evaluation of their tradeoff choices.

This paper studies several of the most important parameters of DDD algo-rithms and their correlations. These parameters include similarity threshold, precision/recall requirem ent, sampling ratio, document size. Among them, sam-pling ratio is of most interest, for it greatly affects the accuracy and scalability of DDD algorithms.

To uncover the correlations of parameters, an empirical analysis is conducted in this paper. The TREC .GOV collection 2 are used as our testing dataset. Al-though the volume of this collection is much smaller than the whole Web, we believe that this collection to some ext ent represents the Web well for DDD al-gorithms [7]. Experiment results show that even using the same sampling ratio, the precision of DDD in documents of different size varies greatly. To be more specific, small sampling ratio heavily hurts the accuracy of DDD for small doc-uments. Based on this observation, we propose an adaptive sampling method for DDD which uses dynamic sampling ratio for different document size with constraint of given precision thresholds. We believe that our analysis is helpful for guiding the future DDD work.

The remainder of this paper is organi zed as follows. Section 2 reviews the prior work on DDD. Section 3 describes the duplicate detection algorithm and the definition of document similarity used in this paper. S ection 4 presents the experimental results on parameter correlations, and then proposes an adaptive sampling strategy. Finally we conclude this paper with Section 6. The prior work of duplicate document detection can be partitioned into two categories based on the ways to calculate document similarity, shingle based and term based algorithms, both of which can be applied offline and online. We review these algorithms in this section. 2.1 Shingle Based Algorithms The algorithms, such as [8] [9] [1] [10] [2] [3] [11] [5] [6], are based on the concept of shingle . A shingle is a set of contiguous terms in a document. Each document is divided into multiple shingles and a hash value is assigned to each shingle. By sorting these hash values, shingles with the same hash value are grouped together. Then the resemblance of two documents is calculated based on the number of shingles they share.

Because of the large size of the docume nt collections to be examined, sev-eral sampling strategies have been prop osed to reduce the number of shingles to compare. Heintze selects s hingles with the smallest N hash values and removes shingles with high frequencies [9]. Broder et al. samples one of 25 shingles by selecting the shingles whose value modulo 25 is zero and choose at most 400 shingles for each document [1]. In this way they process 30 millions web pages in 10 days. Another more efficient alter native is also proposed in [1], which combines several shingles into a supershingle and computes the hash values of supershingles. Although the supershingle algorithm is much faster, the authors noted that it does not work well for small documents and no detailed results of this algorithm are reported. In [10][11], exact copies are removed in advance and then every two or four lines of document are made as a shingle. Fetterly et al. use five-gram as a shingle and apply a 64-bit hash to get fingerprints of shin-gles, then employ 84 different hash functi ons to construct a f eature vector for each document [4][5]. More precisely, they apply 84 diff erent(rando mly selected but fixed thereafter) one-to-one functions to produce shingle fingerprints of each document. For each function, they retain the shingle with numerically smallest hash value of its fingerprints. Thus a vector of 84 shingles is constructed for each document. Then the 84 shingles are separ ated into six supershingles, in other words, each supershingle contains 14 adjacent shingles. The documents having two supershingles in common are clustere d as nearly-duplicate documents. Fet-terly et al. processed 150M web pages by using this method. We summarize some of the previous work in Table 1.

To deal with the large-scale data, almost all the previous work employs sam-pling strategies. However, none of them provides an analysis of how their sam-pling strategies affect the accuracy of DDD algorithms. On the other hand, sampling has to be adopted to scale up with the index volume of search engines. So it is important to study the impact of sampling in DDD.
 2.2 Term Based Algorithms Term based algorithms [12] [13] [14] use individual terms/words as the basic unit, instead of continuous k -gram shingles. Cosine similarity between docu-ment vectors is usually used to calcul ate similarity between documents. Many IR techniques, especially feature selection, are used in these algorithms, which makes them much more complex than shingle-based algorithms. The largest set processed by term based algorithms contains only about 500K web pages [12].
Term based DDD algorithms work well for small-scale IR systems and most of them also achieve good performance when used in online DDD. But for search engines which need to answer over 100M queries everyday, online methods are not a good choice because of their prohibitive computing cost. Meanwhile, in some applications, we have to do DDD offline. In this paper, we focus on shingle based approaches and do not discuss more about term based and online methods. Although much work has been done on DDD algorithms and many applications employ DDD techniques, there is no systematic analysis on how the parameters in DDD correlate, such as accuracy, similarity and sampling ratio. And there is also no formal study on the accuracy and scalability of DDD. This paper aims to explore these problems. We choose the method in [1] for analysis since many DDD algorithms and applications follow it, while we believe our conclusions can also guide other DDD algorithms especially in sampling strategies. 3.1 Document Similarity Since the exactly duplicate documents, which have no differences between two documents, are easily to identify by comparing the fingerprints of the whole doc-ument, this paper focuses on nearly duplicates, which have slightly differences between two documents. We choose the resemblance in [1] as our document sim-ilarity metric for its widely usage in DDD. However, we believe the conclusions based on this similarity can be easily extended to other metrics of document similarity.

The resemblance given by [1] is defined as follows. Each document is viewed as a sequence of words and is transformed into a canonical sequence of tokens. This canonical form ignores minor details such as formatting and HTML tags. Then every document D is associated with a set of subsequences of token S ( D,w ). A contiguous subsequence in D is called a shingle . Given a document D we define its w -shingling S ( D,w ) as the union of all unique shingles with size w contained in
D . Thus, for instance, the 4-shingling of (a, rose, is, a, rose, is, a, rose) is the set { (a,rose,is,a),(rose,is,a,rose),(is,a,rose,is) } .

For a given shingle size, the resemblance r of two documents A and B is defined as: Where | S | represents the number of elements in the set S .

In our experiments, the shingle size w is set to 10, the same as that in [1]. Dif-ferent shingle size affects the performance of DDD. Generally, greater w results in higher precision and lower recall. In our own experiences, although greater w produces fewer shingles for each document, greater w also hurts the recall of DDD. So a moderate w is usually chosen to get a balance between precision and recall. 3.2 Hash Function 32-bit and 40-bit Rabin [15] hash functions are used in some of the prior work [1] [10] [11] [2] [3]. However, for large scale dataset with several millions of documents and several billions of shingles, 32-bit or 40-bit hash may produce many false positives. A 40-bit message digest has the probability 1 / 2 that a collision (false positive) is found with just over 2 20 (about a million) random hashes [16]. In this paper, we use the well known 128-bit MD5 hash for both document fingerprints and shingle fingerprints, which generates many fewer false positives for it requires 2 64 hashes for a collision with 1 / 2 probability. 4.1 Data Description There are several datasets used in prior work, most of which are not public available. [12] chooses 2GB NIST web pages and TREC disks 4&amp;5 collections as their testing data, but these two sets contain only 240k and 530k documents respectively. In this paper we choose the TREC .GOV collection as our testing dataset since it contains about a million documents and is widely used in Web related research. Table 2 summarizes the main properties of this dataset. 4.2 Data Preprocessing First we canonicalize each document by removing all HTML formatting infor-mation. Special characters such as HT (Horizontal Tab), LF (Line Feed) and CR (Carriage Return) are converted into spa ces, and continuous spaces are replaced by one space. Thus each document is conv erted into a string of words separated by single spaces.
Then we remove the exact duplicates fr om the Web collection since we focus on detecting nearly-duplicate documen ts. By calculating MD5 hash for each document, we cluster exactly duplicate documents, then choose a document from each cluster as the representative and remove the other documents in the cluster. As a result, 94,309 documents are removed from the collection and the final set contains 958,725 documents.

The documents are divided into 11 groups based on the number of words they contain, as shown in Table 3.
 4.3 Implementation We implement the algorithm in [1] and run DDD experiments with different similarity thresholds and sampling ratios for each group.

We use three machines with 4GB memory and 1T SCSI disks, one with Intel 2GHz Xeon CPU and the other two with 3GHz Xeon CPU. It takes us two weeks to run about 400 trials of DDD experiments with different combinations of parameters.

Broder et al. [1] processes 30 millions web pages in 10 CPU days. There are two main tradeoffs in their approach. First, they sample one out of 25 shingles and at most 400 shingles are used for each document. They also discard common shingles which are shared by more than 1,000 documents. Second, they divide the data into pieces to fit the main memory. However, [1] does not give the size of each piece. It just mentions that  X  X he final file containing the list of the documents in each cluster took up less than 100Mbytes. X  Thus we believe that the size of each piece can not be too large, and small pieces hurt the recall of DDD since duplicates across different clusters are missed. Moreover, although the CPU speed has been greatly improved since then, the speed of ram and disk advances not so much. So our experiments are rather time consuming although we use much more powerful hardware than theirs.
 4.4 Experimental Results For evaluation we use the result without sampling as the ground truth and com-pare the result using sampling with this ground truth to calculate the precision. If two documents are judged as duplicates in the result using sampling while they are not judged as duplicates in the result without sampling, it is a false positive. The precision of a trial is calculated by the ratio between the number of correctly detected duplic ate document pairs and the number of total detected duplicate pairs in this trial.

For sampling experiments, we make use of the module of the numerical hash value to select shingles. For example, when using 1 / 2 sampling ratio, we select the shingles whose hash value modulo two is zero, that is, the singles with even hash value. We also run multiple trials for each sampling ratio. For example, when the sampling ratio is 1 / 2, we run two trials by selecting shingles with odd and even hash value respectively and then calculate the average performance of these two trials. Thus, when the sampling ratio is 1 /n ,werun n trials by selecting the singles with different remainders. In our experiments, we count the number of both selected shingles and total shingles and find that the selection ratio is consisted with the given sampling ratio. And there are only slight differences between the precision of different trials with the same sampling ratio, which verifies that MD5 is a good hash function for this sampling task.

The experimental results of 1 / 4and1 / 16 sampling ratio are shown in Fig-ure 1(a) and 1(b).

As shown in Figure 1(a), precision of DDD decreases with the increasing of similarity threshold. The curve of Group 0, documents having fewer than 500 words, decreases significantly. In Figure 1(b), the highest precision on Group 0 is lower than 0.8 no matter what similarity threshold is used. Also, the precision on several groups with small documents drops dramatically when the similarity threshold is higher than 0.9. The low precision on groups with small documents proves that small documents are sensitive to sampling and it is hard for them to achieve good precision when small sampling ratio or high similarity threshold is required. On the other hand, for groups with large documents, the precision is high and stable even when the similarity threshold is high and sampling ratio is small. We also ran experiments with sampling ratio 1 / 2and1 / 8, which show the similar properties as 1 / 4and1 / 16 sampling ratios. 4.5 Adaptive Sampling Strategy Based on above observations, we propose an adaptive sampling strategy that applies small sampling ratio on large documents and large sampling ratio on small documents. To show the power of our sampling strategy, we conduct the following experiment. We partition the TREC .GOV collection into 11 groups as previous experiments. For every group we minimize the sampling ratio out of 1 / 2, 1 / 4, 1 / 8, 1 / 16, subjected to different given precisions ranging from 0 . 5to 0 . 99, thus we minimize the total shingles which we have to process. For example, with the precision requirement 0 . 8 and similarity threshold 0 . 6, we choose 1 / 8 sampling ratio for Group 0 and 1 / 16 sampling ratio for the other groups, so only 8% of the total shingles have to be processed. As shown in Figure 2, our algorithm greatly reduces the shingles t o process and thus can deal with larger scale documents sets than the previous unified sampling strategy.

Due to the well known long tailed distribution of web document size, small documents consist of a large proportion of the whole documents collection. In our experiments, the documents having fewer than 500 words consist of 68% of the whole collection. For higher precision we can not do small sampling in these small documents, otherwise it would greatly hurt the overall precision. Fortunately these small documents consist of only 17% shingles, thus our adaptive sampling strategy greatly reduces the total shingles to process by applying small sampling ratio on large documents. 4.6 Summary of Parameter Correlations Here we give a summary of the correlations between precision and other param-eters.  X  Similarity Threshold: precision drops with the increase of similarity thresh- X  Sampling Ratio: precision drops with the decreasing of sampling ratio, espe- X  Document Size: small documents are mor e sensitive to similarity threshold
Generally, sampling ratio does not hurt recall because sampling only generates false positives. While for small documents, recall may drop because some of the documents have no shingle sampled by chance. Although much work has been done on duplicate document detection and many applications employ this technique, li ttle has been explored on the performance and scalability of DDD. In this paper, a systematic study on parameter correla-tions in DDD is conducted and several most important parameters of DDD are analyzed.

Our experiment results show that small sampling ratio hurts the precision of DDD, especially for small documents which consist of a major fraction of the whole Web. Based on this observation, an adaptive sampling strategy is pro-posed, which minimizes the sampling ratio of documents with constraint of given precision thresholds, making DDD feasible to deal with large scale documents collections. We believe the observations in our work are helpful in guiding the future DDD work.

