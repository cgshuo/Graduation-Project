 The task of word embedding is to model the distri-bution of a word and its context words using their corresponding vectors in a Euclidean space. Then by doing regression on the relevant statistics de-rived from a corpus, a set of vectors are recovered which best fit these statistics. These vectors, com-monly referred to as the embeddings , capture se-mantic/syntactic regularities between the words.
The core of a word embedding method is the link function that connects the input  X  the embed-dings, with the output  X  certain corpus statistics. Based on the link function, the objective function is developed. The reasonableness of the link func-tion impacts the quality of the obtained embed-dings, and different link functions are amenable to different optimization algorithms, with different scalability. Based on the forms of the link func-tion and the optimization techniques, most meth-ods can be divided into two classes: the traditional neural embedding models , and more recent low rank matrix factorization methods .

The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimiza-tion, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2003). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the condi-tional distribution, where the two embeddings in-teract directly. They implemented the well-known  X  X ord2vec X , which can be trained efficiently on huge corpora. The obtained embeddings show ex-cellent performance on various tasks.

Low-Rank Matrix Factorization (MF in short) methods include various link functions and opti-mization methods. The link functions are usu-ally not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The ob-jective is usually to minimize the reconstruction error, optionally with other constraints. In this line of research, Levy and Goldberg (2014b) find that  X  X ord2vec X  is essentially doing stochastic weighted factorization of the word-context point-wise mutual information (PMI) matrix. They then factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regres-sion function of the conditional distribution, from which a weighted MF problem on the bigram log-frequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in se-mantic hierarchies, Yogatama et al. (2015) add hi-erarchical sparse regularizers to the matrix recon-struction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embed-dings using sparse vectors of greater dimensional-ity. Dhillon et al. (2015) apply Canonical Corre-lation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embed-dings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equiva-lent to finding a transformed solution of the lan-guage model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a con-straint on the embeddings in a small text window. The maximum likelihood estimate of the embed-dings within this text window approximately re-duces to a squared norm objective.

There are two limitations in current word em-bedding methods. The first limitation is, all MF-based methods map words and their context words to two different sets of embeddings, and then em-ploy Singular Value Decomposition (SVD) to ob-tain a low rank approximation of the word-context matrix M . As SVD factorizes M &gt; M , some in-formation in M is lost, and the learned embed-dings may not capture the most significant regu-larities in M . Appendix A gives a toy example on which SVD does not work properly.

The second limitation is, a generative model for documents parametered by embeddings is absent in recent development. Although (Stratos et al., 2014; Stratos et al., 2015; Arora et al., 2015) are based on generative processes, the generative pro-cesses are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some mod-els, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A genera-tive word embedding model for documents is not only easier to interpret and analyze, but more im-portantly, provides a basis upon which document-level global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incor-porated in a principled manner, to better model the text distribution and extract relevant information.
Based on the above considerations, we pro-pose to unify the embeddings of words and con-text words. Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual captur-ing nonlinear or noisy correlations, and the uni-gram priors. To reduce overfitting, we put Gaus-sian priors on embeddings and residuals, and ap-ply Jelinek-Mercer Smoothing to bigrams. Fur-thermore, to model the probability of a sequence of words, we assume that the contributions of more than one context word approximately add up. Thereby a generative model of documents is con-structed, parameterized by embeddings and resid-uals. The learning objective is to maximize the corpus likelihood, which reduces to a weighted low-rank positive semidefinite (PSD) approxima-tion problem of the PMI matrix. A Block Co-ordinate Descent algorithm is adopted to find an approximate solution. This algorithm is based on Eigendecomposition, which avoids information loss in SVD, but brings challenges to scalability. We then exploit the sparsity of the weight matrix and implement an efficient online blockwise re-gression algorithm. On seven benchmark datasets covering similarity and analogy tasks, our method achieves competitive and stable performance.
The source code of this method is provided at Throughout the paper, we always use a uppercase bold letter as S , V to denote a matrix or set, a low-ercase bold letter as v w mal uppercase letter as N,W to denote a scalar constant, and a normal lowercase letter as s i ,w i to denote a scalar variable.
 Suppose a vocabulary S = { s 1 ,  X  X  X  ,s sists of all the words, where W is the vocab-ulary size. We further suppose s 1 ,  X  X  X  ,s W are sorted in decending order of the frequency, i.e. s 1 is most frequent, and s W is least frequent. A document d i is a sequence of words d i = Name Description tion of M documents D = { d 1 ,  X  X  X  ,d vocabulary, each word s i is mapped to a vector v s in N -dimensional Euclidean space.

In a document, a sequence of words is referred to as a text window , denoted by w i ,  X  X  X  ,w i + l , or w : w i + l in shorthand. A text window of chosen size c before a word w i defines the context of w i as w i  X  c ,  X  X  X  ,w i  X  1 . Here w i is referred to as the focus word . Each context word w i  X  j and the focus word w i comprise a bigram w i  X  j ,w i .

The Pointwise Mutual Information between two words s i ,s j is defined as In this section, we formulate the probability of a sequence of words as a function of their embed-dings. We start from the link function of bigrams, which is the building blocks of a long sequence. Then this link function is extended to a text win-dow with c context words, as a first-order approx-imation of the actual probability. 3.1 Link Function of Bigrams We generalize the link function of  X  X ord2vec X  and  X  X loVe X  to the following:
P ( s i ,s j ) = exp
The rationale for (1) originates from the idea of the Product of Experts in (Hinton, 2002). Sup-pose different types of semantic/syntactic regu-larities between s i and s j are encoded in differ-ent dimensions of v s Q ferent regularities on the probability are combined by multiplying together. If s i and s j are indepen-dent, their joint probability should be P ( s i ) P ( s j In the presence of correlations, the actual joint probability P ( s i ,s j ) would be a scaling of it. The scale factor reflects how much s i and s j are pos-itively or negatively correlated. Within the scale s and s j , the residual a s of relative to a s
Note that we do not assume a s This provides the flexibility P ( s i ,s j ) 6 = P ( s j ,s agreeing with the asymmetry of bigrams in natu-symmetric part between P ( s i ,s j ) and P ( s j ,s i ) . (1) is equivalent to
P ( s j | s i )=exp (3) of all bigrams is represented in matrix form: where G is the PMI matrix. 3.1.1 Gaussian Priors on Embeddings When (1) is employed on the regression of empir-ical bigram probabilities, a practical issue arises: more and more bigrams have zero frequency as the constituting words become less frequent. A zero-frequency bigram does not necessarily imply negative correlation between the two constituting words; it could simply result from missing data. But in this case, even after smoothing, (1) will making v s of embeddings is a sign of overfitting.

To reduce overfitting of embeddings of infre-quent words, we assign a Spherical Gaussian prior N (0 , 1 where the hyperparameter  X  i increases as the fre-quency of s i decreases. 3.1.2 Gaussian Priors on Residuals lations between s i and s j as possible. Thus the smaller a s frequent s i ,s j is in the corpus, the less noise there is in their empirical distribution, and thus the residual a s
To this end, we penalize the residual a s by f (  X  P ( s i ,s j )) a 2 s tive monotonic transformation, referred to as the the total penalty of all residuals are the square of the weighted Frobenius norm of A :
By referring to  X  X loVe X , we use the following weighting function, and find it performs well: where C cut is chosen to cut the most frequent 0 . 02% of the bigrams off at 1 . When s i = s j , two identical words usually have much smaller proba-the true correlation of a word to itself, and should not put constraints to the embeddings. We elimi-nate their effects by setting f ( h ii ) to 0 . then this penalty is equivalent to a Gaussian prior N 0 , 1 Gaussians are determined by the bigram empirical probability matrix H . 3.1.3 Jelinek-Mercer Smoothing of Bigrams As another measure to reduce the impact of miss-ing data, we apply the commonly used Jelinek-Mercer Smoothing (Zhai and Lafferty, 2004) to smooth the empirical conditional probability  X  P ( s j | s i ) by the unigram probability  X  P ( s j ) as:  X 
Accordingly, the smoothed bigram empirical joint probability is defined as  X 
In practice, we find  X  = 0 . 02 yields good re-sults. When  X   X  0 . 04 , the obtained embeddings begin to degrade with  X  , indicating that smoothing distorts the true bigram distributions. 3.2 Link Function of a Text Window In the previous subsection, a regression link func-tion of bigram probabilities is established. In this section, we adopt a first-order approximation based on Information Theory, and extend the link function to a longer sequence w 0 ,  X  X  X  ,w c  X  1 ,w c .
Decomposing a distribution conditioned on n random variables as the conditional distributions on its subsets roots deeply in Information The-ory. This is an intricate problem because there could be both (pointwise) redundant information and (pointwise) synergistic information among the conditioning variables (Williams and Beer, 2010). They are both functions of the PMI. Based on an analysis of the complementing roles of these two types of pointwise information, we assume they are approximately equal and cancel each other when computing the pointwise interaction infor-mation . See Appendix B for a detailed discussion. Following the above assumption, we have PMI ( w 2 ; w 0 ,w 1 )  X  PMI ( w 2 ; w 0 )+ PMI ( w 2 ; w log P Plugging (1) and (3) into the above, we obtain
P ( w 0 ,w 1 ,w 2 )  X  exp
We extend the above assumption to that the pointwise interaction information is still close to 0 within a longer text window. Accordingly the above equation extends to a context of size c &gt; 2 :
P ( w 0 ,  X  X  X  ,w c )  X  exp
From it derives the conditional distribution of w , given its context w 0 ,  X  X  X  ,w c  X  1 : We proceed to assume the text is generated from a Markov chain of order c , i.e., a word only depends on words within its context of size c . Given the hyperparameter  X  = (  X  1 ,  X  X  X  , X  process of the whole corpus is: 1. For each word s i , draw the embedding v s 2. For each bigram s i ,s j , draw the residual 3. For each document d i , for the j -th word, The above generative process for a document d is presented as a graphical model in Figure 1.
Based on this generative process, the probabil-ity of a document d i can be derived as follows, given the embeddings and residuals V , A :
P ( d i | V , A ) =
Y .

The complete-data likelihood of the corpus is: p ( D , V , A ) = =
Z ( H ,  X  )  X 
Y where Z ( H ,  X  ) is the normalizing constant.
Taking the logarithm of both sides of p ( D , A , V ) yields = C 0  X  log Z ( H ,  X  )  X  X  A k 2 where C 0 = 5.1 Learning Objective The learning objective is to find the embeddings V that maximize the corpus log-likelihood (9).
Let x ij denote the (smoothed) frequency of bi-gram s i ,s j in the corpus. Then (9) is sorted as: log p ( D , V , A ) = C 0  X  log Z ( H ,  X  )  X  X  A k 2 + As the corpus size increases, P parameter prior terms. Then we can ignore the prior terms when maximizing (10). max =
As both {  X  P smoothed ( s i ,s j ) } and { P ( s i ,s sum to 1, the above sum is maximized when The maximum likelihood estimator is then: Writing (11) in matrix form: where  X   X   X  is the outer product.
 above optimal. The corpus likelihood becomes log p ( D , V , A ) = C 1  X  X  A k 2 where C 1 = C 0 + log Z ( H ,  X  ) is constant. 5.2 Learning V as Low Rank PSD Once G  X  has been estimated from the corpus using (12), we seek V that maximizes (13). This is to find the maximum a posteriori (MAP) estimates of V , A that satisfy V &gt; V + A = G  X  . Applying this constraint to (13), we obtain Algorithm 1 BCD algorithm for finding a unreg-ularized rank-N weighted PSD approximant.
 Input : matrix G  X  , weight matrix W = f ( H ) , iteration number T , rank N Randomly initialize X (0) for t = 1 ,  X  X  X  , T do G t = W  X  G  X  + (1  X  W )  X  X ( t  X  1)
X ( t ) = PSD Approximate( G t ,N ) end for  X  , Q = Eigen Decomposition( X ( T ) ) V  X  = diag (  X  1 2 [1: N ])  X  Q &gt; [1: N ] Output : V  X  arg max = arg min
Let X = V &gt; V . Then X is positive semidef-inite of rank N . Finding V that minimizes (14) is equivalent to finding a rank-N weighted posi-tive semidefinite approximant X of G  X  , subject to Tikhonov regularization. This problem does not admit an analytic solution, and can only be solved using local optimization methods.

First we consider a simpler case where all the words in the vocabulary are enough frequent, and thus Tikhonov regularization is unnecessary. In this case, we set  X   X  i = 0 , and (14) becomes an unregularized optimization problem. We adopt the (Srebro et al., 2003) to approach this problem. The original algorithm is to find a generic rank-N ma-trix for a weighted approximation problem, and we tailor it by constraining the matrix within the positive semidefinite manifold .

We summarize our learning algorithm in Al-gorithm 1. Here  X   X   X  is the entry-wise prod-uct. We suppose the eigenvalues  X  returned by Eigen Decomposition( X ) are in descending or-der. Q &gt; [1: N ] extracts the 1 to N rows from Q &gt; .
One key issue is how to initialize X . Srebro et al. (2003) suggest to set X (0) = G  X  , and point out that X (0) = 0 is far from a local optimum, thus requires more iterations. However we find G  X  is also far from a local optimum, and this setting con-verges slowly too. Setting X (0) = G  X  / 2 usually yields a satisfactory solution in a few iterations.
The subroutine PSD Approximate( ) computes the unweighted nearest rank-N PSD approxima-tion, measured in F-norm (Higham, 1988). 5.3 Online Blockwise Regression of V In Algorithm 1, the essential subroutine PSD Approximate() does eigendecomposi-tion on G t , which is dense due to the logarithm transformation. Eigendecomposition on a W  X  W dense matrix requires O ( W 2 ) space and O ( W 3 ) time, difficult to scale up to a large vocabulary. In addition, the majority of words in the vocabulary are infrequent, and Tikhonov regularization is necessary for them.

It is observed that, as words become less fre-quent, fewer and fewer words appear around them to form bigrams. Remind that the vocabulary S = { s 1 ,  X  X  X  ,s W } are sorted in decending or-der of the frequency, hence the lower-right blocks of H and f ( H ) are very sparse, and cause these blocks in (14) to contribute much less penalty rela-tive to other regions. Therefore these blocks could be ignored when doing regression, without sacri-ficing too much accuracy. This intuition leads to the following online blockwise regression .
The basic idea is to select a small set (e.g. 30,000) of the most frequent words as the core words , and partition the remaining noncore words into sets of moderate sizes. Bigrams consist-ing of two core words are referred to as core bi-grams , which correspond to the top-left blocks of G and f ( H ) . The embeddings of core words are learned approximately using Algorithm 1, on the top-left blocks of G and f ( H ) . Then we fix the embeddings of core words, and find the em-beddings of each set of noncore words in turn. After ignoring the lower-right regions of G and f ( H ) which correspond to bigrams of two non-core words, the quadratic terms of noncore em-beddings are ignored. Consequently, finding these embeddings becomes a weighted ridge regression problem, which can be solved efficiently in closed-form. Finally we combine all embeddings to get the embeddings of the whole vocabulary. The de-tails are as follows: 1. Partition S into K consecutive groups 2. Accordingly partition G into K  X  K blocks, 4. Set V 1 = V  X  1 , and find V  X  2 that minimizes = arg min 5. For any other set of noncore words S k , find 6. Combine all subsets of embeddings to form We trained our model along with a few state-of-the-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD . The com-petitors include:  X  (Mikolov et al., 2013b): word2vec 2 , or  X  (Levy and Goldberg, 2014b): the PPMI ma- X  (Pennington et al., 2014): GloVe 3 ;  X  (Stratos et al., 2015): Singular 4 , which does  X  (Faruqui et al., 2015): Sparse 5 , which learns All models were trained on the English Wikipedia snapshot in March 2015. After removing non-textual elements and non-English words, 2.04 bil-lion words were left. We used the default hyperpa-rameters in Hyperwords when training PPMI and SVD. Word2vec, GloVe and Singular were trained with their own default hyperparameters.
 The embedding sets PSD-Reg-180K and PSD-Unreg-180K were trained using our online block-wise regression. Both sets contain the embed-dings of the most frequent 180,000 words, based on 25,000 core words. PSD-Unreg-180K was traind with all  X  i = 0 , i.e. disabling Tikhonov regularization. PSD-Reg-180K was trained with  X  regularization as the sparsity increases. To con-trast with the batch learning performance, the per-formance of PSD-25K is listed, which contains the core embeddings only. PSD-25K took advantages that it contains much less false candidate words, and some test tuples (generally harder ones) were not evaluated due to missing words, thus its scores are not comparable to others.

Sparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters.
The benchmark sets are almost identical to those in (Levy et al., 2015), except that (Luong et al., 2013) X  X  Rare Words is not included, as many rare words are cut off at the frequency 100, mak-ing more than 1/3 of test pairs invalid.
 Word Similarity There are 5 datasets: Word-Sim Similarity ( WS Sim ) and WordSim Related-ness ( WS Rel ) (Zesch et al., 2008; Agirre et al., 2009), partitioned from WordSim353 (Finkelstein et al., 2002); Bruni et al. (2012) X  X  MEN dataset; Radinsky et al. (2011) X  X  Mechanical Turk dataset; and (Hill et al., 2014) X  X  SimLex -999 dataset. The embeddings were evaluated by the Spearman X  X  rank correlation with the human ratings.

Word Analogy The two datasets are MSR  X  X  analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google  X  X  analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the cor-pus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul pro-posed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on anal-ogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), probably due to sub-optimal hyperparam-eters. This suggests their performance is unstable. The new embeddings yielded by Sparse systemat-ically degraded compared to the old embeddings, contradicting the claim in (Faruqui et al., 2015).
Our method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still better than other MF-based meth-ods. By comparing to PSD-Unreg-180K, we see Tikhonov regularization brings 1-4% performance boost across tasks. In addition, on similarity tasks, online blockwise regression only degrades slightly compared to batch factorization. Their perfor-mance gaps on analogy tasks were wider, but this might be explained by the fact that some hard cases were not counted in PSD-25K X  X  evaluation, due to its limited vocabulary. In this paper, inspired by the link functions in previous works, with the support from Informa-tion Theory, we propose a new link function of a text window, parameterized by the embeddings of words and the residuals of bigrams. Based on the link function, we establish a generative model of documents. The learning objective is to find a set of embeddings maximizing their posterior likeli-hood given the corpus. This objective is reduced to weighted low-rank positive-semidefinite approxi-mation, subject to Tikhonov regularization. Then we adopt a Block Coordinate Descent algorithm, jointly with an online blockwise regression algo-rithm to find an approximate solution. On seven benchmark sets, the learned embeddings show competitive and stable performance.

In the future work, we will incorporate global latent factors into this generative model, such as topics, sentiments, or writing styles, and develop more elaborate models of documents. Through learning such latent factors, important summary information of documents would be acquired, which are useful in various applications.
 We thank Omer Levy, Thomas Mach, Peilin Zhao, Mingkui Tan, Zhiqiang Xu and Chunlin Wu for their helpful discussions and insights. This re-search is supported by the National Research Foundation, Prime Minister X  X  Office, Singapore under its IDM Futures Funding Initiative and ad-ministered by the Interactive and Digital Media Programme Office. Suppose M is the bigram matrix of interest. SVD embeddings are derived from the low rank approx-values/vectors. When some of these singular val-ues correspond to negative eigenvalues, undesir-able correlations might be captured. The follow-ing is an example of approximating a PMI matrix. A vocabulary consists of 3 words s 1 ,s 2 ,s 3 . Two corpora derive two PMI matrices:
They have identical left singular matrix and sin-gular values (3 , 2 , 1) , but their eigenvalues are (3 , 2 , 1) and (  X  3 , 2 , 1) , respectively.
In a rank-2 approximation, the largest two singular values/vectors are kept, and M (1) and M (2) yield identical SVD embeddings V = the algorithm, without affecting the validity of the following conclusion). The embeddings of s 1 and s 2 (columns 1 and 2 of V ) point at the same di-rection, suggesting they are positively correlated. However as M (2) actually negatively correlated in the second cor-pus. This inconsistency is because the principal eigenvalue of M (2) is negative, and yet the corre-sponding singular value/vector is kept.

When using eigendecomposition, the largest two positive eigenvalues/eigenvectors are kept. M (1) yields the same embeddings V . M (2) preserves the negative correlation between s 1 ,s 2 . Redundant information refers to the reduced un-certainty by knowing the value of any one of the conditioning variables (hence redundant). Syner-gistic information is the reduced uncertainty as-cribed to knowing all the values of conditioning variables, that cannot be reduced by knowing the value of any variable alone (hence synergistic).
The mutual information I ( y ; x i ) and the redun-dant information Rdn ( y ; x 1 ,x 2 ) are defined as:
Rdn ( y ; x 1 ,x 2 ) = E P ( y ) min
The synergistic information Syn ( y ; x 1 ,x 2 ) is defined as the PI-function in (Williams and Beer, 2010), skipped here.
 Figure 2: Different types of information among 3 random variables y,x 1 ,x 2 . I ( y ; x 1 ,x 2 ) is the mutual information between y and ( x 1 ,x 2 ) . dant information and synergistic information be-tween x 1 ,x 2 , conditioning y , respectively.
The interaction information Int ( x 1 ,x 2 ,y ) mea-sures the relative strength of Rdn ( y ; x 1 ,x 2 ) and Syn ( y ; x 1 ,x 2 ) (Timme et al., 2014):
Int ( x 1 ,x 2 ,y ) = Syn ( y ; x 1 ,x 2 )  X  Rdn ( y ; x 1 ,x 2 ) = I ( y ; x 1 ,x 2 )  X  I ( y ; x 1 )  X  I ( y ; x 2 )
Figure 2 shows the relationship of different information among 3 random variables y,x 1 ,x 2 (based on Fig.1 in (Williams and Beer, 2010)).
PMI is the pointwise counterpart of mutual information I . Similarly, all the above concepts have their pointwise counterparts, obtained by dropping the expectation operator. Specifically, the pointwise interaction information is defined as PInt ( x 1 ,x 2 ,y ) = PMI ( y ; x 1 ,x 2 )  X  PMI ( y ; x If we know PInt ( x 1 ,x 2 ,y ) , we can recover PMI ( y ; x 1 ,x 2 ) from the mutual information over the variable subsets, and then recover the joint distribution P ( x 1 ,x 2 ,y ) .
 As the pointwise redundant information PRdn ( y ; x 1 ,x 2 ) and the pointwise synergistic information PSyn ( y ; x 1 ,x 2 ) are both higher-order interaction terms, their magnitudes are usually much smaller than the PMI terms. We assume they are approximately equal, and thus cancel each other when computing PInt. Given this, PInt is always 0 . In the case of three words w 0 ,w 1 ,w 2 , PInt ( w 0 ,w 1 ,w 2 ) = 0 leads to PMI ( w 2 ; w 0 ,w 1 ) = PMI ( w 2 ; w 0 )+ PMI ( w 2 ; w
