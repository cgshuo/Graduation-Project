 Felix Bie X mann felix.biessmann@tu-berlin.de Dept. ML, Berlin Institute of Technology Jens-Michalis Papaioannou jensmicha@googlemail.com Dept. ML, Berlin Institute of Technology Mikio Braun mikio.braun@tu-berlin.de Dept. ML, Berlin Institute of Technology Andreas Harth harth@kit.edu Institute AIFB, Karlsruhe Institute of Technology (KIT) Temporal information is a fundamental aspect of many datasets. Several commercial o  X  erings are based on temporal variation of web sources for data mining 1 . The news domain is a prime example for an industry where time matters. Sources that break a story gain reputation and economic benefits. We thus consider the problem of identifying trendsetting news sources based on temporal correlations found in data. Our definition of a trend setter is simple. If a single web source publishes content that will later on domi-nate the content of a pool of other websites, we con-sider this source as a trend setter; this approach is similar to causality based graph analyses such as in ( Lozano and Sindhwani. , 2010 ). In order to test the trendsetting behavior of a web source we first extract a time series of features from each web source. Then we learn for each web source a convolution in this fea-ture space that predicts the content of all other web sources of interest.
 Our contributions are as follows:  X  We present an approach that detects the canon- X  We propose an unsupervised algorithm that iden- X  We evaluate the approach on a dataset of news As an example data set we collected data from the most influential technology news websites. Bag-of-Word (BoW) features were extracted for each website. We then predict the total information of all websites at time point t using only the information of one web source at prior time points t  X  .Ourresultsshow that some news sites can predict the future temporal dynamics of the tech-news-sphere well, while others fail to do so. The prediction performance can be in-terpreted as how much a given news site can be con-sidered as a trend setter and can be used to rank sites according to this criterion: The better a news site pre-dicts the future information of all other news sites, the more influential the news site is. In the following we discuss some alternative ap-proaches towards analysis of temporal dynamics in web data graphs. The authors of ( Sun et al. , 2007 )use the temporal dynamics within a communication net-work graph to partition the nodes of the graph into groups. The method first extracts adjacency matrices of the graph for di  X  erent time points and then tries to compress this time series of connections. This is done by finding similar connection patterns over time and group them together. The motivation of this approach is very di  X  erent from ours and a direct comparison of these two approaches is not possible. But there is a similarity that is worth noting: If one web source pre-dicts the content of all other nodes perfectly, we can focus on this single node only and forget about the rest of the network. Thus the representation found by our approach can be seen as an optimal compression of the graph, too.
 Other approaches towards network data graphs evolv-ing over time investigate the di  X  usion of influential items, so called memes ( Leskovec et al. , 2009 ; Yang &amp; Leskovec , 2010 ; Gomez Rodriguez et al. , 2011 ). In ( Leskovec et al. , 2009 ; Yang &amp; Leskovec , 2010 ) the au-thors focus on di  X  usion of n-grams in blogs and news media. The method proposed in ( Yang &amp; Leskovec , 2010 ) finds those n-grams that are repeated often, i.e. that account for a large volume of a graph. This objec-tive is very similar to that of this study. The objective of our method is to predict the content of a pool of web sources optimally. This is equivalent to finding nodes that maximize the variance explained of a pool of other web sources. Similar to ( Yang &amp; Leskovec , 2010 ) we use a linear model. A decisive advantage of our approach is that it straightforwardly extends to non-linear dependencies (see section 5.2 ). Another im-portant di  X  erence is that in ( Yang &amp; Leskovec , 2010 ) information transmission is modeled as an indicator function in, meaning information has been transmit-ted at a certain time lag or not. In our approach we do not restrict the analysis to a binary transmis-sion scheme. Instead we learn a gradual information transmission model from the data. Another related approach is ( Gomez Rodriguez et al. , 2011 ). Here the authors analyze the temporal dynamics of information cascades in a temporally evolving graph, in particular how n-grams di  X  use through a network. The cascades are represented as time stamps of selected n-grams. Di  X  erent generative models are fitted to the data us-ing convex optimization. A central assumption is that the transmission rates can be estimated independently for each cascade. This assumption is similar to our ap-proach: We analyze the temporal dynamics of single web sources independently.
 Despite a number of similarities between ( Leskovec et al. , 2009 ; Yang &amp; Leskovec , 2010 ; Gomez Rodriguez et al. , 2011 ) and our method we emphasize an im-portant di  X  erence: All of the above approaches re-quire that the relevant items of information are se-lected prior to the analysis. For example in ( Leskovec et al. , 2009 ; Yang &amp; Leskovec , 2010 ) the authors an-alyze a large data set containing millions of n-grams. But only 1000 information cascades are selected for the final analysis according to some heuristics. Thus the result can depend on data selection during pre-processing. Our approach is di  X  erent in that it takes the full data set and automatically learns the relevant features. Another crucial di  X  erence is that the above approaches do not model dependencies between infor-mation cascades. In real data sets it is very likely that on piece of information is highly correlated with an-other piece of information. The method proposed here takes into account the dependencies between features and models the full multivariate temporal dynamics between web sources. For our approach we extract from each web source f 2 { 1 , 2 ,...,F } in our collection of F web sources the corresponding features x f ( t ) 2 R W at time points t = { 0 , 1 ,...,T } . For the sake of simplicity we here assume regularly sampled time points. In our appli-cation example we will extract Bag-of-Words features, see section 6.2.1 , but our approach is readily applica-ble to other feature representations such as n-grams or collections of hyperlinks. After feature extraction we store the multivariate feature time series in a sparse matrix We are interested not in the dynamics of a single web source but rather the temporal variation of many nodes in the web graph. The joint time series of all web sources Y f can be obtained as the average across all X f where f 0 denotes the indices of all web sources except f . We now represent a canonical trend (CT) y f ( t ) as a combination w y 2 R W of features In our application example of BoW features we chose a linear feature combination as the optimal tradeo  X  between a too simplistic modeling of single word oc-currences 2 and a computationally costly n-gram rep-resentation as e.g. in ( Leskovec et al. , 2009 ). If the re-lationships between single features are more complex, the linear feature combination w y can be replaced by arbitrary non-linear feature combinations simply by using appropriate kernel functions (see section 5.2 ). The aim of our approach is to predict the temporal variation of the overall trend y f ( t ) using the infor-mation published in the past N  X  hours by a single news feed X f . This means we want to find a tem-poral convolution w x (  X  ) that uses the information of x ( t  X  ) ,  X  2 { 1 ,...,N  X  } to predict the canonical trend y ( t ). The optimal prediction of y f ( t ) based on the content published in the past N  X  hours on a single web source x f ( t ) can be formulated as Neglecting the amplitude of y f ( t ) and  X  y f ( t ), minimiz-ing the least-squares error of eq. 4 is equivalent to max-imizing the correlation between y f ( t ) and  X  y f ( t ) The optimal w x (  X  ) and w y can be computed simul-taneously using canonical correlation analysis (CCA) ( Hotelling , 1936 ). CCA has proven very useful for a wide variety of applications ranging from signal pro-cessing ( Akaike , 1976 ) over e cient computation of causality measures ( Otter , 1991 ). The mathemati-cal properties of CCA are as well understood ( Jor-dan , 1875 ) as its statistical convergence criteria ( An-derson , 1999 ; Fukumizu et al. , 2007 ). Instead of stan-dard CCA we use an extension, temporal kernel CCA (tkCCA), that can deal with high dimensional data, small sample sizes and time delayed non-linear depen-dencies between data ( Bie X mann et al. , 2010 ). The interpretation of w y and w x (  X  ) is straightforward. In our application example they are the directions in the BoW feature space that maximize the correlation be-tween a single feed and all other news feeds (or equiv-alently  X  assuming normalized time series  X  minimize the prediction error between the two). CCA simulta-neously optimizes w y and w x (  X  ) such that the correla-tion between y f ( t ) and  X  y f ( t ) is invariant with respect to all linear transformations of the data 3 .Thisiswhy the correlation coe cient in eq. 5 is called canonical . The projection w y maps the data into their respective canonical subspace .Wethusrefertothetimeseries y ( t ) as the canonical trend (CT) in the BoW feature space.
 The correlation coe cient in eq. 5 is obtained from a convolved time series. The convolution in eq. 4 sums over all time lags  X  . Often it can give valuable in-sights in the temporal dynamics between variables if one computes a time lag dependent correlation coe -cient  X  (  X  ) We will refer to  X  (  X  ) as the canonical correlo-gram , in complete analogy to a standard cross-correlogram. The main di  X  erence is that standard cross-correlograms are typically computed between two univariate signals. The canonical correlogram is computed between high dimensional multivariate time series, projected into their canonical subspace. The canonical correlogram  X  (  X  ) and the coe cients of the convolution w x (  X  ) reflect the temporal dynamics in the canonical subspace. An illustrative toy data example is shown in figure 1 , for an explanation see section 6.1 . Informally our approach consists of three steps: 1. Extract feature matrix X f for each feed 2. Temporal Embedding of single news feed X f 3. Kernel CCA between X f and all other feeds Y f In the following we describe steps two and three in detail. Data collection and feature extraction are de-scribed in section 6.2.1 . 5.1. Temporal Embedding The temporal embedding is done by creating for each feed f a new representation  X  X f in which we add copies of the data in X f , shifted back in time by a time lag  X  5.2. Kernel CCA The temporal embedding operation will increase the dimensionality of our data by a factor of N  X  ,thenum-ber of time lags. However using the well known kernel trick ( Aizerman et al. , 1964 ) we can e ciently compute CCA in kernel space. A main advantage of this trick is that computation of non-linear dependencies becomes a linear problem in kernel space, see e.g. ( Fyfe &amp; Lai , 2000 ). Another crucial advantage of kCCA for the given problem setting is that it reduces the problem size substantially: Estimating w y and w x (  X  )inthein-put space requires the inversion of covariance matrices of size ( W + WN  X  ) 2 ,where W denotes the number of features. In kernel space we only have to deal with matrices of size (2 T ) 2 ,where T denotes the number of samples. For the sake of simplicity we consider linear kernels here, but non-linear dependencies can be eas-ily estimated by replacing the linear kernel with other kernel functions. When using linear kernels the CCA solution in input space is a linear expansion of data points The coe cients  X  and the eigenvectors of the gen-eralized eigenvalue problem  X  where K x =  X  X &gt; f  X  X f 2 R T  X  T is the linear kernel ma-matrix of Y f . The eigenvalue is the canonical corre-lation on the training data set 4 , which yields the same result as eq. 5 . The matrices on the right hand side are computed as L x = K 2 x +  X  I and L y = K 2 y +  X  I , where  X  is a regularization parameter controlling the complexity of the solution. For very noisy data  X  will Algorithm 1 Canonical Trend Algorithm
Loop over all news feeds for f =1 to F do end for Rank Feeds according to 1 / 10 be large and thus the optimal  X  and will be a vec-tor with very similar entries  X  i , and the same will be true for . This e  X  ectively means that eq. 8 and eq. 9 will reduce to computing the empirical mean of the data. After optimization of N  X  ,  X  and eq. 10 we can recover the canonical projection w y according to eq. 8 and the canonical convolution w x (  X  ) according to eq. 9 . We then could compute  X  y f ( t ) according to eq. 4 and the overall trend y f ( t )usingeq. 3 . In practice however this is suboptimal in terms of computational cost. Instead of recovering w y ,w x (  X  ) and computing y ( t ) ,  X  y f ( t ), we can stay in kernel space to evaluate the models. This yields a substantial computational speedup once the kernels are computed. The complete canonical trend detection algorithm is summarized in algorithm 1 . 5.3. Model evaluation for time series In order to obtain meaningful prediction accuracies we apply 10-fold cross-validation: we split the available data into training and test data, estimate  X  and on the training set and compute the prediction accu-racy in eq. 5 on test data. When performing cross-validation on time series data special care has to be taken. In contrast to standard classification settings, where one can simply randomly pick a certain subset of the data, the temporal dependencies in time series data do not allow for such a simple resampling. For proper cross-validation we split the time series in 10 blocks of equal length. Due to the temporal embed-ding (see eq. 7 ) consecutive blocks will overlap by N  X  samples. Thus we discarded the first N  X  samples from the training block adjacent to the test data block. This ensured that no data point that we tested on was used for training the KCCA model. We estimated the opti-mal time lag and regularization parameters using 10-fold cross-validation (nested within the training data set) and a grid search over time lags  X  2 { 1 , 2 ,..., 10 } and  X  2 { 10 5 , 10 4 ,..., 10 1 } . Optimal regularizers  X  were in the range of 10 3 to 10 1 , the optimal time lag was  X  = 5 hours. 5.4. Comparison with other approaches The relevant contribution of the CT algorithm is that it maximizes the co-variation of single web sources X f and other web sources Y f . This is accomplished by a joint factorization of  X  X f and Y f (see eq. 10 ). An alter-native approach for topic detection is latent semantic analysis (LSA) ( Deerwester et al. , 1990 ) in which only a single matrix of BoW features is factorized. In LSA the strongest topic v y,f 2 R W is that subspace in the BoW space, here the row space of Y f , that captures most variance argmax The strongest topic v x,f in the single feed BoW space X f is found analogously. Informally the relationship between LSA and CT is similar to the relationship be-tween principal component analysis (PCA) ( Pearson , 1901 ) and CCA: PCA maximizes the variance within one web source X f (or a collection of web sources Y f ) while CCA maximizes the co-variation between multi-ple web sources X f and Y f . We compared the canon-ical trend predictions (eq. 5 ) with the correlation be-Y f separately. As an additional sanity check we also shu  X  ed the data in time and thereby destroyed the temporal dependencies between X f and Y f (results shown in table 1 , middle column). All analyses were performed analogously on this surrogate data set, in order to show that the prediction accuracies were in-deed meaningful and not just overfitted. We first illustrate our approach on a toy data set. Thereafter we present some results on real data ex-tracted from technology news feeds.
 6.1. Canonical Trends: A toy data example For illustrative purposes we consider an event that has been reported extensively on. In 2010 a volcano on Iceland erupted and produced a large ash cloud. Due to this cloud a lot of flights had to be cancelled for security reasons, as the ash could damage aircraft tur-bines. In the course of the events, every news page on the web reported on the eruption and its conse-quence. Not every news page used the same words but the overall trend across all news pages included words like eruption, volcano, iceland, aircraft, traf-fic etc. that co-occurred increasingly. We model this trend in the BoW feature representation time series of two di  X  erent web sources X 2 R 3 and Y 2 R 3 . The trend is reflected in the di  X  erent dimensions of ing to the words Phone, Volcano, Airplane and anal-ogously it is reflected in the dimensions of Y with the words Cloud, iPad, Ash . So one BoW dimension did not carry relevant information ( Phone, iPad ) and the other two dimensions did carry relevant information, respectively. The toy data was generated from an un-derlying trend s ( t ) 2 R 1 , reflecting the volcano erup-tion and its consequences on air tra c, by where  X  ( t )  X  N (0 , 1) was noise drawn from a standard normal distribution in R 3 and =0 . 9 was the signal to noise ratio of the trend. The BoW time series are shown in the right panels of figure 1 . X is generated from the latent trend variable s ( t ) with a temporal lag of 3 temporal units so that X will be ahead of Y by 3 time samples. Note that the dimensions in X were not related to the dimensions of Y . This is a realistic setting: In practice this is di cult to define all possible trends apriori , even with the help of a semantic dic-tionary. But the increased co-occurrence of the above mentioned trend-relevant words, that is the temporal co-variation in the canonical subspace defined by w  X  x and w  X  y , captures the trend very well.
 This canonical subspace is robustly found by the canonical trend detection algorithm. The optimal con-volution w x (  X  ) and the projection w y are plotted on the left of figure 1 . They clearly reflect the structure of w  X  x and w  X  y that gave rise to the trend in the BoW space. In the case of w  X  x , the canonical trend detection yields a convolution, rather than a simple projection. The additional temporal dimension indicates the tem-poral dependency structure in the canonical subspace. At a time lag of 3 temporal units, the web source X predicts the web source Y best. So the optimal BoW features for X , corresponding to w  X  x , are found at a time lag of 3. The canonical correlogram (see eq. 6 ) for our toy data example is plotted in the middle panel on the left and shows a strong peak at  X  = 3, indicat-ing that X published the relevant information 3 time units before Y . 6.2. Trend setter detection in News feeds 6.2.1. Data Collection We collected data from 96 news feeds 5 during the year of 2011. Bag-of-Word (BoW) features were extracted using standard natural language processing tools 6 . Af-ter removal of stop words and stemming our BoW dic-tionary contained W  X  10 5 words. The time series of each word was tf-idf normalized. The feature time se-ries were then stored in sparse matrices X f 2 R W  X  T where f = { 1 ,...,F = 96 } denotes news feed and t = { 1 ,...,T } denotes the time in hours. Time stamps of all news web sources were set to CET. For the sake of comprehensibility in the results presented here we focus on the month of October in 2011. In this month a clearly detectable trend were reports of Steve Jobs X  death. 6.2.2. Canonical Trends in News feeds As we obtain a canonical projection w y for each pool of web sources Y f , the canonical trends that are pre-dicted by each news feed X f could potentially di  X  er. In practice however, the canonical trends are very sim-ilar. Figure 2 shows in the top panel the median and 25th/75th percentiles of all canonical trends in Oc-tober 2011. The percentiles are very close to the me-dian trend, indicating a large similarity of the di  X  erent canonical trends. Reports on Steve Jobs X  death mark a pronounced peak in the first week reflected in all canonical trends. Also note that the trends clearly re-flect the weekly publishing activity on the news feeds, five peaks each week and a trough reflecting the week-end. Our results show that the temporal dynamics in the canonical subspace can be easily interpreted and authentically reflect the impact of relevant information cascades in large web graphs.
 6.2.3. Canonical Trend Prediction We investigated how well we can predict the trends in a pool of web sources from a single web source. Table 1 shows the prediction accuracy as canonical correlation (see eq. 5 ) for the ten best predictors, i.e. the trend setter news feeds, summarized as 25th/50th/75th per-centiles across cross validation folds. Using the in-formation published at t  X  ,  X  = { 1 ,..., 5 } , mean-ing five to one hours before all other feeds, the listed news feeds could predict the overall trend at time t with high accuracy. For instance the web site http://businessinsider.com predicted the content of all other news websites in the data set in more than 50% of the cases tested with a correlation coe cient of 0.8. The trend prediction  X  y f ( t ) of the top trendset-ter http://businessinsider.com is shown in the bottom panel of figure 2 . The time course clearly captures the temporal variation of the overall trend, depicted above in the top panel of fig. 2 . In the top panel of figure 3 the time lag dependent features of w x (  X  ) are depicted. The words to which the canonical trend detection algorithm assigned high weights were asso-ciated with Steve Jobs or Apple. The corresponding canonical correlogram  X  (  X  ) has a pronounced peak at  X  = 5hrs.
 It is important to note that the temporal dynamics of single features in w x (  X  ) can be di  X  erent than those of  X  (  X  ). One reason for this is that w x (  X  ) is non-separable, meaning that it does not factorize into a single temporal component and one component that describes the dependencies in the BoW feature space. So in order to get the full picture of the temporal dy-namics between X f and Y f one has to look at the time courses of all features in w x (  X  ). However we can identify relevant features from w x (  X  ) by picking those with the highest absolute weights, summed over time lags. And we can extract the overall temporal dynam-ics from the canonical subspace from  X  (  X  ). We compared the predictions from the canonical trend algorithm to predictions obtained with a standard topic detection method (LSA, see section 5.4 ). The results are shown in table 1 , right column . In the LSA setting, we extracted topics from the BoW time se-ries of single news feeds and the average BoW time series separately. Predictions of the strongest topics in all news feeds based on the strongest topics in a single feed are lower than the CT predictions. This suggests that canonical trends found in a single web source generalize better to a pool of web sources. This is expected from the di  X  erent objective functions of CT (see eq. 5 ) and LSA (see eq. 11 ).
 We presented a simple, e cient and purely data driven method for detecting news trends and trend setters in web data. By making use of the kernel trick we can e ciently exploit the full multivariate structure of temporal dependencies in the canonical subspace of web graph features such as the BoW representation. Both the detected trends and the features learned by the algorithm authentically reflect the true impact of information cascades in temporally evolving graphs. Future work includes more empirical evaluations to study temporal correlation not only from BoW fea-tures but also from auxiliary data, such as frequency of retweets along the lines of ( Lerman &amp; Hogg , 2010 ), which predict popularity of content based on early user interest. Another useful feature representation could be named entities along the lines of ( Gabrilovich et al. , 2004 ). Independent of the feature representation em-ployed it is important to note that the CT algorithm is unsupervised. The objective of the CT algorithm, maximal co-variation (eq. 5 ), does not necessarily yield the most interesting trends. Some information that is highly relevant might not be reflected as the main os-cillation in all news feeds. However the criterion used in our approach, maximal variance explained, is use-ful if one is interested in the web sources that have the strongest overall impact. For more detailed anal-yses the trend of interest could be manually defined (for instance by picking only a few words of interest). Future research will also have to investigate temporal interactions between multidimensional canonical trend subspaces. Moreover we here assumed that the tem-poral dependencies between web sources are station-ary in the analysis period. In general this might not be the case. Web source dependencies can be highly non-stationary. These non-stationarities have to be in-vestigated using appropriate methods, as for instance ( von B  X unau et al. , 2009 ).
 Acknowledgements
