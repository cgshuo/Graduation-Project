 Automatic summarization has historically focused on summarizing events, a task embodied in the series of Document Understanding Conferences 1 . However, there has also been work on entity-centric summarization , which aims to produce summaries from text collections that are relevant to a particu-lar entity of interest, e.g., product, person, company, etc. A well-known example of this is from the opin-ion mining community where there has been a num-ber of studies on summarizing the expressed senti-ment towards entities (cf. Hu and Liu (2006)). An-other recent example of entity-centric summariza-tion is the work of Filippova et al. (2009) to produce company-specific financial report summaries.
In this study we investigate a variation of entity-centric summarization where the goal is not to sum-marize information about a single entity, but pairs of entities. Specifically, our aim is to jointly gen-erate two summaries that highlight differences be-tween the entities  X  a task we call contrastive sum-marization . An obvious application comes from the consumer reviews domain, where a person consider-ing a purchase wishes to see the differences in opin-ion about the top candidates without reading all the reviews for each product. Other applications include contrasting financial news about related companies or comparing platforms of political candidates.
Contrastive summarization has many points of comparison in the NLP, IR and Data-Mining liter-ature. Jindal and Liu (2006) introduce techniques to find and analyze explicit comparison sentences, but this assumes that such sentences exist. In con-trastive summarization, there is no assumption that two entities have been explicitly compared. The goal is to automatically generate the comparisons based on the data. In the IR community, Sun et al. (2006) explores retrieval systems that align query results to highlight points of commonality and dif-ference. In contrast, we attempt to identify con-trasts from the data, and then generate summaries that highlight them. The novelty detection task of determining whether a new text in a collection con-tains information distinct from that already gathered is also related (Soboroff and Harman, 2005). The primary difference here is that contrastive summa-rization aims to extract information from one col-lection not present in the other in addition to infor-mation present in both collections that highlights a difference between the entities.

This paper describes a contrastive summarization experiment where the goal is to generate contrasting opinion summaries of two products based on con-sumer reviews of each. We look at model design choices, describe an implementation of a contrastive summarizer, and provide an evaluation demonstrat-ing a significant improvement in the usefulness of contrastive summaries versus summaries generated by single-product opinion summarizers. As input we assume a set of relevant text excerpts (typically sentences), T = { t 1 ,...,t m } , which con-tain opinions about some product of interest. The goal of opinion summarization 2 is to select some number of text excerpts to form a summary S of the product so that S is representative of the aver-age opinion and speaks to its important aspects (also proportional to opinion), which we can formalize as: S = arg max where L is some score over possible summaries that embodies what a user might desire in an opinion summary, LENGTH ( S ) is the length of the summary and K is a pre-specified length constraint.
We assume the existence of standard sentiment analysis tools to provide the information used in the scoring function L . First, we assume the tools can assign a sentiment score from -1 (negative) to 1 (pos-itive) to an arbitrary span of text. Second, we as-sume that we can extract a set of aspects that the text is discussing (e.g,  X  X he sound was crystal clear X  is about the aspect sound quality ). We refer the reader to abundance of literature on sentiment analysis for more details on how such tools can be constructed (cf. Pang and Lee (2008)). For this study, we use the tools described and evaluated in Lerman et al. (2009). We note however, that the subject of this discussion is not the tools themselves, but their use.
The single product opinion summarizer we con-sider is the Sentiment Aspect Match model (SAM) described and evaluated in (Lerman et al., 2009). Underlying SAM is the assumption that opinions can be described by a bag-of-aspects generative pro-cess where each aspect is generated independently and the sentiment associated with the aspect is gen-erated conditioned on its identity, where A t is a set of aspects that are mentioned in text excerpt t , p ( a ) is the probability of seeing aspect a , and SENT ( a t )  X  [  X  1 , 1] is the sentiment associ-ated with aspect a in t . The SAM model sets p ( a ) through the maximum likelihood estimates over T and assumes p ( SENT ( a t ) | a ) is normally distributed with a mean and variance also estimated from T . We denote SAM ( T ) as the model learned using the entire set of candidate text excerpts T .

The SAM summarizer scores each potential sum-mary, S , by learning another model SAM ( S ) based on the text excerpts used to construct S . We can then measure the distance between a model learned over the full set T and a summary S by summing the KL-divergence between their learned probability distri-butions. In our case we have 1 + | A T | distributions  X  p ( a ) , and p (  X | a ) for all a  X  A T . We then define That is, the SAM summarizer prefers summaries whose induced model is close to the model induced for all the opinions about the product of interest. Thus, a good summary should (1) mention aspects in roughly the same proportion that they are mentioned in the full set of opinions and (2) mention aspects with sentiment also in proportion to what is observed in the full opinion set. A high scoring summary is found by initializing a summary with random sen-tences and hill-climbing by replacing sentences one at a time until convergence.

We chose to use the SAM model for our exper-iment for two reasons. First, Lerman et al. (2009) showed that among a set of different opinion sum-marizers, SAM was rated highest in a user study. Secondly, as we will show in the next section, the SAM summarization model can be naturally ex-tended to produce contrastive summaries. When jointly generating pairs of summaries, we at-tempt to highlight differences between two products. These differences can take multiple forms. Clearly, two products can have different prevailing sentiment scores with respect to an aspect (e.g.  X  X roduct X has great image quality X  vs  X  X roduct Y X  X  image quality is terrible X ). Reviews of different products can also emphasize different aspects. Perhaps one product X  X  screen is particularly good or bad, but another X  X  is not particularly noteworthy  X  or perhaps the other product simply doesn X  X  have a screen. Regardless of sentiment, reviews of the first product will empha-size the screen quality aspect more than those of the second, indicating that our summary should as well.
As input to our contrastive summarizer we assume two products, call them x and y as well as two corre-sponding candidate sets of opinions, T x and T y , re-spectively. As output, a contrastive summarizer will produce two summaries  X  S x for product x and S y for product y  X  so that the summaries highlight the differences in opinion between the two products.
What might a contrastive summarizer look like on a high-level? Figure 1 presents some options. The first example (1a) shows a system where each sum-mary is generated independently, i.e., running the SAM model on each product separately without re-gard to the other. This procedure may provide some useful contrastive information, but any such infor-mation will be present incidentally. To make the summaries specifically contrast each other, we can modify our system by explicitly modeling the fact that we want summaries S x and S y to contrast. In the SAM model this is trivial as we can simply add a term to the scoring function L that attempts to maxi-mize the KL-divergence between the two summaries induced models SAM ( S x ) and SAM ( S y ) .
This approach is graphically depicted in figure 1b, where the system attempts to produce summaries that are maximally similar to the opinion set they are drawn from and minimally similar from each other. However, some obvious degenerate solutions arise if we chose to model our system this way. Consider two products, x and y , for which all opinions dis-cuss two aspects a and b with identical frequency and sentiment polarity. Furthermore, several opin-ions of x and y discuss an aspect c , but with oppo-site sentiment polarity. Suppose we have to build contrastive summaries and only have enough space to cover a single aspect. The highest scoring con-trastive pair of summaries would consist of one for x that mentions a exclusively, and one for y that men-tions b exclusively  X  these summaries each mention a promiment aspect of their product, and have no overlap with each other. However, they provide a false contrast because they each attempt to contrast the other s ummary, rather than the other p roduct. Better would be for both to cover aspect c .
To remedy this, we reward summaries that in-stead have a high KL-divergence with respect to the other product X  X  full model SAM ( T ) as depicted in Figure 1c. Under this setup, the degenerate solution described above is no longer appealing, as both sum-maries have the same KL-divergence with respect to the other product as they do to their own product. The fact that the summaries themselves are dissim-ilar is irrelevant. Comparing the summaries only to the products X  full language models prevents us from rewarding summaries that convey a false contrast be-tween the products under comparison. Specifically, we now optimize the following joint summary score: Note that we could additionally model divergence between the two summaries (i.e., merging models in figures 1b and c), but such modeling is redundant. Furthermore, by not explicitly modeling divergence between the two summaries we simplify the search space as each summary can be constructed without knowledge of the content of the second summary. Our experiments focused on consumer electronics. In this setting an entity to be summarized is one spe-cific product and T is a set of segmented user re-views about that product. We gathered reviews for 56 electronics products from several sources such as CNet, Epinions, and PriceGrabber. The products covered 15 categories of electronics products, in-cluding MP3 players, digital cameras, laptops, GPS systems, and more. Each had at least four reviews, and the mean number of reviews per product was 70.
We manually grouped the products into cate-gories (MP3 players, cameras, printers, GPS sys-System As Received Consolidated tems, headphones, computers, and others), and gen-erated contrastive summaries for each pair of prod-ucts in the same category using 2 different algo-rithms: (1) The SAM algorithm for each product in-dividually (figure 1a) and (2) The SAM algorithm with our adaptation for contrastive summarization (figure 1c). Summaries were generated using K = 650 , which typically consisted of 4 text excerpts of roughly 160 characters. This allowed us to compare different summaries without worrying about the ef-fects of summary length on the ratings. In all, we gathered 178 contrastive summaries (89 per system) to be evaluated by raters and each summary was evaluated by 3 random raters resulting in 534 rat-ings. The raters were 55 everyday internet users that signed-up for the experiment and were assigned roughly 10 random ratings each. Raters were shown two products and their contrastive summaries, and were asked to list 1-3 differences between the prod-ucts as seen in the two summaries. They were also asked to read the products X  reviews to help ensure that the differences observed were not simply arti-facts of the summarizer but in fact are reflected in actual opinions. Finally, raters were asked to rate the helpfulness of the summaries in identifying these distinctions, rating each with an integer score from 0 ( X  X xtremely useful X ) to 3 ( X  X ot useful X ).
Upon examining the results, we found that raters had a hard time finding a meaningful distinction be-tween the two middle ratings of 1 and 2 ( X  X seful X  and  X  X omewhat useful X ). We therefore present two sets of results: one with the scores as received from raters, and another with all 1 and 2 votes consol-idated into a single class of votes with numerical score 1.5. Table 1 gives the average scores per sys-tem, lower scores indicating superior performance. The scores indicate that the addition of the con-trastive term to the SAM model improves helpful-ness, however both models roughly have average System 2+ raters All 3 raters SAM 0.8 0.2 SAM + contrastive 2.0 0.6 scores in the somewhat-useful to useful range. The difference becomes more pronounced when look-ing at the consolidated scores. The natural question arises: does the relatively small increase in helpful-ness reflect that the contrastive summarizer is doing a poor job? Or does it indicate that users only find slightly more utility in contrastive information in this domain? We inspected comments left by raters in an attempt to answer this. Roughly 80% of raters were able to find at least two points of contrast in summaries generated by the SAM+contrastive ver-sus 40% for summaries generated by the simple SAM model. We then examined the consistency of rater comments, i.e., to what degree did differ-ent raters identify the same points of contrast from a specific comparison? We report the results in table 2. Note that by this metric in particular, the contrastive summarizer outperforms its the single-product sum-marizer by significant margins and provides a strong argument that the contrastive model is doing its job. Acknowledgements: The Google sentiment analy-sis team for insightful discussions and suggestions.
