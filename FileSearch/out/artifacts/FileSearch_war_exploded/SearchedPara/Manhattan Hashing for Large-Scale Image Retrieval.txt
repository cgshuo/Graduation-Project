 Hashing is used to learn binary-code representation for data with expectation of preserving the neighborhood structure in the origi-nal feature space. Due to its fast query speed and reduced storage cost, hashing has been widely used for efficient nearest neighbor search in a large variety of applications like text and image re-trieval. Most existing hashing methods adopt Hamming distance to measure the similarity (neighborhood) between points in the hash-code space. However, one problem with Hamming distance is that it may destroy the neighborhood structure in the original feature space, which violates the essential goal of hashing. In this paper, Manhattan hashing (MH), which is based on Manhattan distance, is proposed to solve the problem of Hamming distance based hashing. The basic idea of MH is to encode each projected dimension with multiple bits of natural binary code (NBC), based on which the Manhattan distance between points in the hashcode space is calcu-lated for nearest neighbor search. MH can effectively preserve the neighborhood structure in the data to achieve the goal of hashing. To the best of our knowledge, this is the first work to adopt Manhat-tan distance with NBC for hashing. Experiments on several large-scale image data sets containing up to one million points show that our MH method can significantly outperform other state-of-the-art methods.
 H.3.3 [ Information Systems ]: Information Search and Retrieval Algorithms, Measurement Hashing, Image Retrieval, Approximate Nearest Neighbor Search, Hamming Distance, Manhattan Distance
Nearest neighbor (NN) search [28] has been widely used in ma-chine learning and related application areas, such as information retrieval, data mining, and computer vision. Recently, with the ex-plosive growth of data on the Internet, there has been increasing interest in NN search in massive (large-scale) data sets. Traditional brute force NN search requires scanning all the points in a data set whose time complexity is linear to the sample size. Hence, it is computationally prohibitive to adopt brute force NN search for massive data sets which might contain millions or even billions of points. Another challenge faced by NN search in massive data sets is the excessive storage cost which is typically unacceptable if tra-ditional data formats are used.

To solve these problems, researchers have proposed to use hash-ing techniques for efficient approximate nearest neighbor (ANN) search [1, 5, 7, 19, 30, 38, 39, 41]. The goal of hashing is to learn binary-code representation for data which can preserve the neigh-borhood (similarity) structure in the original feature space. More specifically, each data point will be encoded as a compact binary string in the hashcode space, and similar points in the original fea-ture space should be mapped to close points in the hashcode space. By using hashing codes, we can achieve constant or sub-linear search time complexity [32]. Moreover, the storage needed to store the bi-nary codes will be dramatically reduced. For example, if each point is represented by a vector of 1024 bytes in the original space, a data set of 1 million points will cost 1GB memory. On the con-trary, if we hash each point into a vector of 128 bits, the memory needed to store the data set of 1 million points will be reduced to 16MB. Therefore, hashing provides a very effective way to achieve fast query speed with low storage cost, which makes it a popular candidate for efficient ANN search in massive data sets [1].
To avoid the NP-hard solution which directly computes the best binary codes for a given data set [36], most existing hashing meth-ods adopt a learning strategy containing two stages: projection stage and quantization stage. In the projection stage, several projected dimensions of real values are generated. In the quantization stage, the real values generated from the projection stage are quantized into binary codes by thresholding. For example, the widely used single-bit quantization (SBQ) strategy adopts one single bit to quan-tize each projected dimension. More specifically, given a point from the original space, each projected dimension i will be asso-ciated with a real-valued projection function f i ( x ) . The i th hash bit of x will be 1 if f i ( x )  X   X  . Otherwise, it will be 0. Here,  X  is a threshold, which is typically set to 0 if the data have been nor-malized to have zero mean. Although a lot of projection methods have been proposed for hashing, there exist only two quantization methods. One is the SBQ method stated above, and the other is the hierarchical quantization (HQ) method in anchor graph hashing (AGH) [18]. Rather than using one bit, HQ divides each dimension into four regions with three thresholds and uses two bits to encode each region. Hence, HQ will associate each projected dimension with two bits. Figure 1 (a) and Figure 1 (b) illustrate the results of SBQ and HQ for one projected dimension, respectively. Till now, only one hashing method, AGH in [18], adopts HQ for quantiza-tion. All the other hashing methods adopt SBQ for quantization.
Currently, almost all hashing methods adopt Hamming distance to measure the similarity (neighborhood) between points in the hashcode space. The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different 1 . As will be stated below in Section 3.1, nei-ther SBQ nor HQ can effectively preserve the neighborhood struc-ture under the constraint of Hamming distance. Hence, although the projection functions in the projection stage can preserve the neighborhood structure, the whole hashing procedure will still de-stroy the neighborhood structure in the original feature space due to the limitation of Hamming distance. This will violate the goal of hashing and consequently satisfactory performance cannot be eas-ily achieved by traditional Hamming distance based hashing meth-ods.

In this paper, we propose to use Manhattan distance for hashing to solve the problem of existing hashing methods. The result is our novel hashing method called Manhattan hashing (MH). The main contributions of this paper are briefly outlined as follows:
The rest of this paper is organized as follows. In Section 2, we introduce the related work of our method. Section 3 describes the details of our MH method. Experimental results are presented in Section 4. Finally, we conclude the whole paper in Section 5.
Due to the promising performance in terms of either speed or storage, hashing has been widely used for efficient ANN search in a large variety of applications with massive data sets, such as http://en.wikipedia.org/wiki/Hamming_ distance text retrieval [33, 39], image retrieval [6, 20], audio retrieval [2], and near-duplicate video retrieval [29]. As a consequence, many hashing methods have been proposed by researchers. In general, the existing methods can be roughly divided into two main classes [6, 39]: data-independent methods and data-dependent methods 2
The representative data-independent methods include locality-sensitive hashing (LSH) [1, 5] and its extensions [3, 16, 17, 21, 24]. The hash functions of these methods are just some simple ran-dom projections which are independent of the training data. Shift invariant kernel hashing (SIKH) [24] adopts projection functions which are similar to those of LSH, but SIKH applies a shifted co-sine function to generate hash values. Many applications, such as image retrieval [24] and cross-language information retrieval [38], have adopted these data-independent hashing methods for ANN. Generally, data-independent methods need longer codes than data-dependent methods to achieve satisfactory performance [6]. Longer codes means higher storage and computational cost. Hence, the data-independent methods are less efficient than the data-dependent methods.

Recently, data-dependent methods, which try to learn the hash functions from the training data, have attracted more and more at-tentions by researchers. Semantic hashing [25, 26] adopts a deep generative model based on restricted Boltzmann machine (RBM) [9] to learn the hash functions. Experiments on text retrieval demon-strate that semantic hashing can achieve better performance than the original TF-IDF representation [27] and LSH. AdaBoost [4] is adopted by [2] to learn hash functions from weakly labeled pos-itive samples. The resulting hashing method achieves better per-formance than LSH for audio retrieval. Spectral hashing (SH) [36] uses spectral graph partitioning strategy for hash function learn-ing where the graph is constructed based on the similarity between data points. To learn the hash functions, binary reconstruction em-bedding (BRE) [15] explicitly minimizes the reconstruction error between the distances in the original feature space and the Ham-ming distances of the corresponding binary codes. Semi-supervised hashing (SSH) [34, 35] exploits both labeled data and unlabeled data for hash function learning. Self-taught hashing [39] uses some self-labeled data to facilitate the supervised hash function learn-ing. Complementary hashing [37] exploits multiple complementary hash tables learned sequentially in a boosting manner to effectively balance the precision and recall. Composite hashing [38] combines multiple information sources into the hash function learning proce-dure. Minimal loss hashing (MLH) [22] tries to formulate the hash-ing problem as a structured prediction problem based on the latent structural SVM framework. SPICA [8] tries to find independent projections by jointly optimizing both accuracy and time. Hyper-graph hashing [42] extends SH to hypergraph to model the high-order relationships between social images. Active hashing [40] is proposed to actively select the most informative labels for hash function learning. Iterative quantization (ITQ) [6] tries to learn an orthogonal rotation matrix to refine the initial projection matrix learned by principal component analysis (PCA) [13]. Experimen-tal results show that ITQ can achieve better performance than most state-of-the-art methods.

Few of the existing methods discussed above have studied the effect of quantization. Because existing quantization strategies can not effectively preserve the neighborhood structure under the con-straint of Hamming distance, most existing hashing methods still can not achieve satisfactory performance even though a large num-ber of sophisticated projection functions have been designed by
In [39], data-independent is called data-oblivious while data-dependent is called data-aware. It is obvious that they have the same meaning. (a) (b) (c) Figure 1: Different quantization methods: (a) single-bit quan-tization (SBQ); (b) hierarchical quantization (HQ); (c) 2-bit Manhattan quantization (2-MQ); (d) 3-bit Manhattan quanti-zation (3-MQ). researchers. The work in this paper tries to study these important factors which have been ignored by existing works.
This section describes the details of our Manhattan hashing (MH) method. First, we will introduce the motivation of MH. Then, the Manhattan distance driven quantization strategy will be proposed. After that, the whole learning procedure for MH will be summa-rized. Finally, we will do some qualitative analysis about the per-formance of MH.
Given a point x from the original feature space R d , hashing tries to encode it with a binary string of c bits via the mapping h : R d  X  X  0 , 1 } c . As said in Section 1, most hashing methods adopt a two-stage strategy to learn h because directly learning h is an NP-hard problem. Let X  X  first take SBQ based hashing as an example for illustration. In the projection stage, c real-valued projection func-tions { f k ( x ) } c k =1 are learned and each function can generate one real value. Hence, we have c projected dimensions each of which corresponds to one projection function. In the quantization stage, the real-values are quantized into a binary string by thresholding. More specifically, h k ( x )=1 if f k ( x )  X   X  . Otherwise, h Here, we assume h ( x )=[ h 1 ( x ) ,h 2 ( x ) ,  X  X  X  ,h c a threshold which is typically set to 0 if the data have been nor-malized to have zero mean. Figure 1 (a) illustrates the result of SBQ for one projected dimension. Currently, most hashing meth-ods adopt SBQ for the quantization stage. Hence, the difference between these methods lies in the different projection functions.
Till now, only two quantization methods have been proposed for hashing. One is SBQ just discussed above, and the other is HQ which is adopted by only one hashing method AGH [18]. Rather than using one bit, HQ divides each projected dimension into four regions with three thresholds and uses two bits to encode each re-gion. Hence, to get a c -bit code, HQ based hashing need only c/ projection functions. Figure 1 (b) illustrates the result of HQ for one projected dimension.

To achieve satisfactory performance for ANN, one important re-quirement of hashing is to preserve the neighborhood structure in the original space. More specifically, close points in the original space R d should be mapped to similar binary codes in the code space { 0 , 1 } c .

We can easily find that with Hamming distance, both SBQ and Figure 2: Hamming distance and Decimal distance between 2-bit codes. The distance between two points (i.e., nodes in the graph) is the length of the shortest path between them. HQ will destroy the neighborhood structure in the data. As illus-trated in Figure 1 (a), point  X  X  X  and point  X  X  X  will be quantized into 0 and 1 respectively although they are very close to each other in the real-valued space. On the contrary, point  X  X  X  and point  X  X  X  will be quantized into the same code 1 although they are far away from each other. Hence, in the code space of this dimension, the Ham-ming distance between  X  X  X  and  X  X  X  is smaller than that between  X  X  X  and  X  X  X , which obviously indicates that SBQ can destroy the neighborhood structure in the original space.

HQ can also destroy the neighborhood structure of data. Let d ( x, y ) denote the Hamming distance between binary codes x and y . From Figure 1 (b), we can get d h ( A, F )= d h ( d ( C, D )= d h ( D,F )=1 , and d h ( A, D )= d h ( C, F )=2 Hence, we can find that the Hamming distance between the two farthest points  X  X  X  and  X  X  X  is the same as that between two rela-tively close points such as  X  X  X  and  X  X  X . The even worse case is that d ( A, F ) &lt;d h ( A, D ) , which is obviously very unreasonable.
The problem of HQ is inevitable under the constraint of Ham-ming distance. Figure 2 (a) shows the Hamming distance between different 2-bit codes, where the distance between two points (i.e., nodes in the graph) is equivalent to the length of the shortest path between them. We can see that the largest Hamming distance be-tween 2-bit codes is 2. However, to keep the relative distances be-tween 4 different points (or regions), the largest distance between two different 2-bit codes should be at least 3. Hence, no matter how we permute the 2-bit codes for the four regions in Figure 1 (b), we cannot get any neighborhood-preserving result under the constraint of Hamming distance. One choice to overcome this problem of HQ is to design a new distance measurement.
As stated above, the problem that HQ cannot preserve the neigh-borhood structure in the data is essentially from the Hamming dis-tance. Here, we will show that Manhattan distance with natural bi-nary code (NBC) can solve the problem of HQ.

The Manhattan distance between two points is the sum of the differences on their dimensions. Let x =[ x 1 ,x 2 ,  X  X  X  ,x [ y ,y 2 ,  X  X  X  ,y d ] T , the Manhattan distance between x and fined as follows: where | x | denotes the absolute value of x .

To adapt Manhattan distance for hashing, we adopt a q -bit quan-tization scheme. More specifically, after we have learned the real-valued projection functions, we divide each projected dimension into 2 q regions and then use q bits of natural binary code (NBC) to encode the index of each region. For example, if q =2 , each pro-jected dimension is divided into 4 regions, and the indices of these regions are { 0 , 1 , 2 , 3 } , the NBC codes of which are If q =3 , the indices of regions are { 0 , 1 , 2 , 3 , 4 , NBC codes are { 000 , 001 , 010 , 011 , 100 , 101 , 110 , shows the quantization result with q =2 and Figure 1 (d) shows the quantization result with q =3 . Because this quantization scheme is driven by Manhattan distance, we call it Manhattan quantiza-tion (MQ). The MQ with q bits is denoted as q -MQ.

Another issue for MQ is about threshold learning. Badly learned thresholds will deteriorate the quantization performance. To achieve the neighborhood-preserving goal, we need to make the points in each region as similar as possible. In this paper, we use k-means clustering algorithm [14] to learn the thresholds from the training data. More specifically, if we need to quantize each projected di-mension into q bits, we use k-means to cluster the real values of each projected dimension into 2 q clusters, and the midpoint of the line joining neighboring cluster centers will be used as thresholds.
In our MH, we use the decimal distance rather than the Ham-ming distance to measure the distances between the q -bit codes for each projected dimension. The decimal distance is defined to be the difference between the decimal values of the correspond-ing NBC codes. For example, let d d ( x , y ) denote the decimal dis-tance between x and y , then d d (10 , 00) = | 2  X  0 | =2 d (010 , 110) = | 2  X  6 | =4 . Figure 2 (b) shows the decimal dis-tances between different 2-bit codes, where the distance between two points (i.e., nodes in the graph) is equivalent to the length of the shortest path between them. We can see that the largest decimal distance between 2-bit codes is 3, which is enough to effectively preserve the relative distances between 4 different points (or re-gions). Figure 1 (c) shows one of the encoding results which can preserve the relative distances between the regions. Figure 1 (d) is the results with q =3 . It is obvious that the relative distances be-tween the regions are also preserved. In fact, it is not hard to prove that this nice property will be satisfied for any positive integer q . Hence, our MQ strategy with q  X  2 provides a more effective way to preserve the neighborhood structure than SBQ and HQ.
Given two binary codes x and y generated by MH, the Manhat-tan distance between them is computed from (1), where x i correspond to the i th projected dimension which should contain q bits. Furthermore, the difference between two q -bit codes of each dimension should be measured with decimal distance. For example, if q =2 , d m (000100 , 110000) = d d (00 , 11) + d d (01 , 00) + d d If q =3 , d m (000100 , 110000) = d d (000 , 110) + d d (100 , 000) It is easy to see that when q =1 , the results computed with Manhattan distance are equivalent to those with Hamming distance, and consequently our MH method degenerates to the traditional SBQ-based hashing methods.
Given a training set, the whole learning procedure of MH, in-cluding both projection and quantization stages, can be summarized as follows: One important property of our MH learning procedure is that MH can choose an existing projection method for the projection stage, which means that the novel part of MH is mainly from the quantization stage which has been ignored by most existing hash-ing methods. By combining different projection functions with our MQ strategy, we can get different versions of MH. For example, if PCA is used for projection, we can get  X  X CA-MQ X . If the random projection functions in LSH are used for projection, we can get  X  X SH-MQ X . Both PCA-MQ and LSH-MQ can be seen as variants of MH. Similarly, we can design other versions of MH.
Because the MQ for MH can better preserve the neighborhood structure between points, it is expected that with the same projec-tion functions, MH will generally outperform SBQ or HQ based hashing methods. This will be verified by our experiments in Sec-tion 4.

The total training time contains two parts: one part is for pro-jection, and the other is for quantization. Compared with SBQ, although MQ need extra O ( n ) time for k-means learning, the to-tal time complexity of MH is still the same as that of SBQ based methods because the projection time is at least O ( n ) . Here n is the number of training points. Similarly, we can prove that with the same projection functions, MH has the same time complexity as HQ based methods.

It is not easy to compare the absolute training time between MH and traditional SBQ based methods. Although extra training time is needed for MH to perform k-means learning, the number of projec-tion functions will be decreased to c q while SBQ based methods need c projection functions. Hence, whether MH is faster or not de-pends on the specific projection functions. If the projection stage is very time-consuming, MH might be faster than SBQ based meth-ods. But for other cases, SBQ based methods can be faster than MH. The absolute training time of HQ based methods is about the same as that of MH with q =2 because HQ also need to learn the thresholds for quantization. For q  X  3 , whether MH is faster than HQ base methods or not depends on the specific projection functions because MH need fewer projection functions but larger number of thresholds.

As for query procedure, the speed of computing hashcode for query in MH is expected to be faster than SBQ based methods be-cause the number of projection operations for MH with q  X  2 only c q of that for SBQ. The query speed of MH with q =2 the same as that of HQ based methods. When q  X  3 , the speed of computing hashcode of MH will be faster than HQ based methods due to the smaller number of projection operations.
To evaluate the effectiveness of our MH method, we use three publicly available image sets, LabelMe [32] 3 , TinyImage [31] ANN_SIFT1M [12] 5 .

The first data set is 22K LabelMe used in [22, 32]. LabelMe is a web-based tool designed to facilitate image annotation. With the help of this annotation tool, the current LabelMe data set contains as large as 200,790 images which span a wide variety of object categories. Most images in LabelMe contain multiple objects. 22K LabelMe contains 22,019 images sampled from the large LabelMe data set. As in [32], we scale the images to have the size of 32x32 pixels, and represent each image with 512-dimensional GIST de-scriptors [23].

The second data set is 100K TinyImage containing 100,000 im-ages randomly sampled from the original 80 million Tiny Images [31]. TinyImage data set aims to present a visualization of all the nouns in the English language arranged by semantic meaning. A total number of 79,302,017 images were collected by Google X  X  image search engine and other search engines. The original images have the size of 32x32 pixels. As in [31], we represent them with 384-dimensional GIST descriptors [23].

The third data set is ANN_SIFT1M introduced in [10, 11, 12]. It consists of 1,000,000 images each represented as 128-dimensional SIFT descriptors. ANN_SIFT1M contains three vector subsets: sub-set for learning, subset for database, and subset for query. The learning subset is retrieved from Flickr images and the database and query subsets are from the INRIA Holidays images [11]. We con-duct our experiments only on the database subset, which consists of 1,000,000 images each represented as 128-dimensional SIFT de-scriptors.

Figure 3 shows some representative images sampled from La-belMe and TinyImage data sets. Please note that the authors of ANN_SIFT1M provide only the extracted features without any orig-inal images of their data. From Figure 3, it is easy to see that La-belMe and TinyImage have different characteristics. The LabelMe data set contains high-resolution photos, in fact most of which are street view photos. On the contrary, the images in TinyImage data set have low-resolution.
As stated in Section 3.3, MQ can be combined with different projection functions to get different variants of MH. In this pa-per, the most representative methods, ITQ [6], SIKH [24], LSH [1], SH [36], and PCA [6, 13], are chosen to evaluate the effectiveness of our MQ strategy. ITQ, SH, and PCA are data-dependent meth-ods, while SIKH and LSH are data-independent methods. These chosen methods are briefly introduced as follows: http://labelme.csail.mit.edu/ http://groups.csail.mit.edu/vision/ TinyImages/ . http://corpus-texmex.irisa.fr/ .
All the above hashing methods can be used to provide projection functions. By adopting different quantization strategies, we can get different variants of a specific hashing method. Let X  X  take PCA as an example.  X  X CA-SBQ X  denotes the original PCA hashing method with single-bit quantization,  X  X CA-HQ X  denotes the combination of PCA projection with HQ quantization [18], and  X  X CA-MQ X  denotes one variant of MH combining the PCA projection with Manhattan quantization (MQ). Because the threshold optimization techniques for HQ in [18] can not be used for the above five methods, we use the same thresholds as those in MQ. All experiments are conducted on our workstation with Intel(R) Xeon(R) CPU X7560@2.27GHz and 64G memory.
We adopt the scheme widely used in recent papers [6, 24, 36] to evaluate our method. More specifically, we define the ground truth to be the Euclidean neighbors in the original feature space. The av-erage distance to the 50 th nearest neighbors is used as a threshold to find whether a point is a true positive or not. All the experimen-tal results are averaged over 10 random training/test partitions. For each partition, we randomly select 1000 points as queries, and leave the rest as training set to learn the hash functions.

Based on the Euclidean ground truth, we can compute the pre-cision, recall and the mean average precision (mAP) [6, 18] which are defined as follows: where q i  X  Q is a query, n i is the number of points relevant to q the data set, the relevant points are ordered as { x 1 ,x R ik is the set of ranked retrieval results from the top result until you get to point x k .
The mAP values for different methods with different code sizes on 22K LabelMe , 100K TinyImage , and ANN_SIFT1M are shown in Table 1, Table 2, and Table 3, respectively. The value of each entry in the tables is the mAP of a combination of a projection function with a quantization method under a specific code size. The best mAP among SBQ, HQ and MQ under the same setting is shown in bold face. To study the effect of q which is the length of NBC for each projected dimension, we evaluate our MH meth-ods on 22K LabelMe and 100K TinyImage by setting the q to three different values (2, 3, and 4). From Table 1 and Table 2, we find that q =2 (2-MQ) achieves the best performance for most cases. Hence, unless otherwise stated, q =2 is a default setting.
From Table 1, Table 2, and Table 3, it is easy to find that our MQ method achieves the best performance under most settings, which means that our MQ with Manhattan distance does be very effective. Furthermore, we can also find that HQ achieves better performance than SBQ under most settings. Because both HQ and our MQ meth-ods adopt more than one bit to encode each projected dimension, it may imply that using multiple bits to encode each projected dimen-sion can be better than using just one single bit. This phenomenon has also been observed by the authors of AGH [18]. The essential difference between HQ and 2-MQ lies in the difference between Hamming distance and Manhattan distance. Hence, the better per-formance of MQ (compared with HQ) shows that Manhattan dis-tance is a better choice (compared with Hamming distance) to pre-serve the neighborhood (similarity) structure in the data. Figure 4 and Figure 5 show the precision-recall curves on 22K LabelMe and ANN_SIFT1M data sets, respectively. Once again, we can easily find that our MQ based MH variants significantly outperform other state-of-the-art methods under most settings.
Most existing hashing methods focus on the projection stage while ignoring the quantization stage. This work systematically studies the effect of quantization. We find that the quantization stage is at least as important as the projection stage. This work might stimulate other researchers to move their attention from the projection stage to the quantization stage, and finally propose better methods simultaneously taking both stages into consideration.
The existing quantization methods, such as SBQ and HQ, will destroy the neighborhood structure in the original space, which vi-olates the goal of hashing. In this paper, we propose a novel quan-tization strategy called Manhattan quantization (MQ) to effectively preserve the neighborhood structure among data. The MQ based hashing method, call Manhattan hashing (MH), encodes each pro-jected dimension with multiple bits of natural binary code (NBC), based on which the Manhattan distance between points in the hash-code space is calculated for nearest neighbor search. MH can effec-tively preserve the neighborhood structure in the data to achieve the goal of hashing. To the best of our knowledge, this is the first work to adopt Manhattan distance with NBC for hashing. The effective-ness of our MH method is successfully verified by experiments on several large-scale real-world image data sets. This work is supported by the NSFC (No. 61100125), the 863 Program of China (No. 2011AA01A202, No. 2012AA011003), and the Program for Changjiang Scholars and Innovative Research Team in University of China (IRT1158, PCSIRT). [1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for [2] S. Baluja and M. Covell. Learning to hash: forgiving hash [3] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [4] Y. Freund and R. E. Schapire. Experiments with a new [5] A. Gionis, P. Indyk, and R. Motwani. Similarity search in [6] Y. Gong and S. Lazebnik. Iterative quantization: A [7] J. He, W. Liu, and S.-F. Chang. Scalable similarity search [8] J. He, R. Radhakrishnan, S.-F. Chang, and C. Bauer. [9] G. E. Hinton. Training products of experts by minimizing [10] H. Jegou, M. Douze, and C. Schmid. Hamming embedding [11] H. Jegou, M. Douze, and C. Schmid. Improving [12] H. J X gou, M. Douze, and C. Schmid. Product quantization [13] I. Jolliffe. Principal Component Analysis . Springer, 2002. [14] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, [15] B. Kulis and T. Darrell. Learning to hash with binary [16] B. Kulis and K. Grauman. Kernelized locality-sensitive [17] B. Kulis, P. Jain, and K. Grauman. Fast similarity search for [18] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing with [19] W. Liu, J. Wang, Y. Mu, S. Kumar, and S.-F. Chang. Compact [20] Y. Mu, J. Shen, and S. Yan. Weakly-supervised hashing in [21] Y. Mu and S. Yan. Non-metric locality-sensitive hashing. In [22] M. Norouzi and D. J. Fleet. Minimal loss hashing for [23] A. Oliva and A. Torralba. Modeling the shape of the scene: A [24] M. Raginsky and S. Lazebnik. Locality-sensitive binary [25] R. Salakhutdinov and G. Hinton. Semantic Hashing. In [26] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int. J. [27] G. Salton and C. Buckley. Term-weighting approaches in [28] G. Shakhnarovich, T. Darrell, and P. Indyk.
 [29] J. Song, Y. Yang, Z. Huang, H. T. Shen, and R. Hong. [30] B. Stein. Principles of hash-based text retrieval. In [31] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny [32] A. Torralba, R. Fergus, and Y. Weiss. Small codes and large [33] F. Ture, T. Elsayed, and J. J. Lin. No free lunch: brute force [34] J. Wang, O. Kumar, and S.-F. Chang. Semi-supervised [35] J. Wang, S. Kumar, and S.-F. Chang. Sequential projection [36] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In [37] H. Xu, J. Wang, Z. Li, G. Zeng, S. Li, and N. Yu.
 [38] D. Zhang, F. Wang, and L. Si. Composite hashing with [39] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught hashing for [40] Y. Zhen and D.-Y. Yeung. Active hashing and its application [41] Y. Zhen and D.-Y. Yeung. A probabilistic model for [42] Y. Zhuang, Y. Liu, F. Wu, Y. Zhang, and J. Shao. Hypergraph
