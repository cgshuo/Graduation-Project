 In machine learning, one often encounters multiple prediction tasks that are related to each other. Multi-task learning (MTL) offers principled frameworks to benefit from synergy of these related tasks via their joint modeling. MTL has been used in diverse applications -digit recognition [ 1 ], face recognition [ 2 ], landmine detection [ 3 ], disease progression modeling [ 4 ], cancer mortality prediction [ 5 ] are some examples.
 hypothesis space of all the tasks. Typically, it is done via using some commonality on task parameters e.g. the use of a common subspace [ 1 , 6 , 7 ], induction of a One of the major challenges in the MTL framework is to find  X  X elated X  tasks and quantify task-to-task relatedness. Initial works in this area [ 9 , 11 ] assumed all the tasks to be well-related and na  X   X vely combined them for joint learning. However, when tasks are unrelated or even negatively related, such combination may lead to poor performance. Addressing this, later works estimate some form of task relatedness and combine the tasks accordingly. For example, Jacob et al. propose a model [ 12 ] that clusters similar tasks into groups and joint modeling is achieved by maximizing the pairwise-similarity between task parameters of all tasks in a group. A similar model via alternating structure optimization is proposed by Zhou et al. in [ 13 ]. Taking a subspace learning approach, Argyriou et al. [ 6 ] develop a model that combines the knowledge from tasks sharing the same basis vectors. To encourage sparsity in the subspace representation, Kumar et al. [ 7 ] propose a model that alleviates noisy task relations. When there are many outlier tasks or unrelated tasks, the performance of these methods suffers as they include all the tasks in joint modeling without any grouping. To overcome this problem, Kang et al. [ 1 ] extend the model in [ 6 ] adding the flexibility to learn multiple task groups and then confining the joint modeling within a group. More advanced models along these lines using nonparametric Bayesian frameworks are done in [ 14 , 15 ]. Although all these models are able to separate unrelated tasks from joint modeling, they are unable to exploit negatively related tasks. This problem is addressed by Zhang et al. [ 16 ], who propose a model that uses a task covariance matrix to learn all types of task relationships. A common problem of all the aforementioned techniques is that they assume same relationship between two tasks along all the features. This assumption is seldom true in reality and causes a problem when task-to-task relationship varies from feature-to-feature. Consider an example of predicting user ratings of desktop computers for a set of users based on features such as CPU speed, RAM size, screen size, price etc. Learning the rating function for each user can be considered as a task. Users may be related as they may have similar preferences over certain features e.g. most might like lower prices. However, users may differ on other features e.g. some prefer higher screen size, whilst others might prefer high performance (faster CPU and larger RAM). Consider two users, say A and B, where both give similar importance to price but user A may give more importance to screen size whilst user B give more importance to CPU speed. Conventional MTL algo-rithms would compute a single relatedness score based on all the features, which assumes both users to have similar level of agreement on both screen size and CPU speed. Clearly, this is not the case. Moreover, due to using a single relat-edness score, which is averaged considering all the features, their agreement on price is underestimated. Therefore, a multi-task learning method that learns feature-specific task relationships is required. Learning task-to-task relationships for every single feature may be unnecessary as tasks may have similar relation-ships along many semantically related features. For example, high CPU speed and large RAM are associated to high performance. Therefore, we hypothesize that it may be sufficient to learn a task relationship for each semantically related feature group.
 We propose a new multi-task learning model that (a) extracts groups of related features, (b) computes task relatedness based on each feature group and (c) uses these relationships for joint modeling of tasks. To extract the groups of related features we learn a low-dimensional subspace from the set of task parameters. Each subspace basis captures the set of semantically related fea-tures. Next, we compute a separate task relationship along each subspace basis. To capture all form of task relationships (low to high, positive and negative) we use a covariance matrix that is computed from the projection of the task param-eters on each basis. Joint modeling of tasks is achieved via an optimization formulation that combines the standard least-squares loss with an appropriate regularization term involving the task covariance matrices. We derive an efficient iterative solution to this optimization problem. Due to the use of multiple rela-tionships, our model is called Multi-Relational Multi-Task Learning (MR-MTL). We illustrate the behavior of our MR-MTL model using a synthetic dataset in scenarios where tasks relationships vary based on different feature groups. We evaluate the effectiveness of MR-MTL on two regression and two classification real-world datasets and demonstrate its superiority over other state-of-the-art multi-task learning methods.
  X  Proposal of a new multi-task learning model, capable of learning different  X  Formulation of the model as an optimization problem, providing an efficient  X  Illustration of the behavior of the proposed model using a synthetic dataset  X  Evaluation of the proposed MR-MTL model on four real datasets validating The significance of our approach is that it is capable of exploiting knowledge across tasks from multiple heterogeneous sources that might differ in their fea-tures. For example, in a multi-hospital scenario, patient records extracted from different hospitals may contain hospital-specific features apart from the usual phenotypical features. Hospital-specific features may include additional infor-mation such as genomic data, which may not be widespread across all hospitals, or differential features may result from different interventions practices across hospitals. Our model can separate the hospital-specific features from the phe-notypical features, and confine the joint modeling only along the common phe-notypical features. This capability offers accurate modeling of real data and is absent in conventional models. We propose a new multi-task learning model that can capture finer relationships between tasks by modeling feature-specific task relatedness. Let us assume we have T 0 learning tasks, indexed as t =1 ,...,T 0 . For the t -th task, the training set is denoted as { ( x ti ,y ti ) | i =1 ,...,N t } where x feature vector and y ti is the target, usually real-valued for regression and binary-valued for binary classification problems. Let  X  t denote the weight vector for the task t , we also refer to this as task parameter . Collectively, we denote the data of t -th task by X t =( x t 1 ,..., x tN t ) T and y t =( y parameters as  X  =(  X  1 ,..., X  T 0 ). When tasks differ in some of the features, a common feature list can be obtained via their union. 2.1 Formulation Since our goal is to develop a MTL model that allows multiple task relationships (one for each correlated feature subset) between any two tasks, we simultane-ously learn several correlated feature subsets and use a task covariance matrix to capture task relationships with respect to each feature subset. In learning these correlated feature subsets, our idea is that relatedness of tasks along the fea-tures of a subset are similar. These feature subsets can be thought of the latent semantic bases of a low dimensional subspace. We represent this low dimensional subspace using a matrix U where each column is a basis vector of the subspace. The task parameter  X  t is represented in this subspace using  X  lectively, we denote these representations as  X  =(  X  1 ,..., X  this matrix is denoted as  X  ( k ) . We unify the subspace learning with the regular-ized multi-task learning to construct a model that allows joint modeling between tasks at a finer level using multiple task relatedness instead of a single aggre-gated relatedness. The proposed model is learnt by minimizing the following cost function min s.t.  X  k 0 , tr(  X  k )=1  X  k, where the multi-task learning is achieved due to the last two terms that reg-ularize the least-square loss using parameters  X  1 , X  2 ,and  X  covariance matrix specific to k -th feature subset. We refer to this model as Multi-Relational Multi-Task Learning (MR-MTL) . 2.2 Optimization The optimization of the cost function in Eq ( 1 ) involves minimization with respect to U ,  X  1: K and  X  1: T 0 .Given U , the cost function is jointly convex in  X  and  X  1: T 0 , and separable for each k . Similarly, given  X  1: tion is convex in U and the optimal solution has a closed form expression. This property of the cost function suggests an iterative algorithm for optimization. Optimizing U given  X  ,  X  1: K : For a fixed  X  ,  X  1: K , the cost in ( 1 ) becomes a regularized least square function in U and has a closed form solution. The optimal solution can be obtained by equating the gradient of Eq ( 1 )tozeroas below To solve the above equation, we apply  X  X ec X  operator. This operator when applied to a matrix concatenates all the columns one-by-one below the previous columns to form a long vector. Applying  X  X ec X  operator, the above linear equation in U can be written as which can be simplified to obtain the following linear equation for U where we use the following property of vec operator: vec ( AXB )=
B T  X  A vec ( A ). The above equation can be solved using LU or QR factoriza-tions, which are more efficient and offer better numerical stability than a matrix inverse based solution.
 Optimizing  X  given U,  X  1: K : Given U ,  X  1: K , the optimization problem in ( 1 ) becomes where the first term involves the columns of matrix  X  and the second term involves the rows of matrix  X  . Although at first instance, it seems like a difficult problem to solve, we can optimize the above cost function in terms of rows of  X  , i.e.  X  ( k ) and obtain a closed form solution. For this, we take its derivative w.r.t.  X  k ) and set it to zero to obtain the following relation where we define C t U T X T t X t U and d t U T X T t y t that C t ( k ) and d t k denotes k -th row of matrix C t and k -th element of vector d respectively. The above equation can be further simplified to a system of linear equations as below
C ,..., C T 0 kk . We note that Eq ( 3 ) can be efficiently solved using Cholesky decomposition.
 Optimizing  X  1: K given  X  ,U: Given  X  , U , the optimization problem in ( 1 ) becomes which can be independently optimized for each  X  k . To get the solution of above problem, define S k =  X  ( k )  X  T ( k ) and consider  X  In the above we have used tr (  X  k ) = 1. Further defining A =  X  and noting the positive semi-definite property of these matrices, we can apply a Cauchy-Schwarz Inequality on the inner product of trace, i.e. tr A (tr ( AB )) 2 to get In the above expression, an  X  k that leads to the equality , corresponds to the optimal solution of ( 4 ). The equality is satisfied when  X  scalar. Since the optimal  X  k has to satisfy the constraint tr (  X   X  =tr S procedure for MR-MTL.
 Computational Complexity: The order of complexity for updating  X  ( 3 )is O T 3 0 . Similarly, the order of complexity to update U is O M Finally the order of complexity to update  X  k for each k is O T fore, overall complexity for the proposed MR-MTL per iteration is of the order O M We present our experimental results on synthetic and real datasets. Synthetic data is used to create a niche scenario where the proposed MR-MTL model is expected to work better than other models. To evaluate the effectiveness of our model for real world applications, we use two classification and two regression Algorithm 1. The proposed MR-MTL datasets and compare MR-MTL with single-task learning ( STL ) and three state-of-the-art multi-task learning baselines: MTFL [ 6 ], GMTL [ 1 ]and MTRL [ 16 ]. All these models are based on optimization frameworks. Similar to the pro-posed MR-MTL, the first two baselines (MTFL and GMTL) learn a low dimen-sional subspace for task parameters. In the optimization, MTFL uses a L mixed norm penalty term for joint modeling of all tasks while GMTL first learns groups of related tasks and uses a L 2 /L 1 mixed norm penalty for each task group. MTRL, on the other hand, uses a covariance matrix for learning task relationship allowing to exploit the knowledge from negatively related tasks. All these models use regularization parameters, which are learnt using a grid search use the following metrics: Explained variance ( R 2 )and root-mean-square-error (RMSE) for regression; area under ROC curve (AUC) and F1-measure for clas-sification.
 3.1 Experiments with Synthetic Data Our synthetic data is generated by creating 30 related tasks where each task is to learn a linear regression model in a 9-dimensional feature space given 15 super-vised training instances. We create three groups of tasks: group-1 (tasks 1-10), group-2 (tasks 11-20) and group-3 (tasks 21-30). We ensure that tasks in each group have the same parameters and are thus strongly correlated. Given these tasks, our idea is to create multiple relationships across task groups by using feature-dependent task relationships in various forms: positive relationship, neg-ative relationship and no relationship. Figure 1 (a) depicts the simulated task parameters (i.e.  X  ) for all the tasks along 9 features. Along the first three fea-tures, task group-2 and task group-3 are positively related but both are unrelated to task group-1. Similarly, along the next three features, task group-1 and task group-3 are positively related but both are unrelated to task group-2. Finally, along the last three features, task group-1 and task group-2 are negatively related but both are unrelated to task group-3. Given these task parameters, feature vectors are randomly drawn from a 9-dimensional multi-variate Gaussian dis-tribution as x ti  X  X  ( 0 , I ). The corresponding target y y training and the remainder for test. We run our proposed MR-MTL algorithm and compare its performance to one of the related baseline, MTRL for illustration purposes. Figure 1 (b) and (d) show the task parameters estimated by MTRL and the proposed MR-MTL respectively. Clearly the task parameter estimates of MR-MTL are much closer to the true task parameters (Figure 1 (a)). The better estimates by MR-MTL can be explained by looking at the task relationships learnt by both methods, which are shown in Figure 1 (c) for MTRL and Figure (g)-(i) for MR-MTL using Hinton plots. The task relationship learnt by MTRL is averaged across all 9 features, causing overestimation of the unrelatedness while underestimation of the strong relatedness. In contrast, the proposed MR-MTL accurately estiamtes task relationships by using three separate feature groups (one feature group represented by each basis of the subspace as we use K = 3) and thus learning one task relationship matrix for each feature group. This added flexibility allows MR-MTL to have a finer and differential level of control in joint modeling of tasks along different features. We use the held out test set to evaluate the performance of MR-MTL and compare it with MTRL in Table 1 . The reported results are averaged over 40 randomly generated datasets along with corresponding standard errors. As seen from the Table, MR-MTL clearly outperforms both STL and MTRL with respect to two evaluation metrics -Explained variance ( R 2 ) and root mean square error (RMSE). Due to presence of different task relationships in data, MTRL is unable to estimate the task relationships and thus performs worse than STL. The performance variations of MR-MTL with respect to subspace dimension ( K ) is shown in Figure 1 (e), wherein the best performance is achieved at K = 3, however, the performance degrades very slowly with increasing values of K . An example of the convergence behavior of proposed MR-MTL is shown in Figure 1 (f) -the algorithm quickly converges within 50 iterations. 3.2 Experiments with Real Data We use the following classification and regression datasets. Landmine Data (Classification): This dataset is created from radar images collected from 19 landmine fields. This is a benchmark dataset and used widely for multi-task learning. Each data instance is a 9-dimensional representation of each image formed by concatenating different image based features. The task is to detect images with landmines. Treating each landmine field as a task, we jointly model them via multi-task learning. For each task we randomly split the data in two parts: 30% instances for training and the remainder for testing. The results are averaged over 40 training-test splits.
 Acute Myocardial Infarction (AMI) Data (Classification): This dataset is col-lected from a hospital in Australia (Ethics approval #12/83). It contains records of patients who visited the hospital during 2007-2011 with AMI as the primary reason for admission. The cohort is first divided into two main AMI types: STEMI and Non-STEMI, each of which is further divided into 4 subcohorts based on the major interventions administered (coronary artery bypass surgery, coronary artery stenting, other intervention or no intervention at all), resulting in a total of 8 subcohorts. The task is to predict readmission within the first 30-days of discharge due to any heart related medical emergency. Out of the original 8 subcohorts only 5 are chosen as they have at least 2 positive examples per year. In the selected subcohorts, total number of patients varied from 50-182 per year. The features used are patients demography (gender, age, occupation) and health status in terms of Elixhauser comorbidities [ 17 ], aggregated over 3 time scales: 1 month, 3 months and 1 year prior to their AMI admission. Evalu-ation is performed progressively with patients from 2009, 2010 and 2011 for test whilst using all past patients data before the test year for training. Computer Survey Data (Regression): This dataset [ 6 ] contains ratings of 20 computers by 190 students based on 13 binary features (cf. Figure 2 ). Each rating value lies between 0-10 indicating likelihood of buying a computer. We treat ratings by each student as a task, thus having a total of 190 tasks. As these tasks are related, we jointly model them under the setting of multi-task learning. Following [ 15 ], we use the first 15 computer ratings for training and test using ratings of the last 5 computers.
 SARCOS Data (Regression): The data relates to an inverse dynamics problem for a seven degrees-of-freedom SARCOS anthropomorphic robot arm. The task is to map from a 21-dimensional input space (7 joint positions, 7 joint velocities, 7 joint accelerations) to the corresponding 7 joint torques, giving rise to 7 mapping tasks. For this dataset 100 random examples are sampled for training and another 400 are sampled randomly for test. This is to demonstrate the efficacy of multi-task learning algorithm for small data. We average the performance over 40 random training-test datasets.
 Experimental Results. Table 2 presents a comparison of our proposed MR-MTL algorithm with STL, and other baseline MTL algorithms on Landmine dataset in terms of both prediction AUC and F1. Predictive performance of MR-MTL for different numbers of feature subsets (K=1, 2, and 3) are also reported. Clearly, MR-MTL with K=2 (AUC 0.775, F1 0.880) outperforms all other methods by a good margin. The closest performer is MTRL (AUC 0.760, F1 0.872), whilst other methods are further lower. The landmine dataset con-tains tasks which can be broadly divided into two groups based on whether a task is a landmine detection problem at a foliated region or in a desert region. Interestingly, MR-MTL also found K=2 to be the best for this dataset. Predictive performance at three different training-test scenarios are presented. For all those settings K=2 is found to give the best performance for MR-MTL. For the test year 2009, MR-MTL closely follows MTRL in terms of AUC and GMTL in terms of F1. For the two other test years, MR-MTL convincingly outperforms all other methods in terms of both AUC and F1. For both the scenarios, the AUC is above 0.6 and F1 is above 0.75, whilst the same for other methods are much lower. There is also gradual improvement of performance by MR-MTL as more and more training data is available when tested on later years, whilst all other methods behaved erratically.
 SARCOS datasets. For computer dataset, MR-MTL with K=3 performs (RMSE 1.664, R 2 0.318) the best followed by MTRL (RMSE 1.766, R baselines have higher RMSE values. To illustrate the behavior of MR-MTL fur-ther, we present the basis vectors corresponding to K=3 in Fig 2 (a). The three basis vectors captures 3 different grouping of features. The first basis (U1) cap-tures positive preference for high performance (CPU speed, RAM size) along with positive preference for having CD-ROM. The second basis (U2) captures positive preference for CD-ROM, whilst non-preference for higher CPU speed with larger cache. The third basis (U3) captures price of the unit as a major factor. Fig 2 (b) shows the histogram of task relatedness along different basis. It is interesting to note that task-relatedness along U3, whose major factor is price shows higher prevalence of positive relatedness (the histogram for U3 is skewed on the positive side), which implies that many raters give importance to price similarly. This is intuitive since price is always a major factor in consumer spending. We see that histogram on U1 have high peak around zero, imply-ing that preference for high performance and CD-ROM is more independent in nature. Conversely, highest disagreement among the raters is observed along U2. For SARCOS dataset, MR-MTL with K=7 performs (RMSE 3.198, R the best, followed by GMTL (RMSE 3.349, R 2 0.821). Other baseline methods have considerable higher RMSE and lower R 2 values. For this dataset, the tasks are low-related, therefore, other MTL methods which tries to regularize strongly performed lower, whereas, MR-MTL with K=7 is able to offer the right balance between the flexibility and regularization leading to better performance. We have presented a novel multi-task learning framework that allows joint mod-eling of tasks based on multiple relationship between them, where each relation is independently defined on a set of semantically related features. This helps in modeling scenarios where task-to-task relationships differ based on feature sets or where tasks have slightly different features sets. To model multiple task relat-edness, we learn several feature subsets using a low dimensional subspace and use a task covariance matrix to capture task relationships (both positive and negative) along each feature subset. We formulate the model as an optimization problem and derive an efficient solution. Using both synthetic and real datasets, we demonstrate that the performance of proposed model is better than several state-of-the-art multi-task learning algorithms.

