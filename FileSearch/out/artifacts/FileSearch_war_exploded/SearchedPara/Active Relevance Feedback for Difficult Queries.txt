 Relevance feedback has been demonstrated to be an effec-tive strategy for improving retrieval accuracy. The exist-ing relevance feedback algorithms based on language models and vector space models are not effective in learning from negative feedback documents, which are abundant if the initial query is difficult. The probabilistic retrieval model has the advantage of being able to naturally improve the estimation of both the relevant and non-relevant models. The Dirichlet compound multinomial (DCM) distribution, which relies on hierarchical Bayesian modeling techniques, is a more appropriate generative model for the probabilis-tic retrieval model than the traditional multinomial distri-bution. We propose a new relevance feedback algorithm, based on a mixture model of the DCM distribution, to ef-fectively model the overlaps between the positive and nega-tive feedback documents. Consequently, the new algorithm improves the retrieval performance substantially for difficult queries. To further reduce human relevance evaluation, we propose a new active learning algorithm in conjunction with the new relevance feedback model. The new active learn-ing algorithm implicitly models the diversity, density and relevance of unlabeled data in a transductive experimental design framework. Experimental results on several TREC datasets show that both the relevance feedback and active learning algorithm significantly improve retrieval accuracy. H.3.3 [ Information Search and Retrieval ]: Relevance Feedback Algorithms
Relevance Feedback [14, 8] is an effective way to enhance retrieval performance. In a relevance feedback cycle, the retrieval engine presents the user with a list of retrieved documents, and then reformulates a new query toward the relevant feedback documents based on the user X  X  relevance judgment. Several relevance feedback algorithms based on query expansion have been proposed in the context of dif-ferent retrieval models. Query expansion, which has been demonstrated to be effective in both vector space models [14] and language models [25], reformulates the original query such that it gets closer to the relevant documents. The Rocchio algorithm [14] in the vector space model formulates the query close to the centroid of the relevant documents. Model based relevance feedback algorithms in the language model [25] update the query model with terms relevant to the search topic, and filter out the general English words.
The probabilistic retrieval model based on the probability ranking principle [13], has the advantage of being able to naturally improve the estimation of the probabilistic models by exploiting explicit relevance information. This is because both the positive and negative relevance judgments from a user provide direct training data for estimating both the relevant and non-relevant models. Relevance feedback ap-proaches in language model, however, are not able to effec-tively learn from negative feedback documents. Various rel-evance feedback approaches based on the term re-weighting scheme [12, 4] have shown improvements on small test col-lections. Nevertheless, these relevance feedback models re-weight the same set of index terms as the original query over and over again, and do not expand the original query with new terms. Consequently, the term re-weighting models are not able to capture new information from the feedback doc-uments, and lack the ability to explore new relevant doc-uments. Therefore, we propose a new relevance feedback algorithm that is able to incorporate new terms from the feedback documents.

Simple query expansion in the vector space model or the language model is not able to naturally incorporate informa-tion contained in the negative feedback documents, because query formulation does not allow negative weights in both models. For the Rocchio algorithm, no new terms are added with negative weights and the contribution of negative feed-back is only to reduce the weight of the expanded terms from positive feedback documents [8]. Negative feedback has been only studied very recently in language models by evaluating the Kullback-Leibler (KL) divergence between the negative topic language model and the document model [19]. Both of these models do not take into account the relevance infor-mation captured by the overlapping terms in the negative feedback documents, and treat all the terms except query terms in the negative feedback documents as non-relevant terms or general English terms. Thus, these two models are not able to capture the overlapping terms between the positive and negative feedback documents. Consequently, there is still a large margin for improvement in the situa-tion where the initial retrieval results are not satisfying (few or no positive feedback documents present) when compared with language modeling approaches and vector space model approaches. Probabilistic retrieval models capture the non-relevant information naturally by using a non-relevant model in the retrieval formula. Negative feedback should be treated cautiously, because too much negative feedback may impair retrieval accuracy as relevant terms get negative weightings. Intuitively, both the positive and negative feedback docu-ments are retrieved by the initial query, and they contain some common terms relevant to the user X  X  search intent. Treating these terms as non-relevant will impair the retrieval accuracy. Thus, we propose a new relevance feedback model which uses mixture modeling to capture the overlaps be-tween the positive and negative feedback documents.
How to develop a rigorous and efficient relevance model in the probabilistic retrieval model remains to be an open re-search problem [9]. In this paper, we apply a new probabilis-tic retrieval model based on the Dirichlet compound multi-nomial (DCM) distribution, which is proposed in [21]. The DCM distribution [10, 6] has been shown to be more appro-priate to accommodate word burstiness : the phenomenon that if a word appears once, it is more likely to appear again, and achieves better performance in text classifica-tion and text clustering. The DCM distribution integrates out the parameters of the multinomial distribution, relying upon hierarchical Bayesian modeling. We design an effec-tive relevance feedback algorithm which is able to capture the overlaps between the positive and negative feedback doc-uments by a new mixture modeling technique, based on the new DCM retrieval model. We model the positive feedback documents as a mixture of the feedback relevant and back-ground noise models, and the negative feedback documents as a mixture of the feedback relevant, feedback non-relevant and background noise models. Therefore, the overlapping terms between the positive and negative feedback documents can be appropriately modeled as feedback relevant informa-tion, in addition to the common background information. The mixture model can be solved by the Expectation and Maximization (EM) algorithm. The EM algorithm, how-ever, is likely to converge to an undesired local optimum. To address this problem, we enhance the estimation result by some insightful modifications to the EM steps, including background collection model reduction , deterministic anneal-ing and query regularization . We also propose a hybrid feed-back scheme which combines pseudo-relevance feedback and relevance feedback to improve the queries which do not have any relevant feedback documents. Instead of using fixed reg-ularization parameter, we adjust the regularization parame-ter based on the percentage of relevant feedback documents.
Traditional relevance feedback algorithms use top ranked documents as feedback documents. In general, it is criti-cal to present the right documents to the user for evalua-tion. The problem of actively choosing unlabeled data for human evaluation belongs to the broad class of active learn-ing problems [3, 17]. This problem, however, has not been fully explored in the information retrieval community. Sev-eral effective and efficient heuristics [22, 15, 20] which cap-ture the diversity of feedback documents have been demon-strated to achieve significant improvement over the baseline relevance feedback algorithms which choose the top ranked documents. The cluster centroid algorithm in [15] selects cluster centroids to capture the diversity of feedback docu-ment set. The Active-RDD algorithm in [22] explicitly incor-porates both diversity and density of feedback documents by using a greedy algorithm. The active learning algorithm in [20] selects feedback documents by reducing model variance in the Bayesian logistic regression learning framework, and the variance reduction algorithm implicitly captures the di-versity, relevance and uncertainty of feedback document set. In this paper, we propose a new active learning algorithm for the DCM relevance feedback model. The new active learning algorithm is based on the transductive experimen-tal design algorithm (TED) [24], which efficiently explores the available unlabeled documents. Intuitively, the trans-ductive experimental design algorithm tends to select a set of documents which span much of the retrieved document space and retain most of the information. The transduc-tive experimental design algorithm has also been success-fully applied in generating query substitution in the spon-sored search context[27]. Because relevance feedback faces an extremely unbalanced data problem, where the number of relevant documents is significantly smaller than the num-ber of non-relevant documents, positive feedback documents become more valuable than negative feedback documents. Therefore, we propose a relevance weighted transductive ex-perimental design active learning algorithm which biases the top retrieved documents favorably.

To summarize, the proposed approach improves the accu-racy of information retrieval by incorporating both positive and negative feedback in the following two ways: 1. A formal relevance feedback algorithm based on the 2. A new active learning approach based on relevance The remainder of this paper is organized as following. Sec-tion 2 briefly introduces the Dirichlet compound multino-mial probabilistic retrieval algorithm, while a detailed de-scription is available in [21]; Section 3 presents the new rel-evance feedback algorithm based on the DCM distribution. Section 4 introduces the active learning algorithm; Section 5 describes the design of experiments to evaluate the perfor-mance of the new algorithm; finally, Section 6 discusses the conclusions and future works.
Classical probabilistic retrieval model faces the difficulty of estimating an effective probabilistic distribution for the relevant and non-relevant class. Lack of ability to cap-ture various key factors that influence retrieval effectiveness inspires the use of a new distribution model in the con-text of probabilistic retrieval. The multinomial distribution has been widely applied in language model, and shown to perform well in terms of both effectiveness and efficiency. Applying the multinomial distribution in the probabilistic model, however, results in a score function which does not satisfy the desirable concavity property and is inefficient to implement [21]. The Dirichlet compound multinomial (DCM) distribution [11, 10] is able to capture word bursti-ness, and thus better addresses the need to capture concavity and document length in the score function. A probabilistic retrieval model based on the DCM distribution has been de-veloped in [21], and we will present a brief introduction of the DCM retrieval model below.

Hierarchical Bayesian modeling treats the generation of a document in the following way: A sample is drawn from the Dirichlet distribution to generate a multinomial distribution, and then a document is generated by the multinomial dis-tribution. This hierarchical Bayesian is called the Dirichlet compound multinomial (DCM) distribution [11, 10, 6]. The DCM distribution is based on the Dirichlet distribution and the multinomial distribution, which are defined as below: where p ( d l |  X  R ) is the multinomial distribution with param-eter  X  R ,and p (  X  R |  X  R ) is the Dirichlet distribution with pa-rameter  X  R . The Dirichlet distribution is the prior distri-bution of the multinomial parameters  X  R .  X  m R indicates the probability of generating word w m ; n ( d l ) is the length of doc-ument d l ; c ( w m ,d l ) denotes the term frequency of word w in document d l ; S R = V m =1  X  m R is the sum of parameters  X 
R over vocabulary size V . In the DCM distribution, the actual parameters are the Dirichlet parameters  X  m R , because the multinomial parameters  X  m R are integrated out. Thus, the DCM distribution for the relevant model is defined as: p ( d l |  X  R )= p ( d l |  X  R ) p (  X  R |  X  R ) d X  R (2) = n ( d l )! The first line of the Equation (2) is derived by directly mul-tiplying the Dirichlet distribution with the multinomial dis-tribution. The second line of Equation (2) is derived by Dirichlet distribution with parameters  X  m R + c ( w m ,d
We can derive the Dirichlet compound multinomial dis-tribution for the non-relevant model p ( d l |  X  N ) in the same way. The classical probability ranking principle [13] sug-gests ranking the documents by the log-odds ratio of their probabilities of being generated by the relevant class against the non-relevant class. Combining with the Dirichlet com-pound multinomial distribution shown in Equation (2), we can rank documents with the following score function: The score function shown in Equation (3) consists of two components: the first term depends on all the words oc-curring in documents and the second term depends on the document length n ( d l ).

In the ad hoc information retrieval tasks, little informa-tion regarding the user X  X  retrieval intent is available. How to construct the relevant model and non-relevant model is a challenging problem in implementing the probabilistic re-trieval model without any relevance feedback information. User query and collection distribution are the only informa-tion available to the retrieval system. Intuitively, the initial parameters of the relevant model should capture the infor-mation in the query, and the initial parameters of the non-relevant model should contain information in the document collection. Based on this intuition, we initialize the parame-ters of non-relevant model  X  N by fitting a DCM distribution to the whole the collection, and initialize the parameters of relevant model  X  R as the query model smoothed with the non-relevant model. Where parameter  X  controls the degree of smoothing in the relevant model; c ( w m ,q ) is the term frequency of word w in query q . The parameter  X  can be tuned manually or es-timated automatically. Based on this initialization, we ob-tain the final score function of the DCM retrieval algorithm, which depends only on the co-occurring words in the query and the document.

Score ( d l )= where n ( q ) is the length of query q . The above score func-tion consists of two components: The first term depends on the frequency of word co-occurring in the query and the doc-ument (that is term frequency); the second term depends on the query length and the document length. The first term of the score function increases with the term frequency, and the increase rate is larger for smaller term frequencies be-cause of the concavity of the log function. The second term decreases with document length. This indicates that if two documents contain the same number of query terms, the shorter document is more likely to be relevant, since it con-tains fewer non-relevant terms. These two intuitive insights agree with the term frequency constraint and length nor-malization constraint considered desirable according to the analysis in [7].
Recall that the goal of using the probabilistic retrieval model as our baseline retrieval algorithm is to effectively incorporate negative feedback document information in rel-evance feedback. It is natural to enhance the relevant model with positive feedback documents d P , and the non-relevant model with negative feedback documents d N . Therefore, the relevance feedback algorithm reestimates the relevant model Figure 1: Graphical model representation of the rel-evance topic model parameters  X  m R and non-relevant model parameters  X  m N by incorporating information in the feedback documents. Be-cause both the positive and negative feedback documents are retrieved by the same query, the negative feedback docu-ments contain query terms and terms semantically related to the query. Consequently, simply estimating the non-relevant model bas-ed on negative feedback documents will offset the weight of the query terms and other topic related terms in the relevant model, and lead to poor performance.
To solve the above problem, we define three latent gener-ative models: feedback relevant model z FR , feedback non-relevant model z FN and background noise model z N . z FR comprises terms pertinent to the user X  X  search intent; z FN comprises the off topic terms occurring in the negative feed-back documents; z N represents the general English terms occurring frequently in the collection, and the parameters of z N are  X  m N estimated from the document collection. All these three models follow the DCM distributions. We model the feedback documents in the following way: the positive feedback document d P is modeled as a mixture of z FR and z , and the negative feedback document d N is modeled as a mixture of z FR , z FN and z N . Thus, the estimated feed-back non-relevant model z FN will generally be concentrated on the terms occurring only in the negative feedback doc-uments, but not the terms occurring in both the positive feedback documents and the negative feedback documents. To summarize, the corresponding graphical model represen-tation is depicted in Figure 1.

In order to speed up the computation, we use the ap-proximated DCM distribution, EDCM distribution, as the underlying generative sources [6]. The EDCM distribution can approximate the DCM distribution when the dimension is large. The EDCM distribution is defined as
The standard procedure for maximum likelihood estima-tion in latent variable models is the Expectation Maximiza-tion (EM) algorithm. EM alternates two steps: (i) an expec-tation (E) step where posterior probabilities are computed for the latent variables z, based on the current estimates of the parameters, (ii) an maximization (M) step, where pa-rameters are updated for given posterior probabilities com-puted in the previous E-step.

If we jointly estimate the feedback models from both the positive and the negative feedback documents, the overlap-ping terms between the positive and negative feedback doc-uments may cause the EM algorithm to converge to a lo-cal optimum where the negative feedback documents have a large probability of P ( z FR | d i ). Thus, we separately esti-mate feedback models from positive and negative feedback documents in two stages. In the first stage, we iterate over the positive feedback documents, and learn feedback rele-vant model z FR by fixing background noise model z N .In the second stage, we iterate over the negative feedback docu-ments, and learn feedback non-relevant model z FN by fixing z
FR learned in the first stage and z N . Both stages converge until the likelihood function does not change new  X  old &lt; . We define  X  ( x )= d dx ln  X  ( x ) as the digamma function, and 1 ( x ) as the indicator function. In the E-step, we calculate the expectation P ( z k | d i ,w m ) by Equation (7); in the M-step, we estimate model parameters: P ( z k | d i ), S k and  X  by Equation (8),(9) and (10)
P ( w m | z k )=  X 
The above basic EM steps can be applied to estimate the latent mixture parameters. However, several drawbacks of basic EM algorithm remain to be solved to enhance the model estimation. First, without fixing the mixing coef-ficients P ( z k | d i ), the EM algorithm will converge to the local optimum P ( z FR | d i ) = 1, where the feedback docu-ments are only generated by the feedback relevant model z FR . Second, the original query is not involved in the above EM estimation steps. Third, in the event of all the feed-back documents being non-relevant, direct estimation of the non-relevant feedback model will reduce the retrieval per-formance. To address these problems, we propose several insightful improvements to the basic EM algorithm. These are the critical elements which lead to improvement of algo-rithmic performance.
Traditional language modeling approaches use the collec-tion model p ( w m | C ) as the background noise model, whose parameters are estimated from the whole collection. Since the feedback document set F contains fewer terms than the whole collection, directly applying the collection model re-P ( w m | Z FR ). Consequently, the underestimated background collection model will cause the EM algorithm to converge to P ( z FR | d i ) = 1 based on Equation (7) and Equation (8). Zhai and Lafferty noticed this convergence problem in [25], but they did not explicitly point out the underlying reason. Instead, they solve the problem by setting P ( z FR | d i P ( z N | d i ) to some constant values, in spite of the fact that feedback documents have different levels of noise. More re-cently, Tao and Zhai [16] addressed this problem by using early stopping to avoid converging to P ( z FR | d i )=1. In contrast to their approach, we use the reduced collection estimate , where we only count the words occurring in the feedback documents S Thus, the values of P ( w m | z N ) are in the same range of the values of P ( w m | z FR ). Consequently, we avoid fixing P ( z k | d i ), and the EM algorithm still converges to a desirable local optimum. In addition, updating the mixing coefficients P ( z k | d i ) helps to converge to a local optimum quickly. This is because both the E-step and M-step increase the likeli-hood to converge to a local optimum. Thus, this algorithm is very efficient and requires fewer EM iterations than the simple mixture model. We also apply the reduced feedback relevant model z FR when we estimate z FN in the second stage. The reduced z FR only counts the words occurring in the negative feedback documents to increase their weight.
A deterministic annealing procedure [18] allows the EM algorithm to find better local optimum of the likelihood func-tion. We replace the original expectation step in Equation (7) with where T is a temperature parameter. In each iteration, we decrease T  X   X T until the EM algorithm converges. The parameter  X  is a large value close to one, and we set  X  =0 . 92 in the experiments. This shows that the effect of the entropy at T is to dampen the posterior probabilities such that they will get closer to the uniform distribution with decreasing T . Therefore, the EM estimation steps are less likely to be stuck in local optimum.
The simple mixture model [25] does not involve the orig-inal query in any way during the EM iteration. Instead, it interpolates the estimated feedback model with original query model by using a fixed interpolation coefficient. More-over, if we estimate the non-relevant model from negative feedback documents directly, the query terms would have large probabilities in the negative feedback model because of their frequent occurrence in the negative feedback docu-ments. This could offset the retrieval power of the original query terms, and thus be unable to identify the semanti-cally relevant documents effectively. To address the above two problems, we apply the query regularization approach, which was proposed in [16] for language model based rele-vance feedback. Using query regularization approach, the original M-steps in Equation (9) and (10) become where M ( n ( d i )) =  X  ( S FR + n ( d i ))  X   X  ( S FR )
P ( w m | z FR )=  X  where n ( q ) denotes the length of query and c ( w m ,q )denotes the term frequency of word w m in query q . In our algorithm, we treat query q as a relevant document occurring  X  times with P ( z FR | q ) = 1 as shown in Equation (13) and(14). The query regularization approach, however, has a new implica-tion in the context of negative feedback. The parameter  X  controls the relative weight of the original query we add to the feedback relevant model, and thus the original queries in-volved in the estimation step naturally. Moreover, the query regularization approach also restrains the probabilities of the query terms in the negative feedback estimation in a prin-cipled manner. Since query regularization is only applied in estimating the feedback relevant model, the probability of gen-erating query terms P ( w q | z FR ) is increased in the feedback relevant model z FR . Consequently, the probability of gen-erating query terms is reduced in the feedback non-relevant model P ( w q | z FN ). To address the same problem, Wang et al. [19] proposed to eliminate the query terms from the neg-ative model by setting their probabilities to zero, while we handle this issue in a more elegant way. In addition, for the feedback non-relevant model z FN , the query regularization approach reduces not only the query term weight, but also the weight of topic related terms occurring frequently in the positive feedback documents.
Moreover, we adjust the query regularization parameter according to the feedback documents quality. Instead of us-ing a fixed query regularization parameter  X  in Equation (13) and (14), we apply an adjusted regularization coeffi-cient  X  exp(  X   X   X | N | /K ), where | N | is the number of neg-ative feedback documents, and K is the number of total feedback documents.  X  is the rate parameter which controls the degree of decaying. The exponential function is a mono-tonic convex function decreasing with percentage of negative feedback documents, and larger parameter  X  indicates faster decreasing rate. Intuitively, when the percentage of nega-tive feedback documents is high, the original query terms does not capture the user X  X  semantic search intent, and we should rely less on the original query terms (by decreasing query regularization parameter), and vice versa.
For some difficult queries, none of the feedback documents are relevant. In such a case, we can not estimate the feed-back relevant model z FR in the first stage, and the original query model can be used as feedback relevant model in the second stage. Relying only on the query terms can prevent the query terms from occurring in the feedback non-relevant model z FN , but cannot prevent the terms which are closely related to the query terms (such as synonyms of query terms) from occurring in the feedback non-relevant model. For in-stance, query 84 in the TREC HARD 2003 track is  X  X ecent Earthquakes X , and none of the top 10 retrieved documents are relevant. The term  X  X uake X  occurs frequently in the same retrieved documents as the query term  X  X arthquake X . In par-ticular, the term  X  X uake X  occurs in the negative feedback documents as well, in spite of the fact that the term  X  X uake X  is a synonym of  X  X arthquake X . If we directly model nega-tive feedback, the term  X  X uake X  will appear in the feedback non-relevant model z FN , and thus the relevance feedback ac-curacy will be impaired. To address this problem, we resort to the technique of pseudo-relevance feedback. We randomly sample K documents in the retrieved document set, where K is the number of feedback documents. We treat these sampled documents as positive feedback documents. Thus, the relevant terms that are closely related to the query, will appear in the feedback relevant model z FR . Consequently, the probabilities of these terms are decreased in Z FN ,since they have large weights in z FR and z FR is fixed in the sec-ond stage estimation. We call the above feedback approach hybrid feedback , since it combines relevance feedback and pseudo-relevance feedback.
Based on the basic EM algorithm and the above improve-ments, we summarize the detailed EM steps in Table 1. Af-ter the algorithm converges, we smooth  X  FR with  X  N to obtain the new relevant model, and smooth  X  FN with  X  N to obtain the new non-relevant model. In the above equation, we first scale down the value of the largest  X  m R to the same value of the parameter  X  in Equa-tion (4)used in the baseline DCM retrieval model, and then use the same scaling coefficient  X  max non-relevant model z FN .  X  is the parameter controls the weight of the negative feedback. When  X  =0,wedonot perform negative feedback. The tuning parameter  X  was also proposed in the Rocchio algorithm [14].
In this section, we propose a modified version of the ac-tive learning approach called transductive experimental de-sign [24] to select the most informative feedback documents. The reason we choose the transductive experimental design algorithm is that it explicitly deliberates upon both labeled and unlabeled documents in a batch scheme, which fits the relevance feedback framework. The design objective is to select K feedback documents such that a high confidence of the ranking prediction of the remaining documents is en-sured. The transductive experimental design algorithm is defined by minimizing the predictive covariance. where F represents the feedback document set [ d 1 ,...,d D represents the set of candidate documents [ d 1 ,...,d N ]  X &gt; 0 is the regularization parameter to improve numerical stability since ( FF T +  X I )isfullrank;Tr(  X  ) denotes the trace of the matrix. Yu et al. [24] indicated that  X  X he trans-ductive experimental design also has a clear geometric in-terpretation: it tends to find the representative documents F that span a linear space to retain most of the informa-tion of the candidate documents set D . X  This is consistent with the diversity criterion in [15],[22] and [20], as well as the density criterion in [22]. The transductive experimental design is an NP-hard combinatorial optimization problem. Therefore, Yu et al. [24] have proposed a sequential greedy optimization algorithm and an alternating optimization al-gorithm. Because of the computational efficiency require-ment, we only use the sequential optimization approach in this paper. The sequential optimization algorithm first ini-tializes similarity matrix L = DD T , and then iteratively performs the following two steps until K documents have been selected.

Step 1: Select d i  X  X  with the highest L d i 2 / ( l ( d i  X  ), and add d i into D ,where L d i and l ( d i ,d i )are d responding column and diagonal entry in current L ; Step 2: update L  X  L  X  L d i L
The L matrix is the similarity matrix between the candi-date documents. We are able to select the set of K docu-ments which are most representative of the whole candidate documents by using the above algorithm. Therefore, the above algorithm implicitly considers the diversity and den-sity of the chosen documents, which are the critical factors in the Active-RDD algorithm [22].

The above algorithm ignores the relevance information, which is a key element influencing the active learning per-formance in relevance feedback. Because the ad hoc infor-mation retrieval encounters a highly unbalanced data prob-lem, the number of relevant documents is far fewer than the number of non-relevant documents, and thus the positive feedback documents should have significantly larger effect on improving the relevance feedback retrieval accuracy. Based on this insight, we modify the original sequential transduc-tive experimental design algorithm to incorporate relevance information. Because we do not know the relevance of the document before human evaluation, the only information to indicate the relevance is the relevance score calculated in Equation (3). Here, we weight the selection score in the original TED algorithm L d i 2 / ( l ( d i ,d i )+  X  ) with the nor-malized retrieval score Scored i to obtain the final selection ranked documents will have a higher selection score by using relevance weighted transductive experimental design algo-rithm than that by using the traditional transductive exper-imental design algorithm. Another intuition of using rele-vance weight is to capture the tradeoff between exploitation and exploration. The traditional transductive experimental design explores the new document space aggressively, and the relevance weighted experimental design shifts gears to exploit the top retrieved documents by adjusting the selec-tion scheme with the relevance score.
To evaluate our active relevance feedback algorithm de-scribed in the previous sections, we experiment with three TREC datasets. The first one is the TREC 2003 HARD dataset, which uses part of the AQUAINT dataset plus two additional datasets (Congressional Record (CR) and Federal Register (FR)) 1 . The second one is the TREC 7 dataset, which contains data from the TREC Disk 4 and 5 (excludes Congressional Record). The third one is the TREC 8 dataset. For all these datasets, we use topic titles as queries, because they are closer to the actual queries used in real applications. Data pre-processing is standard: terms are stemmed using the Porter Stemming and stop words are removed by using standard stop word list.
 We employ the Lemur Toolkit [1] as our retrieval system. To measure the performance of the retrieval algorithms, we use two standard ad hoc retrieval measures: (1) Mean Av-erage Precision (MAP), which is calculated as the average of the precision after each relevant document is retrieved, reflects the overall retrieval accuracy. (2) Precision at 10 documents (Pr@10): this measure gives us the precision for the first 10 documents.

For the queries which already achieve high precision with-out any feedback, pseudo-relevance feedback which uses top ranked retrieved documents can be applied to improve the We do not have the additional datasets in the TREC 2003 HARD track. Our results are still comparable to other pub-lished TREC 2003 HARD results, although the data are a little different.
 performance of these queries. There are very many non-relevant (negative) feedback documents in response to a dif-ficult query. The new relevance feedback algorithm we pro-posed in this paper has the advantage of effectively learning from negative feedback documents. To validate the effec-tiveness of learning from negative feedback documents, we pick the 25 queries with the lowest MAP in the initial re-trieval without feedback. For future comparison, we list the query IDs for the experiments in Table 2. In order to un-derstand how the DCM relevance feedback model performs on all the queries, we also present results of the complete query set. Effectively estimating query difficulty and choos-ing corresponding active feedback scheme will automate the overall process. We will provide a more detailed discussion in Section 6.
To validate the effectiveness of our DCM relevance feed-back algorithm, we compared the performance of our DCM relevance feedback algorithm with the Dirichlet prior lan-guage model (DP) [26], pseudo-relevance feedback algorithm based on the mixture model (Pseudo) [25], relevance feed-back algorithm based on the simple mixture model (SMM) [25], the relevance feedback algorithm based on the diver-gence minimization model(DM)[25], and the Rocchio algo-rithm [14]. In order to evaluate how the negative feed-back and hybrid feedback help to improve retrieval accu-racy, we evaluated three DCM relevance feedback strategies: DCM with positive feedback only (DCM-P), DCM with both positive and negative feedback (DCM-PN), DCM with hy-brid feedback (DCM-H). Recall that the hybrid feedback ap-proach randomly samples unlabeled documents and assumes their relevance.

We optimized parameters of the DCM algorithm without feedback and the language model algorithm without feed-back respectively, and employed different relevance feedback algorithms based on these baseline retrieval algorithms. In order to have a fair comparison, we used TREC 2003 HARD as training data to tune the parameters for different rele-vance feedback algorithms. We used the tuned parameters  X  =0 . 8and  X  =0 . 4 in the divergence minimization model and  X  =0 . 8and  X  =0 . 8 in the mixture model. We used the tuned parameters  X  = 140,  X  =3  X  =0 . 1and T =0 . 92 for the DCM relevance feedback model. We fix these parame-ters for the TREC 7 and TREC 8 datasets. To reduce the computation, we only selected top 10 feedback documents. We chose the top 20 terms with the largest probabilities in the feedback relevant topic model, and the top 10 terms with the largest probabilities in the feedback non-relevant model.
Table 3 shows the performance of these algorithms on both the difficult query set and the full query set, the best results are indicated in bold. We show the percentage performance improvement of the DCM-H algorithm over the simple mix-ture model algorithm (SMM), divergence minimization al-gorithm (DM), the Rocchio algorithm, the DCM-PN algo-rithm and the DCM-P algorithm respectively. From Table 3, we can see that all our DCM relevance feedback algorithms perform substantially better than the simple mixture model (SMM), divergence minimization algorithm (DM) and the Rocchio algorithm on the difficult query set, and achieve a modest improvement on the full query set. The pseudo-relevance feedback algorithm does not gain much improve-ment over baseline retrieval algorithm on difficult query set, since most of the top retrieved documents are non-relevant. Since negative feedback does not perform well in the situa-tion where no positive feedback documents are available, the DCM-PN algorithm which uses both positive and negative feedback documents, does not gain much improvement on the MAP measure over the DCM-P algorithm which only uses positive feedback documents. The DCM-PN algorithm improves the Pr@10 measure, since it offsets the score of documents containing the non-relevant terms. The DCM-H algorithm with hybrid feedback outperforms both the DCM-PN algorithm and DCM-P algorithm as expected. The rea-son lies in that the hybrid feedback approach effectively fil-ters out the relevant terms which are closely related to the query terms in the feedback non-relevant model z FN .
To understand how negative feedback helps to improve the performance, we further investigated two specific queries (i.e. query 398 and query 443) in detail. The results from these two representative queries are very intuitive to un-derstand the DCM relevance feedback mechanism. Table 4 shows the performance of these two queries for the DCM relevance feedback algorithm with negative feedback (DCM-PN), without negative feedback (DCM-P), without any feed-back (DP). The results indicate that using negative feed-back improves the performance significantly. We show the terms and their parameters in the feedback relevant model z
FR and the feedback non-relevant model z FN for these two queries in Table 5. For query 398, the narrative description is  X  X elevant documents may address any of the following is-sues: reducing conventional (non-nuclear) forces in Europe (CFE), efforts toward reduction that have begun or have been accomplished, the numbers of weapons or manpower reduced through this treaty, comparisons of numbers of reductions made by the Warsaw Pact and NATO, and the effect this arms cut is having on European nations. X  From Table 6, the query terms have large weight because of query regulariza-tion . Relevant terms  X  X arsaw X ,  X  X act X ,  X  X FE X  and  X  X ATO X  are included in the feedback relevant model z FR .Sincethe term  X  X on-nuclear X  is explicitly indicated in the query nar-rative description, we can infer the term  X  X uclear X  is not relevant to the search query. The term  X  X uclear X  is included in the feedback non-relevant model z FN with a significantly higher weight (0.2601) than in the feedback relevant model z
FR (0.0452). For query 443, the narrative description is  X  X ll references to U.S. Governmental and private assistance to sub-Saharan Africa are relevant. Documents discussing contributions by reason of U.S. membership in international aid organizations are also relevant. X  Terms closely related to the query such as  X  X frican X  and  X  X nvestor X  are given large weights in the feedback relevant model z FR . Although this query does not specifically indicate what kind of documents are non-relevant, we can still infer that terms included in the non-relevant model:  X  X abor X ,  X  X isincentive X ,  X  X urrency X ,  X  X olitician X , etc., are not related to the query  X  X nvest Africa X . In summary, query regularization assigns a large weight to the query terms in the relevant model, and consequently the feedback non-relevant model can then effectively capture the off-topic terms in the negative feedback documents.
In section 3, we proposed the approach which adjusts the query regularization parameter based on the percentage of non-relevant feedback documents. The intuition is that if the original query already provides satisfying performance, relevance feedback will rely more on the original query term, and vice versa. To validate this approach, we compare the adjusted regularization parameter approach (DCM-P-A) with fixed regularization parameter approach (DCM-P-F), and list their performance in Table 6. Both approaches are based on the DCM-P algorithm, which only uses positive feedback. We set parameter by optimizing on the TREC-2003-HARD dataset. We set  X  = 40 for the fixed regulariza-tion parameter approach (DCM-P-F), and  X  = 140 , X  =3 for the adjusted regularization parameter approach (DCM-P-A). From Table 6, we can see that the adjusted regu-larization parameter approach (DCM-P-A) performs signif-icantly better than the fixed regularization parameter ap-proach (DCM-P-F).
In this section, we evaluated the new active learning algo-rithm: relevance weighted transductive experimental design algorithm (TED-REL). We compare the relevance weighted transductive experimental design algorithm TED-REL with the existing active learning algorithms: TOP K, Cluster Centroid [15], Active-RDD [22] and the transductive experi-mental design algorithm without relevance weighting (TED). We used the above DCM-PN relevance feedback algorithm with both positive and negative feedback as the baseline rel-evance feedback algorithm, and the parameters are set the same as the previous section. This is because the better performing DCM-H algorithm with hybrid feedback offsets the power of active learning. We set the relevance parame-ter equal to 0.6 and the diversity parameter equal to 0.4 in the Active-RDD algorithm by training on the TREC 2003 HARD dataset. The parameter  X  in the TED and TED-REL algorithms is fixed to 0.0001.

We list the performance comparison in Table 7. From Ta-ble 7, the TED-REL performs better than all the other al-gorithms on the TREC 2003 HARD dataset and the TREC 8 dataset; the Top K algorithm performs best on the TREC 7 dataset. All these active learning approaches trade off be-tween relevance and diversity. We can rank these approaches in terms of from incorporating diversity the most to the least in the following order: TED, CLUSTER, RDD, TED-REL, TOP K. The TED algorithm and Cluster algorithm do not incorporate initial retrieval score; TOP K algorithm only considers the initial retrieval score; RDD and TRAN-REL algorithm trade off between relevance and diversity. We can also rank difficulty of the three datasets from the best ini-tial retrieval precision to the worst as: TREC 2003 HARD, TREC 8, TREC 7 (See Table 3 column 1). We can observe the following interesting patterns: For a difficult task, it is better to choose highly ranked documents (exploitation). For a easy task, it is better to choose document set with large diversity (exploration) . The above observation is also in-tuitive: the top ranked documents are likely to be similar for easy queries, so exploring diversified relevant documents will improve relevance feedback; the chance to obtain a rel-evant feedback documents is small for difficult queries, and exploiting the top ranked documents increases the chance.
Using the same active learning approach for different quer-ies may not be a good strategy because of different character-istics of queries and different definitions of topic relevance. A more intelligent approach is the following: Define a set of features which capture query and topic properties, and use some machine learning approaches to train a model which can predict choosing an effective active learning approach.
Query difficulty and clarity estimation has been recently recognized by the IR community, and these estimation ap-proaches can be applied in the context of active learning approach prediction. Here, we briefly introduce some of the notable works in query difficulty estimation. Cronen-Townsend et al. [5] computed the query clarity score by measuring the information-theoretic distance between the query language model and the collection language model, and showed that clarity scores are correlated with query dif-ficulty. Carmel et al. [2] extended the clarity score approach [5] to a more comprehensive distance measure which in-cludes the pairwise information-theoretic distances between distributions associated with the collection, the set of rel-evant documents, and the query. Besides distance based approaches, perturbation based approaches have also been shown to be an effective query difficulty measure approach. The perturbation based approaches predict query difficulty based on the stability of retrieved documents with respect to perturbations in different elements of the retrieval sys-tem. Zhou and Croft [28] introduced ranking robustness as a measure of query hardness, where ranking robustness effec-tively measures the stability in ranked results with respect to collection perturbations. Yom-Tov et al. [23] predicted query hardness by assessing the stability of ranked results with respect to perturbations in the query.

The above query difficulty measure can be applied as in-put features to a classification algorithm, which classifies queries based on their best performing active learning algo-rithm. We can label each query with its best performing active learning algorithm as class label, and train the clas-sifier based on these labeled queries. The trained classifier can be used to predict choosing active learning algorithms for new queries. In such a manner, we will assign different active learning algorithms to different queries to further uti-lize human labeling and improve retrieval accuracy. We will pursue research in this direction in the future work.
We have explored the problem of active relevance feedback to improve performance for difficult queries. In this paper, we first briefly introduced the probabilistic ranking model based on the Dirichlet Compound multinomial (DCM) dis-tribution, and a more detailed introduction can be found in [21]. We then proposed a relevance feedback algorithm based on a mixture modeling approach, which enables us to precisely capture the overlaps between positive and neg-ative feedback documents. Finally, we introduced a rele-vance weighted transductive experimental design algorithm to actively select feedback documents. We validated these algorithms based on several TREC datasets.
We acknowledge support from Cisco, University of Cal-
TREC Pr@10 0.4560 0.3880* 0.4320 0.3440* 0.4080  X  10 . 53% 5 . 16% ifornia X  X  MICRO program. We appreciate comments from Charles Elkan, Yi Zhang, Kai Yu and anonymous reviewers. [1] The lemur toolkit. http://www.lemurproject.org. [2] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. [3] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active [4] W. B. Croft. Experiments with representation in a [5] S. Cronen-Townsend, Y. Zhou, and W. Croft.
 [6] C. Elkan. Clustering documents with an [7] H. Fang, T. Tao, and C. Zhai. A formal study of [8] D. Harman. Relevance feedback revisited. In [9] D. Lewis. Naive (bayes) at forty: The independence [10] R. Madsen, D. Kauchak, and C.Elkan. Modeling word [11] T. Minka. Estimating a Dirichlet distribution. [12] S. Robertson and K. S. Jones. Relevance weighting of [13] S. E. Robertson. The probability ranking principle in [14] J. Rocchio. Relevance feedback in information [15] X. Shen and C. Zhai. Active feedback in ad hoc [16] T. Tao and C. Zhai. Regularized estimation of mixture [17] S. Tong and D. Koller. Support vector machine active [18] N. Ueda and R. Nakano. Deterministic annealing EM [19] X. Wang, H. Fang, and C. Zhai. Improve retrieval [20] Z. Xu and R. Akella. A bayesian logistic regression [21] Z. Xu and R. Akella. A new probabilistic retrieval [22] Z. Xu, R. Akella, and Y. Zhang. Incorporating [23] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow. [24] K. Yu, J. Bi, and V. Tresp. Active learning via [25] C. Zhai and J. Lafferty. Model-based feedback in the [26] C. Zhai and J. Lafferty. A study of smoothing [27] W. Zhang, X. He, B. Rey, and R. Jones. Query [28] Y. Zhou and W. B. Croft. Ranking robustness: A
