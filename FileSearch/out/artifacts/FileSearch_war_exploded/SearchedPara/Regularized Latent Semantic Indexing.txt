 Topic modeling can boost the performance of information retrieval, but its real-world application is limited due to scalability issues. Scaling to larger document collections via parallelization is an ac-tive area of research, but most solutions require drastic steps such as vastly reducing input vocabulary. We introduce Regularized La-tent Semantic Indexing (RLSI), a new method which is designed for parallelization. It is as e ff ective as existing topic models, and scales to larger datasets without reducing input vocabulary. RLSI formalizes topic modeling as a problem of minimizing a quadratic loss function regularized by  X  1 and / or  X  2 norm. This formulation allows the learning process to be decomposed into multiple sub-optimization problems which can be optimized in parallel, for ex-ample via MapReduce. We particularly propose adopting  X  1 on topics and  X  2 norm on document representations, to create a model with compact and readable topics and useful for retrieval. Relevance ranking experiments on three TREC datasets show that RLSI performs better than LSI, PLSI, and LDA, and the improve-ments are sometimes statistically significant. Experiments on a web dataset, containing about 1.6 million documents and 7 mil-lion terms, demonstrate a similar boost in performance on a larger corpus and vocabulary than in previous studies.
 Categories and Subject Descriptors: H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing General Terms: Experimentation Keywords: Topic Modeling, Regularization, Sparse Methods
Recent years have seen significant progress on topic modeling technologies in machine learning, information retrieval, natural lan-guage processing, and other related fields. Given a collection of text documents, a topic model represents the relationship between terms and documents through latent topics. A topic is defined as a probability distribution of terms or a cluster of weighted terms. A document is then viewed as a bag of terms generated from a mixture of latent topics. Various topic modeling methods, such as Latent Semantic Indexing (LSI) [10], Probabilistic Latent Seman-tic Indexing (PLSI) [16], and Latent Dirichlet Allocation (LDA) [3] have been proposed and successfully applied in various settings.
One of the main challenges in topic modeling is scaling to mil-lions or even billions of documents while maintaining a representa-tive vocabulary of terms , which is necessary in many applications such as web search. A typical approach is to approximate the learn-ing processes of an existing topic model.

In this work, instead of modifying existing methods, we intro-duce a new topic modeling method that is intrinsically scalable: Regularized Latent Semantic Indexing (RLSI). Topic modeling is formalized as minimization of a quadratic loss function on term-document occurrences regularized by  X  1 and / or  X  2 norm. Specifi-cally, in RLSI the text collection is represented as a term-document matrix, where each entry represents the occurrence (or tf-idf score) of a term in a document. The term-document matrix is then ap-proximated by the product of two matrices: the term-topic ma-trix which represents the latent topics with terms and the topic-document matrix which represents the documents with topics. Fi-nally, the quadratic loss function is defined as the squared Frobe-nius norm of the di ff erence between the term-document matrix and the output of the topic model. Both  X  1 norm and  X  2 norm may be used for regularization. We particularly propose using  X  1 topics and  X  2 norm on document representations, which can result in a model with compact and readable topics and useful for re-trieval. Note that we call our new method RLSI because it makes use of the same quadratic loss function as LSI. RLSI di ff LSI in that it uses regularization rather than orthogonality to con-strain the solutions.

The learning process of RLSI iteratively updates the term-topic matrix given the fixed topic-document matrix, and updates the topic-document matrix given the fixed term-topic matrix. The formula-tion of RLSI makes it possible to decompose the learning problem into multiple sub-optimization problems and conduct learning in parallel. Specifically, for both the term-topic matrix and the topic-document matrix, the update in each iteration is decomposed into many sub-optimization problems. These may be run in parallel, which is the main reason that RLSI can scale up. We describe our implementation of RLSI in MapReduce [9]. The MapReduce sys-tem maps the sub-optimization problems over multiple processors and then merges (reduces) the results from the processors. During this process, documents and terms are distributed and processed automatically.

For probabilistic topic models like LDA and PLSI, the scalability challenge mainly comes from the necessity of simultaneously up-dating the term-topic matrix to meet the probability distribution as-sumptions. When the number of terms is large, which is inevitable in real applications, this problem becomes particularly severe. For LSI, the challenge is due to the orthogonality assumption in the for-mulation, and as a result the problem needs to be solved by Singular Value Decomposition (SVD) and thus is hard to be parallelized. Regularization is a well-known technique in machine learning. In our setting, if we employed  X  2 norm on topics and  X  1 document representations, RLSI becomes Sparse Coding [19, 25], which is a method used in computer vision and other fields. As far as we know, regularization for topic modeling has not been widely studied, in terms of the performance of di ff erent norms or their scal-ability advantages.

Experimental results on a large web dataset show that 1) RLSI can scale up well and help improve search relevance. Specifically, we show that RLSI can e ffi ciently run on 1.6 million documents and 7 million terms on 16 distributed machines. In contrast, exist-ing methods on parallelizing LDA were demonstrated on far fewer documents and / or far fewer terms. Experiments on three TREC datasets show that 2) The readability and coherence of RLSI top-ics is equal or better than those learned by LDA, PLSI and LSI. 3) RLSI topics can be used in retrieval with better performance than LDA, PLSI, and LSI (sometimes statistically significant). 4) The best choice of regularization is  X  1 on topics and  X  2 on document rep-resentations in terms of topic readability and retrieval performance.
Studies on topic modeling fall into two categories: probabilistic approaches and non-probabilistic (matrix factorization) approaches. In the probabilistic approaches, a topic is defined as a probability distribution over terms and documents are defined as data gener-ated from mixtures of topics. To generate a document, one chooses a distribution over topics. Then, for each term in that document, one chooses a topic according to the topic distribution, and draws a term from the topic according to its term distribution. For example, PLSI [16] and LDA [3] are two widely-used generative models. In the non-probabilistic approaches, the term-document matrix is pro-jected into a K -dimensional topic space in which each axis corre-sponds to a topic. In the topic space, each document is represented as a linear combination of the K topics. LSI [10] is a representative non-probabilistic model. It decomposes the term-document matrix with SVD under the assumption that topics are orthogonal. See also Non-negative Matrix Factorization (NMF) [17, 18] and Sparse Coding methods [19, 25].

It has been demonstrated that topic modeling is useful for knowl-edge discovery, relevance ranking in search, and document classifi-cation [23, 35]. In fact, topic modeling is becoming one of impor-tant technologies in machine learning, information retrieval, and other related fields.

Most e ff orts to improve topic modeling scalability have modi-fied existing learning methods, such as LDA. Newman, et al. [24] proposed Approximate Distributed LDA (AD-LDA), in which each processor performs a local Gibbs sampling iteration followed by a global update. Two recent papers implemented AD-LDA as PLDA [34] and modified AD-LDA as PLDA + [21], using MPI [32] and MapReduce [9]. In [2], the authors proposed purely asynchronous distributed LDA algorithms based on Gibbs Sampling or Bayesian inference, called Async-CGB or Async-CVB, respectively. In Async-CGB and Async-CVB, each processor performs a local computa-tion step followed by a step of communicating with other proces-sors. In all the methods, the local processors need to maintain and update a dense term-topic matrix, usually in memory, which be-comes a bottleneck for improving the scalability. In this paper, we propose a new topic model learning algorithm which can e ffi scale up to large text corpora. The key ingredient of our method is to make the formulation of learning decomposable and thus make the process of learning parallelizable. In [1, 15], online versions of stochastic LDA were proposed. In this paper, we consider batch learning of topic models, which is a di ff erent setting from online learning. For other related work refer to [23, 31, 36].
Regularization is a common technique in machine learning to prevent over-fitting. Typical examples of regularization in machine learning include the use of  X  1 and  X  2 norms. Regularization via norm uses the sum of absolute values of parameters and thus has the e ff ect of causing many parameters to be zero and selecting a sparse model as solution [14, 26]. Regularization via  X  2 the other hand, uses the sum of squares of parameters and thus can make a smooth regularization and e ff ectively deal with over-fitting.
Sparse methods have recently received a lot of attention in ma-chine learning community. They aim to learn sparse representa-tions (simple models) hidden in the input data by using  X  ularization. Sparse Coding algorithms [19, 25] are proposed which can be used for discovering basis functions, to capture meta-level features in the input data. One justification to the sparse methods is that human brains have similar sparse mechanism for informa-tion processing. For example, when Sparse Coding algorithms are applied to natural images, the learned bases resemble the receptive fields of neurons in the visual cortex [25]. Previous work on sparse methods mainly focused on image processing (e.g., [28]). In this paper we propose using sparse methods (  X  1 norm regularization) in topic modeling, particularly to make the learned topics sparse. The use of sparse methods for topic modeling was also proposed very recently by Chen et al. [8]. Their motivation was not to improve scalability and they made an orthogonality assumption (requiring an SVD). In [33], the authors also proposed to discover sparse top-ics based on a modified version of LDA.
One of the key problems in topic modeling is to improve scala-bility, to handle millions of documents or even more. As collection size increases, so does vocabulary size, rather than a maximum vo-cabulary being reached. For example, in the 1.6 million web doc-uments in our experiment, there are more than 7 million unique terms even after pruning the low frequency ones (e.g., with term frequency in the whole collection less than 2). This means that both matrices, term-topic and topic-document, grow as the number of documents increases.

LSI needs to be solved by SVD due to the orthogonality assump-tion. The time complexity of computing SVD is normally of order O (min { MN 2 , NM 2 } ), where M denotes number of rows of the in-put matrix and N number of columns. Thus, it appears to be very di ffi cult to make LSI scalable and e ffi cient.

For PLSI and LDA, it is necessary to maintain the probability distribution constraints of the term-topic matrix. When the matrix is large, there is a cost for maintaining the probabilistic framework. One possible solution is to reduce the number of terms, but the negative consequence is that it can sacrifice learning accuracy.
How to make existing topic modeling methods scalable is still a challenging problem. In this paper, we adopt a di ff erent approach, that is, to develop new methods which can work equally well or even better, but are scalable by design.
We are given a set of documents D with size N , containing terms from a vocabulary V with size M . A document is simply repre-sented as an M -dimensional vector d , where the m th entry denotes the score of the m th term, for example, a Boolean value indicating occurrence, term frequency, tf-idf, or joint probability of the term and document. The N documents in D are then represented in an M  X  N term-document matrix D = [ d corresponds to a term and each column corresponds to a document.
A topic is defined over terms in the vocabulary and is also repre-sented as an M -dimensional vector u , where the m th entry denotes the weight of the m th term in the topic. Intuitively, the terms with larger weights are more indicative to the topic. Suppose that there are K topics in the collection. The K topics can be summarized into an M  X  K term-topic matrix U = [ u 1 ,  X  X  X  , u K ], in which each column corresponds to a topic.

Topic modeling means discovering the latent topics in the doc-ument collection as well as modeling the documents by represent-ing them as mixtures of the topics. More precisely, given topics u ,  X  X  X  , u U v n , where v kn denotes the weight of the k th topic u k in document d . The larger value of v kn , the more important role topic u in the document. Let V = [ v 1 ,  X  X  X  , v N ] be the topic-document ma-trix, where column v n stands for the representation of document d in the latent topic space. Table 1 gives a summary of notations.
Di ff erent topic modeling techniques choose di ff erent schemas to model matrices U and V and impose di ff erent constraints on them. For example, in the generative topic models such as PLSI and LDA, u ,  X  X  X  , u k = 1 ,  X  X  X  , K ; In LSI, topics u SVD can be applied.

Regularized Latent Semantic Indexing (RLSI) learns latent top-ics as well as representations of documents from the given text col-lections in the following way.

Document d n is approximated as U v n where U is the term-topic matrix and v n is the representation of d n in the latent topic space. The goodness of the approximation is measured by the squared norm of the di ff erence between d n and U v n :  X  d n  X  U v more, regularization is made on topics and document representa-tions. Specifically, we suggest  X  1 norm regularization on term-topic compact and readable topics and useful for retrieval.

Thus, given a text collection D = { d 1 ,..., d N } , RLSI amounts to solving the following optimization problem: where  X  1  X  0 is the parameter controlling the regularization on u : the larger value of  X  1 , the more sparse u k ; and  X  2 parameter controlling the regularization on v n : the larger value of  X  , the larger amount of shrinkage on v n .
 Algorithm 1 Regularized Latent Semantic Indexing Requir e: D  X  R M  X  N 1: V (0)  X  R K  X  N  X  random matrix 2: for t = 1 : T do 5: end for
In general, the regularization on topics and document represen-tations (the second term and the third term) can be either or  X  equivalent to Sparse Coding [19, 25]. When both of them are the model is similar to the double sparse model proposed in [28]
We propose using the formulation above (i.e., regularization via  X  norm on topics and  X  2 norm on document representations), be-cause in our experience this regularization strategy leads to a model with compact and readable topics and useful for retrieval.
First,  X  1 norm regularization on topics has the e ff ect of making them compact. We do this under the assumption that the essence of a topic can be captured via a small number of terms, which is reasonable in practice. In many applications, small and concise topics are more useful. For example, small topics can be interpreted as sets of synonyms, roughly corresponding to the WordNet synsets used in natural language processing.

Second,  X  1 norm can make the topics readable, no matter whether it is imposed on topics or document representations, according to our experiments. This has advantages in applications such as text summarization and visualization.

Third, there are four ways of combining  X  1 and  X  2 norms. We per-form retrieval experiments across multiple test collections, showing that better ranking performance is achieved with  X  1 norm on topics and  X  2 norm on document representations.

Last, in both learning and using of topic models, topic sparsity means that we can e ffi ciently store and process topics. We can also leverage existing techniques on sparse matrix computation [4, 20], which are e ffi cient and scalable.
The optimization Eq. (1) is convex with respect to U when V is fixed and convex with respect to V when U is fixed. However, it is not convex with respect to both of them. Following the prac-tice in Sparse Coding [19], we optimize the function in Eq. (1) by alternately minimizing it with respect to term-topic matrix U and topic-document matrix V . This procedure is summarized in Algo-rithm 1. Note that for simplicity we describe the algorithm when  X  norm is imposed on topics and  X  2 norm on document representa-tions; one can easily extend it to other regularization strategies.
Holding V = [ v 1 ,  X  X  X  , v N ] fixed, the update of U amounts to the following optimization problem: where  X  X  X  X  F is the Frobenius norm and u mk is the ( mk ) th
Note that both Sparse Coding and double sparse model formulate optimization problems in constrained forms instead of regularized forms. The two forms are equivalent. U . Let  X  d m = ( d m 1 ,  X  X  X  , d mN ) T and  X  u m = ( u column vectors whose entries are those of the m th row of D and U respectively. Thus, Eq. (2) can be rewritten as which can be decomposed into M optimization problems that can be solved independently, with each corresponding to one row of U : for m = 1 ,  X  X  X  , M .

Eq. (3) is an  X  1 -regularized least squares problem, whose ob-jective function is not di ff erentiable and it is not possible to di-rectly apply gradient-based methods. A number of techniques can be used here, such as interior point method [7], coordinate descent with soft-thresholding [13, 14], Lars-Lasso algorithm [12, 26], and feature-sign search [19]. Here we choose coordinate descent with soft-thresholding.
 those of the k th row of V , V T \ k the matrix of V T with the k removed, and  X  u m \ k the vector of  X  u m with the k th we can rewrite the objective function in Eq.(3) as
L (  X  u m ) =  X  d m  X  V T \ k  X  u m \ k  X  u mk  X  v k 2 where s ij and r i j are the ( ij ) th entries of K  X  K matrix S M  X  K matrix R = DV T , respectively, and const is a constant with respect to u mk . Then, we can conduct the minimization over u while keeping all the u ml fixed for which l , k . Furthermore, L (  X  u is di ff erentiable with respect to u mk except for the point u Forcing the partial derivative to be zero leads to which can be approximated by the following update rule: where (  X  ) + denotes the hinge function. The algorithm for updating U is summarized in Algorithm 2.
The update of V with U fixed is a least squares problem with  X  norm regularization. It can also be decomposed into N opti-mization problems, with each corresponding to one v n and can be solved in parallel: lem (also known as Ridge Regression in statistics) and the solution is: Algorithm 3 shows the procedure. (If K is large such that the matrix inversion the update of v n .) Algorithm 2 Update U Requir e: D  X  R M  X  N , V  X  R K  X  N 1: S  X  VV T 2: R  X  DV T 3: for m = 1 : M do 4:  X  u m  X  0 5: repeat 6: for k = 1 : K do 8: u mk  X  ( 9: end for 10: until convergence 11: end for 12: return U Algorithm 3 Update V Requir e: D  X  R M  X  N , U  X  R M  X  K 1:  X   X  2:  X   X  U T D 3: for n = 1 : N do 4: v n  X   X   X  n , where  X  n is the n th column of  X  5: end for 6: return V
MapReduce [9] is a computing model that supports distributed computing on large datasets. MapReduce expresses a computing task as a series of Map and Reduce operations and performs the task by executing the operations in a distributed computing environ-ment. In this paper, we implement RLSI on MapReduce, referred to as Distributed RLSI, as shown in Figure 1. At each iteration the al-gorithm updates U and V using the following MapReduce operations: Map-1 Broadcast S = VV T and map R = DV T on m ( m = 1 ,  X  X  X  , Reduce-1 Take  X  m ,  X  r m , S  X  and emit  X  m ,  X  u m  X  , where  X  u Map-2 Broadcast  X  = Reduce-2 Take Note that the data partitioning schemas for R in Map-1 and for in Map-2 are di ff erent. R is split such that entries in the same row (corresponding to one term) are shu ffl ed to the same machine while  X  is split such that entries in the same column (corresponding to one document) are shu ffl ed to the same machine.

There are a number of large scale matrix multiplication opera-tions in operation Map-1 ( DV T and VV T ) and Map-2 ( U T U
U ). These matrix multiplication operations can also be con-ducted on MapReduce infrastructure e ffi ciently. As example, DV can be calculated as tails please refer to [4, 20].
We discuss the properties of RLSI with  X  1 norm on U and  X  norm on V as example.
Despite having better scalability properties, RLSI is closely re-lated to existing topic modeling methods such as LSI, PLSI, and Sparse Coding. In [30], the relationship between LSI and PLSI are discussed, from the view point of loss function and regularization. We describe their framework, so we can describe RLSI in the con-text of existing approaches. In that framework, topic modeling is considered as a problem of optimizing the following general loss function where B (  X  X  X  ) is generalized Bregman divergence with non-negative values and is equal to zero if and only if the two inputs are equiv-alent; R (  X  ,  X  )  X  0 is the regularization on the two inputs; solution space; and  X  is a coe ffi cient making trade-o ff divergence and regularization.

Di ff erent choices of B , R , and C lead to di ff erent topic modeling techniques. Table 2 shows the relationship between RLSI and ex-isting methods of LSI, PLSI, and Sparse Coding. (Suppose that we first conduct normalization topic modeling methods in this framework, the major question is how to conduct regularization as well as optimization to make the learned topics coherent and readable. Many non-probabilistic topic modeling techniques, such as LSI, Sparse Coding, and RLSI can be translated into a probabilistic framework, as shown in Figure 2.

In the probabilistic framework, columns of the term-topic matrix u  X  X  are assumed to be independent from each other and columns of the topic-document matrix v n  X  X  are regarded as latent variables. Next, each document d n is assumed to be generated according to a Gaussian distribution conditioned on U and v n , i.e., p ( d exp tionally independent given U .

Di ff erent techniques use di ff erent priors or constraints on u and v n  X  X . Table 3 lists the priors or constraints used in LSI, Sparse Coding, and RLSI, respectively. It can be shown that LSI, Sparse Coding, and RLSI can be obtained with Maximum A Posteriori Figur e 2: Probabilistic framework for non-probabilistic methods. Table 3: Priors / constraints in di ff erent non-probabilistic methods. (MAP) Estimation [22]. That is to say, the techniques can be un-derstood in the same framework.
As explained, several methods for improving the e ffi ciency and scalability of existing topic models, especially LDA have been pro-posed. Table 4 shows the space and time complexities of AD-LDA [24], Async-CBS, Async-CVB [2], and Distributed RLSI, where AvgDL is the average document length in the collection and the sparsity of topics.
 The space complexity of AD-LDA (also Async-CGS and Async-CVB) is of order N  X  AvgDL + NK P + M K , where MK is for storing the term-topic matrix on each processor. For a large text collection, the vocabulary size M will be very large and thus the space complexity will be very high. This will hinder it from being applied to large datasets in real applications.

The space complexity of Distributed RLSI is N  X  AvgDL + (1 K 2 for updating U and V , where K 2 is for storing S or  X  , for storing U and R in P processors, and 2 NK P is for storing V and in P processors. Since K  X  M , it is clear that Distributed RLSI has better scalability. We can reach the same conclusion when compar-ing Distributed RLSI with other parallel / distributed topic modeling methods. The key is that Distributed RLSI can distribute both terms and documents over P processors. The sparsity on the term-topic matrix can also help save the space in each processor.

The time complexities of di ff erent topic modeling methods are also listed. For Distributed RLSI, I is the number of inner itera-tions in Algorithm 2; T U and T V are for the matrix operations in sion), respectively: T T where nnz (  X  ) is the number of nonzero entries in the input matrix. For details please refer to [20]. Note that the time complexities of these methods are comparable.
Topic models can be used in a wide variety of applications. We apply RLSI to relevance ranking in information retrieval (IR) and evaluate its performance in comparison to existing topic modeling methods. The use of topic modeling techniques such as LSI was proposed in IR many years ago [10]. A more recent paper [35] demonstrated improvements in retrieval performance by applying topic modeling on modern test collections. We do not replicate their precise ranking approach here, since it relies on a probabilistic topic model, but we achieve similar gains. ff erent topic modeling methods.
 T able 4: Complexity of parallel / distributed topic models.
The advantage of incorporating topic modeling in relevance rank-ing is to reduce  X  X erm mismatch X . Traditional relevance models, such as VSM [29] and BM25 [27], are all based on term match-ing. The term mismatch problem arises when the authors of docu-ments and the users of search system use di ff erent terms to describe the same concepts, and thus relevant documents get low relevance scores. For example, if a query contains the term  X  X irplane X  but a relevant document instead contains the term  X  X ircraft X , then there is a mismatch and the document may not be easily distinguished from an irrelevant one. In the topic space, however, it is very likely that the two terms are in the same topic, and thus the use of match-ing score in the topic space can help improve relevance ranking. In practice it is beneficial to combine topic matching scores with term matching scores, to leverage both broad topic matching and specific term matching.

To do so, given a query and document, we must calculate their matching scores in both term space and topic space. For query q , we represent it in the topic space: where vector q is the tf-idf representation of query q in the term space 2 . Similarly, for document d (and its tf-idf representation d in the term space) we represent it in the topic space as v matching score between the query and the document in the topic space is, then, calculated as the cosine similarity between v
The topic matching score s topic ( q , d ) is combined with the con-ing. There are several ways to conduct the combination. A simple and e ff ective approach is to use a linear combination. The final relevance ranking score s ( q , d ) is: any of the conventional relevance models such as VSM and BM25.
Another combination approach is to incorporate the topic match-ing score as a feature in a learning to rank model, e.g., LambdaRank [5]. In this paper, we use both approaches in our experiments.
Using v q = ar g min v  X  q  X  U v  X  2 2 +  X  2  X  v  X  1 if  X 
We have conducted experiments to compare di ff erent RLSI reg-ularization strategies, to compare RLSI with existing methods, and to test scalability and retrieval performance of RLSI using several datasets.
Our three TREC datasets were AP, WSJ, and OHSUMED, which have been widely used in relevance ranking experiments. We also used a large real-world web dataset from a commercial web search engine, containing about 1.6 million documents and 10 thousand queries. Each dataset consists of a document collection, a set of queries, and relevance judgments on some documents with respect to each query. For all four datasets, only the retrieved documents were included and a standard list of stop words was removed. For the Web dataset, we further discarded the terms whose frequencies in the whole dataset are less than two. Table 5 gives some statistics on the datasets.

In AP and WSJ the relevance judgments are at two levels:  X  X el-evant X  or  X  X rrelevant X . In OHSUMED, the relevance judgments are at three levels:  X  X efinitely relevant X ,  X  X artially relevant X , and  X  X ot relevant X . In the Web dataset, there are five levels:  X  X erfect X ,  X  X xcellent X ,  X  X ood X ,  X  X air X , and  X  X ad X . In the experiments of re-trieval performance, we used MAP and NDCG at the positions of 1, 3, 5, and 10 for evaluating retrieval performance. In calculating MAP, we consider  X  X efinitely relevant X  and  X  X artially relevant X  in OHSUMED, and  X  X erfect X ,  X  X xcellent X , and  X  X ood X  in Web dataset as  X  X elevant X .

In the experiments on TREC datasets (Section 6.2 and Section 6.3), no validation set was used since we only had small query sets, making it di ffi cult to hold out a validation set of meaningful size in each case. Instead, we chose to evaluate each model in a pre-defined grid of parameters, showing its performance under the best parameter choices. In the experiments on the Web dateset (Section 6.4), the queries were randomly split into training / validation sets, with 6000 / 2000 / 2680 queries, respectively. We trained the ranking models with the training set, selected the best models with the validation set, and evaluated the performances of the methods with the test set.

The experiments on AP, WSJ, and OHSUMED were conducted on a server with Intel Xeon 2.33GHZ CPU, 16GB RAM. The ex-periments on the Web dataset were conducted on a distributed sys-tem and the Distributed RLSI was implemented with SCOPE lan-guage [6].
Our comparison of di ff erent RLSI regularization strategies was carried out on AP, WSJ, and OHSUMED datasets. Regulariza-tion on U and V via either  X  1 or  X  2 norm gives us four RLSI vari-ants: RLSI (U  X  1 -V  X  2 ), RLSI (U  X  2 -V  X  1 ), RLSI (U (U  X  norm on U and  X  2 norm on V . Parameters K ,  X  1 ,  X  2 , and spectively set in ranges of [10 , 50], [0 . 01 , 1], [0 . for all variants.

We first compared the RLSI variants in terms of topic readabil-ity, by looking at the contents of topics they generated. As example, Table 6 shows 10 topics (randomly selected) and the average topic compactness (AvgComp) on AP dataset, for all four RLSI variants, when K = 20 and  X  1 and  X  2 are the optimal parameters for the re-trieval experiment described next. Here, average topic compactness is defined as average ratio of terms with non-zero weights per topic. For each topic, its top 5 weighted terms are shown. From the re-sults, we have found that 1) If  X  1 norm is imposed on either U or V , RLSI can always discover readable topics; 2) Without  X  1 norm reg-ularization (i.e., RLSI( U  X  2 -V  X  2 )), many topics are not readable; 3) If  X  1 norm is only imposed on V (i.e. RLSI (U  X  2 -V  X  discovered topics are not compact or sparse (e.g., AvgComp We also conducted the same experiments on WSJ and OHSUMED and observed similar phenomena. The examining topics on them are not shown due to space limitation.

We also compared the RLSI variants in terms of retrieval perfor-mance. Specifically, for each of the RLSI variants, we combined topic matching scores ( s topic ( q , d ) in Eq. (5)) with term match-ing scores given by conventional IR models of VSM or BM25. Since BM25 performed better than VSM on AP and WSJ, and VSM performed better than BM25 on OHSUMED, we combined the topic matching scores with BM25 on AP and WSJ, and with VSM on OHSUMED. The methods we tested were denoted as  X  X M25 + RLSI (U  X  1 -V  X  2 ) X ,  X  X M25 + RLSI (U  X  2 -V  X  1 (U  X  show the retrieval performance of RLSI variants achieved by the best parameter setting on AP, WSJ, and OHSUMED, respectively. From the results, we can see that 1) All of these methods can im-prove over the baseline and in most cases the improvement is sta-tistically significant (t-test, p-value &lt; 0 . 05); 2) Among the RLSI variants, RLSI (U  X  1 -V  X  2 ) performs best and RLSI (U  X  forms worst.

Table 7 summarizes the experimental results in terms of topic readability, topic compactness, and retrieval performance. From the result, we can see that in RLSI,  X  1 norm regularization is essen-tial for discovering readable topics, and the discovered topics will also be compact if  X  1 norm is imposed on U . Furthermore, between the two RLSI variants with good topic readability and compact-ness, i.e., RLSI (U  X  1 -V  X  2 ) and RLSI (U  X  1 -V  X  1 ), RLSI (U performs better in improving retrieval performance. Thus we con-clude that it is a better practice to apply  X  1 norm on U and on V in RLSI, for achieving good topic readability, topic compact-ness, and retrieval performance.

We will use RLSI (U  X  1 -V  X  2 ) in the following experiments and denote it as RLSI for simplicity.
Figur e 3: Retrieval performance of RLSI variants on AP.
Figur e 4: Retrieval performance of RLSI variants on WSJ.
In this experiment, we compared RLSI with LDA, PLSI, and LSI on AP, WSJ, and OHSUMED datasets.

We first compared RLSI with LDA, PLSI, and LSI in terms of topic readability, by looking at the topics they generated. We made use of the tools available on Internet for creating the baselines The number of topics K was again set to 20 for all the methods. In RLSI,  X  1 and  X  2 were the optimal parameters used in Section 6.2 (i.e.,  X  1 = 0 . 5 and  X  2 = 1 . 0). For LDA, PLSI, and LSI, there was no additional parameter to tune.
 Table 8 shows 10 randomly selected topics discovered by RLSI, LDA, PLSI, and LSI and the average topic compactness (AvgComp) on AP dataset. For each topic, its top 5 weighted terms are shown. From the results, we have found 1) RLSI can discover readable and compact (e.g., AvgComp = 0 . 0075) topics; 2) PLSI and LDA can discover coherent and readable topics as expected, however the dis-covered topics are not compact (e.g., AvgComp = 0 . 9534 and Avg-Comp = 1, respectively); 3) LDA performs better than PLSI. There is some redundancy in the topics discovered by PLSI; 4) The topics discovered by LSI were hard to understand, and this may be due to its orthogonality assumption. We also conducted the same experi-ments on WSJ and OHSUMED and observed similar phenomena. The results on them are not shown due to space limitation.
We also tested the performance of RLSI in terms of retrieval per-formance, in comparison to LSI, PLSI, LDA. The experimental set-tings was similar to that of used in Section 6.2. Parameters K and  X  were respectively set in ranges of [10 , 50] and [0 . 1 , methods, and parameters  X  1 and  X  2 in RLSI were respectively set in performance achieved by the best parameter setting on AP, WSJ, and OHSUMED, respectively. From the results, we can see that
LSI: http://tedlab.mit.edu/~dr/SVDLIBC/ ; PLSI: http://www.lemurproject.org/ ; LDA: http://www. cs.princeton.edu/~blei/lda-c/ size which the topic modeling methods can handle so far. We have also verified that RLSI can help improve web search relevance.
As future work, we plan to further enhance the scale of experi-ments to process even larger datasets. We also want to further study the theoretical properties of RLSI and new applications of RLSI. [1] L. AlSumait, D. Barbara, and C. Domeniconi. On-line lda: [2] A. Asuncion, P. Smyth, and M. Welling. Asynchronous [3] D. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [4] A. Buluc and J. R. Gilbert. Challenges and advances in [5] C. J. Burges, R. Ragno, and Q. V. Le. Learning to rank with [6] R. Chaiken, B. Jenkins, P.-A. Larson, B. Ramsey, D. Shakib, [7] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic [8] X. Chen, B. Bai, Y. Qi, Q. Lin, and J. Carbonell. Sparse [9] J. Dean, S. Ghemawat, and G. Inc. Mapreduce: simplified [10] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, [11] C. Ding, T. Li, and W. Peng. On the equivalence between [12] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least [13] J. Friedman, T. Hastie, H. Hofling, and R. Tibshirani. [14] W. J. Fu. Penalized regressions: The bridge versus the lasso. [15] M. D. Ho ff man, D. M. Blei, and F. Bach. Online learning for [16] T. Hofmann. Probabilistic latent semantic indexing. In [17] D. D. Lee and H. S. Seung. Learning the parts of objects [18] D. D. Lee and H. S. Seung. Algorithms for non-negative [19] H. Lee, A. Battle, R. Raina, and A. Y. Ng. E ffi cient sparse [20] C. Liu, H. chih Yang, J. Fan, L.-W. He, and Y.-M. Wang. [21] Z. Liu, Y. Zhang, and E. Y. Chang. Plda + : Parallel latent [22] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. [23] D. M. Mimno and McCallum. Organizing the oca: Learning [24] D. Newman, A. Asuncion, P. Smyth, and M. Welling.
 [25] B. A. Olshausen and D. J. Fieldt. Sparse coding with an [26] M. Osborne, B. Presnell, and B. Turlach. A new approach to [27] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, [28] R. Rubinstein, M. Zibulevsky, and M. Elad. Double sparsity: [29] G. Salton, A. Wong, and C. S. Yang. A vector space model [30] A. P. Singh and G. J. Gordon. A unified view of matrix [31] A. Smola and S. Narayanamurthy. An architecture for [32] R. Thakur and R. Rabenseifner. Optimization of collective [33] C. Wang and D. M. Blei. Decoupling sparsity and [34] Y. Wang, H. Bai, M. Stanton, W. yen Chen, and E. Y. Chang. [35] X. Wei and B. W. Croft. Lda-based document models for [36] F. Yan, N. Xu, and Y. A. Qi. Parallel inference for latent
