 Food safety is an important health issue in Singapore as the num-ber of food poisoning cases have increased significantly over the past few decades. The National Environment Agency of Singapore (NEA) is the primary government agency responsible for monitor-ing and mitigating the food safety risks. In an effort to pro-actively monitor emerging food safety issues and to stay abreast with devel-opments related to food safety in the world, NEA tracks the World Wide Web as a source of news feeds to identify food safety related articles. However, such information gathering is a difficult and time consuming process due to information overload. In this paper, we present FoodSIS, a system for end-to-end web information gather-ing for food safety. FoodSIS improves efficiency of such focused information gathering process with the use of machine learning techniques to identify and rank relevant content. We discuss the challenges in building such a system and describe how thoughtful system design and recent advances in machine learning provide a framework that synthesizes interactive learning with classification to provide a system that is used in daily operations. We conduct ex-periments and demonstrate that our classification approach results in improving the efficiency by average 35% compared to a con-ventional approach and the ranking approach leads to average 16% improvement in elevating the ranks of relevant articles.
 H.2.8 [ Database Applications ]: Data Mining X  X achine Learning; H.3.3 [ Information Search and Retrieval ]: Selection Process Machine Learning, Text Classification, Ranking, Semi-supervised Learning, Information Gathering
The US Centers for Disease Control and Prevention (CDC) esti-mated that roughly 1 in 6 Americans or 48 million people fall ill, 128,000 are hospitalized and 3,000 die of food-borne diseases each year [6]. WHO estimated that the diseases caused by major food-borne pathogens alone cost up to US $35 billion annually in med-ical expenses and lost productivity [4]. In Singapore, the number of food poisoning cases and the number of people affected have in-creased significantly (by 14 and 17 times, respectively) 1 to 2011 [14]. This increasing trend is an important public health concern as more people tend to consume meals outside of their homes. According to the Health Promotion Board (HPB), Singa-pore residents usually eat out at least four times a week at hawker centers, food courts and coffee shop stalls. The proportion has in-creased from 49% in 2004 to 60% in 2010 [5]. Food recalls also appear to be on the rise especially in the last decade [1]. Anecdotal evidence suggests similar increases food recalls in Singapore due to concerns regarding contamination in neighboring food growing and processing countries [2] or mislabeling [3]. Poor hygiene prac-tices [14], food recalls, food contamination and mislabeling, poor food processing and related technologies are all factors that could cause food safety threats.

Government agencies such as the National Environment Agency (NEA) in Singapore are responsible for monitoring the state of food safety in the country. NEA is extremely pro-active in terms of mon-itoring emerging food safety issues in order to be able to take pre-ventive actions wherever possible. One approach used by NEA to accomplish this is to constantly identify emerging food safety issues and developments by tracking the World Wide Web for arti-cles related to food safety from a variety of local and international sources. NEA has a team to browse information including news on a daily basis from a set of identified web data sources. They sift through web articles on these sources and identify those related to food safety such that they are relevant to the function and scope of the Environmental Health Department at NEA. The identified ar-ticles are then summarized and compiled into weekly newsletters and circulated to the relevant sections in the organization. This pa-per describes FoodSIS, a system built for NEA to provide support for the generation of such newsletters.

The main purpose of the newsletters is to stay abreast with in-formation such as food poisoning outbreaks happening worldwide, news about food recalls from neighboring countries, developments in food technology, and new policies/regulations about food. Each newsletter is an aggregated report of relevant information that is used by the operational teams of the agency to make tactical de-cisions regarding the published food safety alerts that are of rele-
This is not a reflection of hygiene standards. Even as safety and hygiene standards go up, the sheer number of people eating out leads to higher incidents. vance. There are two main steps in the newsletter generation pro-cess: (i) searching and identifying the content about food safety issues such that it is interesting and relevant for the agency, and (ii) summarizing the identified content and compiling it in the form of the newsletter. Due to overload of information on the web and inability of search engines to capture subtleties of an information need, the manual execution of step 1 very difficult and time con-suming. After a set of relevant articles have been identified, it is then relatively straight forward to manually summarize them since they are small in number. As a result, we focus our attention to step 1 in the current system development.

As we explain in later sections, we formulate the identification of relevant content as a document classification problem and rele-vance ranking based on labeled data. The design of such document classification systems traditionally uses supervised learning with positive and negative labeled documents. However in real life it is rare to find sufficient number of labeled documents. As a result, the design of an operational system to classify and rank documents accurately for daily use needs to exploit two additional sources of information: (i) the sources of data and the semantic and organi-zational structure of these sources, and (ii) the interactive aspects of how users interact with the system to consume the results of the classification and provide feedback on the ranked list of documents classified as relevant (in order of relevance).

The news sources that need to be examined for relevant docu-ments provide an organization by separating articles by sections (such as politics, entertainment, sports, health etc). This structure provides a way of identifying and ignoring the sections that are ir-relevant (such as sports, entertainment, ...) for the topics of interest (food safety). In addition, the marginal distribution of articles from different news sources provides useful prior information for the de-sign of an operational system.

The other important requirement is the aspect of interactive learn-ing. FoodSIS classifies articles into relevant and not relevant and presents the relevant articles as a rank ordered list (in order of relevance). Based on this interface, the user provides inputs by choosing documents that are truly relevant (as per her judgment). This interaction provides information beyond the simple labeling of documents as relevant / not relevant . The user implicitly provides pairwise comparisons (not necessarily a total order) between docu-ments and separation among sets of documents by his selection of relevant documents from a set of documents presented to him. This can be modeled and used to adaptively improve the ranking.
The Food S afety I nformation S ystem (FoodSIS) is a pilot system at NEA (National Environment Agency, Singapore) that is in opera-tional test mode for daily operations from last 6 months. FoodSIS is based on a design of a classification and ranking engine using a va-riety of machine learning techniques, to account for the operational needs and the exploitation of the application specific information outlined in the paragraphs above. This system is now deployed on a pilot infrastructure and is in daily use for the past six months in an effort to evaluate its efficacy. The end users now routinely use FoodSIS as the first point of entry for navigating the web for food safety related news articles and report an increasing dependence on its availability for daily operations. FoodSIS is designed to provide a ranked list of the most relevant articles (for any food safety re-lated search). The users routinely provide inputs both by marking the articles that are deemed as "truly relevant" and also qualitative feedback on the ease of use. This interaction provides data for con-tinual improvement of the operational system.

The rest of the paper is organized as follows: Section 2 pro-vides details about the data sources, data sets and the problem, and provides an analysis of the document classification problem. Sec-tion 3 describes the design and architecture of FoodSIS. Section 4 provides details regarding the technical approach for building the operationally useful classifiers using machine learning techniques. Experimental evaluation of the classifiers and results are also pre-sented in section 4. Section 5 discusses the use of pairwise ranking inputs provided (implicitly) by the users and how this is used to improve the ranking of articles classified as relevant . Section 6 is a discussion of some of the challenges and lessons that were learnt from the deployment of the system. We conclude the paper with a discussion of other application areas where this approach can be used to build information gathering systems in section 7.
Search engines are the most widely used tools for information search but there are two fundamental challenges in using them for routine, domain focused, target driven information needs: (i) Digi-tal information overload, (ii) Inability to transform information re-quirements into precise keyword queries. Figure 1 captures a man-ual information gathering process that is commonly used today in organizations. As shown in the figure, they use three different in-formation browsing mechanisms depending on the data sources. For example, some websites provide RSS feeds making it easier to access new content. Some websites are meant to host content on the topic of interest and users visit those directly. All other require-ments are satisfied by search engines. A final step is to go through all the articles presented as search results, RSS feeds or different sections of a website and shortlist those relevant to the department.
The environmental health department of NEA also followed a similar approach to information gathering. The team identified 37 websites on which they find useful posts about food safety. These 37 data sources are categorized into 3 categories as follows: 1. Official government websites: Websites such as World Health 2. Media websites: General global news websites. 3. Food hygiene and safety websites: Websites dedicated to
Food safety related information which is of interest to NEA can roughly be described by the following 5 categories: (i) food poison-ing outbreaks/food borne diseases, (ii) developments in food tech-nology and practices, (iii) food regulatory/policy developments, (iv) enforcement of food safety violation and recognition for food safety compliance, and (v) investigation reports on food hygiene inspec-tions. The 5 categories only provide an idea of the kind of informa-tion that is relevant and are not used for any strict classification.
The team has a list of 19 keyword phrases they use for searching information. The keyword phrases include high level topics such as  X  X ood safety",  X  X ood hygiene",  X  X ood policy",  X  X ood poisoning". Manually crafting a detailed and complete list of keywords for a domain is a time consuming task and it is tougher for a vast domain like food safety. Also, as we try to demonstrate in the example below, the domain understanding required for correct identification of relevant articles for NEA is beyond a list of keywords.  X  X eople have reported vomiting, diarrhoea and other symptoms of food poisoning after eating products including pizza and lasagne made by a subsidiary of Maruha Nichiro Holdings, the nation X  X  largest seafood firm. ... Police began investigating the company last month after it revealed some of its frozen food had been tainted with malathion, an agricultural chemical often used to kill aphids in corn and rice fields."
This is a story of a major food poisoning case in Japan where over 2800 people were affected due to contamination of frozen foods with pesticide. Information about food poisoning outbreaks is one of the important categories of information relevant to NEA. However, the article with the above excerpt and all the other related articles are labeled as not relevant by the users. To understand why, one must understand the cause of contamination reported in the ar-ticle as well as NEA X  X  scope of work. The article suggests that the contamination might have happened due to pesticides during manu-facturing. NEA does not oversee manufacturing stage of food prod-ucts. They mainly deal with retail end of the food supply chain. So while they are interested in food poisoning outbreaks, the focus is more on food poisoning incidents which occur in food retail estab-lishments. Extending the keyword search approach, we might need to craft complicated rules to capture these subtleties. For example, a rule such as  X  X oes not contain  X  X gricultur X  (this word form can be obtained by stemming)" could be in the list of rules, and it should supersede the rule  X  X ontains  X  X ood poisoning X ". This needs a lot of manual effort and such an approach does not have the capability to learn over time. Machine learning techniques, in particular docu-ment classification seemed the most appropriate yet simple choice to automate the content identification step when some training data (labeled articles) is available. We also use a supervised rank learner to improve the ranked list of articles that is presented to the user as output. The next section describes the deployed system and its components.
Information gathering with FoodSIS is as simple as opening the web user interface, browsing through a ranked list of articles to identify the relevant content. The list has articles compiled from all the important data sources and ranking captures their relevance to the users. The system not only simplifies step 1 and eliminates step 2 of the manual process (figure 1), but also significantly improves the efficiency of article screening.

The overall system architecture is as shown in figure 2. The sys-tem is designed such that it is easy to use, flexible, and easily con-figurable. The main components of the system are: web crawler, article text extractor and indexer, document classification and rank-ing, and an interactive web user interface. We cover details of clas-sification and ranking in section 4 and 5 respectively. We disuss the web interface briefly in the next paragraph. Due to space con-straints, we omit the description of other components.
 The web based user interface is an important component of Food-SIS which allows users to browse through the classified and ranked content. Figure 3 shows the web interface. Functionality provided by the web user interface of FoodSIS is designed with inputs from the users. Each of the features has proved very important for using FoodSIS effectively. Following are two important features avail-able through the web interface: (i) Ranked list of articles: The right pane of on the web interface is a ranked list of articles. After arti-cles are classified as relevant , we use a supervised ranking method to rank them in the order of relevance by making use of user feed-back. Each article in the list is associated with a check box. Users can label the article as relevant or not using the check box. This feedback is saved in the system and looped back for training every week. The models are updated at the same frequency. This simple mechanism proved practically very useful, since it served as a way of obtaining more labeled data with no/little extra efforts by the domain experts, (ii) Keyword search: Keyword search essentially makes the system act like a custom search engine allowing users to search using keywords. For example, if they want to see all articles containing "E. coli", they can use simple keyword search. It can also be used to identify articles during bootstrapping phase of the machine learning algorithms.

FoodSIS is thus a tool for end-to-end information gathering and includes components for data collection, pre-processing, classifi-cation, ranking, and a user interface for interacting with the users effectively.

The main challenge in making FoodSIS successful is the ability to present relevant content to the users. As illustrated in section 2, simple keyword search fails to capture the relevance effectively. Machine learning methods, in particular, document classification is well suited for such a task since the task is to separate relevant information from non-relevant. They have the ability to learn com-plex relationships from data and their performance improves with training. We formulate problem of identifying information relevant to food safety as a binary classification task. The class labels are relevant and not relevant . Relevant is a class of food safety related articles that are relevant to NEA and not relevant contains all the other articles that are not relevant. A quick look at the problem and the 5 types of information listed in section 2 might lead us to treat-ing it as a multi-class classification. But we observed the domain and data carefully and noticed that the class boundaries for the 5 classes are not very clear. The 5 types are to provide guidance on the kind of information that is relevant to NEA and the information is not always strictly limited to those. Also, from system usability point of view, the agency does not necessarily need the articles to be categorized into those 5 categories.
The most important consideration in a classification problem is the availability of labeled data for training. Figure 4 shows the datasets we have access to due to the nature of our problem. Each of the datasets can be interpreted and obtained as follows: 1. Relevant: We received around 6 months of food safety newslet-2. Not relevant: Our seed training data was based on the food 3. Unlabeled: We crawl a high number of articles everyday but 4. UNIVERSUM: UNIVERSUM is a set of data points that are just based on 4 out of 37 data sources the ratio of the number of not relevant articles to the number of relevant articles ranges from (30 to 45):1 per week Following subsections explain the key settings and methods we tried, reasons behind our choices and experimental evaluation. For all the methods, we used tf-idf scores of unigrams as our features.
A simple supervised binary classification was our first attempt at solving the task. We applied a simple Naive Bayes approach to classification since it has shown to work very well for document classification and is simple to implement. The first set of train-ing data used for building a Naive Bayes classifier contained 53 positive samples extracted from the newsletters. Negative samples were obtained by randomly sampling 200 articles from unlabeled dataset. With most of the other components of the system in place and the Naive Bayes classifier, users started to browse the articles on the web interface. The article list was ranked by the probability scores output by the classifier for membership of class relevant (no hard classification was performed). We had version 1 running for roughly a couple of months. This allowed us to gather training data which is used for evaluating different paradigms and methods for classification as explained below.
Supervised learning works quite well assuming that there is enough training data. In real life applications, this seldom is the case. Even with our interactive setup of obtaining training data from the do-main experts, the training data we collected over a couple of months is quite small to obtain good classification performance. Advances in machine learning techniques allow us to employ other learning paradigms to handle this challenge. For our setting, unlabeled ar-ticles are available in abundance. Consequently, we chose semi-supervised learning to exploit the availability of unlabeled data to improve classification performance.

Other techniques to handle scarcity of labeled training data are (i) active learning which tries to minimize the labeling effort by se-lecting the most informative examples for labeling [19], (ii) using domain knowledge to label features instead of examples [13], (iii) domain adaptation from a similar domain using transfer learning [7], and (iv) concept labeling which labels concepts in domain on-tologies and uses concept-class associations for generating training data of reasonable quality [12]. Techniques (ii) and (iv) propose to reduce the labeling effort but require the domain experts to work on a labeling task not same as the original classification. Tech-nique (iii) assumes the availability of training data in a similar do-main. For our setting, it would be an additional task for the domain experts to understand and work on a different labeling task. The interactive learning that our system engages in with the users can be termed as a simple form of active learning.

A variety of semi-supervised techniques have been proposed and shown to work well [9], [18], [8] [20]. Transductive Support Vec-tor Machines(TSVM)[20] is an extension of the widely used super-vised Support Vector Machines (SVM). The formulation of TSVM introduces a separate term in the objective function for slack vari-ables of unlabeled data and hence aims to find a hyperplane that separates both labeled and unlabeled data with maximum margin, maintaining the class ratio in unlabeled data. [17] discusses suit-ability of SVMs for information retrieval which shares many char-acteristics with our problem. [15] discusses the effectiveness of TSVMs for text classification where the authors mention that TSVMs are well suited for problem settings where the document vectors are sparse and input space has high dimensionality, outperforming conventional methods substantially while also being more robust. Therefore, we believe TSVMs is the best approach to apply to our case. Experimental results in section 4.5 show that TSVM performs better than SVM, specially when the amount of training data is very small.
Incorporating unlabeled data in classification might not always be useful, as noted in [24], [10], [11]. There are many papers dis-cussing cases where unlabeled data hurts the performance and indi-cating that it should be used carefully. We experimented with ran-domly sampling 20% to 100% of unlabeled data from the available unlabeled dataset and using it for training the TSVM. The com-parison of precision, recall and F1 score for these different runs of TSVM with different amounts of randomly sampled unlabeled data did not show a clear winner. We tried a heuristic for select-ing unlabeled instances based on the following property of our data sources: some data sources in our list, specially those under the category  X  X ood hygiene and safety websites" have a higher ratio of number of relevant articles to the number of not relevant articles. If we sample unlabeled data points from such sources of data, they are likely to create a dense region around relevant class. Follow-ing the TSVM principle, we expect such dense region to guide the separating hyperplane away from labeled data of relevant class, re-sulting in more test data points to be classified as relevant . Such a placement of the separating hyperplane will improve recall, reduc-ing precision. During the first year of deployment of FoodSIS, high recall is important since it gives confidence to the users on ability of the system to provide complete results. The exact steps followed in the heuristic based selection of unlabeled data are: (i) identify a set of data source names (i.e. the hostname of the websites) corre-sponding to labeled examples of class relevant , lets call this set W, (ii) select articles from unlabeled data based on W, i.e. select only those unlabeled instances which appear on the hosts in W. We then used this unlabeled data sample set for training a TSVM. We call this approach TSVM-Meta since it uses the meta attribute  X  X ata source name" in selecting unlabeled data. Section 4.5 compares the performance of TSVM-Meta with other methods. As demon-strated in section 4.5, though there is no formal theoretical support, TSVM-Meta performs very well empirically, due to the intuition discussed above.
UNIVERSUM data has been found to help improve classifica-tion performance since it encodes some useful prior knowledge and there have been many methods proposed in the recent past to ex-ploit the availability of UNIVERSUM [22]. Yang et. al. [21] pro-posed a tri-class support vector machine (3C-SVM) which uses a combination of UNIVERSUM and unlabeled data together as un-labeled data for developing a maximum margin classifier, utilizing the implicit knowledge among all available data. As a by product, it also identifies the irrelevant data from the set of unlabeled ex-amples (which, in our case is a mix of UNIVERSUM and relevant unlabeled data), which in turn helps to improve the classification performance. Further, in 3C-SVM the irrelevant data in fact plays the role of seeking a good subspace of the decision boundary based on two principles as follows: i) the confidence of prediction is re-lated to the distance of the data point from the decision boundary i.e. if the point is farther, the data point is likely to correctly classi-fied, however, if the data point is near the decision boundary, there is lesser probability or confidence that the data point is classified correctly. As a result, ideally, the relevant data points should lie farther from the boundary and the irrelevant class should lie near to the boundary. ii) the maximum entropy principle indicates that a classifier should rely more on the relevant data, while maximally ignore the irrelevant data. Taking these principles into consider-ation, 3C-SVM aims to find a maximum margin hyperplane such that irrelevant data points are closer to the hyperplane than rele-vant examples. As a consequence, it combines the advantages of irrelevant unlabeled data and UNIVERSUM to lead to a maximum margin hyperplane fulfilling the two assumptions.

The identification of irrelevant data points from the set of unla-beled examples is important since 3C-SVM aims to find a hyper-plane in which the irrelevant data points are close to the boundary. To accomplish this, it uses two loss functions:
Consequently, to separate the unlabeled data into relevant and irrelevant data, 3C-SVM calculates, where, f  X  ( x ) is the classifier model. Hence, for an unlabeled data point, according to the value of the l min ( x ) , when the error measured by the -sensitive loss is smaller than the error measured by Symmetric Hinge loss, it can be determined as irrelevant data. 3C-SVM learns a decision boundary in which irrelevant data points are closer to the boundary whereas the relevant examples are farther away from the boundary.
In this section, we report comparison of different classification methods on a dataset obtained by methods described earlier. The dataset we use for all the experiments has 186 relevant , 406 not rel-evant , 1796 unlabeled and 900 UNIVERSUM articles. Our system in its current version crawls data from a subset of the 37 different data sources. The number of articles crawled ranges from around 600 to 1300 per day. We have access to a large set of unlabeled and UNIVERSUM articles, but we intuitively feel that since labeled data is limited, using all the other available data for classification might add a lot of noise. So we randomly sampled unlabeled data to roughly 3 times the size of labeled data and UNIVERSUM data close to 1.5 times the size of labeled data. All the experiments are run 10 times with random train-test split of the labeled data and we report the average precision, recall and F1 score of the meth-ods. Figures 5, 6 and 7 show comparison of of different classifiers based on F1 score, precision and recall respectively. The values on the x-axis are the percentages of test data in the random train-test split of labeled data. Please note that we vary test data percentage from 50% to 90% mainly because one of the objectives of compar-ing different methods was to find out which method performs the best with very few training examples. The motivation behind such objective is to be able to apply the system to a domain other than food safety easily. Availability of labeled data would be a major challenge to apply our methodology to a completely different do-main. Hence it is important to employ a method that can work well with a small training data set. The methods we evaluate are 3C-SVM, SVM, TSVM, TSVM-Meta, and keyword search (referred to as KB in the plots). The keyword based approach uses the list of 19 keyword phrases provided by the NEA users to us and classifies an article as relevant if one or more of those keyword phrases occur in the article.
 Figure 5 shows that 3C-SVM outperforms all other methods in F1 except the case when there are only 10% examples for training, i.e. around 18 examples of positive class and 40 examples of nega-tive class. Improvement in F1 score due to 3C-SVM is about 10% over TSVM, in the range of 3% to 8% over TSVM-Meta, 1.5%-6% over SVM and about 27% to 39% over KB except the case with 10% labeled data for training. SVM performs as good as 3C-SVM when 50% of labeled data is used for training. But the F1 score drops drastically as the training set size becomes smaller. This is expected and is the main reason why semi-supervised learning is ef-fective when training data is too small. Same effect can be observed in comparing SVM with TSVM and TSVM-Meta. Improvement in 3C-SVM over TSVM and TSVM-Meta can be attributed to its use of UNIVERSUM and the way it incorporates such data. An anal-ysis of our data set using 3C-SVM provides the average distances of the 3 classes from the decision boundary (the separating hyper-plane). The average absolute distance of the positive class docu-ments from the separating hyperplane is 2.013 (  X  =0.39) while the average absolute distance for the negative class is 0.65 (  X  =0.23). From within the unlabeled data it is now interesting to note that the average absolute distance of the irrelevant data (UNIVERSUM) is only 0.13 (  X  =0.22) while the rest of the unlabeled data has a abso-lute distance of about 0.72 (  X  =0.31) and therefore can be consid-ered as relevant. These numbers indicate that the underlying prin-ciple of irrelevant data being close to the decision boundary while relevant data being far from decision boundary is satisfied for our data sets and the analysis provides a good design for placing the decision boundary.

It is interesting to observe the significant lift in F1 score (4% to 20%) achieved by the unlabeled instances selection in TSVM-Meta over TSVM. As explained in section 4.3, this could be due to the proximity of the selected unlabeled instances to positive examples. It could also be due to reduction in the size of the unlabeled data resulting from the selection. To verify this, we ran experiments with random sampling of unlabeled data varying the percentage size of the sample and ran TSVM on each such dataset. We did not see any trend or conclusive effect of the size of the unlabeled dataset just based on random sampling. This leads us to believe that careful selection of unlabeled instances exploiting important properties of the dataset is effective and we are working towards formalizing such an approach. The F1 score for KB is more or less constant with different percentage of test data since there is no training phase in KB. It is important to note that the performance of most other methods matches or is lower than KB when only 10% labeled data is used for training. This case corresponds to approximately 18 relevant articles and 40 not relevant articles. It is difficult to apply machine learning techniques for such a small dataset and obtain reasonable performance.

As shown in figure 6 and 7, precision of 3C-SVM is lower than the other learning methods, but recall is very high and beats all other methods. SVM is the reverse case with highest precision and lowest recall for most data splits. TSVM and TSVM-Meta have a good balance of precision and recall compared to 3C-SVM and SVM. KB as expected has poor precision but relatively moderate recall compared to all other methods. For a system like FoodSIS, one of the challenges is to make the users comfortable with the machine learning based concepts and approach. It is difficult for domain experts to understand that it is highly unlikely for a ma-chine learning method to be 100% accurate. The users want the system to capture all the relevant information i.e. high recall with high precision. Due to the trade-off between precision and recall, when discussed with them, they showed greater preference towards higher recall since it is important for them to see completeness in the results. 3C-SVM would still improve the precision over KB by approximately 35% in most cases. Based on high F1 scores and high recall, we chose 3C-SVM as our classifier for version 2 of FoodSIS. Figure 5: Comparison of different classifiers based on F1 score Figure 6: Comparison of different classifiers based on precision
In this section we describe the interactive learning feature of our system. FoodSIS can actively incorporate the knowledge and pref-erences of users/domain experts into the system and use this feed-back to improve the ranking of articles classified as relevant by the classifier. We saw how data for training the classifier is obtained from user feedback provided through the web interface. This feed-back can also be used to infer partial pairwise comparisons between
Figure 7: Comparison of different classifiers based on recall the articles. For example, suppose a ranked list of 10 articles is pre-sented to the user out of which, she labels the 4 th and 7 as relevant, then we can implicitly determine pairwise preferences between these articles such as : 4 th &gt; 1 st , 4 th &gt; 2 4 7 7 th &gt; 8 th , 7 th &gt; 9 th and 7 th &gt; 10 th . These pairwise prefer-ences are collected and stored in the system which are then used to improve the ranked list of relevant articles in future.

This kind of feedback from domain experts has two characteris-tics: (i) It is in the form of pairwise preferences between articles, (ii) It is not necessarily a total order, i.e. the pairwise preferences span over a partial set of document pairs. Due to these proper-ties, we explored ranking techniques which can learn from pair-wise comparisons and a partial order over the set of articles. One of the most popular ranking techniques for this setting is the Rank-ing Support Vector Machine(Rank-SVM) [16] 4 . Rank-SVM takes document pairs as instances for learning and transforms the prob-lem of learning to rank into a classification problem. Moreover, it uses SVM as a classification technique to learn the model. The aim is to learn a linear ranking function f  X  X  X  w such that, where,  X  X  X  w is a weight vector of the SVM classification model and  X  ( d i ) is the feature vector for d i . Specifically, for each document pair, Rank-SVM assigns a label representing the relative relevance of those documents. Then a classification model is trained using this labeled data which is equivalent to finding the weight vector so that the maximum number of pairwise preferences are fulfilled. Further, this classification model is used to obtain a ranking of the
On the web interface, the user saves labels for 10 articles per page, so if on a page, 2 articles are labeled relevant, 8 others will auto-matically be saved as not relevant. We have an understanding with the users to label articles in batches of 10 to make this work as expected.
Note that even though we can obtain a ranked list of articles using the classifiers presented in Section 4, the objective behind using Rank-SVM is to:(i) improve the ranking of articles classified as relevant , (ii) incorporate the active feedback/preferences give the user. test documents. For the weight vector  X  X  X  w , the documents are or-dered by their projection onto  X  X  X  w .

For FoodSIS, the relevant articles given by the classification model explained in Section 4 are then ordered according to the score given by the Rank-SVM model. The efficacy of this interactive learn-ing system was evaluated with experiments that compare Average Precision(AP) [23] scores of the rankings of documents, given by the classifiers and the Rank-SVM. In particular, we compared the AP scores of ranked lists obtained by using classifiers such Naive Bayes (NB) and 3-Class SVM (3CSVM) as well as the ranked list obtained by using Rank-SVM. The ranking for NB is a descending order of probability score output for relevant class for the articles and 3CSVM ranking is based on descending order of the distance of the article feature vectors from the separating hyperplane.
To perform this experiment, we used the set of articles and their pairwise preferences given by the user, collected over a span of 60 days. Out of 60 days, we take out random 20 days as test set. For each day in the test set, we obtained a ranked list of articles for that day by training Rank-SVM over pairwise preferences for ran-domly chosen 40 days. Then, we repeated this procedure 5 times by choosing random 40 days for training Rank-SVM each time and re-port the average AP score for each test day corresponding to these 5 runs in Table 1. The AP scores are obtained over top 30 articles for each day in the test set. For ordering the articles in the test set using NB and 3CSVM, we sorted the documents according to the score given by the classifiers 5 . Finally, we compared the AP scores for different ranked lists given by NB, 3CSVM and Rank-SVM which were calculated as follows: For any test date, where, k is the rank in the sequence of relevant articles, n is the number of relevant articles (30 for our experiments), P ( k ) is the precision at cut-off k in the list, and rel ( k ) is 1 if the article at rank k is a relevant article, zero otherwise. In Table 1 we present the corresponding results. It can be clearly seen that Rank-SVM gives significantly better AP scores for 19 days compared to NB and for 15 days compared to 3CSVM. Further, the average of AP scores over all 20 days in test set is greater for Rank-SVM and shows a 29% improvement over NB and about 16% improvement over 3CSVM. We also performed a paired t-test over these AP scores to find out if the improvement in the performance of Rank-SVM is statistically significant(overall as well as over individual days). To accomplish this, we used the AP values of 5 random runs of Rank-SVM to compute the p-value using paired t-test. We also report the p-values of individual days in Table 1 and highlight the values where the performance improvement was found to be statistically significant. Moreover, it was also observed that the overall perfor-mance improvement of Rank-SVM compared to NB and 3CSVM is statistically significant with a p-value of 1.168e-06 and 6.316e-05 respectively. Furthermore, performance improvement of 3CSVM compared to NB is statistically significant as well with a p-value of 0.0232.
FoodSIS is developed as part of a research project between NEA and IBM. It is currently in its second version and has been deployed within NEA on a research infrastructure. NEA users are using the
However, since NB and 3CSVM do not need pairwise preferences for training, the number of data points used for training of these classifiers was significantly larger than used for Rank-SVM. AVG 0.863 0.671 0.742 Table 1: Comparison of Average Precision(AP) scores for rank-ings based on different approaches web interface for past 6 months and have helped us gather sub-stantial amount of labeled data. FoodSIS is now by default the starting point for searches related to food safety within NEA. How-ever, it is still evolving and we are adding more data sources, and more features to enhance its capability. With the current list of data sources that are crawled, we get an average of around 1000 articles a day. Note that this includes articles only from the identified web-sites and news from sections relevant to food safety. It is clearly impossible for the users to go through such a large number of ar-ticles per day. Employing recent advances in machine learning to classify and rank the crawled articles results in presenting a signif-icantly smaller number of highly relevant articles to the users lead-ing to significant improvement in the newsletter generation process efficiency. This section highlights some of the challenges of the deployment and an illustration of how the system was used to gen-erate food safety newsletters. We also discuss the impact of the newsletters for NEA.
Development of FoodSIS exposed us to many interesting char-acteristics of a real world machine learning task. The foremost being availability of labeled data. In real life, labeled data for train-ing supervised approaches is either absent or is available but very small. We had access to very small number of articles of the pos-itive class as seed data. Another typical situation is the lack of negatively labeled historical data. This is in large part because the users identify and store the positive examples but rarely bother to keep the negative examples especially for historically labeled data. The following 3 design choices helped us handle these challenges and provided a substantial performance lift. 1. Web application: One important decision was to build an MAP 0.58 0.699 0.722 0.862 Table 2: Performance improvements with increase in labeled data 2. Exploiting unlabeled and UNIVERSUM datasets: The avail-3. Extracting pairwise preferences: The labeling done by users It can be clearly seen that though we started with a very small la-beled dataset, but careful design of the system and process allowed us to make the machine learning approach a viable and preferable alternative to information gathering. Another important challenge is to educate the users regarding the difference between the use of machine learning based customized search over a simple infor-mation retrieval based search. After the system was made func-tional, we conducted workshops to highlight this difference and the promise of continual improvement of a machine learning based technique.
As discussed in sections 1 and 2, a team in the environmental health department of NEA generates a weekly newsletter on food safety from information posted on the web. The newsletter is circu-lated to the entire department. We give an example of a newsletter that was generated in the recent past and describe how FoodSIS contributed to increasing the efficiency of the process, thereby sav-ing time and effort. This example is a real newsletter and it contains summaries of the following 3 articles on topic  X  X obile food ven-dors": (i) Food trucks cleanliness, (ii) Food trucks vs restaurants, (iii) Late night mobile food vendors.

These articles were ranked at 1, 5 and 14 on FoodSIS in that order on the day they were published. To illustrate how such a newsletter would have been generated without the use of our sys-tem, we perform search on Google under section "News" and se-lect the exact date for each of the above 3 articles. We created 2 disjunctive queries (Google limits a search query to only 32 words) from the 19 keyword phrases provided to us by the NEA users and use those for search. Under such search setting, the first article on  X  X ood trucks cleanliness" appeared 10 th in the search results and the other 2 did not even appear in the top 50. This example illus-trates that FoodSIS succeeded in its objective of providing relevant information with minimum effort for the user 6 .

On close observation, it can be seen that the newsletter has a central theme:  X  X obile food vendors" and the articles span over a period of 2.5 weeks. As part of the enhancements to the web in-terface, users had emphasized a need for an ability to filter articles based on whether they had been labeled as relevant, not relevant or had been left as unlabeled by the users. The drop down "Show La-bel" on the web interface provides such a filter. Such simple func-tionality provided through the web interface can facilitate browsing of articles labeled as relevant in the past and make it easier for the users to include such articles on related topics but from different time periods. As we mention in section 3, all the features of the web interface have been designed with input from the users and considering the newsletter generation process.
The newsletters keep NEA abreast of the recent developments in topics related to food safety. They can use that information to improve their operational processes. For example, considering the newsletter on  X  X obile food vendors" that we discussed above, it helps NEA to understand how mobile food vendors are managed in other countries. The number of mobile food vans is on increase in Singapore and NEA wants to constantly improve its processes and regulations to handle these changes. Easy access to informa-tion about how they are handled in other countries gives them ideas that can be considered for improving NEA X  X  processes. Newslet-ters about food recalls in neighboring countries act as alerts and
We acknowledge that such comparison with Google search is sim-plistic and biased since FoodSIS might have influenced the user X  X  choice of articles for the newsletter. But the difference in the effec-tiveness of the methods is significant to conclude that FoodSIS was a better tool to help with this example newsletter generation. they can assess the impact of those recalls on Singapore. Other ar-eas of impact of the newsletters include adopting the technology advances, policy, regulation improvements and capability develop-ment programs.
Many groups in an enterprise have the need for keeping up to date with news and information about their domain and they rely on the World Wide Web as a primary source of information. For example, the supply chain management organization is constantly monitoring the web for news related to various echelons of the sup-ply network to understand potential disruptions due to weather on their logistics, impact of political, labor and natural disaster events on productivity and service levels. Similarly the marketing divi-sions monitor the web (and social media) for sentiment or buzz around product release or marketing campaigns. The need to build custom search engines for different enterprise functions is now a universal need.

In this paper, we have outlined our experience and a framework (leveraging machine learning) for building such systems that are tailored for a department X  X  specific function needs. There are sev-eral operational challenges (from availability of labeled data, to leveraging interactive learning) that need to be addressed. We be-lieve additional work and effort in streamlining the use of these techniques and the training of document classifiers would help in the adoption of these techniques broadly in daily enterprise opera-tions. We would like to thank Christine Lee and Jo-Ann Lim from NEA, Singapore for labeling the articles by actively using Food-SIS. We also appreciate the valuable suggestions given by them and Christopher Goh, Atikah Rahman, Lee San Tay, Yoke Leng Toh and Kaiyun Chio from NEA, Singapore for improving the user in-terface. We thank Vikas Sindhwani from IBM Research for useful and insightful discussions and pointers on semi-supervised learning methods. [1] Last accessed on 20 Feb 2014, Available at: [2] Last accessed on 20 Feb 2014, Available at: [3] Last accessed on 20 Feb 2014, Available at: [4] General information related to microbiological risks in food, [5] Health promotion board, press releases 2012. Last accessed [6] Estimates of foodborne illness in the united states, us centers [7] John Blitzer, Mark Dredze, and Fernando Pereira.
 [8] Avrim Blum and Shuchi Chawla. Learning from labeled and [9] Avrim Blum and Tom Mitchell. Combining labeled and [10] O. Chapelle, B. Sch X lkopf, and A. Zien, editors.
 [11] Olivier Chapelle, Vikas Sindhwani, and Sathiya S. Keerthi. [12] Vijil Chenthamarakshan, Prem Melville, Vikas Sindhwani, [13] Gregory Druck, Gideon Mann, and Andrew McCallum.
 [14] Toh HY., Hishamuddin P., and Tay J. Epidemiology and [15] Thorsten Joachims. Transductive inference for text [16] Thorsten Joachims. Optimizing search engines using [17] Ramesh Nallapati. Discriminative models for information [18] Kamal Nigam, Andrew Kachites McCallum, Sebastian [19] Burr Settles. Active learning literature survey. Technical [20] Vladimir Vapnik. Statistical Learning Theory . Wiley, 1998. [21] Haiqin Yang, Shenghuo Zhu, Irwin King, and Michael R. [22] Dan Zhang, Jingdong Wang, Fei Wang, and Changshui [23] Ethan Zhang and Yi Zhang. Average precision. In LING LIU [24] Tong Zhang. The value of unlabeled data for classification
