 Current researches on Open-Domain Question Answering (QA) mainly target more complex questions than factoid ones. The ciQA track in TREC2007 [1] focused on  X  X elationship X  questions, which is defined as the ability of one entity to influence another, including both the means to influence and the motivation for doing so. In NTCIR-8, complex questions [2] are taxonomically divided into five types(Event, Definition, Biography, Relationship and Why). Sentences that contain correct responses are extracted as answer candidates. 
Complex questions refer to complex relations between semantic concepts, or syn-thesizing processes of deep knowledge as we ll as rich information need. Take a ques-tion-answer pair in Table 1 as an example. The question is a definition one, in which the information need is supposed to be the characteristic of some people who have much concern about the security of computer and network, and their activities of breaking computer security as well. Therefore, Answer 1 and Answer 2 are both correct answers for the question. cult to acquire user requirements from the question and; (2) the answer to the complex question is a mixture of complex semantic relations and should be accurate as while as non-redundant. Most current researches combined the two problems as one core problem, namely how to acquire accurate answers. To this end, many systems aim at investigating the sentence patterns of the complex questions, i.e., leverage the syntac-tic styles of complex sentences and convert them to patterns for answer sentence re-trieval. Wu et al. [3] extracted definitional patterns from the Wikipedia data using regular expressions. Cui et al. [4] employed soft patterns (also known as probabilistic lexico-syntactic patterns), which were produced by unsupervised learning. These approaches are supported either by manual work or by annotated corpus more or less. On the other hand, some approaches consider the two problems independently. Hara-bagiu [5] decomposed the complex questions to the factoid ones using WordNet, and answers to the complex questions are appropr iately combined as the answer to the original question. Such approaches cast the problem of acquiring information need of the complex questions to that of question decomposition, by which the unspecific information need is alternated to the specific concepts in terms of hyponymy. How-ever, general lexico-semantic resources against some specific resources, i.e., para-phrases or manually collected entailment rules, may result in low performance be-cause lexical alternation doesn X  X  take the semantically related context into account. For a better performance, most systems employ specific knowledge bases such as Wikipedia or Encarta; and the essential reason is that the specific knowledge involves almost entire information need for complex questions. Hickl et al. [6] searched the original complex questions into Wikipedia and calculate similarity between sentences in Wikipedia and the corpus. Zhang et al. [7] utilized multiple web knowledge bases to improve EAT acquisition. However, most of these systems achieve low perform-ances, since complex questions imply complex(i.e., deep semantic) relations between terms, whereas those systems just statistically retrieve and rank documents or sen-tences by centroid-based (or bag-of-words) methods. 
In this paper, we treat the two problems independently. For acquiring user re-quirements in questions, we summarize texts from web knowledge bases, i.e., Wiki-pedia, as the answer references. At the answer acquisition stage, we employ a topic model for answer sentence acquisition to extract sentences that may contain the an-swers. By mapping into a latent semantic space, sentences that are semantically related with the questions are selected and ranked as the answer candidates. We also consider linguistic information in answer ranking and employ the web knowledge bases to expand the questions. 
The rest of the paper is organized as follows. In Section 2, we give methods of an-the conclusion and the future work are given in Section 4. 2.1 Summarization from Web Knowledge Bases Although web knowledge bases are cleaned up and organized by manual work, they still contain insignificant information that may decrease the performance of document retrieval or answer ranking. In order to get significant information for a question, some systems make use of summarization methods to acquire important portions from web knowledge bases. In this case, the performance of the systems depends on the quality of summarized texts. In other words, when the contents related to a question in web knowledge bases are not rich, the initial query generated is insufficient to retrieve semantically related documents. Since some web knowledges such as Wikipedia pro-vide the links to combine the relevant concepts of documents, they can be leveraged summaries with various lengths. By building an extended document concept lattice model, concepts and non-textual features such as wiki article, infobox and outline are combined. Experiments showed that system performance outperformed not only tradi-tional summarizing methods but also some soft-pattern approaches. In this paper, we utilize the approach to summarize contents in Wikipedia. Sentences summarized for each question are put into a text set. 
Although most relevant texts can be found in Wikipedia, we still utilize other two web knowledges as a supplement. Following an unsupervised summarization ap-text set without sentence redundancy. 
The summarization method carries out with the following steps. First, extract the words and named entities from a question as well as eliminate the stop words or clue question gives the more reliable indication for the summarization method. For exam-ple, the question  X  X hat is the greenhouse effect X  contains two words  X  X reenhouse X  and  X  X reenhouse effect X  that are both indexed by Wikipedia, whereas  X  X reenhouse effect X  is more reliable than  X  X reenhouse X  for the question. More specifically, sum-marized texts of the search result of  X  X reenhouse effect X  are more semantically rele-vant than that of  X  X reenhouse X  for the question. Then, make use of the words and the named entities to search from web knowledge bases. All texts summarized from the searched results are finally combined without redundancy. 2.2 Answer Sentence Acquisition Using Topic Model For a complex question, the accurate answer sentences mostly indicate the complex semantic relations within it, whereas the bag-of-word models that compute the simi-larity between the answers and the questions are clearly insufficient. To this end, some systems [11,12] employ Latent Semantic Analysis(LSA) model to build a se-mantic layer, in which the similarity between documents and words, or documents and documents are capable to compute by using the semantically latent relations. However, the meaning of the decomposition algorithm is indefinite so that the model could lead to an uncontrolled performance for retrieval. For a clear decomposition meaning, Probabilistic Latent Semantic Analysis(PLSA) model is introduced to build a semantic space of underlying topics, in which words and documents can be mapped as vectors. Some QA systems utilize PLSA to improve answer validation, i.e., model-ing languages for document relevance estimation. In our system, we employ PLSA model as the answer sentence retrieval model. Following is the description of the PLSA model in our system. 
Given a sentence set S, a term set W and a topic set Z, the conditional probability of sentence-term P(s, w) can be described as follows: In (1), P(w | z) represents the conditional probability of words in latent semantic lay-Here the count for topic set Z is between 20 and 100. Then the model fits with the EM coming, it is projected to the topic space Z by using EM algorithm. The similarity of the query and each sentence can be acquired by computing the similarity of the prob-abilistic distribution between them in the topic space. 
Our algorithm is described as follows. First, initialize the P(s, w) for each sentence sss s  X  in the retrieved documents(here we make use of a general retrieval model Lucene to search words and named entities extracted in section 2.1 and get the retrieved documents) by using the ratio of the frequency of w in s and in the sentence set; and P(w | z) for each word and P(z | s) for each sentence are iteratively computed by EM algorithm. Then the query built from the question is mapped into the topic documents is computed accordi ng to the formula (2)-(3). query q and each sentence in retrieved documents, and the sentences that the weight-ing values are above a threshold are selected as the answer sentences. For a better performance, we submit each question to Wikipedia and summarizing the results as the additional queries. Then we compute the similarity between each sen-are above the threshold are selected into the answer set. 2.3 Answer Ranking For a better performance of answer validation and ranking, some approaches employ various resources, whereas most of them are language-dependent and less helpful for the complex questions. To implement a general system, our answer ranking model considers two main factors, namely semantically structural and bag-of-word features the sentence are also appear in questions or sentences derived from web knowledge bases. More specifically, although the retrieved sentences have the latent semantic similarity with questions, we should still consider the long distance dependency rela-tions that could probably result in a low score for an answer candidate. On the other hand, a statistical similarity that treats th e sentences as a bag of words could probably balance the impact of the semantic bias. 
For semantic similarity, we consider the similarity of the main semantic roles be-cause they cover most of the meaning of a sentence and easy to acquire. We choose the verb based labeling architecture derived from PropBank, in which  X  X redicate X ,  X  X ubject X ,  X  X bject X  and some modifiers are the core roles for a sentence. For each sen-tence in answer candidate set we only label PRED, A0-A4, AM-LOC and AM-TMP and combine each predicate and the corresponding argument as well as its type of semantic dependency relation to a pair. The structure of a pair is described as follows: If a pair lies in both a sentence and a question, it is viewed as a matched pair of within them. More specifically, every term and its semantic dependency relation should be matched if the pair is matched. Following is the weighting formula that we compute the semantic similarity: denotes the pair in sentence s, while denotes the pair in query . denotes the similarity between and . Here the similarity defines as a boolean value, representing whether the two pairs are full matched. 
For labeling of semantic roles, we utilize a system that we proposed in [10] to ex-tract pairs. The system handles syntactic dependency parsing with a transition-based approach and utilizes MaltParser 1 as the base model. The system also utilizes a Maximum Entropy model to identify predicate senses and classifies arguments. 
For statistical similarity, we simply utilize a cosine similarity to compute it. The weighting formula of our method for answering ranking is as follows:  X  is an adjusting parameter, and q denotes each sentence in the query sentence set described in the previous subsection. According to (5), the sentence that mostly simi-their ranking scores. We select NTCIR-8 CCLQA CS dataset as experimental document collection. The collection contains Chinese questions of five types: Definition(10), Biography(10), Event(20), Relationship(20) and Why(20). The baseline system in our experiments makes use of the same architecture with JAVELIN [13], whereas it does not take answer types into account and simply extracts noun phrases from questions as key terms. For answer extraction and ranking, the system selects sentences that contain key terms from high ranked documents. Table 2 shows the result of the experiment. this paper 0.1933 0.1514 0.2065 0.1697 0.1082 0.1658 baseline 0.1359 0.1178 0.1505 0.1146 0.0508 0.1139 
The results indicate that our approach based on web knowledge bases can improve the overall performance of the complex QA system. We can also see that, results for definition and relationship questions are better than those for other type of questions. It is mainly because the information need of the questions of these two types are sim-plex, while the questions of other types concern more complex relations. Take the biography type as an example, temporal terms almost appear at every biographic text whereas they can not represent a rich semantic concept; the PLSA retrieval model can not have a tight relationship with definitions from web knowledge bases although they could be answers. We also notice that, the system achieves a very low perform-tions in these questions are more complex than others. Actually, most answers logically related with the question rather than synonyms or shallow semantic ones. For example, when asking  X  X hy do the Shenzhou spacecrafts always launched in cold season X , the answer is probably like a reasoning chain: The spacecraft is recovered by the survey vessels.  X  The survey vessels are mainly located in the southern hemisphere.  X  The recovery task befits in summer.  X  The climate is different between the southern and the northern hemisphere.  X  For the sake of recovery, Shenzhou spacecr aft always launched in cold season. However, most keywords, i.e., southern hemisphere, survey vessels and recovery task, do not appear at the question. Alternately, sentences in answers have logical(more specifically, casual) relations that should be inferred from one to another. The answer to the question can be acquired only if all the relative events are chained through in-ference. Hence the inference models should be considered for a better performance of  X  X hy X  questions. 
We also investigate performance of each part in our system. ts means the summari-zation method in our approach mentioned in Section 2.1. For sentence retrieval, two models, which are based on VSM and PLSA mentioned in Section 2.2, are also in-volved in the experiments. For answer ranking, as a comparison, we invoke the ap-proach of the baseline system that rank sentences by key terms to replace our method proposed in Section 2.3(sem). Table 3 shows the result of the experiments. 
From Table 3 we can see that when using VSM model to replace PLSA, the aver-age performance decreases 3.61%. Noticeab ly, the average performance decreases 2.6% by using key term method to replace the method of computing semantic plus statistical similarity. Data show the facts that: (1) effective summarizing method from Wikipedia improves ordinary QA systems to a better performance; (2) a large number of synonyms as well as data sparse phenomenon exist in texts, thus topic based mod-els can be more fit than centroid-based models for definition QA systems; (3) a sen-tence that just contains key terms could no t be considered as an answer unless it has semantic pattern features that the question bears. In this paper, we propose an approach to acquire answer sentences from documents effectively. The main contribution to our approach lies in two folds: 1) a PLSA model based answer acquisition approach is introduced for complex questions; and 2) se-mantic and statistical information are comb ined in the answer ranking model for a better performance. Although a general performance is achieved according to the official evaluation results, we still face two important problems: 1) with the improve-ment of the complexity of questions, deep knowledge in them should be acquired by some inference mechanisms; 2) more refined nugget extraction model are needed to improve the precision of the system. Acknowledgements. This research is supported by Natural Science Foundation of China(Grant Nos. 90820005, 61070082) and the Post-70s Scholars Academic Devel-opment Program of Wuhan University. 
