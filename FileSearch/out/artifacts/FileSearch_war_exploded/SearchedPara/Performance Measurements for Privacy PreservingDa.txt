 In this paper, we address issues related to the performance measurements of privacy preserving data mining techniques. The objective of privacy preserving data mining is to enable data mining without violating the privacy of data being mined.
 and numerous data providers. Each data provider holds one private data point. The data miner performs data mining tasks on the (possibly perturbed) data provided by the data providers. A typical example of this kind of system is online survey, as the sur-vey analyzer (data miner) collects data from thousands of survey respondents (data providers). Most existing privacy preserving algorithms in such system use an ran-domization approach which randomizes the original data to protect the privacy of data providers [1, 2, 3, 4, 5, 6, 8].
 accuracy of data mining results and the privacy protection of sensitive data. Our contri-bution can be summarized as follows.  X  On accuracy side, we address the problem of previous measures and propose a new  X  On privacy protection side, we show that a tacit assumption made by previous mea-Let there be n data providers C 1 ,...,C n and one data miner S in the system. Each data provider C i has a private data point (e.g., transaction, data tuple, etc.) x i . We consider the original data values x 1 ,...,x n as n independent and identically distributed (i.i.d.) variables that have the same distribution as a random variable X . Let the domain of X (i.e., the set of all possible values of X )be V X , and the distribution of X be p X .As such, each data point x i is i.i.d. on V X with distribution p X .
 categories. One category is honest data miners. These data miners always act honestly in that they only perform regular data mining tasks and have no intention to invade pri-vacy. The other category is malicious data miners. These data miners would purposely compromise the privacy of data providers. To protect the privacy of data providers, countermeasures must be implemented in the data mining system. Randomization is a commonly used approach. It is based on an assumption that accurate data mining results can be obtained from a robust estimation of the data distribution [2]. Thus, the basic idea of randomization approach is to distort individual data values but keep an accurate estimation of the data distribution. can be considered as a two-step process. In the first step, each data provider C i perturbs its data x i by applying a randomization operator R (  X  ) on x i , and then transfers the randomized data R ( x i ) to the data miner. We note that R (  X  ) is known by both the data providers and the data miner. Let the domain of R ( x i ) be V Y . The randomization operator R (  X  ) is a function from V X to V Y with transition probability p [ x  X  y ] . Existing randomization operators include random perturbation operator [2], random response operator [4], MASK distortion operator [8], and  X  X elect-a-size X  operator [6]. algorithm on the aggregate data, which intends to reconstruct the original data distribu-tion from the randomized data. Then, the honest data miner performs the data mining task on the reconstructed distribution. Various distribution reconstruction algorithms have been proposed [1, 2, 6, 4, 8]. Also in the second step, a malicious data miner may invade privacy by using a private data recovery algorithm. This algorithm is used to recover individual data values from the randomized data supplied by the data providers. pability of both constructing the accurate data mining results and protecting individual data values from being compromised by the malicious data miners. In previous studies, several accuracy measures have been proposed. We classify these measures into two categories. One category is application-specified accuracy measures [8]. Measures in this category are similar to those in systems without privacy concern and are specific to a data mining task (e.g., classification, association rule mining, etc.). The other category is general accuracy measures. Measures in this category can be applied to any privacy preserving data mining systems based on the randomization ap-proach. An existing measure in this category is information loss measure [1], which is in proportion to the expected error of the reconstructed distribution.
 system designers to choose the optimal randomization operator. As we can see from the privacy preserving data mining process, the randomization operator has to be deter-mined before any data is transferred from the data providers to the data miner. Thus, in order to reach its goal, a performance measure must be estimated or bounded without any knowledge of the data being mined. As we can see, the application-specified accu-racy measures depend on both the reconstructed data distribution and the performance of data mining algorithm. The information loss measure depends on both the origi-nal distribution and the reconstructed distribution. Neither measure can be estimated or bounded when the original data distribution is not known. Thus, previous measures cannot be used by the system designers to choose the optimal randomization operator. given the number of randomized data points, the effective sample size is in proportion to the minimum number of original data points needed to make an estimate of the data dis-tribution as accurate as the distribution reconstructed from the randomized data points. The formal definition is stated as follows.
 Definition 1. Given randomization operator R : V X  X  V Y ,let  X  p be the maximum likelihood estimate of the distribution of x i reconstructed from R ( x 1 ) , ... , R ( x n ) . Let  X  p ( k ) be the maximum likelihood estimate of the distribution based on k variables randomly generated from the distribution p X . We define the effective sample size r as the minimum value of k/n such that where D Kol is the Kolmogorov distance [7], which measures the distance between an estimated distribution and the theoretical distribution 1 .
 the accuracy of the reconstructed distribution. We now show that the effective sample size can be strictly bounded without any knowledge of p X .
 Theorem 1. Recall that p [ x  X  y ] is the probability transition function of R : V X  X  V
Y . An upper bound on the effective sample size r is given as follows. Due to space limit, please refer to [9] for the proof of this theorem. In previous studies, two kinds of privacy measures have been proposed. One is infor-mation theoretic measure [1], which measures privacy disclosure by the mutual infor-This measure was challenged in [5], where it is shown that certain kinds of privacy disclosure cannot be captured by the information theoretic measure. The other kind of privacy measure can be used to solve this problem [5, 10, 2]. In particular, the pri-vacy breach measure [5] defines the level of privacy disclosure as max x,x  X  V X p [ x  X  y ] /p [ x  X  y ] for any given y  X  V Y . This measure captures the worst case privacy disclosure but is (almost) independent of the average amount of privacy disclosure. in different circumstances. As such, the privacy protection measure should depend on two important factors: a) the privacy protection mechanism of the data providers, and b) the unauthorized intrusion technique of the data miner. However, previous measures do not follow this principle. Instead, they make a tacit assumption that all data miners will use the same intrusion technique. This assumption seems to be reasonable as a (rational) data miner will always choose the intrusion technique which compromises the most private information. However, as we will show in the following example, the optimal intrusion technique varies in different circumstances. Thereby, the absence of consideration of intrusion techniques results in problems of privacy measurement. Example 1. Let there be V X = { 0 , 1 } . The original data x i is uniformly distributed on V
X . The system designer needs to determine which of the following two randomization operators, R 1 and R 2 , discloses less private information.

R 1 ( x )= oretic measure, R 2 discloses less privacy. However, R 2 discloses more privacy due to the privacy breach measure. The reason is that if the data miner receives R 2 ( x i )=1 , then it can always infer that x i =1 with probability of 1 . We now show that whether R 1 or R 2 discloses more private information actually depends on the system setting. In particular, we consider the following two system settings. 1. The system is an online survey system. The value of x i indicates whether a survey 2. The system consists of n companies as the data providers and a management con-we can see, R 1 discloses the original data value with probability of 0 . 7 , which is greater than that of R 2 ( 0 . 501 ). Thus, R 2 is better than R 1 in the privacy protection perspective. the data providers. The reason is that the loss from an incorrect estimate of x i is too high to risk. As we can see, when R 1 is used, the expected net benefit from an unauthorized intrusion is less than 0 . However, the data miner will perform the intrusion when R 2 is used. The reason is that when R 2 ( x i )=1 , the data miner has a fairly high probability ( 99% ) to make a successful investment. If a randomized data R 2 ( x i )=0 is received, the data miner will simply ignore it. As such, in this case, R 1 is better than R 2 in the privacy protection perspective.
 sion techniques in different system settings (in the above example, there is an intrude-or-not selection), which will result in different performance of randomization operators. Thus, the system setting has to be considered in the measurement of privacy disclosure. privacy measure, we propose a game theoretic framework to analyze the strategies of the data miner (i.e., privacy intrusion technique). Since we are studying the privacy protection performance of the randomization operator, we consider the randomization operator as the strategy of the data providers.
 between the data providers and the data miner. There are two players in the game. One is the data providers. The other is the data miner. Since we only consider the privacy measure, the game is zero-sum in that the data miner can only benefit from the violation of privacy of the data providers. Let S c be the set of randomization operators that the data providers can choose from. Let S s be the set of the intrusion techniques that the data miner can choose from. Let u c and u s be the utility functions (i.e., expected bene-fits) of the data providers and the data miner, respectively. Since the game is zero-sum, we have u c + u s =0 . We remark that the utility functions depend on both the strategies of the players and the system setting.
 given a certain randomization operator, the data miner always choose the privacy in-trusion technique which maximizes u s . Given a certain privacy intrusion technique, the data providers always choose the randomization operator which maximizes u c .Wenow define our privacy measure based on the game theoretic formulation.
 Definition 2. Given a privacy preserving data mining system G S s ,S c ,u s ,u c ,we define the privacy measure l p of a randomization operator R as where L 0 is the optimal privacy intrusion technique for the data miner when R is used by the data providers, u c is the utility function of the data providers when R and L 0 are used.
 As we can see, the smaller l p ( R ) is, the more benefit is obtained by the data miner from the unauthorized intrusion. Let  X  be the ratio between the benefit obtained by a malicious data miner from a correct estimate and the loss of it from an incorrect estimate. A useful theorem is provided as follows.
 Theorem 2. Let there be max x 0  X  V X Pr { x i = x 0 } = p m in the original data distribu-tion. We have l p ( R )=0 if the randomization operator R : V X  X  V Y satisfies Please refer to [9] for the proof of this theorem. In this paper, we establish the foundation for the measurements of accuracy and pri-vacy protection in privacy preserving data mining. On accuracy side, we address the problem of previous accuracy measures and solve the problem by introducing an ef-fective sample size measure. On privacy protection side, we present a game theoretic formulation of the system and propose a privacy protection measure based on the for-mulation. Our work is preliminary, and there are many possible extensions. We are cur-rently investigating using our performance measurements to derive the optimal trade-off between accuracy and privacy which can be achieved by the randomization ap-proach.

