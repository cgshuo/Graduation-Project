 The e ff ectiveness of retrieval systems is often justi-fied by benchmark test collections. A standard test collection consists of lots of documents, a set of in-formation needs, called topics and human judgment about the relevance status of each document for a topic. Nowadays, it is relatively easy to gather huge set of millions of documents and hundreds of topics. The key obstacle for forming large sized test col-lections lies therefore in the topic assessment pro-cedure. Assessing the whole document sets is un-feasible, even for small sized collection of 800,000 documents (Voorhees and Harman, 1999). In order to keep the assessment process practical, one often selects a certain number of documents for judgment. This is called (document) pooling and the outcome the pool or the qrels ( q uery r elevance s et). The col-lected documents are then judged by humans, doc-uments outside the pool are assumed non relevant. A representative pool is therefore essential to the whole evaluation process.

This paper proposes a method to form the assess-ment set with the support of machine learning algo-rithms. Based on relevance judgments of relatively shallow pools, a ranking algorithm will attempt to give priority for relevant documents so that the as-sessment set can be fixed at a feasible size without skewing the system evaluation result. The judgment process is indeed kept as much subjective-free as possible: the first relevance feeback step is designed appropriately so that the assessor cannot give any bias towards any particular rank or any system, the learning process is completely transparent to the as-sessors and parameters of the ranking function are collection-tailored rather than exported from previ-ous collections.

The method will then be evaluated on TREC ad-hoc collections. Results from our comprehensive experiment confirm that the qrels generated by our method are much more representative than those of the same size by the TREC method. The outcome qrels is substantially smaller, so much cheaper to produce than the o ffi cial TREC qrels, yet their con-clusions about system e ff ectiveness are quite com-patible.

The remaining of this paper is organized as fol-lows. We review related work in Section 2. Sec-tion 3 presents the general framework of apply-ing machine learning techniques to forming test collections. We also give a brief introduction about RankBoost (Freund et al., 2003) and Rank-ing SVM (Joachims, 2002b), the two learning algo-rithms used in our experiment. Section 4 introduces data sets and experimental setup. Section 5 is ded-icated to present experimental results according to di ff erent evaluation criteria. Precisely, Section 5.1 shows the capacity of small pools on identifying rel-evant documents and Section 5.2 illustrates their im-pact on system comparison; Section 5.3 presents sta-tistical validation tests. We conclude and discuss perspectives in Section 6. 2.1 TREC methodology Since the seminal work of test collection forming in 1975 (Sparck Jones and Van Rijsbergen, 1975), pooling has been outlined as the main approach to form the assessment set. The simple solution of round robin pooling from di ff erent systems pro-posed in that report has been adopted in most exist-egy as TREC-style pooling. To have the assessment set, from submissions (restricted length L = 1000 for most TREC tracks), only n top documents per submission are pooled. Despite di ff erent technical tricks to control the final pool size such as gathering only principal runs or reducing the value of n , the assessment procedure is still quite time-consuming. In TREC 8 ad-hoc track, for example, despite lim-iting the pool depth n at 100 and gathering only 71 of 129 submissions, each assessor has to work with approximately 1737 documents per topic (precisely, between 1046 and 2992 documents). Assuming that it takes on average 30 seconds to read and judge a document, the whole judgment procedure for this topic set can therefore only terminate after a round-the-clock month. Meanwhile, a simple analysis on the ad-hoc collections from TREC-3 to TREC-8 re-vealed that there are on average 94% documents judged as non relevant. Since most of existing ef-fectiveness measures do not take into account these non relevant documents, it would be bettter to not waste e ff ort on judging non relevant documents pro-vided that the quality of test collections is always conserved. Several advanced pool sampling meth-ods have been proposed but due to some common drawbacks, none of them has been used in practice. 2.2 Topic adaptive pooling Zobel (Zobel, 1998) forms the shallow pools accord-ing to the TREC methodology. When there are enough judged documents (up to the set of 30 top documents per run in his experiment), an extrapo-lation function will then be estimated to predict the number of unpooled relevant documents. The idea is to judge more documents for topics that have high potential to have relevant documents else. Carterette and Allan (Carterette and Allan, 2005) have recently replaced that extrapolation function by statistical tests to distinguish runs. This method produced in-teresting empirical outcomes on TREC ad-hoc col-lections, lack however a sound theoretical basis and is clearly of very high complexity due to iterative statistical tests of every run pairs. Furthermore, this incremental / online pooling approach raises a major concern about the unbiasness requirement from the human judgment as the assessors know well that documents come later are of lower ranks, thus of lower relevance possibility. 2.3 System adaptive pooling Cormack et al. (Cormack et al., 1998) propose the so-called Move-To-Front (MTF) heuristic to give priority for documents based on the correspond-ing system performance. In their experiment, the latter factor has been simply the number of non relevant documents this system has intro-duced to the pool since the last relevant document. Aslam et al. (Aslam et al., 2003) formulate this pri-ority rule by adopting an online learning algorithm called Hedged (Freund and Schapire, 1997).

Our method relies on this idea of pushing ahead relevant documents by weighting retrieval systems. There are however two major di ff erences. Whilst all aforementioned proposals favor online paradigm with a series of human interaction rounds, our method works in batch mode. We believe that the latter is more suitable for this task since it elimi-nates as much as possible the bias introduced by human assessor towards any document. Moreover, the batch mode enables us to exploit intuitively the inter-topic relationship what is not the case of on-line paradigm. The second di ff erence lies in the way of estimating the ranking function. It is widely ac-cepted that machine learning techniques can deliver more reliable model on previously unseen data given much less training instances than any classical statis-tical techniques or expert rules can. 2.4 Generate pseudo assessment set Several evaluation methodologies, especially for web search engines, have been proposed to evaluate systems without relevance judgment. These propos-als can be grouped into two main categories. The first (Soboro ff et al., 2001; Wu and Crestani, 2003; Nuray and Can, 2006) exploits internal information of submissions. The second (Can et al., 2004; Joachims, 2002a; Beitzel et al., 2003) benefits ex-ternal resources such as document and query con-tent, or those of web environment. We skip the sec-ond category since these resources are not available in generic situations.

Soboro ff et al. (Soboro ff et al., 2001) sam-ple documents of a shallow pool (top ten documents returned by retrieval systems) based on statistics from past qrels. Wu and Crestani (Wu and Crestani, 2003), Nuray and Can (Nuray and Can, 2006) adopt metasearch strategies on document position. A certain number of top out-come documents will then be considered as relevant without any human verification. Di ff erent voting schemes have been tried in the two aformentioned papers. Their empirical experiment illustrated how the quality of these pseudo-qrels is sensible to the chosen voting scheme and to other parameters such as the pool depth or the diversity of systems used for fusion. They also confirm that pseudo -qrels are often unable to identify best systems.

In sum, the thorough literature review confirmed the importance of relevance assessment sets in IR evaluation yet the lack of an appropriate solution to have a reliable set given a moderate amount of judg-ment resource. 3.1 General framework Let M denote the topic set size available for the training purpose, N the number of participating sys-tems, k 1 the pool depth to get the training data from any participating system and K the final pool size. The training process consists of two main steps. Firstly, for each training topic, k 1 first documents of all N systems are gathered and the assessors are asked to assess all of these documents. Let T denote the outcome of this assessing step on all M topics. From the information of T , a function f will then be learned which assigns to each document a value cor-responding to its relevance degree for a given query.
At the usage time, for each given topic, the whole retrieved list of N systems will be fused. These doc-uments will then be sorted in the decreasing order of their values according to f and the K top documents will be sent to the assessor for judgment. This last set of judgements will be the qrels used for the sys-tem evaluation.

In the training framework, it is clear that the sec-ond step plays the major role. An e ff ective scoring function can substantially save the workload at the last assessment step. We will now focus on methods for estimating such scoring function. 3.2 Document ranking principle The scoring function f can be estimated in di ff er-ent ways as seen in the last section. In this study, we adopt the learning-to-rank paradigm for estimat-ing this scoring function. The principle of document ranking will be sketched in this section. The next sub-section will introduce the two specific ranking algorithms used in our experiment.

A ranking algorithm aims at estimating a function which describes correctly all partial orders inside a set of elements. An ideal ranking in information re-trieval must be able to place all relevant documents above non relevant ones for a given topic. The prob-lem can be described as follows. For each topic, the document collection is decomposed into two disjoint sets S tively) documents, R and NR are their cardinality. A ranking function H ( d ) assigns to each document d of the document collection a score value. We seek for a function H ( d ) so that the document ranking generated from the scores respect the relevance rela-tionship, that is any relevant document has a higher score than any non relevant one. Let  X  d  X  d  X   X  sig-nify that d is ranked higher than d  X  . The learning objective can therefore be stated as follows. d
There are di ff erent ways to measure the ranking error of a scoring function h . The natural criterion might be the proportion of misordered pairs (a rele-vant document is below a non relevant one) over the total pair number R . NR . This criterion is an estimate of the probability of misordering a pair P ( d where ~  X   X  is 1 if  X  holds, 0 otherwise; D ( d scribes the importance of the pair in consideration, it will be uniform 1 known.

In practice, we have to average RLoss over the training topic set. This can be done by either macro -averaging at topic level or micro -averaging at docu-ment pair level. For presentation simplification, this operation has been implicit. 3.3 Discriminative ranking algorithms Since RLoss is neither continuous nor di ff erentiable, its direct use as a training criterion raises practical di ffi culties. Also, in order to provide reliable predic-tions on previously unseen data, the prediction error of the learning function has to be bounded with a significant confidence. For both practical and theo-retical reasons, RLoss is then often approximated by a smooth error function.

In this study, we will explore the per-formance of two ranking algorithms, they are RankBoost (Freund et al., 2003) and Ranking SVM (Joachims, 2002b). As far as we know, these algorithms are actually among a few state-of-the-art ranking learning algorithms whose convergence and generalization properties have been theoretically proved (Freund et al., 2003; Joachims, 2002b; Cl  X emenc  X on et al., 2005). 3.3.1 RankBoost
RankBoost (aka RBoost) (Freund et al., 2003) re-turns a scoring function for each document d by min-imizing the following exponential upper bound of the ranking error RLoss (Eq. (2)): This is an iterative algorithm like all other boosting methods (Freund and Schapire, 1997). The global ranking function of a document d is a linear combi-nation of all base functions H ( d ) = P T each iteration t , a new training data sample is gener-ated by putting more weight D ( .,. ) on di ffi cult pairs ( d even be chosen among the features used to describe documents) and the weight  X  t is estimated in order to minimize the ELoss at that iteration.

RBoost has virtues particularly fitting the pool-ing task. First, it can operate on relative values. Second, it does not impose any independence as-sumption between combined systems. Finally, in the case of binary relevance judgment which usu-ally occurs in IR, there is an e ffi cient implementa-tion of RBoost whose complexity is linear in terms of the training instance number (cf: the original text (Freund et al., 2003)). 3.3.2 Ranking SVM
Ranking SVM (Joachims, 2002b), rSVM for short, is a straightforward adaptation of the max-margin principle (Vapnik, 2000) to pairwise object ranking. The score function is often assumed to be linear in some feature space, that is H ( d ) = w T  X  ( d ) where w is the vector of weights to be estimated and  X  is a feature mapping. The max-margin approach minimizes the following approximation of RLoss: rSVMLoss ( H ) = max n 1 + H ( d for all pairs ( d ling the complexity of function space described via the norm of vector w for generalization objective.
Notice that rSVM does not explicitly support rank values as does RBoost. Nevertheless, we will see later that the discriminative nature allows rSVM to work quite well on features merely deduced from rank values. Its behavior di ff erence is in fact ignor-able in comparison with RBoost. Our method is general enough to be applicable to any ad-hoc retrieval information task where pooling could be useful. In this paper, we will however fo-cus on TREC traditional ad-hoc retrieval collections. Experiments have been performed on the three cor-pora TREC-6, TREC-7 and TREC-8. Statistics about the number of runs, of judgments, of rele-vant documents are shown in Tab. 1. Due to limit of space, we will detail results on the TREC-8 case and only mention the results on the two others. Table 1: Information about three TREC ad-hoc col-lections. The three last columns are averaged over the topic set size (50 topics / collection).
Training data is gathered from the top five an-swers of each run. The pool depth of five has been arbitrarily chosen to have both su ffi cient training data and to eliminate potential bias from assessors towards a particular system or towards early iden-tified answers while judging a shallow pool. Fur-thermore, this training data set is large enough for testing the ranking algorithm e ffi ciency.

Each document is described by an N -dimensional feature vector where N is the number of participat-is a function of its position in the retrieved list, ties are arbitrary broken. A document at rank i is as-signed a feature value of ( L + 1  X  i ) where L is the TREC limit of submission run ( L is usually set up at 1000). Documents outside submission runs receive the zero feature value (i.e. it is assumed to be at rank ( L + 1)). For implementation speed, the input for rSVM is further scaled down to the interval [0 , 1].
Due to the small topic set size, we use a leave-one-out training strategy: a model will be trained for each topic by using judgments of all other top-ics. The training data set size is presented in the last column of Tab. 1. The workload for training dataset does not exceed the e ff ort for assessing 5 topics in the full pool of TREC.
 We adopt the e ffi cient RBoost version for bi-nary feedback and binary base functions h t (cf. (Freund et al., 2003)), boosting is iterated 100 times and we impose positive weighting for all coef-ficients  X  t .

The non-interpolated average precision (MAP) has been chosen to measure system perfor-mance 5 . This metric has been shown to be highly stable and reliable with both small topic set size (Buckley and Voorhees, 2000) and very large document collec-tions (Hawking and Robertson, 2003).
 RBoost and rSVM pools will be compared to the TREC-style pools of the same size. We also include  X  X ocal MTF X  (Cormack et al., 1998) in the experi-ment. The  X  X lobal MTF X  has been shown to slightly outperform the local version in the aforementioned paper. However, we believe that the global mode is merely for demonstration but unlikely practical of online judgment since it insists that all queries are judged simultaneously with a strict synchroni-sation among all assessors. Hereafter, for simplic-ity, the TREC-style pool of the first n documents retrieved by each submission will be denoted by Depth-n , the equivalent pool (with the same aver-age final pool size m over the topic set) produced by RBoost, rSVM or MTF will be RBoost-m , rSVM-m or MTF-m respectively. In all figures in the next sec-tion, the abscissa denotes the pool size m and values of n will be present along the Depth-n curve. This section will examine small pools produced ei-ther by the TREC method or by RBoost / rSVM / MTF from two angles: their pooling performance and their influence on system comparison result. 5.1 Identify relevant documents Fig. 1 shows the ratio of relevant documents re-trieved by di ff erent pooling methods (i.e. the re-call). The curves obtained by RBoost and rSVM are quite similar and much higher than that by TREC methodology. The curve of MTF is in the middle of RBoost / rSVM and Depth-n at the beginning and then catches that of RBoost at the pools of about 600 documents. Figure 1: Along the incrementally enlarged pools: relevant documents identified in comparison with the full assessment set. 5.2 Correlation of system rankings Once the pool is obtained by a given method, the assessor will give relevance judgment for all docu-ments of that pool, called qrels for the outcome. This qrels will be used as the ground truth to measure ef-fectiveness of a retrieval system. Figure 2: Kendall X  X   X  correlation of system ranking according to di ff erent qrels methods in comparison with that produced by the full assessment set.
The simplest way to compare di ff erent sys-tems is to sort them by the decreasing e ff ective-ness values. The correlation of each two sys-tem rankings will then be quantified through a correlation statistic. In this study, we follow TREC convention (Buckley and Voorhees, 2004; Carterette and Allan, 2005), that is taking the 0.9 value of Kendall X  X   X  as the su ffi cient threshold to conclude that the di ff erence of two system rankings is ignorable. We compare here the system ranking obtained by the o ffi cial TREC qrels with those by Depth-n where n varies from 1 to 100. We then re-place Depth-n by RBoost-m , rSVM-m and MTF-m . The results are shown in Fig. 2 for TREC-8 and in Tab. 2 for the 7 first pool depths. We observe from the figure the similar order of pooling methods as seen in the previous section. The MTF curve meets those of RBoost and rSVM from qrels of more than 400 documents. The results obtained on the two col-lections of TREC-6 and TREC-7 are in line with those observed on TREC-8 (Tab. 2).

It is clear that system ranking correlation quan-tified by any rank correlation statistics provides necessary but not su ffi cient information about sys-tem comparison. Ranking systems by their sam-ple means is indeed the simplest way with at least two implicit assumptions. First, runs have simi-lar variances , this usually does not hold in practice even after discarding poorest runs. Second, all run swaps have the same importance without taking into account their statistically significant di ff erence and their positions in the system ranking. In practice, swap of adjacent systems does not make much sense if they are not significantly di ff erent to each other according to statstical tests. The next section will be devoted to further statistical validations. 5.3 Statistical Validations 5.3.1 Significant di ff erence detection
We register for a given qrels all system pairs which are significantly di ff erent on the topic set. The quality of a qrels can be measured by the similarity of this significant di ff erence detection in compari-son with that obtained by the o ffi cial TREC qrels. We carry out the paired t-test for each pair of runs with 95% significance level. The recall and the false alarm rate of these detections are shown in Fig. 3. In terms of recall, RBoost and rSVM qrels are much more better than its TREC-style counterparts and MTF is in the middle. In terms of false alarm rate, there are some changes concerning rSVM and MTF. Precisely, rSVM at small qrels of less than 100 doc-uments is the best whilst that is MTF qrels of more than 150 documents. 5.3.2 Tukey grouping based on their statistical di ff erence. We concentrate RBoost-m. Figure 3: Comparing qrels of RBoost-m , rSVM-m , MTF-m and Depth-n in terms of pairs of signifi-cantly di ff erent systems: recall (top) and false alarm rate (bottom) particularly on the top group , called group A which consists of runs on which there is not enough evi-dence to conclude that they are statistically signifi-cantly worse than the top run . In practice, this fig-ure will be meaningful if it is around 10 (one often says about the top 10 runs). It will however become meaningless if the group A is too large, for exam-ple contains more than half of systems in consider-ation. Note that Tukey test relies on the assump-tion of Equality of Variances. This requirement can not be completely satisfied in practice, even after Figure 4: Cardinality of group A (95% confidence level) after the arcsine-root data transformation. some data transformation such as arcsine-root or us-ing rank values.

The size of group A on TREC-8 collection is shown in Fig. 4. We observe from that figure the stability of the two curves of RBoost and rSVM, this implies that the two qrels RBoost-35 and rSVM-35 which have both satisfied the 0.9 requirement of Kendall X  X   X  can replace the o ffi cial TREC qrels. The e ff ort saving is therefore a factor of 50 (if ignoring the cost of training data set preparation) and of 10.5 otherwise. MTF needs qrels of at least 168 docu-ments to produce comparable group A X  X  with that of the o ffi cial TREC qrels. The Depth-n pools how-ever should not be recommended with less than 1000 documents in total (i.e. pooling more than 40 top documents per run). This study has well illustrated that two algorithms of RBoost and rSVM are quite suitable for qrels con-struction task. The final qrels are not only small enough to ask for human judgment but also result in reliable conclusion about system e ff ectiveness in comparison with the counterpart of TREC method-ology and that of MTF.

It is necessary to include other metasearch meth-ods for further study. This will allow us to validate not only the impact of the metasearch training prin-ciple based on pairwise ranking error RLoss but also the capacity of automatic feature selection of the two ranking algorithms used in this paper.

This method needs to be further verified on chal-lenging ad-hoc retrieval scenarios such as Terabyte, Web Topic Distillation or Robust Tracks in TREC context. The hardness of these scenarios involves two main issues. First, the number of document judged relevant varies largely across the whole topic set. Second, some topics might even have no rele-vant document in shallow pools. These matter any statistical inference on shallow pools.
 Acknowledgement The authors thank M.-R. Amini, B. Piwowarski, J. Zobel and the anonymous re-viewers for their thorough comments. We ac-knowledge NIST to make accessible the TREC submissions. This work was supported in part by the IST Programme of the European Commu-nity, under the PASCAL Network of Excellence, IST-2002-506778. The publication only reflects the authors X  views.

