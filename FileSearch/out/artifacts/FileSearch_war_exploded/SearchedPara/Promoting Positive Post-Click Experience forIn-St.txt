 Click-through rate (CTR) is the most common metric used to assess the performance of an online advert; another per-formance of an online advert is the user post-click experience . In this paper, we describe the method we have implemented in Yahoo Gemini 1 to measure the post-click experience on Yahoo mobile news streams via an automatic analysis of ad-vert landing pages. We measure the post-click experience by means of two well-known metrics, dwell time and bounce rate . We show that these metrics can be used as proxy of an advert post-click experience, and that a negative post-click experience has a negative effect on user engagement and future ad clicks. We then put forward an approach that analyses advert landing pages, and show how these can affect dwell time and bounce rate. Finally, we develop a prediction model for advert quality based on dwell time, which was de-ployed on Yahoo mobile news stream app running on iOS. The results show that, using dwell time as a proxy of post-click experience, we can prioritise higher quality ads. We demonstrate the impact of this on users via A/B testing. H.4.0 [ Information Systems Applications ]: General Mobile advertising; Native advertising; Advertising quality; Post-click experience; Dwell time prediction One of the main requirements driving the development of Yahoo Gemini, the Yahoo advertising platform, has been  X  c  X  that of caring not only about the sheer number of clicks on ads but also about the user experience. In our case, user experience means to show advertisements 2 that will not dis-courage users to continue using the platform or, even more challenging, to engage them to click more on ads.

Following from this, we put forward an approach that analyses ad landing pages, and show how these can affect dwell time and bounce rate . Then we develop a model based on dwell time for predicting the quality of native Yahoo Gemini ads, which is currently deployed on Yahoo Gemini. We chose to deploy the version predicting dwell time as we wanted to serve ads on which users spend time, meaning not only they promote a positive post-click user experience but also have the potential of leading to a  X  X onversion X  (e.g., making a purchase). Analyzing one month data through A/B testing, we see that returning high quality ads, as mea-sured in terms of the ad post-click experience, not only in-creases click-through rates by 18%, it has a positive effect on users, as we observe an increase in dwell time (+30%) and a decrease in bounce rate (-6.7%).

As we stated above, we focus in this paper on showing how our approach works on native ads. Let us now describe what a native ad is.

Feed-based layouts, or streams , are becoming an increas-ingly common layout in many applications, and a predomi-nant interface in mobile applications. In-stream advertising has been increasingly emerging as a popular online advertis-ing because it offers a user experience that fits nicely with that of the stream [9]. It is often referred to as native adver-tising . 3 In-stream (or native) ads have an appearance similar to that of the items in the stream, but are clearly marked with a  X  X ponsored X  label or a currency symbol e.g.  X $ X  to in-dicate that they are in fact adverts. Major news sites such as the New York Times, Yahoo News, and the Guardian in-tegrate ads into their streams. Yahoo launched Gemini in 2013, an unified ad marketplace for mobile search and native advertising. Many of the ads served by Yahoo Gemini are in-streams ads. In this work, we focus on such ads served by Gemini on one of Yahoo mobile news streams.
We refer to the ad impression shown within the stream as the creative of the ad. A user decides if he or she is inter-ested in the ad content by looking at its creative, depending on how attractive it is to users. After a user clicks on the creative he or she is redirected to the ad landing page , which is either a web page specifically created for that ad, or the advertiser homepage [2]. It is well known that the way user experiences a landing page, the ad post-click experience , is an important factor that, if properly measured, can help differentiating between a high quality and a low quality ad. Indeed, when the post-click experience is negative users may tend to click less on ads in the future, if not stop accessing the service at all with disruptive consequences in the overall number of visitors and therefore in total revenue.

Looking at the post-click experience is particularly impor-tant in the context of Gemini in-stream ads because (1) the creatives have mostly the same look and feel, as opposed to display ads, and what differs mostly is their landing pages, and (2) accounting for the post-click experience has led to increased performance in many areas, including sponsored search. In this paper, we develop and deploy a method to identify high quality ads, by analysing their landing pages and relate these to the ad post-click experience. Our focus is on mobile in-stream advertising as offered by Yahoo Gemini.
We measure the post-click experience through well-known engagement metrics, dwell time and bounce rate . Dwell time is the time between users clicking on an ad creative until re-turning to the stream; bounce rate is the percentage of  X  X hort clicks X  (clicks with dwell time less than a given threshold). We demonstrate that they can be used as proxy of ad post-click experience. We also show that a negative post-click experience has a negative effect on user engagement and, in particular, future ad clicks. Online advertising. Online advertising has been exten-sively studied in the context of display advertising [1, 22] and sponsored search [3, 23] for desktop users. Studies have been mostly focussed on predicting how an ad will perform according to various effectiveness measures [1, 12, 24].
Many efforts have been devoted to improve the match-ing between the textual content of web queries (in spon-sored search) or web pages (in display advertising) and the ad textual content (creative and/or bid phrases and title). Most works aim at categorizing the ad textual content within some taxonomy and then matched it against the query cat-egory [2, 3] or a web page [5, 14, 18] where the ad could be shown. In particular, [2, 3] studied the effect of landing page categorization on user experience by defining a small taxonomy of landing pages. A year later, [6] studied the effect of landing page features in improving ad relevance in textual advertising. They show that augmenting the ad textual features with features from the content of the page increases relevance metrics. Finally, [11] focused on auto-matically categorizing display ad images into a taxonomy of relevant interest categories. They demonstrate the effective-ness of using image and textual features extracted from the ad landing page in predicting the category of the display ad. We also exploit landing page features to predict the quality of ads with respect to the post-click experience.

Users spend an increasing amount of their time online through their mobile devices. This presents unique opportu-nities for advertisers interested in promoting their products beyond the desktop. Previous studies have investigated the degree in which mobile advertising is accepted by users [4] and how users perceive display advertisements on mobile [8]. Findings suggest that personal relevance of the ad is not as important as matching the ad topics to the content of the page, and that utility, context and trust are crucial for accep-tance of mobile advertising. Efforts have been put in build-ing models to predict when to show an ad [19, 20]. Finally, as highlighted in [9, 21], the dominance of the feed-based structure on mobile makes pop-ups and banners impractical, whereas in-stream ads provide an optimal format as they are seamlessly incorporated to the main feed, thereby promot-ing relatively similar experience across all ads. This is not the same for the ad landing pages, as advertisers have total freedom in how they design these, which can vary greatly in terms of quality. In this paper, we study how an ad post-click experience, as measured with dwell time and bounce rate, can inform about the quality of its landing page. Ad quality measures. Several measures have been pro-posed to evaluate the  X  X erformance X  of an ad in terms of the user experience. The most common measure is the ad click-through rate (CTR), which is the number of times the ad was clicked out of the number of times it has been shown (num-ber of ad impressions) [1]. The higher the CTR the better the ad is considered to perform; it attracts the users to click on it. However, CTR does not account for how users experi-ence the ad when they land on the ad site, namely their post-click experience , for which other measures are better suited. A positive experience with an ad landing page increases the probability of users  X  X onverting X  (e.g., purchasing an item, registering to a mailing list, or simply spending time on the site building an affinity with the brand) [3, 22]. Even if no conversion occurs, publishers are keen to serve high quality ads, as doing otherwise may affect user long-term engage-ment: loosing users would mean less clicks on ads, and lower engagement with the site, which in turn negatively impact revenue. In this paper, we indeed show that when users ex-perienced high quality ads post-click, they were more likely to clicks on an ad in the future.

A positive post-click experience does not necessarily mean a conversion. Therefore, although the former can be mea-sured by the latter (e.g., a high conversion rate), this way of assessing the quality of the post-click experience is re-strictive. There may be many reasons why conversion does not happen, independent of the quality of the ad served to users. In addition, conversion rate information is not always available. A good proxy of the post-click experience is the time a user spends on the ad site before returning back to the publisher site:  X  X he longer the time, the more likely the experience was positive X . The two most common measures used to quantify time spent on a site are dwell time [27] and bounce rate [23]. These measures have been used as proxies of post-click experience in online advertising and or-ganic search, e.g., to improve the performance of ranking algorithms [12], as well as in recommender systems, e.g., to estimate the relevance of an item to a user [26]. In this work, we show that these are also good proxies of post-click experience in both the mobile and desktop contexts.
Finally, [3] showed that conversion rates differ significantly depending on the type of landing page. Also, [6] showed that landing pages could be leveraged to better select which ads to return to users in sponsored search. In the context of mobile advertising, whether the landing page of an ad is mobile-optimised or not was shown to affect post-click ex-perience [16]. Our research adds to this body of work by analysing other features of landing pages for mobile adver-tising, and how these help predict user post-click experience.
We show that dwell time and bounce rate are good proxy measures of ad post-click experience.
 Datasets and metric definitions. We randomly sampled 4,000 ads from a large set of ads served on Yahoo homepage stream in March 2014 both on desktop and mobile. Al-though our focus is on mobile devices, we look at both mo-bile and desktop to demonstrate that dwell time and bounce rate are good measures to act as proxy of ad post-click ex-perience. We defined dwell time and bounce rate as follows: The threshold used to determine the bounce rate on mobile was set to 5 seconds; this was empirically selected based on the dwell time distribution, which showed a  X  X lateau X  just around that value. The threshold on desktop instead was set to 12 seconds; this was chosen so to align with the threshold on mobile, by picking the value which corresponds to the same cumulative frequency of  X  X ouncy X  clicks on both dwell time distributions. Both these thresholds fall into the range between 5 and 60 seconds proposed in [15]. 4
In addition, we removed all clicks with a dwell time higher than 10 minutes, as these clicks may correspond to cases when users left the mobile or desktop device and came back later. By doing so we removed 1.74% of the total ad clicks. Finally, we only considered ads with at least 10 clicks. Dwell Time as a Proxy of Ad Post-Click Experi-ence. The fact that a user takes time to return to the news stream after clicking on an ad seems a good indicator that the experience is positive: the user browsed the site, maybe converted (e.g., purchased a product, registered to the site, shared the page, etc.), and finally went back to the stream. We study the extent to which higher dwell time indeed re-flects a positive experience.

We used a random sample of 200K ad clicks on the mobile stream for which we have records of a click on the ad site. For desktop, we matched the page views associated with the ad clicks to those contained in the Yahoo toolbar X  X  browsing data, 5 which results in page views from 30K ad clicks.
For both datasets, the Spearman X  X  rank correlation coef-ficient between the number of clicks on the ad site and the ad dwell time is, respectively, 0 . 65 for mobile and 0 . 54 for desktop; the higher the dwell time, the higher the number of clicks on the ad site. Assuming that clicking on the ad site suggests a positive  X  X d experience X , high dwell time is indicative of a positive post-click experience. The probabil-ity of a second click as the percentage of users who clicked Figure 1: Post-click experience. The probability of a second click given dwell time. The x-axis values (log-scale) are removed for confidential reasons. on a link on the ad landing page, for given dwell time values, is plotted in Figure 1. This probability increases with the time spent until the users return to the news stream for both mobile and desktop, further indicating that dwell time is a good proxy of post-click experience. It is also worth noticing that to get the same probability of a second click on mobile and on desktop, the dwell time has to be far larger in the latter than in the former. This suggests that dwell time is generally greater on desktop than on mobile.

Dwell time is not the time spent on the ad site, but the time between the click on the ad until the user returns to the stream. It can happen that users visit other sites dur-ing their session before returning. With the Yahoo toolbar dataset, we are able to look into this. We saw that the higher the dwell time, the higher the probability that users visited other websites. However, for all ad clicks with a dwell time up to 3 minutes, this happens for only 7 . 4% of the clicks. For dwell time higher than 3 minutes, this percentage increases to 23 . 3%. Therefore, in general dwell time is a good proxy of users spending time on the ad site, with longer dwell time reflecting a positive experience with the ad site. Next, we relate this to the long-term effect on user engagement. Long-term User Engagement. We now focus on mobile.
 We investigate the effect of the ad post-click experience, as measured by dwell time and bounce rate, on long-term user engagement. In other words, we examine how users are af-fected when they experience ads on which they spend little time (reflecting a negative ad experience) compared to the opposite (reflecting a positive ad experience)
We divided our dataset into three time-periods, covering a four-week period of user interaction with the mobile stream: Our objective is to compare the pre-and post-engagement periods depending on the ad-click-activity between the two.
We used the ad-click-activity dataset to distinguish be-tween a positive ad post-click experience and a negative one. For each ad a , we calculate its mean dwell time dt m ( a ) and its standard deviation dt sd ( a ). Any click c on ad a with dt ( a )  X  dt m ( a )  X  0 . 25  X  dt sd ( a ) is referred to as a short click , and any click c on ad a with dt c ( a )  X  dt m ( a ) + 0 . 25  X  dt is referred to as a long click . Here, dt c ( a ) is the dwell time on ad a for click c . These definitions account for the fact Figure 2: Changes in engagement depending on whether the users experienced short or long clicks. Table 1: Changes in engagement for short and long ad clickers. We report the average and median ( avg | median ) of the ad clicks and clicks per day dif-ference, and the p -values.
 that ads differ in terms of their average dwell time. For ex-ample, we saw that ads related to beauty products have on average a higher dwell time than those related to finance, simply because the ad experience is different (reading about a product versus registering an interest).

For all users that clicked on at least 3 ads, users with only short clicks were said to have had a negative experi-ence ( shortAdClicker ), whereas those with only long clicks were said to have a positive experience ( longAdClicker ). Having a minimum of 3 clicks allow us to select users that have experienced enough ads to be affected by them. These resulted in two sets of similar size, around 800 users each.
We use two metrics to measure pre-and post-engagement: 7 We define the change in the engagement between the pre-and post-engagement time periods as follows: where eng is either adClicks or clicksPerDay . A value above (below) 0 indicates that post-engagement increased (decreased) compared to pre-engagement, and the extent of the increase (decrease). Figure 2 shows the distribution of the two metrics for the two user groups ( shortAdClicker and longAdClicker ). We also report the average and the mean of the two metrics in Table 1. We use a two-sample Kolmogorov-Smirnov test to check whether the distributions are different. The p -values are reported in Table 1. For both user groups, the ad click activity is increasing. This is likely to reflect that users in both groups are becom-ing more engaged with the stream and as a result are more likely to click on ads. However, the median adClicks d for (a) Difference in dwell time. Figure 3: Distributions of the differences in post-click experience between mobile and desktop. the short ad clicker group is 25 . 0%; this value is 50 . 0% for the long ad clicker group. That is, the increase in ad clicks (both in terms of median and average, and distribution) for the long ad clicker group is higher, indicating that a positive ad post-click experience is leading to more ad clicks. The difference is statistically significant ( p -value &lt; 0 . 01).
The metric clicksPerDay d has a median close to 0 . 0% for both user groups. This suggests a similar trend in en-gagement with the stream, some users becoming more en-gaged, while others becoming less engaged. Looking at our dataset in more depth, we could identify users getting more engaged with the stream as time passed, and users that were very engaged with the stream. As such their future engage-ment could not increase further (reaching a certain plateau). However, when looking at the average values and the distri-butions of clicksPerDay d , we see a larger increase for the long click group, compared to the short click group (5 . 6% and 2 . 4%, respectively), suggesting a larger increase in en-gagement with the stream for users in the positive post-click experience group. The difference, although small compared to adClicks d , is significant ( p -value &lt; 0 . 01).
To conclude, using dwell time to measure the ad post-click experience, we showed that a positive experience had a strong effect on users clicking on ads again, and a small effect on user engagement with the stream. The effect is statistically significant for both. Thus, not only can dwell time be used to measure an ad post-click experience, ensur-ing that high quality ads are served to users is important for long-term engagement. We investigate further the two post-click measures, in particular what they can tell about the mobile post-click experience compared to the desktop one. Mobile vs. Desktop. We examine the difference in the ad post-click experience, measured by dwell time and bounce rate, between mobile and desktop. First, we compare whether dwell time and bounce rate of an ad differs between the two.
Users experience ads differently depending on the device they are using . In fact, using Spearman X  X  rank correlation coefficient  X  , for dwell time we obtain  X  = 0 . 50; this value is even smaller for bounce rate with  X  = 0 . 23. 8
Next, we calculate for each ad the difference in percentage of their dwell time and bounce rate, when shown on desktop compared to mobile ( val refers to dwell time or bounce rate): The distributions of the percentage differences are plotted in Figure 3, (a) for dwell time and (b) for bounce rate. For 92 . 9% of the ads the dwell time is higher on desktop than (a) Difference in dwell time. Figure 4: Differences in ad post-click experience be-tween mobile and desktop depending on whether the landing page is mobile-optimised or not. on mobile (Figure 3(a)). This is not surprising, as brows-ing time on mobile has been shown to be shorter generally outside advertising [25]. The highest decrease in dwell time from desktop to mobile is by 35 . 0%. From Figure 3(b), we observe that 64 . 1% of the ads have a higher bounce rate on mobile than on desktop, which is a lower percentage than for dwell time. The highest decrease of bounce rate from desktop to mobile is by 50 . 0%, which is slightly higher when compared to dwell time. The distribution is skewed to the left however, indicating that many ads have a large increase in bounce rate when shown on mobile. For instance, 18 . 9% of the ads have a bounce rate increase higher than 50 . 0%.
Whereas the dwell time differences between desktop and mobile may reflect the browsing behaviour on these two de-vices (shorter sessions on mobile), the bounce rate differ-ences clearly suggest that the ad post-click experience be-tween mobile and desktop differs. The device has an impact on how users experience the ad.
 Mobile-Optimised Ad Landing Pages.

We showed that the ad experience on mobile is different to that on desktop. This can partly be explained by the differ-ent ways users interact with their desktop and their mobile. However, a preliminary analysis done on the landing pages of the ads in our dataset showed that, when served on a mobile device, some of the landing pages were not mobile-optimised, which is likely to have a negative effect on users [16]. In op-timised version, landing pages have typically larger buttons, no long text paragraphs, and a single large image of the product advertised in the middle of the page. To see the effect of this on the ad post-click experience, and the extent to which this reflects the quality of the ad, we designed a mechanism to automatically detect when a landing page is mobile-optimised or not.

We first downloaded the landing pages, rendered them and extracted seven features. We used features such as the size in bytes of the HTML landing page, and whether the page contains an apple touch icon. Next, we manually la-belled a sample of 259 ads as mobile-optimised ( Opt ) or non-optimised ( Npt ). Our training set consisted of 108 Opt and 151 Npt ads. We fed the feature representation of the landing pages in our annotated dataset to a Gradient Boosted Deci-sion Tree classifier, and we estimated its quality using leave-one-out cross validation (LOOCV). The classifier reached a F 1 score of 0 . 995, demonstrating its high accuracy.
We then tested if dwell time and bounce rate of a landing page correlates with it being Opt . Using a sample dataset of 500 ads, with approximately 65 . 0% of them being mobile-optimised and 35 . 0% non-mobile-optimised, we conducted a similar experiment to the previous section, but with Opt ads and Npt as our classes. The results are shown in Figure 4. We employ a Two-sample Kolmogorov-Smirnov test to look at whether the differences are significant.

The distribution of the dwell time difference is very sim-ilar for both groups, Opt and Npt (Figure 4(a)). The av-erage dwell time decreases by 31 . 8% ( median = 31 . 8%) for Opt landing pages, and by 28 . 9% ( median = 33 . 6%) for Npt landing pages. The difference is not significant ( p -value = 0 . 20). Whether a landing page is optimised does not in-fluence how long users spend on the mobile ad site.
When considering bounce rate, however, we observe dif-ferences (Figure 4(b)). The average bounce rate decreases by 6 . 9% (median decreases by 30 . 4%) for Opt landing pages but increases by 13 . 4% (median decreases by 11 . 5%) for Npt landing pages. These differences are statically signif-icant ( p -value = 0 . 003). Therefore, mobile-optimised land-ing pages have a positive influence on users, as they are less likely to lead to bounce, compared to when shown on the desktop. Studies looking at ad post-click experience in the mobile context should account for this property of the land-ing page, which by itself is not surprising. However, since we observe no difference in dwell time other features of the landing pages influence user post-click experience. This is the motivation for the landing page analysis discussed next.
When a landing page has a high dwell time it is likely to be interesting, and potentially more likely to lead to a conversion. On the other hand, if a landing page has a high bounce rate it means that most of the users are annoyed by it. The previous section demonstrated dwell time to be a good proxy of user post-click experience and bounce rate provided additional insights on user post-click experience. In this section, we use these metrics to identify high quality ads on the basis of their landing pages, as experienced by mobile users on Yahoo mobile news stream.
Given a request for an ad to be served on Yahoo mobile news stream, we describe how ads are ranked to fulfill that request. Let P i click  X  [0 , 1] be the predicted probability of an ad i being clicked, and let bid i  X  R be the amount of money the advertiser is willing to pay for its ad to be shown. Rank-ing the ads is done by computing the expected cost per click eCPC i = P i click  X  bid i for each ad, and later sorting them in descending order of this value. Our aim is to predict P i that is the conditional probability of a user being satisfied given that he or she clicked on an ad i . The goal is thus to estimate the overall joint probability of clicking on an ad and being satisfied by its landing page. More formally, we want to compute P i HQ = P i click  X  P i SAT . 9 Finally, the ranking of ads will be computed as eCPC i H Q = P i HQ  X  bid i . The ad with the highest eCPC i H Q will then be served.

We thus want to predict a class label Y i  X  { X  1 , 1 } for a given landing page X i represented by a feature vector  X  ( X The class label Y i is 1 if X i is a high quality page,  X  1 other-wise. Concretely, we aim at estimating the following proba-bility density function: which, in practice, corresponds to the joint probability P
In this work, we focus on the quality of an ad as experi-enced by users on the ad landing page. Within this context, there are many definitions of a high quality ad we could use. We consider two definitions, one based on dwell time and the other on bounce rate, which we have shown to be good proxies of the ad post-click experience.
 Formally, with a web page X i accessed by a set of n users we associate two real numbers  X  X i &gt; 0 its mean dwell time computed over the set of n X i users, and  X  X i  X  [0 , 1] its bounce rate computed as the fraction of the n X i leaving X before a given time threshold. 1. High Dwell Time . Y i = 1 when  X  X i &gt; t  X  where t  X  2. Low Bounce Rate . Y i = 1 when  X  X i &lt;  X   X  where 0 &lt; We experiment with these two definitions, to identify whether they can be predicted based on the ad landing page features. Next we define these features.
Inspired by previous work [2, 3, 6, 11] exploiting features extracted from the landing pages to categorise ads, our un-derstanding of the problem at hand and our own expertise and common sense, we define three sets of features. CONT (C) Features. This group captures the content including the functionality of the landing page: SIM (S) Features. This group of features captures the similarity of the landing page with the creative text dis-played within the stream. Usually, a user sees the creative and decides to click on the basis of the text written there. If the semantics of the creative text is very different from the semantics of the landing page then the user who clicked may be annoyed and leave immediately the page. HIST (H) Features. These features captures historical information about the ad past performance.
Using the features listed above we train several models to predict P i HQ using three well-known learning methods: Logistic Regression ( LogReg ) [28], Support Vector Machines ( SVM ) [7], and Gradient Boosted Decision Trees ( GBDT ) [10]. We use the implementations of these methods available in the Python scikit-learn package. 11 The probability values are extracted using the implementation available with this framework. We adopt standard parameters for each method. For the LogReg classifier we set C , the inverse of regulariza-tion strength, to 100, and L 1 as penalty norm, and 0 . 01 as the tolerance value for stopping the optimization. For the SVM classifier we adopt a RBF kernel with a penalty pa-rameter C of the error term equal to 1 . 0, 0 . 0 as the gamma kernel coefficient, and 10  X  3 as the tolerance used in the stop-ping criterium. Finally, for the GBDT classifier we generate a forest of 100 trees with a max depth of each tree of 4, and a learning rate of 0 . 01.
To assess the validity of our prediction models, we run a traditional offline evaluation based on historical data. Experimental Setup. For each definition of high qual-ity ads, we report the performance of predictors using three standard metrics, Area Under the ROC Curve (AUC), F 1 , and the Matthews Correlation Coefficient (MCC) [17]. The latter is a correlation measure between predictions and la-bels taking into account the popularity of each class.
The dataset used to run the experiments is a uniformly generated sample of our ad set. As a training set we ex-tract a sample of 1,500 ads shown in March 2014 to users of the system. The test set contains a sample of 550 ads shown in April 2014. In all the tests we conduct we experi-ment with several thresholds for dwell time ( t  X  ) and bounce rate (  X   X  ). Finally, we test several combinations of features: content-based or C features; similarity-based or S features; and history-based or H features.

Tables 2 and 3 report the results for predicting the proba-bility of high dwell time and low bounce rate. Only for high dwell time, we report the results for the three classification methods ( LogReg , SVM , and GBDT ). For bounce rate, we report the results using LogReg .

First, the various classification methods perform similarly (with a slight advantage of SVM over the others for high dwell time). Similarity-based features perform bad as they never increase (and in some case are detrimental to) both metrics. Finally, history-based features are very important as they boost, for instance, AUC above 0 . 8 when combined with content. Content-based features alone are already achiev-ing high values with both metrics.

We should note that history-based features are very sparse for ads and, in particular, are non-existing for newly inserted ones. Content-only classifiers can always be used as they provide the perfect solution to the  X  X tem cold-start problem X  that will be often experienced with many ads.
 Table 2: Dwell Time prediction performance on models built on ads data from March 2014 and tested on April 2014. We vary t  X  to evaluate the impact of the threshold chosen on the prediction ability of the model (best results in bold). We show the importance of each set of features, namely C, C-S and C-S-H. We limit our analysis to the GBDT classifier and use the technique implemented in the scikit-learn toolkit. This technique can naturally be used to induce a ranking of the  X  X mportance X  of features in a regression or classification problem. Table 4 shows the top-15 features ranked according to their importance scores as output by GBDT when trained for predicting ad quality on the basis of dwell time being above a fixed threshold 12 . Each column refers to the ranking of a specific set of features: C, C-S and C-S-H. The i -th row Table 3: Bounce rate prediction performance on models built on ads data from March 2014 and tested on April 2014. We vary  X   X  to evaluate the impact of the threshold chosen on the prediction ability of the model (best result in bold). contains the i -th ranked feature, along with its category (C, S or H) and importance score, for each set.

When the classifier is trained using only content features (C) or both content and similarity features (C-S), clickTo-Call is the most important signal. The first similarity fea-ture ( similarityNoun ) is ranked 11-th when using the C-S set. This means that similarity features do not provide sig-nificant insights to discriminate between high and low qual-ity ads. This is even more evident when using the C-S-H set, where no similarity features appear in the top-15 list. We also observe that features related to the functionality of the landing page ( numDropdown , numInputRadio ) impact more than those related to the content itself ( tokenCount , noun-sSumOfScores ) and aesthetic ( isMobileOptimised , media ). Only when considering the similarity features (C-S) as well, two content X  X elated features become more important ( sum-marisabilityScore and numConceptAnnotation ). Finally, for the set C-S-H, the top-3 most important signals come all from historical features; avgdwelltime and avgdwelltimenon-short play a crucial role. This is not surprising as our clas-sifier uses dwell time as the proxy for ad quality.
Using LogReg , we saw that the functionality features have negative coefficients, suggesting that they affect negatively the post-click experience. These features relate to the ex-istence of a form on the landing page, and as such are a strong deterrent to users. The forms when displayed on a mobile device may not be user-friendly, or users are simply not willing to share private information.

In Section 3, we showed that dwell time was a good proxy of an ad post-click experience. The LogReg solution showed very good performance in predicting high dwell time, i.e. dwell time being above a given threshold. Although SVM per-formed slightly better, LogReg supports quick update oper-ations, which is important when deployed in production. We therefore decide to deploy such a model using only content-based features to allow full coverage of the ads in the database, and not just those for which we have historical data. We use 40 seconds as our threshold, as in our dataset it corresponds to the median of the overall dwell times distribution. We also choose to deploy the version predicting high dwell time as we wanted to serve ads on which users spend time. This means that not only they are of high quality but also have the potential of leading to a  X  X onversion X .
To measure the impact that our ad quality prediction model has on users we conduct an online evaluation through A/B testing. We implement an ad ranking scorer on Yahoo Gemini based on the Logistic Regression ( LogReg ) predic-tion model, and assess its performance on the mobile news stream app running on iOS.

We split the incoming traffic into two buckets , i.e. baseline and ad quality . In the first bucket, ads are served using the existing ranking scheme, i.e. the expected cost per click, whereas in the second bucket ads are served according to the newly proposed ranking scorer that accounts for the ad post-click experience (i.e. the probability that users do not return to the stream within the next 40 seconds).

We measure the user post-click experience with dwell time and bounce rate. Specifically, we compute the median of the former to deal with the high variance of dwell times. For the bounce rate we report, instead, the average of the values. Bounce rate is already a normalised score in the [0 , 1] range and also exhibits small variance.
 Experimental Setup. We consider two distinct datasets of ad clicks, randomly sampled from May to June 2014. The first dataset is drawn from the baseline bucket and contains only clicks on ads ranked by the baseline scorer. The second dataset is drawn from the ad quality bucket and contains all the clicks of ads served by the ad quality ranking scorer.
We conduct three analyses, at the (ad-)click-level , at the ad-level , and at the user-level . For all three, we evaluate the two buckets performance. First, we compare the perfor-mance accounting for all ads (users) as they appear in the two datasets; we refer to this experimental setting as All . Then, we measure the performance limited to only those ads (users) common to the two datasets; we call this Shared . Finally, we focus only on those ads (users) that appear in only one of the two datasets; we refer to this as Unique . Click-level Analysis. We discuss how the daily click-through rate behaves on the two buckets. When assessing the effect of any change, e.g., in a ranking algorithm, it is important to do so over a long period of time, as an increased performance shortly after the change may not translate in the long run to better user experience [13].

We collect around 14,500 ads from the dataset relating to the baseline bucket, and about 12,500 ads from the ad quality bucket ( All ). More than 11,000 ads are shared be-tween the two buckets ( Shared ), and therefore are very likely high quality ads. Looking at these two settings ( All and Shared ) we observe that the overall click-through rate (the whole month) increases by about 18% and 13% on the ad quality bucket. Figure 5 shows that for both settings, the click-through rate is always higher for the ad quality bucket. This means that the probability of a user clicking on an ad increases in the ad quality bucket. Interestingly, the two time series are perfectly correlated (Pearson X  X  r = 1 . 0). The paired t-test applied to each pair of time series samples shows that the differences between the two samples, though corre-lated, are statistically significant ( p -value = 0 . 01).
If we consider only the (high quality) ads shared between the two buckets the chance of clicking on an ad seems to depend on which bucket the ad is served. Intuitively, if a high quality ad is served together with other, lower quality ads (i.e. baseline bucket) then users may not perceive it as valuable as it is, and thus the probability of clicking on it decreases. However, if the same high quality ad is served with other high quality ads (i.e. ad quality bucket) the users may be more likely to click on it because they have been exposed to ads leading to positive experience. Therefore we conjecture that the perception of the ad quality is influenced by the ads to which a user has been  X  X xposed X  and by how those have been experienced (either positive or not), which is what Figure 5(b) suggests. This further confirms our re-sults relating ad post-click experience and future ad clicks discussed in Section 3.
 Ad-level Analysis. The second analysis shows how dwell time and bounce rate behave on the two buckets, from an  X  X d perspective X . We first remove all the ad clicks with dwell time greater than 10 minutes (following from Section 3). Afterwards, we consider only those ads that received at least 10 clicks. This is done to avoid the effect of outliers and ensure that we have enough clicks to calculate bounce rates. Finally, a click is considered a bounce if its dwell time is less than or equal to 5 seconds. This resulted in around 1,000 ads in the baseline bucket and 700 in the ad quality bucket ( All ), with around 600 ads common to both buckets ( Shared ). We should note the ads used in this analysis form a subset of those used in the first click-level analysis.
The results for the All , Shared , and Unique datasets are reported in Table 5. Unique is the dataset comprised of ads that are only in one of the two buckets. Each cell of the ta-ble refers to the relative difference (in percentage) between Table 5: Differences (%) in Dwell Time and Bounce Rate between ad quality and baseline ads. the statistics as computed from the ad quality and baseline buckets, respectively. Note also that while a positive differ-ence is desirable in the case of dwell time (i.e. showing the ad quality bucket exhibits more time spent on the ad landing page), for the bounce rate we aim at reducing the number of short clicks, and so we prefer a negative difference.
In all the experimental settings, the ad quality bucket out-performs the baseline bucket. This is particularly visible when considering Unique ads. Interestingly, when looking at the Shared ads, the median dwell time is still higher when the ads are served as part of the ad quality bucket, com-pared to the baseline bucket. In Figure 5(b), we can see that the click-through rate for the Shared ads is higher in the ad quality bucket. Therefore, not only serving high quality ads together (i.e. the ad quality bucket) attract more clicks, the post-click experience, as measured with dwell time, is also positively affected: users engage more with the ads .
Similarly, the average bounce rate is lower with the ad quality bucket, which implies a lower probability of bouncing back once users click on ads that have been deemed to be of high quality. Interestingly, for Shared ads, there is no difference in bounce rate. Bounce rate is more a reflection of an ad being annoying. This suggests that the property of  X  X eing annoying X  is a characteristic of the ad itself (i.e. its landing page as experienced on average by users), and does not depend on which other ads are served during the session.
We compare the distribution of dwell time and bounce rate, as observed in the two buckets. We run a Kolmogorov-Smirnov test for two samples on each pair of observations to test if both samples might have been drawn from the same underlying probability distribution. For all cases ex-cept one, this is not the case with p -value lower than 0 . 01. The exception is with the bounce rate on the Shared setting. This however complies with the results in Table 5, where no difference exists between the two buckets for the average bounce rate for Shared , further confirming that  X  X eing an annoying X  ad or being a bad ad comes from the ad  X  X tself X . User-level Analysis. The aim of the last analysis is close to that of the previous one yet from a  X  X ser perspective X . We remove all the ad clicks having dwell time larger than 10 minutes and a click is considered a bounce as long as its dwell Table 6: Differences (%) in Dwell Time and Bounce Rate between ad quality and baseline users. time is at most 5 seconds. Furthermore, we take into account only those users who clicked on at least 2 unique ads. This results into around 16,000 users in the baseline bucket and 11,000 in the ad quality bucket, with about 2,700 individuals shared between the two buckets. Table 6 shows the relative differences (in percentage) of dwell time and bounce rate as computed from the two buckets.

The median dwell time is higher for the ad quality bucket, which means that when users are served ads deemed of high quality, they spend time on the ad landing page before re-turning to the stream. In particular, looking at the Shared setting (i.e. users appearing in both buckets), serving high quality ads indeed promotes a positive post-click experience. This is further accentuated when users experience only high quality ads (as seen with Unique ).

Concerning the average bounce rate, this is computed as the average fraction of bounce clicks for each user. This is different from the actual definition of bounce rate, which instead is formulated at the ad-level. Still, we observe a de-crease of the average bounce rate in the ad quality bucket. This relates to the fact that more high quality ads are served in the ad quality bucket, which leads to fewer users bouncing back after clicking on them. Finally, the difference between dwell time and bounce rate distributions is statistically sig-nificant ( p -value 0 . 01) using Kolmogorov-Smirnov test.
This section shows that returning high quality ads, as measured in terms of the ad post-click experience, is im-portant. Not only this increases CTR, and as a likely con-sequence revenue in the long-term, it has a positive effect on users, as seen by the increase in dwell time and decrease in bounce rate. Interestingly, when users are served ads deemed of high quality together, their engagement with the ads, in terms of time spent on the ad site, is positively af-fected. In addition, so called bad ads are so because of them-selves, and this independently of whether they are served or not with ads deemed of high quality.
We proposed a method to identify high quality ads served on one of Yahoo mobile stream platforms. In this paper, quality refers to the ad post-click experience. We therefore related the ad landing page to the ad post-click experience, which we measured through well known engagement metrics, dwell time and bounce rate. We first showed that these mea-sures were appropriate proxies of ad quality in our context. We also showed that users clicking on ads that promote a positive post-click experience are more likely to click on ads in the future, and their long-term engagement is positively affected. We also compared these measures in the context of mobile versus desktop devices, and found that a positive ad post-click experience is not just about serving ads with mobile-optimised landing pages; other aspects of an landing page affect the post-click experience.

We then put forward an approach that analyses ad landing pages, and shows how these can affect dwell time and bounce rate. We experimented with three types of features, related to the actual content and organization of the ad landing page, the similarity between the creative and the landing page, and ad past performance. We saw that the later type of features were best at predicting dwell time and bounce rate; we also show that content and organization features perform well, and have the advantages to be applicable for all ads, not only for those that have been served. Overall, our offline evaluation showed that, using dwell time as a proxy of post-click experience, we could predict the quality of an ad with good accuracy.

Finally, we deployed our prediction model for ad qual-ity based on dwell time on Yahoo Gemini , and validated its performance on the mobile news stream app running on iOS. This was conducted through A/B testing to measure the impact of our approach in a real-world scenario. Finally, dwell time and bounce rate exhibited statistically significant differences, when the ads served took into account their pre-dicted dwell time, using the landing pages features, com-pared to when they did not. Dwell time increased by 30% whereas bounce rate decreased by 6.7%.

As future work, we plan to train our model on new fea-tures, such as readability, PageRank, sentimentality level of the landing pages. We are currently carrying out user stud-ies to understand how users perceive the quality of the land-ing pages, and how this can be translated into additional features. Finally, we will be studying alternative metrics, for example combining dwell time and bounce rate, but also others, to fully capture the ad post-click experience.
We would like to thank John Agapiou and Andy Haines for their help in obtaining and polishing the data we used in our experiments and the feedbacks on our results.
