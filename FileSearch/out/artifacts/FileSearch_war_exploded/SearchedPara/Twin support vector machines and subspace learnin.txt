 1. Introduction
Breast cancer is probably the most common non-skin cancer and one of the major causes of death in women as well as a number of men. Aiming at breast cancer detection at the early stage, the digital mammography has become one of the most effective methods. In digital mammography data, an important sign of breast cancer is the existence of microcalcification clusters (MCs), which appear in 30 X 50% of mammo-graphically diagnosed cases with tiny bright spots of a different morphology. Microcalcifications are small calcifications of different shapes and densities, approximately 0.1 X 1 mm in diameter. Isolated microcalcifications are not dangerous, but a microcalcification cluster, which is a region containing three or more microcalcifica-tions per 5 mm 5 mm area, might be an early sign of breast cancer ( Kopans, 2006 ).

Because of its importance in breast cancer diagnosis, accurate detection of MCs has become a key research and application task, and a number of approaches have recently been developed, which have been greatly assisting doctors and radiologists in diagnosing breast cancer ( Cheng et al., 2003 ). Among them, apart from focusing on image segmentation and specification of regions of interest (ROI), several methods have been proposed, such as classical image filtering and local thresholding ( Bagci and Cetin, 2002 ; Nakayama et al., 2006 ), and techniques based on mathematical morphology ( Halkiotis et al., 2007 ; Mossi and
Albiol, 1999 ), fractal models ( Bocchi et al., 2004 ), optimal filters ( Gulsrud and Husoy, 2001 ), wavelet analysis and multiscale analysis ( Bagci and Cetin, 2002 ; Rezai-rad and Jamarani, 2005 ;
Song et al., 2006 ). Various classification approaches have also been presented to characterize MCs, such as rule-based systems ( Riyahi-
Alam et al., 2004 ), fuzzy logic systems ( Cheng et al., 1998 ; Cheng et al., 1998 ; Cheng et al., 2004 ; Jiang et al., 2005 ), statistical methods based on Markov random fields ( D X  X lia et al., 2004 ; Lee and Chen, 1996 ), and support vector machines (SVMs) ( D X  X lia et al., 2004 ; El-Naqa et al., 2002 ). In the past decade, most work reported in the literature has employed neural networks in MCs character-ization ( Bocchi et al., 2004 ; Halkiotis et al., 2007 ; Hernandez-Cisneros and Terashima-Marin, 2006 ; Papadopoulos et al., 2005 ;
Rezai-rad and Jamarani, 2005 ; Yu and Guan, 2000 ). With the development of SVMs, various SVMs have been designed to categorize ROIs ( El-Naqa et al., 2002 ). SVMs are promising as it concerns on the trade-off between statistical structural complexity and empirical risk, though it also encounters some problems. One of the most popular explanations of SVM classifiers is probably the maximum margin that attempts to reduce generalization error by maximizing the margin between two disjoint half planes. The resulting optimization task involves the minimization of a convex quadratic objective function subject to linear inequality con-straints. Recently, Jayadeva et al. (2007) have proposed a nonpar-allel plane classifier for binary data classification, termed as twin support vector machine (TWSVM). This classifier aims to generate two nonparallel planes such that each plane is closer to one of the two classes and is as far as possible from the other. TWSVM solves two quadratic programming problems (QPPs) of smaller size instead of large sized ones as we do in traditional SVMs. This makes TWSVM work almost four times faster than the standard SVM classifier with the linear separable problem.
 over two decades, automated detection of MCs remains very difficult. This is because of the existences of film emulsion error, digitization artifacts, or anatomical structures such as fibrous strands, breast borders, or hypertrophied lobules, which look like microcalcifications in the breast normal tissues, which often cause high false positives. Meanwhile, in some dense tissues, and/or skin thickening, especially in the breasts of younger women, suspicious areas are almost invisible. The dense tissues, especially in younger women may easily be regarded as micro-calcification, which often cause to a false positive. That is the major problem with most of the detection algorithms.
 rithms, we propose a novel framework to detect MCs. In the framework, we employ TWSVM as the classifier to distinguish
MCs from other ROIs in images. The classifier is first trained by the manually labeled image blocks of mammograms from the digital database for screening mammography (DDSM) ( Rose et al., 2006 ). Then it is used to detect other ROIs. The effectiveness of TWSVM is then evaluated.
 grams from the widely recognized DDSM database is selected to extract ROIs as the test bed , in which 2231 MCs exist. These mammograms were divided into two subsets: the first set is for training and validation in training stage and the other for test in the testing stage. Compared with several popular existing meth-ods, this newly proposed approach achieves better performance.
In evaluation stage, curves of receiver operating characteristic (ROC) are used. The best average sensitivity is as high as 94.41 7 1.16% with respect to 8.32 7 1.04% false positive rate and the area under the ROC curve, Az  X  0.9677 7 0.0574 in each testing phase.
 tion framework for MCs detection by incorporating the subspace learning algorithms and TWSVM. Although it is commonly used in pattern recognition, to the best of our knowledge, there are few works employing such a framework to detect MCs in mammo-grams. In our research, we apply subspace learning to extract discriminant features rather than semantic features. Then
TWSVM and SVM-based classifier are trained to distinguish the blocks with MCs from others. By comparing some available subspace learning methods, such as PCA, LAD, TSA and GTDA, and classifiers, such as SVM and TWSVM, some meaningful conclusions are reached, which provide guidance for the proposed framework of MCs detection in mammograms.
 subspace learning and TWSVM is given in Sections 2 and 3 , respectively. Thereafter, the novel TWSVM based MCs detection algorithm is formulated in Section 4 . The new approach is then evaluated as described in Section 5, and accordingly, experimen-tal results are reported in Section 6 . Finally, conclusions are drawn in Section 7 . 2. Subspace learning such as principal component analysis (PCA) and linear discrimi-nant analysis (LDA), scan any type of input data as vectors, and then transform the high dimensional data into a low-dimensional subspace. However, in the real world, an image is intrinsically a matrix, or a 2-order tensor. The extracted feature of an object often has some specialized spatial structures, and such structures are in the form of second-order or even high-order tensors. And also, because of the under sample problems (USP), i.e., the dimensionality of input feature space is much higher than the number of training samples (in our task the training sample number m is much less than the input sample feature dimension-ality, m 5 115 115), PCA and LDA do not always work as well as expected. To overcome this problem, multilinear algebra, the algebra of higher-order tensors, has recently been applied to analyze multifactor structures of image ensembles. Furthermore, a set of promising approaches have been developed, such as tensor space analysis (TSA) ( He et al., 2005 ) and general tensor discriminant analysis (GTDA) ( Tao et al., 2007 ). TSA treats an image as a second-order tensor in R n 1 R n 2 , where R n the two vector spaces. The relationship between row vectors can be naturally characterized by TSA and GTDA, and they can detected by learning the intrinsic local geometrical structure information in the tensor space. This structure information of objects in pattern recognition research is a reasonable constraint to reduce the number of unknown parameters used to represent a learning model.

In this paper, the symbols in bold lowercase represent vectors, such as x , y ; the bold uppercase symbols represent matrices, such as A , B , C ; the italic uppercase symbols represent tensor objects, such as X ; and the italic lowercase symbols stand for scale numbers, such as y , c . 2.1. PCA and LDA
PCA is a typical linear dimensionality reduction algorithm. The basic idea of PCA is to project the data along the directions of maximal variances so that the reconstruction error can be mini-mized. Given a set of data points or patterns x 1 , x 2 ,..., x the transformation vector and y i  X  w T x i . The objective function of
PCA is w opt  X  arg max w where y  X  1 n P y i and C is the data covariance matrix. The basis functions of PCA are the eigenvectors of the data covariance matrix associated with the largest eigenvalue.

Compared with PCA seeking directions that are efficient for representation, LDA seeks directions that are efficient for discri-mination. If we have a set of n samples x 1 , x 2 ,..., x classes. The objective function of LDA is as follows: w opt  X  arg max w S  X  S where m is the total sample mean vector, n i is the number of samples in the ith class, m ( i ) is the average vector of the ith class.
We call S W the within-class scatter matrix and S B the between-class scatter matrix. 2.2. Tensor space analysis
TSA is a new technique which learns a tensor subspace which respects the geometrical and discriminative structures of the original data space. Let X A R n 1 n 2 denote an image of size n where X can be thought of as a 2-order tensor in tensor space R R X  X  This indicates that f u i v T j g forms a basis of the tensor space R 1 R n 2 . Define two matrices U  X  X  u 1 , u 2 , ::: , u of R n 1 spanned by f u i g l 1 i  X  1 and V beasubspaceof R f v g l 2 j  X  1 . Thus the tensor product U V isasubspaceof R projection of X A R n 1 n 2 onto the space U V is Y  X  U T 2.3. General tensor discriminant analysis
GTDA ( Tao et al., 2007 ) is a tensor extension of the differential scatter discriminant criterion (DSDC) ( Fukunaga 1990 ), which is defined as U n  X  arg max where x is a tuning parameter; U A R N N n ( N * { N ), constrained by U
U  X  I , is the projection matrix; S B and S W are defined in (3) and (4). The tensor DSDC is U n  X  arg max  X  arg max where 99 U 99 is the Frobenius norm and the projection matrix U
A R N N n ( N n { N ) is constrained by U T U  X  I . Let X training sample (with tensor representation) in the i th individual class k i , M i  X  1 n M  X  1 n U denotes the l th projection matrix obtained during the training procedure. Moreover, X  X  i  X  j GTDA can be defined by replacing x  X  i  X  j , m ( i ) and m with X M , respectively, as argmax
The optimal problem does not have a closed-form solution, while Tao et al. (2007) have developed an alternating projection method, which is an iterative algorithm, to obtain a numerical solution. As shown in Tao et al. (2007) , we got the optimal dimension with the accumulative contribution ratio of 0.95. In GTDA, the original general tensor X can be transformed by using the projected tensor Y  X  X Q M 3. Twin support vector machines
In this paper, MCs detection is formulated as a binary classi-fication problem. At each block in a mammogram, the proposed classifier is applied to determine whether a MCs object is present or not. We defined x A R n as a pattern to be classified, and y as its class label (i.e., y A { 7 1}). In addition, let {( x i , y a given set of m training examples. And meanwhile we define m row vectors A i ( i  X  1,2,... m ) in the n dimensional real space R where A i  X  ( A i 1 , A i 2 ,..., A in ), A i  X  x i T and A with class label  X  1 form a matrix A , and samples with class label m  X  m 1  X  m 2 , m 1 is the number of positive samples (which contain
MCs), and m 2 is the number of negative samples. 3.1. TWSVM classifier
The twin support vector machine (TWSVM) ( Jayadeva et al., 2007 ) obtains nonparallel planes around which the data points of the corresponding class get clustered. Each of the two quadratic programming problems in a TWSVM pair has the formulation of a typical SVM, except that not all patterns appear with the con-straints of either problem at the same time.

The TWSVM classifier is obtained by solving the following pair of quadratic programming problems:  X  TWSVM1  X  min s : t : - X  Bw  X  1  X   X  e 2 b  X  1  X   X  X  q Z e 2 , q Z 0  X  TWSVM2  X  min s : t :  X  Aw  X  2  X   X  e 1 b  X  2  X   X  X  q Z e 1 , q Z 0 , where c 1 , c 2 4 0 are the parameters and e 1 and e 2 are the vectors of ones of appropriate dimensions.

This approach aims to find two optimal hyperplanes, one for each class, and classifies points as two categories according to which hyperplane a given point is more close to. The first term in the objective function of (9) or (10) is the sum of squared distances from the hyperplane to points of one class. Therefore, minimizing it tends to keep the hyperplane close to points of one class (i.e., class  X  X   X  1 X  X ). The constraints require the hyperplane to be at a distance of at least 1 from points of the other class (i.e., class  X  X  1 X  X ); a set of error variables is used to measure the error wherever the hyperplane is closer than this minimum distance of 1. The second term of the objective function minimizes the sum of error variables, thus attempting to minimize misclassification due to points belonging to class  X  X  1 X  X . As an example in Fig. 1 , one can find the difference between the traditional SVM and TWSVM. 3.2. Kernel TWSVM classifier
The linear TWSVM can be easily extended to a nonlinear classifier by first using a nonlinear operator F ( U ) to map the input pattern x into a higher dimensional space H , where the function K ( U , U ) is defined as K  X  x , y  X  F T  X  x  X  F  X  y  X  X  11  X 
And the kernel-generated surfaces instead of planes are defined as follows: K  X  x , C T  X  u  X  1  X   X  b  X  1  X   X  0 , and K  X  x T , C T  X  u  X  2  X  where C T  X  [ A B] T , and K is an appropriately chosen kernel, such as polynomial kernel, RBF kernel, etc. Compared with the above arguments, the optimization problem of kernel TWSVM (includ-ing, KTWSVM1 and KTWSVM2) is defined as  X  KTWSVM1  X  min s : t : K  X  B , C T  X  u  X  1  X   X  e 2 b  X  1  X   X  q Z e 2 , q  X  KTWSVM2  X  min s : t : K  X  A , C T  X  u  X  2  X   X  e 1 b  X  2  X   X  q Z e 1 , q where c 1 4 0, and c 2 4 0 are the tuned parameters.
 kernels are used, which are known to meet Mercer X  X  condition.
They are defined as follows.
K  X  x , y  X  X  X  x T y  X  1  X  p ,  X  15  X  where p 4 0 is a constant defined as the kernel order.
K  X  x , y  X  X  exp where s 4 0 is a constant defined as the kernel width. 4. TWSVM classifier for MCs detection process consists of the following steps: 4.1. Mammogram preprocessing the film-artifacts from the image. There are small emulsion continuity faults on the image, which look like microcalcifica-tions. These artifacts are usually sharply defined and brighter than the microcalcifications, and the size of the artifacts is within 5 5 pixels in the mammogramms of DDSM database used in our experiments.

Consider two windows centered on a current pixel ( x , y )as shown in Fig. 2 , where R 1 is the inner region and R 2 is the surrounding region. If the difference between the pixel value at the current position( x , y ), I ( x , y ), and the mean value of the surrounding region A ( x , y ) is larger than a threshold value, T , then the pixel value of the current position is substituted by the mean value of the surrounding region R 2 . The output of the film-artifacts removal filtering I 1 ( x , y ) is defined as
I  X  x , y  X  X 
In (17), the threshold value T is empirically selected from many digitized mammograms so that the film-artifacts can be removed, and the microcalcifications can be preserved. In our experiments, we set the threshold T  X  A ( x , y ) 10%. The fixed value 10% is empirically set through a large number of experiments.
After removing the artifacts, the mammogram background should be suppressed. A high-pass filter is designed to preprocess each mammogram before extracting samples. With the Gaussian where m  X  4 s 2  X  1, experimentally in the study s  X  2 px. The output of the high-pass filter is denoted by I 2 ( x , y )  X  I f ( x , y ) * I 1 ( x , y ), where * is the linear convolution.
To enhance the spot-like characteristics (microcalcifications), the top-hat operation is performed, I 3  X  x , y  X  X  I 2  X  x , y  X  X  I B  X  x , y  X  X  B  X  x , y  X  , where is a morphological erosion operation, is a morphological dilation operation, and B ( x , y ) is a structure element. In our experiments, B ( x , y ) is a flat, disk-shaped structur-ing element with a radius of R . Obviously, the value of R depends on the image resolution. In our case, we take an empirical value, 5 for R . As an example, Fig. 3 shows an example of preprocessing results. The filter and the enhancement method seem to be effective in reducing the inhomogeneity of the background, and the microcalcifications are enhanced in the mammogram. 4.2. Input patterns for MCs detection
In the subspace learning stage, we transform A w w into a feature vector x .A w w is a small window of w w pixels, centered at a location of interest. The choice of w should be large enough to include the MCs (in our experiments, we take w  X  115). The task of the TWSVM classifier is to decide whether the input block (window) A w w at each location is a MCs pattern ( y  X  X  1) or not ( y  X  1). 4.3. Training data set preparation
The procedure for extracting training data from the training mammograms is given as follows. For each MCs location in a training mammogram set, a window of w w image pixels centered at its center of mass is extracted; the area is denoted by A i w w , with respect to x i after subspace feature extraction, and then x i is treated as an input pattern for the positive sample ( y  X  X  1). The negative samples are collected ( y i  X  1) similarly, except that their locations are randomly selected from the non-MCs locations in the training mammograms. In the procedure, the window in the training set is allowed to overlap with the other training window with some intervals. 4.4. TWSVM training and optimization parameter selection
After the positive and negative training samples are gathered, some parameters should be initialized first, such as the type of kernel function, its associated parameters, and the regularization parameters C 1 and C 2 in the structural risk function. These parameters were optimized by means of k -fold cross validation. The k -fold cross validation consists of the following steps:
Step 1: Divide all available training samples randomly into k equal-sized subsets.

Step 2: For each model-parameter setting, train a TWSVM classifier k times, during each time one of the k subsets is held out in turn while all the rests of the subsets are used to train the TWSVM, the trained classifier is then tested using the held-out subset, and its classification error is recorded.
Step 3: Average the classification errors to estimate the generalization error of the TWSVM classifier.
 Step 4: Adopt the model with the smallest generalization error.
Once the best parametric settings (i.e., the type of the kernel function and its associated parameters) are determined, the TWSVM classifier is retrained by using all available samples in the training set to obtain the final form of the decision function. The resulting classifier will be used to detect the MCs in mammograms. 5. Performance evaluation 5.1. Mammography database
In this part, the digital database for screening mammography (DDSM) database ( Rose et al., 2006 ) built by University of South
Florida is used, which is available for research at ( Web-Site, 2009 http://marathon.csee.usf.edu/Mammography/Database.html ). In making the database, the optical density range of the scanner was 0 X 3.6 (OD). The 12bits digitizer was calibrated so that the gray values were linearly and inversely proportional to the optical density. In our experiments, all selected images are intensity images, digitized at 43.5 m m /pixel and a 12-bit gray scale. In the
DDSM database, the boundaries for the suspicious regions are derived from markings made on the film by at least two experienced radiologists. Each boundary for the abnormality is specified as a chain code, which allows us to easily extract ROIs for each of the suspicious areas in the image files.

To evaluate the proposed MCs detection framework, a set of 267 images of clinical mammograms from the DDSM database was selected to form the evaluation database. In our experiments, the negative samples were automatically selected from the normal breast region, while the positive sample dataset were manually selected from the suspicious areas of each selected images, following the reported ROI selection method in
Nakayama et al. (2006) . A 115 115 window (approximately 5mm 5 mm) was chosen as the ROI size, since the microcalci-fication cluster was defined as a region containing three or more microcalcifications per 5 mm 5 mm area. Therefore, we need to select ROIs at a shorter interval so that the center of a micro-calcification cluster will be at the center of one of the ROIs.
Although we must select ROIs at intervals of 1 pixel (0.0435 mm) to analyze a mammogram in detail, there were no large differ-ences between adjacent ROIs selected at intervals of 1 mm.
Therefore, we selected the ROIs at intervals of 23 pixels (approxi-mately 1 mm) so that one ROI would overlap with the adjacent
ROIs. So we can get more positive ROIs because of the over-lapping. In our experiments, we got 2231 positive samples in our dataset, and the negative samples were not limited (e.g., we choose 8364 negative samples) because we could get a lot more normal tissues than the suspicious areas.
 5.2. Performance evaluation criteria
TWSVM classifier, receiver operating characteristic (ROC) curves are used as a criterion ( Fawcett, 2006 ). ROC analysis is based on statistical decision theory and is commonly adopted in classifica-tion performance evaluation. A ROC curve is an illustration of the classifier X  X  true positive detection rate (TPR) as a function of the classifier X  X  false positive detection rate (FPR). TPR, which is also called as sensitivity, determines a classifier on classifying positive instances correctly among all positive samples available during the test. FPR, on the other hand, defines how many incorrect positive results occur among all negative samples available during the test. Usually, 1.0-TPR is also known as specificity.
The Area under the ROC curve (Az) is an accepted way of comparing classifier performance. A perfect classifier has a TPR rate of 1.0 (or 100%) and FPR rate of 0.0% and therefore an Az of 1.0. A higher Az indicates a greater discrimination capacity of a classifier. We constructed the ROC curves by using different thresholds to classify the samples of the test set. 6. Experimental results the selection of the DDSM database (available at Web-Site, 2009 ).
The data in the training, test, and validation sets were randomly selected from the preprocessed dataset. Each selected sample was covered by a 115 115 window whose center coincided with the center of mass of the suspected MCs. There are total 10,595 blocks, including 2231 with true MCs and 8364 with normal tissue. Table 1 and Fig. 4 give a summarization of the selected blocks between the two data sets, in which, 75% of the blocks were assigned to the training set, 25% to the test set, while in the training stage, the 75% training set was divided into two subsets: one for training (50%), and another for validation (25%). 6.1. Subspace learning and feature extraction
For the classifier training, the feature extraction is very important. Here, we use the subspace learning methods to realize the feature extraction. That is to say, the subspace learning is used to obtain the mapping function or projection matrix(es) from the sample space to the feature space. Based on the projection matrix(es), the feature vector or tensor of each sample (image block) can be extracted by mapping the sample into the feature space. In our experiments, 500 negative samples and 500 positive samples (about 25% of all positive samples) were randomly selected to form the training set X t for the subspace learning.
Four subspace learning algorithms, PCA, TSA, LDA and GTDA are used to obtain the corresponding projection matrix(es) as shown in Table 2 .

We take the TSA subspace learning as an example to describe the feature extraction procedure. For the given training set
X t A R 115 115 ( t  X  1,2,...,1000), the optimal projection matrices
U  X  X  u 1 , u 2 , ::: , u l 1 A R 115 47 and V  X  X  v 1 , v 2 gotten by tensor subspace analysis with the accumulative con-tribution ratio of 0.95 ( He et al., 2005 ). By the projection formula, one can extract feature for any sample X A R 115 115 , i.e.,
Y  X  U T XV A R 47 33 . Then, Y is vectorized and fed to the classifier (TWSVM or SVM) to learn the model parameters. 6.2. TWSVM training and model selection
To get the optimal parameters in a model selection, such as the type of kernel function, its associated parameters, and the regularization parameter c 1 and c 2 in the structural risk function, we employed the 10-fold cross validation method. As shown in
Table 1 and Fig. 4 , 75% of the blocks were assigned to the training set (also, the cross validation dataset). While in the cross valida-tion stage the 75% dataset was divided into two subsets: one for training (50% of all), and the other for validation (25% of all). Once the best parameters are determined, the TWSVM classifier is retrained by using all available samples in the training set (75% of all) with the optimal fixed parameters to obtain the final form of the decision function.

In addition, the TWSVM classifier was trained by using an increased number of positive and negative training samples. At first, we used 50 positive and 50 negative samples to train the classifier, and then 50 training samples were added in the each next training stage. The experiments show that when we increase the training sample size, the more discriminant classifier is obtained, but when the number was increased to about 200, no significant improvement was observed in the generalization error of the resulting TWSVM classifier based on TSA and GTDA. This is because TWSVM classifier based on TSA and GTDA can work well for the small size samples. While when the number was increased to about 500, no significant improvement was observed in the generalization error of the resulting TWSVM classifier based on LDA and PCA. It is believed that this is maybe due to the redundancy among the collection of negative and positive samples.

In training stage, generalization error, a metric to measure the trained classifier, is used as the total number of incorrectly classified examples divided by the total number of samples classified. Generalization error was computed using only those samples held-out during training. c 1 and c 2 are set to be equal (i.e., c  X  c 2 ). In Fig. 5 (a), we summarize the results for trained TWSVM classifier with a polynomial kernel. The estimated generalization error values are shown versus the regularization parameters c and c 2 for the kernel order p  X  2, p  X  3 and p  X  4. Similarly, Fig. 5 (b) summarizes the results with RBF kernel. Here, the generalization error is plotted for different values of the width s (2, 7, 10, 15, 17, 20).

For the polynomial kernel, it has been observed that the smallest generalization error is got when p  X  3 and c 1 and c between 100 and 1000. A similar error level was also achieved by the RBF kernel over a wide range of parameter settings (e.g.,  X  15 and c 1 , c 2 A [100, 1000]). These results indicate that the performance of the TWSVM classifier is not very sensitive to the model parameters. Indeed, essentially similar performance was achieved when s was varied from 10 to 20.

Having known that the TWSVM results do not vary signifi-cantly over a wide range of parameter settings, next we focus on a particular, representative configuration of the TWSVM classifier, having a RBF kernel with s  X  15 and c 1  X  c 2  X  1000. using the same training procedure as for the TWSVM. The best error level (3.1%) was achieved when an RBF kernel with s  X  15 and the regularization parameter c was set to 10. Similar to
TWSVM, the SVM classifier was retrained using these parameters with all the samples in the training set.
 on the two nonparallel planes) was 83 (500 positive and 500 negative samples); for SVM, the number of SVs was 285 (500 positive and 500 negative samples). The TWSVM classifier is much more effective than traditional SVM methods. 6.3. Performance evaluation results based subspace learning method, using both TWSVM and SVM. To simplify the task, 1000 MCs and 1000 normal samples were randomly selected as a dataset pool for the evaluation task. 75% of all samples (1500) in the dataset pool were selected for training, the rest (500) for test. To evaluate the stability of each method, we repeat the sampling 20 times so that we can compute the mean and standard deviation of the detection accuracy, TPR and FPR. In the each sampling of 20 times, we fixed the test samples (500), and we reduced the fraction of all the training samples from 95% to 5%. That is we will perform the detection task 20 rounds, and we randomly select training samples with the decreased percentage from 95% to 5% to train classifiers in each round. The trained classifiers are evaluated using the 500 test samples. The first round of 20 test results with 50% of the training samples are summarized by using ROC curves in Fig. 5 with the following methods: PCA  X  TWSVM, LDA  X  TWSVM, TSA  X  TWSVM and GTDA  X  TWSVM. For comparison,
ROC curves are also shown for the traditional SVM classifier after subspace learning: PCA  X  SVM, LDA  X  SVM, TSA  X  SVM and
GTDA  X  SVM, correspondingly. Average experimental results of 20 rounds with TWSVM and SVM classifier based on different subspace learning algorithms are shown in Table 3 ,with50%ofallthe training samples to train classifiers.

Fig. 6 also illustrates that TWSVM classifier with GTDA preprocessing has a higher detection performance compared to the other subspace learning methods. Similarly, SVM classifier with GTDA preprocessing also has the same result. It also can be seen that the tensor representation subspace learning algorithms will preserve more discriminate information for MCs detection, which will be fed with the classifier in the next stage. By using the same subspace extracted feature, compared with SVM, TWSVM has a better detection rate and runs about 4 times faster than the SVM when we train the classifier.

In particular, the TWSVM classifier achieved the best average sensitivity of approximately 94.41 7 1.16% with respect to 8.32 7 1.04% false positive rate and Az  X  0.9677 7 0.0574. With the same training data set and test data set, the SVM classifier achieved the best average sensitivity of 94.31 7 1.15%, 9.31 7 1.63% false positive rate and Az  X  0.9482 7 0.0506. Compar-isons of our methodology with others reported in the literatures are not straightforward because those experiments were con-ducted on different datasets. Using fractals models and neural networks, Bocchi et al., (2004) reported cluster detection results of about TPR  X  87% and FPR  X  7% in their test set. Jiang et al. (2007) used a genetic algorithm design to classify and detect MCs with manually selected 300 MC-present blocks and 300 non-MC blocks from DDSM, and achieved their experimental results with
Az  X  0.987. Experiments and MIAS dataset are reported in ( Papadopoulos et al., 2005 ). The performance of the SVM was Az  X  0.79 and 0.77 for the original and enhanced feature set from
Nijmegen database, respectively, while for the MIAS dataset the corresponding characterization scores were Az  X  0.81 and 0.80.
With neural network classifier, the corresponding performance for the Nijmegen dataset was Az  X  0.70 and 0.76 while for the MIAS dataset it was Az  X  0.73 and 0.78.

To further evaluate the proposed application framework, we also carried out experiments by applying the trained subspace learning algorithm (GTDA) and classifier model (TWSVM) to 30 full mammograms randomly selected from the DDSM database. In these experiments, the mammograms are scanned and processed with a window of 115 115 pixel along the raster-scanning order, that is, each such window is regarded as an image block as what we have manually selected in the evaluation stage, and no prior knowledge about any of the full mammograms is given to the trained TWSVM in MCs detection. Therefore, the trained model to this evaluation does not know all the mammograms. In the experiments, each mammogram is first preprocessed as described in Section 4 , and a 115 115 window is then moved from top-left to bottom-right along the raster-scanning order at intervals of 1 mm (approximately 23 pixels) in the resulted mammogram. For each of the processing windows, it is first transformed into the GTDA subspace, and then the mapped data are fed to the trained TWSVM to make a decision whether it contains MCs or not. Our experiments reveal that, on average, one full mammograms with about 5000 3000 pixels form DDSM requires about 10 min to be processed implemented with MATLAB on an Intel Core 2 CPU 1.86 GHz, DDR2 2 GB PC. From these experiments, we got 43 false positive on the 30 evaluation mammograms and achieved with 1.43 FP per image. As an example, we demonstrated the MCs detection result in Fig. 7 , compared with the original image and its corresponding ground truth. 6.4. Experimental result analysis
To compare PCA, LDA, TSA and GTDA based subspace learning methods with different percentages of the total training samples, we present the statistical results of accuracy, TPR, and TNR(1-FPR) in Tables 4 X 6 . We also visualize the average detection accuracy rates in Fig. 8 . Fig. 8 shows a plot of detection accuracy rates versus different fractions of training samples used in the model training stage. From left to right in the figure, there are 10 bar groups (according to the percentage of 95%, 85%, 75%, 65%, 55%, 45%, 35%, 25%, 15%, 5%). In each bar group, there are four bars, which correspond to the performance of PCA  X  TWSVM, LDA  X  TWSVM, TSA  X  TWSVM and GTDA  X  TWSVM, respectively.
Meanwhile, we illustrate TPR and TNR (1-FPR) versus different fraction of training samples in Figs. 9 and 10 in the same way.
Here, we draw the following observations from the above experi-mental results: (1) the more training samples we use, the better the detection accuracy rate we get, when the size of training sample is smaller than a threshold (e.g. the threshold is about 200 in our experiments). This is because that a larger set of training (2) with the increase of the training samples, it is found that the (3) GTDA based detection methods get the best average TPR than (4) In summary, TSA and GTDA based subspace learning methods 7. Conclusions
This paper presents a novel framework to detect microcalcifi-cation clusters (MCs) in digital mammograms. In this framework, subspace learning algorithms, PCA, LDA, TSA and GTDA, are employed to extract subspace features of each positive sample and negative sample, and a twin support vector machine (TWSVM) is trained as supervised learning to examine whether a selected block of the mammogram has MCs or not. The decision function of the trained TWSVM classifier is determined by support vectors that are identified from the samples during training stage. Upon a set of 267 mammograms, experimental results show that the proposed framework is effective and efficient. Based on solid comparison results, the TWSVM classifier achieves relatively lower generalization error while being applied to classify samples that are not included in the training set. In addition, tensor based approaches, e.g., tensor space analysis (TSA) and general tensor discriminant analysis (GTDA), have also been demonstrated to outperform conventional LDA and PCA in small size sample problems. Furthermore, ROC curves clearly illustrate that the newly proposed TWSVM based subspace learning approaches give the best performances.
 References
