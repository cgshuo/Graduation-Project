
Test collections are most useful when they are reusable, that is, when they can be reliably used to rank systems that did not contribute to the pools. Pooled relevance judgments for very large collections may not be reusable for two rea-sons: they will be very sparse and not sufficiently complete, and they may be biased in the sense that they will unfairly rank some class of systems. The TREC 2006 terabyte track judged both a pool and a deep random sample in order to measure the effects of sparseness and bias.
 The relevance assessments were divided into two phases. In the first phase, a subset of the runs submitted to the terabyte track were pooled to rank 50. This pool has an av-erage of 640 documents per topic, 118 of which are relevant. Judgments from this pool are adequate to compute MAP to a reasonable precision for the participating runs.
In the second phase, a random sample was drawn from the pool runs starting from rank 1 and reaching to a topic-specific depth. The sampling strategy, relevance-based sam-pling , has two parameters: the maximum sample depth, and the sampling rate. These are estimated using the judgments from the depth-50 pool with the goal of finding 20 new rele-vant documents out of 200 additional judgments. This goal is called the  X  X elevant-percent-target X , or rpt =20 / 200.
The depth for a single topic is computed as follows. We first compute the probability of relevance in the pool at depth 50 as P ( rel )= | R | / | J | , where | R | is the number of rel-evant documents and | J | is the number of documents judged. As we go deeper into the pool, this probability of relevance drops off exponentially, but for simplicity and because we are focused on relatively early ranks, we approximate this trend with a linear fit with a fixed slope representing  X  P ( rel 50 ranks, and compute a rank x where the relevant fraction y = rpt :
Following this fit, we estimate the size of a pool that con-tains 20 additional relevant documents per 200 judged as
For topics with very few relevant documents in the depth-50 pool, this size can by small or even negative, so we addi-tionally require that the estimated pool size have 200 more documents to judge. We then estimate the depth for a pool of that size to be poolsize  X   X / | J | where  X  is 50. Once this depth is determined, we pool the runs to that depth, and compute the sampling rate as 200 divided by the number of unjudged documents in that pool.

The maximum depth of the sample for the 2006 terabyte topics varies from 57 to 1252, with an average of 314. Topics with very few relevant documents in the depth-50 pool have a shallow sample depth and a sampling rate close to 100% because the best chance to find relevant documents is in the next few ranks. Topics with many relevant in the pool have a deep sample depth and a lower sampling rate.

The sample has an average of 492 documents per topic, of which 36 are relevant. This includes 211 documents on average per topic not in the depth-50 pool, of which 14 are relevant. This shows that our estimation procedure was in fact quite accurate given the simplistic fit. We found at least one new relevant document for 46 out of 50 topics, and 20 or more for 10 topics.
The systems were scored using mean average precision (MAP) with the pooled judgments, and inferred average precision (infAP) [3] with the sampled judgments. InfAP is an estimate of average precision. When judgments are complete, infAP and MAP are equal. In the presence of unjudged documents, if they were in the original pool but were not sampled for judging, infAP estimates the precision at those ranks using the precision at earlier ranks. Other-wise, they are treated as nonrelevant as in MAP.

Figure 1 plots each run X  X  MAP score based on the depth-50 pool against its infAP score based on the sampled judg-ments. For all but four runs, the infAP score is lower than the MAP score. The Kendall X  X   X  between the two rankings is 0.8, implying that the rankings have notable differences despite being highly correlated. Since those topics with few relevant documents in the depth-50 pool are represented by a nearly 100% sample, any difference must come from those topics where we sampled deeply. The most likely reason is that very highly-ranked documents were missed. If a rele-vant document is retrieved at rank 1, this has a very large effect on MAP, but if that document is not sampled, infAP will necessarily be less than MAP. This problem is particu-larly acute in topics with a very low sampling rate.
Another goal of sampling deeply was to try and get differ-ent relevant documents than those higher in the rankings. Buckley et al. observed that in very large collections, pools can be dominated by documents containing the title words of the topics; this bias in the judged documents could be unfair towards retrieval approaches that do not focus exclu-sively on the topic title. They proposed a measure, title-stat rel , which is the occurrence of the average title word in the judged relevant documents [2].

The titlestat rel of the depth-50 pool and the sample are 0.93 and 0.899 respectively. When we consider just the sam-pled documents below rank 50, the titlestat rel is 0.851. A topic-by-topic analysis reveals that the sampling scheme indeed found lower titlestat documents when we sampled deeply but not in every topic, and sometimes they were found without needing to search so deeply. This illustrates that title-word bias has a strong topic effect. For some top-ics, the title words are really the best indicators of document relevance. For others, there are other useful words not in the topic title.
We next looked to see if the sampled judgments are any more or less reusable than the depth-50 pooled judgments. When a group does not contribute to the pool, documents retrieved only by their system are never judged, resulting in a less accurate score. We removed each group X  X  runs from both the pool and the sample, and measured the score dif-ference and movement in the ranking for those runs. The following table shows that a held-out systems X  MAP and infAP scores change by a similar amount, but the infAP ranking changes less, indicating that the sampled judgments are more reusable.
To measure any effect that random sampling itself might have on infAP scores, we drew 100 random subsamples of the depth-50 pool, and used these qrels subsets to score the runs using infAP. The samples were drawn at the same rates as were used above, but we sampled within the pool only so that every document would be judged. An analysis of variance of the infAP scores as a function of topic and sample showed that the sample was not a significant effect (at  X  =0 . 05) but that topic was significant for 45 out of 50 topics. Incorporating the runs into the model is compli-cated simply because runs normally vary in effectiveness in a topic-dependent fashion. We computed a second ANOVA of infAP score by topic and sample within each run. For two runs, sample was moderately significant ( p = 0.016 and 0.014); these were two runs from the same group, and their maximum average infAP scores in any sample were 0.1002 and 0.0866. From this, we conclude that the variance across samples such as those we are drawing should not be a worry.
We have a number of unanswered questions. Are the sampled judgments are  X  X ore fair X  to future runs than the pooled judgments? This is actually quite a difficult question to answer definitively. In smaller TREC collections, we have a sufficiently complete set of judgments that can be thought of as  X  X ruth X , but no such set exists for the terabyte collec-tions. Furthermore, it X  X  hard to know for sure if the sample is less biased than the pool because of the large topic effect in titlestat. Whether we can sample deeply to overcome bias depends on whether the bias exists for the topic.
Is there a simple, optimal sampling strategy that balances meaningful measures of effectiveness with reusability and low bias? Uniform random sampling is not usable because it will not select enough relevant documents at any reason-able sampling rate. If the sample rate is too sparse, then it is likely we will miss judging documents from the first two or three ranks, which are critical to MAP. Our relevance-based sampling strategy suffers from this. On the other hand, sampling strategies such as [1] focus too strongly on these early ranks, and as such fall prey to title-word bias. One option would be to always judge the first one or two ranks, then sample. Our depth-50 pool, while good for es-timating MAP, is almost certainly more than we needed to pick a sampling depth. Additionally, at the  X  X eep end X  of the sample, we can X  X  be certain that we X  X e located enough low-titlestat documents to make the collection sufficiently more fair. Currently, we are investigating whether stratified sampling strategies can solve these problems. [1] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical [2] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. [3] E. Yilmaz and J. A. Aslam. Estimating average
