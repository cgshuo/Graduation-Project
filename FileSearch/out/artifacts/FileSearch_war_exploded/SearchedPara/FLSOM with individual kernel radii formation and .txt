 1. Introduction
The main objective of this paper is to present an application to a pickling line of the steel industry of a proposed modified version of the Fuzzy Labeled Self-Organizing Map (FLSOM) algorithm. A comparison of the FLSOM algorithm with its proposed modifica-tion is carried out in terms of mean quantization and topographic errors. In this proposed version of FLSOM, the kernel radii of the neurons are individually updated, maximizing the mutual information between the input and output of the neuron and minimizing the mutual information between the outputs of the neurons. A significant reduction of the mean quantization error is expected due to this reform.

This work is part of the SensorControl project entitled  X  X  X ensor-based online control of pickling lines X  X . The main objective of the
SensorControl project is the development of a sensor for acidic measurement and supervision techniques of coil surface defects for implementation in steel pickling plants.

The aim of the application is to obtain a model trained with categorical and numerical process variables preserving the topological distribution of the output space in order to reach a visualization of the industrial process. In the pickling line, the categorical variables are the defect types of the coil, whereas the numerical variables are represented by line speed, acid concen-tration in tanks, etc. This supervised classification task can be carried out by the FLSOM algorithm which is based, at the same time, on the topology preservation property. The algorithm constitutes a robust classifier yielding efficient learning even with fuzzy labeled or partially contradictory data. Moreover, the algorithm was modified in order to achieve a better approxima-tion to the numerical variables by means of decreasing the mean quantization error using an individual adaptation of the kernel radii.

Finally, the model was used to obtain an estimation of the optimum line speed that is associated with each acid concentra-tion, temperature range, thickness and type of steel. This optimum speed minimizes as many overpickling defects as underpickling defects which are quantified using an automatic inspection system.
 A kernel-based topographic map formation algorithm ( Van Hulle, 2002b) that is aimed at maximizing the map X  X  joint entropy of the kernel outputs and, at the same time, to minimize the mutual information between these outputs was considered to be integrated into the term of the FLSOM algorithm related to the numerical variables of the process. There are other kernel-based topographic map formation algorithms formulated by the same author where the learning rules and the kernel definitions differ from the first approach: the kernel-based maximum entropy learning rule (kMER) ( Van Hulle, 1998 ) updates the prototypes and kernel radii depending on discrete activations of the kernel outputs, whereas in the local density estimation (LDE) algorithm (Van Hulle, 2002a ) the adaptation of the training parameters is according to the assumed Gaussian local input density.
Other works from differing authors are the soft topographic vector quantization (STVQ) and Soft-SOM (SSOM) algorithms (Graepel et al., 1997 ) which use a different conceptual kernel definition and the adaptation of the kernel radii is not individual, implying that a better mean quantization error cannot be obtained. Yin and Allinson (2001) minimizes the Kullback X  X eibler divergence between the true and the estimated input density using Gaussians with different variances. Although the learning rules of the training parameters are quite similar to Van Hulle (2002b), some distinctions lead to quite different kernel distribu-tions. Van Hulle includes a neighborhood function L helping the topographic preservation and the adaptation depends on the kernel activity, not on the input density as in Yin and Allinson (2001) .

Although Yin and Allinson (2001) obtains very good results, the kernel-based topographic map formation in Van Hulle (2002b) was selected to be integrated into the FLSOM algorithm based on the lower mean quantization error compared with the rest of the described methods obtained for different datasets according to the work of Van Hulle (2002b).
 The paper contains the individual kernel radii updating in
Section 2 and FLSOM algorithm in Section 3. The proposed modification of FLSOM is presented and tested in Sections 4 and 5.
The work related to the application of the proposed algorithm to a pickling line and its results are presented in Section 6. 2. Kernel-based topographic map formation
The SOM algorithm ( Kohonen, 2001 ; Kohonen et al., 1996 ) has a kernel function centered on the winning neuron that is usually considered as a Gaussian. In this way, a neighborhood radius is created influencing to a high degree the neighbor neurons of the winning unit while the values of the prototype vectors are updated during the training. This kernel function is responsible for the SOM property of topology preservation. However, the quantization error can be improved modifying this kernel function by means of maximizing the mutual information between the input and output of the neuron and minimizing the correlations between the outputs of the neurons ( Van Hulle, 2002b). This can be achieved maximizing the differential entropy (Bell and Sejnowski, 1995 ) whenever the kernel output distribu-tion is uniform.

In Van Hulle (2002b) the distribution of the squared Euclidean distances of the data vectors to the mean of the Gaussian kernel is obtained and its output is defined according to the cumulative of that distribution as an incomplete gamma distribution. The learning rules appear in Eqs. (1) and (2). They are obtained from the entropy of the kernel output by means of derivation with respect to the prototype vector w i and the kernel radius r
D w  X  Z w L  X  i ; i ; s L  X   X  Z s L  X  i ; i ; s L  X 
L  X  i ; i ; s L  X  X  exp J r i r i J  X  t  X  X  s L 0 exp 2 s L 0 where v is the input data vector, Z w is the learning rate for prototype vectors, Z s is the learning rate for kernel radii, d is the number of dimensions in the input data space, t is the time step, t is the maximum number of time steps, r i and r i are the lattice coordinates of the updated neuron i and the winning neuron i respectively, L is a monotonous decreasing function of the lattice distance from the winner (in this case a Gaussian), s L and s the neighborhood range, which is used as a neighborhood cooling term and its initial value, respectively. Term (3) is added to allow competitive X  X ooperative learning. The winning neuron and its neighborhood determine the values during the training giving the topological information. Moreover, the values of the kernel radii are updated individually using this term avoiding the statistical dependency between the outputs of the neurons. 3. FLSOM algorithm
Using SOM for classification tasks is an important feature to identify the clusters of the input data space, their relationships between each other and with the process variables. A classic method of classification after training consists of carrying out an assignment of each prototype vector to a certain cluster obtaining a new component plane. In this case, the clustering process is composed of two phases ( Vesanto and Alhoniemi, 2000 ). In the first phase, the SOM network is trained whereas in the second phase a partitive clustering algorithm, e.g. K-means, is applied to obtain several clustering structures of several numbers of clusters since the optimum number of clusters is unknown. The optimum clustering structure is chosen by means of a clustering validation index. This procedure is useful when the number of clusters and the classification (fuzzy or crisp distribution) of each data vector are not known beforehand ( Lo  X  pez and Macho  X  n, 2004; Macho  X  n and
Lo  X  pez, 2006 ). However, these methods have the drawback of not influencing the training since they occur subsequently and do not modify the values of the prototype vectors.

This can be corrected using FLSOM ( Villmann et al., 2006a ). In this algorithm, the classification task influences the values of the prototype vectors and both of them take place at the same time during the training. In this way, FLSOM can be considered as a semisupervised algorithm. The training algorithm is based on an energy cost function of the SOM ( E SOM ) proposed by Heskes (1999) . The cost function of Eq. (5) includes a term ( E represents the labeling or classification error of the prototype vectors y i of the classification map with regard to the probabilistic vectors x supplied by the training dataset: E FLSOM  X  X  1 b  X  E SOM  X  b E FL ;  X  5  X 
Each labeling data vector x , which is associated with a numerical data vector v , is assigned in a fuzzy way according to a probabilistic membership to the clusters of the dataset. A Gaussian kernel in the input data space is included within term E so that the prototype vectors w close to the data vectors v determine the classification task. It is formulated as g  X  v ; w i  X  X  exp J
The learning rules are obtained by means of derivation of energy cost function (5) with respect to the numerical and labeling prototype vectors, w i and y i , respectively. These learning rules are expressed in Eqs. (7) and (8) considering using the squared Euclidean metric in the algorithm:
D y  X  a l b g g  X  v ; w i  X  X  x y i  X  ;  X  7  X 
D w 4. Modification of the algorithm
As stated above, FLSOM is formulated according to energy cost function (5). First term E SOM corresponds to the typical approx-imation of the numerical prototype vectors w to the input data vectors v . This prototype update is carried out by the gradient of E
SOM or @ E SOM =@ w i that turns out the first term of learning rule (8) considering the squared Euclidean as metric to be used. It can be observed that this term corresponds to the basic sequential version of the SOM training algorithm where the numeric prototype vectors w i are updated. In this version, the kernel radii take the same value instead of being individually updated as in the kernel-based topographic map formation proposed in Van
Hulle (2002b). This individual updating should imply an im-provement of the mean quantization error of the trained map. In this way, the original version of the FLSOM has been extended to this individual kernel radius updating obtaining learning rule (9) to update the numerical prototype vectors w i :
D w i  X  a w  X  1 b  X  L  X  i ; i ; s L  X  v w i r 2
The algorithm is completed with learning rules (2) and (7) to update kernel radius r i and labeling prototype vector y i respectively.

Therefore, there are two mean quantization errors. The first is quantization error e qw related to the approximation of numerical prototype vectors w i to numerical data vectors v and the other is quantization error e ql related to the approach of labeling prototype vectors y i to labeling data vectors x . Both errors can be weighted by parameter b of the cost function.

Two maps are obtained after training with this algorithm. One map contains the component planes that correspond to the numerical variables and it is related to topographic error e other map represents the component planes of the clusters defined in the dataset.

Several advantages are obtained by means of the use of this algorithm. The visualization of the clusters improves the data understanding. Moreover, the algorithm is a robust classifier since it uses fuzzy datasets allowing the use of contradictory data. 5. Experimental testing
Four datasets were used to check the algorithm performance according to the mean quantization error and the topographic error. This topographic error measures the percentage of map units whose two BMUs are not adjacent in the output space (Kiviluoto, 1996). The map size must be equal for both algorithms since the errors depend on it. Parameter b was chosen equal to 0.5 for both of them taking an intermediate weight of the quantiza-tion errors e qw and e ql .

In this case, the algorithms are being evaluated on the same data that is used for training. This is tolerable for errors e e tw because both of them measure a kind of distortion which should basically be the same for both training and test data. Since the aim is to improve e qw , that option has been chosen.
In FLSOM algorithm the radii s  X  t  X  decrease monotonically and linearly. The advisable minimum is equal to 1, since any value below produces maps which do not preserve the data topology.
The training was carried out in two phases according to Vesanto et al. (1999): first with large neighborhood radius in phase 1, and then finetuning with small radius in phase 2.

Parameter g represents the kernel radius in the input space and it was considered as a constant since it is not described in detail in Villmann et al. (2006a) . It affects both quantization and topographic errors. Its value was chosen equal to 0.5 because it seems the most appropriate for a normalized input data distribution with zero mean and unitary variance.

Iris dataset : The iris dataset was extracted from UCI repository of machine learning databases ( Newman et al., 1998 ). It was con-sidered as a crisp distribution and is composed of 150 instances, four numerical variables and three clusters. The trained map size was a lattice of 8 8. Regarding FLSOM, the number of epochs was equal to 150 for both training phases. The kernel radius s  X  t  X  was within interval [2,1] in phase 1 and it was equal to 1 in phase 2. The parameters for this algorithm were b  X  0 : 5, a  X  0 : 01, a 0 : 1, g  X  0 : 5. The number of epochs was equal to 150 in relation to the proposed algorithm and its parameters were b  X  0 : 5, a 0 : 001, a l  X  0 : 01, g  X  0 : 5, Z s  X  0 : 0001 a w , s L 0
Balance scale dataset : This dataset was extracted from UCI repository of machine learning databases ( Newman et al., 1998 ). It was considered as a crisp distribution and it has 625 instances, four numerical variables and three clusters. The map size was 10 10. Regarding FLSOM, the number of epochs was equal to 30 for both training phases. The kernel radius s  X  t  X  was within interval [3,1] in phase 1 and it was equal to 1 in phase 2. The parameters for this algorithm were b  X  0 : 5, a  X  0 : 01, a  X  0 : 5. The number of epochs was equal to 30 in relation to the proposed algorithm and its parameters were b  X  0 : 5, a w  X  0 : 5, g  X  0 : 5, Z s  X  0 : 0001 a w , s L 0  X  4, t max
Fuzzy dataset : The fuzzy dataset was formed as a fuzzy distribution of three clusters applying fuzzy c-means to a random dataset composed of 200 samples of two dimensions. It was formulated in this way since it was difficult to find a correct fuzzy distribution in a known database. The trained map size was equal to 10 10. Regarding FLSOM, the number of epochs was equal to 150 for both training phases. The kernel radius s  X  t  X  was within interval [3,1] in phase 1 and it was equal to 1 in phase 2. The parameters for this algorithm were b  X  0 : 5, a  X  0 : 01, a  X  0 : 5. The number of epochs was equal to 150 in relation to the proposed algorithm and its parameters were b  X  0 : 5, a w  X  0 : 01, g  X  0 : 5, Z s  X  0 : 0001 a w , s L 0  X  4, t max
SensorControl dataset : The data from the SensorControl project was considered as a crisp distribution and is composed of 215 instances, five process variables and three clusters. The lattice of the trained map was 11 11. Regarding FLSOM, the number of epochs was equal to 30 for both training phases. The kernel radius  X  t  X  was within interval [3,1] in phase 1 and it was equal to 1 in phase 2. The parameters for this algorithm were b  X  0 : 5, a  X  0 : 5,  X  0 : 5, g  X  0 : 5. The number of epochs was equal to 30 in relation to the proposed algorithm and its parameters were b  X  0 : 5,  X  0 : 01, a l  X  0 : 5, g  X  0 : 5, Z s  X  0 : 0001 a w , s L 0 5.1. Results
In the proposed algorithm, the radii were initialized randomly from a uniform distribution [0, 0.1]. The weights or values of the prototype vectors ( w i and y i ) were randomly initialized for both algorithms. Fifty maps were obtained for each algorithm and dataset. The errors mentioned in Table 1 were calculated for each map and their distributions are shown in Fig. 1 . The boxes represent the typical distribution indicating the lower quartile, median, and upper quartile values. Notches estimate the medians for box to box comparison.

The results seem to prove that the mean quantization using the proposed algorithm is always less than the map trained using FLSOM, especially for e qw . Also, the distribution variance of this type of error is quite narrow and less than obtained with FLSOM.
The mean quantization error e ql related to the classification task is slightly better than obtained with FLSOM. However, the topo-graphic error of the numerical variables e tw seems to be better in
FLSOM. Although a similar value of e tw could be achieved by the proposed algorithm since its variance allowed it to be obtained. It seems that the proposed algorithm reduces the mean quantiza-tion error at the expense of increasing its number of folds in the input space. However, the definition of topographic preservation depends on the chosen tool. Besides Kiviluoto X  X  (1996) approach, other topology preservation indexes ( Bauer and Pawelzik, 1992;
Bezdek and Pal, 1995 ) carry out a more complete analysis, taking into account the neighborhood and the isometry that corresponds between the lattice neurons and the prototype vectors, but their computational costs are higher. Moreover, the original version of
FLSOM uses twice the number of epochs. 6. Optimization of the pickling line 6.1. Assessment of the pickling plant
The pickling plant of Aceralia (ArcelorMittal group) was chosen as the pickling line to be researched in SensorControl project. The flow diagram of the pickling plant is shown in Fig. 2 . Firstly, the coil is loaded in the uncoiler to be processed. The head of the coil is cut with a shear to get a correct shape. This coil is welded to the previous coil because of the continuous strip requirement. Then the strip loop is accumulated allowing the central section of the pickling plant to continue with an acceptable velocity whereas the input section is stopped due to the new coil replacement and the welding. The central section, where the treatment really takes place, begins with a scale breaker to facilitate the scale removal by means of the acid treatment. This acid treatment is carried out in four tanks using hydrochloric acid (HCl). The loop in the tanks is regulated with a magnetic sensor. Inhibitors are used to prevent overpickling. Then the strip is cleaned in five tanks with cold and hot water. At the end of the washing process there are wringer rolls and a hot air dryer. In the final section the strip loop is accumulated to permit the central section to advance during the coil removal from the recoiler and the next coil threading. Finally the strip is oiled, cut and coiled in the recoiler. 6.2. Defect calculation
A definition of the pickling defects of the strip was achieved by means of an automatic inspection system. The inspection technology is the standard Parsytec system which is designed for automatic inspection of fast moving materials and allows recognition of material flaws during the production. The system consists of a surface sensor that is integrated into the inspection machine to check the coil surface and a recognition server on which the images from the sensor are processed with suitable software. The surface sensor uses diffuse and direct light sources and records the images with matrix video cameras. Defect data and the corresponding defect images are stored in the database.
Underpickling defects are considered as scale residues on the edge of the strip. Overpickling defects are caused by an excessive acid attack over the strip. The number of overpickling and underpickling defects N d supplied by the database has been normalized using the coil length L and they must be penalized by the defect severity ratio r according to a simple equation (10). In addition to this defect computation, every coil is labeled with its global defect if applicable. In this case, the global defect can be overpickling or underpickling.

N  X  normalized  X  X 
The acid concentrations in the tanks are important variables but only one of them (acid concentration in tank 4) is considered because the four concentrations (one for each tank) are highly correlated. The cooling temperature at the end of the hot rolling mill line is a key process variable but it has not been taken into account because it is almost constant for the same type of steel.
Anyway, this condition must be checked in the preparation of every dataset. 6.3. Data filtering
The following constraints were taken into account during the data acquisition from the database:
One type of steel was selected and the strip thickness was equal to 2.1mm.

Regarding the dataset corresponding to coils without global defect, i.e. neither overpickling coils nor underpickling coils, the temperature in the fourth tank must be greater than 65 3 C and the line shutdown time must be less than 1s.

Only coils with an available measurement of the cooling temperature at the end of the hot rolling line have been considered since it is necessary to check if their value is the same for all of them.

The dataset of the overpickling coils was collected for coils with a line shutdown time of less than 10s.

The temperature must be greater than 65 3 C for the dataset of underpickling coils. 6.4. General method
The general method of training is the classical one. In a first stage, a selection of the most significant variables has to be carried out. These variables are the line speed, the acid concentration and the temperature in tank 4, the number of overpickling defects, the number of underpickling defects. The strip thickness and the type of steel are previously selected as stated above. In a second stage, data were normalized to a zero mean value and a unitary variance. This allows all the features to be treated by the SOM in the same way. After normalizing the process, an SOM network was trained with these variables using the algorithm proposed in Section 4. After the training process, the SOM is expected to capture the inherent geometry of the process data, allowing it to be displayed in 2D representations such as the component planes. These planes are built using gray or color levels to show the value of a given input feature for each SOM unit in the 2D lattice. 6.5. Results
Each of the probabilistic data vectors x , which is associated with a numerical data vector v (in this case a coil), is assigned in a fuzzy way according to a probabilistic membership to the clusters of the dataset. This stage is critical in the data preprocessing, but obviously the algorithm can also be applied to crisp distributions. The data samples (or coils) can be classified into three clusters: Overpickling coils with overpickling as global defect. Underpickling coils with underpickling as global defect.
Coils without global defect. Although these coils do not have any global defect, they might have any number of overpickling and underpickling defects.

This classification, not only the training variables, influences the values of the prototype vectors (estimated process variables) during the training. In this case, the classification is a crisp distribution. Although the option to employ a fuzzy distribution is very interesting, it requires a critical process understanding and it should be debated. The number of coils without global defect was 140. The number of overpickling coils was equal to 53 and the number of underpickling coils was 22. So the total number of samples (or coils) of the training dataset was equal to 215. In this way, the dataset was considered as a crisp distribution and is composed of 215 instances, five numerical variables (overpickling defects, underpickling defects, line speed, acid concentration in tank 4 and temperature in tank 4) and three labeling variables (correct coils, underpickling coils and overpickling coils). The initial values of the prototypes were randomly initialized.
The map size was a 30 30 lattice. Forty maps were obtained and the most appropriate according to its topographic preservation (Kiviluoto, 1996) is considered to estimate the optimum line speed. The number of epochs was equal to 30 and the values of the parameters were b  X  0 : 5, a w  X  0 : 01, a l  X  0 : 5,  X  0 : 0001 a w , s L 0  X  4, t max =29025.

The objective taken to be developed was to estimate the optimum line speed that minimizes as many overpickling defects as underpickling defects for each acid concentration, temperature range, thickness and type of steel. In this case, an optimum line speed is computed according not only to the process values, but also taking into account the previous classification of the coils with respect to their global defect. This optimum line speed minimizes the prototype vectors of the component planes corresponding to the overpickling and underpickling defects, as well as the labeling prototype vectors corresponding to the estimated classes of overpickling and underpickling global defects.

The procedure consists in searching the subset of neurons that assert a certain value of acid concentration and temperature for each interval of the measurement range. The neuron that has the minimum value of pickling defects is selected within that subset, and the optimum line speed is obtained from the component of the line speed of its prototype vector. The procedure continues to complete all the intervals of the measurement range.

The trained maps are shown in Figs. 3 and 4 . One trajectory on the output space is formed by the sequence of the best matching units varying the acid concentration, minimizing the defects and corresponding to a certain temperature range, see Figs. 5 and 6 . The values of the line speed component plane which belongs to a selected model according to the topographic index give the values of optimum line speed to achieve the proposed estimation. In this way, the graphics of optimum line speed versus acid concentration has been obtained for this type of steel and strip thickness equal to 2.1mm. It must be known that the standard range of acid concentration in tank 4 is from 90 to 150g/l. This graphic is shown in Fig. 7 . 7. Conclusions
In this paper a modified version of FLSOM algorithm is presented and proposed to be applied in a pickling line of the steel industry. The proposal takes advantage of two main topics. Firstly, the available classification of the global defect of the coils can be used since FLSOM is a version of SOM algorithm where the prototype vectors are influenced by the labeling data vectors that define the clusters of the dataset. On the other hand, the proposed application provides a reduction of the mean quantization error because it updates the prototype vectors by means of individual kernel radius updating according to Van Hulle X  X  approach, modifying the term that corresponds to numeric prototype vectors. The proposed modification was compared with the original one using four datasets to test both of them. The approach constitutes a robust classifier yielding efficient learning even with fuzzy labeled or partially contradictory data and achieves a better approximation to the numerical variables by means of decreasing the mean quantization error using an individual adaptation of the kernel radii.

As stated above, the model was trained with the data that belongs to the SensorControl project in order to obtain a visualization of the industrial process and to estimate the optimum line speed that minimizes the surface defects of the steel strip in the pickling line for each acid concentration, temperature range, thickness and type of steel.

A future task could be to train a Fuzzy Labeled Neural Gas (FLNG) network ( Villmann et al., 2006b) in order to obtain a lower mean quantization error estimating the optimum pickling line speed sacrificing the topological map preservation. This procedure would only be valid for this estimation and not for the visualization process since that algorithm does not preserve the output space topology. The FLNG algorithm is quite similar to
FLSOM, but instead of using a Gaussian neighborhood kernel, it uses the typical Gaussian rank kernel of the Neural Gas algorithm (Martinetz et al., 1993 ). Other interesting work would be to integrate the VISOM algorithm ( Yin, 2002, 2008 ) into the FLSOM in order to obtain a map that preserves not only the neighborhood units, but also the input space distances.
 Acknowledgments
Our warmest thanks are expressed to the following for their financial support: The Commission of the European Communities,
European Coal and Steel (ECSC), supporting SensorControl project  X  X  X ensor-based online control of pickling lines X  X  with RFS-CR-04052 as agreement number. The contractors are Betriebs-forschungsinstitut GmbH (BFI), Rasselstein GmbH (TKS-RA),
Centro Sviluppo Materialia S.p.A. (CSM), Universidad de Oviedo (UniOvi) and Svenska Milj  X  oinstitutet AB(IVL).
 References
