 1. Introduction
As the costs of fossil fuels continue to rise, it is becoming economically important to investigate other sources of energy. Additionally, increased customer demands and loads are begin-ning to outstrip the grid X  X  capacity to serve these loads. As such, distributed generation of alternative energy sources is becoming a very heated research and development area.

Currently, there are several alternative energy sources avail-able: wind, solar, hydro-electric and geo-thermal, to name a few. Hydro-electric and geo-thermal plants often require large foot prints that are at odds with developed areas and environmental considerations. Wind energy is currently enjoying very energetic growth, at around 24% per year since the year 2000 in the US ( US Department of Energy X  X  Office of Energy Efficiency And Renewable Energy, 2007 ). But, wind power has its downsides as well since not all locations receive enough sustained winds to be productive, and even then production is sometimes sporadic. Of all of the mentioned sources, solar power seems to be the most promising
In order to make PV systems cheaper (and shorten the payback period), optimal control can be used to more effectively utilize energy generated by the PV array, resulting in smaller required batteries and arrays while still maintaining the critical load. This reduction in system component size leads to a direct reduction in cost for the entire system.

The traditional energy dispatcher for a PV system is called the  X  X  X V-priority X  X  (called  X  X  X V-priority 1 X  X  in this paper) control scheme (Henze and Dodier, 2003 ) and will first attempt to power all loads using energy from the PV array; if there is not enough energy available from the PV array then energy from the battery is used to make up the shortfall (if available), and if there is more energy available from the PV array then the batteries are charged with the difference (if possible).

In this paper, two additional controllers are developed: the  X  X  X V-priority 2 X  X  and an optimal controller based on a class of adaptive critic designs (ACDs) called action-dependent heuristic dynamic programming, or ADHDP ( Werbos, 1992; Venayaga-moorthy et al., 2002 ; Prokhorov and Wunsch, 1997 ). The  X  X  X V-priority 2 X  X  control scheme is similar to the PV-priority 1 scheme in that its performance is rule based, but is different in that it attempts to always power the critical load first, then charge the battery to 70%, and finally use whatever energy is available from the PV array and anything over 70% state of charge in the battery to power the non-critical load.

The ADHDP-based optimal energy dispatcher on the other hand is not rule based, and develops its control action strategy by adapting its performance in response to a measured metric value.
Adaptive critic designs use a combination of dynamic program-ming and reinforcement learning, and the ADHDP method is the simplest of the ACD family (it uses only 2 neural networks). One of the neural networks (called the  X  X  X ction X  X  network or  X  X  X ctor X  X ) is responsible for providing the control signals while the second (called the  X  X  X ritic X  X  network) critiques these control signals over time. The objectives of this energy dispatcher are the following: i). Completely power the critical load at all times. ii). Maintain the battery state of charge as high as possible so as to be able to meet the critical load during times of reduced (or non-existent) energy from the PV array. iii). Power the non-critical load such that the controller is still able to meet the first two objectives.
 Another advantage of the ADHDP-based controller is that since it is not rule based, its behavior can be modified over time to cope with changing meteorological conditions. One such important condition is the behavior of the Sun and its associated output.
Even though the output in general remains relatively constant, there are small deviations and at least some work has been performed into predicting sunspot activity ( Xie et al., 2006 ; Day and Nandi, 2008 ).
 Nomenclature E ( t ) energy stored in the battery at time t E B,max maximum energy stored in the battery E PV ( t ) energy collected from the PV array at time t E
PV,max maximum energy able to be collected from the PV E
PV,wasted energy collected from the PV array that is not been L ( t ) current critical load at time t L CL,max maximum critical load device. Likewise, new energy storage systems ( Jiang and Dougal, 2006; Lemofouet and Rufer, 2006 ) may eclipse the performance of the standard lead-acid-type battery technology primarily used today. Because the focus of this study is primarily to evaluate the performance of the presented control strategies, the assumption of 100% efficiency of the photovoltaic system is made. If other efficiencies are desired, they can be set by modifying the appropriate values within the models, as depicted in Fig. 1 .
Also, the PV array is simulated to be tilted south at an angle equal to the latitude of each test city and the efficiency of the PV array model is taken as 11% to account for various non-optimal conditions (such as array misalignment, dust on the arrays, etc.). This value is representative of the current commercially available range of efficiencies for PV arrays. Generally, PV panels vary in efficiency from 6% to up to 30%; although the high efficiency panels are generally reserved for spacecraft usage because of their high radiation tolerances and higher power-to-weight ratio. A rough equivalent to the PV arrays being simulated in this paper would be an array of eight Kyocera KC200GT panels. These panels are over 16% efficient and will output 200 W during optimal conditions ( Kyocera, 2007 ). The minimum charge for the battery of 30% is required to supply energy to the loads (this is consistent with standard deep cycle lead-acid batteries).
 This controller works well when there is sufficient PV energy.
However, when there is not sufficient PV energy, then the battery will not be fully recharged and the loads will be dropped. The weather and user loads are stochastic in nature; therefore there is no one definitive model at all times. Thus, it makes sense to look at intelligent model-free learning methods of controlling such a system. 3.2. PV-priority 2 controller
The second PV-priority controller that is used in this paper is the PV-priority 2 controller. As mentioned previously, this controller first meets the critical load then attempts to charge the battery to 70% state of charge. Once these two objectives have been met, the controller then attempts to power the non-critical load using any excess PV energy or energy from the battery the effect of these actions until the end of the sequence), it is possible to design an optimal controller using the traditional supervised learning-based neural network.

The adaptive critic method determines an optimal control for a system by adapting two neural networks: an Action network and a Critic network. The Action network is responsible for driving the system to the desired states, while the Critic network is responsible for providing the Action network with performance feedback with respect to reaching the desired states over time. With this feedback, the Action network is able to adapt its parameters continuously to maximize its objective. The Critic network learns to optimize the Action network by approximating the Hamilton X  X acobi X  X ellman equation associated with optimal control theory.

This Actor X  X ritic adaptation process starts with a non-optimal or suboptimal policy by the action network; the Critic network then guides the Action network toward an optimal solution at each successive adaptation. During the adaptations, neither of the networks needs any  X  X  X nformation X  X  of an optimal trajectory, only the desired cost needs to be known. Furthermore, this method determines optimal control policy for the entire range of initial conditions. Additionally, it needs no external training, unlike other neural-controllers ( Venayagamoorthy et al., 2002 ).
The design ladder of ACDs includes three basic implementa-tions: Heuristic Dynamic Programming (HDP), Dual Heuristic Programming (DHP) and Globalized Dual Heuristic Programming (GDHP), in the order of increasing power and complexity. The interrelationships between members of the ACD family have been generalized and explained in ( Prokhorov and Wunsch, 1997 ). In this paper, an Action-dependent HDP (ADHDP) approach is adopted for the design of an optimal PV controller. Action-dependent adaptive critic designs do not need system models to develop the optimal control policy (action network output).
As mentioned, the objective of the optimal PV control is threefold  X  to maximize or fully dispatch the required energy to the critical loads at all times, dispatch energy to charge the battery whenever necessary so as to dispatch energy to the critical loads in the absence of energy from the collector and the last objective is to dispatch energy to the non-critical loads not comprising on the first two objectives. The optimal controller is not used for instances where there is sufficient solar energy to power all loads as well as completely charge the battery. When this occurs, all loads are satisfied and the battery is completely charged.
 This optimal controller uses two networks (the Action and Critic networks) as previously mentioned. The inputs to the Action network correspond to the states of the system while the outputs correspond to the amount of energy to be dispatched to the critical loads, battery and non-critical loads. The inputs to the Critic consist of the inputs to the Action network at time t , t 1 and t 2, as well as the outputs of the Action network at time t , t 1 and t 2. The Critic then uses the information from the current states and actions in the current time step (as well as from non-critical loads.

U  X  t  X  X  A 1 abs  X  1  X  E CL =  X  L CL  X  M non zero L CL ; max  X  X  X 
In the U ( t ) function given in (2), a higher priority is given to meeting the critical load at all times over the batteries being charged or the non-critical load being supplied by assigning different weightings  X  30/23 to the CL term, 15/23 to the BC term and 13/23 to the NCL term. This U ( t ) meets the threefold objective for the optimal PV controller design.

In the training of the Critic network, the objective is to minimize (3) given below.

E 2  X  t  X  X  3  X  where E  X  t  X  X  U  X  t  X  X  g J  X  t  X  J  X  t 1  X  X  4  X 
The weight change and update equations for the Critic network using the BP algorithm is given by (5) and (6), respectively.
D W  X  t  X  X  Z c E  X  t  X  @ J  X  t  X  @ W
W  X  t  X  1  X  X  W c  X  t  X  X  D W c  X  t  X  X  6  X  where Z c and W c are the learning rate and the weights of the Critic neural network, respectively.

One important note about the Critic network design is that when determining how many previous time steps to use, it is necessary to look at how the Critic network training is proceeding.
If the critic network is not training properly, then it may be necessary to increase the number of time steps used as input to the critic network. The Critic network requires multiple time steps because it is implemented using a feedforward network and using multiple delayed time steps gives the network more information to calculate (1). However, with more time steps, the Critic network will also require more neurons in order to handle the complexity. For this study, using the current and previous two time steps as previously outlined was found to allow the Critic to train properly using the specified number of neurons. The size of the hidden layer in the Critic network is determined by trial and error, while the size of the input and output layers are determined by the number of inputs and outputs, respectively. In the case of the Critic network, the only output is the approximated J ( t ) value so the output size is 1. 4.2. Action neural network
The Action network is a multilayer feedforward network trained using the BP algorithm. The input, hidden and output layers of the Action network consists of five linear neurons, 30 sigmoidal neurons and three linear neurons, respectively, as is Eqs. (8) and (9), respectively.
 D W A  X  t  X  X  Z A E A  X  t  X  @ A  X  t  X  W A  X  t  X  1  X  X  W A  X  t  X  X  D W A  X  t  X  X  9  X  Here Z A and W A are the learning rate and the weights of the Action neural network, respectively.
 As with the Critic network, the size of the hidden layer in the Action network is determined by trial and error, while the size of the input and output layers are determined by the number of inputs and outputs, respectively. 4.3. Actor/Critic training The flowchart in Fig. 7 outlines the pre-training steps for the Action network, while Fig. 8 details the iterative training technique used to develop the optimal controller over time. During the iterative training phase, several metrics can be used to determine if the Actor X  X  performance is increasing. For this study, the simple sum of the utility function for each cycle of training the Action network is used. This means that when the sum of the utility function is decreasing, the performance of the Action network is improving. The simulation is run for a fixed number of controllers) are shown in Tables 2 X 4 . All of the results from these tables are obtained using the normal load values (as shown in Fig. 10 ).
 results of the simulations when the loads are lowered by 10% and Table 6 shows selected results when the loads are increased by 10%. Previous and less extensive studies have been explored by the authors ( Welch and Venayagamoorthy, 2006a, 2006b, 2007 ) but did not extensively examine controller performance across wider insolation changes nor did they examine differences in loading. Also, all of the results tables give values for total PV energy collected and total PV energy wasted (energy which could not be stored because the battery is full and all loads can be met by using only energy from the PV array; usually occurs in the summer). Finally, each table contains a column labeled  X  X  X elative Performance X  X , which shows an insolation-independent weighted sum of the results from each test  X  the higher the score, the better the result. The equation used to calculate this metric is based on demanded load and satisfied load on a daily basis, so the demanded load curve is a constant value since the load profile varies on a 24 h cycle. As can be seen by these figures, the optimal controller is able to completely outperform the PV-priority 1 controller in powering the critical load but does so at the expense of powering the non-critical load.
 In Fig. 13 , the daily energy balance is shown for the same area.
This surplus (or deficit) is calculated by subtracting the demanded energy of the loads from the energy received form the PV arrays.
This is a good measure of absolute possible performance of a controller. If there is always a surplus, then the controller has an easier job of supplying energy to the loads, but if there is always a deficit, then the controller would have a more difficult time supplying energy to the loads without dropping them.
 Interestingly, the optimal PV controller trained using the Caribou,
ME data set seems to always equal or perform better than any other charged it would attempt to power both loads. This is in sharp contrast to the PV-priority controller 1 which always tried to power both loads, leading to a much lower average battery state of charge for the battery (and less met critical load demand). This behavior results from the coefficients used in the utility function (2).

Another interesting result was that the optimal controller trained using the Phoenix, AZ data set performed similarly to the PV-priority 1 controller (and in some cases slightly worse) and never as well as the other optimal controllers. This is most likely because the region received so much more sunlight than the other regions that it had less  X  X pportunity X  to learn a more optimal technique, since there were more periods of excess sunlight where all loads could be satisfied. This most likely led to a different operating characteristic that did not lend itself well to
Furthermore, it can be seen that as the load increases, the margins by which the Caribou-trained optimal controller and PV-priority 2 controller outperforms the others increases. This is because more of the loads are able to be met with a constant value of insolation, resulting in a higher overall relative performance.
Because the relative performances all of the ADHDP optimal controllers except the one trained at Phoenix, AZ are nearly always above the relative performance of the PV-priority 1 controller (especially as the load increases), it is shown that the optimal
ADHDP controllers are superior to the standard PV-priority 1 controller, especially when trained using insolation data from cities that receive very little solar radiation and as the operating conditions become increasingly severe, as in the case of lower available insolation and higher loads. However, the other PV-priority controller developed in this paper, PV-priority 2 controller, is able to outperform all other controllers presented. It is only able time to ensure that the critical load demand is met primarily all the time and then the non-critical load demand. The battery state charge is also maintained as high as possible to ensure energy supply to the critical loads during nights and the winter months. This in turn provides the benefit of extended battery life. Results have been presented for six different cities with different solar radiation profiles using five controllers: 2 PV-priority controllers and 3 optimal ADHDP-based controllers trained at 3 locations. In most cases, the optimal PV controller has exhibited better control than the PV-priority 1 controller but not better than the PV-priority 2 controller. The comparison between the two control schemes shows that for the most part, the optimal PV controller satisfies the critical load and some of the non-critical loads demand better than the PV-priority 1 control scheme, while keeping a higher battery state of charge. The hardware of implementation of such an ACD controller is feasible and cheap compared to savings as a result of proper energy management. This scheme is adaptive and therefore can fine tune to different locations and weather profiles within a short period of time. Thus, it is a promising and a potentially inexpensive technique for practical deployment on growing solar energy system installa-tions.

Future work involves real-time investigations to try to further optimize the energy controller to follow more closely the load profiles and provide even better performance. In addition to improving performance, integration with other energy sources and hybrid forms of storage, such as hydrogen fuel cells with battery can be investigated. Also, the grid-connected case can also be considered in which the PV system will no longer be detached from the main electrical grid; instead, the goal of the controller can be to minimize importation of energy from the grid and to maximize exportation of energy to the grid. This optimization process allows for better overall operation of the grid by reducing some of the uncertainty associated with the output of the PV system.
 Acknowledgement
The financial support from the National Science Foundation under CAREER Grant no. ECCS 0348221 and EFRI Grant no. 0836017 are gratefully acknowledged for this work.
 References
