 Andr  X  as Gy  X  orgy GYORGY @ UALBERTA . CA Csaba Szepesv  X  ari SZEPESVA @ UALBERTA . CA
We consider the problem of online learning in discrete-time finite Markov decision processes (MDPs) with arbi-trarily changing cost functions. It is assumed that a learner moves in a finite state space X . Occupying a state x t at time instant t , the learner takes an action a t  X  A ( x t ) , where A ( x t ) denotes the finite set of actions available at state x . Then the agent moves to some new random state x t +1 , where the distribution of x t +1 , given x t and a t is deter-mined by a Markov transition kernel P (  X | x t ,a t ) . Simulta-neously, the agent receives some immediate cost t ( x t ,a where the cost function t : U  X  R is assumed to be bounded (in fact, we will assume that the costs lie in [0 , 1] and U = { ( x, a ): x  X  X ,a  X  A ( x ) } . The goal of the learner, who is assumed to know P before the interaction starts but not { t } t , is to minimize its total cost. We as-sume here that the cost function t can change in an arbi-trary manner between time instants. The performance of the learner is measured against the best stationary policy in hindsight, giving rise to the expected regret: Here, for a given stationary policy  X  ( i.e. ,  X  is such that  X  ( x,  X  ) is a probability distribution over A ( x ) for any X ), ( x  X  would visit in time step t if this policy was used from t =1 (we may assume that x  X  regret-growth, R T = o ( T ) ( T  X  X  X  ) means that the av-erage cost collected by the learning agent approaches that of the best (stationary) policy in hindsight. Naturally, a smaller growth-rate is more desirable.

Motivated by the desire to design robust learning algo-rithms, this problem has been studied under various condi-tions by numerous authors (see, e.g., Even-Dar et al. , 2009 ; can consult these papers for examples and extra motivation.
We consider two variants of the above model with re-spect to what observations are available to the learner. In both models the learner can observe its actual state, x t the full information feedback model, the learner can ob-serve the full cost function t at the end of time instant while in the bandit feedback model the learner only ob-serves the cost t ( x t ,a t ) it receives.

Treating the online MDP problem as a huge but standard online learning problem, it is not hard to obtain algorithms that enjoy good regret bounds but whose computational complexity is huge. Therefore, earlier work in the litera-ture concentrated on obtaining computationally efficient al-gorithms that also achieve near-optimal regret rates. These results either concern the (stochastic) shortest path prob-lem (SSP, an episodic MDP), or uniformly ergodic MDPs. Several methods achieve near-optimal regret rates by run-ning an independent expert algorithm for each x  X  X , see Even-Dar et al. ( 2005 ; 2009 ); Neu et al. ( 2013 ) for the full information case and Neu et al. ( 2010 ; 2011 ; 2013 ) for the bandit case. Yu et al. ( 2009 ) gave other low-complexity methods with inferior performance guarantees.

The disadvantage of these methods is that, although they achieve optimal O ( horizon T , they often scale suboptimally in other problem parameters, such as the mixing time in the uniformly er-godic case or the length of the paths in the SSP case. Fur-thermore, the optimal-order bounds in the literature for the bandit setting require all states in X to be visited with pos-itive probability under any deterministic policy, and the inverse of this, potentially very small probability appears in the regret bounds. In this paper we alleviate this prob-lem and obtain optimal-order bounds that do not deteriorate with the minimum visitation probability.

To achieve this, we treat the MDP problem as an on-line linear optimization problem and show that the result-ing methods can be implemented efficiently (we note that the same idea was applied successfully to the determinis-tic shortest path problem ( Gy  X  orgy et al. , 2007 ), where the minimum visitation probability can also be zero). We rig-orously analyze the regret and the computational complex-ity of our online linear optimization methods, which are approximate versions of the mirror descent and the con-tinuous exponential weights algorithms; we believe that these results are also of independent interest. The mirror descent algorithm (see, e.g., Beck &amp; Teboulle 2003 ) has a usually computationally expensive projection step which we perform approximately using another mirror descent al-gorithm, while our results for the continuous exponential weights algorithm are based on the Dikin-walk approxima-tion of Narayanan &amp; Rakhlin ( 2011 ).

Recently, Zimin &amp; Neu ( 2013 ) have independently ob-tained similar reductions for the SSP case, using the mirror descent algorithm, achieving essentially the same bounds. However, they do not consider the implementation issues of the projection step of the mirror descent algorithm (i.e., the computational complexity of obtaining sufficiently good approximate projections and the effect of this approxima-tion in their final bound).

The rest of the paper is organized as follows. Section 2 introduces the classes of OMDPs we study and reduces them to online linear optimization problems. Section 3 pro-vides the analysis of the impact of approximation errors on the mirror descent and the continuous exponential weights algorithms. In Section 4 we obtain our algorithms for the OMDP problems by applying the methods of Section 3 to the online linear optimization problems from Section 2 . In this section we give a formal description of online Markov decision processes (OMDPs) and show that two classes of OMDPs can be reduced to online linear op-timization. The idea behind the reduction, which goes back to Manne ( 1960 ) (for a modern account, see Borkar ( 2002 )), is to represent policies by their stationary (occu-pation) measures over the set of state-action pairs. Under this representation, the map from policies to their expected cost turns out to be (approximately) linear. Further, when the transition model of the environment is known, it is easy to convert between policies and their stationary occupation measures.

First, let us introduce some notation. Let  X  S denote the set of probability measures over S . 1 Note that for S finite, of Euclidean spaces will be denoted by  X  ,  X  . For p  X  1 , the p -norm of vector v is denoted by v
The structure of an online MDP is given by a finite state space X , finite action spaces A ( x ) ,x  X  X , with U = { ( x, a ): x  X  X ,a  X  A ( x ) } , and probability transition ker-nel P : U  X  X  X  [0 , 1] satisfying state, x 1 , is distributed according to some distribution over X . At each time instant t =1 , 2 ,... , based on its pre-vious observation, state, and action sequences, the learner chooses an action a t  X  A ( x t ) , possibly in a random man-ner. Extending this notion, we can think of a learning agent as if it chose a (randomized) Markov policy  X  t : U  X  [0 , 1] distribution  X  t ( x t ,  X  ) . If  X  t =  X  independently of that the agent X  X  strategy is stationary and we identify such a control strategy with  X  . The set of such stationary Markov policies will be denoted by  X  . 2.1. Online Linear Optimization
In the following subsections we reduce two special classes of OMDPs to online linear optimization, which we briefly review now. Let K be a convex and compact sub-set of a Hilbert space V . In most cases, we take V = R d equipped with the standard inner product. In online linear optimization, an adversary selects a sequence of loss vec-tors 1 ,..., T  X  F  X  V and the learner X  X  goal is to choose a sequence of vectors w t  X  K so as to keep the regret small. Naturally, the choice of w t should only depend on the history of earlier choices and losses. As with OMDPs, we say that there is full-information feedback if the learner observes the entire vector t and bandit-information feed-back if only the actual loss t ,w t is observed. In the semi-bandit case, the situation most related to our OMDP problems, V = R d and only those components of t are ob-served for which the corresponding components of w t are non-zero. 2.2. Loop-Free Stochastic Shortest Path (LF-SSP) Here we assume that X has a layered structure, that is, X can be partitioned into disjunct sets X 1 ,..., X L ( L  X  1 such that if P ( x | x, a ) &gt; 0 then x  X  X l and x  X  X for some l =1 ,...,L  X  1 , or x  X  X L , x  X  X 1 , and P ( x | x, a )=  X  0 ( x ) for any a  X  A ( x ) . This assumption means that starting in X 1 (  X  0 is concentrated on X 1 ), the learner moves through X 2 , X 3 ,... to reach X L , after which the whole process returns to X 1 and is restarted (we assume without loss of generality that each x  X  X is achievable by following a suitable policy). The sequence of transi-tions from a state in X 1 back to some other state in X 1 an episode of the MDP, and in this case t will index the episodes in the process. Since each episode starts from the same distribution, the episodes are memoryless, and any policy  X  introduces an  X  X ccupation measure X   X   X  over U , such that for any stage index l , where U l = { ( x, a ): x  X  X l ,a  X  A ( x ) } . Further-more, for any x  X  X 1 , this we can view K = {  X   X  :  X   X   X  } as a subset of is a convex polytope in R d , since it can be described by a set of linear constraints:
K =  X   X  [0 , 1] U : These constraints guarantee that the probability  X  X lowing into X  each state is equal to the probability  X  X lowing out X  of it. It is unnecessary to explicitly require the probability assigned to each layer to sum to one, thanks to the assump-tion that the transition probability kernel P agrees with on the first layer.

Furthermore, with an immediate cost function : U  X  [0 , 1] , the expected total cost of policy  X  in an episode lem of finding the stationary policy with the smallest per episode expected cost can be written as the linear opti-mization problem of arg min  X   X  K ,  X  : Once the solu-tion of this problem is found, a Markov policy  X   X  is ex-tracted from the optimizing measure  X  by  X   X  ( x, a )=  X  ( x, a ) / a  X  A ( x )  X  ( x, a ) . Then, by construction,  X  .

The above description implies that all paths from the starting layer X 1 back to itself are of the same length. This assumption is not restrictive, though, as any layered MDP can be modified without loss of generality to satisfy this assumption at the price of moderately increasing the state space (see Gy  X  orgy et al. , 2007 ). For convenience, for on-line learning with changing costs in LF-SSPs we redefine the regret to be the regret of the first T episodes and use to be the cost function effective in episode t . With this, where  X  t  X   X  is the Markov policy used in the t th episode . The problem of keeping the regret low is thus viewed as an instance of online linear optimization over the convex set K . Note that when  X  t is a deterministic function of the past then the expectation can be removed. 2.3. Uniformly Ergodic MDPs
Without loss of generality, we assume that X = { 1 ,..., |X|} . Here, following previous works, we as-sume the so-called uniform mixing condition: There ex-ists a number  X   X  0 such that under any policy  X  and any pair of distributions  X  and  X  over X , (  X   X   X  ) P  X  e ing distributions over X as row vectors of R |X| and P  X   X   X  et al. ( 2009 ), we call the smallest  X  satisfying this assump-tion the mixing time of the transition probability kernel and call a resulting MDP problem uniformly ergodic. This assumption is not unrestrictive, but relaxing it would fur-ther complicate the paper and hence we leave this for fu-ture work. As for LF-SSPs, for a Markov policy  X  , let  X   X  be its stationary distribution over U . Under the assump-K = {  X   X  :  X   X   X  }  X   X  U . Again, K is a convex poly-tope in R |U| , since it can be described by a set of linear constraints:
K =  X   X  [0 , 1] U : Again, we will take d = |U| as the  X  X imension X  of the problem. In this case, we are concerned with finding a se-quence of policies whose expected total cost up to time T is not much larger than that of the best policy in hindsight. Similarly to Neu et al. ( 2011 ; 2013 ), we can bound this Lemma 1. Consider a uniformly ergodic OMDP with mix-ing time  X  &lt;  X  and losses t  X  [0 , 1] d . Then the regret of an agent following policies  X  1 ,...,  X  T through the tra-jectory ( x t ,a t ) t relative to a fixed policy  X  can be bounded as E for any k  X  E [  X   X  t  X   X   X  t  X  1 ] , t =2 ,...,T .
Since we can recover a policy from a stationary distribu-tion (as in the LF-SSP case), it is enough to find a slowly changing sequence  X  1 ,..., X  T  X  K such that the first term of the bound is small. This is again an online linear opti-mization problem.

We have now mapped online learning in MDPs, under both sets of assumptions, to online linear optimization, which is a well-studied problem in online learning ( Cesa-Bianchi &amp; Lugosi , 2006 ; Shalev-Shwartz , 2012 ). In the next section, we discuss two general algorithms designed to attack this problem and how they apply to our case.
In this section we consider the challenges of implement-ing two standard algorithms for online linear optimiza-tion: mirror descent (MD) and the continuous exponen-tial weights algorithm (CEWA). When the feasible set K is complicated, some operations from both algorithms have no closed form and need to be approximated iteratively. We analyze the impact of the approximation errors on the re-gret analysis for both methods. With an understanding of how the regret scales with the approximation errors, we are able to determine the necessary precision, which will lead to bounds on the computational complexity of the approxi-mate versions of these methods.

Recall that the goal of online linear optimization is to choose a sequence of vectors w t  X  K  X  V in order to keep the regret R T = T small, no matter how the sequence of loss vectors t  X  F is chosen. 3.1. Mirror Descent with Approximate Projections
Mirror descent is a well-known strategy for achieving low regret in online linear optimization problems. It has two parameters: a step size  X  &gt; 0 and a Legendre function 3 R : A  X  R , called a regularizer. We assume that A is a superset of K . Starting from w 1  X  K , MD makes a sequence of predictions w t defined by where D R ( u, v )= R ( u )  X  R ( v )  X  X  X  R ( v ) ,u  X  v denotes the Bregman divergence induced by R . As is well known, w t +1 can be obtained in the following two-step process: where  X  R (  X  w ) = argmin notes the Bregman projection associated with R . Often  X  w t +1 can be expressed in closed form and computed in constant time; then the main challenge of applying MD is in computing the Bregman projection.

Unless the set K is very simple, there is no closed form for the Bregman projection and we must use inexact iter-ative techniques. Hence, the next iterate w t +1 will be dif-ferent from  X  R (  X  w t +1 ) . The following theorem analyzes the regret of MD with c -approximate projections when Theorem 2. Let R be convex and K be a convex set such that  X  R is  X  -Lipschitz on K with respect to (wrt)  X  . Let D =sup u,v  X  K u  X  v  X  be the diameter of K wrt the dual norm of  X  . Then the regret of MD, with c -approximate projections, step size  X  , and regularizer R satisfies for any w  X  K and losses { t } T even when  X  and/or D are unbounded, in which case we interpret c  X  D =0 .

When the regularizer R is  X  -strongly convex wrt the norm  X  , i.e., if R ( w )  X  R ( w )+  X  R ( w ) ,w  X  w + 2 w  X  w lemma to bound the sum rem 2 .
 Lemma 3. Let R : A  X  R be a  X  -strongly convex Leg-endre function wrt the norm  X  ,  X  &gt; 0 , w  X  A ,  X  R d , and define  X  w  X  A to be the unconstrained MD update:  X  w = argmin u  X  A  X  , u + D R ( u, w ) . Then , w  X   X  w  X  In this section we present a particular implementation of MD with approximate projections that is of interest for the optimization problems presented in Section 2 . When the constraint set K is a subset of the unit cube [0 , 1] d  X  R d , to obtain a regret bound that scales with the logarithm of the dimension d , we use MD with the unnormalized negen-tropy regularizer R ( w )= tunately,  X  R ( w )=(ln( w 1 ) ,..., ln( w d )) is unbounded when any component of w approaches zero. Thus R vi-olates the condition of Theorem 2 that requires  X  R to be Lipschitz continuous and makes it challenging to design ef-ficient methods for computing c -approximate projections. Therefore, for the rest of this section we assume that the el-ements of K have components that are uniformly bounded away from zero by  X  &gt; 0 . In other words, we assume that K  X  [  X  , 1] d  X  R d .

In order to apply MD, we need to provide a method for computing c -approximate projections onto the set K .We propose to use a second instance of MD with the squared 2-norm regularizer R ( w )= 1 this choice of regularizer is that the induced Bregman di-vergence equal to the Euclidean distance and the associ-ated Bregman projection is the Euclidean projection. Then the inner instance of MD will also use approximate pro-jections, but they can be calculated as the solutions to quadratic programming problems, which can be solved ef-ficiently by interior point methods. We call this algorithm Corollary 4. Let  X  &gt; 0 and K  X  [  X  , 1] d . Let w ,...,w T  X  K be the sequence of predictions made by MD 2 on K with losses 1 ,..., T  X  [0 ,  X  ) d , c -approximate projections where accuracy is measured by  X  1 , step size  X  &gt; 0 , and the unnormalized negentropy regularizer R . Then, for any w  X  K , we have where  X  w t +1 is defined component-wise by  X  w t +1 ,u w where W 2 =sup projection step used in the inner MD instance when com-puted with an accuracy of c = 1 rant described by m  X  d linear equality constraints, interior point methods can be used to achieve H = of den Hertog 1994 , or Section 4.3.2 of Nesterov 2004 ). Finally, we note in passing that instead of using mirror-descent to implement the approximate projection, one could also use an interior point algorithm for this purpose. Building on the results of Potra &amp; Ye ( 1993 ), it appears possible to compute an -approximate projection to K as above in time O ( d 3 . 5 (1 + ln(1 /  X  )+lnln( d/ ))) , result-ing in a modest improvement of the total complexity. Fang et al. ( 1997 ) discuss more methods, but with no complexity analysis. 3.2. Continuous Exponential Weights Algorithm with
Consider an online linear optimization problem over a convex closed set K  X  R d . Let the sequence of loss vec-tors be 1 ,..., T and let p 1 be some positive density over K ( i.e. , p 1  X  0 and K p 1 ( x ) dx =1 ). Then, the con-tinuous exponential weights algorithm (CEWA) (see, e.g. , Narayanan &amp; Rakhlin , 2011 ) at time t +1 predicts X t +1 where X t +1  X  p t +1 (  X  ) and p t +1 is a density over K portional to p 1 ( x )exp(  X   X  t f ( x ) g ( x ) dx . Here  X  &gt; 0 is the learning rate of the al-gorithm.

When p 1  X  L 2 ( K ) ( i.e. , p 2 continuous exponential weights algorithm can be inter-preted as an instance of mirror descent with the unnor-malized negentropy regularizer R ( p )= p ( x )ln( p ( x ))  X  p ( x ) dx . Indeed, it is easy to see that in this case p D ( K ) is the set of densities over K ( D ( K )= { p : K  X  [0 ,  X  ) | K p ( x ) dx =1 } ), t ( u )= t ,u for any u  X  K , the inner product over D ( K ) is defined as where ,p =
K p ( x )ln( p ( x ) /p ( x )) dx gence between p, p  X  D ( K ) , which is also the Bregman divergence induced by R . As such, with a straightfor-ward generalization of Theorem 2 with c =0 , we get that the expected regret R T = E T any random variable U with density p U supported on K is bounded by
The advantage of CEWA is that it avoids the usual pro-jection step associated with MD (or similar algorithms, like  X  X ollow the Regularized Leader X  ). The complexity is pushed to sampling from p t +1 , which, as we will see, leads to a different tradeoff. The question of how to efficiently sample from p t +1 , or a distribution sufficiently close to p t +1 , was addressed by Narayanan &amp; Rakhlin ( 2011 ). They proposed a Markov Chain Monte-Carlo (MCMC) method to implement sampling. The stationary distribution of the Markov chain they design at time step t +1 is p t +1 . How-ever, since the chain is only run for finitely many steps, the distribution that X t +1 follows may differ from p t +1 . In fact, in their paper they proposed to make only one step with the Markov chain, that is, to use X t +1 = P t +1 (  X | X where P t +1 (  X | x ) is the Markov kernel underlying the chain they designed. They argue that this is sufficient, since p close to p t +1 . Indeed, they prove a regret bound that shows the usual step with the Markov chain is that the regret blows up by a factor that is proportional to d 5 . Since we wish to avoid this increase of the regret, we propose to run the chain for more time steps. By following the analysis of ( Narayanan &amp; Rakhlin , 2011 ), we get the following result: Proposition 5. Assume that for any t  X  1 , x  X  K , the losses satisfy t ,x  X  [0 ,B ] . Let P t +1 (  X | x ) be the Markov kernel underlying the  X  X ikin walk X  of Narayanan &amp; Rakhlin ( 2011 ) at time step t +1 . Assume that  X  B  X  1 and fix an arbitrary parameter 0 &lt;r  X  1 . If X 1  X  p 1 for t =1 , 2 ,... in time step t +1 , X t +1 = Z ( k ) Z if k  X  C  X  2 d 3 ln((1 +  X  ( e  X  1) B ) 2 +2  X  ( e  X  1) B/r ) p p stant and  X  = O ( d ) is a parameter that depends on the shape of K .

The main work in making a move with the Markov chain is to sample from a Gaussian random variable. Note that the covariance of this distribution depends on the current state. Thus, the cost of one step is dominated by the O ( d cost of computing the Cholesky factorization of this covari-ance matrix once the matrix is computed. Hence, the total cost of sampling at one time step is O ( d 8 ln(1 +  X  B/r ) where we assumed that computing the covariance matrix can also be done in O ( d 3 ) step.

Finally note that if p t then the additional regret due to the  X  X mprecise X  imple-mentation up to time T is bounded by rT B . Consider now the case when B is constant ( i.e. , independent of T ). Then, setting r =1 / bounded by B one should use  X  = O (1 / orem 2 ). Hence, in this case the cost of sampling per time step can be kept constant independently of the time horizon with essentially no increase of the regret. 3.3. Bandit Information
The purpose of this section is to briefly consider bandit online linear optimization. The difference between bandit online linear optimization and the setting considered above is that at the end of time step t the only information received is the scalar loss of the vector chosen in that time step, that is t ,  X  w t , while t is not revealed to the learner. For a re-cent survey of the literature see the review paper by Bubeck &amp; Cesa-Bianchi ( 2012 ). The approach followed by existing algorithms is to construct an estimate t (usually unbiased) of t and use this in place of t in a  X  X ull-information algo-rithm X , like MD of the previous section. The question then is how to construct t and how to control the regret. Our main tool in this latter respect is going to be Theorem 2 . Indeed, if MD is run with t  X  1 in place of t  X  1 , then from Theorem 2 we see that, as far as the expected regret is con-cerned, it suffices to control E t  X  1 ,  X  w t  X   X  w t  X  1 this, we have the following result extracted from Abernethy et al. ( 2008 ): 6 Lemma 6. Let w,  X  V = R d , and define  X  w u = w u e  X   X  for all u =1 ,...,d . Then , w  X   X  w  X   X  Note that the lemma continues to be true even if V = L ( K ) , is the space of square-integrable functions over K in which case the sum should be replaced by an integral over K wrt the Lebesgue measure.
In this section, we consider online learning in MDPs in the so-called full-information setting. In the case of LF-SSPs this means that t is observed at the end of episode t , while in the case of ergodic MDPs t is observed at the end of each time step. We only consider full-information provided for the bandit case (to save space).
 we need the components of the (occupation) measures to be bounded away from zero. This will not be the case gener-ally, since policies may choose actions with arbitrarily low probabilities. Without loss of generality we can assume that there exists a  X  &gt; 0 and a policy  X  exp such that the corresponding (occupation) measure  X  exp =  X   X  exp satisfies  X  exp ( x, a )  X   X  for all ( x, a )  X  U . By the convexity of K ,  X   X  . =(1  X   X  )  X  +  X   X  exp  X  K for any 0 &lt;  X  &lt; 1  X   X  K ( i.e. , there exists a policy inducing  X   X  ), and for any loss function we have instead of K , since  X   X   X  K  X  X  .

First we consider the simple case of the LF-SSP prob-lem. By ( 4 ) and since t ( u )  X  [0 , 1] for all u  X  U , (Recall that L is the number of layers in the state space.) Now let us run MD 2 on K  X  X  with the unnormalized neg-ative entropy regularizer R (  X  )= L  X  1  X  =(  X  0 ,..., X  L  X  1 ) ,  X  l  X  [0 ,  X  ) U l and R l is the unnor-malized negative entropy regularizer over [0 ,  X  ) U l . Since it follows from Pinsker X  X  inequality that R is 1 /L strongly convex wrt the  X  1 -norm (see also Example 2.5 of Shalev-Shwartz 2012 ), combining Corollary 4 , with Lemma 3 and ( 5 ), we obtain the following result: Theorem 7 (MD 2 on LF-SSP, full information) . Let  X  be any policy,  X  1  X  K ,  X   X  (0 , 1] and D max  X  tions 1 ,..., T . Let  X  t be the output of MD 2 on round t and define  X  t =  X   X  abilities). Then the regret of the agent that follows policy  X  at time t relative to policy  X  can be bounded as and the per-time-step computational cost is bounded by
O
The proof follows from the arguments preceding the the-orem combined with Corollary 4 and the remark after it. Also, the next theorem has an almost identical proof, hence we decided to omit this proof.

Note that D max =  X  L ln 1  X  exp ( x, a )  X   X  ). If, for example,  X  exp ( x,  X  ) is selected to be the uniform distribution over A ( x ) , then  X  &gt; 0 and  X  0 =1 / max x |A ( x ) | , making the regret scale with O ( L T ln(max x |A ( x ) | )) when  X  =1 / makes the computational cost  X  O ( d 3 . 5 T 1 / 4 /  X  O (  X  ) hides log-factors. Neu et al. ( 2010 ) gave an algorithm that achieves O ( L 2 T ln(max x |A ( x ) | )) regret with computational complexity per time-step. Thus, our regret bound scales better in the problem parameters than that of Neu et al. ( 2010 ), at the price of increasing the computa-tional complexity. It is an interesting (and probably chal-lenging) problem to achieve the best of the two results.
Consider now the case of uniformly ergodic MDPs. In online linear optimization on the corresponding set K and show that the sequence of policies does not change too quickly. By ( 4 ) and because t  X  [0 , 1] d , we have Therefore, running MD 2 on K  X  X  with the negentropy reg-ularizer R (  X  )= R d (  X  ) gives the following result: Theorem 8 (MD 2 on Ergodic MDPs, full information) . Let  X  be any policy,  X  1  X  K ,  X   X  (0 , 1] and D max  X  tions 1 ,..., T . Let  X  t be the output of MD 2 on round t, and define  X  t =  X   X  lows policy  X  t at time t relative to policy  X  can be bounded as and the per-time-step computational cost is bounded by O
As far as the dependence on  X  is concerned, by choos-ing  X  =1 / state-of-the-art bound ( Neu et al. , 2013 ) that scales as O (  X  3 / 2 T ln |A| ) to O (  X  T ln |A| ) . The update cost of the algorithm of Neu et al. ( 2013 ) is O ( |X| 3 + |X| 2 |A|
The purpose of this section is to consider online learn-ing in the LF-SSP problem under bandit feedback, that is, when at time t , the only information received is t ( x t the cost of the current transition. Based on the previous sections, we see that to control the regret, an MDP learning algorithm has to control the regret in an online linear bandit problem with decision set K .

According to Bubeck et al. ( 2012 ), for online bandit linear optimization over a compact action set K  X  R d , it is possible to obtain a regret of order O ( d regardless the shape of the decision set K , which, in our case would translate into a regret bound of order O ( |U| paper can be implemented efficiently depends, however, on the particular properties of K : Designing the exploration distribution needed by this algorithm requires the compu-tation of the minimum volume ellipsoid containing K and this problem is in general NP-hard even when considering a constant factor approximation ( Nemirovski , 2007 ).
In this section, focussing on LF-SSPs, we design compu-tationally efficient bandit algorithms based on MD and the continuous exponential weights algorithm. In both cases, the immediate costs will be estimated in the same manner: Note that in each stage l , t ( x, a ) is nonzero only for the state-action pair visited in U l ; hence, t is available to the learner. It is easy to see that as long as (B)  X   X  t ( x, a ) bounded away from zero for each state-action pair ( x, a ) the above estimate is unbiased. In particular, denoting by F t the  X  -algebra generated by the history up to the begin-ning of episode t , E t ( x, a ) |F t = t ( x, a ) holds for all ( x, a )  X  U .
 normalized negentropy regularizer on K  X  X  to this problem. Note that the restriction to K  X  X  is now used to ensure both that the projection step can be implemented efficiently and that estimates in ( 7 ) are well-defined. In particular, this im-plies that (B) will be satisfied. Using Lemma 6 then gives the following result: Theorem 9 (MD 2 on Bandit LF-SSP) . Let  X  be any policy,  X   X  K ,  X   X  (0 , 1] and D max  X   X   X  K with the sequence of estimated loss functions 1 ,..., T , defined in ( 7 ) . Let  X  t be the output of MD 2 on round define  X  t =  X   X  policy  X  t at time t relative to policy  X  can be bounded as and the computational cost is bounded as in Theorem 7 .
Selecting  X  exp ( x,  X  ) to be the uniform distribution,  X  &gt; 0 and D max  X  L ln(max x |A ( x ) | ) , results in a O ( dLT ln(max x |A ( x ) | )) bound on the regret for  X  = 1 /  X   X  ( 2010 ) considered the same problem under the assumption that any policy  X  visits any state with probability at least for some  X  &gt; 0 , that is, inf  X  They provided an algorithm with O ( d ) per round complex-ity whose regret is O ( L 2 T max x ( A ( x )ln( A ( x ))) /  X  ) Compared to their result, we managed to lift the assumption  X  &gt; 0 , and also improved the dependence on the size of the MDP, while paying a price in terms of increased computa-tional complexity.

Let us now consider applying CEWA with the Dikin-run the algorithm on K  X  X  with  X  &gt; 0 . As in the full information case, we let (  X  )= t , X  , where t is obtained using ( 7 ). Let  X  p (  X  )= p 1 (  X  )exp(  X   X  t  X  1 s =1 s (  X  )) , while p t = X  p Z t = K  X  X   X  p t (  X  ) d X  . Let  X  t (= X t )  X  K  X  X  be the output of the Dikin-walk at time step t when the walk is run for steps and let  X  t =  X   X  each coordinate of  X  t  X  K  X  X  is bounded away from zero, is well-defined and E t |F t = t and E Combining ( 3 ) with Proposition 5 and Lemma 6 , we get the following result: Theorem 10 (CEWA on Bandit LF-SSP) . Let  X  = the algorithm of Section 3.2 on K  X  X  with the estimated losses { t } , started from the uniform distribution, and with parameters r =2  X  ,  X  =  X  X  /L and k  X  Cd 5 ln(  X  T /d ) for some universal constant C&gt; 0 . Then, for any T&gt; 2 d/  X  the regret against any  X   X  K is bounded by while the per-step computational complexity is bounded by O ( d 3 k ) .

Notice that the regret bound is O ( L dT ln( T ) /  X  ) , while the per-step computational complexity (choosing the smallest k ) is O ( d 8 ln T ) . Thus, this algorithm does not gret bound with 1 / hand, the computational cost of the algorithm is better in be improved if in ( 7 ) we divided by  X  ( x, a ) p t (  X  ) d X  stead of  X  t ), the probability of visiting the state-action pair ( x, a ) . However, we cannot compute this probability in a closed form, and its estimation would require additional sampling, further increasing the computational cost (see Neu &amp; Bart  X  ok 2013 for a similar approach). We finally note that based on the proof it is obvious that for the full-information setting, CEWA would achieve a regret compet-higher in d .
In this paper, viewing online learning in MDPs as online linear optimization, we have proposed novel algorithms based on variants of mirror-descent. We proposed efficient solutions, based on approximate projections and MCMC sampling, to overcome the computational difficulty of the projection step in MD arising from the OMDP structure. We rigorously analyzed the complexity of these algorithms. Our results improve upon the state-of-the-art by improving the regret bounds and lifting some restrictive assumptions that were used earlier in the literature. The price we pay is a somewhat increased computational complexity, though our algorithms still enjoy polynomial computational complex-ity in the problem parameters. It is an interesting (and prob-ably challenging) problem to find out whether the trade-off exposed is  X  X eal X . Extending our results to the bandit-information feedback case for uniformly ergodic OMDPs is also an important problem. One promising approach in this direction is to combine our methods with the reward-estimation technique of Neu et al. ( 2013 ).

This work was supported by the Alberta Innovates Tech-nology Futures and NSERC.

