 We consider the problem of learning incoherent sparse and low-rank patterns from multiple tasks. Our approach is based on a linear multi-task learning formulation, in which the sparse and low-rank patterns are induced by a cardinality regularization term and a low-rank constraint, respectively. This formulation is non-convex; we convert it into its convex surrogate, which can be routinely solved via semidefinite programming for small-size problems. We pro-pose to employ the general projected gradient scheme to efficiently solve such a convex surrogate; however, in the optimization formu-lation, the objective function is non-differentiable and the feasible domain is non-trivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. The computation of projected gradi-ent involves a constrained optimization problem; we show that the optimal solution to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projec-tion subproblem. In addition, we present two projected gradient algorithms and discuss their rates of convergence. Experimental results on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Algorithms Multi-task learning, sparse and low-rank patterns, trace norm
In the past decade there has been a growing interest in the prob-lem of multi-task learning (MTL) [13]. It has been applied suc-cessfully in many areas of data mining and machine learning [2, 3, 9, 10, 31, 38]. MTL aims to enhance the overall generalization performance of the resulting classifiers by learning multiple tasks simultaneously in contrast to single-task learning (STL) setting. A common assumption in MTL is that all tasks are intrinsically re-lated to each other. Under such an assumption, the informative domain knowledge is allowed to be shared across the tasks, imply-ing what is learned from one task is beneficial to another. This is particularly desirable when there are a number of related tasks but only a limited amount of training data is available for learning each task.

MTL has been investigated by many researchers from different perspectives. Hidden units of neural networks are shared among similar tasks [6, 13]; task relatedness are modeled using the com-mon prior distribution in hierarchical Bayesian models [5, 29, 40, 41]; the parameters of Gaussian Process covariance are learned from multiple tasks [21]; kernel methods and regularization net-works are extended to multi-task learning setting [16]; a convex formulation is developed for learning clustered tasks [19]; a shared low-rank structure is learned from multiple tasks [3, 15]. Recently, trace norm regularization has been introduced into the multi-task learning domain [1, 4, 20, 27, 28] to capture the task relationship via a shared low-rank structure of the model parameters, resulting in a tractable convex optimization problem [22].

In many real-world applications, the underlying predictive clas-sifiers may lie in a hypothesis space of some low-rank structure [3], in which the multiple learning tasks can be coupled using a set of shared factors, i.e., the basis of a low-rank subspace [30]. For ex-ample, in natural scene categorization problems, images of differ-ent labels may share similar background of a low-rank structure; in collaborative filtering or recommender system, only a few factors learning tasks may have sufficient differences and meanwhile the discriminative features for each task can be sparse. Thus learning an independent predictive classifier for each task and identifying the task-relevant discriminative features simultaneously may lead to improved performance and easily interpretable models.
In this paper, we consider the problem of learning incoherent sparse and low-rank patterns from multiple related tasks. We pro-pose a linear multi-task learning formulation, in which the model parameter can be decomposed as a sparse component and a low-rank one. Specifically, we employ a cardinality regularization term to enforce the sparsity in the model parameter, identifying the es-sential discriminative feature for effective classification; meanwhile, we use a rank constraint to encourage the low-rank structure, cap-turing the underlying relationship among the tasks for improved generalization performance. The proposed multi-task learning for-mulation is non-convex and lead to an NP-hard optimization prob-lem. We convert this formulation into its tightest convex surrogate, which can be routinely solved via semi-definite programming. It is, however, not scalable to large scale data sets in practice. We propose to employ the general projected gradient scheme to solve the convex surrogate; however, in the optimization formulation, the objective function is non-differentiable and the feasible domain is non-trivial. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gra-dient scheme. The computation of projected gradient involves a constrained optimization problem; we show that the optimal solu-tion to such a problem can be obtained via solving an unconstrained optimization subproblem and an Euclidean projection subproblem separately. In addition, we present two detailed algorithms based on the projected gradient scheme and discuss their rates of conver-gence. We conduct extensive experiments on real-world data sets. Our results demonstrate the effectiveness of the proposed multi-task learning formulation and also demonstrate the efficiency of the projected gradient algorithms.

The remainder of this paper is organized as follows: in Sec-tion 2 we propose the linear multi-task learning formulation; in Section 3 we present the general projected gradient scheme for solving the proposed multi-task learning formulation; in Section 4 we present efficient computational algorithms for solving the op-timization problems involved in the iterative procedure of the pro-jected gradient scheme; in Section 5 we present two detailed al-gorithms based on the projected gradient scheme and discuss their rates of convergence; we report the experimental results in Sec-tion 6 and the paper concludes in Section 7.
 Notations For any matrix A  X  R m  X  n , let a ij be the entry in the i -th row and j -th column of A ; denote by A 0 the number of nonzero entries; let A 1 = m be the set of singular values in non-increasing order, where r = rank ( A ) ; denote by A 2 =  X  1 ( A ) and A  X  = r the operator norm and trace norm of A , respectively; let A
Assume that we are given m supervised (binary) learning tasks, where each of the learning tasks is associated with a predictor f and a set of training data as { ( x i ,y i ) } n 1 ,  X  X  X  ,m ) . We focus on linear predictors as f ( x )= z T x , where z  X  R d is the weight vector for the th learning task.

We assume that the m tasks are related using an incoherent rank-sparsity structure, that is, the transformation matrix can be decom-posed as a sparse component and a low-rank component. Denote the transformation matrix by Z =[ z 1 ,  X  X  X  ,z m ]  X  R d  X  m ; summation of a sparse matrix P =[ p 1 ,  X  X  X  ,p m ]  X  R d  X  m and a low-rank matrix Q =[ q 1 ,  X  X  X  ,q m ]  X  R d  X  m given by as illustrated in Figure 1. The 0 -norm (cardinality) [11], i.e., the number of non-zero entries, is commonly used to control the spar-sity structure in the matrix; similarity, matrix rank [18] is used to encourage the low-rank structure. We propose a multi-task learning formulation with a cardinality regularization and a rank constraint given by where L (  X  ) denotes a smooth convex loss function,  X  provides a trade-off between the sparse regularization term and the general Figure 1: Illustration of the transformation matrix Z in Eq. (1), where P denotes the sparse component with the zero-value en-tries represented by white blocks, and Q denotes the low-rank component. loss component, and  X  explicitly specifies the upper bound of the matrix rank. Both  X  and  X  are non-negative and determined via cross-validation in our empirical studies.

The optimization problem in Eq. (2) is non-convex due to the non-convexity of the components P 0 and rank ( Q ) ; in general solving such an optimization problem is NP-hard and no efficient solution is known. We consider a computationally tractable alter-native by employing recently well-studied convex relaxation tech-niques [11].

Let the function f : C  X  R , where C  X  R d  X  m . The convex envelope [11] of f on C is defined as the largest convex function g such that g (  X  Z )  X  f (  X  Z ) for all  X  Z  X  C . The 1 -norm has been known as the convex envelope of the 0 -norm as [11]: Similarly, trace norm (nuclear norm) has been shown as the convex envelop of the rank function as [17]: Note that both the 1 -norm and the trace-norm functions are convex but non-smooth, and they have been shown to be effective surro-gates of the 0 -norm and the matrix rank functions, respectively.
Based on the heuristic approximations in Eq. (3) and Eq. (4), we can replace the 0 -norm with the 1 -norm, and the rank function with the trace norm function in Eq. (2), respectively. Therefore, we can reformulate the multi-task learning formulation as: The optimization problem in Eq. (5) is the tightest convex relax-ation of Eq. (2). Such a problem can be reformulated as a semi-definite program (SDP) [35], and then solved using many off-the-shelf optimization solvers such as SeDuMi [32]; however, SDP is computationally expensive and can only handle several hundreds of optimization variables. For simplicity, in this paper we assume that all of the m tasks share the same set of training data in Eq. (5), and the derivation below can be easily extended to the case where each learning task has a different set of training data. Related Work: The formulation in Eq. (5) resembles the Alternat-ing Structure Optimization algorithm (ASO) for multi-task learning proposed in [3]. However, they differ in several key aspects: (1) In ASO, the tasks are coupled using a shared low-dimensional struc-ture induced by an orthonormal constraint, and the formulation in ASO is non-convex and its convex counterpart cannot be easily ob-tained. Our formulation encourages the low-rank structure via a trace norm constraint and the resulting formulation is convex. (2) In ASO, in addition to a low-dimensional feature map shared by all tasks, the classifier for each task computes an independent high-dimensional feature map specific to each individual task, which is in general dense and does not lead to interpretable features. In our formulation, the classifier for each task constructs a sparse high-dimensional feature map for discriminative feature identification. (3) The alternating algorithm in ASO can only find a local solution with no known convergence rate. The proposed algorithm for solv-ing the formulation in Eq. (5) finds a globally optimal solution and achieves the optimal convergence rate among all first-order meth-ods. Note that recent works in [12, 14, 37] consider the problem of decomposing a given matrix into its underlying sparse component and low-rank component in a different setting: they study the theo-retical condition under which such two components can be exactly recovered via convex optimization, i.e., the condition of guarantee-ing to recover the sparse and low-rank components by minimizing a weighted combination of the trace norm and the 1 -norm.
In this section, we propose to apply the general projected gradi-ent scheme [11] to solve the constrained optimization problem in Eq. (5). Note that the projected gradient scheme belongs to the cat-egory of first-order methods and has demonstrated good scalability in many optimization problems [11, 25].

The objective function in Eq. (5) is non-smooth and the feasible domain is non-trivial. For simplicity, we denote Eq. (5) as where the functions f ( T ) and g ( T ) are defined respectively as and the set M is defined as M = T T = P Note that f ( T ) is a smooth convex function with Lipschitz contin-uous gradient L f [8] as:  X  f ( T x )  X  X  X  f ( T y ) F  X  L f T x  X  T y F ,  X  T x ,T y g ( T ) is a non-smooth convex function, and M is a compact and convex set [8]; moreover, for any L  X  L f , the following inequality holds [26]: f ( T x )  X  f ( T y )+ T x  X  T y ,  X  f ( T y ) + L where T x ,T y  X  X  .

The projected gradient scheme computes the global minimizer of Eq. (6) via an iterative refining procedure. That is, given T the intermediate solution of the k th iteration, we refine T where P k and t k denote the appropriate projected gradient direc-tion and the step size, respectively. The computation of Eq. (9) de-pends on P k and t k ; in the following subsections, we will present a procedure for estimating appropriate P k and t k , and defer the discussion of detailed projected gradient algorithms to Section 5. Note that since P k is associated with T k and t k , we denote P from the following discussion.
For any L&gt; 0 , we consider the construction associated with the smooth component f ( T ) of the objective function in Eq. (6) as where S, T  X  R d  X  m . It can be verified that f L ( S, T ) is strongly convex with respect to the variable T . Moreover, we denote where g ( T ) is the non-smooth component of the objective func-tion in Eq. (6). From the convexity in g ( T ) , G L ( S, T ) is strongly convex with respect to T . Since the global minimizer of G L ( S, T ) with respect to T can be com-puted as T L,S =argmin Therefore we can obtain the L -projected gradient [25] of f at S via It is obvious that 1 /L can be seen as the step size associated with the projected gradient P L ( S ) by rewritting Eq. (12) as T S  X  X  L ( S ) /L .
From Eq. (12), the step size associated with P L ( S ) is given by 1 /L . Denote the objective function in Eq. (6) as Theoretically, any step size 1 /L satisfying L  X  L f guarantees the global convergence in the projected gradient based algorithms [25]. It follows from Eq. (8) that In practice we can estimate an appropriate L (hence the appropriate step size 1 /L ) by ensuring the inequality in Eq. (14). By applying an appropriate step size and the associated projected gradient in Eq. (9), we can verify that [7, 25]
F ( T )  X  F ( T L,S )  X  T  X  S, P L ( S ) + 1 Moreover, by replacing S with T in Eq. (15), we have Note that the inequality in Eq. (15) characterizes the relationship of the objective values in Eq. (6) using T and its refined version via the procedure in Eq. (9).
The projected gradient scheme requires to solve Eq. (11) for each iterative step given in Eq. (9). In Eq. (11), the objective function is non-smooth and the feasible domain set is non-trivial; we show that its optimal solution can be obtained by solving an unconstrained optimization problem and an Euclidean projection problem sepa-rately.
 Denote T and S in Eq. (11) respectively as Therefore the optimization problem in Eq. (11) can be expressed as where  X  S P and  X  S Q can be computed respectively as Note that  X  P f ( S ) and  X  Q f ( S ) denote the derivative of the smooth component f ( S ) with respect to the variables P and Q , respec-tively. In our experiments, we focus on the least squares loss func-tion, where the gradient of f ( T ) with respect to P and Q can be expressed as We can further rewrite Eq. (17) as subject to T Q  X   X   X , (18) where  X  = L/ 2 . Since T P and T Q are decoupled in Eq. (18), they can be optimized separately as presented in the following subsec-tions.
The optimal T P in Eq. (18) can be obtained by solving the fol-lowing optimization problem: It is obvious that each entry of the optimal matrix T P can be ob-tained by solving where  X  s denotes the entry in  X  S P corresponding to  X  t in T known [33] that the optimal  X  t to Eq. (19) admits an analytical solu-tion; for completeness, we present its proof in Lemma 4.1.
L EMMA 4.1. The minimizer of Eq. (19) can be expressed as
P ROOF . Denote by h (  X  t ) the objective function in Eq. (19), and expressed as where the function sgn (  X  ) is given by of h (  X  t ) at the point  X  t  X  , that is, Since the equation above is satisfied with  X  t  X  defined in Eq. (20), we complete the proof of this lemma.
The optimal T Q in Eq. (18) can be obtained by solving the opti-mization problem as where the constant 1 / 2 is added into the objective function for con-venient presentation. In the following theorem, we show that the optimal T Q to Eq. (21) can be obtained via solving a simple con-vex optimization problem.
  X  S , where q = rank (  X  S Q ) , U  X  R d  X  q , V  X  R m  X  q , and diag (  X  1 ,  X  X  X  , X  q )  X  R q  X  q . Let {  X  i } q following problem: Denote  X = diag (  X  1 ,  X  X  X  , X  q )  X  R q  X  q . Then the optimal solution to Eq. (21) is given by
P ROOF . Assume that the optimal T  X  Q to Eq. (21) shares the same left and right singular vectors as  X  S Q . Then the problem in Eq. (21) is reduced to the problem in Eq. (22). Thus, all that remains is to show that T  X  Q shares the same left and right singular vectors as Denote the Lagrangian function [11] associated with Eq. (21) as Since 0 is strictly feasible in Eq. (21), i.e., 0  X  &lt; X  , the Slater X  X  condition [11] is satisfied and strong duality holds in Eq. (21). Let  X   X   X  0 be the optimal dual variable [11] in Eq. (21). Therefore, singular values on the main diagonal. It is known [36] that the sub-differentials of T Q  X  at T  X  Q can be expressed as On the other hand, we can verify that T  X  Q is optimal to Eq.(21) if and only if 0 is a subgradient of H ( T Q , X   X  ) at T  X  Q of U T and V T , respectively. It follows from Eq. (23) that there exists a point D T = U  X  T  X  d V  X  T T such that of the singular values of D T on the main diagonal. It follows that corresponds to the SVD of  X  S Q . This completes the proof of this theorem.
 Note that the problem in Eq. (22) is convex, and can be solved via Euclidean projection onto the 1 ball.
We present two algorithms based on the projected gradient scheme presented in Section 3 for solving the constrained convex optimiza-tion problem in Eq. (6), and discuss their rates of convergence. Note that the theorems in this section can be proved using standard techniques in [25, 26].
We first present a simple projected gradient algorithm. Let T gradient algorithm refines T k by recycling the following two steps: find a candidate  X  T for the subsequent feasible solution point T via and meanwhile ensure the step size 1 Note that both T k and  X  T are feasible in Eq. (6). It follows from Eq. (16) that the solution sequence generated in the projected gra-dient algorithm leads to a non-increasing objective value in Eq. (6), that is, The pseudo-code of the projected gradient algorithm is presented in Algorithm 1, and its convergence rate analysis is summarized in Theorem 5.1. Note that the stopping criteria in line 11 of Algo-rithm 1 can be set as: the change of objective values in two succes-sive steps are smaller than some pre-specified value (e.g., 10 T HEOREM 5.1. Let T  X  be the global minimizer of Eq. (6); let L f be the Lipschitz continuous gradient defined in Eq.(7). Denote by k the index of iteration, and by T k the solution point in the k th iteration. Then Algorithm 1 converges at the rate of O ( 1 all k  X  1 , we have Algorithm 1 Projected Gradient Method 1: Input: T 0 , L 0  X  R , and max-iter. 2: Output: T . 3: for i =0 , 1 ,  X  X  X  , max-iter do 4: while ( true ) 5: Compute  X  T = T L 6: if F (  X  T )  X  G L 7: else update L i = L i  X  2 . 8: end-if 9: end-while 10: Update T i +1 =  X  T and L i +1 = L i . 11: if stopping criteria satisfied then exit the loop. 12: end-for 13: Set T = T i +1 . where  X  L =max { L 0 , 2 L f } , and L 0 and T 0 are the initial values of L k and T k in Algorithm 1, respectively.
The proposed projected gradient method Section 5.1 is simple to implement but converges slowly. We accelerate the projected gradi-ent method using a scheme developed by Nesterov [26], which has been applied for solving various sparse learning formulations [22]. Algorithm 2 Accelerated Projected Gradient Method 1: Input: T 0 , L 0  X  R , and max-iter. 2: Output: T . 3: Set T 1 = T 0 , t  X  1 =0 , and t 0 =1 . 4: for i =1 , 2 ,  X  X  X  , max-iter do 5: Compute  X  i =( t i  X  2  X  1) /t i  X  1 . 6: Compute S =(1+  X  i ) T i  X   X  i T i  X  1 . 7: while ( true ) 8: Compute  X  T = T L 9: if F (  X  T )  X  G L 10: else update L i = L i  X  2 . 11: end-if 12: end-while 13: Update T i +1 =  X  T and L i +1 = L i . 14: if stopping criteria satisfied then exit the loop. 15: Update t i = 1 2 (1 + 1+4 t 2 16: end-for 17: Set T = T i +1 .

We utilize two sequences of variables in the accelerated pro-jected gradient algorithm: (feasible) solution sequence { T searching point sequence { S k } . In the i -th iteration, we construct the searching point as where the parameter  X  k &gt; 0 is appropriately specified as shown in Algorithm 2. Similar to the projected gradient method, we refine the feasible solution point T k +1 via the general step as: and meanwhile determine the step size by ensuring The searching point S k may not be feasible in Eq. (6), which can be seen as a forecast of the next feasible solution point and hence leads to the faster convergence rate in Algorithm 2. The pseudo-code of the accelerated projected gradient algorithm is presented in Algorithm 2, and its convergence rate analysis is summarized in the following theorem.
 T HEOREM 5.2. Let T  X  be the global minimizer of Eq. (6); let L f be the Lipschitz continuous gradient defined in Eq.(7). Denote by k the index of iteration, and by T k the solution point in the k th iteration. Then Algorithm 2 converges at the rate of O ( for all k  X  1 , we have where  X  L =max { L 0 , 2 L f } , where L 0 and T 0 are the initial values of L k and T k in Algorithm 2.
 Note that the convergence rate achieved by Algorithm 2 is optimal among the first-order methods [26].
In this section, we evaluate the proposed multi-task learning for-mulation in comparison with other representative ones; we also conduct numerical studies on the proposed projected gradient al-gorithms. All algorithms are implemented in MATLAB, and the
We employ six benchmark data sets in our experiments. One of them is AR Face Data [24]: we use its subset consisting of 1400 face images corresponding to 100 persons. Another three are LIB-SVM multi-label data sets 2 : for Scene and Yeast , we use the entire data sets; for MediaMill , we generate several subsets by randomly sampling 8000 data points with different numbers of labels. Refer-ences and Science are Yahoo webpages data sets [34]: we prepro-cess the data sets following the same procedures in [15]. All of the benchmark data sets are normalized and their statistics are summa-rized in Table 1. Note that in our multi-task learning setting, each task corresponds to a label and we employ the least squares loss function for the following empirical studies.
We apply the proposed multi-task learning algorithm on the face images and then demonstrate the extracted sparse and low-rank structures. We use a subset of AR Face Data for this experiment. The original size of these images is 165  X  120 ; we reduce the size to 82  X  68 .

We convert the face recognition problem into the multi-task learn-ing setting, where one task corresponds to learning a linear classi-fier, i.e., f ( x )=( p + q ) T x , for recognizing the faces of one person. By solving Eq. (5), we obtained p (sparse structure) and q (low-rank structure); we reshape p and q and plot them in Fig-ure 2. We only plot p 1 and q 1 for demonstration. The first two plots http://www.public.asu.edu/~jchen74/MTL http://www.csie.ntu.edu.tw/~cjlin Figure 2: Extracted sparse (first and third plots) and low-rank (second and fourth plots) structures on AR face images with different sparse regularization and rank constraint parameters in Eq. (5): for the first two plots, we set  X  =11 , X  =0 . 08 ;for the last two plots, we set  X  =14 , X  =0 . 15 . in Figure 2 are obtained by setting  X  =11 , X  =0 . 08 in Eq. (5): we obtain a sparse structure of 15 . 07% nonzero entries and a low-rank structure of rank 3 ; similarly, the last two plots are obtained by setting  X  =14 , X  =0 . 15 , we obtain a sparse structure of 5 . 35% nonzero entries and a low-rank structure of rank 7 . We observe that the sparse structure identifies the important detailed facial marks, and the low-rank structure preserves the rough shape of the human face; we also observe that a large sparse regularization parameter leads to high sparsity (lower percentage of the non-zero entries) and a large rank constraint leads to structures of high rank.
We compare the proposed multi-task learning formulation with other representative ones in terms of average Area Under the Curve (AUC), Macro F 1 , and Micro F 1 [39]. The reported experimental results are averaged over five random repetitions of the data sets into training and test sets of the ratio 1:9 . In this experiment, we stop the iterative procedure of the algorithms if the change of the objective values in two consecutive iterations is smaller than 10 or the iteration numbers larger than 10 5 . The experimental setup is summarized as follows: 1. MixedNorm : The proposed multi-task learning formulation with the least squares loss. The trace-norm constraint parameter is tuned and k is the label number; the one-norm regularization parameter { 2  X  i } 10 2. OneNorm : The formulation of the least squares loss with the one-norm regularization. The one-norm regularization parameter { 2  X  i } 10 3. TraceNorm : The formulation of the least squares loss with the trace-norm constraint. The trace-norm constraint parameter is tuned and k denotes the label number. 4. ASO : The alternating structure optimization algorithm [3]. The regularization parameter is tuned in { 10  X  3  X  i } 10 i ity of the shared subspace is tuned in { 2  X  i } p and k denotes the label number. 5. IndSVM : Independent support vector machines. The regulariza-tion parameter is tuned in { 10  X  i } 3 i =1  X  X  2  X  i } 50 6. RidgeReg : Ridge regression. The regularization parameter is { 2  X  i } 10
We present the averaged performance (with standard deviation) of the competing algorithms in Table 2. From Table 2, we have 0 . 9 . the following observations: (1) MixedNorm achieves the best per-formance among the competing algorithms on all benchmark data sets in this experiment, which gives strong support for our ratio-nale of improving the generalization performance by learning the sparse and low-rank patterns simultaneously from multiple tasks; (2) TraceNorm outperforms OneNorm on Scene and Yeast data sets, which implies that the shared low-rank structure may be important for image and gene classification tasks; meanwhile, OneNorm out-performs TraceNorm on MediaMill and yahoo webpage data sets, which implies that sparse discriminative features may be impor-tant for multimedia learning problems; (3) the multi-task learn-ing algorithms in our experiments outperform SVM and RidgeReg, which verifies the effect of improved generalization performance via multi-task learning.
We conduct sensitivity studies on the proposed multi-task learn-ing formulation, and study how the training ratio and the task num-ber affect its generalization performance.
 Effect of the training ratio We use Scene data for this experiment. We vary the training ratio in the set { 0 . 1  X  i } 9 i =1 obtained generalization performance for each training ratio. The experimental results are depicted in Figure 3. We can observe that (1) for all of the compared algorithms, the resulting general-ization performance improves with the increase of the training ra-tio; (2) MixedNorm outperforms other competing algorithms in all cases in this experiment; (3) when the training ratio is small (e.g., smaller than 0 . 5 ), multi-task learning algorithms can significantly improve the generalization performance compared to IndSVM and RidgeReg; on the other hand, when the training ratio is large, all competing algorithms achieve comparable performance. This is consistent with previous observations that multi-task learning is most effective when the training size is small.
 Effect of the task number We use MediaMill data for this ex-periment. We generate five data sets by randomly sampling 8000 data points with the task number set at 20 , 40 , 60 , 80 , 100 , respec-tively; for each data set, we set the training and test ratio at 1:9 and record the average generalization performance of the multi-task learning algorithms over 5 random repetitions. The experimental results are depicted in Figure 4. We can observe that (1) for all of the compared algorithms, the achieved performance decreases with the increase of the task numbers; (2) MixedNorm outperforms or perform competitively compared to other algorithms with different task numbers; (3) all of the specific multi-task learning algorithms outperform IndSVM and RidgeReg. Note that the learning problem becomes more difficult as the number of the tasks increases, lead-ing to decreased performance for both multi-task and single-task learning algorithms. We only present the performance comparison in terms of Macro/Micro F 1 ; we observe a similar trend in terms of average AUC in the experiments. Figure 4: Performance comparison of the six competing multi-task learning algorithms with different numbers of tasks in terms of Macro F 1 (top plot) and Micro F 1 (bottom plot). We empirically compare the projected gradient algorithm (PG) in Algorithm 1 and the accelerated projected gradient algorithm (AG) in Algorithm 2 using Scene data. We present the comparison results of setting  X  =1 , X  =2 and  X  =6 , X  =4 in Eq. (5); for other parameter settings, we observe similar trends in our experiments. Figure 5: Convergence rate comparison between PG and AG: the relationship between the objective value of Eq. (5) and the iteration number (achieved via PG and AG, respectively). For the left plot, we set  X  =1 , X  =2 ; for the right plot, we set  X  =6 , X  =4 .
 Comparison on convergence rate We apply PG and AG for solv-ing Eq. (5) respectively, and compare the relationship between the obtained objective values and the required iteration numbers. The experimental setup is as follows: we terminate the PG algorithm when the change of objective values in two successive steps is smaller than 10  X  5 and record the obtained objective value; we then use such a value as the stopping criterion in AG, that is, we stop AG when AG attains an objective value equal to or smaller than the one attained by PG. The experimental results are presented in Figure 5. We can observe that AG converges much faster than PG, and their respective convergence speeds are consistent with the theoretical convergence analysis in Section 5, that is, PG converges at the rate of O (1 /k ) and AG at the rate of O (1 /k 2 ) , respectively. Comparison on computation cost We compare PG and AG in terms of computation time (in seconds) and iteration numbers (for attaining convergence) by using different stopping criteria { 10 Figure 6: Comparison of PG and AG in terms of the computa-tion time in seconds (left column) and iteration number (right column) with different stopping criteria. The x-axis indexes the stopping criterion from 10  X  1 to 10  X  10 . Note that we stop PG or AG when the change of the objective value in Eq. (5) is smaller than the value of stopping criteria. For the first row, we set  X  =1 , X  =2 ; for the second row, we set  X  =6 , X  =4 .
 We stop PG and AG if the stopping criterion is satisfied, that is, the change of the objective values in two successive steps is smaller Figure 6. We can observe from these results that (1) PG and AG re-quire higher computation costs (more computation time and larger numbers of iterations) for a smaller value of the stopping criterion (higher accuracy in the optimal solution); (2) in general, AG re-quires lower computation costs than PG in this experiment; such an efficiency improvement is more significant when a smaller value is used in the stopping criterion.
 Table 3: Comparison of PG and AG in terms of computation time (in seconds) and iteration number using different stopping criteria.
We consider the problem of learning sparse and low-rank pat-terns from multiple related tasks. We propose a multi-task learning formulation in which the sparse and low-rank patterns are induced respectively by a cardinality regularization term and a low-rank constraint. The proposed formulation is non-convex; we convert it into its tightest convex surrogate and then propose to apply the general projected gradient scheme to solve such a convex surrogate. We present the procedures for computing the projected gradient and ensuring the global convergence of the projected gradient scheme. Moreover, we show the projected gradient can be obtained via solv-ing two simple convex subproblems. Additionally, we present two detailed algorithms based on the projected gradient scheme and dis-cuss their rates of convergence. Our experiments on benchmark data sets demonstrate the effectiveness of the proposed multi-task learning formulation and the efficiency of the proposed projected gradient algorithms. In the future, we plan to conduct a theoretical analysis on the proposed multi-task learning formulation and apply the proposed algorithm to other real-world applications. This work was supported by NSF IIS-0612069, IIS-0812551, IIS-0953662, NGA HM1582-08-1-0016, the Office of the Director of National Intelligence (ODNI), and Intelligence Advanced Research Projects Activity (IARPA), through the US Army. [1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new [2] R. K. Ando. BioCreative II gene mention tagging system at [3] R. K. Ando and T. Zhang. A framework for learning [4] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task [5] B. Bakker and T. Heskes. Task clustering and gating for [6] J. Baxter. A model of inductive bias learning. Journal of [7] A. Beck and M. Teboulle. A fast iterative [8] D. P. Bertsekas, A. Nedic, and A. E. Ozdaglar. Convex [9] J. Bi, T. Xiong, S. Yu, M. Dundar, and R. B. Rao. An [10] S. Bickel, J. Bogojeska, T. Lengauer, and T. Scheffer. [11] S. Boyd and L. Vandenberghe. Convex Optimization . [12] E. J. Cand X s, X. Li, Y. Ma, and J. Wright. Robust principal [13] R. Caruana. Multitask learning. Machine Learning , [14] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. [15] J. Chen, L. Tang, J. Liu, and J. Ye. A convex formulation for [16] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning [17] M. Fazel, H. Hindi, and S. Boyd. A rank minimization [18] G. Gene and V. L. Charles. Matrix computations . Johns [19] L. Jacob, F. Bach, and J.-P. Vert. Clustered multi-task [20] S. Ji and J. Ye. An accelerated gradient method for trace [21] N. D. Lawrence and J. C. Platt. Learning to learn with the [22] J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with Efficient [23] J. Liu and J. Ye. Efficient euclidean projections in linear [24] A. Martinez and R. Benavente. The AR face database. [25] A. Nemirovski. Efficient Methods in Convex Programming . [26] Y. Nesterov. Introductory Lectures on Convex Programming . [27] G. Obozinski, B.Taskar, and M. Jordan. Joint covariate [28] T. K. Pong, P. Tseng, S. Ji, and J. Ye. Trace norm [29] A. Schwaighofer, V. Tresp, and K. Yu. Learning gaussian [30] A. Shapiro. Weighted minimum trace factor analysis. [31] S. Si, D. Tao, and B. Geng. Bregman divergence-based [32] J. F. Sturm. Using sedumi 1.02, a matlab toolbox for [33] R. Tibshirani. Regression shrinkage and selection via the [34] N. Ueda and K. Saito. Single-shot detection of multiple [35] L. Vandenberghe and S. Boyd. Semidefinite programming. [36] G. A. Watson. Characterization of the subdifferential of [37] J. Wright, A. Ganesh, S. Rao, and Y. Ma. Robust principal [38] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. Multi-task [39] Y. Yang and J. O. Pedersen. A comparative study on feature [40] K. Yu, V. Tresp, and A. Schwaighofer. Learning gaussian [41] J. Zhang, Z. Ghahramani, and Y. Yang. Learning multiple
