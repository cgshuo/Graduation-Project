 In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database. Our main contribution is to introduce a novel method called safe pat-tern pruning (SPP) for a class of predictive pattern mining problems. The SPP method allows us to efficiently find a su-perset of all the predictive patterns in the database that are needed for the optimal predictive model. The advantage of the SPP method over existing boosting-type method is that the former can find the superset by a single search over the database, while the latter requires multiple searches. The SPP method is inspired by recent development of safe fea-ture screening . In order to extend the idea of safe feature screening into predictive pattern mining, we derive a novel pruning rule called safe pattern pruning (SPP) rule that can be used for searching over the tree defined among patterns in the database. The SPP rule has a property that, if a node corresponding to a pattern in the database is pruned out by the SPP rule, then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model. We apply the SPP method to graph mining and item-set mining problems, and demon-strate its computational advantage.
 Predictive pattern mining, Graph mining, Item-set mining, Sparse learning, Safe screening, Convex optimization  X  Corresponding author
In this paper, we study predictive pattern mining. The goal of predictive pattern mining is discovering a set of patterns from databases that are needed for constructing a good predictive model. Predictive pattern mining prob-lems can be interpreted as feature selection problems in su-pervised machine learning tasks such as classifications and regressions. The main difference between predictive pattern mining and ordinal feature selection is that, in the former, the number of possible patterns in databases are extremely large, meaning that we cannot naively search over all the patterns in databases. We thus need to develop algorithms that can exploit some structures among patterns such as trees or graphs for efficiently discovering good predictive patterns.

To be concrete, suppose that there are D patterns in a database, which is assumed to be extremely large. For the i -th transaction in the database, let z i 1 ,...,z iD represent the occurrence of each pattern. We consider linear predictive model in the form of where A  X  { 1 ,...,D } is a set of patterns that would be selected by a mining algorithm, and { w j } j  X  X  and b are the parameters of the linear predictive model. Here, the goal is to select a set of predictive patterns in A and find the model parameters { w j } j  X  X  and b so that the predictive model in the form of (1) has good predictive ability.

Existing predictive pattern mining studies can be catego-rized into two approaches. The first approach is two-stage approach, where a mining algorithm is used for selecting the set of patterns A in the first stage, and the predictive model is fitted by only using the selected patterns in A in the sec-ond stage. Two-stage approach is computationally efficient because the mining algorithm is run only once in the first stage. However, two-stage approach is suboptimal as predic-tive model building procedure because it does not directly optimize the predictive model. The second approach is direct approach, where a mining algorithm is integrated in a fea-ture selection method. An advantage of direct approach is that a set of patterns that are useful for predictive modeling is directly searched for. However, the computational cost of existing direct approach is usually much greater than two-stage approach because the mining algorithm is run multiple times. For example, in a stepwise feature selection method, the mining algorithm is run at each step in order to find the pattern that best improves the current predictive model.
In this paper, we study a direct approach for predictive pattern mining based on sparse modeling . In the literature of machine-learning and statistics, sparse modeling has been intensively studied in the past two decades. An advantage of sparse modeling is that the problem is formulated as a convex optimization problem, and it allows us to investigate several properties of solutions from a wide variety of per-spectives. In addition, many efficient solvers that can be applicable to high-dimensional problems (although not as high as the number of patterns in databases as we consider in this paper) have been developed.

Predictive pattern mining algorithms based on sparse mod-eling have been also studied in the literature [12, 14, 13]. All these studies rely on a technique developed in the context of boosting [4]. Roughly speaking, in each step of the boosting-type method, a feature is selected based on a certain criteria, and an optimization problem defined over the set of features selected so far is solved. Therefore, when the boosting-type method is used for predictive pattern mining tasks, one has to search over the database as many times as the number of steps in the boosting-type method.

Our main contribution in this paper is to propose a novel method for sparse modeling-based predictive pattern min-ing. Denoting the set of patterns that would be used in the optimal predictive model as A  X  , the proposed method can find a set of patterns  X  A  X  A  X  , i.e.,  X  A contains all the pre-dictive patterns that are needed for the optimal predictive model. It means that, if we solve the sparse modeling prob-lem defined over the set of patterns  X  A , then it is guaranteed that the resulting predictive model is optimal. The main advantage of the proposed method over the above boosting-type method is that a mining algorithm is run only once for finding the set of patterns  X  A .

The proposed method is inspired by recent safe feature screening studies [5, 20, 18, 1, 9, 17, 19, 6, 10]. In ordinary feature selection problems, safe feature screening allows us to identify a set of features that would never be used in the optimal model before actually solving the optimization problem It means that these features can be safely removed from the training set. Unfortunately, however, it cannot be applied to predictive pattern mining problems because it is computationally intractable to apply safe feature screening to each of extremely large number of patterns in a database for checking whether the pattern can be safely removed out or not.

In this paper, we develop a novel method called safe pat-tern pruning (SPP) . Considering a tree structure defined among patterns in the database, the SPP method allows us to prune the tree in such a way that, if a node corre-sponding to a pattern in the database is pruned out, then it is guaranteed that all the patterns corresponding to its descendant nodes would never be needed for the optimal predictive model. The SPP method can be effectively used in predictive pattern mining problems because we can iden-tify an extremely large set of patterns that are irrelevant to the optimal predictive model by exploiting the tree structure among patterns in the database, A superset  X  A X  X   X  can be obtained by collecting the set of patterns corresponding to the nodes that are not pruned out by the SPP method. We use the following notations in the rest of the paper. For any natural number n , we define [ n ] := { 1 ,...,n } . For an n -dimensional vector v and a set I  X  [ n ], v I represents a sub-vector of v whose elements are indexed by I . The true, and I ( z ) = 0 otherwise. Boldface 0 and 1 indicate a vector of all zeros and ones, respectively.

Here is the outline of the paper.  X  2 presents problem setup and existing methods.  X  3 describes our main contribution where we introduce safe pattern pruning (SPP) method.  X  4 covers numerical experiments for demonstrating the advan-tage of the SPP method.  X  5 concludes the paper.
We first formulate our problem setting.
In this paper we consider predictive pattern mining prob-lems. Let us consider a database with n records, and denote rected graph in the case of graph mining, while it is a set of items in the case of item-set mining. The response variable y is defined on R and on { X  1 } for regression and classifica-tion problems, respectively. Let T be the set of all patterns in the database, and denote its size as D := |T| . For ex-ample, T is the set of all possible subgraphs in the case of graph mining, while T is the set of all possible item-sets in the case of item-set mining. Alternatively, G i is represented as a D -dimensional binary vector x i  X  { 0 , 1 } D whose t -th element is defined as The number of patterns D is extremely large in all practical pattern mining problems. It implies that any algorithms that naively search over all D patterns are computationally infeasible.

In order to study both regression and classification prob-lems in a unified framework, we consider the following class of convex optimization problems: where f : R  X  R is a gradient Lipschitz continuous loss function and  X  &gt; 0 is a tuning parameter. We refer the problem (2) as primal problem and write the optimal so-lution as w  X  . When f ( z ) := 1 2 z 2 and  X  i := x i  X  :=  X  y i  X  i  X  [ n ], the general problem (2) is reduced to the following L 1 -penalized regression problem defined over D + 1 variables: On the other hand, when f ( z ) := 1 2 max { 0 , 1  X  z } 2 y x i ,  X  i := y i ,  X  i := 0  X  i  X  [ n ], the general problem (2) is reduced to the following L 1 -penalized classification problem defined over D + 1 variables: Remembering that D is extremely large, we cannot solve these L 1 -penalized regression and classification problems in a standard way.

The dual problem of (2) is defined as where  X  = y ,  X  =  X  X  X  for regression problem in (3), and  X  = 1 ,  X  = 0 for classification problem in (4). The dual optimal solution is denoted as  X   X  .

The key idea for handling an extremely large number of patterns in the database is to exploit the tree structure de-fined among the patterns. Figure 1 shows tree structures for graph mining (left) and item-set mining (right). As shown in Figure 1, each node of the tree corresponds to each pat-tern in the database. Those trees are constructed in such a way that, for any pair of a node t and one of its descendant node t 0 , they satisfy the relation t  X  t 0 , i.e., the pattern t a superset of the pattern t . It suggests that, for such a pair of t and t 0 , and, conversely
To the best of our knowledge, except for the boosting-type method described in  X  1 and its extensions or modifi-cations [12, 14, 13], there is no other existing method that can be used for solving the convex optimization problem (2) for predictive pattern mining problems defined over an extremely large number of patterns D . The boosting-type method solves the dual problem (5). The difficulty in the dual problem is that there are extremely large number of ing from the optimization problem (5) without these con-straints, in each step of the boosting-type method, the most violating constraint is added to the problem, and an opti-mization problem only with the constraints added so far is solved. In optimization literature, this approach is generally known as the cutting-plane method, for which its effective-ness has been also shown in some machine learning problems (e.g., [7]). The key computational trick used by [12, 14, 13] is that, for finding the most violating constraint in each step, it is possible to efficiently search over the database by using a certain pruning strategy in the tree as depicted in Fig-ure 1. This method is terminated when there is no violating constraints in the database.

In each single step of the boosting-type method, one first has to search over the database by a mining algorithm, and then run a convex optimization solver for the problem with the newly added constraint. Boosting-type method is com-putationally expensive because these steps must be repeated until all the constraints corresponding to all the predictive patterns in A  X  are added. In the next section, we propose a novel method called safe pattern pruning , by which the op-timal model is obtained by a single search over the database and a single run of convex optimization solver.
In this section, we present our main contribution.
It is well known that L 1 penalization in (2) makes the solution w  X  sparse, i.e., some of its elements would be zero. The set of patterns which has non-zero coefficients are called active and denoted as A  X   X  X  , while the rest of the patterns are called non-active . A nice property of sparse learning is that the optimal solution does not depend on any non-active patterns. It means that, after some non-active patterns are removed out from the dataset, the same optimal solution can be obtained. The following lemma formally states this well-known but important fact.
 Lemma 1. Let  X  A be a set such that A  X   X   X  A  X  T , and P  X  ( w  X  A ,b ) be the objective function of (2) in which w 0 is substituted: Then, the optimal solution of the original problem (2) is given by Lemma 1 indicates that, if we have a set of patterns  X  A X  X  we have only to solve a smaller optimization problem defined only with the set of patterns in  X  A . It means that, if such an  X  A is available, we do not have to work with extremely large number of patterns in the database.

In the rest of this section, we propose a novel method for finding such a set of patterns  X  A X  X   X  by searching over the database only once. Specifically, we derive a novel pruning condition which has a property that, if the condition is sat-isfied at a certain node, then all the patterns corresponding to its descendant nodes and the node itself are guaranteed to be non-active. After traversing the tree, we simply define  X  A be the set of nodes which are not pruned out. Then, it is guaranteed that  X  A satisfies the condition in Lemma 1. The proposed method is inspired by recent studies on safe fea-ture screening. We thus call our new method as safe pattern pruning (SPP) .
The following theorem provides a specific pruning condi-tion that can be used together with any search strategies on a tree. Let T sub ( t )  X  T be a set of nodes in a subtree of T having t as a root node and containing all descendant nodes of t . We derive a condition for safely screening the entire T sub ( t ) out, which is computable at the node t with-out traversing the descendant nodes. This means that, our rule, called safe pattern pruning rule , tells us whether a pat-tern t 0  X  T sub ( t ) has a chance to be active or not based on the information available at the root node of the subtree t . An important consequence of the condition below is that if the condition holds, i.e., any t 0  X  T sub ( t ) cannot be active, then we can stop searching over the subtree (pruning the subtree).
 Theorem 2 (Safe pattern pruning (SPP) rule).
 Given an arbitrary primal feasible solution (  X  w , arbitrary dual feasible solution  X   X  , for any node t 0  X  T the following safe pattern pruning criterion (SPPC) provides a rule where u := max for t  X  [ D ] , and The proof of Theorem 2 is presented in  X  3.3.

SPPC( t ) depends on three scalar quantities u t , v t and r The first two quantities u t and v t are obtained by using in-formation on the pattern t , while the third quantity r  X  not depend on t . Noting that all these three quantities are non-negative, the SPP rule would be more powerful (have more chance to prune the subtree) if these three quantities are smaller. The following corollary is the consequence of the simple fact that the first two quantities u t and v a descendant node are smaller than those at its ancestor nodes.
 Corollary 3. For any node t 0  X  X  sub ( t ) , The proof of Corollary 3 is presented in Appendix. This corollary suggests that the SPP rule would be more powerful at deeper nodes.

The third quantity r  X  represents the goodness of the pair of primal and dual feasible solutions measured by the duality gap , the difference between the primal and dual objective values. It means that, if sufficiently good pair of primal and dual feasible solutions are available, the SPP rule would be powerful. We will discuss how to obtain good feasible solutions in  X  3.4.
In order to prove Theorem 2, we first clarify the condition for any pattern t  X  T to be non-active by the following lemma.
 Lemma 4. For a pattern t  X  X  , Proof of Lemma 4 is presented in Appendix. Lemma 4 in-dicates that, if an upper bound of | P i  X  [ n ]  X  it  X   X  than 1, then we can guarantee that w  X  t = 0. In what fol-lows, we actually show that SPPC( t ) is an upper bound of
In order to derive an upper bound of | P i  X  [ n ]  X  it  X  use a technique developed in a recent safe feature screening study [10]. The following lemma states that, based on a pair of a primal feasible solution (  X  w ,  X  b ) and a dual feasible solution  X   X  , we can find a ball in the dual solution space in which the dual optimal solution  X   X  exists.

Lemma 5 (Theorem 3 in [10]). Let (  X  w ,  X  b ) be an arbi-trary primal feasible solution, and  X   X  be an arbitrary dual fea-sible solution. Then, the dual optimal solution  X   X  is within a ball in the dual solution space R n with the center the radius r  X  := q 2( P  X  (  X  w ,  X  b )  X  D  X  (  X   X  )) / X  . See Theorem 3 and its proof in [10]. This lemma tells that, given a pair of primal feasible and dual feasible solutions, we can bound the dual optimal solution within a ball.
Lemma 5 can be used for deriving an upper bound of | P i  X  [ n ]  X  it  X   X  i | . Since we know that the dual optimal solu-tion  X   X  is within the ball in Lemma 5, an upper bound of any t  X  T can be obtained by solving the following convex optimization problem:
UB( t ) := arg max Fortunately, the convex optimization problem (7) can be explicitly solved as the following lemma states.
Lemma 6. The solution of the convex optimization prob-lem (7) is given as
UB( t ) = X Proof of Lemma 6 is presented in Appendix.

Although UB( t ) provides a condition to screen any t  X  X  , calculating UB( t ) for all t  X  X  is computationally prohibit-ing in our extremely high dimensional problem setting. In the next lemma, we will show that SPPC( t )  X  UB( t 0 ) for  X  t 0  X  X  sub ( t ), i.e., SPPC( t ) in Theorem 2 is an upper bound of UB( t 0 ), which enables us to efficiently prune subtrees dur-ing the tree traverse process.
 Lemma 7. For any t 0  X  X  sub ( t ) ,
UB( t 0 ) = X Finally, by combining Lemmas 4, 5, 6 and 7, we can prove Theorem 2.
 Proof of Theorem 2 .
 Proof. From Lemmas 5, 6 and 7, From Lemma 4 and (8),
Safe pattern pruning rule in Theorem 2 depends on a ble solution  X   X  . Although the rule can be constructed from any solutions as long as they are feasible, the power of the rule depends on the goodness of these solutions. Specif-ically, the criterion SPPC( t ) depends on the duality gap P (  X  w ,  X  b )  X  D  X  (  X   X  ) which would vanish when these primal and dual solutions are optimal. Roughly speaking, it suggests that, if these solutions are somewhat close to the optimal ones, we could expect that the SPP rule is powerful.
In practical predictive pattern mining tasks, we need to find a good penalty parameter  X  based on a model selec-tion technique such as cross-validation. In model selection, a sequence of solutions with various different penalty pa-rameters must be trained. Such a sequence of solutions is sometimes referred to as a regularization path [11]. Regu-larization path of the problem (2) is usually computed from larger  X  to smaller  X  because more sparse solutions would be obtained for larger  X  . Let us write the sequence of  X  s as  X  0 &gt;  X  1 &gt; ... &gt;  X  K . When computing such a sequence of solutions, it is reasonable to use warm-start approach where the previous optimal solution at  X  k  X  1 is used as the initial starting point of the next optimization problem at  X  k . In such a situation, we can also make use of the previous so-lution at  X  k  X  1 as the feasible solution for the safe pattern pruning rule at  X  k .

In sparse modeling literature, it is custom to start from the largest possible  X  at which the primal solution is given as w  X  = 0 and b  X  =  X  y , where  X  y is the sample mean of { y The largest  X  is given as In order to solve this maximization problem over the database, for a node t and t 0  X  X  sub ( t ), we can use the following upper bound
X and this upper bound can be exploited for pruning the search over the tree.

Algorithm 1 shows the entire procedure for computing the regularization path by using the SPP rule.
 Algorithm 1 Regularization path computation algorithm 2: for k = 1 ,...,K do 3: Find  X  A (  X  k )  X  A  X  (  X  k ) by searching over the tree 4: Solve a small optimization problems in (6) with 5: end for Output: { ( w  X  (  X  k ) ,b  X  (  X  k )) } k  X  [ K ] and { (  X 
In this section, we demonstrate the effectiveness of the proposed safe pattern pruning ( SPP ) method through numer-ical experiments. We compare SPP with the boosting-based method ( boosting ) discussed in  X  2.2.
We considered regularization path computation scenario described in  X  3.4. Specifically, we computed a sequence of optimal solutions of (2) for a sequence of 100 penalty param-eters  X  evenly allocated between  X  0 =  X  max and 0 . 01  X  logarithmic scale. For solving the convex optimization prob-lems, we used coordinate gradient descent method [16]. The optimization solver was terminated when the duality gap felled below 10  X  6 . In both of SPP and boosting , we used warm-start approach. In addition, the solution at the previ-ous  X  was also used as the feasible solution for constructing the SPP rule at the next  X  . We used gSpan algorithm [21] for mining subgraphs. We wrote all the codes (except gSpan part in graph mining experiment) in C++. All the compu-tations were conducted by using a single core of an Intel Xeon CPU E5-2643 v2 (3.50GHz) with 64GB MEM.
We applied SPP and boosting to graph classification and regression problems. For classification, we used CPDB and mutagenicity datasets, containing n = 648 and n = 4377 chemical compounds respectively, for which the goal is to predict whether each compound has mutagenicity or not. For regression, we used Bergstrom and Karthikeyan datasets where the goal is to predict the melting point of each of the n = 185 and n = 4173 chemical compounds. All datasets are downloadable from http://cheminformatics.org/datasets/. We considered the cases with maxpat  X  { 5 , 6 , 7 , 8 , 9 , 10 } , where maxpat indicates the maximum number of edges of subgraphs we wanted to find.
 Figure 2 shows the computation time of the two methods. In all the cases, SPP is faster than boosting , and the dif-ference gets larger as maxpat increases. Figure 2 also shows the computation time taken in traversing the trees ( tra-verse ) and that taken in solving the optimization problems ( solve ). The results indicate that traverse time of SPP are only slightly better than that of boosting . It is because the most time-consuming component of gSpan is the mini-mality check of the DFS (depth-first search) code, and the traverse time mainly depends on how many different nodes are generated in the entire regularization path computation process 1 . In terms of solve time, there are large differences between SPP and boosting . In SPP , we have only to solve a single convex optimization problem for each  X  . In boost-ing , on the other hand, convex optimization problems must be repeatedly solved every time a new pattern is added to the working set. Figure 4 shows the total number of tra-
A common trick used in graph mining algorithms with gSpan is to keep the minimality check results in the memory for all the nodes generated so far. versed nodes in the entire regularization path computation process. Total number of traversed nodes in SPP is much smaller than those of boosting , which is because one must repeat searching over trees many times in boosting .
We applied SPP and boosting to item-set classification and regression problems. For classification, we used splice dataset ( n = 1000 and the number of items d = 120) and a9a dataset ( n = 32561 and d = 123). For regression, we used dna dataset ( n = 2000 and d = 180) and protein dataset ( n = 6621 and d = 714) 2 . All datasets were obtained from LIBSVM Dataset site [3]. We considered the cases with maxpat  X  { 3 , 4 , 5 , 6 } , where maxpat here indicates the maximum size of item-sets we wanted to find.

Figure 3 compares the computation time of the two meth-ods. In all the cases, SPP is faster than boosting . Here again, Figure 3 also shows the computation time taken in traversing the trees ( traverse ) and that taken in solving the optimization problems ( solve ). In contrast to the graph mining results, traverse time of SPP are much smaller than that of boosting because it simply depends on how many nodes are traversed in total. Figure 5 shows the total num-ber of traversed nodes in the entire regularization path com-putation process. Especially when  X  is small where the num-ber of active patterns are large, boosting needed to traverse large number of nodes, which is because the number of steps of boosting is large when there are large number of active patterns.
This dataset is provided for classification. We used it for regression simply by regarding the class label as the scalar response variable.
In this paper, we introduced a novel method called safe pattern pruning (SPP) method for a class of predictive pat-tern mining problems. The advantage of the SPP method predictive patterns that are used in the optimal predictive model by a single search over the database. We demon-strated the computational advantage of the SPP method by applying it to graph classification/regression and item-set classification/regression problem As a future work, we will study how to integrate the SPP method with a technique for providing the statistical significances of the discovered patterns [15]. For this work, MK was partially supported from JSPS KAKENHI 26280083 and 26730120, KT was partially sup-ported from JST CREST 15656320, IT was partially sup-ported from JST CREST 15656320, and JSPS KAKENHI 26280083, 16H00886. [1] A. Bonnefoy, V. Emiya, L. Ralaivola, and [2] S. Boyd and L. Vandenberghe. Convex optimization . [3] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [4] A. Demiriz, K. P. Bennett, and J. Shawe-Taylor. [5] L. El Ghaoui, V. Viallon, and T. Rabbani. Safe [6] O. Fercoq, A. Gramfort, and J. Salmon. Mind the [7] T. Joachims. Training linear svms in linear time. In [8] T. Kudo, E. Maeda, and Y. Matsumoto. An [9] J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe Screening [10] E. Ndiaye, O. Fercoq, A. Gramfort, and J. Salmon. [11] M. Y. Park and T. Hastie. L1-regularization path [12] H. Saigo, T. Kadowaki, and K. Tsuda. A linear [13] H. Saigo, S. Nowozin, T. Kadowaki, T. Kudo, and [14] H. Saigo, T. Uno, and K. Tsuda. Mining complex [15] S. Suzumura, K. Nakagawa, M. Sugiyama, K. Tsuda, [16] P. Tseng and S. Yun. A coordinate gradient descent [17] J. Wang, J. Zhou, J. Liu, P. Wonka, and J. Ye. A [18] J. Wang, J. Zhou, P. Wonka, and J. Ye. Lasso [19] Z. J. Xiang, Y. Wang, and P. J. Ramadge. Screening [20] Z. J. Xiang, H. Xu, and P. J. Ramadge. Learning [21] X. Yan and J. Han. gspan: Graph-based substructure Proof. For any pair of nodes t and t 0  X  X  sub ( t ), First consider the case where u t = P i :  X  u Next, consider the case where u t =  X  P i :  X  u thermore, it is clear that v t  X  v t 0 . Since r  X  &gt; 0, SPPC( t )  X  SPPC( t 0 ).

Proof. Based on the convex optimization theory (see, e.g., [2]), the KKT optimality condition of the primal prob-lem (2) and the dual problem (5) is written as It suggests that
Proof. Let  X  : ,t := [  X  1 t ,..., X  nt ] &gt; . First, note that the objective part of the optimization problem (7) is rewritten as Thus, we consider the following convex optimization prob-lem: Let us define the Lagrange function and then the optimization problem (12) is written as The KKT optimality conditions are summarized as where note that  X  &gt; 0 because the problem does not have a minimum value when  X  = 0. Differentiating the Lagrange function w.r.t.  X  and using the fact that it should be zero, By substituting (15) into (13), Since the objective function is a quadratic concave function w.r.t.  X  , we obtain the following by considering the condition (14c): By substituting this into (15), Since  X  &gt; 0 and (14d) indicates k  X   X   X   X  k 2 2  X  r substituting (16) into this equality, Then, from (16), the solution of (12) is given as and the minimum objective function value of (12) is Then, substituting (17) into (11), the optimal objective value of (7) is given as
Proof. First, using the bound introduced in [8], Next, it is clear that By combining them,
X
