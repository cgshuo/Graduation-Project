 Web search is increasingly exploiting named entities like per-sons, places, businesses, addresses and dates. Entity ranking is also of current interest at INEX and TREC. Numerical quantities are an important class of entities, especially in queries about prices and features related to products, ser-vices and travel. We introduce Quantity Consensus Queries (QCQs), where each answer is a tight quantity interval dis-tilled from evidence of relevance in thousands of snippets. Entity search and factoid question answering have benefited from aggregating evidence from multiple promising snippets, but these do not readily apply to quantities. Here we pro-pose two new algorithms that learn to aggregate information from multiple snippets. We show that typical signals used in entity ranking, like rarity of query words and their lexical proximity to candidate quantities, are very noisy. Our al-gorithms learn to score and rank quantity intervals directly, combining snippet quantity and snippet text information. We report on experiments using hundreds of QCQs with ground truth taken from TREC QA, Wikipedia Infoboxes, and other sources, leading to tens of thousands of candidate snippets and quantities. Our algorithms yield about 20% better MAP and NDCG compared to the best-known col-lective rankers, and are 35% better than scoring snippets independent of each other.
 Categories and Subject Descriptors: H.3.3 [ Information Search and Retrieval ]: Retrieval models General Terms: Algorithms, Experimentation Keywords: Quantity search, Aggregating evidence from snippets, Learning to rank
Search engines are getting increasingly sophisticated in extracting and exploiting structured data from unstructured and semistructured Web pages. Most major search engines identify mentions of people, places, organizations, street ad-dresses, ZIP codes, dates, prices, disease names, and several other types of named entities mentioned on the Web pages they crawl.

Entity search has become a standard task in the research community as well. INEX ( http: // inex.is.informatik. uni-duisburg.de / ) features a track where the aim is to re-turn entities that satisfy a query. The TREC enterprise track ( http: // trec.nist.gov / pubs / trec15 / ) includes an expert search task, an important special case of entity search.
Approaches to entity and expert ranking include prob-abilistic generative models that capture relations between the query, documents, latent topics [9, 2], and lexical prox-imity between query words and candidate entities [16, 6, 2]. Answers to TREC-style factoid questions (TREC-QA, http: // trec.nist.gov / data / qamain.html ) are frequently named entities.

Corroboration of an entity, mentioned redundantly across multiple sites, often increases ranking accuracy and robust-ness [7]. Syntactic variations ( X  X ashington X  vs.  X  X eorge Washington X ) may exist in candidate mentions, and each mention may have a score based on query, language, topic and proximity considerations.

Some researchers [15, 17] have devised type theories with rule systems to conflate syntactic and quantitative variations of candidate answers, aggregate evidence across these vari-ations, and perhaps explain them. Substantial handcrafting of type systems and conflation rules are required in this ap-proach. A recent probabilistic graphical model [12] gives a principled means for learning a model to collectively rank candidate entities. However, this technique does not readily apply to quantity search.
In this paper we focus on quantity search , an important special case of entity search. A quantity may be a unitless number or have an associated unit like length, mass, tem-perature, currency, etc. TREC-QA 2007, 2006, and 2005 have 360, 403 and 362 factoid queries, of which as many as 125, 177, and 116 queries seek quantities. As against  X  X pot queries X  seeking unique answers like date of birth, we are specifically interested in what we call quantity consensus queries (QCQs), where there is uncertainty about the an-swer quantity ( X  X riving time from Paris to Nice X  or  X  X attery life of Lenovo X300 X ). TREC-QA 2007, 2006, and 2005 have at least 61, 39 and 28 such queries. To learn a reasonable distribution over the uncertain quantity, the user may need to browse thousands of pages returned by a regular search engine. A QCQ system reduces this cognitive burden by zooming down from document to snippet to quantity level. QCQ engines can also support sites that offer comparison of prices and features related to products, services and travel.
In the information extraction, integration and warehous-ing literature, a curate-and-query approach is popular; it assumes the existence of entity and relationship extractors [1, 3] for limited domains, which populate (possibly proba-bilistic) relational databases [4, 20]. We argue that open-domain ad-hoc QCQs cannot leverage the curate-and-query strategy, because the queries are too diverse and the sources are too unstructured for a priori schema design or informa-tion extraction. Our hypothesis is that some combination of string-oriented IR and structured aggregation is essential at query time.
We introduce QCQs (Section 2) and give novel algorithms that aggregate evidence in favor of candidate quantities and quantity intervals from snippets in a collective and corrobo-rative fashion, without attempting deep NLP on snippets.
For a given query, the i th snippet is a segment of text to-kens, centered around the mention of a quantity x i .A quan-tity is a number or a range accompanied by an (optional) unit of measurement. To the left and right of the central quantity mention are other context tokens of the snippet.
As baseline, we first consider (Section 4) algorithms that learn to rank items (documents or snippets) represented as feature vectors [10, 11, 22] (for a comprehensive list visit http: // research.microsoft.com / users / LETOR / ). An item (here, a snippet) is usually represented as a feature vector z i  X  R d in response to a query. In our case, z i will en-code the presence of query words in the snippet context, lexical proximity between query words and quantity x i , and rarity of matched query words in the corpus (IDF). Using manually-provided snippet relevance labels y i  X  X  1, these algorithms learn a model vector w  X  R d such that the score of the i th test snippet is w z i , and snippets are then sorted by decreasing score. We show that scoring using z i performs poorly, because z i by itself is a very noisy relevance signal.
We then evaluate a recent technique [21] that aggregates evidence across snippets i, j only if x i ,x j match exactly. This fails in the face of close but not identical quantities in dominant clusters. Next we adapt a graph Laplacian smoothing technique [12, 18] that balances between individ-ual snippet score evidence w z i and quantity proximity, say, | x  X  x j | . This formulation cannot ignore quantity proximity among irrelevant snippets, and gives only modest gains.
These trials and observations prompt us to propose (Sec-tion 5) new scoring mechanisms for entire intervals of values, instead of individual snippets, as was done in prior work. We show how to aggregate snippet scores into candi-date interval scores, and then pick the best intervals. This dramatically boosts accuracy.

In Section 6, we give another algorithm: it represents an interval I with novel feature vectors  X  z I , where some features are aggregated from snippet-level scores w z i . Note that indexes individual snippets and I represents an interval. We use max-margin methods [10] to learn a  X  X tacked X  model  X  During testing,  X  w z I is used to sort candidate intervals.
Our stacked ranker further enhances accuracy compared to interval scoring using w alone. It achieves over 20% rel-ative improvements in snippet-level MAP and NDCG com-pared to Laplacian smoothing, which in turn is 10 X 15% bet-ter than independent snippet ranking. We compare favor-ably with the best TREC-QA participants wrt precision-at-1. We also present a new way to evaluate sequences of quantity intervals, as against snippet lists.

Providing snippet labels y i is more tedious than providing ground truth x i values per query. In Section 7, we propose a very simple alternative to training w and  X  w using only ground truth x i , with a very small drop in quality.
Given the extreme diversity and noise in snippets, it is astonishing that clear and often correct consensus can be mined without the help of deep NLP, even for completely ad-hoc queries.
A QCQ has two main parts: a set of words or phrases, and a quantity type specifier. Some words or phrases may be marked compulsory with a prefixed  X + X . The latter may be unitless, if a count is desired, or have an unit. Some example QCQs are shown in Figure 1. As with ordinary Web queries, the onus of getting better snippets, through the use of  X + X  and phrases, lies with the user.

A third optional component of QCQs that gives additional control is a user-defined relative width parameter r , where 0  X  r 1, meaning that the user is looking for a quantity interval [ x, x ], such that x  X  (1+ r ) x , which has strong col-lective evidence from snippets. r is necessarily user-defined: a QCQ about Olympic record times has a fundamentally different expectation of precision compared to a QCQ about the distance between the Sun and Pluto. Only the user can provide that domain knowledge. In practice, a large number of QCQs run well with a default setting like  X  r =0 . 05 X . In any case, r is an upper bound on the relative width, and our system will tighten the interval if it can.
A snippet is a suitably large window of tokens around a candidate quantity which matches the unit specified in the QCQ. A quantity scanner (Section 3.1) identifies token segments that express quantities. The quantity, including unit, is called x i for the i th snippet for a given query. The surrounding text is turned into a suitable feature vector rep-resentation z i  X  R d .( z i depends also on the query.)
The design of z i must consider the proximity between the central quantity mention to snippet tokens that match query tokens, and is described in detail in Section 3.2. Any snip-pet that has one or more token matches with the query is potentially a relevant snippet, and its quantity a candidate quantity . Some sample relevant and irrelevant snippets for the above QCQs are shown in Figure 1. The snippets make clear the great variety of contexts in which plausible quan-tities appear close to significant query words.
As is clear from the examples, QCQs are characterized by an absence of an absolute or single truth. Our first impulse was to model the quantity of interest as a random variable, and build a system to return a distribution over it. But the event space is too complex: it involves natural language usage and extraction accuracy, among other uncertainties. We therefore avoid generative models for quantities, and ex-plore discriminative, collective ranking techniques for snip-pets. Informally, a consensus interval is a tight range [ of quantities that enjoys strong collective support from high-scoring snippets. We will give more precise proposals in Sec-tions 5 and 6. There, we will see that this simple notion of consensus performs very well.

To be fair, consensus is not the only form of useful ag-gregation; in some cases, it may be limiting or misleading. E.g., plutonium has multiple isotopes with diverse half lives, and a name may refer to many people with diverse birth years. Our QCQ system performs reasonably despite such ambiguity, because it reports (snippets from) not one but a number of top-scoring x -intervals. Time-variant quanti-ties offer another challenge. E.g., the QCQ +"bill gates", assets, worth; USD may give an outdated answer, depend-ing on Web coverage. A complete solution would require  X  X arbon dating X  each snippet, which appears even more chal-lenging than reliable timestamping of whole Web pages. The causes of multi-valued answers have been analyzed in some detail [15, 17].
In this section, we give an overview of our QCQ system, sketched in Figure 2. We will first describe the modules for annotating x i (Section 3.1) and turning snippet text into z (Section 3.2). Then we will describe how we collected queries and ground truth y i for candidate snippets..
A quantity scanner annotates character spans that are likely to be quantity mentions, which come in diverse forms. Some have unit prefixes, like currency symbols. Some have unit suffixes, like scientific measures. Some have exponent modifiers, like  X 10 million liters X  or  X  e 50 million X . Units are expressed diversely, e.g.,  X $ X  vs. USD,  X  X  X  vs. meter vs. metre. Even the numerals are written in diverse styles. Scientific quantities may be written without commas, commas after every third digit, or at irregular spacing, as in  X  X s 1,20,000 X . There may be spurious spaces before or after commas. Pe-riods may end sentences or be decimal points. Very large or small quantities may be written in mantissa-exponent form. Small numerals like 1, 2, 30 may be written as words. 1889 might be a unitless count or a year.  X $ X  may indicate different currencies. x i may also be a range, e.g., 10 X 20 feet .
We used the rule-based JAPE engine, which is part of the well-known GATE NLP package ( http: // gate.ac.uk / ). We compiled about 150 rules covering mass, mileage, power, speed, density, volume, area, money, time duration, time epoch, temperature length and so on. Augmenting our rule base to capture more types of quantities should be straight-forward. Manual spot checks on our annotator led to esti-mates of precision, recall and F1 as 0.92, 0.97, 0.95. Luckily, ranking intervals using consensus is robust to this small rate of scanner glitches.

Unit normalization: In the example QCQs above, each query has an associated specific unit (unless the answer is a count). In a deployed system, more generic units should be allowed, such as length in place of mile or km ,or time interval in place of hour or year . This would also assist collecting consensus across candidate quantities expressed in different units. Our prototype does not handle this issue, except identifying different standard forms of a unit (e.g. foot, feet, ft), but it can be added on easily.
We defined two families of features on (the query and) snippet text: first, standard vector-space ranking features [14, 13], and second, features that encode lexical proximity between query word matches and quantity tokens [16, 6, 2]. Each snippet was characterized by the tokens in five fields F : snippet, a window of 10 sentences above and below the snippet, the text of the page from where the snippet is origi-nated, the HTML title of the page, and the URL of the page. For each of the five fields F , three features were added to feature vector z i : TFSum: t  X  q  X  F TF( t, F ) IDFSum: t  X  q  X  F IDF( t ) TFIDFSum: t  X  q  X  F TF( t, F )IDF( t ) TF( t, F ) is the term frequency of t in F and IDF( t )isthe standard IDF of t with respect to a reference corpus (union of all documents over all queries). In addition, we used:
Guided by earlier work on locality or proximity based ranking [8, 6, 16, 2], we defined the proximity between the mention of quantity x i and a query token match t in its vicinity as the reciprocal of the number of tokens between the mention of x i and t (zero if no t exists).

Queries have a variable number of tokens. Therefore we define four proximity features aggregated over query tokens: The weights in w corresponding to these proximity features were among the highest when w was learnt using Rank-SVM [10]. To keep our system robust and scalable, we avoided deeper NLP techniques like learning to spot rela-tions from dependency parse trees.

Altogether, we used 21 features: 4 proximity, 5  X  3 simi-larity features and 2 other features.
We collected 162 QCQs from diverse sources. Each QCQ q was collected along with ground truth quantity set X q . Most X q s contained multiple values or ranges. Unless noted oth-erwise, we report performance on the union of these QCQs. Infobox: We created 40 QCQs by sampling Wikipedia In-TREC-QA: We chose TREC-QA queries that had non-Misc.: 9 queries were contributed by W&amp;M [21]. 36 QCQs Growing our QCQ set is limited only by snippet-labeling effort (described next).
We used Web search APIs to collect snippets. Unlike QA-oriented text indices, major Web search APIs do not allow us to ask for documents containing, say, a distance in feet within 20 tokens of the word elephant . This necessitated a two-step filtering approach. In the first step, we sent words, phrases and unit names in the QCQ to the engine. Re-sponse URLs were fetched, tokenized, and quantities anno-tated. Quantities that matched the QCQ unit, and were within one sentence (or a maximum token window) from a query word were retained, with their snippet context. We retained a total of about 15,000 snippets over 162 queries. We have made this data available in the public domain 1 .
For training, a selection of 100 snippets per QCQ were presented, using a browser-based GUI, for manual labeling of y i  X  X  1, the relevance of snippet i . Six volunteers, in-cluding the authors, annotated the snippets. There were (infrequent) inconsistencies between the contributed answer quantities and y i labels. I.e., snippets with quantities not in the ground truth ranges were sometimes marked relevant, mostly because the Web has a more up-to-date ground truth. We did not attempt to make these consistent, insisting that a robust algorithm must take this in stride.
QCQ systems may return a ranked list of snippets, with the quantities highlighted (Figure 1). The advantage is that the user can glance over and judge the snippets directly. Tra-ditional criteria [13], such as Mean Average Precision (MAP) or Normalized Discounted Cumulative Gain (NDCG) can then be used directly. (Mean Reciprocal Rank or MRR is not appropriate for QCQs because it does not give credit for comprehensive coverage of consensus values.)
Alternatively, to display many promising quantities within scarce real estate, QCQ systems may report a list of
Details at http: // www.cse.iitb.ac.in / ~soumen / doc / QCQ intervals, each subject to the user-provided relative width constraint. Evidence snippets can be shown if an interval is clicked. Evaluating a list of intervals, or comparing a system that ranks snippets with one that ranks intervals, are new challenges. We will discuss these in Sections 5 and 6. We describe existing approaches that can be adapted for QCQs, culminating in a comparison shown in Figure 5.
The minimal baseline (that any useful QCQ system must beat) is to send QCQ words/phrases to a search engine, get the top snippets, scan them for qualifying quantities with proper units, and list them if they appear within a stipulated distance of at least one query token. A listed quantity x judged correct if it matches (or is contained by) a ground truth quantity (or interval).
 Such snippet-level evaluation gives very poor MAP and NDCG (below 0.15), partly because search engines have no mechanism to promote to top ranks those snippets that con-tain quantities of specified types and query words. We can be generous and give credit for unsupported but correct quantities anywhere on the pages (not just reported snip-pets), which is what we show in Figure 5. We use two major engines (called Web1 and Web2). Our algorithms are better at promoting relevant snippets to top positions, comfortably beating the generous evaluation of Web search engines.
We tried several techniques for learning [10, 22, 13] a snippet-level w given snippets ( z i ,y i )( x i is ignored here), with the score w z i used for ranking snippets. We found standard pairwise RankSVM [10] (formulation given below) as good as direct optimizers of MAP [22] or NDCG [5].  X  i s.t. y i =1 ,  X  j s.t. y j =  X  1 ,  X  ij  X  0 w z lated and C balances between violations and | w | . Figure 5 compares the accuracy of various baseline algorithms. (For all RankSVM -style learning algorithms in this paper, five-fold cross validation was used and the best value of C in (1) was picked from among { 10  X  3 , 10  X  2 ,. 1 , 1 , 10 } .) Figure 5 shows that RankSVM is generally better than Web1 and Web2. As for MAP, remember that Web1 and Web2 are given massive advantage while RankSVM snip-pets are evaluated stringently. However, a closer look at RankSVM (next) provides key actionable insight.
RankSVM considers only w z i scores, but how do these relate to corresponding x i s? Figure 3 plots scatters of ( y -axis) against x i ( x -axis) for three representative queries. For visual uniformity across queries, both axes have been scaled to [0 , 1]. Snippets are also called  X  X oints X . If opti-mization (1) were perfect, all good points would lie higher along the y -axis than all bad points. This is rarely the case: although z i was designed with considerable care, de-cent separation between relevant and irrelevant snippets is never achieved on the basis of w z i alone.

But the scatter plots also show a valuable clue: relevant points often cluster in vertical bands . From Figure 3 and the simplified sketch in Figure 4, it seems that for each query, one of few upward-open rectangular strips capture most good snippets with very few bad snippets.

It is natural to ask at this point why we cannot use deci-sion trees (which naturally find rectangle discriminators) or SVMs with nonlinear kernels. The reason is that the width and location of the semi-open rectangles (equivalently, pa-rameters of non-linear kernels) change from query to query. Parameters learnt by decision trees or nonlinear SVMs will not generalize across diverse queries. We need a more non-parametric approach.
A first approach to integrating x from snippets is to take weighted majority votes, similar to exploiting redundancy in QA. W&amp;M accumulates a score for each distinct x from snippets where x occurs. The snippet score is determined by the following considerations: Score aggregation happens only on exact equality of x . Fig-ure 5 shows that W&amp;M is consistently worse than Rank-SVM . Often, relevant snippets are found at quite poor ranks, because the whole-page ranking imposed by Web1 and Web2 Figure 4: Our proposed  X  X ypothesis class X  of semi-open rectangles. are often not suited for QCQs. Recall again that Web1 and Web2 X  X  accuracy may be substantial overestimates. A second way to combine x i and w z i is via a graph Laplacian approach [18]. Each snippet is made a node in a graph G =( V, E ). Each node/snippet has an associated feature vector z i as before, inducing a (noisy) local score w z i . Meanwhile, the x i values at nodes are used to define edges weights R ( i, j ), inversely related to | x i  X  x j
The formulation seeks a model w while assessing a loss ( f i  X  w z i ) scores, and a roughness loss { i,j } X  E R ( i, j )( f i  X  f f  X  R n  X  1 is the column vector of final scores. Finally, there is the usual training loss if the final score of a good snippet is less than the final score of a bad snippet. Training involves solving a quadratic program with linear constraints [18].
The design of edge weights R critically determines the algorithm, but there is no generic guideline. We tried the following reasonable definitions: x = x j equality: Following W&amp;M X  X  majority semantics, we | x i  X  x j | distance: R ( i, j ) = max 0 , 1  X  | x i  X  x j | decay: R ( i, j ) is defined as exp(  X  s x i  X  x Snippet cosine: Following the pseudorelevance feedback Figure 5 summarizes accuracies of all approaches discussed thus far. Laplacian smoothing with the  X  X ecay X  option gives modest gains over Web1, Web2, RankSVM , and W&amp;M. The gains are limited by two factors. First, the Laplacian formulation assesses the roughness penalty on all edges, even those between snippets putatively labeled irrelevant. For Figure 5: Initial results (bold =  X  max in column). 1: inputs: snippet set S with x i and w z i values, interval 2: sort snippets S in increasing x i order 3: for i =1 ,...,n do 4: for j = i,...,n do 5: if x j &lt; (1 + r ) x i then 6: let I =[ x i ,x j ] 7: merit  X  GetIntervalMerit ( S, I ) 8: maintain intervals with top-k merit values 9: for surviving intervals I in decreasing merit order do 10: present snippets in I in decreasing w z i order QCQ, we should favor smoothness of f i only among relevant snippets. Second, there is no ready way to tune the width parameter s reliably across diverse queries and associated quantities. Our algorithms get around these issues.
Instead of scoring and ranking snippets, we shift our fo-cus to quickly enumerating and scoring rectangular regions as shown in Figure 4. We begin with searching for the posi-tion and width of a promising rectangle on the x -axis, i.e., searching over intervals I =[ x, x ], with x  X  (1 + r ) x specified in Section 2.1. We will overload I to also mean a set of snippets. A snippet s i =( x i ,z i ) is said to belong to if x i  X  I . In case a snippet mentions a range (such as 10 X 20 feet ), the snippet belongs to I if the range is contained in
For a query q with n q snippets, there are at most n q +1 functionally distinct (in terms of the snippets they contain) intervals on the x axis. Some of these intervals I =( x, x are too wide ( x &gt; (1 + r ) x ) and can be discarded. Usually r 1, so the enumeration of valid candidates I  X  X  r can be done efficiently using a left-to-right sweep that takes close to linear time in practice. For simplicity Figure 6 shows a naive O ( n 2 q ) enumeration of intervals.

Figure 4 suggests that we should also search over all pos-sible bottom boundaries of I . In practice, this makes negli-gible difference. Our results in Section 7 may explain why this is the case.
As we enumerate over intervals I , we need to use the sig-nal from w z i for i  X  I and potentially i  X  I , to evaluate GetIntervalMerit ( S, I ). If there is any useful signal in we should prefer intervals I such that points in I have gener-ally larger values of w z i than points not in I . Accordingly, we provide three choices of merit (to maximize over I ): Observe that terms in (Diff) can be positive or negative; favorable and unfavorable score pairs can cancel out. This is prevented in (Hinge). In machine learning one minimizes hinge loss rather than maximize hinge gain , but in QCQ, the former leads to tiny proposed relevant clusters that are often incorrect.
We compare three algorithms: the best two approaches from Figure 5 ( RankSVM and Laplacian Decay) and in-terval merit enumeration (for which the snippet-level model w was trained using RankSVM ). For MAP (Figure 7), we vary interval width tolerance r (shown as a percentage). For NDCG (Figure 8) we hold r = 8% and report NDCG at ranks 1 ... 10. Note that r = 0 means an interval of width zero, but this can contain multiple snippets if they mention the exact same quantity. RankSVM and Laplacian Decay do not depend on r .
 ation with (Diff) beats all other approaches by a wide mar-gin. Interval enumeration with (Hinge) is second, still beat-ing all others.
 Effect of r . (Diff) and (Hinge) show significant boost in ac-curacy as r is increased beyond 0. (Diff) is stable between r = 3% and 9%. This is direct evidence that robust aggre-gation over x i values is critical to success. (Diff) better than (Hinge) . Occasionally, avoiding deep NLP leads to systematic pollution from irrelevant but dense inter-vals. E.g., for the QCQ +giraffe +height: foot , an ir-relevant cluster (as per predominant human interpretation) develops around 6 feet thanks to snippets like this:  X  X ewborn giraffe calves begin their lives by falling from a height of 6 feet  X ,  X  X  young giraffe has to survive a fall of six feet  X , o r  X  X  giraffe  X  X  legs alone are taller than many humans X  X bout 6 feet  X . These intervals have a lower average w z i and (Diff) reveals this better than (Hinge).
In the previous section we proposed a way to score in-tervals, based on aggregating w z i scores of snippets inside and outside the intervals. In this section we design a learner that directly learns to rank intervals instead of individual snippets.
 As in Section 5, we will use relative tolerance r to define I , the set of candidate intervals satisfying r . We already know that |I r | = O ( n 2 q ).

Every candidate interval I  X  X  r will be represented by an interval feature vector  X  z I . The interval ranker will learn a corresponding scoring model vector  X  w .
Unlike in snippet-level RankSVM ,weareatlibertyto define collective features of intervals, rather than just aggre-gate { z i : x i  X  I } , in simple ways as in Section 5. Specifi-cally, a simple average of feature vectors may fail to capture certain significant clustering in the z i space. There may be much stronger clues to guess how good an interval is.
For example, an interval is good if most of the points in the interval are relevant to the query, if the interval has high merit (as defined in Section 5.1) and most of the points in the interval have consensus on a quantity or there are relative few distinct quantities. We capture these clues by design-ing a set of additional features that are collective across an interval. We call them interval features : 1. Whether all snippets in I contain some query word 2. Whether all snippets in I contain the minimum IDF 3. Whether all snippets in I contain the maximum IDF 4. Number of distinct words found in snippets in I 5. Number of words that occur in all snippets in I 6. One minus the number of distinct quantities mentioned 7. Number of snippets in I , divided by n q 8. Three features corresponding to the three merit func-Apart from the above interval features we also append to  X  the vector average of the feature vectors z i with i  X  I .
Recall that we want to learn to compare intervals, but our ground truth y i is collected over snippets. The next piece is to define a relevance score over each interval I  X  X  We assign a relevance score to an interval I based on the fraction of relevant snippets in I . I.e., if I has n + I snippets and n I snippets overall, then its relevance score is defined as n + I /n I . Thus, snippet-level y i labels determine the relevance score of intervals.

For two intervals I and I , if the relevance score of I is larger than that of I , we assert a pairwise preference I I between the intervals. These interval comparisons will re-place individual snippet comparisons in (1). (Other algo-rithms [22, 5, 13] may be used in place of RankSVM .)
Initial experience with the algorithm shown in Figure 9 suggested that we were generating too many preference pair constraints based on insignificant interval relevance differ-ences. We improved both training speed and accuracy by discretizing interval relevance to an ordinal scale of 0 X 10. In other words, the relevance of an interval was defined as 10 n + I /n I . We tried between 5 and 10 ordinal levels and the accuracy was not very sensitive to the number of levels.
Suppose the interval ranker learns model  X  w . Given a test query, I r is enumerated as before. Then the intervals in 1: inputs: snippets s i with labels y i , tolerance r 2: for each interval I  X  X  r do 3: compute the relevance of I using snippet labels y i 4: compute feature vector  X  z I 5: generate interval pair preferences I I 6: set up a RankSVM problem involving intervals: 7: train using RankSVM to get  X  w 8: return  X  w I r are ranked by decreasing  X  w  X  z I . If a snippet list must be provided, we run down the intervals in decreasing  X  w order, and order snippets within each interval using snippet score w z i .
We compare the best algorithm from Section 5, viz., (Diff) merit score for intervals, against the (IntervalRank) algo-rithm presented in this section.
 Figure 10 compares MAP obtained by IntervalRank vs. Diff as width tolerance r is varied. IntervalRank is bet-ter, reaching a MAP of 0.511 against 0.421 by Laplacian smoothing and 0.369 by RankSVM . The story with NDCG (Figure 11) is almost similar, the gains increasing with rank. IntervalRank achieves NDCG@10 of 0.513 against 0.435 by Laplacian smoothing and 0.406 by RankSVM .

We did an ablation study by removing one feature from all  X  z at a time. The maximum MAP reduction was for feature #6. This shows that aggregating evidence from snippets supporting intervals is critical.
Direct comparison is impossible: the corpora are different. Figure 10: Comparison of Merit-Diff and interval ranking algorithms (MAP). Figure 11: Comparison of Merit-Diff and interval ranking algorithms (NDCG). IntervalRank recall 0.521 0.581 0.637 0.647 0.685 Lapl. decay recall 0.510 0.569 0.614 0.634 0.655 IntervalRank prec 0.443 0.432 0.416 0.388 0.371 In terms of precision-at-1, for our sample of TREC-QA 2007, we are second-best. For our sample of TREC-QA 2004, we are at rank 5 out of 63 teams. While not very meaning-ful for QCQ, this shows that our system is competitive wrt precision-at-1.
Our algorithms rank intervals, but to evaluate them wrt snippet-level y i ground truth, we iterated through intervals by decreasing  X  w z I , listing snippets i  X  I by decreasing w z i . Snippet-level NDCG or MAP is suitable when users inspect snippet lists [19]. If a QCQ system presents a list of intervals, the user may inspect at most a small number of evidence snippets per interval, so snippet-level MAP or NDCG may not accurately reflect cognitive burden. We pro-pose recall and precision criteria that recognize an interval, not a snippet, as a unit of attention. Suppose there are n + snippets marked relevant for a QCQ, and our algorithm A outputs I 1 ,...,I m , where I j contains n j snippets, of which k are good. The interval-oriented precision of A at interval rank j is defined as ( k 1 +  X  X  X  + k j ) / ( n 1 +  X  X  X  + n interval-oriented recall is defined as ( k 1 +  X  X  X  + k j ) compare with a snippet-listing algorithm A we simply line up the first n 1 +  X  X  X  + n j snippets, assume that A reported in-tervals I 1 ,...,I m , and evaluate similar to I 1 ,...,I that IntervalRank cannot cheat at recall using arbitrarily large r , because precision will plummet. Results in Fig-ure 12 show that collective interval scoring and presentation can increase both recall and precision, particularly for the top few intervals. Laplacian decay is between RankSVM and IntervalRank.
We have assumed throughout that the label y i is known for each training snippet ( X  X omplete X  supervision). However, it is much more natural and efficient to train a QCQ system based on ground truth quantity set X q and z i . Another advantage of this form of  X  X artial X  training is that we can semi-automatically glean training data from social media, such as Wikipedia Infoboxes.

Suppose we sloppily impute y i values using X q : any snip-pet with x i  X  X q , or contained in a range in X q , is consid-ered relevant. These imputed  X  y i s may conflict with  X  X rue X  y s (if available). How drastically might w,  X  w deteriorate because of using  X  y i s to train our system in place of
We sampled queries leading to 14,562 y i -labeled snippets.  X  y i gave only 571 false positives and 395 false negatives. These modest fractions may explain why modeling the bot-tom boundary of rectangles in Figure 4 did not make a sig-nificant difference. Figure 13 shows the effect of imputed training on test MAP score. The drop in test accuracy is very mild. Our algorithm continues to beat all baselines.
Figure 13: Effect of imputation on test MAP.
We introduced QCQs, and proposed algorithms for return-ing consensus intervals in response to QCQs. We showed that corroborative ranking of intervals is more accurate than ranking snippets independently. We next hope to improve our system by replacing search APIs with our own quantity index on Web-scale corpora.
