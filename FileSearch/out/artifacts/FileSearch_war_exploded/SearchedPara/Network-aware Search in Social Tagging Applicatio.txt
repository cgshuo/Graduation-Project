 We consider in this paper top-k query answering in social applica-tions, with a focus on social tagging . This problem requires a sig-nificant departure from socially agnostic techniques. In a network-aware context, one can (and should) exploit the social links, which can indicate how users relate to the seeker and how much weight their tagging actions should have in the result build-up. We propose algorithms that have the potential to scale to current applications.
While the problem has already been considered in previous lit-erature, this was done either under strong simplifying assumptions or under choices that cannot scale to even moderate-size real-world applications. We first revisit a key aspect of the problem, which is accessing the closest or most relevant users for a given seeker. We describe how this can be done on the fly (without any pre-computations) for several possible choices  X  arguably the most natural ones  X  of proximity computation in a user network. Based on this, our top-k algorithm is sound and complete, addressing the applicability issues of the existing ones. Moreover, it performs sig-nificantly better in general and is instance optimal in the case when the search relies exclusively on the social weight of tagging actions.
To further address the efficiency needs of online applications, for which the exact search, albeit optimal, may still be expensive, we then consider approximate algorithms. Specifically, these rely on concise statistics about the social network or on approximate shortest-paths computations. Extensive experiments on real-world data from Twitter show that our techniques can drastically improve response time, without sacrificing precision.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search Process social applications; social search; threshold algorithms  X  Work performed while the authors were affiliated with Institut Mines-T X l X com -T X l X com ParisTech.

Unprecedented volumes of data are now at everyone X  X  fingertips on the Web. The ability to query them effectively, by fast retrieval and ranking algorithms, has largely contributed to the rapid growth of the Web, making it irreplaceable in our every day life.
A new dynamics to this development has been recently brought by the social Web , applications that are centered around users, their relationships and their data. Indeed, user-generated content is be-coming a significant and highly qualitative portion of the Web. To illustrate, one of the two most visited Web sites today is a social one. This calls for adapted, efficient retrieval techniques, which can go beyond a classic Web search paradigm where data is decoupled from the users querying it. An important class of social applications are the social tagging ones, with popular examples including Delicious, Flickr, or Twitter. Their general setting is the following: 1. users form a social network , which may reflect proximity, 2. items from a public pool of items (tweets, documents, URLs) 3. users search for items having certain tags.
 Social tagging, and social applications in general, can offer an en-tirely new perspective to how one searches and accesses information. The main reason for this is that users can (and often do) play a role at both ends of the information flow, as producers and also as seekers of information. Consequently, finding the most relevant items that are tagged by some keywords should be done in a network-aware manner. In particular, items that are tagged by users who are  X  X loser X  to the seeker  X  where the term closer depends on model assumptions that will be clarified shortly  X  should be given more weight than items that are tagged by more distant users.

We consider in this paper the problem of top-k retrieval in social tagging systems, with a focus on efficiency, targeting techniques that have the potential to scale to current applications on the Web, in an online context where the network, the tagging data and even the seekers X  search ingredients can change at any moment. In this context, a key sub-problem for top-k retrieval that we need to address is computing scores of top-k candidates by iterating not only through the most relevant items with respect to the query, but also (or mostly) by looking at the closest users and their tagged items.

We associate with the notion of social network a general interpre-tation: a user graph whose edges are labeled by social scores , which give a measure of proximity or similarity between users. While we focus on social tagging in this paper, we believe this represents a good abstraction for many types of social applications, to which our techniques could apply.

E XAMPLE 1. C onsider the social tagging configuration of Fig-ure 1 (left). Users have associated lists of tagged documents and Figure 1: Social tagging scenario and its social network (left); some paths from a seeker towards a relevant item (right). they are interconnected by social links. Each link is labeled by its (social) score, assumed to be in the (0 , 1] interval. Let us consider user Alice in the role of the seeker. The user graph is not a complete one, as the figure shows, and only two users have an explicit social score with respect to Alice. For the remaining ones, Danny, . . . , Jim, only an implicit social score could be computed from the existing links if a precise measure of their relevance with respect to Alice X  X  queries is necessary in the top-k retrieval.

Let us assume that Alice looks for the top two documents that are tagged with news and site . Looking at Alice X  X  immediate neighbors and their respective documents, intuitively, D 3 should have a higher score than D 4 , since the former is tagged by a more relevant user ( Bob , having the maximal social score relative to Alice). If we expand the search to the entire graph, e.g., via a shortest paths-like interpretation, the score of D 4 may however benefit from the fact that other users, such as Eve or even Holly , also tagged it with news or site . Furthermore, documents such as D 2 and D 1 may also be relevant for the top-2 result, even though they were tagged only by users who are indirectly linked to Alice.

Figure 1 (right) gives a different perspective on our scenario, illustrating how Alice can reach one of the relevant documents, D 1 , by following three paths in the social network. Intuitively, an aggregation of such paths from a seeker towards data items will be used in the scoring model. Under certain assumptions to be clarified shortly, the top-2 documents for Alice X  X  query will be, in descending score order, D 4 and D 2 . The rest of the paper will present the underlying model and algorithms that allow to build this answer. Main related work. Top-k retrieval algorithms, such as the Thresh-old Algorithm (TA) and the No Random Access algorithm (NRA) [12], rely on precomputed inverted-index lists with exact scores for each query term (in our setting, a term is a tag). Revisiting the setting in Figure 1, we would have two per-tag inverted lists IL ( news ) = { D 4 : 7 ,D 2 : 2 ,D 1 : 2 ,D 3 : 1 ,D 6 : 1 ,D 5 : 1 } and IL ( site ) = { D 2 : 5 ,D 4 : 2 ,D 3 : 1 ,D 6 : 1 ,D 1 : 1 ,D 5 : 1 } , which give the number of times a document has been tagged with the given tag.

When user proximity is an additional ingredient in the top-k retrieval process, a direct network-aware adaptation of the threshold algorithm and variants would need precomputed inverted list indices for each user-tag pair. For instance, if we interpret explicit links in the user graph as friendship, ignoring the link scores, and only tagging by direct friends matters, Alice X  X  lists would be IL
Alice ( news ) = { D 4 : 1 ,D 6 : 1 } , IL Alice ( site ) = { D 3 : 1 ,D 6 : 1 } . Other 18 such lists would be required; clearly, this would have prohibitive space and computing costs in real settings.
Amer-Yahia et al. [1] is the first to address this issue, considering the problem of network-aware search in social tagging sites, though in a simplified setting. The authors consider an extension to classic top-k retrieval in which user proximity is seen as a binary function (0-1 proximity): only a subset of the users in the network are selected and can influence the top-k result. This introduces two strong simplifying restrictions: (i) only documents tagged by selected users should be relevant in the search, and (ii) all the users thus selected are equally important. The base solution of [1] is to keep for each tag-item pair, instead of the detailed lists per user-tag pair, only an upper-bound on the number of taggers: the maximal number of taggers from any user X  X  neighborhood. For example, the upper-bound for ( news,D 4) would be 2 , since for any user there are at most two neighbors who tagged D 4 with news . A more refined version, which trades space for efficiency, keeps such upper-bounds for clusters of users, instead of the network as a whole.
Only in Schenkel et al. [19], the network-aware retrieval problem for social tagging is considered under a general interpretation, the one we also adopt in this paper. It considers that even users who are only indirectly connected to the seeker can be relevant for the top-k result. Their C ONTEXT M ERGE algorithm follows the intuition that the users closest to the seeker will contribute more to the score of an item, thus maximizing the chance that the item will remain in the final top-k . The authors describe a hybrid approach in which, at each step, the algorithm chooses either to look at the documents tagged by the closest unseen user or at the tag-document inverted lists (a seeker agnostic choice). In order to obtain the next (unseen) closest user at any given step, the algorithm precomputes in advance the proximity value for all possible pairs of users. These values are then stored in ranked lists (one list per user), and a simple pointer increment allows to obtain the next relevant user.

E XAMPLE 2. In Fig. 1, for seeker Alice, the list of users ranked by proximity would be { Bob : 0 . 9 ,Danny : 0 . 81 ,Charlie : 0 . 6 , Frank : 0 . 4 ,Eve : 0 . 3 ,George : 0 . 2 ,Holly : 0 . 1 ,Ida : 0 . 1 , Jim : 0 . 05 } , with proximity between two users built as the maximal product of scores over paths linking them (formalized in Section 2.1). For example, the scores of Frank and Eve correspond to paths of Fig. 1 (right), and will contribute to D 1  X  X  score w.r.t. the tag news .
Our contributions. We present an algorithm for top-k answering in social tagging, which has the potential to scale to current online applications, where network changes and tagging are frequent. For it, we first address a key aspect: accessing efficiently the closest users for a given seeker. We describe how this can be done on the fly (without pre-computations) for a large family of functions for prox-imity computation in a social network, including the most natural ones (as the one used in [19]).

Based on this, our top-k algorithm SNSis sound and complete, and, when the search relies exclusively on the social weight of tag-ging actions, it is instance optimal  X  i.e., it visits a minimal number of users  X  in a large and important class of algorithms. Extensive experiments on real-world data from Twitter show that SNS per-forms significantly better than state-of-the-art techniques, up to two times faster (see Sec. 5). Going further, since in real-world online applications the joint exploration of the social and textual space may remain costly, even by optimal algorithms, we consider directions for approximate results. More specifically, these are motivated by the fact that estimates for termination conditions may be too loose in practice and that exact shortest paths-like computations may be too expensive in a large network. Our approaches present the ad-vantages of negligible memory consumption  X  relying on concise statistics and pre-computed information about the social network and proximity values (via landmarks )  X  and reduced computation overhead. Moreover, these statistics can be maintained up to date with little effort, even when the social network is built based on tagging history. Experiments show that approximate techniques can drastically improve the response time of the exact approach, even by an order of magnitude, with reduced impact on precision.
The main focus of our work is on the social aspects of top-k retrieval in tagging applications, and our techniques are designed to perform best in settings where tagging actions are mostly (if not exclusively) viewed through the lens of social relevance.
Other related work. The topic of search in a social setting has received increased attention lately. Studies and models of person-alization of social tagging sites can be found in [20, 13, 11, 21]. Other studies have found that including social knowledge in scoring models can improve search and recommendation algorithms. In [8], personalization based on a similarity network is shown to outper-form other personalization approaches and the non-personalized social search. A study on a last.fm dataset in [16] has found that incorporating social knowledge in a graph model system improves the retrieval recall of music track recommendation algorithms. An architecture for social data management is given in [2, 3], with a framework for information discovery and presentation in social content sites. Another approach to rank resources in social tagging environments is CubeLSI [6], which uses a vector space model and extends Latent Semantic Indexing to include taggers in the feature space of resources, in order to better match queries to documents.
Several approaches for modifying the classic PageRank algo-rithm for bookmarking applications have been proposed. Algorithm FolkRank [14] proposes a ranking model in social bookmarking sites, for recommendation and search, based on an adaptation of PageRank over the tripartite graph of users, tags and resources. It follows the intuition that a resource that is tagged with important tags by important users becomes important itself and, symmetrically, for tags and users. An alternative approach to social-aware search, using personalized PageRank, was presented in [5]. There, the same tripartite model of annotators, resources and annotations is used to compute measures of similarities between resources and queries, and to capture the social popularity of resources. However, none of these approaches incorporate user-to-user relationships in their ranking model. In contrast, the social network is an integral part of the scoring model in our setting, if not the decisive one, while it can have various semantics (e.g., tagging similarity or trust).
The scoring model used in [19] is revisited in [22]. There, a textual relevance and a social influence score are combined in the overall scoring of items, the latter being computed as the inverse of the shortest path between the seeker and the document publishers. This model is also used in top-k retrieval of spatial web objects [7], where a prestige-based relevance score is computed by combining the overall relevance of an object with its spatial distance.
Person search and shortest paths related work gives another facet of  X  X ocial search X : the search of highly relevant persons for a given seeker and keyword query. Generally, the principle of approaches used in this type of application is to generate the k most relevant users, filtering them by the query keywords. These most relevant users are computed based on their shortest-path distances to the seeker, either in a centralized setting, as in [17], or in a distributed one, as in [4]. A characterization of proximity functions, including shortest paths, has also been recently given in [9]. These approaches are not directly applicable to our problem setting, as we have no prior knowledge on the identity and number of users who are highly relevant for the query and the top-k items to be returned. However, we describe here an adaptation to our setting of [17] X  X  landmark-based approach for shortest paths computations, in order to obtain faster approximate results.

Outline. We formalize in Sec. 2 the top-k retrieval problem in social tagging. We describe a key aspect of our approach, the on-the-fly computation of proximity in Sec. 2.1. We then describe our exact top-k algorithm, first in an exclusively social form, in Sec. 3, and show it is instance optimal (Sec. 3.1). The general algorithm is presented in Sec. 3.2. Approaches for improving efficiency by ap-proximation are given in Sec. 4. Experimental results are presented in Sec. 5. and we discuss future research directions in Sec. 6.
In short, our model relies on scores that are obtained, either fully or partially, by some aggregation of shortest paths (in the social space) from a seeker towards good items (as illustrated in Figure 1, right).

More precisely, we consider a social setting in which we have a set of items (could be text documents, tweets, URLs, photos, etc) I = { i 1 ,...,i m } , each tagged with one or more distinct tags from a dictionary T = { t 1 ,t 2 ,...,t l } by users from U = { u We assume that users form an undirected weighted graph G = ( U ,E, X  ) called the social network . In G , nodes represent users and the  X  function associates to each edge e = ( u 1 ,u 2 ) a value in (0 , 1] , called the proximity (or social) score between u 1 and u 2
Given a seeker user s , a keyword query Q = ( t 1 ,...,t r r distinct tags) and an integer value k , the top-k retrieval problem is to compute the (possibly ranked) list of the k items having the highest scores with respect to the seeker and query.

We assume the following tagging relation Tagged ( v,i,t ) , which says that a user v tagged the item i with tag t (a user can tag a given item with a given tag at most once).

We first model by score ( i | s,t ) , for a seeker s , an item i and one tag t , the score of i for the given seeker and tag. Generally, score ( i | s,t ) = h ( fr ( i | s,t )) , where fr ( i | s,t ) is the overall frequency of item i w.r.t s and t , and h is a positive monotone function (e.g., tf-idf or BM25; see Section 5).

Overall frequency fr ( i | s,t ) is defined as a combination of a network-dependent component and a document-dependent one:
The former component, tf ( t,i ) , is the term frequency of t in i , i.e., the number of times i was tagged with t . The latter component stands for social frequency , a measure that depends on the seeker.
If we consider that each user brings her own weight (proximity) to an item X  X  score, we can define the measure of social frequency as Then, given a query Q as a set of tags ( t 1 ,...,t r ) , the overall score of i for seeker s and query Q , score ( i | s,Q ) = g ( score ( i | s,t 1 ) ,...,score ( i | s,t is obtained using a monotone aggregate function g over the individ-ual scores for each tag (in our experiments, the aggregation function g is assumed to be a summation, giving P t
Extended proximity. The above scoring model takes into ac-count only the neighborhood of the seeker (the users directly con-nected to her). But this can be extended to deal also with users that are indirectly connected to the seeker, following a natural interpre-tation that user links (e.g., similarity or trust) are (at least to some extent) transitive. We denote by  X  + an extended proximity , which is
The kind of linear combination of Eq. (2.1) is often used when a local retrieval score and a global one are to be combined, e.g., in spatial search [7] or in social search [19]; any monotone combination of the two score components can be used instead. to be computable from  X  for any pair of users connected by a path in the network. Now,  X  + can replace  X  in the definition of social frequency we consider before (Eq. (2.2)), yielding an overall item scoring scheme that depends on the entire network instead of only the seeker X  X  vicinity. We discuss shortly possible alternatives for  X  + by means of aggregating  X  values along paths in the graph. In the rest of this paper, when we talk about proximity, we refer to the extended one. For a given seeker u , by her proximity vector we denote the list of users with non-zero proximity with respect to u , ordered in descending order of these proximity values.
We describe in this section a key aspect of our algorithm for top-k search, namely on-the-fly computation of proximity values with respect to a seeker s . The issue here is to facilitate at any given step the retrieval of the most relevant unseen user u in the network, along with her proximity value  X  + ( s,u ) . This user will have the potential to contribute the most to the partial scores of items that are still candidates for the top-k result, by Eq. (2.1) and (2.2).
We start by discussing possible candidates for  X  + , arguably the most natural ones, drawing inspiration from studies in the area of trust propagation for belief statements. We then give a wider characterization for the family of possible functions for proximity computation, to which these candidates belong.
 Candidate 1( f mul ). Experiments on trust propagation in the Epinions network (for computing a final belief in a statement) [18] or in P2P networks show that (i) multiplying the weights on a given path between u and v , and (ii) choosing the maximum value over all the possible paths, gives the best results (measured in terms of precision and recall) for predicting beliefs. We can integrate this into our scenario, by assuming that belief refers to tagging with a tag t . We thus aggregate the weights on a path p = ( u 1 (with a slight abuse of notation) as  X  + ( p ) = Q i  X  ( u
For seeker Alice in our running example, we gave in the previous section (Example 2) the proximity values and the ordering of the network under this candidate for  X  + .

Candidate 2( f min ). A possible drawback of Candidate 1 for proximity aggregation is that values may decrease quite rapidly. A  X  + function that avoids this could be obtained by replacing multipli-cation over a path with minimal, as  X  + ( p ) = min i {  X  ( u
Under this  X  + candidate, the values w.r.t. seeker Alice would be { Bob : 0 . 9 ,Danny : 0 . 9 ,Charlie : 0 . 6 ,Frank : 0 . 6 ,Eve : 0 . 5 ,George : 0 . 5 ,Holly : 0 . 5 ,Ida : 0 . 25 ,Jim : 0 . 25 } .
Candidate 3( f pow ). Another possible definition for  X  + on an aggregation that penalizes long paths, i.e., distant users, in a be seen as an exponential decay factor; the greater its value the more rapid the decrease of proximity. 2 Under this candidate, for  X  = 2 , non-zero rounded values w.r.t Alice are { Bob : 0 . 46 ,Charlie : 0 . 31 ,Danny : 0 . 21 ,Eve : 0 . 07 ,Frank : 0 . 052 ,George : 0 . 01 } .
The key common feature of the candidate functions previously discussed is that they are monotonically non-increasing over any path, when  X  draws values from the interval [0 , 1] :
P ROPERTY 1. For a social network G and a path p = { u 1 ,...,u in G , we have  X  + ( u 1 ,...,u l )  X   X  + ( u 1 ,...,u l  X  1
We then define  X  + for any pair of users ( s,u ) who are connected in the network by taking the maximal weight over all their connect-ing paths. More formally, we define  X  + ( s,u ) as This is similar to Katz measures for social proximity [15].
Note that when the first candidate (multiplication) is used, we ob-tain the same aggregation scheme as in [18], which is also employed in [19] in the context of top-k network aware search.

E XAMPLE 3. In our running example, if we use multiplication in Eq. (2.3), for the seeker Alice , for  X  = 0 (hence exclusively social relevance), by Eq.( 2.1) we obtain the following values for social frequency: SF Alice ( news ) = { D 4 : 2 . 6 ,D 2 : 1 . 01 ,D 1 : 0 . 7 ,D 6 : 0 . 6 ,D 3 : 0 . 1 ,D 5 : 0 . 05 } and SF Alice 1 . 11 ,D 2 : 1 . 1 ,D 3 : 0 . 9 ,D 6 : 0 . 6 ,D 1 : 0 . 05 ,D 5 : 0 . 05 } .
We argue next that to all aggregation definitions that satisfy Prop-erty 1 and apply Eq.(2.3), a greedy approach is applicable. This will allow us to browse the network of users on the fly, at query time, visiting them in the order of their proximity w.r.t the seeker.
More precisely, by generalizing Dijkstra X  X  shortest paths algo-rithm [10], we maintain a max-priority queue, denoted H , whose top element score top ( H ) will correspond at any moment to the most relevant unvisited user . A user is visited when her tagged items are taken into account for the top-k result, as described in the following sections (this can occur at most once). At each step advancing in the network, the top of the queue is extracted (vis-ited) and its unvisited neighbors (adjacent nodes) are added to the queue (if not already present) and are relaxed . Let  X  denote the aggregation function over a path (satisfying Property 1).
It can be shown by straightforward induction that this greedy ap-proach allows us to visit the nodes of the network in non-increasing order of their proximity with respect to the seeker, under any func-tion for proximity aggregation that satisfies Property 1.
As the main focus of this paper is on the social aspects of search in tagging systems, we detail first our top-k algorithm, SNS, for the special case when the parameter  X  is 0 . In this case, fr ( i | s,t ) is simplified as fr ( i | s,t ) = sf ( i | s,t ) .

For each user u and tag t , we assume a precomputed projection over the Tagged relation for them, giving the items tagged by u with t ; we call these the user lists . No particular order is assumed for the items appearing in a user list. We keep a list D of candidate items, sorted in descending order by their minimal possible scores (to be defined shortly). An item becomes candidate when it is first met in a Tagged triple.

As usual, we assume that, for each tag t , we have an inverted list IL ( t ) giving the items i tagged by it, along with their term frequencies tf ( t,i ) 3 , in descending order. The lists can be seen as unpersonalized indices; starting from the top item, they will be consumed one item at a time, whenever the current item becomes candidate for the top-k result. We denote by CIL ( t ) the items alrea-dy consumed (as known candidates), by top _ item ( t ) the item present at the current (unconsumed) position of IL ( t ) , and we use top _ tf ( t ) as short notation for the term frequency associated with this item.

We detail mostly the computation of social frequency, sf ( i | u,t ) , as it is the key parameter in the scoring function of items. Since when  X  = 0 we do not use metrics that are tag-only dependent, it is not necessary to treat each tag of the query as a distinct dimension and to visit each in round-robin style (as done in the threshold algorithm or in C ONTEXT M ERGE ). It suffices for our purposes to get at each step, for the currently visited user, all the items that were tagged by her with query terms (one user list for each term).
Even though the social frequency does not depend directly on tf scores, we will exploit the inverted lists and the tf scores by which they are ordered, to better estimate score bounds. In particular, as detailed later, this allows us to achieve instance optimality.
For each tag t j  X  Q , by unseen _ users ( i,t j ) we denote the maximal number of yet unvisited users who may have tagged item i with t j . This is initially set to the maximal possible term frequency of t j over all items (value that is available at the current position of the inverted list of IL ( t j ) , as top _ tf ( t ) ).

Each time we visit a user u who tagged item i with t j we can (a) update sf ( i | s,t j ) (initially set to 0 ) by adding  X  (b) decrement unseen _ users ( i,t j ) .

When unseen _ users ( i,t i ) reaches 0 , the social frequency value sf ( i | s,t j ) is final. This also gives us a possible termination condition, as discussed in the following.
 At any moment in the run of the algorithm, the optimistic score M
AX S CORE ( i | s,Q ) of an item i that has already been seen in some user list is estimated using as social frequency, for each tag t query, the value top ( H )  X  unseen _ users ( i,t j ) + sf ( i | s,t
Symmetrically, the pessimistic overall score, M IN S CORE is estimated by the assumption that, for each tag t j , the current social frequency sf ( i | s,t j ) will be the final one. The list of candidates D is sorted in descending order by this lowest possible score. An upper-bound score on the yet unseen items, M AX S CORE SEEN is estimated using as social frequency for each tag t top ( H )  X  top _ tf ( t )) .

When the optimistic scores of items already in D but not in its top-k are less than the pessimistic score of the last item in the current top-k (i.e., D [ k ] ), the algorithm can terminate; we are guaranteed that the top-k can no longer change. (Note however that at this point the top-k items may have only partial scores and, if a ranked answer is needed, the process of visiting users should continue.) Algorithm 1 gives the flow of SNS. Key differences w.r.t. C TEXT M ERGE  X  X  social branch are (i) the on-the-fly computation of proximity values, in lines 1-7 and 29-31, and (ii) the consuming of inverted list positions, when they become candidates, in lines 20-28. For clarity, we first exemplify a SNS run without the latter aspect (this would correspond to a C ONTEXT M ERGE run).

E XAMPLE 4. Revisiting Example 1, recall we want the top-2 items for the query Q = { news,site } from Alice  X  X  point of view. To simplify, let us assume that score ( i | u,t ) = sf ( i | u,t ) and g is addition. We detail next the execution of our algorithm.
At the first iteration of the line 8 loop, we visit Bob  X  X  user list, adding D 3 to the candidate buffer. At the second iteration, we visit Danny  X  X  user list, adding D 2 , D 4 to the buffer. At the third iteration ( Carol  X  X  user list) we add D 6 to the buffer. D 1 is added to the candidate buffer when the algorithm visits Frank  X  X  user lists, at iteration 4. Recall top _ tf ( news ) = 7 and top _ tf ( site ) = 5 .
The 6th iteration of the algorithm is the final one, visiting George  X  X  user lists, finding D 2 tagged with news,site and D 4 tagged with site . D 4 and D 2 are the top-2 candidates, with M IN S = 2 . 61 and M IN S CORE ( D 2 ,Q ) = 2 . 21 . The closest candidate is D 6 , with M IN S CORE ( D 6 ,Q ) = 1 . 2 and M AX S CORE ( D 6 ,Q ) = 1 . 2 + 6  X  0 . 1 + 4  X  0 . 1 = 2 . 2 . Also, M AX S CORE = 7  X  0 . 1 + 5  X  0 . 1 = 1 . 2 . Finally, M AX S CORE ( D 6 ,Q ) &lt; M
IN S CORE ( D 2 ,Q ) and since we have M AX S CORE U NSEEN M IN S CORE ( D 2 ,Q ) , the algorithm stops returning D 4 and D 2 .
We discuss next the interest of consuming the inverted list po-sitions, when these become candidates (illustrated in Example 5). In lines 20-28, we aim at keeping to a minimum the worst-case estimation of the number of unseen taggers. More precisely, we test whether there are top-k candidates i (i.e., items already seen in user lists) for which the term frequency for some tag t j of Q , tf ( t  X  X ithin reach X , as the one currently used (from IL ( t j ) ) as the basis for the optimistic (maximal) estimate of the number of yet unseen users who tagged candidate items with t j . When such a pair ( i,t is found, we can do the following adjustments: 1. refine the number of unseen users who tagged i with t j 2. advance (one sequential access) beyond i in IL ( t j ) to the Algorithm 1: SNS  X  =0 : top-k algorithm for  X  = 0 12: if i 6 X  D then 13: add i to D 14: for all tags t l  X  Q do 16: end for 17: end if 19: end for 27: end for 28: end while 31: end for 33: break 34: end if 35: end while 36: return D [1] ,...,D [ k ]
E XAMPLE 5. Let us now consider how the choice of advancing in the inverted lists, when possible, influences the number of needed iterations. At first, top _ tf ( news ) = 7 , top _ item ( news ) = D 4 , and top _ tf ( site ) = 5 , top _ item ( site ) = D 2 .

The first iteration only introduces D 3 and we cannot advance in any of the two inverted lists. Then, the discovery of D 2 and D 4 in step 2 allows us to fix their exact tf values and advance in the lists. The new positions are: top _ tf ( news ) = 2 , top _ item ( news ) = D 1 , 3 rd iteration allows us to advance further in the inverted lists. Fi-nally, in step 4 , the discovery of D 1 allows us to advance in the inverted lists to top _ tf ( news ) = 1 , top _ item ( news ) = D 5 , and top _ tf ( site ) = 1 , top _ item ( site ) = D 5 (only undiscov-ered item). This allows for some drastic score estimation refine-ments. We have the same top-2 candidates, D 4 and D 2 , with M
IN S CORE ( D 4 ,Q ) = 1 . 81 , M IN S CORE ( D 2 ,Q ) = 1 . 21 . Clos-est item remains D 6 , by M IN S CORE ( D 6 ,Q ) = M AX S CORE ( D 6 , Q ) = 1 . 2 , since we know that we have visited all users who tagged D 6 . M AX S CORE U NSEEN ( Q ) = 1  X  0 . 3 + 1  X  0 . 3 = 0 . 6 , since the top unseen document could be tagged only once with each tag. Then, since M AX S CORE U NSEEN ( Q ) &lt; M IN S CORE ( D 2 ,Q ) and M
AX S CORE ( D 6 ,Q ) &lt; M IN S CORE ( D 2 ,Q ) we can exit the loop, two steps before the unrefined version, with D 4 , D 2 as the top 2 . We can prove the following property of our algorithm:
P ROPERTY 2. For a given seeker s , SNS  X  =0 visits the network in decreasing order of the  X  + values with respect to s .
As a corollary of Property 2, we have that SNS  X  =0 visits users who may be relevant for the query in the same order as the state-of-the-art algorithm of [19]. More importantly, we prove that our algorithm visits as few users as possible, i.e., it is instance optimal with respect to this aspect when  X  = 0 . Moreover, the experiments show that overall SNS can drastically reduce the number of visited user lists in practice (see Section 5).
We use the definition of instance optimality of [12]. For a class of algorithms A , a class of legal inputs (instances) D , cost ( A , D ) denotes the cost of running algorithm A X  A on input D  X  D . An algorithm A is instance optimal for its class A over inputs D if for ev-ery B  X  A and every D  X  D we have cost ( A , D ) = O ( cost ( B , D )) .
Let c UL be the abstract cost of accessing the user list -a process which involves the relatively costly operations of finding the prox-imity value of the user and retrieving the items tagged by the user with query terms -and let users ( A , D ) be the number of user lists needed for establishing the top-k for algorithm A on input D . Let c
S be the abstract cost of sequentially accessing the data in IL and let seqitems ( A , D ) be the total number of sequential accesses to IL for algorithm A on input D . In practice, c UL c S reasonable assumption, hence, for two algorithms A and B , we have users ( A , D )  X  c UL + seqitems ( A , D )  X  c S users ( B , D )  X  c UL + seqitems ( B , D )  X  c S
Hence, for a fair cost estimate in practical social search settings, a reasonable assumption is to consider cost ( A , D ) = users ( A , D ) .
Let us now define the class of  X  X ocial X  algorithms S to which both SNS  X  =0 and C ONTEXT M ERGE (when  X  = 0 ) belong. These algorithms correctly return the top-k items for a given query Q and seeker s , they do not use random accesses to IL ( t ) indexes in order to fetch a certain tf value, and they do not include in their working buffers (e.g., candidate buffer D ) items that were not yet encountered in the user lists. The last assumption could be seen as a  X  X o wild guess X  policy, by which the algorithm cannot guess that an item might be encountered in some later stages. This is a reasonable assumption in practice, as the number of items needed for computing a top-k result for a given seeker should in general be much smaller than the total number of items tagged by query terms.
The class D of accepted inputs consists of the inputs that respect the setting described in Section 2. We can prove the following result:
T HEOREM 1. SNS  X  =0 is instance optimal over S and D , when the cost is defined as cost ( A , D ) = users ( A , D ) .
For the general case of  X   X  [0 , 1]  X  with the on-the-fly processing of user proximities  X  at each iteration, the algorithm can alternate, by calling C HOOSE B RANCH (), between two possible execution branches: the social branch (lines 8-31 of Algorithm 1) and the textual branch , which is a direct adaptation of the NRA algorithm.
As in the exclusively social setting of the previous section, we read tf-scores tf ( t j ,i ) from the inverted lists, when needed, either as in line 21 of SNS  X  =0 , or when advancing in the textual branch. Initially, all unknown tf-scores are assumed to be set to 0 .
The optimistic overall score M AX S CORE ( i,Q ) of an item i that is already in the candidate list D will now be computed by setting fr ( i | s,t ) , defined in Eq. (2.1), to The last term accounts for the textual weight of the score, and uses either the exact term frequency (if known), or an upper-bound for it (the score in the current position of IL ( t ) ).

Symmetrically, for the pessimistic overall score M IN S CORE the frequency fr ( i | u,t ) will be computed as where partial _ tf is the count of visited users who tagged i with t and is used as lower-bound for tf ( t j ,i ) when this is not yet known.
A score upper-bound for yet unseen items, M AX S CORE U NSEEN is estimated using as overall frequency for each tag t j the value fr ( i | s,t ) =  X   X  top _ tf ( t ) + (1  X   X  )  X  top ( H )  X  top _ tf ( t )) . We present the flow of the general case algorithm in Algorithm 2. Method I NITIALIZE () amounts to lines 1-6 of SNS  X  =0 , and method P
ROCESS S OCIAL () amounts to lines 8-31 of SNS  X  =0 (modulo the straightforward adjustment for the count partial _ tf ).

The difference between the  X  = 0 case and the general case is the processing of the inverted lists (textual branch), which is done as in the No Random Access algorithm (NRA) [12] (see lines 7-13 of Algorithm 2). We discuss how the choice of the branch to be followed is done, by the C HOOSE B RANCH () subroutine hereafter. Algorithm 2: SNS: top-k algorithm for the general case 10: end if 13: end for 14: end if 16: break 17: end if 18: end while 19: return D [1] ,...,D [ k ] Choice between social and textual branches. SNS  X  =0 only the social branch matters) is instance optimal (Th. 1), with the cost being estimated as users ( SNS  X  =0 , D ) . As the NRA algo-rithm, when only the textual branch matters, SNS  X  =1 is instance optimal, with the cost being estimated as seqitems ( SNS  X  =0
When  X  is not one of the extreme values, under a cost function as a combination of the two above, of the form users ( SNS  X  =0 , D )  X  c UL + seqitems ( SNS  X  =1 , D )  X  c a key role for efficiency is played by C HOOSE B RANCH ().
In [19], the choice between the textual branch or the social one was done by estimating the maximum potential score of each, in round-robin manner over the query dimensions. We use a different  X  potentially more efficient  X  heuristic for the branch choice. At any point in the run of SNS, unless termination is reached, we have at least one item r with M AX S CORE ( r,Q ) &gt; M IN S CORE We consider the item r = D [ argmax l&gt;k ( M AX S CORE ( D [ l ] ,Q )] , which has the highest potential score, and we choose the branch that is the most likely to refine r  X  X  score (put otherwise, the branch that counts the most in the M AX S CORE estimation for r ). The intuition behind this branch choice mechanism is that it is more likely to advance the run of the algorithm closer to termination.

For each tag t j  X  Q , we set M AX T EXTUAL ( t j ) to  X   X  top if the term frequency tf ( t j ,r ) is not yet known, or to 0 otherwise. For the social part of the score, we set M AX S OCIAL ( t j ) = (1  X   X  )  X  unseen _ users ( t j ,r )  X  top ( H ) . Then, we follow the social branch if, for at least one of the tags, M
AX S OCIAL is greater than M AX T EXTUAL . Note that we deal with the tags of the query  X  X n bulk X , and advance simultaneously on their inverted lists when the textual branch is followed.
The algorithm described in Section 3 is sound and complete, and requires no prior (aggregated) knowledge on the proximity values with respect to a certain seeker (e.g., statistics); this was also the assumption in [19] X  X  C ONTEXT M ERGE algorithm. Moreover, it is instance optimal in the exclusively social setting (one main focus of this paper) with respect to the number of visited users. While we improve the running time in both this setting and the general one (more on experiments in Section 5), in practice, however, two aspects can have a significant impact on efficiency:
Reason 1. The search may still visit a significant part of the network and users X  item lists, before being able to conclude that the top-k answer is final. Yet we observed that, in most practical cases, the last top-k change occurs much sooner, so there is a clear opportunity to stop the browsing of the network earlier. For that, we can adopt tighter yet approximate worst score or best score bounds  X  to be used in Algorithm 1 X  X  line 32  X  based on statistics about the proximity vectors.

Reason 2. The on-the-fly exploration of the social network in decreasing order of proximity may still place a significant execution overhead on query processors, even when relying on efficient max-priority queue structures. As our instance optimality result indicates, this is unavoidable in cases where exact computations are required. However, if approximate results are accepted, we can speed-up this step by techniques that yield approximate proximity values  X  replacing line 9 of Algorithm 1  X  and visit the network accordingly.
Approximate score bounds and proximity scores may obviously lead to approximate final results. The techniques we present here provide a good trade-off between accuracy drop on one hand, and speed-up and the amount of necessary storage, on the other hand. We detail next our approaches for the two outlined directions. In Algorithm 2, the M AX S CORE , M AX S CORE U NSEEN and M S
CORE bounds have all used the safest possible values for the prox-imities of yet unseen users: either the top (maximum) value of the max-priority queue ( top ( H ) ) for the first two bounds, or its minimal possible value (zero) for the third one. In practice, however, any of these extreme configurations is rarely met, and values in proximity vectors fall rapidly in many real-world networks.

Hence one possible direction for reducing the number of visited users is to pre-compute and materialize a high-level description of users X  proximity vectors. This would allow us to use a tighter estimation for the remaining (unseen) users, instead of uniformly associating them the extreme score ( top ( H ) or 0 ).
 sider as a proximity vector description one that is very concise yet generally-applicable and effective, keeping for a given seeker two parameters: the mean value of the proximities in the vector and the variance of these values. We adopt here the simplifying assumption that the values in any seeker X  X  proximity vector are independent, essentially interpreting proximity vectors as random ones.
At any step in the algorithm X  X  run, using mean and variance, for the remaining (unvisited) unseen _ users ( i,t ) for a given item i and tag t  X  Q , we can derive (a) lower bounds for the average of their proximity values, for M IN S CORE estimates, or (b) upper bounds for the average of proximity values, for M AX S CORE estimates. The guarantees of these bounds can be controlled in a probabilistic sense by a precision parameter  X   X  (0 , 1] , by which lower values lead to higher precision and 1 leads to absence of guarantees.

More precisely, let p be the current position in the proximity vector and let  X  + p : ( s ) be the vector containing the remaining (unseen) values of  X  + ( s ) . Knowing the overall mean and variance of the entire proximity vector  X  + ( s ) , and having the proximity values seen so far (denoted  X  + 0: p ( s ) ), we can easily compute the mean and variance of the remaining proximity values (those in  X  + p :
The mean and variance of unseen _ users ( i,t ) randomly chosen proximity values from the remaining ones can be obtained  X  under an independence assumption  X  as follows: V ar [  X  + p : ,unseen _ users ( i,t )] =
When the input query contains several tags, its size | Q | needs to be taken into account in estimations. In order to avoid computational overhead, we uniformly chose a non-optimal per-tag probabilistic parameter  X  0 that ignores per-tag score distributions, as
E ST M AX ( p, X  ) represents, for each query tag, the upper bound of the expected value of the average of unseen _ users ( i,t ) values drawn from  X  + p : ( s ) , which holds with probability at least 1  X   X  Similarly, E ST M IN ( p, X  ) represents the lower bound of the expected value of the average of unseen _ users ( i,t ) values drawn from  X  ( s ) , which holds with probability at least 1  X   X  0 . For estimating M
IN S CORE when i 6 X  CIL ( t ) , having no information about the difference between tf ( i,t ) and partial _ tf ( t,i ) (the users who tagged item i with t so far) means that we cannot assume that other users may have tagged i , so we keep this estimation as in the initial (exact) algorithm. By Chebyshev X  X  inequality, these bounds are approach described previously is twofold: low memory require-ments and estimated bounds that are applicable for any proximity distribution. However, it may still offer bounds that are too loose in practice, and hence not achieve the full efficiency potential of ap-proximate score bounds.. To address this issue, we can imagine, as a compromise between keeping only these two statistics and keeping the entire pre-computed proximity vector, an approach in which we describe the distribution at a finer granularity, based on histograms .
More precisely, for seeker s , we denote this histogram as h (  X  It consists of b buckets, and each bucket b i , for i  X  X  1 ,...,b } , has n items in the interval ( low i ,high i ] (the 0 values are assigned to bucket b ). Then, the probability that there exists a proximity value x greater than low i , knowing the histogram h (  X  + ( s )) , is
At any step p in the run of the algorithm, we maintain a partial his-togram denoted as h (  X  + p : ( s )) , obtained by removing from h (  X  the p already encountered proximity values.

Similar to the previous approach, we can drill down the overall  X  parameter to a  X  0 one for each query tag. Then, E ST M AX be given by the minimal value in the partial histogram, such that the resulting estimation of M AX S CORE ( i,t ) holds with at least proba-bility 1  X   X  0 . Conversely, E ST M IN ( p, X  ) is given by the maximal value in the partial histogram, such that the resulting estimation of M
IN S CORE ( i,t ) holds with at least probability 1  X   X  0
In manner similar to Eq.(4.1), we need to account for the fact that some unseen _ users ( i,t ) such estimated values lead to an overall approximate estimation, for both E ST M IN and E ST M AX . So each of these values is uniformly estimated using a stronger probabilistic parameter  X  00 ( i,t ) , depending on unseen _ users ( i,t ) , as and hence we can estimate E ST M AX ( p, X  ) and E ST M IN
The space needed for keeping such histograms is linear in the number of users and buckets. For example, by setting the latter using the square-root choice, the memory needed is O ( n 3 2 ) . Also, as a consequence of the on-the-fly computation of proximity values, we can easily update the histogram of the seeker by merging the partial,  X  X resh X  histogram obtained in the current run (until termination) with the remaining values from the pre-computed histogram.
We present in this section our approach for the second approx-imate direction, by computing proximity values in approximate, efficient manner. For this, we adapt the landmark-based approach of Potamias et al. [17], which studies fast and accurate estimations of shortest paths in large graphs.

The principle of the approach is the following. Given a set L of nodes, called landmarks , we compute their proximity vectors (i.e., proximity w.r.t. all other nodes of the network). The number of landmarks can be considered to be a small constant, not depending on the network size ( 10 in our experiments). Then, for a given seeker s , by knowing her proximity values  X  + ( s,l i ) to each of the landmarks, we can use the triangle inequality to compute upper and lower bounds on the distance between s and any other node v ,  X  ( s,v ) . For instance, when path multiplication is used to compute proximity, these bounds can be obtained as follows: min(  X  (Similar bounds can be obtained for any proximity computation function verifying Property 1.) The tightest proximity interval for v is then given by the minimal upper bound value over all landmarks and the maximal lower bound value over all landmarks.

Algorithm 3: G ET M AX L ANDMARKS ( s, L , { h 1 ,...,h |L| 6: return u
We detail the computation of the next closest user in Algorithm 3, which replaces line 9 of Algorithm 1. We keep the |L| vectors ordered descending by the proximity scores. Let h i denote the current (unvisited) user at the head of l i  X  X  list. Then, at each step, we consider the unvisited user u having the highest proximity value among all the landmarks X  proximity vectors. We advance the pointer in the respective vector, and then random-access u  X  X  score in each of the other landmarks X  proximity vectors, obtaining  X  + ( s,u ) as the maximal proximity upper bound. The proximity upper bound for yet unvisited users, maxUB (to be used in the score estimations of line 32 in Algorithm 1, instead of top ( H ) ) is obtained by computing the maximal distance from the seeker to any of the unvisited users; by monotonicity, it suffices to only look at the values at the heads of the |L| vectors. This operation is equivalent to one loop of the Threshold Algorithm (TA) [12], using the function max as aggregation. Dataset. In Twitter, a highly popular microblogging and social tagging application, one broadcasts to followers short messages ( tweets ). One can also re-broadcast incoming tweets from followees ( re-tweets ); when doing so, the re-tweet may be tagged with certain tags or short descriptions ( hashtags ). So we can see a tweet and its re-tweeted instances as representing one data item, which may be tagged with various tags by the users broadcasting it. This data can thus be modeled naturally as triples ( user,item,tag ) . Our dataset comes from a collection of tweets obtained via the public Twitter Streaming API. From the initial stream of tweets, we filtered out those that were never re-tweeted. Then, from the resulting set of triples, we filtered out those corresponding to (i) items not tagged by at least 2 distinct users, or (ii) users who did not tag at least 2 distinct items.

From this collection of triples, we generated three user similarity networks, in which the proximity between two users is given the Dice coefficient of either (i) the sets of tags used by those users (a tag similarity network), (ii) the sets of items tagged by those users (an item similarity network), or (iii) the sets of item-tag pairs of those users (an item-tag similarity network ). The properties of this dataset and its similarity networks are as follows (as expected, all networks have long tail degree distributions and low diameter): Setup. We generated 20 top-10 queries, formed by 2 or 3 seman-tically coherent tags, from those having a medium frequency. For each network, 10 users were also randomly chosen in the seeker X  X  role.

In the score model, the aggregation function g was sum . For the ranking function (the h -function), we used either the standard tf-idf: score ( i | u,t ) = fr ( i | u,t )  X  idf ( t ) , or BM15 as in [19]: where inverse document frequency idf is defined in a standard way. We ran Java implementations of our algorithms on a 2.8GHz Intel Core i7 under Ubuntu10.04, with 8GB of RAM, using PostgreSQL9.
As our focus is on optimizing social top-k retrieval, we report here on our results for the cases when the social branch is at least as important as the textual one, i.e., for  X   X  X  0 ,..., 0 . 5 } . As [19], multiplication over the paths was chosen as the proximity aggrega-tion function, as the best suited candidate for implicit similarity.
For the testing environment described previously, we report on efficiency and scalability for both exact and approximate algorithms, and on precision for the latter. For efficiency (running time), we ignore differences in SNS  X  X  favor that are hard to account for , namely we do not distinguish between the user accesses by C TEXT M ERGE (which in a real setting, would be to disk) and the ones by SNS (which could even be to main memory). For that, we plugged our on-the-fly computation of proximity in the run of C TEXT M ERGE . Recall that an immediate consequence of Property 2 is that SNS (exact) and C ONTEXT M ERGE give the same results.
The relevance of personalized query results is a topic that has been extensively treated ([20, 11, 16]). It is not our focus here, and we view result relevance as a consequence of the scoring functions g and h . The relevance of social search results was also extensively evaluated in [19], over Delicious data, in a similar setting. Efficiency. Fig. 2 gives the running time comparison for tf-idf and BM15 scores. First, note that, in general, SNS in its exact version drastically improves efficiency  X  especially for low  X  values  X  when compared to the state-of-the-art algorithm. Note that our branch choice heuristic (in both the exact and approximate variants) brings Figur e 3: Precision rates vs relative speedup,  X  = 0  X  left: SNS /MV , right: SNS /H significant improvements (e.g., consider the difference between the savings for  X  = 0 and  X  = 0 . 1 , in the tag similarity network).
Note that instance optimality is indeed not a synonym of efficiency in some cases (for example, in tag similarity networks). In these cases, our approximate approaches lead to further improvements, supporting the intuition that even limited statistics (like mean and variance) can render the termination conditions more tight.
The running times of SNS /MV and SNS /H were obtained for the probabilistic threshold  X  = 0 . 9 . While this is a rather weak guarantee, we found that it still yields a good precision/efficiency tradeoff. (For a better understanding of this tradeoff, we consider later on the impact of  X  on precision.) For  X  &gt; 0 , visiting the per-term inverted lists in parallel with the proximity vector helps in deriving tighter bounds for unseen items, leading to faster ter-mination of the approximate approaches. For the landmarks-based variant, consistent with [17] X  X  results, where choosing landmarks based on centrality is shown to yield precise estimations, we took as landmarks the 10 nodes having the highest centrality values.
To better understand how the instance optimality of SNS  X  =0 reflects in the performance results, Table 1 reports on the number of visited users (by thousands) by C ONTEXT M ERGE and SNS (columns users ). Note that SNS  X  =0 achieves good savings (in terms of visited users), while relying on only few sequential accesses in the inverted lists (column CIL ). SNS /H can further decrease the number of visited users in item and item-tag similarity networks, and SNS /L performs best in tag similarity networks.
 Precision versus speedup. We studied the impact of the proba-bilistic parameter  X  on precision and speedup, in the approximate al-gorithms. We define precision as the ratio between the size of the ex-act result and the number of items returned by both the approximate approach and SNS, i.e., precision = | T SNS /app  X  T SNS | / | T T
SNS /app is the set of items returned as top-k by an approximate approach ( SNS /MV , SNS /H or SNS /L ), and T SNS is the set of items returned by the exact algorithm.

The relative speedup is defined as speedup = time ( SNS ,D ) /time ( SNS /app,D )  X  1 . We present in Figure 3 the results for the two approximate approaches based on statistics. For SNS /MV , note that  X  has a limited influence on precision (with a minimum of 0 . 997 for  X  = 1 ), while ensuring reasonable speedup. The speedup potential is greater when using histograms, with reasonable precision levels (e.g., precision of around 0 . 805 when  X  = 0 . 9 , for a speedup of around 2 . 5 ). For values of  X  &gt; 0 . 9 , we note however a rapid drop in precision. The fact that SNS /MV yields better precision than SNS /H may seem counterintuitive, since histograms give a more detailed description of proximity vectors. This difference is due to looser bounds for MV , as they directly influence the termination
Note that we cannot compare with [1] X  X  approach, as it only extends classic top-k retrieval by interpreting user proximity as a binary function (0-1 proximity), by which only users who are directly connected to the seeker can influence the top-k result. conditions, result in a longer run and better chances of returning a more precise results. For the landmark-based variant, SNS /L , we mention here that choosing 10 landmarks yields a precision above 0 . 65 in both datasets, when  X  = 0 , and above 0 . 95 for  X  &gt; 0 . (We also observed that with more landmarks there is no precision improvement, but execution becomes slower.) Scalability. We also evaluated how our approaches scale with two key parameters, namely the result size k and the network size (number of users), in the exclusively social case. As shown in the top row of Figure 4, the exact approach maintains relatively constant gains. Approximate approaches, especially SNS /H and SNS /L , exhibit higher gains and their cost increases relatively slowly. The  X  X lateau X  in running times for high k values is mainly due to the fact that  X  as the number of retrieved items increases  X  the need for accessing more items in the textual lists and more users increases. However, beyond a certain point (here, around k = 20 ), the termination conditions become easier to satisfy, and hence the increase in cost is less sharp than for low k values. We observed a different behavior when varying the network size (bottom row of Figure 4). First, we can note a linear cost increase. The gains of our algorithms w.r.t. the state-of-the-art one increase with the network size, both for the exact and approximate versions. As with varying k , the approximate approaches exhibit a slower cost increase in most cases.
 Wrapping up. It can be seen that, in most cases, instance op-timality does indeed translate into significant performance gains. These are important when top-k results need to be exact, as in cases when pre-computed information (such as average values, histograms or shortest paths) is not available. On the other hand, when exact results are not needed, keeping partial aggregates such as histograms seems to work best in sparser networks, while approaches based on precomputed shortest paths (landmarks) seem to have a bigger impact in denser networks. Moreover, when the result size k is large and so the impact of  X  X issed X  items may be more limited, relying on histograms and landmarks seem the best options for efficiency.
We see many directions for future work. Optimizing the branch choice heuristic is one promising direction. Another one is experi-menting with probabilistic bounds and statistics tailored to certain assumptions (e.g, power-law distributions) or richer descriptions for proximity vectors. Also, we intend to adapt our approach to networks with positive and negative links (e.g., for trust and distrust). Acknowledgments . This work was partially supported by the EU project ARCOMEM FP7-ICT-270239.
