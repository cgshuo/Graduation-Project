 Aggregator websites typically present documents in the form of representative clusters. In order for users to get a broader perspec-tive, it is important to deliver a diversified set of representative doc-uments in those clusters. One approach to diversification is to max-imize the average dissimilarity among documents. Another way to capture diversity is to avoid showing several documents from the same category (e.g. from the same news channel). We combine the above two diversification concepts by modeling the latter ap-proach as a (partition) matroid constraint, and study diversity max-imization problems under matroid constraints. We present the first constant-factor approximation algorithm for this problem, using a new technique. Our local search 0 . 5 -approximation algorithm is also the first constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof tech-nique for maximizing diversity under matroid constraints uses the existence of a family of Latin squares which may also be of inde-pendent interest.

In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool, we develop greedy clustering algo-rithms that maximize weighted coverage of a predefined set of top-ics. Our algorithms are based on computing a set of cluster centers, where clusters are formed around them. We show the better perfor-mance of our algorithms for diversity and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts. Finally we perform a user study validating our algorithms and diversity metrics. F.2.2 [ Analysis of Algorithms and Problem Complexity ]: [Non-numerical Algorithms and Problems]; H.2.8 [ Information Systems ]: Applications X  Data Mining Diversity Maximization, Approximation Algorithms, Local Search Algorithms, Matroid Constraints, Clustering.
Aggregator websites such as news aggregators have become in-creasingly common during the past decade. These websites (e.g., Google News) typically present snippets of documents (e.g., news articles) in the form of clusters. In these clusters, related documents are aggregated and a few of the representative items in that cluster are highlighted to give a glimpse of the documents in that cluster. In order for users to get a broader perspective on different topics, it is important to deliver a diversified set of representative documents in those clusters. Presenting a diversified set of documents is also important in the context of product search or commerce search [3]. There are two ways to capture diversity among documents: In order to model the above ways to capture diversity, we define the diversity maximization problem under matroid constraints: assum-ing that we can show a limited number of representative documents in the output, the goal of the diversity maximization problem under matroid constraints is to choose a subset of a small number of rep-resentative documents with the maximum diversity such that they form an independent set of a matroid constraint (e.g., not more than 1 (or 2 ) of the documents belong to the same category). 1
More generally, we consider the problem of choosing a set of di-versified representative documents inside a given set of (possibly overlapping) clusters, and study the clustered diversity maximiza-tion problem under matroid constraints (See Section 2 for more details). this problem from an algorithmic perspective as well as experimen-tally using simulations and a user study. Diversity maximization under matroid constraints. As our main technical contribution, we present the first constant-factor approx-imation algorithm for the diversity maximization problem under matroid constraints using a new local search technique. Previ-ous approximation algorithms for diversity maximization (or max-imum dispersion) are based on a greedy approach that does not handle the matroid constraints [12, 3]. On the other hand, the lo-cal search technique is appropriate for handling such matroid con-straints. Moreover, our algorithm can handle an extra constraint to find representative documents inside a given set of (possibly over-lapping) clusters. Our local search 0 . 5 -approximation algorithm is also the first constant-factor approximation for the max-dispersion problem under matroid constraints [12, 11, 3]. Our combinatorial proof technique for maximizing diversity under matroid constraints uses the existence of a family of Latin squares which may find other theoretical applications.
 Preprocessing for Clustering. As part of the input to the diversity maximization problem discussed above, we need to pre-compute a set of clusters ( C 1 ,...,C k ) . Such a set of clusters can be found using many different techniques. While this is not the focus of this paper, we discuss two simple algorithms to produce these clus-ters and evaluate them in the experimental evaluation section. We show that our greedy algorithm gives a 1  X  1 e -approximation for this problem using submodular maximization properties.
 Experimental Evaluation. We show the effectiveness of our al-gorithms in practice through experiments and a user study. We perform experiments on both real and randomly generated data to confirm the performance of our algorithms for maximizing diver-sity and weighted-coverage.
 Data. The algorithms developed in this paper can be applied to dif-ferent applications of aggregator websites such as news aggregators (like Google News), commerce search [3], and real-time search for online social networks. As for the real dataset , we focus on the application of developing an aggregator website in the context of real-time search for the stream of Twitter micro-posts. We compare the performance of the local search algorithm, a greedy algorithm, and a naive sorting algorithm on these datasets.
 Observations. For several of the randomly generated datasets, the diversity of the local search algorithm is more than 300% higher than that of the greedy or the naive sorting algorithm. For the real data set, we observe that the greedy algorithm and the local search algorithm consistently outperform the sorting algorithm for maxi-mizing weighted-coverage and diversity respectively: For example, we observe that on average, weighted-coverage of the greedy algo-rithm is 78% higher than that of the sorting algorithm, and the local search algorithm achieves 28% higher diversity than the sorting al-gorithm.
 User Study. Finally, we conduct a user study through Amazon Mechanical Turk validating our algorithms and our metrics. This user study shows that most users prefer to see a diversified set of documents across clusters in an aggregator website. Diversity Maximization Problem. The diversity maximization problem studied in this paper generalizes the maximum dispersion problem [12, 11, 3]. This problem has been explored in the con-text of diversity maximization for recommender systems [11], and commerce search [3]. A 1 / 2 -approximation greedy algorithm has been developed for the unconstrained variant of this problem [12], and the variant with knapsack constraints [3]. None of these greedy algorithms gives a constant-factor approximation for this problem under matroid constraints. By developing a local search algorithm and a different proof technique, we get the first 1 / 2 -approximation algorithms for the maximum dispersion problem under matroid con-straints. Such matroid constraints are also natural in the context of product search and can be directly applied to similar problems stud-ied in [3].
 Diversity in Recommender Systems and Web Search Ranking and relevance maximization along with diversification have been extensively studied in recommender systems, web search, and database systems. While these results differ from our work in various as-pects, we point out some related work in this extensive literature which implies the importance of diversification in these applica-tions.

In the context of web search, maximizing diversity has been ex-plored as a post-processing step [4, 31, 1, 2]. Other papers explore search result diversification by re-ranking the top searches through reformulating the queries while taking into account diversity [24, 30, 23]. Other methods are based on clustering results into groups of related topics [18], or expectation maximization for estimating the model parameters and reaching an equilibrium [25]. More-over, in the context of recommender systems, diversification has been explored in various recent papers [20, 16, 32]. For example, explanation-based diversity maximization is explored in [32]. From the information retrieval perspective, more realtime diversification ranking methods have been developed exploiting user browsing be-haviors [8] or exploiting query reformulations for web search re-sults [26]. This topic has been also explored in database systems [7, 17]. For example, notions of diversity has been suggested based on presenting decision trees to users [7]. Also computing a diversified set of results for online shopping [29], image search [15], and on-line social networks [22] has been studied. Finally diversity max-imization can be done through submodualr maximization [28, 9] which are related to the GREEDY and Max. Coverage algorithms studied in this paper. Most of the above work consider other types of diversity metrics, and to the best of our knowledge, our diversity maximization problem subject to matroid constraints has not been explored formally prior to our results. In particular, our local search technique and our proof technique to show the 1 / 2 -approximation for the problem are new and have not been explored in previous work. Documents and Distance Function. Our model consists of a set of documents D and pairwise distance function ` : D  X  D  X  R capturing the dissimilarity among documents. Throughout this pa-per, we assume that the distance function ` is a metric satisfying the triangle inequality, i.e., for any three documents, ` ( d weighted Jaccard Distance as the pairwise dissimilarity between documents which is a metric, our diversity maximization results hold for all metric distance functions. For more details on this, see Section 2.2. The documents d i  X  D may correspond to various ob-jects in different applications: in the context of a news aggregator, these documents correspond to news articles; for product search, the documents represent descriptions of a product, and for real-time search, they correspond to online (micro-)posts by users. Diversity Function. Our main focus in this paper is to find a small set of representative documents maximizing diversity . Given a set of documents S  X  D , the diversity of documents in S is defined as the sum of pairwise distances between documents in S , i.e.,
This diversity measure has been motivated and studied in the context of recommender systems and commerce search [11, 3]. For example, Bhattacharya, Gollapudi, and Munagala [3] argue that maximizing this diversity measure is desirable in the context to commerce search, and validate this claim through a user study. The simplest version of the diversity maximization problem is to choose a set S of k documents with maximum diversity. Below, we discuss more general variants of this problem in the presence of a (possibly overlapping) clustering of document and also under matroid constraints.
 Clustered Diversity Maximization. Given a set of clusters C = ( C 1 ,C 2 ,...,C k ) , find a set S of p representative documents r ,r i 2 ,...,r i p in each cluster C i maximizing diversity ( S ) .
Note that in the definition of the problem above, clusters C need not to be disjoint, and can be arbitrary overlapping subsets of nodes. In particular, each cluster may include the whole set of documents, i.e, C i = D . For this special case, the diversity max-imization algorithm is to choose a set of pk nodes in the graph to maximize diversity (independent of their coverage). Since the dis-tance function ` satisfies triangle inequality [5], this special case of the clustered diversity maximization problem is equivalent to the maximum dispersion problem [12].
 Motivation for Matroids. In order to identify diversified represen-tative documents, a natural requirement is to retrieve documents from different categories. In settings that such categories are ex-plicitly given, this constraint might be enforced in addition to max-imizing the diversity function defined above. One example of such constraints is as follows: Consider a partitioning of documents D into q subcategories D 1 ,D 2 ,...,D q , where D =  X  1  X  i  X  q D s are disjoint sub-categories of documents. For example, in the context of news aggregators, each category D i may correspond to a news domain; for product search, the subcategory D i may cor-respond to a specific brand, and in the context of real-time search, a subcategory may correspond to online posts from the same user. We would like to find a subset of documents maximizing total di-versity such that at most one document from each category D present in the whole set (or each given cluster C i ). The constraint of not having more than one document from each subcategory can be captured as a special matroid constraint, called the partition ma-troid constraint . Here, we first define a matroid constraint, then define the diversity problem under a general set of matroid con-straints.
 Matroid Constraints. A matroid M is defined as a family of sub-sets of the ground set of documents E ( M ) = D , called indepen-dent sets. The set of independent sets S of a matroid M is denoted by I ( M ) . For a given matroid M , the associated matroid con-straint is S  X  I ( M ) . As is standard, M is a uniform matroid of rank r if I ( M ) := { X  X  X  ( M ) : | X | X  r } . A partition matroid is the direct sum of uniform matroids. Note that uniform matroid constraints are equivalent to cardinality constraints, i.e, | S |  X  k . For more details about matroids, see [27].
 Now, we formally define the problem: Clustered Diversity Maximization Under Matroid Constraints. Let M be a matroid over the set of documents D with a family of independent sets I ( M ) . Given a set of clusters C = ( C our goal is to find an independent set R i  X  I ( M ) of p represen-tative documents R i = { r i 1 ,r i 2 ,...,r i p }  X  C i ( 1  X  i  X  k ) while maximizing diversity (  X  1  X  i  X  k R i ) .

A special class of matroids is a partition matroid M : Given a partitioning of documents D to q subsets ( D 1 ,D 2 ,...,D set S  X  D is an independent set of partition matroid M iff S has at most one document from each subset D j for each 1  X  j  X  q . As discussed earlier, our main motivation for studying matroid con-straints is partition matroids.
As discussed earlier, our diversity maximization results hold for any metric distance function ` , however, in Section 4 and our ex-perimental evaluation, we focus on the generalized Jaccard distance function. In this section, we define this distance function and other preliminaries related to it.
 Topics. In order to define the Jaccard distance function between documents, we consider a set of topics T . The topics t  X  T cor-respond to important themes or subjects of those documents, e.g. they may correspond to hot news or micro-post topics for news ag-gregators or micropost real-time search; or to their brand and their features for the product search application.
 Pairwise relevance and edge weights w ( d,t ) . For each document d  X  D and any topic t  X  T , the weight w ( d,t ) models the rele-vance of a topic t to a document d , i.e., it represents the extent to which a document is related to a topic t  X  T . The larger the weight w ( d,t ) is, the more relevant document d is to topic t . Throughout the this paper, we assume that the weights are given and computed in advance 2 For the theoretical part we assume that the weights are given.

We construct an edge-weighted bipartite topic-document bipar-tite graph G ( D,T,E ) between documents and topics with edge weights w ( d,t ) . Our goal is find a set of clusters with a set of rep-resentative documents in each cluster covering a large portion of topics while maintaining diversity. A central property that we need to satisfy in this paper is matroid constraints.
 Generalized Jaccard Distance. To further investigate the diver-sity of a set of documents, we define the following generalized Jac-card distance between documents: Given an edge-weighted bipar-tite graph G ( D,T,E ) , for two documents d 1 and d 2 , the general-ized Jaccard distance between d 1 and d 2 is defined as:
This distance function is a natural generalization of the Jaccard distance functions over sets with weighted elements. This gen-eralized Jaccard distance has been studied already and it has been shown that it satisfies the triangle inequality [5].
Here, we present a local search algorithm to choose a set of di-versified representatives in each cluster under a (partition) matroid constraint M . We prove that it achieves an approximation ratio of 1 2 in the worst case. Roughly speaking, the algorithm starts from an arbitrary set R i of representative documents in each clus-ter, and at each step, it considers all pairs ( d,d 0 )  X  R of documents inside and outside of the representatives such that R \{ d } X  X  d 0 }  X  I ( M ) , and examines swapping these two doc-uments, i.e, removing document d and adding document d 0 to the set of representatives. If this swap increases the total diversity of the documents by a factor of 1 + | D | for a small constant , i.e, if
These weights can be computed in different ways for different applications. In Section 5, we describe one specific way to compute those weights for the real-time application. Local search algorithm to choose diversified representatives
Input: A set of clusters C = ( C 1 ,C 2 ,...,C k ) over a set of documents D , a matroid M over documents, and and a distance function ` among documents.
 Output: For each cluster C i ( 1  X  i  X  k ), a set of p documents
R i = { r i 1 ,r i 2 ,  X  X  X  ,r i p } X  X  ( M ) where R i  X  C
Goal: Find a set of representatives maximizing total 1. Initialize: Let R i be a set of top documents in C i . 2. While there exists a local improvement do 3. For any pair of documents ( d,d 0 )  X  R i  X  C i \ R i 4. If removing d from R i and adding d 0 to R i increases 5. Let R i := R i \{ d } X  X  d 0 } .
 Figure 1: Local Search Algorithm for Cluster Center Selection it increases the sum of pairwise distances of representatives by that factor, we make this swap, i.e, we let R i = R i \{ d } X  X  d
One desirable property of this local search algorithm is its flexi-bility to optimize other objective functions in the case where there are more than one choice for local improvements. Examples of these objective functions are quality or coverage of the documents or the popularity of the news channels. To optimize a different ob-jective function, one can try the sequence of local operations in the order of that objective, for example, we initialize the representa-tives to the set of documents with the highest quality score, and consider swapping other documents with these documents in the order of their quality score. This quality score may take into ac-count the reliability and coverage of each document, or the popu-larity of owner of the micro-post. We first prove that the algorithm achieves a good guaranteed approximation ratio, and then we study its computational time complexity.
 Approximation Factor. Here, we show that the exact local search algorithm achieves a guaranteed approximation ratio of 1 / 2 . Such a proof for the exact local search algorithm simply implies a guar-anteed approximation of 0 . 5  X  n for the approximate local search algorithm above. This proof is based on the existence a class of diagonal Latin squares, and thus this technique may find other the-oretical applications.

T HEOREM 3.1. Given a set of clusters C = ( C 1 ,C 2 ,...,C the above local search algorithm is a 1 / 2 -approximation algorithm for the problem of choosing a set of p representatives R i for each cluster C i , maximizing the total diversity of the represen-tatives.

P ROOF . Consider an optimum solution O = { O 1 ,...,O where O i = { O i 1 ,...,O i p }  X  I ( M ) . Let the output of the lo-cal search algorithm be L = { L 1 ,L 2 ,...,L k } where L = { L i 1 ,...,L i p } X  X  ( M ) . Our goal is to show that To prove this, we need to employ a useful exchange property of matroids (see [27]). Intuitively, this property states that for any two independent sets I and J , we can add any element of J to the set I , and kick out at most one element from I while keeping the set independent. Moreover, each element of I is allowed to be kicked out by at most one element of J .
 P ROPOSITION 3.2. [[27]] Let M be a matroid and I,J  X  I ( M ) be two independent sets. Then there is a mapping  X  : J \ I  X  ( I \ J )  X  X   X  } such that: 1. ( I \  X  ( b ))  X  X  b } X  X  ( M ) for all b  X  J \ I . 2. |  X   X  1 ( e ) | X  1 for all e  X  I \ J .

Applying Proposition 3.2, we may consider a mapping  X  i be-tween each subset L i and O i such that L i \ L i j  X  O i Without loss of generality, we may assume  X  i ( L i j ) = O from Proposition 3.2, L i \ L i j  X  O i j  X  X  ( M ) . Now consider remov-ing L i j from L j and adding O i j to L j . Using the local optimality of L and since L i \ L i j  X  O i j  X  X  ( M ) , we know that for any 1  X  i  X  k and 1  X  j  X  p :
Now, adding the above inequality for all 1  X  i  X  k and 1  X  j  X  p , we get:
For ease of notation, let o i = O b i k c +1 let o 0 ,o 2 ,...,o n  X  1 correspond to the set of all representatives in the optimal solution O i j  X  X  for 1  X  i  X  k and 1  X  j  X  p . Similarly, let l 0 ,l 2 ,...,l n  X  1 correspond to the set of representatives in the local optimal solution L i j . In this new notation, we can re-write the above inequalities as:
In order rewrite the above inequalities, we use a pattern similar to a family of Latin squares. Consider an n  X  n diagonal Latin square, with elements a ij for 0  X  i  X  n  X  1 and 0  X  j  X  n  X  1 . This Latin square has the following properties (that are useful later to finish the proof): In each row and column of Latin square, each number 0 , 1 , 2 ,...,n  X  1 appears exactly once, and a ii = i . The existence of such Latin squares have been already proved [14]. Therefore, the formula may be rewritten as Now consider the following set of triangle inequalities among the documents: ` ( o i ,l a ij ) + ` ( o j ,l a ij )  X  ` ( o i ture of the Latin square, and by putting it all together, we get: This completes the proof.
 Running time of the local search algorithm. It can be seen that this algorithm converges as the value of the metric increases at each step. In fact, this algorithm has a polynomial running time, since a swap only is done if the metric increases by a factor of 1+ appropriately small constant &gt; 0 . To show this, first we observe that by starting from a set S including two furthest documents, we have g ( S )  X  OPT k 2 for the initial set S . Starting from such a set, and using the approximate local improvements, it follows that if the algorithm performs t local improvements, then (1 + n ) since at each step, the metric increases by 1 + n factor and the value of the metric is upper bounded by k 2 times the initial value of the metric. It follows that t  X  O (log 1+ Therefore the algorithm finishes at most after O ( n log( k )) local improvements, and thus it runs in polynomial time.
As another variant of the diversity maximization problem, one can consider the matroid constraint over all representatives (instead of having the matroid constraint for the representatives in each clus-ter). In this variant, we find a set of p representatives R such that the whole set  X  1  X  i  X  k R i is an independent set in the ma-troid. For example, we may enforce the constraint that at most one representative in  X  1  X  i  X  k R i can be chosen from each D 1  X  j  X  q . The significance of this variant is that it generalizes the max-dispersion problem under matroid constraints. To see this, consider this problem with all equal clusters C i = D and p = 1 . Therefore, a 1 2 -approximation algorithm for this problem implies a 2 -approximation algorithm for the max-dispersion problem with matroid constraints. In the following, we formally define this vari-ant.
 Clustered Diversity Maximization Under Global Matroid Con-straints. Let M be a matroid over the set of documents D and fam-ily of independent sets I ( M ) . Our goal is to find an independent set R i  X  C i of p representative documents R i = { r i 1 for each cluster C i ( 1  X  i  X  k ) such that  X  1  X  i  X  k R maximize diversity (  X  1  X  i  X  k R i ) .

The local search algorithm can be easily modified to solve this new variant. To see this, consider a local search algorithm in which at each step, we only allow swap operations that keep the whole set of representatives an independent set of the matroid. One can easily check that the above proof can be adapted to show that such a local search algorithm gives a 1 2  X  -approximation algorithm. This, in turn, implies the first constant-factor approximation for the maximum dispersion problem under matroid constraints.
As part of the input to the diversity maximization problem dis-cussed above, we need to pre-compute a set of clusters ( C Such a set of clusters can be found using many different techniques. While this is not the focus of this paper, for the sake of complete-ness, we discuss two simple algorithms to produce these clusters. We will also evaluate these algorithms in the experimental evalua-tion section. The ideas behind both of these algorithms are based on finding a subset S = { d 1 ,...,d k } of top k center documents, and then building a cluster around each center d i . In other words, we think of each document in this set as a center document for a cluster around it, i.e., after computing these center documents, we construct a clustering C = ( C 1 ,C 2 ,...,C k ) of documents where d i  X  C i by associating each document d  X  D to one or more close center documents in S . In computing the set of cen-ters S , we aim to cover a large range of topics, and we take two approaches: maximize a weighted-coverage function of the docu-ments in S , and maximize the diversity ( S ) while taking into ac-count their weighted-coverage. We will first define the weighted-coverage function, and then algorithms aiming to optimize it.
In this section, we define the weighted-coverage function and present an algorithm for the maximum weighted-coverage problem. Weighted-coverage. Given a set of documents D and topics T , and an edge-weighted bipartite graph G ( D,T,E ) with edge weights w ( d,t ) , the weighted-coverage of subset S  X  D is the sum of the weighted coverage of topics cov-ered by documents in S , where the weighted coverage of each topic t is max d  X  S w ( d,t ) i.e., the extent to which this item t is covered. More formally, the weighted-coverage of documents in S is equal to coverage ( S ) = P t  X  T max d  X  S w ( d,t ) .
 Maximum weighted-coverage problem. Given a parameter k (i.e., the number of documents) and an edge-weighted bipartite graph G ( D,T,E ) , the goal of the maximum weighted-coverage problem is to choose a subset S of k center documents maximizing the weighted-coverage coverage ( S ) .

Note that this problem is different from the well-studied set cover or the maximum k -coverage problem [10], but it is still NP-hard, since the same problem with 0/1 as edge weights is equivalent with an unweighted version of maximum k -coverage problem [10] which is NP-Hard.
 Greedy Algorithm for Coverage Maximization. Given the def-inition of the coverage function, the greedy algorithm to choose the top k documents is as follows: start from an empty set S of documents, at each step choose a document d with the maximum marginal increase in function coverage (i.e, max coverage ( S  X  X  d } )  X  coverage ( S ) , and add this document to set S . Repeat this greedy selection procedure until k elements are picked.

By proving the submodularity of the weighted-coverage function defined above, we can prove that greedy algorithm above achieves at least 1  X  1 e of the optimum of the maximum weighted-coverage problem [21].
One way of choosing the set of centers in the clusters is to find k center documents with maximum diversity. However, other than the diversity metric, one may care about other objective functions such as relevance, weighted-coverage, and popularity of the micro-posts in the set of centers to form the clusters. A desirable property of the local search algorithm for maximizing diversity is its flexi-bility in choosing the order of local improvements, and the initial Combined Heuristic (for Coverage and Diversity of Centers)
Input: The edge-weighted bipartite graph G(D,T,E) with edge weights w ( d,t ) for each d  X  D and t  X  T .
 Output: A set S of k documents S = { d 1 ,d 2 ,  X  X  X  ,d k } .
Goal: Find a set S of cardinality k . 1. Initialize: S =  X  2. Sort documents D in the non-increasing order of 3. Let the sorted documents be ( d 1 ,d 2 ,...,d | D | ) 4. For i from 1 to | D | do 5. if | S | &lt; k then 6. Let S := S  X  X  d i } . 7. elseif swapping d i with any d j for j &lt; i increases diversity, then 8. Let S := S \{ d j } X  X  d i } .
 Figure 2: Combined Heuristic for Cluster Center Selection Table 1: Average of DIVERSITY, normalized DIVERSITY, W-COVERAGE, and normalized W-COVERAGE of the initial set of centers various algorithms. DIVERSITY is normalize com-pared to the maximum possible diversity, i.e., m 2 , where m is the number of documents. W-COVERAGE is normalized based on the coverage of the SORT algorithm. The first table is for real data sets, and the second and third tables are for random graphs with p = 1% and p = 10% .
 SORT(TOP-k ) 145.82 77% 68755 100% GREEDY 1% 44.86 99% 198.9 102% Comb-H 10% 44.1 98% 683.71 84% GREEDY 10% 43.39 96% 933.63 115% set of documents to start with. For example, if we want to simul-taneously maximize the weighted coverage and diversity metrics, we can start with a set of documents with high weighted coverage, and then run the local search algorithm as a post-processing step. Also in running the local search algorithm, we should try to swap documents with higher weighted-coverage at each step. We use this guideline in the implementation of the local search algorithm. As an alternative approach to find the clusters, we also study the following heuristic for choosing the initial k centers by combin-ing ideas for maximizing coverage and diversity. The algorithm is described in Figure 2.

Note that the algorithm in Figure 2 is a simple heuristic algo-rithm combing the ideas behind the greedy and the local search al-gorithms for maximizing weighted-coverage and diversity. While we do not prove a worst-case approximation factor for this com-bined heuristic, we will show its reasonable performance on real and random data in our experimental evaluations.
In order to evaluate our algorithms in practice, in this section, we perform experiments on real world data (from Twitter) and on randomly generated data. More specifically, we examine the practi-cal performance of our algorithms and their variants by comparing them with each other and a with baseline algorithm. We begin by describing the data and our metrics.
 Twitter Data. We run our experiments on 61 families of twitter posts, each of which is constructed as follows: We consider 10,000 to 25,000 top queries for Google real-time search, and for each query, we consider 30 top micro-posts returned for that query. This ranking is done based on various criteria such as the relevance of those twitter posts as well as their popularity and importance for the topic of the query. For each dataset, this process results in tens of thousands of documents, and a mapping of those top queries to thousands of topics. We construct the edge-weighted bipartite graph between these documents and topics using the algorithm de-scribed in Section 5. As a result, we get 61 edge-weighted bipartite graphs with an average of 1,277 topics and average of 12,500 doc-uments. For this data set, the categories D i forming the (partition) matroid constraints are sets of micro-posts from the same Twitter user. We aim to identify 10 or 20 centers, and similar to Google News, we consider clusters with three representatives in each clus-ter.

Processing Twitter data. An important source of identifying hot topics in the online micro-blogging environment is the set of popu-lar queries by users. As a first step of identifying important micro-posts, we identify popular queries that have been searched more often recently by users, and use these queries to retrieve a set of micro-posts matching those queries. In this setting, since tweets do not contain many terms, there is less need to discount term fre-quency by inverse document frequency (as in TF-IDF). We assume that a real-time search engine is available and one can identify the most relevant documents for those queries.

Let Q be the set of top queries on a real-time search engine. Each query has a query score query_score ( q ) , representing the popular-ity of this query, e.g., the number of users who searched for this query. For each q in Q , let D q be a ranked list of top l micro-posts or documents for q , and for each document d  X  D q , we also have a relevance_score ( q,d ) .

For each q in Q , let T q  X  T be the set of topics corresponding to q . We transform each query q to a set of topics T q , and construct an edge-weighted bipartite graph G ( D,T,E ) between documents D =  X  q  X  Q D q and all topics T =  X  q  X  Q T q . The weight of the edge between a topic t  X  T and and a document d  X  D is denoted by w ( d,t ) and is computed as follows: for each query q  X  Q and each topic t  X  T q , we associate a weight weight ( q,t ) which is propor-tional to the score of the query normalized by the number of topics associated with this query q , e.g, weight ( q,t ) = query Now the weight w ( d,t ) in the topic-document bipartite graph is captures the extent to which online entry d covers topic t model-ing the relevance of this topic to the micro-post. Intuitively, these topics represent the main topics of hot trends over the online envi-ronment. After forming the above edge-weighted topic-document bipartite graph, our goal is find a set of top documents covering the maximum amount of topics.
 Randomly generated data. We generate the underlying bipartite graph between documents and topics at random. In particular, we generate families of random bipartite graphs each with 20000 doc-uments and 2000 topics. We examine random bipartite graphs in which each edge is present with probability p independent of all other edges. We consider six values for the probability p , i.e., p = 0 . 1% , 0 . 5% , 1% , 2% , 5% , 10% for having each edge in the bipartite graphs. The categories D i for the partition matroid con-Figure 3: Vectors of the W-COVERAGE metric for all the in-stances. X-axis corresponds to instances, and Y axis corre-sponds to the coverage metric. From top to bottom, curves correspond to GREEDY, COMB-H, and SORT algorithms. In-stances are ordered based on the coverage value of the output for SORT algorithm. straint are generated at random among 100 categories. We consider identifying 10 or 100 centers and clusters with three representatives in each cluster.
 Metrics. We divide the practical evaluation of our algorithms into two main parts: 1) evaluation of the center selection algorithm for clustering, 2) evaluation of diversity maximization for representa-tive selection given a set of clusters. For each part of the evaluation, we use an appropriate set of metrics. The main metrics are related to diversity and weighted-coverage as defined in this paper, and other metrics are inspired by the k -median and k -means problems with and without outliers [6]. Some of the metrics for evaluating both first and second parts are as follows: (I) Weighted-coverage (W-COVERAGE): For a subset S of doc-uments, the weighted-coverage of S is P t  X  T max d  X  S w ( d,t ) . (II) Diversity (DIVERSITY): For a subset S of documents, the diversity of S is P d  X  S P d 0  X  S ` ( d,d 0 ) . (III) Average distance to the centers (DIST-ALL): For a set S = { c 1 ,c 2 ,...,c k } of centers, the DIST-ALL objective function is the sum of distances of each document to the closest center, i.e., P (IV) Percentage of documents covered in clustering (PERC): For a family of clusters C = ( C 1 ,C 2 ,...,C k ) , this metric is the per-centage of documents covered in this clustering, i.e., | X  (V) Average distance of covered documents to the centers (DIST-COVERED): For a set of clusters C = ( C 1 ,C 2 ,...,C center c i  X  C i for each 1  X  i  X  k , the mean-distance DIST-
One other metric for evaluating the second part (i.e, diversity maximization for choosing representative documents inside each given cluster) is as follows: Intra-cluster diversity (INTRA-DIVERSITY): For a family of clusters C = ( C 1 ,C 2 ,...,C k ) and a subset R C of representatives inside each cluster, we define the intra-cluster Baseline Algorithms and Heuristics. In the first part, we study the following two baseline algorithms and our two heuristics for finding a set of centers, and compare them in terms of diversity, weighted-coverage, and other metrics: Table 2: Average percentage of documents covered by each algorithm (PERC), average of average generalized Jaccard distance (GJD) of covered documents to the centers(DIST-COVERED), and average of average GJD distance of all docu-ments to the centers(DIST-ALL).

In the second part, we compare different algorithms for diver-sity maximization for representative selection in each cluster. We assume that given a set of center documents, clusters have been formed by putting each document in the cluster associated with the closest center. Documents that are not overlapping their set of top-ics with the any of the centers are thrown away, i.e., if the minimum generalized Jaccard distance ( ` ) of a document to centers is 1, then this document is not in any cluster. In particular, we compare the performance of the following three algorithms in terms of diversity, weighted-coverage, intra-cluster diversity, clustering coverage. We examine three ways (including a baseline) to find the clusters (SORT(TOP-k ), Comb-H, Max-Cov), and three ways (including a baseline) to pick representatives (BASE, LOCAL, INTRA-L), and compare the results of the nine different combinations (e.g., Comb-H/LOCAL). Table 3: Average of W-COVERAGE, DIVERSITY, and INTRA-DIVERSITY of the set of representatives for nine al-gorithms on real data sets. The three SORT/... are baseline algorithms for picking the centers.
 Max-Cov./SORT(BASE) 1730 22.58 136462 Observations. Our experiments on both real and randomly gener-ated data confirm the better performance of our algorithms for max-imizing diversity and weighted-coverage. The better performance of the local search algorithm for diversity maximization compared to the greedy algorithm is more clear for randomly generated data sets.

For the real data, we report statistics for the set of 20 centers, or the 20 clusters based on these 20 centers, and the set of 60 represen-tative documents in these clusters. First we observe that GREEDY and Comb-H algorithms consistently outperform both the SORT and RANDOM algorithms for maximizing the weighted-coverage and diversity metrics respectively. For example, Figure 3 shows that the weighted-coverage of these two algorithms outperform that of SORT for each of the instances. Moreover, Table 1 shows that on average, coverage of GREEDY is 78% higher than that of SORT, and coverage of Comb-H is 25% higher than the coverage of SORT. Also the coverage of GREEDY is 42% higher than that of Comb-H. In addition, Table 1 shows that on average, diversity of both GREEDY and Comb-H is 28% higher than diversity of SORT. Al-though we expect the diversity of Comb-H to be better than that of GREEDY, our real datasets do not show a significant difference between the diversity of GREEDY and Comb-H. However, we will observe such a difference on the random datasets (See Figure 4).
Other than the coverage and diversity metrics for the 20 centers, we also construct 20 clusters out of these centers by assigning each document to the closest center and report clustering metrics like PERC and DIST-COVERED, as well as the DIST-All metrics for these centers and clusters (See Table 2). Intuitively, we expect algo-rithms with good coverage and diversity to have high percentage of covered documents, and small average distance of other documents to clusters, i.e., smaller DIST-ALL. We observe that GREEDY and Comb-H achieve comparable PERC and DIST-ALL. They both have larger PERC and smaller DIST-All compared to SORT. On the other hand, GREEDY has a slightly larger DIST-COVERED, im-plying that since GREEDY covers more topics, the resulting clus-ters may end up being less coherent, and thus there seem to be tradeoff between the coverage of the centers of the clusters and the coherency (DIST-COVERED) of the clusters. Finally, we report the average of Diversity, Intra-Diversity, and Coverage for the set of 60 representatives for the six algorithms combining Comb-H. and GREEDY (or Max-Cov.) to find the centers and BASE, LO-CAL, and INTRA-L to find the representative inside clusters (See Figure 3). The results from this part are similar to previous part, i.e., the coverage of Max-Cov. algorithm is larger, but the diversity Figure 4: Average diversity metric for four algorithms on six random family of networks. On the X axis, from left to right, the bars correspond to random networks with probabilities p = 0 . 1% , 0 . 5% , 1% , 2% , 5% , 10% on the edges of the bipartite graph.
 Table 4: Average of W-COVERAGE, and DIVERSITY of two algorithms for finding centers and four algorithms for picking representatives for random graphs with p = 1% . The results for other random graphs are similar.
 of all the algorithms are comparable. The Intra-diversity of repre-sentatives for the algorithms using Intra-L is larger than the other algorithms.
 The results for randomly data sets are similar in most aspects. Here, we discuss the main difference: while, for real data sets, the diversity metric for the output of GREEDY (or Max-Cov.) algo-rithm was comparable to that of Comb-H, we have observed fami-lies of instances of random data sets in which the average diversity of the output of Comb-H which is aiming to maximize the diversity is much higher. This is particularly significant for dense instances in which the probability of an edge in the document-topic bipartite graph is large, e.g. p = 5% or p = 10% . For instance, for p = 5% , the Diversity of the output of Comb-H. is more than 300% higher than that of GREEDY (See Figure 4). For brevity, we report the details of these metrics for a subset of algorithms (See Tables 3 and 4),but the results for other algorithms follow the same pattern.
We conducted a user study with the goal to validate our metrics and algorithms. Our survey was designed to find out if people pre-fer to see a diversified set of clusters.

This user study was designed in the form of a survey on Ama-zon X  X  Mechanical Turk(MTurk) simulating a trends X  aggregator web-site. MTurk is a crowdsourcing online marketplace where requesters use human intelligence of workers to perform certain tasks, also known as HITs (Human Intelligence Tasks). Workers browse among existing tasks and complete them for a monetary payment set by the requester [19]. Once a worker completes the task, the requester can decide whether to approve it. In particular, if the requester believes that the worker completed the task randomly, she can reject her work. In that case, the worker does not get paid for the particular task and her approval rate is decreased. For our studies, we only hired workers that had approval rates of over 95%, that is, workers who had performed well in the past.

We present four questions each with two choices where each op-tion is a set of clusters. The user is then asked which option she liked to see if she visited a realtime trends aggregator website. The two choices differed in the degree of diversity in them, e.g., one was generated by the diversity maximization algorithm and the other one by the baseline sorting algorithm. In each survey, we repeated two of the questions with the answers in reverse order for valid-ity check. If the answers to those questions were inconsistent, we discarded the survey answers from that user. Overall, 130 out of 300 respondents had answered these two questions inconsistently. Therefore, we analyzed the remaining 170 valid responses.
Each question consists of two main parts: 1)  X  X hich one do you prefer?" and 2)  X  X hich one do you think others prefer?" The an-swer to the first question provides information on how a particular user likes diversity, and the answer to the second question shows how users expect an aggregator to look like. Moreover, we in-cluded the second question in the survey for two reasons. First, by asking the second question the subjects felt that the survey X  X  goal was to study how people think their decisions are similar to the oth-ers, weakening the reactivity effect [13]. Second, we offered three bonus payments ($5 each, which is 50 times the amount we paid for each HIT) to the three workers whose answers to the second question was closest to the others X  answers to the first question in order to deter workers from answering randomly.
 Observations. On average (over 4 questions) 65% of the respon-dents preferred to see a more diversified set of clusters. Moreover, 69% of the respondents thought that most people would like to see a diversified set of clusters.
In this paper, we study the diversity maximization problem un-der matroid constraints which is useful in a range of applications  X  in news aggregators, for aggregating trending topics in a micro-blogging environment, and in commerce search [3]. On the theo-retical side, we present the first constant-factor approximation al-gorithm for this problem applying a new local search technique which implies the first constant-factor approximation algorithm for the maximum dispersion problem subject to matroid constraints. From a practical standpoint, we show reasonable performance of these algorithms by running a user study validating our metrics, and by running experiments on synthetic and real data. From an algorithmic perspective, it would be interesting to improve the ap-proximation factor of 2 for the maximum dispersion (or diversity maximization) problem under matroid constraints, or cardinality constraints. From a practical standpoint, it would be nice to apply these ideas in the context of commerce search [3] where matroid constraints are also relevant.
