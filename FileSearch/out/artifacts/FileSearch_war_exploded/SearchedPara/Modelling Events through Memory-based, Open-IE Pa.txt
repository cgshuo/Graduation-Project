 Text summarization beyond extraction requires a semantic representation that abstracts away from words and phrases and from which a summary can be generated (Mani, 2001; Sp  X  arck-Jones, 2007). Following and extending recent work in semantic parsing, information extraction (IE), paraphrase generation and summarization (Titov and Klemen-tiev, 2011; Alfonseca et al., 2013; Zhang and Weld, 2013; Mehdad et al., 2013), the represen-tation we consider in this paper is a large collec-Figure 1: An example of abstracting from input sentences to an event representation and genera-tion from that representation. tion of clusters of event patterns. An abstractive summarization system relying on such a represen-tation proceeds by (1) detecting the most relevant event cluster for a given sentence or sentence col-lection, and (2) using the most representative pat-tern from the cluster to generate a concise sum-mary sentence. Figure 1 illustrates the summa-rization architecture we are assuming in this pa-per. Given input text(s) with resolved and typed entity mentions, event mentions and the most rele-vant event cluster are detected (first arrow). Then, a summary sentence is generated from the event and entity representations (second arrow).

However, the utility of such a representation for summarization depends on the quality of pattern clusters. In particular, event patterns must cor-respond to grammatically correct sentences. In-troducing an incomplete or incomprehensible pat-tern (e.g., PER said PER ) may negatively affect both event detection and sentence generation. Re-lated work on paraphrase detection and relation extraction is mostly heuristics-based and has re-lied on hand-crafted rules to collect such patterns (see Sec. 2). A standard approach is to focus on binary relations between entities and extract Figure 2: A generic pipeline for event-driven ab-stractive headline generation. the dependency path between the two entities as an event representation. An obvious limitation of this approach is there is no guarantee that the extracted pattern corresponds to a grammatically correct sentence, e.g., that an essential preposi-tional phrase is retained like in file for a divorce .
In this paper we explore two novel, data-driven methods for event pattern extraction. The first, compression-based method uses a robust sentence compressor with an aggressive compression rate to get to the core of the sentence (Sec. 3). The second, memory-based method relies on a vast collection of human-written headlines and sen-tences to find a substructure which is known to be grammatically correct (Sec. 4). While the lat-ter method comes closer to ensuring perfect gram-maticality, it introduces a problem of efficiently searching the vast space of known well-formed patterns. Since standard iterative approaches com-paring every pattern with every sentence are pro-hibitive here, we present a search strategy which scales well to huge collections (hundreds of mil-lions) of sentences.

In order to evaluate the three methods, we con-sider an abstractive summarization task where the goal is to get the gist of single sentences by recog-nizing the underlying event and generating a short summary sentence. To the best of our knowledge, this is the first time that this task has been pro-posed; it can be considered as abstractive sentence compression , in contrast to most existing sentence compression systems which are based on selecting words from the original sentence or rewriting with simpler paraphrase tables. An extensive evalua-tion with human raters demonstrates the utility of the new pattern extraction techniques. Our analy-sis highlights advantages and disadvantages of the three methods.

To better isolate the qualities of the three ex-traction methodologies, all three methods use the same training data and share components of the very same summarization architecture, as shown in Figure 2: an event model is constructed by clus-tering the patterns extracted according to the se-lected extraction method. Then, the same extrac-tion method is used to collect patterns from sen-tences in never-seen-before news articles. Finally, the patterns are used to query the event model and generate an abstractive summary. The three differ-ent pattern extractors are detailed in the next three sections. In order to be able to work in an Open-IE man-ner, applicable to different domains, most existing pattern extraction systems are based on linguisti-cally motivated heuristics. Zhang and Weld (2013) is based on R E V ERB (Fader et al., 2011), which uses a regular expression on part-of-speech tags to produce the extractions. An alternative system, O
LLIE (Schmitz et al., 2012), uses syntactic de-pendency templates to guide the pattern extraction process.
 The heuristics used in this paper are inspired by Alfonseca et al. (2013), who built well formed re-lational patterns by extending minimum spanning trees (MST) which connect entity mentions in a dependency parse. Algorithm 1 details our re-implementation of their method and the specific set of rules that we rely on to enforce pattern gram-maticality. We use the standard Stanford-style set of dependency labels (de Marneffe et al., 2006). The input to the algorithm are a parse tree T and a set of target entities E . We first generate com-binations of 1-3 elements of E (line 10), then for each combination C we identify all the nodes in T that mention any of the entities in C . We con-tinue by constructing the MST of these nodes, and finally apply our heuristics to the nodes in the MST. The procedure A PPLY H EURISTICS (:16) re-cursively grows a nodeset N 0 by including chil-based on dependency labels. For example, we in-clude all children of verbs in N 0 whose label is listed in V c (:3), e.g., active or passive subjects, direct or indirect objects, particles and auxiliary verbs. Similarly, we include the parent of a noun in N 0 if the dependency relation between the node and its parent is listed in N p . Sentence compression is a summarization tech-nique that shortens input sentences preserving the most important content (Grefenstette, 1998; Mc-Donald, 2006; Clarke and Lapata, 2008, inter alia). While first attempts at integrating a com-pression module into an extractive summarization system were not particularly successful (Daum  X  e III and Marcu, 2004, inter alia), recent work has been very promising (Berg-Kirkpatrick et al., 2011; Wang et al., 2013). It has shown that drop-ping constituents of secondary importance from selected sentences  X  e.g., temporal modifiers or relative clauses  X  results in readable and more in-formative summaries. Unlike this related work, our goal here is to compress sentences to obtain an event pattern  X  the minimal grammatical struc-ture expressing an event. To our knowledge, this application of sentence compressors is novel. As in Section 2, we only consider sentences mention-ing entities and require the compression (pattern) to retain at least one such mention.

Sentence compression methods are abundant but very few can be configured to produce out-put satisfying certain constraints. For example, most compression algorithms do not accept com-pression rate as an argument. In our case, sen-tence compressors which formulate the compres-sion task as an optimization problem and solve it with integer linear programming (ILP) tools un-der a number of constraints are particularly attrac-tive (Clarke and Lapata, 2008; Filippova and Al-tun, 2013). They can be extended relatively easily with both the length constraint and the constraint on retaining certain words. The method of Clarke and Lapata (2008) uses a trigram language model (LM) to score compressions. Since we are inter-ested in very short outputs, a LM trained on stan-dard, uncompressed text would not be suitable. In-stead, we chose to modify the method of Filippova and Altun (2013) because it relies on dependency parse trees and does not use any LM scoring.
Like other syntax-based compressors, the sys-tem of Filippova and Altun (2013) prunes depen-dency structures to obtain compression trees and hence sentences. The objective function to maxi-mize in an ILP problem (Eq. 1) is formulated over weighted edges in a transformed dependency tree and is subject to a number of constraints. Edge weight is defined as a linear function over a fea-ture set: w ( e ) = w  X  f ( e ) .
 In our reimplementation we followed the algo-rithm as described by Filippova and Altun (2013). The compression tree is obtained in two steps. First, the input tree is transformed with determin-istic rules, most of which aim at collapsing indis-pensable modifiers with their heads (determiners, auxiliary verbs, negation, multi-word expressions, etc.). Then a sub-tree maximizing the objective function is found under a number of constraints.
Apart from the structural constrains from the original system which ensure that the output is a valid tree, the constraints we add state that: 1. tree size in edges must be in [3, 6], 2. entity mentions must be retained, 3. subject of the clause must be retained, 4. the sub-tree must be covered by a single Since we consider compressions with different lengths as candidates, from this set we select the one with the maximum averaged edge weight as the final compression. Figure 3 illustrates the use of the compressor for obtaining event pat-terns. Dashed edges are dropped as a result of constrained compression so that the output is John Smith married Mary Brown and the event pattern is PER married PER . Note that the root of a sub-clause is allowed to be the top-level node in the extracted compression.

Compared with patterns obtaines with heuris-tics, compression patterns should retain preposi-tional verb arguments whose removal would ren-der the pattern ungrammatical. As an example consider [C. Zeta-Jones] and [M. Douglas] filed for divorce . The heuristics-based pattern is PER and PER filed which is incomplete. Unlike it, the compression-based method keeps the essential prepositional phrase for divorce in the pattern be-cause the average edge weight is greater for the tree with the prepositional phrase. Neither heuristics-based, nor compression-based methods provide a guarantee that the extracted pattern is grammatically correct. In this sec-tion we introduce an extraction technique which makes it considerably more likely because it only extracts patterns which have been observed as full sentences in a human-written text (Sec. 4.1). However, this memory-based method also poses a problem not encountered by the two previous methods: how to search over the vast space of ob-served headlines and sentences to extract a pattern from a given sentence? Our trie-based solution, which we present in the remainder of this sec-tion, makes it possible to compare a dependency graph against millions of observed grammatical utterances in a fraction of a second. 4.1 A tree-trie to store them all. . .
 Our objective is to construct a compact representa-tion of hundreds of millions of observed sentences that can fit in the memory of a standard worksta-tion. This data structure should make it possible to efficiently identify the sub-trees of a sentence that match any complete utterance previously ob-served. To this end, we build a trie of depen-dency trees (which we call a tree-trie ) by scan-ning all the dependency parses in the news training data, and index each tree in the tree-trie accord-ing to Algorithm 2. For better clarity, the process is also described graphically in Figure 4. First, each dependency tree (a) is linearized , resulting in a data structure that consists of two aligned se-quences (b). The first sequence ( tokens ) encodes word/parent-relation pairs, while the second se-quence ( structure ) encodes the offsets of parent nodes in the linearized tree. As an example, the first word  X  X he X  is a determiner ( X  X et X ) for the sec-ond node (offset 1) in the sequence, which is  X  X at X . In turn,  X  X at X  is the subject ( X  X subj X ) of the node in position 2, i.e.,  X  X leeps X . As described in Algo-rithm 2, we recursively store the token sequence in the trie, each word/relation pair being stored in a node. When the token sequence is completely consumed, we store in the current trie node the structure of the linearized tree. Combining struc-tural information with the sequential information encoded by each path in the trie makes it possi-ble to rebuild a complete dependency graph. Fig-ure 4(c) shows an example trie encoding 4 differ-ent sentences. We highlighted in bold the path cor-responding to the linearized form (b) of the exam-ple parse tree (a).

The figure shows that the tree contains two kinds of nodes: end-of-sentence (EOS) nodes (red) and non-terminal nodes (in blue). EOS nodes do not necessarily coincide with trie leaves, as it is possible to observe complete sentences embed-ded in longer ones. EOS nodes differ from non-terminal nodes in that they store one or more struc-tural sequences corresponding to different syntac-tic representations of observed sentences with the same tokens.
 Space-complexity and generalization. Storing all the observed sentences in a single trie requires huge amounts of memory. To make it possible to store a complete tree-trie in memory, we adopt the following strategy. We replace the surface form of entity nodes with the coarse entity type (e.g., PER, LOC, ORG) of the entity. Similarly, we replace proper nouns with the placeholder  X  X P] X , thus sig-nificantly reducing lexical sparsity. Then, we en-code each distinct word/relation pair as a 32-bit unsigned integer. Assuming a maximum tree size of 255 nodes, we represent structure sequences as vectors of type unsigned char (8 bit per element). Finally, we store trie-node children as sorted vec-tors instead of hash maps to reduce memory foot-print. As a result, we are able to load a trie encod-ing 400M input dependency parses, 170M distinct nodes and 48M distinct sentence structures in un-der 10GB of RAM. 4.2 . . . and in the vastness match them At lookup time, we want to use the tree-trie to identify all sub-graphs of an input dependency tree T that match at least a complete observed sen-tence. To do so, we need to identify all paths in the trie that match any sub-sequence s of the lin-earized sequence of T nodes. Whenever we en-counter an EOS node e , we verify if any of the structures stored at e matches the sub-tree gener-ated by s . If so, then we have a positive match. As a sentence might embed many shorter utter-ances, each input T will generally yield multiple matches. For example, querying the tree-trie in Figure 4(c) with the input tree shown in (a) would yield two results, as both The cat sleeps and The cat sleeps under the table are complete utterances stored in the trie.

Algorithm 3 describes the lookup process in more detail. The first step consists in the lineariza-tion of the input tree T . Then, we recursively tra-verse the trie calling L OOKUP R ECURSIVE . The inputs of this procedure are: the input tree T , its linearization L and an offset o (starting at 0), the trie node currently being traversed n (starting with the root), the set of offsets in L that constitute a partial match P (initially empty) and the set of complete matches found M . We recursively tra-verse all the nodes in the trie that yield a partial match with any sub-sequence of the linearized to-kens of T . At each step, we scan all the tokens in L in the range [ o, L. L ENGTH ()) looking for to-kens matching any of the children of n . If a match-ing node is found, a new partial match P 0 is con-structed by extending P with the matching token Figure 5: Time complexity of lookup operations for inputs of different sizes. offset i (line 11), and the recursion continues from the matching trie node n 0 and offset i (line 15). Every time a partial match is found, we verify if the partial match is compatible with any of the tree structures stored in the matching node. If that is the case, we identify the corresponding set of matching nodes in T and add it to the result M (lines 12-14). A pattern is generated from each complete match returned by L OOKUP after apply-ing a simple heuristic: for each verb node v in the match, we enforce that negations and auxiliaries in T depending from x are also included in the pat-tern.
 Time complexity of lookups. Let k be the max-imum fan-out of trie nodes, d be the depth of the trie and n be the size of an input tree (num-ber of nodes). If trie node children are hashed (which has a negative effect on space complex-ity), then worst case complexity of L OOKUP ( ) is our memory-efficient implementation, theoretical complexity becomes O ( nk log( k )) d  X  1 . It should be noted that worst case complexity can only be observed under extremely unlikely circumstances, i.e., that at every step of the recursion all the nodes in the tail of the linearized tree match a child of the current node. Also, in the actual trie used in our experiments the average branching factor k is very small. We observed that a trie storing 400M sentences (170M nodes) has an average branching factor of 1.02. While the root of the trie has unsur-prisingly many children (210K, all the observed first sentence words), already at depth 2 the aver-age fan-out is 13.7, and at level 3 it is 4.9. For an empirical analysis of lookup complexity, Figure 5 plots, in black, wall-clock lookup time as a function of tree size n for a random sample of 1,600 inputs. As shown by the polynomial re-gression curve (red), observed lookup complexity is approximately cubic with a very small constant factor. In general, we can see that for sentences of common length (20-50 words) a lookup operation can be completed in well under one second. 5.1 Experimental settings All the models for the experiments that we present have been trained using the same corpus of news crawled from the web between 2008 and 2013. The news have been processed with a to-kenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milne and Witten, 2008). We use Freebase types as fine-grained named entity types, so we are also able to label e.g. instances of sports teams as such instead of the coarser label ORG.

Next, the news have been grouped based on temporal closeness (Zhang and Weld, 2013) and cosine similarity (using tf  X  idf weights). For each of the three pattern extraction methods we used the same summarization pipeline (as shown above in Figure 2): 1. Run pattern extraction on the news. 2. For every news collection Coll and entity set 3. Run a clustering algorithm to group together Table 2: Patterns extracted in each method, before Noisy-OR inference.
At generation time we proceed in the following way: 1. Given the title or first sentence of a news ar-2. Find the model clusters that contain this pat-3. Return a ranked list of model patterns 4. Replace the entity placeholders in the top-
In all cases the parameters of the network were predefined as 20,000 nodes in the hidden layer (model clusters) and 40 Expectation Maximization (EM) training iterations. Training was distributed across 20 machines with 10 GB of memory each.
For testing we used 37,584 news crawled dur-ing December 2013, which had not been used for training the models. Table 3 shows one pattern cluster example from each of the three trained models. The table shows only the surface form of the pattern for simplicity.
 Table 3: Examples of pattern clusters. In each cluster c i , patterns are sorted by p ( p j | c i ) . 5.2 Results Table 2 shows the number of extracted patterns from the test set, and the number of abstractive event descriptions produced.

As expected, the number of extracted patterns using the memory-based model is smaller than with the two other models, which are based on generic rules and are less restricted in what they can generate. As mentioned, the memory-based model can only extract previously-seen structures. Compared to this model, with heuristics we can obtain patterns for more than twice more news ar-ticles. At the same time, looking at the number of summary sentences generated they are com-parable, meaning that a larger proportion of the memory-based patterns actually appeared in the pattern clusters and could be used to produce sum-maries. This is also consistent with the fact that us-ing heuristics the space of extracted patterns is ba-sically unbounded and many new patterns can be generated that were previously unseen  X  X nd these cannot generate abstractions. A positive outcome is that restricting the syntactic structure of the ex-tracted patterns to what has been observed in past news does not negatively affect end-to-end cover-age when generating the abstractive summaries.
Table 1 shows some of the abstractive sum-maries generated with the different methods. For manually evaluating their quality, a random sam-ple of 100 original sentences was selected for each method. The top ranked summary for each origi-nal sentence was sent to human raters for evalua-tion, and received three different ratings. None of the raters had any involvement in the development of the work or the writing of the paper, and a con-straint was added that no rater could rate more than 50 abstractions. Raters were presented with the original sentence and the compressed abstraction, and were asked to rate it along two dimensions, in both cases using a 5-point Likert scale:  X  Readability : whether the abstracted com- X  Informativeness : whether the abstracted Inter-judge agreement was measured using the Intra-Class Correlation (ICC) (Shrout and Fleiss, 1979; Cicchetti, 1994). The ICC for readability was 0.37 (95% confidence interval [0.32, 0.41]), Table 4: Results for the three methods when rating the top-ranked abstraction. and for informativeness it was 0.64 (95% confi-dence interval [0,60, 0.67]), representing fair and substantial reliability.

Table 4 shows the results when rating the top ranked abstraction using either of the three dif-ferent models for pattern extraction. The abstrac-tions produced with the memory-based method are more readable than those produced with the other two methods (statistically significant with 95% confidence).

Regarding informativeness, the differences be-tween the methods are bigger, because the first two methods have a proportionally larger number of items with a high readability but a low informa-tiveness score. For each method, we have man-ually reviewed the 25 items where the difference between readability and informativeness was the largest, to understand in which cases grammatical, yet irrelevant compressions are produced. The re-sults are shown in Table 5. Be+adjective includes examples where the pattern is of the form Entity is Adjective , which the compression-based systems extracts often represents an incomplete extraction. Wrong inference contains the cases where patterns that are related but not equivalent are clustered, e.g. Person arrived in Country and Person arrived in Country for talks . Info. missing represents cases where very relevant information has been dropped and the summary sentence is not complete. Pos-sibility contains cases where the original sentence described a possibility and the compression states it as a fact, or vice versa. Disambiguation are en-tity disambiguations errors, and Opposite contains cases of patterns clustered together that are op-posite along some dimension, e.g. Person quits TV Program and Person to return to TV Program .
The method with the largest drop between the readability and informativeness scores is C OM -PRESSION . As can be seen, many of these mis-takes are due to relevant information being miss-ing in the summary sentence. This is also the largest source of errors for the H EURISTIC system. For the M EMORY -BASED system, the drop in read-wrong inferences can be observed. ability score is much smaller, so there were less of these examples. And most of these examples be-long to the class of wrong inferences (patterns that are related but not equivalent, so we should not abstract one of them from the other, but they were clustered together in the model). Our conclusion is that the examples with missing information are not such a big problem with the M EMORY -BASED system, as using the trie is an additional safeguard that the generated titles are complete statements, but the method is not preventing the wrong infer-ence errors so this class of errors become the dom-inant class by a large margin.

Some examples with high readability but low informativeness are shown in Table 6. Most Open-IE systems are based on linguistically-motivated heuristics for learning patterns that ex-press relations between entities or events. How-ever, it is common for these patterns to be incom-plete or ungrammatical, and therefore they are not suitable for abstractive summary generation of the relation or event mentioned in the text.

In this paper, we describe a memory-based ap-proach in which we use a corpus of past news to learn valid syntactic sentence structures. We discuss the theoretical time complexity of look-ing up extraction patterns in a large corpus of syntactic structures stored as a trie and demon-strate empirically that this method is effective in practice. Finally, the evaluation shows that sum-mary sentences produced by this method outper-form heuristics and compression-based ones both in terms of readability and informativeness. The problem of generating incomplete summary sen-tences, which was the main source of informative-ness errors for the alternative methods, becomes a minor problem with the memory-based approach. Yet, there are some cases in which also the mem-ory based approach extracts correct but misleading utterances, e.g., a pattern like PER passed away from the sentence PER passed the ball away . To solve this class of problems, a possible research direction would be the inclusion of more complex linguistic features in the tree-trie, such as verb sub-categorization frames.

As another direction for future work, more ef-fort is needed in making sure that no incorrect in-ferences are made with this model. These happen when a more specific pattern is clustered together with a less specific pattern, or when two non-equivalent patterns often co-occur in news as two events are somewhat correlated in real life, but it is generally incorrect to infer one from the other. Im-provements in the pattern-clustering model, out-side the scope of this paper, will be required.
