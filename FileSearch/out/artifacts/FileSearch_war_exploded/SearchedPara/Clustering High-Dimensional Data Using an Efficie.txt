 This paper introduces a new algorithm for clustering data in high-dimensional feature spaces, called GARDEN HD .The algorithm is organized around the notion of data space re-duction, i.e. the process of detecting dense areas (dense cells) in the space. It performs effective and efficient elimination of empty areas that characterize typical high-dimensional spaces and an efficient adjacency-connected agglomeration of dense cells into larger clusters. It produces a compact rep-resentation that can effectively capture the essence of data. GARDEN HD is a hybrid of cell-based and density-based clustering. However, unlike typical clustering methods in its class, it applies a recursive partition of sparse regions in the space using a new space-partitioning strategy. The proper-ties of this partitioning strategy greatly facilitate data space reduction. The experiments on synthetic and real data sets reveal that GARDEN HD and its data space reduction are effective, efficient, and scalable.
 H.2.8 [ Database Management ]: Database Applications X  data mining. ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  clustering.
 Algorithms, Design, Performance, Experimentation. Data Mining, Data Clustering, Space Partitioning, Data Di-mensionality.
Clustering is typically interpreted as a process of grouping data objects in a way that increases intra-cluster and reduces inter-cluster similarity between the objects. In this paper, we consider the process of detecting dense areas ( dense cells ) in the space with minimum amounts of empty space, which we refer to as data-space reduction .Inthiscontext, data clustering is a process of detecting the largest areas with this property, called data clusters .

Scalable clustering methods that perform effective data space reduction can have many applications, including ma-chine learning, pattern recognition, data mining, and storage organization. This is because effective data space reduction implies the ability to represent compactly the essential prop-erties of the data distribution. The clustering algorithms designed around this goal must efficiently isolate areas with points, effectively reducing their internal empty space.
By detecting clusters, all clustering methods perform im-plicit data space reduction. However, the reduction is often neither efficient nor scalable. Moreover, it typically does not preserve the essence of data in high-dimensional spaces. This is the case with the algorithms that require prior knowl-edge about the number of clusters, which include K -means [8], CLARANS [10], and CLUTO [18]. BIRCH [17] requires several sensitive input parameters, which are often hard to estimate ahead of time. Experiments show that the perfor-mance of CLARANS is close to quadratic in the number of points [2, 16]. While DBSCAN [2] outperforms CLARANS, it still does not perform well in high-dimensional spaces [1]. WaveCluster [15] is an effective and efficient algorithm, but it can be applied to low-dimensional data only. Similarly, STING [16] is an efficient algorithm, but it has no straight-forward extension for high-dimensional data.
 The clustering algorithms for high-dimensional spaces, e.g. DENCLUE [4], OptiGrid [5], CLIQUE [1], MAFIA [9], SUB-CLU [6], and HyCeltyc [14], tend to be the hybrids of grid-based (cell-based) and density-based clustering. In these spaces, they employ some form of dimensionality reduction or derive the clusters in a subspace. Dimensionality reduc-tion [3] is a useful technique that works well on correlated data. Even then, it may cause a loss of clusters and the dis-tortions of the densities and spatial properties of clusters, which does not facilitate data space reduction.

This paper introduces a new technique for data space re-duction and data clustering, called GARDEN HD ( GAmma Region DENsity clustering in High Dimensionalities ). It performs effective and efficient data space reduction, fol-lowed by an efficient adjacency-connected agglomeration of detected dense cells into larger clusters. As such, GARDEN is appropriate for any system that supports advanced content-based retrieval, e.g. data warehouses as well as multime-dia and scientific databases [12]. Even in high-dimensional spaces, the compact representation of data produced by GARDEN HD can preserve all essential properties of data, including the locations, shapes, and densities of clusters. As a result, many data mining tasks can be performed effi-ciently on this representation. Experimental evidence pre-sented later in the paper also indicates that the algorithm can be effective as a general-purpose clustering method.
The efficiency and effectiveness of the proposed data space reduction technique are due to the application of a new space-partitioning strategy called  X  , a variant of  X  partition-ing [13], which has important advantages over traditional strategies. For example, when splitting each axis once, a grid-like partition [15, 16] of a D -dimensional region would produce 2 D sub-regions. Because of this exponential ex-plosion, a high-dimensional region can be partitioned only along a small subset of dimensions. This can prevent differ-entiation of data along most dimensions and elimination of empty space that appears along these dimensions.

In contrast,  X  can partition any given region along every side creating only O( D ) sub-regions. Moreover, since the partition is performed at virtually negligible costs, both in terms of the memory and computational requirements, ef-fective and efficient identification as well as separation of densely populated areas in the space becomes possible. A formal definition and an extensive treatment of the proper-ties of  X  partitioning appear in this paper for the first time.
More details about GARDEN HD and its motivation can be found in the technical report on GARDEN HD [12]. Here, Section 2 gives the definition of  X  partitioning. Section 3 presents the space-reduction technique of GARDEN HD .Sec-tion 4 gives the operational details of GARDEN HD and dis-cusses its efficiency and scalability. Section 5 presents ex-perimental evidence. Section 6 summarizes the paper and discusses broader applications of this technology.
In this section, we present a new variant of the  X  partition-ing strategy [13] used in GARDEN HD .Like X ,thisstrategy eliminates the exponential explosion discussed in Section 1, which accompanies grid partitioning strategies. In order to denote the fact that this is a different partitioning strategy descending from  X , we refer to it as  X  partitioning .
To define  X  partitioning, we assume a D -dimensional fea-ture space U and use: p and p i to denote a vector (point) in U and its i -th coordinate, respectively; R = l, h to denote a hyper-rectangle in U defined by its low ( l ) and high ( h ) endpoint; [ i,j ] to denote a range of values between i and j inclusively; and  X   X   X  to denote an assignment.
 Definition 1: Let d define an order of dimensions. Let k  X  [1 ,D ] be an integer; B = l ( B ) , h ( B ) ahyper-rectangle  X  h d i .Then,  X  = B, d,k, g is a well-formed  X  partition of B into k +1 non-overlapping hyper-rectangles G ( i ) = l ( i provided l ( i ) and h ( i ) can be derived as fol lows:  X  l  X  i
Informally, the definition says that each region G ( i ) in a  X  partition of a hyper-rectangle B is derived iteratively, in the order defined by d , by setting at most one coordinate in the low endpoint of B and the high endpoint of the previous region G ( i  X  1) , if any, to the corresponding coordinate of a given vector g  X  B . Each coordinate d i of g ,where i  X  gives the position of a hyper-plane that separates G ( i ) the space in which the regions G ( j&gt;i ) lie. In the rest of the paper, we refer to B as base region , d as vector of dimensions , g as generating vector ,andeach G ( i ) as a  X  region . We refer to k as the number of split dimensions because B is split once along k  X  D different sides.

Figure 1 shows a  X  partition of a 3D base region whose oneside(dimension x ) is much shorter than the other two (dimensions y and z ). The example assumes that the di-mensions are ordered by d = z , y , x , and that only the first k = 2 dimensions are partitioned. This partition has three  X  regions. As in all  X  partitions, the high endpoint of the last region represents the generating vector g .
The  X  partitioning strategy [13] is more restrictive than  X  in that it assumes: B = U , d is fixed, and k = D .It allows more than one generating vector (i.e., it can split each side of B more than once), each of which must satisfy a less restrictive  X  X esting X  condition than g in Definition 1. Using a single generating vector ensures that the number of  X  regions is at most D +1, enabling a tightly bound space and time complexity.

Explicit in Definition 1 is an important property of  X  par-titions: a  X  partition is fully defined by B, d,k, g .Thus,to define a  X  partition, one must reserve memory for only O(1) D -dimensional vectors, including the low and high endpoints of the base region B . We will later see that, for a given B , a  X  partition can be derived within the computational com-plexity equivalent to O(1) point comparisons.

Since the space-reduction technique must detect empty space that separates any two disjoint dense cells, which can appear along any or multiple dimensions, this ability of  X  to efficiently split a region along any number of dimensions is very important for accurate and efficient data space reduc-tion. Equally important is the following property of  X  : Lemma 1: Given a well-formed partition  X  = B, d,k, g and an arbitrary point p  X  B : a)  X  i  X  [1 ,k ] , p  X  G ( p G
Informally, Lemma 1 states that, for any given point that lies within the base region, one can identify the  X  region in which the point belongs just by comparing the coordinates of the point against the corresponding coordinates of the generating vector g in the order determined by d .Thus,the time required to locate the  X  region where the point belongs is O(1) comparisons of D -dimensional points. The proof of Lemma 1 follows straightforwardly from Definition 1 after observing that each  X  region G ( i ) , i  X  k , lies above the hyper-plane that separates it form the space in which the subsequent regions G ( j&gt;i ) lie.

A  X  partition induces non-overlapping regions that cover the extent of the base region B . However, the base region may have a large amount of internal empty space. In order to isolate areas of B containing points, a live region is com-puted within each of its  X  regions. The live region is the minimum bounding hyper-rectangle containing all points in the  X  region. With Lemma 1, the process of data space reduction has a basis for efficient computation of live re-gions. This important lemma will also serve as the basis for efficient agglomeration of dense cells detected during data space reduction.
This section introduces the data-space reduction tech-nique of GARDEN HD . This recursive density-based tech-nique uses  X  partitioning, an efficient method to isolate ar-eas with points, and several heuristics designed to deal with certain  X  X alse dense cells X  that can bridge distant clusters. The result is a vastly reduced data space represented as a set of dense cells. The algorithm of the technique appears in Section 4, as part of the algorithm of GARDEN HD .Begin-ning with a brief description of the technique, this section presents the details of its constituent mechanisms.
Given a density threshold  X  as the only input, the process of data space reduction starts by scanning the data once in order to compute the live region LR U enclosing all points in the universe. This becomes the initial base region. The technique proceeds by partitioning LR U into multiple  X  re-gions. Within each  X  region, its live region is identified. Sparse live regions are recursively partitioned into smaller regions using  X  partitioning and live-region identification, until each of the smaller live regions has a density of at least  X  , becoming a dense cell. The process produces a compact  X  X ignature X  of data consisting of the identified dense cells, represented by their low and high endpoints.

The process is illustrated in Figure 2. Figure 2a, in which the dark ovals represent areas populated by points, shows the  X  partition of the initial base region as well as the live regions within the resulting  X  regions. In Figures 2b and 2c, sparse live regions LR 1and LR 3 are partitioned further until all their enclosed dense cells are identified. Whenever a high-density live region is detected (e.g., LR 2 in Figure 2a), it is included into the set of dense cells. The resultant dense cells of this example are shown in Figure 2d.
The  X  partition of a base region is performed by the pro-cedure SplitBaseRegion of Figure 3. Given a base region B , a vector of dimensions d (called DV in the procedure) and the number of split dimensions k  X  D , the procedure
Figure 2: Example of recursive space reduction. completes the  X  partition by deriving a generating vector g (i.e., GV ) that produces k +1  X  regions of equal volume.
For simplicity of density computations, GARDEN HD as-sumes a normalized universe [0 , 1] D . However, SplitBaseRe-gion has no such restriction. In principle, the dimensions can be of any numeric type. The origin of the base region can lie anywhere in the space. Observe that the  X  partition of Figure 1 was derived using this procedure.

In order to compute each coordinate of the generating vec-tor, SplitBaseRegion examines two coordinates of the tem-porary region R and performs a small number of single-value assignments. Thus, the computational complexity of SplitBaseRegion is equivalent to O(1) comparisons of D -dimensional points. A  X  partition can be derived using the statistics about the data distribution in the base region. However, this can lead to much higher computational com-PROCEDURE SplitBaseRegion INPUT B ;//baseregion
DV ; // vector of dimensions, i.e. d k ; // number of split dimensions OUTPUT GV ; // generating vector, i.e. g BEGIN
R  X  B .Region; // temporary region scale  X  1/(1+ k ); // regions must have equal volume for i =1 to k end for END Figure 3: Procedure that partitions a base region into  X  regions of equal volume. PROCEDURE FindLiveRegions INPUT B , DV , k , GV ; // same as in SplitBaseRegion OUTPUT LRL ; // list of live regions BEGIN
LRL = InitializeLiveRegions ( k +1); for each point  X  B end for UnlinkEmptyLiveRegions ( LRL ); END
Figure 4: Procedure that computes live regions. plexity and many more levels of recursion. In Section 5.1, we will investigate whether the adopted partitioning into  X  regions of equal volume is adequate for the purposes of effective data space reduction.
As noted earlier, for each  X  region, its live region is com-puted. In the context of data space reduction, these live regions serve a dual purpose: to eliminate large empty areas that characterize typical (especially high-dimensional) data spaces, greatly contributing to the effectiveness of the space reduction; and to reduce the volume of space that needs to be inspected, increasing the efficiency of the reduction.
Figure 4 gives the procedure FindLiveRegions, which com-putes the live regions of the  X  regions produced by partition-ing a base region B . Drawing on the property of  X  partitions stated in Lemma 1, this procedure performs a very efficient identification of live regions. For a given point that lies within the base region, it locates the  X  region G ( i ) in which the point belongs just by comparing the point against the generating vector g (i.e., GV ) one coordinate at a time.
For the best performance, this procedure must directly access all points in a base region. We assume a simple array of points internally organized as a set of linked lists, each of which is anchored in the memory block of a live region. Each element contains a data point and a pointer to the next data point in the live region. After FindLiveRegions determines the  X  region for the given point p ,itupdatesthe corresponding live region LR and links p to LR .Thetime required to determine the  X  region where a data point be-longs is O(1) comparisons of D -dimensional points, and so is the time to update the corresponding live region. Since only the points in the base region B are accessed, the complexity of FindLiveRegions is O( N ) point comparisons, where N is the number of points in B .
As noted in Section 1, traditional grid partitioning strate-gies can split a high-dimensional region only along a subset of its sides. As a result, a combination of grid partitioning and live-region identification would lead to the creation of many sparse and elongated live regions. Such regions could appear dense even if each has just few distant points.
In particular, whenever the length of one side of a live region LR is less than 2 / X  , the density of LR is above  X  even if LR has only 2 points and the length of every other side is 1. These false dense cells (elongated sparse live re-gions that appear dense) may erroneously  X  X ridge X  distant clusters, creating an illusion of a single cluster.
Since  X  partitioning can perform efficient partitioning of a region along all its sides, it has a natural mechanism that reduces the magnitude of this problem. Yet even with  X  , these false dense cells may still appear. To deal with the problem, our space-reduction technique applies a set of sim-ple heuristics incorporated in two procedures, called Con-structDV and SparsityTest. The former tries to reduce the likelihood of false cells; the latter tries to detect them, in which case they are (just as other sparse regions) split.
Before partitioning a sparse region B , the ConstructDV procedure determines k  X  D sides of B that must be parti-tioned, and returns a vector of dimensions d in which these sides are ordered first. The lengths of these sides must be greater than a computed value  X  (see below) and at least half the length of its longest side. Since the procedure requires only two passes over the coordinates of B , its complexity is O(1) point comparisons. With the given d and k , the proce-dure SplitBaseRegion partitions B only along these k sides in the order given by d (this is why the regions LR 1and LR 3 . 2 in Figures 2b and 2c, respectively, are split into only two regions of equal size.) This improves the  X  X quareness X  of not only the resulting  X  regions, but also their live regions, making the false dense cells less likely to appear.
The boolean function SparsityTest determines if a live re-gion is sparse or not. In order to facilitate this function, when the program begins, GARDEN HD computes an array  X  of density thresholds for all dimensionalities i  X  D based on the given density threshold  X  .Using  X  , it computes a value  X  for which a region of size  X  D with just two points is dense. From 2 / X  D =  X  ,oneobtains  X  =(2 / X  ) 1 /D . Then,  X  i  X   X  ,if i = D ,and X  i  X  2 / X  i ,if i&lt;D .

SparsityTest computes the density of a live region LR in its effective dimensionality D  X  D , ignoring all sides of LR that are shorter than 2 / X  . It also computes the den-sity density 1 ( LR )inthe LR  X  X  longest side alone. If either density 1 ( LR ) &lt;  X  1 or density D ( LR ) &lt;  X  D ,then LR is considered sparse and, thereby, split. The test against  X  designed to avoid narrow dense cells with very few points.
The recursive space reduction creates a tree structure, in which leaves represent dense cells and interior nodes rep-resent instances of  X  partitioning. As the recursion folds back, GARDEN HD agglomerates adjacent dense cells, form-ing the final set of clusters. Like the space-reduction tech-nique, GARDEN HD requires only one input parameter, i.e. the density threshold  X  . The result is a cluster representa-tion of data, which represents each cluster by the list of its internal dense cells.

The two processes of GARDEN HD ( data space reduction and agglomeration ) are performed by the recursive proce-dure SplitAndMerge of Figure 5. The first invocation of this procedure takes the initial base region LR U as input. The procedure has two parts, one for space reduction and the other for agglomeration. Note that, with only the first part, the procedure would support a stand-alone process of PROCEDURE SplitAndMerge INPUT B ; // base region; initially LR U CL ; // list of clusters; initially empty BEGIN
GV  X  SplitBaseRegion ( B , DV , k );
LRL  X  FindLiveRegions ( B , DV , k , GV ); for each LR  X  LRL end for // END OF REDUCTION
LR  X  GetFirstLR ( LRL ); while LR = null end while // END OF AGGLOMERATION END data space reduction. Since the steps of the first part are explained in Section 3, we focus now on the second part.
InGARDEN HD , two cells are merged when their distance along every dimension is below the minimum distance  X  , which is derived from the density threshold  X  .Morepre-cisely,  X  is set to  X  ,where  X  =(2 / X  ) 1 /D was introduced in Section 3.4. By linking the minimum distance to the density threshold, the algorithm provides a single mechanism to reg-ulate the space reduction and separation of data. When the procedure SplitAndMerge begins agglomeration, the dense cells of each live region LR in the list LRL are identified and linked to the memory block of LR . Note that these live regions appear in the list LRL in the order of their corre-sponding  X  regions. For each LR in LRL , its dense cells are immediately attached to the dense-cells list of the enclosing base region B and tested for connectivity with the dense cells of subsequent live regions in LRL .  X  ; // distance threshold DV ; // vector of dimensions, i.e. d GV ; // generating vector, i.e. g LR , LR 1; // live regions of  X  regions G ( i ) and G ( j&gt;i DC , DC 1; // dense cells in LR and LR 1, respectively x = DV [ i ]; ProximityTestForLR: GV [ x ]  X  LR .Region[ low , x ]- X  ProximityTestForLR1: LR 1.Region[ high , x ]  X  LR .Region[ low , x ]- X  ProximityTestForDC: GV [ x ]  X  DC .Region[ low , x ]- X  ProximityTestForDC1: DC 1.Cluster = DC .Cluster  X  DC 1.Region[ high , x ]  X  DC .Region[ low , x ]- X  Figure 6: The proximity tests in SplitAndMerge.

Since the complexity of agglomeration is determined by the number of times the connectivity test Connected is in-voked, the  X  X roximity X  tests in the procedure SplitAnd-Merge are used to eliminate unnecessary invocations of Con-nected based on one or two comparisons of numeric values. These tests also rely on the properties of  X  partitioning.
For example, let LR be a live region of the  X  region G ( i created by partitioning the base region B using the vector of dimensions d . Based on Lemma 1, the coordinate d i of each point p in a  X  region G ( j&gt;i ) ,ifany,islessthan g which defines the position of the separating plane for the  X  region G ( i ) . Then, if LR is more than the distance threshold  X  apart from the separating plane of G ( i ) , the live regions of all subsequent  X  regions must be more than the distance  X  away from LR along the dimension d i .Ifso, LR is enclosed by G ( i ) in such a way that no dense cell of any subsequent live region can be merged with any dense cell in LR .
Figure 6 gives the details of the proximity tests, each of which returns true if the corresponding condition is satisfied. In ProximityTestForDC1, DC .Cluster represents a parame-ter in the memory block of a dense cell DC indicating the cluster to which DC belongs at the given moment.
Even though GARDEN HD does not restrict the depth of recursion, for a large number of points N and almost any  X  , the depth of the recursion is O(log N ). For a uniform distri-bution,  X  partitioning of a base region produces a balanced distribution of points among the resulting  X  regions, ensur-ing O(log N ) levels of recursion regardless of  X  . For skewed data, the fact that  X  partitioning is performed on live re-gions and that it is multi-way partitioning ensures that the volumes of live regions at some level O(log N ) are so small that they would be dense even when  X  is very high.
The cost of the data space reduction is dictated by the procedure FindLiveRegions (all other procedures of the re-duction require O(1) point comparisons). To derive all live regions at any level of recursion, at most N points will be ex-amined, no point will be examined twice, and each point will be processed in time equivalent to O(1) point comparisons. Since O(log N ) is the depth of the recursion, O( N log N ) point comparisons is the cost of recursive space reduction.
On the other hand, the agglomeration is more sensitive to the selected value of  X  , and the combination of very high  X  and D can lead to O( N 2 )time. 1 However, assuming that N&gt;&gt; D ,  X  can be selected so that the number L of the re-sulting dense cells is at most O( N/D ) and that the minimum distance  X  =(2 / X  ) 1 /D is a small value. Even without trials on the data, but with some prior thinking how to select  X  , both of these conditions would typically be satisfied.
When this is the case, the neighborhood of any dense cell is small. For any two live regions LR and LR 1ina  X  parti-tion and a dense cells DC in LR , typically just a few cells in LR 1 closest to DC will require a full test for connectivity with DC . Due to the  X  X roximity X  tests in the procedure SplitAndMerge, other cells in LR 1 are quickly eliminated from inspection, either because they are immediately known to lie too far from DC or because they already belong to the same cluster as DC .Sincethereareatmost D live regions that need to be examined at any level of recursion, O( D ) gives a realistic estimate of the number of neighboring cells examined for connectivity with DC , leading to just O( D ) point comparisons. Since at any level of recursion at most L dense cells are examined and the recursion is O(log N )lev-els deep, the cost of agglomeration is O( DL log N ). When L is at most O( N/D ), this cost is O( N log N ). Thus, with a simple data structure and a bounded parameter  X  , the total cost of GARDEN HD clustering is just O( N log N )compar-isons of D -dimensional points. This estimate implies linear performance deterioration as dimensionality grows.
A comprehensive set of experiments was performed to ob-serve the effectiveness and efficiency of GARDEN HD as well as the effectiveness of its data space reduction. The experi-ments were performed on a PC with a 3.6GHz CPU, 3.25GB memory, and 1MB CPU cache. They were performed on: 1)  X  X enter-corners X   X  synthetic data of varying dimension-ality (up to 100) and number of points (between 100,000 and 500,000); in high-dimensional spaces, the pre-computed clusters were placed in the center and 10 corners of the space (origin, far corner, and 8 randomly selected corners); 2)  X  X ovtype X   X  a set of real data with 54 dimensions (the 55th dimension records the class of objects) and 581,012 points; obtained from the UC IMachine Learning Reposi-tory (www.ics.uci.edu/  X mlearn/MLRepository.html); 3)  X  X nimals X   X  a set of synthetic data with 72 dimensions (the 73rd dimension records the class of objects) and 500,000 points; produced by the  X  X nimals.c X  program obtained from the UC IML Repository.

Because the experiments reported here were performed on a faster machine with a somewhat different implemen-tation of GARDEN HD , the results are somewhat different than the ones reported in [12]. The implementation used in the experiments of [12] keeps data points in a structure called the  X  tree, and the timing results reported there in-clude the construction of the tree. On data sets of moder-ate size, this construction represents a significant overhead. The implementation adopted here, which can be found on www.cs.iit.edu/  X egalite, keeps points in a single array in the order they appear in the input file. However, on very large
Turning the minimum distance  X  into an input parameter would reduce the sensitivity on  X  at the cost of having an additional parameter to be estimated in advance. data sets, this implementation has a much greater potential for virtual memory thrashing when accessing points during the recursive space reduction.
The effectiveness of the recursive space reduction in higher dimensionalities was monitored using the adjusted volumet-ric measure M Q = dense cells and V i is the volume of the i -thcellinitsef-fective dimensionality D (not counting the sides that have extensions 0). Since dense cells whose computed volume would otherwise be 0 are treated as D -dimensional cells with anon-0volume, M Q can produce values greater than 1. However, we adopted this metric because it enables effec-tive tracking of space reduction in progress and provides a good idea how much empty space is eliminated. Without the exponent D/D , M Q could heavily distort the sense of the latter. Since data space reduction is an implicit data-reduction mechanism, we also monitor the data-reduction ratio R Q =2 L/N . Assuming no compression, R Q indicates how much storage dense cells would occupy relative to the size of the data file.

Table 1, where the notation  X  X -y X  stands for 10  X  y ,gives the results obtained on the  X  X enter-corners X ,  X  X ovtype X , and  X  X nimals X  sets. The third column gives the natural loga-rithm of the parameter  X  (the choice of density thresholds is explained in the next sub-section). The fourth column ( Q ) gives the depth of the recursion. On the  X  X enter-corners X  sets, each with 100,000 points, we varied the dimensional-ity D . On the  X  X ovtype X  and  X  X nimals X  sets, we varied the parameter  X  . Since  X  X ovtype X  has 44 binary dimensions, the corresponding values M Q significantly underestimate the achieved data space reduction. Nevertheless, even they indicate very high degrees of the reduction.

Figure 7 shows the progress of the space reduction on the  X  X ovtype X  and  X  X nimals X  sets at different levels of recursion for the density thresholds as in Table 1. For each level of recursion i , the plotted value M i was computed counting all dense cells at levels j&lt;i , if any, and all live regions at the level i . The reduction terminated or slowed down significantly after only 5 or 6 levels.

The curves of Figure 7a suggest that deriving a  X  partition using more extensive statistics about the data distribution in the given live region could be appropriate at the deeper levels of recursion. However, very high degrees of the reduc-tion achieved at levels 4 and 5 suggest that, for the purposes of a stand-alone space reduction, the recursion can be ter-minated before it reaches deeper levels. Since the remaining false dense cells could still present problems for the effec-tiveness of data clustering, we did not restrict the depth of recursion in the experiments reported below.
As the measures of clustering effectiveness, we use the overall purity and entropy of clusters. These are generic measures of clustering accuracy that can be applied to la-beled data with class information. The purity of 1 and en-tropy of 0 indicate the best results. Depending on how the labels have been assigned, on many data sets, it may not possible to achieve the best purity and entropy. However, using these metrics, we can monitor how well GARDEN HD isolates pre-defined clusters in our synthetic data and evalu-ate the effectiveness of GARDEN HD as a general clustering Table 1: Space reduction on different data sets.

Data Set D ln(  X  ) Q L R Q M Q Figure 7: Space reduction on (a)  X  X ovtype X  and (b)  X  X nimals X  at each level of recursion (log scale). method on both synthetic and real data. These metrics have also been used to evaluate our benchmark for comparison.
The results of GARDEN HD are compared to the results of the default algorithm of CLUTO, a suite of clustering methods for high-dimensional data sets [7, 18]. For a given L C , this algorithm, referred to here as CLUTO RB , derives L C clusters via recursive bisections. Like GARDEN HD , CLUTO RB is optimized for both efficiency and effective-ness, and it operates directly in the given multi-dimensional space. It is a general-purpose method known to be very ef-ficient. These are some of the reasons that we adopted this method as benchmark for comparison. However, like other general-purpose algorithms, CLUTO RB is still not suited for data space reduction, in part because it takes the desired number of clusters as input.

Table 2 gives the results of GARDEN HD and CLUTO RB on  X  X ovtype X  and  X  X nimals X , respectively, for two different inputs. The column L C gives the detected or selected num-ber of clusters for GARDEN HD and CLUTO RB ,respec-tively. The recorded times in seconds represent the averages over 10 runs. The clustering time of each run was measured from the moment data was loaded in memory until the out-put began. This included the computation of the inital base region LR U and the entire process of splitting and merg-ing. For GARDEN HD , the input density thresholds (same as in Table 1) were selected to achieve high purity and low entropy of clusters within the constraints for  X  stated in Section 4.2. On  X  X ovtype X , the selected number of clus-ters for CLUTO RB was 7 (the number of different classes in  X  X ovtype X ) and 84 (the number of clusters detected by GARDEN HD for  X  = e 80 ). On  X  X nimals X , we set this num-Table 2: Clustering on  X  X ovtype X  and  X  X nimals X .

Data Method ln(  X  ) L C Purit Entro Time covt GAR 80 84 0.651 0.389 1.46 covt GAR 100 112 0.652 0.389 1.70 covt CLU -7 0.494 0.583 86.05 covt CLU -84 0.518 0.548 201.4 anim GAR 160 1250 0.763 0.240 2.89 anim GAR 180 6314 1.0 0.0 3.28 anim CLU -4 0.818 0.212 39.30 anim CLU -10 0.908 0.144 75.95 Figure 8: Average clustering times of GARDEN HD and CLUTO RB on synthetic  X  X enter-corners X  data. ber to 4 (the number of classes in the set) and 10 (the input that gives purity above 0.9 in the least amount of time).
The number of clusters L C detected by GARDEN HD varies with  X  because the minimum distance  X  is derived from  X  . Nevertheless, GARDEN HD was not only much faster but generally more effective than CLUTO RB .

For the experiments on synthetic high-dimensional data with  X  X enter-corners X  distribution, the data of every cluster was labeled with its own class, based on which we computed the overall purity and entropy of clusters. For GARDEN HD the selected density threshold  X  was set to the lowest esti-mated density of generated clusters. The input to CLUTO RB was the exact number of clusters (11). Figure 8 compares the average clustering times of GARDEN HD and CLUTO RB as dimensionality and data volume increased. In the experi-ment of Figure 8a, GARDEN HD detected 11 clusters with the overall purity 1 and entropy 0, while CLUTO RB pro-duced lower purities (between 0.839 and 0.53) and higher en-tropies (between 0.14 and 0.447) in a much greater amount of time. Figure 8b gives the average clustering times of the two methods in a 10-dimensional space as the volume of data increased. While GARDEN HD detected all clus-ters correctly, CLUTO RB produced purities and entropies of about 0.84 and 0.14, respectively.
In this paper, we introduced GARDEN HD ,aneffective and scalable algorithm for clustering data in multi-dimensional feature spaces. The technique is a hybrid of cell-based and density-based clustering. However, with its effective tech-nique for data space reduction, it represents a different ap-proach to data clustering that even in spaces with many dimensions can efficiently and accurately detect clusters of virtually any shape and spatial orientation.

Because of its efficiency, scalability, and the ability to ef-fectively isolate areas with points, GARDEN HD can support effective reduction of the search space, facilitating advanced content-based retrieval on clustered data storage [12]. It can also be used to support new ways of storing and accessing massive data on hierarchical storage, like the organization with two levels of data clustering proposed in [11]. However, experimental evidence indicates that GARDEN HD can be effective even as a general-purpose clustering algorithm.
The data space reduction technique of GARDEN HD relies on the properties of a new partitioning strategy, called  X  . Using a density-based recursive application of  X  partitioning, the technique can efficiently detect densely populated areas in the space. The properties of  X  partitioning also enable efficient identification of live regions, which in huge high-dimensional spaces significantly reduce the amount of space that must be inspected.

The proposed data space reduction technique can be used not only to support effective reduction of the search space, but also as a data-reduction mechanism, alternative to (or in conjunction with) dimensionality reduction, compression, and sampling. It produces a concise representation that can provide data-mining applications useful insights into the data and its distribution. Like the agglomeration in GARDEN HD , many data mining processes can be performed both efficiently and effectively on this representation alone.
Our future work includes the adaptation of GARDEN HD for stream data analysis. Similar to STING [16], this tech-nique can be fully automatic, requiring only a single pass over the data. However, unlike STING, the technique would be able to capture the essence of streaming data with not only low but also high dimensionality. Simple adaptations of GARDEN HD would also make it appealing for fast and scalable data classification and outlier detection. This material is based upon work supported by the National Science Foundation under grant no. IIS-0312266. [1] R. Agrawal, J. Gehrke, D. Gunopulos, and [2] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [3] C. Faloutsos and K. Lin. Fastmap: A fast algorithm [4] A. Hinneburg and D. Keim. An efficient approach to [5] A. Hinneburg and D. Keim. Optimal grid-clustering: [6] K. Kailing, H. Kriegel, and P. Kroger.
 [7] G. Karypis. Cluto: A clustering toolkit. Technical [8] J. MacQueen. Some methods for classification and [9] H. Nagesh, S. Goil, and A. Choudhary. A scalable [10] R. Ng and J. Han. Efficient and effective clustering [11] R. Orlandic. Effective management of hierarchical [12] R. Orlandic, Y. Lai, and W. Yee. Clustering [13] R. Orlandic, J. Lukaszuk, and C. Swietlik. The design [14] E. Otoo, A. Shoshani, and S. Hwasng. Clustering high [15] G. Sheikholeslami, S. Chatterjee, and A. Zhang. [16] W. Wang, J. Yang, and R. Muntz. Sting: A statistical [17] T. Zhang, R. Ramakrishnan, and M. Livny. Birch: An [18] Y. Zhao and G. Karypis. Evaluation of hierarchical
