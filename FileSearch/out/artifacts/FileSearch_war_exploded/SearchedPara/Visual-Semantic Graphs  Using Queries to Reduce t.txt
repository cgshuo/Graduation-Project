 We explore the application of a graph representation to model simi-larity relationships that exist among images found on the Web. The resulting similarity-induced graph allows us to model in a unified way different types of content-based similarities, as well as seman-tic relationships. Content-based similarities include different image descriptors, and semantic similarities can include relevance user feedback from search engines. The goal of our representation is to provide an experimental framework for combining apparently un-related metrics into a unique graph structure, which allows us to enhance the results of Web image retrieval. We evaluate our ap-proach by re-ranking Web image search results.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.2.8 [ Database Applications ]: Image Databases Algorithms, Experimentation, Performance Web Image Search, Web Image Re-ranking, Query Log Analysis, Content-based Image Features
A key challenge in Web image retrieval is to efficiently combine the two most important types of image features: visual content-based features and semantic features. This problem poses addi-tional questions such as: which are the correct image descriptors to use in Web image retrieval? and which semantic features are the most appropriate for this task? . Moreover, which combination of visual and semantic features works best? . For example, there are several image descriptors which reflect different image quali-ties based on their contents. Additionally, many semantic features associated to images are not always reliable.

In spite of the existence of general frameworks for different ob-ject similarity integration [11, 10, 3, 9, 8], these do not provide the recipe for successful combination in specific domains. Following this motivation, we present an experimental framework for relat-ing similarity metrics of visual and semantic nature in Web image retrieval. The main contributions of our work are the experimen-tal framework (the visual-semantic graph ), a methodology for as-sessing unbiased Web image relevance based on click-through data from query logs, using time windows, and an evaluation over a large-scale real world data set.
We present two types of similarity graphs that can be used to model relationships among images on the Web: the visual similar-ity graph and the semantic similarity graph . We discuss how to aggregate in a unified manner the information contained by them.
We define a visual similarity graph as an undirected graph that represents content-based similarity relationships in a collection of images. The nodes of this graph correspond to images, and the edges of this graph connect images that are similar (given an image descriptor and a similarity measure  X  ). Each edge has an associ-ated weight: the larger this weight, the more similar the connected images are.

The visual similarity graph is composed of a collection of im-ages and an image descriptor with an associated distance measure (which is used to compute the weights of the edges). While our approach is not restricted to any particular descriptor, we imple-mented three different image descriptors for computing visual sim-ilarity graphs, listed below.  X  Edge Histogram Descriptor (EHD) [6]. It captures the spatial  X  Color Histogram (HSV) . According with the Scalable Color  X  Ordinal Measurement Descriptor (OMD) [1]. It partitions the
For each descriptor, we compute its associated maximum dis-tance M (the largest distance between two images using that de-scriptor). We are only interested in connecting very similar images in the visual similarity graph. Thus, we define a threshold value that indicates us which images must be connected. If the distance between images is smaller or equal than  X  , the nodes associated to those images are linked.

Therefore, let I be the set of images. We define the visual sim-ilarity graph G  X  =( I,E ) , where E is the set of edges of edge ( i, j )  X  E is defined if  X  ( i, j )  X   X  . To each edge we associate a weight w ( i, j )=  X  ( i, j ) that represents the content-based similarity between both images.
We define the semantic similarity graph as an undirected bipar-tite graph that represents semantic-based similarity relationships between a collection of term-sets and a collection of images. The edges in this graph connect term-sets with images that have a se-mantic relationship with them. Each edge has a weight associated to it which is a measure of the relevance of the term-set to the con-nected image. Formally, let T = t 1 ,...,t m be a set of unique term-sets defined by users. Let I be the set of relevant images for term-sets in T . We define the the semantic similarity graph as
G S =( T  X  I,E ) , where E is the set of edges which connect nodes in G S . An edge ( t, i )  X  E exists if at least one image is considered relevant to the term set t  X  T . Each edge has an as-sociated weight w ( t, i ) which corresponds to the importance of to t .

In particular, we consider the click graph as our semantic similar-ity graph. The click graph is a bipartite graph of queries and images which denotes user searching behavior extracted from a search en-gine query log. Edges in this graph connect queries to the images which users selected in their searches. To apply the definition of the semantic graph to the click graph we must make the following considerations:  X  T = Q ,where Q is the set of unique queries submitted to the  X  I corresponds to the set of Web images which have been clicked  X  An edge ( t, i )  X  E exists if at least one user clicked on an  X  The weight w ( t, i ) is defined as the number of unique session
Our selection of the click graph in this case is related to two characteristics which make it appropriate: 1) It gives a measure of relevance of term-sets to images, which is the click frequency, 2) it conveys user-relevance feedback, i.e. users select (click) on images which match their information need.
We define the visual-semantic graph G  X S as the union of the vi-sual similarity graph and the semantic graph. There is an undirected weighted edge between two images i 1 and i 2 of weight w ( i if both images are similar according to the visual similarity graph. There is an undirected weighted edge between a term-set t image i if there is a user defined semantic relationship between and i . The weight of this edge is given by w ( t, i ) .
In this section we describe the random-walk process for the visual-semantic graph. According to the definitions introduced in our prior work [7], a graph of N nodes is described by an N  X  N matrix P of transition probabilities, where an entry P i,j the transition probability between the nodes i and j .Arowvec-tor  X  T represents the stationary distribution over the graph after performing a random-walk process. After k iterations of the pro-cess, the equation  X  ( k ) T =  X  ( k  X  1) T  X  P is satisfied, where and  X  ( k  X  1) T represents the vector  X  T calculated at iterations k  X  1 , respectively. Under certain conditions of the process (irre-ducibility, finiteness and aperiodicity) the vector  X  ( k ) T to  X  T . Then, the i -th coordinate of  X  T corresponds to the frequency with which a random surfer visits the i -th node of the graph during the process.
 Random-walk on a visual similarity graph. In this process a user begins its image viewing process by selecting a random image from the collection. After viewing this image the user uses it to select a second image to view, selection which is biased by the degree of similarity between the first image and the second ( w ( i, j )=  X  ( i, j ) ). This process is repeated iteratively until the user begins a new search from a different image with probability (1  X   X  )
Let A  X  be the adjacency matrix of the visual graph G  X  ,inwhich the entry ( i, j ) has the value of w ( i, j )=  X  ( i, j ) row-normalized version of A  X  . The transition-probability similarity graph is given by: where  X  is the dumping factor of the process and 1 is a matrix that has the value 1 N in all its entries (teleportation matrix). Random-walk on the semantic graph. This process corresponds to a random-walk on an undirected bipartite graph T  X  I ,where represents the query terms and I the set of clicked images. Let be the M  X  N adjacency matrix of the semantic graph, whose rows correspond to the terms of T and the N columns corresponds to the images of I . Each entry ( t, i ) in this matrix has a value w ( t, i ) , which corresponds to the query-to-image click frequency found in the query log. The transpose, denoted by A T S , models the fact that it is possible to go back from an image to a query. represents the ( N + M )  X  ( N + M ) adjacency matrix that considers both situations: Let N S be the row normalized version of A S . Then the random-walk process on the semantic graph is given by: Random-walk on the visual-semantic graph. We combine both graphs performing a convex union: where  X  is the probability of the user choosing a text-based image retrieval system, as opposed to the content-based image system. Then, the random-walk process over the visual-semantic graph is defined as follows:
Haveliwala and Kamvar [4] showed that the convergence of the random-walk process depends on the second eigenvalue of the tran-sition matrix which corresponds to the dumping factor. When increases, the convergence rate decreases. They showed also that a good balance between the convergence rate and the forced behav-ior introduced by the teleportation is achieved when  X  =0 . 85 effect of the choice of  X  will be evaluated in the following section.
We present an experimental evaluation of our approach over a large-scale dataset. We evaluate by re-ranking search results at query level. We re-rank using the stationary distribution scores ob-tained for random-walks on the visual-semantic graph. The goal of our evaluation is to find a combination of visual similarity and se-mantic graphs that provide additional information than either graph on its own.
 Dataset. The query log used in the evaluation was obtained from Yahoo! image search. Experiments were performed over a two-weeks period, from March 1st 2010 to March 13 2010. We con-sider the first week to build the dataset and the second week for evaluation purposes. Each week contains approximately 7 million unique images, with a 4.4 million images repeated in both weeks. Overall, each week registered around 11.2 million unique-session clicks on images. Additionally, 2.7 million queries where repeated in the first and the second week of data. We used these queries for the re-ranking experiment.
Originally, the image collection was intended to include all of the clicked images in the query log. Nevertheless, due to the com-plexity involved in the generation of the visual similarity graph, for our evaluation we select a random sample of these images ( the original collection). Therefore, we generate the visual similar-ity graph over this reduced image set. This optimization still allows us to validate our proposal.

On the other hand, we keep all of the click-through information, considering all of the clicked images and queries in the log to build the semantic similarity graph (in this case, the click graph). Visual similarity graph generation. We create the visual simi-larity graph using an incremental algorithm that uses a pivot-based approach [12] to find similar images. We use Sparse Spatial Se-lection (SSS) index structure [2] to compute efficiently the pairs of similar images. The algorithm is as follows: ii. Compute a range query using the index. This finds all in-iii. Add the correspondent links to the graph (from u to the nodes iv. After all images have been processed, the last step is the
Although in the worst case the computation time for computing the similarity graph using the proposed algorithm is O ( n 2 ) brute force algorithm), in practice we obtained large reductions in the processing time (except for OMD), as Table 1 shows. Table 1: Time (in hours) needed to compute the similarity graph for each image descriptor.
 Semantic graph generation. The nodes of the click graph are all of the unique queries and all of the images recorded in the query log. We consider only queries which register at least one click on an image. The weight in an edge ( t, i ) is the number of clicks from different sessions from the query t to the image i .
In this section we evaluate if different descriptors provide differ-ent amounts of information for the Web image retrieval task. Fur-thermore, we also evaluate how different values of  X  affect the re-sults of the visual-semantic graph. To do this, we use the stationary distribution scores of each combination (using the different image descriptors and  X  values) to re-rank search results. In this stage, it is important to note that we use two datasets, one for comput-ing the visual-similarity graphs (1st week in the query log) and the next time window for computing our gold standard or ideal rank . Therefore we are evaluating how well our approach performs with new data (which was not used for  X  X raining X ). Equally important is the fact that we generate  X  X lobal X  stationary distribution scores, which are then used to generate  X  X ocal X  re-ranking (at query level). We use the re-ranking induced by the click graph as our baseline (  X  =1 ).
 Figure 1 shows that the original ranking induces a click-bias. Positions farther down the list of responses consistently concen-trate fewer clicks than the first positions. Therefore, an interesting observation is that the click-bias is the same as would be expected if image results were displayed in a vertical listing, instead of a matrix-like interface.

To avoid the effect of the click-bias for our evaluation purposes, we calculate the fraction of clicks that each image concentrates over the total number of clicks related to a given query. Figure 2 shows that using this measure (instead of clicks), we significantly decrease the image click-bias that existed for each query.

To evaluate the perform of each descriptor we compute the Nor-malized Discounted Cumulative Gain measure (NDCG for short) [5] which consider an explicit position discount factor in its def-inition. We calculate this measure is calculated at query level. Then, the values obtained from the NDCG measure are are aver-aged across the queries at each rank value.

We combine each visual graphs with the semantic graph (click graph) using  X  =0 , 0 . 25 , 0 . 50 , 0 . 75 , 1 . It should be noted that a value of  X  =0 creates a graph with only the visual similarity graph, and that  X  =1 corresponds to using only the click graph. We obtain 15 possible visual-semantic graphs for evaluation. The results are displayed in Table 2. Figure 2: Relevance distribution per position (top-20 results).
Table 2 shows that the best combination with the click graph is reached when we use  X  =0 . 5 . This is an important result because it allows us to claim that the combination of the visual similarity graph and the semantic graph works better than either graph on its own. In fact, we can observe that the combination of the click graph(shownfor  X  =1 ) with any visual similarity graph improves its results. The best results are obtained when the click graph is combined with the OMD(0) graph, showing that the unification of the best graphs produces also the best combination. Moreover, the combination of the click graph and the OMD(0) graph improves the re-ranking induced by the click graph by more than 5%.
We have presented a new type of graph that combines visual and semantic characteristics that are useful for web image retrieval. Performing a random-walk process over this graph and using the steady-state probability distribution as scores for image re-ranking, our experiments show that it is possible to improve over 5% a base-line. We have also shown that not all combinations of visual fea-Table 2: NDCG results for combination of the visual-semantic graphs. Bold fonts indicate best results. tures are useful, illustrating that only one of them is recommend-able for web image retrieval.

Currently we are working on new combinations of visual de-scriptors with semantic graphs, defining new strategies to optimize the combination. We are also exploring scalable-methods to in-crease the number of nodes used in our graphs.
