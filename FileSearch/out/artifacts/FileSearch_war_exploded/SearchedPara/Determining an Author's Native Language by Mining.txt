 In this paper, we show that stylistic text features can be exploited to determine an anonymous author's native language with high accuracy. Specifically, we first use automatic tools to ascertain frequencies of various stylistic idiosyncrasies in a text. These frequencies then serve as features for support vector machines that learn to classify texts according to author native language. I.2.6 [ Artificial Intelligence ]: Learning  X  Analogies, Concept learning, Connectionism and neural nets, Induction, Knowledge acquisition, Language acquisition, Parameter learning Algorithms, Measurement, Experimentation Text mining, author profiling Stylistic analysis of a text might offer hints towards a psychological or demographic profiling of the text X  X  author. For example, it has already been shown that automated text analysis methods can be used to identify an anonymous author X  X  gender with accuracy above 80% [1]. In this paper, we will show that stylistic idiosyncrasies can be used to identify the native language of the author of a given English language text. Writers X  spelling, grammar and usage in a second language are often influenced by patterns in their native language [2] [3]. Thus, it is plausible that certain writing patterns  X  function word selection, syntax and errors  X  might be particularly prevalent for native speakers of a given language. Some work [4] has been done on categorizing transcripts of English speech utterances as by native or non-native English speakers. In our experiments, we know that the writer is not a native English speaker but we wish to determine which language is native to the author. We consider written text, which offers the benefit of grammar and spelling cues, but loses the benefit of mispronunciation cues. To the best of our knowledge, this is the first published work on the automated determination of author native language from written text. Identifying an author X  X  native language is a type of authorship attribution problem. Instead of identifying a particular author from among a closed list of suspects, we wish to identify an author class, namely, those authors who share a particular native language. Researchers in authorship attribution typically seek the kinds of features use of which is roughly invariant for a given author (or author class) across topics but which might vary from one author (or author class) to another. Generally, researchers use feature sets that are relatively common. Thus, for example, the seminal authorship attribution work of Mosteller and Wallace [5] on the Federalist Papers used a set of several hundred function words, that is, words that are context-independent and hence unlikely to be biased towards specific topics. Other features used in even earlier work [6] are complexity-based: average sentence length, average word length, type/token ratio and so forth. Recent technical advances in automated parsing and part-of-speech (POS) tagging have facilitated the use of syntactic and quasi-syntactic features such as POS n-grams [7] [8] [9] [10]. Other recent work [11] considers language modeling using letter n-grams. However, human experts working on real-life authorship attribution problems do not work this way. They typically seek idiosyncratic usage by a given author that serves as a unique fingerprint of that author. For example, Foster [12] describes his techniques for identifying a variety of notorious anonymous authors including the author of the novel, Primary Colors, and the Unabomber. These techniques include repeated use of particular types of neologisms or unusual word usage. Significantly, Foster identifies these linguistic idiosyncrasies manually. In the case of unedited texts, spelling and grammatical errors, which are typically eliminated in the editing process, can be exploited as well. might be helpful for determining an author X  X  native language. Very crudely, we can break these feature types into three broad categories. 1. Function words  X  As noted above, function words 2. Letter n-grams  X  As noted, letter n-grams have 3. Errors and Idiosyncrasies  X  As noted, errors and Flagging of various types of writing errors has been used in a number of applications including teaching English as a foreign language (EFL) [13] [14] student essay grading [15] and, of course, word processing. The approaches used are either partially manual or do not flag the types of errors relevant to our task. Consequently, we develop our own automated methods for error-tagging. in tagging. After that we will show how to automate the tagging process. The error types we consider fall into the following four categories: 1. Orthography  X  We consider here a range of spelling The first six of these represents multiple error types since the specific letter(s) are specified as part of the error. For example, a  X  X etter  X  instead of  X   X  represents 26*25/2 = 325 separate error types. We will see below though that most of these occur so infrequently that we can consider only a small subset of them. It should be emphasized that we use the term "error" or "idiosyncrasy" to refer to non-standard usage or orthography in U.S. English, even though often such usage or orthography simply reflects different cultural traditions or deliberate author choice. 2. Syntax  X  We consider non-standard usage as Our system supports these error types for use in a variety of applications not considered in this paper. These errors are not appropriate for the native language problem we consider here, so we do not use them in the experiments reported below. 3. Neologisms  X  In order to leverage an observation of Foster 4. Parts-of-speech bigrams  X  We consider 250 rare POS based upon deeper linguistic analysis of the text, besides those that were presented in the previous section. However, we restrict ourselves here to those considered above because they can be identified with relative ease, as we now show. Thus, for most of the error types in categories 1 and 2 above, we use the following procedure: We run a text through the MS-Word application and its embedded spelling and grammar checker. Each error found in the text by the spell checker is recorded along with the best suggestion (to correct the error) suggested by the checker. Each pair &lt;error, suggestion&gt; is then processed by another program, which assigns it an  X  X rror type X  from among those in the list we constructed. Obviously, automated spelling and grammar checkers are far from perfect: certainly, suggested corrections may not reflect an author X  X  intention. Nevertheless, since we are not interested in any frequencies, such automated checkers are adequate for our purpose. Still, for certain classes of errors we found MSWord's spell and grammar checker to be especially inadequate, so we prepared scripts ourselves for capturing them. In particular, we found that MSWord X  X  spell checker was very weak at handling non-standard words with grammatical suffixes ( -ism, -ist, -ble, -ive, -logy, -tion , etc.) For categories 3 and 4, we run a text through the Brill [17] tagger. For category 4, we juxtapose results from MSWord X  X  spelling checker (and our own routines for words with identifiable grammatical suffixes) with results of the Brill tagger. When we ran our entire corpus of flawed texts through this process, we found that many error types on our list are so infrequent as to not be worth considering. Consequently, we reduced our list of error types to only those 185 types that occurred at least three times in a large corpus of chat group posts used for gathering error statistics (in addition to the 250 rare part-of-speech bigrams). We use the International Corpus of Learner English [18], which was assembled for the precise purpose of studying the English writing of non-native English speakers from a variety of countries. All the writers included in the corpus are university students (mostly in their third or fourth year) studying English as a second language. All are roughly the same age (in their twenties) and are assigned to the same proficiency level in English. We consider sub-corpora contributed from Russia, Czech Re public, Bulgaria, France and Spain. The Czech sub-corpus, consisting of essays by 258 authors, is the smallest of these, so we take exactly 258 authors from each sub-corpus (ra ndomly discarding the surplus). Each of the texts in the collection is of length between 579-846 words.
 Each document in the corpus is represented as a numerical vector of length 1035, where each vector entry represents the frequency (relative to document length) of a given feature in the document. The features are: We use multi-class linear support vector machines (SVM) [19] to learn models for distinguishing vectors belonging to each of the five classes. The efficacy of linear SVMs for text categorization is already well attested [20]. In order to test the effectiveness of models learned by SVMs to properly categorize unseen documents, we ran ten-fold cross-validation experiments: the corpus was divided randomly into ten sets of (approximately) equal size, nine of which were used for training and the tenth of which was used for testing the trained model. This was repeated ten times with each set being held out for testing exactly once. In Figure 1, we show accuracy results of ten-fold cross-validation experiments for various combinations of feature classes. As can be accuracy of 80.2%. The confusion matrix for the experiment with all features is shown in Table 1. It should be noted that a document is only regarded as being correctly classed if it is assigned to its correct class and to no other class. Thus, since we have five possible classes of roughly equal size, 20% accuracy is a reasonable baseline for this experiment. 
Figure 1 Accuracy (y-axis) on ten-fold cross-validation using various feature sets (x-axis) without (diagonal lines) and with (white) errors, and classifying with multi-class linear SVM. Note that  X  X rrors X  refers to all error types, including rare 
Table 1Confusion matrix for the author X  X  native language The success of the system depends on the interaction of hundreds features that proved particularly helpful in the learned models. Table 2 shows a variety of features along with the number of documents in each category in which the feature appears. We find that a number of distinctive patterns were exploitable for identifying native speakers of particular languages. For example:  X  Some features that appear in more documents in the  X  A relatively large number of authors in the Spanish  X  A relatively high number of authors of documents in the  X  Documents in the French corpus are characterized by  X  Authors in the Russian corpus are more prone to use the The above examples are all features that distinguish one language corpus from all the rest. Of course there are many features that distinguish some subset of languages from the others. For example, the frequency of the word the is significantly less frequent in the documents by Czech (47.0 per 1000 words), Russian (50.1) and Bulgarian (52.3) authors than in those by French (63.9) and Spanish (61.4) authors. (Russian and Czech suffix.) Unsurprisingly, as can be seen in Table 1, most mistakes were among the three Slavic languages (Russian, Czech, Bulgarian). 
Table 2 A selection of features and the number of documents We have implemented a fully automated method for determining the native language of an anonymous author. In experiments on a corpus including authors from five different countries, our method achieved accuracy of above 80% in categorizing unseen documents. The authors of these documents were generally reasonably proficient in English (and may have even used automated spell-checkers), which made the task particularly difficult. It may be, however, that we were able to take unfair advantage of differences in overall proficiency among the different sub-corpora. For example, the Bulgarian authors were on average considerably less prone to errors than the Spanish authors. One way to ensure robustness against such artifacts of the available data would be to run similar experiments in which error frequency is normalized not against document length but rather against overall error frequency. The applicability of these methods depends on a number of factors. Is the method precise enough to handle tens if not hundreds of different candidate native languages? How short can the documents be and still permit accurate categorization? Each of these questions requires further investigation. [1] Koppel, M., S. Argamon, A. Shimony. Automatically [2] Lado,R. Linguistics Across Cultures, Ann Arbor: (1961) [3] Corder, S. P. Error Analysis and Interlanguage. (1981) [4] Tomokiyo, L.M. and R. Jones. "You're Not From [5] Mosteller, F. and Wallace, D. L. Inference and Disputed [6] Yule, G.U. 1938. On sentence length as a statistical [7] Baayen, H., H. van Halteren, F. Tweedie, Outside the [8] Argamon-Engelson, S., M. Koppel, G. Avneri. Style-[9] Stamatatos, E., N. Fakotakis &amp; G. Kokkinakis, [10] Koppel, M., J. Schler. Exploiting Stylistic [11] Peng, F., D. Schuurmans, S. Wang. Augmenting Naive [12] Foster, D. Author Unknown: On the Trail of [13] Dagneaux, E., Denness, S. &amp; Granger, S. Computer-[14] Tono, Y., Kaneko, T., Isahara, H., Saiga, T. and Izumi, [15] Chodorow, M. and C. L eacock. An unsupervised 
