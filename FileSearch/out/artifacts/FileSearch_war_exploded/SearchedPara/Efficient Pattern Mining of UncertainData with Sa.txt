 In frequent itemset mining, the transaction dataset is typically represented as a binary matrix where each line represent satransactionand every column cor-responds to an item. An element M ij represents the presence or the absence of the item j in transaction i by the value 1 or 0 respectively. For this basic traditional model, where an item is eith er present or absen tinatransaction, many algorithms have been proposed for mining frequent itemsets; i.e., sets of columns of M that have all ones in at least a given number of transactions (see e.g. [5] for an overview on fr equent itemset mining).

In many applications, however, an ite m is not present or absent in a trans-action, but rather an existence probability of being in the transaction is given. This is the case, for example, for data co llected from experimental measurements or from noisy sensors. Mining frequent patterns from this kind of data is more difficult than mining from traditional transaction datasets. After all, computing the support of an itemset now has to rely on the existence probabilities of the items, which leads to an expected support as introduced by Chui et al. [4].
If the binary matrix is transformed into a probabilistic matrix, where each element takes values in the interval [0 , 1], we have the so called uncertain data model. Under the assumption of statistical independence of the items in all transactions in the dataset, the support of an itemset in this model, as defined by Chui et al. [4], is based on the possible world interpretation of uncertain data. Basically, for every item x and every transaction t there exist two sets of possible worlds, one with the worlds in which x is present in t and one with the worlds where x is not present in t . The probability of the first set of worlds is given by the existence probability of x in t ( P ( x, t )) and the probability of the second set of worlds by 1  X  P ( x, t ). The probability of a single world is then obtained by multiplying the probabilities for all its individual items; i.e., P ( W )= be obtained by summing the support of that itemset over all possible worlds, while taking into consideration the probability of each world. There exist 2 | D | X | I | worlds, where | D | is the total number of transactions in the probabilistic dataset and | I | is the total number of items. This rather complicated formula can be reduced to: Every transaction thus supports an itemset with the probability given by the product of existence probabilities of all items in the itemset and in that trans-action. The expected support of an itemset over the entire dataset is the sum of the existence probabilities of that itemset in every transaction of the dataset.
In the remainder of this paper we revisit the related work, then we present our proposed method based on sampling, followed by theoretical and empirical analysis of the quality of the results. The efficient data structures and techni ques used in frequent itemset mining such as TID-lists [2], FP-tree, which adopts a prefix tree structure as used in FP-growth [6], and the hyper-linked array based structure as used in H-mine [8] can no longer be used as such directly on the uncertain data. Therefore, recent work on frequent itemset mining in uncertain data that inherits the breadth-first and depth-first approaches from traditional frequent itemset mining adapts the data structures to the probabilistic model.

U-Apriori [4] is based on a level wise algorithm and represents a baseline algorithm for mining frequent itemsets f rom uncertain datasets. Because of the generate and test strategy, level by l evel, the method does not scale well.
UCP-Apriori [3] is based on the decremental pruning technique which con-sists in maintaining an upper bound of the support and decrementing it while scanning the dataset. The itemset is pruned as soon as its most optimistic value falls below the threshold. This approach represents the state of the art for mining frequent patterns from uncertain data with a generate-and-prune strategy.
UF-growth [7] extends the FP-Growth algorithm [6]. It is based on a UF-tree data structure (similar to FP-tree). The difference with the construction of a FP-tree is that a transaction is merged with a child only if the same item and the same expected support exist in the transaction and in the child node, leading to a far lower compression ratio as in the original FP-tree. The improvements consist in discretization of the expected suppor t to avoid the huge number of different values and in adapting the idea of co-o ccurrence frequent itemset tree (COFI-tree). The UF-trees are built only for the first two levels. It then enumerates the frequent patterns by traversing the tr ee and decreasing the occurrence counts.
Aggarwal et al. [1] extended several existing classical frequent itemset mining algorithms for deterministic data sets, and compared their relative performance in terms of efficiency and memory usage. The UH-mine algorithm, proposed in their paper, provides the best trade-offs. The algorithm is based on the pattern growth paradigm. The main difference with UF-growth is the data structure used which is a hyperlinked array.

The limitations of these existing methods are the ones inherited from the original methods. The size of the data for the level-wise generate-and-test tech-niques affects their scalability and the pattern-growth techniques require a lot of memory for accommodating the dataset in the data structures, such as the FP-tree, especially when the transactions do not share many items. In the case of uncertain data, not only the items have to be shared for a better compression but also the existence probabilities, which is often not the case. The first method we propose, called Concatenating the Samples , takes the un-certain dataset and samples according to the given existential probabilities. For every transaction t and every item i in transaction t we generate an independent random number 0  X  r  X  1 (coin flip) and we compare it with the probability p associated with the item i .If p  X  r then item i will appear in the currently sampled transaction. For every transac tion in the uncertain dataset we repeat the step above n times, for a given n . The result is a dataset which can be mined with any traditional frequent itemset mining algorithm. To obtain the estimated support of an itemset in the uncertain dataset, its support in the sampled dataset still needs to be divided by n .

The difficulty of this method resides in the fact that we physically instantiate and store the sampled  X  X ertain X  dataset which can be up to n times larger than the original uncertain dataset. Fortunately, for most efficient itemset mining al-gorithms, we do not actually have to materialize this samples database. After all, most efficient techniques read the dat abase from disk only once, after which their advanced data structures contain t he database in the main memory. There-fore, the sample can be generated imme diately in memory when the database is being read from disk for the first time. We call this method Inline Sampling .
To this end, we made minor modifications of the frequent itemset mining algorithms. We will briefly describe U-Eclat and UFP-growth , the modified versions of the ECLAT and FP-growth algorithms.
 U-Eclat is an adaptation of the ECLAT algorithm [11] with an improvement based on diffsets as described in [10]. In only one scan of the dataset the relevant items are stored into memory together with the list of transactions where the items appear, called tid-list. The candidates are then generated using a depth-first search strategy and their support is computed by intersecting the tid-list of the subsets. The only adaptation for U-Eclat consists in reading the uncertain transactions and instantiating them as described above. More specifically, given the number of iterations n , for every transaction t and every item i in transaction t we generate n independent random numbers r 1 ,...,r n betweeen 0 and 1 and we compare them with the probability p associated with the item i .If p  X  r j , for 1  X  j  X  n ,then n  X  t + j will appear in the tid-list of item i . From there on, the standard Eclat algorithm is being executed.
 UFP-growth extends the initial FP-growth algorithm [6]. The FP-tree con-struction needs two scans of the dataset. Th e first scan collects the frequent items and their support and in the second scan every transaction is accommodated in the FP-tree structure. The frequent items ets are then generated recursively from the FP-tree. In order to adapt this algorithm to our method, the first scan com-putes the expected support of every itemse t exactly by computing their support as the sum of existential probabilities in every transaction where it occurs. In the second scan, every transaction is instantiated n times, according to the ex-istential probability of the items in the transaction and then it is inserted in FP-tree structure. The algorithm then extracts the frequent itemsets the same way as the FP-growth algorithm. As before, D denotes the set of transactions, and I the set of items. P ( x, t ) denotes the probability assigned to item x by transaction t .Weextendthis notation to itemsets X ; i.e., P ( X, t ) will denote x  X  X P ( x, t ). Our whole analysis will be based on the numbers P ( X, t ) only and hence, will not depend on the assumption of independence between the items. Notice that this implies that our sampling-based method, in contrast to the other existing proposals, could also be applied when a more involved probabilistic model is assumed. We first start our analysis for a single itemset X and will extend it later on for the complete collection of itemsets.

Suppose that, for every transaction t  X  D ,wesample n deterministic versions X t =1if X tically independent as they are sampled using independent coin flips. X i t follows a Bernoulli distribution with mean P ( X, t ). It is easy to see that the stochastic variable X t = n i =1 X i t follows a binomial distribution with mean nP ( X, t )and variance nP ( X, t )(1  X  P ( X, t )). Consider now the sum: S ( X ):= t  X  D X t n The expected value and variance of this sum are as follows: E [ S ]= expSup ( X ) V [ S ]= V t  X  D Hence, not surprisingly, the sum S we use to approximate the expected support is an unbiased estimator with a vari ance that decreases linearly with n .Forthe
We now apply Hoeffding X  X  inequality. This inequality is as follows: given independent (but not necessarily identically distributed) stochastic variables X 1 ,...,X m such that for all i =1 ...n , P ( a i In our case, for all X i t , X i t  X  E ( X i t )isintheinterval[  X  1 , 1], and hence we get: If we now rewrite in function of rS ( X )and rsupp ( X ):= expSup ( X ) | D | ,weget: then p [ | rS ( X )  X  rsupp ( X ) | X  ]  X  1  X   X .
 The significance of this result can best be illustrated by an example. Suppose D contains 100 000 probabilistic transactions and X is an itemset. In order to guarantee that the support of X is approximated with 99% probability with approximately 1 sample per transaction in D to achieve this result. Furthermore, suppose that we have a collection of 1 000 000 frequent itemsets. In order to guarantee that all these itemsets have less than 1% error with 99% probability, than 4 samples per transaction.

As a side note, even tighter bounds can be gotten by approximating the distri-bution of rS with a normal distribution, using a weaker form of the Central Limit Theorem, called Lyapunov X  X  central limit theorem .Thatis, S  X  supp ( X ) nV [ S ] converges in probability to N (0 , 1). The experiments were conducted on a GNU/Linux machine with a 2.1GHz CPU with 2 Gb of main memory. We used the dat asets and the executables for com-parison from [1]. Kosarak contains anonymized click-stream data. It is a very sparse dataset, with a density of less than 0 . 02%, about 1 million transactions, 42170 distinct items and an average of 8 item per transaction. The dataset T40I10D100K was generated using the IBM synthetic data generator, having 100K transactions, 942 distinct items and a density of 4 . 2%. The original datasets were transformed by Aggarwal et al. [1] into uncertain datasets by assigning to every item in every transaction existential probabilities according to the nor-mal distribution N (  X ,  X  2 ), where  X  and  X  were randomly and independently generated with values between [0 . 87 , 0 . 99] and [1 / 21 , 1 / 12] respectively. For different values of the minimum support, we ran our implementations of U-Eclat and UFP-growth . The number of times we instantiate the uncertain dataset varies between 1 and 50. The higher the number of instantiations, the better the accuracy of the results becomes, at the cost of an increase in exe-cution time. We also experimented with the original ECLAT and FP-growth algorithms after materializing the sampled datasets. Obviously the size of these datasets become very large for multiple iterations, and thus, those experiments always resulted in a decrease in performance as compared to their inline versions. Experimentally we show that for relatively low number of instantiations we reach highly accurate results. The gain in time motivates the use of our method which outperforms in execution time the existing state of the art methods mentioned in [1]. For every dataset, we plot the execution times we obtained for different values of the minimum support and for some different numbers of iterations. It turns out that U-Eclat always outperformed UFP-growth . In many cases, the FP-tree simply became too large to handle [5]. In the experiments, for clarity, we thus only show the results for U-Eclat . For a fair comparison we also only show the best performing implementations of the algorithms mentioned in [1], being UCP-Apriori and UH-mine. The execution times are depicted in Figures 1(a) and 2(a).

For low support threshold, our U-Eclat outperforms UCP-Aprori and UH-mine for up to 5 sampling iterations of the dataset. Note that more efficient frequent set mining algorithms as can be found in the FIMI repository will per-form even better, also for a higher number of iterations. As our th eoretical results already indicated that 2 iterations already result in a very accurate approxima-tion of the expected supports of all itemsets. This also shows in practice.
To this end, we compare the collections of frequent patterns and their support obtained using the exact method and our sampling method. First, the collection of frequent itemsets is generated using UCP-Aprori [1]. Based on this, we eval-uate the errors in support computed with the sampling method.

In terms of support error, we compute the average of the absolute difference between the support of itemsets found by both methods. The error is depicted in Figures 1(b) and 2(b). It can be s een that, as expected and predicted by the statistical evaluation, the higher the number of iterations grows, the lower the error becomes. But even for relatively low number (5 or 10 iterations), the average error in support estimation drops below 1%.

For itemsets having the support close to the minimum support threshold, small variations of support can introduce false positives when the real support is overestimated or false negatives when the real support is underestimated. To evaluate the impact of this, we report Precision and Recall of our method w.r.t. the true collection in terms of patterns found as frequent. We plot in Figures 1(c), 1(d), 2(c) and 2(d) the values of precision and recall for different number of iterations. A summary of these values is reported in Figure 3 as the overall minimum, average and maximum for each dataset and different numbers of iterations. The values confirm the quality of the approximation.
 Acknowledgements. We thank to the authors of Aggarwal et al. [1] for making available the executables and datasets. This research was partially founded by FWO project  X  X oundations for Inductive Databases X .

