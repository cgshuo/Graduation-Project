 Sentiment classification has unde rgone significant development in recent years. However, most existing studies assume the balance between negative and positive samples, which may not be true in reality. In this paper, we investigate imbalanced sentiment classification instead. In particular, a novel clustering-based stratified under-sampli ng framework and a centroid-directed smoothing strategy are proposed to address the imbalanced class and feature di stribution problems respectively. Evaluation across different datase ts shows the effectiveness of both the under-sampling framework and the smoothing strategy in handling the imbalanced problems in real sentiment classification applications. I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  Text analysis Algorithms, Experimentation Opinion Mining, Sentiment Cl assification, Imbalanced Classification Sentiment classification aims to predict sentiment polarity of a text [8] and it plays a critical role in many NLP applications However, most existing studies on sentiment classification assume the balance between the numbers of positive and negative samples, which may not hold in practice. Actually, many sentiment classification a pplications involve imbalanced class distributions in that the sa mple number of one class in the training data is much larger than the other class. We call this specific kind of sentiment classification as imbalanced sentiment classification, in whic h the class with a larger amount of samples is referred to as majority class and the other class with a smaller amount of samples is referred to as minority class . In fact, imbalanced classification has been proven challenging in the machine learning research community [4]. Many approaches have been proposed to deal with the imbalanced class distribution problem, such as re-sampling [2], one-class Unfortunately, none of the above approaches can be readily applied to imbalanced sentiment classification due to its specific characteristics. In imbalanced classification, majority class normally contains more kinds of occurring features than minority class . For simplicity, we refer to this phe nomenon as imbalanced feature distribution. Such phenomenon becomes worse in imbalanced sentiment classification since sentiment classification often involves a small number of positiv e and negative samples. It further worsens due to the sparseness of effective sentimental features. On one hand, sentiment classification faces the same challenge of high feature dimensi on as text categorization. On the other hand, the effective sentimental features in a sample are rather rare in sentiment classification, considering infrequent occurrence of sentimental words in text. For example, while the feature dimension of a typical sentiment classifier may be up to tens of thousands, there are only dozens of effective sentimental features (e.g., sentimental words) in a sample. The imbalanced feature distribu tion problem can cause severe problems in the training proce ss of imbalanced sentiment classification. Normally, the features that merely occur in the majority class (not occurring in minority class , called majority unique features) can be a strong distinguishing clue in the classifier. Nevertheless, consider ing that the number of effective sentimental features (e.g., sen timental words) is significantly fewer than that of other features (e.g., those words about facts) in sentiment classification, most of the majority unique features will contribute abnormally. As a result, if we use all the training samples to train a classifier, the classifier will have a strong tendency to wrongly predict a sample from the minority class as the majority class . This indicates the nece ssity of dealing with the imbalanced feature distri bution problem in imbalanced sentiment classification. In this paper, we propose a clustering-based stratified under-sampling framework to overc ome the imbalanced class distribution problem in imbalan ced sentiment classification. Under this framework, the samples in the majority class are first grouped into several clusters and then a suitable number of samples are selected from each cluster to form the training samples of majority class . The intuition is that these selected samples using the stratified under-sampling framework should be more representative than those by random selection. Moreover, a centroid-directed smoothing strategy is proposed to overcome the imbalanced feat ure distribution problem by linearly interpolating a sample with the centroid of the cluster to which this sample belongs. Sin ce the centroid represents the average feature distribution of all occurring features in the cluster, our smoothing strategy can greatly increase the sample robustness and reduce its feature sparseness. Early studies on sentiment cl assification mainly focus on unsupervised learning methods , which build a sentiment classifier without any labeled data. In such methods, the relationship between two words (e.g., a seed word and any other word) is usually first extracted from some knowledge resources, such as WordNet and unlabeled da ta. Then, such relationship is used to compute the semantic orie ntation of a word or even the sentiment polarity of a text [12] . In general, the performance of unsupervised learning methods is too low to meet the requirements of real applications. Compared to unsupervised leani ng, supervised learning methods often perform much better due to the availability of labeled data and become more popular since the pioneer work on sentiment classification by Pang et al. [8]. In particular, various kinds of information have been explored to improve the bag-of-words model [5][6][11]. Unfortunately, the performance of a supervised learning method drops dramatically when adapted to a new domain. This arouses wide interests on the research of domain adaptation in sentiment classification [1]. Besides domain adaptation, the imbalanced class distribution problem is another major reason which hurts the wide application of sentiment classification. To the best of our knowledge, our work is the first study on imbalanced sentiment classification. Just as described in the introduction, imbalanced feature distribution in imbalanced sentim ent classification is much due to the conflict between the high feature dimension problem (the high number of possible features in sentiment classification) and the feature sparseness problem (infrequent occurrence of sentimental words in a sample). Such imbalance in the feature distribution becomes even worse due to the imbalanced class distribution since the number of occurring features in the minority class would be much fewer than that in the majority class . To have a better understanding of the imbalanced feature distribution phenomenon in imbalanced sentiment classification, Table 2 gives the statistics over two typical domains on the number of features occurring in the positive and negative classes, denoted as n  X  and n  X  respectively, with the ratios of 
Table 1: Feature distributions on the number of occurring As a popular sampling method in statistics, stratified sampling first groups the members of a population into a few relatively homogeneous subgroups (i.e. strata) according to one certain property and then selects samples from each stratum. It is believed that stratified sampling is able to select better samples to represent the distribution of the whole dataset. Previous work justifies its effectiveness theoretically and empirically in both general applications [7] and speci fic NLP applications such as semantic relation extraction between named entities [9 ][ 10]. The basic motivation of our us ing clustering-based stratified sampling is to select some "representative" samples from the  X  X epresentative X  samples is selected from the minority class . Therefore, our sampling approach is basically a non-random under-sampling approach. The reason why we adopt under-sampling instead of over-sampling is basically due to its better performance. Please refer to Figur e 2 in Section 6.2 for details. Clustering groups the samples in the majority class into several strata. Considering that the stra ta may be skewed, the number of selected samples from each cluster is tuned according to the size of each stratum. Given M A N samples in the majority class and 
N samples in the minority class , the number of samples selected from the i-th stratum i S should be || MI ii Input: The training data and the number of strata being clustered, denoted as K Output: Balanced training data Algorithm: Given the strata and the number of selected samples from each stratum, a natural question arises as to how to select the samples from each stratum. This can be viewed as intra-stratum sampling, which chooses a cer tain amount of samples from inside individual stratum [10]. In particular, we employ a diversity-motivated scheme to perform intra-stratum sampling with the objective to maximize the training utility of all the samples from a stratum. That is, those samples with high variance to each other are preferred, avoiding similar samples from a stra tum. In particular, we first select a random candidate sample and then exclude its nearest two samples. This process repeats until enough samples are obtained. Figure 1 illustrates th e clustering-based stratified under-sampling framework with intra-stratum sampling. In this paper, a centroid-direct ed smoothing strategy is proposed to alleviate the imbalanced f eature distribution problem in imbalanced sentiment classification. Although under-sampling can balance both class and feature distributions by elimina ting many samples from the majority class to keep the balance between positive and negative samples, a lot of majority class features are excluded. Without these excluded features, the selected samples may not be able to well represent the feature distribution of the whole dataset in the majority class , even when the clustering-based stratified under-sampling is used. The centroid-directed smoothing strategy merges the feature vector of each sample in the majority class with that of the centroid of the corresponding cluster it belongs to. Accordingly, the feature vector of each sample in the minority class is extended with itself. As the number of non-zero elements in the centroid is much larger than the number of the non-zero elements in a minority class sample, the feature imbalanced distribution problem still exists. However, the centroid-directed smoothing strategy actually introduces another imbalanced factor in imbalanced feature weights since the feature weights in the centroid are usually much lower than the Boolean weights in a sample in the minority class . Therefore, these two kinds of imbalanced factors result in a slightly more balanced classifier classifying the samples from both the positive and negative classes. Formally, the centroid feature vector calculated as the mean of feature vectors of all the samples in the i-th cluster /| | In summary, the centroid-directed smoothing strategy maps the feature vector of a sample x to a new feature vector follows, Where M A X represents the samples in the majority class , represents the samples in the minority class , and Test X represents the samples in the whole test data, regardless of what class they belong to. In this paper, two datasets are used to investigate the performance of our approach on imbalanced sentiment classification. The first one 1 is a widely-used public dataset collected by Blitzer et al. [1] which consists of four domains: Book, DVD, Electronic, and Kitc hen. For our experiment, each domain contains 400 negative sa mples (randomly selected from the 1000 original negative samples) and 1000 positive samples. We adopt the popular geometric mean ( G-mean ), defined 
Gmean TP TN  X  X   X  , where rate TP is the true positive rate (also called positive recall or sensitivity) and true negative rate (also called negative recall or specificity) [4]. Finally, all the classifiers adopt the Maximum Entropy (ME) algorithm available with the Mallet 2 tool , and the same Boolean-weighted unigram. For thorough comparison on imbalanced sentiment classification, various settings are explored: 1) Full training (FullT): directly throwing all the training data for training. 2) Random over-sampling (OverS): performing over-sampling by randomly selecting the samples from the minority class . 3) Random under-sampling (UnderS): performing under-sampling by randomly selecting the samples from the majority class . 4) One-class classification (OneClass): performing one-class classification as proposed by Juszczak and Duin [3] using the lib-SVM tool 3 . 5) Cost-sensitive classi fication (CostSensitive): performing cost-sensitive classification as proposed by Zhou and Liu [14] using the lib-SVM tool . Here, the cost weight for a majority-class sample is set to the imbalanced ratio between the minority class and majority class samples in each domain while the cost weight for a minority-class sample is 1. 6) Clustering-based under-sampling (ClusterU): performing clustering-based stratified under-sampling. 7) Clustering-based under-sampling plus centroid-directed smoothing (ClusterUC): performing both clustering-based stratified under-sampling and centr oid-directed smoothing, as proposed in this paper. Since most of the above settings involve random selection of samples, we run 20 times for each setting and report the average performance. Table 2 compares the seven settings. It shows that both random over-sampling and random under-sampling significantly outperform full training due to balance keeping between positive and negative samples. It also shows that random under-sampling significantly outperforms random over-sampling largely due to the ignorance of imbalanced feature distribution by random over-sampling. Furthe rmore, it shows that one-class classification does not fit our ta sk at all and that random under-sampling is rather difficult to beat. Generally, random under-sampling performs slightly better than cost-sensitive classification. Although clus tering-based under-sampling (ClusterU) employs cleverer selection strategies, they can only achieve comparable performances with random under-sampling. Observation on the features occurring in the selected samples shows that only about half of the features remain regardless of what kind of under-sampling (random or clustering-based) is used. Since half of the features can hardly well represent the feature distribution of the whole data, this justifies why cleverer under-sampling fails to improve the performance. It also shows that centriod-directed smoothing strategy significantly improves the performance of clustering-based under-sampling in all domains. This suggests the importance of resolving the imbalanced feature distribution problem and the effectiveness of our proposed centroid-directed smoothing strategy. In this paper, we address the issue of imbalanced sentiment classification by taking into acc ount both the imbalanced class and feature distribution problems. In particular, a clustering-based stratified under-sampling framework and a centroid-directed smoothing strategy are proposed to deal with the imbalanced class and feature distribution problems respectively. Evaluation shows the effectiveness of our approach. The research work described in this paper has been partially supported by three NSFC grants, No. 61003155, No. 60873150 and No. 90920004 and Open Projects Program of National Laboratory of Pattern Recognition. [1] Blitzer J., M. Dredze, and F. Pereira. 2007. Biographies, [2] Chawla N., K. Bowyer, L. Hall, and W. Kegelmeyer. [3] Juszczak P. and R. Duin. 2003. Uncertainty sampling [4] Kubat M. and S. Matwin. 1997. Addressing the curse of [5] Li S., S. Lee, Y. Chen, C. Huang, and G. Zhou. 2010. [6] Nakagawa T., K. Inui, and S. Kurohashi. 2010. [7] Neyman J. 1934. On the two different aspects of the [8] Pang B., L. Lee, and S. Vaithyanathan. 2002. Thumbs [9] Qian L., G. Zhou, F. Kong, and Q. Zhu. 2009. Semi-[10] Qian L. and G. Zhou. 2010. Clustering-based stratified [11] Riloff E., S. Patwardhan, a nd J. Wiebe. 2006. Feature [12] Turney P. 2002. Thumbs up or thumbs down? Semantic [13] Zhou Z. and X. Liu. 2006. Tr aining cost-sensitive neural 
