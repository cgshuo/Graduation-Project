 Recommender systems are widely used on the Internet nowadays. Web sites such as Amazon, EBay, Hulu and Netflix rely on recommender systems to pro-mote sales and improve user experience. These systems analyze users X  purchasing records, predict users X  preference (ratings) to different items, and recommend those with the highest ratings, i.e., the highest potential to attract users. Due to the great commercial value, recommender system has attracted much attention in different fields of research during the last decade, such as information retrieval [1 X 3], data mining [4 X 7] and machine learning [8].

Matrix factorization [9] is one of the most commonly used approaches in recommender systems. Despite of its efficiency, MF still suffers from sparsity problem, i.e., users who rate only a small portion of items could not get proper recommendation, and items with few ratings may not be recommended well.
Researchers have tried to utilize auxiliary datasets to alleviate sparsity prob-lem, such as social recommendation and transfer learning [10 X 12, 1, 13 X 15, 2, 16, 6, 5, 17, 7, 18, 19]. From the auxiliary datasets, recommender system could get additional profile of sparse users/items, thus making a more accurate recommen-dation. Although these methods are very useful to cope with sparse datasets, they have 4 major drawbacks: 1) Auxiliary datasets are not always available. The trans-fer learning method requires that the auxiliary data have at least one common set (user set or item set). The social recommenda tion needs user-user interaction data. 2) Auxiliary datasets may not be of good quality even if they are available. Data un-der different assumption or data with noise would be of limited help, or even harm-ful. [20] reports a negative case in which social data contributes nothing to the final result, probably due to the poor quality of data. 3) The balance between target and auxiliary datasets needs fine-tuning. [17] and [7] both mentioned that relying too much on either side would not generate good performance. 4) These models usu-ally have high complexity and could not be easily implemented and scaled. Most of the above models involve graph computation, which would be a challenge when the amount of users/items goes up.

Instead of taking in auxiliary datasets, we focus more on further utilizing the existing dataset to get better per formance. In this paper, we combine the commonly used cosine similarity with the matrix factorization model to form a novel approach, called Cosine Matrix Factorization (CosMF). There are two main contributions in CosMF:  X  We explain matrix factorization in an information retrieval scenario. We  X  We developed a novel approach, the CosMF. CosMF requires no auxiliary
The rest of the paper is organized as follows. Chapter 2 gives an overview of the related work, especially how MF model works. In Chapter 3, we analyze why MF does not work so well in sparse dataset. Chapter 4 introduces the CosMF method. Chapter 5 demonstrates the experiments and results. Chapter 6 draws a conclusion and discusses the future work. 2.1 Overview The Matrix Factorization (MF) technique aims to train a lower-rank model to approximate the rating matrix R by user latent vector U and item feature vector V . The underlying assumption of MF is that user preference is influenced by a small number of latent factors, and the rating of an item is determined by how each of its feature factor applies to the user preference [21 X 23, 8, 24, 25, 9]. MF works efficiently and effectively and becomes one of the most widely used methods in collaborative filtering.
 Lots of research had been done to improve the performance of basic Matrix Factorization. In [21] Lee et al. proposed Non-negative Matrix Factorization (NMF) to enforce non-negativity in U and V ,whichwasprovedtobeuseful in computer vision fields. In [23], Zhang et al. proposed Weighted Non-negative Matrix Factorization (WNMF) to improve NMF by using weights as indicator matrix to denote the observability of entries in R . In [8], Salakhutdinov et al. gave Probabilistic Matrix Factorization (PMF), which used Gaussian distribution to initialize U and V , and applied logistic function to limit the range of predicted to [0,1]. Koren et al. summarized these wor k in [9] and gave an generic framework for Matrix Factorization.
 Researchers also managed to incorporate information from other data sources. Zhang et al. used review sentiment analysi s to construct virtual ratings for users who have not explicitly expressed their opinions on the item [10]. Gu et al. proposed the Graph Weighted Nonnegative Matrix Factorization (GWNMF) [19] to use user/item neighborhood graph to preserve neighbourhood information in user/item latent vectors. [5, 1, 17] utilized social network information under the assumption that friends share similar tastes and interests. 2.2 Matrix Factorization Suppose we have n users, m items and a rating matrix R =[ R i,j ] m  X  n with each element R i,j representing the rating of user i to item j . To make a recommen-dation for user i , the recommender system needs to predict missing value of i -th row of R , and picks items with high predicted ratings. In Matrix Factorization, the rating of user i to item j is predicted as where U i denotes the latent preference row vector of user i and V j denotes latent feature row vector of item j , and both of them have k dimensions (latent factors). To train the model, we need to minimize the squared error between predicted and observed ratings while keeping the model as simple as possible. This leads to the following objective function of Matrix Factorization(MF), L =min where ( R i,j  X  U i V j T ) 2 is the loss function. I n  X  m is an indicator matrix with observed ratings set to 1 and unobserved ones 0. The  X  Frob denotes the Frobe-nius norm, which acts as a regularizer and constrains the model X  X  complexity.  X  , X  v  X  0 are penalty parameters of the above constraints.
 In this chapter, we give an alternative interpretation of Matrix Factorization in the perspective of information retr ieval. We treat recommending items as re-trieving items with the highest similarity. Thus R i,j represents the inner product similarity between U i and V j , which is determined by the angle between these two vectors and their magnitudes:
The  X  denotes the L2-norm of a vector, and cos(  X  ,  X  ) denotes the cosine sim-ilarity of two vectors. The norm and cosine play different roles in the preference approximation. U i and V j respectively carry the macroscopic(global) infor-mation of the strength of user i  X  X  preference vector and item j  X  X  feature vector. A small U i means that user i generally has low preferen ce on most of the items, and a small V j means that item j generally has low attraction to most of the users. While cosine similarity cos( U i ,V j ) carries the microscopic(local) informa-tion of the pairwise inter-action between U i and V j .Asmallcos( U i ,V j )means that the feature of item j does not appeal to user i  X  X  preference.

To better understand how cosine and norm work in the similarity approxima-tion, we conduct the following experiment. We use the famous Epinions dataset and sample it with different ratios 20%, 40%, 60% and 80% to construct training set with different sparsity. For each training set, matrix factorization model is trained. We calculate user average ratings and the U-norm U i for user i .For each user average rating, we further compute the average of U-norm, thus form-ing the  X  X ser Average Rating X - X  X verage U-norm X  graph in Figure 1(a). For ex-ample, a data point (0.8, 0.9) means that for all users whose average ratings is 0.8, their average U-norm is 0.9. The same goes for  X  X tem Average Rating X - X  X verage V-norm X  graph in Figure 1(b). Then, for each training instance i,j,rating ,we calculate the average cosine similarity cos( U i ,V j ) for each rating and draw the  X  X ating X - X  X verage Cosine X  graph in Figure 1(c). From Figure 1 we observe that both vector norm and cosine similarity have a positive correlations with the rat-ings. A higher rating usually indicates a higher cosine similarity. Users and items with higher average ratings tend to hav e higher vector norms. Also, we notice that as data sparsity decreases, the cosine curve gets lower, while the U-norm and V-norm curves become steeper. We c an draw 2 insights from the above observation: 1. The focus of low-rank approximation shifts from cosine similarity to vector 2. In MF model, cosine and norm get trained in parallel, since the cosine sim-
Insight 1 is in consistent with how people make recommendation in their daily life. For example, if we know that Bob has watched Star Wars and rated it 5-star, we may guess Bob is a Science-Fiction movie lover. This only one message gives us the basic direction of Bob X  X  us er latent vector. We name this process direction identification . Without further information, we still could not know how much he likes Sci-Fi. But if we get more Bob X  X  rating history, we could depict Bob X  X  the length of latent factor vect or more precisely. We name this process magnitude adjustment . MF works in a similar way. For example, suppose MF has already learned that Bob likes watching Star Wars. If it is told that Bob often watches Sci-Fi movie and gives them high ratings, the angle of U Bob stays the same, while the norm inclines to fit the high ratings. If it further knows that he watches thrill movie too, the direction of U Bob deviates from V Sci Fi ,andtheir cosine similarity declines. To make up for the loss of cosine, the norm of U Bob increases. The only problem of MF resides at the very beginning, where MF starts to do direction identification . According to insight 2, MF trains cosine and norm in parallel, which means that it processes direction identification and magnitude adjustment simultaneously. During the direction identification (cosine training), the magnitudes of U i and V j both get adjusted to make up the approximation error. Figure 2 demonstrates the above situation. If data is sparse, e.g., only one training instance is related to U i or V j , the cosine similarity could not get enough training (under-fitted) while norm gets too much training (over-fitted). 4.1 Motivation As we mention in chapter 3, norm representsthe macroscopicstrength of user/item vectors. Such corpus-wide information cannot be estimated accurately if data is sparse. While cosine similarity indicate s the microscopic int er-action of user i to item j . Even only one rating given by user i to item j can also give the preference direction of him/her (but not the strengths). This motivates us to focus on cosine similarity approximation at the first stage and norm approximation at the second stage. 4.2 Direction Identification: Factorization with Cosine Similarity We focus on direction identification by eliminating norm of U and V from (1): which means that the task of the low-rank factorization from answering  X  X hat rating user i will give item j  X  to an easier question  X  X ow likely user i interacts with item j  X . The objective function of (4) is: L =min This objective function has no global optimal solution, yet a local minimal could be acquired by performing stochastic gradient descent. The derivation of U i and V j are given by the following two equations: The purpose of this step is to train U i and V j to capture the detail of U  X  V interactions microscopically, thus avoiding the under-fitting of cosine similarity in the ordinary MF methods. 4.3 Magnitude Adjustment: Compensating with Traditional MF The above cosine based model discards the magnitude information of U i and V , which means that it loses the ability to describe U i and V j with detailed preference/attraction stre ngths. Thus, for most non-sp arse users/items, the per-formance of the above model should not be better than the traditional MF. To compensate the limitations of them, w e separately train these two models, and ensemble their respective results through linear combination. That is to say, there are two user preference vectors, U i for MF, U i for cosine-based model, and two item feature vectors, V j for MF, V j for cosine-based model. U i and V j are trained by stochastic gradient descent using the following equations: The final predicted rating  X  R i,j is given by: where  X  is the weight balance parameter between MF and the cosine-based model. When  X  = 1, the model degenerates into the traditional MF. When  X  = 0, the model discards all magnitudes method. Notice that  X  does not take part in the training process, and could be adjusted and fixed by cross-validation afterwards. 5.1 Datasets CosMF requires the original rating dataset only. Hence, this model could be used in most of the recommendation syst em. We use 4 datasets to testify the effectiveness of our model.

Epinions 1 Epinions.com is a well-known web site where customers could read and post reviews on a variety of items. Each review relates to a rating ranging from 1 to 5. This dataset is issued in [6]. It consists of 40,163 users, 139,738 items and totally 664,824 ratings.

Douban 2 Douban.com is a Chinese web site where users could share their interests on films, books, music and events by scoring the items (ranging from 1 to 5). This dataset is crawled and shared by Ma et al. [17]. It consists of 129,490 users, 58541 items and 16,830,839 ratings.

Flixster 3 Flixster.com is a web site where users can rate movies. The ratings are in the range of [0.5, 5] with step size 0.5. After eliminating invalid users and items, the dataset contains 787,214 users, 48,795 items and 8,196,077 ratings.
MovieLens10M 4 MovieLens10M consists of 10,000,054 ratings and 95,580 tags of 71,567 users for 10,681 movies. The preference of the user for a movie is rated from 1 to 5 and 0 indicates that the movie is not rated by any user.
The Douban, Flixster and MovieLens10M are all dense datasets. We lower the sample ratio of training set, manually creating more sparse users/items. This would provide us more test cases for each sparse user/item. For Douban, Flixster and MovieLens10M, 20% and 40% sampling are applied. Epinions is sparse enough, so we only sample 80% as training set.
 We use U t to denote the set of users who have rated no more than t items, I t to denote the set of items which have been rated by no more than t users. R test to evaluate the performance of MF and CosMF on sparse users/items.
Finally we constructed 7 datasets in total, and each dataset has its sparse set. These datasets are in various sparsity, and should be sufficient to give us an overview of the performance. Basic descr iptions of all datasets could be found inTable1.FromTable1wecanseethatall these datasets are extremely sparse (sparsity below 0.1%). Epinions and Flixster are extremely sparse both in user and item. In Epinions, sparse instances can even takes up to 15% of the whole training set. Douban and MovieLens10M are dense in user while sparse in item. 5.2 Metrics We adopt two widely used evaluation metrics for our experiment, the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE): where R i,j denotes the rating user i gives to item j , R i,j denotes the predict rating for user i with item j , | T E | is the amount of ratings in test dataset. By definition, smaller MAE and RMSE are better. 5.3 Baseline and Parameters To the best of our knowledge, we are the first one to attempt to improve Matrix Factorization from information retrieval perspective in extremely sparse datasets without auxiliary data. Other methods either use auxiliary data, or do not ad-dress the sparsity problem (especially for users and items with only one training data). So we only use method proposed by Koren et al. in [9] as the evaluation baseline.
For each of the training set, we compare MF with CosMF on sparse set as well as the whole set. Different numbers of latent factors k settings { 5, 10, 15, 20 } ,and different balance parameter  X  { 0.0-1.0, 0.01 each step } are tried. The learning rate of the stochastic gradient descent is set to 0.03, and the regularization parameter  X  u ,  X  v are set to 0.001 empirically. Each method is repeated 3 times, and the average MAE and RMSE are reported. Each experiment iterates until its objective function converges. 5.4 Performance The experiment results using  X  =0 . 5 are shown in Table 2. The standard devia-tions of the results are all around 0.002. Best result are highlighted in bold style. From the results, we observe that the proposed CosMF consistently outperform the original MF in both cold set and the whole set. Percentages in italic are the improvement of CosMF over the original MF. Due to the higher sparse test set ratio, the improvement on whole set of Epinions is much higher than others. 5.5 Parameter Sensitivity We investigate the parameter sensitivity of dimension k and weight balance factor  X  by varying their values and demonstrate the results in Figure 3 and Figure 4.

As Figure 3 shows, on Epinions 80%, performance of MF and CosMF both im-prove as k goes bigger. While on Douban 40%, Flixster 40% and MovieLens10M 40%, MF starts to perform bad when k get even bigger, while CosMF almost stays the same. The reason for this is that as k goes up, the norms of U i and V j also tend to become bigger, and impact of magnitude grows stronger, making the model more vulnerable to over-fitting. CosMF relies less on the magnitude and does not get over-fitted easily. Douban 20%, Flixster 20% and MovieLens10M are too sparse that models trained on them over-fit at the very beginning. Increasing k only makes things worse.

Figure 4 shows the impact of  X  .When  X  = 0, we discard all magnitude information, resulting in performance worse than the original MF. When  X  = 1, it becomes the traditional MF.  X  around 0.5 always give the best result, which means that we should neither rely too much on the magnitude(norm) information, nor directly ignore and discard it. In this paper, we focused on the sparsity problem in collaborative filtering rec-ommender system without the help of auxiliary data. Based on the observation that people learn features of an object th rough two process, the direction iden-tification and the magnitude adjustment, we found that traditional MF trains cosine (the direction) and norm (the magnitude) in parallel, which would lead to under-fitted cosine similarity value and over-fitted vector norm value when data is sparse. We proposed to eliminate norm in the factorization objective func-tion to solve that problem, and combine the new objective function with MF to form the CosMF method. Experiments on 4 r eal life datasets demonstrated that CosMF out-perform the traditional MF.

The success of CosMF increased our confidence to incorporate cosine simi-larity in matrix factorization. In the field of collaborative filtering recommender system, there are many scenes where magnitude of vectors do not matter greatly, such as the one-class collaborative filtering [4] and social recommendation [17]. We believe that the cosine similarity function would be of great help for those problems.

