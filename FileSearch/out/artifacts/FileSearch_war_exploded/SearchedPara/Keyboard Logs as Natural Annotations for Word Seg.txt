 The first step of almost all natural language processing (NLP) for languages with ambiguous word boundaries (such as Japanese and Chinese) is solving the problem of word identification am-biguity. This task is called word segmentation (WS) and the accuracy of state-of-the-art methods based on machine learning techniques is more than 98% for Japanese and 95% for Chinese (Neubig et al., 2011; Yang and Vozila, 2014). Compared to languages like English with clear word bound-aries, this ambiguity poses an additional problem for NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degrada-tion in WS performance. Examples include ma-chine translation of patents, text mining of med-ical texts, and marketing on the micro-blog site on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these do-mains (Mori and Neubig, 2014; Kaji and Kitsure-gawa, 2014; Liu et al., 2014)
To cope with this problem, we propose a way to collect information from people as they type Japanese or Chinese on computers. These lan-guages use far more characters than the number of keys on a keyboard, so users use software called an input method (IM) to type text in these languages. Unlike written texts in these languages, which lack word boundary information, text entered with an IM can provide word boundary information that can used by NLP systems. As we show in this pa-per, logs collected from IMs are a valuable source of word boundary information.

An IM consists of a client (front-end) and a server (back-end). The client receives a key se-quence typed by the user and sends a phoneme sequence ( kana in Japanese or pinyin in Chinese) or some predefined commands to the server. The server converts the phoneme sequence into normal written text as a word sequence or proposes word candidates for the phoneme sequence in the region specified by the user. We noticed that the actions performed by people using the IM (such as typ-ing and selecting word candidates) provide infor-mation about word boundaries, including context information.
 In this paper, we first describe an IM for Japanese which allows us to collect this informa-tion. We then propose an automatic word seg-menter that uses IM logs as a language resource to improve its performance. Finally, we report exper-imental results showing that our method increases the accuracy of a word segmenter on Twitter texts by using logs collected from a browser add-on ver-sion of our IM.

The three main contributions of this paper are:  X  an IM server that proposes word candidates  X  a publicly usable IM that logs user behavior  X  a method for improving word segmentation To the best of our knowledge, this is the first paper proposing a method for using IM logs to success-fully improve WS. The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early 90 X  X  (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the word-POS pairs into clusters inspired by the class-based n -gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed bet-ter performance than POS-based Markov models (Kudo et al., 2004). The training time of sequence-based methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based meth-ods and fast enough to make active learning prac-tically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments.

Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictio-naries for human into a WS system with a differ-ent word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially anno-tated sentences for WS training data, word bound-ary information exists only between some charac-ter pairs and is absent for others. This extension was adopted in Chinese WS to make use of so-called natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but con-tain much more noise. In addition, we need an IM that is specifically designed to collect logs as nat-ural annotations.

Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical lan-guage modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their param-eters are trained from manually segmented sen-tences whose words are annotated with phoneme sequences, and from sentences automatically an-notated with NLP tools which are also based on machine learning models trained on the annotated sentences. Thus normal IM servers are not capa-ble of presenting out-of-vocabulary (OOV) words (which provide large amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on (Mori et al., 2006), and ensure that it is compu-tationally efficient enough for practical use by the public.

The target domain in our experiments is Twit-ter, a site where users post short messages called tweets. Since tweets are an immediate and power-ful reflection of public attitudes and social trends, there have been numerous attempts at extracting information from them. Examples include infor-mation analysis of disasters (Sakai et al., 2010), estimation of depressive tendencies (Tsugawa et al., 2013), speech diarization (Higashinaka et al., 2011), and many others. These works require pre-processing of tweets with NLP tools, and WS is the first step. So it is clear that there is strong de-mand for improving WS accuracy. Another reason why we have chosen Twitter for the test domain is that the tweets typed using our server are open and we can avoid privacy problems. Our method does not utilize any other characteristics of tweets. So it also works in other domains such as blogs. In this section we propose a practical statistical IM server that suggests OOV word candidates in ad-dition to words in its vocabulary. 3.1 Statistical Input Method An input method (IM) is software which converts a phoneme sequence into a word sequence. This is useful for languages which contain far more char-acters than keys on a keyboard. Since there are some ambiguities in conversion, a conversion en-gine based on a word n -gram model has been pro-posed (Chen and Lee, 2000). Today, almost all IM engines are based on statistical methods.

For the LM unit, instead of words we propose to adopt word-pronunciation pairs u =  X  y , w  X  . Thus given a phoneme sequence y l 1 = y 1 y 2  X   X   X  y l as the input, the goal of our IM engine is to output a word sequence  X  w m 1 that maximizes the probabil-ity P ( w , y l 1 ) as follows: where the concatenation of y i in each u i is equal to the input: y l 1 = y 1 y 2  X   X   X  y m . In addition u j ( j  X  0 ) are special symbols introduced to simplify the notation and u m +1 is a special symbol indicating a sentence boundary.

As in existing statistical IM engines, parame-ters are estimated from a corpus whose sentences are segmented into words annotated with their pro-nunciations as follows: where F (  X  ) denotes the frequency of a pair se-quence in the corpus. In contrast to IM engines based on a word n -gram model, ours does not re-quire an additional model describing relationships between words and pronunciations, and thus it is much simpler and more practical.

Existing statistical IM engines only need an ac-curate automatic word segmenter to estimate the parameters of the word n -gram model. As the equation above shows, our pair-based engine also needs an accurate way of automatically estimat-ing pronunciation (phoneme sequences). How-ever, recently an automatic pronunciation estima-tor (Mori and Neubig, 2011) that delivers as accu-rate as state-of-the-art word segmenters has been proposed. As we explain in Section 6, in our ex-periments both our IM engine and existing ones delivered accuracy of 91%. 3.2 Enumerating Substrings as Candidate Essentially, the IM engine which we have ex-plained above does not have the ability to enumer-ate words which are unknown to the word seg-menter and the pronunciation estimator used to build the training data. The aim of our research is to gather language information from user behav-ior as they use an IM. So we extend the basic IM engine to enumerate all the substrings in a corpus with all possible pronunciations. For that purpose, we adopt the notion of a stochastically segmented corpus (SSC) (Mori and Takuma, 2004) and ex-tend it to the pronunciation annotation to words. 3.2.1 Stochastically Segmented Corpora An SSC is defined as a combination of a raw cor-pus C r (hereafter referred to as the character se-quence x n r the form P i , which is the probability that a word boundary exists between two characters x i and x i +1 . These probabilities are estimated by a model based on logistic regression (LR)(Fan et al., 2008) trained on a manually segmented corpus referring to the same features as those used in (Neubig et al., 2011). Since there are word boundaries be-fore the first character and after the last character of the corpus, P 0 = P n frequencies on an SSC are calculated as follows: Word 0-gram frequency: This is defined as the expected number of words in the SSC: Word n -gram frequency ( n  X  1 ): Consider the situation in which a word sequence w n 1 occurs in the SSC as a subsequence beginning at the ( i + 1) -th character and ending at the k -th char-acter and each word w m in the word sequence is equal to the character sequence beginning at the b m -th character and ending at the e m -th charac-ter ( x e m b m +1 , 1  X   X  m  X  n  X  1 ; b 1 = i + 1 ; e n = k ) (See Figure 1 for an example). The word n -gram frequency of a word sequence f r ( w n 1 ) in the SSC is defined by the summation of the stochastic frequency at each occurrence of the over all of the occurrences in the SSC: , where e n 1 = ( e 1 , e 2 ,  X   X   X  , e n ) and O n = We calculate word n -gram probabilities by divid-ing word n -gram frequencies by word ( n  X  1) -gram frequencies. For a detailed explanation and a mathematical proof of this method, please refer to (Mori and Takuma, 2004). 3.2.2 Pseudo-Stochastically Segmented The computational costs (in terms of both time and space) for calculating an n -gram model from an nique for implementing an IM engine. In order to reduce the computational costs we approximate an SSC using a deterministically tagged corpus, which is called a pseudo-stochastically segmented corpus (pSSC) (Kameko et al., 2015). The follow-ing is the method for producing a pSSC from an SSC.  X  For i = 1 to n r  X  1 Now we have a corpus in the same format as a standard segmented corpus with variable (non-constant) segmentation. 3.2.3 Pseudo-Stochastically Tagged Corpora We can annotate a word with its all possi-ble pronunciations and their probabilities, as is done in an SSC. We call a corpus containing sequences of words ( w 1 w 2  X   X   X  w i  X   X   X  ) annotated with a sequence of pairs of a pronunciation and  X  using an LR model built from sentences annotated with pronunciations (Mori and Neubig, 2011).
Similar to pSSC we then produce a pseudo-stochastically tagged sentence (pSTC) from an STC as follows:  X  For each w i in the sentence Now we have a corpus in the same format as a standard corpus annotated with variable pronunci-ation.

By estimating the parameters in Equation (1) from a pSTC derived from a pSSC, our IM en-gine can also suggest OOV word candidates with various possible segmentation and pronunciations without incurring high computational costs. 3.2.4 Suggestion of OOV Words Here we give an intuitive explanation why our IM engine can suggest OOV words for a certain phoneme sequence. Let us take an OOV word example:  X   X  X  X  / yo-ko-a-ri , X  an abbreviation of  X   X  X  X  X  X  X  X  X   X  (Yoko hama city are na). A WS system tends to segment it into  X   X   X  (side) and  X   X  X   X  (ant) because they are frequent nouns. In a pSSC, however, some occurrences of the string  X   X  X  X   X  are remain concatenated as the correct word. For pronunciation, the first character has two possible pronunciations  X  yo-ko  X  and  X  o-u . X  So deterministic pronunciation estimation of this new word has the risk of outputting the erroneous result  X  o-u-a-ri . X  This prevents our engine from presenting  X   X  X  X   X  as a conversion candidate for the input  X  yo-ko-a-ri . X  The pSTC, however, con-tains two possible pronunciations for this word and allows our engine to present the OOV word  X   X  X  X   X  for the input  X  yo-ko-a-ri . X 
Thus when the user of our IM engine types  X  yo-ko-a-ri-ni-i-ku  X  and selects  X   X  X  X  X  (to)  X  X  X  (go), X  the engine can learn an OOV word  X   X  X   X  / yo-ko-a-ri  X  with context  X   X  / ni  X  X  X  / i-ku  X . In this section we first propose an IM which al-lows us to collect user logs. We then examine the characteristics of these logs and some difficulties in using them as language resources. 4.1 Collecting Logs from an Input Method As Figure 2 shows, the client of our IM, running on the user X  X  PC, is used to input characters and to modify conversion results. The server logs both input from the client and the results of conversions performed in response to requests from the client.
Our IM has two phases: phoneme typing and conversion result editing. In each phase, the client sends the typed keys to the server with a timestamp and its IP address.
 Phoneme typing: First the user inputs ASCII Conversion result editing: Then the user presses 4.2 Characteristics of Input Method Logs Table 1 shows an example of interesting log mes-users type sentence fragments but not a complete sentence. So in the example there are six frag-ments within a short period indicated by the times-tamps. If the user selects the phoneme sequence as-is without going to the conversion result editing phase, we can expect that there are word bound-aries on both sides of the phoneme sequence. In-hama arena).
 Timestamp Phoneme sequence Edit result Note 18:37:11.21  X  X  X  X  X  X  X  / yo-ko-a-ri-ni  X  X  X  / yo-ko-a-ri  X  / ni (with Yokohama arena) 18:37:12.60  X  X  X  X  X  X  X  / ku-ra-b-be-ru  X  X  X  X  / ku-ra-b  X  X  X  / be-ru Mistyping 18:37:14.94  X  X  X  X  X  X  / ku-ra-be-ru  X  X  X  / ku-ra-be  X  / ru Revised input (compare) 18:37:15.32  X  / to N/A ( inflectional ending ) 18:37:19.82  X  X  X  X  / mo-no-no N/A Discarded in the twitter 18:37:22.42  X  X  X  X  X  X  X  / ya-su-me-ka-to  X  X  X  / ya-su-me  X  /ka  X  /to (cheap) side the phoneme sequence, however, there is no information. If the user goes to the conversion result editing phase, we can expect that the final word sequence has correct word boundary infor-mation.

There are two main problems that make it dif-ficult to directly use IM logs as a training cor-pus for word segmentation. The first problem is fragmentation. IM users send the phoneme se-quences for sentence fragments to the engine to avoid editing long conversion results that require many cursor movements. Thus the phoneme se-quence and the final word sequence tend to be sentence fragments (as we noted above) and as a result they lose context information. The second problem is noise. Word boundary information is unreliable even when it is present because of mis-takenly selected conversions or words entered sep-arately. From these observations, the IM logs are treated as partially segmented sentence fragments that include noise. In this section we first explain various ways to generate language resources for a word segmenter from IM logs. We then describe an automatic word segmenter which utilizes these resources. In the examples below we use the three-valued notation (Mori and Oda, 2009) to denote partial segmenta-tion as follows: 5.1 Input Method Logs as Language The phoneme sequences and edit results in the fi-nal selection themselves are considered to be par-tially segmented sentences. We call the corpus generated directly from the logs  X  Log-as-is . X  Ex-amples in Table 1 are converted as following.  X  - X  - X  |  X   X   X  - X  - X  |  X  - X   X   X   X   X  - X  |  X   X  - X  |  X  |  X  Here the number of annotations is the sum of  X  - X  and  X  |  X . In this example, one entry corresponds to one entry of the training data for the word seg-menter. As you can easily imagine, Log-as-is may contain mistaken results (noise) and short entries (fragmentation). Both are harmful for a word seg-menter.

To cope with the fragmentation problem, we propose to connect some logs based on their times-tamps. If the difference between the timestamps of two sequential logs is short, both logs are proba-bly from the same sentence. So we connect two sequential logs if the time difference between the last key of the first log and the first key of the sec-ond log is smaller than a certain threshold s . In the experiment we set s = 500[ ms ] based on observa-as  X  Log-chunk . X  Using this method, we obtain the following from the examples in Table 1.  X  - X  - X  |  X  |  X  - X  - X  |  X  - X   X  - X  |  X  |  X  |  X   X   X   X  - X  |  X  |  X  We see that Log-chunk contains more context in-formation than Log-as-is.

For preventing the noise problem, we propose to filter out logs with a small number of conver-sions. We expect that an edited sentence will have many OOV words and not much noise. Therefore we use logs which were converted more than n c times. In the experiment we set n c = 2 based on Training BCCWJ 56,753 1,324,951 1,911,660 Newspaper 8,164 240,097 361,843 Conversation 11,700 147,809 197,941 Test BCCWJ-test 6,025 148,929 212,261
TWI-test 2,976 37,010 58,316 ferred to as  X  Log-mconv . X  Using this method, the examples in Table 1 becomes the following.
  X  - X  - X  |  X  As this example shows, Log-mconv contains short entries (fragmentation) like Log-as-is. However, we expect that the annotated tweets do not include mistaken boundaries or conversions that were dis-carded.
 Obviously we can combine Log-chunk and Log-mconv to avoid both the fragmentation and noise problems. This combination is referred to as  X  Log-chunk-mconv . X  5.2 Training a Word Segmenter on Logs The IM logs give us partially segmented sentence fragments, so we need a word segmenter capa-ble of learning from them. We can use a word segmenter based on a sequence classifier (Tsuboi et al., 2008; Yang and Vozila, 2014; Jiang et al., 2013) or one based on a pointwise classifier (Neu-big et al., 2011). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy.

Here is a brief explanation of the word seg-menter based on the pointwise method. For more detail the reader may refer to (Neubig et al., 2011). The input is an unsegmented character sequence x = x 1 x 2  X   X   X  x k . The word segmenter decides if there is a word boundary t i = 1 or not t i = 0 by using support vector machines (SVMs) (Fan et al., 2008) 7 . The features are character n -grams
Table 3: Language resources derived from logs. Log-as-is 32,119 39,708 Log-chunk 8,685 63,144 Log-mconv 4,610 10,852
Log-chunk-mconv 1,218 14,242 and character type n -grams ( n = 1 , 2 , 3 ) around the decision points in a window with a width of 6 characters. Additional features are triggered if character n -grams in the window match with char-acter sequences in the dictionary. This approach is called pointwise because the word boundary deci-sion is made without referring to the other deci-sions on the points j 6 = i . As you can see from the explanation given above, we can also use partially segmented sentences from IM logs for training in the standard way. As an evaluation of our methods, we measured the accuracy of WS without using logs (the base-line) and using logs converted by several methods. There are two test corpora: one is the general do-main corpus from which we built the baseline WS, and the other is the same domain that the IM logs were collected from, Twitter. 6.1 Corpora The annotated corpus we used to build the base-line word segmenter is the manually annotated part (core data) of the Balanced Corpus of Con-temporary Written Japanese (BCCWJ) (Maekawa, 2008), plus newspaper articles and daily conver-sation sentences. We also used a 234,652-word dictionary (UniDic) provided with the BCCWJ. A small portion of the BCCWJ core data is reserved for testing. In addition, we manually segmented the same period as the log collection for the test corpus. Table 2 shows the details of these corpora. 6.2 Models using Input Method Logs To make the training data for our IM server, we first chose randomly selected tweets (786,331 sen-tences) in addition to the unannotated part of the BCCWJ (358,078 sentences). We then trained LR models which estimate word boundary probabili-ties and pronunciation probabilities for words (and word candidates) from the training data shown in Table 2 and UniDic. We made a pSTC for our IM engine from 1,207,182 sentences randomly ob-tained from Twitter by following the procedure We launched our IM as a browser add-on for Twitter and collected 19,770 IM logs from 7 users between April 24 and December 31, 2014. Fol-lowing the procedures in Section 5.1, we obtained the language resources shown in Table 3. We com-bined them with the training corpus and dictionar-ies to build four WSs, which we compared with the baseline. 6.3 Results and Discussion Following the standard in WS experiments, the evaluation criteria are recall, precision, and F-measure (their harmonic mean). Recall is the number of correctly segmented words divided by the number of words in the test corpus. Preci-sion is the number of correctly segmented words divided by the number of words in the system out-put.

Table 4 and 5 show WS accuracy on TWI-test and BCCWJ-test, respectively. The difference in accuracy of the baseline method on BCCWJ-test and TWI-test shows that WS of tweets is very dif-ficult. The fact that the precision on TWI-test is much higher than the recall indicates that the base-line model suffers from over-segmentation. This over-segmentation problem is mainly caused by OOV words being divided into known words. For example,  X   X  X  X   X  (Yokohama arena) is divided into the two know words  X   X   X  (side) and  X   X  X   X  (ant).

When we compare the F-measures on TWI-test, all the models referring to the IM logs outperform the baseline model trained only from the BCCWJ. The highest is the Log-chunk-mconv model and the improvement over the baseline is statistically significant (significance level: 1%). In addition the accuracies of the five methods on the BCCWJ (Table 5) are almost the same and there is no statis-tical significance (significance level: 1%) between any two of them.
 We analyzed the words misrecognized by the WSs, which we call error words. Table 6 shows the number of error words, the number of OOV words, and the ratio of OOV words to error words. Here the vocabulary is the set of the words appear-ing in the training data or in UniDic (see Table 2). Although the result of the WS trained on Log-as-is contains more error words than the baseline, the OOV ratio is less than the baseline. This means that the IM logs have a potential to reduce errors caused by OOV words.

Table 6 also indicates that the best method Log-chunk-mconv had the greatest success in reducing Figure 3: Relationship between WS accuracy on the tweets and log size. errors caused by OOV words. However, the ma-jority of error words are in-vocabulary words. It can be said that our log chunking method (Log-chunk or Log-chunk-mconv) enabled the WSs to eliminate many known word errors by using con-text information.

To investigate the impact of the log size, we measured WS accuracy on TWI-test when vary-ing the log size during training. Figure 3 shows the results. Table 4 says that Log-chunk-mconv and Log-chunk increase the accuracy nicely. The graph, however, clarifies that Log-chunk-mconv achieves high accuracy with fewer training data converted from logs. In other words, the method Log-chunk-mconv is good at distilling the in-formative parts and filtering out the noisy parts. These characteristics are very important properties to have as we consider deploying our IM to a wider audience. An IM is needed to type Japanese and the number of Japanese speakers is more than 100 million. If we can use input logs of even 1% of propose in this paper can improve WS accuracy on various domains efficiently and automatically.
As a final remark, this paper describes a suc-cessful example of how to build a useful tool for the NLP community. This process has three steps: 1) design a useful NLP application that can collect user logs, 2) deploy it for public use, and 3) devise a method for mining data from the logs. This paper described the design of a publicly us-able IM which collects natural annotations for use as training data for another system. Specifically, we (1) described how to construct an IM server that suggests OOV word candidates, (2) designed a publicly usable IM that collects logs of user behavior, and (3) proposed a method for using this data to improve word segmenters. Tweets from Twitter are a promising source of data with great potential for NLP, which is one reason why we used them as the target domain for our ex-periments. The experimental results showed that our methods improve accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain, so it is worth testing on other domains and with much longer pe-riods of data collection.
 This work was supported by JSPS Grants-in-Aid for Scientific Research Grant Number 26280084 and Microsoft CORE project. We thank Dr.
 Hisami Suzuki, Dr. Koichiro Yoshino, and Mr. Daniel Flannery for their valuable comments and suggestions on the manuscript. We are also grate-ful to the anonymous users of our input method.
