 NathanaelChambers nchamber@usna.edu United States Naval Academy, Annapolis, MD 21402 USA DanJurafsky jurafsky@stanford.edu Stanford University, Stanford, CA 94305 This talk describes a new approach to knowledge ac-quisition and extraction that learns rich structures of events (e.g., plant, detonate, destroy) and participants (e.g., suspect, target, victim) over a large corpus of news articles, beginning from scratch and without hu-man involvement. Early models in Natural Language Processing (NLP) relied on similar high-level repre-sentations like frames and scripts (structured repre-sentations of events, their causal relationships, and their participants) to drive interpretation of syntax and word use. Scripts, in particular, were central to research in the 1970s and 1980s ( Schank &amp; Abel-son , 1977 ). However, scripts were hand-coded, unable to generalize to new domains. Modern statistical ap-proaches and advances in NLP now enable new repre-sentations and large-scale learning over many domains. The main problem with scripts and similar common-sense knowledge structures was that the need for hand construction, specificity, and domain dependence pre-vented robust language understanding. The many di-verse and varied situations in the world cannot real-istically be hand coded for every language application and domain. For example, a script about  X  X ating in a restaurant X  (the most famous Schankian script) can-not assist the understanding of situations like corpo-rate acquisitions and football games. The development of scripts proved too time intensive and too brittle when changing contexts.
 In this talk, I describe attempts to learn script-like in-formation about the world, including both event struc-tures and the roles of their participants, but without pre-defined frames, roles, or tagged corpora. Consider the following event schema , informally represented. The events on the left follow a set of participants through a series of connected events that constitute a narrative sequence of events: Being able to robustly learn sets of related events (left) and frame-specific role information about the argu-ment types that fill them (right) could assist a variety of NLP applications, including summarization, ques-tion answering and information extraction (IE). I will briefly describe its successful application to IE below. Extracting structured knowledge about the world from raw text is a relatively new goal in NLP. The major-ity of machine learning approaches have focused on shallower representations of meaning. Fact and rela-tion extraction has focused on extracting true state-ments about the world, typically in the form of rela-tion triples: A relation B . Examples from the Open IE system ( Banko , 2009 ) include, Napoleon married Josephine , Einstein born in Ulm , and oranges contain Vitamin C . The system learns synonymous patterns (e.g., sign on, enter, and join), as well as classes of similar concepts (e.g., company, business, inc., organi-zation) by observing patterns with the same argument types. Other approaches vary in their use of (or lack of) seed examples, domain text, and target knowledge ( Kok &amp; Domingos , 2008 ; Carlson et al. , 2010 ; Huang &amp; Rilo ff , 2010 ). These are largely unsupervised. The event schemas in this talk are a complimentary, but di ff erent type of knowledge from the above rela-tions/facts. Event schemas represent sets of dependent relations and entities in specific roles, while the above systems learn lists of independent relations. This work is thus unique in learning relations across relations, or imposing a structure over otherwise independent rela-tions. Further, my algorithm does not capture specific facts, like napoleon married josephine, but instead cap-tures general events/relations like People marry Peo-ple. It then learns the related events and roles such as meet, date, bear children, retire, etc.
 Briefly, the key to learning rich event knowledge is identifying syntactic clues that connect events. My learning algorithm focuses on entities in text (e.g., people, places, things) and uses repeated mentions of the same entity to link related events. For instance, the phrases  X  X ruce pushed the man X  and  X  X he man fell down X  convey implicit knowledge about the world, namely, that push and fall down are related. We know this because the man appears with both verbs. We can thus loosely conclude that the two events occur together in the real world. I identify these connections between events over a million document corpus and learn event structures for thousands of verbs. The re-sult is a connected graph with events as nodes, and mutual information scores as edges.
 Given this graph, we cluster events using the edge weights as defined by the entities we observed. Fig-ure 1 shows three possible clusters extracted from the graph. Each cluster represents a unique entity, defined over the same events, and can be merged into one event schema representation.
 The algorithm learns schemas for over 1,800 English verbs over a range of domains. However, the end goal is not to simply learn knowledge for knowledge X  X  sake. I have successfully applied event schemas to an IE task that traditionally required supervised learning. Briefly, the task is to extract entities that performed a known action (e.g., bombings). We are the first to attempt this task without knowing the actions in ad-vance, and without labeled data. The system had to first learn that bombings and kidnappings exist in the world from raw text. It then learned that each is char-acterized by a series of events and unique entities like perpetrators, targets, and weapons. Below is one such event template that was learned automatically. All previous work on this task depends on knowing ahead of time that perpetrators exist, as well as hu-man annotations of example entities. I instead induced this event knowledge, and then extracted entities as is standard in the task, achieving accuracies approaching those of supervised learning algorithms.

