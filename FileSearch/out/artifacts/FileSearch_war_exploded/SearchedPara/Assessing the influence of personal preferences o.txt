 1. Introduction
Natural Language Generation (NLG) is a subfield of Artificial Intelligence and Computational Linguistics that covers the design and construction of systems that produce text in human languages. The general process of text generation ( Reiter decided and these messages are organized into a specific order and structure ( content planning ), and particular ways of the domain concepts and relations which appear in the messages. A final stage of surface realization assembles all the rele-vant pieces into linguistically and typographically correct text.
 Referring Expression Generation (REG) is one of the most studied problems of the natural language generation process. 1998, 2002; Krahmer, van Erk, &amp; Verleg, 2003; Gatt, 2007 ), each one taking into account different considerations and  X  choice of lexical items and syntactic structures for referring expressions has not been treated in depth.
Within the referring expression generation process, lexicalization chooses an appropriate word or phrase to express each man , guy , and gentleman , depending on factors such as context or style. The implementation of a referring expression gen-eration algorithm with the capacity to perform lexical choices presents an important challenge. In those contexts where we have more than one alternative for the lexicalization of the elements we need heuristics to decide which alternatives are more appropriate for each reference to an object in a given text. These heuristics should take into account aspects such as terminological restrictions or common practices according to different styles or situations.

Although different systems have tried to model the  X  X  X orrect X  X  way of creating referring expressions, we believe that there uation or channel in which the communication is being developed. As in other human X  X omputer interaction tasks, such as user being addressed and how they use language themselves. This kind of personalization effort would be useful if the sys-by the user in applications such as dialogue systems.

As a first step towards demonstrating this idea and providing personalized solutions for this problem, we empirically ana-lyze a corpus of lexicalizations of referring expressions and study the influence of language preferences in how people lex-icalize new referring expressions in different situations. In order to do so we present a corpus-based approach to referring ducing previous solutions to a problem. In order to compare the influence of personal preferences we compare the results obtained after training the system in two different ways: taking language preferences into account or not considering them.
We obtain a 50% decrease in the similarity error obtained by our system when considering personal preferences against the solution that does not. We think this is an important basis for considering new approaches to REG involving this kind of phenomenon, making them more human-like in their responses, or biased to specific types of users if there are available cor-pora showing their preferences.

This work is structured as follows. Sections 2 and 3 present an outline of the referring expression generation field and the case-based reasoning paradigm. Section 4 shows our CBR approach to referring expression lexicalization and Section 5 the results obtained when training the system in two different ways: one that takes language preferences into account and one that does not. Finally, Section 6 outlines some conclusions and future work. 2. Referring expression generation
Referring Expression Generation (REG) is concerned with how to produce a description of an entity that enables the count semantic information about the entities we want to refer to.

Nominal phrases (usually in the form adjectives + noun) are one of the most common forms to express a reference to an respond to its values for specific attributes. There are two important steps to take into account in order to generate such referring expressions: to decide which set of attributes can distinguish the entity from any other distracting entities and to decide how that information will be expressed in the text.

If we consider the type of entity we want to mention as given, for the Lexicalization stage.
 The Incremental Algorithm by Dale and Reiter (1995) is probably the most studied solution for the selection of attributes.
The authors describe a fast algorithm for generating referring expressions in the context of a natural language generation system. The algorithm they present is based on psycholinguistic evidence. As such, it provides an acceptable baseline for the basic operations and the performance expected from such an algorithm.
This algorithm relies on the following set of assumptions about the underlying knowledge base that must be used: (1) the knowledge base may organize some attribute values as a subsumption hierarchy.

To construct a reference to a particular entity, the algorithm takes as input a symbol corresponding to the intended ref-preference to some attribute over another when the other criteria give no clear choice. 2.1. Lexicalization of referring expressions
Lexicalization is understood as the process of deciding which specific words and phrases should be chosen to express the linguistic resources which convey meaning, like for example particular syntactic structures. The most common model of lex-icalization is one where the lexicalization module converts an input graph whose primitives are domain concepts and rela-tions into an output graph whose primitives are words and syntactic relations. This scheme can be valid for most applications where the domain is restricted enough in order that direct correspondence between the content and the words improvement.

Cahill (1998) differentiates between  X  X  X exicalization X  X  and  X  X  X exical choice X  X . The first term is used to indicate a broader meaning of the conversion of something to lexical items, while the second is used in a narrower sense to mean deciding be-hill X  X   X  X  X exicalization X  X , and it is usually referred to as  X  X  X yntactic choice X  X .

A classic example of lexical choice for natural language generation systems is Michael Elhadad X  X  PhD Thesis ( Elhadad, 1992 ), which addresses this particular problem for a system employing the FUF surface realizer ( Elhadad, 1993 ). Elhadad X  X  solution operates by unifying a conceptual representation of the input with a grammar that encodes the set of linguistic complex structures. The grammar used is an extended version of a functional description, with the added peculiarity that it allows the inclusion of choice points. During unification, for each choice point in the grammar the system attempts to unify the input with each of the alternatives until a matching one is found. The system allows backtracking in case of failures.
Bangalore and Rambow (2000) maintained that choosing the best lexeme to realize a meaning in natural language gen-monds and Hirst (2002) developed a computational model of lexical knowledge that can adequately account for near-synonymy, and deployed such a model in a computational process that could  X  X  X hoose the right word X  X  in any situation of language production.

From the point of view of the lexicalization of references, Horacek (1997) stated the problem of how most of the referring expression generation algorithms do not take into account the linguistic realization of the conceptual information they choose. This unawareness summed up in two assumptions found in most of these approaches: (1) the instant availability are also the secondary goals of expressing the chosen information in a natural way and applying a suitable processing strategy.

Siddharthan and Copestake (2004) described an algorithm for generating referring expressions in open domains strongly nam and Lemon (2009) addressed the problem of referring expression lexicalization in spoken dialogue systems where the algorithm that adapts its vocabulary to the one employed by the user. Stoyanchev and Stent (2009) also considered that, when generating referring expressions in interactive settings, lexical choices are quite dependent on the previous dialogue ceptual and textual content of the utterances. 2.2. Personal preference considerations in referring expression generation
Although it has been generally considered that there could be many correct lexicalizations for the same referring expres-sion and that there are no clear means of deciding between them, the issue of studying how personal preferences can influ-most of them in the scope of the shared tasks for REG ( Belz &amp; Gatt, 2007; Gatt, Belz, &amp; Kow, 2008, 2009 ).
In Bohnet (2008, 2009) different algorithms for attribute selection and lexicalization of referring expressions were pre-sented, all of them based on the individual styles of each of the authors of the TUNA corpus. From the point of view of lex-determiners or favourite words, were created for different people.
 corpus was to be lexicalized, the system failed because of lack of a suitable template. 2.3. Evaluation for referring expression generation
As in other fields within Artificial Intelligence, it is important to assess the generation of referring expressions in or-of them.

In order to evaluate the similarities between the language generated by humans and the language generated by a gener-
Natural Language Processing. However, there has also been much discussion about whether it is possible to consider existing these resources are usually generated with a great number of authors, it is usual to find so much variation in them that it becomes impossible to extract a  X  X  X orrect X  X  way in which the studied task must be performed. In the case of the generation heterogeneity in the generation of referring expressions. In this case, a corpus created by several authors would be a very useful resource. 2.3.1. TUNA corpus
One of the most important projects in the field of referring expression generation is the TUNA project ( van Deemter, van the form of nominal phrases was developed for visual entities in the domains of people and furniture. The corpus was ob-tained during an experiment in which subjects were asked to write textual descriptions for target entities in a situation where there were also six other entities called distractors. Each referring expression from the corpus is accompanied by the conceptual representation of the situation in which it was generated. The TUNA corpus has been used in this work as a case base for our CBR solution and as reference for the evaluation of the implemented algorithm. An exhaustive description of the corpus can be found in van der Sluis, Gatt, and van Deemter (2006) and Gatt, van der Sluis, and van Deemter (2008) .
The TUNA corpus contains 398 XML documents for the furniture domain and 340 for the people domain. Each data file of the corpus consists mainly of the following nodes: DOMAIN : Representation of entities in terms of their attributes.
 STRING-DESCRIPTION : The string describing the target referent in the domain.

ATTRIBUTE-SET : The set of domain attributes included in the description. Attributes represent the characteristics of an entity using attribute-value pairs. The possible attributes and values for both domains are shown in Table 1 . Empty cells represent attributes that are not used in the domain. X -erent in a 5 (column) 3 (row) matrix in which the objects were presented during the experiment.
 DESCRIPTION : The string in STRING-DESCRIPTION where the relevant substrings are annotated with attributes from the ATTRIBUTE-SET. The substrings corresponding to attributes could be just words ( big ) or phrases ( that is big ). Fig. 2 in Section 4.1 presents an example of one of these XML documents.
 There were 57 authors per domain who were identifiable via a unique identification number ( identify the examples in the corpus created by each person. More specifically, each author created seven examples for the furniture domain and six for the people domain. Mismatches between these numbers and the total number of elements in the corpus correspond to examples that were incorrectly classified and were not used in the experiments. 2.3.2. Evaluation metrics
The evaluation metrics we have used in this work are the usual ones in the literature for assessing the final output of NLG systems. Even when they were intended as metrics for machine translation and document summarization, they have been widely used as intrinsic evaluation metrics for comparison of system vs. human generated texts, specially in the REG Shared
Tasks ( Belz &amp; Gatt, 2007 ; Gatt, Belz, et al., 2008 ; Gatt et al., 2009 ) and later studies ( Gatt &amp; Belz, 2010 ): ference between two sequences (in this case the sequences compared are the string generated by the system and the one in the corpus). The Levenshtein distance between two strings is given by the minimum number of operations needed to for insertions and deletions was set to 1, and 2 for substitutions. Therefore, this metric results in an integer bounded by the length of the longest description in the pair being compared.
 gram precision is calculated with respect to reference lexicalizations. The n -gram lengths usually range from 1 to 4. The metric is adjusted in order to penalize over-generation of common n -grams and favour short and simple sentences. BLEU ranges between 0 and 1.
 quality of an automatically extracted summary by comparing it to other ideal texts created by humans. The measures count the number of overlapping units such as n -gram, word sequences, and word pairs between the computer-generated text to be evaluated and the ideal ones created by humans. We have used the following ROUGE measures:  X  ROUGE-N , that counts the number of n -grams that match in the texts being compared. The n -gram lengths usually  X  ROUGE-L , based on longest common subsequence (LCS) statistics between a candidate and a reference text.  X  ROUGE-SN , that takes into account bigrams that do not have to be consecutive in the text, but could present a max-
As an example, Table 2 presents the evaluation results obtained for two pairs of compared sentences: (a) A big green desk (b) A big green desk
When both sentences are equal (a) the edit distance is 0 (no edit operations required) and all the BLEU and ROUGE mea-sures are 1 as all n -grams are equal. However, when one word is different (b) the edit distance is 2 (one deletion and one so BLEU-{3,4} and ROUGE-{3,4} results are 0.

These metrics must be used with care. It has been observed that there is a lack of correlation between automatic metrics
However, we consider that the chosen metrics (string-edit distance, BLEU and ROUGE) are appropriate for this work as we are concerned with whether taking individual author preferences gives rise to a better match (on average) to a corpus that represents outputs by those authors. This task can be accomplished by comparison with a multi-authored corpus as
TUNA using automatic metrics that compute how similar two texts are in syntactic and lexical terms. The quality of the gen-erated references in terms of content or task fulfillment are beyond the scope of this work.

Although both BLEU and ROUGE are intended to be calculated against multiple reference outputs they have also been 2008 ; Gatt et al., 2009 ) but also in other works like ( Reiter &amp; Belz, 2009 )or( Gatt &amp; Belz, 2010 ). 3. Case-based reasoning derived from past experiences. Each problem is considered as a case from the domain, and a new problem is solved by retrieving one or more past cases from a case base, reusing them somehow, revising the solutions obtained, and retaining the new experience by including it in the case base.

Reasoning by reusing past experiences is frequently applied to solve problems. Several studies ( Schank, 1982; Anderson, 1983 ) have given empirical evidence for the dominant role of specific, previously experienced situations in human problem-solving.

In CBR terminology, a case usually denotes a problem situation. A previously experienced situation, which has been cap-3.1. CBR cycle
The general CBR cycle may be described by the following four processes: 1. RETRIEVE the most similar case or cases. This task starts with a (partial) problem description, and ends when the best-matching previous case has been found. 2. REUSE the information and knowledge in the case retrieved to solve the problem. The reuse of the retrieved case solution in the context of the new case focuses on two aspects: the differences between the past and the current case and what part of a retrieved case can be transferred to the new case. success. Otherwise, it repairs the case solution using domain-specific knowledge. is useful to retain from the new problem-solving episode into the existing knowledge. 3.2. Case Retrieval Nets: a model for storing the case base
Case Retrieval Nets (CRNs) ( Lenz &amp; Burkhard, 1996 ) are a memory model developed to improve the efficiency of the retrieval task in the CBR cycle. As its name indicates, CRNs are organized in nets. The most fundamental item in the con-text of the CRNs is Information Entities (IEs). These represent any basic knowledge item in the form of an attribute-value additional nodes denoting the particular cases. IE nodes are connected by similarity arcs, and a case node is reachable from its constituting IE nodes via relevance arcs. Different degrees of similarity and relevance are expressed by varying arcs weights. Given this structure, case retrieval is carried out by activating the IEs given in the query case, propagating case nodes.

An example of a Case Retrieval Net applied to the domain of travel agencies is shown in Fig. 1 . Rectangles represent entity nodes, with their corresponding attribute-value pairs. Hexagons are case nodes, with the description that identifies them univocally. Entity nodes are related among themselves by arcs with black arrowheads, and they are related with cases by arcs with white arrowheads. Weights associated to arcs are not represented in the figure, and arcs with zero weight are omitted.

CRNs present two important features that make them especially suitable for the task we are undertaking. First, CRNs can conceptual representation. Not always will all sentences have the same structure (e.g. some may have complements and oth-ers may not), and in the specific case of referring expressions, not all of them may contain the same attributes when being for some cases but not for others. Many CBR solutions describe exactly what the elements we can expect in any case are. But mation to be conveyed in the final text. 4. A CBR approach to referring expression lexicalization use a case-based reasoning approach where past referring expressions can be reused for lexicalizing new ones. Note that our lexicalization module. As case base, we have used the two domains of the TUNA corpus: people and furniture. 4.1. The case base
As a model for storing the case base, we have used a Case Retrieval Net. This model is appropriate because our cases are formed by attribute-value pairs (elements ATTRIBUTE inside the the queries to the module will not always have the same elements. When the system is asked for the lexical realization of a new referring expression, it looks for other referring expressions related to the set of attributes that define them. In our approach, a case consists of a description of the problem ( ( DESCRIPTION considered as a text template). For each attribute-value pair in the (IE) is created, and for each case a node containing links to the IEs that compose the case is created too. Each element in DESCRIPTION is considered as a slot of the template that will be later completed with the corresponding infor-mation. DESCRIPTIONs are therefore considered as lexicalization solutions to the cases. An example can be seen in Fig. 2 and Table 3 .

As the different entities are inserted while composing the net, similarities have to be established between them. Taking bute. In addition, the similarity between equal values of the same attribute will always be 1 (the maximum possible) and the similarity between values of different attributes will always be 0. The rest of defined similarities are the following: type and colour . All the values that are not equal have a similarity of 0.2. It was decided to use a non-zero similarity because at least all the values are of the same type (different kinds of furniture or different colours), and this is always more similar than being of different types. (as in the case of left and right ). x-dimension and y-dimension . The similarity between the two values is calculated as the relative distance between them, considering the total number of rows and columns using Formulas (1) and (2) . The similarity between equal values will always be 1, but for example in the case of x = 2 and x = 4 it will be 0.4, or with y = 1 and y = 2 it will be 0.33.
The rest of the attributes ( size , age , hairColour , hasShirt butes will always be 0.

Similarities were determined in an intuitive way, but did not consider that people may find colours like blue and green more similar than blue and red , or opposite orientations more similar than adjacent ones. More work about how people per-ceive the similarities between these values will be addressed in future work. 4.2. CBR cycle for lexicalization of referring expressions
Each process in the CBR cycle is explained in the following subsections. 4.2.1. Case retrieval
Attribute-value pairs from the ATTRIBUTE-SET that has to be lexicalized are considered as the query. In our module the retrieval of cases is directly handled by the Case Retrieval Net and its method of similarity propagation. Starting from the value for each case in the case base. The cases retrieved with higher activation are more similar to the given query and are ordered by preference taking into account the attributes they contain. The system organizes these cases into four different groups from higher to lower preference: 1. Matching case . Cases that contain exactly the same attributes as the query. 2. Encompassing case . Cases that contain all the attributes in the query, and some additional ones. 3. Included case . Cases that lack some attributes from the query and have no extra ones. 4. Overlapping case . Cases that lack some attributes from the query, but have some extra ones that were not in the query. A graphical representation of each situation is presented in Fig. 3 .

The cases with maximum activation are classified using these groups, and the order given is the preferred order to choose the most suitable case for the query. If more than one case from the same group have the maximum activation, the retrieved case is chosen randomly among them (more refined strategies like favoring the case that is easiest to adapt or that has min-imal information loss will be addressed in the future). 4.2.2. Case adaptation or reuse butes in the query not appearing in the case are lost.

In order to adapt each of the attributes inside the template, the system checks whether the values of each attribute are the expressions are the ones most frequently used in the corpus. Determiners or other parts of the template that do not corre-spond to any of the attributes are always included. 4.2.3. Case revision and retainment
Once a case solution has been generated, whether correctly or not, an opportunity to learn arises. At the moment, this task is not implemented in our CBR module. It would be very useful to incorporate new knowledge in the case base, but when dealing with natural language this could be a very challenging task. Due to the constraints associated with language use, the presence of an expert in the domain is required to revise the achieved results achieved from the module, retaining complex. 4.3. An example of system operation An example is given to show how the algorithm works. Suppose that the algorithm is presented with a query like:
TYPE: COLOUR: Y-DIMENSION: chair grey 2
The net retrieves a case from the ones considered matching cases because it contains exactly the same attributes as the query:
TYPE: COLOUR: Y-DIMENSION: chair grey 3 with the grey chair in the bottom row as associated referring expression, and corresponding to the following template:  X  X  the  X  X  &lt;
ATTRIBUTE name = colour value = grey string = X  X  grey X  X  / &gt; &lt;
ATTRIBUTE name = type value = chair string = X  X  chair X  X  / &gt; &lt; ATTRIBUTE name = y-dimension value = 3 string = X  X  in the bottom row X  X  &gt;
In order to adapt the template retrieved we must check whether the values in the case retrieved match the values in the row . 5. Evaluation of personal preferences
We tested the system explained in Section 4 following two different approaches: one that takes personal preferences into ized, personal preference would not influence the results obtained. We also analyzed the performance of the algorithm in terms of the type of case retrieved and the usage of default values in the lexicalizations. 5.1. Lexical variation in the TUNA corpus
The TUNA corpus contains 398 examples for the furniture domain and 340 for the people domain, with 6 and 12 attri-butes per element respectively. Table 4 shows the lexical variation found for the different attributes in both domains (num-ber of different lexicalizations used per attribute vs. total number of mentions to this attribute). The number of possible calized using single words or expressions (e.g. large ), and sometimes using phrases (e.g. that is big ). Therefore, we are corpus.

For each attribute the distribution of lexicalizations is homogeneous for the different values. The only exception is the value grey of the attribute colour in the furniture domain. Whereas the other colours have an average of 2 lexicalizations the problem of ambiguity in annotation. Usage of grey in TUNA corpus annotations is overloaded, being used indistintly to used sometimes (but not always) as a synonym of grey.

It is interesting to note that the attributes orientation , x -dimension and y-dimension present high variation in both do-mains. In the people domain attributes hasShirt , hasSuit and hasTie have a 100% variation, with a different lexicalization iation correspond to features that do not have an easy and fixed lexicalization as could be the case of colours.
When analyzing the corpus separately by its individual authors, variation is quite low. It seems that authors are fairly for both furniture and people. The total number of mentions for each attribute is added for easier comparison of results. An average variation of 0% means that none of the authors was inconsistent with themselves, that is, everytime they used the same value for an attribute they used the same lexical expression. This kind of coherence (as inverse to variation) is only perfect in attributes like hasHair or hasTie that were barely used. All authors present a degree of variation lower than resentations of objects), whereas people items were real photographs. This could have helped to make authors rely on ste-reotypical verbalisations of  X  X  X asic X  X  visual properties like size and colour.

The high lexical variation shown by the TUNA corpus, which only contains two different domains and a limited number of language in general and referring expressions in particular. However, it seems that this variation is due to differences be-tween different authors as they tend to be consistent with themselves. 5.2. Experiments
For the approach without taking into account any kind of personal preferences, we used all the examples in the corpus at the same time (separated by domains). In our second approach we tried to reproduce the lexicalization preferences of dif-ferent people so we divided the corpus by its individual authors. Here we faced the problem of each author having only a few examples in the corpus (seven for furniture and six for people).

For the approach without taking into account personal preferences we divided the complete corpus in random sets of se-ven and six examples (for furniture and people respectively) in order to have results easily comparable with the second ap-proach. We then performed a 7-fold cross validation for the furniture domain and a 6-fold cross validation for the people domain for each of these sets, so each example in the corpus was generated by the CBR algorithm trained with only six (fur-niture) or five (people) examples. The results obtained were measured using string-edit distance, BLEU and ROUGE (see Sec-tion 2.3.2 ). They are shown in Table 7 .

An important issue is how to resort to default lexicalizations when the values of the case retrieved and the query are not the same. When no personal data is taken into account, the default values for each attribute and value are computed over the complete corpus. When the system is working over each author X  X  data, the default values used correspond to those belonging to this specific author.

The results obtained for this initial system are rather poor when it comes to matching the specific expressions used in the corpus. We consider this to be due to the fact that the nature of the corpus indicates a broad variation in the type of expression used, aimed at identifying a number of possible ways of describing referents as actually done by human authors, expressions, and most times the chosen lexicalization is correct but does not correspond to the original one in the corpus. matches, and in these cases the best case is chosen randomly.

In our second approach we tried to reproduce the lexicalization preferences of the different authors of the corpus. We divided the corpus by authors, and for the set of examples created by each of them we also performed a cross validation.
A 7-fold cross validation was performed for the furniture domain and a 6-fold cross validation for the people domain. A Wil-coxon Signed Ranks Test was performed in order to test if differences between both versions of the system were reliably dif-ferent. Results are shown in Table 8 .

As we can observe from the results, the system that takes into account the preferences of the authors reduced the string-importance of taking personal preferences into account when dealing with the lexicalization of referring expressions.
Improvement in the BLEU and ROUGE values also supports these observations: there is a higher coincidence of n -grams when personal preferences are considered. The Wilcoxon Test results show that all differences between both approaches are statistically significant ( p &lt; 0.001).

The distribution of the type of cases retrieved by the system can be seen in Table 9 . Both approaches present similar num-butes as the query) and the encompassing type (cases with the same attributes as the query and some more). And for the people domain in the approach that does not take personal preferences into account there are more encompassing and over-lapping cases whereas in the second approach the number of matching cases is higher. These results are expected if we con-of attributes (increasing the number of overlapping and encompassing cases when preferences are not taken into account), though any given person will manifest personal preferences in the choice of attributes (hence we have more matching cases when such preferences are taken into account). Considering the performance of the CBR system, the high number of over-lapping and included cases is expected given the low number of examples used for training. From the point of view of pref-erence considerations, matching and encompassing cases are more used when personal preferences are taken into account.
This is good as these two types are the only ones where there is no loss of information from the query, and it is due to the more similar cases in the training set when it is generated with the examples made by an individual author.
Another important issue is how often the system has to resort to default lexicalizations when the values of the case re-trieved and the query are not the same. The percentages of default lexicalizations used for both approaches can be seen in 5.3. Some examples
Here we show a few examples of the referring expressions generated by the system with and without personal prefer-ences. We will start with the following query:
TYPE: ORIENTATION: chair right that corresponds to the referring expression a chair facing to the right in the corpus.

If we do not consider personal preferences, the net retrieves a case from the encompassing case group (with more attri-butes than the query):
TYPE: COLOUR: ORIENTATION: chair red right lique angle facing right when it was facing right.

Taking into account personal preferences, the net retrieves one of the cases corresponding to the same user that is also of type encompassing case :
TYPE: COLOUR: ORIENTATION: chair grey back in the use of determiners and the orientation expression.
 In order to exemplify the algorithm in the people domain, the algorithm receives a query like:
TYPE: Y-DIMENSION: person 1 that corresponds to the referring expression top man in the corpus.
 Not considering personal preferences, the net retrieves an encompassing case, with more attributes than the query:
TYPE: HAS _ BEARD: Y-DIMENSION: person true 1 ( male subject on top ) is very different from the original one.

If we take into account personal preferences, the net also retrieves an encompassing case from the ones that correspond to the same user:
TYPE: X-DIMENSION: Y-DIMENSION: person 5 1 with last top man as the associated referring expression. After adaptation the referring expression top man is generated. In this case both the original and the obtained expressions are the same even when the cases were quite different. This is be-cause both cases show the same preferences in syntax and vocabulary (short sentence without prepositions and the use of the word man for the type attribute). 5.4. Discussion
Some strategies developed for referring expression generation are related to this work as they have considered the per-sonal preferences of different people (see Section 2.2 ). Although those systems used similar approaches to the generation and lexicalization of referring expressions, none of them have compared the impact of personal preferences on these tasks ( Bohnet, 2008, 2009; Fabbrizio et al., 2008 ). However, it would be interesting to know whether their systems would have obtained the same results even if they had not taken into account the personal preferences of the authors. Their results are presented in Table 11 for comparison with our own results. Note that data is not directly comparable as their results are computed over a 20% of the TUNA corpus (the remaining 80% was used for training). We have not added to the compar-their reports. In the cases where the participants presented more than one system, we have taken only the one with better results. As we can see in the table, our approach outperforms the other systems in both domains, with the only exception of Bohnet (2008) in the people domain.
 It is not the first time that CBR techniques have been used for referring expression generation. Both Kelleher and Mac Namee (2008) and Herv X s and Gerv X s (2009) applied case-based reasoning for the lexicalization of referring expressions.
Using each of the situations from the TUNA corpus as a template, Kelleher and Mac Namee relied on similarity between cases in both the attributes and their values in the templates, whereas Herv X s and Gerv X s considered the presence of attributes important and took into account the values in a more refined adaptation of the cases to create the solution. 6. Conclusions and future work
The data presented in this paper displays the high lexical variability of natural language in general and the TUNA corpus in particular. We have demonstrated that the performance of a lexicalization algorithm that tries to imitate human-generated referring expressions improves greatly by modelling particular preferences. Our case-based approach was easily adapted to model this personalization effort by narrowing the training set to only those descriptions produced by a single person and having the software model that particular way of generating referring expressions.

For this solution the CBR approach turned out to be quite appropriate, as the case base could store only the descriptions taining the same style. However, sometimes the system is not capable of finding a perfect match for a given query, but a solution with more attributes than the query, or one that lacks some of its attributes. This could be avoided by improving the reuse stage of the CBR module. In the former case the resulting adaptation is a partial solution to the problem posed slots for which the query does not specify any value. Better results can be obtained by consulting the system knowledge base retrieved case for those attributes.
 Our approach, and other similar ones already mentioned, are based on the idea of each person having specific preferences.
However, all of them miss the broader idea of preference features that can be shared by different people. For example, many people will share preferences in the use of determiners or the choice of vocabulary for certain concepts. In future work we accommodate its vocabulary and means of expression to those shown by the user ( Pickering &amp; Garrod, 2004 ). Style-based approach to the lexicalization of referring expressions could then be used to detect the style of expression the user is employing, and then adapt system responses to that style. The style detection would be especially important in adapting systems of this kind as the system is unlikely to have information about who it is communicating with. Recently, style &amp; Evans, 2005; Mairesse &amp; Walker, 2011 ).
 Acknowledgments
This research is funded by the Spanish Ministry of Education and Science (TIN2009-14659-C03-01) and Universidad Com-plutense de Madrid (GR58/08). We thank the editors and reviewers, as well as Mark Finlayson and Pablo Moreno, for their helpful comments and discussion.
 References
