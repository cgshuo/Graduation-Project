 Topic models, exemplified by latent Dirichlet alloca-tion (LDA) (Blei et al., 2003), discover latent themes present in text collections.  X  X opics X  discovered by topic models are multinomial probability distribu-tions over words that evince thematic coherence. Topic models are used in computational biology, com-puter vision, music, and, of course, text analysis.
One of LDA X  X  virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum  X  e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2).

These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn X  X  cheap. Particularly for interactive settings (Hu et al., 2011), efficient inference would improve perceived latency.
 S PARSE LDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA based on a refac-torization of the conditional topic distribution (re-viewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we pro-vide a factorization for tree-based models within a broadly applicable inference framework that empiri-cally improves the efficiency of inference (Section 5). Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used tree-structured multinomials to model selectional restric-tions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009).

Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior that generates K topics. Like vanilla LDA, these topics are distribu-tions over words. Unlike vanilla LDA, their structure correlates words. Internal nodes have a distribution  X  k,i over children, where  X  k,i comes from per-node Dirichlet parameterized by  X  i . 1 Each leaf node is associated with a word, and each word must appear in at least (possibly more than) one leaf node. To generate a word from topic k , start at the root. Select a child x 0  X  Mult (  X  k, ROOT ) , and traverse the tree until reaching a leaf node. Then emit the leaf X  X  associated word. This walk replaces the draw from a topic X  X  multinomial distribution over words. The rest of the generative process for LDA remains the same, with  X  , the per-document topic multinomial, and z , the topic assignment.

This tree structure encodes correlations. The closer types are in the tree, the more correlated they are. Because types can appear in multiple leaf nodes, this encodes polysemy. The path that generates a token is an additional latent variable we must sample.
Gibbs sampling is straightforward because the tree-based prior maintains conjugacy (Andrzejewski et al., 2009). We integrate the per-document topic dis-tributions  X  and the transition distributions  X  . The remaining latent variables are the topic assignment z and path l , which we sample jointly: 2 where n k | d is topic k  X  X  count in the document d ;  X  k is topic k  X  X  prior; Z  X  and L  X  are topic and path assignments excluding w d,n ;  X  i  X  j is the prior for edge i  X  j , n i  X  j | t is the count of edge i  X  j in topic k ; and j 0 denotes other children of node i .
The complexity of computing the sampling distri-bution is O ( KLS ) for models with K topics, paths at most L nodes long, and at most S paths per word type. In contrast, for vanilla LDA the analogous conditional sampling distribution requires O ( K ) . The S PARSE LDA (Yao et al., 2009) scheme for speeding inference begins by rearranging LDA X  X  sam-pling equation into three terms: 3
Following their lead, we call these three terms  X  X uckets X . A bucket is the total probability mass marginalizing over latent variable assignments (i.e., s ets). The three buckets are a smoothing only bucket s
LDA , document topic bucket r LDA , and topic word bucket q LDA (we use the  X  X DA X  subscript to contrast with our method, for which we use the same bucket names without subscripts).

Caching the buckets X  total mass speeds the compu-tation of the sampling distribution. Bucket s LDA is shared by all tokens, and bucket r LDA is shared by a document X  X  tokens. Both have simple constant time updates. Bucket q LDA has to be computed specifi-cally for each token, but only for the (typically) few types with non-zero counts in a topic.

To sample from the conditional distribution, first sample which bucket you need and then (and only then) select a topic within that bucket. Because the topic-term bucket q LDA often has the largest mass and has few non-zero terms, this speeds inference. In this section, we extend the sampling techniques for S PARSE LDA to tree-based topic modeling. We first factor Equation 1:
Henceforth we call N k, X  the normalizer for path  X  in topic k , S  X  the smoothing factor for path  X  , and O k, X  the observation for path  X  in topic k , which are
Equation 3 can be rearranged in the same way as Equation 5, yielding buckets analogous to S
Buckets sum both topics and paths. The sampling process is much the same as for S PARSE LDA : select which bucket and then select a topic / path combina-tion within the bucket (for a slightly more complex example, see Algorithm 1).
Recall that one of the benefits of S PARSE LDA was that s was shared across tokens. This is no longer possible, as N k, X  is distinct for each path in tree-based LDA. Moreover, N k, X  is coupled; changing n i  X  j | k in one path changes the normalizers of all cousin paths (paths that share some node i ).
This negates the benefit of caching s , but we re-cover some of the benefits by splitting the normalizer to two parts: the  X  X oot X  normalizer from the root node (shared by all paths) and the  X  X ownstream X  normal-izer. We precompute which paths share downstream normalizers; all paths are partitioned into cousin sets, defined as sets for which changing the count of one member of the set changes the downstream normal-izer of other paths in the set. Thus, when updating the counts for path l , we only recompute N k,l 0 for all l in the cousin set.

S PARSE LDA  X  X  computation of q , the topic-word bucket, benefits from topics with unobserved (i.e., zero count) types. In our case, any non-zero path, a path with any non-zero edge, contributes. 4 To quickly determine whether a path contributes, we introduce an edge-masked count (EMC) for each path. Higher order bits encode whether edges have been observed and lower order bits encode the number of times the path has been observed. For example, if a path of length three only has its first two edges observed, its EMC is 11000000 . If the same path were observed seven times, its EMC is 11100111 . With this formu-lation we can ignore any paths with a zero EMC. Efficient sampling with refined bucket While caching the sampling equation as described in the previous section improved the efficiency, the smooth-ing only bucket s is small, but computing the asso-ciated mass is costly because it requires us to con-sider all topics and paths. This is not a problem for SparseLDA because s is shared across all tokens. However, we can achieve computational gains with an upper bound on s ,
A sampling algorithm can take advantage of this by not explicitly calculating s . Instead, we use s 0 as proxy, and only compute the exact s if we hit the bucket s 0 (Algorithm 1). Removing s 0 and always computing s yields the first algorithm in Section 4. Sorting Thus far, we described techniques for ef-ficiently computing buckets, but quickly sampling assignments within a bucket is also important. Here we propose two techniques to consider latent vari-able assignments in decreasing order of probability mass. By considering fewer possible assignments, we can speed sampling at the cost of the overhead of maintaining sorted data structures. We sort top-ics X  prominence within a document ( S D) and sort the topics and paths of a word ( S W).

Sorting topics X  prominence within a document (
D) can improve sampling from r and q ; when we need to sample within a bucket, we consider paths in decreasing order of n k | d .

Sorting path prominence for a word ( S W) can im-prove our ability to sample from q . The edge-masked count (EMC), as described above, serves as a proxy for the probability of a path and topic. If, when sam-pling a topic and path from q , we sample based on the decreasing EMC, which roughly correlates with path probability. In this section, we compare the running time 5 of our sampling algorithm ( F AST ) and our algorithm with the refined bucket ( RB ) against the unfactored Gibbs sampler (N A  X  IVE ) and examine the effect of sorting.
Our corpus has editorials from New York Times from 1987 to 1996. 6 Since we are interested in vary-ing vocabulary size, we rank types by average tf-idf and choose the top V . WordNet 3.0 generates the cor-relations between types. For each synset in WordNet, we generate a subtree with all types in the synset X  that are also in our vocabulary X  X s leaves connected to a common parent. This subtree X  X  common parent is then attached to the root node.
 We compared the F AST and F AST -RB against N
A  X  IVE (Table 1) on different numbers of topics, var-ious vocabulary sizes and different numbers of cor-relations. F AST is consistently faster than N A  X  IVE and F AST -RB is consistently faster than F AST . Their benefits are clearer as distributions become sparse (e.g., the first iteration for F AST is slower than later iterations). Gains accumulate as the topic number increases, but decrease a little with the vocabulary size. While both sorting strategies reduce time, sort-ing topics and paths for a word ( S W) helps more than sorting topics in a document ( S D), and combining the two is (with one exception) better than either alone.
As more correlations are added, N A  X  IVE  X  X  time in-creases while that of F AST -RB decreases. This is be-cause the number of non-zero paths for uncorrelated words decreases as more correlations are added to the model. Since our techniques save computation for every zero path, the overall computation decreases as correlations push uncorrelated words to a limited number of topics (Figure 1). Qualitatively, when the synset with  X  X ing X  and  X  X aron X  is added to a model, it is associated with  X  X rug, inmate, colombia, water-front, baron X  in a topic; when  X  X ing X  is correlated with  X  X ueen X , the associated topic has  X  X ing, parade, museum, queen, jackson X  as its most probable words. These represent reasonable disambiguations. In con-trast to previous approaches, inference speeds up as topics become more semantically coherent (Boyd-Graber et al., 2007). We demonstrated efficient inference techniques for topic models with tree-based priors. These methods scale well, allowing for faster exploration of models that use semantics to encode correlations without sac-rificing accuracy. Improved scalability for such algo-rithms, especially in distributed environments (Smola and Narayanamurthy, 2010), could improve applica-tions such as cross-language information retrieval, unsupervised word sense disambiguation, and knowl-edge discovery via interactive topic modeling. We would like to thank David Mimno and the anony-mous reviewers for their helpful comments. This work was supported by the Army Research Labora-tory through ARL Cooperative Agreement W911NF-09-2-0072. Any opinions or conclusions expressed are the authors X  and do not necessarily reflect those of the sponsors.

 Topic models, exemplified by latent Dirichlet alloca-tion (LDA) (Blei et al., 2003), discover latent themes present in text collections.  X  X opics X  discovered by topic models are multinomial probability distribu-tions over words that evince thematic coherence. Topic models are used in computational biology, com-puter vision, music, and, of course, text analysis.
One of LDA X  X  virtues is that it is a simple model that assumes a symmetric Dirichlet prior over its word distributions. Recent work argues for structured distributions that constrain clusters (Andrzejewski et al., 2009), span languages (Jagarlamudi and Daum  X  e III, 2010), or incorporate human feedback (Hu et al., 2011) to improve the quality and flexibility of topic modeling. These models all use different tree-based prior distributions (Section 2).

These approaches are appealing because they preserve conjugacy, making inference using Gibbs sampling (Heinrich, 2004) straightforward. While straightforward, inference isn X  X  cheap. Particularly for interactive settings (Hu et al., 2011), efficient inference would improve perceived latency.
 S PARSE LDA (Yao et al., 2009) is an efficient Gibbs sampling algorithm for LDA based on a refac-torization of the conditional topic distribution (re-viewed in Section 3). However, it is not directly applicable to tree-based priors. In Section 4, we pro-vide a factorization for tree-based models within a broadly applicable inference framework that empiri-cally improves the efficiency of inference (Section 5). Trees are intuitive methods for encoding human knowledge. Abney and Light (1999) used tree-structured multinomials to model selectional restric-tions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al., 2007). In both cases, the tree came from WordNet (Miller, 1990), but the tree could also come from domain experts (Andrzejewski et al., 2009).

Organizing words in this way induces correlations that are mathematically impossible to represent with a symmetric Dirichlet prior. To see how correlations can occur, consider the generative process. Start with a rooted tree structure that contains internal nodes and leaf nodes. This skeleton is a prior that generates K topics. Like vanilla LDA, these topics are distribu-tions over words. Unlike vanilla LDA, their structure correlates words. Internal nodes have a distribution  X  k,i over children, where  X  k,i comes from per-node Dirichlet parameterized by  X  i . 1 Each leaf node is associated with a word, and each word must appear in at least (possibly more than) one leaf node. To generate a word from topic k , start at the root. Select a child x 0  X  Mult (  X  k, ROOT ) , and traverse the tree until reaching a leaf node. Then emit the leaf X  X  associated word. This walk replaces the draw from a topic X  X  multinomial distribution over words. The rest of the generative process for LDA remains the same, with  X  , the per-document topic multinomial, and z , the topic assignment.

This tree structure encodes correlations. The closer types are in the tree, the more correlated they are. Because types can appear in multiple leaf nodes, this encodes polysemy. The path that generates a token is an additional latent variable we must sample.
Gibbs sampling is straightforward because the tree-based prior maintains conjugacy (Andrzejewski et al., 2009). We integrate the per-document topic dis-tributions  X  and the transition distributions  X  . The remaining latent variables are the topic assignment z and path l , which we sample jointly: 2 where n k | d is topic k  X  X  count in the document d ;  X  k is topic k  X  X  prior; Z  X  and L  X  are topic and path assignments excluding w d,n ;  X  i  X  j is the prior for edge i  X  j , n i  X  j | t is the count of edge i  X  j in topic k ; and j 0 denotes other children of node i .
The complexity of computing the sampling distri-bution is O ( KLS ) for models with K topics, paths at most L nodes long, and at most S paths per word type. In contrast, for vanilla LDA the analogous conditional sampling distribution requires O ( K ) . The S PARSE LDA (Yao et al., 2009) scheme for speeding inference begins by rearranging LDA X  X  sam-pling equation into three terms: 3
Following their lead, we call these three terms  X  X uckets X . A bucket is the total probability mass marginalizing over latent variable assignments (i.e., s ets). The three buckets are a smoothing only bucket s
LDA , document topic bucket r LDA , and topic word bucket q LDA (we use the  X  X DA X  subscript to contrast with our method, for which we use the same bucket names without subscripts).

Caching the buckets X  total mass speeds the compu-tation of the sampling distribution. Bucket s LDA is shared by all tokens, and bucket r LDA is shared by a document X  X  tokens. Both have simple constant time updates. Bucket q LDA has to be computed specifi-cally for each token, but only for the (typically) few types with non-zero counts in a topic.

To sample from the conditional distribution, first sample which bucket you need and then (and only then) select a topic within that bucket. Because the topic-term bucket q LDA often has the largest mass and has few non-zero terms, this speeds inference. In this section, we extend the sampling techniques for S PARSE LDA to tree-based topic modeling. We first factor Equation 1:
Henceforth we call N k, X  the normalizer for path  X  in topic k , S  X  the smoothing factor for path  X  , and O k, X  the observation for path  X  in topic k , which are
Equation 3 can be rearranged in the same way as Equation 5, yielding buckets analogous to S
Buckets sum both topics and paths. The sampling process is much the same as for S PARSE LDA : select which bucket and then select a topic / path combina-tion within the bucket (for a slightly more complex example, see Algorithm 1).

Recall that one of the benefits of S PARSE LDA was that s was shared across tokens. This is no longer possible, as N k, X  is distinct for each path in tree-based LDA. Moreover, N k, X  is coupled; changing n i  X  j | k in one path changes the normalizers of all cousin paths (paths that share some node i ).
This negates the benefit of caching s , but we re-cover some of the benefits by splitting the normalizer to two parts: the  X  X oot X  normalizer from the root node (shared by all paths) and the  X  X ownstream X  normal-izer. We precompute which paths share downstream normalizers; all paths are partitioned into cousin sets, defined as sets for which changing the count of one member of the set changes the downstream normal-izer of other paths in the set. Thus, when updating the counts for path l , we only recompute N k,l 0 for all l in the cousin set.

S PARSE LDA  X  X  computation of q , the topic-word bucket, benefits from topics with unobserved (i.e., zero count) types. In our case, any non-zero path, a path with any non-zero edge, contributes. 4 To quickly determine whether a path contributes, we introduce an edge-masked count (EMC) for each path. Higher order bits encode whether edges have been observed and lower order bits encode the number of times the path has been observed. For example, if a path of length three only has its first two edges observed, its EMC is 11000000 . If the same path were observed seven times, its EMC is 11100111 . With this formu-lation we can ignore any paths with a zero EMC. Efficient sampling with refined bucket While caching the sampling equation as described in the previous section improved the efficiency, the smooth-ing only bucket s is small, but computing the asso-ciated mass is costly because it requires us to con-sider all topics and paths. This is not a problem for SparseLDA because s is shared across all tokens. However, we can achieve computational gains with an upper bound on s ,
A sampling algorithm can take advantage of this by not explicitly calculating s . Instead, we use s 0 as proxy, and only compute the exact s if we hit the bucket s 0 (Algorithm 1). Removing s 0 and always computing s yields the first algorithm in Section 4. Sorting Thus far, we described techniques for ef-ficiently computing buckets, but quickly sampling assignments within a bucket is also important. Here we propose two techniques to consider latent vari-able assignments in decreasing order of probability mass. By considering fewer possible assignments, we can speed sampling at the cost of the overhead of maintaining sorted data structures. We sort top-ics X  prominence within a document ( S D) and sort the topics and paths of a word ( S W).

Sorting topics X  prominence within a document (
D) can improve sampling from r and q ; when we need to sample within a bucket, we consider paths in decreasing order of n k | d .

Sorting path prominence for a word ( S W) can im-prove our ability to sample from q . The edge-masked count (EMC), as described above, serves as a proxy for the probability of a path and topic. If, when sam-pling a topic and path from q , we sample based on the decreasing EMC, which roughly correlates with path probability. In this section, we compare the running time 5 of our sampling algorithm ( F AST ) and our algorithm with the refined bucket ( RB ) against the unfactored Gibbs sampler (N A  X  IVE ) and examine the effect of sorting.
Our corpus has editorials from New York Times from 1987 to 1996. 6 Since we are interested in vary-ing vocabulary size, we rank types by average tf-idf and choose the top V . WordNet 3.0 generates the cor-relations between types. For each synset in WordNet, we generate a subtree with all types in the synset X  that are also in our vocabulary X  X s leaves connected to a common parent. This subtree X  X  common parent is then attached to the root node.
 We compared the F AST and F AST -RB against N
A  X  IVE (Table 1) on different numbers of topics, var-ious vocabulary sizes and different numbers of cor-relations. F AST is consistently faster than N A  X  IVE and F AST -RB is consistently faster than F AST . Their benefits are clearer as distributions become sparse (e.g., the first iteration for F AST is slower than later iterations). Gains accumulate as the topic number increases, but decrease a little with the vocabulary size. While both sorting strategies reduce time, sort-ing topics and paths for a word ( S W) helps more than sorting topics in a document ( S D), and combining the two is (with one exception) better than either alone.
As more correlations are added, N A  X  IVE  X  X  time in-creases while that of F AST -RB decreases. This is be-cause the number of non-zero paths for uncorrelated words decreases as more correlations are added to the model. Since our techniques save computation for every zero path, the overall computation decreases as correlations push uncorrelated words to a limited number of topics (Figure 1). Qualitatively, when the synset with  X  X ing X  and  X  X aron X  is added to a model, it is associated with  X  X rug, inmate, colombia, water-front, baron X  in a topic; when  X  X ing X  is correlated with  X  X ueen X , the associated topic has  X  X ing, parade, museum, queen, jackson X  as its most probable words. These represent reasonable disambiguations. In con-trast to previous approaches, inference speeds up as topics become more semantically coherent (Boyd-Graber et al., 2007). We demonstrated efficient inference techniques for topic models with tree-based priors. These methods scale well, allowing for faster exploration of models that use semantics to encode correlations without sac-rificing accuracy. Improved scalability for such algo-rithms, especially in distributed environments (Smola and Narayanamurthy, 2010), could improve applica-tions such as cross-language information retrieval, unsupervised word sense disambiguation, and knowl-edge discovery via interactive topic modeling. We would like to thank David Mimno and the anony-mous reviewers for their helpful comments. This work was supported by the Army Research Labora-tory through ARL Cooperative Agreement W911NF-09-2-0072 and NSF grant #1018625. Any opinions or conclusions expressed are the authors X  and do not necessarily reflect those of the sponsors.

