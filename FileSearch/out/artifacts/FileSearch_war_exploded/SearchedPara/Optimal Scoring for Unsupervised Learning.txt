 statistical properties such as regularized LDA [5] and sparse discriminant analysis [2]. vector. In this paper we consider unsupervised learning problems by optimal scoring, which was framework by using the optimal scoring and the ridge penalty.
 This framework can be used for dimensionality reduction and clustering simultaneously. We are have sparse unsupervised learning algorithms.
 of A , tr( A ) be the trace of A , rk( A ) be the rank of A and k A k F = norm of A . cardinality of V j is n j so that [ x x i is in class j and e ij = 0 otherwise. Let  X  = diag( n 1 , . . . , n c ) ,  X   X  = ( n 1 , . . . , n c ) 0 and  X  = n , E 0 E =  X  and  X   X  1  X  = 1 c . 2.1 Scoring Matrices scoring or scaling for the j th class. Here we refine this definition as: c  X  ( c  X  1) matrix  X  is referred to as the class scoring matrix if it satisfies a specific example for  X  = (  X  1 , . . . ,  X  c  X  1 ) 0 . That is,  X  0 1 = for l = 2 , . . . , c  X  1 . Especially, when c = 2 ,  X  = ( 2.2 Optimal Scoring for LDA model, which is defined by H n XW = 0 . We thus impose 1 0 n E X  =  X  0  X  = 0 for consistency.
 Denote vectors of R . That is,  X  satisfies  X  0  X  = I c  X  1 and  X  0  X  1 2 = 0 . Here [  X  , 1  X  between-class scatter matrix is given by Accordingly, we can also write the generalized eigenproblem for the penalized LDA as because the total scatter matrix  X  is  X  = X 0 H n X . We now obtain between A in the penalized LDA and W in the penalized optimal scoring model (1). to a new framework for dimensionality reduction and clustering analysis simultaneously. 3.1 Framework model: this problem.
 W can be treated as a non-orthogonal projection matrix and H n XW is then the low-dimensional in clustering analysis. 3.2 Optimal Discriminant Clustering this case, we have without the explicit use of the feature matrix  X  X . Moreover, we can compute Z by X X X  X  R such that K ( x i , x j ) =  X  x 0 i  X  x j and K =  X  X  X  X 0 . Algorithm 1 Optimal Discriminant Clustering Algorithm 1: procedure ODC( H n X , c,  X  2 ) 2: Estimate  X  Y and  X  W according to Theorem 2; 3: Calculate Z = [ z 1 , . . . , z n ] 0 = H n X  X  W ; 4: Perform K -means on the z i ; 5: Return the partition of the z i as the partition of the x i . 6: end procedure 3.3 Related Work clustering.
 transformed feature space F , namely reproducing kernel Hilbert space (RKHS) tries to solve the problem of lowing relaxation problem can obtain that increasing in  X  . We now directly obtain the following theorem from Theorem 3.1 in [13]. Theorem 3 Let Y  X  and M  X  be the solution of Problem (3). Then of the input vector, and n is the number of samples in the dataset.
 optimal scoring framework in (2). four UCI datasets. Further details of these datasets are summarized in Table 1. To effectively evaluate the performance, we employed two typical measurements: the Normalized mance. More details and the corresponding implementations for both can be found in [11]. worth noting that two discriminative clustering algorithms: DisCluster [3] and DisKmeans [13], tation code for NC is available at http://www.cis.upenn.edu/  X  jshi/software/ . parameters in other clustering algorithms compared here are also searched in a wide range. NMI values in Table 2, our ODC outperforms other clustering algorithms on five datasets: ORL , SRBCT , iris , yeast and image segmentation . According to the CE values in Table 2, it datasets, and NC and DisKmeans algorithms can achieve the almost same performance with ODC dramatically different performance based on the NMI and CE. The main reason is that the final solution in DisCluster is very sensitive to the initial variables and numerical computation. Figure 1: The NMI versus the parameter  X  tuning in ODC on all datasets, where the NMI of K -image segmentation ; (h) statlog landsat satellite .
 In order to reveal the effect of the parameter  X  on ODC, Figures 1 and 2 depict the NMI and CE to the result in Figure 1, the effect of the parameter  X  becomes less pronounced in Figure 2. Table 2: Clustering results: the Normalized Mutual Information (NMI) and the Clustering Error (CE) (%) of all clustering algorithms are calculated on different datasets. Measure Dataset K -means NC DisCluster DisKmeans ODC CE (%) In this paper we have proposed a regression framework to deal with unsupervised dimensionality relationship with the discriminative clustering and spectral clustering. Figure 2: The CE (%) versus the parameter  X  tuning in ODC on all datasets, where the CE (%) of (g) image segmentation ; (h) statlog landsat satellite .
 natively consider the following optimization problem: under the constraints 1 0 n Y = 0 and Y 0 Y = I c  X  1 . We will study this further. Acknowledgement This work has been supported in part by program for Changjiang Scholars and Innovative Research Team in University (IRT0652, PCSIRT), China.
 Lagrange function: L ( Y , W , B , b ) = where B is a q  X  q symmetric matrix of Lagrange multipliers and b is a q  X  1 vector of Lagrange multipliers. By direct differentiation, it can be shown that Letting  X  X   X  Y = 0 , we have  X  Y = 0 and Substituting the second equation into the first equation, we further have and the associated eigenvectors of I n  X  X ( X 0 X +  X  2 I p )  X  1 X 0 . There exists such an n  X  ( n  X  p ) orthogonal matrix U 3 that its last column is 1  X   X  due to U 0 1 U 0 1 = I q and X 0 1 n = 0 . Moreover, we have f (  X  Y ,  X  W ) achieves its minimum 0 , otherwise the minimum value is q  X  rk( X ) 2 . ( w 11 , . . . , w 1 q , w 21 , . . . , w pq ) 0 . The Hessian matrix is then given by such that C 0 1 [ 1 n ,  X  Y ] = 0 , which is equivalent to C 0 1 1 n = 0 and C 0 1 U 1 = 0 . If rk( X )  X  q , we have C 0 1 X = 0 . Hence, This implies that (  X  Y ,  X  W ) is a minimizer of problem (2).
 and V = [ V 1 , V 2 ] where V 1 and V 2 are p  X  q and p  X  ( p  X  q ) . Thus, Moreover, we use the fact that because  X  i I q  X   X  2 for i = 1 , . . . , q are positive semidefinite. is n  X  n . Using this SVD, we have the same result as the case of n  X  p . [1] C. M. Bishop. Pattern Recognition and Machine Learning . Springer, first edition, 2007. [4] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification . John Wiley and Sons, New [8] C. H. Park and H. Park. A relationship between linear discriminant analysis and the gener-[9] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern
