 1 Weijieying Ren, 2 Lei Zhang, 2 Bo Jiang, 1 Zhefeng Wang, 1 Guangming guo, automatic multi-media annotation [1], text classification [12] and image annota-tion [17]. In multi-label learning, each instance is associated with a set of labels which share simialr semantic spaces. The major issue in multi-label classification is how to harness label correlations to improve the generalization performance [9, 23].
 that all the proper and actual labels of each instance are given [1, 8, 22]. However, in practical applications, training labels are obtained via crowd-sourcing and we may not have access to true labels of each training sample [24]. To tackle this problem, various methods have been further proposed along different directions, including random walk [7], metric learning algorithm[16], a mixed graph design-ing three types of label dependencies [19], a semi-supervised low-rank mapping method [11], etc. However, as stated in [21], these methods based on modeling the original label matrix may not accurately capture the relations between labels and features due to the missing labels.
 that they only considered data coming from a single view (a single resource) [13], which may not precisely characterize the intrinsic information of instances. As shown in Figure 1.(a), in the training process, we learned strong label correla-tions between green  X  X rass X  and white  X  X loud X . For the image (b),  X  X loud X  are less prominent and thus could be difficult to predict. Nevertheless, the label relevance based on color-feature still makes a correct judgement. Inversely, for the image (c), the same label correlations will mislead us to predict  X  X loud X  rather than  X  X iver X . Instead of single color-feature, multiple visual features termed as multi-view [2] will provide us with more comprehensive cognizance of the relations between input features and label information. structure, which can generate multiple types of views. Existing works included a feature selection method [18] that enforced a sparsity-inducing norm on the concentrated features from multiple views; a multi-view matrix recovery method [13] which seeked a shared low-rank feature representation; a multi-view spec-tral clustering algorithm[20] based on low-rank and sparse decomposition, etc. The nuclear norm was often adopted to address the rank minimization problem. These algorithms emphasized the significance of a shared feature representation of multiple views, while overlooking label relevance. Meanwhile, a low-rank con-straint was often restricted to obtain a robust subspace, and the relationships among nuclear norm and other norms (e.g. Frobenius norm) need to be further studied.
 address the Multi-view Multi-label Classification problem with Missing Labels (abbreviated as RLM-MCML framework). In this paper, we integrate input data from multiple views into a mixed feature matrix and augment the initial label matrix with correlation matrix to obtain a more authentic label assignments. A group-structured sparsity norm is adopted on the weight matrix for the ease of feature selection. Then, a nuclear norm is imposed to exploit global label correla-tions, and the manifold regularization is introduced to enhance local smoothness. The major contributions of this paper are summarized as follows:  X  Introducing a principle way of exploiting label correlations in the presence  X  Adopting Frobenius norm as a substitution for the geneal low-rank constraint  X  Employing an efficient algorithm to address the optimization issue of RLM- X  Conducting experiments on three authoritative datasets with six metrics to view multi-label problem and propose the RLM-MCML method. The technique to optimize the model is then discussed. Next, a variety of experiments are reported, followed by the conclusion and future work. view, Y  X &lt; N  X  m is the corresponding label matrix, and m is the total label size. To strengthen label correlations among multiple views, we adopt the method in [3] which integrates input data X i of each view into a mixed feature matrix square loss function, the labeling approximation error of N instances given in the weight matrix W  X &lt; m  X  d can be written as: tial set of labels are available. We can translate this problem into another two tangible guidelines: (1) anthentic label assignments can be estimated via label correlations; (2) the points close to each other are more likely to share a label, and multi-view data can be leveraged to preserve the label structure. into two components to formulize the two assumptions:  X  ( S ) utilizes multi-view data and label correlations to preserve the local ge-ometry structure of labels. Inspired by the idea of label propogating [21], we dealing with points near the intersection of two subsapces and also sensitive to the neighbor size [4]. In the following, we construct a low-rank graph to obtain the affinity matrix Z , which contains more comprehensive and complementary information than the neighborhood graph.
 The computation of the affinity matrix To guarantee the affinity matrix achieve good performance, we expect it to satisfy the following two assumptions: (1) each instances can be represented by its nearest neighbors; (2) similar in-stances can be clustered together. Inspired by the success of self-representation which has been widely used in subspace learning [4], we adopt this thchnique to encode the relationship between each instance and its neighbors. To further explore the comprehensiveness of features derived from multiple views, we seek a low-rank structure in the feature space to approximate the affinity matrix Z : mation of the input matrix X . Based on the certification in [6], the minimal cost of equation (5) can be denoted as: F = r +  X  2 P j&gt;r  X  2 j . Where r ,  X  is the rank and singular value of the input matrix X , respectively. Besides, the relationship between between nuclear norm and other sparsity norm, e.g. Frobenius needs to be further studies. Here we also restrict the affinity matrix Z with Frobenius norm to substitute the nuclear norm. in (5), while there is a closed-form solution in the former formulation. Theorem 1. For any given affinity matrix Z , which is the approximate expres-sion of the initial input matrix, the subspace matrix restricted by Frobenius norm in problem (6), can be viewed as a surrogate of a low-rank constraint. Proof. Suppose the singular value decomposition of D is: D = U X V T , and its rank-r SVD is A = U 1  X  1 V T 1 . We employ Augmented Lagrangian method [] to formulate a convex problem as follows: Let V 2 be a basis which is orthonormal to V 1 , so that I = V 1 V T 1 + V 2 V T 2 . We make the first derivative of L versus A zero: and also make the first derivative of L versus C zero: (8), we can get U 2 BV 2 V T 2 =  X  ( D  X  A ). Without loss of generality, we can minimized when the optimal BV 2 is a diagonal matrix  X  2 . It follows that D = [ U cost of the objective is: 2.3 Problem formulation final optimization formulation can be written as: model is able to learn a label mapping function to approximate the authentic label assigments, and select significant features from multiple views to boost the classification results. describe the experimental setting with different evaluations. We also mask a different ratio of labels to explore the effectiveness of our model with missing labels. 3.1 Datasets Pascal VOC dataset [5] comprises 9963 images which can be classified into 20 categorizations. In this paper, we chose three representative feature views: the global GIST [15], the local SIFT [14] and the tag information. The dimensions of GIST, SIFT, and tags are 512, 1000 and 804, respectively.
 NUS-WIDE dataset [2] comprises 30000 images with 31 classes, which con-tains 225-dimension block-wise color moments(CM), 64-D color histogram (CH), 144-D color correlation (CoRR), 128-D wavelet texture (WT), 73-D edge distri-bution (EDH), and 500-D SIFT-based BoW histograms.
 Mirflickr dataset [10] comprises 25000 instances with 38 classes, which con-tains 512-D global GIST, 1000-D local SIFT and 804-D tag information. We randomly sampled a subset of 12200 instances with 12 labels.
 Table 1: The comparison results(%) of six algorithms on three datasets with respect to 8 metrics. The best result is marked in bold.  X  (  X  ) implies the smaller (larger), the better.
 3.2 Evaluation Criteria and Algorithms tion: mAUC , Rank Loss (RL) , Average Precision (AP) , Hamming Loss (HL) , Macro-F1 , Micro-F1 , Accuracy and Coverage . Lower values indicate a better performance in terms of RL, HL and Coverage, while higher values sign better performance for the rest of criteria.
 of-the-art algorithms as our baselines: (1) ML-KNN [22]: the classic extension of KNN algorithm which generates a set of independent classifiers. (2) SLRM [11]: constructing a neighborhood graph, and learning a lowrank label mapping. (3) ML-LRC [21]: adopting a low-rank constraint to capture the label correla-tions globally. (4) Best Single View (B-SV): using the single view feature which achieved the best classification performance on the corresponding dataset. (5) lrMMC [13]: proposing a low-rank multi-view matrix completion method. (6) SMML [18]: integrating heterogeneous features by using the joint structured sparsity regularization. Since the former three methods are designed for single-view multi-label classification, we take the concatenation of all features as their input data. constructs a shared low-rank affinity matrix for equation (5); (2) RLM-MCML F which takes Frobenius norm as a surrogate for the nuclear norm. We run ML-KNN and lrMMC with the code provided by authors. All the parameters are set as what papers suggested or tuned by 5-fold cross-validation. Meanwhile, performance. 3.3 Classification results method shows its significant performance on all the datasets under most criteria. Taking the advantage of comprehensive information derived from multiple views, RLM-MCML outperforms B-SV. Besides, lrMMC which seeks a low-dimensional subspace shared by individual of views, is inferior to other methods. There are two possible reasons: (1) the shared subspace may drop the discriminative yet critical components contained in each view. (2) the ignorance of label corre-lations makes the prediction inaccurate. We also observe that ML-KNN gains advantages over our methods on NUS dataset, w.r.t. HL, Micro-F1, and Accu-racy. Because the top 13 selected labels lead to various instances containing the same label, and in this case instances will share similar labels with its neighbors. It indicates that SMML do improve the predictive performance on VOC dataset. This observation stresses the importance of feature selection. Meanwhile, pervi-ous works suggest Coverage requires modeling dependencies among labels. The ignorance of label correlations in SMML will degrade the classification results in terms of Coverage on three datasets.
 and visualize the low-rank structure of label correlations in Figure 2. Instance points sampled from three datasets all exhibit a strong block structure which confined with our assumption. Specially, as shown in Figure 2.(c), the label correlations in Mir dataset are more compact than the other two datasets. 3.4 Impact of missing labels An important question is that whether comparison algorithms, especially for our proposed method, can handle the incomplete labels problem effectively? To answer this question, we randomly dropped out different ratios of missing labels, from 10% to 50%, with 10% as an interval. For the space limitation, we only present the experiments on the Mir dataset under four assessment criteria (i.e., Macro-F1, Micro-F1, Hamming Loss and Accuracy). In order to make sure the figure is displayed more clearly, we employ RLM-MCML N method as our representative algorithm. All the experiments are run with 5-fold crossvalidation, and the average results are shown in Figure 3. mances across all the datasets. Its noteworthy that when the ratios of missing labels are increased, the prominent performance of our method gradually reveals. For example, when we mask 10% of labels, the performance of RLM-MCML N is inferior to SMML. Eventhough RLM-MCML N shows relative improvements of 2 . 2%, 3 . 3%, and 5 . 3% w.r.t 30%, 40% and 50% of missing labels. In terms of Macro-F1 criterion, when we drop 10% of observed labels, there are 22 . 2%, 5 . 5%, 10 . 1%, 22 . 6% and 45% improvements compared to ML-KNN, SMML, ML-LRC, SLRM, lrMMC. 3.5 Parameter sensitivity analysis measures the contribution of the global structure of label correlations. The pa-rameter  X  4 controls the local smoothness property of our model. To study how these parameters affect the classification results, we conduct parameter sensi-tivity analysis for the RLM-MCML F method. To make the classification perfor-to exhibit the effectiveness of local smoothness term. Besides, to demonstrate the effectiveness of the two components individually, we set  X  4 = 0 if we only consider the global part, and  X  3 is fixed with 0 if the local part is concerned. The performance of different parameter settings on the three dataset are shown in Figure 4, Figure 5 and Figure 6. performance of RLM-MCML F method has small variations when  X  4 is chosen from a suitable range: from 0.001 to 10. Our model is relatively insensitive to its parameter setting, which makes the parameter tuning easier. (2) The influence of local geometry structure (  X  3 = 0) guides better mAUC and Average Precision (AP) than the global part (  X  4 = 0) on VOC and NUS dataset. However, as can be seen from Figure 5, the classification performance of the global component on Mir dataset (  X  4 = 0) outperforms the individual local part, even when we consider the two components simultaneously. One possible reason is that data sampled from Mir dataset exhibits a strong label correlations than the other two datasets, which can be verified in Figure 2. Meanwhile, one important assumption aside in local smoothness part is that the local geometry structures are consistent between the input space X and the label space Y . While some work proposed that there is a semantic gap between the low-level features and semantic labels. How to alleviate the semantic gap and propose a general framework to guide the local smoothness is still an open question. label learning with missing labels. The proposed method leveraged label corre-lations to estimate authentic targets and exploit label correlations from both global and local perspective. The affinity matrix appeared in the local smooth-ness term was also built with the Frobenius norm, which could substitute the general low-rank constraint.
 multi-label problems. Another viable scheme is to extract impactful features from multiple views while training the multi-label model simultaneously.
