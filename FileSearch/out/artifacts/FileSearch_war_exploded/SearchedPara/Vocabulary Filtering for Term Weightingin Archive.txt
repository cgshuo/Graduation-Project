 As large Community-based Question Answering (cQA) archives are built up through user collaboration, the knowledge is accumula ted and made ready for sharing. Research on archived questions has emerged recently [5,14,15]. An application that facilitates knowledge sharing and diversity maintaining is Archived Question Search (AQS). It is a function that makes the huge resource reusable by returning relevant answered ques-tions given a new question as a query. If good matches are found, the lag time involved with waiting for a personal response can be avoided, thus improving user satisfaction and avoiding repeating questions.

As a specific application of IR, AQS in cQA repository is distinct from the search of web pages or news articles because it d eals with long queries and short documents that are both in the form of questions (for simplicity, we call query questions in AQS as queries, and candidate questions to be searched as documents thereafter): Long query: each AQS queries consist of natural language sentences that are supposed to be understood and answered by other community members. This kind of queries is usually longer, noisier, and more verbose than keyword queries. Thus the salient terms and trivial terms are weaved together and the information needs are usually more specific. Short document: AQS documents are essentially the same as its queries since both are cQA questions. Shorter document length means that most terms might appear only once in a document, resulting in term frequency ( tf ) approximates a weak binary factor. This characteristic of cQA data makes the e xisting term weighting schemes, such as TF-IDF model [13], Okapi BM25 [10], and Divergence From Randomness [2], less ca-pable if directly applied. The major difficulty is that documents and collections statistics are not adequate to provide enough information. The proper functioning of the existing term weighting schemes is under the assumption that documents are long and queries are short. The same difficulty may also be encountered by other forms of community content,like blogs and forums, which have the concise and noise-prone nature.
This motivates us to explore beyond the document and collection. We propose the notion of vocabulary filtering as a complementary dimension of term weighting. By def-inition, a vocabulary refers to the body of words used on a particular setting or in a par-ticular domain. Although different vocabularies may share a similar set of terms, their respective weights are specific. Given the underlying setting or dom ain, the weights can be determined, independent of any specific collection and document that instantiates the vocabulary. In a way, stopword removal can be seen as a simple binary vocabulary filter, in that it assigns 0 or 1 score to a term in additional to a weighting scheme.
We propose to measure term saliency in a vocabulary by estimating a heuristic evaluation function that accepts terms X  poi nt-wise divergence feature as input. The assumption under the point-wise divergence feature is that terms that have distinct distributions in a specific vocabulary vs. the general vocabulary are important. For instance, we expect  X  X pod X  to have a much higher frequency in a vocabulary about music &amp; music players than it does in the general vocabulary, whereas the universal stopword  X  X he X  would have similar frequencies in the two vocabularies. The vocabulary filter, in the form of a heuristic term saliency evaluation function, is integrated into the existing term weighting sch emes as an enhancement. Archived Question Search (AQS) in cQA re pository was investigated recently by [5] and [15] using translation-based language m odel. The translation probability trained on similar collections can be seen as a form of collection-level filtering of term weights. We attribute the success of their proposed model to the integration of the collection-level evidence into the document-level language model.

In related research on term weighting models, the TF-IDF model has been widely used and accepted. Recently, the justificatio n and interpretation of TF-IDF has been studied in [1,4,12], from the perspectives of information theory, probabilistic language modeling, binary independence retrieval, and Poisson distribution. [11] tried to interpret the Okapi BM25 model from the perspectiv e of poisson model, the language model, the TF-IDF model, and a Divergence From Randomness model. However, none of these ef-forts attempt to separate the evidences between document-level and the collection-level in term weighting. Since our investigation is focused on the vocabulary-level evidences, it further raises question on the roles of document level, collection level as well as vo-cabulary level analysis in term weighting and IR effectiveness.

As for the vocabulary-level filtering for term weighting, the building of a customized stopword list [7,8] and the extraction of domain-specific keywords [3] can be seen as the most relevant work. [6] evaluated the interestingness of a term using the KL divergence and the JS divergence from the distribution of the human interested corpora. This work inspires the method we use for vocabulary filtering. Our goal to build a vocabulary-level filter is to quantitatively measure term saliency for a specific vocabulary. We thus emphasize the specificity of vocabularies in construct-ing the vocabulary-level filters. Term distribution in a specific collection is biased as compared to that in a general collection. This specificity of term distribution in a col-lection reflects the specificity of its vocabular y, and enables us to highlight the specific important terms for a vocabulary. 3.1 Divergence Feature for Vocabulary Filtering To capture the vocabulary level term importance, we propose to take a novel point-wise divergence feature for each individual term, ra ther than divergence of two distributions. We see the term distribution of a vocabulary as the background knowledge to instanti-ate vocabulary filtering. More broadly, the combining of all vocabularies consists of a general background that can be used to compare against a specific vocabulary.
Jensen-Shannon (JS) divergence is a well adopted distance measure between two probability distributions. It is defined as the mean of the relative entropy of each distri-bution to the mean distribution, with the following formula: where S and G denote the specific and general vocabularies, and p s ( t i ) and p g ( t i ) denote their corresponding probability distribution.

As we evaluate the divergence at term lev el rather than at the whole sample set, we examine the point-wise function as follows:
The point-wise JS function is an appropriate choice since it is symmetric and ranges over + . Specifically, d JS ( t ) assigns a point-wise divergence score to term t highest, means the specialized terms in the vocabulary and generally recognized content repre-sentative terms are ranked high; lower, when p s ( t ) and p g ( t ) get closer to each other. These properties suggest that d JS emphasizes divergence at both the most frequent terms in the specific vocabulary and the most frequent terms in the general vocabu-lary. p s ( t ) and p g ( t ) are estimated using the Maximum Likelihood Estimator over the specific vocabulary and the general vocabulary respectively. 3.2 Estimating Term Saliency from Divergence Feature Given a point-wise divergence feature, we aim to estimate the term saliency score, which can be integrated with any existing t erm weighting scheme. More specifically, we define a mapping function f v : d JS  X  W v , which produces as output an estimation W v (denotes the term saliency score on + )given d JS as input.

We propose a heuristic evaluation function based on logistic function L ( x ) as below. where L ( x )= 1 1+ e  X  x is the logistic function that maps + to [0 . 5 , 1) monotonically.
Logistic function grows more slowly as | x | increases. This property enables us to control the rate of normalization by shifting the curve along the horizontal axis. Thus a tuning parameter  X  is introduced.Equation 4 represents the final form of the the heuristic evaluation function of vocabulary level term saliency. where the parameters  X  and  X  are tuned in Section 5.2. Since there is no direct obser-vation of term saliency available, we tune th e parameters by using the retrieval perfor-mance as an indirect guidance. After discussing the method for term weighting accounts for the vocabulary-level infor-mativeness of a term, we next investigate on how it can be naturally integrated into an IR framework.

Term weighting lies in the core of current bag-of-word IR algorithms. Terms weights discriminate the importance of terms for content representation as document descrip-tors. The general form of the bag-of-wor ds retrieval function with regards to term weighting is given as: where t i is the i th query term that appears in both the query Q and the document D ,and W (  X  ) is the term weighting model. Generally, W (  X  ) is a function that takes in evidences such as term frequency, document length, document frequency, etc .
 Figure 1 illustrates the pipeline of our proposed three-level filtering framework. Given a term t i as input, the pipeline of three filters outputs W t ( t i ) , a quantity that indicates t i  X  X  importance. Accordingly, we break down the term weighting model into three components. The scoring function in Equation (5) is thus rewritten as ter ,and Document-Level Filter respectively. The sequence of filters in the pipeline is naturally decided by the scope of the filters and the implementation process. 5.1 Data Collection and Evaluation Method We assembled a collection of 325,274 questions posted on Yahoo! Answers(YA) cate-gory Consumer Electronics from March 2008 to December 2008 using YA APIs 1 . The archived questions have an average length of approximately 60 words, consisting of  X  X ubject X  and  X  X ontent X . The statistic for replicating the term distribution in the gen-eral vocabulary was acquired from a project called Web Term Document Frequency and Rank, a joint effort of the UC Berkeley and Stanford WebBase Projects 2 .

For evaluation, 100 questions were assembled by randomly selecting questions from the whole collection. 93 questions were finally used after manually removing the noisy and redundant ones. The top 20 results of the 9 3 queries by all the experimented models were labeled by two independent assessors to be relevant or not. The evaluation system, as well as the testing set and the archive, are publicly accessible 3 .

We use Terrier [9] for indexing and retrieving, and Porter Stemmer to stem the cQA collection, the queries, and the general we b vocabulary. The evaluation metrics are Mean Average Precision(MAP) and Mean Reciprocal Rank (MRR). 5.2 Archived Question Search with Vocabulary Filters Experimental Setup. In this suit of experiment, we use vocabulary filtering to re-place the role of stopword lists in order to test the overall quality of the new retrieval function. The efficiency aspect of stopword removal is not considered here. The com-parison systems are (1) D. ( tf ); (2) C.D. ( idf -tf ); (3) V.D. ( js -tf ); and (4) V.C.D. ( js -idf -tf ), where V. denotes the proposed vocabulary-level filter f v ( d JS ) ; C. denotes the collection-level filtering ( f c ( t )= ln (1 + N df )), D. denotes the document-level filter-ing ( f d ( t )=1+ ln tf dl ).
 Parameter Tuning for Term Salience Function. A small set of 20 queries are used for tuning the parameters of the term salience estimation function as in in Equation 4. For simplicity, we fix  X  to be 1 . 0 as it does not influence the shape of the function. Therefore, only the optimal  X  is studied in this section.

Figure 2 shows the influence of  X  on MAP for V.C.D. term weighting scheme. At the point  X  =2 . 0 , the MAP starts to approach its maxi mum. This is because the logistic function begins to saturate around 2 and grows slowly thereafter as the input increases on + . This slow growth satisfies the requirement for a deep normalization on d JS ( t ) . It is worth noticing that this optimal  X  ranges over [2 . 0 , 2 . 9] . This relatively wide range suggests the stability and tolerance of the pr oposed heuristic term salience estimation function. For the rest of the experiments,  X  is set to be 2 . 0 .
 Overall Results and Discussion. Table 1 presents the overall results in term of MAP and MRR. MAP is based on the top 20 returned results and MRR evaluates the quality of the top results returned. We draw following observations from the table: 1) Vocabulary Filter in conjunction with collection and document level filters ( i.e. V.C.D), boosts tf and idf.tf significantly in both MAP and MRR. MAP comparison in Table 1 shows that V.C.D. improves over D. by 93 . 28% ,and C.D. by 13 . 83% .The consistent improvement suggests that vocabulary level evidence is complimentary to collection level and document factors for term weighting. The scale of improvement over C. is higher than that over C.D. , which indicates V.D. is a much stronger baseline than D. . In other words, collection level evid ence is also critical for measuring term importance. The only negative improvement is V.D. over C.D. , which shows that V.D. is not as effective as the classical tf.idf model. This also suggests that V. , C. ,and D. are three orthogonal factors c ritical for term weighting. 2) Vocabulary Filter improves the top results when the baselines are already very high. By examining MRR comparison in Table 1, we find that two baseline systems both have MRR of over 0 . 5 , which suggests that YA archives have considerable number of similar questions and it is relatively easy to find a similar one with a high ranking. The two systems with V. , i.e. , V.D. and V.C.D. both have MRR of over 0 . 7 ,whichshows that both systems have most of their top retrieval results correct. We also notice that V.D. has a higher MRR than C.D. , while the latter has higher MAP, which confirms our assertion of the orthogonal of V. , C. ,and D. .
 5.3 Study on Rank Terms Usi ng Vocabulary Filtering To examine the effect of the two divergence kernels, we utilize them to rank terms from a subcategory of ConsumerElectronics , i.e. ,the music &amp; music players question archive. From the top 10 terms in Table 2, we find that the vocabulary filter has suc-cessfully captured the salient terms of the recent music &amp; music players vocabulary, such as  X  X pod X ,  X  X tune X , and  X  X ync X . We may guess that if the archive was collected years earlier, the top terms might be  X  X alkman X ,  X  X ape X  and the like. It suggests that the vocabularies are evolving, or more generally, are specific. We notice that terms like  X  X y X  is ranked high, because of the user-collaborative nature of the YA archive. f v ( d js ) also ranks some general terms high, such as  X  X ail X  an d  X  X opyright X . This is because  X  X ail X  and  X  X opyright X  have high probabilities in the general web vocabulary, but low probabilities in the music &amp; music players vocabulary. They are considered to be informative though relatively less frequent in the specific vocabulary. This shows that f ( d JS ) is capable of capturing term importance in a vocabulary.

The stopwords are expected be among the lowest in a descending ranked list. Left part of Table 2 lists the lowest 10 terms by f v ( d JS ) . Generally this lowest 10 terms meet our expectation of the commonly recognized stopwords. We thus think of eliminating the lowest ranked terms from indexing, as what stopword removal does, for the purpose of improving the efficiency of the whole retrieval system. As a complementary explo-ration, we construct stopword lists by taking the lowest 5% , 10% ,and 15% f v ( d JS ) ranked terms. In stead of implementing the full-fledged V.C.D. filtering term weight-ing scheme, we use the 3 stopword lists of different size upon C.D. term weighting scheme and find that the retrieval performance at 5% removal actually improves over those without stopword removal and with standard stopword list removal. Moreover, 10% removal is slightly worse than 5% removal since less terms are used for indexing, but still acceptable considering that the efficiency is improved at a small price. In this paper, we proposed a novel notion of vocabulary-filtering to capture term im-portance by using the whole vocabulary as the background knowledge. JS divergences are utilized to characterize a specific vocabulary by contrasting its term distribution to that of of a general vocabulary. The normali zed vocabulary filters are integrated into a framework that consists of a pipeline of three filters at the document level, collection level, and vocabulary level. Our proposed model has been empirically shown to be sig-nificantly better than TF-IDF model in tackling the archived question search problem.
In future work, we plan to explore the use of vocabulary filtering and the three-level term weighting schemes in other text processing tasks like document clustering and categorization.

