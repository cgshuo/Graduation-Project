 L 1 regularization has become an important tool over the last de cade with a wide variety of ma-chine learning applications. In the context of linear regre ssion, its use helps prevent overfitting and enforces sparsity in the problem X  X  solution. Recent work ha sdemonstratedhow L 1 regularization can be applied to the value function approximation problem i nMarkovdecisionprocesses(MDPs). Kolter and Ng [1] included L 1 regularization within the least-squares temporal differe nce learning [2] algorithm as LARS-TD, while Petrik et al. [3] adapted an a pproximate linear programming algo-rithm. In both cases, L 1 regularization automates the important task of selecting r elevant features, thereby easing the design choices made by a practitioner.
 LARS-TD provides a homotopy method for finding the L 1 regularized linear fixed point formulated by Kolter and Ng. We reformulate the L 1 regularized linear fixed point as a linear complementarity problem (LCP). This formulation offers several advantages .Itallowsustodrawupontherichtheory of LCPs and optimized solvers to provide strong theoretical guarantees and fast performance. In addition, we can take advantage of the  X  X arm start X  capabili ty of LCP solvers to produce algorithms that are better suited to the sequential nature of policy imp rovement than LARS-TD, which must start from scratch for each new policy. First, we introduce MDPs and linear value function approxim ation. We then review L 1 regulariza-tion and feature selection for regression problems. Finall y, we introduce LCPs. We defer discussion of L 1 regularization and feature selection for reinforcement le arning (RL) until section 3. 2.1 MDP and Value Function Approximation Framework We aim to discover optimal, or near-optimal, policies for Ma rkov decision processes (MDPs) defined by the quintuple M =( S,A,P,R,  X  ) .Givenastate s  X  S ,theprobabilityofatransitiontoastate s  X  S when action a  X  A is taken is given by P ( s ! | s,a ) .Therewardfunctionisamappingfrom states to real numbers R : S " X  R .Apolicy  X  for M is a mapping from states to actions  X  : s " X  a and the transition matrix induced by  X  is denoted P  X  .Futurerewardsarediscountedby  X   X  [0 , 1) . The value function at state s for policy  X  is the expected total  X  -discounted reward for following  X  from s .Inmatrix-vectorform,thisiswritten: where T  X  is the Bellman operator for policy  X  and V  X  is the fixed point of this operator. An optimal policy,  X   X  ,maximizesstatevalues,hasvaluefunction V  X  ,andisthefixedpointofthe T  X  operator: Of the many algorithms that exist for finding  X   X  , policy iteration is most relevant to the presentation herein. For any policy  X  j ,policyiterationcomputes V  X  j ,thendetermines  X  j +1 as the  X  X reedy X  the algorithm will converge to an optimal policy and the uniq ue, optimal value function V  X  . The value function, transition model, and reward function a re often too large to permit an exact rep-resentation. In such cases, an approximation architecture is used for the value function. A common choice is  X  V =  X  w ,where w is a vector of k scalar weights and  X  stores a set of k features in an n  X  k matrix with one row per state. Since n is often intractably large,  X  can be thought of as populated by k linearly independent basis functions ,  X  1 ...  X  k ,implicitlydefiningthecolumnsof  X  . For the purposes of estimating w ,itiscommontoreplace  X  with  X   X  ,whichsamplesrowsof  X  , though for conciseness of presentation we will use  X  for both, since algorithms for estimating w are essentially identical if  X   X  is substituted for  X  .Typicallinearfunctionapproximationalgorithms[2] solve for the w which is a fixed point: where  X  is the L 2 projection into the span of  X  and  X  !  X  is P  X   X  in the explicit case and composed of sampled next features in the sampled case. Likewise, we ov erload T  X  for the sampled case. 2.2 L 1 Regularization and Feature Selection in Regression In regression, the L 1 regularized least squares problem is defined as: where y  X  R n is the target function and  X   X  R  X  0 is a regularization parameter. This penalized regression problem is equivalent to the Lasso [4], which minimizes the squared residual subject to a constraint on % x % 1 .Theuseofthe L 1 norm in the objective function prevents overfitting, but als o serves a secondary purpose of promoting sparse solutions (i .e., coefficients w containing many 0s). Therefore, we can think of L 1 regularization as performing feature selection .TheLasso X  X objective function is convex, ensuring the existence of a global (thou gh not necessarily unique) minimum. Even though the optimal solution to the Lasso can be computed in a fairly straightforward manner using convex programming, this approach is not very efficien tforlargeproblems. Thisisamo-tivating factor for the least angle regression (LARS) algor ithm [5], which can be thought of as a homotopy method for solving the Lasso for all nonnegative values of  X  .Wedonotrepeatthede-tails of the algorithm here, but point out that this is easier than it might sound at first because the homotopy path in  X  -space is piecewise linear (with finitely many segments). Furthermore, there exists a closed form solution for moving from one piecewise l inear segment to the next segment. An important benefit of LARS is that it provides solutions for all values of  X  in a single run of the algorithm. Cross-validation can then be performed to selec tanappropriatevalue. 2.3 LCP and BLCP Given a square matrix M and a vector q ,alinearcomplementarityproblem(LCP)seeksvectors w  X  0 and z  X  0 with w T z =0 and The problem is thus parameterized by LCP ( q, M ) .EventhoughLCPsmayappeartobesimple feasibility problems, the framework is rich enough to expre ss any convex quadratic program. The bounded linear complementarity problem (BLCP) [6] includes box con straints on z .TheBLCP computes w and z where w = q + Mz and each variable z i meets one of the following conditions: with bounds  X  X  X  X  l i &lt;u i  X  X  X  .TheparameterizationiswrittenBLCP ( q, M, l, u ) .Noticethatan LCP is a special case of a BLCP with l i =0 and u i =  X  ,  X  i .LiketheLCP,theBLCPhasaunique solution when M is a P-matrix 1 and there exist algorithms which are guaranteed to find this s olution [6, 7]. When the lower and upper bounds on the BLCP are finite, th eBLCPcaninfactbeformulated as an equivalent LCP of twice the dimensionality of the origi nal problem. A full derivation of this equivalence is shown in the appendix (supplementary materi als).
 There are many algorithms for solving (B)LCPs. Since our app roach is not tied to a particular algo-rithm, we review some general properties of (B)LCP solvers. Optimized solvers can take advantage of sparsity in z .Azeroentryin z effectively cancels out a column in M .If M is large, efficient solvers can avoid using M directly, instead using a smaller M ! that is induced by the nonzero entries of z .Thecolumnsof M ! can be thought of as the  X  X ctive X  columns and the procedure of swapping columns in and out of M ! can be thought of as a pivoting operation, analogous to pivot sinthesim-plex algorithm. Another important property of some (B)LCP a lgorithms is their ability to start from an initial guess at the solution (i.e., a  X  X arm start X ). If th einitialguessisclosetoasolution,thiscan significantly reduce the solver X  X  runtime.
 Recently, Kim and Park [8] derived a connection between the B LCP and the Karush-Kuhn-Tucker (KKT) conditions for LARS. In particular, they noted the sol ution to the minimization problem in equation (1) has the form: where the vector  X  c follows the constraints in equation (2) with l i =  X   X  and u i =  X  .Althoughwe describe the equivalence between the BLCP and LARS optimali ty conditions using M  X  (  X  T  X  )  X  1 , the inverse can take place inside the BLCP algorithm and this operation is feasible and efficient as it is only done for the active columns of  X  .KimandPark[8]usedablockpivotingalgorithm, originally introduced by J  X  udice and Pires [6], for solving the Lasso. Their experiment sshowthe block pivoting algorithm is significantly faster than both L ARS and Feature Sign Search [9]. Recent work has emphasized feature selection as an importan tprobleminreinforcementlearn-ing [10, 11]. Farahmand et al. [12] consider L 2 regularized RL. An L 1 regularized Bellman residual minimization algorithm was proposed by Loth et al. [13] 2 .JohnsandMahadevan[14]investigate the combination of least squares temporal difference learn ing (LSTD) [2] with different variants of the matching pursuit algorithm [15, 16]. Petrik et al. [3] consider L 1 regularization in the con-text of approximate linear programming. Their approach off ers some strong guarantees, but is not well-suited to noisy, sampled data. The work most directly related to our own is that of Kolter and Ng [1]. They propose augmenting the LSTD algorithm with an L 1 regularization penalty. This results in the following L 1 regularized linear fixed point ( L 1 TD) problem: Kolter and Ng derive a set of necessary and sufficient conditi ons characterizing the above fixed point 3 in terms of  X  , w ,andavector c of correlations between the features and the Bellman residu al T  X   X  V  X   X  V .Morespecifically,thecorrelation c i associated with feature  X  i is given by: Introducing the notation I to denote the set of indices of active features in the model (i.e., I = { i : w i # =0 } ), the fixed point optimality conditions can be summarized as follows: Kolter and Ng show that it is possible to find the fixed point usi ng an iterative procedure adapted from LARS. Their algorithm, LARS-TD, computes a sequence of fixed points, each of which sat-isfies the optimality conditions above for some intermediat e L 1 parameter  X   X   X   X  .Successive solutions decrease  X   X  and are computed in closed form by determining the point at wh ich a feature must be added or removed in order to further decrease  X   X  without violating one of the fixed point requirements. The algorithm (as applied to action-value fu nction approximation) is a special case of the algorithm presented in the appendix (see Fig. 2). Kolter and Ng prove that if  X  T (  X   X   X   X  "  X  ) is aP-matrix,thenforany  X   X  0 ,LARS-TDwillfindasolutiontoequation(3).
 LARS-TD inherits many of the benefits and limitations of LARS .Thefactthatittracesanentire homotopy path can be quite helpful because it does not requir ecommittingtoaparticularvalueof  X  .Ontheotherhand,theincrementalnatureofLARSmaynotbet he most efficient solution for any single value of the regularization parameter, as shown by Le eetal.[9]andKimandPark[8]. It is natural to employ LARS-TD in an iterative manner within the least squares policy iteration (LSPI) algorithm [17], as Kolter and Ng did. In this usage, ho wever, many of the benefits of LARS are lost. When a new policy is selected in the policy iteration loop, LARS-TD must discard its solution from the previous policy and start an entirely new h omotopy path, making the value of the homotopy path in this context not entirely clear. One might c ross-validate a choice of regularization parameter by measuring the performance of the final policy, b ut this requires guessing a value of  X  for all policies and then running LARS-TD up to this value for each policy. If a new value of  X  is tried, all of the work done for the previous value must be disc arded. We show that the optimality conditions for the L 1 TD fixed point correspond to the solution of a (B)LCP. This reformulation allows for (1) new algorithms to compute the fixed point using (B)LCP solvers, and (2) a new guarantee on the uniqueness of a fixed po int.
 The L 1 regularized linear fixed point is described by a vector of cor relations c as defined in equation (4). We introduce the following variables: that allow equation (4) to be simplified as c = b  X  Aw .Assuming A is a P-matrix, A is invert-ible 4 [18] and we can write: Consider a solution ( w and z )totheequationabovewhere z is bounded as in equation (2) with l =  X   X  and u =  X  to specify a BLCP. It is easy to verify that coefficients w satisfying this BLCP acheive the L 1 TD optimality conditions as detailed in section 3. Thus, any appropriate solver for the BLCP ( A  X  1 b,A  X  1 ,  X   X  ,  X  ) can be thought of as a linear complementarity approach to sol ving for the L 1 TD fixed point. We refer to this class of solvers as LC-TD algorithms and parameterize them as LC-TD (  X  ,  X  "  X  ,R,  X  ,  X  ) .
 Proposition 1 If A is a P-matrix, then for any R ,the L 1 regularized linear fixed point exists, is unique, and will be found by a basic-set BLCP algorithm solvi ng BLCP ( A  X  1 b,A  X  1 ,  X   X  ,  X  ) . This proposition follows immediately from some basic BLCP r esults. We note that if A is a P-matrix, so is A  X  1 [18], that BLCPs for P-matrices have a unique solution for an y q ([7], Chp. 3), and that the the basic-set algorithm of J  X  udice and Pires [19] is guaranteed to find a solution to any BLCP with a P-matrix. This strengthens the theorem by Kolter and Ng [1], which guaranteed only that the LARS-TD algorithm would converge to a solution when A is a P-matrix.
 This connection to the LCP literature has practical benefits as well as theoretical ones. Decoupling the problem from the solver allows a variety of algorithms to be exploited. For example, the ability of many solvers to use a warm start during initialization off ers a significant computational advantage over LARS-TD (which always begins with a null solution). In t he experimental section of this paper, we demonstrate that the ability to use warm starts during pol icy iteration can significantly improve computational efficiency. We also find that (B)LCP solvers ca nbemorerobustthanLARS-TD,an issue we address further in the appendix. As mentioned in section 3, the advantages of LARS-TD as a homo topy method are less clear when it is used in a policy iteration loop since the homotopy path i stracedonlyforspecificpolicies.Itis possible to incorporate greedy policy improvements into th eLARS-TDloop,leadingtoahomotopy path for greedy policies. The greedy L 1 regularized fixed point equation is: We propose a modification to LARS-TD called LARQ which, along with conditions C 1 -C 3 in sec-tion 3, maintains an additional invariant: It turns out that we can change policies and avoid violating t he LARS-TD invariants if we make policy changes at points where applying the Bellman operato ryieldsthesamevalueforboththe old policy (  X  )andthenewpolicy(  X  " ): T  X   X  V = T  X  !  X  V .TheLARS-TDinvariantsalldependon the correlation of features with the residual T  X   X  V  X   X  V of the current solution. When the above equation is satisfied, the residual is equal for both policie s. Thus, we can change policies at such points without violating any of the LARS-TD invariants. Due to space limitations, we defer a full presentation of the LARQ algorithm to the appendix.
 When run to completion, LARQ provides a set of action-values t hat are the greedy fixed point for all settings of  X  .Inprinciple,thisismoreflexiblethanLARS-TDwithpolicy iteration because it produces these results in a single run of the algorithm. In pr actice, LARQ suffers two limitations. The first is that it can be slow. LARS-TD enumerates every poin tatwhichtheactivesetoffeatures might change, a calculation that must be redone every time th eactivesetchanges. LARQmust do this as well, but it must also enumerate all points at which the greedy policy can change. For k features and n samples, LARS-TD must check O ( k ) points, but LARQ must check O ( k + n ) points. Even though LARS-TD will run multiple times within a policy i teration loop, the number of such iterations will typically be far fewer than the number of tra ining data points. In practice, we have observed that LARQ runs several times slower than LARS-TD wi th policy iteration.
 AsecondlimitationofLARQisthatitcanget X  X tuck. X  Thisoc curs when the greedy policy for a particular  X  is not well defined. In such cases, the algorithm attempts to s witch to a new policy immediately following a policy change. This problem is not u nique to LARQ. Looping is possible with most approximate policy iteration algorithms. What mak es it particularly troublesome for LARQ is that there are few satisfying ways of addressing this issue without sacrificing the invariants. To address these limitations, we present a compromise betwe en LARQ and LARS-TD with policy iteration. The algorithm, LC-MPI, is presented as Algorith m1. Itavoidsthecostofcontinually that the  X  values are in decreasing order with  X  (1) set to the maximum value (i.e., the point such that w (1) is the zero vector). At each  X  ( j ) ,thealgorithmusesapolicyiterationloopto(1)determine the current policy (greedy with respect to parameters  X  w ( j ) ), and (2) compute an approximate value function  X  w ( j ) using LC-TD. The policy iteration loop terminates when w ( j )  X   X  w ( j ) or some predefined number of iterations is exceeded. This use of LC-T Dwithinapolicyiterationloopwill typically be quite fast because we can use the current featur esetasawarmstart.Thewarmstartis indicated in Algorithm 1 by supp ( X  w ( j ) ) ,wherethefunction supp determines the support, or active elements, in  X  w ( j ) ;many(B)LCPsolverscanusethisinformationforinitializ ation.
 Once the policy iteration loop terminates for point  X  ( j ) ,LC-MPIsimplybeginsatthenextpoint  X  ( j +1) by initializing the weights with the previous solution,  X  w ( j +1)  X  w ( j ) .Thiswasfound initialization method performed worse experimentally tha nthesimpleapproachdescribedabove. We can view LC-MPI as approximating LARQ X  X  homotopy path sin ce the two algorithms agree for any  X  ( j ) reachable by LARQ. However, LC-MPI is more efficient and avoi ds the problem of getting stuck. By compromising between the greedy updates of LARQ an dthepurepolicyevaluation methods of LARS-TD and LC-TD, LC-MPI can be thought of as form of modified policy iteration [20]. The following table summarizes the properties of the a lgorithms described in this paper. We performed two types of experiments to highlight the poten tial benefits of (B)LCP algorithms. First, we used both LARS-TD and LC-TD within policy iteratio n. These experiments, which were run using a single value of the L 1 regularization parameter, show the benefit of warm starts fo r LC-TD. The second set of experiments demonstrates the benefi tofusingtheLC-MPIalgorithm.A single run of LC-MPI results in greedy policies for multiple values of  X  ,allowingtheuseofcross-validation to pick the best policy. We show this is significan tly more efficient than running policy iteration with either LARS-TD or LC-TD multiple times for di fferent values of  X  .Wediscussthe details of the specific LCP solver we used in the appendix.
 Both types of experiments were conducted on the 20-state cha in [17] and mountain car [21] domains, the same problems tested by Kolter and Ng [1]. The chain MDP co nsists of two stochastic actions, left and right, a reward of one at each end of the chain, and  X  =0 . 9 .Onethousandsampleswere generated using 100 episodes, each consisting of 10 random s teps. For features, we used 1000 Gaussian random noise features along with five equally space dradialbasisfunctions(RBFs)and aconstantfunction. ThegoalinthemountaincarMDPistodri ve an underpowered car up a hill Algorithm 1 LC-MPI by building up momentum. The domain is continuous, two dimen sional, and has three actions. We used  X  =0 . 99 and 155 radial basis functions (apportioned as a two dimensi onal grid of 1, 2, 3, 4, 5, 6, and 8 RBFs) and one constant function for features. Sample sweregeneratedusing75episodes where each episode started in a random start state, took rand om actions, and lasted at most 20 steps. 6.1 Policy Iteration To compare LARS-TD and LC-TD when employed within policy ite ration, we recorded the number of steps used during each round of policy iteration, where a step corresponds to a change in the active feature set. The computational complexity per step of each a lgorithm is similar; therefore, we used the average number of steps per policy as a metric for compari ng the algorithms. Policy iteration was run either until the solution converged or 15 rounds were exceeded. This process was repeated 10 times for 11 different values of  X  .Wepresenttheresultsfromtheseexperimentsinthefirsttw o columns of Table 1. The two algorithms performed similarly f or the chain MDP, but LC-TD used significantly fewer steps for the mountain car MDP. Figure 1 s hows plots for the number of steps used for each round of policy iteration for a single (typical )trial. Noticethedecliningtrendfor LC-TD; this is due to the warm starts requiring fewer steps to find a solution. The plot for the chain MDP shows that LC-TD uses many more steps in the first round of p olicy iteration than does LARS-TD. Lastly, in the trials shown in Figure 1, policy iteration using LC-TD converged in six iterations whereas it did not converge at all when using LARS-TD. This wa sduetoLARS-TDproducing solutions that violate the L 1 TD optimality conditions. We discuss this in detail in appen dix A.5. 6.2 LC-MPI When LARS-TD and LC-TD are used as subroutines within policy i teration, the process ends at a single value of the L 1 regularization parameter  X  .Thepolicyiterationloopmustbereruntoconsider different values of  X  .Inthissection,weshowhowmuchcomputationcanbesavedby running LC-MPI once (to produce m greedy policies, each at a different value of  X  )versusrunningpolicy iteration m separate times. The third column in Table 1 shows the average number of algorithm steps per policy for LC-MPI. As expected, there is a significant red uction in complexity by using LC-MPI for both domains. In the appendix, we give a more detailed exa mple of how cross-validation can be Figure 1: Number of steps used by algorithms LARS-TD and LC-T Dduringeachroundofpolicy iteration for a typical trial. For LC-TD, note the decrease i nstepsduetowarmstarts.
 used to select a good value of the regularization parameter. We also offer some additional comments on the robustness of the LARS-TD algorithm. In this paper, we proposed formulating the L 1 regularized linear fixed point problem as a linear complementarity problem. We showed the LCP formulation lea ds to a stronger theoretical guarantee in terms of the solution X  X  uniqueness than was previously sh own. Furthermore, we demonstrated that the  X  X arm start X  ability of LCP solvers can accelerate the co mputation of the L 1 TD fixed point when initialized with the support set of a related problem. This w as found to be particularly effective for policy iteration problems when the set of active features do es not change significantly from one policy to the next.
 We proposed the LARQ algorithm as an alternative to LARS-TD. The difference between these algorithms is that LARQ incorporates greedy policy improve ments inside the homotopy path. The advantage of this  X  X reedy X  homotopy path is that it provides asetofaction-valuesthatareagreedy fixed point for all settings of the L 1 regularization parameter. However, this additional flexib ility comes with increased computational complexity. As a compro mise between LARS-TD and LARQ, we proposed the LC-MPI algorithm which only maintains the LA RQ invariants at a fixed set of values. The key to making LC-MPI efficient is the use of warm st arts by using an LCP algorithm. There are several directions for future work. An interestin gquestioniswhetherthereisanatural way to incorporate policy improvement directly within the L CP formulation. Another concern for L
TD algorithms is a better characterization of the condition sunderwhichsolutionsexistandcan be found efficiently. In previous work, Kolter and Ng [1] indi cated the P-matrix property can always hold provided enough L 2 regularization is added to the problem. While this is possibl e, it also decreases the sparsity of the solution; therefore, it would be useful to find other techniques for guaranteeing convergence while maintaining sparsity.
 Acknowledgments This work was supported by the National Science Foundation ( NSF) under Grant #0937060 to the Computing Research Association for the CIFellows Project, NSF Grant IIS-0713435, and DARPA CSSG HR0011-06-1-0027. Any opinions, findings, and conclus ions or recommendations expressed in this material are those of the authors and do not necessari ly reflect the views of the National Science Foundation or the Computing Research Association. [1] J. Kolter and A. Ng. Regularization and feature selectio ninleast-squarestemporaldifference [2] S. Bradtke and A. Barto. Linear least-squares algorithm sfortemporaldifferencelearning. [3] M. Petrik, G. Taylor, R. Parr, and S. Zilberstein. Featur eselectionusingregularizationin [4] R. Tibshirani. Regression shrinkage and selection via t he Lasso. Journal of the Royal Statistical [5] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Lea st angle regression. The Annals of [6] J. J  X  udice and F. Pires. A block principal pivoting algorithm for large-scale strictly monotone [7] K. Murty. Linear Complementarity, Linear and Nonlinear Programming .HeldermannVerlag, [8] J. Kim and H. Park. Fast active-set-type algorithms for L 1 -regularized linear regression. In [9] H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse cod ing algorithms. In Advances in [10] S. Mahadevan and M. Maggioni. Proto-value functions: A Laplacian framework for learning [11] R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. Li ttman. An analysis of linear models, [12] A. Farahmand, M. Ghavamzadeh, C. Szepesv  X  ari, and S. Mannor. Regularized fitted Q-iteration [13] M. Loth, M. Davy, and P. Preux. Sparse temporal differen ce learning using LASSO. In IEEE [14] J. Johns and S. Mahadevan. Sparse approximate policy ev aluation using graph-based basis [15] S. Mallat and Z. Zhang. Matching pursuits with time-fre quency dictionaries. IEEE Transac-[16] Y. Pati, R. Rezaiifar, and P. Krishnaprasad. Orthogona lmatchingpursuit:Recursivefunction [17] M. Lagoudakis and R. Parr. Least-squares policy iterat ion. Journal of Machine Learning [18] S. Lee and H. Seol. A survey on the matrix completion prob lem. Trends in Mathematics , [19] J. J  X  udice and F. Pires. Basic-set algorithm for a generalized li near complementarity problem. [20] M. Puterman and M. Shin. Modified policy iteration algor ithms for discounted Markov deci-[21] R. Sutton and A. Barto. Reinforcement Learning: An Introduction .MITPress,1998.
