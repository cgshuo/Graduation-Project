 The PageRank algorithm is used in Web information re-trieval to calculate a single list of popularity scores for each page in the Web. These popularity scores are used to rank query results when presented to the user. By using the struc-ture of the entire Web to calculate one score per document, we are calculating a general popularity score, not particu-lar to any community. Therefore, the PageRank scores are more suited to general queries. In this paper, we introduce a more general form of PageRank, using Web multi-resolution community-based popularity scores, where each document obtains a popularity score dependent on a given Web comm-unity. When a query is related to a specific community, we choose the associated set of popularity scores and order the query results accordingly. Using Web-community based popularity scores, we achieved an 11% increase in precision over PageRank.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]:Information Search and Retrieval General Terms: Experimentation, Performance, Theory Keywords: Symmetric non-negative matrix factorisation, PageRank
The Web is one of the greatest information sources in the world. It contains billions of pages of dynamic, unstruc-tured data, which makes it a challenge to search. Fortu-nately, there is structure found in the links from one page to another. Many link analysis methods have been devel-oped over the past decade, the most popular of those being PageRank created for the Google search engine.

The PageRank algorithm [6, 4] is a method of calculating a popularity score for each document in the Web. The pop-ularity of a page is measured by observing which other pages link to it. The set of popularity scores are precomputed and stored so that when a query is given, the results can quickly be ordered by their PageRank value. PageRank uses infor-mation popularity as a measure of information trust. Un-Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. fortunately, PageRank scores are independent of the query, therefore they are popularity scores based on general popu-larity. This implies that PageRank is best suited to general knowledge queries. To answer specific knowledge queries, we need to obtain popularity scores relative to the specific knowledge or a specific community.

The HITS algorithm [5] is used to analyse only those pages that are relevant to the query and their neighbours. Us-ing this method allows us to calculate popularity based on those documents associated to the query, therefore specific knowledge popularity scores are obtained. Unfortunately, the HITS algorithm is dependent on the query and must perform all of its computations during the query time.
Topic-sensitive PageRank [3] uses a static set of topics and calculates a set of PageRank scores based on each topic. These topic-sensitive PageRank lists provide popularity of each document based on some topic. This allows better in-formation retrieval for queries concerning the selected topics. Unfortunately, the Web is a dynamic environment, therefore a set of static topics that proved sufficient today would not be sufficient tomorrow.

In this article, we introduce the concept of multi-resolution community-based popularity scores and show how they can be used to provide improved ordering for Web search results. The multi-resolution community-based popularity scores are dynamically generated using Web link information and stored to be used at query time. We have made the following im-portant contributions:
Our analysis of multi-resolution community-based popu-larity lists and their use for querying has been applied to the problem of finding information using Web information retrieval. This has allowed us to perform controlled exper-iments on a huge Web document collection containing 25 million Web documents. Our multi-resolution community-based popularity analysis can also be applied to any data mining data set using link or even correlation data to dis-cover the community popularity lists.

The article will proceed as follows: Section 2 contains an analysis multi-resolution popularity and how we can mea-sure Web page popularity. Section 3 provides a method of dynamically computing multi-resolution popularity lists and shows how the lowest resolution community popularity list is equal to PageRank. Section 4 examines how we can use the multi-resolution community-based popularity lists during querying to increase the precision of information re-trieval.
Web page popularity is an important measure of informa-tion trust. In this section we will examine why information popularity is important and we will also examine why we must be selective of the community where we obtain the popularity from. We will also show how to obtain Web page popularity from Web hyper-links.
Knowledge is the source of learning. Unfortunately, in this world we must sort though piles of information and misin-formation in order to find knowledge. When we gather any new information, our trust in the correctness of the infor-mation is greater if it is the opinion of many. Therefore, our belief in new information is dependent on the information X  X  popularity. As the complexity of the information increases, its popularity amongst the general community decreases. Therefore, when choosing information based on popularity, we must make sure that we are measuring popularity relative to the best community. To demonstrate this concept of in-formation trust dependent on community-based popularity, we have provided a three simple scenarios:
Scenario 1: If we were to approach a number of people in the city and ask them for advice on where to buy a CD, the majority would direct us a non-specialist department store like  X  X -Mart X . Since we were not after any particular CD, we would be happy with the suggestions and proceed to the shops to browse. In this case, the popular opinion on music stores has helped us, since we wanted popular knowledge. To obtain the information, we treated the entire population as a community and obtained the popular opinion from this community.

Scenario 2: If instead, we wanted to by a movie sound-track, the advice from the general community to go to K-Mart would not help us since K-Mart only sells popular music. If we were to approach only the community who buy CDs, we would find that they would direct us to a major music store like  X  X MV X . In this case, the popular opinion of the CD buying community has helped us to obtain an an-swer to our music specific query. To obtain the information, we found a community within the general population that specialised in music and obtained the popular opinion from this smaller community.

Scenario 3: If we were to approach a number of people in the city and ask them for advice about where to buy the best CD containing 70 X  X  synthesiser movie hits , the majority would again direct us to stores such as  X  X MV X  or  X  X -Mart X . Both stores might have the desired music, but it is unlikely that there would be someone to assist us in the choice of CD. On the other hand, if we were to speak to a number of people who happened to be synthesiser music experts, we would find that we would be directed to the specialist music store  X  X teve X  X  70 X  X  funk and more X . After speaking to Steve, we find that he suggests buying  X  X tar Wars and other disco classics X . In this case, the popular knowledge from the general community and the CD buying community was not satisfactory, we had to obtain the popular opinion from the synthesiser music community to satisfy our infor-mation need. To obtain this specialist information, we found Figure 1: The multi-resolutions of communities.
 The joined circles represent entities within the com-munities and their relationships. Depending on the resolution we use in this relationship graph, we could establish that there is one large community, two small communities, or four smaller communi-ties. The size of the community depends on how specific the relationship must be. a community within the music community that specialised in the synthesiser music and obtained the popular opinion from this smaller community.

This example shows how we acquire information through community-based popularity, where the community is cho-sen based on the complexity of the information required. For simple information, the entire population is treated as our community. As the desired information becomes more complex, the size of the community we wish to query re-duces to include only those who are specialists in the area. Therefore, we can say that the population has multiple res-olutions of communities. We can see this effect in figure 1, where the low resolution shows one community (the entire population), the medium resolution shows two communities and the high resolution shows four communities. The great-est resolution that can be obtained would have one person per community. The community resolution required for each query depends on the complexity of the query.
The World Wide Web (the Web) is a collection of docu-ments that has quickly grown to become one of the greatest information source in the world. The properties of the Web that has lead to its success are: The first two points relate to the freedom of information transferal through the Web, while the third point is related to the organisation of the content found within the Web.
Hyper-links are pointers found within Web pages that are inserted by the document author. They are used to link to other content on the Web (such as other Web pages) that are somehow associated. By providing such control over the structure of the Web to Web page authors, we have allowed the organisation of the Web to shape and evolve as necessary.
From the perspective of Web users, hyper-links allows us to follow an information flow through many linked pages that would have been otherwise unobtainable. The Web is so well connected through hyper-links that most of the Web can be traversed by simply following hyper-links.
The nature of hyper-links makes them useful in determin-ing Web page popularity. Hyper-links are inserted into Web pages to link to other pages; they are exit points of a page in order to enter another recommended page. Therefore a hyper-link would only be inserted if it was important to the Web page author. Due to this giving nature implied by us-ing hyper-links, we infer that a hyper-link from page A to page B is equivalent to a recommendation of page B from page A . If we take all of the Web hyper-links into account, we would be able to obtain a score that measures each page X  X  popularity. Multi-resolution communities are also found in the World Wide Web (Web). A Web community is a set of Web pages that contain similar hyper-links. The level of similarity is dependent on how tight a community we want. At the lowest level of similarity, we have the entire Web being a comm-unity. As we increase the level of similarity, many smaller communities form. At the highest level of similarity, each Web page is its own community. Therefore, by adjusting the level of similarity, we are adjusting the resolution of the community.

When searching for information on the Web, we can use the popularity scores from the multi-resolution communities by first selecting a set of candidate Web pages that match the query, then selecting a community that best suits the query and ordering the candidate pages by popularity rela-tive to the selected community.

To date, Web search engines have only used the lowest resolution of Web communities (the whole Web) to assist in their retrieval precision. An example of one such search engine is Google [6, 1] which calculates the page popularity using the PageRank algorithm. When using the PageRank algorithm, we treat every hyper-link as a vote for the page linked to. Using these votes we are able to calculate the popularity of every Web page. Google uses these popular-ity scores to order their search results. Therefore for every query, the most popular Web page matching the query ap-pears at the top of the results, where the popularity is taken from the entire Web population. If we compare this to our scenarios, this is the same as asking everyone your ques-tion (scenario 1), implying that the PageRank method will provide good results for general queries, but not for those requiring complex information.
If we formalise this idea of hyper-links being recommen-dations, we are able to equate the popularity of a page as being the sum of a fraction of the popularity of the pages that link to it, where the fraction of popularity assigned is proportional to the number of hyper-links present in the parent pages. This can be written as the equation: where p i is the popularity score associated to page i , #( l is the number of Web links found in page j ,  X  is a constant and B i is the set of pages linking to page i .

Given that the Web is a collection of Web pages that are connected through hyper-links, we can associate it to a directed graph, where each node is a Web page and each directed edge is a hyper-link. The proposed popularity score for each page is equivalent to a random walk on a directed graph. Therefore the popularity score of page A is equal to the probability of landing on that page after following an infinite amount of hyper-links.

Unfortunately there are regions of the Web that cause problems such as hyper-link loops and Web pages that con-tain no hyper-links. Web pages with such properties cause problems with the popularity calculations. If a Web user was caught in such a situation, they would move to another region of the Web. Therefore to emulate this action, we add a small probability of taking a jump to a random page on the Web: where  X  is the probability of taking a random jump and N is the number of Web pages. Equation 1 is known as the PageRank equation [6], where the popularity score p i the PageRank score for Web page i . This implies that each PageRank score is a measure of the associated Web pages global popularity. To solve equation 1, we first convert it to a vector form: where: and where  X  i,j = 1 if there is a hyper-link from page i to page j , otherwise  X  i,j = 0, and: By placing the constraint that P N i =1 p i = 1, we are able to equate  X  m T = M  X  p T , where: Therefore, equation 2 simplifies to: which is the eigenvalue equation, where L = A + M . This implies that the vector containing the Web page popularity scores is an eigenvector of the matrix L . Therefore there are as many solutions to the popularity rank equation as there are eigenvalues of L which is equal to the rank of L . This result suggests that there are many communities within the Web and the popularity of a Web page is relative to the community. For example, a Web page describing toasters would not be very popular for those in the tennis community, but would be popular to those in the electrical appliances or things to do with sliced bread community.

The PageRank scores are found in the eigenvector with the largest associated eigenvalue. Therefore, rather than Figure 2: A simple example where PageRank is not useful. calculate page popularity based on a community, PageRank is simply a global popularity score. A global popularity score is based on hyper-links from the entire Web.

If we examine a simple example, we can observe the prob-lem with using the whole Web to compute a popularity score. The Web page set in figure 2 shows four Web pages, where pages A and B link to each other and pages C and D link to each other. This set of Web pages obviously contains two tightly linked communities. Using  X  = 0 . 15 (suggested in [6]), we obtain the link matrix: and its first eigenvector: The popularity score of each page is 0 . 5 implying that each page is as popular as the others. This is the best PageRank can do, because it is limited to one popularity list. For the best results, we would want a list for each community, showing pages a and b being popular to one community and pages c and d being popular to the other community.
Note that even though all of the eigenvectors are solu-tions to the page popularity problem, only the eigenvector with the greatest associated eigenvalue is guaranteed to con-tain real positive values, being a consequence of the Perron-Frobenius theorem. Therefore using other eigenvalues intro-duces problems such as dealing with negative and complex popularity scores.
We have shown that there is a need for more than one list of popularity scores due to the different levels of complex-ity found in Web queries. In this section we will introduce symmetric non-negative matrix factorisation (SNMF) and show how we can use it to obtain multi-resolution Web page popularity lists.
Symmetric non-negative matrix factorisation (SNMF) [2] is a specialised version of non-negative matrix factorisation (NMF). NMF is the process of calculating a matrix decom-position of the form: where F and G contain non-negative elements, rank( F ) = rank( G ) = n and: for all non-negative matrices B of rank n . SNMF adds the additional constraint that F = G . Therefore, we will find the matrix decomposition:
Using PageRank, we are able to calculate a popularity list based on the whole set of Web links. The popularity list is the eigenvector of the Link matrix associated to the largest eigenvalue. Additional eigenvectors cannot be used due to their complex nature. We proposed the use of SNMF to calculate additional popularity lists based on community groups within the Web. In this section, we will show how the first eigenvector is equal to the vector generated using the one dimensional SNMF (SNMF-1).

We will begin by examining the simpler case where the link matrix L is symmetric. If L is symmetric, it can be decomposed into the two factors: To obtain the best rank one approximation of H in terms of least squares, we obtain its singular value decomposition: where  X  is a diagonal matrix containing the singular val-ues of H , and U and V are matrices containing the left and right singular values respectively. The singular vector ma-trices contain orthonormal vectors, therefore U U T = I and V V T = I . Using these values, the best rank n approxima-tion is: where  X  n a diagonal matrix containing the n greatest sin-gular values of H , and U n and V n are matrices containing the associated n left and right singular vectors. If we substitute equation 4 into equation 3, we obtain: where G is the rotated form of H using the isometric trans-formation V ( GV = H ). Therefore, to obtain the factors of L , we only need the matrices U and  X .
To obtain U and  X  directly from L , we use the relation-ship: where equation 7, is the eigenvalue decomposition of L . Therefore, if L is symmetric, the factors of L are found using its eigenvalue decomposition.

Using equation 5, we can obtain the best rank n factors of L by calculating the n greatest eigenvalues and the n associated eigenvectors.

The eigenvalue decomposition of L has provided us with the factors of L when ignoring the non-negative constraint. Using the Perron-Frobenius theorem, we can guarantee that the eigenvector associated to the greatest eigenvalue will contain non-negative values. Therefore, we can use the eigenvalue decomposition of L to calculate the rank 1 ap-proximation of H . If we choose greater ranks, we are not guaranteed non-negative values.

It is unlikely that L will be symmetric due to the Web being a directed graph, therefore we will now examine the case where L is asymmetric. To begin, we need to show that L can be decomposed in to S + N , where S is symmetric and N is orthogonal to S ( SN T = 0). To do so, we use the following theorem:
Theorem 1. Any asymmetric matrix L can be decom-posed into S + N , where S is symmetric and S and N are orthogonal, if and only if the matrix rank of S is less than the matrix rank of L , and all of the eigenvector, eigenvalue pairs of S are also eigenvector,eigenvalue pairs of L .
Proof. If L is asymmetric, we will assume that we can separate it into the two orthogonal matrices S and N where S is a symmetric matrix and N is an asymmetric ma-trix. The symmetric matrix S can be shown as its eigenvalue decomposition U  X  2 U T , where U T U = I and U is a basis of S . By substituting the eigenvalue representation of S , we receive: If we post-multiply L by U , we will show L in terms of the basis U : Since U is a basis of S , U is also orthogonal to N , implying that N U = 0. We also know that U T U = I , therefore: which implies that U contains eigenvectors of L and  X  2 con-tains eigenvalues of L . If S has the same matrix rank as L and hence U contains all of the eigenvectors of L , this im-plies that L = U  X  2 U T , making L symmetric, which is not true. Therefore S must have a matrix rank less than the matrix rank of L . Thus proving the existence and form of an orthogonal S and N .

For the one-dimensional case, the symmetric matrix S must be of the form  X  u X  2  X  u T , therefore the symmetric fac-tor H =  X  u X  . We want the symmetric factor H that best approximates L using HH T , therefore we must choose  X  u and  X  , such that H has the greatest norm. The norm of a matrix is its greatest singular value. The matrix H has one singular value  X  which is equal to the square root of the eigenvalue of S . Therefore, choosing the greatest singu-lar value of H is equivalent to choosing the greatest eigen-value to construct S . We showed that S can only be con-structed using eigenvalue, eigenvector pairs of L . Therefore the largest eigenvalue, eigenvector pair for S must also be the largest eigenvalue, eigenvector pair of L . Hence H must be equal to  X  u 1  X  1 , where  X  1 is the greatest eigenvalue of L and  X  u 1 is the associated eigenvector. This shows that the one dimensional symmetric factor of L is proportional to its largest eigenvector.

The Perron-Frobenius theorem guarantees that for any matrix containing positive elements, the eigenvector asso-ciated to the largest eigenvalue contains non-negative ele-ments. Therefore the one-dimensional SNMF of L is propor-tional to the largest eigenvector of L . In our case, where the link matrix contains normalised columns, the largest eigen-value  X  2 is equal to 1, therefore the corresponding eigenvec-tor  X  u 1 is equal to the one dimensional SNMF of L . There-fore, the one-dimensional factor of L using SNMF is equal to the PageRank of L .
We have shown that if we choose the factor F to have a rank of one, it is equal to the PageRank scores. If we choose F to have a rank of n , we will obtain n lists of popularity scores. Each of the calculated lists are measures of popu-larity that have centred themselves on various parts of the Web due to their link structure. The portions of the Web that are highly connected are considered Web communities, therefore the popularity scores computed using SNMF that are based in these portions of the Web, are representations of the popularity from that particular Web community.
The choice of the number of lists to calculate has more effect on SNMF than it would on an eigenvalue decompo-sition. When performing an eigenvalue decomposition, the first eigenvector is always the same vector, regardless of the number of eigenvectors calculated. The same goes for each other eigenvector. When performing SNMF, the value of the factors is affected by the choice of rank. For example, if a factor F of rank 2 is calculated providing us with two popularity score lists, neither of these lists will be equal to the list calculated using F of rank 1. This implies that even though the rank 1 factor is equal to the PageRank vector, no other lists from factors of higher rank will be equal to the PageRank vector.

We can see an example one-dimensional and two-dimen-sional SNMF decomposition in figure 3. This figure shows a set of communities and their relationship to document i and document j . By treating the entire set as one community, we use SNMF-1 to calculate the community centre, and hence the community popularity scores. We can see in the figure that this popularity score vector is placed in the middle of the data set, showing document j has a popularity of ap-proximately 0.7 and document i has a popularity score of approximately 0.55. If we state that there are two commu-nities, we use two dimensional SNMF (SNMF-2) to calculate the two popularity lists. This calculation shows that the first popularity list provides popularity scores of approximately 0.9 and 0.3 to documents j and i respectively and the sec-ond popularity list provides scores of approximately 0.3 and 0.7 to documents j and i respectively. From this example, we have shown the lowest resolution community popularity scores (SNMF-1) which takes all of the Web into account, and we have shown the second level resolution community popularity scores which splits the Web into the two tightest Figure 3: A set of Web communities and their rela-tionships to documents i and j . By using the PageR-ank vector, we can see that document j has a higher score than document i . When using SNMF-2 (two-dimensional SNMF), we achieve scores based on the cluster of communities we take into account. communities and provides popularity scores for each Web page based on each of the two community groups.

If we apply SNMF to the example Web page set in fig-ure 2, we obtain the Web page popularity scores shown in table 1. We can see in this table that SNMF-1 is equal to PageRank, implying that our application of SNMF to the Web link matrix is a generalisation of PageRank. We can also see that SNMF-2 provides two popularity score lists, the first showing high popularity to pages a and b and the second showing high popularity to pages c and d . By using SNMF-2, we were able to detect that there are two com-munities in the Web page set of figure 2 and hence produce popularity scores relative to each community. If we choose to calculate popularity scores relative to more that two com-munities (SNMF-3 and SNMF-4), we see in table 1, that the additional popularity lists reflect the popularity found in the first two lists. The first two lists for SNMF-3 show high popularity for pages c and d , while the third contains high popularity for pages a and b . For SNMF-4, two of the lists contain high popularity for pages a and b and the other two lists contain high popularity for pages c and d .
When using PageRank, we obtain one popularity list and use this to rank the documents returned relative to a specific query. Now that we have multiple lists, we need to choose which list best matches the query. Each list is representative of some community on the Web, therefore, selecting a list for a query is the same as choosing the associated community as an authority on the answers to the query.

In this section, we will examine a selection of schemes that we have derived in order to select a popularity list based on a query. To perform these experiments, we will use the GOV2 Web document collection used in TREC 1 . This document collection contains 25 million documents taken from the Web .gov domain. We will use the query set 701 to 800 used in TREC and the associated relevance judgements for the GOV2 document collection.

For these experiments, we computed popularity lists using one community, two communities, three communities, and four communities, giving us ten community-based popular-ity lists overall. Note that the number of lists computed is not an indication of how many lists that should be computed for use on the Web. These experiments are to examine if there is any benefit in using the multi-resolution community popularity lists over PageRank.

Typical Web search engines provide the top ten results to the user and typical users do not look past this top ten. Therefore, to measure the precision of the chosen method, we have used precision after 10 document (Prec10). This is the fraction of documents that are relative to the query in the top ten documents returned.

The Zettair text search engine 2 was used to obtain the initial document candidate list for each query. For each query q , we receive a set of candidate documents D from the text search engine. The system must then choose which multi-resolution community popularity list P i is ap-propriate for the query.

Our first experiment was to examine the precision ob-tained where only one list was used for all of the 100 queries. The results are shown in table 2 for each of the ten multi-resolution community-based popularity lists.

We can see from this table that using the first popularity list (resolution 1, community 1) for all of the 100 queries is equal to using PageRank (as we proved earlier) and that every other list provides a higher precision than PageRank (the second last list providing an almost 10% increase over PageRank). The matched queries row shows us a count of the number of queries that achieved the highest precision us-ing the associated list (e.g. 20 queries obtained the highest precision when using the list from resolution 3, community 1). Out of the 100 queries, best matched queries to each list is evenly distributed, showing that all of the lists are needed for maximum precision.
Rather than choosing one list for all queries, the list choice should be query dependent. In this section we will examine the precision that can be obtained if the list that provides the best precision is chosen for each query. Using this or-acle selection method, we will obtain an understanding of the best possible precision that can be obtained using our set of ten multi-resolution community popularity lists. The results of using this oracle method are shown in table 3
We are astonished to see that we are able to achieve a 50% increase in precision over PageRank by choosing the best lists for each query. Therefore, we hypothesise that there is a query-based selection method that can achieve a 50% increase in precision over PageRank using our set of multi-resolution community-based popularity lists. In the next http://trec.nist.gov http://www.seg.rmit.edu.au/zettair/ separations.
 how many queries the associated list provided the best precision for. Table 3: The precision after 10 documents (Prec10) obtained using the best community list for each query. This score shows the best possible score that can be obtained using our ten multi-resolution community popularity lists, giving an almost 50% improvement over PageRank. section, we will investigate several methods of popularity list selection based on the query.
Using rank based selection, we examine the rank of each of the candidate documents in each list and make our selection based on some metric. If a document is ranked highly (close to rank 1) in the community popularity list, then it is likely that the document is related to the community. On the other hand, if the document is ranked poorly, it is not likely that the document is related to the community. This implies that the rank mean is a good indication of relevance to the community popularity list.

A set of documents retrieved using a query are related to the query in some way. If that set of document are all ranked close together in some community popularity list, then it implies that the community has an opinion of them and hence the query is related to the community. If a set of documents are scattered all over the list, it implies that the community has no relationship to the query. This implies that the standard deviation of the rank is a good indication of relevance of a document set to a community popularity list
Given the candidate document with the j th highest score d j  X  D , we define r i,j = rank( d j , P i ) to be the rank of document d j in popularity list P i . Using this notation, we can define a set R i,j containing the rank of the top j scoring documents as R i,j = { r i, 1 , r i, 2 , . . . , r set R  X  1 i,j containing the reciprocal rank of the top j scor-ing documents as R  X  1 i,j = { 1 /r i, 1 , 1 /r i, 2 , . . . , 1 /r the ranks are taken from popularity list P i . Using R i,j R i,j , we will now define our experimental popularity list se-lection metrics minimum mean rank (mMr( j )), maximum mean reciprocal rank (xMRr( j )), minimum standard devia-tion rank (mSDr( j )), and minimum standard deviation re-ciprocal rank (mSDRr( j )): by combing the metrics we are able to define four more ex-perimental metrics:
Before we continue, we will observe a small example of how to perform rank-based list selection. Given a query q , the text retrieval system provides us with a set of can-didate documents, in this example five documents are re-turned d 1 , d 2 , d 3 , d 4 and d 5 . To keep the example simple, we only have two lists to choose from. The ranks of the five candidate documents in list 1 and 2 are shown in table 4.
To calculate mMr(3), we use only the top three documents in our selection calculations (since j = 3). From the first list of ranks, the mean of the top three documents is 19, for the second list, the mean of the top three documents is approxi-mately 27.33. mMr(3) returns the list number that produces the minimum value, therefore we return list 1. To calculate multi-resolution community popularity lists.

Candidate documents Rank in list 1 Rank in list 2 Table 4: A small set of candidate documents and ranks from list 1 and 2 used to illustrate the rank-based list selection calculations. mSDr(4), we use the top four documents in our selection calculations (since j = 4). From the first list of ranks, the standard deviation of the top four documents is approxi-mately 15.25, for the second list the standard deviation of the top four documents is approximately 5.23. Therefore mSDr(4) returns list 2.

The results for j = 1 to 50 for each of these metrics used on our experimental document set are shown in figure 4. When looking at these plots, the first thing we notice is that nearly all of the points are above 1, implying that using multi-resolution community popularity lists with any list selection method will produce an increase in precision over PageRank. When varying j , we notice that mSDr( j ), mM-SDr( j ) and xMR-SDr( j ) are very chaotic due to them all being based on mSDr(( j ). mMr( j ), xMRr( j ), and mSDRr( j ) show to be stable and almost independent of j , while xMR-SDRr( j ) and mM-SDRr( j ) show to be monotonically increasing with j . From this analysis, we should use mSDRr( j ) for small
Candidate documents Score in list 1 Score in list 2 Table 5: A small set of candidate documents and scores from list 1 and 2 used to illustrate the score-based list selection calculations. j (providing an 8% increase in precision over PageRank) due to its stability and mM-SDRr( j ) for large j (providing an 11% increase in precision over PageRank) due its mono-tonicity.
In the previous section, we examined list selection metrics based on the list ranks. The rank shows where a document is situated in the list, but it does not show how close it is to the rest of the documents. In this section, we will examine the effect of using the list scores rather than the list ranks.
Given the candidate document with the j th highest score d j  X  D , we define s i,j = score( d j , P i ) to be the score of document d j in popularity list P i . Using this notation, we can define a set S i,j containing the rank of the top j scoring documents and the set S  X  1 i,j containing the reciprocal rank of the top j scoring documents, where the ranks are taken from popularity list P i . Using S i,j and S  X  1 i,j , we will now de-fine our experimental popularity list selection metrics maxi-mum mean score (xMs( j )), minimum mean reciprocal score (mMRs( j )), minimum standard deviation score (mSDs( j )), minimum standard deviation reciprocal score (mSDRs( j )), and their combinations:
Before we examine the experimental results, we will show an example of how use score-based list selection to select one of our popularity lists for a query. Give query q , the text retrieval system returned documents d 1 , d 2 , d 3 , d d . From our two lists (in the previous example), the ranks are calculated by ordering the entire set of Web documents from highest scores to lowest score. Each document obtains a rank according to their position. In this example, we are using the scores, rather than the rank, to select the list. The set of candidate documents and their associated scores from each popularity list is shown in table 5.

To select a popularity list based on mMRs(3), we must calculate the mean of the reciprocal score of the top three documents. If we examine the first list, the reciprocal of 0 . 31, 0 . 18 and 0 . 09 are 3 . 23, 5 . 56 and 11 . 11 respectively. Therefore the mean of the reciprocal scores for list 1 is 6 . 63. and 14 . 29. The mean of the reciprocal scores is 26 . 98. The first list provides the lowest mean reciprocal score, therefore we choose list 1.

To calculate mMR-SDs(3), we calculate the mean recip-rocal score of the first three documents using each list (as we just calculated to be 6 . 63 and 26 . 98 for list 1 and 2 re-spectively) and multiply it by the standard deviation of the scores from each list. Using list 1, the standard deviation of the top three document popularity scores is 0 . 11. Therefore, for list 1, the mean reciprocal scores multiplied by the stan-dard deviation of the scores is 6 . 63  X  0 . 11 = 0 . 73 Using list 2, the standard deviation of the top three document popu-larity scores is 0 . 023. Therefore the mean reciprocal score multiplied by the standard deviation of the scores for list 2 is 26 . 98  X  0 . 023 = 0 . 62, which is less than the score for list 1. Therefore, using mMR-SDs(3) we choose list 2.

The results for j = 1 to 50 for each of these metrics used on our experimental document set are shown in figure 5. Again, we can see that almost all of the points are greater than 1, showing that using any of the multi-resolution comm-unity popularity lists selection methods, with any j pro-vides greater precision than when using PageRank. The plot can be classified into chaotic, stable and monotonic, where chaotic implies that increasing j will have an unknown ef-fect on precision, stable means increasing j will have little effect on precision, and monotonic mean increasing j will in-crease precision. The chaotic methods are mMRs, mSDRs, mMR-SDs, xM-SDs and mMR-SDRs and therefore should not be used. The two stable methods are xMs and xM-SDRs both providing a 6% increase over PageRank. The mono-tonic method is mSDs, providing a 10% increase in precision over PageRank for large j .
We have shown in table 3 that when using multi-resolution community-based popularity lists, we are able to achieve a huge improvement of 50% over PageRank. Using our list selection methods, we provided an 11% improvement. The open question remaining is what other possible list selection methods should we use to achieve the additional 39% gain in precision to meet the 50% improvement over PageRank.
The Web contains many communities that have many opinions, therefore a single popularity list such as PageR-ank is limited in its use. In this article, we introduced the method of mining multi-resolution community-based pop-ularity in the Web using symmetric non-negative matrix factorisation. We showed an example of a typical comm-unity separation problem that exists in the Web and can-not be found using a single popularity list (such as PageR-ank), but the communities can be identified using multiple lists.

We proved that the lowest resolution of our multi-resolution community-based popularity is equal to PageRank, there-fore our method can be thought of as a generalisation of PageRank.

Experiments were performed on the GOV2 Web document collection, containing 25 million Web documents, to exam-ine how we could use the multi-resolution community lists to return documents of more relevance to a query. We found that using a mixture of the mean reciprocal rank and the the multi-resolution community popularity lists. standard deviation of the reciprocal rank provided an 11% increase in precision when compared to using PageRank.
Our analysis of multi-resolution community-based popu-larity has been applied to the field of Web information re-trieval. The same analysis can easily be applied to any data mining data set when given data relationship information, such as links or correlations between the data. [1] S. Brin and L. Page. The anatomy of a large-scale [2] C. Ding, X. He, and H. D. Simon. On the equivalence [3] T. H. Haveliwala. Topic-sensitive pagerank. In WWW [4] G. Jeh and J. Widom. Scaling personalized web search. [5] J. M. Kleinberg. Authoritative sources in a hyperlinked [6] L. Page, S. Brin, R. Motwani, and T. Winograd. The
