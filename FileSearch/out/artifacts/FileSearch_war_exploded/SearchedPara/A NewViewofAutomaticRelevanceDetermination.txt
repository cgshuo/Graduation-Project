 Here we will be concerned with the generati ve model where 2 R n m is a dictionary of features, Section 4 we will address extensions to more general models. The basic ARD prior incorporated by SBL is p ( of lik elihood [7, 10, 15]. Mathematically , this is equi valent to minimizing where a at hyperprior on is assumed, Note that if any an EM version operates by treating the unkno wn and the M-step gradient of (2), equate to zero, and then form the x ed-point update even a saddle point of L ( ) ; both have x ed points whene ver a hyperparameter without introducing additional approximations.
 relates to MAP approaches which operate directly in often works better in selecting optimal feature sets.
 con veniently optimized by solving a series of re-weighted ` as discussed in Section 4. iterati ve minimum ` when structured dictionaries are being used. 2.1 Algorithm Deri vation and so can be expressed as a minimum over upper -bounding hyperplanes via where g ( although for our purposes we will never actually compute g ( bounding auxiliary cost function For any x ed , the optimal (tightest) bound can be obtained by minimizing over value of This formulation naturally admits the follo wing optimization scheme:
Step 1: Initialize each z
Step 2: Solv e the minimization problem
Step 3: Compute the optimal Step 4: Iterate Steps 2 and 3 until con vergence to some .
Step 5: Compute Lemma 1. The objecti ve function in (11) is con vex.
 oper ator or `Lasso' [14 ] optimization problem according to the follo wing: regularized cost function and then setting in (11) using This leads to an upper -bounding auxiliary function for L which is jointly con vex in solving over and then (14) we obtain (12). When solv ed for (11) via the stated transformation.
 ily achie vable using a variety of simple strate gies. Additionally , if algorithm then renes this estimate through the specied re-weighting procedure. 2.2 Global Con vergence Analysis Let A ( ) denote a mapping that assigns to every point in R m Theor em 1. From any initialization point f mum (or saddle point) of (2).
 innity . If fact, for any x ed r , L leads to escape.
 this matter is not discussed in [16 ]. MAP estimation directly on vie wed as arising from an explicit MAP estimate in Theor em 2. Let x 2 , [ x 2 from (3) solv e the MAP problem where h ( function of function on L ( ) : If we optimize rst over instead of conjug ate function h ( by ARD. The conca vity of h ( by p ( dependent on both the dictionary and the regularization term . . The only exception occurs when T = I ; here h ( form independently of , although dependenc y remains. 3.1 Pr operties of the implicit ARD prior solutions, meaning the value of the shape of the cost function.
 sparse representations. Since h ( factorial ` tional resources. It is expressed via k a combinatorial number of local minima and so the traditional idea is to replace kk approximation. For this purpose, the ` ` the non-f actorable, -dependent h ( promotes greater sparsity than k using k term can achie ve similar results. Consequently , the widely used family of ` k x prior [4] pro vably fail in this regard. 3.2 Benefits of dependency To explore the properties of h ( h ( x 2 ) is factorable and can be expressed in closed form via which is independent of . A plot of h ( x 2 conca ve function of each j x e.g., the ` space. Therefore, a tighter prior more closely resembling the ` running alone the coordinate axis). In the limit as ! 0 , h ( ` feasible solution with closely resembles a scaled version of the ` easy to sho w [18]: Lemma 3. When T = I , (15) has no local minima whereas (17) has 2 M local minima. Use of the ` approximation of ` log p ( x ) ) Figure 1: Left : 1D example of the implicit ARD prior . The ` prior given by log p ( to the ` 3.3 Benefits of a non-factorial prior In contrast, the benets the typically non-f actorial nature of h ( equal Q i exp [ 1 = 2 f i ( x i )] fewer local minima than when solving (15).
 P possible to have a and that any f oneself to factorial priors when maximal feature pruning is paramount. of one. Consequently , any feasible solution to v norm solution). We can now plot any prior distrib ution p ( 1D feasible region of entries. We then computed (i) the non-f actorial implicit ARD prior , and (ii) the prior p ( we observ e that, while both priors peak at the its global minimum anchored at [12 , 18] is as follo ws. First, the data p ( Y j X ) / exp Here each of the d matrices C are pruned by minimizing the analogous type-II lik elihood function which a variety of techniques exist for its minimization [2, 9]. introduced in [15 ]. via a globally con vergent re-weighted ` selection using the ` MAP estimation in terms/priors as is commonly done, then what is the optimal prior p ( to empirical priors based on the Gaussian distrib ution. Note that the `
