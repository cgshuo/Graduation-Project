 The accessibility of large quantities of off-line discrete -time dynamic data X  X tate-action sequences drawn from real-world domains X  X epresents an untapped oppor tunity for widespread adoption of reinforcement learning. By real-world we imply domains tha t are characterized by continuous state, the Markov property. If we assume that the reward function is part of the problem description, then to learn from this data we must ensure the Markov property is p reserved before we approximate the optimal policy with respect to the reward function in a model -free or model-based way. For many domains, particularly those governed by different ial equations, we may leverage the in-ductive bias of locality during function approximation to s atisfy the Markov property. When ap-plied to model-free reinforcement learning, function appr oximation typically assumes that the value function maps nearby states to similar expectations of futu re reward. As part of model-based rein-forcement learning, function approximation additionally assumes that similar actions map to nearby based approaches [1, 2] and global model-free approaches [6 , 17] have been achieved by exploiting the locality of dynamics in fully observable state-space re presentations of challenging real-world problems.
 principle models offer some guidance in defining local dynam ics, but the existence of known first principles cannot always be assumed. Rather, we desire a gen eral framework for reconstructing dynamic analysis has long used manifold embeddings to recon struct locally Euclidean state-spaces of unforced, partially observable systems [24, 18] and has i dentified ways of finding these embed-dings non-parametrically [7, 12]. Dynamicists have also us ed embeddings as generative models of partially observable unforced systems [16] by numerically integrating over the resultant embedding. Recent advances have extended the theory of manifold embedd ings to encompass deterministically and stochastically forced systems [21, 22].
 servable forced systems. We do this by first identifying an ap propriate embedding for the system of interest and then leveraging the resultant locality to pe rform reinforcement learning in a model-based way. We believe it may be more practical to address rein forcement learning under partial observability in a model-based way because it facilitates r easoning about domain knowledge and off-line validation of the embedding parameters.
 The primary contribution of this paper is to formally combin e and empirically evaluate these ex-isting, but not well-known, methods by incorporating them i n off-line, model-based reinforcement learning of two domains. First, we study the use of embedding s to learn control policies in a par-tially observable variant of the well-known Mountain Car do main. Second, we demonstrate the embedding-driven, model-based technique to learn an effec tive and efficient neurostimulation pol-among the hardest classes of learning domain X  X  continuous-v alued state-space that is nonlinear, partially observable, prohibitively expensive to explore , noisy, and governed by dynamics that are currently not well-described by mathematical models drawn from first principles. In this section we combine reinforcement learning, partial observability, and manifold embeddings into a single mathematical formalism. We then describe non-parametric means of identifying the manifold embedding of a system and how the resultant embeddi ng may be used as a local model. 2.1 Reinforcement Learning Reinforcement learning (RL) is a class of problems in which a n agent learns an optimal solution to a multi-step decision task by interacting with its environm ent [23]. Many RL algorithms exist, but we will focus on the Q -learning algorithm.
 Consider an environment (i.e. forced system) having a state vector, s  X  R M , which evolves ac-cording to a nonlinear differential equation but is discret ized in time and integrated numerically according to the map, f . Consider an agent that interacts with the environment by se lecting action, a decision task. Thus, for each time, t , RL is the process of learning the optimal policy function,  X   X  , that maximizes the expected sum of future rewards, termed the optimal action-value function o r Q -function, Q  X  , such that, where  X  is the discount factor on [0 , 1) . Equation 4 assumes that Q  X  is known. Without a priori knowledge of Q  X  an approximation, Q , must be constructed iteratively. Assume the current Q -function estimate, Q , of the optimal, Q  X  , contains error,  X  , the approximation of Q by where  X  is the learning rate. By selecting action a that maximizes the current estimate of Q , Q -learning specifies that over many applications of Equation 5 , Q approaches Q  X  . 2.2 Manifold Embeddings for Reinforcement Learning Under Pa rtial Observability Q -learning relies on complete state observability to identi fy the optimal policy. Nonlinear dynamic systems theory provides a means of reconstructing complete state observability from incomplete state via the method of delayed embeddings, formalized by Ta kens X  Theorem [24]. Here we present the key points of Takens X  Theorem utilizing the notation of H uke [8] in a deterministically forced system.
 Assume s is an M -dimensional, real-valued, bounded vector space and a is a real-valued action input to the environment. Assuming that the state update f and the policy  X  are deterministic functions, Equation 1 may be substituted into Equation 2 to compose a new function,  X  , which specifies the discrete time evolution of the agent acti ng on the environment. If  X  is a smooth map  X  : R M  X  R M and this system is observed via function, y , such that tiable we may apply Takens X  Theorem [24] to reconstruct the c omplete state-space of the observed system. Thus, for each  X  s ( t ) , we can construct a vector s such that s connectivity of the original vector-space, in the context o f RL the mapping  X  , may be substituted for f (Eqn. 6) and vectors s s ( t ) in Equations 1 X 5 without loss of generality. 2.3 Non-parametric Identification of Manifold Embeddings Takens X  Theorem does not define how to compute the embedding d imension of arbitrary sequences the intrinsic dimension, M , of a system is unknown. Finding high-quality embedding par ameters the fields of subspace identification and nonlinear dynamic a nalysis. Numerous methods of note exist, drawn from both disciplines. We employ a spectral app roach [7]. This method, premised by the singular value decomposition (SVD), is non-parametric , computationally efficient, and robust to additive noise X  X ll of which are useful in practical applic ation. As will be seen in succeeding sections, this method finds embeddings which are both accura te in theoretical tests and useful in practice.
 We summarize the spectral parameter selection algorithm as follows. Given a sequence of state ob-large refers to a cardinality of dimension which is certain t o be greater than twice the dimension in which the actual state-space resides. For each embedding window size,  X  T 1) define a matrix S rule, where  X  =  X  T singular values,  X  (  X  T values,  X  is the approximate embedding window, T the number of non-trivial singular values of  X  ( T than the long-term trend of  X  parameters E and T 2.4 Generative Local Models from Embeddings The preservation of locality and dynamics afforded by the em bedding allows an approximation of the underlying dynamic system. To model this space we assume that the derivative of the Voronoi region surrounding each embedded point is well-approximat ed by the derivative at the point itself, gration of the local state and gradient. We define the model an d integration process formally. method to D yields a sequence of vectors s on these tuples, A ( m ( t ))  X  a ( t ) , S ( m ( t ))  X  s and U ( M , a )  X  M state we define the gradient according to our definition of loc ality, namely the nearest neighbor. This step is defined differently for models having discrete a nd continuous actions. The model X  X  A , according to Equation 11 and in the continuous case it is defi ned by Equation 12, where  X  is a scaling parameter on the action space. The model gradien t and numerical integration are defined, respectively, as, the derivative estimate in R E , via the embedding rule (Eqn. 10). In practice, a small amoun t of additive noise facilitates generalization. 2.5 Summary of Approach Our approach is to combine the practices of dynamic analysis and RL to construct useful policies in partially observable, real-world domains via off-line lea rning. Our meta-level approach is divided into two phases: the modeling phase and the learning phase.
 We perform the modeling phase in steps: 1) record a partially observable system (and its rewards) embedding method; and 3) construct the embedding vectors an d define the local model of the system. During the learning phase, we identify the optimal policy on the local model with respect to the approximation of the model and Q -function, thus, we define the Q -function as a set of values, Q , i Equation 11 or 12, depending on whether the action is discret e or continuous. Note, our technique density of data exists to reconstruct the embedded state-sp ace with minimal bias. continuous-valued state and action spaces. This domain is p erhaps the most studied continuous-where the velocity component of state is unobserved. While no t a real-world domain as imagined in the introduction, Mountain Car provides a familiar benchma rk to evaluate our approach. Figure 1: Learning experiments on Mountain Car under partia l observability. (a) Embedding spec-trum and accompanying trajectory ( E = 3 , T performance as a function of embedding parameters and quant ity of training data. (c) Embedding spectrum and accompanying trajectory ( E = 3 , T We use the Mountain Car dynamics and boundaries of Sutton and Barto [23]. We fix the initial state for all experiments (and resets) to be the lowest point of the mountain domain with zero velocity, state is observable. During the modeling phase, we record th is domain under a random control policy for 10,000 time-steps (  X  t = 0 . 05 seconds), where the action is changed every  X  t = 0 . 20 seconds. We then compute the spectral embedding of the obser vations ( T that the embedding of Mountain Car under the random policy re quires dimension E = 3 with a maximum embedding window of T To evaluate learning phase outcomes with respect to modelin g phase outcomes, we perform an ex-periment where we model the randomly collected observation s using embedding parameters drawn from the product of the sets T we fix the size of the local model to 10,000 elements we vary the total amount of training samples observed from 10,000 to 200,000 at intervals of 10,000. We us e batch Q -learning to identify the optimal policy in a model-based way X  X n Equation 5 the transit ion between state-action pair and execute the learned policy on the real system for 10,000 time -steps, recording the mean path-to-goal length over all goals reached. Each configuration is execute d 30 times.
 We summarize the results of these experiments by log-scale p lots, Figures 1(b) and (c), for embed-dings of dimension two and three, respectively. We compare l earning performance against three measures: the maximum performing policy achievable given t he dynamics of the system (path-to-embedding dimension, and the random policy. Learned perfor mance is plotted as linear regression fits of the data.
 vations. Performance positively relates to the quantity of off-line training data for all embedding parameters. Except for the configuration ( E = 2 , T performance relative to E is small. Learning performance of 3-dimensional embedding s dominate all but the shortest 2-dimensional embeddings. These obser vations indicate that the parameters of the embedding ultimately determine the effectiveness of RL under partial observability. This is not surprising. What is surprising is that the best performing pa rameter configurations are linked to dynamic characteristics of the system under both a random po licy and the learned policy. To support this claim we collected 1,000 sample observation s of the best policy ( E = 3 , T 0 . 70 sec., N train = 200 , 000 ) during control of the real Mountain Car domain (path-to-go al = 79 steps). We computed and plotted the embedding spectrum and fi rst two dimensions of the embedding We observe that the spectrum of the learned system has shifte d such that the optimal embedding parameters require a shorter embedding window, T dimension E = 2 (i.e.,  X  window length). We confirm this by observing the embedding di rectly, Figure 1(d). Unlike the requires a 3-dimensional embedding to preserve locality, t he learned policy exhibits a 2-dimensional unstable spiral fixed point. Thus, the fixed-point structure (embedding structure) of the combined policy-environment system changes during learning.
 To reinforce this claim, we consider the difference between a 2-dimensional and 3-dimensional em-bedding. An agent may learn to project into a 2-dimensional p lane of the 3-dimensional space, thus decreasing its embedding dimension if the training data supports a 2-dimensional policy. We belie ve it is no accident that ( E = 3 , T of training data. This configuration can represent both 3-di mensional and 2-dimensional policies, depending on the amount of training data available. It can al so select between 2-dimensional em-beddings having window sizes of T third dimension is projected out. One resulting parameter c onfiguration ( E = 2 , T near the optimal 2-dimensional configuration of Figure 1(b) . Epilepsy is a common neurological disorder which manifests itself, electrophysiologically, in the form of intermittent seizures X  X ntense, synchronized firing of neural populations. Researchers now recognize seizures as artifacts of abnormal neural dynamic s and rely heavily on the nonlinear dy-namic systems analysis and control literature to understan d and treat seizures [4]. Promising tech-niques have emerged from this union. For example, fixed frequ ency electrical stimulation of slices of the rat hippocampus under artificially induced epilepsy h ave been demonstrated to suppress the frequency, duration, or amplitude of seizures [9, 5]. Next g eneration epilepsy treatments, derived from machine learning, promise maximal seizure suppressio n via minimal electrical stimulation by roscientists have only vague notions of what effective neur ostimulation treatments should look like. impractical without computational models.
 quency (number of stimulations divided by the time the polic y is active) of less than 1.0 Hz (1.0 Hz As a further complication, on-line exploration is extremel y expensive because the brain slices are experimentally viable for periods of less than 2 hours.
 Again, we approach this problem as separate modeling and lea rning phases. We first compute the embedding spectrum of our dataset assuming  X  E = 15 , presented in Figure 2(b). Using our knowl-edge of the interaction between embedding parameters and le arning we select the embedding di-mension E = 3 and embedding window T T of spontaneous seizure formation, however, varies substan tially between slices. We select a shorter embedding window and rely on integration of the local model t o unmask long-term dynamics. (a) Sample observations from the fixed-frequency stimulati on dataset. Seizures are labeled with horizontal lines. (b) The embedding spectrum of the fixed-fr equency stimulation dataset. The large maximum of  X  * Detail of the embedding spectrum for T scale of individual stimulation events. (c) The resultant n eurostimulation model constructed from embedding the dataset with parameters ( E = 3 , T desampled 5  X  in the plot.
 In this complex domain we apply the spectral method differen tly than described in Section 2. Rather than building the model directly from the embedding ( E = 3 , T of basis on the embedding (  X  E = 15 , T gular vectors, analogous to projecting onto the principal c omponents. This embedding is plotted in from discrete frequencies to a continuous scale of time-ela psed-since-stimulation. This allows us to combine all of the data into a single state-action space and t hen simulate any arbitrary frequency. Based on exhaustive closed-loop simulations of fixed-frequ ency suppression efficacy across a spec-Hz in the hopes of easing the learning problem. We then perfor m batch Q -learning over the model ( function to penalize each electrical stimulation by  X  1 and each visited seizure state by  X  20 . frequency policy, stimulation events comprise 5.0% and sei zures comprise 6.8% of the simulation states. The policy learned by the agent also reduces the perc ent of seizure states to 5.2% of sim-simulation, therefore, the learned policy achieves the goa l.
 We then deployed the learned policy on real brain slices to te st on-line seizure suppression perfor-mance. The policy was tested over four trials on two unique br ain slices extracted from the same achieved equilibrium. (Note: seizures occurring at the ons et of stimulation are common artifacts of neurostimulation). Figure 3 displays two of these trials spaced over four sequential phases: (a) a control (no stimulation) phase used to determine baseline seizure activity, (b) a learned policy recompute baseline seizure activity, and (d) a learned poli cy trial lasting 2,130 seconds. Figure 3: Field potential trace of a real seizure suppressio n experiment using a policy learned from simulation. Seizures are labeled as horizontal lines above the traces. Stimulation events are marked by vertical bars below the traces. (a) A control phase used to determine baseline seizure activity. * 10 minutes of trace are omitted while the algorithm was reset . The RL community has long studied low-dimensional represen tations to capture complex domains. Approaches for efficient function approximation, basis fun ction construction, and discovery of em-been limited to the fully observable (MDP) case and has not be en extended to partially observable environments. The question of state space representation i n partially observable domains was tack-led under the POMDP framework [14] and recently in the PSR fra mework [19]. These methods address a similar problem but have been limited primarily to discrete action and observation spaces. The PSR framework was extended to continuous (nonlinear) do mains [25]. This method is signifi-criteria used to select the appropriate representation. Fu rthermore, it has not yet been applied to real-world domains. An empirical comparison with our appro ach is left for future consideration. The contribution of our work is to integrate embeddings with model-based RL to solve real-world problems. We do this by leveraging locality preserving qual ities of embeddings to construct dynamic models of the system to be controlled. While not improving the quality of off-line learning that controlled system. To demonstrate our approach, we applied it to learn a neurostimulation treatment of epilepsy, a challenging real-world domain. We showed tha t the policy learned off-line from an embedding-based, local model can be successfully transfer red on-line. This is a promising step toward widespread application of RL in real-world domains. Looking to the future, we anticipate the ability to adjust the embedding a priori using a non-parametric policy gradient approach over consideration.
 Acknowledgments Institute for generating the time-series described in Sect ion 4. The authors also thank Arthur Guez, Robert Vincent, Jordan Frank, and Mahdi Milani Fard for valu able comments and suggestions. The authors gratefully acknowledge financial support by the Nat ural Sciences and Engineering Research Council of Canada and the Canadian Institutes of Health Rese arch.
