 Uppsala University University of Padua lexicalization. 1. Introduction
Many contemporary linguistic theories give lexical accounts of syntactic phenomena, where complex syntactic structures are analyzed as the combinations of elementary structures taken from a finite lexicon. In the computational linguistics community, this trend has been called lexicalization , and has been extensively investigated since the 1990s. From a mathematical perspective, the main question that arises in the context of has any impact on the generative or computational properties of the formalism. that a CFG is in Greibach normal form if the right-hand side of every rule in the gram-mar starts with a terminal symbol, representing an overt lexical item. Although several procedures for casting a CFG in Greibach normal form exist, all of them substantially alter the structure of the parse trees of the source grammar. In technical terms, these procedures provide a weak lexicalization of the source grammar (because the string language is preserved) but not a strong lexicalization (because the sets of parse trees that the two grammars assign to the common string language are not the same). Strong lexicalization is highly relevant for natural language processing, however, where the parse tree assigned by a grammar represents the syntactic analysis of interest, and is used by other modules such as semantic interpretation or translation. In this article, we investigate the problem of strong lexicalization. (1990). The first result is that CFGs are not closed under strong lexicalization. (The author actually shows a stronger result involving a formalism called tree substitution grammar, as will be discussed in detail in Section 3.) Informally, this means that we cannot cast a CFG G in a special form in which each rule has an overt lexical item in its right-hand side, under the restriction that the new grammar generates exactly the same in Greibach normal form, under the additional condition that the generated parse trees are preserved.
 class of tree-adjoining grammars (TAGs) (Joshi, Levy, and Takahashi 1975; Joshi and
Schabes 1997). A TAG consists of a finite set of elementary trees, which are phrase structure trees of unbounded depth, and allows for the combination of these trees by means of two operations called substitution and adjunction (described in more detail in the next section). A lexicalized TAG is one where each elementary tree contains at least one overt lexical item called the anchor of the tree; the elementary tree is intended to encapsulate the syntactic and semantic dependencies of its anchor. Because CFG rules can be viewed as elementary trees of depth one, and because context-free rewriting can be simulated by the substitution operation defined for TAGs, we can view any CFG as a special TAG. Under this view, one can ask whether lexicalized TAGs can provide a means that, given a CFG G , one can always construct a lexicalized TAG generating the same set of parse trees as G , and consequently the same string language. stating that TAGs are closed under strong lexicalization. Schabes (1990) states that this is the case, and provides an informal argument to justify the claim. The same claim still appears in two subsequent publications (Joshi and Schabes 1992, 1997), but no precise proof of it has appeared until now. We speculate that the claim could be due to the fact that adjunction is more powerful than substitution with respect to weak generative capacity. It turns out, however, that when it comes to strong generative In other words, there are TAGs that lack a strongly equivalent lexicalized version. variant of TAG called tree insertion grammars (TIGs). This formalism severely restricts the adjunction operation originally defined for TAGs, in such a way that the class of generated string languages, as well as the class of generated parse trees, are the same as those of CFGs. Schabes and Waters then conjecture that TIGs are closed under strong lexicalization. In this article we also disprove their conjecture. 2. Preliminaries We assume familiarity with the TAG formalism; for a survey, we refer the reader to
Joshi and Schabes (1997). We briefly introduce here the basic terminology and notation for TAG that we use in this article. 2.1 Basic Definitions
A TAG is a rewriting system that derives trees starting from a finite set of elementary 618 with nonterminal symbols and frontier nodes labeled with terminal and nonterminal trees serve as the starting point for derivations, and may combine with other trees by means of an operation called substitution . Tree substitution replaces a node labeled with a nonterminal A in the frontier of some target tree with an initial tree whose root is labeled with A . The nodes that are the target of the substitution operation are of Figure 1.
 same nonterminal label as the root node. This special node is called the foot node and is identified by an asterisk (  X  ). Auxiliary trees may combine with other trees by means of an operation called adjunction . The adjunction operation entails splitting some target tree at an internal node with label A , and inserting an auxiliary tree whose root (and Figure 1.
 tree whose nodes are labeled with (instances of) elementary trees, and whose edges are labeled with (addresses of) nodes at which substitution or adjunction takes place.
More specifically, an edge v  X  u v in d represents the information that the elementary tree at v is substituted at or adjoined into node u of the elementary tree at v . When we combine the elementary trees of our TAG as specified by d , we obtain a (unique) phrase structure tree called the derived tree associated with d , which we denote as t ( d ).
 ranging over initial trees, and  X  as a variable ranging over auxiliary trees. We also use the symbols u and v as variables ranging over nodes of generic trees (elementary, of some type  X  , and the root node of  X  is labeled with the start symbol of the grammar, denoted as S .
 which for purposes here is a label in the set { NA,OA } . The label NA denotes Null
Adjunction, forbidding adjunction at u ; the label OA denotes Obligatory Adjunction, forcing adjunction at u . A derivation tree d is called saturated if, at each node v of d thereisanarc v  X  u v ,forsome v , for every node u of the elementary tree at v that requires substitution or is annotated with an OA constraint.
 some sentential and saturated derivation tree d obtained in G . Each such derived tree is all terminal symbols labeling the frontier of t , from left to right. The string language generated by G is the set
ATAG G is said to be finitely ambiguous if, for every string w those trees in T ( G ) that have w as their yield is finite.

A grammar G is called reduced if none of its elementary trees is useless. Throughout this article we shall assume that the grammars that we deal with are reduced. 2.2 Lexicalization
In a tree, a node labeled with a terminal symbol is called a lexical node .ATAGis called lexicalized if each of its elementary trees has at least one lexical node. Observe that a lexicalized grammar cannot generate the empty string, denoted by  X  , because every derived tree yields at least one lexical element. Similarly, a lexicalized grammar is always finitely ambiguous, because the length of the generated strings provides an upper bound on the size of the associated derived trees. Let of the class of all TAGs. We say that G strongly lexicalizes
G  X  G that is finitely ambiguous and that satisfies  X   X  L ( G ), there exists a lexicalized grammar G  X  G such that T ( G ) = T ( G ). We also say that lexicalization if the class G strongly lexicalizes itself.

Schabes (1990) about strong lexicalization for subclasses of TAGs, already mentioned in lexicalization. Here we view a CFG as a special case of a TAG using only substitution and elementary trees of depth one. Informally, this means that we cannot cast a CFG
G in a special form in which each rule has an overt lexical item in its right-hand side, under the restriction that the new grammar generates exactly the same tree set as G .The as the only tree combination operation, and thus includes all context-free grammars.
This means that, given a TSG or a CFG G , we can always construct a TAG that is lexicalized and that generates exactly the same tree set as G . 3. Tree Substitution Grammars Are Not Closed Under Strong Lexicalization related result for TSGs.
 Theorem 1 Tree substitution grammars are not closed under strong lexicalization. specific TSG G 1 , reported in Figure 2. It is not difficult to see that G 620 and derives a contradiction. We provide here an alternative, direct proof of Theorem 1.
This alternative proof will be generalized in Section 4 to obtain the main result of this article.
 root node of t leading to u . 3.1 Intuition
In order to convey the basic idea behind Schabes X  X  proof and our alternative version herein, we first consider a specific candidate grammar for the lexicalization of G example, one might think that the following TSG G 1 lexicalizes G
This grammar is obtained from G 1 by taking the lexicalized tree  X  elementary tree that can be obtained by substituting  X  1 into the non-lexicalized tree  X 
The grammar G 1 only generates a subset of the trees generated by G following tree, for example, cannot be generated by G 1 :
To see this, we reason as follows. Consider a lexical node v in an elementary tree  X  of G 1 ,andlet t be a tree obtained by substituting some elementary tree into  X  . Because
More generally, the depth of a lexical node in an elementary tree  X  isthesameinalltrees derived starting from  X  . Because the maximal depth of a lexical node in an elementary tree of G 1 is 2, we deduce that every tree generated by G depth at most 2. In contrast, all lexical nodes in the tree t tree t 1 is not generated by G 1 . 3. 2Main Part
We now generalize this argument to arbitrary candidate grammars. For this, we are derived by G 1 : For a grammar G  X  G 1 , we define the d-index of G as the maximum in minimal depths of a -labeled nodes in trees derived by G :
Note that, for two grammars G , G  X  G 1 , T ( G ) = T ( G ) implies that G and G have the same d-index. This means that two grammars in G 1 with different d-indices cannot gen-erate the same tree language. Then Theorem 1 directly follows from the two statements in the next lemma.
 Lemma 1 The grammar G 1 has infinite d-index. Every lexicalized grammar in d-index.
 Proof
The first statement is easy to verify: Using longer and longer derivations, the mini-mal depth of an a -labeled node in the corresponding tree can be pushed beyond any bound.
 t  X  be any such node in t ,andlet u a bethenodeof  X  that corresponds to v that the only tree combination operation allowed in a TSG derivation is substitution.
Because substitution can only take place at the frontier of a derived tree, we must therefore depth ( u a ,  X  ) must be upper bounded by some constant depending only on G ,
T ( G ), we must conclude that d-index ( G )isfinite. 3.3 Lexicalization of Tree Substitution Grammars
What we have just seen is that lexicalized TSGs are unable to derive the tree structures generated by the grammar G 1 in Figure 2. This is essentially because tree substitution tion allows the insertion of additional structure at internal nodes of elementary trees, 622 and enables TAGs to provide a strong lexicalization of TSGs. For example, the following TAG G 1 lexicalizes G 1 .

Note that this grammar looks almost like G 1 , except that adjunction now is allowed at internal nodes, and substitution nodes have become foot nodes. The following deriva-tion tree witnesses that the tree t 1 can be derived in G node of an elementary tree, and 1 to denote its leftmost child.
Schabes (1990) provides a general procedure for constructing a lexicalized TAG for a given context-free grammar. 4. Tree-Adjoining Grammars Are Not Closed Under Strong Lexicalization In this section we develop the proof of the main result of this article. Theorem 2
Tree-adjoining grammars are not closed under strong lexicalization. 4.1 Proof Idea
The basic idea underlying the proof of Theorem 2 is essentially the same as the one used in the proof of Theorem 1 in Section 3. Some discussion of this issue is in order at this allows the insertion of additional structure at internal nodes of elementary trees, and enables TAGs to provide a strong lexicalization of TSGs. One might now be tempted to believe that, because the depth-based argument that we used in the proof of Lemma 1 can no longer be applied to TAGs, they might be closed under strong lexicalization. however. Let us first look at substitution as an operation on the yield of the derived terminal symbol in the yield of a derived tree with a new string consisting of terminals and nonterminals, representing the yield of the tree that is substituted. Under the same perspective, adjunction is more powerful than tree substitution, as is well known. But just as substitution can be seen as context-free rewriting on tree yields, adjunction can be seen as context-free rewriting on the paths of trees: It replaces a nonterminal symbol in some path of a derived tree with a string representing the spine ofthetreethatis adjoined X  X he unique path from the root node of the tree to the foot node. to TAGs. We will specify a TAG G 2 such that the paths of the derived trees of G in a string form the derived trees of the counterexample grammar G is exemplified in Figure 3. Each internal node of a derived tree of G the spine of the corresponding derived tree of G 2 as a pair of matching brackets. By our encoding, any TAG generating trees from T ( G 2 ) will have to exploit adjunction at same restrictions as the grammar G 1 which used substitution at nodes in the yield. This will allow us to lift our argument from Lemma 1. The only difference is that instead of working with the actual depth of a lexical node in a tree t to work with the depth of the node in the encoded tree. As will be explained later, this measure can be recovered as the excess of left parentheses over right parentheses in the spine above the lexical node. 4. 2Preliminaries As already mentioned, our proof of Theorem 2 follows the same structure as our proof of
Theorem 1. As our counterexample grammar, we use the grammar G this grammar generates the encodings of the derived trees of G previously. Note that the left parenthesis symbol  X  (  X  and the right parenthesis symbol  X  )  X  are nonterminal symbols. As with the grammar G 1 before, it is not difficult to see that G 2 is finitely ambiguous and that  X /  X  L ( G 2 ).
 624 at most two children, and the left child of every node with two children is always a leaf node. The path from the root node of a right spinal tree t to the rightmost leaf of t is called spine . To save some space, in the following we write right spinal trees horizontally and from left to right, as already done in Figure 3. Thus the grammar G can alternatively be written as follows:
We exploit this function to compute the excess of left parentheses over right parentheses in a sequence of nodes, and write:
Let t be some right spinal tree in T ( G 2 ), and let v be some node in t . Assume that u 1 , ... , u n = v is the top X  X own sequence of all the nodes in the path from t  X  X  root u to v . We write excess ( v , t ) as a shorthand notation for excess ( u short hand notation for excess ( u 1 , ... , u n ).
 sequence of nodes in the spine of t is always zero. Thus, we omit the proof of the following statement.
 Lemma 2 Every derived tree t  X  T ( G 2 ) is a right spinal tree, and excess ( t ) = 0. proofs, it is useful at this point to come back to our discussion of the relation between that construction and the construction presented in Section 3. We observe that for each spine encodes t 1 , following the scheme exemplified in Figure 3. Using such encoding, we can establish a bijection between the a -labeled nodes in the frontier of t a -labeled nodes in the frontier of t 2 . Furthermore, if v nodes related by such a correspondence, then it is not difficult to see that depth ( v excess ( v 2 , t 2 ). 4.3 Intuition
Before we give the actual proof of Theorem 2, let us attempt to get some intuition about why our counterexample grammar G 2 cannot be strongly lexicalized by some other TAG. One might think that the following TAG G 2 is a lexicalized version of G 2 :
This grammar is obtained from G 2 by taking the lexicalized tree  X  as  X  5 ), as well as all trees that can be obtained by adjoining  X  elementary tree. G 2 does not generate all trees generated by G tree t 2 for example is not generated by G 2 :
Note that this tree is the encoded version of the counterexample tree t section (cf. Figure 3).
 in an elementary tree  X  of G 2 ,andlet t be a tree obtained by adjoining some elementary the excess of a lexical node in an elementary  X  is constant in all trees derived starting from  X  . From this we conclude that every tree generated by G with excess at most 2; this is the maximal excess of a lexical node in an elementary tree of G 2 . In contrast, all lexical nodes in the tree t 2 have excess 3. This shows that t generated by G 2 . 4.4 Main Part
In what follows, we consider the class G 2 of (reduced) TAGs that generate subsets of the trees derived by G 2 : For a grammar G  X  G 2 , we define the e-index of G as the maximum in minimal excess of a -labeled nodes in trees derived by G : in Section 3. 626 of Lemma 1) that the minimal depth of lexical nodes in a derived tree t is bounded by the minimal depth of lexical nodes in the elementary tree  X  from which t was derived. by a grammar-specific constant. This observation is expressed in the following lemma.
It is the correspondent of Lemma 4 in Knuth X  X  paper on parenthesis languages (Knuth 1967), and is proved in essentially the same way. Recall that a derivation tree d is of type  X  ,  X  some elementary tree, if d is derived starting from  X  .
 Lemma 3
Let G  X  G 2 . For each elementary tree  X  of G , there exists a number e (  X  )suchthat,for every saturated derivation tree d of type  X  , excess ( t ( d )) = e (  X  ). Proof node of d labeled with  X  .Let d 1 be the subtree of d rooted at v . Observe that t ( d be a spinal tree. We then let e (  X  ) = excess ( t ( d 1 )).
 let d 2 = d 1 be some derivation tree of type  X  occurring within some other sentential and saturated derivation tree of G . We can replace d 1 with d sentential and saturated derivation tree d = d . Every derived tree in T ( G )mustbea right spinal tree: This follows from the assumption that G can then write
Because excess ( t ( d )) = 0and excess ( t ( d )) = 0 (by Lemma 2), we conclude that Lemma 4 The grammar G 2 has infinite e-index. Every lexicalized grammar in Proof let s be the maximal number of nodes in an elementary tree in  X  . We show that
Note that k is a constant that only depends on G .
Here  X  is some initial tree, m  X  0, each u i is a node of  X  at which a tree combination operation takes place, each  X  i is an elementary tree, and each d obtained by substituting or adjoining the derived trees t ( d of  X  .
 tributed by  X  .Let v a be any such node, and let u a bethenodeof  X  that corresponds root to the node v a , can be computed as follows. Let u 1 sequence of nodes in the path from the root node of  X  to u we define
Because G  X  G 2 and because t ( d ) is a right spinal tree (Lemma 2), we can write
By Lemma 3, we have excess ( t ( d j )) = e (  X  j ), for each j with 1
Thus, every derived tree t in T ( G ) contains at least one node v excess ( v a , t )  X  k . Therefore, e-index ( G )  X  k .

Two grammars in G 2 that have a different e-index cannot generate the same tree lan-guage, thus we have concluded the proof of Theorem 2. 5. Tree Insertion Grammars Are Not Closed Under Strong Lexicalization
As mentioned earlier Schabes and Waters (1995) introduce a restricted variant of TAG which are trees derived starting from auxiliary trees with overt lexical material on both of all TIGs is closed under strong lexicalization. 628 does not derive wrapping trees; this means that G 2 actually is a TIG. Using the proof of Section 4, we then obtain the following result.
 Theorem 3 Tree insertion grammars are not closed under strong lexicalization.
 the class of TIGs. 6. Conclusion
We have shown that, in contrast to what has been claimed in the literature, TAGs are not strong generative capacity.

Adjunction constraints. A third kind of adjunction constraint that has been used in the some node. It is not difficult to see that the proofs of Lemma 3, Lemma 4, and Theorem 3 still hold if Selective Adjunction constraints are used.
 weak lexicalization, defined in Section 1? We know that, in the case of CFGs, this ques-tion can be answered affirmatively, because Greibach normal form is a special case of lexicalized form, and for every CFG there is a weakly equivalent grammar in Greibach normal form. But to our knowledge, no comparable result exists for TAG. Second, if
TAGs cannot strongly lexicalize themselves, what would a grammar formalism look like that is capable of providing strong lexicalization for TAGs? Acknowledgments References
