 Most Web users use search engines to find the information they want from the Web. One common complaint about the current search engines is that they return too many useless results for users X  queries. Both the search engines and the users contribute to this problem. On the one hand, current search engines make little effort to understand users X  intentions and they retrieve documents that match query words literally and syntactically. On the other hand, Internet users tend to submit very short queries (average length is about 2.3 terms and 30% have a single term [8]). One way to tackle that all results in the same category corresponds to the same meaning of the query. 
In this paper we propose a new technique to group the search result records ( SRR s) returned from any search engine. Our focus will be on SRRs retrieved by single term queries . For queries with multiple terms, the specific meaning of each term is easier to determine because other terms in the same query can provide the meanings of each query term. Second, we apply a merging algorithm to merge synsets that have very close meanings into a super-synset. Third, we employ a two-step process to categorize SRRs into super-synsets. Fourth, our method also deals with SRRs that do not correspond to any WordNet-provided synsets of the query terms by clustering them. For example, when a word is used as a name, like  X  X pple X  and  X  X aguar X , it does not have its traditional meanings. The rest of the paper is organized grouping algorithm. Preliminary evaluation results are reported in section 4. The future work and conclusion are presented in section 5. The general problem of document clustering and categorization has been studied extensively [7] and they will not be reviewed in this paper. Instead, we focus on related works that deal with the clustering and categorization of the search result records (SRRs) returned from search or metasearch engines. 
Techniques for clustering web documents and SRRs have been reported in many papers and systems such as [14, 17, 18]. However, these techniques perform our method employs both categorization and clustering techniques and it also utilizes similarities that are computed using both syntactical and semantic information. 
Techniques for clustering and categorizing web documents using WordNet or other However, our approach differs from these techniques significantly. First, we use more features of WordNet such as hypernym, hyponym, synonym and domain. Second, we employ a sense-merging algorithm to merge similar senses before grouping. Third, our SRR grouping algorithm employs both categorization and clustering in a unique way. Fourth, our method also copes with SRRs that do not match any sense of the query term in WordNet. In other words, we utilize senses provided by WordNet but are not limited by them. Different techniques for clustering WordNet word senses are presented in [1] but they do not actually perform sense merging. These techniques can potentially be used for merging WordNet senses, e.g., merging the senses that are in term, not for general sense merging. Consequently, our technique can be more efficiently applied to grouping SRRs. In addition, our merging algorithm is also different from the existing ones. Our method groups the SRRs not only based on their syntactically similar words but topics but they have less similar words while two other SRRs have more common words but they are less similar in reality. Since we compare the meanings and semantic relations between words, our method is more likely to yield better-grouped SRRs compared to current methods. 3.1 Method Overview Our SRR grouping system for a user query Q consists of the following steps (Fig.1): We will explain each step in our algorithm in detail in the following subsections. 3.2 Submitting Query and Processing Results retrieved and are used as input to our SRR grouping algorithm. Each result (SRR) snippet of each SRR will be utilized to perform the grouping in our current approach. For each SRR, we first remove the stop words and stem each remaining word. Next, the SRR is converted as a vector of terms. For each term, its term frequency ( tf ) than words in the snippet (we currently double the tf of each term in the title). 3.3 Sending Query to WordNet Sending a user X  X  query to WordNet means that certain information about the query term is obtained from WordNet and processed. This step is done in parallel to sending the query to the search engine and processing the returned SRRs. The fact that relationships between synsets are explicit is the motivation behind using WordNet in our approach. Synsets are linked using various types of relationship links. In our current approach, the following types of synsets are utilized for a given synset S :  X  Hypernyms: Synsets that are more general in meaning than S  X  Hyponyms: Synsets that are more specific in meaning than S  X  Domains: Synsets that represent the domain of S  X  Synonyms: Keywords that have the same meaning with the user query This step (i.e., sending the query to WordNet) involves two procedures: 1. Get the senses/meanings for the query term. For each sense (synset), the 2. Merge similar senses if it X  X  applicable. This step will be explained in section 3.4. 3.4 Sense Merging same concept or very similar concepts. The example below illustrates one such a case. Example 1. Consider the following two synsets for query term  X  X eb X  Sense 1: web: (an intricate network suggesting something that was formed by weaving or interweaving;  X  X he trees cast a delicate web of shadows over the lawn X ) Sense 2: web, entanglement: (an intricate trap that entangles or ensnares its victim) 
These two senses are very similar because both talk about physical webs with a subtle difference that the former emphasizes how the web is formed and the latter emphasizes how the web is used. The following is another sense of  X  X eb X : Sense 3: World Wide Web, WWW, web presence of synsets with similar meanings poses challenges to the SRR grouping algorithm as well as to the users who consume the grouped results. We propose to tackle this problem by merging the similar senses. Our sense-merging algorithm consists of five merging rules, each of which gives one condition under which two senses S1 and S2 can be merged. The five rules are given below: Rule 1. If S1 and S2 have the same direct hypernym synset or one is a direct hypernym of the other, then merge S1 and S2. Rule 2. If S1 and S2 have the same direct hyponym synset or one is a direct hyponym of the other, then merge S1 and S2. Rule 3. If S1 and S2 have the same coordinate terms (i.e., there exist a synset S3 such that S1 and S3 share a direct hypernyn, and S2 and S3 also share a direct hypernym), then merge S1 and S2. Rule 4. If S1 and S2 have common synonyms, then merge S1 and S2. Rule 5. If S1 and S2 have the same direct domain synset or one is the domain of the other, then merge S1 and S2. 
Intuitively, each condition in the above rules indicates that S1 and S2 are semantically similar. 3.5 Computing the Similarity Between SRRs and Super-Synsets In this paper, we use a revised Okapi function to compute the similarity between SRRs and super-synsets. We made the changes to the original Okapi function [11] to and the other is the super-synset set S*, whereas in the traditional information retrieval context, one document (the query) is compared with a set of documents. Our revised Okapi function for computing the similarity between an SRR R and a super-synset S is: with 2 , 1 , where N 1 and N 2 are the numbers of SRRs in R* and super-synsets in S*, is the length of R , and avgdl ( R* ) is the average length of all the SRRs in R *; k = 1.2 replaced S and R * is replaced by S *. 3.6 SRR Grouping Algorithm Our SRR grouping algorithm (Algorithm CCC ) consists of the following three steps: 1. Preliminary C ategorization . Categorize SRRs based on their similarities with 2. Further C ategorization . Categorize the remaining SRRs from step 1 (i.e., those 3. Final C lustering . If there are still uncategorized SRRs left, we cluster them using 
In our current implementation, the two thresholds T1 and T2 are determined using a training set. When training T1, we try to find the value that achieves the maximum the correct synset (i.e., close to 100% precision ). In step 1, we try to be more conservative since we will have another chance to categorize the remaining SRRs in step 2. Consequently, after the SRRs categor ized into a synset are merged at the end of step 1, each category is as accurately represented as possible. In step 2, the cluster C is considered because we want each rema ining SRR to have a fair chance to be categorized or stay uncategorized, as it is possible that some SRRs do not match any senses from the WordNet. Step 3 is needed because many English words have non-standard uses in practice (such as used as a name of a company) that do not match any senses the WordNet has about these words and a query term may be a non-standard English word (such as  X  X llinone X ). We implemented our algorithm using Java. We use JWNL to connect to WordNet 2.0. Two datasets are used in this paper and each dataset contains 10 single-term queries and the 500 SRRs (50 unique SRRs per query) from search engine Yahoo. The 10 queries for the first dataset DS1 are (notebook, jaguar, mouse, metabolism, piracy, suicide, magnetism, web, people, salmon), and the 10 queries for the second dataset DS2 are (apple, dish, trademark, map, music, car, game, tie, poker, mold). DS1 is used for training to obtain the thresholds (T1 = 4 and T2 = 0.1 are obtained). 4.1 Alternative Solutions As mentioned before for some terms, there are categories that are not covered in WordNet. For example, for query  X  X aguar X , there are two categories not in WordNet, the first is the brand name for car and the second is unknown (some company names). 
For our evaluation, we also compare the SRR grouping algorithm described in section 3.6 with two other intuitively reasonable solutions. Basically, each of the two alternative solutions replaces the last two steps ( Further categorization and Final clustering ) while the first step ( Preliminary categorization ) remains the same. In word. Our first alternative solution is based on the frequency of use. Note that during sense merging, the frequency of use of a super-synset is computed as the sum of the frequencies of use of all the individual synsets it contains. 684 R. Hemayati, W. Meng, and C. Yu  X  Largest frequency of use ( LF ): Assign all remaining SRRs (after the Preliminary 
The rationale for this method is that the super-synset with the largest frequency of use represents the most common sense of the term among those covered by WordNet. probably the most popular category for the retrieved SRRs.  X  Largest category ( LC ): Assign the remaining SRRs to the category that has the 4.2 Performance Measures We evaluate the sense-merging algorithm as well as the three SRR grouping algorithms (CCC, LF and LC). For all algorithms, we use the recall , precision and F1 measure (which combines recall and precision) as the performance measures. For the merging algorithm, we define precision = |A  X  B|/|B| and recall = |A  X  B|/|A|, where A is the set of merges that should be performed as judged by a human expert and B is the set of merges our merging algorithm performed. All the 20 queries in both datasets are used. Note that our merging algorithm does not need any training. For the SRR grouping algorithms, the recall and precision are defined below [10]:  X  Precision p : For a given category, the precision is the ratio of the number of 2* p * r / ( p + r ). The F1 measure is high only when both precision and recall are high. 4.3 Experimental Results Our sense merging algorithm has a precision of 100, recall of 66 and F1-measure of 80. The results show that all merged senses are correct, but our algorithm still couldn X  X  find all possible merges. Tables 1 and 3 show the results for the three SRR grouping algorithms based on DS1 and DS2 when merged senses are used. It can be seen that Algorithm CCC performs significantly better than Algorithms LF and LC. This is mainly due to the fact that the former can group the SRRs beyond the synsets in WordNet while the latter two methods force the SRRs that do not match any DS1, indicating that the trained thresholds are reasonably robust. Tables 2 and 4 show the results when un-merged senses are used. It can be seen that sense merging helped the performance improve by approximately 5 percentage points. One of the reasons that causes incorrect grouping is the lack of common terms between some SRRs and the correct synset representations. We plan to investigate this problem in the future. from search engines (or metasearch engines) for single-term queries. Single-term queries are often ambiguous because many English words have multiple meanings. proposed a novel three-step grouping algo rithm that combines both categorization and clustering techniques. We also proposed an algorithm to merge similar senses returned from WordNet. Our preliminary ex perimental results indicated that our SRR grouping algorithm is effective, achieving an accuracy of about 90%. We also showed sense-merging algorithm can improve grouping accuracy by about 5%. 
We plan to continue this research in the following directions. First, we plan to conduct more experiments using a significantly larger dataset. Second, we will try to 686 R. Hemayati, W. Meng, and C. Yu improve our sense-merging algorithm and SRR grouping algorithm as there are still rooms for improvement. Third, while WordNet is very useful, it is far from perfect in providing all the senses for many words. We plan to see if other online semantic dictionaries, such as Wikipedia, can also be utilized. Finally, we also plan to develop good SRR grouping solutions for multi-term queries. Acknowledgment. This work is supported in part by the following NSF grants: IIS-0414981, IIS-0414939 and CNS-0454298. 2. G. Attardi, A. Cisternino, F. Formica, M. Simi, A. Tommasi. PiQASso 2002. TREC11 3. E. W. De Luca and A. N X rnberger, O. von-Guericke. Ontology-Based Semantic Online 4. T. de Simone and D. Kazakov. Using WordNet Similarity and Antonymy Relations to Aid 6. A. Hotho, S. Staab, G. Stumme. WordNet Improves Text Document Clustering. ACM 7. A.K. Jain, M.N. Murty. Data Clustering: A Review. ACM Computing Surveys, 1999. 8. B. Jansen, B. Spink, J. Bateman, T. Saraceric. Real Life Information Retrieval: A Study of 9. S. Liu, C. Yu, and W. Meng. Word Sense Disambiguation in Queries. CIKM, 2005. 10. Q. Peng, W. Meng, H. He, and C. Yu. WISE-Cluster: Clustering E-Commerce Search 11. S. Robertson, S. Walker, M. Beaulieu. Okapi at Trec-7: Automatic Ad Hoc, Filtering, Vlc, 13. WordNet; http://wordnet.princeton.edu/ 14. Vivisimo, http://www.vivisimo.com 16. Y. Yang. A Study of Thresholding Strategies for Text Categorization. ACM SIGIR, 2001. 
