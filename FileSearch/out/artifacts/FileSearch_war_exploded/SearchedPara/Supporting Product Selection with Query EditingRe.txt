 Consider a conversational product recommender system in which a user repeatedly edits and resubmits a query until she finds a product that she wants. We show how an advi-sor can: observe the user X  X  actions; infer constraints on th e user X  X  utility function and add them to a user model; use the constraints to deduce which queries the user is likely to try next; and advise the user to avoid those that are unsat-isfiable. We call this information recommendation . We give a detailed formulation of information recommendation for the case of products that are described by a set of Boolean features. Our experimental results show that if the user is given advice, the number of queries she needs to try before finding the product of highest utility is greatly reduced. We also show that an advisor that confines its advice to queries that the user model predicts are likely to be tried next will give shorter advice than one whose advice is unconstrained by the user model.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation Human Factors recommender systems, user models
Recommender systems are intelligent e-commerce appli-cations that suggest products or services which best suit a user X  X  needs and preferences, in a given situation and con-text [1, 2]. They have been successfully exploited for rec-ommending travel services, books, CDs, financial services, insurance plans, news, and in many other application mar-kets. From a technical point of view, recommender systems emerged as supervised learning approaches using a data set of numerical ratings on products (e.g., from 1=bad to 5=ex-cellent), expressed by a collection of users on a catalogue o f products, to make a prediction for products not yet rated by the target user, i.e., the user for whom a recommenda-tion is sought. Prediction algorithms include collaborati ve-filtering, content-based filtering and case-based reasonin g [1, 2, 3]. However, these classical approaches typically suppo rt a simple human-computer interaction model, which basicall y first collects user related information (ratings or product preferences) and then exploits the background knowledge to make ratings X  predictions and derive product recommenda-tions.

More recently, a number of conversational approaches have been proposed. In conversational recommender systems the advisor not only suggests and ranks products, it also guides the user by asking for more information about her prefer-ences or providing information about the product and the search process, e.g., explaining the rationale of the ranki ng or explaining the failure of a search initiated by the user [8 , 5, 3, 9, 10, 11, 7]. The user may not know, or may not be aware, of all her preferences at the start of the interaction . Preferences are revealed, or even constructed, during the i n-teraction as the product space is explored. By contrast, the user of a non-conversational ( X  X ingle-shot X ) recommender is expected to be able to articulate all preferences up-front.
In this paper, we introduce the idea of information rec-ommendation , bringing the concept of conversational recom-mender systems to its more radical interpretation. A conver -sational recommender can use information recommendation to suggest actions that help the user to efficiently search for products, as well as using product recommendation to suggest products that the user may like.

Consider a conversational product recommender system in which a user repeatedly edits and resubmits a query until she finds a product that she wants [9, 5]. An advisor can infer constraints on the user X  X  utility function by observing the user X  X  actions. For instance, the system might infer that th e features constrained by the user query are more important, for the user, than the features not yet used in the query. The advisor can add these constraints to a particular kind of dynamic user model [4, 8]. The advisor can then use the constraints to deduce new preference relations between product features and ultimately to rank the next possible search actions of the user, rather than, or as well as, using the preference relations to select and rank products in the catalogue. In other words, in information recommendation, the advisor uses the user model to determine the feasibility of actions (the satisfiability of queries in this case) that it thinks the user is more likely to try. It can then advise the user of which to try or which to avoid.

In the rest of this section we compare the use that infor-mation recommendation makes of its user model with the use that product recommendation makes. In the Adaptive Place Advisor, for example, the user model is used for prod-uct selection [11]. The advisor infers feature weights and defaults, and uses them in product retrieval. Similarly, in the work of Pu et al. , the system uses its knowledge of users and of the product space to select sets of products, albeit se ts that it hopes will provoke the user into volunteering furthe r preferences [7]. In collaborative filters also, the user mod el (the ratings profile) is for product retrieval and ranking.
But in information recommendation, the user model is used to guide the user X  X  search rather than to retrieve or ran k products. Work on question selection in dynamic dialogues can be seen as an example of information recommendation. The system dynamically selects questions to elicit user pre f-erences. Its goal is to choose a sequence of questions that most effectively homes in on desirable products. In most such work, questions are selected based on the user X  X  partia l query and the product distribution. But, Schmitt X  X  simVar system also builds and uses simple user models too [10]. Reilly et al.  X  X  use of the query history to dynamically rec-ommend compound critiques can also be regarded as infor-mation recommendation [8]. In their work, the system shows the user some products which the user can critique, but it also advises the user by displaying dynamically-computed critiques that are known to be satisfiable, which the user can select.

Section 2 describes our assumptions about how products and queries are represented; it describes users X  utility fu nc-tions; and it describes the idea of a conversational product recommender in which the user repeatedly edits and resub-mits her query in a search for the products of highest utility . The section also explains the assumptions we make about user rationality. Section 3 explains how the advisor can inf er the constraints that it adds to the user model and it explains different strategies for giving advice to the user. Section 4 presents our experimental methodology and results.
We assume a very simple product data model. Products are described by a fixed number of n Boolean features. For instance, hotels may have: a sauna, a pool, parking, etc. Hence, each product can be represented by a fixed-length string of bits, p = p 1 , . . . , p n , where p i = 1 means that the product has the i th feature and p i = 0 means that it does not have the feature.

User queries are defined by a product pattern q = q 1 , . . . , q where q i is either 1 or 0, i = 1 , . . . , n . If q i = 1, the user is interested in products that have the i th feature; if q i the user has not (yet) declared any special interest in the i th feature. It must be kept in mind that, e.g., q = 1010 does not mean that the user wants to see products that lack the second and fourth features; it simply means that the user wants a product that has the first and the third features.
Given query q , the query engine retrieves products P where if q i = 1 then p i = 1, for all p i  X  P . In other words, each retrieved product must possess at least all the feature s that are explicitly requested in the query, but may possess other features too. For example, q = 1010 is matched by products such as 1011 and 1111 as well as 1010; it is not matched by products such as 0010. We describe a query as satisfiable if it is matched by at least one product; otherwise, we call it unsatisfiable . Given query q , we expect the query engine to at least tell the user whether q is satisfiable or not.
We assume the user has a fixed utility function. The util-ity of product p = p 1 , . . . , p n is defined as follows: where ( w 1 , . . . , w n ) is a vector of weights. We assume only that the weights are non-negative and do not exceed 1 (0  X  w i  X  1) and that there is at least one non-zero weight ( w i &gt; 0). The weight of a feature says how strong the de-sire of the user for that feature is. If a weight w i is zero, then the user has no desire for the i th feature; if w i &gt; w the i th feature is preferred to the j th; if w i = w j ( i 6 = j ), then the user is indifferent between the i th and j th features. We assume that the goal of the user is to find a product that maximises the utility function.

We can also define the utility of a query q = q 1 , . . . , q In fact, we define two types of query utility. The potential utility of query q is given by Hence, the potential utility of a query is the utility to the user of a product that offers exactly the features requested in the query, whether such products exist or not.
 The actual utility of query q is given by Hence, the actual utility of a query is zero if no product matches the query; otherwise, the actual utility equals the potential utility.
In the kind of conversational recommender that we are considering in this work, the user is engaged in an inter-active, incremental search process. The states of the searc h space are different queries. The successors of state (or quer y) q , succ ( q ), will be the queries that the user obtains by editing q . succ ( q ) will depend on the actions that the user interface makes available, e.g., it might allow the user to add a featur e to a query, to delete a feature from a query, to restart, etc. The user X  X  goal in query editing is to move to a state with higher actual utility.

We assume a certain rationality in user behaviour. A min-imal requirement is that a user will not search for products that are less preferable than those already selected:
Axiom 1. If q is the user X  X  current query and q 0 is a suc-cessor query, q 0  X  succ ( q ) , then the user will try q 0 if and only if U ( q 0 )  X  U ( q ) , and may choose to accept q 0 if and only if V ( q 0 ) 6 = 0 .
 In other words, we assume that the user will only try q 0 (i.e. issue the query to the query engine) if its potential util-ity is greater than or equal to q  X  X ; and then the user may choose to accept q 0 (i.e. move to this state) if q 0 has higher or equal actual utility (i.e. if the query engine reports that
Axiom 2. If q is the user X  X  current query and the user is contemplating a set Q  X  succ ( q ) of successor queries each of whose potential utility is greater than or equal to q  X  X  ( U ( q 0 )  X  U ( q ) for q 0  X  Q ), then the user will try a member of Q  X   X  Q , where Q  X  contains those members of Q that have maximal potential utility: In this axiom we are saying that the user will choose to try one of the queries that has maximal potential utility from those successor queries Q that the user is contemplating. Of course, this leaves open the question of which succes-sor queries are in the  X  X ontemplation set X , Q , at any point in the user X  X  search. This will depend on the user X  X  back-ground knowledge, what she has learned so far during the interaction and her cognitive resources (e.g. her memory an d reasoning capabilities).

In fact, an advisory system may not know exactly how the user computes her contemplation set at any point in the in-teraction. But, it can infer user preferences by observing t he queries the user tries. If the user tries query q 0 , the system may infer that U ( q 0 )  X  U ( q 00 ) for all q 00  X  Q 0 , where Q 0 is the set that the system assumes the user is contemplating. But if the set the user actually contemplated is a proper subset of Q 0 then the system may make wrong inferences: it may incorrectly infer U ( q 0 )  X  U ( q 00 ) for all q 00  X  Q 0 . Hence, in the following we will make some assumptions about the nature of the user X  X  contemplation set and, from the queries the user issues, we will derive constraints on the definition of the user X  X  utility function.
As we noted above the contemplation set is a subset of the successor queries that the user interface make availabl e. Hence, following the above discussion, one way to keep the contemplation set small, thereby making it less likely that the system makes incorrect assumptions, is to keep also succ ( q ) as small as possible by offering only a small num-ber of easily-understood editing operations. Since real us er behaviour in query editing processes tends to proceed with minimal modifications, restricting moves to ones of limited  X  X each X  will not impose a true limitation on real users.
Hence, here we assume that just three editing operations are available: Op1  X  Add a feature This means changing a 0 in q to Op2  X  Switch a feature for another This means simul-Op3  X  Trade a feature for two features This means si-
We now discuss the rationale for considering only these three moves.

The first move is quite obvious: given non-negative weights, an Op1 move will never decrease the potential utility. In fact, if q 0 = Op1 ( q, i ), then the gain in potential utility is: We note that Op1 moves may not change the actual utility by this amount since the new query might be unsatisfiable, in which case actual utility will fall from U ( q ) to 0.
Consider now the second move. If q 0 = Op2 ( q, i, j ), the effect on utility is In general, this sum can be negative. But Axiom 1 says that a user will try this editing operation only if  X  w i + w j i.e. if there is no loss in potential utility.

In the case of the third move, if q 0 = Op3 ( q, i, j, k ), this has the following effect on the utility: In general this sum can also be negative. But Axiom 1 says that a user will try this editing operation only if  X  w i w k  X  0, i.e. if the gain in potential utility brought by the two features added is greater than or equal to the loss in potential utility due to the feature removed.

We do not consider the move of deleting a feature from the current query since this always has a zero or negative ef-fect on the potential utility. In principle, we could also al low the user to apply more complex transformations, for exam-ple discarding two features while adding three new features . But, as explained above, restricting the available editing op-erations helps both the user and the system.

It is worth noting that there are situations where an Op3 move can be preferred to an Op1 move. If for instance the current query q is 100 and the user knows that both Op1 ( q, 2) = 110 and Op1 ( q, 3) = 101 are unsatisfiable but Op3 ( q, 1 , 2 , 3) = 011 is not known to be unsatisfiable, then the Op3 move is the best option for the user. It is also worth noting that there are cases where an Op2 move is useful, even in the presence of the Op1 and Op3 operations. For lack of space we shall not describe here an example of this situation.

It might then be thought that we could restrict our atten-tion to a system that offers only Op1 and Op2 edits, since an Op3 edit can be composed of an Op1 and an Op2 : e.g. Op3 ( q, i, j, k ) = Op3 ( Op2 ( q, i, j ) , k ). This is true but the Op2 move required to obtain the given Op3 move may not be rational, i.e. it may decrease the potential utility.
A final observation is that with these three edit operations the user is not guaranteed to reach the global optimum state. For lack of space we shall not describe an example. But we note, happily, that such examples are quite contrived and therefore unlikely to arise in practice for rational users.
Assumptions about the  X  X ationality X  of the user (Axioms 1 and 2) allow an observer to deduce knowledge about that user X  X  utility function. In this section, we will focus on th is issue. First, let us introduce some notation. If q = q 1 , . . . q is a query and i is an index (1  X  i  X  n ) and q i = 0, then we shall denote with q + i the query obtained by setting q i if q i = 1, then we shall denote with q  X  i the query obtained by setting q i to 0. Further, let idx 0( q ) = { i : q i = 0 } and idx 1( q ) = { i : q i = 1 } , the indexes of the bits in q that are 0 and 1, respectively.

Proposition 1. Assume that the user tries an initial query q . For all i  X  idx 1( q ) and j  X  idx 0( q ) , then w i  X  w V ( q  X  i + j ) = 0 .
 Proposition 1 says that the user will not add to her initial query a feature i that is less preferable than another feature j that is not included in the query, unless the query that contains j and not i is unsatisfiable. This proposition derives from Axiom 2 and the observation that the user could have chosen the query q  X  i + j instead of q , hence it must be the case that U ( q  X  i + j )  X  U ( q ), and this is true if and only if w i  X  w j . Here we are  X  X laying safe X  by assuming that if V ( q  X  i + j ) = 0, then the user knows it. Obviously the user may not know all these unsatisfiable queries and, from the initial query, we could derive more inequalities. For insta nce, we could simply assume that all the features included in the query are preferred to the features not included, i.e. w i for all i  X  idx 1( q ) , j  X  idx 0( q ). But this is not safe because in fact the user may have background knowledge that some feature combinations are not possible and therefore she has not tried these combinations in her initial query.
To illustrate this issue, consider the following simple ex-ample. Let us imagine that the user knows that there is no golf course in the centre of the city to which she is travel-ling. Suppose hotels are described by three features, and in decreasing order of preference for this user they are centre , golf and parking . Then from an initial query in which cen-tre = 1, golf = 0 and parking = 1, an observer would be wrong to infer that parking is preferred to golf ; the observer would be wrong because the switch from parking to golf is not satisfiable.

Proposition 2. If the user tries to add a feature i to the current query q , i.e. she applies Op1 ( q, i ) , then w i all j  X  idx 0( q ) i 6 = j , unless V ( q + j ) = 0 . Proposition 2 says that the user will not have added a feature whose additional utility is lower than that of another featu re also not yet in the query, unless she knows that adding the feature with greater additional utility would result in an unsatisfiable query. There are similar motivations to that mentioned in the previous case for  X  X laying safe X  by assumin g that the user knows which are the unsatisfiable queries.
Proposition 3. If the user tries to switch feature i to feature j , i.e. she applies Op2 ( q, i, j ) , then w i  X  w w j  X  w k for all k  X  idx 0( q ) k 6 = j unless V ( q  X  i + k ) = 0 . Proposition 3 says that the user will not switch a feature for one that has inferior preference; and the feature to which th e user is switching must not have utility inferior to any other feature not constrained in the query, unless this alternati ve query is unsatisfiable.

Proposition 4. If the user tries to trade feature i for features j and k , i.e. she applies Op3 ( q, i, j, k ) , then w w j + w k ; and for all j 0 , k 0  X  idx 0( q ) such that { j, k }6 = Finally, Proposition 4 says that the user will not discard a feature for two features having inferior sum of preference; and the two features added must not have sum of utility inferior to two other features not constrained in the query, unless this alternative query is unsatisfiable.

Note how several of these propositions assume only a lim-ited amount of reasoning capability of the user. In fact, if the user is more fully rational, we could deduce more. For example, in the case of Proposition 4, we could also deduce that the selected Op3 ( q, i, j, k ) is better than all the such that { i, j, k }6 = { i 0 , j 0 , k 0 } ,  X  w i + w j w Using the same reasoning we can also infer that a selected Op2 move is better than or equal to all the satisfiable Op1 moves, and a selected Op3 move is better than or equal to all the satisfiable Op1 and Op2 moves.

These are what we might infer if we attribute ever greater rationality to the user. In this paper, we are not going to attribute these higher levels of rationality to the user, an d hence we will infer only what is stated in Propositions 1 X  4. An important observation is that it is not a problem to our proposed methods if we assume that users are less rational than they really are. The reason this does not pose a problem to our proposed methods is that assuming users are less rational than they really are will result only in us making fewer deductions when observing their moves; it will not result in us drawing incorrect inferences. In fact, it is more dangerous to assume a fully rational user, who can really take the best move, since this will cause us to draw inferences that may be incorrect.
We now describe the help that an advisory system can provide to the user to let her more effectively find the prod-ucts that have maximal actual utility.

If the system could somehow know the user X  X  utility func-tion then it could use a simple search procedure to identify the product(s) with highest actual utility. But in our work, we are assuming that the system does not know the user X  X  utility function and it is not going to ask the user about fea-ture preferences (weights) . Instead, the advisor infers con-straints on the user X  X  utility function by observing the use r X  X  moves, and adds these constraints to a user model.
In addition to telling the user the findings of the query engine, i.e. how many products, or which products if any, match the current query, the advisor is also capable of telli ng the user about the satisfiability of related queries (especi ally those that it thinks that the user is likely to try). We note that in contrast with query relaxation , e.g., [5, 6], where the goal is to repair an unsatisfiable query (especially in non-conversational recommenders), our focus is on methods that support the user in avoiding the generation of unsatisfiable queries in a conversational setting.

Imagine for instance a user whose current query requests hotels that have air-con and golf, and the advisor knows that, while the user X  X  query is satisfiable, there are no such hotels located in the city centre but there are some that also have swimming pools. In this situation the system could help the user in the search process by supplying exactly this information: this will dissuade the user from fruitlessly t ry-ing to add to her query a request for a city centre location, 1100 1010 1001 0110 0101 0011 Figure 1: Example of all queries on four Boolean attributes; shaded queries are ones we are taking to be unsatisfiable in this example. Op1 moves are shown as solid lines; Op3 moves are shown as dashed lines. Op2 moves are not allowed in this example. and it will enable her to realise that she can satisfy a desire for a pool. Moreover, if it is satisfiable to have air-con, gol f and a sauna but the advisor has inferred from earlier actions that a sauna is less preferable than a pool, then it is better to tell the user that there are no hotels with air-con and golf in the centre but there are some that have a pool rather than to tell her that there are no hotels with air-con and golf in the centre but there are some that have a sauna.

Obviously the advisor could tell the user the satisfiability of all the possible moves, but such a list would not be of much use, as it could potentially be very long. Hence the advisor must put in place methods to minimise the quantity of information that is passed to the user during their inter-action. In other words, the advisor must pass to the user the information that has the greatest value for the user.
The information that has the greatest value is here con-sidered to be that which minimises the total quantity of in-formation exchanged and the interaction length, while stil l finding the product(s) having maximal actual utility.
In this section we illustrate with an example the typical human-computer interaction we want to support. Figure 1 depicts all possible queries on four Boolean features, i.e. all 16 possible feature combinations. The shaded boxes repre-sent queries we will take to be unsatisfiable in this example, i.e. no products match those queries. The arrows represent Op1 and Op3 editing operations. To simplify the example and to keep it relatively concise, we are going to assume that the query engine does not support Op2 editing operations.
Let us assume that the user has issued an initial query q = 1100. This means that she is looking for a product that has the first two features, and we can see that this is satisfiable.

From this query the advisor infers that w 1  X  w 3 , w 2  X  w and w 2  X  w 4 , but it does not infer that w 1  X  w 4 because switching the first and fourth features produces an unsatis-fiable query. As we have discussed previously, the advisor  X  X lays it safe X , in case the user somehow already knew that 0101 was unsatisfiable. It also cannot deduce anything about the relationship between w 1 and w 2 because they have been introduced simultaneously in the same initial query.
The advisor knows that 1110, 1101 and 1111 have a higher utility for the user (if the weight of the added feature is non -zero) or the same utility (if the weight of the added feature i s zero) but they are all unsatisfiable. Conversely, the adviso r is still uncertain whether 1011 and 0111 have a higher, equal or lower utility than that of 1100, although it does know that 0111 is unsatisfiable.

In this situation, the advisor can, for instance, take a con-servative approach and tell the user about the satisfiabilit y or unsatisfiability of each of the queries that can be obtaine d by a single Op1 or Op3 operation applied to 1100. In this case, it would tell the user that 1110, 1101 and 0111 are unsatisfiable but 1011 is satisfiable.

Then there follow three possibilities. If for this user in fa ct w 2 &lt; w 3 + w 4 , then the user at the next step can immediately find the optimal solution, i.e. she can decide to edit using Op3 and retrieve the products satisfying 1011; if for this user in fact w 2 &gt; w 3 + w 4 , then the user will know she is already at an optimal solution; and if for this user w 2 = w 3 + w then she must try the Op3 edit in case subsequent edits are available that can increase utility. Hence in the best case s he will do zero moves and in the worst case one. Unfortunately the advisor, at this point, cannot know whether w 2 &lt; w w : for some users it will be true and for others it will not.
On the other hand, the advisor could tell the user only the unsatisfiable next queries (1110, 1101 and 0111).
For the user, the same reasoning applies. If for the user w 2 &lt; w 3 + w 4 , then she will use Op3 to move to 1011; if w 2 &gt; w 3 + w 4 , she can stay in 1100; if w 2 = w 3 + w must try the edit to see whether it brings opportunities to subsequently improve utility. Hence, this is a better behav -iour for the advisor: it reveals information about only thre e queries (one fewer than before), yet there is no extension of the interaction in both the worst and best cases.

Suppose instead the advisor tells only the unsatisfiability of the Op1 moves (i.e. that 1110 and 1101 are unsatisfiable). Then the user may require (in the worst case) two moves, i.e. a move to 0111 (with an Op3 move), only to learn that it is unsatisfiable, and then the alternative move to 1011 (also with Op3 ). The best case is always 0 moves.

Finally, suppose the advisor (with an even more incom-plete strategy) were to tell of the unsatisfiability of just t he second Op1 move (i.e. that 1101 is unsatisfiable, without mentioning the unsatisfiability of 1110). Then the user will always try 1110 since, without the knowledge that 1110 is unsatisfiable, it has a utility no worse than the two Op3 moves for the user. In the worst case in this scenario, the user will need three moves (1110, 0111 and 1011) to get the optimal query. A similar situation occurs if the system reveals only that 1110 is unsatisfiable.

In this simple example, we have seen that there is a trade-off between the quantity of information revealed by the ad-visor and the length of the interaction. Moreover, we have seen that there are actions made by the advisor that are not minimal, in the sense that revealing less information will result in the same interaction length.
Every user model can contain the facts that weights are assumed to be non-negative and to not exceed 1: 0  X  w i  X  1. And every user model can contain the fact that at least one weight is non-zero: w i &gt; 0. Beyond this, the advisor will infer the constraints that it can deduce according to Propo-sitions 1 X 4. Let us denote the contents of a user model, i.e. the set of constraints the advisor has collected at a certain point in the interaction, by C .

We assume that, after each query submitted by the user, the query engine tells the user whether the query is satis-fiable or not. And, after each satisfiable query, the advisor tells the user whether certain (related) queries that the us er has not yet tried but is likely to try would be satisfiable or not. Note that we assume that the advisor only needs to give advice when the user X  X  query is satisfiable as only in this case can it be further extended (unsatisfiable queries will not change the current query).

The goal is to identify, at each step of the interaction, the queries whose (un)satisfiability the advisor should re-port to the user such that the user will find the products that maximise her utility function in a process with mini-mal cost, where the cost of the process is a monotone func-tion of the number of queries tried and the number of query (un)satisfiabilities revealed by the advisor to the user.
Given a user model (a set of inequalities) C , we can com-pute (using linear programming techniques) a partial order  X  C on a set of queries Q , where for each q, q 0  X  Q , q  X  iff we can derive from C that U ( q )  X  U ( q 0 ). Let us de-note by Best ( C, Q ) the set of queries that are maximal in Q according to the partial order defined by C . We have the following:
Proposition 5. If the current query is q and the user model at this point is C , the next query that the user will try is a member of Best ( C, succ ( q )) where succ ( q ) as before is the set of queries that can be generated by modifying q with a single Op1 , Op2 or Op3 operation. This proposition says that the partial order tha t the advisor builds from the user model is compatible with the user X  X  utility function, and that the best move, i.e. the one maximising the potential utility, is among those that are reachable by Op1 , Op2 or Op3 and that are maximal according to this partial order.
 Hence, we expect the user to try a move that belongs to Best ( C, succ ( q )). Actually this is a superset of the moves that the user will consider since the information that the system has derived from the interaction can only partially reveal the user X  X  utility function. This is one reason why the system can only deduce a partial order on the queries, whereas the user will have a set of one or more equally good best next actions.

We can decompose Best ( C, succ ( q )) according to two or-thogonal distinctions. First, there are three types of quer ies in Best ( C, succ ( q )), i.e. Op1 , Op2 and Op3 moves. We will denote these by Best ( C, succ 1 ( q )), Best ( C, succ 2 Best ( C, succ 3 ( q )), respectively. Second, the advisor will know that some of the queries are satisfiable and some are un-satisfiable. We will denote these by Best ( C, succ ( q )) + and Best ( C, succ ( q ))  X  , respectively.

Hence, the advisor has 3  X  2 = 6 types of information that it can reveal, and it must decide what to tell the user given a query history. If the advisor provides only a subset of the best next moves, then the user may still try one of the moves that she was not told about and it may prove to be unsatisfiable.

One rational strategy, which we call the Complete Recom-mendation on Maxima Strategy , is to reveal either Best ( C, succ ( q ))  X  , i.e. all moves that are unsatisfiable (and advise the user to avoid them), or Best ( C, succ ( q )) + , i.e. all moves that are satisfiable (and advise the user to confine her attention to them). To minimise the amount of advice which is given we assume this strategy involves telling the user about the smaller of the two sets.

Another strategy, which we call the Incomplete Recom-mendation on Maxima Strategy , is to tell the user about Op1 moves (again choosing to tell the user either about the satisfiability of Best ( C, succ 1 ( q )) + or the unsatisfiability of Best ( C, succ 1 ( q ))  X  , whichever is the smaller set) and only if there is no satisfiable Op1 move to provide advice about the Op2 and Op3 moves (again, the satisfiable queries or unsat-isfiable queries, whichever is the smaller set). This is clea rly an incomplete strategy since there might be an Op3 move with a higher utility gain than the Op1 moves suggested.
We have evaluated five information recommendation strate-gies in a simulation. Two of the five are the Complete and In-complete Recommendation on Maxima Strategies described in the previous section. We also have three  X  X aseline X  strat e-gies. The first is Null Recommendation, i.e. providing no advice at all. The other two are based on the Complete and Incomplete Strategies but they use succ ( q ) in place of Best ( C, succ ( q )), i.e. they form their advice from all next possible moves, hence they generate longer advice : their ad-vice is not confined to queries that, according to the user model, will be favoured by the user. By comparing the two strategies that use the maxima with the ones that use all next moves, we can see how much shorter the advice is when the advisor exploits a user model.

We use three separate product databases, each describing hotels by their amenities expressed as Boolean features suc h as airport shuttle , pets permitted , restaurant on-site , etc. De-tails of the product databases are given in Table 1. Many hotels offer the same amenities, which explains the differ-ence between the number of (physical) hotels and, from an amenities point of view, the number of ( distinct ) products.
Our evaluation uses three different types of simulated user: optimizing, prioritizing and random . They have several things in common. They do not try queries that they have tried before; they only try Op1 , Op2 and Op3 moves that do not lessen potential utility; and they take heed of all advic e given, i.e. if the advisor tells them to confine their queries to a certain set or to avoid queries in a certain set, then they do so. They differ in the way they select from moves that remain. An optimising user chooses the best possible move from the set, as per Axiom 2. In the event of ties (more than one equally good best move), the user uses ran-dom tie-breaking. A prioritising user has a preference for Op1 moves. It will first try to use Op1 to add the most useful feature to its current query (again with random tie-breaking). If no satisfiable move results from adding each most useful feature, its next moves will be to try to add each second most preferred additional feature (with random tie-breaking). Only when all Op1 moves have been exhausted (in decreasing order of usefulness) will it try Op2 or Op3 moves, in descending order of potential utility gain. A ran-dom user chooses its next move randomly.

In the experiments, we pair each of the three user types with each of the five advisors. In pairings which include ran-dom users with advisors that use user models, the advisor will assume greater rationality of the user than the user act u-ally exhibits. In such pairings, the inferences that the adv i-sor draws may be incorrect. Worse, the user model may be-come inconsistent, at which point we take Best ( C, succ ( q )) to be equal to succ ( q ), i.e. the advice degenerates to that which would be given with no user model at all.
For each product database, we randomly generate 50 weight vectors (where weights are randomly-chosen real numbers to one decimal place in [0 , 1]) and a satisfiable initial query compatible with each weight vector. The use of a set of random queries gives us fair coverage of the query space, but it would be useful in the future to incorporate a bias towards the kinds of queries and weight vectors that real users exhibit.

For each user-advisor pairing, we ran 50 dialogues starting from each of the initial queries. The results, in Table 2, sho w the following, averaged over the 50 interactions: A) The number of queries the user tries;
B) the number of queries the user tries that are successful,
C) the total number of queries whose (un)satisfiability is
D) the amount of utility, if any, by which the final query We will focus to begin with on Optimising users (Opt) and Complete on Maxima strategies (CoM). We can clearly see the advantage of giving advice. Compare the figures for Null advisors with the rest. The number of queries that the user must try on average (column (A)) is substantially reduced if the user receives advice: from 21.24 to 4.67 (Marriott-NY); from 37.14 to 5.55 (Cork); and from 16.12 to 6.09 (Trentino-10). Furthermore, with Null advisors the majority of querie s the user tries are unsatisfiable (column (B)).

We can also clearly see the advantage of using a user model to restrict the advice to the moves that we infer the user will try next. Compare the figures for advisors that base their advice on all next moves (CoAN) with those that com-pute the maxima (CoM). The number of queries in the ad-vice (column (C)) is substantially reduced when the maxima is used: from 75.36 to 45.96 (Marriott-NY); from 96.84 to 59.02 (Cork); and from 109.20 to 69.88 (Trentino-10). A similar story can be told for the incomplete strategies. But, furthermore, the number of queries within the advice given by incomplete strategies is much lower (and more rea-sonable) than that given by corresponding complete strate-gies. Consider, for example, the Complete and Incomplete on Maxima strategies (CoM and IoM): from 45.96 to 5.14 (Marriott-NY); from 59.02 to 8.51 (Cork); and from 69.88 to 6.27 (Trentino-10). And this comes at only fairly modest increases in the number of queries that the user must try: from 4.67 to 5.98 (Marriott-NY); from 5.55 to 12.13 (Cork); and from 6.09 to 9.36 (Trentino-10). Of course, some of these extra queries are unsatisfiable (column (B)).
Finally, let X  X  look across the three types of users. Un-surprisingly, Random users suffer longer interactions (col -umn (A)) and may fail to find the product of highest util-ity, although the average shortfalls in utility (column (D) ) are small. But Optimising users do not gain substantially over Prioritising users. The latter prefer Op1 moves, and this could lead to suboptimal behaviour. But in practise we are not seeing much of this on these datasets. It turns out (observing detailed simulation logs) that rarely are Op2 and Op3 moves tried (either because they are not satisfiable or because they worsen utility). This helps explain why, at least for these hotel databases, prioritising users and i n-complete strategies fare no worse than optimising users and complete strategies.

We might conclude that the Incomplete on Maxima strat-egy strikes the best balance: interactions are not much long er than interactions with the corresponding Complete strateg y, and the amount of advice it gives is much lower.
We have described information recommendation, a process by which an advisor observes the actions of the user of, for example, a conversational product recommender system; builds a user model, in this case, by inferring constraints on the user X  X  utility function; and then gives advice about actions it thinks the user is likely to try. We have given a detailed formulation of this in the case where products and queries are described by sets of Boolean features.
Our experimental results show that information recom-mendation can dramatically reduce interaction length and that the length of the advice given is much shorter when the advice is constrained by the user model than when it is not. For our hotel data, an incomplete advice strategy strikes a good balance between amount of advice and length of interaction.
 Efficiency is an issue that requires further investigation. The cost of finding which successor queries are unsatisfiable is polynomial. But the major computational cost in our approach is, given a query q , deciding whether, according to the user model that we have built so far, each successor query q 0 has larger potential utility than each other successor in deciding this, but we plan to compare with alternative techniques such as constraint satisfaction.

There remain many other avenues to explore, including: inferring richer user models and further considering how to handle the case where users are less rational than the advi-sor assumes; allowing user models to be carried over from one dialogue to another, or from one user to another, with the risk that they incorrectly characterise the user X  X  pref er-ences; and testing the ideas with real users. We also need to extend the approach to product databases where features are multi-valued. Formally, this can be done by considering each feature-value to be a new Boolean feature, and then applying the methods we have described in this paper. In some domains, this might even be a practicable approach; in other domains, it will result in too many Boolean fea-tures and unreasonably long advice. For these domains, we may need to develop a more concise formalism, and ways of generalising the advice to make it shorter. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] S. S. Anand and B. Mobasher. Intelligent techniques [3] D. Bridge, M. G  X  oker, L. McGinty, and B. Smyth. [4] G. Fisher. User modeling in human-computer [5] D. McSherry. Retrieval failure and recovery in [6] N. Mirzadeh, F. Ricci, and M. Bansal. Supporting [7] P. Pu, P. Viappiani, and B. Faltings. Increasing user [8] J. Reilly, K. McCarthy, L. McGinty, and B. Smyth. [9] F. Ricci, D. Cavada, N. Mirzadeh, and A. Venturini. [10] S. Schmitt. simVar : A similarity-influenced question [11] C. A. Thompson, M. G  X  oker, and P. Langley. A Table 2: Results: averages for 50 dialogues between different users (Opt = Optimising; Pri = Prioritis-ing; Rnd = Random) and different advisors (Null = no advisor; CoM = Complete on Maxima; IoM = Incomplete on Maxima; CoAN = Complete on All Next; IoAN = Incomplete on All Next)
