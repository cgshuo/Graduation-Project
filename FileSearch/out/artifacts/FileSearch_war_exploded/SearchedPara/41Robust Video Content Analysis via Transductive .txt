 RALPH EWERTH, MARKUS M  X  UHLING, and BERND FREISLEBEN, In recent years, several technological innova tions have fostered an enormous increase of multimedia data, including increased ha rd disk capacities, processor power, and network bandwidth; the improvement of audio, image, and video compression tech-nologies; and digital photo and video cameras. In addition, the proliferation of video content and podcasts in the World Wide Web (e.g., www.youtube.com ) has increased the need for efficient Web video search and retrieval facilities.

A prerequisite for effective video search is to analyze and index video content auto-matically and reliably. A large number of methods has been developed to support video content analysis and retrieval: shot boundary detection (e.g., Yuan et al. [2005]), scene segmentation (e.g., Truong et al. [2003]), camera motion estimation (e.g., Ewerth et al. [2004]), text detection (e.g., Jung et al. [2004]) and segmentation (Video OCR, e.g., Lienhart [2003]), face detection (e.g., Yang et al. [2002]), face recognition (e.g., Zhao et al. [2003]), person and cast recognition (e.g., Everingham and Zisserman [2004]), and semantic video annotation (e.g., Naphade and Smith [2004]) with respect to a large number of predefined semantic concepts (between 20 and 374). Unfortunately, due to the difficulty of the task at hand, even the best video content analysis approaches are not perfect, and the higher the challenge of human-like scene understanding is, the more dissatisfactory the approaches are. Many video content analysis approaches are considered to be robust by their inventors, but in most cases, this means that an al-gorithm or system has proven to work well for one or a few (hopefully large) test sets. Recent results [Kraaij et al. 2007; M  X  uhling et al. 2007] at the TRECVID [Smeaton and Over 2005] evaluation workshop show that the generalizability of concept detection approaches is rather limited. This finding is also supported by the study of Yang and Hauptmann [2008] who indicate that support vector machine (SVM) models [Burges 1998] do not generalize well for some concepts.

This finding is not surprising. In the vast majority of video content analysis ap-proaches, a classification model or decision threshold is applied to all (test) videos in the same way. This might be a learned model that has been generated using machine learning techniques or a set of predefined parameters that have been estimated em-pirically. Obviously, this is a problem as soon as a video database does not only include videos belonging to a particular domain. Videos can vary in many ways X  X n the kind of recording device, in the recording circumsta nces, in the used compression technology, editing, genre, and, of course, in terms of content. Consequently, the question is how to build video content analysis approaches that work reliably for arbitrary videos.
Transductive learning [Vapnik 2006] is a promising approach for addressing the variability of video content, since it is not aimed at obtaining a general classification model for all possible test data items (as in inductive learning), but at obtaining an optimal classification model for the given test data only. Hence, transductive learning is possibly well suited for video content analysis, because the appearance of many objects and events is often related to a particular video, episode, or TV program. In this article, a transductive learning frame work for robust video content analysis is proposed that exploits this observation. In contrast to related transductive approaches for video concept detection, the framework is designed in a general form and not only for a single task. Also, contrary to related work, it is applied at the granularity of a sin-gle video sequence. The goal is to improve the robustness of several and partially very different video content analysis tasks. The proposed framework is based on feature selection and ensemble classification and employs an initial classification or clustering result to improve its quality for a particular video. Based on the initial model, object samples of a given video v are labeled automatically as positive and negative training samples. Then, features are selected with respect to the object X  X  appearance in this video v . This feature set is split to train additional classifiers using only samples of video v with different views on the data to form an ensemble of classifiers. This ensemble is finally used to reclassify the samples of this particular test video v .The proposed framework is applied to the following video analysis tasks: shot boundary detection, face recognition, semantic concept retrieval, and semantic indexing of computer game sequences. Experimental re sults on large test sets demonstrate the very good performance obtained by using th e proposed transductive framework. The experiments also show that simply applying transductive support vector machines is not an appropriate approach for robust video content analysis, since in particular video content analysis tasks they do not perform in a satisfactory manner.
 The article is organized as follows. Related work is discussed in Section 2. In Section 3, the novel transductive learning framework is introduced. Four applications of the framework are presented in Section 4 , including experimental results for each application. Experiments investigating the impact of several components of the frame-work are reported in Section 5. Section 6 concludes the article and outlines areas for future research. First, several state-of-the-art video indexing approaches (shot boundary detection, camera motion estimation, face detection, and face recognition) are reviewed with re-spect to their robustness. Then, some approaches in the field of video indexing that use model adaptation, semi-supervised learning, or transductive learning are presented. Finally, general feature selection and transductive learning approaches are discussed.
An analysis of all shot detection approaches evaluated at TRECVID 2005 shows that the results vary significantly in terms of recall and precision, depending on the different video sources  X  X ASA X  and  X  X ews X  [Smeaton and Over 2005]. A closer look at successful shot detection approaches reveals that even the best approaches are not designed to adapt to a particular video source. Typically, predefined thresholds or parameters are used, as exemplified by two of the best performing shot boundary de-tection approaches at TRECVID 2005 (Tahaghoghi et al. [2005], Yuan et al. [2005]).
Considering the best TRECVID 2005 result s for the task of camera motion esti-mation yields a similar picture. Yuan et al. [2005] estimate the motion parameters of a two-dimensional affine model and finally apply thresholding rules to decide the presence of motion. The authors achieved the best results for pan and tilt detection. Ewerth et al. [2004] obtained the best results for zoom detection by computing the parameters of a 3D-camera model and appl ying thresholds for decision making.
Two of the most successful face detection approaches employ machine learning and need a large training set. Schneiderman and Kanade [2004] apply the wavelet trans-form and use wavelet features to train a Naive Bayes classifier. Viola and Jones [2004] train a cascade of Adaboost classifiers and mainly focus on real-time processing of video frames. Once the learned models are obtained, they are applied to new data always in the same way.

The same is true for approaches for face recognition in movies and TV casts. Ever-ingham et al. [2006] present a system for character identification in videos that em-ploys subtitles, script processing, speaker, and clothing information. Arandjelovic and Zisserman [2005] present an approach for character retrieval that includes the sup-pression of face background, refinement of pose with respect to a given face exemplar, and a distance measure in a feature subspace to deal with occlusion and change of facial expressions. Cour et al. [2009] present an approach that learns face appearance from ambiguous labels of faces.

All of these mentioned approaches are applied to new data in the same way, that is, they do not adapt to new video material. Up to now, there are only a few approaches that propose model adaptation for video indexing and retrieval purposes, for example, by using semi-supervised or transductive learning. Wu et al. [2005] state that the appearance of concepts changes over time and address the problem of concept drifting in videos. The authors use Gaussian mixture models (GMM) to model a concept and propose an incremental online learning fra mework to cope with concept drifting. For this purpose, videos are processed in a batch mode. The first batch of prelabeled data is used to learn a global GMM for each concept. The next batch of data is used to learn a set of locally optimized GMMs for each concept from the first unlabeled portion of the new data, aiming at an optimal classification performance on the current test batch.
Recently, transductive learning has been considered in the field of video concept detection. Qi et al. [2007] present a transductive concept detection system for home videos relying on two clustering steps. The training and test samples are clustered to-gether and an EM-based tuning of clusters aims at purifying the clusters. Tian et al. [2008] propose a transductive approach for video annotation using local kernel classi-fier. To check its applicability, a measure is suggested to test whether a sample can be classified using the neighbored samples. Wang et al. [2008] define several subdomains for the domain of news videos to model and analyze videos using a transductive frame-work. Transductive inference is based on clustering and Vapnik X  X  combined bound. Finally, Wang et al. [2008] present an approach for transductive multilabel learning for video concept detection. The authors propose a discrete hidden Markov random field that also assures that multilabel interdependence over unlabeled data is coher-ent with interdependence of labeled data.

There are some proposals for other pattern recognition domains using semi-supervised or transductive approaches. For example, Lieb et al. [2005] propose a self-supervised approach for adaptive road following for driving vehicles to reduce the need that a road must be represented by unique identifying features. Seemann et al. [2007] present an approach for pedestrian detection in image sequences that is able to adapt a generative model based on implicit shape models to particular object instances. Wu and Huang [2000] suggest self-supervised learning using labeled and unlabeled train-ing data for object recognition to overcome the tedious and expensive task of label-ing large training datasets. They extend a linear discriminant-EM with a nonlinear kernel and show that their novel learning technique is competitive with SVMs and outperforms various approaches for hand-gesture recognition and fingertip tracking tasks. Oudot et al. [2004] present a self-supervised method for writer adaptation in an online-text recognition system. In the self-supervised method, lexical results are com-pared with the classification hypothesis to find errors that are then used to reestimate classifier parameters. Nigam et al. [2000] present an approach for text classification that exploits unlabeled data using a naive Bayes classifier and EM clustering. An initial naive Bayes classifier is trained using only the labeled data. Then, using this classifier, the unlabeled data are labeled probabilistically, and finally, EM is iterated until convergence on all document samples. Joachims [1999] suggests a transductive SVM for a text classification task. With the transductive SVM, the maximum margin that separates the two classes is first estimated using the labeled data. Then, the maximum margin is recomputed using the unlabeled test data.

Apart from applications to multimedia analysis and pattern recognition, semi-supervised learning and transductive learning have been researched for several years. The same is true for feature selection. The popular face detection approach of Viola and Jones [2004] uses Adaboost for feature-selection-employing decision stumps and weak classifiers, whereas Wiratunga et al. [2004] use boosted decision stumps for retrieving textual cases. Redpath and Lebart [2005] present a variant of Adaboost aimed at re-ducing the number of features for each classifier in an ensemble. Semi-supervised or transductive learning methods often rely on the semi-supervised smoothness assump-tion [Zhou et al. 2004], cluster assumption, or manifold assumption [Chapelle et al. 2006]. Typically, semi-supervised approaches can be divided into generative models (e.g., Nigam et al. [2000]), low-density separation methods, and graph-based methods. An example for a low-density separation approach is Joachims X  X  transductive SVM [Joachims 1999], while Bengio et al. X  X  approach 2006 using label propagation is an example of a graph-based approach. Zhou and Burges [2007] present a graph-based approach for transductive inference that exploits multiple views on the feature set, where each view (i.e., feature set) is represented by a separate graph. In this section, a transductive learning f ramework for addressing the issue of robust video content analysis is presented. As, mentioned, transductive learning is not aimed at obtaining a general classification model for all possible test data items (as in induc-tive learning), but at obtaining an optimal classification model for the given test data only. One of the contributions of this article is to consider the analysis process for a particular video as a transductive learning setting: the unlabeled data of a particular previously unseen video are incorporated into the learning and classification process. An initial classification or clustering result of a baseline system is exploited to improve its quality for a particular video. The proposed framework is based on feature selection and ensemble classification.

In the sequel, the framework that is used in the proposed approaches for cut detec-tion, face recognition, semantic video retrieval, and semantic video indexing is intro-duced. The framework is presented in a general form and its adaptation to a video content analysis task is described as part of the corresponding approach in the related sections.

The goal of the proposed framework is to obtain a robust indexing or retrieval result for a given video based on an initial result. The initial result can be obtained for differ-ent granularities: frames, sequences of frames, shots, scenes, etc. Subsequently, these units are considered as samples, and each sample is represented by a set of features. For example, in the case of cut detection, the (dis-)similarity of consecutive frames is commonly used as a feature. First, the feature vectors of all samples are used to obtain the initial clustering or (inductive) classification result for the given video. To adapt an object or event model to the object X  X  or event X  X  appearance in a particular test video v , the baseline model is used either to classify the frames or shots or to obtain prob-ability scores for them. This result serves as the source for automatically generating the training set for the subsequent learning and model adaptation process. This train-ing set consists only of samples of the test video v under consideration. If the initial result is a binary classification result, all samples are used for the subsequent training process. Otherwise, if probability or confidence scores are available, they can be used to select the top (or bottom) p percent of the best (or worst) samples as positive and negative training data. The automatically generated training data are used to select the best features via Adaboost Freund and Schapire [1997] for the classification task of this video. Then, the set of selected features is split into two disjoint sets in order to enable the training of different classifier views for video v . The feature sets are used to train new classifiers directly on video v . Finally, the newly trained classifiers and the initial classifier form an ensemble, and the video samples (i.e., the feature vectors) are reclassified using this ensemble and majority voting. The initial classifier is incorporated to prevent performance degradation for inappropriate cases. The main components of the framework are shown in Figure 1. Feature selection is aimed at finding the features that characterize an object, event, or concept in a particular video. Once the positive and negative training samples have been obtained automatically for a test video v , feature selection is conducted using a slightly modified version of the Adaboost procedure. It has been shown by Freund and Schapire [1997] that Adaboost minimizes the error on the training data as the number of training rounds (and hence the number of classification models) increases, as long as each selected classification model achieves an error rate below 0.5, that is, it is a weak classifier. In our framework, a weak classifier is built for each feature, and a feature is evaluated based on the classification error in each training round. A classification error is estimated for each weak classifier by estimating the best thresh-old that separates positive and negative samples using the one-dimensional data of a single feature. This classification error depends on the current sample weights that normally change from round to round. The training samples are weighted equally in the beginning. Training samples that are misclassified by the currently chosen weak classifier are reweighted such that they ha ve more impact in the next training round for the next weak classifier. Thus, a newly selected feature has a higher probability of correctly classifying the training samples that have been misclassified in preceding rounds. A modified version of Adaboost (see Algorithm 1) is used to perform feature selection: in round k , the feature whose related weak classifier yields the lowest error on the current weighting of the training data is selected as the k th feature. We have modified Adaboost such that a feature cannot be selected more than once (for a weak classifier) during the Adaboost training rounds (lines 8 and 9 in Algorithm 1). In clas-sical Adaboost training in conjunction with weak classifiers, a feature can be selected several times to build (different) weak classifiers.
 After the best features have been selected, they are used to train new classifiers on the automatically labeled training samples of the test video v under consideration. In our system, an SVM is used since it has proven to work well for many pattern recognition and video indexing tasks. However, training a new classifier only on the automatically labeled samples possibly degrades the initial result. In particular, it is not guaranteed that the samples are labeled correctly, and the quality of the new classifier depends on the accuracy of the initial result. On the other hand, the initial result might be a good one and should not be discarded. To address these issues, an ensemble approach is utilized in our framework. Besides Adaboost, another possibility for building an ensemble of classifiers is majority voting. In this case, the classifiers that form an ensemble should have a certain level of independence. There is some evidence [Kuncheva et al. 2003] that a reasonable degree of independence of ensemble classifiers improves accuracy.

In our framework, the boosting procedure of Adaboost is exploited to generate two disjoint feature sets that have a certain degree of independence. Features are assigned alternatingly to two different feature set s according to their odd or even ranks in the feature selection process. Subsequently, two new classifiers are trained using only one of the feature sets. This approach is motivated by the fact that during the Adaboost process, the weights of the samples that have been misclassified by the preceding weak classifier are increased. Thus, the next selected classifier should be partially indepen-dent of its direct predecessor. In case of two classifiers, the feature with rank k is assigned to classifier k modulo 2. In subsequent SVM training, models are learned based only on the automatically labeled training examples taken from the current test video v , yielding two new SVM models. Finally, at least three SVM models are avail-able for each video X  X he initial (inductive) model and two (transductive) models. The latter models were learned with the training data that were labeled automatically for this particular test video v using the initial model.

Apart from this ensemble solution, we have tested other possibilities for reclassi-fying a video. For example, Adaboost itself can be used for re-classifying the video data. In this case, a classification weight is computed for each selected feature (i.e., its weak classifier) based on the classification error in the corresponding training round. Of course, it is also possible to train only one new SVM model based on the selected features and to use it for reclassification. In this section, applications of the proposed framework for four different video indexing tasks are presented: video cut detection, face recognition/clustering in videos, semantic video retrieval of news videos, and semantic indexing of video games. The concrete realization of the framework is described in the subsequent sections for each task. The used features differ in each application. While the feature selection process reduces the number of features, it is also possible to generate good initial results for certain tasks with a very small feature set but to perform feature selection on an enlarged set of features. For example, this is the case for video cut detection where few features are sufficient to obtain a good initial result. The fraction of selected training samples varies from task to task.

In most applications, an RBF (radial basis function) kernel is used for learning new video-specific SVM models. The following ranges are checked via cross-validation for the parameters C and gamma: C ranges from 2  X  4 to 2 8 , and gamma ranges from 2  X  16 to 2 3 , both with stepsize 1 in the exponent. Regarding the experiments for semantic video retrieval, we have also used a  X  2 -kernel for the baseline approach that relies on densely sampled SIFT (scale-invariant feature transform [Lowe 2004]) features and a codebook of 1,000 words. In the experiments using a transductive SVM, an RBF kernel is used with default settings for C and gamma. Finally, it should be noted that basically any classifier could be used in our framework in the initial stage, as well as in the final stage. A video sequence typically consists of a large number of shots that have been put together during a production process either to tell a story or to communicate some kind of information (e.g., news or documentary). A shot itself consists of a number of frames that were continuously recorded by a camera device without any interruption. There are two kinds of shot boundaries in videos: abrupt shot changes (called cuts) and gradual transitions between two different shots.

An unsupervised clustering approach [Ewerth and Freisleben 2004] is used as the baseline system to initially classify frames as cuts or noncuts. As just described, the proposed approach uses the initial result for a given video v using an initial set A of features. In this way, training data are generated automatically from the video itself. Only two features are used in this approach: motion compensated pixel differences of DC frames and the ratio of the second largest dissimilarity value divided by the local maximum within a temporal sliding window. Then, this initial result (including clas-sification errors) is used to select the best features for this video from a larger set B of features. Forty-two features have been defined for the feature set B for a certain frame distance describing frame dissimilarity by means of (1) motion compensated pixel differences, (2) histogram differences, (3) luminance mean and variance, (4) edge histograms of Sobel-filtered (vertically and horizontally) DC frames, (5) local his-togram differences, and (6) ratio of the second largest dissimilarity value divided by the local maximum for several sliding window sizes. Two frame distances (1 and 2) are investigated, yielding a total number of 84 features. Adaboost is applied to obtain a ranking of the best features for the particular test video v .

According to the ranking order, these sele cted features are split and passed to the transductive learning stage to train two different SVMs directly on the video v using the different feature sets. Together with the unsupervised system, they form an en-semble of three classifiers: a cut is detected if at least two of them vote that a frame is acut.

Experimental Results. The MDC library 1 was used for MPEG decoding and the libSVM library [Chang and Lin 2001] for SVM implementation. The proposed transductive learning framework has been tested on the TRECVID 2005 shot boundary test set [Smeaton and Over 2005]. There are 3,372 abrupt transitions in the video test set. The experiments were evaluated using measures recall, precision, and f1. They are computed as follows. where N is the total number of cuts in the test set, C is the number of cuts that were correctly detected by the system, and D is the number of all detected cuts by the system including false alarms.

The experimental setup investigates whether it is useful to form an ensemble of classifiers that is built adaptively for a given video. The unsupervised baseline system is extended with the proposed framework that uses feature selection, splitting the se-lected feature set to train two new SVMs. The feature set is split alternatingly depend-ing on each feature X  X  rank during the Adaboost selection process. In Table I, the results of the proposed approach are presented using 45 selected features. The results in Table I are also compared with a state-of-the-art approach of Bescos [2004] for which we have replaced a simple classifier with an SVM classifier. The results demonstrate that the transductive ensemble improves the detection performance of the original approach. The recognition of persons in videos and movies is important for understanding the video content, since in most cases, the plot is related to the persons or actors. The following question is sought to be answered.  X  X n which shot Y does person X appear? X  While frontal face detection in images and videos and face recognition in constrained environments has reached a certain level of maturity, the recognition of persons in videos without any constraints and a priori knowledge (e.g., training of statistical models) remains a challenging and yet unsolved problem, due to the low quality and resolution of video recordings and the high degree of freedom of face appearances in terms of illumination and pose. If we want to recognize (i.e., cluster) the persons in a particular video, there is no need to build a classifier that distinguishes a face from all other possible faces in the world X  X t is only required that a classifier distinguishes one person from the other persons who appear in the same video. Hence, the task is well suited for a transductive learning setting.

To use the transductive learning framework for the problem of person recognition in videos, it is proposed to adaptively learn face appearances in a video by estimating relevant features for a person X  X  face. For this purpose, an initial clustering result is ob-tained via our automatic video person indexing system [Ewerth et al. 2007], consisting of state-of-the-art building blocks for face detection, tracking, and recognition. First, faces are detected Viola and Jones [2004] in frames and tracked [Lucas and Kanade 1981] across several frames in order to obtain reliable face sequences. Then, the recog-nition process is based on elastic bunch graph matching, where Gabor wavelet features with different frequencies and orientations are extracted from several face positions. Finally, an agglomerative clustering process is applied to group similar faces. This clustering process outputs a number of clustered face sequences as its result. How-ever, in this clustering process, all the wavelet features have contributed equally to the similarity measure of two faces. In this way, the characteristic features of a person (e.g., beard, eyes) might be impaired by many other features that are not suitable for distinguishing between two persons. Furthermore, it is observable that the clustering process sometimes results in a number of clusters with only one face sequence, often related to a less frequent pose or facial expression of a person. Here, the idea is to terminate the clustering process early by using an adequate low threshold to prevent the merging of face sequences of different persons.

At this point, the idea of transductive learning is applied. Each cluster that has a minimum number of face sequences is further considered as a training set for a person. The members of this cluster are the positive training samples for this person, whereas the face sequences of the other clusters are considered as negative training samples. Those clusters should also exceed a minimum number of face sequences to reduce the number of false (negative) training samples. At this stage, the training sets may contain some face samples that have the wrong labels. The idea is that as long as the correctly labeled samples dominate the training set for a person, the classifier should generate a generalized model that mainly represents the good samples of the training data. In the training stage, not only the amplitudes of the Gabor wavelet features are used, but also the phase information. Adaboost is utilized to select the features that best distinguish between a considered cluster X and all the other clusters. These features are then used to train a classifier that is expected to work well only for this video. Several possibilities have been investigated. First, since Adaboost is a metaclassifier itself, it can be used for subsequent classification, too. Second, the selected features can be used to train any other classifier: in our experiments, an SVM is used. Finally, another possibility is to train an ensemble of classifiers according to the proposed framework.

Experimental Results. Experimental results are presented for a TV news video (about four minutes duration, four persons), for a talk show sequence (about eight minutes duration, six persons), and for a TV soap video (about 38 minutes duration, 4 main characters). Some system components are realized using Intel X  X  computer vision library OpenCV 2 ]. It is used for face detection, feature detection and optical flow estimation (feature tracking); the libSVM library [Chang and Lin 2001] is used for the SVM implementation; and the Elastic Bunch Graph Matching code. 3 is used for face recognition and comparison . In all reported experiment s, single linkage clustering [Ester and Sander 2000] was applied.

The experiments were evaluated as follows. The ground truth data were created per shot and per frontal face appearance. Given a clustering result, a particular cluster should contain all the shot indices in which a certain person appears. A cluster C is assumed to represent the person who dominates the cluster. If two or more clusters are dominated by the same person, only the cluster C x that contains the maximum number of face sequences for this person x is considered for the calculation of recall and precision. Let c x be the number of shot indices that are correctly assigned to cluster C x for a person x ,andlet T x be the total number of appearances of this person. Then recall and precision for person x are defined as. The overall recall and p recision are defined as where | C x | = c x + f x ; f x is the number of falsely assigned face sequences. The F1-measure is computed as just described using RECALL and PRECISION. If a person dominates more than one cluster, only the cluster with the highest number of detected face sequences (of this person) is taken into account for recall and precision. Recogni-tion rate is measured on the basis of the detected face sequences.

The impact of the transductive learning approach using either a single classifier or an ensemble is investigated in our experiments as described next. For the news test video (duration of three minutes), the baseline system achieves a perfect recogni-tion result without transductive learning. Results for the more demanding talk show sequence using different systems are presented in Table II.

For the baseline system that uses an additional correction step of in-plane rotation of faces, recall for the clusters is between 100% and 60%, while precision is at 94.0% in total. Three different strategies for trans ductive learning were investigated for the talk show sequence. First, Adaboost was used for both feature selection and reclassi-fication of the face sequences. The number of selected features was set to 30. In two further experiments, a single SVM or an ensemble of SVMs, respectively, was used for reclassification after the feature selection process (realized via Adaboost). Overall, there is no significant difference between the three different approaches, but each of the transductive approaches improved the results noticeably compared to the base-line system. Furthermore, it was tested how many features per face are necessary to achieve good clustering/classification results using the transductive learning approach with a single SVM. The results are displa yed in Table III. Interestingly, already a small number of five features is sufficient to achieve a very good result.

Finally, we have investigated the potential of self-supervised learning for the video camiloefilho (duration 38 minutes, four characters) from the MPEG-7 [MPEG-7: ISO/IEC 15938] video test set. Only clusters with a minimum number of ten face sequences were used to train a face model. U sing the transductive learning framework via an Adaboost classifier with 45 features for each person, the overall recognition rate increased from 71.1% up to 81.7%, while a high precision could be preserved (89.1% after learning, 90.7% before). The same features were used to train an SVM for each person: here, a similar overall recognition rate of 81.2% and a precision of 88.5% was achieved. The detection of semantic concepts in video content is an important prerequisite for video search. It can be observed for some concepts that their visual appearance is strongly related to a particular video sou rce. For example, the appearance of anchor shots, maps or weather forecast shots in news videos are typically related to a TV broadcaster or program. For example, the spatial composition of the shot will be spe-cific for a TV cast, such as the moderator X  X  position, the camera distance in an anchor shot, or the used colors for displaying a map. Examples for concepts whose appearance depends on the program channel are presented for the concepts  X  X nchor X  and  X  X aps X  in Figures 2 and 3, which show keyframes from TRECVID X  X  2005 development data set [Wilkins et al. 2007] of the high-level feature detection task [Smeaton et al. 2009]. 4.3.1. Semantic Video Retrieval Using the Transductive Learning Framework. As previously discussed, a common approach to detecting semantic concepts in videos is based on a mapping between low-level features and high-level features. To improve the re-trieval performance for semantic concept s, the proposed framework is applied to the task of concept detection to learn the specific appearance of a concept in a particular unlabeled test video v based on an initial model. Its main processing steps are as fol-lows. First, a baseline system using supervised learning (e.g., the SVM data of the MediaMill Challenge system [Snoek et al. 2006] is used to map low-level features to high-level concepts. Second, the learned model is used to label the shots automatically with SVM confidence scores. In this way, a first ranking of the shots is obtained sep-arately for each test video v . Third, the best features are selected for this test video v and optionally split into two disjoint feature sets. These feature sets are used to train new SVM classifiers using only the automatically labeled data from the current test video v under consideration. Then, the additional classifier(s) and the original classi-fier form an ensemble that is used to recompute a total confidence score for each shot in the test video v . Finally, the confidence scores for all shots of all test videos are used to rank them, and shots are returned to the user according to this ranking.
Experimental Results. In our experiments, the usefulness of the proposed learning framework has been investigated for several video concepts that were expected to have a specific appearance in a certain video or TV program, respectively. The TRECVID 2005 training set consisting of 137 videos and approximately 45,000 shots were used for this purpose. Two different state-of-the-art systems serve as baseline systems. First, the MediaMill challenge system [Snoek et al. 2006] consisting of features, SVM models, and related confidence scores is used as a baseline system. However, since the publication of the MediaMill Challenge system in 2006, some advances in the field of feature extraction enabled a considerable improvement for semantic video analy-sis. Related comparison studies [van de Sande et al. 2008; Yang et al. 2007] show that codebook-based approaches (bag of words) of densely sampled SIFT features cur-rently achieve the best performance. To investigate whether the proposed framework is also beneficial for such state-of-the-art systems relying on this kind of features, we have implemented a second baseline system using SIFT features that are densely sam-pled across an image, and a codebook is constructed using 1,000 codewords. Codebook construction is realized by clustering the SIFT descriptors using k-means. After clus-tering, the cluster centers are considered as representative codewords. This system is called DSIFT baseline from now on.
 Features are extracted from the TRECVID 2005 training video set, and (inductive) SVM models are learned using 70% of this training set, whereas the proposed system is tested on the remaining 30%. For the proposed transductive framework, the mini-mum probability of a shot serving as a positive training sample was set to 0.01, and at least one shot in a test video v must have fulfilled this condition. Otherwise, transduc-tive learning is not conducted for this test video v . The libSVM [Chang and Lin 2001] is used in our implementation to learn SVM models and to obtain confidence scores. Average precision is used to measure the retrieval performance. where A is the result set of returned documents, L k = { l 1 , ..., l k } is the subset of the k responses that are the most similar responses in A with respect to a confidence score, R is the set of relevant documents, and  X  is a function which evaluates to 1 for l k  X  R and to 0 for l k /  X  R .

The following concepts were expected to ha ve an appearance that is related to a spe-cific video: anchor, charts, entertainment, maps, and studio . Several experiments were conducted for these concepts. In the first experiment, the performance of the proposed transductive learning framework was compared with the MediaMill baseline system. In this scenario, 90 features were selected, and this feature set was split to train two additional SVMs for each video. For this experiment, the results are presented in Table IV in terms of average precision. Average precision is calculated for the whole set of retrieved shots (i.e., 12,913 shots) in order to be consistent with the reported MediaMill Challenge baseline results [Snoek et al. 2006]. The proposed transductive learning scheme improves the average precision by about 3  X  8% for three of those five concepts, except for the concepts charts and entertainment .

In the second experiment, the DSIFT baseline system was investigated. A  X  2 -kernel was used, since we have observed that it achieves better performance for this codebook-based approach than an RBF kernel. The mean average precision of this baseline is noticeably higher (70.4%) than that of the Mediamill Challenge system (44.4%) for the five concepts under consideration. We have tested two variations. First, the DSIFT features were used for learning the initial inductive SVM models and also in the trans-ductive learning process. In the second va riation, the DSIFT features were used for the inductive baseline model, but the challenge features were used in the transductive learning process. It is an advantage of the proposed framework that the features in the second stage can differ from the initial training. When the DSIFT features were used during the video-specific training round, 100 features were selected (out of 1,000); when the Challenge features were used, 90 features were selected (out of 120). In this experiment, one additional SVM was trained. The results are presented in Table V. When using the selected DSIFT features in the video-specific training, there is no im-provement over the baseline system. However, when using the Challenge features, there is an improvement for the five concept s using the transductive learning frame-work, particularly for entertainment. The results for the oracle demonstrate the po-tential of the transductive learning framework, since the DSIFT baseline is improved by more than 3%; the oracle chooses the best parameter setting for each concept (by means of selected features, minimimum probability for positive training samples, etc.). 4.3.2. Semantic Video Retrieval Using the Transductive SVM. In this section, a concept de-tection system for semantic video retrieval using a transductive SVM (TSVM) proposed by Joachims [1999] is compared to our framework. A transductive SVM uses the unla-beled test samples and tries to achieve a model that is optimal for the particular test set. Therefore, the test samples are incorporated into the learning process (the process of estimating the maximum margin hyperplane).

For each video v in the test set, a concept model c is adapted with respect to its appearance in a video v via transductive support vector machines. As a result, a class model is computed for each test video v separately and, based on this model, prediction values are computed for each shot of this video that indicate whether the concept is present in a shot or not. In this way, prediction values are obtained based on the cor-responding transductive model for all shots of a particular video in the test set. These values indicate whether the concept c is present in them or not. Finally, the predic-tions for all test video shots in the database are used to rank them according to the probability that the concept c is present in them.

Experimental Results. The performance of the transductive SVM was compared with the results of the proposed transductive lea rning framework for the five selected video concepts. The Mediamill Challenge features and training and test data were used as described in the previous section. To compute the transductive SVM models, the SVM implementation svmLight 4 was used, whereas the (inductive) SVM models in the Challenge system are computed using libSVM [Chang and Lin 2001]. To avoid different performance measurements being caused only by different implementations or parameter settings of the (inductive) SVMs, the inductive SVM of svmLight was also used to compute the prediction values and performance of a baseline system for TSVM. Thus, it is observable how much of a performance improvement is caused by the transductive extension of the SVM or if it is only due to a particular SVM im-plementation. We have observed that the performance of transductive SVM degrades noticeably when the percentage of positive samples in the training set is below 2%. The performance degradation could be avoided if the parameter for the number of samples that are labeled by the inductive baseline SVM as positive is set to 0. Then, no falsely assigned positive labels deteriorated the computation of the hyperplane, while the un-labeled negative samples still influenced the maximum margin hyperplane. In this way, the best results could be achieved for the concepts charts and maps .
 For this experiment, the results for the five selected concepts are presented in Table VI in terms of average precision for the whole set of retrieved documents. The transductive SVM improves the average precision for four of the five concepts ( anchor , charts , entertainment ,and studio ). The average precision for these concepts is improved noticeably, between 3.7% and 5.7% (and by 3.5% on the average for all five concepts). Transductive SVM learning achieves a similar improvement as our framework. Most of the performance differences of both systems are only due to the different baseline SVM implementations (libSVM vs. svmLight). It can be concluded that the transductive SVM is suited for the detection of video-specific concepts, too. Computer games play an important role in today X  X  entertainment media and belong to the most popular entertainment products. There is an extensive ongoing debate about the question of whether playing violent games causes aggressive cognitions, aggressive affects, or aggressive behavior, in particular with respect to teens and young adults.
Mathiak and Weber [2006] developed neurophysiologically grounded measures for the  X  X uman experience of media enjoyment X . Their study investigates the brain activ-ity of young adults while playing a video game with violent content. The experimental design presented by Weber et al. [2006] is based on the definition of certain game states and captures a player X  X  brain activity while he (only male players were investigated) is playing a violent computer game. Functional magnetic resonance imaging (fMRI) scans are taken during video game playing to capture brain activity. The  X  X ature X  rated first-person ego-shooter game  X  X actical Ops: Assault on Terror X  5 has been used in the experiment. Several semantic game events are distinguished, such as preparation, search and explore, or fighting and killing. Once the game recordings are annotated with these semantic categories, the relationship between violent game events and the underlying neurophysiological basis (brain activity) of the player can be investigated. Normally, human annotators are required to index such game content according to the current game state, but this is a very time-consuming task.
We have developed an automatic semantic video analysis system based on our learn-ing framework and the transductive SVM, respectively, that supports the experimental design described above by automatically iden tifying four different game categories: (1) inactive, (2) preparation, (3) search/explore, and (4) violence/attack (see Figure 4).
Since the appearance of the categories depends on the corresponding game levels, this video analysis task is also well suited for a transductive learning setting. Con-tent analysis relies on audiovisual low-level features and on mid-level features. The considered mid-level features are the results of shot boundary detection [Ewerth and Freisleben 2004], camera motion estimation Ewerth et al. [2004], audio segmentation, text detection [Gllavata et al. 2004], and face detection Viola and Jones [2004]. The low-level and mid-level features are extracted from each video frame and used to learn an initial SVM model for each game category. Only a single video sequence with a duration of 12 minutes is required to provide training data, and hence, the human annotation effort is quite low. Then, the test video is labeled automatically according to these models, and a fraction of these labeled data is subsequently used to find the best features for this video for each game category. Then, the set of selected features is split into two sets according to odd and even ranks in the selection process, and two new SVM classifiers are learned on the test video data. Finally, the three SVMs (initial model + two new models) are used to reclassify the video frames.

Experimental Results. Several experiments were made to test the system X  X  applicabil-ity for the psychological study. Four computer game videos were used to evaluate the system performance. All computer game videos have a resolution of 352 x 288 pixels and a video frame rate of 25 frames per second. The ground truth data were created by Weber et al. [2006].

A  X  X eave k-1 videos out X  cross validation scheme was used. Since the main goal was to reduce the human annotation effort, only a single video was used as training data in each test, while the remaining three videos were used as test videos. The SVM has been implemented using the WEKA library [Witten and Frank 2005]; for the transductive SVM, we used the implementation provided by Joachims [1999]. The following system variants were tested. (1) The baseline system: All features are used to learn a SVM model for each semantic game class. (2) The transductive approach based on the learning framework: after an initial classification of a test video, 50% of the frames that are classified with highest (lowest) confidence are used as positive (negative) training data. These training data are used to learn a new SVM model, and finally the same video is classified using these models. (3) In addition, the ensemble variant of the transductive learning framework was tested, and (4) the transductive SVM based on Joachims X  implementation was used. For the transductive SVM, a poly-nomial kernel with an exponent d =2wasused.

The results for these experiments are presented in Table VII. The measure  X  X otal accuracy X  is defined as the percentage of co rrectly classified frames with respect to all semantic game classes. Several observations can be made. The automatic baseline system achieves a frame-based total accuracy of 87.5%. This is a very good result if one considers that the intercoder reliability in the original psychological experimental setting between the human annotators was 0.85, as reported by Weber et al. [2006]. In terms of total accuracy, the transductive framework outperformed the baseline approach. It improved the results for the more frequent game categories (inactive, search, preparation), but not for the game ca tegory violence. The transductive ensem-ble approach was slightly better than the transductive approach that used only one newly trained SVM for classification. In contrast, the transductive SVM approach yielded a significant degradation of annotation quality, in contrast to the results for concept detection described in the previous section. Overall, the transductive learning framework outperforms the TSVM noticeably. In this section, several experiments are presented that investigate the impact of the following components and aspects of th e transductive learning framework. (1) How many positive training samples should be selected for a given task, and how (2) How does feature selection affect the performance of the framework, and how many (3) How does splitting the feature set and training a different classifier on each split (4) What is the runtime behavior of the framework?
To investigate these questions, we have conducted experiments using different numbers of positive training samples, different numbers of features, and enabled or disabled feature splitting, respectively. The experimental results for video cut detec-tion are presented in Tables VIII and IX, for face recognition in Tables X and XI, for semantic video retrieval in Tables XII and XIII, and for semantic computer game anno-tation in Tables XIV and XV. The results are discussed in the corresponding sections. The framework exploits an initial clustering or classification result for selecting posi-tive training samples from the test data of the video to analyze. Video content analysis tasks are quite diverse, and this is also reflected by our selected applications. Our cut detection approach relies on k-means cluste ring, while face sequences are grouped us-ing agglomerative clustering. Semantic video retrieval is based on supervised learning and SVM, while the annotation of computer games is a multiclass problem solved also by SVM. For specific concepts in the retrieval task, there are only very few or even no positive training samples in a particular video. For cut detection, the precision of de-tected cuts is approximately 90% or above, whereas for some concepts in the retrieval task, it is only slightly more than 15%. Hence, there is no single good strategy for all tasks. Moreover, the algorithm designer has to take the characteristics of a task into consideration. We have conducted several experiments for all four tasks varying the number of positive training samples to gain insights and some recommendations for sample selection. For video cut detection, it turns out that choosing the 90% of ini-tially detected cuts with the highest confidence scores yields finally the best results for the transductive framework (Table VIII and Table IX). This is intuitive, assuming a precision of 90% in the initial result. Probably many of the false positives are filtered out this way. Using much less than 90% leads to a significant decrease of recall. This indicates that it is apparently not possible to learn a model that generalizes well to the other cuts in a video from only 50% or 10% of the initially detected cuts.
 This behavior is similar for the other content analysis tasks (Tables XI, XIV, and XV) except for semantic video retrieval. Leaving aside a small percentage of posi-tive training samples seems to be an effective way to filter out false positives and to improve the quality of the training set that is automatically generated from the test video to be analyzed. In semantic video retrieval, even the precision of a state-of-the-art baseline system is quite low. In this case, the experimental results in Tables XII and XIII suggest that it is beneficial to also accept samples with rather low probability scores as positive training samples. In general, a positive effect of selecting only part of the positive data is that the training time of the transductive model is reduced. In the proposed framework, the purpose of f eature selection is to find the character-istic features of an object (e.g., faces), event (e.g., cuts), or semantic categories in a given test video or computer game. Another important goal of feature selection is the reduction of training time of the transductive model(s) for a given test video. The ex-perimental results in Tables VIII X  X  and in T ables XII X  X V for the four tasks show that feature selection is important and effective in achieving these goals. For video cut de-tection, best results are obtained using 21 sel ected features; face recognition in a video is successful even if only five or six features are selected from an original feature set consisting of 1,800 features. The same is also true for semantic video retrieval and computer game annotation: even selecting only as few as 12 features yields good re-sults that are comparable to the best results. Thus, we conclude that feature selection is an important part of the framework that i ndeed selects features that are relevant in the particular video and analysis task. The goal of splitting the features in two disjoint sets in the transductive ensemble is to achieve some independence between the ensemble classifiers. The video cut detection results in Table IX show that the ensemble using the feature set split is consistently better for all combinations using at least 11 features and at least 90% of the positive training samples. In addition, the best result (f1 = 91.6) for cut detection is achieved using the split of the feature set. For face recognition and semantic video retrieval, the presented results indicate that there are no significant differences between the systems using the split or not, respectively. However, splitting the feature set for semantic computer game analysis yields results similar to the cut detection results. When at least 12 features and at least 90% of the positive training samples are used for training, the accuracy is improved modera tely. To summarize, splitting the feature set never degrades performance for the investigated analysis tasks but moderately improves the quality of content analysis in two tasks. In this section, we discuss the runtime behavior of the proposed framework. For this purpose, we exemplarily measured runtimes for the face recognition system that has been implemented in C++ and Java for an eight-minute test video. On an Intel Pentium 3GHz with 4GB RAM under Windows XP, the generation of face sequences (face detec-tion and tracking) took approximately 52 minutes, while in-plane rotation of faces and extraction of Gabor wavelet feature took 4:43 minutes, and agglomerative clustering of face sequences took 51 seconds. Overall, the processing time without the learning framework is 6,573 seconds (ca. 110 minutes). Learning the models using feature selec-tion and SVM differs depending on the number of selected features. We have measured the runtime of training models for eight persons using one additional SVM selection and preceding Adaboost feature selection of either six, 30, or 60 features. The results are presented in Table XVI. When using six features, the processing time of the trans-ductive learning framework is less than 10% of the baseline system X  X  processing time; in the case of 60 features, the processing time is increased up to 46% (of the baseline processing time). This demonstrates the feasibility of the approach and emphasizes once again the importance of feature selection in the framework. In this article, we have proposed a transductive learning framework for robust video content analysis. It is aimed at recognizing the specific appearances of objects or events in a single video or episode. The framework is based on the assumption that an appropriate baseline system with sufficient classification accuracy exists. The idea is to employ an initial clustering or classification result to obtain automatically labeled training data for this video and to train additional transductive classifiers on and for this video using these training data. The best features to classify the objects of interest are selected using Adaboost based only on the automatically labeled training data. To obtain a robust final classification result, the newly created transductive classifiers and the initial classifier form an ensemble using majority voting. The newly trained classifiers are trained on different feature sets: the set of best features is split according to the odd and even ranks in t he selection process. The transductive framework has been successfully applied to the following tasks. (1) Video cut detection. (2) Face recognition in video. (3) Semantic video retrieval. (4) Semantic annotation of computer game sequences.

The investigated tasks are quite diverse. The feature sets vary with respect to size and used feature types. The baseline performance ranges from 44.4% average preci-sion for semantic video retrieval up to an f1-measure of 87.8 for cut detection. In the case of semantic video retrieval, there are videos that do not even contain the concept of interest. For example, maps are not present in each news video. Of course, the ques-tion is whether the transductive learning framework is able to improve the robustness of the underlying content analysis approaches. The experiments for the four different video analysis tasks demonstrate that the framework indeed improves the initial re-sults of the state-of-the-art approaches. An investigation of the impact of selecting good positive training samples, feature selection, or spitting the features revealed interest-ing insights in the components of the trans ductive learning framework. For analysis tasks with high precision in the initial result, it turned out that it is a good strategy to use a large fraction (of positive samples) that correlates with the precision of the baseline approach. In contrast, in the case of semantic video retrieval, samples with low probability scores also are useful for learning a new model. Feature selection has been proven to be powerful and essential for the proposed framework. Experiments show that it selects the relevant features for a video and reduces training time of the transductive framework noticeably as a side effect. The impact of splitting the feature set is comparatively low, but it moderately improved the quality of the analysis tasks of cut detection and semantic analysis of computer games. The presented framework was also compared with a transductive SVM for the semantic video retrieval task and the semantic annotation of computer games.

The transductive SVM approach worked ve ry well for the semantic video retrieval task. However, its performance was not satisfactory for the task of semantic computer game indexing, and the analysis of the reasons for this behavior is future work. A pos-sible reason is that the training data and the test data were of equal size, in contrast to the semantic video retrieval task. Overall, it has been demonstrated that adaptive and robust video content analysis can be achieve d via the proposed transductive learning framework. The transductive learning ensemble approach is always a good choice. It achieved the best performance for three out of four tasks, and its performance never degraded compared to the baseline system.

There are several areas for future work. First, it will be investigated whether an it-erative learning process could be beneficial. Second, the boosting approach of Redpath and Lebart [2005] could be an interesting alternative approach to realizing feature se-lection in the framework. Finally, other recent advances in the field of machine learn-ing will be considered with respect to their applicability for robust and adaptive video content analysis and retrieval. For example, the recent work of Bickel et al. [2007] addresses the issue of building classifiers for differing training and test distributions.
