 Data mining are becoming more and more important as the development of data science. In data mining, classification and clustering analysis are the core issues, in which we often need to compute the di stances or similarities between two data objects. Therefore, distance metr ic plays an crucial role in data mining. A good metric is one of the most important key points, especially in clustering, classification and kernel methods, that reflect the relationships between data [15]. Even though the traditional distance metrics, such as Euclidean distance, are simple and easy to compute, these metrics cannot handle the data set with complex distribution, and cannot represent the latent information which cannot handle heterogeneous data well [16]. More over, the Euclidean distance assumes that each feature has the same importance and have no relevance between each other, but this assumption does not hold for real world data. In contrast, Dis-tance Metric Learning (DML) [15] com putes distances between instances in data set with complex distributions and rev eal relevance between each feature.
In recent years, researchers found many applications of DML in many areas, such as clustering [15], classification [14] and information retrieval especially in ranking [11]. The distance metric learning was first proposed in [15] which mini-mize the distance between instances that are similar and subject to the constraint that dissimilar instances have a larger distances. Although this method gave a novel idea, it requires to solve a semi-definite programming problem, which is difficult in computation [4]. To avoid the problem of [15], Relevant Component Analysis (RCA) was then proposed to learn metric through linear transformation on Mahalanobis [2]. Weinberger has taken the advantages of Neighborhood Com-ponent Analysis (NCA) [7] to avoid the non-convex problem, and then proposed Large Margin Nearest Neighbor (LMNN) [14]. Davis X  X  Information-Theoretic Metric Learning (ITML) [6] combining the information theory, expressed the DML problem as a particular Bregman optimization problem that minimize the LogDet divergence subject to linear constraints.

Even though the proposed methods coul d weight each feature according to its effect, the complex distribution data with non-linearly separable features, which is common in real life, is not wel l solved by the above methods [16]. The above methods are mostly based on a single Mahalanobis metric which is d M ( x i ,x j )= ( x i  X  x j ) T A ( x i  X  x j ), and take optimization ideas to solve the DML problems. For the constraints of optimization problem, they use the labeled data, often is pairwise similar or dissimilar. Minimize distances between similar pairs and constrain on dissimilar pairs in order to indicate pairs should or should not be grouped. The objective of Mahalanobis based metric learning is to optimize the matrix A so that it could be viewed as an appropriate linear transformation that fits the distribution of data set[9].

To overcome the shortcomi ngs of existing methods, we propose a novel metric learning method. Instead of linear space transformation, our method is based on non-linear transformation. To represen t non-linear transformation effectively, we design a tree-based model to solve thi s problem. Our method keep the latent structural information as the growth of a tree, and the feature sampling strategy extends internal relevance between features. In order to reduce the generalization error of single tree model, we design a co mbination model and develop a random-ize algorithm which combine each tree generated with a randomize strategy to represent non-linear transformation. Such randomizing-combining strategy could not only keep the information of original data but also extend latent structure and probability information. We prove that the proposed method is effective and correct, and demonstrate the algorithm is stable and suitable for data mining with extensive experiments.

Our contributions in this paper are summarized as follows. 1. We propose a novel idea of tree representation-based metric learning task 2. We develop a leaf index feature representation algorithm to represent data 3. We employ a Bayesian specific tree path feature representation algorithm to 4. We exploit a randomized tree algorithm to reduce generalization errors and
The paper is organized as follows. In Sect ion 2, we present the problem defini-tion and notation. Two tree based feature representation algorithms are proposed in Section 3 which includes the leaf index feature representation algorithm and Bayesian specific tree path feature representation algorithm. We show a ran-dom trees representation strategy in S ection 4 and prove the correctness of our methods. Experiment results are shown in the Section 5. At last, we draw the conclusions in Section 6. In this section, we define the problem as well as the notations. In the area of data mining, each record in database may have multiple attributes, such as name, topic, description, etc. Different algorithms for distance computation may take different choices of attributes. For instance, during the distance computation be-tween tuples, one algorithm mainly takes the authorship or name, another may focus on the topic, and the description information could be used in the third algorithm. Here comes the problem, which one should be measured? Different attributes may have different semantics, thus they worth differently for the dis-tance computation. In this paper, we aim to find out a plausible distance metric for data mining tasks.

The input of this problem is a data set with n points { x i } n i =1  X  R d .Whentwo records x i ,x j fall into Euclidean Space, we co uld measure them by Euclidean only take Euclidean distance. However, different attributes may have different semantics and weights. The goal of metric learning is to find the weight for each attribute, which could also be thought as the transformation from Euclidean Space to some other. Traditional methods [15,14,7] seek a semi-positive definite matrix A which parameterizes the Mahalanobis distance:
The matrix A  X  R d  X  d is semi-positive definite, which means A could be decomposed into v T v . Then the above distance is transformed into
The equation (2) is just the general form of distance metric with an activator function  X  (  X  ). The core problem of metric learning is to find out the activator function  X  (  X  ). Then the problem is defined in Definition 1. Definition 1. The function  X  (  X  ) is a space transformation function for distance metric. We want to find out a proper activator function  X  (  X  ) for non-linear trans-formation task.

Traditional activator function, which is often the linear, cannot represent com-plex data. Therefore, we aim to find out a proper activator function that has enough capabilities to explain the data which not only keeps the original infor-mation but also extends latent information and do not lose accuracy. The equation (2) could be regarded as linear transformation with a vector v , and  X  (  X  ) means we times a weight within ea ch feature. However, this kind of linear transformation may not handle the situation with data correlation and complex distribution. Theref ore, we attempt to replace the  X  (  X  ) with a non-linear transformation. In this paper, we propose a robust non-linear and tuple transformations of input feature space that based on decision trees, which is one of non-linear methods usually used for classification or regression.
We build several decision trees within different selected input features, and treat individual tree as a transformation function. Here we propose two basic method to handle this situation: 1. generate new categorical features with the index of the leaf that an instance 2. generate new sequence features with the path of prediction of an instance In this section, we first talk about tree l eaves representation method and then introduce tree path representation. The two methods represent input data to another forms from different aspects. 3.1 Tree Leaves Representation When talking about tree-based method, people often only care about the result of classification or regression output, and ignore the structural representation infor-mation. However, the structure information is often important for representing latent factors [3]. Therefore, we can utilize the tree to do the representation task. Our Tree Leaves Representation algorithm is described in Algorithm 1. From Line 1 to 7, we create the transformation function F T based on tree T .We first build decision trees with different se lected features, and make a prediction for an instance. In Line 8, unlike general decision tree algorithm, we sample features from the whole feature set A , and in each node, we re-sample the sub-feature set in order to increase the diversity of data representation and reveal the real meaning and distribution. The benefit is that this method could retain the original information on the maximum extent due to the randomized sample algorithm in each node. We split tree nodes based on the ID3 [8,12] which is Algorithm 1. Leaf Index Feature Representation algorithm (LIFT) simple but effective in practice. Once th e tree has been built, we keep the decision tree for transformation. Once an instan ce comes, the model will predict which leaf it will end up, and we should keep the index of this leaf. Then we represent the instance by the leaf all 0s but 1s which the leaf this instance falls in.
As the growth of decision tree, the capability of explaining and representing will increase exponentially according to t he tree structure. However, the single tree model will not include the whole information [3]. Therefore, we often build multiple trees to keep enough information.

For example, in Fig. 1, if an instance falls into leaf 3 in the first subtree, and falls into leaf 6 in the second subtree, then the input features of this instance a decision tree, the number of leaves will increase fast, and this model will have more capability to represent input feature. On the other hand, the generated features are totally sparse representa tion which have been proven efficient and accurate.
 3.2 Tree Path Representation We give a strategy to represent input data based on decision path of a tree. The second method is shown in algorithm 2. As the first method, we first build several decision trees, and make predic tion on each instance. Then keep the path of prediction within the tree. The pseudo code is shown in Algorithm 2, and we also take the ID3 algorithm to build decision tree. However, the mainly difference is that we do not only keep the leaf information but also the decision path information.

This algorithm represents the hierarchical relation of data, and include the latent structure information. Then we introduce the Bayesian method that re-main the decision process to represent data in Line 6. A path keep not only the index but also the transition probabilities from parent node to child node, which is p ( child | parent ).

For example, in the Fig. 2, an instance falls into the leaf 10, and we keep the path { C 12  X  C 23  X  C 31  X  L 10 } . From another tree node, we could consider this path as a bayesian process, which is { p ( X  X  C 12 ) ,p ( C 12  X  C 23 ) ,p ( C 23  X  C For this instance, when we perform the transform task, the results include a vector that includes path with index of each layer, and the probabilities of each tree node in the path. To increase the capability of this method and reduce the generalization errors, we will propose a randomize algorithm for feature representation in section 4 , and prove the effectiveness. The single decision tree will lead to overfit, which perform well in training set, but weakly in testing set [12]. It usually has low bias and high variance in real Algorithm 2. Bayesian Specific Tree Path Feature Representation (BSTP) application. In light of [5], an effective approach for reducing generalization errors would reduce the prediction variance, and the respective bias would be the same or not be increased too much [10]. Therefore, we will introduce a randomize algorithm for DML in this section to increase capability of the method and reduce the generalization errors, then analyze the time complexity and prove the effectiveness. In this section, we em ploy ensemble strategy that combines several results and then produces a new result. 4.1 Combining Random Tree Algorithm Combining models method will solve the overfit problem in a simple way [10]. The method takes random combinations for learning process, such that they could produce different models only from a single data set L .Afterthat,this method combines the results to form a n ew results based on the generated mod-els. In Algorithm 3, we generate feature representation based on algorithm 1 Algorithm 3. Random Trees for Feature Transformation algorithm(RTFT) and algorithm 2. We use the transformation function set to represent input fea-tures. For each transformation function, we generate a representation vector, and combining them to become a new representation in Line 4.

Assume that there is a set of k random tree models. All of them are trained based on the same data set but built from the independent parameter t i which is viewed as a random seed.
 Theorem 1. Our method combines the transformation results into a new rep-model which is Err ( F t i ) .
 Proof. The combination of transformation results is equivalent to the average prediction that minimizes the average errors with respect to the single result of the models. The errors of a single model are shown in formula (4)
Similarly, the error of combining model and its variance of the single result of the combining result is: In statistic, the error evaluation can be viewed as:
While the total error of combining model is smaller than the single model because the variance  X  V is non-negative. Therefore, the combining model leads to a more accurate model with less gener alization error, and not increase the expectation bias. 4.2 Random Trees Representation In Algorithm 1 and algorithm 2, we select sub feature space randomly, which means that each feature has the same probability to be chosen. We assume that if two instances have similarity p , then each feature has the probability p to be same [13]. First of all, we prove that our method could keep the original information, then we will show that this method will represent enough latent information include structure and probability information.
 Theorem 2. The probability of two similar instances getting the same result in one tree will approach 1. Otherwise, the probability will approach 0. Proof. Suppose we use k trees to represent input features, and each tree has the average height  X  h . The similarity between two instances is p . Due to the above assumption, two instances has the probability p to have the same choose for them to select split node at each layer as the tree growth. With the av-erage height  X  h , for a single tree, the probability that two instances agree in all layers is p  X  h . Then the probability that the instances do not agree in at least on layer which will lead the two end up in the different leaves will be 1  X  p  X  h . The probability that the instances end up differently for all trees will be (1  X  p  X  h ) k . Therefore, there will be 1  X  (1  X  p  X  h ) k probability that the instances agree in all layers of at least one tree. This form is more like the the form of an S-curve. With the p  X  1 which means the instances are very similar, then the probability that the two will be considered as the same in at least one tree tends to be 1. On the other hand, once the p  X  0, the probability tends to be 0. Once two instances has high similarity, the probability that the two will end up in same leaf for at least one tree, will approach 1, or it will approach 0. From another point, our method will keep the original information with ex-tremely high probability. As the growth of a decision tree, each split is based on random sampled sub-feature space. Each feature has the same probability to be chosen to extend the internal informa tion of itself, which is an efficient non-linear representation. In the next sectio n, we will show the experimental results to validate the effectiveness of our algorithm. 4.3 Time Complexity Suppose we use k trees and each tree involves the whole data set X ,andwe assume the size of X is n . To build a single tree with an average height  X  h ,in each layer, the algorithm will scan the data set to decide in which the node will split. Thus, the time complexity of a single tree will be O ( n  X  h ). Generally, the height  X  h of a single tree will be not greater than the height of a binary tree which is log ( n ). Then the time complexity will be O ( n  X  log( n )). For k trees, the total complexity will be O ( k  X  n  X  log( n )). However, each tree is independent and could be built independently. Therefore, the trees could be built in parallel with non loss of accuracy. In this section, we give the experimental results on the UCI data sets. We run experiments on the Ubuntu server with the 4GB RAM and 4 cores Intel(R) Core(TM) i5-2400 CPU. We test our model with both classification and clus-tering tasks. We compare our approach on classical data mining methods. For classification, we compare the methods in t he accuracy, while for clustering, the measure is sum square errors (SSE) [12]. 5.1 Experiments on Classification In Table 1, we test on the Movement Libras data set for k -NN classification task. The Movement Libras data set publics a data set for training, and five data sets that for testing. This data set has 90 attributes/features, and 15 classes.
From Table 1 we observe that the accuracies of our algorithms are better than the point-based position-dependent k -Nearest Neighbors. The point-based position-dependent k -Nearest Neighbors ignore the structure representation in-formation, but the structural information is often very important for representing latent factors. Therefore our algorithms increase the accuracies. Additionally, the BSTP algorithm has a better performance on many test sets. The BSTP algo-rithm keeps not only the leave information but also the decision path information of each tree.

However, not all results of the test sets are such good. It shows that even though the BSTP algorithm keeps more information, sometimes it will include noise and the influence of bad cases which often leads to overfit on training set that the training results are highly with the consistent with the distribution of training set that has been shown in Fig. 3 and Fig. 4.

Our algorithm has two essential parameters , including max tree depth h and number of trees k .The h decides the complexity of a single tree model, and the k decides the whole model. As discussed in the section 4, the capability of representation will increase while the h and k are going to be large. We conduct 100 experiments for the two parameters respectively. The results have beenshowninFig.3andFig.4.Astheparameter k and h goes up, the errors first decrease then increa se. When the parameter is small, the model cannot represent enough information and the e rrors seem to be large. While it increases gradually, the model has enough capability to represent latent information. Thus, the errors are going down. However, when parameters are large enough, the model has enough structure information of train data so that the model falls into the distribution of train data and the performance on the test set are worse and worse. As a result, our algorithm can r epresent data effectively by choosing a proper parameter set.
 5.2 Experiments on Clustering Analysis Clustering is one of important applications of metric learning. We experimented on drift data set of UCI data sets. The e xperiment takes the K-means as the base algorithm and compare the k-means result on Euclidean distance with the k-means result on our random trees distance metric, and the measure rule is the sum square errors. The implementation strategy of k-means is based on the [1], which is an effective algorithm to reduce the influence of the initialization. Another thing we should claim is that our algorithm is to represent input to another form, and the output will have more dimensions than the original. How-ever, when the number of dimension increases, the corresponding square errors increase. Therefore, we should reduce the dimensions of representation results in order that they can have the same scale with the original input. The results are shown in Fig. 5. As observed, our algorithm has reduced the errors largely. Distance metric plays a core role in data mining. Traditional distance metric could not handle the complex distribution situation. Metric learning has been proposed to overcome this problem. Most metric learning methods, however, are based on Mahalanobis metric that could be viewed as some kind linear transformation on distance space. Such method has many limitations. In this paper, we propose a randomize algorithm to solve the distance metric learning problem. Our algorithm, which based on combining random decision trees, has demonstrated the capability of representation of data. The mainly purpose of a representation task is to keep the original information and then to extend the latent information. The combining random trees could keep the original infor-mation as proved. And the decision trees model will reveal the latent structural and probability information. The experimental results show the effectiveness of our model is effective on both the classific ation task and the clustering task. For further studying, we will research on regularization in order to reduce the risk of over-fitting.

