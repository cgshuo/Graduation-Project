 Steffen Gr  X  unew  X  alder 1 STEFFEN @ CS . UCL . AC . UK Massimilano Pontil M . PONTIL @ CS . UCL . AC . UK CSML and  X  Gatsby Unit, University College London, UK,  X  MPI for Intelligent Systems In recent years a framework for embedding probability dis-tributions into reproducing kernel Hilbert spaces (RKHS) has become increasingly popular (Smola et al., 2007). One example of this theme has been the representation of condi-tional expectation operators as RKHS functions, known as conditional mean embeddings (Song et al., 2009). Con-ditional expectations appear naturally in many machine learning tasks, and the RKHS representation of such ex-pectations has two important advantages: first, conditional mean embeddings do not require solving difficult interme-diate problems such as density estimation and numerical integration; and second, these embeddings may be used to compute conditional expectations directly on the basis of observed samples. Conditional mean embeddings have been successfully applied to inference in graphical models, reinforcement learning, subspace selection, and conditional independence testing (Fukumizu et al., 2008; 2009; Song et al., 2009; 2010; Gr  X  unew  X  alder et al., 2012). The main motivation for conditional means in Hilbert spaces has been to generalize the notion of conditional ex-pectations from finite cases (multivariate Gaussians, con-ditional probability tables, and so on). Results have been established for the convergence of these embeddings in RKHS norm (Song et al., 2009; 2010), which show that conditional mean embeddings behave in the way we would hope (i.e., they may be used in obtaining conditional ex-pectations as inner products in feature space, and these es-timates are consistent under smoothness conditions). De-spite these valuable results, the characterization of condi-tional mean embeddings remains incomplete, since these embeddings have not been defined in terms of the opti-mizer of a given loss function . This makes it difficult to extend these results, and has hindered the use of standard techniques like cross-validation for parameter estimation. In this paper, we demonstrate that the conditional mean em-bedding is the solution of a vector-valued regression prob-lem with a natural loss, resembling the standard Tikhonov regularized least-squares problem in multiple dimensions. Through this link, it is possible to access the rich the-ory of vector-valued regression (Micchelli &amp; Pontil, 2005; Carmeli et al., 2006; Caponnetto &amp; De Vito, 2007; Capon-netto et al., 2008). We demonstrate the utility of this con-nection by providing novel characterizations of conditional mean embeddings, with important theoretical and practical implications. On the theoretical side, we establish novel convergence results for RKHS embeddings, giving a signif-icant improvement over the rate of O ( n  X  1 / 4 ) due to Song et al. (2009; 2010). We derive a faster O (log( n ) /n ) rate which holds over large classes of probability measures, and requires milder and more intuitive assumptions. We also show our rates are optimal up to a log( n ) term, following the analysis of Caponnetto &amp; De Vito (2007). On the prac-tical side, we derive an alternative sparse version of the em-beddings which resembles the Lasso method, and provide a cross-validation scheme for parameter selection. In this section, we recall some background results concern-ing RKHS embeddings and vector-valued RKHS. For an introduction to scalar-valued RKHS we refer the reader to (Berlinet &amp; Thomas-Agnan, 2004). 2.1. Conditional mean embeddings Given sets X and Y , with a distribution P over random variables ( X,Y ) from X  X Y we consider the problem of learning expectation operators corresponding to the con-ditional distributions P ( Y | X = x ) on Y after condition-ing on x  X  X . Specifically, we begin with a kernel L : Y X Y  X  R , with corresponding RKHS H L  X  R Y , and study the problem of learning, for every x  X  X  , the condi-tional expectation mapping H L 3 h 7 X  E [ h ( Y ) | X = x ] . Each such map can be represented as where the element  X  ( x )  X  H L is called the (conditional) mean embedding of P ( Y | X = x ) . Note that, for every x ,  X  ( x ) is a function on Y . It is thus apparent that  X  is a mapping from X to H L , a point which we will expand upon shortly.
 We are interested in the problem of estimating the embed-dings  X  ( x ) given an i.i.d. sample { ( x i ,y i ) } n i =1 P n . Following (Song et al., 2009; 2010), we define a sec-ond kernel K : X  X X  X  R with associated RKHS H K , and consider the estimate where  X  i ( x ) = P n j =1 W ij K ( x j ,x ) , and where W := ( K +  X n I )  X  1 , K = ( K ( x i ,x j )) n ij =1 , and  X  is a cho-sen regularization parameter. This expression suggests that the conditional mean embedding is the solution to an un-derlying regression problem: we will formalize this link in Section 3. In the remainder of the present section, we introduce the necessary terminology and theory for vector valued regression in RHKSs. 2.2. Vector-valued regression and RKHSs We recall some background on learning vector-valued functions using kernel methods (see Micchelli &amp; Pon-til, 2005, for more detail). We are given a sample { ( x i ,v i ) } i  X  m drawn i.i.d. from some distribution over X  X  V , where X is a non-empty set and ( V ,  X  X  ,  X  X  V ) is a Hilbert space. Our goal is to find a function f : X  X  V with low error, as measured by This is the vector-valued regression problem (square loss). One approach to the vector-valued regression problem is to model the regression function as being in a vector-valued RKHS of functions taking values in V , which can be de-fined by analogy with the scalar valued case.
 Definition A Hilbert space ( H ,  X  X  ,  X  X   X  ) of functions h : X  X  V is an RKHS if for all x  X  X ,v  X  V the linear functional h 7 X  X  v,h ( x )  X  V is continuous.
 The reproducing property for vector-valued RHKSs fol-lows from this definition (see Micchelli &amp; Pontil, 2005, Sect. 2). By the Riesz representation theorem, for each x  X  X and v  X  V , there exists a linear operator from V to H
 X  written  X  x v  X  X   X  , such that for all h  X  X   X  , It is instructive to compare to the scalar-valued RKHS H K for which the linear operator of evaluation  X  x mapping h  X  H
K to h ( x )  X  R is continuous: then Riesz implies there exists a K x such that yh ( x ) =  X  h,yK x  X  K .
 We next introduce the vector-valued reproducing kernel, and show its relation to  X  x . Writing as L ( V ) the space of bounded linear operators from V to V , the reproducing kernel  X ( x,x 0 )  X  X  ( V ) is defined as From this definition and the reproducing property, the fol-lowing holds (Micchelli &amp; Pontil, 2005, Prop. 2.1). Proposition 2.1. A function  X  : X  X  X  X  L ( V ) is a kernel if it satisfies: (i)  X ( x,x 0 ) =  X ( x for all n  X  N , { ( x i ,v i ) } i  X  n  X  X  X  V we have that P It is again helpful to consider the scalar case: here,  X  K x ,K x 0  X  K = K ( x,x 0 ) , and to every positive definite kernel K ( x,x 0 ) there corresponds a unique (up to isome-try) RKHS for which K is the reproducing kernel. Sim-ilarly, if  X  : X  X  X  X  L ( V ) is a kernel in the sense of Proposition 2.1, there exists a unique (up to isometry) RKHS, with  X  as its reproducing kernel (Micchelli &amp; Pon-til, 2005, Th. 2.1). Furthermore, the RKHS H  X  can be described as the RKHS limit of finite sums; that is, H  X  is up to isometry equal to the closure of the linear span of the set {  X  x v : x  X  X  ,v  X  X } , wrt the RKHS norm k X k  X  . Importantly, it is possible to perform regression in this set-ting. One approach to the vector-valued regression prob-lem is to replace the unknown true error (2) with a sample-element of an RKHS H  X  (of vector-valued functions), and regularizing w.r.t. the H  X  norm, to prevent overfitting. We thus arrive at the following regularized empirical risk, Theorem 2.2. (Micchelli &amp; Pontil, 2005, Th. 4) If f  X  imises b E  X  in H  X  then it is unique and has the form, where the coefficients { c i } i  X  n , c i  X  X  are the unique solu-tion of the system of linear equations In the scalar case we have that | f ( x ) |  X  p K ( x,x ) k f k Similarly it holds that k f ( x ) k  X  |||  X ( x,x ) |||k f k ||| X ||| denotes the operator norm (Micchelli &amp; Pontil, 2005, Prop. 1). Hence, if |||  X ( x,x ) ||| X  B for all x then Finally, we need a result that tells us when all functions in an RKHS are continuous. In the scalar case this is guaran-teed if K ( x,  X  ) is continuous for all x and K is bounded. In our case we have (Carmeli et al., 2006)[Prop. 12]: Corollary 2.3. If X is a Polish space, V a separabe Hilbert space and the mapping x 7 X   X (  X  ,x ) is continuous, then H is a subset of the set of continuous functions from X to V . In this section, we show the problem of learning condi-tional mean embeddings can be naturally formalised in the framework of vector-valued regression, and in doing so we derive an equivalence between the conditional mean em-beddings and a vector-valued regressor. 3.1. The equivalence between conditional mean Conditional expectations E [ h ( Y ) | X = x ] are linear in the argument h so that, when we consider h  X  H L , the Riesz representation theorem implies the existence of an element  X  ( x )  X  H L such that E [ h ( Y ) | X = x ] =  X  h, X  ( x )  X  all h . That being said, the dependence of  X  on x may be complicated. A natural optimisation problem associated to this approximation problem is to therefore find a function  X  : X  X  X  L such that the following objective is small E [  X  ] := sup Note that the risk function cannot be used directly for esti-mation, because we do not observe E Y [ h ( Y ) | X ] , but rather pairs ( X,Y ) drawn from P . However, we can bound this risk function with a surrogate risk function that has a sam-ple based version, sup where the first and second bounds follow by Jensen X  X  and Cauchy-Schwarz X  X  inequalities, respectively. Let us denote this surrogate risk function as The two risk functions E and E s are closely related and in Section 3.3 we examine their relation.
 We now replace the expectation in (6) with an empirical estimate, to obtain the sample-based loss, Taking (8) as our empirical loss, then following Section 2.2 we add a regularization term to provide a well-posed prob-lem and prevent overfitting, We denote the minimizer of (9) by  X   X   X ,n , Thus, recalling (3), we can see that the problem (10) is posed as a vector-valued regression problem with the training data now considered as { ( x i ,L ( y i ,  X  )) } we identify H L with the general Hilbert space V of Sec-tion 2.2). From Theorem 2.2, the solution is where the coefficients { c i } i  X  n , c i  X  H L are the unique solution of the linear equations It remains to choose the kernel  X  . Given a real-valued ker-nel K on X , a natural choice for the RKHS H  X  would be the space of functions from X to H L whose elements are defined as functions via ( h,K ( x,  X  ))( x 0 ) := K ( x,x which is isomorphic to H L  X  X  K , with inner product for all g,h  X  H L . Its easy to check that this satis-fies the conditions to be a vector-valued RKHS X  in fact it corresponds to the choice  X ( x,x 0 ) = K ( x,x 0 )Id , where Id : H L  X  X  L is the identity map on H L . The solution to (10) with this choice is then given by (11), with where W = ( K +  X  I )  X  1 , which corresponds exactly the embeddings (1) presented in (Song et al., 2009; 2010) (after a rescaling of  X  ). Thus we have shown that the embeddings of Song et al. are the solution to a regression problem for a particular choice of operator-valued kernel. Further, the loss defined by (7) is a key error functional in this context since it is the objective which the estimated embeddings attempt to minimise. In Sec. 3.3 we will see that this does not always coincide with (5) which may be a more natural choice. In Sec. 4 we analyze the performance of the em-beddings defined by (10) at minimizing the objective (7). 3.2. Some consequences of this equivalence We derive some immediate benefits from the connec-tion described above. Since the embedding problem has been identified as a regression problem with training set scheme for parameter selection in the usual way: by hold-ing out a subsample D val = { ( x t i ,L ( y t j ,  X  )) } can train embeddings  X   X  on D\D val over a grid of kernel or
Input/Output space (i) X is Polish.
Space of regressors (iv) H  X  is separable.
True distribution (vii) L ( y,y ) &lt;  X  for all y  X  X  . regularization parameters, choosing the parameters achiev-ing the best error P T j =1 ||  X   X  ( x t j )  X  L ( y t j ,  X  ) || dation set (or over many folds of cross validation). Another key benefit will be a much improved performance analysis of the embeddings, presented in Section 4. 3.3. Relations between the error functionals E and E s In Section 3.1 we introduced an alternative risk function E s for E , which we used to derive an estimation scheme to recover conditional mean embeddings. We now examine the relationship between the two risk functionals. When the true conditional expectation on functions h  X  H L can be represented through an element  X   X   X  H  X  then  X   X  min-imises both objectives.
 Theorem 3.1 (Proof in App. A) . If there exists a  X   X   X  H  X  such that for any h  X  H L : E [ h | X ] =  X  h, X   X  ( X )  X  P
X -a.s., then  X   X  is the P X -a.s. unique minimiser of both objectives: Thus in this case, the embeddings of Song et al. (e.g. 2009) minimise both (5) and (7). More generally, however, this may not be the case. Let us define an element  X   X  that is  X  close w.r.t. the error E s to the minimizer  X  0 of E s in H might for instance be the minimizer of the empirical regu-larized loss for sufficiently many samples). We are inter-ested in finding conditions under which E ( X   X  ) is not much worse than a good approximation  X   X  in H  X  to the condi-tional expectation. The sense in which  X   X  approximates the conditional expectation is somewhat subtle:  X   X  must closely approximate the conditional expectation of func-tions  X   X  X   X  under the original loss E (note that the loss E was originally defined in terms of functions h  X  X  L ). Theorem 3.2 (Proof in App. A) . Let  X  0 be a minimiser of E s and  X   X  be an element of H  X  with E [ X   X  ]  X  E s [  X  0 ] +  X  . Define, A := { (  X ,  X   X  ) |  X  E [ X   X  ]  X  inf Apart from the more obvious condition that  X  be small, the above theorem suggests that ||  X   X  ||  X  should also be made small for the solution  X   X  to have low error E . In other words, even in the infinite sample case, the regularization of  X   X  in H
 X  is important. The interpretation of the mean embedding as a vector val-ued regression problem allows us to apply regression mini-max theorems to study convergence rates of the embedding estimator. These rates are considerably better than the cur-rent state of the art for the embeddings, and hold under milder and more intuitive assumptions.
 We start by comparing the statements which we derive from (Caponnetto &amp; De Vito, 2007, Thm.s 1 and 2) with the known convergence results for the embedding estimator. We follow this up with a discussion of the rates and a com-parison of the assumptions. 4.1. Convergence theorems We address the performance of the embeddings defined by (10) in terms of asymptotic guarantees on the loss E s defined by (7). Caponnetto &amp; De Vito (2007) study uni-form convergence rates for regression. Convergence rates of learning algorithms can not be uniform on the set of all probability distributions if the output vector space is an in-finite dimensional RKHS (Caponnetto &amp; De Vito, 2007)[p. 4]. It is therefore necessary to restrict ourselves to a sub-set of probability measures. This is done by Caponnetto &amp; De Vito (2007) by defining families of probability mea-sures P ( b,c ) indexed by two parameters b  X  ]1 ,  X  ] and c  X  [1 , 2] . We discuss the family P ( b,c ) in detail below. The important point at the moment is that b and c affect the optimal schedule for the regulariser  X  and the convergence rate. The rate of convergence is better for higher b and c values. Caponnetto &amp; De Vito (2007) provide convergence rates for all choices of b and c . We restrict ourself to the best case b =  X  ,c &gt; 1 and the worst case 1 b = 2 ,c = 1 . We recall that the estimated conditional mean embed-dings  X   X   X ,n are given by (10), where  X  is a chosen reg-ularization parameter. We assume  X  n is chosen to fol-low a specific schedule, dependent upon n : we denote by  X   X  n the embeddings following this schedule and  X  0 :=  X  , Thm. 1 of Caponnetto &amp; De Vito (2007) yields the following convergence statements for the estimated embed-dings, under assumptions to be discussed in Section 4.2. Corollary 4.1. Let b =  X  ,c &gt; 1 then for every &gt; 0 there exists a constant  X  such that Let b = 2 and c = 1 then for every &gt; 0 there exists a constant  X  such that lim sup &lt; .
 The rate for the estimate  X   X  n can be complemented with minimax lower rates for vector valued regression (Capon-netto &amp; De Vito, 2007)[Th. 2] in the case that b &lt;  X  . Corollary 4.2. Let b = 2 and c = 1 and let  X  n := { l n | l ( X  X  Y ) n  X  H  X  } be the set of all learning algorithm working on n samples, outputting  X  n  X  X   X  . Then for every &gt; 0 there exists a constant  X  &gt; 0 such that lim inf &gt; 1  X  .
 This corollary tells us that there exists no learning algo-rithm which can achieve better rates than n  X  2 3 uniformly over P (2 , 1) , and hence the estimate  X   X  n is optimal up to a logarithmic factor.
 State of the art results for the embedding The current convergence result for the embedding is proved by Song et al. (2010, Th.1). A crucial assumption that we discuss in detail below is that the mapping x 7 X  E [ h ( Y ) | X = x ] is in the RKHS H K of the real valued kernel, i.e. that for all h  X  X  L we have that there exists a f h  X  X  K , such that The result of Song et al. implies the following (see App. C): if K ( x,x ) &lt; B for all x  X  X and the schedule  X  ( n ) = n  X  1 / 4 is used: for a fixed probability measure P , there exists a constant  X  such that where  X   X  n ( x ) is the estimate from Song et al. No comple-mentary lower bounds were known until now.
 Comparison The first thing to note is that under the as-sumption that E [ h | X ] is in the RKHS H K the minimiser of E s and E are a.e. equivalent due to Theorem 3.1: the assumption implies a  X   X   X  H  X  exists with E [ h | X ] =  X  h, X   X  ( X )  X  L for all h  X  V (see App. B.4 for details). Hence, under this assumption, the statements from eq. 14 and Cor. 4.1 ensure we converge to the true conditional expectation, and achieve an error of 0 in the risk E . In the case that this assumption is not fullfilled and eq. 14 is not applicable, Cor. 4.1 still tells us that we converge to the minimiser of E s . Coupling this statement with Thm. 3.2 allows us to bound the distance to the minimal error E [  X  where  X   X   X  X   X  minimises E .
 The other main differences are obviously the rates, and that Cor. 4.1 bounds the error uniformly over a space of prob-ability measures, while eq. 14 provides only a point-wise statement (i.e., for a fixed probability measure P ). 4.2. Assumptions Cor. 4.1 and 4.2 Our main assumption is that H L is fi-nite dimensional. It is likely that this assumption can be weakened, but this requires a deeper analysis.
 The assumptions of Caponnetto &amp; De Vito (2007) are sum-marized in Table 1, where we provide details in App. B.2. App. B.1 contains simple and complete assumptions that ensure all statements in the table hold. Beside some mea-sure theoretic issues, the assumptions are fulfilled if for ex-ample, 1) X is a compact subset of R n , Y is compact, H L is a finite dimensional RKHS,  X  and L are continuous; 2) E [  X  0 ] = inf  X   X  X   X  E s [  X  ] . This last condition is unintuitive, but can be rewritten in the following way: Theorem 4.3 (Proof in App.) . Let || h || V , ||  X  ( x )  X  h || integrable for all h  X  H  X  and let V be finite dimensional. Then there exists a  X  0  X  H  X  with E s [  X  0 ] = inf  X   X  X  iff a B &gt; 0 exists and a sequence {  X  n } n  X  N with E inf  X   X  X   X  E s [  X  ] + 1 /n and ||  X  n ||  X  &lt; B . The intuition is that the condition is not fulfilled if we need to make  X  n more and more complex (in the sense of a high RKHS norm) to optimize the risk.
 Definition and discussion of P ( b,c ) The family of probability measures P ( b,c ) is characterized through spectral properties of the kernel function  X  . The assump-tions correspond to assumptions on the eigenvalues in Mer-cer X  X  theorem in the real valued case, i.e. that there are finitely many eigenvalues or that the eigenvalues decrease with a certain rate. In detail, define the operator A through A (  X  )( x 0 ) := R X  X ( x 0 ,x )  X  ( x ) dP X , where  X   X  L A can be written as (Caponnetto &amp; De Vito, 2007, Rem. 2) A = P N n =1  X  n  X  X  , X  n  X  P  X  n , where the inner product is the L 2 inner product with measure P X and N =  X  is al-lowed. As in the real valued case, the eigendecomposition depends on the measure on the space X but is independent of the distribution on Y . The eigendecomposition measures the complexity of the kernel, where the lowest complexity is achieved for finite N  X  that is, the case b =  X  ,c &gt; 1  X  and has highest complexity if the eigenvalues decrease with the slowest possible rate,  X  n &lt; C/n for a constant C . The case b = 2 ,c = 1 correspond to a slightly faster decay, namely,  X  n &lt; C/n 2 . In essence, there are no assumptions on the distribution on Y , but only on the complexity of the kernel  X  as measured with P X .
 Embedding The results of Song et al. (2010) do not rely on the assumption that V is finite dimensional. Other con-ditions on the distribution are required, however, which are challenging to verify. To describe these conditions, we recall the real-valued RKHS H K with kernel K , and define the uncentred cross-covariance operator C Y X : H
K  X  H L such that  X  g,C Y X f  X  H L = E XY ( f ( X ) g ( Y )) , with the covariance operator C XX defined by analogy. One of the two main assumptions of Song et al. is that C C
Y X and C XX are compact operators, meaning C XX is not invertible when H K is infinite dimensional (this gives rise to a notational issue, although the  X  X roduct X  operator C Hilbert-Schmidt (or even bounded) will depend on the un-derlying distribution P XY and on the kernels K and L . At this point, however, there is no easy way to translate prop-erties of P XY to guarantees that the assumption holds. The second main assumption is that the conditional expec-tation can be represented as an RKHS element (see App B.4). Even for rich RKHSs (such as universal RKHSs), it can be challenging to determine the associated conditions on the distribution P XY . For simple finite dimensional RKHSs, the assumption may fail, as shown below.
 Corollary 4.4 (Proof in App. C ) . Let V be finite dimen-sional such that a function  X  h  X  X  exists with  X  h ( y )  X  &gt; 0 for all y  X  Y . Furthermore, let X := [  X  1 , 1] and the re-producing kernel for H K be K ( x,y ) = xy . Then there exists no measure for which the assumption from eq. (13) can be fulfilled. In many practical situations it is desirable to approximate the conditional mean embedding by a sparse version which involves a smaller number of parameters. For example, in the context of reinforcement learning and planning, the sample size n is large and we want to use the embeddings over and over again, possibly on many different tasks and over a long time frame.
 Here we present a technique to achieve a sparse approxi-mation of the sample mean embedding. Recall that this is given by the formula (cf. equation (11)) where W = ( K + n X I )  X  1 . A natural approach to find a sparse approximation of  X   X  is to look for a function  X  which is close to  X   X  according to the RKHS norm k X k  X  (in App. D we establish a link between this objective and our cost function E ). In the special case that  X  = KId this amounts to solving the optimization problem where  X  is a positive parameter, k W k 1 , 1 := P n i,j | M Problem (15) is equivalent to a kind of Lasso problem with n 2 variables: when  X  = 0 , M = W at the optimum and the approximation error is zero, however as  X  increases, the approximation error increases as well, but the solution obtained becomes sparse (many of the elements of matrix M are equal to zero).
 A direct computation yields that the above optimization problem is equivalent to In the experiments in Section 5.1, we solve problem (17) with FISTA (Beck &amp; Teboulle, 2009), an optimal first or-der method which requires O (1 / accuracy of the minimum value in (17), with a cost per it-eration of O ( n 2 ) in our case. The algorithm is outlined be-low, where S  X  ( Z ij ) = sign ( Z ij )( | Z ij | X   X  ) + and ( z ) if z &gt; 0 and zero otherwise.
 Algorithm 1 LASSO-like Algorithm input: W ,  X  , K , L output: M
Z 1 = Q 1 = 0 , X  1 = 1 ,C = k K kk L k for t=1,2,. . . do end for Other sparsity methods could also be employed. For exam-ple, we may replace the norm k  X  k 1 , 1 by a block ` norm. That is, we may choose the norm k M k 2 , 1 := P the rows of M . This penalty function encourages sparse approximations which use few input points but all the out-puts. Similarly, the penalty k M &gt; k 2 , 1 will sparsify over the outputs. Finaly, if we wish to remove many pair of examples we may use the more sophisticated penalty P 5.1. Experiment We demonstrate here that the link between vector-valued regression and the mean embeddings can be leveraged to develop useful embedding alternatives that exploit proper-ties of the regression formulation: we apply the sparse al-gorithm to a challenging reinforcement learning task. The sparse algorithm makes use of the labels, while other al-gorithms to sparsify the embeddings without our regres-sion interpretation cannot make use of these. In particular, a popular method to sparsify is the incomplete Cholesky decomposition (Shawe-Taylor &amp; Cristianini, 2004), which sparsifies based on the distribution on the input space X only. We compare to this method in the experiments. The reinforcement learning task is the under-actuated pen-dulum swing-up problem from Deisenroth et al. (2009). We generate a discrete-time approximation of the continuous-time pendulum dynamics as done in (Deisenroth et al., 2009). Starting from an arbitrary state the goal is to swing the pendulum up and balance it in the inverted po-sition. The applied torque is u  X  [  X  5 , 5] Nm and is not sufficient for a direct swing up. The state space is de-fined by the angle  X   X  [  X   X , X  ] and the angular veloc-ity,  X   X  [  X  7 , 7] . The reward is given by the function r (  X , X  ) = exp(  X   X  2  X  0 . 2  X  2 ) . The learning algorithm is a kernel method which uses the mean embedding estima-tor to perform policy iteration (Gr  X  unew  X  alder et al., 2012). Sparse solutions are in this task very useful as the policy iteration applies the mean embedding many times to per-form updates. The input space has 4 dimensions (sine and cosine of the angle, angular velocity and applied torque), while the output has 3 (sine and cosine of the angle and angular velocity). We sample uniformly a training set of 200 examples and learn the mean conditional embedding using the direct method (1). We then compare the sparse approximation obtained by Algorithm 1 using different val-ues of the parameter  X  to the approximation obtained via an incomplete Cholesky decomposition at different levels of sparsity. We assess the approximations using the test er-ror and (16), which is an upper bound on the generalization error (see App. D) and report the results in Figure 1. We have established a link between vector-valued regres-sion and conditional mean embeddings. On the basis of this link, we derived a sparse embedding algorithm, showed how cross-validation can be performed, established better convergence rates under milder assumptions, and comple-mented these upper rates with lower rates, showing that the embedding estimator achieves near optimal rates.
 There are a number of interesting questions and problems which follow from our framework. It may be valuable to employ other kernels  X  in place of the kernel K ( x,y )Id that leads to the mean embedding, so as to exploit knowl-edge about the data generating process. As a related ob-servation, for the kernel  X ( x,y ) := K ( x,y )Id ,  X  x is not a Hilbert-Schmidt operator if V is infinite dimensional, as however the convergence results from (Caponnetto &amp; De Vito, 2007) assume  X  x to be Hilbert-Schmidt. While this might simply be a result of the technique used in (Capon-netto &amp; De Vito, 2007), it might also indicate a deeper problem with the standard embedding estimator, namely that if V is infinite dimensional then the rates degrade. The latter case would have a natural interpretation as an over-fitting effect, as Id does not  X  X mooth X  the element h  X  X  . Our sparsity approach can potentially be equipped with other regularisers that cut down on rows and columns of the W matrix in parallel. Certainly, ours is not the only sparse regression approach, and other sparse regularizers might yield good performance on appropriate problems. The authors want to thank for the support of the EPSRC #EP/H017402/1 (CARDyAL) and the European Union #FP7-ICT-270327 (Complacs).

