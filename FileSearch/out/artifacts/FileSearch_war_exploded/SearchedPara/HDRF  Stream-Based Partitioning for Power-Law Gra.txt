 Balanced graph partitioning is a fundamental problem that is receiving growing attention with the emergence of dis-tributed graph-computing (DGC) frameworks. In these frame-works, the partitioning strategy plays an important role since it drives the communication cost and the workload bal-ance among computing nodes, thereby affecting system per-formance. However, existing solutions only partially exploit a key characteristic of natural graphs commonly found in the real-world: their highly skewed power-law degree distribu-tions. In this paper, we propose High-Degree (are) Repli-cated First ( HDRF ), a novel streaming vertex-cut graph partitioning algorithm that effectively exploits skewed de-gree distributions by explicitly taking into account vertex degree in the placement decision. We analytically and exper-imentally evaluate HDRF on both synthetic and real-world graphs and show that it outperforms all existing algorithms in partitioning quality.
 E.1 [ Data Structures ]: Graphs and Networks Graph Partitioning; Streaming Algorithms; Distributed Graph-Computing Frameworks; Replication; Load Balancing.  X  This work has been partially supported by the TENACE PRIN Project (n. 20103P34XC) funded by the Italian Min-istry of Education, University and Research.
 c  X  2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00.
The last few years have witnessed a huge growth in infor-mation production. Some corporations like IBM estimate that  X 2.5 quintillion bytes of data are created every day X , amounting to 90% of the data in the world today having been created in the last two years [10]. On the face of this growth, researchers from both academia and industry have focussed their efforts on the design of new, efficient, approaches for parallel data analysis able to withstand the deluge of data expected in forthcoming years.

Given the proliferation of data which can be represented as graphs of interconnected vertices, a graph-based com-putation paradigm provides a nice, suitable, abstraction to perform computation on it. Large amounts of data, particu-larly scale-free graphs or power-law graphs 1 , fall within this paradigm. An example is recommendation systems where the input data is usually provided in the form of votes (edges) that users (vertices) express on products (vertices). Addi-tionally, graph-based computation finds application in many diverse and important fields such as social networks, com-putational biology, chemistry, and computer security. A key problem in graph computation is that it is often difficult to scale with increasing input data sizes as graphs are not easily partitionable into independent subgraphs that can be computed in parallel.

To be able to work on large datasets, distributed graph-computing (DGC) frameworks (such as GraphLab [18] or Pregel [20]) forcibly partition the input graph by placing its constituting elements, be they either vertices or edges, in dis-tinct partitions, one for each available computing resource. During the partitioning phase, data elements that share con-nections with other elements already placed in other parti-tions result in having remote connections amongst them. Since these partitions are usually placed on different ma-chines, this can incur unnecessary or excessive network and computation costs. To address this issue, one frequently used technique is to create and locally place replicas of re-motely connected data among these partitions. While this reduces the access cost, replicated data elements must be synchronized during computation so as to avoid replica states
We use scale-free and power-law graphs synonymously. from diverging and generating meaningless computation re-sults. This synchronization can significantly hinder perfor-mance as it forces replicas to coordinate and exchange data several times during computation.

The way the input dataset is partitioned has a large im-pact on the performance of the graph computation. A naive partitioning strategy may end up replicating a large frac-tion of the input elements on several partitions, severely hampering performance by inducing a large replica synchro-nization overhead during the computation phase. Further-more, the partitioning phase should produce evenly balanced partitions (i.e. partitions with similar sizes) to avoid pos-sible load skews in a cluster of machines over which the data is partitioned. Several recent approaches have looked at this problem. Here we focus our attention on stream-based graph partitioning algorithms, i.e. algorithms that partition incoming elements one at a time on the basis of only the current element properties and on previous assignments to partitions (no global knowledge on the input graph). Fur-thermore, these algorithms are usually one-pass , i.e. they refrain from changing the assignment of a data element to a partition once this has been done. These algorithms are the ideal candidates in settings where input data size and con-straints on available resources restrict the type of solutions that can be employed.

Other characteristics of input data also play an impor-tant role in partitioning. It has been shown that vertex-cut algorithms are the best approach to deal with input graphs characterized by power-law degree distributions [1, 12]. This previous work also clearly outlined the important role high-degree nodes play from a partitioning quality standpoint. Nevertheless, few algorithms take this aspect into account [27, 24]. Understandably, this is a challenging problem to solve for stream-based approaches due to their one-pass na-ture.

In this paper, we leverage the idea that a partitioning algorithm should do its best to cut, i.e., replicate , high-degree vertices. In particular, we introduce High Degree (are) Replicated First ( HDRF ), a stream-based graph par-titioning algorithm based on a greedy vertex-cut approach that leverages information on vertex degrees.
 HDRF is characterized by the following desirable properties: (i) it outputs partitions with the smallest average replica-tion factor among all competing solutions when applied on power-law graphs (Figure 2) while (ii) providing close to op-timal load balancing (Figure 3). The former is obtained by greedily replicating vertices with larger degrees, while the latter is provided by a parametrizable balancing term whose impact can be tuned to adapt the algorithm behavior to any data input order. On the one hand, lowering the average replication factor is important to reduce network bandwidth cost, memory usage and replica synchronization overhead at computation time. A fair distribution of load on partitions, on the other hand, allows a more efficient usage of avail-able computing resources. HDRF takes into account both of these aspects in an integrated way, significantly reduc-ing the time needed to perform computations on large-scale graphs.

Summing up, this paper provides the following contribu-tions:
The rest of this paper is organized as follows: we define the problem in Section 2; we briefly describe existing solutions in Section 3; we introduce HDRF in Section 4; we show theoretical bounds for HDRF in Section 5; we present the results of an extensive experimental evaluation in Section 6 and we conclude the paper in Section 7.
The problem of optimally partitioning a graph to mini-mize vertex-cuts while maintaining load balance is a funda-mental problem in parallel and distributed applications as input placement significantly affects the efficiency of algo-rithm execution [25]. An edge-cut partitioning scheme re-sults in partitions that are vertex disjoint while a vertex-cut approach results in partitions that are edge disjoint. Both variants are known to be NP-Hard [16, 11, 2] but have dif-ferent characteristics and difficulties [16]; for instance, one fundamental difference between the two is that a vertex can be cut in multiple ways and span several partitions while an edge can only connect two partitions.

One characteristic observed in real-world graphs from so-cial networks or the Web is their skewed power-law degree distribution: most vertices have relatively few connections while a few vertices have many. It has been shown that vertex-cut techniques perform better than edge-cut ones on such graphs (i.e., create less storage and network overhead) [12]. For this reason modern graph parallel processing frame-works, like GraphLab [19], adopt a vertex-cut approach to partition the input data over a cluster of computing nodes. The focus of this paper is on streaming vertex-cut partition-ing schemes able to efficiently handle graphs with skewed power-law degree distribution.
 Notation  X  Consider a graph G = ( V,E ), where V = ( v 1 ,  X  X  X  ,v n ) is the set of vertices and E = ( e 1 ,  X  X  X  ,e set of edges. We define a partition of edges P = ( p 1 ,..,p be a family of pairwise disjoint sets of edges (i.e. p i ,p p  X  p j =  X  for every i 6 = j ). Let A ( v )  X  P be the set of par-titions each vertex v  X  V is replicated. The size | p | of each partition p  X  P is defined as its edge cardinality, because computation steps are usually associated with edges. Since we consider G having a power-law degree distribution, the probability that a vertex has degree d is P ( d )  X  d  X   X   X  is a positive constant that controls the  X  X kewness X  of the degree distribution, i.e. the smaller the value of  X  , the more skewed the distribution.
 Balanced k-way vertex-cut problem  X  The problem consists in defining a partition of edges such that (i) the average number of vertex replicas (i.e. the number of parti-tions each vertex is associated to as a consequence of edge partitioning) is minimized and (ii) the partition load (i.e. the number of edges associated to a partition) is within a given bound from the theoretical optimum (i.e. | E | / | P | ) [2]. More formally, the balanced | P | -way vertex-cut partitioning problem aims at solving the following optimization problem: where  X   X  1 is a small constant that defines the system tol-erance to load imbalance. The objective function (Equation (1)) is called replication factor ( RF ), which is the average number of replicas per vertex.
 Streaming setting  X  Without loss of generality, here we assume that the input data is a list of edges, each identified by the two connecting vertices and characterized by some application-related data. We consider algorithms that con-sume this list in a streaming fashion, requiring only a single pass. This is a common choice for several reasons: (i) it han-dles situations in which the input data is large enough that fitting it completely in the main memory of a single comput-ing node is impractical; (ii) it can efficiently process dynamic graphs; (iii) it imposes the minimum overhead in time and (iv) it X  X  scalable, providing for straightforward parallel and distributed implementations. A limitation of this approach is that the assignment decision taken on an input element (i.e., an edge) can be based only on previously analyzed data and cannot be later changed.
Balanced graph partitioning is a well known NP-hard prob-lem with a wide range of applications in different domains. We do not discuss offline and edge-cut partitioning tech-niques since they are out of the scope of the paper. It is possible to divide existing streaming vertex-cut partitioning techniques in two main families: hashing and constrained partitioning algorithms and greedy partitioning algorithms. Hashing and constrained partitioning algorithms  X  All of these algorithms ignore the history of the edge as-signments and rely on the presence of a predefined hash function h : N  X  N . The input of the hash function h can be either the unique identifier of a vertex or of an edge. All these algorithms can be applied in a streaming setting and achieve good load balance if h guarantees uniformity. Four well-known existing heuristics to solve the partitioning problem belong to this family: hashing , DBH , grid and PDS . The simplest solution is given by the hashing technique that (pseudo-)randomly assigns each edge to a partition: for each input edge e  X  E , A ( e ) = h ( e ) mod | P | is the identifier of the target partition. This heuristic results in a large num-ber of vertex-cuts in general and performs poorly on power-law graphs [12]. A recent paper describes the Degree-Based Hashing ( DBH ) algorithm [27], a variation of the hashing heuristic that explicitly considers the degree of the vertices for the placement decision. DBH leverages some of the same intuition as HDRF by cutting vertices with higher degrees to obtain better performance. Concretely, when processing edge e  X  E connecting vertices v i ,v j  X  V with degrees d and d j , DBH defines the hash function h ( e ) as follows: Then, it operates as the hashing algorithm.

The grid and PDS techniques belong to the constrained partitioning family of algorithms [14]. The general idea of these solutions is to allow each vertex v  X  V to be replicated only in a small subset of partitions S ( v )  X  P that is called the constrained set of v . The constrained set must guaran-tees some properties; in particular, for each v i ,v j  X  V : (i) S ( v i )  X  S ( v j ) 6 =  X  ; (ii) S ( v i ) 6 X  S ( v j ) and S ( v | S ( v i ) | = | S ( v j ) | . It is easy to observe that this approach naturally imposes an upper bound on the replication factor. To position a new edge e connecting vertices v i and v j , it picks a partition from the intersection between S ( v S ( v j ) either randomly or by choosing the least loaded one. Different solutions differ in the composition of the vertex constrained sets. The grid solution arranges partitions in a X  X  Y matrix such that | P | = XY . It maps each vertex v to a matrix cell using a hash function h , then S ( v ) is the set of all the partitions in the corresponding row and column. It this way each constrained sets pair has at least two parti-tions in their intersection. PDS generates constrained sets using Perfect Difference Sets [13]. This ensure that each pair of constrained sets has exactly one partition in the in-tersection. PDS can be applied only if | P | = x 2 + x + 1, where x is a prime number.
 Greedy partitioning algorithms  X  This family of meth-ods uses the entire history of the edge assignments to make the next decision. The standard greedy approach [12] breaks the randomness of the hashing and constrained solutions by maintaining some global status information. In particular, the system stores the set of partitions A ( v ) to which each already observed vertex v has been assigned and the current partition sizes. Concretely, when processing edge e  X  E con-necting vertices v i ,v j  X  V , the greedy technique follows this simple set of rules: Case 1: If neither v i nor v j have been assigned to a par-tition, then e is placed in the partition with the smallest size in P .
 Case 2: If only one of the two vertices has been already assigned (without loss of generality assume that v i is the assigned vertex) then e is placed in the partition with the smallest size in A ( v i ).
 Case 3: If A ( v i )  X  A ( v j ) 6 =  X  , then edge e is placed in the partition with the smallest size in A ( v i )  X  A ( v j ). Case 4: If A ( v i ) 6 =  X  , A ( v j ) 6 =  X  and A ( v i )  X  A ( v e is placed in the partition with the smallest size in A ( v A ( v j ) and a new vertex replica is created accordingly. Symmetry is broken with random choices. An equivalent formulation consists of computing a score C greedy ( v i ,v for all partitions p  X  P , and then assigning e to the parti-tion p  X  that maximizes C greedy . The score consists of two elements: (i) a replication term C greedy REP ( v i ,v j balance term C greedy BAL ( p ). It is defined as follows: where maxsize is the maximum partition size, minsize is the minimum partition size, and is a small constant value.
A recent paper [24] proposes an hybrid solution that tries to combine both edge-cut and vertex-cut approaches to-gether. The resulting heuristic, called Ginger , aims at op-timizing the partitioning in a DGC framework. However, Ginger is not a streaming solution, since it needs extra re-assignment phases after the original streaming graph parti-tioning.

We remark there are other facets of graph partitioning that may affect performance of a DGC framework and have been addressed in other works. For example, some applica-tions are based on dynamic graphs and provided a hashing-based partitioning solution to manage such type of input [21]. Another aspect is the use of other metrics for opti-mization. For example [28] proposes a solution aimed at aggressively replicating vertices to improve the performance of queries on the graph and to keep them local to each single partition as much as possible. Further contributions along these lines are orthogonal and out of the scope of this paper.
In this section, we present HDRF , a greedy algorithm tai-lored for skewed power-law graphs.

In the context of robustness to network failure, Cohen et al. [7, 8] and Callaway et al [6] have analytically shown that if only a few high-degree vertices (hubs) are removed from a power-law graph then it is turned into a set of isolated clusters. Moreover, in power-law graphs, the clustering co-efficient distribution decreases with increase in the vertex degree [9]. This implies that low-degree vertices often be-long to very dense sub-graphs and those sub-graphs are con-nected to each other through high-degree vertices.

Our partitioning scheme leverages these properties by fo-cusing on the locality of low-degree vertices. In particular, it tries to place each strongly connected component with low-degree vertices into a single partition by cutting high-degree vertices and replicating them on a large number of parti-tions. As the number of high-degree vertices in power-law graphs is very low, encouraging replication for only these vertices leads to an overall reduction of the replication fac-tor.

Concretely, when HDRF creates a replica, it does so for the vertex with the highest degree. However, obtaining de-grees of vertices for a graph that is consumed in a stream-ing fashion is not trivial. To avoid the overhead of a pre-processing step (where the input graph should be fully scan-ned to calculate the vertex exact degrees), a table with par-tial degrees of the vertices can be maintained that is con-tinuously updated while input is analyzed. As each new edge is considered in the input, the degree values for the corresponding vertices are updated in the table. The partial degree values collected at runtime are usually a good indi-cator for the actual degree of a vertex since it is more likely that an observed edge belongs to a high-degree vertex rather than to a low-degree one. 2
More formally, when processing edge e  X  E connecting vertices v i and v j , the HDRF algorithm retrieves their par-tial degrees and increments them by one. Let  X  ( v i ) be the partial degree of v i and  X  ( v j ) be the partial degree of v The degree values are then normalized such that they sum
During experiments, we noticed no significant improve-ments in the algorithm performance when using exact de-grees instead of their approximate values. up to one: As for the greedy heuristic, the HDRF algorithm computes a score C HDRF ( v i ,v j ,p ) for all partitions p  X  P , and then assigns e to the partition p  X  that maximizes C HDRF . The score for each partition p  X  P is defined as follows: C The  X  parameter allows control of the extent of partition size imbalance in the score computation. We introduced this parameter because the standard greedy heuristic may result in highly imbalanced partition sizes, especially when the input is ordered somehow. To see this problem note that C BAL ( p ) (Equation 4) is always smaller than one, while C this reason, the balance term C BAL in the greedy algorithm or when 0 &lt;  X   X  1 is used only to choose among partitions that exhibit the same value for the replication term C REP thereby breaking symmetry.
 However, this may not be enough to ensure load balance. For instance, if the stream of edges is ordered according to some visit order on the graph (e.g., breadth first search or depth first search), when processing edge e  X  E con-necting vertices v i and v j there is always a single partition p 1) and all the other partitions p  X  P s.t. p 6 = p  X  C REP ( v i ,v j ,p ) = 0 (resp. C case, the balance term is useless as there is no symmetry to break, and the heuristic ends up placing all edges in a single partition p  X  . This problem can be solved by setting a value for  X  &gt; 1. In our evaluation (Section 6), we em-pirically studied the trend of the replication factor and the load balance by varying  X  (Figure 6). Moreover, note that when  X   X   X  the algorithm resembles a random heuristic, where past observations are ignored and it only matters to have partitions with equal size. The following summarizes the behavior of the HDRF algorithm with respect to the  X  parameter:
When  X  = 1 the HDRF algorithm can be represented by a set of simple rules, exactly as in greedy , with the exception of Case 4 that is modified as follows: Case 4 If A ( v i ) 6 =  X  , A ( v j ) 6 =  X  and A ( v i )  X  A ( v -if  X  ( v i ) &lt;  X  ( v j ), e is assigned to the partition with the smallest size p  X   X  A ( v i ) and a new replica of v j is created in p  X  ; -if  X  ( v j ) &lt;  X  ( v i ), e is assigned to the partition with the smallest size p  X   X  A ( v j ) and a new replica of v i is created in p  X  .

HDRF can be run as a single process or in parallel in-stances to speed up the partitioning phase. As with greedy , HDRF also needs some state to be shared among parallel instances during partitioning. In particular, we noticed that sharing the values of A ( v ),  X  v  X  V is sufficient to let HDRF perform at its best. Note that optimizing the execution time of HDRF was a goal beyond the scope of this work; we will consider it as part of our future work.
In this section we characterize the HDRF algorithm be-havior from a theoretical perspective, focussing on the ver-tex replication factor. In particular we are interested in an average-case analysis of HDRF . A worst-case analysis would provide poor performance, as expected for any sim-ilar greedy algorithm, while failing to capture the typical behavior of HDRF in real cases. In the rest of this section we assume  X  = 1 for the sake of simplicity.

Cohen et al. [8] considered the problem of a scale-free network (characterized as a power-law graph) attacked by an adversary able to remove a fraction c of vertices with the largest degrees. In particular they characterized the approx-imate maximum degree  X  M observable in the graph X  X  largest component after the attack. If | V | 1 /c this value can be approximated by the following equation: where m is the (global) minimum vertex degree and  X  is the parameter characterizing the initial vertex degree distribu-tion.

Let us now consider the algorithm aHDRF as an approx-imation of HDRF : aHDRF performs exactly as HDRF , but for the fact that we assume it knows the exact degree of each input vertex (and not the observed degree as for HDRF ).
Theorem 1. Algorithm aHDRF achieves a replication fac-tor, when applied to partition a graph with | V | vertices on | P | partitions, that can be bounded by: RF  X   X  | P | + 1 | V | (1  X   X  )
Proof. The replication factor bound is the sum of two distinct parts. The first part considers the fraction  X  of vertices with the largest degrees in the graph, referred to as hubs . The worst case for hubs is to be replicated in all the partitions, with a corresponding replication factor of  X  | P | .  X  represents the fraction of vertices that must be removed from the graph such that the maximum vertex degree in the remaining graph is | P | X  1; this value is obtainable through Equation (9) by imposing  X  M = | P | X  1.

The second part of the equation consider the contribution to the replication factor from non-hub vertices, i.e. all ver-tices whose degree is expected to be smaller than | P | X  1 after the  X  vertices with the largest degrees have been re-moved from the graph (together with their edges). When aHDRF processes an edge connecting a hub vertex with a non-hub vertices, it always favors the replication the hub vertex (that has a larger degree) and replicates the non-hub vertex only if executes Case 1 or Case 2 , that is only if it is the first time it observes that vertex. Since the degree of non-hub vertices, ignoring the connections with hub ver-tices, is bounded by m X  1 / (1  X   X  ) , and since the connections with hub vertices can produce at most one replica, the worst case replication factor for non-hub vertices is bounded by:
This bound can be further improved by considering what happens to the graph once the non-hub vertex v 0 with the largest degree is removed. The previous bound is valid for v . However, the removal of v 0 from the graph will change the degree distribution, thus also reducing the bound for the next non-hub vertex with the largest degree. Using this consideration, it is possible to iteratively bound the degree of each non-hub vertex v i with m (  X  + i/ | V | ) 1 / (1  X   X  ) 0  X  i  X | V | (1  X   X  )  X  1. Hence, the total worst case replication factor for non-hub vertices, is bounded by:
If edges arrive in random order, aHDRF gives an approx-imation of HDRF . In this case, the observed values for ver-tex degrees are a good estimate for the actual degrees. We can conclude that, assuming random order arrival for edges, HDRF is expected to achieve a replication factor, when ap-plied to partition a graph with | V | vertices on | P | partitions, of at most RF of Theorem 1.

For example, consider a graph with  X  = 2 . 2, | P | = 128, m = 1 and 1M vertices. The average-case upper bound for the replication factor of HDRF is  X  5 . 12 while the actual result it achieves is  X  1 . 37. The bounds for DBH and hash-ing [12, 27] with this configuration are respectively  X  5 . 54 and  X  5 . 88, while the actual results they achieve are  X  1 . 89 and  X  2 . 52.

The upper bound given by Theorem 1 cannot be extended to other algorithms (e.g., greedy ). Informally, HDRF breaks network at hubs by replicating a small fraction of vertices with large degrees. In contrast, greedy and other algorithms are agnostic to the degree of vertices when replicating them. Intuitively, these algorithms try to break network by remov-ing random vertices. Unfortunately, power-law graphs are resilient against removing random vertices (see [7] for de-tails). This implies that, in order to fragment a scale-free network, a very large number of random vertices should be removed. In other words, greedy and other algorithms tend to replicate a large number of vertices in different partitions. This intuition is verified in our experiments (see Section 6).
This section presents experimental results for the HDRF partitioning algorithm. The evaluation was performed on real-world graphs by running the proposed algorithm both in a stand-alone partitioner (useful for scaling up to large par-tition numbers) and running an implementation of HDRF (c) MovieLens 10M integrated into GraphLab 3 . The evaluation also reports ex-periments on synthetic graphs generated randomly with in-creasingly skewed distributions to study the extent to which HDRF performance is sensitive to workload characteristics. Evaluation Metrics  X  We evaluate the performance of HDRF by measuring the following metrics:
Replication factor: is the average number of replicas per vertex. This metric is a good measure of the synchronization overhead and should be minimized.

Load relative standard deviation: is the relative standard deviation of the number of edges hosted in target partitions. An optimal partitioning strategy should have a value for this metric close to 0.

Max partition size: is the number of either vertices or edges hosted in the largest partition. We consider this met-ric with respect to both vertices and edges as each conveys different information. Edges are the main input for the com-putation phase, thus more edges in a partition mean more computation for the computing node hosting it; conversely, the number of vertices in the system, and, therefore, in the largest partition, also depends on the number of replicas generated by the partitioning algorithm.
 Execution time: is the number of seconds needed by the DGC framework to perform the indicated computation on the whole input graph. Better partitioning, by reducing the number of replicas, is expected to reduce the synchronization overhead at runtime and thus reduce the execution time as well.
 Datasets  X  In our evaluation, we used as datasets both synthetic power-law graphs and real-word graphs. The for-mer were used to study how HDRF performance vary when the degree distribution skewness of the input graph gradu-ally increases. In particular, each synthetic graph was gen-erated with 1M vertices, minimum degree of 5 and edges using a power law distribution with  X  ranging from 1 . 8 to 4 . 0. Therefore, the number of edges in the graphs ranges
The stand-alone software and the GraphLab patch are available at https://github.com/fabiopetroni/VGP . from  X  60M (  X  = 1 . 8) to  X  3M (  X  = 4). Graphs were gen-erated with gengraph [26]. We also tested the performance of HDRF on real-world graphs: twitter-2010 from LAW (Laboratory for Web Algorithmics) [5, 4], Tencent Weibo from the KDD-Cup 2012 [22], Netflix from the Netflix Prize [3] and MovieLens 10M from the GroupLens research lab ( http://grouplens.org ). Table 1 reports some statistics for these 4 datasets.
 System Setup  X  We implemented a stand-alone version of a graph partitioner that captures the behavior of a DGC framework during the graph loading and partitioning phase. Within our partitioner, we implemented the five different al-gorithms described so far: hashing , DBH , grid , PDS , greedy and HDRF . Furthermore, we compared our solution against two offline methods: Ginger [24] and METIS [15], a well-known edge-cut partitioning algorithm. To compute the replication factor delivered by METIS, we used the same strategy of [12]: every edge-cut forces the two spanned par-titions in maintaining a replica of both vertices and a copy of the edge data. To run realistic tests needed to measure execution time, we implemented and integrated HDRF into GraphLab v2 . 2. Experiments with GraphLab where con-ducted on a cluster consisting of 8 machines with dual 16-core Intel Xeon CPUs and 128 GB of memory each. We experimented with 32, 64 and 128 partitions by running mul-tiple instances on a single machine.
 Data input order  X  Since the input dataset is consumed as a stream of edges, the input order can affect the perfor-mance of the partitioning algorithm. We considered three different stream orders as in [25]: random , where edges ar-rive according to a random permutation; BFS , where the edge order is generated by selecting a vertex uniformly at random and performing a breadth first search visit starting from that vertex; DFS , that works as BFS except for the visit algorithm that is depth-first search. All reported re-sults are based on a random input order unless otherwise mentioned in the text.
The experimental results reported in this section are or-ganized as follows: we first report on experiments that show the ability of HDRF to deliver the best overall performance in terms of execution time with the smallest overhead (repli-cation factor) and close to optimal load balance when exe-cuted on real-world graphs. We then study how HDRF per-formance is affected by changes in the characteristics of the input dataset and changes in the target number of parti-tions. Finally, the last set of results analyze the sensitivity of HDRF to input stream ordering. replication factor (b) solutions.
We first measured HDRF performance against other stream-ing partitioning algorithms on our set of real-world graphs. These experiments were run by partitioning the input graphs on a set of target partitions in the range [3 , 256] with our stand-alone partitioner. Figure 1 reports the replication factor that the considered partitioning algorithms achieve on different input graphs 4 . Moreover, Figure 2a provides a snapshot of the evaluation, by setting the number of target partition to 133, a number compliant with PDS constraints. It can be observed that HDRF is the algorithm that pro-vides the smallest replication factor for all the considered datasets.

In particular, for the Weibo dataset, characterized by large edge count differences among high-degree and low-degree vertices, it is possible to observe how HDRF and DBH are the best performers as they both exploit vertex degrees. In all the other datasets HDRF is always the best performer, albeit with larger absolute RF values. Summarizing, on the considered datasets HDRF achieves on average a replica-tion factor about 40% smaller than DBH, more than 50% smaller than greedy , almost 3  X  smaller than PDS , more than 4  X  smaller than grid and almost 14  X  smaller than hashing . We experimented with other datasets as well (i.e. arabic-2005 , uk-2002 , indochina-2004 from LAW, and Yahoo! Mu-sic from the KDD-Cup 2011). In all our test HDRF out-performs competing solutions, simultaneously guaranteeing close to perfect load balance (results omitted due to space constraints).

Next, we compared HDRF against two offline partition-ing algorithms: Ginger and METIS . Note that these offline solutions have full knowledge of the input graph that can be exploited to drive their partitioning choices. Figure 2b com-pares the replication factor achieved by these two solutions and HDRF (we maintain | P | = 133 to be coherent with Fig-ure 2a). We do not report the results for the twitter-2010 dataset since METIS produced greatly unbalanced parti-
Due to specific constraints imposed by the PDS and grid al-gorithms on the total number of partitions, their data points are not aligned with those of the other algorithms. Figure 3: Load relative standard deviation produced by different partitioning algorithms on the Movie-Lens 10M dataset. tions 5 , making a comparison on this dataset unfair. The poor performance of METIS was an expected result since it has been proved that edge-cut approaches perform worse than vertex-cut ones on power-law graphs [12]. However, HDRF outperforms Ginger as well, by reducing its replica-tion factor by 10% on average. In addition, HDRF has the clear advantage of consuming the graph in a one-pass fash-ion while Ginger needs several passes over the input data to converge. These results show that HDRF is always able to provide a smaller average replication factor with respect to all other algorithms, both streaming and offline, when used to partition graphs with power-law degree distributions.
Figure 3 reports the load relative standard deviation pro-duced by the tested streaming algorithms when run on the MovieLens 10M dataset with a variable number of target partitions (results for other datasets showed similar behav-ior so we omit them). The curves show that HDRF and greedy provide the best performance as the number of tar-get partitions grows. As expected, hashing provides well bal-
Note that the scope of Metis is to balance vertex load among partitions. Figure 4: Speedup in the execution time for the SGD algorithm on the Tencent Weibo dataset by ap-plying HDRF with respect to greedy and PDS , with 32 , 64 and 128 partitions. anced partitions, but it still performs worse that the other algorithms as its expected behavior with respect to load bal-ancing is only probabilistic. Grid performs similarly, even if its more complex constraints induce some skew in load. DBH and PDS are the worse performers, with load skew growing at a fast pace as the number of target partitions grows. Note that replication factor reflects communication cost, and edge-imbalance reflects workload-imbalance; pro-viding good performance with these two metrics means that HDRF can provide partitioning results that make the exe-cution of application algorithms more efficient.

To this end, we studied how much all of this translates to a performance improvement with respect to the execu-tion time. Since a DGC framework has to periodically syn-chronize all the vertex replicas, having fewer replicas in the system is expected to provide an advantage and to speed up the execution time. To investigate the impact of the different partitioning techniques we ran the Stochastic Gra-dient Descent (SGD) algorithm for matrix completion [17, 23] on GraphLab, using the Tencent Weibo dataset and 100 latent factors, with 32, 64 and 128 partitions respectively. Figure 4 reports the measured speed-up, obtained by using HDRF to partition the input over greedy and PDS 6 . The SGD algorithm runs up to 2  X  faster using HDRF as input partitioner with respect to greedy , and close to 3  X  faster than PDS . The actual improvement is larger as the number of target partitions grows. Moreover, the speedup is pro-portional to the gain in RF (see Figure 1a) and, as already shown in [12], halving the replication factor approximately halves runtime. Furthermore, having partitions with fewer replicas also help SGD to converge faster [23].

We tested the speedup for other datasets and algorithms as well, namely Single Source Shortest Path (SSSP), Weakly Connected Components (WCC), Page Rank (PR) and Al-ternating Least Squares (ALS). The results (not reported here due to space constraints) confirmed our intuitions: the speedup is proportional to both the advantage in replication factor and the actual network usage of the algorithm. The speedup it is larger for IO-intensive algorithms (e.g. SGD,
We needed to use respectively 31, 57 and 133 partitions, to fit PDS constraints.
 ALS and PR) and smaller for algorithm with less network IO (SSSP and WCC). None of the tests we conducted with HDRF showed a slowdown with respect to other solutions.
Our results show that HDRF is the best solution to parti-tion input graphs characterized by skewed power-law degree distributions. HDRF achieves the smallest replication factor with close to optimal load balance. These two character-istics combined make application algorithms execute more efficiently in the DGC framework.
We next analyze how the input graph degree distribution affects HDRF performance. To this end, we used HDRF to partition a set of synthetic power-law graphs. In doing so, we experimentally characterize the sensitivity of the average replication factor on the power-law shape parameter  X  , and on the number of partitions. Figure 5a reports the repli-cation factor improvement for HDRF with respect to other algorithms, expressed as a multiplicative factor, by varying  X  in the range [1 . 8 , 4 . 0] with | P | = 128 target partitions ( | P | = 133 and | P | = 121 for PDS and grid respectively). The curves show two important aspects of HDRF behavior: (1) with highly skewed degree distributions (i.e. small val-ues of  X  ), its performance is significantly better than greedy and other algorithms (with the exception of DBH); (2) with less skewed degree distributions, the performance of HDRF approaches that provided by greedy , while all the other so-lutions (including DBH ) perform worse. These results show how HDRF behavior approximates greedy  X  X  behavior as the number of high degree vertices in the input graph grows as in this case making a different partitioning choice on high-degree vertices is less useful (as there are a lot of them). Note that Gonzalez et al. [12] showed that the effective gain of a vertex-cut approach relative to an edge-cut approach actu-ally increases with smaller  X  . Our solution boosts this gain, not only with respect to constrained techniques but also over the greedy algorithm. Figure 5b reports the replication fac-tor, and clearly shows that HDRF is able to outperform all competing algorithms for all values of  X  . At the extremes, HDRF is better than DBH when  X  is very small and per-forms slightly better than greedy when  X  is very large.
A shortcoming of the standard greedy algorithm is its in-ability to effectively handle streams of input edges when they are ordered. If the input stream is ordered such that two subsequent edges always share a vertex, greedy always places all the edges and their adjacent vertices in a single partition, whatever the target partition number is. The fi-nal result is clearly far from being desirable as all of the computation load will be incurred by a single node in the computing cluster.

To overcome this limitation, we explicitly introduced the parameter  X  in the computation of the score for each par-tition in HDRF (Equations (6) and (8)), that defines the importance of the load balance in the edge placement deci-sion (Section 4). Figure 6 shows the result of an experiment run on the Netflix dataset, where the input stream of edges is ordered according to either a depth-first-search (DFS) or a breadth-first-search (BFS) visit on the graph. The figure shows the average replication factor (Figure 6a), the size of the largest partition expressed as number of contained edges (Figure 6b) and the size of the largest partition ex-replication factor pressed as number of contained vertices (Figure 6c) all while varying the value of  X  in the range [0 . 1 , 100] (log-log axes). All three figures report the performance obtained with the greedy , hashing and PDS algorithms as horizontal grey lines for reference.

With  X   X  1 HDRF behaves exactly as greedy (curves BFS and DFS): all edges are placed in a single partition and no vertex is replicated. This behavior is confirmed by the size of the largest partition that in this case contains exactly | E | edges and | V | vertices (Figures 6b and 6c). For  X  &gt; 1 the C
BAL factor starts to play a fundamental role in balancing the load among the available partitions: the average repli-cation factor for HDRF with both DFS and BFS inputs is just slightly larger than what is achievable with a random input 7 and still substantially lower than what is achievable with PDS or hashing (Figure 6a). At the same time, the size of the largest partition drops to its minimum (Figures 6b and 6c) indicating that the algorithm immediately delivers close to perfect load balancing (i.e. | E | / | P | edges per partition), while the number of vertices hosted in the largest partition
The difference is due to HDRF  X  X  usage of partial informa-tion on vertex degrees. Such values are not a good proxy of real vertex degrees if the input stream is not random. reaches its minimum. By further increasing  X  toward larger values, the effect of C BAL dominates the HDRF score com-putation and the algorithms behavior quickly approaches the behavior typical of hashing : large average replication factor, with close to perfect load balancing.

These results show that i) the C BAL term in HDRF score computation plays an effective role in providing close-to-perfect load balancing among partitions while keeping a low average replication factor, and ii) by setting the value of  X  slightly larger than 1, it is possible to let HDRF work at a  X  X weet spot X  where it can deliver the best performance, even when working on an ordered stream of edges. This last point makes HDRF particularly suitable for application settings where it is not possible to randomize the input stream before feeding it to the graph partitioning algorithm.
Distributed graph-computing frameworks provide program-mers with convenient abstractions to enable computation on large datasets. In these frameworks, system performance is often determined by the graph data partitioning strategy, which impacts the communication cost and the workload balance among compute resources. In this paper, we pro-posed HDRF , a novel stream-based graph partitioning algo-rithm for distributed graph-computing frameworks. HDRF is based on a greedy vertex-cut approach that leverages in-formation on vertex degrees. Through a theoretical analysis and an extensive experimental evaluation on real-world as well as synthetic graphs using both a stand-alone partitioner and implementation of HDRF in GraphLab, we showed that HDRF is overall the best performing partitioning algorithm for graphs characterized by power-law degree distributions. In particular, HDRF provides the smallest average replica-tion factor with close to optimal load balance. These two characteristics put together allow HDRF to significantly re-duce the time needed to perform computation on graphs and makes it the best choice for partitioning graph data. [1] R. Albert, H. Jeong, and A.-L. Barab  X asi. Error and [2] K. Andreev and H. R  X  acke. Balanced graph [3] J. Bennett and S. Lanning. The netflix prize. In [4] P. Boldi, M. Rosa, M. Santini, and S. Vigna. Layered [5] P. Boldi and S. Vigna. The webgraph framework i: [6] D. S. Callaway, M. E. Newman, S. H. Strogatz, and [7] R. Cohen, K. Erez, D. Ben-Avraham, and S. Havlin. [8] R. Cohen, K. Erez, D. Ben-Avraham, and S. Havlin. [9] S. N. Dorogovtsev and J. F. Mendes. Evolution of [10] C. Eaton, D. Deroos, T. Deutsch, G. Lapis, and [11] U. Feige, M. Hajiaghayi, and J. R. Lee. Improved [12] J. E. Gonzalez, Y. Low, H. Gu, D. Bickson, and [13] H. Halberstam and R. Laxton. Perfect difference sets. [14] N. Jain, G. Liao, and T. L. Willke. Graphbuilder: [15] G. Karypis and V. Kumar. A fast and high quality [16] M. Kim and K. S. Candan. SBV-Cut: Vertex-cut [17] Y. Koren, R. Bell, and C. Volinsky. Matrix [18] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, [19] Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, [20] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, [21] J. Mondal and A. Deshpande. Managing large [22] Y. Niu, Y. Wang, G. Sun, A. Yue, B. Dalessandro, [23] F. Petroni and L. Querzoni. Gasgd: stochastic [24] Y. C. R. Chen, J. Shi and H. Chen. Powerlyra: [25] C. Tsourakakis, C. Gkantsidis, B. Radunovic, and [26] F. Viger and M. Latapy. Efficient and simple [27] C. Xie, L. Yan, W.-J. Li, and Z. Zhang. Distributed [28] S. Yang, X. Yan, B. Zong, and A. Khan. Towards
