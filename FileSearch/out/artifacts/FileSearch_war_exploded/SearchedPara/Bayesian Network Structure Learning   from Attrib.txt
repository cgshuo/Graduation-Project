 In the past, researchers in data mining and machine learning area usually assume data many applications such as sensor network and user privacy protection. Uncertain data can be divided into two categories by uncertainty source: artificial uncertain data and inherent uncertain data. People sometimes add noise to data for some purpose such as user privacy protection. As a result, the da ta become artificially uncertain. There are also inherently uncertain data. For example the scientific measurement techniques and tools are inherently imprecise and they are responsible for the generation of inherent uncertain data. Many researchers focus on uncertain data management and mining in ____________________ models of uncertain data. The first one is tuple uncertainty model [1] [2]. Each tuple in a probabilistic database is associated with a probability which represents the likelihood the tuple exists in the relation. The second one is attribute uncertainty independent probability distribution. Correlated uncertainty model [5] is the third one. Attributes are described by a joint probability distribution. 
Many data mining algorithms have been proposed to analyze uncertain data, for example, mining frequent patterns from uncertain transaction database [2], na X ve Bayesian classifiers for correlated uncertain data [7] [8], and clustering uncertain objects [6]. However there are few works on data mining from attribute uncertain data, and to the best of our knowledge no work has focused on how to learn Bayesian Network ( BN ) structure from such data. Attribute independency is a common assumption in database and data mining area, but it is not always reasonable, because there are dependency relationships among attributes. Attribute uncertainty due to measurement error or inherent uncertainty shouldn X  X  be the reason for independence assumption. The structure learning from attribute uncertain data can reveal the essential relationship between attributes. 
In this paper, we propose the problem of BN structure learning from attribute uncertain data and an algorithm named DTAU to solve the problem. Experiments demonstrate the effectiveness of our proposed algorithm. The rest of the paper is organized as follows. Section 2 discusses related work on Bayesian Network structure learning for uncertain data. Section 3 gives relevant definitions. Section 4 introduces our structure learning algorithm DTAU . Section 5 is experimental study and Section 6 concludes the paper. Bayesian Network ( BN ) is a powerful tool to represent joint probability distribution over a set of variables or attributes. A BN is made up of two components: a directed acyclic graph ( DAG ), whose nodes represent variables and a set of conditional probability tables the DAG . Given a BN structure ( DAG ), there have been many algorithms for parameter learning. However, sometimes the BN structures are unknown for lack of domain knowledge. Thus the BN learning problem is of great importance. It has been proved that BN structure learning problem is NP completed [9]. 
Many researchers have proposed approximation algorithms to solve the structure learning from certain data problem. These methods are divided into two categories. The methods in the first category are based on information theory which is used to measure dependency relationships between nodes [10]. The methods [11] [12] in the second category aim to maximize score function of the possible structure considering bigger than 1, heuristic rules based methods are usually used. For the attribute uncertain data, we make use of the information theory to solve the problem and assume the structure is a tree. In this section we describe some concepts about the problem of learning BN structure from attribute uncertain data. The term observation is a concept in BN learning from certain data problem. Definition 1 ( Attribute ). An attribute X i is a component or aspect of an object O . X i can take any value in D ( X i ) which is the possible value domain of X i . D i represents the size of D ( X i ) . The attribute X i is represented by a node (a random variable) in the BN structure. function over D ( X i ) . Definition 3 ( Uncertain observation ). Given an attribute X i , an observation P i ( X i ) of X (1  X  i  X  m ) in an uncertain example ue is an uncertain observation. Definition 4 ( Attribute Uncertain training dataset ). An uncertain training dataset D is composed of uncertain examples, D = { ue 1 , ue 2 , ..., ue n }. 
In this paper we focus on the problem of learning a BN structure from attribute uncertain training dataset. We assume the structure of Bayesian Network is a tree. 
In the following parts of the paper we study the problem of learning structure from discrete attribute uncertain data. If they is continuous, we can discrete them. algorithm based on the exponential possible worlds. Then we will explain why the na X ve method is unacceptable. At last we will show our approximation method which takes polynomial time. Definition 5 ( Possible world ). Given an uncertain dataset about m attributes, it generates possible worlds, where each world is a certain dataset about the m attributes which has the same size of examples with the uncertain one. Each possible world W i is associated with a probability Pr ( W i ) that the world exists.

The na X ve method is based on p ossible w orld over a ll a ttributes ( PWAA ). The idea is converting the attribute uncertain dataset to some certain datasets. Given an W i is a possible value in the domain of the corresponding attribute. Those possible W as a certain training dataset, and then we learn a dependency tree under the corresponding training data set. The tree with the highest score can be recognized as the right one. The total number of trees (obviously containing the duplicates) is the number of the possible worlds. The score of a tree T i is  X  Wj.tree=Ti Pr ( W j ). 
The number of possible worlds is exponential, so the solution presented above costs exponential time complexity to construct the dependency tree. We propose an algorithm named DTAU (D ependency T ree learning from A ttribute U ncertain data) to construct a dependency tree without enumeration of possible worlds and reduce the enormous computation. Our idea is to make use of the attribute uncertain dataset directly. 
For a traditional certain training dataset, a popular way to construct the dependency tree with the closest probability approximation is called Chow-Liu tree [10]. The kernel idea in [10] is how to compute the dependency between each two attributes. The dependency between nodes X i and X j is measured by mutual entropy I ( X i , X j ). The computation is defined by Equation 1. The value of mutual entropy I ( X i , X j ) is always two attribute is weaker. The zero value means they are independent. We propose the DTAU algorithm to learn a dependency tr ee from attribute uncertain data. The DTAU algorithm is consistent with Chow-Liu tree under certain training data. The key point in the DTAU algorithm is how to compute the dependency between each two approximation of I ( X i , X j ). The two equations are from [10]. Equation 3, 4 and 5 shows how to approximate the frequency in equation 2 and we get the final approximation of I ( X i , X j ) by equation 3, 4 and 5. From the equations above we can learn that if the probability P ki ( X i =s ) is the highest occurrence probability for pair ( s , t ) may be the highest. The idea behind the equation is that the independence assumption doesn X  X  have effect on the overall dependency computation. In other words, if the two attributes are independent, the computation details of the proof for the limitation of space in this paper. 
The DTAU algorithm is divided into three step s. The first step is to compute the dependency between each two uncertain attributes and construct a weighted undirected graph. Then we follow the tree construction method in the Chow-Liu tree algorithm. The second step is to get a maximum spanning tree by a greedy algorithm which is 2-ratio approximation of the optimal tree. The last step is to add the direction by width first traverse. The pseudo-code is given below: 
The time complexity of DTAU algorithm is O ( nm 2 ), which is smaller than the na X ve bigger than t , the dependency tree created by the DTAU method is the same with the one in the possible world with the highest probability. Because if the t-condition is satisfied, the partial orders for all mutual entropy are the same. The partial orders can proof or the computation of t for the limitation of space. Experiments show that our method performs well even when the t -condition can X  X  be satisfied. As there hasn X  X  been any public attribute uncertain dataset, the attribute uncertain datasets we use are generated from certain datasets artificially. We generate the attribute uncertain datasets by adding noise to the UCI machine learning datasets which are standard for traditional BN learning problems. We convert the Letter recognition and Balance datasets to attribute uncertain datasets. The noise addition strategy is described as follows. For each training example in the original certain dataset, we assign a probability p which is not smaller than a bound  X  to the corresponding attribute X  X  observation in the original certain training example, and example of the original certain dataset, where D ( Attribute 1) = { a , b , c } and addition with  X  being 0.5. By this way we get attribute uncertain training datasets denoted by AU-Letter- X  , and AU-Balance- X  respectively. 
For each uncertain dataset we generate, th e uncertain observations in an attribute uncertain dataset are closer to the ones of the original certain dataset when  X  is closer dependency measure (information entropy) in attribute uncertain data is almost the same with the one in the certain dataset. Figure 1 shows the tree we learn from AU-Letter-0.5 and the Chow-Liu tree learned from the certain dataset. 
The two trees share the black solid edges. Edge&lt; X 13 , X 12 &gt; belongs to the tree from uncertain dataset and edge &lt; X 4 , X 14 &gt; belongs to the tree from certain dataset. We find less than 1.3%. 
We design experiments on AU-Balance to demonstrate the effectiveness of the dependency tree by clustering results. We use DTAU algorithm to learn a dependency tree from AU-balance data. Then we generate a certain sample dataset i for the uncertain dataset and the dependency tree as BN structure to learn the joint probability distribution on all attributes in the uncertain dataset and then the uncertain example ue i turns to be an uncertain object. We cluster those uncertain objects by algorithm UK -means [4] and the original certain objects by K -means and compare the two clustering results. We do experiments on the AU-Balance dataset with different values of  X  . 
We test three different values of parameter  X  , 0.6, 0.8 and 0.9. For each dataset, the different uncertain dataset. The numbers in column 2, column 3 and column 4 represent the size of correct examples in the corresponding cluster. The measure precision shows the percentage of correct cl ustered examples. From this table we can certain one, for the certain one is always the possible world with the highest probability. The experiments show that the dependency tree generated by our method is acceptable and  X  is an important factor to the cluster results. In this paper we propose the Bayesian Network structure learning problem on attribute uncertain dataset, and we propose algorithm DTAU by which we can learn a dependency tree in polynomial time. We conducted experiments to demonstrate the effectiveness of our proposed algorithm. The experiment results show the dependency further analyze the effect of parameters  X  . Acknowledgement. This work was supported by the NSFC under Grant No. 70871068 and 71110107027 and the Major National Sci. and Tech. Project of China under Grant No. 2010ZX01042-002-002-03. 
