 Keyword based search interfaces are extremely popular as a means for e ffi ciently discovering items of interest from a huge collec-tion, as evidenced by the success of search engines like Google and Bing. However, most of the current search services still return results as a flat ranked list of items. Considering the huge num-ber of items which can match a query, this list based interface can be very di ffi cult for the user to explore and find important items relevant to their search needs. In this work, we consider a search scenario in which each item is annotated with a set of keywords. E.g., in Web 2.0 enabled systems such as flickr and del.icio.us, it is common for users to tag items with keywords. Based on this anno-tation information, we can automatically group query result items into di ff erent expansions of the query corresponding to subsets of keywords. We formulate and motivate this problem within a top-k query processing framework, but as that of finding the top-k most important expansions. Then we study additional desirable proper-ties for the set of expansions returned, and formulate the problem as an optimization problem of finding the best k expansions satis-fying all the desirable properties. We propose several e ffi cient al-gorithms for this problem. Our problem is similar in spirit to recent works on automatic facets generation, but has the important di ff er-ence and advantage that we don X  X  need to assume the existence of pre-defined categorical hierarchy which is critical for these works. Through extensive experiments on both real and synthetic datasets, we show our proposed algorithms are both e ff ective and e ffi cient. H.3.4 [ Information Storage and Retrieval ]: Systems and Soft-ware -Information Networks Algorithms Top-k Query Processing, Top-k Query Expansions, Semantic Re-dundancy, E ffi ciency, Keywords and Tags
Keyword based search interfaces are extremely popular as a mea-ns for e ffi ciently discovering items of interest from a huge collec-tion, as evidenced by the success of search engines like Google [5], Bing [2] and Yahoo! [7]. However, most of the current search ser-vices still return results as a flat ranked list of items. As has been found by a recent study [8], this list-based interface can make it very di ffi cult for the user to explore and find important items rel-evant to their search needs. We claim that automatically grouping search results into semantically independent  X  X opics X  can signifi-cantly enhance the usability of the results . Below, we discuss a few motivating examples to illustrate this point.
 Consider an academic search engine like Google Scholar [1]. Though it works perfectly for those queries which target a specific paper, when handling a general purpose query like  X  X ind all papers which are relevant to the topic of database histogram X , the search engine returns a huge list of papers ranked by their relevance to the query. This ranked list is di ffi cult for user to work from for exploring papers related to the query and e ffi ciently finding the pa-pers they want. It is clear that the user may benefit significantly if the search service can automatically group all the papers into se-mantically independent  X  X opics X . As a second example, consider search on social annotation websites like Del.icio.us [3] and Flickr [4]. These websites have rich user generated metadata for each item, however current search engines on these websites only uti-lize them to generate a ranked list of items for a query based on keyword relevance. In case of Del.icio.us, search results are pre-sented using a faceted interface, but this is based on expanding the user X  X  keyword query with one of a fixed set of tags. E.g., search-ing on the tag  X  X rogramming X  returns more than 1.3 million hits on Del.icio.us, however, only three single word tags  X  X jax X ,  X  X oft-ware X  and  X  X avascript X  are suggested to expand the query, while many other useful expansions such as  X  X  ++ programming tutorial X  and  X  X atabase programming language X  cannot be found on the in-terface. As a third example, consider a community question an-swering forum like Quora [6]. A user may wish to search the Q&amp;A in Quora using keywords and it is often helpful for the system to present the search results in an automatically grouped form, where di ff erent groups somehow correspond to di ff erent  X  X ubtopics X  of the  X  X opic X  that the user may have questions about.

Indeed, to help users explore the returned search results, cur-rent search engines like Google, Bing and Yahoo! often add to the search result interface a set of facets, like publishing time, size of document, price etc. Faceted interfaces can greatly facilitate user navigation through the results. However, these facets are often pre-defined, and may not capture the attributes of the item which are the most important. E.g., for Google Scholar, current facets for the search results contain only types of publication and publica-tion date, whereas the users may want to explore  X  X opics X  of pa pers among the search results. Similar comments apply to the other two example search scenarios above.

Our problem is similar in spirit to recent works on automatic facets generation [19], but has the important di ff erence and ad-vantage that we don X  X  need to assume the existence of pre-defined categorical hierarchy which is critical for these works. Indeed, in the applications we consider such as above, we cannot assume any prior taxonomy.

Motivated by these drawbacks of current search result interfaces, we consider a search scenario in which each item is annotated with a set of keywords. These can be keywords associated with pa-pers or tags assigned to items by users of social annotation sys-tems or keywords occurring in question and answers in Q&amp;A sys-tems. Based on this annotation information, we want to automati-cally group query result items into di ff erent expansions of the query corresponding to subsets of keywords. Items may have a number of attributes, either explicitly stored or computed, which can signify their importance or utility to the user. E.g., papers have citation score, pagerank of their authors, etc. In social annotation systems, popularity of items and pagerank of the annotator can be important attributes. Finally, in a system like Quora, we have attributes like importance scores of questioner or answerer, number of people in-terested in a question, etc. Intuitively, the expansions that we wish to return to a user should be driven by how important the items are that match an expansion and how many such important items match it. This raises the problem of what expansions of a query should we show the user? We formulate and motivate this problem within a top-k query processing framework, but as that of finding the top-k most important expansions, where the importance or utility of an expansion is driven by the utility of the items matching it.
We make the following contributions:
Related work is discussed in Section 6. We summarize the work in Section 7 and discuss open research problems.
Consider a set of items S = { t 1 ,..., t n } , each item t sociated with a set of m attributes { a 1 ,..., a m } . We denote the value work, without loss of generality, we assume large attribute values are preferred, so larger the value larger the utility. The overall util-values on all attributes, i.e., u ( t i ) = P j  X  1 ... m positive weight associated with attribute a j . These weights may be chosen by a user or the system may learn the  X  X est X  weights from user behavior using models like linear regression [10].
W e assume each item t i  X  S is also associated with a set of l keywords or tags { k 1 ,..., k l } which summarize the contents of t denote the set of keywords associated with an item t i as K query Q in our system is a set of keywords, Q = { k 1 ,..., k item t matches query Q i ff Q  X  K w ( t ). The answer to Q , denoted S , is then the set of matching items in S , i.e., S Q = { t | t  X  S , Q  X  K w ( t ) } .

As mentioned in the introduction, the number of items match-ing a query Q can often be huge, and merely returning the top-k matching items may not help the user find the items they are most interested in. So we want to group items into di ff erent expansions of Q and return high quality expansions. Let K denote the set of all keywords in the system. We call a subset of keywords e  X  K  X  Q an expansion 1 of Q . The size of an expansion e , | e | , is the number of keywords in e . Let the set of all possible expansions for Q be E , then given an expansion e  X  E Q , we say an item t  X  S matches e if Q  X  e  X  K w ( t ). Let S e denote the set of matching items for e , then we have S e = { t | t  X  S , Q  X  e  X  K w ( t ) } .

Given a query Q , all possible expansions of Q can be orga-nized as a lattice based on the subset-of relationship, E.g., Figure 1 shows the lattice structure of all possible expansions for K  X  Q = { k ,..., k 4 } . We will shortly define the quality of an expansion, and we will show in Section 3.2 that this lattice structure can be used to improve the e ffi ciency of our algorithm for determining high qual-ity expansions.

Intuitively, the importance of an expansion e can be captured by aggregating the utilities of items which match e . Let g be a monotone aggregation function which calculates the overall utility or quality of expansion e , i.e., define u ( e ) : = g ( { u ( t ) | t  X  S we can define our top-k expansion problem as follows.

D efinition 1 (T op -k E xpansions ). Given a set S of items and a keyword query Q, find the top-k expansion set E k = { e 1  X  e  X  E k and  X  e  X   X  E Q  X  E k , u ( e )  X  u ( e  X  ) .

The intuition is that in response to a user query, we want to return the best quality expansions where the quality of an expansion is monotonically determined by the utility of the items matching it.
The set of annotations used in this paper is summarized in Ta-ble 1. Below, we give an example of an aggregation function with certain desirable properties.
To determine the utility u ( e ) of expansion e , a natural idea is to use a monotone function to aggregate utility values of all items which can match e . However, this approach means for every ex-pansion, we may need to retrieve all items that are relevant to this expansion which can be prohibitive considering the huge size of matching items. Besides, low quality items matching e intuitively should not determine its importance. So we will only consider top-1 N ote, actually the real expansion is Q  X  e but for technical conve-nience, we deal with the part of the expansion outside Q .
Symbol Description K w ( t ) the set of keywords associated with t L K ( e ) the sub-lattice induced by e N m atching items for determining importance of an expansion e , where N is a parameter that is tuned for each application.
Let V S e = { u ( t ) | t  X  S e } be the multiset of utility values of all items matching e . We will apply a function top N to V retrieves the top-N highest utility values from V S e . Then to deter-mine the importance of e , we will sum up all values in top properties for determining the importance of an expansion, as g ( S will be large when there are high quality items matching e and also when the number of these high quality items is large.

It can be easily shown that the aggregation function g defined is subset-monotone , which means for two expansions e 1 and e S 1  X  S e 2 , then g ( e 1 )  X  g ( e 2 ). And it is worth noting that the algorithms proposed in this work can be easily adapted to other subset-monotone score functions.

On top of the basic top-k expansion problem, we will want to impose some additional desirable properties for the k expansions returned by our algorithms. In Section 4, we study these proper-ties and propose additional algorithms which can return expansions satisfying these properties.
In this section, we present two algorithms for finding top-k ex-pansions. In order to facilitate both algorithms, we assume m in-verted lists are materialized, one each for the m attributes, where for each inverted list, items are sorted in the non-increasing order of their value on the corresponding attribute.
Inspired by the NRA algorithm proposed in [13], a na X ve way of generating top-k expansions can be described as follows: 1. access items in the non-increasing order of their attribute value; 2. for each matching item accessed, enumerate all possible expansions and update their lower bound and upper bound utility value; 3. stop the iterative process once top-k expansions have been identified. The pseudo-code for the above process is given in Algorithm 1.
In Algorithm 1, all attribute lists are accessed in a round-robin fashion (line 4), and for each matching item t obtained from list I we will enumerate the set E t of all possible expansions (line 5 X 7).
Consider an expansion e  X  E t , let S S e be the current set of ac-cessed items which can match e . Then for each t  X  S S e , because all attribute values are normalized to [0 , 1] and items are accessed in the non-increasing order of their attribute values, similar to the NRA algorithm [13], we can determine the lower bound on t  X  X  util-ity u ( t ) by summing up all current accessed attribute values of t , and we can determine its upper bound utility  X  u ( t ) by summing up all accessed attribute values of t along with the last accessed values of other attributes in the inverted lists.

Then because of the subset-monotonicity of the score function g , the lower bound utility u ( e ) for expansion e  X  E the upper bound utility  X  u ( e ) of an expansion e can be estimated by considering both S S e and the maximum utility value which can be achieved by any unseen items. Let the last accessed values on each are accessed in the non-increasing order of their attribute value, an  X  X maginary X  item t  X  with m attribute values  X  v 1 ,...,  X  v maximum value that can be achieved by any unseen items. So the upper bound utility of e can be estimated as the sum of all values in top N ( {  X  u ( t ) | t  X  S S e } X  X  u ( t  X  1 ) ,..., u ( t  X  imaginary items which have the same utility as t  X  .

After the lower and upper bounds for all expansions in E t been updated (line 8 X 10), we can estimate the upper bound util-ity for all possible unseen items by summing up the values of N maximum possible imaginary items (updateUpperBound(  X  ) in line 12).

Henceforth, by the value of an expansion, we mean its utility value. Let EQ be the expansion query which contains the set of expansions of Q that have been materialized by the algorithm at a given point. We can rank all expansions in EQ by their lower bound values. Let EQ k be the top-k expansions in EQ and UB  X  be the maximum upper bound value of an expansion in EQ  X  EQ then the maximum of UB  X  and the upper bound value for all unseen items determines the overall upper bound value UB (line 11 X 12). If the lower bound value for the k th expansion in EQ k is already larger than UB , the algorithm can be safely terminated, exactly in the spirit of NRA.

Algorithm 1: T opExp-Naive( Q , I , g , k ) 1 E Q  X  expansion queue; 2 UB  X  upper bound threshold; 3 while | EQ | &lt; k OR u ( EQ . k th expansion ) &lt; UB do 4 I a  X  g etNextListRR(); 5 t  X  I a .getNextItem(); 6 if Q * K w ( t ) then continue; 7 E t  X  enumerateExpansion(K w ( t )  X  Q ); 8 foreach e  X  E t do 9 if e &lt; E Q then EQ .push( e ); 10 e .updateLower&amp;UpperBound( t , g ); 11 U B  X   X  maximum upper bound value of an expansion in 12 UB  X  MAX( UB  X  , updateUpperBound(  X  ));
The correctness of Algorithm 1 easily follows from the fact th at both lower bound and upper bound of an expansion are correct. And to assess the performance of Algorithm 1, we borrow the no-tion of instance optimality as proposed by Fagin et al. in [13].
D efinition 2. Instance Optimality: Let A be a class of algo-rithms that make no random accesses to the m inverted lists, and let I be a class of problem instances. Given a non-negative cost measure cost ( A , I ) of running algorithm A  X  A over I  X  I , an algorithm A  X  A is instance optimal over A and I if for every A  X   X  A and every I  X  X  we have cost ( A , I )  X  c  X  cost ( A constants c and c  X  . Constant c is called the optimality ratio.
Let the cost of an algorithm for the top-k expansion problem be determined by the number of items accessed, we first show the following result.

L emma 1. Given any instance I of the top-k expansion problem and any algorithm A with the same access constraints as TopExp-Naive, assume A accesses x items, then TopExp-Naive will access at most m  X  x items.
 P roof . (Sketch) Assume another algorithm B  X  A stops in list I after accessing x items, then it must be true that at the time when B stops, the current utility of the k th maximum utility expansion e will have its utility larger than or equal to the upper bound util-ity of all generated non-result expansions and all possible unseen expansions. This is because otherwise, we can come up with an configuration of the remaining unseen items such that we can have an expansion of which the utility is larger than e k . Then it is clear that our algorithm TopExp-Naive will also stop at the same position in list I a . By considering the fact there are m lists in total, we can infer that the total number of items accessed by TopExp-Naive is at most m  X  x , for the scenario where B accesses x items in list I zero item in other lists, and TopExp-Naive accesses x items in each list.

T h eorem 1. Let I be the class of all top-k expansion problem instances, and A be the class of all possible algorithms that find the top-k expansions, that are constrained to access items sequentially in non-increasing order of their attribute values, then TopExp-Naive is instance optimal over A and I with an optimality ratio of m.
E x ample 1. We show an example of algorithm TopExp-Naive in Figure 2. In this example, for simplicity of presentation, we assume all 4 items t 1 ,..., t 4 can match the query Q , so Q will be ignored from the keyword list of all items. Furthermore, we as-sume N = 1 which means we are using the best item to determine the importance of each expansion, and k = 1 which means we are looking for top-1 expansion. Keywords associated with each item are shown in Figure 2 (a), and each item in the example has two attributes a 1 and a 2 . The inverted lists for these two attributes are shown in Figure 2 (b).

As described in TopExp-Naive, the algorithm will enumerate all possible expansions for each item accessed, e.g., after t bound values will be updated using the attribute value of t this case, because only t 1 . a 1 is known, we know  X  u ( t u ( t 1 ) = 0 . 9 , these will be the upper and lower bound utility value for all three expansions. After we have accessed the first two items of both lists, the expansions generated are shown in Figure 2 (c), while the items contained in each generated expansion and the util-ity bound values for each generated expansion are shown in Fig-ure 2 (d). It is clear that at this moment, the upper bound utility for all generated expansions is 1.6, the upper bound utility for all un-seen expansions is 1.4 (sum of the last accessed utility value from each list) and the lower bound utility for expansions { k { k , k 4 } are all 1.6, so we can stop the algorithm now and return any one of these three expansions.
O ne serious drawback of the na X ve algorithm is that every time an item t containing keywords K w ( t ) is accessed, all 2 | sible expansions for this item are explicitly enumerated and their bounds are maintained. In this section, we propose an e ffi cient al-gorithm which can leverage the lattice structure of expansions to avoid enumerating and maintaining unnecessary expansions. We also address the challenge of determining the bounds of unseen (un-materialized) expansions, which is necessary for early termination of the top-k algorithm.
Given a query Q and a newly accessed item t , let e t = K Q be the largest size expansion in E t , then the na X ve algorithm will enumerate all possible expansions of t by considering all non-empty subsets of e t . However, this may not be necessary. E.g., let K &lt; t be the set of keywords which have been seen before t ; if as all other expansions generated from e t will have the same current matching itemset as e t and thus the same lower and upper bounds as e . This indicates that there are opportunities to avoid the expansion enumeration process for each newly accessed item.

Let L K be the lattice of all possible keywords K . For an ex-pansion e  X  L K , we let L K ( e ) denote the sub-lattice induced by e , algorithm will enumerate all expansions e  X  L K ( e t ), however, as discussed above that this isn X  X  always necessary.
 The idea of the new algorithm can be described as follows. Let L denote a partially materialized lattice which contains the set of expansions generated so far before the current item t . If  X  e  X  L s.t. e = e t , then it is clear we just need to update the lower bound and upper bound utilities of all existing expansions in L which are subsets of e t . Otherwise, all expansions in L will correspond to di ff erent sets of items compared with e t , so we need to first generate the expansion e t and update its lower bound and upper bound utility. Then we consider the following two cases: 1. if  X  e  X  L , e  X  e  X  , then all e  X   X  L K ( e t ) correspond to the same set of items, their lower bound and upper bound utilities are the same, and we just need to maintain one expansion e t which can concisely represent all expansions in L K ( e t ); 2. On the other hand, if there exists an expansion e  X  X  s.t. e  X  e t ,  X  , then for each such expansion e , we need to further consider the following three sub-cases: 1. if e  X  e t , we don X  X  need to generate additional expansions, 2. if e t  X  e , similar to case 1, we don X  X  need to generate ad-
Algorithm 2: T opExp-Lazy( Q , I , g , k ) 1 U B  X  upper bound threshold; 2 L X  partial materialized lattice structure; 3 while | EQ | &lt; k OR u ( EQ . k th expansion ) &lt; UB do 4 I a  X  g etNextListRR(); 5 t  X  I a .getNextItem(); 6 if Q * K w ( t ) then continue; 7 e t  X  K w ( t )  X  Q ; 8 if e t  X  X  then 9 foreach e  X  { e | e  X  X  X  e  X  e t } do 10 e . updateLower&amp;UpperBound( t ); 11 else 12 T Q  X  temporary update expansion queue; 13 T Q .push( e t ); 14 while  X  T Q.empty() do 15 e = T Q .pop(); 16 E l  X  X  e  X  | e  X   X  X  X  (  X  e  X  X   X  X  : e  X   X  e  X  X  ) } ; 17 foreach e l  X  E l  X  e l  X  e ,  X  do 18 E  X  { e  X  | e  X   X  X  X  e  X   X  e l } ; 19 if  X  e  X   X  E s.t. e  X  * e  X  e * e  X  then 20 F ind all e  X   X  E s.t. e  X  * e  X  e * e  X  and 21 T Q .push( e  X  e  X  ); 22 if e &lt; L t hen L .add( e ); 23 foreach e  X  L X  e  X  e t do 24 e . updateLower&amp;UpperBound( t ); 25 U B  X  upper bound value of the ( k + 1) th expansion in L ; 26 UB  X  MAX( UB , getUpperBound(  X  )); 3. if e * e t o r e t * e , let e  X  = e  X  e t , then it is clear that S
Since we don X  X  explicitly maintain all possible expansions for each item accessed, this will create a challenge for determining lower bound and upper bound utilities for all possible expansions. For all expansions which are maintained in L , the lower bound and upper bound are determined as discussed in Section 3.1. For each remaining expansion e not materialized in L , depending on the po-sition of e in the lattice L K , we need to consider the following two cases:
E xample 2. Consider the lattice in Figure 1 and assume we have only materialized two expansions e 1 = { k 1 } and e 2 = { k Then for an expansion e 3 = { k 2 , k 4 } , because there is no such ma-terialized expansion e  X   X  L s.t. e 3  X  e  X  , then we know we haven X  X  accessed any item which corresponds to this expansion. So we only need to consider its upper bound utility, which is the maximum possible utility for all possible expansions. And for another un-materialized expansion e 4 = { k 1 , k 2 } , we can find out that e so e 4 and e 2 correspond to the same set of items, and e bounds are the same as those of e 2 .

So the general idea of our l azy expansion generation based algo-rithm is that we only need to maintain expansions which correspond to a unique set of items. For a set of expansions which are matched by the same set of items, we can simply represent them using the largest expansion in the set.
The pseudo-code for the lazy expansion generation based algo-rithm is given in Algorithm 2. Similar to TopExp-Naive, TopExp-Lazy iteratively retrieves items from the attribute lists (line 4 X 6). However, unlike TopExp-Naive, we maintain only necessary ex-pansions in the partially materialized lattice L . For a newly ac-cessed item t , if e t has already been generated by a previous item, we simply update the itemset and lower / upper utility bounds of the corresponding expansions which contain this item (line 8 X 10). Oth-erwise, as discussed above, we may need to generate some addi-tional expansions which correspond to a unique matching itemset (line 11 X 24). The procedure works as follows. First, we identify from L all leaf expansions E l which are maximal expansions, i.e., they are not included in other expansions in L (line 16). Then for each leaf expansion e l  X  E l , if e t doesn X  X  overlap with e ignore all expansions which are subsets of e l as they can X  X  overlap E of expansions in L which are subsets of e l : if there is an expan-sion e  X   X  E s.t. e  X  * e t and e t * e  X  , then we need to find all of the largest such expansions e  X  in E , and as described in Section 3.2.1, for each of them, a new expansion e  X   X  e t needs to be inserted to L as it corresponds to a unique set of matching items, so we will recursively insert this new expansion into L (line 19 X 21).
The procedure for updating lower bound and upper bound utili-ties for each expansion is very similar to TopExp-Naive. However, because in TopExp-Lazy each expansion may represent more than one expansion, in order to determine which expansions in the ex-pansion bu ff er L are current top-k expansions, we need to calculate for each expansion e  X  L the exact number of ungenerated expan-sions which have the same utility bounds as e . The pseudo-code for this procedure are listed in Algorithm 3 and Algorithm 4.
The idea of Algorithm 3 is that we first find from L the expansion set S T e of which the expansions are subset of e (line 1), we prune away those expansions in S T e of which a superset is also present in S T e (line 2 X 3), then for the pruned expansion set S T e , we can use the classical inclusion-exclusion principle to count the total number of expansions covered by S T e (line 4). Algorithm 4 is a simple implementation of the counting procedure.

E xample 3. Figure 3 shows an example that illustrates how Al-gorithm TopExp-Lazy works. The configuration of this example, including query, item attributes, item attribute values, keywords of
Algorithm 3: u pdateCount( L , e ) 1 S T e  X  X  e  X  | e  X   X  L  X  e  X   X  e } ; 2 S T  X  e  X  X  e  X  | e  X   X  S T e  X  X  X  e  X  X   X  E  X  e  X   X  e  X  X  } ; 3 S T e = S T e  X  S T  X  e ; 4 count  X  countGeneratedExpansions( S T e ); 5 e .count  X  2 | e |  X  count  X  1;
Algorithm 4: c ountGeneratedExpansions( S T e ) 1 c ount  X  0; 2 for outeridx from 2 to | S T e | do 3 S T  X  e =  X  ; 4 for inneridx from 1 to outeridx -1 do 5 S T  X  e .insert( S T e [ outeridx ]  X  S T e [ inneridx ]); 8 if S T  X  e .hasOverlap() then 9 c ount += countGeneratedExpansions( S T  X  e ); 10 else 11 foreach e  X   X  S T  X  e do 12 c ount += 2 | e  X  | -1; each item and parameters k , N , is the same as Example 1. How-ever, because we are using the lazy expansion generation based algorithm, we don X  X  need to enumerate all possible expansions for each item accessed. E.g., when the first item t 1 is accessed, we only need to generate expansion { k 1 , k 2 } and don X  X  need to gener-ate expansions { k 1 } and { k 2 } , as they correspond to the same cur-rent set of matching items as { k 1 , k 2 } . The utility bound values for { k , k 2 } will be the same as in the TopExp-Naive algorithm, and again we don X  X  need to maintain these utility bound values for { k and { k 2 } since they are the same as for { k 1 , k 2 } . After accessing two items from each lists, the expansions materialized for TopExp-Lazy are shown as bolded expansions in Figure 3 (c). Compared with TopExp-Naive, it X  X  worth noting that 5 expansions don X  X  need to be maintained. At this point, similarly to TopExp-Naive, the al-gorithm can also stop as the top expansion { t 3 , t 4 }  X  X  lower bound utility is already larger than or equal to the maximum upper bound utility for all expansions.

Note that for the lazy expansion generation based algorithm, for each expansion which needs to materialized, we need to use Algo-rithm 3 to count how many expansions correspond to the same set of items. E.g., after accessing t 3 in the inverted list of a consider how many expansions are  X  X overed by X  the current expan-sions which are subsets of { k 1 , k 2 , k 3 } , for this case, { k Then as in line 4 of Algorithm 3, Algorithm 4 will be called to enu-merate the number of non-empty expansions covered by these two expansions. Because there is no overlap between { k 1 , k in line 10 X 12 of Algorithm 4, we can simply sum up the number of non-empty expansions covered by these two expansions, which is 4. Then this number will be used to determine the number of non-empty expansions covered by { k 1 , k 2 , k 3 } in line 5 of Algorithm 3, which is 3 ( { k 1 , k 2 , k 3 } , { k 1 , k 3 } and { k 2
Figure 3 (d) shows all the expansions that should be considered and whether they are  X  X overed by X  some other expansions in the lattice. Compared with Figure 2 (d), it is clear that the lazy ex-pansion generation based algorithm will maintain just one expan-sion for each set of expansions which correspond to the same set of items.
Though algorithms described in Section 3 can correctly find ex-pansions which have the k highest utility, there are two kinds of issues with these algorithms. First, the basic algorithm will favor small expansions (i.e., fewer keywords) as these expansions have more matching items than larger expansions. Second, in the re-turned top-k expansions, it may happen that two expansions have the subset-of relationship, which is not ideal. Indeed, we would like the resulting expansions to have little overlap with each other.
In this section, we propose two solutions to remedy the above drawbacks. In Section 4.1, we will study weighting schemes which can penalize expansions that are either too small or too large. Then in Section 4.2, we propose to find the k most interesting expansions which don X  X  have overlap with each other.
It is clear that expansions which have small size (e.g.,  X  X ML X ) correspond to  X  X eneral topics X  which are related to the query, wher-eas expansions which have large size (e.g.,  X  X ML, schema, confor-mance, automata X ) correspond to  X  X pecific topics X  which are re-lated to the query. To help users quickly locate interesting informa-tion from the returned results, intuitively neither too general topics nor too specific topics should be returned early, so we want to favor those expansions which are neither too large nor too small.
We assume there is a function f w : N  X  R (where N / R denote the sets of natural / real numbers) which can return the weight f for an expansion of size p . The intuition is that f w penalizes too small and too large expansions for we expect them to be intuitively too general or too specific. Then we can use this function to weight the utility of all expansions under consideration. In this work, we consider the Gaussian function f w ( p ) = e  X  ( x  X   X  ) is set as the most ideal size of an expansion and the variance  X  can be adjusted by the system for di ff erent problem instances.
So how are the top-k algorithms impacted? For TopExp-Naive, the weighting function can be simply applied to the utility bounds of each expansion enumerated, and other parts of the algorithm won X  X  be a ff ected. However, for TopExp-Lazy, for each materi-alized expansion e  X  L , the unmaterialized expansions which have the same set of matching items as e have the same utility bounds as e when no weighting is applied. But once weighting is applied, these expansions may have di ff erent utility bounds depending on the size. E.g., consider that there is only one materialized e xpan-sion e = { k 1 , k 2 } in L and let the lower and upper utility bounds of { k } and { k 2 } , though they correspond to the same set of matching items as e , their lower and upper utility bounds are u ( e )  X  f  X  u ( e )  X  f w (1) respectively.

So for the weighted lazy expansion generation based algorithm, for a set E of expansions which correspond to the same set of matching items, we may need to maintain multiple expansions whe-re the number of expansions to be maintained depends on the size of the largest expansion in E . For an expansion e , Algorithm 5 and Algorithm 6, which are adapted from Algorithm 3 and Algorithm 4, can be used to count for each possible expansion size, the number of expansions which correspond to the same set of matching items as e . These counts can be utilized along with the weighting function to determine the corresponding lower and upper utility bounds.
Algorithm 5: u pdateCount(  X  , e ) 1 S T e  X  X  e  X  | e  X   X  L  X  e  X   X  e } ; 2 S T  X  e  X  X  e  X  | e  X   X  S T e  X  X  X  e  X  X   X  E  X  e  X   X  e  X  X  } ; 3 S T e = S T e  X  S T  X  e ; 4 vec c  X  a new count vector of size | e | ; 5 for expsize from 1 to | e | do 7 v ec c  X  countGeneratedExpansions( S T e , vec c ); 8 e .count = vec c ;
Algorithm 6: c alculateSameLBExpansionVec( S T e , vec c ) 1 for o uteridx from 2 to | S T e | do 2 S T  X  e =  X  ; 3 for inneridx from 1 to outeridx -1 do 4 S T  X  e .insert( S T e [ outeridx ]  X  S T e [ inneridx ]); 5 S T  X  X  e  X  X  e  X  | e  X   X  S T  X  e  X  X  X  e  X  X   X  E  X  e  X   X  e  X  X  7 if S T  X  e .hasOverlap() then 8 c ountGeneratedExpansions( S T  X  e , vec c ); 9 else 10 for e xpsize from 1 to | e | do 11 foreach e  X   X  S T  X  e do 12 v ec c [ expsize ] = vec c [ expsize ] -| e  X  | expsize
T o lessen the semantic overlap between di ff erent expansions re-turned to the user, intuitively we may not want to return two dif-ferent expansions e 1 and e 2 , such that either e 1  X  e 2 Note that there can be many sets of expansions satisfying this pair-wize comparability, and the set of highest utility expansions may not satisfy this property. So in order to guarantee the quality of the expansions returned, we want to maximize the sum of the utili-ties for the set of expansions returned under the constraint that the expansions returned should satisfy the pairwise comparability.
D efinition 3. (Maximum k Path-Exclusive Expansion) Given a set S of items and a keyword query Q, find the top k-expansion set E k = { e 1 ,..., e k } s.t.  X  e i , e j  X  E k , i , j, e i P e  X  E k u ( e ) is maximized.

Let L = { e 1 ,..., e n } be the set of all expansions materialized by the algorithm. Consider a weighted undirected graph G = ( Vtx , Edg ), with nodes Vtx = L where each node e i is associated with a weight u ( e i ), i.e., the utility of e i . Whenever two expansions e are such that either e 1  X  e 2 or e 2  X  e 1 , Edg contains the edge ( e , e j ). Then it is straightforward to show that the maximum k path-exclusive expansion problem is NP-hard by a direct reduction from the maximum weighted independent set problem [18].

A simple greedy algorithm for the maximum weighted indepen-dent set was proposed by [18]: repeatedly select a node in G with minimum weighted degree in each iteration and add it to the cur-rent solution; then delete this node and all of its neighbors from the graph; stop when all nodes are removed from G . It has been proven in [18] that this algorithm gives a max(  X  w , 1)-approximation, where  X  w w ( S ) denotes the sum of weights of a set of nodes S , and N is the set of neighbors of v in G .

Furthermore, if we rank all generated expansions by their up-per bound utility, because items are accessed in the non-increasing order of their attribute values, it is clear that the sum of the top-k expansions X  upper bound utilities is an upper bound for the value of all possible k path-exclusive expansions.

So based on this information, we propose the following algo-rithm called Top-PEkExp, which can be used to calculate an ap-proximate solution for the maximum k path-exclusive expansion problem. In Top-PEkExp, similar to the previous algorithms, we iteratively retrieve items from the attribute lists (line 3 X 5), then we use TopExp-Lazy to generate necessary expansions in L (line 6). The set of expansions in L are sent to the greedy algorithm for the maximum weighted independent set problem (line 7), and if the re-sult is already larger than 1  X  o f the maximum upper bound utility, for some constant  X  &gt; 1 that is chosen by the system, we can stop the algorithm (line 8 X 11).

Algorithm 7: T op-PEkExp( Q , I , g , k ) 1 L  X  partial materialized lattice structure; 2 while true do 3 I a  X  g etNextListRR(); 4 t  X  I a .getNextItem(); 5 if Q * K w ( t ) then continue; 6 Generate necessary expansions using TopExp-Lazy; 7 R G  X  GreedyMWIS( L ); 8 E topk  X  k expansions in L which have the largest upper 10 if u ( R G )  X  1  X   X  U  X  t hen 11 return
It is clear that Algorithm Top-PEkExp can correctly return an  X  a pproximate answer for the maximum k path exclusive expan-sion problem. However, because we are using a greedy algorithm for calculating the k path exclusive expansions in each iteration, there may exist a better algorithm, e.g., which utilizes an exact algorithm, which can find an  X  approximate answer much earlier compared with our Top-PEkExp algorithm. This would trade more work done per iteration for achieving early termination. We leave a detailed study of optimal algorithms for the maximum k path ex-clusive expansion problem for future work.
I n this section we will discuss the performance of our proposed algorithms. We use synthetic datasets to demonstrate the relative e ffi ciency, scalability, and memory savings of various algorithms with respect to the naive algorithm. We also use a real dataset to demonstrate the quality of the expansions returned by our algo-rithms.
The goal of our experiments is two-fold: (i) Evaluate the e ffi -ciency and scalability of the algorithms proposed in this paper. (ii) Evaluate the quality of the expansions discovered by various algo-rithms. The experiments are done on a Xeon 2.5GHz Dual Core Windows 7 machine with 4GB RAM. All algorithms are imple-mented in Java using JDK / JRE 1.6.

We use two kinds of datasets in our experiments. First, we gen-erated synthetic datasets to compare the performance of various algorithms with the naive algorithm. The metrics we used for the comparison include the running time, number of items accessed and number of expansions generated during the process. We gen-erated 5 synthetic datasets with size from 8000 to 12000, and for all these datasets, attributes and keyword values are sampled from a power law distribution x = e  X   X  with  X  = 2, which is in accordance with our observation in the real datasets.

The second dataset is a partially crawled dump of the ACM Dig-ital Library. We obtained the items by combining result paper lists of 3 queries: (1)  X  X ml X  (2)  X  X istogram X  (3)  X  X rivacy X . We chose these queries because they are all representative interesting research fields in the database community and also feature a good number of publications. The attributes we use for each paper are the average author publication number and the citation count. Val-ues of both attributes are normalized into the range [0 , 1]. For each paper, we extract its keywords from the title, keywords list and ab-stract. Stop words are removed and also stemming is done on all the keywords obtained. For each paper under consideration, we se-lect the top 15 BM25 scored [20] keywords as its topic keywords. A manually created mapping table is used to map similar keywords into a common keyword, namely the most frequent among them. Note that some recent work on tag clustering / recommendation [22] can be leveraged to automate the creation of the mapping table.
After preprocessing, there are in total 48656 papers in the dataset, and 9000 distinct topic keywords. The quality of the expansions (keywords) generated is manually evaluated by domain experts con-sisting of grad students, who investigated the top expansions dis-covered, and checked whether the keywords make sense for the field.
First, the e ffi ciency comparison of various algorithms on the syn-thetic datasets with fixed N and k is presented in Figure 4 A ( N = k = 10). Note that for the path exclusive algorithm, we choose  X  = 0 . 1 and  X  = 0 . 3 as two settings for the algorithm. The run-ning time, the number of items accessed and the number of expan-sions generated during execution of these algorithms are presented in Figure 4 A(1)-A(3) respectively.

For running time, both TopExp-Lazy and TopExp-LazyW (Weig-hted TopExp-Lazy) run much faster than the baseline TopExp-Naive algorithm. And the running time of PekExp based algorithms highly depend on the  X  parameter chosen.

For the number of items accessed, TopExp-LazyW, TopExp-Lazy and TopExp-Naive access roughly the same number of items. And for PekExp based algorithms, the number of items accessed may vary depending on the parameter  X  . With relatively small  X  , PekExp accesses significantly less items compared with other algorithms, hence much more e ffi cient. Through our observation, for small  X  , PekExp usually stops very soon, and as we will illustrate in the next section, the quality of the expansions returned by PekExp based al-gorithms are also comparable compared with other algorithms. If the application needs immediate response in most cases and tolera-ble for occasion failure, PekExp would be a reasonable choice from the aspect of e ffi ciency.

For the number of expansions generated, while accessing the same number of items, both TopExp-LazyW and TopExp-Lazy gen-erate much less expansions than TopExp-Naive does. This could significantly lessen the space required and the related computations of generating and updating expansions. For TopExp-LazyW and TopExp-Lazy, the weighted algorithm takes more time and gen-erates more expansions. This is because the weighted algorithm needs to keep di ff erent bounds for expansions which correspond to the same set of seen items but with di ff erent number of keywords. However, the space and time cost is still reasonable considering the flexibility we could achieve by customizing the weights.
In Figure 4 B(1)-B(3), we show e ffi ciency comparison of the algorithms with k ranging from 10 to 15(number of items = 10000, N = 10). With larger k , more information needs to be kept for the top expansions, so the updating cost is higher. Furthermore, because more items need to be accessed before the algorithm stop, the algorithm needs longer running time. This coincides with what shows in Figure 4 B(1)-B(2). The only exception is PekExp-alpha = 0.3, for this setting, the number of items accessed and the running time may decrease as k increases. This is because that as k grows, the upper bound of the top-k expansions will drop much faster as more low utility value expansions are included.

Similarly, in Figure 4 C(1)-C(3), we compare the algorithms with N ranging from 10 to 15(number of items = 10000, k = 10). With larger N , because we need to keep track of more items to determine the bounds for each expansion, we could expect longer running time, more items to be accessed and more expansions to be generated.

We can conclude from these plots that the performance of our proposed algorithms are very robust with respect to di ff erent set-tings, and the performance of the algorithms grows linearly with respect to the size of the dataset, so these algorithms can easily scale to larger datasets. Furthermore, it is clear that the TopExp-Lazy algorithms (weighted and unweighted) always outperform the TopExp-Naive algorithm. PekExp based algorithms is an exception to these observations. With proper configuration it could give good performance, but generally the performance varies depending on the value selected for the parameter  X  .
The quality of the expansions generated can be measured by the quality of the keywords of each top expansion. In Table 2, we show the top-20 expansions generated by the TopExp-Lazy and PekExp (shown as Path-exclusive in Table 2) algorithms on the ACM Dig-ital Library dataset, as well as the corresponding keywords for the 3 selected queries for which expansions were generated. By man-ually analyzing the expansions generated, we can see that TopExp-Lazy works fairly well in generating expansions related to major subtopics of each query. E.g., for the query  X  X istogram X , accord-ing to the survey [17], the two most important applications of his-togram techniques in databases have been  X  X electivity estimation X  and  X  X pproximate query answering X . For both topics we can find corresponding expansions in the list of expansions generated, e.g.  X  X elect X ,  X  X pproxim X . Furthermore, Approximation is an estab-lished application of histograms, which constitutes a large portion of [17]. Indeed, accordingly we can see various low level expan-sions like  X  X pproxim wavelet X ,  X  X pproxim olap X  and so on. Notice that other expansions of  X  X istogram X  also correspond to important aspects in the survey. For example,  X  X ulti-dimension X  histogram is a relatively new and important subfield of histogram, and  X  X p-tim X  refers to an important application of approximation using his-togram  X  query optimization. So in general, the expansions gener-ated can cover most important aspects of the survey. Similar results can be observed for the other two queries as well.

While the quality of expansions returned by TopExp-Lazy are quite good, as can be found in Table 2, these expansions may con-tain redundant information. E.g., for query  X  X istogram X , the expan-sions returned include both the high level expansion  X  X pproxim X  and low level (i.e., more refined) expansions  X  X pproxim wavelet X ,  X  X pproxim olap X . Similarly, for query  X  X rivacy X , we have  X  X nonym X  returned along with  X  X nonym protect X  and  X  X nonym k X , and for query  X  X ml X , we have  X  X uery X  returned along with  X  X uery xpath X  and  X  X uery store X . Our path exclusive algorithm can avoid this problem by making sure that no expansion in the returned result set is a subset of other expansions in the result, thus avoiding such redundancy. By a manual inspection of the list of expansions in Ta-ble 2, it is easy to see that such redundancy is avoided by PekExp and also that the set of expansions returned by the that algorithm can also cover most of the important expansions for each query.
In sum, while both TopExp-Lazy and PekExp provide good qual-ity results in terms of meaningful and most important expansions of queries on a real data set, as demonstrated by our experiments, PekExp has the added advantage that it can avoid redundancy in the returned expansions.
The area of top-k query processing has been studied extensively in the past several years [16]. Most of the top-k algorithms are based on the TA and NRA family of algorithms and their variants or enhancements [13]. The majority of them assume a monotone aggregation function for combining scores of items for di ff erent at-tributes. For each attribute, a non-increasing score-sorted list of items is maintained. In [11], Chakrabarti et al. consider the prob-lem of finding top-k entities in a document corpus, where the score of an entity is defined as a weighted aggregation of the scores of its related documents. The proposed algorithm is similar to our na X ve algorithm for the top-k expansion problem, and both algorithms are extensions of the NRA algorithm. However, the number of entities arising in [11] can be considered as a constant, whereas in our con-text, the number of expansions is exponential w.r.t. the total number of keywords, which can be huge. Thus, it is critical to generate the expansions as lazily as possible.

Our top-k expansion problem is also related to the recent e ff orts on faceted search [23, 15]. Li et al. [19] propose the problem of automatic generation of top-k facets for query results on Wikipedia. However, their work makes crucial use of a pre-defined category hi-erarchy in Wikipedia whereas we don X  X  need to make such assump-tions. In fact, as mentioned in the introduction, for the applications we consider, this assumption cannot be made.

Citation recommendation is another area where previous works [12, 9, 14, 12] often generate results as a ranked list of documents. By contrast, in our work we automatically group all the results into di ff erent expansions, and return to the user the top-k interesting expansions along with the relevant items.
I n [21], the authors propose a way to facilitate the search process by suggesting interesting additional query terms. To measure the interestingness of an additional query term or keyword, they pro-posed the surprising score which is based on the co-occurrence of two keywords. Compared with our work, they don X  X  consider the overlap between di ff erent sets of query keywords. Besides, their score function needs to access all items which are relevant to the query. Since this cannot be calculated at the query time in a scal-able manner, they instead propose an approximate solution.
In this paper, we started with the observation that years after search engines first came into being, most current search services still return results as a flat ranked list of items, which is not ideal for users to easily get to the items they are really interested in. We studied the problem of how to better present search / query results to users. We considered a search scenario in which each item is anno-tated with a set of keywords and is equipped with a set of attributes, and proposed novel ways to automatically group query result items into di ff erent expansions of the query, corresponding to subsets of keywords. We proposed various e ffi cient algorithms which can cal-culate top-k expansions, and we also studied additional desirable properties for the set of expansions returned, from a semantic per-spective, whereby certain redundancies in the expansions returned can be avoided. With a detailed set of experiments, we not only demonstrated the performance of the proposed algorithms, we also validated the quality of the expansions returned by doing a study on a real data set. It is interesting to explore more desirable properties of the expansions returned, and to investigate more e ffi cient algo-rithms which can return high quality expansion set and can handle the web scale. This work was supported in part by the Institute for Computing, Information and Cognitive Systems (ICICS) at UBC.
