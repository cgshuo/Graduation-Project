 Data clustering is the process of grouping data tuples into different clusters, such that tuples within one cluster are similar with each other but are dissimilar to tuples in other clusters. It is an important problem in data mining, and has been successfully used in various domains such as image processing, market research, and bioinformatics [2]. In many cases, it is important to conduct data clustering on private data without violating the privacy of data owners.

In this paper, we address the privacy protection problem for a distributed system in which data being mined are stored across m ultiple autonomous en tities. In our previous work [7], we classified such distributed systems into two categories based on their in-frastructures, namely Server-to-Server (S2S) and Client-to-Server (C2S), respectively. An S2S system consists of several servers, each of which holds a private database. The servers collaborate to perform data mining across their databases without disclosing private information to each other. A C2S system consists of one data miner (server) and multiple data providers (clients), each of which holds a private data tuple. The data miner is supposed to collect data from the data providers and perform data mining on the collected data. Online survey is a typical example of a C2S system, as there is one survey analyzer (data miner) and numerous survey respondents (data providers).
Both C2S and S2S systems have a broad range of applications. Nevertheless, we focus on C2S systems in this paper. Most previous work in C2S systems aims to pro-tect (only) the values of the original private data (i.e., the data being mined) in such systems [5]. While this is an important task, we find that in many cases, the data own-ers have privacy concerns on not only the original data, but also the cluster labels of their individual data. For example, a customer may be willing to provide (the privacy-protected version of) his/her purchase r ecord for a company to analyze its customer bases and thereby reduce advertisement cost. Nonetheless, the customer may not want the company to label him/her in a special group (e.g., gamer, new-product enthusiast, or deal hunter).

In order to provide comprehensive privacy protection in data clustering systems, a privacy-preserving mechanism must simultaneously prevent the disclosure of original data as well as the labels of such data. In particular, the data miner should not learn which data tuples belong to the same cluster. Ideally speaking, the only information that the data miner can learn from the collected data should be the clustering rules that can accurately classify different clusters. For example, if k -means clustering algorithm is used, the data miner should learn the accura te center point of each cluster, and nothing else, after the data collec tion and clustering process.

In this paper, we propose an algebraic-approach-based scheme that protects the pri-vacy of both the original data and their cluster labels, while allowing the data miner to generate accurate clustering rules. We will show that our new scheme has the following important features that distinguish it from previous approaches:  X  Up to our knowledge, this paper is the first to address the protection of both original  X  Our scheme allows each data provider to choos e a different level of privacy disclo- X  Our scheme is transparent to the clustering algorithm used and can be readily inte-
The algebraic-techniques-based approach was first proposed in our previous work for other data mining problems (e.g., data classification [7]). There are significant dif-ferences between the work presented in this paper and our previous work which include:  X  The data mining problem is different: we are dealing with data clustering problem  X  The private information is different: we are preserving not only the privacy of orig-
The rest of the paper is organized as follows. We introduce the system model and problem definition in Section 2. In Section 3, we present the baseline architecture of our new scheme. We introduce the basic components of our scheme in Section 4. In Section 5, we evaluate the performance of our scheme theoretically and derive bounds on privacy and accuracy measures. We experi mentally evaluate the performance of our scheme on real data set in Section 6, and conclude with final remarks in Section 7. Let there be one data miner S and m data providers P 1 , ... , P m in the system. Each data provider P i holds a data tuple t i ( i  X  [1 ,m ]) with n attributes a 1 ,...,a n . The clustering process is consisted of two steps. In this first step, the data miner collects data from the data providers. As in an online survey system where each survey respondent joins the system at a different time, we consider this step to be iteratively carried out by a group of independent processes, within each of whic h a data provider tran smits its data to the data miner. In a system with privacy concerns, a data provider may perturb its data first and transmit (only) the perturbed data to the data miner.

Let T be the set of all m original data tuples. Let d ( t i ,t j ) be the distance function be-tween two data tuples t i and t j . Suppose there are k clusters within T . As is commonly assumed, both the data miner and the data providers know k as pre-knowledge. We use f ( t i )  X  [1 ,k ] to denote the cluster label of t i . The objective of clustering without privacy concern is for the data miner to obtain f ( t i ) for all i  X  [1 ,m ] .
As we mentioned in Section 1, in order to provide comprehensive privacy protection in data clustering systems, our objective is to achieve the following three goals:  X  ( Value Privacy ) Minimize the amount of private information disclosed about t i .  X  ( Label Privacy ) Minimize the amount of private information disclosed about f ( t i ) .  X  ( Accuracy ) Enable the data miner to generate a clustering function f R (  X  ): t  X  Note that the accuracy goal does not contradi ct the label privacy goal. Although the data miner can generate the clustering function f R (  X  ) , it cannot derive f ( t i ) because due to value privacy, the data miner does not know the value of t i . In this section, we introduce our new scheme that protects both value and label pri-vacy while maintaining the accuracy of cluste ring rules. Figure 1 depicts the baseline architecture of our new scheme.

Note that due to sociological survey results, different people have different levels of privacy concern on their data [3]. Thus, we introduce a important parameter for each data provider P i called the maximum acceptable disclosure level , denoted by l i .The maximum acceptable disclosure level measures the level of privacy concerns of each data provider. Generally speaking, if we consider the original data tuples as random vectors, then l i is the degree of freedom of the perturbed vector R ( t i ) ,whichinmost cases is much smaller than the degree of freedom of the original data tuple t i .Therela-tionship between l i and the level of privacy disclosure is further analyzed in Section 5.
When a data provider joins the system, it first inquires the data miner about the cur-rent system disclosure level l  X  , which is minimum disclosure level (currently) required by the data miner to maintain an accurate estim ation of the clustering rules. The compu-it can wait for a certain amount of time and try again. As we demonstrate in Section 6, the value of l  X  decreases rapidly when the data mi ner receives more data tuples. Since l varies among different data providers, the system disclosure level will soon be ac-ceptable to most data providers.

If a data provider accepts l  X  , the data provider then inquires the data miner about the current perturbation guidance G  X  . Basically speaking, G  X  tells the data provider how to project its original data tuple t into an l  X  -dimensional subspace, such that 1) the cluster label information is removed from the projected data, and 2) the private information retained in the subspace is the minimum necessary to generate clustering rules. The computation of G  X  is presented in Section 4. After receiving G  X  , the data provider first checks the validity of G  X  , and then compute the perturbed data R ( t ) based on t and G  X  . Details of the validation and perturbation process are also presented in Section 4. The data provider then transmits R ( t ) to the data miner.

After the data miner receives all data tuples, it can directly use the collected data as input (to any clustering algorithm) to generate clustering rules. As we can see, our scheme is transparent to the clustering algorithm used, and can be integrated into exist-ing systems as middle-ware. There are two key components in our scheme: perturbation guidance of the data miner and perturbation of the data providers. We introduce these two components in detail in the next section. The basic components of our scheme are 1) t he perturbation guidance component of the data miner, which generates the current system disclosure level l  X  and the perturbation guidance G  X  ; and 2) the perturbation component of the data providers, which validates the received G  X  and computes R ( t ) based on t and G  X  . In this section, we first introduce some basic notions, and then introduce the design of these two components respectively. 4.1 Basic Notions Recall that there are m data providers in the system, each of which holds a private data tuple t i with n attributes a 1 ,...,a n . We assume that all attributes are categorical. If an attribute is continuous, it can be discretized first. Let d i be the number of distinct values of a i . Without loss of generality, we assume that a i  X  X  1 ,...,d i } . We can denote a data tuple t i by a ( d 1 +  X  X  X  + d n ) -dimensional vector as follows. Within the d i bits assigned for a i ,the j -th bit is 1 if and only if a i = j . All other bits are 0 . For example, a binary attribute a i =0 has two corresponding bits as 1 , 0 .
Let there be n 0 = d 1 +  X  X  X  + d n . Each data tuple t i can be represented by an n 0 -dimensional vector. As such, we can represent the set of all data tuples by an m  X  n 0 matrix T =[ t 1 ; ... ; t m ] .The i -row of T is the corresponding vector of t i .Weuse T ij to denote the element of T with indices i and j (i.e., the j -th bit of t i ). We denote the transpose of T by T . 4.2 Perturbation Guidance As we are considering systems where the data tuples are iteratively fed to the data miner, the data miner needs to maintain a copy of all received (perturbed) data tuples. Let T  X  the current matrix of such received data tupl es. When a new (perturbed) data tuple is received, it is directly appended to T  X  . Our scheme computes l  X  and G  X  based on T  X  . In order to compute them for the first-come data provider, we assume that the initial value of T  X  is an m 0  X  n 0 matrix ( m 0  X  k  X  n 0 ), which is consisted of either data tuples collected from privacy-unconcerned data providers or randomly generated data tuples, or a combination of them.

When a new data provider joins the system, the computation of l  X  and G  X  is consisted of two steps 1 . In the first step, the data miner groups the received (perturbed) data tuples number of data tuples in T  X  i be m  X  i . Then, the data miner computes the singular value decomposition (SVD) of each cluster as follows.
 where U i is an m  X  i  X  n 0 unitary matrix (i.e., U i U i = I where I is the n 0  X  n 0 identity matrix),  X  i = diag(  X   X  i 1 ,..., X   X  in contains the right singular vectors of T  X  i . Note that when m is large, it is always possible to incrementally compute the SVD of T  X  i when new data tuples are received.
In the second step, the data miner computes l  X  and G  X  based on  X   X  i and V  X  i .In particular, l  X  is the minimum integer in [1 ,n 0 ] that satisfies  X  i  X  [1 ,k ] , where  X   X  [0 , 1] is a parameter pre-determined by th e data miner. A data miner that can tolerate a relatively lower level of accuracy can choose a large  X  to reduce l  X  .Adata miner that requires a higher level of accuracy can maintain a small  X  to ensure accurate clustering rules. In order to choose a cut-off  X  that reduces l  X  rapidly while maintaining accurate clustering rules, a textbook heuristic is to set  X  =0 . 15 .

Given l  X  , the perturbation guidance G  X  is a set that contains: 1) the clustering func-k -means clustering algorithm is used by the data miner, the clustering function f  X  (  X  ) is consisted of the center points of the k clusters. Each vector s  X  i ( i  X  [1 ,k ] ) is consisted first k right singular vectors of T  X  i (i.e., the first l  X  columns of V i ). That is, to the data provider. 4.3 Perturbation After accepting l  X  and receiving G  X  , a data provider needs to 1) check the validity of G  X  , and 2) compute the perturbed data tuple R ( t ) . The validation process is simple: P i only needs to check if every received V V
After the validation process, the data provider computes R ( t ) as follows. First, the data provider determines the cluster label of its data by computing f  X  ( t ) . After that, it computes k ( l  X   X  l  X  ) diagonal matrices  X  i for i  X  [1 ,k ] , such that  X  j  X  [1 ,l  X  ] ,
 X  intermediate (perturbed) vector  X  t as follows. always a diagonal matrix, the inverse of  X  i always exists. Also note that the inverse of V i does not exist because all V it is impossible to recover t from  X  t .

As we can see, the elements in  X  t are real values. Thus, we need an additional step to transform  X  t to a binary vector. In particular, for every j  X  [1 ,n 0 ] ,wehave where  X  t j is the j -th element of  X  t ,and r is generated uniformly at random from [0 , 1] . We now analyze the performance of our scheme. We define performance measures on 1) the amount of disclosure on value privacy (i.e., the disclosure of original data tu-ples), 2) the amount of disclosure on label privacy (i.e., the disclosure of individual cluster labels), and 3) the error of clustering rules built on the perturbed data. We derive bounds on these measures, in order to provide guidelines for system administrators to set parameters in practical systems. 5.1 Value Privacy Recall that our scheme allows each data provider P i to choose its individual maximum acceptable disclosure level l i . Thus, we define the level of value-privacy disclosure based on l i of individual data providers. Note that we need to consider all possible G  X  that can pass the validity test. In particular, we use a information-theoretic measure. Let H ( t i ) be the information entropy of t i (i.e., the amount of information in t i )and mation about t i that can be disclosed by R ( t i ) ). Please refer to [1] for the details of information theory.
 Definition 1. The degree of value-privacy disclosure P v ( l i ) is the maximum expected percentage of information disclosed by R ( t i ) when P i computes R ( t i ) based on a sys-tem disclosure level l  X   X  l i and an arbitrary perturbation guidance G  X  that passes the validity test. That is, As we can see from the definition, the larger P v ( l i ) is, the more value-privacy-related information about P i is disclosed to the data miner. We derive an upper bound on P v ( l i ) as follows.
 Theorem 1. When m is sufficiently large, we have where  X  j is the j -th singular value of T .
 Due to space limit, please refer to [6] for the proof of this theorem. 5.2 Label Privacy Definition 2. The degree of label-privacy disclosure P c is the probability that R ( t i ) and R ( t j ) belong to the same cluster given t i and t j in the same cluster. That is, For the sake of simplicity, we consider the problem in a 2 -dimensional setting. That is, we consider the cases where the clustering algorithm groups data tuples based on two (optimal) attributes (chosen from the n attributes) that have the most discrepancy between different clusters. We use P c (2) to denote the value of P c in this 2 -dimensional setting.
 Theorem 2. When m is sufficiently large, we have P c (2) = 1 /k .
 Please refer to [6] for the proof of this theorem.
 5.3 Accuracy Recall that f R (  X  ) is the clustering function generated from the perturbed data. For every possible value t S , we measure the error on the mined clustering rules about t S as Definition 3. The degree of error E is defined as the maximum of E ( t S ) on all possible values of t S .
 Theorem 3. When m is sufficiently large, we have where  X  j 1 is the largest singular value of T j .
 Please refer to [6] for the proof of this theorem. We conduct our experiments on the congressional voting records database from the UCI machine learning repository [4]. The data set includes 16 key votes (0-no or 1-yes) for each of the 435 United Stated House Representatives in 1984. The task is to cluster the records into two clusters: republicans and democrats. There are 61.38% democrats and 38.62% republications in the data set. The data set contains 392 missing votes, which we fill with values generated uniformly at random from { 0 , 1 } .Weuse k -means clustering algorithm in our evaluation.

Since each data tuple contai ns 16 binary elements, we represent each data tuple by a 32 -dimensional binary vector. We apply our scheme on the data set when  X  varies from 0 . 1 to 0 . 9 . The initial value of T  X  is consisted of 32 uniformly generated tuples and 32 tuples randomly chosen from T . The data miner updates the pe rturbation guidance once every 40 data tuples are received.

Figure 2 shows the performance of our scheme on value privacy, label privacy, and accuracy. We evaluate the level of value privacy and label privacy by the disclosure measures defined in Section 5. In order to demonstrate the accuracy of our scheme intuitively, we measure accuracy by the per centage of original data tuples that can be correctly clustered by the clustering ru les generated from the perturbed data.
As we can see from the results, our scheme can achieve fairly high level of accuracy (over 90% when  X   X  0 . 6 ) while maintaining a low level of disclosure on both value privacy (less than 0.45 when  X   X  0 . 3 ) and label privacy (less than 0.65 when  X   X  0 . 3 ). Note that since there are 61.38% democrats in the data set, the lowest possible level of label disclosure is 0.6138. As we can see, the disclosure level of our scheme is very close to this lower bound when  X   X  0 . 3 .

In order to demonstrate the change of l  X  when the data miner receives more (per-turbed) data tuples, we show the change of l  X  with | T  X  | in Figure 3 when  X  =0 . 15 . As we can see, the value of l  X  decreases fairly quickly (e.g., reduced to 7 when 50 data tuples are received) when the data miner receives more data tuples. In this paper, we address the comprehensive protection of privacy in data clustering. Compared with previous work, we identify a new privacy concern in data clustering which is the privacy of individual cluster labels. In order to achieve comprehensive pri-vacy protection, we propose a privacy-preserving clustering scheme that can simultane-ously protect the privacy of original data and individual cluster labels. In particular, our scheme allows each data provider to choose a different level of maximum acceptable privacy disclosure level that reflects its i ndividual privacy concern. Our scheme also al-lows the data miner to distribute perturbation guidance to the data providers. Using this intelligence, the data providers perturb their data tuples and transmit the perturbed tu-ples to the data miner. As a result, our scheme can achieve both value and label privacy while maintaining the accuracy of clustering rules. We demonstrate the performance of our scheme by theoretical bounds and experimental evaluation.

Our work is preliminary and many extensions can be made. We are currently in-vestigating privacy concerns beyond original data being mined in other data mining problems. We would also like to investigate the integration of our scheme with crypto-graphic approach in Server-to-Server (S2S) systems.

