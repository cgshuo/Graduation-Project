 Recent years have seen a surge of interest in sin-gle word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language ap-plications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Se-bastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simula-tions of human behavior involving semantic prim-ing, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representa-tions from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings.

Word meaning, however, is also tied to the physical world. Words are grounded in the exter-nal environment and relate to sensorimotor experi-ence (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptu-ally grounded distributional models have emerged. These models learn the meaning of words based on textual and perceptual input. The latter is ap-proximated by feature norms elicited from humans (Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012), visual information extracted automatically from images, (Feng and Lapata, 2010; Bruni et al., 2012a; Silberer et al., 2013) or a combination of both (Roller and Schulte im Walde, 2013). Despite differences in formulation, most existing models conceptualize the problem of meaning representation as one of learning from multiple views corresponding to different modali-ties. These models still represent words as vectors resulting from the combination of representations with different statistical properties that do not nec-essarily have a natural correspondence (e.g., text and images).

In this work, we introduce a model, illus-trated in Figure 1, which learns grounded mean-ing representations by mapping words and im-ages into a common embedding space. Our model uses stacked autoencoders (Bengio et al., 2007) to induce semantic representations integrating vi-sual and textual information. The literature de-scribes several successful approaches to multi-modal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most pre-vious work, our model is defined at a finer level of granularity  X  it computes meaning representa-tions for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric repre-sentation is expedient for several reasons.
Firstly, attributes provide a natural way of ex-pressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al., 2005) where humans often employ attributes when asked to describe a concept. Secondly, from a modeling perspective, attributes allow for eas-ier integration of different modalities, since these are rendered in the same medium, namely, lan-guage. Thirdly, attributes are well-suited to de-scribing visual phenomena (e.g., objects, scenes, actions). They allow to generalize to new in-stances for which there are no training exam-ples available and to transcend category and task boundaries whilst offering a generic description of visual data (Farhadi et al., 2009).

Our model learns multimodal representations from attributes which are automatically inferred from text and images. We evaluate the embed-dings it produces on two tasks, namely word sim-ilarity and categorization. In the first task, model estimates of word similarity (e.g., gem  X  jewel are similar but glass  X  magician are not) are compared against elicited similarity ratings. We performed a large-scale evaluation on a new dataset consist-ing of human similarity judgments for 7,576 word pairs. Unlike previous efforts such as the widely used WordSim353 collection (Finkelstein et al., 2002), our dataset contains ratings for visual and textual similarity, thus allowing to study the two modalities (and their contribution to meaning rep-resentation) together and in isolation. We also assess whether the learnt representations are ap-propriate for categorization, i.e., grouping a set of objects into meaningful semantic categories (e.g., peach and apple are members of FRUIT , whereas chair and table are FURNITURE ). On both tasks, our model outperforms baselines and related models. The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We re-view related work in these areas below.
 Grounded Semantic Spaces Grounded seman-tic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singu-lar Value Decomposition.
 Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas oth-ers (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic mod-els which combine both feature norms and vi-sual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute clas-sifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss.

The visual and textual modalities on which our model is trained are decoupled in that they are not derived from the same corpus (we would expect co-occurring images and text to correlate to some extent) but unified in their representation by natu-ral language attributes. The use of stacked autoen-coders to extract a shared lexical meaning repre-sentation is new to our knowledge, although, as we explain below related to a large body of work on deep learning.
 Multimodal Deep Learning Our work employs deep learning (a.k.a deep networks) to project lin-guistic and visual information onto a unified rep-resentation that fuses the two modalities together. The goal of deep learning is to learn multiple lev-els of representations through a hierarchy of net-work architectures, where higher-level representa-tions are expected to help define higher-level con-cepts.

A large body of work has focused on projecting words and images into a common space using a va-riety of deep learning methods ranging from deep and restricted Boltzman machines (Srivastava and Salakhutdinov, 2012; Feng et al., 2013), to au-toencoders (Wu et al., 2013), and recursive neural networks (Socher et al., 2013b). Similar methods have been employed to combine other modalities such as speech and video (Ngiam et al., 2011) or images (Huang and Kingsbury, 2013). Although our model is conceptually similar to these studies (especially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representa-tion for each modality and their optimal combi-nation. Secondly, our problem setting is different from the former studies, which usually deal with classification tasks and fine-tune the deep neural networks using training data with explicit class la-bels; in contrast we fine-tune our autoencoders us-ing a semi-supervised criterion. That is, we use indirect supervision in the form of object classifi-cation in addition to the objective of reconstruct-ing the attribute-centric input representation. 3.1 Background Our model learns higher-level meaning represen-tations for single words from textual and visual input in a joint fashion. We first briefly review autoencoders in Section 3.1 with emphasis on as-pects relevant to our model which we then de-scribe in Section 3.2.
 Autoencoders An autoencoder is an unsuper-vised neural network which is trained to recon-struct a given input from its latent representation (Bengio, 2009). It consists of an encoder f  X  which a non-linear activation function, such as a sig-moid function. A decoder g  X  0 then aims to recon-termination of parameters  X   X  = { W , b } and  X   X  0 = { W 0 , b 0 } that minimize the average reconstruction where L is a loss function, such as cross-entropy. Parameters  X  and  X  0 can be optimized by gradient descent methods.

Autoencoders are a means to learn representa-tions of some input by retaining useful features in the encoding phase which help to reconstruct the input, whilst discarding useless or noisy ones. To this end, different strategies have been employed to guide parameter learning and constrain the hid-den representation. Examples include imposing a bottleneck to produce an under-complete rep-resentation of the input, using sparse representa-tions, or denoising .
 Denoising Autoencoders The training criterion with denoising autoencoders is the reconstruction (Vincent et al., 2010). The underlying idea is that the learned latent representation is good if the au-toencoder is capable of reconstructing the actual input from its corruption. The reconstruction error One possible corruption process is masking noise , Stacked Autoencoders Several (denoising) au-toencoders can be used as building blocks to form a deep neural network (Bengio et al., 2007; Vin-cent et al., 2010). For that purpose, the autoen-coders are pre-trained layer by layer, with the cur-rent layer being fed the latent representation of the previous autoencoder as input. Using this unsuper-vised pre-training procedure, initial parameters are found which approximate a good solution. Subse-quently, the original input layer and hidden repre-sentations of all the autoencoders are stacked and all network parameters are fine-tuned with back-propagation.

To further optimize the parameters of the net-work, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent repre-sentation). 3.2 Semantic Representations To learn meaning representations of single words from textual and visual input, we employ stacked (denoising) autoencoders (SAEs). Both input modalities are vector-based representations of words, or, more precisely, the objects they refer to (e.g., canary , trolley ). The vector dimensions cor-respond to textual and visual attributes, examples of which are shown in Table 1. We explain how these representations are obtained in more detail visual attributes, respectively (see Table 1). in Section 4.1. We first train SAEs with two hid-den layers (codings) for each modality separately. Then, we join these two SAEs by feeding their re-spective second coding simultaneously to another autoencoder, whose hidden layer thus yields the fused meaning representation. Finally, we stack all layers and unfold them in order to fine-tune the SAE. Figure 1 illustrates the model.
 Unimodal Autoencoders For both modalities, we use the hyperbolic tangent function as activa-tion function for encoder f  X  and decoder g  X  0 and an entropic loss function for L . The weights of each autoencoder are tied, i.e., W 0 = W T . We employ denoising autoencoders (DAEs) for pre-training the textual modality. Regarding the visual autoen-coder, we derive a new ( X  X enoised X ) target vector autoencoder is thus trained to denoise a given in-put. The target vector is derived as follows: each object o in our data is represented by multiple im-ages, and each image is in turn represented by a attribute vectors representing object o .
 Bimodal Autoencoder The bimodal autoen-coder is fed with the concatenated final hidden codings of the visual and textual modalities as in-put and maps these inputs to a joint hidden layer  X  y with B units. We normalize both unimodal input codings to unit length. Again, we use tied weights for the bimodal autoencoder. We also encourage the autoencoder to detect dependencies between the two modalities while learning the mapping to the bimodal hidden layer. We therefore apply masking noise to one modality with a masking fac-tor v (see Section 3.1), so that the corrupted modal-ity optimally has to rely on the other modality in order to reconstruct its missing input features. Stacked Bimodal Autoencoder We finally build a stacked bimodal autoencoder (SAE) with all pre-trained layers and fine-tune them with re-spect to a semi-supervised criterion. That is, we unfold the stacked autoencoder and furthermore add a softmax output layer on top of the bimodal layer  X  y that outputs predictions  X  t with respect to the inputs X  object labels (e.g., boat ): is the number of unique object labels. The over-all objective to be minimized is therefore the weighted sum of the reconstruction error L r and the classification error L c : L = where  X  r and  X  c are weighting parameters that give different importance to the partial objectives, L c and L r are entropic loss functions, and R is a regularization term with R =  X  5 j = 1 2 || W ( j ) || 2 + O -dimensional one-hot vector 1 .

The additional supervised criterion drives the learning towards a representation capable of dis-criminating between different objects. Further-more, the semi-supervised setting affords flexibil-ity, allowing to adapt the architecture to specific tasks. For example, by setting the corruption pa-rameter v for the textual modality to one and  X  r to zero, a standard object classification model for images can be trained. Setting v close to one for ei-ther modality enables the model to infer the other (missing) modality. As our input consists of nat-ural language attributes, the model would infer textual attributes given visual attributes and vice versa. In this section we present our experimental setup for assessing the performance of our model. We give details on the tasks and datasets used for eval-uation, we explain how the textual and visual in-puts were constructed, how the SAE model was trained, and describe the approaches used for com-parison with our own work. 4.1 Data We learn meaning representations for the nouns contained in McRae et al. X  X  (2005) feature norms. These are 541 concrete animate and inanimate ob-jects (e.g., animals, clothing, vehicles, utensils, fruits, and vegetables). The norms were elicited by asking participants to list properties (e.g., barks , an animal , has legs ) describing the nouns they were presented with.
As shown in Figure 1, our model takes as in-put two (real-valued) vectors representing the vi-sual and textual modalities. Vector dimensions correspond to textual and visual attributes, respec-tively. Textual attributes were extracted by run-ning Strudel (Baroni et al., 2010) on a 2009 dump automatic method for extracting weighted word-attribute pairs (e.g., bat  X  species:n , bat  X  bite:v ) from a lemmatized and POS-tagged corpus. Weights are log-likelihood ratio scores expressing how strongly an attribute and a word are associated. We only retained the ten highest scored attributes for each target word. This returned a total of 2,362 dimensions for the textual vectors. Association scores were scaled to the [  X  1 , 1 ] range.
To obtain visual vectors, we followed the methodology put forward in Silberer et al. (2013). Specifically, we used an updated version of their dataset to train SVM-based attribute classifiers that predict visual attributes for images (Farhadi et al., 2009). The dataset is a taxonomy of 636 vi-sual attributes (e.g., has wings , made of wood ) and nearly 700K images from ImageNet (Deng et al., 2009) describing more than 500 of McRae et al. X  X  (2005) nouns. The classifiers perform reason-ably well with an interpolated average precision of 0.52. We only considered attributes assigned to at least two nouns in the dataset, obtaining a 414 dimensional vector for each noun. Analo-gously to the textual representations, visual vec-tors were scaled to the [  X  1 , 1 ] range.

We follow Silberer et al. X  X  (2013) partition of the dataset into training, validation, and test set and acquire visual vectors for each of the sets. We use the visual vectors of the training and development set for training the autoencoders, and the vectors for the test set for evaluation. 4.2 Model Architecture Model parameters were optimized on a subset of the word association norms collected by Nelson ing participants with a cue word (e.g., canary ) and asking them to name an associate word in response (e.g., bird, sing, yellow ). For each cue, the norms provide a set of associates and the frequencies with which they were named. The dataset con-tains a very large number of cue-associate pairs (63,619 in total) some of which luckily are cov-we used correlation analysis (Spearman X  X   X  ) to monitor the degree of linear relationship between model cue-associate (cosine) similarities and hu-man probabilities.

The best autoencoder on the word association task obtained a correlation coefficient of 0.33. This performance is superior to the results re-ported in Silberer et al. (2013) (their correlation coefficients range from 0.16 to 0.28). This model has the following architecture: the textual autoen-coder (see Figure 1, left-hand side) consists of 700 hidden units which are then mapped to the sec-ond hidden layer with 500 units (the corruption parameter was set to v = 0 . 1); the visual autoen-coder (see Figure 1, right-hand side) has 170 and 100 hidden units, in the first and second layer, re-spectively. The 500 textual and 100 visual hidden units were fed to a bimodal autoencoder contain-ing 500 latent units, and masking noise was ap-plied to the textual modality with v = 0 . 2. The weighting parameters for the joint training objec-tive of the stacked autoencoder were set to  X  r = 0 . 8 and  X  c = 1 (see Equation (4)).

We used the model described above and the meaning representations obtained from the out-put of the bimodal latent layer for all the eval-uation tasks detailed below. Some performance gains could be expected if parameter optimization took place separately for each task. However, we wanted to avoid overfitting, and show that our pa-rameters are robust across tasks and datasets. 4.3 Evaluation Tasks Word Similarity We first evaluated how well our model predicts word similarity ratings. Al-though several relevant datasets exist, such as the widely used WordSim353 (Finkelstein et al., 2002) or the more recent Rel-122 norms (Szum-lanski et al., 2013), they contain many abstract words, (e.g., love  X  sex or arrest  X  detention ) which are not covered in McRae et al. (2005). This is for a good reason, as most abstract words do not have discernible attributes, or at least attributes that par-ticipants would agree upon. We thus created a new dataset consisting exclusively of McRae et al. (2005) nouns which we hope will be useful for the development and evaluation of grounded semantic Initially, we created all possible pairings over McRae et al. X  X  (2005) nouns and computed their semantic relatedness using Patwardhan and Peder-sen (2006) X  X  WordNet-based measure. We opted for this specific measure as it achieves high corre-lation with human ratings and has a high coverage on our nouns. Next, for each word we randomly selected 30 pairs under the assumption that they are representative of the full variation of semantic similarity. This resulted in 7,576 word pairs for which we obtained similarity ratings using Ama-zon Mechanical Turk (AMT). Participants were asked to rate a pair on two dimensions, visual and semantic similarity using a Likert scale of 1 (highly dissimilar) to 5 (highly similar). Each task consisted of 32 pairs covering examples of weak to very strong semantic relatedness. Two con-trol pairs from Miller and Charles (1991) were in-cluded in each task to potentially help identify and eliminate data from participants who assigned ran-dom scores. Examples of the stimuli and mean ratings are shown in Table 2.

The elicitation study comprised overall 255 tasks, each task was completed by five volun-teers. The similarity data was post-processed so as to identify and remove outliers. We consid-ered an outlier to be any individual whose mean pairwise correlation fell outside two standard de-viations from the mean correlation. 11.5% of the annotations were detected as outliers and re-moved. After outlier removal, we further ex-amined how well the participants agreed in their similarity judgments. We measured inter-subject agreement as the average pairwise correlation co-efficient (Spearman X  X   X  ) between the ratings of all annotators for each task. For semantic similarity, the mean correlation was 0.76 (Min = 0.34, Max Table 2: Mean semantic and visual similarity rat-ings for the McRae et al. (2005) nouns using a scale of 1 (highly dissimilar) to 5 (highly similar). = 0.97, StD = 0.11) and for visual similarity 0.63 (Min = 0.19, Max = 0.90, SD = 0.14). These re-sults indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consistency. For comparison, Patwardhan and Pedersen X  X  (2006) measure achieved a coefficient of 0 . 56 on the dataset for semantic similarity and 0 . 48 for vi-sual similarity. The correlation between the aver-age ratings of the AMT annotators and the Miller and Charles (1991) dataset was  X  = 0 . 91. In our experiments (see Section 5), we correlate model-based cosine similarities with mean similarity rat-ings (again using Spearman X  X   X  ).
 Categorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by Fountain and Lapata (2010). The dataset contains a classification, produced by human participants, of McRae et al. X  X  (2005) nouns into (possibly multiple) semantic categories (40 in
To obtain a clustering of nouns, we used Chi-nese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor-respond to words and edges to cosine similarity scores between vectors representing their mean-ing. CW is a non-parametric model, it induces the number of clusters (i.e., categories) from the data as well as which nouns belong to these clusters. In our experiments, we initialized Chinese Whis-pers with different graphs resulting from different vector-based representations of the McRae et al. (2005) nouns. We also transformed the dataset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Foun-tain and Lapata, 2010). CW can optionally ap-ply a minimum weight threshold which we opti-mized using the categorization dataset from Ba-roni et al. (2010). The latter contains a classifica-tion of 82 McRae et al. (2005) nouns into 10 cate-gories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation.

We evaluated the clusters produced by CW us-ing the F-score measure introduced in the Se-mEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster di-vided by the number of items in the cluster and the number of items in the gold-standard class, re-spectively. 4.4 Comparison with Other Models Throughout our experiments we compare a bi-modal stacked autoencoder against unimodal au-toencoders based solely on textual and visual in-put (left-and right-hand sides in Figure 1, respec-tively). We also compare our model against two approaches that differ in their fusion mechanisms. The first one is based on kernelized canonical cor-relation (kCCA, Hardoon et al., 2004) with a lin-ear kernel which was the best performing model in Silberer et al. (2013). The second one emulates Bruni et al. X  X  (2014) fusion mechanism. Specifi-cally, we concatenate the textual and visual vec-tors and project them onto a lower dimensional la-tent space using SVD (Golub and Reinsch, 1970). All these models run on the same datasets/items and are given input identical to our model, namely attribute-based textual and visual representations. We furthermore report results obtained with Bruni et al. X  X  (2014) bimodal distributional model, which employs SVD to integrate co-occurrence-based textual representations with visual repre-Models T V T+V T V T+V McRae 0.71 0.49 0.68 0.58 0.52 0.62 Attributes 0.58 0.61 0.68 0.46 0.56 0.58 SAE 0.65 0.60 0.70 0.52 0.60 0.64
SVD  X   X  0.67  X   X  0.57 kCCA  X   X  0.57  X   X  0.55 Bruni  X   X  0.52  X   X  0.46 RNN-640 0.41  X   X  0.34  X   X  Table 3: Correlation of model predictions against similarity ratings for McRae et al. (2005) noun pairs (using Spearman X  X   X  ). sentations constructed from low-level image fea-tures. In their model, the textual modality is represented by the 30K-dimensional vectors ex-visual modality is represented by bag-of-visual-words histograms built on the basis of clustered SIFT features (Lowe, 2004). We rebuilt their model on the ESP image dataset (von Ahn and Dabbish, 2004) using Bruni et al. X  X  (2013) publicly available system.

Finally, we also compare to the word embed-dings obtained using Mikolov et al. X  X  (2011) re-current neural network based language model. These were pre-trained on Broadcast news data port results with the 640-dimensional embeddings as they performed best. Table 3 presents our results on the word simi-larity task. We report correlation coefficients of model predictions against similarity ratings. As an indicator to how well automatically extracted at-tributes can approach the performance of clean hu-man generated attributes, we also report results of a distributional model induced from McRae et al. X  X  (2005) norms (see the row labeled McRae in the table). Each noun is represented as a vector with dimensions corresponding to attributes elicited by participants of the norming study. Vector compo-nents are set to the (normalized) frequency with which participants generated the corresponding at-tribute. We show results for three models, using all attributes except those classified as visual (T), only Table 4: Word pairs with highest semantic and vi-sual similarity according to SAE model. Pairs are ranked from highest to lowest similarity. visual attributes (V), and all available attributes mance of a model based solely on textual attributes (which we obtain from Strudel), visual attributes (obtained from our classifiers), and their concate-nation (see row Attributes in Table 3, and columns T, V, and T+V, respectively). The automatically obtained textual and visual attribute vectors serve as input to SVD, kCCA, and our stacked autoen-coder (SAE). The third row in the table presents three variants of our model trained on textual and visual attributes only (T and V, respectively) and on both modalities jointly (T+V).

Recall that participants were asked to provide ratings on two dimensions, namely semantic and visual similarity. We would expect the textual modality to be more dominant when modeling se-mantic similarity and conversely the perceptual modality to be stronger with respect to visual sim-ilarity. This is borne out in our unimodal SAEs. The textual SAE correlates better with seman-tic similarity judgments (  X  = 0.65) than its vi-sual equivalent (  X  = 0.60). And the visual SAE correlates better with visual similarity judgments (  X  = 0.60) compared to the textual SAE (  X  = 0.52). Interestingly, the bimodal SAE is better than the unimodal variants on both types of similarity judg-ments, semantic and visual. This suggests that both modalities contribute complementary infor-mation and that the SAE model is able to extract a shared representation which improves general-ization performance across tasks by learning them Table 5: F-score results on concept categorization. jointly. The bimodal autoencoder (SAE, T+V) outperforms all other bimodal models on both sim-ilarity tasks. It yields a correlation coefficient of  X  = 0.70 on semantic similarity and  X  = 0.64 on visual similarity. Human agreement on the former task is 0.76 and 0.63 on the latter. Table 4 shows examples of word pairs with highest semantic and visual similarity according to the SAE model.
We also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA. This indicates that the attribute-based representation is a powerful predictor on its own. Interestingly, both Bruni et al. (2013) and Mikolov et al. (2011) which do not make use of attributes are out-performed by all other attribute-based sys-tems (see columns T and T+V in Table 3).

Our results on the categorization task are given in Table 5. In this task, simple concatenation of vi-sual and textual attributes does not yield improved performance over the individual modalities (see row Attributes in Table 5). In contrast, all bimodal models (SVD, kCCA, and SAE) are better than their unimodal equivalents and RNN-640. The SAE outperforms both kCCA and SVD by a large margin delivering clustering performance similar to the McRae et al. X  X  (2005) norms. Table 6 shows examples of clusters produced by Chinese Whis-pers when using vector representations provided by the SAE model.

In sum, our experiments show that the bi-modal SAE model delivers superior performance across the board when compared against competi-tive baselines and related models. It is interesting to note that the unimodal SAEs are in most cases better than the raw textual or visual attributes. This indicates that higher level embeddings may be beneficial to NLP tasks in general, not only to those requiring multimodal information.
 Table 6: Examples of clusters produced by CW using the representations obtained from the SAE model. In this paper, we presented a model that uses stacked autoencoders to learn grounded meaning representations by simultaneously combining tex-tual and visual modalities. The two modalities are encoded as vectors of natural language attributes and are obtained automatically from decoupled text and image data. To the best of our knowl-edge, our model is novel in its use of attribute-based input in a deep neural network. Experimen-tal results in two tasks, namely simulation of word similarity and word categorization, show that our model outperforms competitive baselines and re-lated models trained on the same attribute-based input. Our evaluation also reveals that the bimodal models are superior to their unimodal counterparts and that higher-level unimodal representations are better than the raw input. In the future, we would like to apply our model to other tasks, such as im-age and text retrieval (Hodosh et al., 2013; Socher et al., 2013b), zero-shot learning (Socher et al., 2013a), and word learning (Yu and Ballard, 2007). Acknowledgment We would like to thank Vit-torio Ferrari, Iain Murray and members of the ILCC at the School of Informatics for their valu-able feedback. We acknowledge the support of EPSRC through project grant EP/I037415/1. Agirre, Eneko and Aitor Soroa. 2007. SemEval-2007 Task 02: Evaluating Word Sense Induc-tion and Discrimination Systems. In Proceed-ings of the Fourth International Workshop on
Semantic Evaluations . Prague, Czech Republic, pages 7 X 12.
 Andrews, M., G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data to Learn Semantic Representations. Psycholog-ical Review 116(3):463 X 498.
 Baroni, M., B. Murphy, E. Barbu, and M. Poe-sio. 2010. Strudel: A Corpus-Based Semantic
Model Based on Properties and Types. Cogni-tive Science 34(2):222 X 254.
 Barsalou, Lawrence W. 2008. Grounded Cogni-tion. Annual Review of Psychology 59:617 X 845. Bengio, Y., P. Lamblin, D. Popovici, and
H. Larochelle. 2007. Greedy Layer-Wise Train-ing of Deep Networks. In Bernhard Sch  X  olkopf,
John Platt, and Thomas Hoffman, editors, Ad-vances in Neural Information Processing Sys-tems 19 . MIT Press, pages 153 X 160.
 Bengio, Yoshua. 2009. Learning Deep Architec-tures for AI. Foundations and Trends in Ma-chine Learning 2(1):1 X 127.
 Biemann, Chris. 2006. Chinese Whispers  X  an Ef-ficient Graph Clustering Algorithm and its Ap-plication to Natural Language Processing Prob-lems. In Proceedings of TextGraphs: the 1st
Workshop on Graph Based Methods for Natu-ral Language Processing . New York, NY, pages 73 X 80.
 Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3:993 X 1022.
 Bruni, E., G. Boleda, M. Baroni, and N. Tran. 2012a. Distributional Semantics in Technicolor.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics . Jeju Island, Korea, pages 136 X 145.
 Bruni, E., U. Bordignon, A. Liska, J. Uijlings, and
I. Sergienya. 2013. Vsem: An open library for visual semantics representation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demon-strations . Sofia, Bulgaria, pages 187 X 192. Bruni, E., N. Tran, and M. Baroni. 2014. Multi-modal distributional semantics. J. Artif. Intell. Res. (JAIR) 49:1 X 47.
 Bruni, E., J. Uijlings, M. Baroni, and N. Sebe. 2012b. Distributional Semantics with Eyes: Us-ing Image Analysis to Improve Computational
Representations of Word Meaning. In Proceed-ings of the 20th ACM International Conference on Multimedia . Nara, Japan, pages 1219 X 1228. Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural Language Processing (almost) from Scratch.
Journal of Machine Learning Research 12:2493 X 2537.
 Deng, J., W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale
Hierarchical Image Database. In Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition . Mi-ami, Florida, pages 248 X 255.
 Farhadi, A., I. Endres, D. Hoiem, and D. Forsyth. 2009. Describing Objects by their Attributes.
In Proceedings of the IEEE Computer Soci-ety Conference on Computer Vision and Pat-tern Recognition . Miami Beach, Florida, pages 1778 X 1785.
 Feng, Fangxiang, Ruifan Li, and Xiaojie Wang. 2013. Constructing Hierarchical Image-tags Bi-modal Representations for Word Tags Alter-native Choice. In Proceedings of the ICML 2013 Workshop on Challenges in Representa-tion Learning . Atlanta, Georgia.
 Feng, Yansong and Mirella Lapata. 2010. Visual
Information in Semantic Representation. In Hu-man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the Association for Computational Linguistics . Los Angeles, California, pages 91 X 99.
 Finkelstein, L., E. Gabrilovich, Y. Matias,
E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems 20(1):116 X 131.
 Fountain, Trevor and Mirella Lapata. 2010. Mean-ing Representation in Natural Language Cat-egorization. In Proceedings of the 31st An-nual Conference of the Cognitive Science Soci-ety . Amsterdam, The Netherlands, pages 1916 X  1921. Golub, Gene and Christian Reinsch. 1970. Sin-gular Value Decomposition and Least Squares
Solutions. Numerische Mathematik 14(5):403 X  420.
 Griffiths, T. L., M. Steyvers, and J. B. Tenenbaum. 2007. Topics in Semantic Representation. Psy-chological Review 114(2):211 X 244.
 Hardoon, D. R., S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analy-sis: An Overview with Application to Learning
Methods. Neural Computation 16(12):2639 X  2664.
 Harris, Zellig. 1970. Distributional Structure. In
Papers in Structural and Transformational Lin-guistics , pages 775 X 794.
 Hinton, Geoffrey E. and Ruslan R. Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science 313(5786):504 X  507.
 Hodosh, Micah, Peter Young, and Julia Hocken-maier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation
Metrics. Journal of Artificial Intelligence Re-search 47:853 X 899.
 Huang, Jing and Brian Kingsbury. 2013. Audio-visual Deep Learning for Noise Robust Speech
Recognition. In Proceedings of the 38th Inter-national Conference on Acoustics, Speech, and
Signal Processing . Vancouver, Canada, pages 7596 X 7599.
 Jones, R., B. Rey, O. Madani, and W. Greiner. 2006. Generating Query Substititions. In Pro-ceedings of the 15th International Conference on the World-Wide Web . Edinburgh, Scotland, pages 387 X 396.
 Landau, B., L. Smith, and S. Jones. 1998. Object
Perception and Object Naming in Early Devel-opment. Trends in Cognitive Science 27:19 X 24. Landauer, Thomas and Susan T. Dumais. 1997. A
Solution to Plato X  X  Problem: the Latent Seman-tic Analysis Theory of Acquisition, Induction, and Representation of Knowledge. Psychologi-cal Review 104(2):211 X 240.
 Lowe, D. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Jour-nal of Computer Vision 60(2):91 X 110.
 Manning, C. D., P. Raghavan, and H. Sch  X  utze. 2008. Introduction to Information Retrieval . Cambridge University Press, New York, NY.
 McRae, K., G. S. Cree, M. S. Seidenberg, and
C. McNorgan. 2005. Semantic Feature Pro-duction Norms for a Large Set of Living and
Nonliving Things. Behavior Research Methods 37(4):547 X 559.
 Mikolov, T., S. Kombrink, L. Burget, J.  X  Cernock  X  y, and S. Khudanpur. 2011. Extensions of Recur-rent Neural Network Language Model. In Pro-ceedings of the 2011 IEEE International Con-ference on Acoustics, Speech, and Signal Pro-cessing . Prague, Czech Republic, pages 5528 X  5531.
 Mikolov, T., Wen-tau Yih, and G. Zweig. 2013. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013 Conference of the North American Chap-ter of the Association for Computational Lin-guistics: Human Language Technologies . At-lanta, Georgia, pages 746 X 751.
 Miller, George A. and Walter G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes 6(1).
 Nelson, D. L., C. L. McEvoy, and T. A. Schreiber. 1998. The University of South Florida Word Association, Rhyme, and Word Fragment Norms.
 Ngiam, Jiquan, Aditya Khosla, Mingyu Kim,
Juhan Nam, Honglak Lee, and Andrew Ng. 2011. Multimodal Deep Learning. In Pro-ceedings of the 28th International Conference on Machine Learning . Bellevue, Washington, pages 689 X 696.
 Patwardhan, Siddharth and Ted Pedersen. 2006.
Using WordNet-based Context Vectors to Es-timate the Semantic Relatedness of Concepts.
In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Compu-tational Linguistics and Psycholinguistics To-gether . Trento, Italy, pages 1 X 8.
 Ranzato, Marc X  X urelio and Martin Szummer. 2008. Semi-supervised Learning of Com-pact Document Representations with Deep Net-works. In Proceedings of the 25th International Conference on Machine Learning . Helsinki, Finland, pages 792 X 799.
 Regier, Terry. 1996. The Human Semantic Poten-tial . MIT Press, Cambridge, Massachusetts. Roller, Stephen and Sabine Schulte im Walde. 2013. A Multimodal LDA Model integrating Textual, Cognitive and Visual Modalities. In
Proceedings of the 2013 Conference on Empir-ical Methods in Natural Language Processing . Seattle, Washington, pages 1146 X 1157.
 Sebastiani, Fabrizio. 2002. Machine Learning in
Automated Text Categorization. ACM Comput-ing Surveys 34:1 X 47.
 Silberer, C., V. Ferrari, and M. Lapata. 2013. Mod-els of Semantic Representation with Visual At-tributes. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics . Sofia, Bulgaria, pages 572 X 582. Silberer, Carina and Mirella Lapata. 2012. Grounded Models of Semantic Representation. In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning . Jeju Island, Korea, pages 1423 X 1433. Socher, R., M. Ganjoo, C. D. Manning, and A. Y.
Ng. 2013a. Zero-shot learning through cross-modal transfer. In Advances in Neural Informa-tion Processing Systems 26 , pages 935 X 943. Socher, R., Quoc V. Le, C. D. Manning, and A. Y.
Ng. 2013b. Grounded Compositional Seman-tics for Finding and Describing Images with Sentences. In Proceedings of the NIPS Deep Learning Workshop .
 Socher, R., J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. 2011. Semi-Supervised Re-cursive Autoencoders for Predicting Sentiment
Distributions. In Proceedings of the 2011 Con-ference on Empirical Methods in Natural Lan-guage Processing . Edinburgh, Scotland, pages 151 X 161.
 Srivastava, Nitish and Ruslan Salakhutdinov. 2012. Multimodal Learning with Deep Boltz-mann Machines. In Advances in Neural In-formation Processing Systems 25 , pages 2231 X  2239.
 Steyvers, Mark. 2010. Combining Feature Norms and Text Data with Topic Models. Acta Psycho-logica 133(3):234 X 342.
 Szumlanski, S. R., F. Gomez, and V. K. Sims. 2013. A New Set of Norms for Semantic Re-latedness Measures. In Proceedings of the 51st
Annual Meeting of the Association for Compu-tational Linguistics . Sofia, Bulgaria, pages 890 X  895.
 Turney, Peter D. and Patrick Pantel. 2010. From
Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research 37(1):141 X 188.
 Vincent, P., H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol. 2010. Stacked Denoising Au-toencoders: Learning Useful Representations in a Deep Network with a Local Denoising Cri-terion. Journal of Machine Learning Research 11:3371 X 3408. von Ahn, Luis and Laura Dabbish. 2004. Labeling
Images with a Computer Game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . Vienna, Austria, pages 319 X 326.
 Wu, Pengcheng, Steven C. H. Hoi, Hao Xia, Peilin Zhao, Dayong Wang, and Chunyan Miao. 2013.

Online Multimodal Deep Similarity Learning with Application to Image Retrieval. In Pro-ceedings of the 21st ACM International Con-ference on Multimedia . Barcelona, Spain, pages 153 X 162.
 Yih, Wen-tau, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics . Sofia, Bulgaria, pages 1744 X 1753. Yu, C. and D. H. Ballard. 2007. A Unified Model of Early Word Learning Integrating Statistical and Social Cues. Neurocomputing 70:2149 X  2165.
