 1. Introduction
One of the main concerns in the design of multi-objective evolutionary algorithms (MOEAs) has been to ensure the quality of the sample set of Pareto-optimal estimates that is generated by the algorithm. The quality measure is, by itself, multi-dimensional, and there are not, up to now, any de fi nitive standards that represent it ( Zitzler et al., 2003; Auger et al., 2012 ). A high-quality solution set can be de fi ned as a set of samples that reach, as long as possible, the exact Pareto-set, and is representative of al., 2010 ). It should be noticed that a MOEA can be built with the purpose of describing a subset of the Pareto set, in the cases in which some a priori or online decision information is available ( Kim et al., 2012; Sinha et al., 2013 ). In those cases, the quality measures should refer to the representation of such subsets ( Auger et al., 2012 ).
 enhance the ability of a MOEA to represent the Pareto set in detail, with a uniform spread of the samples: the sphere-control operators . Such operators are based on the information about the distances between every pair of solution samples in a set motivates the denomination of  X  sphere  X  operators. The key idea is the usage of a feedback-control inspired scheme ( Ogata, 2001 )in order to establish a dynamic equilibrium along the algorithm iterations, associated to the high-quality description of the Pareto-set. While such a high-quality description is not reached, some measured variables do not reach an equilibrium, causing a control action that will enhance a quality metric.
 of the operational and screw geometrical parameters of a single screw polymer extrusion system. In this problem, a modeling routine describing properly the complex process developed is necessary. This involves the mathematical description of the different phases suf-fered by the polymer inside the machine ( Gaspar-Cunha, 2009 ). This practical problem of strong indus trial relevance concerns the employment of multiobjective op timization on a simulation model for the purpose of gaining knowledge about possible ef fi operating modes to be implemented in a real machine. The main purpose of such a multiobjective optimization study is to obtain different optimal solutions that will help the polymer engineers in selecting the best operating conditions and/or screw geometry.
Further discussions about the appl ication of evolutionary computa-tion methods for problems in materials science and engineering are reported in Paszkowicz (2013) .

An instance of application of a feedback-control inspired scheme, as proposed here, is represented in Fig. 1 . In this the measured variable is the error e  X  q a , which feeds a mechanism that is inspired in the proportional-integral (PI) controller, which in turn determines the value of the control variable, while the variable a , which represents the number of
The pre-established reference number of points in the archive, be reached due to the feedback mechanism.

In the equilibrium, e  X  0 (which means the desired result of a  X  q ). As in other contexts of application of feedback-control techniques, the role of the feedback-control inspired scheme here is to induce an overall system behavior that presents low sensi-tivity to variations in the initial conditions and in the algorithm parameter values, delivering stable results, with repeatability in the reach of high-quality solution sets ( Ogata, 2001 ). The error variables are de fi ned here such that the feedback loop reaches an equilibrium only when a good description of the Pareto-set is attained.

Speci fi cally, two sphere-control operators are presented here: an archive-set reduction operator, which controls the number of non-dominated solutions that are stored, and a crossover operator archive-set reduction plays the role of solution density reduction in the most crowded regions of the Pareto-set. The surface-crossover is to be applied in the less crowded regions of the
Pareto-set, in order to fi ll eventual gaps in its description. The equilibrium between the opposite actions of surface-fi lling and archive-reduction, attained by a feedback-control inspired scheme, leads to an ultimate description of the Pareto-set that is composed of samples that are evenly distributed in the space of objectives.
Feedback-control inspired mechanisms, based on the principles of a switched controller and of a proportional-integral (PI) con-troller, are employed in the surface-fi lling operators and in the archive-set reduction operator, in order to enhance the distribu-tion of solutions along the Pareto surface. This motivates the denomination of  X  sphere-control  X  operators. These operators are to be employed together, since their effects are complementary, and their dynamic interaction is necessary in order to achieve the desired behavior.

It should be noticed here that the idea of feedback-control inspiration constitutes a further step beyond some studies that have considered the theme of parameter adaptation in evolution-ary computation. Examples of those studies can be found for instance in Vasconcellos et al. (2001) , McGinley et al. (2011) , Lin and Chen (2013) , and Jebari et al. (2013) . The introduction of feedback control concepts in order to state the parameter adapta-tion procedures allows the recovery, within the fi eld of evolu-tionary computation, of some well-established results from control theory concerning closed-loop system dynamics and stability. Some preliminary studies concerning the proposed feedback-control scheme have been presented by the authors in Takahashi et al. (2009) . The ideas presented here also have connection with the ones presented in Takahashi et al. (2004) and Silva et al. (2007) which employ  X  sphere  X  operations which are similar to the archive-set reduction operator presented here, yet without any feedback adaptation scheme.

The basic idea, both in that references and here, is that a  X  sphere  X  means roughly a domain in which the information gained by a solution point in its center would be representative need of further function evaluations inside such a sphere. Bui et al. (2008, 2009) also employ the concept of  X  spheres  X  for construc-tion of an MOEA (multi-objective evolutionary algorithm), with a dual meaning: in that cases, the  X  sphere  X  is the domain in which a local search is conducted, with sub-populations assigned to per-form searches inside each sphere.

In the speci fi c formulation that is presented here, the proposed operators are structured for continuous-variable spaces. However, the adaptation for discrete-variable problems can be performed directly, provided that some distance metric becomes de fi the discrete-variable space. This can be performed according to the guidelines presented by Carrano et al. (2010) .

It should be noticed that the proposed methodology may be supplemented by the usage of local search operators, that can enhance the precision of solutions if applied along the algorithm iterations ( Wanner et al., 2008 ), or even provide a certi optimality in some cases ( Takahashi et al., 2011 ). Those hybridiza-tions are not discussed further, in order to privilege the presenta-tion of the main aspects of the methodology proposed here.
This paper is structured as follows. A discussion about multi-objective evolutionary algorithms is provided in Section 2 . The proposed feedback-control inspired operators are presented in Section 4 . Section 3 presents a modi fi ed version of the classical NSGA-II algorithm, in which the proposed sphere-control opera-tors are included. The results of numerical tests on some bench-mark problems are presented in Section 5 . The numerical tests show that the proposed methodology leads to a signi fi cant enhancement of the ability of the algorithm for reaching a the proposed algorithm is employed in the problem of polymer extrusion process design. The results are compared with the ones obtained using the basic NSGA-II and with another algorithm, the RPSGA, that was employed formerly for dealing with the same problem. The results support the conclusion that the proposed methodology is able to signi fi cantly enhance the description of the ef fi cient solution set in this practical problem too. 2. Multiobjective evolutionary algorithms
Consider f  X  X  : R n  X  R m a vector-valued real function. Let f denote the i -th coordinate of the function in the image space. The multiobjective problems appear from the partial ordering induced by the relation of dominance : u ! v 3 Consider the Pareto-set P de fi ned by
P 9 f x n A  X  j  X  x A  X  such that x ! x n g X  2  X  in which x A R n is the decision variable vector, and  X  D R n feasible set. A multiobjective optimization problem is de the task of generating samples of the set P . A multiobjective evolutionary algorithm (MOEA) is an evolutionary algorithm which is intended to produce a set of samples of P . At the end of the execution, the archive set A k contains the algorithm out-come, which constitutes an estimate of the Pareto-set P . 3. MOEA with sphere-control operators
This paper presents new evolutionary operators which are not intended to be algorithm-speci fi c. Instead, they should be suitable to be inserted in any MOEA that works with archive solutions. For this reason, in this paper the proposed operators are inserted within a standard MOEA, the Non-Dominated Sorting Genetic
Algorithm II (NSGA-II) ( Deb et al., 2002 ), in order to de working algorithm. The resulting algorithm, named Sphere-
Control Multiobjective Genetic Algorithm (SCMGA), is compared here with the basic NSGA-II over a set of benchmark problems, in order to establish some fi gures about the effect of introducing the sphere-control operators inside a known MOEA. The basic NSGA-II and the proposed SCMGA are described brie fl y in this section. 3.1. The NSGA-II
The Non-dominated Sorting Genetic Algorithm ( Deb et al., 2002 ), or simply NSGA-II, has been used for dealing with the multiobjective optimization problems considered here. For a full description of that algorithm, the reader should refer to the original reference. The following procedures, which are part of
NSGA-II, will be employed in the next section in the construction of the SCMGA (the Sphere-Control Multiobjective Genetic Algorithm):
A  X  non _ dominated  X  P  X  returns the individuals that lie in the fi rst front of the population P ;
F  X  fast _ non _ dominated _ sorting  X  P  X  sorts the individuals of population according to fronts, the solutions in each front dominated only by solutions in the former ones;
C i  X  crowding _ distance _ assignment  X  F i  X  employs a crowding distance assignment procedure to estimate how the solutions of front i are spread in the objective space;
Q  X  selection  X  P ; N  X  uses binary stochastic tournaments with replacement for performing selection of a new population Q , composed of N individuals, from population P . 3.2. The SCMGA The structure of the Sphere-Control Multiobjective Genetic Algorithm (SCMGA), is similar to the one of the original NSGA-II.
Basically, this algorithm presents three major differences in rela-tion to the original one:
Archive size control scheme : In the original NSGA-II, an elitist selection scheme allows the current population to serve as an archive, which is built upon the crowding distance measure.
However, while the control of the archive size based on crowd-ing distance is ef fi cient for two objectives, it is not so reliable when more objective functions are considered ( Kukkonen and
Deb, 2006 ). In the proposed algorithm, an external archive uses section. Algorithm 1 shows the pseudo-code for the SCMGA algorithm.
 Algorithm 1. Pseudocode for SCMGA.

A  X   X  _ Archive _ Control  X  A ;  X   X   X  Update _  X   X  A ; q  X  end while the controlled parameters  X  ,  X  and r.  X  is set in such a way that q spheres of dimension  X  m 1  X  occupy a volume equivalent to the unitary simplex of dimension  X  m 1  X  .  X  and r are started with the mean value of the intervals which are de fi ned by the lower and the upper bounds respectively. 4. The sphere operators
Consider the sets A and P respectively meaning the archive and the current population:
A 9 f ~ x 1 ; ... ; ~ x a g
P 9 f x 1 ; ... ; x p g X  3  X  with j A j X  a and j P j X  p . The images of such sets in the objective space are denoted by
A  X f ~ y 1 ; ... ; ~ y a g 9 f f  X  ~ x 1  X  ; ... ; f  X  ~ x a  X g
P  X f y 1 ; ... ; y p g 9 f f  X  x 1  X  ; ... ; f  X  x p  X g  X  4  X 
In this paper it is considered that both sets A y and P y ized between 0 and 1, according to extreme values of A y (0 and 1 are assigned to the minimum and maximal value in each dimension respectively). These sets are re-normalized every time
A is updated.

The main idea which is behind the  X  sphere operators  X  is to enhance representativity of the archive A by evaluating its reg-ularity. If the solution samples regularly cover the set P should be located in relation to their nearest neighbors such that the distances to them become of the same order of magnitude.
Therefore, a parameter  X  , which has the meaning of a reference domain radius for each point, must be de fi ned.

This parameter is employed in order to guide the algorithm operations, with the intent to generate points which approxi-mately  X  represent  X  the region inside the sphere of such radius centered in that point. Any two neighbors should be separated, densely distributed, an operation of decimation should be per-formed. In the regions in which the points are too sparsely distributed, new solution points should be found and included in the archive. The indication of what operation should be performed in each region is obtained via the comparison of the distances between the points with this parameter. The parameter  X  is dynamically adjusted during the algorithm execution, in order to reach a good dispersion of the sample points along the Pareto-set estimate, considering a reference value of the number of sample points that is to be found.

The sphere-control operators to be presented in this section are: (i) the archive size control, and (ii) the surface-fi over. The surface fi lling mutation operator should also be men-tioned, presented in Takahashi et al. (2009) , which is not discussed 4.1. Archive size control
Consider the set A  X f x 1 ; x 2 ; ... ; x m ; x m  X  1 ; ... ; individual minima of the m objective functions have been put in the fi rst m positions, and the remainder a m points have been ordered randomly. The set A y is ordered correspondingly. The archive set size is controlled by Algorithm 2 . After this operation, there will be no two points in A y with pairwise distances smaller than  X  , with the individual minima receiving precedence for being kept in the archive.
 Algorithm 2. Pseudocode for archive size control [  X  _ Archive _ Control ].
 Inputs: A ; A y ;  X 
Outputs: A ; A y i  X  1 while i r j A j do end while Algorithm 3. Pseudocode for  X  Control [ Update _  X  ].
 Inputs: A ; q ;  X  Output:  X 
Parameters: s ; K p ; K n a  X  j A j if a 4 q then else end if
If this operation was executed with a pre-de fi ned value of since the exact extension of the set P is not known a priori . On the other hand, the number of ef fi cient solutions which are kept in the archive is a variable that can be measured easily, and this information can be used in order to adjust  X  , in order to reach a coverage of the set P with a  X  resolution  X   X  .

The adjustment of parameter  X  takes into account a reference number of points that should be stored in the archive set A . Let such a reference number be denoted by q , and let a  X j A j . The adjustment procedure is described in Algorithm 3 . Default values can be indicated for the control parameters: K p  X  0.6, s  X  0.1, K  X  0.9. The initial value of  X  is de fi ned such that q spheres of dimension  X  m 1  X  occupy a volume equivalent to the unitary simplex of dimension  X  m 1  X  .

Algorithm 3 performs the adjustment of the value of  X  in the following way. If the reference number of points q is greater than the current number of points in the archive, a , then the value of should be increased, in order to decrease the archive size. The K p times the relative error e between a and q . However, if e is greater than a threshold s , the value of  X  is saturated as K this threshold s . This saturation is introduced in order to avoid a the number a of points in the archive, possibly harming its convergence.

On the other hand, if the reference number of points q is smaller than the number a of points in the archive, then  X  be decreased, in order to cause the archive size to increase. For the purpose of this reduction in  X  , a constant factor K n is employed, instead of employing a factor proportional to the relative error e . This procedure avoids a very fast decreasing of the size of fi rst steps of the algorithm, when the number of archive solutions is much smaller than the reference number q , and also causes a small step decrease of  X  on any time that a decrease becomes necessary when the system is near the equilibrium. A diagram of such a closed-loop feedback control scheme is presented in Fig. 1 .
This algorithm resembles the PI (proportional-integral) controllers with control signal saturation, which are employed in industrial control systems.

It should be noticed that, as the control action over variable incremental, the net effect has the form of an integral control. This integral term in the PI controller is necessary in order to induce an error-less steady behavior in the closed-loop system. Indeed, if a simple P (proportional only) controller were adopted, the steady state value of the controlled variable a would not become equal to which there is a positive error between a and q ,meaningthat of  X  was itself proportional to the error e then there would be a value of that error that, when multiplied by the constant K give rise to a value of  X  which would cause exactly that value of error to appear  X  causing an equilibrium situation that would not increment to the value of  X  will make the value of  X  to always to zero. In this way, the PI controller structure is the simplest one that ful fi lls the requirement of producing exactly a evenly spaced solutions (most textbooks on classic control theory, for instance
The adoption of such a closed-loop control scheme makes the size of  X  to reach an equilibrium point by itself, avoiding the need of an a priori knowledge about such a parameter, and rendering the multiobjective genetic algorithm robust in relation to this parameter. Notice that even in the degenerate case of the Pareto-set surface being of dimension less than m 1 (one or more objectives being redundant), the algorithm still works as expected, forming an archive set of Pareto estimates which will still have q evenly spaced elements. 4.2. Surface-fi lling crossover v  X  y i  X  X jf y j j J y i y j J o 3  X  gj  X  5  X  which means the cardinality of the set of points from the archive set A y which is inside a ball of radius 3  X  around the function argument y j . Without loss of generality, consider that the set A ordered in increasing order of v  X  y i  X  : A y  X f y i j v  X  y i  X  r v  X  y i  X  1  X g  X  6  X 
The set A receives the corresponding ordering, and j A j X  a . After receiving the ordering as de fi ned by (6) , the archive A becomes arranged in such a way that the fi rst solutions are the ones which have less neighbors in the objective space.
 proportion of elements in archive that will be submitted to the the control variables that will be adjusted dynamically by
Algorithm 5 . The r j A j fi rst solutions of A (after sorting) are chosen to perform the SFXO. The ordering is chosen such that a random solution is initially chosen to be the fi rst parent p
Afterward, the solution which is nearest to the current parent p is chosen, among the solutions which have not been selected yet.
In the end, the procedure delivers a set of parents f p 1 which will suffer crossover. The SFXO will perform the crossover operations of ( p 1 vs. p 2 ), ( p 2 vs. p 3 ), ( p 3 vs. p time. The SFXO operator is de fi ned in Algorithm 4 .
 Algorithm 4. Pseudocode for SFXO [ Surf _ Fill _ XO ]. k ; k 2 ; k 3 ; k 4 ; k 5 ; k 6  X  0
T ;
F  X   X  p  X 
Sequence _ SFXO  X  A ; r  X  for i  X  1to  X  r j A j 1  X  do b  X  Beta  X   X  ;  X   X  ^ x  X   X  1 b  X  p i  X  b p i  X  1 ^ y  X  f  X  ^ x i  X  vf  X  FALSE if J ^ y i f  X  p i  X  J o  X  then end if if J ^ y i f  X  p i  X  1  X  J o  X  then end if if ^ y i ! f  X  p i  X  or ^ y i ! f  X  p i  X  1  X  then else if ^ y i g f  X  p i  X  or ^ y i g f  X  p i  X  1  X  then
Else end if end for
In this algorithm, the command p  X  Sequence _ SFXO  X  A ; r  X  returns the sequence of parents f p 1 ; p 2 ; ... ; p r j A j SFXO operator will be executed.

The main idea of the SFXO is to interpolate solutions which a distance smaller than 3  X  ). These individuals are subjected to order to generate a better description of the Pareto set. The parameter  X  of the Beta distribution has been kept fi xed in  X  4, in order to generate concave probability distribution functions (PDFs). On the other hand, the parameter  X  has been modeled as a controlled variable, which can vary dynami-cally inside the algorithm in order to adjust the Beta distribu-tion: lower values of  X  lead the median of the PDF to approach 1 and higher values of Beta lead the median of the PDF to approach 0. The variable  X  therefore can be used to approximate the offspring solution to either p 1 or p 2 . The shapes of the PDF of Beta distribution for  X   X  4 and different values of  X  are shown in Fig. 3 . Each new solution obtained by the surface-crossover is analyzed, with the results driving the following counters: Counter k 1 : ^ x i dominates p i or p i  X  1 ; Counter k 2 : ^ x i is dominated by p i or p i  X  1 ; Counter k 3 : ^ x i is not dominated by neither p i , p i  X  1 Counter k 4 : ^ x i is not dominated by neither p i nor p Counter k 5 : The distance between ^ x i and p i is lower than Counter k 6 : The distance between ^ x i and p i  X  1 is lower than the respective condition to become true. These six counters are used to estimate the ef fi ciency of the operator and to update r and The pseudo-code for r and  X  tuning is shown in Algorithm 5 . Algorithm 5. Pseudocode for r and  X  control in SFXO [ Update _ r _ Inputs: r ,  X   X  controlled parameters
Inputs: k 1 ; k 2 ; k 3 ; k 4 ; k 5 ; k 6  X  counters used for
Outputs: r ,  X  b r n
SFXO  X  k 1  X  k 2  X  k 3  X  k 4  X  number of SFXO operations S
SFXO  X  k 1  X  k 3  X  number of successful SFXO U SFXO  X  k 2  X  k 4  X  number of unsuccessful if k 5 Z 0 : 25 n SFXO or k 6 Z 0 : 25 end if if S SFXO Z U SFXO then  X  r control else end if if b exp o 0 : 25 then  X   X  controller saturation -else if b exp 4 1 : 25 then  X   X  controller saturation -end if if r inv 4 10 then  X  r controller saturation -else if r inv o 2 then  X  r controller saturation -end if r  X  1
The parameters  X  and r are updated according to the results observed in SFXO operations:
When a signi fi cant part (more than 25%) of the offspring solutions are concentrated near p 1 , the Beta distribution is adjusted in such a way that, in next generation, the operator will have higher chance of generating solutions which are more far away from it. The same holds for the case of p 2 . In this way, the operator tends to generate new solutions which are located not too close to any parent solution.

When the SFXO presents more successful than unsuccessful cases in one generation, the number of expected crossover operators is increased for the next iteration. On the other hand, when more unsuccessful operations are performed this num-ber is decreased.

The adjustment of variables is always performed in small steps, by increasing or reducing 10% of the current value. This is adopted in order to avoid abrupt variations which could cause instability in the algorithm. Both controllers also have lower and upper bounds in the control action.

After the execution of SFXO, all the solutions which are not dominated by any solution of A and whose distance to p 1 and p are higher than  X  are delivered by the SFXO as a set T , which is inserted in the population Q of the GA, in order to improve the quality of the current population.

Assuming that solutions which are near on the parameter space are usually not so far away on the objective space, it is reasonable to admit that a crossover operation would be suitable for blank spaces within archive, performing the interpolation of solutions. It should be noticed that the surface-fi lling crossover operator is likely to generate new points that are located nearby the same manifold in which the parent points lie. This means that, especially in the case of Pareto-set manifolds of relatively low dimension embedded in high-dimensional decision variable spaces, the surface-fi lling crossover tends to be ef fi task of fi nding new Pareto-optimal points. Indeed, in the tests conducted within this study, the surface-fi lling crossover has been was proposed in Takahashi et al. (2009) . Due to this, the surface-fi lling mutation has not been considered in the present work. 4.3. Time complexity employed to perform archive size control is built upon three major (and more costly) operations: (i) extreme solution identi cation: before performing archive size control, it is necessary to identify the extreme points of the current Pareto-front approx-imation, in order to ensure that these points are not displaced during the operator execution. It can be performed in O  X j in which A is the archive that stores the current Pareto-set approximation; (ii) solution distance evaluation: the archive size control algorithm requires that the distances between all points in the archive are known. It is possible to accomplish such a of Algorithm 2 can, in the worst case, compare each solution with all the j A j 1 remaining ones. Therefore, the execution of such a loop is bounded to O  X j A j 2  X  time complexity.
 procedure, it is possible to conclude that the archive size control can be executed in O  X j A j 2  X  time. It should be noticed that the desired number of solutions in the archive is a user-de fi parameter and, therefore, it is not affected neither by the problem dimension nor by the number of objectives.
 operator that is, usually, very similar to the one employed in the original algorithm. Therefore, the complexity of the SFXO can be estimated as r j A j times the computational complexity of the crossover operator. Since the size of j A j is controlled, and r is bounded by a maximum value, the SFXO computational complex-ity is then controlled by the algorithm parameters.
 function evaluations are the main responsible for the algorithm processing time. In those cases, the additional time required to perform SFXO calculations (excluding function evaluation) could be ignored, since it would be insigni fi cant when compared to the time required to evaluate the solutions generated by the operator itself. The number of solutions evaluated inside SFXO is counted as normal algorithm evaluations, in order to keep algorithm com-parisons fair. 5. Numerical tests a series of numerical tests has been conducted over a set of 18 benchmark functions. The description of those benchmark multi-objective optimization problems is presented in the Appendix. tion found in PISA ( Zitzler, 2010 ) has been translated to Matlab 7, in order to be used as benchmark. This algorithm employs exactly the same operators (simulated binary crossover and polynomial mutation, Deb and Agrawal, 1995 ) and structure of the original one. Such as in the PISA implementation, the probability distribu-tion function of the operators are adjusted in order to generate size of the archive has been included in order to keep it at a pre-de fi ned size. This scheme is based on the crowding distance assignment, and it is referred as crowding _ archive _ reduction in
Section 3.1 .
This algorithm has been used also as the basis for the implementation of the sphere-control hybrid algorithm presented in this paper (SCMGA). In the SCMGA, the control of the archive executed at each algorithm generation, as shown in Algorithm 1 . The procedures that adjust  X  ,  X  and r are also executed in the SCMGA variant.

The performances of these two algorithms on the benchmark problems considered here have been compared in order to estimate the effect of the sphere-control operators on the algo-rithm convergence. The performances of the algorithms, in each test problem, are compared through the following steps:
Performance comparison procedure. 1. Each algorithm is executed k times for the same parameter set and stop criterion; 2. Based on the minimum and maximum values achieved for each objective, considering all algorithms and runs, normalize the fronts between 0 and 1 (the values 0 or 1 are not necessarily present on each Pareto-front approximation); 3. For each run i of the k runs: (a) Join the archive obtained by the original NSGA-II ( A (b) Perform dominance on A and create A 0 with the non-(d) Employ some Pareto quality metric PQM for evaluating the 4. Sort the q NSGA II and q SCMGA vectors in ascending order.
This process returns k quantiles associated with the conver-gence performance of the algorithms that are being considered.
These quantiles can be used for performing a stochastic dom-inance analysis, as discussed in Carrano et al. (2010) . From the stochastic dominance principle, it is possible to say that a multi-objective algorithm A 1 stochastically dominates another algorithm
A ( A 1 g A 2 ) if, and only if
A g A 2 :
Therefore, A 1 dominates A 2 if all k quantiles of A 1 are higher or equal the quantiles of A 2 and at least one quantile of A than the same quantile of A 2 . This kind of analysis provides an accurate comparison of the algorithm outcomes.

Although this process is considerably better than single run comparison or visual analysis, it still has a problem: for a moderate number of algorithm executions, the order in which the archives are compared can effect the vectors q , because the result of the dominance analysis is dependent on the sets that are being compared. This order dependence can give rise to inaccurate results, since two different executions of the proposed scheme could lead to different (or even opposite) results. In order to avoid this inconvenient, permutation test procedures have been used for performing algorithm analysis. In each permutation test, random permutations of the indexes of the archives obtained by the algorithms are generated. The archives are compared following such indexes, and the process is repeated for a pre-determined number of times P . Each permutation test provides a set of quantiles q for each algorithm. The fi nal value of the quantile q is obtained by determining the median of all P values assigned to the quantile q i in the permutation tests. This procedure avoids order biasing of the solutions, and reduces the variability between the responses provided by the method when comparing the same algorithms. 5.1. Pareto quality metrics
In this paper, three Pareto quality metrics (PQM) have been used to estimate the quality of the Pareto approximations deliv-ered by the algorithm: the C metric ( Zitzler and Thiele, 1998 ) for integrated sphere counting metric ( Silva et al., 2007 ), which tries to handle with both aspects at once.

C Metric ( C M): The C metric, proposed by Zitzler and Thiele (1998) , estimates the coverage of two Pareto set approximations, based on the dominance principle. Let X be the set of candidate decision vectors for a given problem and let A D X , B D X two sets of decision vectors. The coverage of A with regard to C  X  A ; B  X  , can be evaluated as follows: C  X 
A ; B  X  X  jf a A A j ( b A B : b  X  a gj In this expression, the fraction gives the ratio of solutions from that are dominated by solutions from B .If C  X  A ; B  X  X  0, then not have any dominated solutions when compared to B ; on the at least one solution of B . In this way, a lower value of C means a more suitable Pareto front approximation.

 X  Metric (  X  M): The  X  metric, proposed by Deb et al. (2002) ,is used to estimate how the solutions of the approximated Pareto-set are spaced into the objective domain. Let X be the set of candidate decision vectors for a given problem and let A D X be any set of decision vectors. The  X  metric for A ,  X   X  A  X  , can be calculated as follows:  X  between the extreme points in A and in X n ; d i is the Euclidean distance between the point i A A and its closest neighbor, in the objective space; d is the average of all distances d i ;and lower values of  X  are associated to better spaced Pareto-fronts.
Integrated sphere counting ( ISC ): The integrated sphere counting is used to estimate the quality of the approximated Pareto-sets achieved by the algorithms under analysis. The procedure for ISC calculation is presented in Algorithm 6 . In this algorithm, the symbol n r is the number of radii for which the sphere counting is applied and r d is the vector of radii to be evaluated. In all executions here, the values n r  X  11 and r d  X  X  0 : 01 ; ... ; adopted. Notice that, when comparing Pareto-estimates coming from two algorithms, the ISC metric is applied only to the points of an algorithm that are non-dominated with regard to the points obtained by the other algorithm. In this way, the only relevant feature to be measured is the spread of solutions in the Pareto-estimate set. The ISC metric measures the sample effectiveness of each Pareto-set estimate under different sampling distances. The more regular is the distribution of samples, and the more fi ne-grained is the sampling, the ISC metric will be greater. In this sense, the ISC assigns higher values to better Pareto approximations.

Algorithm 6. Pseudocode for the ISC algorithm. 1: procedure ISC ( A , n r , r d ) 2: ISC  X  0 3: for i  X  1to n r do 4: C  X  A 5: c i  X  1 6: r  X  r d  X  i  X  7: Place a sphere of radius r centered at any point of C 8: while j C j 4 0 do
These quality metrics have been chosen due to their ef fi and low computational complexity. Note that the comparison procedure described earlier in this section can be used with any of these three metrics. However, in the speci fi c case of steps 3a  X  3c of the performance comparison procedure should not be executed, since this metric requires that the input sets have the same number of elements. Besides, the permutation tests are not necessary for this metric, since it does not require pairwise comparisons.

The comparison of the algorithms has been performed using stochastic dominance, as shown in Eq. (7) . A visual information of this comparison is provided using box plots, in which the hor-izontal lines represent (from bottom to top): quantile 0.025, quantile 0.25, quantile 0.50, quantile 0.75 and quantile 0.975. 5.2. Numerical results
This section presents the results achieved by the NSGA-II and the SCMGA algorithms in the benchmark problems that are stated in the Appendix. The comparison of the results has been per-formed using the stochastic dominance procedure. Finally, the results of the comparisons are illustrated using box plots. Both algorithms have been set using the same parameters: Population size: 100 individuals.

Stop criterion  X  maximum number of function evaluations: Pareto-fronts. The exact extreme points have been used in the
QUAD, KUR, FONSECA, VIENNET and DTLZ to evaluate such a metric, since these points are known from the literature. On the other hand, the exact mono-objective optima are not known for the TSP instances, since they have been generated at random. In order to surpass such a limitation, the best solution achieved for each objective, considering all algorithm runs, has been adopted as the respective extreme point.

KUR20 and KUR30 respectively. Figs. 7  X  10 show the results observed for the instances QUAD2 , QUAD4 , QUAD8 and QUAD16 . Fig. 11 shows the result which has been observed for the instance
FONSECA , and Fig. 12 shows the result which has been observed for the instance VIENNET . Figs. 13  X  19 show the results observed for the seven DTLZ instances. Finally, Figs. 20 and 21 show the results observed for the instances TSP2 and TSP3 .
 stochastically dominates the NSGA-II algorithm in most part of cases for all performance metrics (the quantiles of the SCMGA are better than the corresponding quantiles of the NSGA-II). The only exception is the coverage metric ( C M) for the instance DTLZ7, in which NSGA-II achieved better results. However, it should be noticed that the proposed algorithm outperformed the original
NSGA-II for both,  X  M and ISC , in the DTLZ7 instance. The analysis of the instances of KUR problem suggest that the SCMGA is less affected by the increase in the problem dimension than the NSGA-
II, since the difference between the two algorithms increases with the problem dimension. In all instances of problem QUAD, the decision variable space has dimension 20, and the dimension of suggests that the number of objectives does not affect the relative ef fi ciency between the algorithms. In a wide analysis, it is possible to see that the proposed algorithm can obtain a bigger amount of ef fi cient solutions than the NSGA-II. Besides, the solutions achieved by the SCMGA are often better spaced into the objective domain, what is also highly desirable.

The computation times required by both algorithms are shown in Table 1 . The simulations have been performed on a single core of an iMac 27 (model 2011), with Core i7 3.4 GHz processor and 16 GB RAM, using Matlab 2010. In this table, it is shown the mean and the standard deviation observed amongst the runs, in p.u. values. It is possible to note that the difference between the algorithm processing times is not signi fi cant in any of the instances considered. In the worst case, the proposed algorithm is less than 10% slower than the original NSGA-II and it is faster in some instances. It means that the overhead caused by the employ-ment of the self-controlled operators is not signi fi cant when compared to the other operations required by the algorithm. The reader should notice that such a difference would be even smaller in problems in which the time required to evaluate the function is high, such as several real-world problems as the application example presented next in this paper. 6. Application study: polymer extrusion process
This section presents a brief explanation about the polymer extrusion process, which constitutes the system to which the proposed methodology is applied.

Single screw extrusion is one of the most important polymer processing technologies, allowing the production of products such feeding a solid polymer at the beginning of the system (in the hopper), melting and homogenizing it and forcing the melted polymer to pass through a tool called the die that gives the shape to the product to be obtained. Fig. 22 illustrates this procedure. The extruder is constituted by: (i) a hopper, where the solid polymer with the shape of pellets is fed; (ii) a heated barrel; (iii) an Archimedes-type screw rotating inside the barrel at a given speed ( N ); (iv) heater bands, with temperature de the operator and (v) a die.

The process performance depends on three different type of parameters: the polymer properties, the system geometry and the operating conditions. The different polymers are characterized by properties such as thermal (e.g., heat conduction coef fi melting temperature and heat capacity), physical (e.g., friction coef fi cients and density) and rheological (which is a measure of the resistance of the polymer to the fl ow). Usually, the process is optimized in order to process a single polymer, thus the properties are, in this case, constant. In the most simple case, a conventional screw with three geometrical zones, as the one shown in Fig. 22 , is used. First, appears a screw section with constant depth ( H feed section. Then, there is a compression section where the depth a constant but smaller depth ( H 3 ). The screw is also characterized are the variables that are controlled by the operator of the machine. In the present case these variables are the screw rotation speed ( N ) and the barrel temperature pro fi le ( T b1 , T
The main functions of the extruder are to transport the solid material from the hopper to the heated barrel zone, to melt the polymer, to homogenize and mix the melted polymer with the additives usually present and to create the necessary pressure which enables the polymer to pass through the die at the desired output. As stated above, the performance of the extruder depends on the polymer properties, system geometry and operating con-ditions. Taking this into account a thermo-mechanical environ-ment is developed in which the polymer passes through different into the hopper where, by action of gravity, they are transported inside the barrel (solids conveying in the hopper); (ii) then, by action of the screw rotation and due to the friction between the screw and barrel walls the solid polymer is pressurized and a solid bed is formed and, simultaneously, the polymer is transported to the heated barrel zone (solids conveying in the screw); (iii) at this point, due the heat generated by friction and the heat conducted from the barrel a melt fi lm is formed (delay zone); (iv) then, a speci fi c melting mechanism develops, characterized by the exis-tence of a melt pool and melt fi lms around the solid bed (melting zone); (v) fi nally, the polymer is pressurized and is transported to the die (conveying zone).
 functional zones using the appropriate boundary conditions. The program developed is based on fi nite differences, were central and backward differences are applied for the fi rst order derivatives and the implicit Crank  X  Nicolson scheme is applied for the second order derivatives. The system of equations resulting from the analysis, momentum and energy equations, was solved using the method of Gauss elimination with pivoting. The stability of the solutions was assured taking into account the convergence of the velocity fi eld developed, simultaneously with the convergence 1.51.551.61.651.71.751.81.851.9 1.51.551.61.651.71.751.81.851.9 in temperature. More details of the modeling routine implemen-ted can be found elsewhere ( Gaspar-Cunha, 2009 ). Also, due to the complexity of the process, in each functional zone (as identi
Fig. 23 ) the existence of multiple zones is considered and solved simultaneously, i.e., the convergence must be assured simulta-neously for the different zones.

The program developed is able to compute some important performance characteristics of the process as a function of the polymer properties, system geometry and operating conditions.
The process performance is characterized by the mass output of the machine ( Q ), the average melt temperature of the polymer at die exit ( T melt ), the power consumption required to rotate the screw ( Power ), the capacity of pressure generation ( P max mixing capacity measure by the average of deformation ( WATS ) and the length of screw required to melt the polymer ( L melt are the objectives that are employed in the de fi nition of the multiobjective optimization problem studied here.

The interested reader may fi nd further information about the simulation of polymer extrusion processes in such as Vergnes et al. (1998) and Cassagnau et al. (2007) . The topic of optimization of Length (m) Tmelt ( X C) WATS polymer extrusion processes has been also studied in several works, for instance Smith et al. (1998) , Sienz et al. (2006) , and
Lebaal et al. (2009) . It is also worthy to mention that, as the simulation of the polymer extrusion process is computationally expensive, the usage of meta-models (or surrogate models) may be useful for reducing the number of calls of the system numerical model. Some examples of usage of such models may be found, for instance, in Wanner et al. (2008) which presents a local meta-modeling approach, and in Giri et al. (2013) which presents a global approach. 7. Results: application to polymer extrusion
In this section, the proposed algorithm is employed for the study of variable setup in a polymer extrusion system. In the study conducted here, three algorithms are compared: the basic NSGA-II, the proposed SCMGA, and the RPSGA ( Gaspar-Cunha and Covas, 2004; Gaspar-Cunha, 2009 ). This last algorithm is also used here because it was employed, in former studies, for the multiobjective optimization of the same polymer extrusion system studied here, and therefore it provides some reference for the evaluation of the behavior of other algorithms in this problem. All the tests reported in this section were performed with a fi xed budget of 20,000 function evaluations for each algorithm.
 Three different types of studies were carried out as shown in
Table 2 . First, only the operating conditions (i.e., N , T
T ) were considered as decision variables  X  in cases 1  X  4. In a second set of studies, the aim was to optimize the screw geometry (i.e., L 1 , L 2 , H 1 , H 3 , P , and e )  X  in cases 5  X  decision variables were considered  X  in the cases 8  X  10. Due to the complexity of the process and with the aim of understanding better the variable interactions in the optimization procedure, in some preliminary studies only two objectives were considered in each run. In this case the mass output was considered the most all optimization runs. Table 3 lists the objectives used, the aim of optimization and their range of variation.
 to compare the fi nal population for all runs. The method attributes in one single run. It is not possible to compute the true attainment function, but it can be estimated based on an approximation to set samples, i.e., different approximations obtained in different runs, denoted as empirical attainment function (EAF) ( Fonseca and
Fleming, 1996 ). The differences between two algorithms can be visualized by plotting the points in the objective space where the differences between the empirical attainment functions of the two algorithms are signi fi cant ( L X pez-Ib X nez et al., 2010 ). and SCMGA. Ten different runs for case 1 were performed. For this particular case, NSGA-II is slightly better than RPSGA, but SCMGA is better than the NSGA-II, since there are some black dots that indicate that the SCMGA is better in at least 80% of the runs than
NSGA-II. Figs. 25  X  27 show the results obtained by the different methods (RPSGA, NSGA-II and SCMGA) for the remaining cases studied. Fig. 25 shows that for the cases were the operating conditions were optimized the better performance of the SCMGA is mainly accomplished in the extreme limits of the objective functions. In case study 4, in which all fi ve objectives are considered, the SCMGA was able to fi nd Pareto-optimal solutions much more spread over the search space. The results for cases 7 and 10 are very similar to results obtained for case 4. Finally,
Fig. 27 shows the results for cases 6 and 9 were both objectives are Tmelt ( X C) WATS to be maximized. In these cases the differences between the two methods considered were not so evident due to the fact that now, in the extrusion process, the geometry of the system is allowed to vary. The effect of geometry variation is evident if these results were compared with the results of case 3 represented in Fig. 25 (e) and (f).

The outcomes of the multiobjective optimization  X  the Pareto-optimal solutions  X  may be used by a decision-maker as a means to assist the choice of the parameter con fi guration to be employed in the manufacturing of a speci fi c product. The Pareto-optimal set carries the information about the limits of performance of the plant for that task, including the information about the trade-offs involved in the choice between different Pareto-optimal solutions.
In this speci fi c case study, the trade-offs are related to energy consumption, production capacity and product quality. A good description of the Pareto-set covering all its extension with a uniform sampling is important in order to provide the full set of alternatives to the decision-maker. The SCMGA method was shown to be an effective way to fi nd out good descriptions of the Pareto-set that were not possible to obtain by other algo-rithms, mainly when more than two objectives are considered.
This suggests that the proposed methodology is suitable for dealing with problems in application domains similar to the one studied here. 8. Conclusion
The main contribution of this paper is the proposal of feedback-control techniques for performing parameter tuning within the operators of evolutionary algorithms. This concept has been demonstrated by the construction of feedback-control-based operators that are intended to enhance the description of Pareto-sets in multiobjective evolutionary algorithms. The proposed new operators, the archive reduction operator and the space-fi crossover were shown to present the predicted behavior.
The archive reduction plus the space-fi lling crossover were included in the canonical NSGA-II, with the hybrid algorithm (the SCMGA) used for the purpose of performing some comparisons with the original algorithm. In this way, some quantitative comparisons were performed, showing a signi fi cant enhance-ment of the resulting Pareto-set descriptions obtained by the hybrid algorithm, in several benchmark problems. The numerical experiments that have been conducted on those benchmark functions also showed that the computational overhead caused by the proposed operators is smaller than 10%, not considering the computational effort devoted to objective function evaluation.

The proposed SCMGA was employed in the multiobjective optimization of a polymer extrusion system. Some speci fi clusions about the case study are: (i) The SCMGA was able to generate a much enhanced description of the Pareto-set of the problem, providing both a more uniform sampling and a more complete description of the whole extension of that set. (ii) It was also veri fi ed that the difference of the performance of SCMGA in comparison with the other algorithms was greater in the case of three objectives than in the case of two objectives.

Those features indicate that the proposed SCMGA may be a good choice as a tool for providing the detailed information about the Pareto-optimal set, in order to assist the decision-maker in tasks such as the choice of the con fi guration of a polymer extrusion machine.
 Acknowledgments This work was supported by the Brazilian agencies CNPq, CAPES and FAPEMIG. The authors also acknowledge the support by a Marie Curie International Research Staff Exchange Scheme Fellowship within the 7th European Community Framework Programme.
 Appendix A. Description of benchmark problems
The benchmark multiobjective optimization problems employed in the numerical tests are described in this appendix. Kursawe problem ( Kursawe, 1991 ): X n  X  arg min subject to 5 r x i r 5 8 i  X  1 ; ... ; n  X  11  X  in which n is the number of problem variables.
 Three instances of the Kursawe problem are considered here: KUR10 : n  X  10; KUR20 : n  X  20;
KUR30 : n  X  30. Quadratic problem: X n  X  arg min subject to 10 r x i r 10 8 i  X  1 ; ... ; n  X  13  X  in which n is the number of problem variables; I is the n n identity matrix; C i is the minimum of the quadratic function i :
C  X 
Although quadratic problems are often easy, this example is useful to evaluate the performance of the proposed algorithm when the number of objective function increases. Four instances of the quadratic problem have been considered:
QUAD2 : Two objective problem, considering only the functions f 1 and f 2 ;
QUAD4 : Four objective problem, considering the functions
QUAD8 : Eight objective problem, considering the functions QUAD16 : Sixteen objective problem, considering all functions. In all instances it has been assumed n  X  20.
 Fonseca and Fleming problem ( Fonseca and Fleming, 1995 ): X n  X  arg min subject to 4 r x i r 4 8 i  X  1 ; ... ; n  X  15  X  in which n is the number of problem variables.
 It has been considered a single instance of the Fonseca and Fleming problem: FONSECA : n  X  15.
 Viennet problem ( Viennet, 1996 ): X n  X  arg min subject to 3 r x i r 3 8 i  X  1 ; 2  X  17  X 
A single instance of the Viennet problem has been considered, which has been labeled as VIENNET .
 DTLZ problems ( Deb et al., 2001 ):
All the seven unconstrained instances of the DTLZ family have been tested in this work, as follows.

DTLZ1 : n  X  7, k  X  5, M  X  3: min
DTLZ2 : n  X  12, k  X  10, M  X  3: min
DTLZ3 : n  X  12, k  X  10, M  X  3: min
DTLZ4 : n  X  12, k  X  10, M  X  3,  X   X  100: min
DTLZ5 : n  X  12, k  X  10, M  X  3: min
DTLZ6 : n  X  12, k  X  10, M  X  3: min
DTLZ7 : n  X  22, k  X  20, M  X  3: min g 1  X  x  X  X  100 fj x j X   X  n g 2  X  x  X  X   X  n g 3  X  x  X  X   X  n g 4  X  x  X  X  1  X  9 j x j  X  n h  X  x  X  X  3  X  2 subject to 0 r x i r 1 8 i A 1 ; ... ; n  X  32  X  to employ them with any number of objective functions. The number of objectives in such problems is de fi ned by two para-meters: n , which is the number of decision variables, and; k , which is a parameter chosen a priori . The k values have been chosen such as recommend on the original reference and the n values have been set in order to build problems with three objective functions. Traveling salesman problem ( Schrijver, 2005 ):  X  arg min subject to which is 1 when the cities i and j are directly connected or 0 otherwise; C k is the n n cost matrix k in which each cell c de fi nes the cost of connecting the cities i and j directly.
The only difference between the three objective functions shown in (33) is the cost matrix C k . In a practical case it could be interpreted as three cost matrices that describe different physical quantities, such as distance, time and fuel consumption.
Two instances of the traveling salesman problem (TSP) have been considered:
TSP2 : Two objective problem, considering only the functions f and f 2 ;
TSP3 : Three objective problem, considering all the three functions.
 In both instances it has been assumed that n  X  50 (50 cities).
Solution encoding and decoding : In all problems described above, the solutions have been encoded in real vectors
V  X  X  1 n , in which each cell v i can assume a continuous value in the interval f 0 ; 1 g .
 In the continuous problems (Kursawe, Quadratic, Fonseca and
Fleming, Viennet and DTLZs) the solutions have been decoded to the problem domain by using the following equation: x  X  L i  X  v i  X  U i L i  X  X  35  X  in which x i is the real value of the i -th variable in the problem domain, and L i and U i are the lower and upper bounds of the i -th variable respectively.

In the case of the TSP, the solutions have been decoded using the random keys encoding scheme ( Bean, 1994 ). This procedure transforms a real vector 1 n in a permutation of n elements.
This permutation is used as a candidate solution for the TSP (sequence of city visitations). With this trick, the same Euclidean distance of real variable spaces could be used in this combinatorial problem.
 References
