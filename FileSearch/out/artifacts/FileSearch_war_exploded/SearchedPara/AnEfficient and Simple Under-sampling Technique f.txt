 Imbalanced time series classification (TSC) involving many real-world applications has increasingly captured attention of researchers. Previous work has proposed an intelligent-structure preserving over-sampling method (SPO), which the authors claimed achieved better performance than oth-er existing over-sampling and state-of-the-art methods in TSC. The main disadvantage of over-sampling methods is that they significantly increase the computational cost of training a classification model due to the addition of new minority class instances to balance data-sets with high di-mensional features. These challenging issues have motivated us to find a simple and efficient solution for imbalanced TSC. Statistical tests are applied to validate our conclusions. The experimental results demonstrate that this proposed simple random under-sampling technique with SVM is efficient and can achieve results that compare favorably with the existing complicated SPO method for imbalanced TSC.
 H.2.8 [ Database Management ]: Database Applications -Data Mining.
 Algorithms, Performance, Experimentation, Verification. Imbalanced Time Series Classification, Under-sampling, SVM.
The problem of mining time series classification (TSC) has captured the interest of machine learning and data mining communities. Mining TSC is still an active research area as a result of the various challenging issues yet to be solved, especially in imbalanced TSC (ITSC).

ITSC refers to the training samples of TSC which are non-uniformly distributed with unequal cost among class-es. In this study, only binary ITSC is considered. ITSC involving many real world applications in various domains, such as financial stock market data analysis [11], and bioin-formatics [24], has increasingly captured the attention of researchers. The various challenges, e.g., high dimensionali-ty, large scale, non-uniform distribution with different cost-s of mis-classification errors between different classes, and the nature of numerical attributes considered as a whole in-stead of individual numerical attributes, e.g., the sequence of attributes carrying information with a special connection between them [9], mean that many supervised learning algo-rithms are not effective for ITSC. Moreover, classical kernel functions, e.g., Gaussian RBF and polynomial, are normally not appropriate for SVM-based TSC [25].

Support Vector Machines (SVM) with a strong theoreti-cal foundation has been popularly applied and has achieved great success in many real world applications; however it is unclear whether it works well for ITSC. The use of sampling techniques is a common way to solve imbalanced class dis-tributions; however it is unclear whether an under-sampling technique is effective and efficient for ITSC. Buza et al. [2] s-tate that the simple nearest-neighbor (1-NN) classifier using Dynamic Time Warping (DTW) [20] as a distance measure performs well for TSC and is  X  X xceptionally hard to beat X  [7]. It is clear that one-nearest-neighbor with DTW distance is extremely hard to beat for the TSC problem [23]; howev-er, it is equally unclear whether it is effective for extremely ITSC. Previous work has proposed an intelligent  X  X tructure preserving over-sampling X  method (SPO) to solve the prob-lem of ITSC; the authors [3] claim that it can achieve bet-ter performance than other existing over-sampling methods and state-of-the-art methods in ITSC. Their claim is based only on the comparison of the average values of two evalu-ation metrics, F value and Geometric Mean ( G mean ) without statistical analysis to support their conclusion; in addition, the main disadvantage of over-sampling methods is that they significantly increase the computational cost of training a classification model due to the addition of new minority class instances to balance the data-sets with high dimensional features. These challenging issues have moti-vated us to find simple and efficient solutions for ITSC. In addition to these challenges is the following issue in data mining research: how to evaluate the performance of multi-ple methods to draw valid conclusions.
 This work investigates whether SVM performs well for ITSC and whether a simple random under-sampling (RUS) met hod with SVM (RUS S VM) outperforms the complex intelligent SPO method for ITSC; moreover, it provides a correction of the claim for the SPO method by applying statistical analysis. Our experimental results demonstrate that this efficient and simple RUS S VM can achieve results that are comparable to the existing complicated SPO and state-of-the-art methods for ITSC.

The paper is organized as follows. Section 2 outlines the designed framework. Section 3 reviews sampling techniques. Sections 4 and 5 provide the experimental setting and ex-perimental analysis. Section 6 concludes this work.
Figure 1 shows the designed framework. The evaluation is divided into five parts as follows: (1) construct binary-class ITSC data-sets; (2) utilize a 10-trial 10-fold cross-validation evaluation method for this work; (3) assess whether SVM performs well for ITSC based on five evaluation metrics; (4) compare RUS SV M results from this work with SPO and other existing over-sampling methods (results from SPO [3]) based on two evaluation metrics, F-value and G-mean; and (5) apply statistical tests to draw valid conclusions.
The random under-sampling technique is used to vary the levels of class distribution of the altered binary-class ITSC data-sets to investigate the performance of RUS S VM, i.e., each binary-class ITSC data-set, D with sample size M is altered into i data-sets, Di with different sample size M i , re-spectively. The sample size of the positive class and negative class, is P i and N i , respectively. The formula (1) is used to vary the levels of class distribution (integer i (3 &lt; i &lt; 8)).
All positive samples, P are selected as positive class, and the proportions of the positive class are 40%, 50%, 60% and 70% of Mi, respectively. Each original data-set is altered into four degrees of class distribution; and the four altered data-sets are used to run 10-trial 10-fold cross-validation. The maximum results of the evaluation metrics are then selected for report.
The misclassification error rate is an ineffective evaluation metric for the imbalanced classification task [14, 18, 17, 16, 15, 15] . This is especially true for ITSC, as it cannot present a true prediction for the minority class, which normally has a higher misclassification error cost than the majority class. Therefore, we have adopted five evaluation metrics for this work as follows: ErrorRate , True Positive Rate ( T P R ), True Negative Rate ( T N R ), F value , and G mean .
Sampling techniques are widely used to treat imbalanced class distribution problems. There are two main categories of sampling methods: under-sampling the majority class [19, 12] and over-sampling the minority class [4, 5, 8] to modify the degree of class distribution to any desired level [1].
There are advantages and disadvantages to using under-sampling and over-sampling methods. One advantage of under-sampling is that it only uses a subset of the major-ity class for training; thus the training process is fast and very efficient. A disadvantage is that many majority class samples are ignored, discarded or removed, and potentially important information may be lost. The disadvantages of over-sampling are that (1) making exact copies of existing examples increases the computational cost and may cause an over-fitting problem, and (2) increasing the number of training examples increases the learning time.

Comparative studies of various re-sampling techniques show that simple RUS and ROS perform better than the intelli-gent techniques mentioned above [1, 21]. In addition, Lau-rikkala [13] compared several sampling methods and deter-mined that both over-sampling and under-sampling methods are very effective in dealing with imbalanced class distribu-tion problems; there are no significant advantages in using more sophisticated over-sampling and under-sampling meth-ods over simple random over-sampling and random under-sampling methods. Furthermore, previous work [19] indi-cates that under-sampling is more effective than over-sampling techniques; and under-sampling is an efficient and popular method for learning from imbalanced class distribution.
This section includes data-set characteristics and selection of the learning algorithm.
 T able 1 displays a summary of the characteristics of the 5 TSC data-sets from the public UCR time series repository [10], which were used as the benchmark data-sets of SPO [3]. We also alter three out of five data-sets from multi-class change to binary-class as follows. For the Adiac data-set, the second class with 23 samples is considered as positive class, and the remaining samples are considered as negative class. For FaceAll and S-Leaf data-sets, the first class is considered as the positive class with 112 and 75 samples, respectively. Java platform is used to implement the RUS technique to varying class distributions. Sequential Minimal Optimization (SMO) of SVM is employed from WEKA [22].
This section contains two subsections, as follows: (1) eval-uation of the performance of SVM on ITSC; and (2) compar-ison of the performance under-sampling and over-sampling methods for ITSC.
Dem  X  Za r Dem X sar [6] suggests that it is inappropriate to validate the conclusions by using the averaged results over multiple data-sets when the performances of multiple classi-fiers or methods are compared. The main reason is that the averages are susceptible to outliers; for example, SPO has an excellent G mean performance (0.999 on one data-set, Adiac) to compensate for the three out of five bad G mean performances. It is preferable for the classifiers to perform well on as many problems as possible, which is why it is inap-propriate to draw conclusions by averaging the results over multiple data-sets. Previous authors have based their con-clusions on the averaged results of F value and G mean ; thus, their conclusions cannot be validated. Moreover, it is not possible to distinguish from the averaged results whether one method is significantly better than others. This work therefore utilizes statistical tests, the Friedman test and post-hoc Nemenyi test, to compare the performance of the multiple learning methods in subsections 4.2 and 4.3, as sug-gested by [6]. T able 2 shows the performance of SVM to examine whether SVM performs well for ITSC. The experimental results in-dicate that SVM does not perform well on four out of five ITSC data-sets. Our experimental results demonstrate that the error rate is an ineffective measure for ITSC. For exam-ple, both data-sets Adiac and S-Leaf have a low error rate (or high overall accuracy), but their true positive rate and true negative rate is 0 and 1, respectively. This means that none of the minority class samples have been correctly predicted and all the majority class samples have been correctly pre-dicted. Moreover, SVM is not a suitable learning algorithm for ITSC, and this is especially true for these extremely im-balanced time series classifications, such as the Adiac and S-Leaf data-sets. SVM is also not suitable for large and high dimensional almost balanced time series classification, such as the yoga data-set. Table 3: Comparison of the performance of under-sampling with existing over-sampling methods
T able 3 presents a comparison of RUS S VM (results from this work) with learning methods and over-sampling meth-ods (results from existing work [3]) by using two evaluation metrics: F value and G mean . The experimental re-sults indicate that RUS SV M achieves better performance of F value than all learning and over-sampling method-s, while on average value and average rank of F value , RUS SV M achieves 0.88 and 2.2, respectively, which is bet-ter than all learning and over-sampling methods. On aver-age value and average rank of G mean , however, the SPO over-sampling method achieves 0.915 and 2.3, respectively, which is the best among all learning and sampling methods. The results highlighted in red are corrections of the previous work [3].
 Fi gure 2: Comparison of learning and sampling methods Fi gure 3: Comparison of learning and sampling methods
Figures 2 and 3 present a comparison of all learning and sampling methods with the Nemenyi test, where the x-axis indicates the ranking order of learning and the sampling methods; the y-axis indicates the average rank of F value and G mean performance, respectively, and the horizontal bars indicate the  X  X ritical Difference X . The groups of sam-pling methods that are not significantly different at 95% confidence interval are indicated when the horizontal bars overlap. The results indicate that there is no statistically significant difference between the methods, RUS SV M and SPO based on both F value and G mean evaluation metrics, even though RUS S VM and SPO have better per-formance on average F value and G mean , respectively.
Comparing SPO with the other learning and sampling methods, the complex over-sampling method SPO is on-ly statistically significantly better than the  X  X asy X  learning method based on G-mean metric. Therefore, the statistical tests demonstrate that there is no statistically significant d-ifference between SPO and the other learning and sampling methods, except for the  X  X asy X  method.
This work has proposed a simple random under-sampling method: an efficient solution for ITSC. Statistical tests were applied to investigate whether the complex over-sampling method, SPO, outperforms the simple RUS SV M and oth-er learning and over-sampling methods for ITSC. The ex-perimental results and statistical analysis demonstrate that there is no statistically significant difference between the complex over-sampling methods SPO, simple RUS SV M, and five other over-sampling methods based on the evalua-tion metrics F value and G mean . Therefore, the simple random under-sampling method achieves comparable result-s to the complex over-sampling method SPO. Because the under-sampling method discards a large number of nega-tive samples for training, while the over-sampling method creates more new samples to balance the training set, the under-sampling method is clearly computationally more ef-ficient than the over-sampling methods. [1] G. Batista, R. Prati, and M. Monard. A study of the [2] K. Buza, A. Nanopoulos, and L. Schmidt-Thieme. [3] H. Cao, X. Li, Y. Woon, and S. Ng. SPO: Structure [4] N. Chawla, K. Bowyer, L. Hall, and W. Kegelmeyer. [5] N. Chawla, A. Lazarevic, L. Hall, and K. Bowyer. [6] J. Dem X sar. Statistical comparisons of classifiers over [7] H. Ding, G. Trajcevski, P. Scheuermann, X. Wang, [8] H. Han, W. Wang, and B. Mao. Borderline-SMOTE: [9] B. Hidasi and C. G  X asp  X ar-Papanek. ShiftTree: An [10] Z. Q. H. B. H. Y. X. X. W. L. . R. C. A. Keogh, E. [11] K. Kim. Financial time series forecasting using [12] M. Kubat and S. Matwin. Addressing the curse of [13] J. Laurikkala. Improving identification of difficult [14] G. Liang. An investigation of sensitivity on bagging [15] G. Liang and C. Zhang. An empirical evaluation of [16] G. Liang and C. Zhang. Empirical study of bagging [17] G. Liang, X. Zhu, and C. Zhang. An empirical study [18] G. Liang, X. Zhu, and C. Zhang. The effect of varying [19] X. Liu, J. Wu, and Z. Zhou. Exploratory [20] H. Sakoe and S. Chiba. Dynamic programming [21] J. Van Hulse, T. Khoshgoftaar, and A. Napolitano. [22] I. Witten and E. Frank. Data Mining: Practical [23] X. Xi, E. Keogh, C. Shelton, L. Wei, and [24] N. Zavaljevski, F. Stevens, and J. Reifman. Support [25] D. Zhang, W. Zuo, D. Zhang, and H. Zhang. Time
