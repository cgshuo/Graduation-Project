 Voice search applications are typically evaluated by com-paring the predicted query to a reference human transcript, regardless of the search results returned by the query. While we find that an exact transcript match is highly indicative of user satisfaction, a transcript which does not match the ref-erence still produces satisfactory search results a significant fraction of the time. This paper therefore proposes an evalu-ation method that compares the search results of the speech recognition hypotheses with the search results produced by a human transcript. Compared with a strict sentence match, a human evaluation shows that search result overlap is a better predictor of (a) user satisfaction and (b) search re-sult click-through. Finally, we propose a model predicting the Expected Search Satisfaction Rate (ESSR), conditioned on search overlap outcomes. On a held out set of 1036 voice search queries, our model predicted an ESSR within 0.9% (relative) of the ground truth satisfaction averaged over 3 human judges.
 Evaluation, voice search, automatic speech recognition, search engines, entity resolution
Voice search applications are typically evaluated by com-paring the automated speech recognition (ASR) output to a single human reference transcript, and measuring either the Word Error Rate (WER) or the ratio of exact string matches (Sentence Error Rate or SER) [4, 1]. We find that while an exact sentence match is highly indicative of user satisfaction, a sentence error is a poor indicator of user dis-satisfaction (see Sections 3 and 4). Sentence errors are not predictive of user satisfaction because transcribers produce a single transcript per audio sample, while many other tran-scription variants could produce satisfactory search results. Table 1 shows how a normalization variant between t-shirts and t shirts affects search results in a product search engine. An error analysis reveals that 37% of the SER is due to arbi-trary normalization choices (e.g. tooth brush vs toothbrush ), function word deletion (e.g. a , the ), possessives, plurals, or transcription errors. This is partly due to the fact that the language model is trained on search queries, hence the system is likely to output search-like utterances. A first solution is to handcraft normalization rules, however those do not scale and can mask errors that impact search (e.g., ignoring white-space mismatches could artificially increase accuracy). Another solution would be to ask transcribers to provide an exhaustive set of transcripts for each audio sample, however it is not cost effective.

In contrast, information retrieval systems are evaluated by asking human judges to estimate the relevance of individual results, and then compute the Mean Average Precision or Normalized Discounted Cumulative Gain (NDCG) [3]. Such metrics are costly when used for evaluating voice search sys-tems, since the human judgments must be collected for new queries resulting from ASR model updates. While previous work has focused on jointly optimizing ASR and information retrieval systems [2], it assumes the existence of search rele-vance judgments for every test query. Our goal is to remove the need for collecting large amounts of search evaluation data for offline testing, by building a user satisfaction model from a small set of judgments.

In order to evaluate the effect of voice recognition errors more accurately without requiring additional transcription resources, in Section 2 we propose an evaluation method that compares the search results of the speech recognition hypotheses with the search results produced by a human transcript. This is related to the Google WebScore, which is defined as the number of times the first result of the ASR output matches the first result of the human transcript [5]. However, we find that using search results beyond the first one produces metrics that correlate better with human judg-ment. Section 3 reports experiments showing that partial overlap metrics outperform SER as a predictor of human search result ratings and user clicks. Finally, Section 4 pro-poses a model of user satisfaction conditioned on search over-lap outcomes. Experiments on a held out test set of 1036 utterances predicted an Expected Search Satisfaction Rate (ESSR) within 0.9% relative to the ground truth satisfaction averaged over 3 human judges.
An ideal search-based evaluation would compare the re-turned search results with the set of satisfactory results that the user intended to find. Labeling search results is costly first 10 results are in bold.
 and prone to ambiguities, since the annotator does not know what the user had in mind. However, human transcripts can be used to estimate the intended search results. Let us define R ref as the set of the first N results returned by sending the reference transcript to search, and R 1: N asr, 1 the first N results returned by sending the 1-best ASR hypothesis to search.
Since the true user intent is unknown, we rely on R ref as an approximation to the search results the user is looking for. Table 1 shows that search engines sometime fail to re-turn consistent results between equivalent surface forms. As a result, the next section shows that a strict ordered result match correlates poorly with human judgments of search ac-curacy ( r = 0 . 65), i.e., it over-penalizes the system. This pa-per therefore investigates a family of search overlap metrics characterizing the minimum number of results in common N min between the two sets of N search results:  X   X   X   X   X 
Let us abbreviate o ( R asr , R ref , N min , N ) as o ( N The search results in Table 1 have 6 results in common. Since the first 2 results do not overlap we can measure that o (1 , 2) = o (2 , 2) = 0. The first 4 results have exactly 3 items in common, hence o (1 , 4) = o (2 , 4) = o (3 , 4) = 1 and o (4 , 4) = 0. Note that the Google WebScore corresponds to o (1 , 1) since it only takes the first result into account [5].
In order to identify the most suitable metric for evaluat-ing ASR systems, we asked human judges to evaluate 3500 search results produced by the 1-best hypothesis of a mo-bile shopping voice recognition application, based on the corresponding human reference transcript. See instructions in Figure 1. Human judges associated each ASR hypoth-esis with a search quality rating, and discarded any non-shopping utterances. Additionally, we asked human judges to evaluate the search results produced by the reference tran-script. Note that in a first experiment 2 judges rates 500 voice searches, and in a follow up experiment 6 groups of 3 judges each rated additional slices of 500 voice searches, for a total of 3500 utterances.
Each voice search with a rating of 3 is labeled as satis-factory while ratings below 3 are labeled as unsatisfactory . The Rating column of Table 2 shows the Pearson X  X  corre-lations between the satisfaction ratings and the evaluation metrics mentioned in the previous section (for various val-ues of N min and N ), as well as sentence error rate (SER). In order to focus on the effect of ASR errors, these were obtained after removing non-shopping utterances (e.g., nav-igation queries, phatic utterances) and utterances for which the search results of the reference transcript were labeled as unsatisfactory. Out of the remaining 2462 utterances, a training set of 1426 utterances was used in Table 2, while 1036 utterances from distinct groups of annotators were held out for testing.

Results show that whether or not the reference results and the ASR results have one result in common (i.e., o(1,10) ) is the best predictor of human ratings ( r = . 82). All proposed search-based metrics are better predictors of search ratings than sentence match ( r = . 60). We find that the Google WebScore (i.e., o(1,1) ) does not predict satisfaction as well as metrics that go beyond the first search result ( r = . 71).
Additionally, we investigated whether overlap metrics can predict user clicks following a voice search. The Click col-umn in Table 2 shows the correlations between the result overlap and click events over 63,000 live voice queries. A query is associated with a click event only if the user clicks Table 2: Pearson X  X  correlation coefficient between overlap and SER metrics with (1) human search rat-ing and (2) whether or not a user clicked on a search result. Largest correlations are in bold.
 on a product on the first page returned by the voice search. The results confirm that low result overlap metrics are better predictors than sentence match ( r = . 22 vs r = . 14, respec-tively), however the correlations are weaker. This could be partly due to users not clicking on a result even though the result is relevant. Additionally, the reference search results were retrieved one month after the live results, hence the variation of the search results over time is likely to weaken the correlations too.
This section focuses on estimating a user satisfaction model based on the features investigated in the previous sections. Since the satisfaction model will be used to evaluate produc-tion ASR systems, it is important to (1) avoid any overfit-ting and (2) be able to inspect the learned model. For these reasons, we estimate a discrete conditional probability table based on rating counts of the training data detailed in Sec-tion 3. Table 3 shows the estimated probability distribution of user satisfaction conditioned on the search overlap. Given such a table, one can compute the expected probability of satisfaction over a set on unseen utterances by taking the average of the probability that the search results were rated as satisfactory, over all utterances in the test set.
In order to focus the attention on the effect of ASR on search accuracy, Table 3 only includes utterances for which the reference transcript X  X  results are rated as satisfactory. Hence our approach provides an upper bound on user satis-faction. The method could trivially be extended to include utterances for which the search engine did not return rele-vant results to model the effect of search engine errors.
While observing an overlap is not more indicative than observing a sentence match, the absence of overlap is a better indicator of poor rating than a sentence mismatch ((P(r=non-sat | X  o(1,10))=.79) vs P(r=non-sat | X  sentence match)=.46).

This table provide a model of the expected user satisfac-tion s resulting from an ASR output given that the reference transcript produces valid results, by marginalizing over all search overlap outcomes. Because we focus on voice queries for which the reference transcript is a valid search, the prob-ability of satisfaction is 1 if the ASR output matches the reference transcript. Hence, the overlap metric comes into play only when the transcripts do not match. As a result Table 3: Probability of satisfactory rating (r= sat ) conditioned on various metric outcomes, computed over 1426 training utterances. Utterances for which the reference transcript results in a non-satisfactory search were removed.
 Table 4: Probability of satisfactory rating (r= sat ) given a sentence mismatch, computed over the 486 training utterances not matching the reference tran-script.
 our final model relies on the estimated satisfaction distribu-tion conditioned on a sentence error, as shown in Table 4. The estimates in this table are computed over the subset of 486 training utterances for which the ASR output did not match the reference transcript. This table is our final user satisfaction model.

Equation 1 illustrates how the expected user satisfaction is computed based on the conditional probability table. P ( sat ) = (1)  X   X   X   X   X 
This value can then be averaged over an arbitrary dataset, to produce the Expected Search Satisfaction Rate (ESSR). A more fine-grained model could be derived by conditioning on However a larger data collection would be required to esti-mate each parameter correctly.
In order to validate the satisfaction model introduced in the last section, we held out 1500 voice search ratings, each labeled by 3 annotators who did not label any of the training utterances. As in Section 3, the reference rating is computed through majority voting. Table 5 compares the estimated satisfaction probability based on the model in (1) with the actual percentage of satisfactory search results. Results are computed on 1036 utterances, after removing utterances for which the reference human transcript produced unsatisfac-Table 6: Individual satisfaction variation and relative ESSR error based on ground truths de-rived from individual human judges, using the ESSR model in Table 4. Satisfaction variation is the dif-ference between the mean ground truth satisfaction and the satisfaction reported by individual judges. tory search results. Results show that the satisfaction mod-els predict the true user satisfaction within  X  1 . 3% relative error, with the best model predicting an ESSR 0.9% lower than the ground truth satisfaction percentage. In contrast, the percentage of exact sentence match (1-SER) is 21.8% lower than the ground truth satisfaction.
In order to assess the impact of individual judges on our evaluation method, we evaluated the percentage of satisfac-tion according to a single human judge, rather than taking the majority vote. Table 6 reports the satisfaction estimated on the two slices of the test data used in Section 4.1, which was annotated by different groups of 3 judges. We find that on the same set the perceived satisfaction varies by up to 4.1% absolute in the worst case scenario. Since we remove utterances for which the reference transcript is out of do-main or producing unsatisfactory results, it X  X  important to note that this variation results from two factors: (1) whether or not an utterance is in domain and intelligible, and (2) whether the results are satisfactory.

Regarding inter-rater agreement on the full set of 3000 searches, we find that the judges agree 90.0% of the time on whether or not an utterance is in domain and intelligible (vs 50% by chance), with an average inter-rater correlation of 0.38. Regarding satisfaction judgments (1, 2 or 3), judges agrees 67.1% of the time (vs 33.3% by chance), for an average correlation of 0.65. Results are averaged over 15 judge pairs.
An issue with the proposed approach is that search en-gines are not static over time. This can affect our evalu-ation method in two ways: (1) the reference search results should be collected at the same time as the test search results for the comparison to be meaningful; and (2) the satisfac-tion model might need to be re-estimated over time. While query/result pairs can be cached to speed up offline model evaluations, the cache needs to be updated frequently to address issue (1). Given the small number of parameters in our satisfaction model, we believe that the risk of overfitting is limited. However, significant changes in either the ASR model or the search engine could make the predicted ESSR inaccurate over time.

A second limitation is that the learned parameters are only valid for the search engine on which they were esti-mated. However the cost of estimating new parameters is fixed and relatively small compared to the cost of evaluat-ing the search results produced by any ASR model update. Note that this paper focuses on product search as an ex-ample, however the methodology is content agnostic. Hence future work will assess how the method scales to other do-mains (e.g., text document retrieval), as well as to search engines with different levels of performance.
This paper has presented a voice search evaluation method which is a better predictor of user satisfaction than an ex-act sentence match, with no additional human annotation cost. Our model can be seen as a data-driven extension of the Google Webscore beyond the 1-best search result, which is crucial given that search engines are sensitive to the tok-enization produced by the ASR. The satisfaction model pre-dicts the Expected Search Satisfaction Rate (ESSR) within  X  0 . 9% relative to the ground truth satisfaction on held out data, while an exact sentence match underestimates the true satisfaction by 21.8% relative. Those results suggest that ESSR is a better objective function for (a) tuning voice search models and (b) monitoring accuracy over time. [1] A. Franz and B. Milch. Searching the web by voice. In [2] X. He and L. Deng. Speech-centric information [3] K. Jarvelin and J. Kekalainen. IR evaluation methods [4] P. Jyothi, L. Johnson, C. Chelba, and B. Strope. [5] J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne,
