 Detecting and monitoring compe titors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple net-works. How to discover the underlying pattern s and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this paper, we study the problem of min-ing competitive relationships by learning across heterogeneous net-works. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We fi nd that the two networks exhibit different but complemen-tary patterns of competitions. Our proposed model, Topical Factor Graph Model (TFGM), de fi nes a latent topic layer to bridge the two networks and learns a semi-supervised learning model to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46% improvement over alternative methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.2.8 [ Database Management ]: Database Applica-tions X  Data Mining Algorithms, Experimentation Web mining, Social network, Competitive relationship  X  X ompetitive strategy is an area of primary concern to man-agers, depending critically on a subtle understanding of industries and competitors X  [21]. Indeed, competition is becoming extremely fi erce in every domain with companies all over the world striving for limited resources and markets. Detecting and monitoring com-petitors becomes a critical issue for a company to make marketing strategies. Traditional competitor detections are usually based on observations, conjectures or sales reports. However, it is highly infeasible to manually collect the competitive relationships, con-sidering the innumerous companies/products in the world. 1
Recently, a few researchers have studied the problem of competi-tor detection. For example, Bao et al. [1] proposed an algorithm called CoMiner to identify competitors for a given entity. In this work, competitors are ranked according to the combination of sev-eral metrics including mutual information, match count, and candi-date con fi dence. Sun et al. [22] studied the comparative web search problem, in which the user inputs a set of entities (keywords) and the system tries to fi nd relevant and comparative information from the web for those entities. However, both works are motivated by only mining competitive relationships and not trying to reveal in which topic two entities are competing on (such as Game, Hard-ware, or Operation System).

In this work, we aim to conduct a systematic investigation of the problem of mining competitive relationships between entities (e.g., companies or products). Different from the related works, we try to utilize and learn from two data sources: text documents (patents) and social networks (Twitter). The reason we are using more than one data source is to avoid potential problems caused by information asymmetry. For example, some emerging companies or startups may not have any patent records. A challenge we met is how to intertwine the two sources X  information properly. After all, Twitter is usually a place where t he public discusses about outside apparent features yet patent records document inner core technolo-gies that enable such features. They are entirely different in terms of contents and perspectives. Ideally, the method should combine the two pieces of information together as a heterogeneous network and thereby mine competitive relationships within it.

To clearly demonstrate the problem, Figure 1 gives an exam-ple of competitive relationships. The centered nodes are two com-panies: Google and Microsoft. The labels on each link indicate the fi elds on which the linked two companies compete with each other and the probab ility that connected nodes are competitors. For example, there are some well-known competitive relationships: Google competes with Facebook on social network, and competes with Microsoft on search engine. Some other competitive relation-
Merely in U.S., there are more than 27 million companies, http://www.census.gov/econ/smallbus.html. Figure 1: Examples of topic-level competitors. Each edge is as-ships (such as Microsoft competes with Kingsoft 2 ) that are not so obvious and may be ignored by manual analysis can also be found in the fi gure. Such a graph of competitive relationships would be signi fi cantly helpful for a company to design market strategies. The problem is non-trivial and poses a set of challenges:
In this paper, we precisely de fi ne the problem of mining compet-itive relationships by learning across heterogeneous networks and propose a semi-supervised Topical Factor Graph Model (TFGM). An ef fi cient algorithm is developed to learn the proposed model. We evaluate the proposed model on a large patent network and the
Kingsoft has the second largest market share in Japan on of fi ce suite.
 Twitter network. Experimental results demonstrate that the pro-posed model can extensively improve the performance (averagely +46% in terms of F1-Measure) over several alternative methods. To summarize, we have the following fi ndings through this study: Organization Section 2 formulates the problem. Section 3 intro-duces the data sets and some observations we discovered. Section 4 explains our proposed model and describes the algorithm for model learning. Section 5 introduces our experiment that validates the ef-fectiveness of our methodology, including its setup, baseline meth-ods and results. Finally, Section 6 reviews some previous works related to ours and Section 7 concludes this work.
We introduce some necessary de fi nitions and then formulate the problem. To keep things concrete, we will use company as the ex-ample to explain the competitive relationship mining problem. The problem can be easily generalized to other entitie s such as products. We consider two heterogeneous data sources: Patent and Twitter. From patent records, we extract companies, inventors, and patents. We create a network of companies G =( V,E, S ) ,E  X  V  X  V , where V represents a set of companies, E represents the relation-ship between companies, and S is a matrix describing attributes associated with the companies, in which every row corresponds to a vector of attribute values of a company. For example, the attributes of a company can be inventors of those patents owned by the company and keywords occurring in the patent descrip-tions. Moreover, we augment the company network with social networking information. Speci fi cally, we consider Twitter users who have discussed the companies and tweets which have men-tioned the company names. Thus, the augmented network is rep-resented as G =( V,E, S , U , M ) , with each row of matrix U de-noting users who have posted tweets containing the corresponding company name and each row of matrix M denoting tweets which contain the corresponding company name. As a conclusion, S is correlated to the text document (patent) data source. U and M are correlated to the social network (Twitter) data source. We further assume that each company is associated with a topic distribution. In particular, we have the following de fi nition:
De fi nition 1. Topic model of company. A topic model  X  d patent d is a multinomial distribution of words { P ( w | company v i is considered as a mixture of topic models, denoted as  X  , extracted from those patents owned by the company.

The underlying assumption for the topic model is that words ap-pearing in the patents are sampled from a distribution correspond-ing to each topic, i.e., P ( w |  X  d ) . Thus, words with the highest prob-abilities associated w ith each topic would suggest the semanteme represented by the topic. For example, a  X  X earch Engine X  topic can Figure 2: (a) Patent similarity correlation. X-axis: patent similarity be represented by keywords  X  X earch X ,  X  X dvertisement X , and  X  X ank-ing X .

For each edge e  X  E , we associate it with a label y  X  X  0 , 1 y =1 indicates corresponding two companies have a competitive relationship. Given that, we can de fi ne the problem addressed in this paper:
Problem 1. Competitive relationship mining. Given a net-work, G =( V,E, S , U , M ) and topic models {  X  } of all companies, the goal is to learn a predictive function f :( E | G )  X  the competitive label of each relationship between companies.
There are two things worth mentioning. The fi rst is in the net-work G , we may have some labeled data, i.e., labeled competitive relationships from some online databases, but for most of the rela-tionships the labels are unknown. The second is that the network is theoretically a complete network. We could use some parame-ters or human knowledge to control the density of the network. For example, only when the similarity of two companies (based on con-tent or network information) is larger than a prede fi ned threshold, we add an edge between them.
Before presenting our approach for competitor detection, we fi rst convey a series of discoveries we observed from the data. In this study, we consider two data sources: Patent and Twitter. We have collected all t he patents (3,770,411 patents) from USPTO 3 , from which we extracted 195,263 companies and 2,430,375 inventors. For each company, we used it as the query to search Twitter and retrieved the top returned tweets, from which we further extracted the information of users. So far, we have collected 1,033,750 tweets writte n by 87,603 Twitter users, which cover 1393 major companies. In looking for benchmark data, we turn to Yahoo! Finance 4 and use it as the ground truth source 5 . Each company name was sent as a query to obtain its competitor list.
The probability of two randomly picked companies being com-petitors among the whole data set (1.59%, testi fi ed) is assigned to be the baseline probability. We compare our observation with it in order to see how different network features affect the probability.
We evaluate how patent information and Twitter re fl ect compa-nies X  competitive relationship from several aspects: (1) probability http://www.uspto.gov/ http:// fi nance.yahoo.com/
For example, IBM X  X  competitors can be found at this page: http:// fi nance.yahoo.com/q/co?s=IBM+Competitors Figure 3: Tweet-level analysis. Y-axis: the probability of two com-Figure 4: User-level analysis. Y-axis: the probability of two compa-that two companies are competitors, conditioned on whether or not they have published similar patents or employed same inventors; (2) probability that tw o companies are competitors, conditioned on their names were mentioned in a same tweet or mentioned by a same user. We also study whether the phenomenon of  X  X y enemy X  X  enemy is my friend X  exists in the competitive network.
 Patent Analysis Social theory homophily suggests that similar in-dividuals tend to associate with each other [14]. Here, we show how similarity degree of two companies correlates with the com-petitive relationship between them. We consider two types of sim-ilarities. The fi rst one is based on words occurring in the descrip-tions of patents owned by the two companies. The second one is based on the number of common inventors, i.e., inventors that used to work for both companies at different times. For the former, we respectively generate two topic distributions  X  v i and  X  companies v i and v j by PLSA [7] (see  X  4 for details). The similar-ity between the two companies is calculated by cosine similarity:
Figure 2(a) clearly shows that, when the similarity of two compa-nies increases from zero, the lik elihood of them being competitors rapidly increases and becomes four times the likelihood of two ran-dom companies. We observe a similar pattern for the analysis on inventors as shown in Figure 2 (b). When no common employed in-ventors can be found from two companies, the pr obability of them being competitors drops to 1.32%, lower than the baseline proba-bility. However, with more common inventors being detected, the probability outnumbers the baseline data and keeps increasing. Twitter Analysis We study the likelihood of two companies be-ing competitors when their names co-occur in tweets. Figure 3 shows the analysis results. It is striking that when the names of two companies are mentioned together in one tweet, the likelihood of the two companies being competitors becomes more than 10 times Figure 5: Comparison of two triad relati onships. C-C-N: competitor-higher than chance. Figure 3(b) further demonstrates that the like-lihood will continue to increase when the number of co-occurring tweets increase.

Besides the tweet-level co-occurrence, we conduct another anal-ysis on the user-level. Figure 4(a) shows that when a user mentions two companies in 10 minutes (may in different tweets), the like-lihood of the two companies being competitors is 25 times higher than chance. Figure 4(b) further illu strates that the likelihood drops when we set the time interval larger. We suppose that since tweets have length limitation, when a user discusses a company or its prod-uct in one tweet, she may follow up with another to mention its competitors or comp etitors X  products.
 Is my enemy X  X  enemy my friend? We study whether competitors form a balanced network structure. The phenomenon of  X  X he enemy of my enemy is my friend X  is one of the underlying balanced triad suggested by the social balance theory [6].

In particular, we split the data into three domains: Tech. (tech-nology), Energy and Health. In each domain, companies are grouped in triads. Suppose e ij =1 means company v i and v competitors, while e ij =0 means not. Given a triad ( v i we compare the likelihood of ( e ij =1  X  e jk =1)  X  e ik =0 (denoted as C-C-N) and that of ( e ij =1  X  e jk =1)  X  e ik (denoted as C-C-C). Figure 5 shows the probability of balanced triad in the three domains, from which we could have the following summary: my enemy X  X  enemy X  X  is not necessary my friend, but can hardly be my enemy again.

To sum up, according to the statistics shown above, we have the following discoveries: 1. As expected, similar companies tend to be competitors, with 2. Social network information is a very important indicator for 3. My enemy X  X  enemy is not necessary my friend, but should
In this section, we fi rst brie fl y discuss two basic models: topic models and factor graphs. We then propose a Topical Factor Graph Model (TFGM), which leverages the power of the two basic mod-els and formulates the competitor detection problem in a uni fi ed learning framework. Topic Model We fi rst discuss the basic statistical topic models, which have been successfully applied to many text mining tasks [2, 7]. The basic idea of these models is to model documents with a fi nite mixture model of K topics and estimate model parameters by fi tting a data set with the model. Two basic statistical topic models are Probabilistic Latent Semantic Analysis (PLSA) [7] and Latent Dirichlet Allocation (LDA) [2]. For example, the log likelihood of a collection D to be generated with PLSA is given as follows: where n ( w,d ) denotes the occurrences of word w in a text docu-ment d , z j is a topic and the parameters to estimate in PLSA model are p ( w | z j ) and p ( z j | d ) (or  X  d ). An example of PLSA X  X  graphical representation is shown in Figure 6(b).  X  in the fi gure stands for the topic distribution of each text document in the data set. Given this, we can de fi ne the topic distribution of each vertex (or entity, e.g., company, product) v i in G as a mixture of topic distribution over text documents (e.g., patents) D v i associated with v i , i.e., Factor Graph A factor graph consists of two layers of nodes, i.e., variable nodes and factor nodes, with links between them. The joint distribution over the whole set of variables can be factorized as a product of all factors. A factor graph can be learned via some ef fi cient algorithms like the sum-product algorithm [12].
Figure 6(c) gives an example of modeling our problem with the factor graph, which incorporates entity pairs X  information and la-bels of their relationships. For each pair of entities ( v create an instance node c k in the factor graph. For easy explana-tion, we use c 1 k and c 2 k to denote v i and v j respectively. The hidden variable y k stands for the label of the relationship, with y indicating c 1 k and c 2 k have a competitive relationship, y and y k =? unknown. Our objective in the factor graph is to assign a value to the unknown y k with high accuracy. We propose a novel model referred to as Topical Factor Graph Model (TFGM) for mining competitive relationships. As we men-tioned in  X  3, entities that have similar topic distributions are more likely to be competitors and vice versa, competitors may have sim-ilar topic distributions. Thus, the basic idea of the proposed model is to combine factor graph and topic model together, and learn them simultaneously.

Given a network G =( V,E, S , U , M ) with some labeled rela-tionships Y , our objective can be formalized as to maximize the following posterior probability: where D is a collection of all text documents. The fi rst term on the right side of Eq. (4) can be de fi ned according to the topic model and the second term can be de fi ned as a factor graph. Further, to incorporate the intuition that competitors may have a similar topic distribution, we de fi ne a regularizer, which is similar to the graph harmonic function in [31], to quantify the difference between topic distributions of two entities: where K is the total number of topics.

By integrating Eqs. (4) and (5) together, we can de fi ne the fol-lowing objective function to our problem:
O ( G )=(1  X   X  )log p ( D |  X ) p ( Y | G, D,  X )  X   X R ( Y,  X ) (6) where  X  is a parameter to balance the importance of the two terms.
Now we discuss how to instantiate the objective function. We can use any statistical topic model to de fi ne p ( D |  X ) . In this paper, we use PLSA. As to formalize p ( Y | G, D,  X ) ,westudythecorre-sponding en tities X  correlation and attributes, and we de fi ne the fol-lowing three factors according to the intuitions we have discussed.  X  Attribute factor : F ( x i ,y i ) represents the posterior probability  X  Balanced triangle factor : G ( Y c ) re fl ects the correlations be- X  Topic factor : H ( y i , X  c 1
Jointing the factors de fi ned above, we have p ( Y | G, D,  X ) = where Y c is a triad derived from the input network. The three fac-tors can be instantiated in different ways. In this work, we use exponential-linear functions. In particular, we de fi ne the three fac-tors as follows: where Z 1 , Z 2 and Z 3 are normalization factors. f j ( x h ( y i , X  v 1 valued function. g ( Y c ) can be de fi ned as an indicator function. Finally, by plugging Eqs. (2) and (7-10) into Eq. (6), we have where  X  is the collection of parameters, i.e.,  X = { p ( w { p ( z j | d ) } X  X   X  i } X  X   X  } X  X   X  } ,and Z = Z 1 Z 2 Z 3 is a normaliza-tion factor. Our goal is to estimate a parameter con fi guration  X  to maximize the objective function O ( X ) .
 The graphical representation of TFGM is shown in Figure 6(d). The upper layer is used for modeling the topic extraction task and the bottom layer is designed to model the competitor detection task. Actually we can combine R ( Y,  X ) and H ( Y,  X ) together as one factor function H to bridge the two tasks. We separate R ( Y,  X ) and H ( Y,  X ) to easily explain how we learn the model in the rest of this section.
To estimate the parameters in TFGM, let us fi rst consider the special case when  X  =0 . The objective function degenerates to log p ( Y | G ) with no regular function in this case. To maximize log p ( Y | G ) ,we fi rst apply an Expectation Maximization (EM) al-gorithm, a standard way of parameter estimation of PLSA, to it-eratively compute a local maximum of log p ( D |  X ) . After that, we compute the values of  X  based on Eq. (3) and maximize log p ( Y | G, D,  X ) by a gradient descent method. We repeat the two steps until the objective function converges.

The details of how to estimate the parameters of PLSA can be seen in [7]. When computing p ( Y | G, D,  X ) , we need to sum up the likelihood of possible states for all the nodes, including the unlabeled ones, to normalize Z . To deal with this, we infer the unlabeled labels from known ones. Y U is denoted as a labeling con fi guration inferred from known labels. We then have: where Q ( Y ) = (( i f j ( x ij ,y i )) T , c g ( Y c ) , and  X  =(  X  T , X , X  ) T .
 We introduce the gradient descent method to solve the function. The gradient for each parameter  X  is calculated as: One challenge here is to directly calculate the two expectations. The graphical structure of TFGM may be arbitrary and contain cy-cles. Thus, we adopt Loopy Belief Propagation (LBP) [20] approx-imate algorithm to compute the marginal probabilities of Y and Y
U . We are then able to obtain the gradient by summing over all the label nodes. An important point here is that the LBP process needs to be proceeded twice during the learning procedure, one for estimating p ( Y | G, D,  X ) and the other for p ( Y U | G, D,  X ) .We update each parameter with a learning rate  X  with the gradient.
We now discuss the case when  X  =0 . In this general case the objective function does not have a closed-form solution. Here, we propose a simple and ef fi cient algorithm which primarily consists of two steps. In the fi rst step, we update p ( z j | d ) , p ( w according to the same method in case  X  =0 . In the second step, we fi x p ( w | z j ) and  X  to update p ( z j | d ) as follows: where D v k denotes the text documents associated with v k D v i ,and y ( v i ,v k ) stands for the label correlated with entities v v . Clearly, j p n ( z j | d v i )=1 and p n ( z j | d v i ) &gt; 0 always hold in Eq. (14). When the step parameter  X  is set to 1, it means the new topic distribution of a text document, which belongs to entity v , is the average of the old distributions from all documents of v competitors. This is related to th e random-walk int erpretation. A similar algorithm was also used in [19]. See details in Algorithm 1.
In factor graph, we can also consider making use of topic model X  X  results to help mining competitive relationships; however, the topics are treated equally including ones that might be irrele-vant to competitions. In contrast, Topical Factor Graph Model, with the regularizer, can di stinguish  X  X omp etition topics X  from irrelevant topics thus to mine competitive relationships more effectively.
In this section, we validate the effectiveness of the proposed ap-proach.
 Data Preparation We consider two data sets in our evaluation: Company and Product.

Company. Description of the company data set is given in  X  As there is no standard ground truth t o quantitatively evaluate the performance of mining competitive relationships, for evaluation purpose, we have collected the competitive relationships between companies from Yahoo! Finance . Speci fi cally, Yahoo! Finance provides a list of competitors for each company. 6 It also catego-rizes all the companies into different domains (called sector) such as technology, energy, and health. Each company may be classi fi ed into two domains. In this way, we create a ground truth for eval-uating topic-level competitive relationships mining. In total, the company data set contains 1,393 companies from three domains.
Product. The product data was extracted from Epinions, a web-site on which users pose reviews on their purchased products. We extracted information between two products such as price differ-ence, reviewers who had reviewed on both of the products, com-ments that had both of the products X  names as social networks fea-tures. The text information which supports the topic model was de-rived from the products X  reviews. The data set consists of 120 prod-ucts, 972 reviews of the products, and 861 users who wrote com-ments on these products. Some example products include Canon 550D, Canon 5D Mark II (5d mii), Nikon D90, iPhone 4, iPad 2 and Amazon Kindle 2.
 Evaluation We conduct two types of experiments to evaluate the proposed approach. The fi rst one is to identify global competitors. We evaluate the proposed model and compare it with alternative methods in terms of Precision (Prec.), Recall (Rec.), F1-Measure (F1), and Accuracy (Accu.). The second experiment is to detect competitors at speci fi c topics, we de fi ne the probability of two com-petitors v 1 and v 2 competing in an area described by speci fi c topic z as
In each experiment, we randomly picked 40% in each category as training (labeled) data and the rest as test (unlabeled) data. For evaluating the performance of topic-level competitor detection: we fi rst determine whether two companies have a competitive relation-
For example, http:// fi nance.yahoo.com/q/co ?s=MSFT+Competitors Table 1: Competitor detection performance of different meth-ods in three domains.
 Domain Method Prec. Rec. F1 Accu.
 ship or not. After that, given a topic z and a company v 1 its competitors by p ( v 1 ,v 2 | z ) . At last we compare the rank with the ground truth from Yahoo! fi nance in terms of precision at position n (P@n), mean average precision (MAP) and normalized discount cumulative gain at position n (N@n). A similar method was previ-ously used in [25].
 We compare TFGM with the following baseline methods.

Content Similarity (CS). It calculates the cosine similarity be-tween two companies X  topic distributions and labels companies as competitors if their similarity value is greater than a threshold (0.2). We design it to see how unsupervised method works in this task.
Twitter Filtering (TF). It simply labels companies who have been mentioned in a same tweet at least one time as competitors. It is also an unsupervised method.

Random Walk with Restart (RW). It uses the network informa-tion to identify competitive relationships. Speci fi cally, it builds up a tripartite graph whi ch contains three types of node: inventors, companies, and patent categories (topics). For each company node v and topic node z , it creates a link from v to z and a link with op-posite direction. Then the random walk with restart algorithm [28, 27] is applied to rank competitors.

SVM. It uses all the features we de fi ned in TFGM (see Ap-pendix for details) to train a classi fi cationmodel(butSVMdoes not consider the correlation among the identi fi ed competitive rela-tionships). We then employ it to predict the company pairs X  labels in the test data. For SVM, we choose LIBSVM [3].

LR. It uses the same features as in the SVM method. The only difference is the way in which it uses logistic regression classi fi -cation to predict the labels in the test data. The method was used in [15] to predict positive and negative links in social networks. FGM. It trains a factor graph model with partially labeled data Table 2: Topic-level competitor detection performance of dif-ferent methods in three domains.
 Domain Method P@5 P@10 MAP N@5 N@10 Tech. RW 0.3556 0.2616 0.3614 0.3917 0.3137 Energy RW 0.2455 0.1712 0.0518 0.2391 0.1898 Health RW 0.1067 0.1046 0.0094 0.1143 0.1104 and all factors we de fi ned in  X  4. This method can also been re-garded as a special case of TFGM when  X  =0 . This method was used in [26] to classify the type of social relationships.
All algorithms are implemented in C++, and all experiments are performed on a Mac running Mac OS X with Intel Core i7 2.66 GHz and 4 GB memory. We empirically set the number of topics in TFGM as 100, and set parameters  X  =0 . 1 and  X  =0 . 5 in all other experiments. We will give the sensitivity analysis of these parameters later. We also set the maximum iteration number I = 500 and J =20 . In general, the ef fi ciency of TFGM is acceptable. It takes 2 hours to learn from the company data set.
Table 1 shows the results of detect competitors globally with different approaches on the company data set. We can see that TFGM clearly outperforms CS, TF, RW, SVM and LR in all do-mains (+57.98% in terms of the average F1). CS, TF and RW methods only consider content information, which leads to a bad performance. Compared with SVM and LR, one of TFGM X  X  ad-vantages is making use of the unlabeled data. Essentially, it further considers some latent correlations in the data set, which cannot be leveraged with only the labeled training data. At the same time, TFGM also shows satisfying robustness. We can see that SVM and LR have unstable performances over different domains. For example, in Tech. domain, SVM has F1 of 0.62 which falls to 0.18 in Health domain. This is because competitive relationships in Health domain are quite sparse, which makes SVM mostly la-bel company relationships as not competitive. Compared to FGM, with topic model incorporated, TFGM differentiates  X  X ompetition topics X  from those irrelevant topics and obtains a further improve-ment (e.g., +5% F1-score in Tech. domain).

There are two ways to detect topic-level competitors. One is the method we introduced above in  X  5.1. However, there is a differ-ent method for Random walk with restart: if we remove all  X  X opic nodes X  except one of them, the resu lt would be the competitors in the corresponding topic. There are many ways in implementing the fi rst method, e.g., all baselines. However due to the space limita-tion, we only present results generated by TFGM. Also, baseline methods produced poor results in the fi rst few steps, thus it is rea-sonable to ignore them. Table 2 shows comparison result of TFGM and RW, from which we can see that TFGM clearly outperforms RW. Factor Contribution To determine the contributions of different factors to the model performance, we remove them one by one ( fi rst balanced triangle factor function, followed by the topic factor func-tion), and then train and evaluate the performance. Figure 7 shows Figure 7: Factors contribution. TFGM-B: ignoring balanced trian-Figure 8: Network contribution. TFGM-P: ignoring the patent the F1-Measure score after ignoring the factor functions. We can observe clear drops on the performance, which indicates that each factor incorporated in the model has its speci fi c contribution to the fi nal result.
 How Heterogeneous Networks Help Social network and patent network are two fundamental constituent parts of the heterogeneous network we are studying on. To study how heterogeneous network helps solve this problem, we dismiss the two data source respec-tively. Furthermore, we design another method to make use of the heterogeneous data source: we regard two companies as competi-tors if either of the methods based on a single data source labels them as competitors. Figure 8 shows the F1-Measure of these three methods comparing to the original approach. We can see that the model with both components incorporated exceeds the other two incomplete TFGM greatly in performance, which indicates that our model works better by learning across a heterogeneous network than either of the two networks. P+S X  X  score drops greatly com-pared with TFGM X  X . It even underperforms methods based on a single data source. By investigation, we fi nd that if either one of TFGM-P and TFGM-S mistakenly labeled two entities as competi-tors, P+S keeps the mistake, which has severe adverse impact on the precision of the model.
 Sensitivity Analysis We conduct two experiments to test how pa-rameter  X  and  X  in fl uence TFGM X  X  performance. Figure 9 shows the trend of each measure following the changes of  X  in all domains (  X  is fi xed as 0.5). TFGM has low sensitivity of  X  in Energy and Health domains (the largest difference of F1 is less than 4% in both domains). However, in Tech. domain, the precision value slowly rises as  X  grows and then falls after  X  =0 . 6 . The recall value over-all stays stable, yet it has a rapid fall from  X  =0 . 1 to  X  =0 . 2 .We then fi x  X  =0 . 1 and see how F1-score changes by varying  X  .As Figure 10 shows, the score increases slowly at the beginning but falls a bit more quickly when  X  becomes larger (&gt; 0.5).
In this section, we demonstrate some examples generated from our experiments to show the effectiveness of our approach. Topic-level Competitor Analysis We study on topic-level com-petitor cases to see in real how text topics information helps com-petitor analysis. Table 3 displays results of several examples, listing the top competitors under the area given by a topic. As in Topic #4 describing graphic design, while the top competitors given by our model, NVIDIA and Autodesk, are two of the industry leaders.
On the other hand, given a pair of competitors, we try to fi gure out under which areas they are competing. Table 4 shows the top two topics for each pair of competitors according to p ( v We can tell that our model fi nds Samsung and Apple actually correlating to topics like  X  X ommunicating X  etc, indicating mobile phones, and  X  X rogram X , X  X rocessor X , indicating computers  X  corre-sponds to the real situation. Similar results can be seen in topics correlated to Microsoft and Google.
 Competitive Relationships between Products Our model is fl ex-ible and can be easily applied to other data sets. We apply it to fi nd competitive relationships between products. Table 5 shows an example result compared with FGM. As we can see, both TFGM and FGM detect Nikon D90 as a competitor of both of the Canon cameras. But FGM wrongly labels Kindle 2 and 550D as com-petitors. Under our study, we fi nd that many users discussed about how Kindle 2 or 550D is better than older versions, which makes the two products X  distributions of the topic  X  X ersion X  similar to each  X  Competitors Topic Hot words
Vs. Google Topic #81 operating, program, service Table 5: Examples of competitors among products. :there-sults of TFGM, :theresultsofFGM. 550d 5d mii iphone4 ipad2 kindle other. It, therefore, contributes a positive weight to labeling them as competitors. Yet they are obviously not competitors and  X  X ersion X  is not a classic topic about competition. FGM is misled by this phenomenon while TFGM distinguishes this irrelevant topic from valuable ones.

Another interesting fact is that TFGM considers iPhone 4 and 550D as competitors. This is feasible since iPhone 4, with excel-lent photo-shooting performance and a similar price, is quite an al-ternative of 550D from customers X  perspectives. At the same time, although iPad 2 has a camera built in, it is not often used for tak-ing pictures. Thus, TFGM does not treat it as competitors of the cameras.
In this section, we review the related works from three aspects: competitor detection, studies on Twitter, and patent mining. Competitor Detection Similar studies have been conducted with regards to competitor detection on the web. Using semantic analy-sis and text mining technique, Chen et al. [4] propose a framework to extract information from a user X  X  website and learn his/her back-ground knowledge. An al gorithm that also infers competitive anal-ysis is CoMiner, that Bao el al. [1] propose. CoMiner conducts a Web-scale mining for a company X  X  competitive candidates, domain and competitive strength. Thei r methods, however, are signi fi cantly different from ours. We not only consider the text information, but also incorporate the social network information. Another related work is Liu et.al X  X  methods of discovering unexpected information from competitors X  web sites [17]. This work focuses on analyz-ing competitors X  features rather than detecting them, which is obvi-ously different from what we are trying to do. Other related works including Li et al. [16] and Yang et al. X  X  [30] extract comparable entities by detecting keywords describing comparisons from online text documents. The two works study on a single data source while our method utilizes hete rogenous networks.
 Twitter Study Existing Twitter studies mainly include: Math-ioudakis and Koudas [18] present a system, TwitterMonitor, to ex-tract emerging topics from tweets X  content; [13, 29, 9, 23] mainly focus on identifying in fl uential users in Twitter or examining and predicting tweeting behaviors of users; Kwak et al. [13] conduct a study on Twitter network and perceive some notable properties of Twitter; Hopcroft et al. [8] explore the problem of reciprocal rela-tionship prediction on Twitter; Tang et al. [24] have developed a framework for classifying the type of social relationships by learn-ing across heterogeneous networks. As far as we know, few works in the literature have tried to use Twitter or other microblog data for competitor detection.
 Patent Mining In this paper, we also employ a set of patents infor-mation to assist for this competitor detections problem.There are also many related works on patent mining. Kasravi et al. [11] propose a method to discover business value from patent reposito-ries, Jin et al. [10] introduce a new problem of patent maintenance prediction and propose a method to solve it, while Ernst [5] uses patent information for strategic technology management including competitor monitoring. But these works only consider patent infor-mation, while we combine social networks and patents together to solve the competitor detection problem more effectively.
In this paper, we study the problem of mining competitive rela-tionships by learning across heterogeneous networks. Some fea-tures of competitive relationships, which re fl ect social network and patent information, are discovered and analyzed. We then formally de fi ne the problem in a semi-supervised framework and propose a Topical Factor Graph Model (TFGM) for detecting competitors with social network and text document attributes given. In TFGM, factor graph and topic model are incorporated. Ef fi cient algorithms are proposed for learning parameters as to infer unknown relation-ships. Experiments on two different data sets have been conducted and results outperform several alternatives greatly.

Another interesting topic to think about is how to detect potential collaborators. We believe that methods of collaborator analysis will resemble the ones that we propose in this paper. In future work, we will try to apply the existing methods on competitive detection to collaborative detection and fi gure out whether additional theories or algorithms will need to be involved.
 [1] S. Bao, R. Li, Y. Yu, and Y. Cao. Competitor mining with the [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [3] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support [4] X. Chen and Y.-F. B. Wu. Web mining from competitors X  [5] H. Ernst. Patent information for strategic technology [6] F. Heider. Attitudes and cognitive organization. Journal of [7] T. Hofmann. Probabilistic latent semantic indexing. In [8] J. E. Hopcroft, T. Lou, and J. Tang. Who will follow you [9] B. A. Huberman, D. M. Romero, and F. Wu. Social networks [10] X. Jin, S. Spangler, Y. Chen, K. Cai, R. Ma, L. Zhang, [11] K. Kasravi and M. Risov. Patent mining -discovery of [12] F. R. Kschischang, B. J. Frey, and H. andrea Loeliger. Factor [13] H. Kwak, C. Lee, H. Park, and S. B. Moon. What is twitter, a [14] P. F. Lazarsfeld and R. K. Merton. Friendship as a social [15] J. Leskovec, D. P. Huttenlocher, and J. M. Kleinberg. [16] S. Li, C.-Y. Lin, Y.-I. Song, and Z. Li. Comparable entity [17] B. Liu, Y. Ma, and P. S. Yu. Discovering unexpected [18] M. Mathioudakis and N. Koudas. Twittermonitor: trend [19] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic modeling with [20] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy belief [21] M. E. Porter. Competitive Strategy: Techniques for Analyzing [22] J.-T. Sun, X. Wang, D. Shen, H.-J. Zeng, and Z. Chen. Cws: [23] C. Tan, J. Tang, J. Sun, Q. Lin, and F. Wang. Social action [24] J. Tang, T. Lou, and J. Kleinberg. Inferring social ties across [25] J. Tang, B. Wang, Y. Yang, P. Hu, Y. Zhao, X. Yan, B. Gao, [26] W. Tang, H. Zhuang, and J. Tang. Learning to infer social [27] H. Tong, C. Faloutsos, and Y. Koren. Fast direction-aware [28] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random walk with [29] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank: fi nding [30] S. Yang and Y. Ko. Extracting comparative entities and [31] X. Zhu and J. Lafferty. Harmonic mixtures: Combining We introduce how we de fi ne the factor functions in our model. For attribute factor function ,wede fi ne three categories of features. Social correlation In the company data set, we consider tweets related to both companies in two features: the number of tweets with their co-occurrence and the number of tweet-pairs published by one user in a small time interval, in which one tweet is related to one company respectively. In the product data set, we also consider the two similar features corresponding to reviews on products. Social homophily Whether two companies or products have equal social status. In the company data set, we de fi ne three features: the number of tweets related to each company, the number of com-pany X  X  of fi cial account X  X  Twitter followers, and the number of users who follow both companies. We de fi ne two features in the product data set: the price difference of the two products and the number of reviews on each product.
 Local homophily In the company data set, we extract patent and inventor information of each company and consider whether two companies have common points in this. We use two features: the number of common inventors and the number of patents they have. In the product data set, we consider only one feature: the number of users who reviewed both products.

For balanced triangle factor function ,wede fi ne eight features to capture all the possible situations for every three links. We de fi ne topic factor function as the cosine similarity between the two topic distributions.
