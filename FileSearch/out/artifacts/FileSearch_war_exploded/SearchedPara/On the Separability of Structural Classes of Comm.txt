 Three major factors govern the intricacies of community ex-traction in networks: (1) the application domain includes a wide variety of networks of fundamentally different na-tures, (2) the literature offers a multitude of disparate com-munity detection algorithms, and (3) there is no consensus characterizing how to discriminate communities from non-communities. In this paper, we present a comprehensive analysis of community properties through a class separabil-ity framework. Our approach enables the assessment of the structural dissimilarity among the output of multiple com-munity detection algorithms and between the output of al-gorithms and communities that arise in practice. To demon-strate this concept, we furnish our method with a large set of structural properties and multiple community detection algorithms. Applied to a diverse collection of large scale network datasets, the analysis reveals that (1) the different detection algorithms extract fundamentally different struc-tures; (2) the structure of communities that arise in practice is closest to that of communities that random-walk-based al-gorithms extract, although still significantly different from that of the output of all the algorithms; and (3) a small subset of the properties are nearly as discriminative as the full set, while making explicit the ways in which the algo-rithms produce biases. Our framework enables an informed choice of the most suitable community detection method for a given purpose and network and allows for a comparison of existing community detection algorithms while guiding the design of new ones.
 I.5.1 [ Computing Methodology ]: Pattern Recognition  X  Design Methodology Networks, Community Structure, Detection Algorithms, Class Separability
Community structure captures the tendency of entities in a network to group together in meaningful subsets whose members have a distinctive relationship to one another. The identification of these subsets allows for the analysis of net-works at different levels of detail, which is instrumental in illuminating the structure underlying large-scale systems [5, 9, 10, 22, 23].

Despite playing a fundamental role in the structure and function of networks, community structure has proved to be frustratingly difficult to define, quantify, and extract. In addition to challenges related to computational tractability, three major factors account for the intricacies of commu-nity extraction. First, the application domain includes a wide variety of networks of fundamentally different natures. Each of these networks possesses meaningful communities that may possess their own distinctive structural profiles. Second, the literature offers a multitude of disparate com-munity detection algorithms. Due to differences in concept and design, the output of these procedures exhibits high structural variability across the collection. Last, there is no established consensus on the question of what properties distinguish subgraphs that are communities from those that are not communities.

In this paper, we tackle these challenges through a com-prehensive analysis of community properties. We present a framework that enables researchers and practitioners to as-sess the structural dissimilarity among the output of multi-ple community detection algorithms and between the output of algorithms and communities that arise in practice. Our approach analyzes communities by taking account of a broad spectrum of structural properties. The analysis reveals nu-ances of the structure of real and extracted communities.
We frame our approach as a class separability problem, which simultaneously handles many classes of communities and a diverse set of structural properties. To this end, we specify a learning problem in which we map the distinct communities into a feature space, where the dimensions rep-resent measures that characterize a community X  X  link struc-ture. The separability of classes provides information on the extent to which different communities come from the same (or fundamentally different) distributions of feature values. We extract different classes of communities that can be grouped into two categories: intrinsically-defined and extrinsically-defined communities.

We define the first set of communities by properties in-trinsic to their link structure. For our purposes, these are the sets that community detection algorithms may output. Each class of intrinsically defined communities comprises a set of examples that a specific algorithm extracts. The sep-arability of these classes demonstrates the extent to which different algorithms output structurally distinguishable sub-graphs. A feature selection analysis can then be employed to highlight the properties that exhibit the highest degree of inter-class variability, thereby making explicit the structural bias produced by different algorithms.

We also define communities by the context, the dynam-ics, or the function associated with the networks, but ex-trinsic to the link structure. We identify these communities through meaningful annotations provided with the datasets, such as explicit declaration of membership, product cate-gories, grouping by protein function, and so on. In this fashion, for each network, we form a class of extrinsically-defined communities, henceforth called annotated communi-ties . These communities enable a large-scale rigorous anal-ysis of community detection methods. The separability of the class comprising annotated communities from the classes of intrinsically-defined communities determines the extent to which community detection algorithms succeed in extracting subgraphs that are structurally comparable to the communi-ties formed by nodes sharing extrinsic properties in common.
To demonstrate our approach, we furnish our framework with a large set of structural properties and ten different community detection procedures to produce examples of dif-ferent structural classes. Our selection is representative of categories of popular algorithms available in the literature. We consider a diverse collection of large scale real networks whose domains span biology, on-line shopping, and social systems. Assessing separability using supervised classifiers both parametric, namely Support Vector Machines [30], and nonparametric, namely k -Nearest Neighbors [1], together with a feature selection analysis using correlation-based meth-ods [11], we reach the following conclusions about the com-munities in question. First, for all networks, the strong cross validation performance indicates that the different commu-nity detection algorithms produce fundamentally different structures that are separable on the feature space defined. Second, we observe that in nearly all cases, the annotated communities are structurally distinguishable from the out-put of all community detection algorithms. Nevertheless, the communities bearing the closest structural resemblance to annotated communities are those that random-walk-based algorithms extract. Surprisingly, in spite of the diversity of the domains from which our networks are drawn, this obser-vation applies to all of the networks, except to two of them for which we have a small population size. Finally, a small subset of the features is consistently observed as the most discriminative. This observation allows for a dimensionality reduction by a factor as large as 4, preserving an equivalent 10-fold cross validation performance. The most discrimina-tive features identify the ways in which the different algo-rithms produce biases. As illustrated by our experiments, by producing artificial or real examples of communities that possess the structure we wish to find, we can use our frame-work to enable an informed choice of the most suitable com-munity detection method for a given network. In addition, it allows for a comparison of existing community detection algorithms and may guide the design of new ones.

This paper is organized as follows. Section 2 discusses background information and related work. Section 3 intro-duces the datasets we use, the algorithms we consider, and the measures we apply to construct the feature space. Sec-tion 4 describes the heart of our framework and presents an experimental analysis thereof. Next, this section closes with a feature selection analysis. Finally, Section 5 offers our concluding remarks.
The work by Girvan and Newman [10] has recently sparked a wave of interest in the notion of community structure as a decomposition of a network that reflects meaningful proper-ties of the underlying system [9]. Nevertheless, this area has its roots in the related problem of graph partitioning whose initial contributions date back to the 1970s [14].
Given the diverse nature of networks, the notion of mean-ingful communities is necessarily context dependent, involv-ing interpretations and expectations of domain experts. There-fore, many attempts to define communities are grounded on the notion of mathematical optimization. Starting with an a priori expectation about what a community should look like, researchers specify an objective function for a search method, whose solution for a given instance provides the de-sired communities. This process has given rise to a a large collection of community detection algorithms, which aim at optimizing various objective functions. As mentioned in the previous section, the multitude of community structure def-initions represents a source of high variability between the output of different community detection algorithms.
Among the objective functions introduced in previous work, the notion of modularity [10] has become an influential one. Modularity assigns high scores to communities whose inter-nal edges outnumber the ones established in expectation by a random-network model that preserves the degree distri-bution of the original network. A related notion inspired by electrical networks is that of conductance [5]. The con-ductance of a set S with complement S C is the ratio of the number of edges connecting nodes in S to nodes in S C by the total number of edges incident to S or to S C (whichever number is smaller). The common theme underlying the pre-ceding notions is the search for node sets that are internally cohesive and yet sparsely connected to the rest of the net-work. Therefore, these measures tend to penalize sets having a large number of edges crossing the set relative to the count of internal edges.

Communities in general, however, display features that modularity and conductance may not capture, such as a preponderance of links to the outside over internal links and an arbitrary degree of overlap. This fact is substanti-ated by an investigation of real networks revealing that they do not split well into low-conductance communities [17] as most networks are expander-like [12]. These considerations lead to the development of alternative definitions, such as (  X , X  )-community [20], and algorithms, such as Link Com-munities [2] and Clique Percolation [24].

Communities in real networks often emerge as a result of multiple driving forces that make up the underlying com-plex system. Therefore, the attempt to capture community structure by maximizing a given objective function may rep-resent an unrealistic expectation. As a consequence, commu-nities identified by methods that reflect mathematical con-structs may differ structurally from real communities that arise in practice. Despite the vast literature on commu-nity detection, the work by Ahn et al. [2], as well as ours, are among the few that attempt to analyze the structural resemblance between communities extracted by algorithms and annotated communities, which represent examples of meaningful communities in various domains.

Even though network analysts expect the outputs of the different algorithms to display dissimilar structural profiles due their conceptual diversity, the structural variability does not hinge simply on the choice of optimization problem. In most cases of interest, the search for a collection of node sets that maximize a given objective function is computa-tionally intractable [9]. Therefore, in an attempt to handle the massive scale of today X  X  networks, popular methods of community detection rely on efficient heuristics. As a con-sequence, previous work have quantified a significant output variability among different approximation algorithms that aim at maximizing the exact same function [15, 18].
In the spirit of studying the structural variability exhib-ited by different algorithms, closest to ours is the work by Leskovec et al. [17], which discusses properties of communi-ties produced by multiple algorithms that aim at maximizing conductance. They consider the values of a handful of fea-tures, e.g., set compactness and internal conductance, pro-duced by different algorithms. In contrast, here we present the first study that is simultaneously comprehensive with respect to the diversity of structural properties, of domains, of algorithms, and of scale. We take account of a set of 36 features, measured from the output produced by 10 different community detection processes, which are representative of classes of available algorithms that aim at maximizing vari-ous different objective functions in the literature. We derive our results from a diverse collection of datasets from large-scale networks arising from multiple domains.
Before describing our framework and delving into our anal-ysis, in this section we present the datasets we use, as well as our methodology for building structural classes of com-munities from the network data. We also define the feature space for our learning problem. We analyze eight large scale datasets, namely DBLP, Live-Journal, two portions of the Facebook network (denoted by Facebook  X  Rice University Undergraduate and Graduate), Amazon, and three biological networks denoted by HS, SC, and Fly. The collection encompasses different forms of enti-ties and relationships and originate from diverse domains.
The LiveJournal dataset consists of a snapshot of a large network of bloggers, previously explored in [3]. The snap-shot includes 4,847,571 bloggers who explicitly declare their friendship links. Due to the massive size of this dataset, we consider two portions of it, which we obtain by starting at a random node and performing a breadth-first search from that node. The datasets, henceforth named LJ1 and LJ2, contain 500,000 nodes each. LJ1 and LJ2 contain 10,736,588 and 10,640,429 edges, respectively.

DBLP data is publicly collectible and our dataset consists of a snapshot taken in May 2009 of the on-line publications database site DBLP. The data include a collection of edi-tions of publication venues (i.e., conferences and journals) in computer science. A pair of the 744,386 authors present in the dataset are linked if they have co-authored at least one paper in any of the venues.
 Facebook  X  Rice University Undergraduate (Ugrad) and Graduate (Grad) are an anonymized portion of the Facebook network which include Rice University students, collected by crawling public friends lists on Facebook on May 17, 2008. They consist of two disjoint sets of 1220 undergraduate stu-dents and 503 graduate students, respectively. Mislove et al. [21] present a detailed description of these datasets.
The Amazon dataset [16] is a product co-purchasing net-work from the on-line retailer Amazon.com. Each node rep-resents a book, and an edge exists between two nodes if one was frequently purchased with the other. The network con-tains 270,347 nodes and 741,142 edges. For each book, Ama-zon.com reports up to 5 other items that were frequently purchased with the book.

Biological networks HS, SC, and Fly describe protein-protein interactions for H. Sapiens (human), S. Cerevisiae (a type of yeast), and Drosophila (a fruit fly species) [25], respectively. In these networks, a node represents a protein, and two nodes are connected if scientific evidence of their in-teraction exists. HS contains 10,298 nodes and 54,655 edges, SC contains 5523 nodes and 82,656 edges, and Fly contains 15,326 nodes and 486,970 edges.
The networks we analyze contain annotations reflecting examples of communities that arise in these domains 1 . Some of these sets are user-defined, i.e., users explicitly declare their participation in the community, while others reflect contextual information of the underlying process or organi-zation, e.g., department, protein function, product category, etc.. Below we describe how we identify and clean the an-notated communities for each dataset.

For the social networks, in LiveJournal, users explicitly declare their membership in zero or more communities cre-ated and administered by users. In DBLP, conferences where authors publish their research work reflect the community memberships. Finally, for Facebook  X  Rice University Un-dergraduate and Graduate, users who possess common aca-demic attributes, such as department, major, or dormitory, form the communities. These attributes were obtained by matching Facebook names with student records from the university X  X  directory [21].

For each item in Amazon.com, the on-line store provides several product categories, such as  X  X hoto Essays X  or  X  X and-scape Architecture Textbooks X . We identify a set of nodes possessing a common categorical label as a community.
For HS, SC, and Fly, a number of proteins (though not all) have annotations regarding one or more gene ontology IDs describing the known functions that the protein serves (e.g., metabolic regulation). We use these gene ontology values to identify the communities.

Since we define annotated communities extrinsically to the link structure, the sets formed by the preceding definitions may induce disconnected graphs. However, it is reasonable to limit the definition of community to include only con-nected subgraphs of the network. Therefore, to capture the community information implicit in the annotations, we con-sider each connected component of the graph induced by a node set possessing a common label as an annotated com-munity by itself. Moreover, since we are interested in the
These communities, however, may not represent an unbi-ased sample of communities in these networks as other com-munities that are not annotated might also exist. structure of reasonably sized communities, we filtered out small communities with less than 10 members.
 Overall, we identified 29,955 annotated communities for LJ1, 39,598 for LJ2, 10,595 for DBLP, 24 for RICE-grad, and 41 for RICE-ugrad, 9439 for Amazon, 64 for HS, 76 for SC, and 54 for Fly.
In this section we describe how to produce examples that constitute the structural classes and how to build the fea-ture space for our learning framework. The process consists of two steps. First, we produce the examples by applying community detection algorithms, one for each class, to the network data. Second, we extract features by measuring a broad spectrum of properties of the subgraphs induced by communities. This latter step uses a set of examples consist-ing of the output produced in the previous step along with the set of annotated communities.
To study classes of intrinsically defined communities, we selected a collection of 10 community detection procedures, which are representative of strategies employed by a broad range of algorithms in the literature. We applied these pro-cedures to each of the nine network datasets to extract ex-amples of subgraphs produced by these methods. We labeled examples with the identity of the community detection pro-cedure that produced them. In total, for each network, we created 11 structural classes of communities: one class of extrinsically-defined communities, which comprises exam-ples of annotated communities, and each of the other 10 classes corresponding to intrinsically-defined communities, which comprise examples extracted by each of the 10 com-munity detection algorithms respectively. Below we briefly describe the community detection procedures we consider. 1. Breadth First Search (BFS): To establish a base-2. Random Walk 0 (RW0): The central idea in many 3. Random Walk 0.15 (RW15): This is similar to the 4. (  X , X  ) (AB) : An (  X , X  )-community, for  X  &lt;  X  , requires 5. Link Communities (LC): In contrast with the ma-6. Infomap (IM): The Infomap algorithm [27] views the 7. Louvain: The Louvain method [4] is a popular method 8. Newman-Clauset-Moore (Newman): This method 9. Markov Clustering Algorithm (MCL): MCL [7] 10. Metis: Metis [13] is a graph partitioning method which
In the process of generating examples, we discard com-munities of size less than 10 or greater than 1000, as well as those communities that contain multiple components. The number of examples extracted varies among the procedures. However, the methods we use for class separability are sen-sitive to class imbalance. Thus, we under-sample the large classes and to a lesser extent over-sample small classes to reduce this source of bias.

Our algorithm selection has the purpose of illustrating the applicability of our framework. The approach, however, is not limited to the list we consider. Our method scales to a large number of classes, and a collection of classes should include enough information to reflect the analysis intended. As discussed in the next section, a pair of classes may be highly correlated to each other (e.g., RW0 and RW15). As a result, they may split the predictions in such a way as to obfuscate the interpretation of the outcome. To avoid this pitfall, an inter-class correlation analysis can be employed to assess the independence of the algorithm selection [29].
In the feature extraction phase, we measure the subgraphs induced by the communities produced in the previous step and those induced by annotated communities. We use a large spectrum of measurements that cover many proper-ties of both the internal link structure and the external in-teraction of the community with the rest of the network. Each measurement corresponds to a dimension of our fea-ture space. Table 1 lists the features and describes their corresponding measures.

Most of the features can be understood from their ta-ble description. The feature Information Centrality, how-ever, deserves further explanation. This measure captures a node X  X  degree of centrality as a function of how fast its information can sequentially reach every other node in the network. For a given node, the information centrality com-putes a harmonic mean of the amount of  X  X ignal X  that a node receives from other nodes. A signal between two nodes cor-responds to a path between them, which varies according to the  X  X oise X , instantiated here as the path length [28].
By measuring the structural properties described in Ta-ble 1 for each example of a community derived in the previ-ous phase, we obtain 11 classes of labeled examples in feature space, which constitute the input in our framework.
In this section we present an experimental application us-ing the data we processed through the steps described in the previous section.
Methods for measuring class separability are popular in machine learning for guiding feature selection analysis. Ac-cordingly, effective feature sets for classification tasks are the ones that simultaneously lead to high inter-class and low intra-class variability [29]. Methods of class separability allow for a rigorous analysis of independence among classes. Unfortunately, many of these methods are computationally demanding or dependent on assumptions which are often mismatched with applications [8].

In this work, we frame the research question of discrim-inating the structure of different communities as a class separability problem. The separability of structural classes of communities provides information on whether different communities come from the same (or fundamentally differ-ent) distributions of feature values. This analysis is infor-mative of the extent to which different algorithms produce structural differences and the extent to which community detection algorithms succeed in producing sets that resem-ble annotated communities. Aiming at achieving computa-tional scalability and fine-grained separability information, we use the performance of existing supervised classifiers as a measure of class separability. To our definition, classes are separable to the extent that a classifier can correctly distin-guish their structure by exhibiting an accurate classification. More specifically, we employ two techniques, one paramet-ric, namely Support Vector Machines (SVM) [30], and one nonparametric, namely k -Nearest-Neighbors (kNN) [1], to confirm each other X  X  outcomes while ruling out variability due to the specifics of each algorithm. We select hyperpa-rameters in both cases via grid search using the performance of 10-fold cross validation as the objective function. Both methods are capable of handling a large number of classes and scale to a large volume of data.

We measure class separability using the performance of a 3-fold cross-validation. For each network, we train a multi-class classifier on a set containing two thirds of the examples, which are selected at random, and then evaluate the perfor-mance of the model on the remaining third, which constitute the test set. We perform 3 rounds of this process and average the outcomes. For each element in a test set, the probabilis-tic SVM model outputs a probability mass vector indicating the probability that each data point belongs to each class.
Our primary goal is to gain insight into whether the classes are separable in the feature space defined. Second, building on the preceding observations, we are interested in finding the algorithms whose output structurally reflects the anno-tated communities. Finally, we study the features that are most useful for distinguishing between the structural classes of network communities.

Our first experiment performs the cross-validation on all the 11 classes. We first observe that the experiments suffer little variability between the two classifiers.

To illustrate the method X  X  output, Figure 1(a) presents the analysis of the outcome produced by the SVM-based method applied to the DBLP network. In the picture, we show a bar graph of the distribution of probability mass for each class derived from the network DBLP. This graph vi-sually demonstrates that the bulk of the probability mass from each class was correctly classified. Table 2 contains a summary of results for all networks. Each entry in the table represents the fraction of probability mass from that class that was correctly assigned. When a value appears in paren-theses, this indicates that most of the probability mass was assigned to some other class. While the the SVM provides a breakdown of values by classes, the last two rows present global scores of separability computed using scatter matri-ces [29], which is a standard measure of class separability in pattern recognition 2 . The last row presents a reference point of little separability for each network, where we shuffle the labels of points before computing the score.

As this table shows, only 17 out of 99 network-class pairs failed to have a plurality of the probability mass correctly classified. Newman Modularity is frequently misclassified; however, it is a small class in all networks, especially on the smaller ones (e.g., on network SC, Newman Modularity found only 3 communities of size between 10 and 1000). In the case of annotated communities a plurality of their corresponding classes tends to be correctly classified, with the exception of network Fly. Figure 1(a) serves as a visual reference of a network whose classes have global separability score of 22 . 7. The other networks exhibit comparable scores, with the exception of network Fly, whose classes are not well separated.

The previous experiment shows that annotated commu-nities tend to form their own, separable class that is signif-icantly distinct from all other classes. However, a question We use a criterion referred to as J 3 by [29].
 of interest to the design and application of community de-tection procedures is what algorithms output communities bearing the closest structural resemblance to the annotated communities. To answer this question we perform a varia-tion of the classification task previously described. We train a classifier on the 10 classes corresponding to the community detection algorithms and leave the class of annotated com-munities out of the training set. The goal of this experiment is to evaluate to which class of intrinsically defined commu-nities the annotated examples of the test set are classified.
Figure 1(b) shows the distribution of probability mass of the annotated communities classified into the different classes corresponding to community detection algorithms. The structure that random-walk-based algorithms produce is clearly the most similar to that of the annotated commu-nities. For 7 of the 9 networks, a plurality of the probability mass from the annotated communities was assigned to the classes RW15 and RW0. Due to their high similarity, the classifier confuses these two random-walk-based algorithms as shown in Figure 1(a). The exceptions to this are net-works Grad and Fly. For Grad, their annotated commu-nities X  probability is spread across all classes, where Metis received the plurality of the mass. In the Fly network, the greatest share of the mass of annotated communities is as-signed to LC. These exceptions are associated with small network datasets, therefore the variability could be due to small population sample size. Given the diverse nature of these networks, it is perhaps surprising that in virtually all domains the random-walk communities bear the closest structural resemblance to the annotated communities.
As we have seen in the preceding experiment, each com-munity detection algorithm extracts a distinct structure, which our method is able to separate when projected onto the feature space we define. In this section, we are concerned with identifying the ways in which the algorithms produce bias by finding which properties exhibit the highest degree of between-class variability.

To address this question we use the Correlation-based Fea-ture Selection algorithm (CFS) [11] to identify subsets of the most discriminative features for each network. CFS is intended to give a high score to sets of features that are highly predictive of the class and are not redundant with one another. CFS begins with no nodes in the feature set, and it then employs a hill-climbing algorithm to search the space of feature subsets.
 Table 3 lists the features selected by CFS for each net-the most discriminative features selected by CFS. work ranked in order of the frequency with which they ap-pear in the selection over the networks. The table lists the most frequent features, or sets of features for those proper-ties calculated with quartiles. The entries for row  X  X eatures X  and column  X  X etwork X  that contain the value 1 indicate the presence of that feature in the feature selection applied to that particular network data, whereas empty cells indicate the absence thereof. Integers larger than 1 can be found in some of the entries and indicate the number of quartiles from that feature that were selected by CFS. In nearly every network, conductance, diameter, information centrality, and node betweeness were the most discriminative features.
Surprisingly, in several cases, multiple quartiles of a fea-ture appear: e.g., Fly has 3 path length quartiles, and LJ1 and LJ2 each contain all 5 node betweenness features. We had expected that different quartiles of the same feature would be highly correlated to each other, and therefore they would be unlikely to co-occur among the features selected by CFS. Instead, these results suggest that varying the choice of community detection algorithm results in fine-grained vari-ation in the distribution of such features.

To assess the effectiveness of the features CFS found, Ta-ble 4 presents for all networks the classification performance of the kNN cross validation using both the full set of fea-tures and the subset of features found by CFS. We see that in most cases, there is very little loss in accuracy. We observe a similar qualitative outcome for the SVM cross validation. In the table, the largest drops happened for LJ1 and LJ2 and reduced the accuracy by less than 15%. Being nearly as discriminative as the full set, a reduced set containing a handful of features retains the relevant information needed to analyze the bias produced by different algorithms.
We use the sets of the most discriminative features to study which tendencies in feature values are associated with which algorithms. To this end, we conducted a range anal-ysis which distinguishes the different algorithms according to the value of their features. In the interest of space, we summarize the qualitative outcome of this experiment in Ta-ble 5. The entries correspond to the bias produced by each of the algorithms, considering all networks. Features take on a varying range of values across different networks. Thus, to label the magnitude of features, we compute the mean value of each class and compute a global median of these averages over all classes. The averages occurring between the 33rd and 67th percentile constitute the medium denom-ination; whereas those below the 33rd and above the 67th constitute low and high , respectively. Finally, we count how many times each feature produced each of the denomina-tions across all the networks. From this count, we determine the most frequent tendency which make up the entries we present in the table.

Using this analysis, we are able to group algorithms with similar behavior. For example, the random-walk-based algo-rithms produce the same structural bias. The same holds for Table 5: Tendency of different algorithms with re-spect to the most discriminative features.
 Louvain and Newman; and AB and LC. The profile of an-notated communities is close to that of random-walk-based algorithms, with a few nuances. Annotated communities exhibit medium conductance whereas RW0 and RW15 ex-tract low conductance sets. In addition, the diameter of annotated communities was measured as high for four of the networks, medium for one of them, and low for the re-maining four. This contrasts with RW0 and RW15, which produce set with high diameter. Nevertheless, the similarity due to other features explains the ways in which annotated communities resemble the output of random-walk-based al-gorithms. Finally, Metis and IM differ only in behavior of the node betweenness feature.
In this paper we tackle the complexity involved in the task of extracting communities in networks by illuminating structural properties of different algorithms and communi-ties that arise in networks across a diverse set of domains. Our approach differs fundamentally from previous work in the area due to its supervised nature. The existing com-munity detection algorithms treat the problem as an unsu-pervised decomposition of a network with little sensitivity to different purposes, structures of interest, and the various do-mains of application. Accordingly, our supervised approach may be used by a practitioner to make an informed decision about the most suitable algorithm for a given network in the following way. First, we produce a test set comprising examples of the communities we are interested in finding, which could be either real or synthetic. Second, we choose a set of algorithms we want to evaluate. Finally, we apply our approach using the target network and present the clas-sifier with the test set. The classifier assigns the probability mass of the test set to the class of algorithm that bears close resemblance to the examples. The algorithm that receives the bulk of the mass is the algorithm that may succeed in extracting communities that structurally resemble the ones we are interested in. Researchers may also benefit from our methodology when designing new community detection al-gorithms as a way to compare the behavior of new methods with existing ones.

Structural similarity is a weaker requirement than accu-racy. In other words, communities with similar properties to real communities may not correspond exactly to the commu-nities we may expect to find. Nevertheless, mastering struc-ture is a fundamental stepping stone in the development of algorithms to accurately find the communities of interest.
Finally, our approach suggests a change in the way we approach the problem of community detection. Instead of developing multiple general purpose algorithms that find a particular type of community, one could use a supervised approach that allows the user to specify what they intend to find through examples. Then, one could develop an algo-rithm that learns from these examples and retrieves similar structures. This is part of our agenda for future work. We are grateful to Eduardo Valle for valuable discussions. Support for this research is provided by AFOSR grants FA9550-09-1-0100 and FA9550-09-1-0675. [1] D. W. Aha, D. Kibler, and M. K. Albert.
 [2] Y. Ahn, J. P. Bagrow, and S. Lehmann. Link [3] L. Backstrom, D. Huttenlocher, J. Kleinberg, and [4] V. D. Blondel, J. Guillaume, R. Lambiotte, and [5] F. R. K. Chung. Spectral Graph Theory . American [6] A. Clauset, M. E. J. Newman, and C. Moore. Finding [7] S. V. Dongen. Graph clustering via a discrete [8] N. Fatemi-Ghomi, P. Palmer, and M. Petrou. The [9] S. Fortunato. Community detection in graphs. [10] M. Girvan and M. Newman. Community structure in [11] M. A. Hall. Correlation-based Feature Subset Selection [12] S. Hoory, N. Linial, and A. Wigderson. Expander [13] G. Karypis and V. Kumar. A fast and high quality [14] B. W. Kernighan and S. Lin. An efficient heuristic [15] A. Lancichinetti and S. Fortunato. Community [16] J. Leskovec, L. Adamic, and B. Huberman. The [17] J. Leskovec, K. Lang, A. Dasgupta, and M. Mahoney. [18] J. Leskovec, K. Lang, and M. Mahoney. Empirical [19] R. Lyons and Y. Peres. Probability on Trees and [20] N. Mishra, R. Schreiber, I. Stanton, and R. Tarjan. [21] A. Mislove, B. Viswanath, K. Gummadi, and [22] M. Newman. Detecting community structure in [23] M. Newman. Modularity and community structure in [24] G. Palla, I. Derenyi, I. Farkas, and T. Vicsek. [25] D. Park, R. Singh, M. Baym, C.-S. Liao, and [26] P. Pons and M. Latapy. Computing communities in [27] M. Rosvall and C. Bergstrom. Multilevel compression [28] K. Stephenson and M. Zelen. Rethinking centrality: [29] S. Theodoridis and K. Koutroumbas. Pattern [30] V. N. Vapnik. Statistical Learning Theory .
 [31] E. Weinan, T. Li, and E. Vanden-Eijnden. Optimal
