 Human conversation is a seemingly simple, ever y-day phenomenon that requires a complex mental process of turn -taking, in which participants ma n-age to yield and hold the floor with little pause in -between speaking turns. Most linguis ts subscribe to the idea that this process is g overned by a su b-conscious internal mechanism, that is, a set of cues or rules that steers humans toward proper turn -taking (Duncan, 1972). These cues may include lexical fe a tures such as the words used to end the turn, or prosodic features such as speaking rate, pitch, and intensity (Cutler and Pearson, 1986).
While successful turn -taking is fairly easy for humans to accomplish , it is still difficult for mo d-els to be implement ed in spoken dialogue sy s-tems. Ma ny systems use a set time -out to decide when a user is finished speaking, often r e sulting in unnaturally long pauses or awkward overlaps (Ward, et. al., 2005) . Others detect when a user interrupts the system, known as  X  X arge -in X , though this is chara c teri s tic of failed turn -taking rather than successful conversation (Glass, 1999). 
Improper turn -taking can often be a source of u s-er discomfort and dissatisfaction with a spoken dialogue system. Little work has been done to study turn -taking in tutoring, so we hope to inve s-tigate it further while using a human -human (HH) tutoring corpus and language technologies to e x-tract useful info r mation about turn -taking cues . This analysis is particular ly interesting in a tuto r-ing domain because of the speculated uneq ual st a-tuses of participants. The goal is to eventually develop a model for turn -taking based on this ana l-ysis which can be implemented in an existent tuto r-ing system, ITSPOKE, an intelligent tutor for college -level Newtonian physics ( Litman and Si l-liman, 2004 ). IT S POKE currently uses a time -out to determine the end of a student turn and does not re c ognize student barge -in . We hypo thesize that improving upon the turn -taking model this system uses will help engage students and hopefully lead to i n creased student learning , a standard perfo r-mance measure of intelligent tutoring systems (Litman et. al., 2006) . T urn -taking has been a recent focus in spoken d i-alogue system work , with research producing many different models and approaches . Raux and Eskenazi (2009) proposed a finite -state turn -taking model, which is used to predict e nd -of -turn and pe r formed significantly better than a fixed -threshold baseline in reducing endpointing latency in a spoken dialo gue system . Selfridge and He e-man (2010) took a different approach and p r e-sented a bidding model for turn -taking, in which dialogu e participants compete for the turn based on the i m portance o f what they will say next .

Of considerable inspiration to the research in this paper was Gra vano and Hirschberg X  X  (2009) anal y-sis of their games corpus, which showed that it was pos s ible for turn -yielding cues to be identified in a n HH corpus . A similar method was used in this analysis , though it was adapted based on the tools and data that were readily available for our corpus . Since these differences may prevent direct compa r-ison between corpo ra, future work will f o cus on making our method more analogous .

Since our work is similar to that done by Grav a-no and Hirschberg (2009), we hypothesize that turn -yielding cues can also be identified in our HH tutoring corpus. However, it is possible tha t the cues identified will be very different, due to factors specific to a tutoring enviro n ment. These include , but are not limited to , status differences b e tween the student and tutor, engagement of the student, and the di f ferent goals of the student and tutor.
Our hypothesis is that for certain prosodic fe a-tures, there will be a significant differ ence between places where students yield their turn (allow the tutor to speak) and places where they hold it (co n-tinue talking) . This would designate these fea tures as turn -taking cue s , and would allow them to be used as features in a turn -taking model for a sp o-ken dialogue sy s tem in the future. The data for this analysis is from a n HH tutoring corpus recorded during the 2002 -2003 school year. This is a n audio corpus of 17 university st u-dents , all native Standard English speak ers, wor k-ing with a tutor (the same for all subjects) on physics problems ( Litman et. al., 2006 ). Both the student and the tutor were sitting in front of sep a-rate work stations, s o they could communicate only through microphon es or, in the case of a student -written essay, through the shared computer env i-ronment. Any potential turn -taking cues that the tutor received from the student were very co m p a-rable to what a spoken dialogue s ystem would have to analyze during a user int e raction .
 lated and segmented into breath groups. A breath group is defined as any segment of speech by one dialogue participant bounded by 200 ms of s i lence or more based on a certain threshold of intensity (Li scombe et. al., 2005). This break -down a l lowed for feature me a surement and co m parison at places th at were and were not turn boundaries. Although Gravano and Hirschberg (2009) se g mented their corpus by 50 ms of silence , we used 200 ms to d i-vide the breath groups , as this data had already been calculated for another exp e riment done with the HH corpus ( Liscombe et. al., 2005) . 1 one of the followi ng: HOLD, when a breath group was immediately followed by a second breath group from the same person , YIELD, when a breath group was immediately followed by speech from the other participa nt, or OVERLAP, when speech from another participant started b e fore the cu r rent one ended . Figure 1 is a diagram of a h y-p o thetical conversation b e tween two parti ci pants, with exa m ples of HOLD X  X  and YIELD X  X  labeled. These groups were determined strictly by tim e and not by the actually speech being spoken. Speech acts such as bac k channels, then, would be included in the YIELD group if they were spoken during clear gaps in the tutor X  X  speech, but would be placed in the OVERLAP group if they occurred during or ov erlapping with tutor speech. There were 9,169 total HOLD's in the corpus and 4,773 YIELD  X  s; these were used for comparison, while the OVERLAP X  X  were set aside for f u ture work.
Four prosodic features were calculated for each breath group: duration, pitch, RMS, and percent silence. Duration is the length of the breath group in seconds. Pitch is the mean fundamental fr e-quency (f0) of the speech. RMS (the root mean squared amplitude) is the energy or loudness. Pe r-cent silence was the amount of internal sil ence within the breath group. For pitch and RMS, the mean was taken over the length of the breath group. These features were used because they are sim ilar to those used by Gravano and Hi r schberg (2009) , and are already used in the spoken dial o-gue system we will be using (Forbes -Riley and Litman, 2011) . While only a small set of features is examined here, future work will include expan d-ing the fe a ture set.
 Mean values for each feature for HOLD X  X  and YIELD X  X  were calculated and comp ared usi ng the student T -test in SPSS Statistics software . Two separate tests were done, one to compare the means for each student indiv i dually, and one to compare the means across all students. p  X  .05 is considered signif i cant for all statistical tests . The p -values given are the probability of obtaining the difference between groups by chance. 4.1 Individual Cues First, means for each feature for HOLD X  X  and YIELD X  X  were compared for each su bject indiv i-dually. These i ndividual results indicated that while turn -taking cues could be identified, there was much variation between students. Table 1 displays the results of the ana l ysis for one subject, student 111. For this student, all four pros odic fe a-tures are turn -taking cues, as there is a significant different between the HOLD and YIELD groups for all of them. Ho w ever, for all other stu dents, this was not the c ase. As shown in Table 3 , m u l-tiple significant cues could be identified for most st u dents , and there was only one which appeared to have no si g nificant turn -yielding cues.

Because there was so much individual variation, a paired T -test was used to compare the means across su b jects. In this analysis , duration, pitch , and RMS were all found to be signifi cant cues . P ercent s i lence , however , was not. The results of this test are summarized in Ta ble 2 . A more d e-tailed look at each of the three significant cues is done below.

Duration : The mean duration for HOLD X  X  is longer than the mean duration for YIELD X  X . This suggests that students speak for a longer uninte r-rupted time when they are trying to hold their turn, and yield their turns wit h shorter utterances. This is the opposite of Gravano and Hirschbe rg X  X  (2009) r e sults, which found that YIELD X  X  were longer .
Pitch : The mean pitch for YIELD X  X  is higher t han the mean pitch for HOLD X  X . Gravano and Hirschbe rg (2009) , on the other hand, foun d that YIELD X  X  were lower pitch ed than HOLD X  X  . This diffe r ence may be accounted for by the difference in tasks . During tutoring , students are possibly more uncertain , which may raise the mean pitch of the YIELD breath groups.

RMS : The mean RMS, or energy , for HOLD X  X  is higher than the mean energy for YIELD X  X . This is co n sistent with student X  X  speaking more softly, i.e., trailing off, at the end of their turn, a usual phen o-menon in human speech . This is consistent with the results from the Columbia games corpus (Gr a-vano and Hirshberg , 2009). 4.2 Combining Cues Gravano and Hirschberg (2009) were able to show using their cues and corpus that there is a positive rel a tionship between the number of turn -yielding cues present and the probability of a turn actually being taken . This suggests that i n order to make sure that the other participant is aware whether the turn is going to continue or end, the speaker may subconsciously give them more info rmation through multiple cues.

To see whether this relationship exist ed in our data, each breath group was marked with a bi nary value for each significant cue , representing whet h-er the cue was present or not present within that breath group. A cue was considered present if the value for that breath group was strictly close r to the student X  X  mean for YIELD X  X  than HOLD X  X . The number of cues present for each breath group was totaled. Only the three cues found to be significant cues were used for these calculations. For each number of cues possible x (0 to 3, inclusively), t he probability of the turn b e-ing taken was calculated by p( x ) = Y / T , where Y is the nu m ber of YIELD X  X  with x cues present, and T is the total number of breath groups with x cues present.

According to thes e r e sults, a positive relation ship seems to exist for these cues and this cor pus. Fi g-ure 2 shows the results plotted with a fitted regre s-sion. The number of cues present and probability of a turn yield is strongly correlated (r = .923 , p=.038 ). A r e gression analysis done using SPSS showed that the adjusted r 2 = . 779 (p = .077) .
W hen no turn -yielding cues are present, there is still a majority chance that the st u dent will yield their turn; h owever, this is under s tandable due to the small number of cues being a nalyzed. Regar d-less, this gives a very preliminary su p port for the idea that it is possible to pr e dict when a turn will be ta ken based on the number of cues present. This paper presented prelimina ry work in using an HH tutoring corpus to cons truct a turn -taking mo d-el that can later be implemented in a spoken dial o-gue sy s tem. A small set of prosodic features was used to try and identify turn -taking cues by co m-paring their values at places where students yielded their turn to the tutor and plac es where they held it. R e sults show that turn -taking cues such as those i n vestigated can be identified for the corpus, and may hold predictive ability for turn bound a ries. 5.1 Future Work When building on this work, there are two diffe r-ent directions in which we can go. While this work uncovers some interesting results in the tuto r-ing domain, there are some shortcomings in the method that may make it difficult to effectively evaluate the results. As the breath group is diffe r-ent from the segment used in Grav ano and Hi r-schbe rg X  X  (2009) experiment, and the set of prosodic features is smaller, direct comparison b e-comes quite difficult. The differences between the two methods provide enough doubt for the results to truly be interpreted as contradictory. Thus th e first line of future inquiry is to redo this method using a smaller silence boundary (50 ms) and di f-ferent set of prosodic features so that it is truly comparable to Gravano and Hir schbe rg X  X  (2009) work with the game corpus. This could yield i n-teresting discoveries in the differences between the two corpora , shedding light on phenomena that are pa r ticular to tutoring scenarios .

On the other hand, other researche rs have used different segments ; for e x a m ple, Cl e mens and D i-ekhaus (2009) divide their corpu s by  X  X o p ic units X  that are gra m matically and sema n t i cally complete. In add i tion, Litman et. al. (2009) were able to use word -level units to ca l culate pro s ody and cla s sify turn -level unce r tainty. Perhaps direct compa r ison is not e n tirely ne ce s sary, and instead this work should be cons i dered an isolated look at a n HH corpus that pr o vides insight into turn -taking , sp e-cifically in tutoring and other d o mains with u n-equal power levels. Future work in this direction would include growing the set of fe a tures by ad d-ing more proso d ic ones and intr o du c ing le x ical ones such as bi -grams and uni -grams. A l ready, work has been done to investigate the features used in the INTERSPEECH 2009 Emotion Challenge using openSMILE (Eyben et. al., 2009). When a l arge feature bank has been developed, significant cues will be used in co n junction with machine learning techniques to build a model for turn -taking which can be impl e mented in a spoken d i-alogue tutoring system . The goal would be to learn more about h u man turn -taking while seeing if be t-ter turn -taking by a computer tutor ultimately leads to i n creased st u dent learning in an intelligent tuto r-ing system.
 This work was supported by the NSF ( # 0631930 ). I would like to thank Diane Litman, my advisor, Scott Silliman, for softwa re assistance, Joanna Drummond, for many helpful comments on this paper, and the ITSPOKE research group for their feedback on my work . 
