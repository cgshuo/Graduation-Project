 Coreference resolution aims to find the set of lin-guistic expressions that refer to a common entity. It is a discourse-level task given that the ambiguity of many referential relationships among linguistic ex-pressions can only be correctly resolved by examin-ing information extracted from the entire document.
In this paper, we focus on exploiting the struc-tural information (e.g., speaker and turn in conversa-tional documents) represented in the metadata of an input document. Such metadata often coincides with the discourse structure, and is presumably useful to coreference resolution. The goal of this study is to quantify the effect metadata. To this end, informa-tion contained in metadata is encoded as features in our coreference resolution system, and statistically significant improvement is observed.
 The rest of the paper is organized as follows. In Section 2 we describe the data set on which this study is based. In Section 3 we first show how to incorporate information carried by metadata into a statistical coreference resolution system. We also quantify the impact of metadata when they are treated as extraneous data. Results and discussions of the results are also presented in that section. This study uses the 2007 ACE data. In the ACE program, a mention is textual reference to an object of interest while the set of mentions in a document referring to the same object is called entity . Each mention is of one of 7 entity types: FAC(cility), GPE (Geo-Political Entity), LOC(ation), ORG(anization), PER(son), VEH(icle), and WEA(pon). Every entity type has a prede-fined set of subtypes. For example, ORG sub-types include commercial , governmental and educational etc, which reflect different sub-groups of organizations. Mentions referring to the same entity share the same type and subtype. A mention can also be assigned with one of 3 men-tion types: either NAM (e), NOM (inal), or PRO (noun). Accordingly, entities have  X  X evels: X  if an entity con-tains at least one NAM mention, its level is NAM ; or if it does not contain any NAM mention, but contains at least one NOM mention, then the entity is of level NOM ; if an entity has only PRO mention(s), then its level is PRO . More information about ACE entity annotation can be found in the official annotation guideline (Linguistic Data Consortium, 2008).
The ACE 2007 documents come from a variety of sources, namely newswire, broadcast conversation, broadcast news, Usenet, web log and telephone con-versation. Some of them contain rich metadata, as illustrated in the following excerpt of one broadcast conversation document:
In this example, SPEAKER and TURN informa-tion are marked by their corresponding SGML tags. Such metadata provides structural information: for instance, the metadata implies that Begala is the speaker of the utterance  X  X ell, we X  X l debate ...,  X  and Novak the speaker of the utterance  X  X aul, as I understand your definition ... X  Intuitively, knowing the speakers of the previous and current turn would make it a lot easier to find the right antecedent of pronominal mentions I and your in the sentence:  X  X aul, as I understand your definition ... X 
Documents in non-conversational genres (e.g. newswire documents) also contain speaker and quo-tation, which resemble conversational utterance, but they are not annotated. For these documents, we use heuristics (e.g., existence of double or single quote, a short list of communication verb lemmas such as  X  X ay, X   X  X ell X  and  X  X peak X  etc) to determine the speaker of a direct quotation if necessary. In this section we describe how metadata is used to improve our statistical coreference resolution sys-tem. 3.1 Resolution System The coreference system used in our study is a data-driven, machine-learning-based system. Mentions in a document are processed sequentially by men-tion type: NAM mentions are processed first, fol-lowed by NOM mentions and then PRO mentions. The first mention is used to create an initial entity with a deterministic score 1. The second mention can be either linked to the first entity, or used to cre-ate a new entity, and the two actions are assigned a score computed from a log linear model. This pro-cess is repeated until all mentions in a document are processed. During training time, the process is ap-plied to the training data and training instances (both positive and negative) are generated. At testing time, the same process is applied to an input document and the hypothesis with the highest score is selected as the final coreference result. At the core of the coreference system is a conditional log linear model P ( l | e,m ) which measures how likely a mention m is or is not coreferential with an existing entity The modeling framework provides us with the flexi-bility to integrate metadata information by encoding it as features.

The coreference resolution system employs a va-riety of lexical, semantic, distance and syntactic features(Luo et al., 2004; Luo and Zitouni, 2005). The full-blown system achieves an 56 . 2% ACE-value score on the official 2007 ACE test data, which is about the same as the best-performing sys-tem in the Entity Detection and Recognition (EDR) task (NIST, 2007). So we believe that the resolution system is fairly solid.

The aforementioned 56 . 2% score includes men-tion detection (i.e., finding mention boundaries and predicting mention attributes) and coreference res-olution. Since this study is about coreference res-olution only, the subsequent experiments, are thus performed on gold-standard mentions. We split the ACE 2007 data into a training set consisting of 499 documents, and a test set of 100 documents. The training and test split ratio is roughly the same across genres. The performance numbers reported in the subsequent subsections are on the 100-document de-velopment test set. 3.2 Metadata Features For conversational documents with speaker and turn information, we compute a group of binary features for a candidate referent r and the current mention m . Feature values are 1 if the conditions described below hold:  X  if r is a speaker, m is a pronominal mention and  X  if r is a speaker, m is pronoun and r utters the  X  if mention r and mention m are seen in the  X  if mention r and mention m are in two consec-Note that the first feature is not subsumed by the third one since a turn may contain multiple sen-tences. For the same reason, the last feature does not subsume the second one. For the sample document in Section 2, the first feature fires if r = Novak and m = I ; the second features fires if r = Begala and m = I ; the third feature fires if r = Paul and m = I ; and lastly, the fourth feature fires if r = We and m = I . For ACE documents that do not carry turn and speaker information such as newswire, we use heuristic rules to empirically de-termine the speaker and the corresponding quota-tions before computing these features.

To test the effect of the feature group, we trained two models: a baseline system without speaker and turn features, and a contrast system by adding the speaker and turn features to the baseline system. The contrast results are tabulated in Table 1. We observe an overall 0.7 point ACE-value improvement. We also compute the ACE-values at document level for the two systems, and a paired Wilcoxon (Wilcoxon, 1945) rank-sum test is conducted, which indicates that the difference between the two systems is statis-tically significant at level p  X  0 . 002 .

Note that the features often help link pronouns with their antecedents in conversational documents. But ACE-value is a weighted metric which heav-ily discounts pronominal mentions and entities. We suspect that the effect of speaker and turn informa-tion could be larger if we weigh all mention types equally. This is confirmed when we looked at the un-reported by the official ACE08 scorer (column B 3 in Table 1): the overall B 3 score is improved from 73 . 8% to 76 . 4%  X  a 2.6 point improvement, which is almost 4 times as large as the ACE-value change. 3.3 Metadata: To Use Or Not to Use? In the ACE evaluations prior to 2008, mentions in-side metadata (such as speaker and poster) are anno-tated and scored as normal mentions, although such metadata is not part of the actual content of a doc-ument. An interesting question is: how large an ef-fect do mentions inside metadata have on the system performance? If metadata are not annotated as men-tions, is it still useful to look into them? To answer this question, we remove speaker mentions in con-versational documents (i.e., broadcast conversation and telephone conversation) from both the training and test data. Then we train two systems:  X  System A: the system totally disregards meta- X  System B: the system first recovers speaker
After mentions in the test data are chained in Sys-tem B, speaker mentions are then removed from sys-tem output so that the coreference result is directly comparable with that of System A.

The ACE-value comparison between System A and System B is shown in Table 2. As can be seen, System B works much better than System A, which ignores SPEAKER tags. For telephone con-versations (cts), ACE-value improves as much as 4.6 points. A paired Wilcoxon test on document-level ACE-values indicates that the difference is statisti-cally significant at p&lt; 0 . 016 .

The reason why metadata helps is that speaker mention can be used to localize the coreference pro-cess and therefore improves the performance. For example, in the sentences uttered by  X  X ovak X  (cf. the sample document in Section 2), it is intuitively straightforward to link mention I with Novak , and your with Begala  X  when speaker mentions are made present in the coreference system B. On the other hand, in System A,  X  X  X  is likely to be linked with  X  X aul X  because of its proximity of  X  X aul X  in the absence of speaker information.

The result of this experiment suggests that, unsur-prisingly, speaker and turn metadata carry structural information helpful for coreference resolution. Even if speaker mentions are not annotated (as in System A), it is still beneficial to make use of it, e.g., by first identifying them automatically as in System B. There is a large body of literature for coreference resolution based on machine learning (Kehler, 1997; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; Luo et al., 2004) approach. Strube and Muller (2003) presented a machine-learning based pronoun resolution system for spoken dialogue (Switchboard corpus). The document genre in their study is simi-lar to the ACE telephony conversation documents, and they did include some dialogue-specific fea-tures, such as an anaphora X  X  preference for S, VP or NP, in their system, but they did not use speaker or turn information. Gupta et al. (2007) presents an algorithm disambiguating generic and referential  X  X ou. X 
Cristea et al. (1999) attempted to improve coref-erence resolution by first analyzing the discourse structure of a document with rhetoric structure the-ory (RST) (Mann and Thompson, 1987) and then using the resulted discourse structure in coreference resolution. Since obtaining reliably the discourse structure itself is a challenge, they got mixed results compared with a linear structure baseline.

Our work presented in this paper concentrates on the structural information represented in metadata, such as turn or speaker information. Such metadata provides reliable discourse structure, especially for conversational documents, which is proven benefi-cial for enhancing the performance of our corefer-ence resolution system.
 This work is partially supported by DARPA GALE program under the contract number HR0011-06-02-0001. We X  X  also like to thank 3 reviewers for their helpful comments.

