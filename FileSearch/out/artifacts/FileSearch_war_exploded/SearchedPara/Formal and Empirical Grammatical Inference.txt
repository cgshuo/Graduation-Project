 I. Formal GI and learning theory (de la Higuera) language classes (Heinz) classes (van Zaanen) What is grammatical inference? What does learning or having learnt imply? Reasons for considering formal learning probabilistic setting Learning = building, inferring strings, or trees, or graphs actively querying elle 7 X  [ pred ], a 7 X  [#(  X  clit  X  3 d  X  obj ) \ #(  X  clit  X  a  X  obj ) \ pred \ S / aux  X  a  X  d ], Why grammar and not language ? Why a and not the ? Combinatorial characterisation The learning problem becomes an optimisation problem! Then we often have theorems saying that Simplicity Coverage Usefulness Occam argument Compression argument Kolmogorov complexity MDL argument concerned with saying something about: Suppose you are building a random number generator. How are you convinced that it works? Empirical approach Experimental approach Formal approach For example, genetic algorithms or neural networks Or some mathematical principle (Occam, Kolmogorov, MDL,. . . ) Can become a principled approach Benchmarks Competitions Necessary but not sufficient How do we know that all the cases are covered? How do we know that we dont have a hidden bias? Is impossible: But we can say something about the algorithm: favourable setting Thus there is a hidden bias inside class L 2 , with L 2  X  L 1 corrupt, then, given enough time, we will learn it Replace the notion of learning by that of identifying (something to learn + good data + enough of it) algorithm Complexity theory should be used: the total or update changes, the number and weight of errors. . . . . . should be measured and limited. is supposed to know how to learn, something is wrong somewhere (preposterously, the maths can X  X  be wrong. . . ) Identification in the limit Resource bounded identification in the limit Active learning (query learning) hypothesis after inspecting each piece of data concept and not change from it
Number Presentation Analysis of hy-9234 aaaaaaaa -consistent a ( aa )  X  where X is some set, function Yields : Yields (  X  ) = L If  X  ( N ) =  X  ( N ) then Yields (  X  ) = Yields (  X  )  X  : N  X   X   X  such that  X  ( N ) = L  X  is an infinite succession of all the elements of L (note : small technical difficulty with  X  ) function  X  : N  X   X   X   X { X  , + } such that  X  ( N ) = ( L , +)  X  ( L ,  X  ) indicate if they belong or not to L oracle) through queries A membership query An equivalence query Illegal presentation from text: ab , ab , ab ,. . . Illegal presentation from text: Goooool ( un ,+), ( lugar ,+), ( lugor ,-), ( xwszrrzt ,-),  X  .
 input a set  X  n and returns a grammar of a language. Given a grammar G , L ( G ) is the language generated/recognised/ represented by G . A converges to G with  X  if A (  X  n )  X  n depending on the order in which it receives the data. Usually an order independent learner is better. We can try to bound an implicit prediction error.
 The CS is small. number of queries made before halting with a correct grammar is polynomial in Cannot learn Nfa , Cfg s from an informant in most polynomial settings (Pitt 1989, de la Higuera 1997) Cannot learn Dfa from text (Gold 1967) (Angluin 1981 &amp; 1987).
 (Oncina and Garc  X  X a 1992); Can learn Dfa from membership and equivalence queries (Angluin 1987). unknown distributions) Identification with probability 1 (about identifying distributions) distributions) We have a distribution over  X   X  We sample twice: The Pac setting: Les Valiant, Turing award 2010 L a class of languages G a class of grammars  X  &gt; 0 and  X  &gt; 0 m a maximal length over the strings n a maximal size of machines most  X  we require at most p ( m , n , 1 than  X  . Unless there is a surprise there should be no surprise 2008) First surprise is  X  , second surprise is  X  examples (Kearns &amp; Vazirani) directly the distributions! Means that the probability of not converging is 0 Mainly a (nice) theoretic setting finite state automata (Carrasco and Oncina, 1994) and Oncina, 2004) Pac definition applies But error should be measured by a distance between the target distribution and the hypothesis How do we measure the distance: L 1 , L 2 , L  X  , Kullback-Leibler? Too easy to learn with L  X  Too hard to learn with L 1 Oncina, 2004) Nice algorithms for biased classes of distributions Alternative views on learnability? reasonable? Can we learn transducers? Probabilistic transducers? Why regular? What are the general GI strategies? What are the main results? The main techniques? The main lessons? Classes of formal languages may exist which better because the instance space of the problem is smaller. data by an incremental learner.
 reversible languages.
 this paper. Targets of Learning Learning Frameworks State-merging Results for learning regular languages, relations, and distributions Regular expressions Generalized regular expressions Finite state acceptors Right or left branching rewrite rules . . . Regular expressions (for relations) Generalized regular expressions (for relations) Finite state transducers . . . Weighted finite state automata Hidden Markov Models Weighted right or left branching rewrite rules . . . Canonical forms relate to algebraic properties (Nerode determined. For example, there are no canonical (e.g. shortest) regular expressions for regular languages. 1967) Non-enumerative algorithms for regular languages: in the limit (Gold 1967) setting. reversible languages (Angluin 1982) strictly local languages (Garcia et al. 1990) strictly piecewise languages (Heinz 2010) . . . subsequential functions (Oncina et al. 1993) . . . computable classes of r.e. texts (Gold 1967) The class of r.e. distributions are identifiable from and Vitany  X  X  2007) (de la Higuera and Thollard 2000) in a modified PAC setting (Clark and Thollard, 2004) Angluin 1982 (reversible languages) Muggleton 1990 (contextual languages) Garcia et al. 1990 (strictly local languages) Oncina et al. 1993 (subsequential functions) Clark and Thollard 2004 (PDFA distributions) . . . languages, many others) Kasprizk and K  X otzing 2010 (function-distinguishable lanaguages, pattern languages, many others) Tellier (2008) Builds a FSA representation of the input Generalize by merging states Each word its own FSA (Nondeterministic) Prefix Trees (deterministic) Suffix Trees (reverse determinstic) 16 States are identified as equivalent and then merged . All transitions are preserved.
 machine accepts, possibly more.
 0 1 2 1 2 3 The merged machine may not be deterministic. Q  X  =  X  (the states are the blocks of  X  ) I  X  = { B  X   X  : I  X  B 6 =  X  X  F  X  = { B  X   X  : F  X  B 6 =  X  X  For all B  X   X  and a  X   X ,  X  What is  X  ? Primary stress falls on the initial syllable k -length suffix. The algorithm then is simply: al. 1990). 44 of them 81 are learned.
 Oncina et al. 1993) . . . merging, which results in generalizations. will be lost) The trouble is that an observer who notices everything and constructible to describe a situation [emphasis in original]. up to length k ) (Garcia et al. 1990) k-deterministic for some k ) (Angluin 1982) k -contextual languages (Muggleton 1990) . . . transliteration translation . . . anything with finite state transducers data. the function is defined. which are determinstic on the input and which have an  X  X utput X  string associated with every state. have a canonical form. input (Mohri 1997). German word-final devoicing rule from data present in adapted dictionaries of English or German improve state-merging choices.
 M When the structure of a Deterministic FSA is known in advance, MLE is easy to do. Consonantal harmony 2004, and many others) Vowel harmony (Ringen 1988, Bakovi  X c 2000, and many others) cf. *[ s tojonowonowa S ] and cf. *[ S tojonowonowa s ] solely make distinctions on the basis of potentially discontiguous subsequences up to some length k model theory, and the algebraic theory of automata (Fu et al. 2011) representations (Rogers et al. 2010) al. 2005) like the ones in Samala.

A0 A1 B0 B1 C0 C1  X   X  Training corpus 4800 words from a dictionary of Samala and Thollard 2000). 2004). The algorithms presented employ state-merging methods. with n-gram distributions or with SP distributions. tasks in CL. of languages. goals? Identify the range and kind of patterns (linguistics). linguistics). computational linguistics) State-merging is a well-studied strategy for inferring acceptors and transducers. learning frameworks. hardest learning settings. Recent advances yield algorithms for large classes (probabilistic DFAs) and how such patterns in these classes can be learned. Empirical grammatical inference Family of languages Information contained in input Overview of systems Evaluation issues From empirical to formal GI Language learning Formal grammatical inference Empirical grammatical inference Try to identify language given samples Underlying language class is unknown If identification is impossible, provide approximation What is the underlying family of languages? Choice has impact on learning algorithm Many possibilities Extract all subsequences of length n ( n -grams) Count occurrences of n -grams in texts Assign probabilities to each n -gram based on counts Unseen n -grams How likely is the sentence  X  X ohn likes Mary X ? N -grams provide a probability for each sequence Starting from a treebank (sentences with structure) Count occurrences of grammar rules in treebank Assign probabilities to grammar rules based on counts Over-generalization,  X  X ncorrect X  probabilities He Extract counts from treebank  X  probabilities Reestimate probabilities Starting from a corpus Identify regularities that may serve as grammar rules Output: Learning system has to deal with both Fixed versus flexible is really a sliding scale Language modelling using n -grams Language modelling using extracted grammar rules  X  X earning structure X  Choices: EMILE Alignment-Based Learning (ABL) ADIOS CCM+DMV U-DOP . . . Given a collection of plain sentences On what basis are we going to assign structure? Should structure be linguistically motivated? Learns context-free grammars Using plain sentences Originally used to show formal learnability Starting from simple sentences Store recurring subsequences and contexts Introduce grammar rules when there is enough evidence sees . . . . . . Modified sequences may again contain terms/contexts Terms may consist of multiple words Based on substitutability test Using plain sentences Similar to EMILE, but Output is structured version of input or grammar (van Zaanen 2000a, b, 2002) Alignment learning (Clustering) Selection learning Align pairs of sentences Unequal parts of sentences are stored as hypotheses Align all sentences in a corpus to all others I need ( I need) ( Alignment learning can generate overlapping brackets Underlying grammar is considered context-free  X  X rong X  brackets have to be removed Represent language as a graph Compress graph (Recursion may be added as a post-processing step) Initialization Pattern distilation Generalization Repeat 2 and 3 until no new patterns are found P
R describes path to the right similarly P L describes path to the left Pick most significant pattern (Klein 2002) DMV aims to learn dependency relations Model describes likelihood of CCM and DMV can be combined Both models have different view on structure (Klein 2004) Similar to CCM in that it U-DOP uses Data-Oriented Parsing (DOP) as formalism Requires practical implementation choices (Bod 2006a, b) Extract all subtrees Estimate probabilities on subtrees using EM S V NP Remove either all or no elements on a level Each subtree receives a probability Subtrees can be recombined into a larger tree Same parse may be created using different derivations S V NP Essentially, U-DOP uses implied substitutability Recall (completeness) Precision (correctness) F-Score (combination of Precision and Recall) (van Zaanen and Adriaans 2001) Air Travel Information System (ATIS) Taken from Penn Treebank II 568 English sentences Macro Count constituents and average per sentence Macro 2 Compute Macro Precision/Recall, average at end No standard evaluation exists but de facto evaluation datasets arise Systems have different input/output Evaluation settings influence results Learning context-free grammars is hard Is learning context-sensitive grammars impossible? We may not need  X  X ull X  context-sensitiveness Mildly context-sensitive grammars may be enough for NL (Huybrechts 1984, Shieber 1985) Open research area Some work has already been done Example: consider the case of substitutability There are situations in which substitutability breaks: This suggests that learning based on substitutability learns a different family of languages (not CFG) Non-terminally separated (NTS) languages Grammar G= h  X  , V , P , S i is NTS Additional restriction: In other words: non-terminals correspond exactly with substitutability It can be shown that NTS grammars are Unfortunately, natural language is not an NTS language Ultimate goal: Relation between formal GI and empirical GI Ultimate aim: Find family of languages that is transducers, CFGs, MCSGs) happen (as well as exciting challenges, competitions, benchmarks etc.) and natural language processing is taking place. The future is bright!
