 The two key challenges in hierarchical classification are to leverage the hierarchical dependencies between the class-labels for improving performance, and, at the same time maintaining scalability across large hierarchies. In this pa-per we propose a regularization framework for large-scale hierarchical classification that addresses both the problems. Specifically, we incorporate the hierarchical dependencies between the class-labels into the regularization structure of the parameters thereby encouraging classes nearby in the hierarchy to share similar model parameters. Furthermore, we extend our approach to scenarios where the dependen-cies between the class-labels are encoded in the form of a graph rather than a hierarchy. To enable large-scale train-ing, we develop a parallel-iterative optimization scheme that can handle datasets with hundreds of thousands of classes and millions of instances and learning terabytes of param-eters. Our experiments showed a consistent improvement over other competing approaches and achieved state-of-the-art results on benchmark datasets.
 I.5.2 [ Pattern Recognition ]: Design Methodology; Clas-sifier Design and Evaluation; H.4 [ Information Systems ]: General Algorithms,Design,Experimentation Hierarchical Classification, Recursive Regularization, Paral-lel Optimization, Large-scale Evaluation  X 
A preliminary version of this paper was presented to the Large-scale Hierarchical Text Classification Workshop, ECML 2012
Hierarchies provide a natural way to browse and orga-nize unstructured data at multiple levels of granularity. The large taxonomies for Web Page categorization at Yahoo! Di-rectory and the Open Directory Project, the International Patent Classification Hierarchy for patents are examples of such widely used hierarchies. Firstly, unlike binary classifi-cation, we need to address how to use the hierarchy. Sec-ondly and importantly, we need to develop scalable meth-ods than can handle large-scale problems, as most real world Hierarchical Classification (HC) are characterized by large hierarchies and training set sizes. For example, consider the Open Directory Project 1 , one of the largest human edited hierarchy of the entire Web containing more than 4.4 million webpages categorized into a hierarchy of 766,930 class-labels with more than 10 levels of depth. Developing techniques that leverage hierarchical dependencies between these hun-dreds of thousands of class-labels as well as scaling to the millions of training instances is a non-trivial task.
A primary problem in HC is the data-sparsity issue for majority of class-labels [22],[36],[5]. For example, 76% of the class-labels in the Yahoo! Directory have less than 5 positive instances [22] and 72% of the Open Directory Project have less than 4 positive instances 1 . Learning independent mod-els (one per class-label) with such limited training examples might lead to poor performance due to over-fitting. This raises one of the important questions in HC research, i.e., Can we develop solutions that can use the positive train-ing examples from other classes based on the hierarchical dependencies between them? and can we do this both effec-tively and efficiently for large-scale hierarchical classification (LSHC)?
Previous works have often concentrated on different parts of the problem. For instance, the most popular in the early stage of LSHC research are the  X  X achinko-machine models X  [11], [36] [22], [18] where the classification task is decom-posed into sub-tasks recursively, and each node of the hier-archy has an independently trained classifier. The hierarchy is only used to partition the training data and not used any further in the training. The simplicity makes these meth-ods easy to scale, but also makes them limited in effectively using the hierarchical dependencies.

Several approaches have been proposed for making better use of the hierarchical structure. In [5], a cascading strat-egy is employed to add the output of lower-level classifiers as additional features for higher-level classifiers. In [8], a Bayesian aggregation on the results of the individual binary http://www.dmoz.org classifiers was proposed. In [34], a data-driven pruning strat-egy is proposed for reducing the size of the original hierarchy. Some improvements over the results of the pachinko-machine models have been reported; however, those approaches are heuristic by nature.

The more principled methods include the large-margin models by [31],[6],[26],[9],[7] where the discriminant func-tions take the contributions from all nodes along the path to the root, and the model parameters are jointly learned to minimize a global loss over the hierarchy. Similar ideas have been explored in [40] where orthogonality conditions are imposed between the parent and children classifiers, in [27] [16] with Bayesian multinomial logistic models, and in [24] using Naive Bayes classifiers with hierarchical shrink-age. Empirical improvements of most of these methods over simpler approaches have been shown on small datasets, typi-cally with hundreds (or less) of class-labels and thousands of instances. The difficulty for most of these methods in scaling is due to the high-degree of inter-dependencies among model parameters and the parameters for all the classes cannot be held in memory at the same time.

What we want is an approach that is both principled and at the same time computationally tractable for prob-lems with a very large number of classes, high-dimensional features and large volume of data. Further, we want the approach to be generally applicable to both large-margin classifiers (like Support Vector Machines) as well as prob-abilistic classifiers (like logistic regression), and finally, to be flexible for leveraging both hierarchical dependencies as well as graph-based dependencies among class labels. We propose such an approach in this paper, namely the recur-sive regularization framework for Support Vector Machines (SVM) and logistic regression (LR). It uses the dependencies among classes and sub-classes to define a joint objective for regularization of model parameters; the model parameters of the siblings nodes who share the same parent are regu-larized towards the common parent node. Intuitively, it is based on the assumption that the nearby classes in the hier-archy are semantically close to each other and hence share similar model parameters. Notice that this model is simpler than the fully Bayesian models in [27] and [16] where the de-pendencies are modeled in richer forms, controlling both the means and covariances in Gaussain models. The simplicity of the proposed approach makes it easier to scale than the fully Bayesian approaches.

For the scalability of our method, we develop a paral-lel and iterative co-ordinate descent scheme that can easily tackle the large datasets with millions of instances and hun-dreds of thousands of classes. The key idea here is to for-mulate the co-ordinate descent objective function in a way that the dependencies between the various parameters can be easily localized, and the computations for all the local regions can be carried out in parallel. More specifically, we formulate the objective such that the parameters at each node is independent of the rest of the hierarchy given the parameters of its parent and children; therefore by fixing the parameters of the parent and children, the node can be optimized independently from the rest of the hierarchy. This aids in achieving a large degree of parallelization and gets a speedup close to linear in the number of processors used. Moreover, the local computations at leaf nodes are dualized in order side-step non-differentiability issues when using loss functions such as Hinge loss. As we shall see in section 4, this combination of using iterative parallelization of local computations and fast dual co-ordinate descent methods for each local computation leads to optimization schemes that can easily scale to real life web-scale data.

We tested our proposed approaches in terms effectiveness as well efficiency by conducting evaluations against other state-of-the-art methods on nine benchmark datasets for LSHC problems, including the large-scale datasets from the Large-scale Hierarchical Text Classification Challenge With respect to effectiveness, we found that our methods outperformed all the other methods on most tested datasets. With respect to efficiency, we show for the first time that global hierarchical optimization (with our proposed meth-ods) can be efficiently computed for the largest datasets such as wikipedia with 600,000 classes and 2 Million training in-stances in a matter of 37 hours.

To summarize, the contribution of our work is multifold, 1. We propose a regularization framework that goes be-2. We propose a fast iterative co-ordinate descent scheme 3. Our proposed schemes achieve state-of-the-art results Indirectly related to our paper are a few works in multitask learning [12], [2],[3], [32] where regularization was used as a tool to share information between tasks. However, their fo-cus is not scalability and their techniques cannot be directly applied to large scale HC. Other works include regulariza-tion on graphs such as [39], [29], but the focus is on graphical dependencies between the instances and not between class-labels.
Let the hierarchy be a tree defined over a set of nodes N by the parent child relationships given by  X  : N  X  X  where  X  ( n ) is the parent of node n . Let D = { x i ,t the training dataset of M instances where each x i  X  X  and t  X  T is a label, T  X  N is the set of all leaf nodes in the hierarchy. We assume that each instance is labeled to one or more leaf nodes in the hierarchy. If there are any instances assigned to an internal node, spawn a leaf-node under it and re-assign all the instances from the internal node to this new leaf node. For convenience, let C n denote the set of all children of node n , and binary variable y in  X  { +1 ,  X  1 } denote if x i belongs to class n  X  T i.e. y in = (2 I ( t n )  X  1). The problem of HC is to learn a prediction function f : X  X  T that predicts the target class-label of a given input instance with smallest possible error. More over, the hierarchical dependencies between the classes are encoded in the form of a hierarchy used in the learning process.
Following the principle of statistical decision theory the risk (or error) of a prediction function f is defined as the http://lshtc.iit.demokritos.gr/ expected value of a loss function over all possible inputs. The Structural Risk Minimization framework prescribes choosing f to minimize a combination of the Empirical Risk based on the training dataset and a regularization term to penalize the complexity of f . Typically the prediction function f is parameterized by an unknown set of parameters w which are then estimated in the learning process. The estimated parameters  X  w is given by where R emp denotes the Empirical Risk or Loss on the train-ing dataset,  X  ( w ) denotes the regularization term and C is a parameter that controls the trade-off between fitting to the given training instances and the complexity of f .

In the problem of HC, the prediction function is parame-terized by a set of parameters W = { w n : n  X  N} i.e each node n in the hierarchy is associated with a parameter vec-tor w n . First, we define the Empirical Risk in our model as the loss incurred by the instances at the leaf-nodes of the hierarchy where L could, in principle, be any convex loss function. Sec-ond, we propose to use the hierarchy in the learning process by incorporating a recursive structure into the regularization term for W . Specifically, we propose the following form of regularization This recursive form of regularization enforces the parame-ters of the node to be similar to the parameters of its par-ent under euclidean norm. Intuitively, it models the hier-archical dependencies in the sense that it encourages pa-rameters which are nearby in the hierarchy to be similar to each other. This helps classes to leverage information from nearby classes while estimating model parameters and helps share statistical strength across the hierarchy. We hope that this would especially enable classes with very few training instances to pool in information, as well as gain information from classes with a larger number of training examples, to yield better classification models despite the limited training examples.

We explore two choices for the loss function L and de-fine two different variants of our approach -Hierarchically Regularized Support Vector Machines (HR-SVM) using the hinge-loss function and Hierarchically Regularized Logistic Regression (HR-LR) using the logistic loss function,
HR-SVM min HR-LR min
The key advantage of HR-{ SVM,LR } over other hierar-chical models such as [31], [6], [4], [40] is that there are no constraints that maximizes the margin between correct and incorrect predictions. This keeps the dependencies between the parameters minimal and in turn enables us to develop a Input : D , C ,  X  ,T, N
Result : weight vectors W  X  while Not Converged do end
Algorithm 1 : Optimization of HR-SVM and HR-LR parallel-iterative method to optimize the objective (details in 3.3) thereby scaling to very large HC problems.
Although the objective function in (2) is convex and has a unique maximum, it is not differentiable and straight-forward methods such as gradient descent cannot be used. To address the non-differentiability, many works have gener-ally resorted to subgradient techniques such as [40], [28], [1]. However, the general problem with subgradient approaches is that the learning rate of the optimization routine needs to be specified [17]. In our initial experiments using sub-gradient approaches, we found that the optimization was highly sensitive to the learning rate used and tweaking this parameter for each dataset was a difficult task involving sev-eral trials and adhoc heuristics. In order to overcome such issues, we resorted to an iterative approach where we up-date the parameters associated with each node n iteratively by fixing the rest of the parameters. To tackle the non-differentiability in some of the updates (i.e. the updates at the leaf-nodes), we converted these sub-problems into their dual form which is differentiable and optimized it using co-ordinate descent. This iterative scheme offers several advan-tages compared to standard approaches, 1. There are no learning rates that need to be tweaked. 2. The non-differentiability of the objective at the leaf 3. It can easily incorporate closed form updates for the
For each non-leaf node n /  X  T , differentiating eq (2) w.r.t w n yields a closed-form update for w n given by, For each leaf node n  X  T , the objective cannot be differen-tiated due to a discontinuous Hinge-loss function. Isolating the terms that depend on w n and introducing slack variables  X  , the primal objective of the subproblem for w n is given by, The dual of the above subproblem by introducing appropri-ate dual variables  X  i , i = 1 ..M is min To solve this subproblem, one can easily use any second or-der methods such as interior-point methods etc. The down-side of such solvers is that it takes a long time even for a single iteration and requires the entire kernel matrix of size O ( M 2 ) to be stored in memory. Typical large-scale HC problems have at least hundreds of thousands of instances and the memory required to store the kernel matrix is in the order of 100 GB for each class, thereby rendering it impractical. Instead we propose a co-ordinate descent ap-proach which has minimal memory requirements and con-verges quickly even for large problems. Our work is based on the dual co-ordinate descent developed in [17].

The core idea in co-ordinate descent is to iteratively up-date each dual variable. In the objective function eq (5), the update for each dual variable has a simple closed form solution. To derive the update for the i th dual variable  X  given by  X  i + d , we solve the following one-variable problem, Basically, we substituted  X  i by  X  i + d in eq (5) and discarded all the terms that do not depend on d . This one-variable problem can be solved in closed form by considering the gradient of the objective and appropriately updating  X  i to obey the constraints. The gradient G for the above objective and the corresponding update for  X  i is given by, where a 0 = and updated throughout the optimization of the subprob-lem. The time complexity for each  X  i update is O(#nnz in x ) -the number of non-zero dimensions in x i and the mem-ory requirement for solving the entire subproblem is O ( M ) -far more efficient than that O ( M 2 ) compared to the second order methods. Finally, after solving the dual subproblem, the update for w n in the primal form is given by the K.K.T conditions for the subproblem, The pseudocode for the entire optimization routine is shown in Algorithm 1. Note that the convergence of the above op-timization method can be derived by viewing the procedure as a block co-ordinate descent scheme on a convex function where the blocks corresponds to parameters at each node of the hierarchy [23], [30].

Although co-ordinate descent methods are easy to imple-ment, have low memory requirements and reach an accept-able solution quickly, they have their own pitfalls. If the data is highly correlated or dense in the dimensions, they take a much longer time to converge [25] . Even in simpler cases, identifying the rate of convergence is hard and the stopping criteria might not be accurate [23]. However, in the case of HC text classification, the documents are sparse and there is not much correlation between the dimensions. Moreover, most of the classes are linearly separable and finding a deci-sion boundary is an  X  X asy X  problem -this makes co-ordinate descent a natural choice for optimization in the context of text classification [13], [38]. We follow a similar iterative strategy for optimizing HR-LR, i.e. we update the parameter w n of each node n it-eratively by fixing the rest of the parameters. Unlike HR-SVM, the objective function in HR-LR is convex and differ-entiable and therefore second order methods such exact new-ton X  X  methods as well as quasi-newton methods are applica-ble. Typically, exact newton methods require the computa-tion of the inverse of the Hessian of the objective function. For large-scale problems with high dimensions, it might not even be feasible to store the Hessian in memory. There-fore, we resort to a limited memory variant of a quasi new-ton method -Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) [21].
 The update for each non-leaf node is exactly the same as in HR-SVM eq (4). For each leaf node n , isolating the terms that depend on w n , the objective and the corresponding gradient G can be written as min Since the gradient can be computed in closed-form it is pos-sible to directly apply quasi newton methods such as LBFGS to solve the above optimization problem. The pseudocode for the optimization routine is shown in Algorithm 1
For large hierarchies, it might be impractical to learn the parameters of all classes, or even store them in memory, on a single machine. We therefore, devise a parallelization scheme for our optimization algorithm. The key idea is to note that the interactions between the different w n  X  X  are only through the parent and child nodes. By fixing the parame-ters of the parent and children for a node n , the parameter w n associated with node n can be optimized independently of the rest of the parameters. Following our previous work [16], we iteratively optimize the odd and even levels in the hierarchy -if we fix the parameters at the odd levels, the parameters of parents and the children of all nodes at even levels are fixed, and the w n  X  X  at all even levels can be opti-mized in parallel. The same goes for optimizing the odd level parameters. To aid in convergence, we also used other tricks such as warm-starting with the previously found solution and random permutation of subproblems in co-ordinate de-scent method [17]. For large hierarchies, this method yields a speedup almost linear in the number of processors. Note that for HR-SVM, since only some of the dual variables (  X   X  X ) are non-zero, we can reduce the storage requirements by rep-resenting w n by the corresponding non-zero dual variables instead of a full vector.

We tested our parallelization framework on a cluster run-ning map-reduce based Hadoop 20.2 with 64 worker nodes having 8 cores and 16GB RAM each. Around 300 cores were used as Mappers and 220 cores were used as Reducers.
In several real life scenarios, the dependencies between the class-labels are given in the form of a graph rather than a hierarchy. For instance, in Wikipedia where thousands of people edit pages at the same time, it is common to see editors quickly linking a topic of interest with other related topics. To enable our approach to be applicable in such scenarios, we extend our method to be able to handle graph based dependencies.

The regularization paradigm in the case of graphs can be stated as follows -the parameters of each node in the graph is regularized towards each of its neighbours (instead of its parent and children). Specifically, given a graph with a set of edges E = { ( i,j ) : i,j  X  N} , the regularization term is given by , The optimization algorithms for HR-SVM and HR-LR can be derived similarly. In the interest of space, we just present a brief outline. For those nodes which do not have any training instances, the update is given by, where S n denotes the number of neighbours of node n . For the nodes which have associated training instances; in HR-SVM, the optimization objective at node n is given by To derive the dual, we re-write the above objective using the mean of the neighbouring parameters. Define the mean It can be easily shown that the optimal solution for objec-tive (11) is the same as the optimal solution for (10) by expanding the regularization term and discarding constant terms in the optimization. Now (11) can be solved using the same co-ordinate descent technique as described earlier.
For HR-LR, we have a similar optimization of the sub-problem, the only difference is that the gradient in eq (9) would now have a summation over all the neighbours instead its parent and children.

Parallelization: The parallelization scheme for hierar-chies cannot be straight-forwardly extended to graphs as there is no notion of odd or even levels. The best possi-ble parallelization on graphs involves finding the chromatic number of the graph -which is the smallest K such that each node of the graph is assigned one of K different col-ors from 1 to K and no two adjacent nodes have the same color. Once the nodes have been assigned colors, we can iterate over the K different colors and parallely optimize the parameters of the nodes which have been assigned that color. However, finding the chromatic number of the graph is a NP-complete problem, therefore, we can only resort to approximate schemes to find the chromatic number. The degree of parallelization in graphs is given by |N| K which is in sharp contrast to |N| 2 for hierarchies. Alternatively, one could also resort to other schemes such as performing mul-tiple iterations of optimizing all nodes in parallel (although convergence is not guaranteed in theory). A complete dis-cussion of the various approximate parallelization schemes on graphs is however beyond the scope of this paper. For re-lated issues, refer [14] which discuss parallelizing belief prop-agation on graphs.
We used several large-scale benchmark datasets whose statistics are listed in Table 1. To maintain comparability with previously published evaluations, we used the conven-tional train-test splits where-ever available.
Note that RCV1, DMOZ-2011, SWIKI-2011, LWIKI are multi-label datasets, meaning that an instance may have multiple correct labels; the other datasets only have one correct label per instance.
We include three categories of methods for comparison:
We implemented all the above methods, and tuned the regularization parameter using cross-validation with a range of values from 10  X  3 to 10 3 . On the multi-label datasets, to make the baselines as competitive as possible, we used an instance-based thresholding strategy as proposed in [15]. This provided a better performance than using the default threshold of zero as well as other thresholding methods like rcut or scut [35]. Note that HSVM, OT and HBLR are in-herently multiclass methods and are not applicable in mul-tilabeled scenarios.
 To scale up to the larger datasets (e.g., LWIKI), for the HR models we used the approximate parallelization as dis-cussed in section 3.3. The parallelization for the flat base-lines (SVM and LR) is straightforward, i.e., simply learn-ing the models for all the class-labels in parallel. Among http://lshtc.iit.demokritos.gr/node/3 the hierarchical baselines, TD can be easily parallelized as the class models can be trained independently; HSVM and OT cannot be parallelized and hence cannot be scaled to the larger datasets. Therefore, we only report the results of HSVM and OT on the smaller datasets (CLEF and LSHTC-small) where they scaled. In addition, we also include the best results in benchmark evaluations for comparison, ac-cording to the numbers available on the LSHTC website.
We use the following standard evaluation metrics [37] to measure the performance of all the methods.
We present three sets of results. The first set of results compares the performance of our proposed HR-models with the best results on the datasets in the benchmark evaluations conducted by LSHTC 4 . The second set of results presents pairwise comparison for SVM/HR-SVM and LR/HR-LR, re-spectively, to examine the same classifiers with and without recursive regularization. The third set of the results focus on the efficiency in computational time.
Table 2 compares the results of our proposed HR-models (HR-SVM and HR-LR) with some of the well established re-sults on the large-scale datasets released by the LSHTC com-munity. We focus on only those datasets for which bench-mark evaluations were available on the website 4 . Table 2 shows that the HR-models are able to perform better than the state-of-the-art results reported so far on most of these datasets. In fact, on four out of the five datasets, HR-SVM shows a consistent 10% relative improvement than the cur-rently published results.

Table 3 summarizes the results of pairwise comparison be-tween HR models against the corresponding non-HR base-lines i.e. HR-SVM against SVM and HR-LR against LR. For an informative comparison, we also include the results http://lshtc.iit.demokritos.gr/lshtc2 evaluation, excluding our own own submissions to these evaluations. of the other hierarchical baselines TD, HSVM and OT as well as the results from our work [16] where ever applica-ble. Note that all our baseline implementations have been thoroughly tested and all convergence parameters have been appropriately set to optimize the objective as much as pos-sible. Regularization parameters have been appropriately tuned using cross validation and have NOT been arbitrarily In fact we found that setting the regularization parameter to  X 1 X  was suboptimal and lowered the performance of the baselines.

The results in Table 3 shows that the HR models consis-tently outperforms the non-HR counterparts on all tested datasets. They are able to successfully leverage the hierar-chical dependencies and further push (especially in Macro-F ) beyond the performance of the baseline methods.

To further validate our results, we conducted pairwise sig-nificance tests between SVM and HR-SVM; LR and HR-LR, on the CLEF, IPC, LSHTC-small and RCV1 datasets. We used the sign test for Micro-F 1 and wilcoxon rank test for Macro-F 1 . We are unable to conduct significance tests on the other datasets since we did not have access to class-wise performance scores and true-test labels -our reported eval-uation measures on these datasets are from the output of an online evaluation system 5 which does not reveal classwise performance measures nor true test labels. The results of the significance tests (Table 3) on the four datasets show that the HR-models significantly outperform the non-HR models on three out of the four tested datasets.

Comparing the performance of the HR-models to the other hierarchical baselines (TD, HSVM and OT), we see that the former outperforms the latter on most of the datasets. The unusually low performance of OT seems contrary to the results published in [40], we believe the reason is be-cause the regularization parameter was arbitrarily set to  X 1 X  without using cross-validation. In some of the datasets like LSHTC-small, the HR-models show a significant gain in performance, more than 16% relative improvement in both Macro and Micro measures. http://lshtc.iit.demokritos.gr/LSHTC2 oracleUpload
Table 4 reports the training time taken for all the HR models and the flat baselines. The results on the CLEF, RCV1, IPC and LSHTC-small dataset are from a 32 core Intel Xeon X7560 @ 2.27GHz, 32GB RAM. For all the other datasets, we used the Hadoop cluster as described in section 3.3.

Comparing the training time of the HR-models with cor-responding the non-HR counterparts, HR-SVM is on an av-erage about 1.92 slower than SVM and HR-LR about 2.87 slower than LR. This is not surprising -the better perfor-mance of the HR models comes at the cost of increased computational time. However, even on the largest dataset LWIKI, SVM takes about 18 hours while the HR-SVM about 37 hours. Although the HR-models which offer better per-formance are slower, the computation time certainly falls within the tractable range even for the largest datasets.
Comparing the HR-models against other hierarchical base-lines; TD is the only one among the baseline methods that scaled to all the data sets but has low performance. HSVM and OT could only scale to the smallest data sets (CLEF and LSHTC-small) and both of them are about 3.5x slower on CLEF and about 86x slower on LSHTC-small compared to HR-SVM. On the rest of the datasets, neither of these could even be trained successfully. HBLR, even with a su-perior performance, is on an average 7.3x slower than HR-SVM. None of the hierarchical baselines are applicable to graphical dependencies between clases (SWIKI and LWIKI dataset).
In this paper, we proposed a recursive regularization frame-work along with scalable optimization algorithms for large-scale classification with hierarchical and graphical depen-dencies between class-labels. We explored 2 different vari-ants of our framework using the logistic loss function and the hinge-loss function. Our proposed approaches achieved state-of-the-art results on multiple benchmark datasets and showed a consistent improvement in performance over flat and hierarchical approaches. were unavailable on the other datasets (refer section 5)
This work is supported, in part, by the National Sci-ence Foundation (NSF) under grant IIS 1216282. We thank Alexandru Niculescu-Mizil for useful discussions about model design and parallelization, Guy Blelloch for sharing his com-putational resources, and the Open Cloud cluster for the Hadoop framework . The Open Cloud cluster is supported, in part, by NSF, under award CCF 1019104, and the Gor-don and Betty Moore Foundation, in the eScience project. [1] http://leon.bottou.org/projects/sgd. [2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex [3] A. Argyriou, C.A. Micchelli, M. Pontil, and Y. Ying. [4] S. Bengio, J. Weston, and D. Grangier. Label [5] P.N. Bennett and N. Nguyen. Refined experts: [6] L. Cai and T. Hofmann. Hierarchical document [7] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni.
 [8] C. DeCoro, Z. Barutcuoglu, and R. Fiebrink. Bayesian [9] O. Dekel, J. Keshet, and Y. Singer. Large margin [10] I. Dimitrovski, D. Kocev, L. Suzana, and S. D X zeroski. [11] S. Dumais and H. Chen. Hierarchical classification of [12] T. Evgeniou and M. Pontil. Regularized multi X  X ask [13] A. Genkin, D.D. Lewis, and D. Madigan. Large-scale [14] J. Gonzalez, Y. Low, and C. Guestrin. Residual splash [15] S. Gopal and Y. Yang. Multilabel classification with [16] Siddharth Gopal, Yiming Yang, Bing Bai, and [17] C.J. Hsieh, K.W. Chang, C.J. Lin, S.S. Keerthi, and [18] D. Koller and M. Sahami. Hierarchically classifying [19] D.D. Lewis, R.E. Schapire, J.P. Callan, and R. Papka. [20] D.D. Lewis, Y. Yang, T.G. Rose, and F. Li. Rcv1: A [21] D.C. Liu and J. Nocedal. On the limited memory bfgs [22] T.Y. Liu, Y. Yang, H. Wan, H.J. Zeng, Z. Chen, and [23] Z.Q. Luo and P. Tseng. On the convergence of the [24] A. McCallum, R. Rosenfeld, T. Mitchell, and A.Y. Ng. [25] T.P. Minka. A comparison of numerical optimizers for [26] J. Rousu, C. Saunders, S. Szedmak, and [27] B. Shahbaba and R.M. Neal. Improving classification [28] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [29] A. Smola and R. Kondor. Kernels and regularization [30] P. Tseng. Convergence of a block coordinate descent [31] I. Tsochantaridis, T. Joachims, T. Hofmann, and [32] C. Widmer, J. Leiva, Y. Altun, and G. R  X  atsch. [33] IPC WIPO. http://www.wipo.int/classifications [34] G.R. Xue, D. Xing, Q. Yang, and Y. Yu. Deep [35] Y. Yang. A study of thresholding strategies for text [36] Y. Yang, J. Zhang, and B. Kisiel. A scalability [37] Yiming Yang. An evaluation of statistical approaches [38] T. Zhang and F.J. Oles. Text categorization based on [39] T. Zhang, A. Popescul, and B. Dom. Linear prediction [40] D. Zhou, L. Xiao, and M. Wu. Hierarchical
