 Visual categorization is a difficult task in large part due to the large variation seen between images belonging to the same class. Within one semantic class, ther e can be a large differences in shape, color, and texture, and objects can be scaled or translated w ithin an image. For some rigid-body objects, appearance changes greatly with viewing angle, an d for articulated objects, such as ani-mals, the number of possible configurations can grow exponen tially with the degrees of freedom. Furthermore, there is a large number of categories in the wor ld between which humans are able to distinguish. One oft-cited, conservative estimate puts th e total at about 30,000 categories [1], and this does not consider the identification problem (e.g. tell ing faces apart).
 One of the more successful tools used in visual classificatio n is a class of patch-based shape and texture features that are invariant or robust to changes in s cale, translation, and affine deformations. These include the Gaussian-derivative jet descriptors of [ 2], SIFT descriptors [3], shape contexts [4], and geometric blur [5]. The basic outline of most discrimina tive approaches which use these types of features is as follows: (1) given a training image, select a s ubset of locations or  X  X nterest points X , (2) for each location, select a patch surrounding it, often elli ptical or rectangular in shape, (3) compute a fixed-length feature vector from each patch, usually a summ ary of edge responses or image gradi-ents. This gives a set of fixed-length feature vectors for each training image. (4) Define a function which, given the two sets from two images, returns a value for the distance (or similarity) between the images. Then, (5) use distances between pairs of images a s input to a learning algorithm, for example an SVM or nearest neighbor classifier. When given a tes t image, patches and features are extracted, distances between the test image and training im ages are computed, and a classification is made.
 Figure 1: These exemplars are all drawn from the cougar face category of the Caltech 101 dataset, but we can see a great deal of variation. The image on the left is a clear, color image of a cougar face. As with most cougar face exemplars, the locations and appearances of the eyes and ears are a strong signal for class membership, as well as t he color pattern of the face. Now consider the grayscale center image, where the appearance o f the eyes has changed, the ears are no longer visible, and hue is useless. For this image, the marki ngs around the mouth and the texture of the fur become a better signal. The image on the right shows th e ears, eyes, and mouth, but due to articulation, the appearance of all have changed again, per haps representing a common visual sub-category. If we were to limit ourselves to learning one model of relative importance across these features for all images, or even for each category, it could r educe our ability to determine similarity to these exemplars.
 In most approaches, machine learning only comes to play in st ep (5), after the distances or simi-larities between training images are computed. In this work , we learn the function in step (4) from the training data. This is similar in spirit to the recent bod y of metric learning work in the ma-chine learning community [6][7][8][9][10]. While these met hods have been successfully applied to recognizing digits, there are a couple drawbacks in applyin g these methods to the general image classification problem. First, they would require represen ting each image as a fixed-length feature vector. We prefer to use sets of patch-based features, consi dering both the strong empirical evidence in their favor and the difficulties in capturing invariances in fixed-length feature vectors. Second, these metric-learning algorithms learn one deformation fo r the entire space of exemplars. To gain an intuition as to why this is a problem, consider Figure 1.
 The goal of this paper is to demonstrate that in the setting of visual categorization, it can be useful to determine the relative importance of visual features on a finer scale. In this work, we attack the problem from the other extreme, choosing to learn a distance function for each exemplar , where each function gives a distance value between its training im age, or focal image , and any other image. These functions can be learned from either multi-way class l abels or relative similarity information in the training data. The distance functions are built on top of elementary distance measures between patch-based features, and our problem is formulated such th at we are learning a weighting over the features in each of our training images. This approach has tw o nice properties: (1) the output of the learning is a quantitative measure of the relative importan ce of the parts of an image; and (2) the framework allows us to naturally combine and select feature s of different types.
 We learn the weights using a generalization of the constrain ed optimization formulation proposed by Schultz and Joachims [7] for relative comparison data. Us ing these local distance functions, we address applications in image browsing, retrieval and clas sification. In order to perform retrieval and classification, we use an additional learning step that a llows us to compare focal images to one another, and an inference procedure based on error-correct ing output codes to make a class choice. We show classification results on the Caltech 101 object reco gnition benchmark, that for some time has been a de facto standard for multi-category classification. Our mean recog nition rate on this benchmark is 60.3% using only fifteen exemplar images per cat egory, which is an improvement over the best previously published recognition rate in [11] . In this section we will describe the distance functions and t he learning procedure in terms of abstract patch-based image features. Any patch-based features coul d be used with the framework we present, and we will wait to address our choice of features in Section 3 .
 If we have N training images, we will be solving N separate learning problems. The training image for which a given learning problem is being solved will be ref erred to as its focal image . Each problem is trained with a subset of the remaining training im ages, which we will refer to as the learning set for that problem. In the rest of this section we will discuss o ne such learning problem and focal image, but keep in mind that in the full framework th ere are N of these.
 We define the distance function we are learning to be a combina tion of elementary patch-based distances , each of which are computed between a single patch-based fea ture in the focal image F and a set of features in a candidate image I , essentially giving us a patch-to-image distance. Any function between a patch feature and a set of features could b e used to compute these elementary distances; we will discuss our choice in Section 3. If there a re M patches in the focal image, we have M patch-to-image distances to compute between F and I , and we notate each distance in that set as d F distance function D that we learn is a linear combination of these elementary dis tances. Where w F is a vector of weights with a weight corresponding to each pat ch feature: Our goal is to learn this weighting over the features in the fo cal image. We set up our algorithm to learn from  X  X riplets X  of images, each composed of (1) the foc al image F , (2) an image labeled  X  X ess similar X  to F , and (3) an image labeled  X  X ore similar X  to F . This formulation has been used in other work for its flexibility [7]; it makes it possible to use a relative ranking over images as training input, but also works naturally with multi-class labels by c onsidering exemplars of the same class as F to be  X  X ore similar X  than those of another class.
 To set up the learning algorithm, we consider one such triple t: ( F , I d , I s ) , where I d and I s refer to the dissimilar and similar images, respectively. If we co uld use our learned distance function for F to rank these two images relative to one another, we ideally w ould want I d to have a larger value to w F d F ( I d ) &gt; w F d F ( I s ) . Let x elementary distance vectors for this triplet, now indexed b y i . Now we can write the condition as w F x i &gt; 0 .
 For a given focal image, we will construct T of these triplets from our training data (we will discuss how we choose triplets in Section 5.1). Since we will not be ab le to find one set of weights that meets this condition for all triplets, we use a maximal-marg in formulation where we allow slack for triplets that do not meet the condition and try to minimize th e total amount of slack allowed. We also increase the desired margin from zero to one, and constr ain w F to have non-negative elements, which we denote using . 1 . We chose the L desirable, and an L between the two within this framework.
 This optimization is a generalization of that proposed by Sc hultz and Joachims in [7] for distance metric learning. However, our setting is different from the irs in two ways. First, their triplets do not share the same focal image as they apply their method to le arning one metric for all classes and instances. Second, they arrive at their formulation by assu ming that (1) each exemplar is represented by a single fixed-length vector, and (2) a L 2 appear to preclude our use of patch features and more interes ting distance measures, but as we show, this is an unnecessary restriction for the optimization. Th us, a contribution of this paper is to show that the algorithm in [7] is more widely applicable than orig inally presented.
 We used a custom solver to find w F , which runs on the order of one to two seconds for about 2,000 triplets. While it closely resembles the form for support vec tor machines, it differs in two important ways: (1) we have a primal positivity constraint on w F , and (2) we do not have a bias term because we are using the relative relationship between our data vect ors. The missing bias term means that, in the dual optimization problem, we do not have a constraint that ties together the dual variables for the margin constraints. Instead, they can be updated sep arately using an approach similar to the row action method described in [12], followed by a projectio n of the new w F to make it positive. Denoting the dual variables for the margin constraints by  X  cycle through the triplets, performing these two steps for t he i th triplet: where the first max is element-wise, and the min and max in the s econd line forces 0  X   X  stop iterating when all KKT conditions are met, within some p recision. The framework described above allows us to naturally combin e different kinds of patch-based fea-tures, and we will make use of shape features at two different scales and a rudimentary color feature. Many papers have shown the benefits of using filter-based patc h features such as SIFT [3] and geo-metric blur [13] for shape-or texture-based object matching and recogn ition [14][15][13]. We chose to use geometric blur descriptors, which were used by Zhang e t al. in [11] in combination with their KNN-SVM method to give the best previously published result s on the Caltech 101 image recogni-tion benchmark. Like SIFT, geometric blur features summari ze oriented edges within a patch of the image, but are designed to be more robust to affine transforma tion and differences in the periphery of the patch. In previous work using geometric blur descript ors on the Caltech 101 dataset [13][11], the patches used are centered at 400 or fewer edge points samp led from the image, and features are computed on patches of a fixed scale and orientation. We follo w this methodology as well, though one could use an interest point operator to determine locati on, scale, and orientation from low-level information, as is typically done with SIFT features. We use two different scales of geometric blur features, the same used in separate experiments in [11]. The larger has a patch radius of 70 pixels, and the smaller a patch radius of 42 pixels. Both use four orie nted channels and 51 sample points, for a total of 204 dimensions. As is done in [13], we default to normalizing the feature vector so that the L Our color features are histograms of eight-pixel radius pat ches also centered at edge pixels in the image. Any  X  X ixels X  in a patch off the edge of the image are cou nted in a  X  X ndefined X  bin, and we convert the HSV coordinates of the remaining points to a Ca rtesian space where the z direction ( x, y ) space into an 11  X  11 grid, and make three divisions in the z direction. These were the only parameters that we tested with the color features, choosing not to tune the features to the Caltech 101 dataset. We normalize the bins by the total number of pixe ls in the patch.
 Using these features, we can compute elementary patch-to-i mage distances. If we are computing the distance between the j th patch in the focal image to a candidate image I , we find the closest feature of the same type in I using the L image distance. We only compare features of the same type, so large geometric blur features are not compared to small geometric blur features. In our experimen ts we have not made use of geometric relationships between features, but this could be incorpor ated in a manner similar to that in [11] or [16]. The learned distance functions induce rankings that could n aturally be the basis for a browsing application over a closed set of images. Consider a ranking o f images with respect to one focal image, as in Figure 2. The user may see this and decide they wan t more sunflower images. Clicking on the sixth image shown would then take them to the ranking wi th that sunflower image as the focal image, which contains more sunflower results. In essence, we can allow a user to navigate  X  X mage We also can make use of these distance functions to perform im age retrieval: given a new image Q , return a listing of the N training images (or the top K ) in order of similarity to Q . If given class labels, we would want images ranked high to be in the same clas s as Q . While we can use the N distance functions to compute the distance from each of the f ocal images F are not directly comparable. This is because (1) the weight v ectors for each of the focal vectors are not constrained to share any properties other than non-n egativity, (2) the number of elementary distance measures and their potential ranges are different for each focal image, and (3) some learned distance functions are simply better than others at charact erizing similarity within their class. To address this in cases where we have multi-class labels, we do a second round of training for each labels and learned distances. Now, given a query image Q , we can compute a probability that the query is in the same class as each of the focal (training) imag es, and we can use these probabilities to rank the training images relative to one another. The prob abilities are on the same scale, and the To classify a query image, we first run the retrieval method ab ove to get the probabilities for each training image. For each class, we sum the probabilities for all training images from that class, and the query is assigned to the class with the largest total. For mally, if p training image I be shown to be a relaxation of the Hamming decoding scheme for the error-correcting output codes in [17] in which the number of focal images is the same for each class. able object recognition, it has up to this point been one of th e de facto standard benchmarks for multi-class image categorization/object recognition. Th e dataset contains images from 101 different categories, with the number of images per category ranging f rom 31 to 800, with a median of about 50 images. We ignore the background class and work in a forced -choice scenario with the 101 object categories, where a query image must be assigned to one of the 101 categories.
 We use the same testing methodology and mean recognition rep orting described in Grauman et al. [15]: we use varying numbers of training set sizes (given in number of examples per class), BACKGROUND Google class. Recognition rate per class is computed, then average d across classes. This normalizes the overall recognition rate so that the per formance for categories with a larger num-ber of test images does not skew the mean recognition rate. 5.1 Training data The images are first resized to speed feature computation. Th e aspect ratio is maintained, but all images are scaled down to be around 200  X  300 . We computed features for each of these images as described in Section 3. We used up to 400 of each type of featur e (two sizes of geometric blur and one color), for a maximum total of 1,200 features per image. F or images with few edge points, we computed fewer features so that the features were not overly redundant. After computing elementary distances, we rescale the distances for each focal image and feature to have a standard deviation of 0.1.
 For each focal image we choose a set of triplets for training, and since we are learning similarity for the purposes image classification, we use the category la bels on the images in the training set: images that have the same label as the focal image are conside red  X  X ore similar X  than all images that are out of class. Note that the training algorithm allow s for a more nuanced training set where an image could be more similar with respect to one image and less similar with respect to another, but Figure 2: The first 15 images from a ranking induced for the foc al image in the upper-left corner, trained with 15 images/category. Each image is shown with it s raw distance distance, and only those marked with (pos) or (neg) were in the learning set for this fo cal image. Full rankings for all experi-mental runs can be browsed at http://www.cs.berkeley.edu/  X  afrome/caltech101/ nips2006 . we are not fully exploiting that in these experiments. Inste ad of using the full pairwise combination of all in-and out-of-class images, we select triplets using elementary feature distances. Thus, we refer to all the images available for training as the training set and the set of images used to train with respect to a given focal image as its learning set . We want in our learning set those images that are similar to the focal image according to at least one eleme ntary distance measure. For each of the M elementary patch distance measures, we find the top K closest images. If that group contains both in-and out-of-class images, then we make triplets out o f the full bipartite match. If all K images are in-class, then we find the closest out-of-class im age according to that distance measure and make K triplets with one out-of-class image and the K similar images. We do the converse if all K images are out of class. In our experiments, we used K = 5 , and we have not yet performed experiments to determine the effect of the choice of K . The final set of triplets for F is the union of the triplets chosen by the M measures. On average, we used 2,210 triplets per focal image , and mean training time was 1-2 seconds (not including the time to compute the features, elementary distances, or choose the triplets). While we have to solve N of these learning problems, each can be run completely independently, so that for a training set o f 1,515 images, we can complete this optimization on a cluster of 50 1GHz computers in about one mi nute. 5.2 Results We ran a series of experiments using all features, each with a different number of training images per category (either 5, 15, or 30), where we generated 10 indepen dent random splits of the 8,677 images from the 101 categories into training and test sets. We repor t the average of the mean recognition rates across these splits as well as the standard deviations . We determined the C parameter of the training algorithm using leave-one-out cross-validation on a small random subset of 15 images per category, and our final results are reported using the best va lue of C found (0.1). In general, however, the method was robust to the choice of C , with only changes of about 1% in recognition with an order of magnitude change in C near the maximum. Figure 3 graphs these results with most of t he published results for the Caltech 101 dataset.
 In the 15 training images per category setting, we also perfo rmed recognition experiments on each of our features separately, the combination of the two shape features, and the combination of two shape features with the color features, for a total of five dif ferent feature combinations. We per-formed another round of cross-validation to determine the C value for each feature combination 6 . Recognition in the color-only experiment was the poorest at 6% (0.8% standard deviation) 7 The next best performance was from the bigger geometric blur fea tures with 49.6% (  X  1 . 9% ), followed by the smaller geometric blur features with 52.1% (  X  0 . 8% ). Combining the two shape features together, we achieved 58.8% (  X  0 . 8% ), and with color and shape, reached 60.3% (  X  0 . 7% ), which Figure 3: Number of training exemplars versus average recog nition rate across classes (based on the graph in [11]). Also shows results from [11], [14], [16], [15 ], [13], [19], [20], [21], and [18]. Figure 4: Average confusion matrix for 15 training examples per class, across 10 independent runs. Shown in color using Matlab X  X  jet scale, shown on the right si de. is better than the best previously published performance fo r 15 training images on the Caltech 101 dataset [11]. Combining shape and color performed better th an using the two shape features alone for 52 of the categories, while it degraded performance for 4 6 of the categories, and did not change performance in the remaining 3. In Figure 4 we show the confus ion matrix for combined shape and color using 15 training images per category. The ten wors t categories starting with the worst were cougar body , beaver , crocodile , ibis , bass , cannon , crayfish , sea horse , crab , and crocodile head , nine of which are animal categories.
 Almost all the processing at test time is the computation of t he elementary distances between the focal images and the test image. In practice the weight vecto rs that we learn for our focal images are fairly sparse, with a median of 69% of the elements set to zero after learning, which greatly reduces the number of feature comparisons performed at test time. We measured that our unoptimized compute linear combinations and compare scores across foca l images, which amounts to negligible processing time. This is a benefit of our method compared to th e KNN-SVM method of Zhang, et al. [11], which requires the training of a multiclass SVM for every test image, and must perform all feature comparisons.
 We would like to thank Hao Zhang and Alex Berg for use of their p recomputed geometric blur features, and Hao, Alex, Mike Maire, Adam Kirk, Mark Paskin, and Chuck Rosenberg for many helpful discussions.

