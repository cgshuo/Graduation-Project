 In this paper, we propose a new weakly supervised abstractive news summarization framework using pattern based approaches. Our system first generates meaningful patterns from sentences. Then, in order to precisely cluster patterns, we propose a novel semi-supervised pattern learning algorithm that leverages a hand-crafted list of topic-relevant keywords, which are the only weakly super-vised information used by our framework to generate aspect-oriented summarization. After that, our system generates new patterns by fusing existing patterns and selecting top ranked new patterns via the recurrent neural network language model. Finally, we intro-duce a new pattern based surface realization algorithm to generate abstractive summaries. Automatic and manual evaluations demon-strate the effectiveness and advantages of our new methods. Code is available at: https://github.com/jerryli1981 I.2.7 [ Natural Language Processing ]: Text analysis Algorithms, Experimentation Summarization; Recurrent Neural Network; Capped Norm Semi-Supervised Learning
Multi-document summarization (MDS) problem has been mainly solved by the sentence extraction and compression based approaches [12, 4, 8, 13] for decades. However, the top systems so far were only considered as barely acceptable by the human assessment. Moreover, the empirical study [6] found the performance ceiling of pure sentence extraction approaches, which is much lower than hu-man summaries and not much better than the current best automatic  X  Corresponding author.
 c  X  systems. Meanwhile, user behavior experiments conducted inde-pendently by [6, 20] demonstrated that human always do abstrac-tion instead of simply selecting relevant sentences. More specif-ically, human tempt to interpret the text to understandable repre-sentation in their mind. After that, such representation is trans-formed to summary representation, also known as content selec-tion. Finally, human generate summary text from summary repre-sentation [11]. Therefore, based on human cognitive process, we believe that a fully abstractive approach has the most potential for generating summaries at the level comparable to human summary.
There are some pioneer works towards abstractive summariza-tion (see Section 2). For example, in meeting conversation anal-ysis community, some pattern based approaches first extract pat-terns from human-authored meeting summaries. Then, their ap-proaches select relevant patterns via clustering and ranking. Fi-nally, the template filling is used to generate summary text. In this paper, we focus on a new and distinct problem  X  how to generate news events summaries using pattern based abstractive summariza-tion framework. Unlike abstractive meeting conversation summa-rization, abstractive news summarization faces several unique chal-lenges. First of all, there exist several informative long sentences that have complex syntax structure in each news corpus. Secondly, the approaches of meeting conversation summarization can lever-age domain information such as dialogue act or meeting partici-pants to help generate abstractive summaries. However, domain information in news events are not easy to be captured. Thirdly, the standard text generation technologies such as template filling may lead to worse readability in news summarization scenario.
To solve these challenges and achieve the goal of generating high quality abstractive summarization, we propose a new abstrac-tive summarization framework which includes an enhanced pattern extraction module via integrating advanced information extraction technology, a novel robust semi-supervised pattern learning algo-rithm, and a novel surface realization algorithm for generating ab-stractive news summaries. More specifically, we first extract tuples by OpenIE 1 , recognize argument and predicate X  X  head via Stanford CoreNLP 2 , and annotate the type of phrase head via SEMAFOR and wordnet etc . Secondly, in order to precisely cluster patterns that better describe user interested information such as what hap-pened, reason, damages and countermeasures, we propose a novel semi-supervised pattern learning algorithm based on a new capped norm based objective function, which can achieve more optimal semi-supervised learning results than existing approaches. Thirdly, http://openie.cs.washington.edu http://nlp.stanford.edu/software/corenlp. shtml http://www.ark.cs.cmu.edu/SEMAFOR/ Figure 1: Abstractive news summarization framework flowchart our system generate new patterns by fusing learned patterns and select top ranked new patterns and sentences via recurrent neural network language model. Finally, we use Integer Linear program-ming to select top ranked sentences cross clusters. Figure 1 shows the flowchart of our new abstractive summarization framework.
The key contributions in our work are summarized as follows: 1) The first weakly supervised abstractive news summarization system that only leverages a small number of hand-crafted key-words. Note that interpretation and generation phases of our sys-tem are purely unsupervised. The reason for us to use keywords for helping clustering patterns is that we hope to generate aspect-oriented summarization 4 . The goal of aspect-oriented summa-rization is to present the most important content to the user in a condensed form and a well-organized structure to satisfy the user X  X  needs. For example, a summary about  X  X ttacks X  event should in-clude aspects about what happened, when/where it happened, rea-sons, damages, rescue efforts, etc . Table 1 shows the aspects and keywords for the  X  X ttacks X  event. We use keywords to accomplish domain-specific abstractive summarization. 2) In order to improve pattern clustering purity, we make use of continuous word vectors derived from a recurrent neural network architecture [19]. The vector-space word representations learned from the language models were shown to capture syntactic and se-mantic regularities. The word relationships are characterized by vector offsets, where in the embedded space, all pairs of words sharing a particular relation are related by the same constant off-set. Considering that this distributional semantic theory may bene-fit our pattern clustering component, we leverage word representa-tions trained from large external data to differentiate patterns. 3) To effectively utilize hand-crafted keywords, we propose a new semi-supervised pattern learning model to group the patterns. Because the number of labeled data is very small, the supervised learning models and many semi-supervised learning ( e.g. trans-ductive support vector machine) methods cannot get good perfor-mance on the pattern labeling. Thus, we use the label propagation based semi-supervised learning model. The previous label propa-gation based semi-supervise learning models use the ` 2 -norm loss, which cannot achieve the ideal semi-supervised learning results. Although the ` 0 -norm based objective is desired in semi-supervised learning, it is an NP-hard problem. To reach the optimal label-ing results, we propose a new capped norm based objective, which leads to more optimal results than traditional methods. We also derived a new and concise optimization algorithm to solve the pro-posed non-smooth and non-convex objective with proved conver-gence. http://www.nist.gov/tac/2010/Summarization/ 4) In order to control wrong facts and generate meaningful sen-tences given high quality pattern template, our system first filters out noisy templates by estimating the grammaticality with the re-current neural network language model. After that, in order to gen-erate meaningful sentences, we fuse tuples and select the best paths from the tuple graph that satisfies two ranking criteria. One crite-ria is the path should best match the most fluent pattern template, and the other criteria is the path should be the most fluent path com-pared with other paths. Finally, we use Integer Linear programming method to select top ranked facts cross clusters.
One line of related works focus on the task of news headline ab-straction. Alfonseca et al. [1] proposed a framework to generate abstractive news headline. They trained a pattern clustering model via Noisy-OR Bayesian network and computed probability distri-bution of newly extracted patterns from testing news collection via two-step random walk on bayesian network. They selected the pat-tern with highest ranking score from the most representative pattern cluster as template to generate headline sentence. Pighin et al. [25] solved the same problem with the same framework. However, they found that the memory-based pattern extraction method is better than heuristic or sentence compression based pattern extraction.
Another line of research works focus on abstractive meeting con-versation summarization. Murray et al. [20] followed the frame-work proposed by Jone [11] for generating abstractive summaries. In interpretation phase, they trained several classifiers to map sen-tences to domain dependent conversation ontology, then further mapped to predefined message patterns. After that, they utilized in-teger linear programming to select informative patterns in content selection phase. Finally, ontology based text plan and surface re-alization was implemented to generate abstractive summarization. Wang et al. [26] followed the same generation framework to solve meeting conversation summarization. In interpretation phase, they first clustered human-authored summary sentences and applied a multiple-sequence alignment algorithm to generate templates. In content selection phase, they identified the cluster describing a spe-cific aspect and extracted all summary-worthy relation instances. In generation phase, the templates were filled with these relation in-stances, then found the top ranked instances to form final summary. Mehdad et al. [17] grouped similar sentences with supervised clas-sifier and used the trained entailment graph to select representative sentences. After that, they built a word graph over similar sentences for fusion and selecting highly ranked paths as the final summaries. Oya et al. [24] extracted templates from clustered human-authored summaries and further got the generalization via template fusion in interpretation phase. In content selection phase, they borrowed training data to select the best templates. Finally, the templates were filled with matching labels to create summaries. Mehdad et al. [16] constructed word graph on human utterances and then se-lected the top ranked paths corresponding to the queries to generate the final summary.

Genest et al. [7] presented a fully abstractive news summariza-tion approach. They pointed out a fully abstractive summarization framework should comprise analysis of the text, content selection and summary generation. However, they created event template manually with linguistic resources. Mausam et al. [15] automati-cally extracted relations from text without requiring a pre-specific vocabulary, by identifying relation phrases and associated argu-ments in arbitrary sentences. Following their work, Balasubrama-nian et al. [2] generated coherent event schemas. Zhang et al. [29] focused on Chinese poetry generation task. Their poetry generator jointly performs content selection and surface realization by learn-
COUNTERMEASURES countermeasures, rescue efforts, prevention efforts, ing representations of individual characters via the recurrent neural network.
We observed that nearly half of the sentences comprise multiple aspects and complicated grammatical structures. In order to bet-ter interpret complicated sentences, we expect to extract meaning-ful relations and split structure-complicated sentences into simple ones. Table 2 illustrates the results of tuple extraction and pattern generation by our system.

In order to generate high quality patterns, we first utilize the sophisticated relation extraction tool such as openIE to generate tuples which have the triple form of {argument1, predicate, argu-ment2}. After that, the head of arguments are replaced with their semantic types. There are two major challenges here. One is how to find argument head precisely if argument contains complicated phrase structure. The other is where to find type ontology and how to design type mapping rules effectively. To solve these chal-lenges, we need linguistic resources and toolkits such as Framenet and Stanford CoreNLP. To leverage Stanford CoreNLP annotation pipeline for pattern quality enhancement, we need map openIE an-notation schema to Stanford CoreNLP annotation schema.
Predicate in each relation tuple may share some common gram-matical structures such as verb  X  noun  X  prep, verb, verb  X  prep, be  X  verb, be  X  verb  X  prep, adv  X  verb  X  noun  X  prep, be  X  prep etc. However, we found OpenIE put prepositional to-ken into argument 2. For example, { X  X e X ,  X  X tood two hours X ,  X  X fter killing three of them X  X . Therefore, when facing these special cases, we move the preposition into predicate. In order to locate prepo-sitional word, two grammatical relations 6 are used with heuristic rules. One is  X  X obj X  relation, the other is  X  X comp X  relation. Af-ter reassembling, the tuple may become { X  X e X ,  X  X tood two hours after X ,  X  X illing three of them X  X .
Before argument head extraction via Stanford CoreNLP toolkit, we need map string tokens in each tuple to Stanford IndexedWord objects. The mapping is not easy to implement because openIE and Stanford CoreNLP use different tokenization algorithm that may result in tokenization inconsistence. For example, Stanford https://framenet.icsi.berkeley.edu/ fndrupal/ http://nlp.stanford.edu/software/ dependencies_manual.pdf CoreNLP treats  X  X s-led X  as one token but openIE just considers  X  X ed X  as tuple X  X  predicate. We therefore use the token offset as a bridge to conduct token mapping due to tokens in each tuple may not be continuous. More specifically, our system first generates relation tuples and core annotations including pos, lemma, NER, parse tree and semantic dependency graph, and stores token start offset to IndexedWord mappings. If the mapping relation can X  X  be found due to tokenization inconsistence, we use heuristic rules to match correct start offset. If original sentence doesn X  X  contain argu-ment mention due to token order inconsistence, we search begin po-sition in substring via the regular expression and ignore the isolated tokens. After mapping between tuple tokens and IndexedWord ob-jects, we can fully make use of semantic dependencies between tuple tokens to extract phrase head.
Argument and predicate head extraction module is quite impor-tant for later head type recognition and pattern clustering. The main challenge here is how to extract head without loosing readability. The arguments and predicate in each relation tuple may have com-plicated phrase structure. For example, { X  X he suspect X ,  X  X ppar-ently called his wife saying X ,  X  X e was acting out in revenge for something X  X , { X  X irchner X ,  X  X old The Associated Press X ,  X  X hat six people were killed X  X . The argument 2 of these two tuples both contain subsentence or clause. Oya et al. [24] treated the right most nouns as the head nouns, but this heuristic rule is not always true. Paper [2] used several parts of speech based heuristic rules to conduct argument head extraction. However, we found this ap-proach didn X  X  work very well especially when facing such compli-cated phrase structures in tuples. We found phrase head extraction on complicated phrases may destroy tuple readability. Therefore, when facing argument mention contains grammatical relation like  X  X subj X ,  X  X subjpass X  or  X  X comp X , we just ignore it. This kind of ignorance would make sense due to the fact that openIE will find { X  X ix people X ,  X  X ere X ,  X  X illed X  X  relation tuple occasionally.
After the above step, we conduct depth first search on parser tree with the help of CollinHeadFinder rules to locate phrase head. For those heads can X  X  be recognized by CollinHeadFinder, we de-sign heuristic rules on semantic dependency graph to locate poten-tial heads. For example, if grammatical relation is one of { X  X et X ,  X  X mod X ,  X  X oss X ,  X  X um X ,  X  X dvmod X  }, then the governor token is the head. If grammatical relation is one of { X  X mod X ,  X  X n X  X , then the dependent is the head.
After phrase head extraction, we construct patterns via replac-ing the heads with their types. Standford NER as the state of
Countermeasures { X  X uggies X ,  X  X arried X ,  X  X ourn-the art type tagging tool have good performance on a small num-ber of types. However, we found it can X  X  recognize the tokens like  X  X an X ,  X  X irl X ,  X  X hilldren X  as person. Also we recognize that the SEMAFOR as the state-of-the-art semantic role labeling tool didn X  X  work well to recognize pronoun. In our case, some argument heads are not name entities, and they are concepts such as  X  X chool X ,  X  X an X . Therefore, a combination of several tools to conduct type tagging is a good choice.

Balasubramanian et al. [2] used wordnet and Stanford NER com-bination to conduct head type tagging. They manually selected 29 semantic types from WordNet. Because they built event schema in open domain, their types are too general for our task. Alfon-seca et al. [1] used Wikipedia and Freebase to annotate head with all its Freebase types and got better results. However, their tag-ging tool is company private property, not for sharing. Inspired by Chen et al. [3] on slot filling, we use SEMAFOR semantic parser based on framenet combined with Stanford NER and wordnet to recognize argument head X  X  type. In our implementation, Stanford NER has priority to find person, location, time and date. For those heads can X  X  be recognized by Stanford NER, we search Framenet and try to match frame elements via SEMAFOR tool. Note that SEMAFOR may produce noisy when labeling. For example, Tuple { X  X e X ,  X  X icked up X ,  X  X ilk X  X  will generate pattern like { X  X erson X ,  X  X ick up X ,  X  X ood X  X , { X  X  truck X ,  X  X icked up X ,  X  X ilk X  X  will gener-ate pattern like { X  X ehicle X ,  X  X ick up X ,  X  X hosen X  X . The same  X  X ilk X  generates two different labels. Therefore, We use wordnet, DBpe-dia and freebase to filter wrong labeling.
After generating patterns, we use hand-crafted keywords to group aspect relevant patterns for later template generation. In order to precisely group patterns that better describe user interested event aspects such as what happened, reason, damages and countermea-sures, we propose a novel semi-supervised pattern labeling algo-rithm.
We use word2vec 7 as the word embedding resources. It is gen-erated by learning a recurrent neural network [19]. The recurrent neural network language models use the context history to include long-distance information. The word relationships are character-ized by vector offsets, where in the embedded space, all pairs of words sharing a particular relation are related by the same con-stant offset. Considering that this distributional semantic theory may benefit our pattern clustering task, we leverage word represen-tations trained from large external data to generate pattern feature vectors. Word2vec contains pre-trained vectors trained on part of https://code.google.com/p/word2vec Google News dataset(about 100 billion words). The model con-tains 300-dimensional vectors for 3 million words and phrases.
Recall that our pattern has the triple form of {arg1Type, predi-cate, arg2Type}. Therefore, we concatenate the vectors of arg1Type, head of predicate and arg2Type to form pattern feature vector. On the other hand, we use merge strategy to construct labeled feature vectors of hand crafted keywords.
In order to accurately label the patterns via the hand crafted key-words, we propose a novel capped norm based semi-supervised pat-tern learning model. Given input { ( x 1 , y 1 ) ,  X  X  X  , ( x c elements are hand crafted keywords with labels as { y 1 where y i is a label indicator vector of size c containing the la-bels assigned to the pattern x i : y i ( k ) = 1 if x i belongs to the k -th class, and 0 otherwise. Our goal is to predict the label sets { y c +1 ,  X  X  X  , y n } for the unlabeled patterns { x c +1 ,  X  X  X  , x
Traditional graph based semi-supervised learning model solves the following problem [31, 30]: where L is the Laplacian matrix and Q  X &lt; n  X  c is the predicted de-cision values matrix to be solved. Given an affinity matrix W , the Laplacian is defined as L = D  X  W , where D = diag ( W e ) , e = (1 ,..., 1) T . The way to learn affinity matrix will effect the final re-sults, thus in our experiments we test two popularly used methods: heat kernel and cosine similarity.

We can further re-write problem (1) as: The ideal solution Q for semi-supervised learning is that q if x i and x j belong to the same class, which means many rows of Q are equal and thus has ideal class indicators. That is to say, it is desired that || q i  X  q j || 0 = 0 for many pairs ( i,j ) (the ` norm of w is a count of the nonzero elements in w ). However, the ` -norm minimization problem is an NP-hard problem. Although many recent works used ` 1 -norm to approximate ` 0 -norm [23], the gap between them is still large. To solve this problem and obtain a more ideal solution Q , we propose to use the capped norm based loss function [27, 28, 10] and solve the following problem for semi-supervised learning: min Algorithm 1 The algorithm to solve the proposed capped norm semi-supervised learning model as in (3).

Input The original weight matrix W  X  &lt; n  X  n . D is a diagonal matrix with the i -th diagonal element as P j W ij . The initial label matrix Y  X &lt; n  X  c . t = 1 . Initialize Q t  X  &lt; n  X  c using standard semi-supervised learning result as in Eq. (1); while Not Converge do end while
Output The decision value matrix Q t  X &lt; n  X  c . where  X  &gt; 0 is a small parameter and can be automatically se-lected, e.g. we can consider 5% of data as noise. In recent work [27, 28], the capped ` 1 -norm was used to approximate the ` with better results than ` 1 -norm. When our new objective is min-imized, the capped norm leads || q i  X  q j || 2 = 0 for many pairs ( i,j ) such that the ideal label assignments are achieved.
The other key challenge is to solve the above non-smooth and non-convex objective. All previous works used tedious algorithms to solve capped norm minimization problem. Based on our previ-ous re-weighted optimization method [21, 22], we propose a novel, concise, and efficient algorithm to solve the proposed new objec-tive.

Taking the derivative of Eq. (3) w.r.t. Q , and setting the derivative to zero, we have: where the new Laplacian matrix e L = e D  X  f W and the re-weighted weight matrix f W is defined by e D is a diagonal matrix with the i -th diagonal element as P Note that ( e L + I )  X  1 is dependent on Q , we propose an iterative algorithm to obtain the solution Q such that Eq. (4) is satisfied. The algorithm is guaranteed to converge to a local optimum, which will be proved in the next subsection. The algorithm is summarized in Algorithm 1.
To prove the convergence of the Algorithm 1, we need the fol-lowing lemma:
L EMMA 1. For any nonzero vectors q , q t  X  &lt; d , the following inequality holds:
P ROOF . Beginning with an obvious inequality  X  ( || q || 0 , we have  X  ( || q || 2  X  X | q t || 2 ) 2  X  0  X  2 || q || 2 || q t || which completes the proof.

The following theorem guarantees that the Algorithm 1 will con-verge to the local optimum of the problem (3):
T HEOREM 1. The Algorithm 1 will monotonically decrease the objective of the problem (3) in each iteration, and converge to the local optimum of the problem.

P ROOF . According to the Step 3 in the algorithm 1, we know that:
Because f W ij = where C is the set in which || q l  X  q k || 2  X   X  for all k,l  X  X  .
Based on Lemma 1, we have: || q for any pair ( l,k ) . Thus, for the set C , we have: Summing Eq. (9) and Eq. (10) on both sides, we obtain: which can be re-written as:
Thus the Algorithm 1 will monotonically decrease the objective of the problem (3) in each iteration t . In the convergence, Q e L t will satisfy the Eq. (4). As the problem (3) is a non-convex problem, satisfying the Eq. (4) indicates that Q t is the a local opti-mum solution to the problem (3). Therefore, the Algorithm 3 will converge to the local optimum of the problem (3).
In this step, we merge duplicated and complementary patterns to build templates in each cluster. Duplicates may caused by two reasons. One reason is that the same sentence generates duplicated tuples. For example, the two tuples { X  X hey X ,  X  X efused to divulge X ,  X  X etails X  X  and { X  X hey X ,  X  X efused X ,  X  X o divulge details X  X  extracted from the same sentence. The other reason is different sentences generate the similar tuples. For example, { X  X  steady job X ,  X  X ork-ing X ,  X  X ights X  X  and { X  X e X ,  X  X eld X ,  X  X  steady job working nights X  X  extracted from different sentences. Figure 2: A word graph with cycle generated by tuple fusion
We follow graph-based pattern fusion approach proposed by Oya et al. [24]. Instead of using general pattern form of {arg1Type, predicate, arg2Type}, we use specific pattern form in which only replace head of arguments with it X  X  type and keep other parts. Then, a graph is constructed by iteratively adding patterns to it. A node is added to the graph for each word in the pattern, and words ad-jacent are linked with directed edges. When adding a new pattern, a word from the pattern is merged to an existing node in the graph providing that they have the same POS tag and they share the same lemma. Note that some words such as  X  X e X  and  X  X is X  have the same POS tag  X  X RP X  and the same lemma, but they should not be merged together. Also some words like  X  X he X ,  X  X o X ,  X  X f X  and  X  X ave X  should not be merged together, unless it may lead to pro-duce so many noisy paths. After word graph is constructed, we enumerate all paths from start node to end node. Figure 2 illus-trates the word graph generated via tuple fusion instead of pattern fusion for convenience. We can see that the graph may contain cy-cles. Therefore, when the cycle appears, a backtracking procedure need to be executed.
In order to control wrong facts and generate meaningful sen-tences given pattern template, our system first filters out noisy tem-plates by estimating the grammaticality using the recurrent neural network language model. Then, in order to generate meaningful sentences, we fuse tuples and select the best paths from tuple graph that satisfy two ranking criteria. One criterion is the path should best match the most fluent pattern template, and the other crite-rion is the path should be the most fluent path compared with other paths. Finally, we use Integer Linear programming to select top ranked facts cross clusters.

In order to filter out noisy templates, we leverage a neural net-work language model to rank the generated paths and select the top ranked one. We employ a character-based recurrent neural net-work language model [18] (RLM) interpolated with a Kneser-Ney trigram and find the n-best candidates with a stack decoder. A RLM models the probability P ( S ) that the sequence of words S occurs in a given language. Let S = w 1 , ... , w m be a sequence of m words: The model explicitly computes without simplifying assumptions an RLM comprises a vocabulary V that contains the words w the language as well as three transformations: an input vocabulary transformation I  X  R q  X  X  V | , a recurrent transformation R  X  R and an output vocabulary transformation O  X  R | V | X  q . For each word w k  X  V , we indicate by i ( w k ) its index in V and by v ( w R | V | X  1 an all zero vector with only v ( w k ) i ( w
For a word w i , the result of I  X  v ( w i )  X  X  q  X  1 is the input contin-uous representation of w i . The parameter q governs the size of the word representation. The prediction proceeds by successively ap-plying the recurrent transformation R to the word representations and predicting the next word at each step. In detail, the computation of each P ( w i | w 1: i  X  1 ) proceeds recursively. For 1 &lt; i &lt; m ,
In above equations,  X  is a nonlinear function such as tanh . The conditional distribution is given by:
The RLM is trained by back propagation through time [18]. The error in the predicted distribution calculated at the output layer is backpropagated through the recurrent layers and cumulatively added to the errors of the previous predictions for a given number d of steps.
In order to find the best tuple path on tuple graph, we have de-vised the following ranking strategy. First, we prune the paths in which a verb does not exist. Then we rank other paths as follows: To identify the summary sentence with the highest coverage of se-lect pattern, we propose a score to measure the similarity between pattern path and tuple path.

Note that distributional semantics can be captured by continuous space word representation. We transform each token x into its em-bedding vector x by pre-trained distributed word representations, and then the similarity between a pair of x a and x b can be com-puted as their cosine similarity.
 In order to improve the grammaticality of the generated sentence, we estimate the grammaticality of generated paths F ( S ) using the recurrent neural network language model.

In order to generate an abstractive sentence that combines the scores above, we employ a simple linear ranking model. The pur-pose of such a model is: 1) to cover the pattern optimally; 2) to generate a more readable and grammatical sentence. Therefore, the final ranking score of the path S is calculated over the normalized scores as: where  X  and  X  are the coefficient factors to tune the ranking score and they sum up to 1. In order to rank the tuple graph paths, we select all the paths that contain at least one verb, and re-rank them using our proposed ranking function to find the best path as the summary of the original patterns in each cluster.
After sentence ranking, we prepare for the final abstractive sum-mary generation. In this step, we select one sentence from each cluster. We use Integer Linear Programming to optimize a global objective function for sentence selection. We formulate the opti-mization problem based on sentence ranking information. More specifically, we would like to select exactly one sentence which receives the highest possible ranking score from each cluster sub-ject to some constraints. We employed lp_solver 8 , an efficient mixed integer programming solver using the Branch-and-Bound al-gorithm to select sentences.

Assume that there are in total K clusters in an event topic. For each cluster j , there are in total R ranked sentences. The variables S jl is a binary indicator of the sentence. That is, S sentence is included in the final summary, and S jl = 0 otherwise. l is the ranked position of the sentence in this cluster.
 Top ranked sentences are the most relevant corresponding to the related user interested aspects which we want to include in the final summary. Thus we try to minimize the ranks of the sentences to improve the overall responsiveness: To prevent redundancy in each aspect, we choose one sentence from each general or specific aspect cluster. The constraint is for-mulated as follows: We add the following constraint to ensure that the length of the final summary is limited to L words. len jl is the length of S jl
We use TAC2011 Summarization task 9 data set for the summary content evaluation. This data set provides 44 events. Each event falls into a predefined event topic. Each specific event includes an event statement and 20 relevant newswire articles which have been divided into 2 sets: Document Set A and Document Set B. http://lpsolve.sourceforge.net/5.5/ http://www.nist.gov/tac/2011/Summarization/ Each document set has 10 documents, and all the documents in Set A chronologically precede the documents in Set B. We just use document Set A for our task. Assessors wrote model summaries for each event, so we can compare our automatic generated summaries with the model summaries.
We use the ROUGE [14] metric for measuring the summariza-tion system performance. Ideally, a summarization criterion should be more recall oriented. So the average recall of ROUGE-1, ROUGE-2, ROUGE-SU4 were computed by running ROUGE-1.5.5 with stemming but no removal of stop words. We compare our method with the following baseline methods.
 In this baseline, we compare our method with traditional rank-ing and selection extractive summary generation framework [5] to show that the abstractive framework is superior to extractive frame-work in aspect-oriented summarization scenario. In implementa-tion, we follow LexRank based sentence ranking combined with greedy sentence selection methods. The similarity graph building threshold is 0.2, damping factor is 0.2 and error tolerance for Power Method in LexRank is 0.1. After ranking sentences, we select top ranked sentences as long as the redundancy score (similarity) be-tween a candidate sentence and current summary is under 0.5. This is repeated until the summary reaches a 100 word length limit. In this baseline, in order to prove the effectiveness of pattern fu-sion and surface realization, we replace our pattern fusion and sur-face realization with natural language generation technology. After grouping patterns via clustering, we choose the pattern that clos-est to centroid of the cluster. Then realize sentences with these representative patterns via natural language generation technology. More specifically, due to each pattern contains predicate, in order to generate sentence, we need generate noun phrase (See Algorithm 2) to build subject and generate verb phrase(See Algorithm 3) to build object. Then combine subject, predicate and object to become a new sentence. We use Simplenlg 10 which is a simple Java API de-signed to facilitate the realization of sentences.
 In this baseline, we try to compare our capped norm based semi-supervised learning model with the semi-supervised learning method using gaussian fields and harmonic functions (Har) [31] and the semi-supervised clustering with local and global consistency (LGC) [30]. To construct weight matrix, we use Heat Kernel (HK) [9] and Cosine similarity. Equation 23 illustrates how to construct weight matrix with Heat Kernel. where x id is the d -th component of pattern x i represented as a con-tinuous vector x i  X  R m and  X  1 ,..., X  m are length scale hyper parameters for each dimension.

For fair comparison, all the baselines have the summary length no more then 100 words. In Table 3, we show the average ROUGE metrics of 44 summaries generated by our method and three base-line methods. Note that we present the best ROUGE scores after trying different  X  settings. http://code.google.com/p/simplenlg/ Algorithm 2 Generate Noun Phrase NPPhraseSpec np , tmpNp ; PPPhraseSpec pp ;
Stack stack stack.add ( head ) while ! stack.isEmpty () do end while We compared our capped norm based semi-supervised learning model to two other popularly used semi-supervised learning methods: Har-monic function (Har) [31] and LGC [30]. In the comparison results, our new model consistently outperforms two other methods. Be-cause we utilize the capped norm based loss, our new objective can achieve better semi-supervised learning label indicator matrix than traditional methods which use the ` 2 -norm based loss function.
Our abstractive aspect-oriented summarization system shows sta-tistically significant improvements over Baselines 2 and 3 and pure extractive summarization systems for ROUGE. This means our sys-tems can effectively aggregate the extracted patterns and generate abstract sentences based on the relevant keywords. We can also observe that our abstractive summarization system produces the highest ROUGE-1 score among all models, which further confirms the success of our framework that can cover more human-authored words. However, compared with TAC 2011 best system and com-pression based approaches [12] that use extractive based super-vised learning, our system performance is bad. This proved that fully unsupervised abstractive summarization is a very challenge task, however our pattern based approach shows the feasibility and usefulness of this new direction (our framework only uses a small number of hand-crafted keywords for aspect-oriented summariza-tion and all the rest modules are unsupervised).
To judge the quality of generated summaries, we ask three grad-uate students to score them. The judges will give a grammaticality and coherence scores to each summary. These two scores reflect the fluency and readability of the summary. Also the judges will give a informativeness score to each summary. This score reflects the coverage of all required aspects. The judges follow 5-point Likert scale to score each summary. We then compute the average scores. Note that we use LGC in Baselines 2 and 3 because they can get relative better clustering results. The manual evaluation results are shown in Table 4.
 Algorithm 3 Generate Verb Phrase V PPhraseSpec vp
NPPhraseSpec dirobjNp , indirObjNp vp.sertV erb ( verb ) if object ! = null then else end if Table 3: ROUGE evaluation results on TAC2011 Summariza-tion data sets, The improvements made by our method over the baselines are all statistically significant at 95% confidence level (p&lt;0.05).
 As expected, Baseline 1 received the highest rating of grammati-cality because it use extractive based approach. Baseline 2 received the lowest rating of grammaticality, this result reflect heuristic sum-mary generation via construct subject and object still need more linguistic knowledge. Referring to coherence rating, Baseline 1 is the lowest due to other methods leverage hand crafted keywords as hints to improve final summary coherence. Referring to infor-mativeness rating, our label propagation clustering can better find semantic similar and complementary patterns that match user in-terested aspects. Overall, abstractive summary generation via label propagation, tuple/pattern fusion and recurrent neural network lan-guage model can improve the fluency, readability and coverage of final summary. BL-1 Ext 3.4 2.7 3.2 BL-2 NLG 2.9 3.0 3.3 BL-3 Fuse 3.1 3.2 3.4 Our Method 3.3 3.3 3.6 Table 4: Manual evaluation results on TAC2011 Summariza-tion data sets
Table 5 presents the output summary of the subject  X  X mish Shoot-ing X  and  X  China Water Shortage X  generated by our method. Com-paring against the human-authored summary, our method can cap-ture additional information related to damage of the accident such as  X  X ix people were dead. X  On the other hand, we realized that n-gram cooccurance based ROUGE metric may not be suitable for abstractive summarization, and the new sentences generated by our approach may not match human-authored golden standard sum-mary well.
In this paper, we study the problem of using pattern based ap-proaches to generate abstractive summarization. Our system first generates meaningful patterns from sentences. In order to pre-cisely cluster patterns, we propose a new capped norm based semi-supervised pattern learning algorithm that leverages a hand-crafted list of topic-relevant keywords. Our system generates new patterns by fusing existing patterns and selects top ranked new patterns via the recurrent neural network language model. Finally, we use In-teger Linear programming to select top ranked facts cross clusters. Although fully unsupervised abstractive summarization is a chal-lenging task (the performance is still low compared with supervised extractive summarization system), our work shows the feasibility and usefulness of this new direction for summarization research.
There are a number of directions we plan to pursue in the future in order to improve our method. First, we can possibly apply more linguistic knowledge to improve the quality of head type tagging. Currently the tagging pipeline may generate meaningless tags. Sec-ond, we may explore more domain knowledge to improve the qual-ity of pattern clustering. For example, we know that the  X  X ho-affected X  aspect is related to person, and  X  X hen, where X  are re-lated to time and location, we can leverage these annotated phrases to help group relevant sentences. Third, we want to enhance our semi-supervised clustering model to precisely find similar patterns.
This work was supported in part by Australian Research Council (ARC) grants and U.S. NSF-IIS 1117965, NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628. [1] E. Alfonseca, D. Pighin, and G. Garrido. Heady: News [2] N. Balasubramanian, S. Soderland, Mausam, and O. Etzioni. Table 5: A comparison between a human-authored summary and a summary generated by our system [3] Y.-N. Chen, W. Y. Wang, and A. I. Rudnicky. Leveraging [4] J. M. Conroy, J. D. Schlesinger, P. A. Rankel, and D. P. [5] G. Erkan and D. R. Radev. Lexrank: Graph-based lexical [6] P.-E. Genest and G. Lapalme. Text generation for abstractive [7] P.-E. Genest and G. Lapalme. Fully abstractive approach to [8] D. Gillick, B. Favre, D. Hakkani-Tur, B. Bohnet, Y. Liu, and [9] X. He and P. Niyogi. Locality preserving projections. In [10] W. Jiang, F. Nie, and H. Huang. Robust dictionary learning [11] K. S. Jones. Automatic summarizing: factors and directions. [12] C. Li, Y. Liu, F. Liu, L. Zhao, and F. Weng. Improving [13] P. Li, Y. Wang, W. Gao, and J. Jiang. Generating [14] C.-Y. Lin and E. Hovy. Automatic evaluation of summaries [15] Mausam, M. Schmitz, R. Bart, S. Soderland, and O. Etzioni. [16] Y. Mehdad, G. Carenini, and R. T. Ng. Abstractive [17] Y. Mehdad, G. Carenini, F. W. Tompa, and R. Ng.
 [18] T. Mikolov, M. Karafi X t, L. Burget, J. Cernock ` y, and [19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [20] G. Murray, G. Carenini, and R. Ng. Generating and [21] F. Nie, H. Huang, X. Cai, and C. Ding. Efficient and robust [22] F. Nie, H. Huang, C. Ding, D. Luo, and H. Wang. Principal [23] F. Nie, H. Wang, H. Huang, and C. Ding. Unsupervised and [24] T. Oya, Y. Mehdad, G. Carenini, and R. Ng. A [25] D. Pighin, M. Cornolti, E. Alfonseca, and K. Filippova. [26] L. Wang and C. Cardie. Domain-independent abstract [27] T. Zhang. Multi-stage convex relaxation for learning with [28] T. Zhang. Analysis of multi-stage convex relaxation for [29] X. Zhang and M. Lapata. Chinese poetry generation with [30] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [31] X. Zhu, Z. Ghahramani, J. Lafferty, et al. Semi-supervised
