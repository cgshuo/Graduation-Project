 Text Summarization starts off from the 1950s X  when people use simple statistical and linguistic knowledge to generate summary of the documents [ 12 ]. Till now, many summarization methods have been proposed, they can be categorized as extractive summarization [ 5 ] and abstractive summarization. The former ranks the sentences in the document and extracts the top-ranked sentences to com-prise the summary, the latter usually needs sentence compression [ 8 ], information fusion [ 1 ] and re-formulation [ 13 ], it requires natural language understanding and representation knowledge, it X  X  more complicated and not mature even now. So most summarization methods belong to extractive summarization. Summa-rization can be also categorized as generic summarization and query-focused summarization from different purposes of the summary, generic summarization generates summary in all aspects about the document, while query-focused summarization generates summary mainly considering the user X  X  preference or the query. Our approach is for extractive query-focused multi-document summarization.
 The purpose of the query-focused summarization is to extract sentences that are highly relevant to the query and representative of the documents, so measur-ing the relevance between the sentence and query accurately is very important. The paper [ 14 ] first proposed to use wikipedia concepts to help to generate query-focused summarization, as Wikipedia has grown to become the largest encyclope-dia with over 2 million articles [ 18 ], using wikipedia concepts can greatly enrich the literal meaning of the sentences. The method proposed in [ 14 ]istomap sentences to concept-vector and use the concept-vector and concept-relatedness matrix to get sentence-query relatedness, sentences that are most correlated with the query are selected to comprise the summary. This method generates more accurate summary compared to the previous term-vector method. There also exist summarization methods that exploit the conceptual relation of sentences in generic multi-document summarization [ 20 ], because concept is the basic ele-ment of language that can express an entity or action, by further exploring the conceptual relations we can get more informative summary. While in query-focused summarization, in order to generate more accurate summary, we are the first to further explore the conceptual relation in the effective graph model and use the ranked weighted concepts to help to rank the sentences. In this paper, we first map the query and sentences in the documents to the concept set, con-struct a bipartite-graph between the concepts of the query and concepts of the sentences, by mutual reinforcement between the concepts, we can get the ranked weighted concepts. Then we use a hyper-graph model to represent the compli-cated relations between sentences and concepts and get top-ranked sentences to comprise the summary, sentences that are more correlated with the query, contain more important concepts and also correlated with other sentences in the documents ranked higher. Finally remove redundancy and reorder the sentences comprising the summary according to the original sentence position. The contribution of this paper is as follows: (1) We explore the implicit semantic information of the sentence using Wikipedia concept and take the concept as the basic element of the documents which is more close to human X  X  comprehension. (2) We get the ranked weighted concepts by mutual reinforcement between the concepts of sentences and concepts of query in a bipartite-graph model, which help to rank the sentences comprising the summary. (3) We utilize hyper-graph model to represent the complicated relation between sentences and concepts, and get sentences that are central in the graph, contain important concepts and correlate with the query to comprise the sum-mary.
 work about the query-focused summarization, Sect. 3 introduces our proposed method in detail, Sect. 4 presents the experiment of our method, Sect. 5 concludes this paper. Query-focused summarization is to extract sentences that can satisfy the query X  X  need to comprise the summary. The most direct way is to calculate the similarity between sentence and query, sentences that are more similar to the query are ranked higher and comprise the summary. To calculate the similarity between sentences, Miao [ 14 ] proposed to use Wikipedia concept vector and concept-relatedness matrix to get the similarity between sentence and query, compared to the previous TFIDF vector method, Miao [ 14 ] gets more accurate summary because it takes into account the implicit semantic meaning of the sentences. The most query-focused summarization methods used graph-based model [ 3 , 4 , 16 , 17 ]. It X  X  an extension to the generic graph-based model. In a graph-based model, take the sentences as vertex, the similarity between sentences as the edge weight between vertexes. By random walk on the graph, get sentences that are more central in the graph to comprise the summary. while the previous graph-based method only consider the relation between two sentences and ignore the implicit semantic meaning of sentences, the paper [ 2 ]proposedtousehyper-graph to represent textual content to get generic text summarization, hyper-graph can represent the relation between more than two sentences as in real-world condition, but this method takes word as the basic language element and don X  X  consider the semantic meaning of sentences. There are also some methods concerning the expansion of query through external corpus such as Word Net [ 19 ] or selecting important terms in the documents [ 21 ], these methods can enrich the query and get more information about the query but it can also result in noise in the query, so many unnecessary terms may add to the query and result in unwanted result. There also exist some other query-focused summarization methods extended from the classical generic summarization methods like MMR which also takes into account the query-related feature and the previous features to decide the weight of sentences [ 11 ]. We propose a method to generate query-focused multi-document summarization based on concept importance(QMSCI). Our method is composed of three com-ponents: (1) map the query and sentences in the document to the concept set, construct bipartite-graph between the concepts of the query and concepts of the sentences, the weight between two concept is calculated using WLVM(Wikipedia Link Vector based Measure) metric and is calculated based on the Wikipedia hyperlink structure. After running the random walk algorithm like HITS [ 7 ]on the bipartite-graph, we get the ranked weighted concepts; (2) construct hyper-graph using sentence as hyper-edge and concepts as node, initiate the sentence weight according to the concepts importance of the sentence and the similar-ity between the sentence and the query, random walk on the hyper-graph to get the ranked weighted sentences; (3) select the top-ranked sentences, remove redundancy and reorder the sentences according to the original position in the document and generate the summary. The main framework of the method is shown as follows in Fig. 1 .
 3.1 Bipartite-Graph Construction There exist two methods to map the sentences to concepts [ 6 ]. We use the related-match method to map the query and sentences in the documents to the concepts. The mapping process is shown as follows in Fig. 2 . s i represent the ith sentence in the documents, and c mapped from the sentences, w ij are the relatedness between the sentence s and concept c j , for convenient processing, we only select the top-N concepts that most correlated with the sentence as the sentence-mapped concept vector. After mapping the query and sentences to the concept vector, we construct a bipartite-graph between all concepts of the sentences and concepts of the query. The bipartite-graph is shown as follows in Fig. 3 .
 Q i represent the concepts of the query, C j represents the concepts of the sentences, the relatedness between Q i and C j is measured by WLVM metric, a sophisticated measure to calculate the relatedness between two concepts based on the hyperlink structure in Wikipedia [ 15 ]. The principle behind this measure is that more common concepts the two concepts point to, while less other concepts point to the target concepts, the more correlated is between the two concepts. The process of calculating the relatedness between two concepts by WLVM is as follows: c i , c j are two concepts and represented as a vector of the weight pointing to the target concepts: point from concept c i to L i , t is the total number of concepts in Wikipedia, the relatedness between two concepts c i and c j is then calculated as the cosine similarity between the two weight vectors.
 the authority nodes and concepts of the sentences as the hub nodes, run the random walk algorithm like hits on the bipartite-graph until convergence to get the ranked-weighted concepts. We initiate the hub node weight as total number of hub nodes, and initiate the authority node weight as total number of authority nodes. In the random process, every iteration step,the score of every authority node is the total score of the hub node that point to it, and the score of every hub node is the total score of the authority node that it point to. To guarantee the random walk process is convergent, every iteration step the weight of the vertexes is normalized. 3.2 Hyper-graph Construction Hyper-graph is a generalization of graphs [ 2 ], the previous graph models the pairwise-relation between two vertexes as an edge between the two vertexes, it can only represents one-to-one relationship between two vertexes, while in real world, there may exist many complicated relations among more than two vertexes, so hyper-graph can better represent the real-world problem and express more complicated relations between more than two vertexes. The random walk process in the hyper-graph can favor the sentences that are more important and informative, while in the previous graph model, a sentence which is uninformative but have some unimportant common words with many other sentences can be ranked higher by random walk on the graph. A hyper-graph can be defined as G = {
E, V, W e ,W v } , where V is the vertex set, E is hyper-edge set, a hyper-edge is a subset of the total vertex set, v is incident to e when v belong to e, and W is the hyper-edge weight, W v is the vertex weight. We construct a hyper-graph using the sentences as hyper-edge and the concepts as vertexes, our constructed hyper-graph is shown as follows in Fig. 4 .
 S i represents sentences of the documents; C i represent the concepts, C incident to S j if C i belong to the concept set of S j . In the above hyper-graph, S contains concepts C 1 , C 4 , C 5 and S 2 contains concepts C vertex weight and hyper-edge weight is the weight of the corresponding sentences and concepts . From the above section we have get the concept weight, and we initiate the sentence weight as C is the concept of sentence S, w ( C i ) is the weight of the concept C ate the sentence weight because in the random walk process, we want to favor sentences that have high initial score. The initial score of a sentence depends on two factors: the average score of the concepts the sentence mapped to and the similarity between the sentence and the query, the similarity is measured by the term overlap measure. The parameter  X  is used to balance the proposition of the two factors influencing the initial sentence score. In the experiment we set  X  to 0.8. Sentences that are more similar to the query and contain more important concepts get higher initial weight. Then random walk among the hyper-edges, the process of random walk from a hyper-edge S i to another hyper-edge is as follows: choose a vertex that is incident to S i proportional to the vertex weight, choose another hyper-edge the vertex incident to proportional to the hyper-edge weight, the probability of random walk from hyper-edge S i where C i is the vertexes that incident to both S i and S that incident to the two hyper-edges, and more important the vertexes are, it is more probable to random walk from S i to S j , and the random walk process favors the high score initiated sentences. Run page rank algorithm to get the ranked weighted sentences. Sentences that is more central in the graph and have more initial scores will rank higher at last.
 3.3 Summary Generation Since sentences are ranked, reorder the sentences by weight in descending order. Use list A to store the reordered sentences and list B to store the top ranked sentences, initiate list B as an empty list. Every time we choose the first element in list A and calculate the similarity with the sentences in B, if the similarity is beyond a given threshold, the sentence is discarded, else the sentence is removed from list A and added to list B. Repeat this procedure until the total sentence length in B is up to the summary length. Reorder sentence in list B according to the original position of the sentence in the document to comprise the summary. 4.1 Experiment Setup We use DUC2006 and DUC2007 datasets, DUC2006 contains 50 topics, every topic includes 25 documents, DUC2007 contains 45 topics and every topic also includes 25 documents. For every topic of the documents, we use the sentences inserted between the title and narr tag as the query sentences of the corre-sponding documents set. We used the ROUGE (Recall-Oriented Understanding for Gisting Evaluation) as evaluation method, ROUGE measure the quality of the system-generated summary by comparing the overlap between the system-generated summary and human-generated summary [ 9 ]. The length of the sum-mary is 250 words. The ROUGE-N metrics is defined as: ROU GE  X  N = S Rouge-N is based on n-gram overlap between the system-generated summary and a set of model summary that is the human generated summary [ 10 ]. N rep-resent the length of n-gram, Count match ( gram n ) is the maximum number of n-grams co-occurring in a candidate summary and the human-generated sum-mary, Count ( gram n ) is the total number of n-gram in the human-generated summary. We used F-SCORE as the final result of the ROUGE metrics. We get the F-SCORE of Rouge-1(unigram-based), Rouge-2 (bigram-based) and Rouge-SU4(based on skip bigram with a maximum skip distance of 4), where Rouge-1 and Rouge-SU4 are regarded as the most correlated results with the human eval-uation. For every topic of the dataset, we get a F-SCORE value, and the final result is the average F-SCORE of all the topics. 4.2 Experiment Results In our experiment, we choose the extended graph-based method used for query-focused summarization named as Graph-based, query expansion method [ 19 ] named as Query-expansion and improved sentence-query similarity method which used concept-vector and concept-relatedness matrix to rank the sentences [ 14 ] named as Word+Matrix, the experimental results on duc2006 and duc2007 are shown in Tables 1 and 2 .
 The experimental result demonstrates the effectiveness of our proposed method, in duc2006 and duc2007, our method performs better than other meth-ods. Because we not only consider the implicit semantic information of sentences, but also utilize bipartite graph model and hyper-graph model to get the ranked weighted concepts and ranked weighted sentences, we rank sentences according to the concept importance of the sentence, the sentence-query similarity and sentence centrality in the graph, so the generated summary is more correlated with the query X  X  need. We also study the influence of different summary length to the summary quality, we define the summary size as 100,150,200,250,300 respec-tively, because the ROUGE-1 and ROUGE-SU4 are considered as the most cor-related metrics compared with our human comprehension, we use ROUGE-1 and ROUGE-SU4 metrics to test the influence of the summary length using the DUC 2006 dataset. The experimental result is as follows in Fig. 5 . the summary is increased. Because with the increase of the summary length, the summary contains more important information, and the quality of the sum-mary is increased. Other phenomenon is that with the length of the summary increasing, our method has more advantages. It may because the basic language element we consider is concept, with the increase of the summary length, more important concepts are added to the summary, while other methods may add more redundancy. There is another phenomenon, in the ROUGE-SU4 metric, when the summary length is less 200, the extended graph-based method out-performs our method, when the summary size increases, the advantage of our method can merge. In this article, we propose a new query-focused multi-document summarization method based on concept importance. First we use a bipartite-graph model to get ranked weighted concepts, then we use the weighted concepts to help to rank sentences in a hyper-graph model. We are the first to utilize the concept-concept and concept-sentence relation and reinforcement to get query-focused summa-rization. The experimental results demonstrate the effectiveness of our proposed method. In the future, we want to do some research on how to reduce the spa-tial complexity and time complexity of this algorithm. And we want to improve the sentence-concept mapping method by considering the whole document con-tent and not only the sentence to adjust the mapping process more tailored to document summarization and help to generate more accurate summary.

