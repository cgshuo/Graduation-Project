 ORIGINAL PAPER Emli-Mari Nel  X  J. A. du Preez  X  B. M. Herbst Abstract This paper describes a skeletonization approach that has desirable characteristics for the analysis of static handwritten scripts. We concentrate on the situation where one is interested in recovering the parametric curve that pro-duces the script. Using Delaunay tessellation techniques where static images are partitioned into sub-shapes, typi-cal skeletonization artifacts are removed, and regions with a high density of line intersections are identified. An eval-uation protocol, measuring the efficacy of our approach is described. Although this approach is particularly useful as a pre-processing step for algorithms that estimate the pen tra-jectories of static signatures, it can also be applied to other static handwriting recognition techniques.
 Keywords Skeletonization  X  Thinning  X  Pseudo skeleton  X  Document and text processing  X  Document analysis 1 Introduction Producing a handwritten signature, or any other handwriting, is a dynamic process where the pen X  X  position, pressure, tilt and angle are functions of time. With digitizing tablets, it is possible to capture this dynamic information. For some appli-cations, dynamic information is invaluable, e.g. during sig-nature verification dynamic information contains significant additional biometric information which may not be readily available to the potential forger. It is therefore not surprising that efficient and reliable signature verification systems based on dynamic information exist. There are, however, important applications where the dynamic information is absent, such as handwritten signatures on documents, or postal addresses on envelopes. In these applications, one has access to only the static images of the handwritten scripts. The absence of dynamic information in these applications generally causes a deterioration in performance compared to systems where the dynamic information is available.

A key pre-processing step in the processing of static scripts is thinning or skeletonization where the script is reduced to a collection of lines. A number of algorithms that specialize in character/word recognition [ 1  X  6 ], or the vectorization of line drawings [ 7  X  10 ] have been developed. (Note that skel-etonization algorithms compute the centerlines (skeletons) from image boundaries, whereas thinning algorithms remove outer layers of an image while simultaneously preserving the image connectivity. In this paper, we describe a pseudo-skeletonization approach where we approximate the center-line of the script).

Given the advantages of systems based on dynamic information it is natural to consider the possibility of recov-ering dynamic information from static scripts. We are par-ticularly interested in extracting the temporal sequence of the curve (or curves) of the script from its static image, given different dynamic representatives of the script [ 11  X  13 ]. The pen trajectory of a static script is typically constructed by assembling the collection of lines that constitute a sta-tic skeleton (essentially the curves between intersections). Most existing techniques rely on some local smoothness cri-terion to compute the connectivity of lines at intersections; see, e.g. [ 14  X  19 ]. Finding the necessary connections is gen-erally based on local line directions, and so preserving local line directions becomes an important attribute of a desirable skeletonization process.

Local smoothness heuristics that are used to unravel sta-tic scripts are typically based on the continuity criterion of motor-controlled pen motions which assumes that muscular movements constrain an individual X  X  hand (holding the pen) to move continuously, thus constraining the pen to maintain its direction of traversal [ 20 ].

To apply heuristics based on local line directions (or more generally on the continuity criterion of motor-controlled pen motions) becomes challenging in areas with a dense con-centration of line crossings. This problem is compounded by skeletonization artifacts; since artifacts are not image fea-tures, they introduce more ambiguities. It is therefore not sur-prising that skeletonization artifacts have a negative impact on the performance of off-line handwriting recognition sys-tems, as observed by [ 4 , 11 , 15 , 17 , 21  X  23 ]. In the absence of a good skeletonization algorithm, it might be appropriate to follow a different approach by analyzing handwriting directly from the grey-scale images and therefore circumventing the skeletonization step [ 24  X  26 ].

The most obvious artifacts for line drawings are periph-eral artifacts , such as spurs attached to the skeleton of an image, and intersection artifacts , where two or more lines that should intersect fail to cross each other in a single point. Figure 1 a contains a static signature with its skeleton depicted in Fig. 1 b. The skeleton follows the centerline of the original image, and was obtained through a standard thinning algo-rithm [ 27  X  30 ]. Note that the skeleton retains the topology of the original image, where topology refers to the connectivity of the original image. This skeleton contains many intersec-tion artifacts (some of them are enclosed in dotted circles).
Figure 2 a focusses on an intersection artifact (line 2) pro-duced by a typical skeletonization procedure. This artifact causes a discontinuity in the transition from line 1 to line 3 (indicated by the arrows), i.e., the transition is not smooth. By removing the intersection artifact (line 2) and connecting the lines that enter the indicated region in Fig. 2 b, a smooth skeleton can be constructed, as indicated in Fig. 2 c. Fixing intersection artifacts by following local line directions (as illustrated in Fig. 2 ) becomes problematic in regions with multiple intersections. Note, e.g., the ambiguous line direc-tions in the left-hand side of Fig. 1 a, which can lead to the loss of the correct connections between incoming and outgoing curves, this can be problematic for applications that recover the temporal information of static handwritten scripts from their unordered skeleton curves.

One can think of our skeletonization algorithm as a pre-processing step to exploit the continuity criterion of motor-controlled pen motions: we identify regions of multiple crossings where this criterion can not be applied reliably, and remove intersection artifacts so that one can apply the continuity criterion locally at simple intersections. Although our skeletonization approach is primarily motivated by the temporal recovery problem, it is expected that other hand-writing recognition applications that rely on the continuity criterion, can also benefit from our skeletonization approach.
Our skeletonization approach modifies the work of Zou and Yan [ 7 ] to better preserve local line directions, and to identify complicated intersections. In Fig. 3 a the first of two modifications is illustrated: The algorithm is optimized to retain the topology of the original image, while removing many of the artifacts. Note however, that intersection artifacts are not removed in complicated regions (e.g. the left-hand side of the signature) as local line directions (used to identify and fix artifacts) become too unreliable. The resulting skele-tonisreferredtoasa conventional skeleton, as it retains the topology of the original image.

For applications that are concerned with the temporal recovery of static handwritten scripts, there is the danger that the local directions are not always smooth enough in regions of multiple crossings in the conventional skeletons in order to trace the curves through these regions. It is therefore also important to provide a second option where the visual appear-ance is sacrificed for smoother connections. This is achieved by a second modification to the Zou and Yan algorithm which introduces the web-like structures visible in Fig. 3 b. Since this skeleton does not preserve the topology of the origi-nal skeleton, it is more accurate to refer to it as a pseudo skeleton . Smoother trajectories can improve the chances of a global optimizer (see [ 11 ]) to find the correct connections, as shown in Fig. 3 c. Note that the indicated trajectories are much smoother than similar trajectories that one can extract from Fig. 3 a. Finally note that these modifications are not computationally expensive, the complexity of our algorithm is similar to that of Zou and Yan.

Another problem arises when one has to evaluate the efficacy of a skeletonization algorithm. A suitable proto-col is one that can quantitatively and accurately distinguish between different algorithms. Existing comparisons concen-trate mainly on quantitative aspects such as computation time and their ability to preserve the topology and geometric properties of the original image [ 31 , 32 ]. Subjective tests are also used, where a few skeletons are depicted and evaluated according to their visual appearance [ 7  X  9 , 33 , 34 ]. Plamondon et al. [ 35 ] however, compare different algorithms indirectly through their performance as part of an optical character recognition system. Lam [ 32 ] compares the skele-tons with a ground truth in order to determine to what extent the topology of the original image is preserved. Our approach uses the same principle. For each static image in our test set, the dynamic equivalent, referred to as the dynamic counter-part of the image is also recorded and serves as a ground truth for the skeletonization. The details of the comparison between the static skeleton and its dynamic equivalent are nontrivial and is treated in detail in Sect. 3.1 . The result of the evaluation procedure is a local correspondence between the static skeleton and its dynamic equivalent. This corre-spondence allows one to determine local skeleton distortions and leads to a quantitative measure of the efficacy of the skeletonization procedure.

The rest of this paper is organized as follows: Sect. 2 describes our algorithm in detail, and results are presented in Sect. 3 . Section 4 draws some conclusions. 2 Our pseudo-skeletonization algorithm This section describes our algorithm in detail. As mentioned in Sect. 1 , our skeletonization scheme is based on the algorithm by Zou and Yan [ 7 ] henceforth referred to as the Zou X  X an algorithm. Section 2.1 presents a brief overview of the extensions we made to the Zou X  X an algorithm. Section 2.2 describes how the original images are tessel-lated and how to use the computed sub-shape information to derive the centerlines of the images. Section 2.3 shows how to remove skeleton artifacts, and Sect. 2.4 summarizes our algorithm. 2.1 A brief overview of the modifications to the Zou X  X an The key idea of the Zou X  X an approach is to tessellate an image into smaller regions so that regional information can be exploited to identify artifacts. The tessellation is derived from the Delaunay triangulation of the image. The Delaunay triangulation is a triangulation that maximizes the minimum angle over all the constructed triangles [ 36 ]. The Zou X  X an algorithm first identifies the edges that represent the bound-aries of the original image, where edges refer to lines con-necting successive boundary points. These boundary points form the control points for the Delaunay triangulation of the original image. This triangulation enables one to compute a skeleton that follows the centerline of the image [ 7 ]. Regions that comprise artifacts can also be identified through sets of connected triangles and removed.

End regions are defined as regions that contain skeleton lines connecting endpoints with other endpoints or cross-points. An endpoint is a skeleton point connected to only one adjacent skeleton point, whereas a crosspoint is a skel-eton point connected to more than two adjacent skeleton points. Typically, end regions are likely to constitute periph-eral artifacts if they are short in length in comparison with their width. Spurious end regions are simply removed. Inter-section regions contain crosspoints. Multiple crosspoints are joined in a single point by uniting their corresponding inter-section regions, thereby removing intersection artifacts. The term merging is used to describe the process that unites two or more intersection regions. Typically, the directions of skel-eton lines that enter intersection regions are used as basis for calculating whether nearby intersection regions should be merged.

A part of a binary signature is shown in Fig. 4 a. In our algorithm we smooth the image boundaries and compute the corresponding Delaunay triangles from the smoothed bound-aries as indicated in Fig. 4 b. It is shown in Sect. 2.2 that such smoothing greatly reduces artifacts (the Zou X  X an approach does not apply any smoothing). The skeleton (black line) fol-lows the centerline of the boundaries. Figure 4 b also depicts an intersection region (largest triangle in figure) with its cor-responding crosspoint indicated by a grey circle. A typical endpoint is also indicated by the second black circle, where its corresponding end region consists of the connected set of triangles from the triangle containing the crosspoint circle to the triangle containing the endpoint circle.
Two simple examples are also shown in Fig. 5 to illus-trate the basic steps for artifact removal. Figure 5 a depicts the skeleton of an image containing spurious intersection regions (dashed boxes.) Line directions (arrows) are used as basis for merging the two intersection regions. Thus, the two regions in Fig. 5 a are merged into a single intersection region, as shown in Fig. 5 b, removing the artifact between them. The lines that enter this intersection region are interpolated to cross each other in a single crosspoint p , as shown in Fig. 5 c. Figure 5 d shows spurious end regions (dashed boxes) which are removed to obtain the skeleton in Fig. 5 e.

Problems are typically encountered in areas where many intersection regions are located within close proximity. Such regions would typically result from the left-hand side of Fig. 1 a. The Zou X  X an algorithm assumes that lines do not change their orientation after entering an intersection. Due to the nature of human handwriting, especially signatures, this is not always the case. When an image becomes indis-tinct due to multiple crossings in a small region, the lines that enter the intersection regions are too short to make accu-rate estimates of local line directions. In such cases it is not always clear which curves should be connected X  X f the skeletonization algorithm follows a dominant curve along its direction, the wrong curves may be connected, with the result that important trajectories become irretrievably lost. It is therefore important to maintain all possible connec-tions, while smoothing lines as much as possible. In short, one has to avoid merging unrelated intersection regions. This necessitates further refinements to the basic Zou X  X an algo-rithm. Accordingly, we impose additional constraints, based mainly on local line width, introducing the web-like struc-tures mentioned in Sect. 1 .

Figure 6 illustrates how our pseudo skeletonization algorithm deals with complicated regions. Figure 6 ashows a complicated intersection extracted from Fig. 1 a. Figure 1 b illustrates what can happen if merging is not carefully con-trolled in such regions (a typical problematic region for the Zou X  X an algorithm). The dashed arrows 1 and 2 indicate line directions that have been enhanced through the merging pro-cess. However, close inspection of all the line directions in Fig. 1 a reveals that the directions of the straight lines indi-cated by the arrows 3 and 4 are damaged, complicating the extraction of the correct lines in a stroke recovery algorithm. The difficulties of using heuristics in these regions should be clear. The result from our skeletonization algorithm is shown in Fig. 6 c, where merging is controlled by introduc-ing web-like structures to allow more choices of possible line directions. The correct connections can now be extracted by a more comprehensive stroke-recovery algorithm that takes global line directions into account.

Since there is a direct relationship between the noise in the boundaries of an image and the number of artifacts in the image skeleton, we apply a smoothing procedure to the original boundaries as well as the final skeletons. The details of the algorithm are described in the following two sections. 2.2 Shape partitioning Image boundaries are extracted in the first step to tessel-late static handwritten scripts, as illustrated by Fig. 4 .The discrete points that constitute the image boundaries become the control points of a polygon that represents the static script. We refer to the boundary polygon as the approximat-ing polygon of the script. Since these boundaries are noisy (see Fig. 4 a), smoothing is necessary: 1. Various techniques exist to smooth parametric curves. 2. Image boundaries that enclose three or less connected 3. For the sake of simplicity, all boundary points are inter-The effect of boundary smoothing is illustrated in Fig. 7 d X  X , where (d) depicts a static signature. The skeleton computed directly from its boundary is shown in (e), and the skeleton computed from its smoothed boundary in (f). Note the significant reduction of spurs as a result of a smoother boundary. The few remaining spurs are simply removed from the skeleton.

In the next step, we use the boundary points of the static image as control points for a Delaunay triangulation that can be used to compute the centerline (skeleton) of the static image.

The Delaunay triangulation of a set of points P is the straight-line dual of the Voronoi diagram of P . The Voronoi diagram V of P is a uniquely defined decomposition or tes-sellation of a plane into a set of N polygonal cells, referred to as Voronoi polygons [ 29 ]. The polygon cell V i contains one point p i  X  P and all points in the plane that are closer to p i than any other point in P , i.e.,  X  p j  X  P , j = i , q if dist ( q , p i )&lt; dist ( q , p j ) . The edges defining a Voronoi polygon are generated from the intersections of perpendicu-lar bisectors of the line segments connecting any one point to all its nearest neighbors in P [ 29 ]. A typical Voronoi dia-gram V of a set of points (black dots) is shown in Fig. 8 a. The Delaunay triangulation can now be computed from V by connecting all pairs of points that share the same edge [ 37 ], as illustrated in Fig. 8 b.

In order to proceed we need to recall some concepts from [ 7 , 8 , 34 ]:  X  External triangles occur because the Delaunay triangles  X  Internal triangles are the Delaunay triangles inside the  X  External edges are the sides of internal triangles that coin- X  Internal edges are triangle edges inside the approximat- X  Internal triangles having zero, one, two, or three
A primary skeleton , coinciding mostly with the centerline of the original image, is obtained as follows: for N-Ts, the skeleton connects the midpoints of their internal edges. For E-Ts and J-Ts, the skeletons are straight lines connecting their centroids to the midpoints of their internal edges, whereas the skeletons for I-Ts are their centroids. The skeleton derived from internal triangles of Fig. 8 c are shown in Fig. 8 d. 2.3 Removing artifacts Parts of handwritten static scripts that are difficult to unravel, as well as intersection and peripheral artifacts, are identified by means of a parameter  X  , which is the ratio between the width w and length of a ribbon, i.e.,  X  = w , where a ribbon is a set of connected N-Ts between two J-Ts, or between a J-T and an E-T. A long ribbon is identified when  X   X   X  0 , whereas a short ribbon is identified when  X   X   X  0 . Our skeletoniza-tion algorithm consists of seven steps. Different thresholds are used during each of these steps, as empirically optimized using the signatures from the Dolfing database [ 39 , 40 ] and the Stellenbosch dataset developed by Coetzer [ 41 ]. These signatures form our training set and vary significantly in line thickness. A different test set is used for quantitative mea-surements; see Sect. 3.3 . The width w of a ribbon is taken as the trimean length over all internal edges that constitute the ribbon. The trimean , also known as Tukey X  X  trimean or best easy systematic (BES) estimate, is a statistically resis-tant measure of a distribution X  X  central tendency [ 42 ], and is computed as the weighted average of the 25th percentile, twice the 50th percentile and the 75th percentile. The length is the path length of the skeleton associated with the ribbon. Figure 9 a shows a typical ribbon between an E-T and a J-T. The length of the ribbon is computed as the path length of the skeleton line that connects the midpoints of the internal edges from h to i . The width w of the ribbon is given by the trimean of { ab , bc , cd , ce , ef , fg } , where xy = y  X  x and x , y are both 2D boundary coordinates. The algorithm proceeds in several steps: Step 1: Removing spurs. The first step in the skeletonization is to remove all peripheral artifacts remaining after bound-ary smoothing. Following [ 8 ], short spurs belong to sets of connected triangles that are short in comparison with their width; they are removed. Recall that a short ribbon is iden-tified where  X   X   X  0 . In this case,  X  0 = 2 to identify a short ribbon. From this, the J-T in Fig. 9 b becomes an N-T.
The threshold  X  0 depends on the boundary noise X  X ess boundary noise results in shorter spur lengths. Thus, increases as the boundary noise decreases. Figure 9 c and d show the result after Step 1 is applied to Fig. 7 e and f with  X  0 = 2. Note that most of the important image features from Fig. 7 d are preserved in Fig. 9 d, whereas it becomes difficult to calculate  X  0 for Fig. 7 c to simultaneously remove spurs without removing important image features. Clearly, boundary smoothing significantly improves spur removal as spurs are shortened in a natural way, making it easier to com-pute a robust value for  X  0 .
 Step 2: Identifying complicated intersections. Figure 10 a indicates the typical locations of J-Ts, as derived from a com-plicated part in a signature. If many lines cross in a small area, it is difficult, if not impossible, to maintain the integrity of lines, i.e., it is difficult to follow individual ribbons through intersections. The Delaunay triangles enable us to identify region where many J-Ts are within close proximity, as shown in Fig. 10 a. Instead of forcing poor decisions in such compli-cated parts, the primary skeletons are retained in these parts for conventional skeletons. For pseudo skeletons, web-like structures are introduced, resulting in additional intersections to model multiple crossings.

Recall that during the primary skeletonization, the cen-troids of all J-Ts become skeleton points. As mentioned above, it is important to avoid forcing decisions in compli-cated parts of a static script. Hence, for complicated inter-sections in conventional skeletons, the centroids of J-Ts are their final skeleton points. For pseudo skeletons, the primary skeleton points of the J-Ts are removed, and the lines that enter the J-Ts are directly connected. The resulting web-like structures contribute to smoother connections than the orig-inal primary skeleton points. We proceed to discuss the heu-ristic measures employed to identify J-Ts that belong to such complicated regions.

Firstly, J-Ts that are connected to two or three short rib-bons, are labelled complicated J-Ts . In this case,  X  0 = used to identify short ribbons. The primary skeleton points of complicated J-Ts are replaced by lines connecting the mid-points of the J-T internal edges. The same is done for other J-Ts that are connected to complicated J-Ts through short ribbons. This is illustrated in Fig. 10 b, where the J-Ts from Fig. 10 a are numbered, followed by the number of long rib-bons connected to the J-Ts. Note that J-Ts 1 and 7 are also labeled as complicated J-Ts although they are both connected to two long ribbons, as they are connected to complicated J-Ts through short ribbons. For conventional skeletons the primary skeleton points of all complicate J-Ts are retained, as shown in Fig. 10 c. For pseudo skeletons the primary skel-eton points are replaced with web-like structures, as shown in Fig. 10 d. Note that the skeletonization algorithm differs only in Step 2 for our conventional and pseudo skeletons. Step 3: Labeling of skeleton points. The remaining simple J-Ts are either connected to two or three long ribbons. The skeleton points of such simple J-Ts (recall that the primary skeleton selected the centroid) are recalculated following a similar approach to [ 7 ] as briefly described below.
Recalculating the skeleton points of simple J-Ts. Recall from Sect. 2.1 that a crosspoint is a skeleton point that is connected to two or more adjacent skeleton points and is associated with an intersection region. Thus, in our case, the crosspoint p i is the skeleton point associated with J-T i this point p i is initialized to be the centroid of J-T i
The midpoints of internal edges belonging to the first few triangles (we use 13) in all three ribbons connected to a J-T are connected, as shown in Fig. 11 a. The average directions of these curves are calculated. Straight lines are then computed passing through the midpoints of the J-T in the calculated directions, as illustrated by the dashed lines in Fig. 11 b. All the points where the straight lines intersect are calculated and averaged. The crosspoint p i is then replaced by this average intersection point p i , as indicated by the circle in Fig. 11 b.
Recalculation of skeleton points that are out of bounds. In some cases p i falls outside the J-T i and all the ribbons connected to J-T i i.e., in the image background. To preserve local line directions, p i is relocated to an appropriate triangle closest to it. Specifically, for each ribbon j connected to J-T i , the nearest triangle T ij to p i is computed. Thus, T can be any triangle that forms part of the j th ribbon con-nected to J-T i for j  X  X  1 , 2 , 3 } . For each T ij , the angle computed, where  X  where p ij is the centroid of T ij . The triangle T ( ij ) sponding to the minimum  X  ij is chosen as the triangle that should contain p i .If T ( ij ) min is an E-T, p i , the replacement of p i , is the centroid of the E-T. If T ( ij ) min is an N-T, p centroid of the midpoints of the N-T X  X  two internal edges. In summary, the skeleton point p i for each simple J-T i is recalculated as p i = p i ,or p i = p i (if p i is out of bounds).
Associating a ribbon with each crosspoint. Now that the skeleton point of J-T i is known (again denoted by p i ), one of the three ribbons attached to J-T i is associated with p future use. The distances between p i and the midpoints of J-T i  X  X  internal edges are calculated. The ribbon with the near-est terminating internal edge j is associated with p example, the ribbon connected to edge x in Fig. 12 a is asso-ciated with p i .
 Step 4: Removing intersection artifacts ( criterion 1 ) . This step identifies which of the simple J-Ts contribute to inter-section artifacts, by adapting some of the criteria used by [ 7 ]. A J-T i is labelled unstable , i.e., contributing to an arti-fact, if its skeleton point p i lies outside it. In this case, the sequence of connected triangles from J-T i up to the triangle in which p i falls, are removed, thereby merging them into a single polygon. Figure 12 a illustrates that J-T i is unsta-ble since its skeleton point p i falls outside it. The intersec-tion region resulting from the removal of all the triangles up to p i is shown in Fig. 12 b. Note that p i is now associated with a pentagon (five-sided polygon rendered as dotted lines). The skeleton is now updated by connecting the midpoints of the two remaining internal edges of the original J-T i to p The directions of the lines that enter the intersection region (polygon) are more similar to the directions of the skeleton lines connecting them to p i than in Fig. 12 a. Thus, in this case, the transitions to crosspoints are smoothed. The ribbon associated with p i is also shorter than in Fig. 12 a as its length is reduced by the number of triangles that were removed.
An extension of Step 4 is illustrated in Fig. 13 . The primary skeletons of J-T 1 and J-T 2 in Fig. 13 a must be joined to remove the intersection artifact ( solid line between J-T J-T 2 ). In this case, the skeleton point p 2 of J-T 2 falls inside J-T 1 , or similarly, p 1 falls inside J-T 2 . We therefore unite the two J-Ts into a four-sided polygon ( dotted rectangle ) with skeleton point p ( cir cle ), as shown in Fig. 13 b, where p ( p
Note that the primary functions of Step 4 are to improve y-shaped patterns and to fix simple x-shaped patterns that contain artifacts. More intersection artifacts are identified and removed during the next step.
 Step 5: Removing intersection artifacts ( criterion 2 ) . We now make use of the information about the location of skel-eton points and their associated ribbons obtained in Step 3. After the application of Step 4, it often happens that the same short ribbon (  X  0 = 2) is associated with two crosspoints p and p 2 , as shown in Fig. 14 a. In this case, the two intersection regions and the ribbon between them are united into a new intersection region, as shown in Fig. 14 b. Note that, after the application of Step 4, a ribbon that is connected to an inter-section region (triangle/polygon) must terminate in either an intersection region or an E-T. The skeleton point for the new intersection region (dotted polygon) is p = ( p 1 + p 2 )/
In addition, three J-Ts must sometimes be merged, as illus-trated in Fig. 15 . Figure 15 a depicts three J-Ts and their skel-eton points p 1 , p 2 and p 3 . Conditions for such a merge occur if, according to Step 4, J-T 2 and J-T 3 must be united, whereas according to Step 5, J-T 1 and J-T 2 must be united. In such cases, a new intersection region is created with a single skel-eton point p = ( p 1 + p 2 + p 3 )/ 3, as shown in Fig. 15 b. Step 6: Removing spurs by reapplying Step 1. The removal of intersection artifacts may shorten some of the ribbons since triangles are typically removed to relocate crosspoints. The occurrence of peripheral artifacts can consequently be re-evaluated. If a crosspoint p is associated with a new short ribbon (  X  0 = 2 . 5) and the short ribbon is connected to an intersection region and an E-T, the short ribbon is removed. Step 7: Final skeletons are smoothed using Chaikin X  X  corner-cutting subdivision method [ 43 , 44 ]. This smoothing scheme treats the points that constitute a parametric curve as control points of a polygon and iteratively  X  X uts X  the corners of the polygon while doubling the numbers of points that consti-tute the curve. It is shown by Lane and Riesenfeld [ 44 ] that Chaikin X  X  algorithm converges to a quadratic B-spline curve. Due to Chaikin X  X  geometric approach to smooth curves, a wide variety of shapes can be handled easily and efficiently, e.g., straight lines and closed curves are treated in the same manner. Skeletons are represented as sets of curves that are connected to endpoints and/or crosspoints, as well as closed curves (e.g., the character  X  X  X  which contains no crosspoints or endpoints.) Chaikin X  X  corner-cutting subdivision method is then applied to each curve. 2.4 Algorithm summary Our skeletonization algorithm is briefly summarized as follows: 1. The boundaries of static handwritten images are 2. Peripheral artifacts are removed by removing short 3. Parts of the signature that are difficult to unravel are iden-4. Unstable J-Ts and other intersection regions that con-5. The last peripheral artifacts are identified after the recal-6. Skeletons are smoothed using a corner-cutting subdivi-3 Results As mentioned in Sect. 1 , a specific application of our skel-etonization algorithm is to serve as a preprocessing step for extracting dynamic information from static handwritten scripts. To assess the efficacy of our skeletonization algo-rithm we therefore measure the correspondence between a skeleton and its dynamic counterpart. For testing purposes we collected a signature database, US-SIGBASE [ 11 ]. More details are given in Sect. 3.1 , see also [ 13 ]. These signatures are available at [ 45 ].

All signatures in the US-SIGBASE were obtained by sign-ing on paper placed on a digitizing tablet. A paper signature and its dynamic counterpart were therefore recorded simul-taneously. The problem is to compare the skeleton, derived from the static image, with its dynamic counterpart. Since the skeleton and its dynamic counterpart are obtained through different processes, at different resolutions, a direct, point-wise comparison is not possible. In addition, the skeleton is derived from a static image that typically contains various artifacts and/or other defects in the image such as broken lines caused by dry ink or too little pressure. A technique to compare a skeleton with its dynamic counterpart is described in Sect. 3.2 , allowing us to quantify the performance of our algorithm, as described in Sect. 3.3 . Examples of typical skel-etons extracted with our algorithm are presented in Sect. 3.4 . 3.1 The US-SIGBASE database The US-SIGBASE database is used to test our algorithm. This database consists of 51 static signatures and their dynamic counterparts. All signatures were recorded on paper placed on a Wacom UD-0608-R digitizing tablet. Each indi-vidual was instructed to sign within a 50 mm  X  20 mm bound-ing box using a medium-size ball-point pen compatible with the digitizing tablet. The paper signatures were scanned as gray-scale images at 600 dpi and then binarized. To reduce spurious disconnections (broken lines), the scanner was set to a very sensitive setting which introduced significant back-ground noise. The noise was reduced by applying a median ing, the document was binarized with a global threshold set by the entropy method described in [ 29 ]. The line widths of the resulting static signatures vary between eight and twelve pixels in parts where the lines do not intersect.

Figure 16 a depicts a typical grey-scale grid extracted from a page in US-SIGBASE. The document X  X  binarized version, after filtering, is shown in Fig. 16 b. Note that the signatures in Fig. 16 b appear thicker than in Fig. 16 a. This is due to the binarization and has the advantage of reducing the number of broken lines, but makes dense intersections harder to unravel.
One expects the dynamic counterparts to be perfect skelet-onizations of the static images. After the recording procedure we therefore aligned all the images with their dynamic coun-terparts, see [ 46 ]. The alignment presents a few difficulties. Firstly, some individuals seem to be unable to sign within a grid X  X  constraints, writing not only over the grid lines, but also over neighboring signatures. In these cases, it is impos-sible to extract the individual signatures without corrupt-ing them. Secondly, the recording devices introduce several different types of noise, corrupting the data [ 47  X  49 ]. (We use a Hewlett X  X ackard (HP) scanjet 5470C.) Thirdly, digitizing tablets fails to capture signatures if insufficient pressure is applied, leading to spurious broken lines. Finally, differences in orientations between the tablet, the paper on the tablet, and the paper on the scanner X  X  surface need to be considered. If the paper shifts on the tablet while the signature is recorded, discrepancies between the static signature and dynamic coun-terpart occur that is difficult to correct afterwards. The align-ment was obtained by a brute-force, exhaustive search over various similarity transforms between the static signatures and their dynamic counterparts. If no satisfactory alignment was obtained the signature was discarded. We emphasize that this is for evaluation purposes only where we need a ground-truth in order to obtain a quantitative comparison. 3.2 Comparing a skeleton with its dynamic counterpart To establish a local correspondence between a skeleton and its dynamic counterpart, a similar approach to Nel et al. [ 11  X  13 ] is followed. That is, a hidden Markov Model (HMM) (see [ 50 , 51 ], for example) is constructed from the skeleton. Matching the dynamic counterpart to the HMM of the static signature, using the Viterbi algorithm, provides the optimal pointwise correspondence between the static skeleton and its dynamic counterpart.

The dynamic counterpart consists of a single parametric curve as well as the pen pressure, used to identify pen-up events. Thus, each coordinate x t of the dynamic counterpart X =[ x 1 , x 2 ,..., x T ] consists of three components X  X wo pen position components ( x t , y t ) written as x 1 , 2 t pressure component x 3 t , where x 3 t = 0 at pen-up events, and x yields a state sequence s =[ s 1 ,..., s T ] representing the optimal match between the HMM of the static skeleton and its dynamic counterpart X . Following [ 11  X  13 ], each state in the HMM is associated with a unique skeleton point p i for i { 1 ,..., N } , where N is the number of skeleton points. Thus, the final state sequence s =[ s 1 ,..., s T ] from the Viterbi algorithm yields the pointwise correspondence between the skeleton points P =[ p s ( 1 ) , p s ( 2 ) ,..., p s ( T ) ton X  X  dynamic counterpart X =[ x 1 , x 2 ,..., x T ] , where p is the skeleton point associated with state s ( t ) .
The local correspondence computed by the Viterbi algorithm can now be used to accurately determine the posi-tions of local distortions in the skeleton, as shown in the next section. 3.3 Quantitative results To generate quantitative results, the skeletons of the static scripts are computed, as described in Sects. 2.2 , 2.3 . Opti-mal pointwise correspondences are then established between the skeletons and their dynamic counterparts using the pro-cedure described in Sect. 3.2 . For each corresponding point, the difference in pen positions is computed as e where x 1 , 2 t indicates the first two components ( x and y coor-dinates) of the t th coordinate of the dynamic counterpart x eton coordinate as computed by the Viterbi algorithm for t =[ 1 ,..., T ] . Note that e
Similarly, the difference in pen direction (normalized velocity) is computed as v = x for all values of t =[ 2 ,..., T ] where the pressure is non-zero, i.e., x 3 t = 1 = x 3 t  X  1 . Note that the maximum dis-tance between two successive coordinates is x v  X  X  0 , 2 } .

To obtain a global comparison, the median d P of all the values for e t from ( 2 ) is calculated. The value of d P indication of how closely the position coordinates of the skeleton and its dynamic counterpart are aligned. Similarly, the median d D of all the values for v t from ( 3 ) is calculated. This provides an indication of how well the local line direc-tions of the skeleton and its dynamic counterpart are aligned. To obtain an indication of the number of intersection artifacts in the skeleton, the number of crosspoints n C in the conven-tional skeleton is computed and expressed as a percentage of the number of crosspoints in the dynamic counterpart. This is then compared with the number of crosspoints that occur in a skeleton obtained from the standard thinning algorithm described in [ 28 ]. Although by no means state-of-the-art, it is readily available and similar algorithms are used by many existing temporal recovery techniques. It should therefore provide a reasonable base line for comparison. In addition, being a thinning as opposed to a skeletonization algorithm, it should produce less peripheral artifacts than standard skel-etonization algorithms. Final results are computed by taking the averages and standard deviations of d P , d D and n C the 51 users.

We choose the thinning algorithm described in [ 28 ]as our baseline, which is compared with our pseudo skeleton as shown in Table 1 . From the values of d P and  X ( d P ) clear that, on average, there is little to choose between the two methods as far as pen position is concerned. Note how-ever, the difference in the line directions, as measured by d Recalling that the maximum value of d D is 2.0. Thus, the pseudo skeleton represents a 6% absolute improvement and a relative improvement of 60%.

The average number of crosspoints n C in the baseline skeletons are compared to that of our conventional skele-tons in Table 2 , where n C is expressed as a percentage of the number of crosspoints in the dynamic counterpart. There are considerably less crosspoints in our conventional skeletons as compared with the baseline skeletons, and the standard deviation is also considerably less. This indicates that our conventional skeletons contain significantly less intersection artifacts compared to the baseline skeletons.

An experiment to determine the sensitivity to pen width was conducted with synthetic data. The dynamic counter-parts of all 51 users were artificially thickened using a disc-shaped structuring element in a simple dilation operation. Using such a synthetic set ensures that the line widths of the signatures are similar for each dilation iteration, while also preserving the exact dynamic counterparts. The results for two significantly different pen widths are shown in Table 3 . The average number of crosspoints in our conventional skele-tons is closer to that of the dynamic counterparts as compared with the baseline skeletons for all the experiments. Note that the standard deviations  X ( d P ) ,  X ( d D ) and  X ( n C ) increase for our pseudo skeletons for the thicker signatures. Thicker sig-natures in general have more complicated regions with the result that fewer artifacts are removed.

In [ 11 ] the same baseline and pseudo-skeletonization algo-rithms were used for experimental evaluation as in this paper. In that paper, the skeletonization algorithms were used as preprocessing steps before unraveling static signatures and we specifically exploited the continuity criterion of motor-controlled pen motions at relatively simple crossings by unravelling them prior to introducing a global optimizer to unravel the rest. Those results confirm the findings in the pres-ent paper, i.e., the present algorithm provides smoother skele-tons with less intersection artifacts compared to the baseline method. Animations illustrating the local correspondences between the skeletons and their dynamic counterparts can be found on our website [ 45 ]. 3.4 Examples In this section, we present a number of typical examples.
The value of  X  was empirically determined through visual inspection where we looked for visual attractiveness (smooth skeletons with few artifacts) while preserving all the possi-ble trajectories. Two datasets were used: the first database was collected by Dolfing and consists of dynamic signatures, where static images were obtained by dilating the dynamic signatures with a 3  X  1 structuring element of ones. The result-ing static images were therefore free from any contamina-tion such as smears on the paper and scanner noise. The second dataset consists of scanned static signatures captured with regular pens, as collected by Coetzer [ 41 ]. Although the signatures vary considerably as far as line width and boundary noise levels are concerned, the same threshold value for  X  as presented in Sect. 2.3 , is used in all cases. One might con-sider computing  X  more robustly through an automatic opti-mization algorithm. The algorithm in [ 13 ] can, e.g. be used to determine the accuracy of a pen trajectory that is recon-structed from a static skeleton, given a specific value of
Examples of the original signatures and their skeletons from the test signature set are shown in Fig. 17 .
Signatures 1, 10 and 11 are examples of signatures that do not have complicated intersections and are free from any web-like structures in their pseudo skeletons. The pseudo and conventional skeletons for these signatures are therefore the same. Recall that the first smoothing stages of our algorithm reduces artifacts, as shown in Fig. 9 . Note, however, how local line directions are improved in the skeleton of signa-ture 1 after the application of Steps 2 X 6 as compared with Fig. 9 d. Furthermore, web-like structures preserve all possi-ble connections while smoothing local directions in regions of multiple intersections (regions that are difficult to unravel). The pseudo skeletons of signatures 5 and 8 illustrate that our algorithm is able to identify these difficult parts (evident from the webs on the left-hand parts), whereas intersection and peripheral artifacts are corrected in parts that are rel-atively straightforward to unravel (right-hand parts.) Note that the conventional skeletons retain the topology of the original images, whereas the pseudo skeletons have web-like structures, i.e. more  X  X oles X , but smoother intersections in complicated regions. Unravelling algorithms can make use of the smoother paths and the additional local transitions. Unused web-like structures can be discarded after global optimization.

The thicker the pen width, the more difficult it becomes to unravel a static signature. A complicated thick signature from US-SIGBASE is depicted in Fig. 18 a, with its dynamic counterpart (shown in white) superimposed on it in Fig. 18 b. Note that the dynamic counterpart is well aligned with the static image, using the technique described in Sect. 3.1 .The pseudo skeleton (with the web-like structures), is shown in Fig. 18 c. Our quantitative evaluation protocol described in Sect. 3.2 is used to compute a pointwise correspondence between the dynamic signature in Fig. 18 b and the skele-toninFig. 18 c. The resulting skeleton lines that match the dynamic counterpart according to this protocol are shown in Fig. 18 d. Note that some web-like structures are discarded compared to Fig. 18 c as part of the unravelling process. 4 Conclusions The main purpose of our modifications to the skeletoniza-tion algorithm of Zou, Yan and Rocha [ 7 , 34 ], is to produce a skeletonization that is close, ideally identical, to the pen trajectory that produced the handwritten script. Three main contributions of these modifications are: 1. It is shown that smoothing of image boundaries prior 2. It is shown how one can modify the conventional skele-3. An evaluation protocol is introduced that allows a detailed References
