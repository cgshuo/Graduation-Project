 Raymond J. Mooney MOONEY @ CS . UTEXAS . EDU Children acquire language through exposure to linguistic input in the context of a rich, relevant, perceptual envi-ronment. By connecting words and phrases to objects and events in the world, the semantics of language is grounded in perceptual experience (Harnad, 1990). Ideally, a ma-chine learning system would be able to acquire language in a similar manner without human supervision. As a step in this direction, we present a commentator system that can describe events in a simulated soccer game by learn-ing from sample human commentaries paired with the sim-ulation states. A screenshot of our system with generated commentaries is shown in Figure 1.
 Although there has been some interesting computational work in grounded language learning (Roy, 2002; Bailey et al., 1997; Yu &amp; Ballard, 2004), most of the focus has been on dealing with raw perceptual data and the com-plexity of the language involved has been very modest. To help make progress, we study the problem in a simu-lated environment that retains many of the important prop-erties of a dynamic world with multiple agents and ac-tions while avoiding many of the complexities of robotics and vision. Specifically, we use the Robocup simulator (Chen et al., 2003) which provides a fairly detailed physi-cal simulation of robot soccer. While several groups have constructed Robocup commentator systems (Andr  X  e et al., 2000) that provide a textual natural-language (NL) tran-script of the simulated game, their systems use manually-developed templates and are incapable of learning. Our commentator learns to semantically interpret and gen-erate language in the Robocup soccer domain by observing an on-going commentary of the game paired with the dy-namic simulator state. By exploiting existing techniques for abstracting a symbolic description of the activity on the field from the detailed state of the physical simulator (Andr  X  e et al., 2000), we obtain a pairing of natural language with a symbolic description of the perceptual context in which it was uttered. However, such training data is highly ambiguous because each comment usually co-occurs with several events in the game. We integrate and enhance ex-isting methods for learning semantic parsers and NL gen-erators (Kate &amp; Mooney, 2007; Wong &amp; Mooney, 2007a) in order to learn to understand and produce grounded lan-guage from such ambiguous training data. Systems for learning semantic parsers induce a function that maps NL sentences to meaning representations (MRs) in some formal logical language. Existing work has fo-cused on learning from a supervised corpus in which each sentence is manually annotated with its correct MR (Mooney, 2007). Such human annotated corpora are ex-pensive and difficult to produce, limiting the utility of thi s approach. The systems described below assume they have access to a formal context-free grammar, called the mean-ing representation grammar (MRG), that defines the MR language (MRL). 2.1. KRISP and KRISPER K
RISP (Kate &amp; Mooney, 2006) uses SVMs with string ker-nels (Lodhi et al., 2002) to learn semantic parsers. For each production in the MRG, the system learns an SVM string classifier that recognizes the associated NL words or phrases. The resulting suite of classifiers is then used to construct the most probable MR for a complete NL sen-tence. Given the partial matching provided by string ker-nels and the over-fitting prevention provided by SVMs, K
RISP has been experimentally shown to be robust to noisy training data.
 K
RISPER (Kate &amp; Mooney, 2007) is an extension to K RISP that handles ambiguous training data, in which each sen-tence is annotated only with a set of potential MRs, only one of which is correct. It employs an iterative approach analogous to EM that improves upon the selection of the correct NL X  X R pairs in each iteration. In the first itera-tion, it assumes that all of the MRs paired with a sentence are correct and trains K RISP with the resulting noisy su-pervision. In subsequent iterations, K RISPER uses the cur-rently trained parser to score each potential NL X  X R pair, selects the most likely MR for each sentence, and retrains the parser. In this manner, K RISPER is able to learn from the type of weak supervision expected for a grounded lan-guage learner exposed only to sentences in ambiguous con-texts. However, the system has previously only been tested on artificially corrupted or generated data. 2.2. WASP W
ASP learns semantic parsers using statistical machine translation (SMT) techniques (we use the Wong &amp; Mooney (2007b) version). It induces a probabilistic synchronous context-free grammar (PSCFG) (Wu, 1997) to translate NL sentences into logical MRs using a modification of recent methods in syntax-based SMT (Chiang, 2005). Since a PSCFG is symmetric with respect to input/output, the same learned model can also be used to generate NL sentences from formal MRs. Thus, W ASP learns a PSCFG that sup-ports both semantic parsing and natural language gener-ation. Since it does not have a formal grammar for the NL, the generator also learns an n -gram language model for the NL and uses it to choose the overall most probable NL translation of a given MR using a noisy-channel model (Wong &amp; Mooney, 2007a). To train and test our system, we assembled human-commentated soccer games from the Robocup simulation league (www.robocup.org). Since our focus is language learning not computer vision, we chose to use simulated games instead of real game video to simplify the extrac-tion of perceptual information. Symbolic representations of game events were automatically extracted from the sim-ulator traces by a rule-based system. The extracted events mainly involve actions with the ball, such as kicking and passing, but also include other game information such as whether the current playmode is kickoff, offside, or cor-ner kick. The events are represented as atomic formulas in predicate logic with timestamps. These logical facts const i-tute the requisite MRs, and we manually developed a sim-ple MRG for this formal semantic language.
 For the NL portion of the data, we had humans commen-tate games while watching them on the simulator. The com-mentators typed their comments into a text box, which were recorded with a timestamp. To construct the final ambigu-ous training data, we paired each comment with all of the events that occurred five seconds or less before the com-ment was made. A sample set of ambiguous training data is shown in Figure 2. Note that the use of English words for predicates and constants in the MR is for human read-ability only, the system treats these as arbitrary conceptu al tokens and must learn their connection to English words. We annotated a total of four games, namely, the finals for the Robocup simulation league for each year from 2001 to 2004. Summary statistics about the data are shown in Ta-ble 1. The 2001 final has almost twice the number of events as the other games because it went into double overtime. For evaluation purposes only, a gold-standard matching was produced by examining each comment manually and selecting the correct MR if it exists. The bold lines in Fig-ure 2 indicate the correct matches. Notice some sentences do not have correct matches (about one fifth of our data). For example, the sentence  X  X urple team is very sloppy to-day X  cannot be represented in our MRL and consequently does not have a corresponding correct MR. On the other hand, in the case of the sentence  X  X ink11 makes a long pass to Pink8 X , the correct MR falls outside the 5-second win-dow. For each game, Table 1 shows the total number of NL sentences, the number of these that have at least one recent extracted event to which it could refer, and the number of these that actually do refer to one of these recent extracted events. The maximum, average, and standard deviation for the number of recent events paired with each comment is also given. While existing systems are capable of solving parts of the sportscasting problem, none of them are able to perform the whole task on their own. We introduce three new end-to-end systems below which are able to learn from the am-biguous supervision in our training data and generate com-mentaries on unseen games. 4.1. WASPER Since our primary goal is to learn a sportscaster rather than a parser, we use W ASP to learn a system that can also generate NL from MRs produced by the perceptual sys-tem. However, W ASP requires unambiguous training data which is not available for our domain. Therefore, we ex-tend W ASP using EM-like retraining similar to K RISPER to handle ambiguously annotated data, resulting in a system we call W ASPER . In general, any system that learns se-mantic parsers can be extended to handle ambiguous data as long as it can produce confidence levels for given NL X  MR pairs. 4.2. KRISPER-WASP K
RISP has been shown to be superior to W ASP at handling noisy training data (Kate &amp; Mooney, 2006). Consequently, we can expect K RISPER  X  X  parser to outperform W ASPER  X  X  because EM-like training on ambiguous data initially cre-ates a lot of noisy, incorrect supervision. Even if the aver-age number of possible MRs per sentence is only 2, it still results in at least 50% noise in the training data in the first iteration. However, K RISPER cannot learn a language gen-erator, which is necessary for our sportscasting task. As a result, we create a new system called K RISPER -W ASP that is both good at disambiguating the training data and capa-ble of generation. We first use K RISPER to train on the ambiguous data and produce a disambiguated training set by using its prediction for the most likely MR for each sen-tence. This unambiguous training set is then used to train W
ASP to produce both a parser and a generator. 4.3. WASPER-GEN In both K RISPER and W ASPER , the criterion for selecting the best NL X  X R pairs during retraining is based on maxi-mizing the probability of parsing a sentence into a particu-lar MR. However, since W ASPER is capable of both parsing and generation, we could alternatively select the best NL X  MR pairs by evaluating how likely it is to generate the sen-tence from a particular MR. Thus, we built another version of W ASPER (W ASPER -G EN ) that disambiguates the train-ing data in order to maximize the performance of genera-tion rather than parsing. It uses a generation-based score rather than a parsing-based score to select the best NL X  X R pairs. Specifically, an NL X  X R pair ( n, m ) is scored by using the current trained generator to generate an NL sen-tence for m and then comparing the generated sentence to n to compute the NIST score. NIST score is a machine translation (MT) metric that measures the precision of a translation in terms of the proportion of n -grams it shares with a human translation (Doddington, 2002). It is also used to evaluate NL generation. Another popular MT met-ric is BLEU score (Papineni et al., 2002) but we found it inadequate for our domain because it overly penalizes translations shorter than the target sentences. Most of our generated commentaries are shorter than the human com-mentaries due to the fact that humans are more verbose and many details of the human descriptions are not represented by our MRL. 4.4. Learning for Strategic Generation A language generator alone is not enough to produce a sportscast. In addition to knowing how to say something, one must also know what to say. A sportscaster must also choose which events to describe. In NLP, deciding what to say is called strategic generation .
 We developed a simple method for learning which events to describe. For each event type (i.e. for each predicate lik e pass , or goal ), the system uses the training data to esti-mate a probability that it is mentioned by the sportscaster. Given the gold-standard NL X  X R matches, this probability is easy to estimate; however, the learner does not know the correct matching. Instead, the system must estimate the probabilities from the ambiguous training data. We com-pare two basic methods for estimating these probabilities. The first method uses the inferred NL X  X R matching pro-duced by the language-learning system. The probability of commenting on each event type, E percentage of events of type E some NL sentence.
 The second method, which we call Iterative Generation Strategy Learning (IGSL), uses a variant of EM, treating the matching assignments as hidden variables, initializin g each match with a prior probability, and iterating to im-prove the probability estimates of commenting on each event type. Unlike the first method, IGSL uses MRs not associated with any sentences explicitly in training. Algo -rithm 1 shows the pseudocode. Each sentence accounts for at most one occurrence of an event being commented (some comments do not correspond to any MRs), so we enforce that the counts associated with a sentence add up to exactly one. In the initial iteration, every possible match gets as-signed a weight inversely proportional to its amount of am-biguity. Thus, a sentence associated with five possible MRs erations, we use the learned estimates for each event type to assign weights to the edges, again normalizing to make sure that the weights of the edges coming out of each sen-tence sum to one.
 To generate a sportscast, we first use the learned probabil-ities to determine which events to describe. For each time step, we only consider commenting on the event with the highest probability. The system then generates a comment for this event stochastically based on the estimated proba-bility for its event type.
 Algorithm 1 Iterative Generation Strategy Learning input event types E = { E of occurrences of each event type totalCount ( E tences S and their associated sets of meaning represen-tations M R ( s ) , output probabilities of commenting on each event type
P r ( E i ) for event type E end for repeat until Convergence or MAX ITER reached This section presents experimental results on the Robocup data for four systems: K RISPER , W ASPER , K RISPER W
ASP , and W ASPER -G EN . To better gauge the effect of ac-curate ambiguity resolution, we also include results of un-modified W ASP . Since W ASP requires unambiguous train-ing data, we randomly pick a meaning for each sentence from its set of potential MRs. Finally, we also include the result of W ASP trained using gold matching which con-sists of the correct NL X  X R pairs annotated by a human. This represents an upper-bound on what our systems could achieve if they disambiguated the training data perfectly. We evaluate each system on three tasks: matching, pars-ing, and generation. The matching task measures how well the systems can disambiguate the training data. The pars-ing and generation tasks measure how well the systems can translate from NL to MR, and from MR to NL, respectively. Since there are four games in total, we trained using all possible combinations of one to three games, and in each case, tested on the games not used for training. Results were averaged over all train/test combinations. We evalu-ated matching and parsing using F-measure, the harmonic mean of recall and precision. Precision is the fraction of th e system X  X  annotations that are correct. Recall is the fracti on of the annotations from the gold-standard that the system correctly produces. Generation is evaluated using NIST scores which roughly estimates how well the produced sen-tences match with the target sentences. 5.1. Matching NL and MR Since handling ambiguous training data is an important as-pect of grounded language learning, we first evaluate how well the various systems pick the correct NL X  X R pairs. Figure 3 shows the F-measure for identifying the correct set of pairs for the various systems. W ASPER does bet-ter than random matching, but worse than the other two systems. While we expected K RISPER to perform better since it is more adept at handling noisy data, it is some-what surprising that W ASPER -G EN does about the same. A potential explanation is that W ASPER -G EN avoids making certain systematic errors typical of the other systems. Thi s is discussed further in section 5.3. 5.2. Semantic Parsing Next, we present results on the accuracy of the learned se-mantic parsers. Each trained system is used to parse and produce an MR for each sentence in the test set that has a correct MR in the gold-standard matching. A parse is con-sidered correct if and only if it matches the gold standard exactly. Parsing is a fairly difficult task because there is usually more than one way to describe the same event. For example,  X  X layer1 passes to player2 X  can refer to the same event as  X  X layer1 kicks to player2. X  Thus, accurate pars-ing requires learning all the different ways people describ e an event. Synonymy is not limited to verbs. In our data,  X  X ink1 X ,  X  X inkG X  and  X  X ink goalie X  all refer to player1 on the pink team. Since we are not providing the systems with any prior knowledge, parsers have to learn all these differ-ent ways of referring to the same entity.
 Results are shown in Figure 4, and, as expected, follow the matching results. Systems that did better at disambiguat-ing the training data also did better on parsing since their supervised training data is less noisy. When trained on 3 games, K RISPER does the best since it is most effective at handling the noise in the final supervised data. However, it tends to do worse than the other systems when given less training data. 5.3. Generation The third evaluation task is generation. All of the W ASP based systems are given each MR in the test set that has a gold-standard matching NL sentence and asked to generate an NL description. The quality of the generated sentence is measured by comparing it to the gold-standard using NIST scoring.
 This task is easier than parsing because the system only needs to learn one way to accurately describe an event. This property is reflected in the results, shown in Figure 5, where even the baseline system W ASP does fairly well, outper-forming W ASPER and K RISPER -W ASP . As the number of event types is fairly small, only a relatively small number of correct matchings is required to perform this task well as long as each event type is associated with a correct sentence pattern more often than any other sentence patterns. Con-sequently, it is far more costly to make systematic errors as is the case for W ASPER and K RISPER -W ASP .
 Even though systems such as W ASPER and K RISPER -W
ASP do fairly well at disambiguating the training data, the mistakes they make in selecting the NL X  X R pairs of-ten repeat the same basic error. For example, a bad pass event is often followed by a turnover event. If initially the system incorrectly determines that the comment  X  X layer1 turns the ball over to the other team X  refers to a bad pass , it will parse the sentence  X  X layer2 turns the ball over to the other team X  as a bad pass as well since it just reinforced that connection. Even if the system trains on a correct ex-ample where a bad pass is paired with the linguistic input  X  X layer1 made a bad pass X , it does not affect the parsing of the first two sentences and does not correct the mistakes. As a result, a bad pass becomes incorrectly associated with the sentence pattern  X  X omeone turns the ball over to the other team. X  On the other hand, W ASPER -G EN does the best due to the imbalance between the variability of natural language com-ments and the MRs. While the same MR will typically oc-cur many times in a game, the exact same comments are almost never uttered again. This leads to two performance advantages for W ASPER -G EN .
 W
ASPER -G EN avoids making the same kind of system-atic mistakes as W ASPER and K RISPER -W ASP . Follow-ing the previous example, when W ASPER -G EN encounters the correct matching for bad pass , it learns to associate bad passes with the correct sentence pattern. When it goes back to those first two incorrect pairings, it will likely cor -rect its mistakes. This is because the same MR bad pass is present in all three examples. Thus, it will slowly move away from the incorrect connections. Of course, parsing and generation are symmetrical processes, so using gener-ation to disambiguate data has its own problems. Namely, it is possible to converge to a point where many events gener-ates the same natural language description. However, since there is much more variability in natural language, it is ver y unlikely that the same sentence pattern will occur repeat-edly, each time associated with different events. Another performance advantage of W ASPER -G EN can be found by looking at the objective differences. Systems such as W ASPER and K RISPER -W ASP which use parsing scores, try to learn a good translation model for each sen-tence pattern. On the other hand, W ASPER -G EN only tries to learn a good translation model for each MR pattern. Thus, W ASPER -G EN is more likely to converge on a good model as there are fewer MR patterns than sentence pat-terns. However, it can be argued that learning good transla-tion models for each sentence pattern will help in producing more varied commentaries, a quality that is not captured by the NIST score. 5.4. Strategic Generation The different methods for learning strategic generation ar e evaluated based on how often the events they describe co-incide with those the human decided to describe in the test data. For the first method, results from using the inferred matchings produced by K RISPER , W ASPER , K RISPER -W
ASP , and W ASPER -G EN as well as the gold and random matching for establishing baselines are all presented in Fi g-ure 6. From the graph, it is clear that IGSL outperforms learning from the inferred matchings and actually performs at a level close to using the gold matching. However, it is important to note that we are limiting the potential of learn -ing from the gold matching by using only the predicates to decide whether to talk about an event.
 The top scoring predicates from IGSL as well as the best result from using inferred matchings, W ASPER -G EN , are shown in Table 2. While both systems learn to talk about frequent events such as passing, W ASPER -G EN does poorly on rare, but significant events such as goal scoring. This is because W ASPER -G EN saw those events very rarely in training and did not learn to correctly match them to sen-tences. It is worth noting that IGSL learns a higher prob-ability for events in general. This improves its recall and hurts its precision. However, since many of its top-ranked events such as goals are rare, the overall quality is main-tained without becoming overly verbose. Therefore, we used IGSL for the human evaluations below. 5.5. Human Evaluation Automatic evaluation of generation is an imperfect approx-imation of human assessment at best. Moreover, automati-cally evaluating the quality of an entire generated sportsc ast is even more difficult. Consequently, we recruited four fluent English speakers with no previous experience with Robocup or any of our systems to serve as human judges. We compared their subjective evaluations of human and machine generated sportscasts. Each judge was given 8 clips of simulated game video along with subtitled com-mentaries. The 8 clips use 4 game segments of 2 minutes each, one from each of the four games. Each of the 4 game segments is shown twice, once with human commentary and once with generated commentary. We use IGSL to de-termine the events to comment on and use W ASPER -G EN (our best performing system for generation) to produce the commentaries. The system was always trained on three games, leaving out the one from which the test segment was extracted. The videos are shown in random order with the human and machine commentaries of a segment flipped be-tween judges to ensure no consistent bias toward segments being shown earlier or later. We asked the judges to score the commentaries using the following metrics:
Score Fluency Correctness Ability Fluency and semantic correctness, or adequacy, are stan-dard metrics in human evaluations of NL translations and generations. Fluency measures how well the commentaries are structured, including syntax and grammar. Semantic correctness indicates whether the commentaries actually describe what is happening in the game. Finally, sportscast -ing ability measures the overall quality of the sportscast. This includes whether the sportscasts are interesting and flow well. The scores are averaged over all four games and across all the judges. Table 3 shows the results. While human commentaries are clearly superior to the ma-chine X  X , the largest difference between the average scores is only 0.7. Moreover, the judges indicated that they were able to understand and follow the generated commentaries without trouble. Part of the reason for the lower scores actually result from our impoverished MRL. Semantic cor-rectness scores were deducted when the machine misses commenting on certain facts not represented in our MRL such as the location of the ball and the players. The lack of temporal or locality information also results in dry and repetitive comments which hurt the sportscasting score. This is an important point that is not captured by the NIST score. In our NIST score evaluation, each sentence is treated separately and no attempt was made at measuring how well the individual comments fit together. However, it is clear from the human evaluations that variability of sen-tence pattern is vital to a good sportscast. The machine can correctly comment on all the factual events in a game and still produce a bad sportscast that no one wants to listen to. Robotics and vision researchers have worked on inferring a grounded meaning of individual words or short refer-ring expressions from visual perceptual context, e.g. (Roy , 2002; Bailey et al., 1997; Barnard et al., 2003; Yu &amp; Bal-lard, 2004). However, the complexity of the natural lan-guage used in this existing work is very restrictive, many of the systems use pre-coded knowledge of the language, and almost all use static images to learn language describing objects and their relations, and cannot use dynamic video to learn language describing actions. Some recent work on video retrieval has focused on learning to recognize events in sports videos and connect them to English words (Fleis-chman &amp; Roy, 2007). There has also been recent work on grounded language learning in simulated computer-game environments (Gorniak &amp; Roy, 2005). However, none of this prior work makes use of modern statistical-NLP pars-ing techniques, learns to build formal meaning representa-tions for complete sentences, or learns to generate natural language.
 There has been some recent work on learning generation strategies using reinforcement learning (Zaragoza &amp; Li, 2005). In contrast, our domain does not include interaction with the users and no feedback is available.
 The current system is limited by its simple MRL. For ex-ample, the location of players or the ball is not represented . Moreover, we do not keep contextual information which makes it difficult to generate interesting, non-repetitive sportscasts. Contextual information would also help us pro -vide comments not directly induced by the events happen-ing now, such as the current score. Finally, it is clear that we need a more hierarchical representation that captures the relationships between events in order to avoid mak-ing systematic matching errors on frequently co-occurring events.
 With respect to algorithms, using learned strategic-generation knowledge (information about what events are likely to illicit comments) could improve the resolution of ambiguities. We would also like to eventually apply our methods to real captioned video input using the latest meth-ods in computer vision. We have presented an end-to-end system that learns from sample commentaries and generates sportscasts for novel games. Dealing with the ambiguity inherent in the training environment is a critical issue in learning language from perceptual context. We have evaluated various methods for disambiguating the training data in order to build a lan-guage generator. Using a generation evaluation metric as the criterion for selecting the best NL X  X R pairs produced the best results overall. Our system also learns a simple model of strategic generation from the ambiguous training data by estimating the probability that each event type in-vokes a comment. Experimental evaluation verified that the system learns to accurately parse and generate comments and to generate sportscasts that are competitive with those produced by humans.
 We thank Adam Bossy for his work on simulating percep-tion for the Robocup games. This work was funded by the NSF grant IIS X 0712907X. Most of the experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.

