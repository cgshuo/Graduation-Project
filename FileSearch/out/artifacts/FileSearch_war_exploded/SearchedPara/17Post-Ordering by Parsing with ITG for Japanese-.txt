 Reordering target language words into an appropriate word order in the target lan-guage is one of the most difficult problems for statistical machine translation (SMT), in particular when translating between languages with widely different word orders, such as Japanese and English. In order to handle this problem, a number of reorder-ing methods have been proposed in statistical machine translation research. Those methods can be classified into the following three types.  X  Type-1: Conducting target word selection and reordering jointly . These methods include phrase-based SMT [Koehn et al. 2003], hierarchical phrase-based SMT [Chiang 2007], and syntax-based SMT [Chiang 2010; Ding and Palmer 2005; Galley et al. 2004; Liu et al. 2006; Quirk et al. 2005; Yamada and Knight 2002].  X  Type-2: Pre-ordering [Isozaki et al. 2010b; Xia and McCord 2004] . First, these meth-ods reorder the source language sentence into a target language word order. Then, they translate the reordered source word sequence using SMT methods.  X  Type-3: Post-ordering [Matusov et al. 2005; Sudoh et al. 2011b] . First, these meth-ods translate the source sentence almost monotonously into a target language word sequence. Then, they reorder the target language word sequence into a target lan-guage word order. In other words, the order of the word reordering and selection processes in post-ordering are the reverse of those in pre-ordering.

Sudoh et al. [2011b] indicated that type-3 performed better than existing type-1 methods for Japanese-to-English translations. As for type-2, different translation di-rections have different reordering problems, even if the language pair is the same, be-cause the performance of pre-ordering methods using a parser depends on the difficulty of estimating the target language word order and the parse accuracy for the source lan-guage. In fact, one type-2 method for English-to-Japanese translation obtained a large gain, but another type-2 method for Japanese-to-English translation could not obtain a large gain [Goto et al. 2011; Sudoh et al. 2011a]. The reason for the high performance of the English-to-Japanese translation is that estimating a Japanese word order based on English is not difficult. This is because Japanese-like word order can be obtained by simply moving an English headword to the end of its syntactic siblings, since Japanese is a typical head-final language [Isozaki et al. 2010b]. On the other hand, English is not a head-final language, which makes estimating English word order more difficult than estimating Japanese word order. Namely, pre-ordering is effective for translating into a target language where estimating word order is not difficult. In contrast, type-3 post-ordering is thought to be effective for translating from a source language where estimating word order is not difficult. The reason is as follows: a post-ordering model is built using a parallel corpus consisting of target language sentences and correspond-ing sentences containing the same words, but in the source language word order. The sentences in the source language word order are produced by changing the target lan-guage word order into the source language word order. This change is reliable when estimating source language word order is not difficult.

We employ the post-ordering framework for Japanese-English translation. The post-ordering method consists of a two-step process: (1) almost monotonously translating a Japanese sentence into an English word sequence in a Japanese-like word order; (2) reordering the English word sequence in a Japanese-like word order into an English word order. The first process can be conducted by traditional phrase-based SMT meth-ods. For the second process, Sudoh et al. [2011b] proposed a method using phrase-based SMT for the English word reordering.

In this article, we propose a reordering method based on parsing with inversion transduction grammar (ITG) [Wu 1997] for the post-ordering framework. The focus of this article is the second process of the post-ordering framework, which reorders an English word sequence in a Japanese-like word order into an English word order. Our method uses syntactic structures, which are essential for improving the target word order in translating long sentences between Japanese (a subject-object-verb (SOV) lan-guage) and English (an SVO language). Our reordering model parses an English word sequence in a Japanese-like word order using ITG to obtain derivations of Japanese-like syntactic structures, then reorders by transferring the Japanese-like syntactic structures into English structures based on the ITG. Experiments found that our reordering model improved translation quality as measured by both RIBES [Isozaki et al. 2010a] and BLEU [Papineni et al. 2002].

The rest of this article is organized as follows. Section 2 shows the post-ordering framework and a previous method; Section 3 describes the proposed reordering model for post-ordering; Section 4 explains the proposed method in detail; Section 5 gives and discusses the experiment results; Section 6 shows related work; and Section 7 concludes. In this article, we take a post-ordering approach [Sudoh et al. 2011b] for Japanese-English translation, which performs translation as a two-step process of word selection and reordering. The translation flow for the post-ordering method is shown in Figure 1, where  X  X FE X  is an abbreviation of  X  X ead Final English X , which is English words in a Japanese-like structure. 1 The two-step process is as follows. (1) Translating first almost monotonously transforms Japanese into HFE, which is (2) Reordering then transforms the HFE into English.
 In the post-ordering framework, the reordering model that reorders HFE into English is important. Sudoh et al. [2011b] proposed a reordering model that consisted of an HFE-English phrase-based SMT, which reordered by translating an HFE sen-tence into an English sentence. In general, syntactic structures are important for re-ordering in translating between languages with widely different word orders. However, the reordering model consisted of phrase-based SMT for post-ordering cannot fully use syntactic structures. In contrast, our reordering model for post-ordering can utilize these useful syntactic structures, which gives our reordering model an advantage.
In order to train a Japanese-HFE SMT model and an HFE-English reordering model, a Japanese-HFE parallel corpus and an HFE-English parallel corpus are needed. These corpora can be constructed by parsing the English sentences in a Japanese-English parallel corpus and applying the head-finalization rules [Isozaki et al. 2010b] to the parsed English sentences. The head-finalization rules change English sentences into HFE sentences, which is in Japanese-like word orders. Then a Japanese-HFE-English parallel corpus is built.

Here, we explain how the head-finalization rules change English into HFE. Japanese is a typical head-final language, where a syntactic head word comes after nonhead (dependent) words. The head-finalization rules move each syntactic head to the end of its siblings. English sentences are parsed by a parser, Enju [Miyao and Tsujii 2008], which outputs syntactic heads. Consequently, the parsed English sentences can be reordered into Japanese-like word ordered HFE sentences using the head-finalization rules.

Training for the post-ordering method is conducted via the following steps: first, the English sentences in a Japanese-English parallel corpus are converted into HFE sentences using the head-finalization rules. Next, a monotone phrase-based Japanese-HFE SMT model is built using the Japanese-HFE parallel corpus whose HFE sen-tences were converted from English sentences. Finally, an HFE-to-English word reordering model is built using the HFE-English parallel corpus. In this section, we describe our reordering model for post-ordering, which we con-centrate on in this article. We explain how the reordering model reorders HFE into English and how to train the reordering model. The proposed reordering model for post-ordering, which we have called the ITG pars-ing model , is based on two fundamental frameworks: (i) parsing using probabilistic context free grammar (PCFG) and (ii) the inversion transduction grammar (ITG) [Wu 1997]. ITG between HFE and English is used as the PCFG for parsing. In this article, parsing using ITG is called ITG parsing .
 We assume that there is an underlying HFE binary tree derivation that produces English word order. The reordering process by the ITG parsing model is shown in Figure 2. An HFE sentence is parsed using ITG to obtain an HFE binary tree deriva-tion, which is similar to the syntactic tree structure of the input Japanese sentence. Each nonterminal node that has two child nodes is augmented by either an  X  ST X  (indicating  X  X traight X ) suffix or an  X  SW X  (indicating  X  X wap/inversion X ) suffix. The English word order is determined by the binary tree derivation and the suffixes of the nonterminal nodes. We swap the child nodes of the nodes augmented with the  X  SW X  suffix in the binary tree derivation in order to produce an English sentence. In order to train the ITG parsing model, the structures of the HFE sentences with  X  ST X  and  X  SW X  suffixes are used as the training data. The training data can be obtained from the corresponding English sentences as follows.

First, each English sentence in the training Japanese-English parallel corpus is parsed into a binary tree structure by applying the Enju parser. Then, for each non-terminal node in the English binary tree structure, the two child nodes of each node are swapped if the first child is the head node (see [Isozaki et al. 2010b] for more in-formation on head-finalization rules). At the same time, these nodes with swapped child nodes are annotated with  X  SW X . When the two child nodes of each node are not swapped, these nodes are annotated with  X  ST X . A node with only one child is not an-notated with  X  ST X  or  X  SW X . The result is an HFE sentence in a binary tree structure augmented with straight or swap/inversion suffixes.

Binary tree structures can be learnable by using an off-the-shelf PCFG learning algorithm. Therefore, HFE binary tree structures can also be learnable. HFE binary tree structures augmented with the straight or swap/inversion suffixes can be regard as derivations of ITG [Wu 1997] between HFE and English. Therefore, a parsing model learned from the HFE binary tree structures using a PCFG learning algorithm is an ITG model between HFE and English.

In this article, we used the state split probabilistic CFG [Petrov et al. 2006] for learn-ing the ITG model. The learned ITG model for parsing is the ITG parsing model . The HFE sentences can be parsed by using the ITG parsing model. Then the derivations of the HFE structures can be converted into their corresponding English structures by swapping the child nodes of the nodes with the  X  SW X  suffix. Note that this ITG parsing model jointly learns how to parse and swap the HFE sentences. This section explains the proposed translation method, which is based on the post-ordering framework using the ITG parsing model, in detail. Machine translation is formulated as a problem of finding the most likely target sen-tence E given a source sentence F . In the post-ordering framework, we divide the translation process into two processes using an HFE sentence M . The summation is approximated by maximization to reduce computational costs and weighting parameters  X  x ( x is r , s , or others) are introduced to be tunable by weighting each model in the same manner as a log-linear model. P ( M | F ) in Equation (1) is the probability of translation from a Japanese sentence F into an HFE sentence M . We use the SMT score S of a log-linear SMT model as the log-arithm of P ( M | F )  X  s ,thatis,  X  s log ( P ( M | F )) SMT score [Koehn et al. 2007] from F to M translation as S ( the Moses SMT score is calculated, feature values, such as a language model probabil-ity, are scaled by a set of weighting parameters. The set of weighting parameters are usually tuned by a tuning algorithm (e.g., minimum error rate training (MERT) [Och 2003]).  X  s approximately represents the scaling by the set of weighting parameters.
We compared two reordering models for estimating P ( E | M , F ) The first reordering model is independent of F given M , and we assume that an under-lying HFE tree derivation T M , which is augmented with  X  SW X  and  X  ST X , produces an English word order. We use the ITG parsing model as P ( T M | M ) . That is, to obtain high probability T parse M using the ITG parsing model described in Section 3.1. Equation (2) is approx-imated by introducing independent weight parameters  X  r 1 be tunable by weighting each model in the same manner as a log-linear model; dividing is produced from T M and M deterministically by swapping the child nodes of the nodes with the  X  SW X  suffix described in Section 3.1. This production process is expressed by P ( E | T M , M ) . Thus, P ( E | T M , M )  X  r 2 is1for E produced from T 0 for other E . P ( E ) is the language model probability of an English sentence E . Here, we explain why we introduce P ( E ) , which has fewer conditions than P ( E | T M , M ) . (i) In general, actual models used for calculating probabilities are ap-proximations of equations and not perfect. For example, an n-gram language model appropriately smoothed by a liner combination of an n-gram model and an (n model is usually better than a simple n-gram language model based on the maximum likelihood estimation by relative frequencies. (ii) When the architectures of the two models that calculate the probabilities of the same object are quite different, each model can capture different aspects. Therefore, the n-gram language model of P ( E ) will remedy the deficiencies of the ITG parsing model of P ( T uate generative probability of E because the word order of E is produced from T determinately. The first reordering model (reordering model 1) is independent of F . If some noise is included in M when M is produced from F using SMT or if tree derivations of M are more ambiguous than tree structures of F , the tree structure of F will be useful in obtaining a tree derivation of M . This is because F is not a translation result, and a correct tree derivation of M is expected to be similar to a correct tree structure of F , since an HFE sentence is regarded as English words in a Japanese structure.
In this section, we introduce the second reordering model that uses a Japanese syn-tactic structure. The second reordering model uses the maximum probability Japanese syntactic structure T F and the maximum probability word alignments A between F and M to obtain an underlying HFE tree derivation T M , and we also assume that T produces the following English word order.  X  E ,  X  T M ,  X  M  X  argmax In Equation (4), we assume that E is conditionally independent of A , T T M and M ;that T M is conditionally independent of F given A , T ditionally independent of T F given M and F ;andthat T F is conditionally independent of M given F . P ( T F | F ) in Equation (5) is constant given F . is approximately assumed as a constant. Equation (6) is approximated by introducing independent weight parameters  X  r 1 ,  X  r 2 ,and  X  r 3 instead of  X  one of the divided models. We use the ITG parsing model with consideration of T P ( T M | A , T F , M ) , that is, to obtain high probability T model with consideration of T F . P ( E | T M , M ) represents the deterministic production of E from T M and M described in Section 3.1. P ( E | T M , M ) deterministically and is 0 for other E .
 What differs between Equation (3) of the previous reordering model 1 and Equation (7) of this reordering model 2 is that Equation (7) uses P ( T stead of the P ( T M | M ) of Equation (3). We use the following simple method using a weighting parameter w ( 0 &lt; w &lt; 1 ) , which is tuned using development data, as one implementation of P ( T M | A , T F , M ) = P ( T M | A , T be similar to a correct T F , since an HFE sentence is regarded as English words in a Japanese structure. To reflect this expectation, we change the rule probabilities of the state split PCFG slightly, depending on T M and T F , using a weighting parameter w ( 0 &lt; w &lt; 1 ) as follows.  X  If a subtree in T M does not cross the word span of any subtree in T  X  If a subtree in T M crosses the word span of any subtree in T p  X  w is used to reduce the probability because p 2  X  w is thought to be a symmetric form of p w , since when w is 1, both p w and p 2  X  w are the same as p ,andas w becomes smaller, the effects increase for both p w and p 2  X  w . Note that the rule score for each application of the same rule can vary depending on the situation.
 Although the resulting rule scores are ad hoc, this assists in making the analysis of T
M closer to T F . This section gives more details about HFE [Sudoh et al. 2011b]. In HFE sentences, the following hold. (1) Each syntactic head is moved toward the end of its siblings except for coordination. (2) Pseudo-particles are inserted after verb arguments: va0 (the subject of the sen-(3) Articles (a, an, the) are dropped.
 Although these were specified by Sudoh et al. [2011b], we attempt to explain the rea-sons for the specifications. The reason for (1) is that Japanese is a head-final language. The reasons for (2) and (3) are because translating is usually easier in SMT when words in a parallel sentence correspond one to one than when words correspond one to null. Specifications (2) and (3) try to reduce the one-to-null word correspondences. Japanese sentences contain particles that are case markers for subjects and objects, but English has no such corresponding words. The pseudo-particles in HFE correspond to these Japanse particles. On the flip side, Japanese does not contain articles, and thus they are dropped.
 There is one point of difference between our HFE construction and that of Sudoh et al. [2011b]: in our method, plural nouns were left as plural instead of being converted to singular, because our reordering model does not change words; it only reorders them. Applying our reordering model to an HFE sentence produces an English sentence that does not have articles but does have pseudo-particles. We removed the pseudoparticles from English sentences produced from HFE sentences before calculating the probabil-ities of P ( E ) in Equations (3) and (7) because the language model P ( E ) without pseu-doparticles is simpler than that with pseudo-particles and is more robust than that with pseudoparticles, since E without pseudo-particles is not influenced by insertion errors from inserting pseudo-particles into training data. A language model P ( E ) was trained from English sentences whose articles were dropped.

In order to output a genuine English sentence E from E , articles must be inserted into E . A language model trained using genuine English sentences is used for this purpose. E is obtained by where S is a set consisting of E with articles. We calculate the maximum probabil-ity word sequence through a dynamic programing technique for obtaining a genuine English sentence.

Articles are inserted by building a lattice structure which inserts one of the articles { a, an, the } or no article for each word e i in E = e 1 e structure in the case of I = 3. In Figure 4, &lt;s&gt; is a special word representing beginning of sentence, and &lt;/s&gt; is a special word representing end of sentence. The maximum probability word sequence is calculated by applying the Viterbi algorithm for the lat-tice structure and an n-gram language model. We investigated the effectiveness of our method by comparing it with other methods for Japanese to English translation. We used patent sentence data for the Japanese-to-English translation subtask from the NTCIR-9 [Goto et al. 2011] and NTCIR-8 [Fujii et al. 2010]. The training data and the development data for NTCIR-9 and NTCIR-8 are the same, but the test data is different. There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. There were approximately 3.18 million sentence pairs for the training data and 2,000 sen-tence pairs for the development data. XML entities included in the data were decoded to UTF-8 characters before use.

We used Enju [Miyao and Tsujii 2008] to parse the English side of the training data. Mecab 3 was used for the Japanese morphological analysis and Cabocha Japanese dependency parsing. We adjusted the tokenization of alphanumeric charac-ters and parentheses in Japanese to be the same as for the English. The translation model was trained using sentences of 64 words or less from the training data [Sudoh et al. 2011b]. Approximately 2.97 million sentence pairs were 64 words or less. We used 5-gram language models with modified Kneser-Ney discounting [Chen and Goodman 1998] using SRILM [Stolcke et al. 2011]. The language models were trained using all of the English sentences from the bilingual training data.

We used the Berkeley parser [Petrov et al. 2006], which is an implementation of the state split PCFG based parser, to train the ITG parsing model for HFE and to parse HFE. The ITG parsing model was trained using 0.5 million sentences randomly selected from training sentences of 40 words or less. We performed six split-merge iterations as the same iteration of the parsing model for English [Petrov et al. 2006]. We used the phrase-based SMT system Moses [Koehn et al. 2007] to calculate SMT scores and to produce HFE sentences. The SMT score S was used as the logarithm of P ( M | F )  X  s in Equation (1), that is,  X  s log ( P ( M based SMT was set to 0. With this setting, the phrase-based SMT translates almost monotonously. The SMT weighting parameters were tuned by MERT using the first half of the development data.

For the process of Equation (1) through the intermediary M , we used a beam search using the ten-best results of M from Moses outputs. For the processes of parsing M to produce T M , which is represented by P ( T M | M ) in Equation (3) and P ( T in Equation (7), we used the ten-best parsing results. The probabilities of the ten-best parsing results were approximated to a constant. With this approximation, the value of P ( T M | M )  X  r 3 in Equation (3) and the value of P ( T (7) are constant for the ten-best parsing results. Therefore, the value of  X  affect the results and  X  r 3 does not need to set for this experiment. As explained in Sections 4.2 and 4.3, P ( E | T M , M )  X  r 2 in Equations (3) and (7) is 1 for the E produced from T M deterministically and is 0 for the other E . Therefore, the value of  X  affect the results and  X  r 2 does not need to set for this experiment.

Consequently, the parameters to be set for this experiment are  X  rameter  X  r 1 scales P ( E ) in Equations (3) and (7). We used the value of the weighting parameter for the language model feature in the Japanese-HFE SMT model as the value of  X  r 1 in order to adjust the scale of P ( E )  X  r Equation (1). The parameter w adjusts the strength of the effect from T M for the reordering model 2. w was tuned 5 using the second half of the development data. The tuning was based on the BLEU score [Papineni et al. 2002]. In the exper-iment, using the Moses SMT score S from F to M translation, we searched for the maximum  X  r 1 log ( P ( E )) + S in the beam search to obtain We used the following six comparison methods.  X  Phrase-based SMT (PBMT) [Koehn et al. 2003].  X  Hierarchical phrase-based SMT (HPBMT) [Chiang 2007].  X  String-to-tree syntax-based SMT (SBMT) [Hoang et al. 2009].  X  Post-ordering based on phrase-based SMT (PO-PBMT) [Sudoh et al. 2011b].  X  Post-ordering based on hierarchical phrase-based SMT (PO-HPBMT).  X  Post-ordering based on string-to-tree syntax-based SMT (PO-SBMT).
 We used Moses [Koehn et al. 2007; Hoang et al. 2009] for these systems. PO-PBMT was the method proposed by Sudoh et al. [2011b]. For PO-PBMT, a distortion limit 0 was used for the Japanese-to-HFE translation, and a distortion limit 20 was used for the HFE-to-English translation. These distortion limit values are the val-ues that achieved the best results in the experiments by Sudoh et al. [2011b]. The PO-HPBMT method changes the post-ordering method of PO-PBMT for the HFE-to-English translation from a phrase-based SMT to a hierarchical phrase-based SMT. The PO-SBMT method changes the post-ordering method of PO-PBMT for the HFE-to-English translation from a phrase-based SMT to a string-to-tree syntax-based SMT. We used a max-chart-span of  X  (unlimited) for the hierarchical phrase-based SMT of PO-HPBMT and the string-to-tree syntax-based SMT of PO-SBMT. We used dis-tortion limits of 12 or 20 for PBMT and max-chart-span sof15or HPBMT and SBMT. For PBMT, a lexicalized reordering model [Koehn et al. 2005], that is, msd-bidirectional-fe configuration was used. The default values were used for the other system parameters.
 The SMT weighting parameters were tuned by MERT. For PBMT, HPBMT, and SBMT, all of the development data was used for tuning. For the Japanese-to-HFE translation of PO-PBMT, PO-HPBMT, and PO-SBMT, the first half of the develop-ment data was used for tuning. For the HFE-to-English translation of PO-PBMT, PO-HPBMT, and PO-SBMT, the following three kinds of data were used for tuning.  X  dev1 . The second half of the development data with HFE produced by translating  X  dev1-oracle . The second half of the development data with HFE that are oracle- X  dev2-oracle . The first half of the development data with HFE that are oracle-HFE We evaluated translation quality based on the case-insensitive automatic evaluation scores RIBES v1.01 [Isozaki et al. 2010a] and BLEU-4 [Papineni et al. 2002]. RIBES is an automatic evaluation measure based on the word-order correlation coefficients between reference sentences and translation outputs. The results are shown in Table I. The method using reordering model 1 described in Section 4.2 is  X  X roposed (without T ) X , and the method using reordering model 2 described in Section 4.3 is  X  X roposed (with T F ) X .
 We compare the proposed method with T F to the comparison methods.

First, we made a comparison based on RIBES. For the NTCIR-9 data, the score of the proposed method without T F was 6.05 points higher than the best score from PO-PBMT and 2.60 points higher than the best score from all of the compared methods (the best method was PO-SBMT (dev1-oracle)). For the NTCIR-8 data, it was 5.64 points higher than the best score from PO-PBMT and 2.69 points higher than the best score from all of the compared methods (the best method was PO-SBMT (dev1-oracle)). The proposed method is thought to be better than the compared methods for global word ordering, since RIBES is sensitive to global word order.

Next, we made a comparison based on the widely used BLEU. For the NTCIR-9 data, the score of the proposed method without T F was 2.56 points higher than the best score from PO-PBMT and 0.75 points higher than the best score from all of the compared methods (the best method was PO-SBMT (dev1)). For the NTCIR-8 data, it was 2.48 points higher than the best score from PO-PBMT and 0.98 points higher than the best score from all of the compared methods (the best method was PO-SBMT (dev1 and dev1-oracle)). The proposed method is also thought to be better than the compared methods for local word ordering, since BLEU is sensitive to local word order. The differences between the scores of the proposed method without T scores from the compared methods were statistically significant at a significance level of  X  = 0.01 for both RIBES and BLEU, using a bootstrap resampling method [Koehn 2004] for a statistical significance test. These comparisons demonstrate the effective-ness of the proposed method without T F for reordering.
 When comparing the proposed method with T F and without T than without T F for both RIBES and BLEU for both NTCIR-9 and NTCIR-8. Since the improvements were not large, we calculated a statistical significance test using a boot-strap resampling method [Koehn 2004] for the differences. For the NTCIR-9 RIBES scores, the difference was statistically significant at a significance level of  X  For the NTCIR-8 RIBES scores, the difference was statistically significant at a signif-icance level of  X  = 0.01. For the NTCIR-9 BLEU scores, the difference was not statis-tically significant at a significance level of  X  = 0.05, but was statistically significant at a significance level of  X  = 0.1. For the NTCIR-8 BLEU scores, the difference was statistically significant at a significance level of  X  = 0.01. This demonstrates that the method using a Japanese syntactic structure for parsing does have some effectiveness.
In order to investigate the effects of our ITG parsing model more fully, the results with different settings are given here.

We checked different beam widths for the K -best parsing results. Changing the beam widths for K of the K -best parsing results is shown in Figure 5 for the NTCIR-9 test data and in Figure 6 for the NTCIR-8 test data. The beam width K has a slight effect. However, even when K is 1, that is, only the best parsing results were used, the dif-ferences between its RIBES and BLEU scores and the best scores were not large. This indicates that the top-ranked parsing results were relatively trustworthy compared to the non-top-ranked parsing results. The top ranked parsing results, for example, three-to ten-best, seem almost sufficient.

Figure 7 shows the ranking rates of the ten-best parsing results used to produce the final translations for the NTCIR-9 test data. 6 The top-ranked parsing results were used to produce the final translations. This also indicates that the top-ranked parsing results were relatively trustworthy compared to the non-top-ranked parsing results for the following reason: the E of a large P ( E ) in Equations (3) and (7) is used to produce the final translation. The English sentence E produced from a correct tree derivation T
M will be a natural English sentence E ,whose P ( E ) will be large, and will be used to produce the final translation.

We checked different beam widths for the N -best results of M . The different beam widths N of the N -best results of M are shown in Figure 8 for the NTCIR-9 test data and in Figure 9 for the NTCIR-8 test data. From these figures, a beam width of at least 3 is needed to produce the best results, a beam width of 10 is almost sufficient, and a beam width of 50 is thought to be sufficient.

In these experiments, we did not compare our method to pre-ordering methods. How-ever, some groups used pre-ordering methods in the NTCIR-9 Japanese-to-English translation subtask. The NTT-UT group [Sudoh et al. 2011a] used a pre-ordering method that used parsing trees and manually defined pre-ordering rules. The NAIST group [Kondo et al. 2011] used a pre-ordering method [Tromble and Eisner 2009] that learned a pre-ordering model automatically. These groups were unable to produce both RIBES and BLEU scores that were better than those of the baseline systems of HPBMT and PBMT. In contrast, both the RIBES and BLEU scores for our method were higher than the baseline systems of HPBMT and PBMT. A detailed comparison with pre-ordering methods is our future work. In order to investigate the effects of our post-ordering method more thoroughly, we conducted an  X  X FE-to-English reordering X  experiment which focuses on the effects of word reordering for the post-ordering framework. This experiment confirms the main contribution of our post-ordering method in the framework of post-ordering SMT, as compared with Sudoh et al. [2011b]. In this experiment, we changed the word order of the oracle-HFE sentences made from reference sentences into English using reorder-ing models. This is the same way as in Table 4 in Sudoh et al. [2011b]. Only the test data (input data) differs from the experiment in the previous section. All other settings are the same. In the experiment in Section 5.3, Japanese sentences were used for the input data. On the other hand, in the experiment in this section, oracle-HFE sentences were used for the input data. The oracle-HFE sentences were produced by (1) parsing the reference English sentences using the Enju parser and (2) applying the head finalization rules [Isozaki et al. 2010b] to the parsing results. Note that since the oracle-HFE sentences were not produced from Japanese sentences, we only used the proposed method without T F .

The results are shown in Table II. This results show that our post-ordering method is more effective than PO-PBMT, PO-HPBMT, and PO-SBMT. Since RIBES is based on the rank order correlation coefficient, these results show that the proposed method correctly recovered the word order of the English sentences. These high scores also indicate that the parsing results for high quality HFE are fairly trustworthy.
The causes of reordering errors are classified into distinguishing errors between  X  ST X  and  X  SW X  and parsing errors. We investigated how often distinguishing errors occurred. We checked the agreement rate of suffixes ( X  ST X  or  X  SW X ) between the pars-ing results by the ITG parsing model (parsed trees) and the tree structures of the test data (oracle trees) for the labels with the following conditions: (1) labels that had suf-fixes ( X  ST X  or  X  SW X ); (2) the subtree spans of the labels are the same in the parsed trees and the oracle trees; and (3) labels without suffixes are the same in the parsed trees and the oracle trees. The agreement rate of suffixes was 99.3% for the NTCIR-9 dataset. We checked the number of hidden states learned for the ITG parsing model. The top three labels are VP ST (61), VP SW (56), and NP ST (53). The number in the parenthesis represents the number of hidden states. Some other major labels are PP ST (43), S SW (33), PP SW (32), S ST (32), and NP SW (25). From the high agree-ment rate, these numbers of hidden states are thought to be enough for learning the distinction between  X  ST X  and  X  SW X , and the main cause of errors is thought to be parsing errors. To improve parsing, techniques for parsing such as these of Petrov [2010] will be useful.

Since there are large differences between the values in Table I and Table II, prob-lems in post-ordering are not entirely solved by improving the reordering accuracy of oracle-HFE. Noise may be included during Japanese-HFE monotone translation. Er-rors such as word selection errors or lack of translation at the Japanese-HFE monotone translation step cannot be recovered at the reordering step. Using the N-best results for Japanese-HFE monotone translation reduces the effects of these errors compared with using the 1-best result for Japanese-HFE monotone translation. However, this cannot solve the problem perfectly. Word selection is not the only cause of problems. It is rare, but there are word orders in Japanese that cannot be covered by ITG between HFE and English. For example, the fundamental word order of Japanese is SOV, but a word order of OSV is also acceptable in Japanese. An HFE sentence in an OSV word order monotonously translated from a Japanese sentence in an OSV word order cannot be transferred into (S (V O)) by ITG because O and V are not continuous. In this case, it is necessary to convert a Japanese sentence in an OSV word order into a Japanese sentence in an SOV word order at preprocessing. This section describes related research other than the aforementioned post-ordering [Matusov et al. 2005; Sudoh et al. 2011b]. Features of our method are as follows.  X  Monotonously translated sentences are parsed for reordering in the post-ordering  X  Word reordering is done by syntactic transfer based on an ITG model merged with
The post-ordering method splits the word selection and reordering processes. There are many pre-ordering methods that also split the word selection and reordering pro-cesses.

Some pre-ordering methods use parsers and manually defined rules for translat-ing different languages. These languages include German to English [Collins et al. 2005], Chinese to English [Wang et al. 2007], English to Hindi [Ramanathan et al. 2008], English to Arabic [Badr et al. 2009], English to Japanese [Isozaki et al. 2010b], and English to five SOV languages (Korean, Japanese, Hindi, Urdu, and Turkish) [Xu et al. 2009]. In English-to-Japanese translation, a pre-ordering method using head fi-nalization rules [Isozaki et al. 2010b], which are used in our post-ordering method, achieved the best quality measured by both RIBES and BLEU, and by the human evaluations which were conducted for the NTCIR-9 patent machine translation task [Goto et al. 2011; Sudoh et al. 2011a]. The reason why this method worked out well is that Japanese is a head-final language, so estimating a Japanese word order based on English is not difficult. On the other hand, English is not a head final language, which makes pre-ordering for Japanese to English more difficult than pre-ordering for the opposite direction, and the pre-ordering method using the head finalization rules cannot be applied. Pre-ordering methods for Japanese to English estimate an English word order based on Japanese. In contrast, the post-ordering methods estimate an English word order based on HFE, which consists of English words. Estimating an English word order based on English words (HFE) is more tractable than estimating an English word order based on Japanese words. This is an advantage of post-ordering methods over pre-ordering methods for Japanese to English translation.

Some pre-ordering methods use parsers and automatically constructed rules [Dyer and Resnik 2010; Ge 2010; Genzel 2010; Habash 2007; Li et al. 2007; Visweswariah et al. 2010; Wu et al. 2011a, 2011b; Xia and McCord 2004]. Li et al. [2007] used N-best parsing results. Habash [2007] used labeled dependency structures. Dyer and Resnik [2010] used forests based on parsers. Ge [2010] used a manually-aligned cor-pus to build a pre-ordering model. Genzel [2010] used a dependency parser and tested English into seven languages, including Japanese, and German into English. Wu et al. [2011a] investigated the automatic acquisition of Japanese to English pre-ordering rules using bilingual Japanese and English parsing trees. Wu et al. [2011b] used predicate-argument structures to extract pre-ordering rules and tested English to Japanese.

Some pre-ordering methods do not use supervised parsers. Rottmann and Vogel [2007] proposed a pre-ordering method based on POS. Tromble and Eisner [2009] used ITG constraints to reduce computational costs. DeNero and Uszkoreit [2011] and Neubig et al. [2012] proposed methods for inducing binary tree structures automati-cally from a parallel corpus with high-quality word alignments and using these struc-tures to preorder source sentences based on ITG. They tested English to Japanese, and Neubig et al. [2012] also tested Japanese to English. Visweswariah et al. [2011] trained a model that used pairwise costs of a word by using a small parallel corpus with high-quality word alignments. They tested Hindi to English, Urdu to English, and English to Hindi.

These are all pre-ordering methods, not post-ordering modes, and thus are different from our method.

The post-edit methods also use a two-step translation process that translates first using a rule-based MT system then post-edits the outputs of the rule-based MT using a phrase-based SMT system [Aikawa and Ruopp 2009; Dugast et al. 2007; Ehara 2007; Simard et al. 2007], or translates first using a syntax-based SMT system then post-edits the outputs of the syntax-based SMT using a phrase-based SMT system [Aikawa and Ruopp 2009]. For Japanese-English translation, the first process changes the word order of Japanese into an English word order and translates, then the post-edit pro-cess corrects word selection errors from the first process. This method is similar to pre-ordering methods because the first process mainly decides word order and the sec-ond process mainly decides word selection. Thus, these post-edit methods are different from our method.

Our method learns the ITG model [Wu 1997] for reordering. There has also been work done using the ITG model in SMT for joint word selection and reordering. These methods include grammar induction methods from a parallel corpus [Blunsom et al. 2009; Cherry and Lin 2007; Neubig et al. 2011; Zhang et al. 2008]; hierarchical phrase-based SMT [Chiang 2007], which is an extension of ITG; reordering models using ITG [Chen et al. 2009; He et al. 2010]; and ITG constraint for reordering in SMT [Petrov et al. 2008; Zens et al. 2004; Zhang and Gildea 2008]. Note that the aforementioned methods of DeNero and Uszkoreit [2011] and Neubig et al. [2012] also use ITG for training pre-ordering model. However, none of these methods using the ITG model are post-ordering methods.

Our method uses linguistic syntactic structures for reordering. Linguistic syntactic structures have also been used in various works. There are methods that use target language syntactic structures (string-to-tree) [Galley et al. 2004; Shen et al. 2008; Yamada and Knight 2002], methods that use source language syntactic structures (tree-to-string) [Huang et al. 2006; Liu et al. 2006; Quirk et al. 2005], and methods that use both the source and the target language syntactic structures (tree-to-tree) [Chiang 2010; Ding and Palmer 2005; Liu et al. 2009]. These methods do word selec-tion and reordering simultaneously. In contrast, our method does word selection and reordering separately.

Our method is related to tree-to-tree translation methods using syntactic trans-fer for word reordering. Since Japanese words and English words do not always correspond one to one, there are large differences between Japanese and English syn-tactic structures. This makes it difficult to learn syntactic transfer for word reordering. On the other hand, since HFE words and English words always correspond one to one, the difference between HFE and English syntactic structures are smaller than that of Japanese and English. This makes it easier to learn syntactic transfer for word re-ordering. From these, our method can be regarded to treat a task that learns word reordering based on syntactic transfer for Japanese to English as a more tractable task. This article has described a new post-ordering method. Our reordering model consists of a parsing model based on ITG. The proposed method parses sentences that consist of target language words in a source language word order, and does reordering by trans-ferring the syntactic structure similar to the source language syntactic structure into the target language syntactic structure based on ITG. We conducted experiments using Japanese-to-English patent translation. In the experiments, our method outperformed phrase-based SMT, hierarchical phrase-based SMT, string-to-tree syntax-based SMT, and post-ordering methods based on SMT for both RIBES and BLEU. Since RIBES is sensitive to global word order and BLEU is sensitive to local word order, we concluded that the proposed method was better than the compared methods at global word or-dering and local word ordering. We also conducted experiments focusing on reordering. These experiments confirmed that our method was able to correctly recover an English word order for high-quality HFE.

