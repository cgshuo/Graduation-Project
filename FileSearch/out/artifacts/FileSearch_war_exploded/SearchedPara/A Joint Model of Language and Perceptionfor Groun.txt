 Cynthia Matuszek cynthia@cs.washington.edu Nicholas FitzGerald nfitz@cs.washington.edu Luke Zettlemoyer lsz@cs.washington.edu Liefeng Bo lfb@cs.washington.edu Dieter Fox fox@cs.washington.edu Physically grounded settings provide exciting oppor-tunities for learning. For example, a person might be able to teach a robot about objects in its environment. However, to do this, a robot must jointly reason about the different modalities encountered (for example lan-guage and vision), and induce rich associations with as little guidance as possible.
 Consider a simple sentence such as  X  X hese are the yel-low blocks, X  uttered in a setting where there is a phys-ical workspace that contains a number of objects that vary in shape and color. We assume that a robot can understand sentences like this if it can solve the as-sociated grounded object selection task . Specifically, it must realize that words such as  X  X ellow X  and  X  X locks X  refer to object attributes, and ground the meaning of such words by mapping them to a perceptual system that will enable it to identify the specific physical ob-jects referred to. To do so robustly, even in cases where words or attributes are new, our robot must learn (1) visual classifiers that identify the appropriate object properties, (2) representations of the meaning of indi-vidual words that incorporate these classifiers, and (3) a model of compositional semantics used to analyze complete sentences.
 In this paper, we present an approach for jointly learn-ing these components. Our approach builds on exist-ing work on visual attribute classification (Bo et al., 2011) and probabilistic categorial grammar induction for semantic parsing (Zettlemoyer &amp; Collins, 2005; Kwiatkowski et al., 2011). Specifically, our system in-duces new grounded concepts (groups of words along with the parameters of the attribute classifier they are paired with) from a set of scenes containing only sen-tences, images, and indications of what objects are being referred to. As a result, it can be taught to rec-ognize previously unknown object attributes by some-one describing objects while pointing out the relevant objects in a set of training scenes. Learning is on-line, adding one scene at a time, and EM-like, in that the parameters are updated to maximize the expected marginal likelihood of the latent language and visual components of the model. This integrated approach allows for effective model updates with no explicit la-beling of logical meaning representations or attribute classifier outputs.
 We evaluate this approach on data gathered on Ama-zon Mechanical Turk, in which people describe sets of objects on a table. Experiments demonstrate that the joint learning approach can effectively extend the set of grounded concepts in an incomplete model initial-ized with supervised training on a small dataset. This provides a simple mechanism for learning vocabulary in a physical environment.
 Problem We wish to learn a joint language and per-ception model for the object selection task. The goal is to automatically map a natural language sentence x and a set of scene objects O to the subset G  X  O of objects described by x . The left panel of Fig. 1 shows an example scene. Here, O is the set of objects present in this scene. The individual objects o  X  O are extracted from the scene via segmentation (the right panel of Fig. 1 shows example segments). Given the sentence x = X  X ere are the yellow ones, X  the goal is to select the five yellow objects for the named set G . Model Components Given a sentence and seg-mented scene objects, we learn a distribution P ( G | x,O ) over the selected set. Our approach combines recent models of language and vision, including: (1) A semantic parsing model that defines P ( z | x ), a distribution over logical meaning representations z for each sentence x . In our running example, the desired representation z =  X x.color ( x,yellow ) is a lambda-calculus expression that defines a set of objects that are yellow. For this task, we build on an existing se-mantic parsing model (Kwiatkowski et al., 2011). (2) A set of visual attribute classifiers C , where each classifier c  X  C defines a distribution P ( c = true | o ) of the classifier returning true for each possible object o  X  O in the scene. For example, there would be a unique classifier c  X  C for each possible color or shape an object can have. We use logistic regression to train classifiers on color and shape features extracted from object segments recorded using a Kinect depth camera. Joint Model We combine these language and vision models in two ways. First, we introduce an explicit model of alignment between the logical constants in the logical form z and classifiers in the set C . This alignment would, for example, enable us to learn that the logical constant yellow should be paired with a classifier c  X  C that fires on yellow objects. Next, we introduce an execution model that allows us to determine what scene objects in O would be selected by a logical expression z , given the classi-fiers in C . This allows us to, for example, execute  X x.color ( x,green )  X  shape ( x,triangle ) by testing all of the objects with the appropriate classifiers (for green and triangle ), then selecting objects on which both classifiers return true. This execution model includes uncertainty from the semantic parser P ( z | x ), classifier confidences P ( c = true | o ), and a deterministic ground-truth constraint that encodes what objects are actually intended to be selected. Full details are in Sec. 5. Model Learning We present an approach that learns the meaning of new words from a dataset D = { ( x i ,O i ,G i ) | i = 1 ...n } , where each example i con-tains a sentence x i , the objects O i , and the selected set G i . This setup is an abstraction of the situa-tion where a teacher mentions x i while pointing to the objects G i  X  O i she describes. As described in detail in Sec. 6, learning proceeds in an online, EM-like fashion by repeatedly estimating expectations over the latent logical forms z i and the outputs of the clas-sifiers c  X  C , then using these expectations to update the parameters of the component models for language P ( z | x ) and visual classification P ( c | o ). To bootstrap the learning approach, we first train a limited language and perception system in a fully supervised way: in this stage, each example additionally contains labeled logical meaning expressions and classifier outputs, as described in Sec. 6. To the best of our knowledge, this paper presents the first approach for jointly learning visual classifiers and semantic parsers, to produce rich, compositional mod-els that span directly from sensors to meaning. How-ever, there is significant related work on the model components, and on grounded learning in general. Vision Current state-of-the-art object recognition systems (Felzenszwalb et al., 2009; Yang et al., 2009) are based on local image descriptors, for example SIFT over images (Lowe, 2004) and Spin Images over 3D point clouds (Johnson &amp; Hebert, 1999). Visual attributes provide rich descriptions of objects, and have become a popular topic in the vision commu-nity (Farhadi et al., 2009; Parikh &amp; Grauman, 2011); although very successful, we still lack a deep un-derstanding of the design rules underlying them and how they measure similarity. Recent work on ker-nel descriptors (Bo et al., 2010) shows that these hand-designed features are equivalent to a type of match kernel that performs similarly to sparse cod-ing (Yang et al., 2009; Yu &amp; Zhang, 2010) and deep networks (Lee et al., 2009) on many object recogni-tion benchmarks (Bo et al., 2010). We adapt kernel descriptors as feature extractors for attribute classi-fiers because of their strong empirical performance. Semantic Parsing There has been significant work on supervised learning for inducing semantic parsers (Zelle &amp; Mooney, 1996; He &amp; Young, 2006; Wong &amp; Mooney, 2007). Our research builds on work on supervised learning of CCG parsers (Zettlemoyer &amp; Collins, 2005; Kwiatkowski et al., 2011); there is also work on performing semantic analysis with alternate forms of supervision. Clarke (2010) and Liang (2011) describe approaches to learning semantic parsers from questions paired with database answers, while Gold-wasser (2011) presents work on unsupervised learning. However, none of these approaches include joint mod-els of language and vision.
 Grounding There has been significant work on grounded learning more generally in the robotics and vision communities. A full review is beyond the scope of this paper, so we highlight a few examples. Roy de-veloped a series of techniques for grounding words in visual scenes (Mavridis &amp; Roy, 2006; Reckman et al., 2010; Gorniak &amp; Roy, 2003). In computer vision, the grounding problem often relates to detecting objects and attributes in visual information (e.g., see (Barnard et al., 2003)); however, these approaches primarily fo-cus on isolated word meaning, rather than compo-sitional semantic analyses. Most closely related to our work are approaches that learn probabilistic lan-guage models from natural language input (Matuszek et al., 2012; Chen &amp; Mooney, 2011), especially those that include a visual component (Tellex et al., 2011). However, these approaches ground language into pre-defined language formalisms, rather than extending the model to account for entirely novel input. Our grounded language learning incorporates a state-of-the-art model, FUBL, for semantic parsing, as re-viewed in this section. FUBL (Kwiatkowski et al., 2011) is an algorithm for learning factored Combina-tory Categorial Grammar (CCG) lexicons for seman-tic parsing. Given a dataset { ( x i ,z i ) | i = 1 ...n } of natural language sentences x i , which are paired with logical forms z i that represent their meaning, UBL learns a factored lexicon  X  made up of a set of lexemes L and a set of lexical templates T . Lex-emes combine with templates in order to form lexi-cal items, which can be used by a semantic parser to parse natural language sentences into logical forms. For example, given the sentence x = X  X his red block is in the shape of a half-pipe X  and the logical form z =  X x.color ( x,red )  X  shape ( x,arch ), FUBL learns a parse like the example in figure 2. In this parse, the lexeme (half-pipe, [ arch ]) has combined with the tem-plate  X  (  X ,~v ) . [  X  ` NP : ~v 1 ] to yield the lexical item half -pipe ` NP : arch .
 FUBL also learns a log-linear model which produces the probability of a parse y that yields logical form z given the sentence x : where  X  ( x,y,z ) is a feature vector encompassing the lexemes and lexical templates used to generate y , amongst other things.
 In this work, we initialize our parse model using the standard FUBL approach, followed by automatically inducing lexemes paired with new visual attributes not present in the initial training set, as we will see in the next section.
 As described in Sec. 2, the object selection task is to identify a subset of objects, G , given a scene O and an NL sentence x . We define a possible world w to be a set of classifier outputs, where w o,c  X  { T,F } specifies the boolean output of classifier c for object o . Our joint probabilistic model is: where the latent variable z over logical forms models linguistic uncertainty and the latent w over possible worlds models perceptual uncertainty.
 We further decompose (2) into a product of models for language, vision, and grounded execution. This final model selects the named objects G , motivated in Sec. 2 and described below; the final decomposition is: Here, the language model P ( z | x ) and vision model P ( w | O ) are held in agreement by the conditional prob-ability term P ( G | z,w ). Let z ( w ) be the set of objects that are selected, under the assignment in w , when z is applied to them. For example, the expression z =  X x.shape ( x,cube )  X  color ( x,red ) would return true when applied to the objects in w for which the classi-fiers for the cube and red logical constants return true. Now, P ( G | z,w ) forces agreement and models object selection by putting all of its probability mass on the set G that equals z ( w ).
 In this formulation, the language and vision dis-tributions are conditionally independent given this agreement. The semantic parsing model P ( z | x ) builds on previous work, as described in eqn. (1). The perceptual classification P ( w | O ) is defined as follows: we assume each perceptual classifier is applied independently, decomposing this term into: where the probability of a world is simply the product of the probabilities of the individual classifier assign-ments for all of the objects.
 Each classifier is a logistic regression model, where the probability of a classifier c on a given object o is: where  X  P c is the parameters in  X  P for classifier c . This approach provides a simple, direct way to couple the individual language and vision components to model the object selection task.
 Inference There are two key inference problems in a model of this type. During learning, we need to com-pute the marginal distribution P ( z,w | x,O,G ) over la-tent logical forms z and perceptual assignments w (see next section). At test time, we must compute arg max G P ( G | x,O ) to find the set of named objects. Computing this probability distribution requires sum-ming the total probability of all world/logical form pairs that name G . For each possible world w , de-termining if z names G is equivalent to a SAT prob-lem, as z can theoretically encode an arbitrary logi-cal expression that will name the appropriate G only when satisfied. Computing the marginal probability is then a weighted model counting problem, which is in #-P. However, the logical expressions allowed by our current grammar X  X onjunctions of unary attribute descriptors X  X dmit efficient exact computation, de-scribed below. The physically grounded joint learning problem is to induce a model P ( G | x,O ), given data of the form D = { ( x i ,O i ,G i ) | i = 1 ...n } , where each example i contains a sentence x i , the objects O i , and the selected set G i . We consider the case where the learner already has a partial model, including a CCG parser with a small vocabulary and a small set of attribute classi-fiers. The goal is to automatically extend the model to induce new classifiers that are tied to new words in the semantic parser. We first describe the learning algorithm, then present how we initialize the approach by learning decoupled models from small datasets with more extensive annotations.
 Aligning Words to Classifiers One key challenge is to learn to create new attribute classifiers associ-ated with unseen words in the sentences x i in the data D . We take a simple, exhaustive approach by creating a set of k new classifiers, initialized to uniform dis-tributions. Each classifier is additionally paired with a new logical constant in the FUBL lambda-calculus language. Finally, a new lexeme is created by pairing each previously unknown word in a sentence in D with either one of these new classifier constants, or the logi-cal expressions from an existing lexeme in the lexicon. The parsing weights for the indicator features for each of these additions are set to 0. This approach learns, through the probabilistic updates described below, to jointly reestimate the parameters of both the new clas-sifiers and the expanded semantic parsing model. Parameter Estimation We aim to estimate the language parameters  X  L and perception parameters  X 
P from data D = { ( x i ,O i ,G i ) | i = 1 ...n } , as defined above. We want to find parameter settings that maximize the marginal log likelihood of D : This objective is non-convex due to the sum over latent assignments for the logical form z and attribute classi-fier outputs w in the definition of P ( G i | x i ,O i ;  X  from eqn. (2). However, if z and w are labeled, the overall algorithm reduces to simply training the log-linear models for the semantic parser P ( z | x i ;  X  L ) and attribute classifiers P ( w | O i ;  X  P ), both well-studied problems. In this situation, we can use an EM algorithm to first estimate the marginal P ( z,w | x ,O i ,G i ;  X  L ,  X  P ), then maximize the expected like-lihood according to the distribution, with a weighted version of our familiar log-linear model parameter up-dates. We present an online version of this approach, with updates computed one example at a time.
 Computing Expectations For each example i , we must compute the marginal over latent variables given by: Since computing all possible parses z is exponential in the length of the sentence, we use beam search to find the top-N parses. This exact inference could be replaced with an approximate method, such as MC-SAT, to accommodate a more permissive grammar. Conditional Expected Gradient For each example, we update the parameters with the expected gradient, according to the marginal distribution above. For the language parameters  X  L , the gradient is where the inner difference of expectations is the fa-miliar gradient of a log-linear model for conditional random fields with hidden variables (Quattoni et al., 2007; Kwiatkowski et al., 2010), and is weighted ac-cording to the expectation.
 Similarly, for the perception parameters  X  P , the gra-dient is: where the inner sum ranges over the objects and adds in the familiar gradient for logistic regression binary-classification models.
 Online Updates We use a simple, online parameter estimation scheme that loops over the data K = 10 (picked on validation set) times. For each data point i consisting of the tuple ( x i ,O i ,G i ), we perform an update where we take a step according to the above expected gradient over the latent variables. We use a learning rate of 0.1 with a constant decay of .00001 per update for all experiments.
 Discussion This complete learning approach pro-vides an efficient online algorithm that closely matches the style of interactive, grounded language learning we are pursuing in this work. Given the decayed learning rate, the algorithm is guaranteed to converge, but lit-tle can be said about the optimality of the solution. However, as we see in Sec. 7, the approach works well in practice for the object set selection task we consider. Bootstrapping To construct the initial limited lan-guage and perceptual models, we make use of a small, supervised data set D sup = { ( x i ,z i ,w i ,O i ,G i 1 ...m } , which matches our previous setup but ad-ditionally labels the latent logical form z i and clas-sifier outputs w i . As mentioned above, learning in this setting is completely decoupled and we can es-timate the semantic parsing distribution P ( z i | x i ;  X  with the FUBL learning algorithm (Kwiatkowski et al., 2011) and the attribute classifiers P ( w i | O i ;  X  P ) with gradient ascent for logistic regression. As we show ex-perimentally, D sup can often be quite small, and will in general not contain many of the words and attributes that must be additionally learned in the full approach. Exploring approaches for learning without D sup , such as replacing it with interactive dialog with a human teacher, is an important area for future work. Data Set Data was collected using a selection of toys, including wooden blocks, plastic food, and build-ing bricks. For each scene, we collected short RGB-D videos with a Kinect depth camera, showing a per-son gesturing to a subset of the objects. Natural language annotations were gathered using Mechanical Turk; workers were asked to describe the objects be-ing pointed to in the video (see Fig. 3). The referenced objects were then marked as belonging to G , the posi-tive set of objects for that scene. A total of 142 scenes were shown, eliciting descriptions of 12 attributes, di-vided evenly into shapes and colors. In total, there were 1003 sentence/annotation pairs.
 Perceptual Features To automatically segment ob-jects from each scene, we performed RANSAC plane fitting on the Kinect depth values to find the ta-ble plane, then extracted connected components (seg-ments) of points more than a minimum distance above that plane. After getting segmented objects, features for every object are extracted using kernel descrip-tors (Bo et al., 2011). We extract two types of features, for depth values and RGB values; these correspond to shape and color attributes, respectively. During train-ing, the system learns logistic regression classifiers us-ing these features. In the initialization phase used to bootstrap the model, the annotation provides informa-tion about which language attributes relate to shape or color. However, this information is not provided in the training phase.
 Language Features We follow (Kwiatkowski et al., 2011) in including a standard set of binary indicator features to define the log-linear model P ( z | x ;  X  L ) over logical forms, given sentences. This includes indicators for which lexical entries were used and properties of the logical forms that are constructed. These features allow the joint learning approach to weight lexical se-lection against evidence provided by the compositional analysis and the visual model components. This section presents results and a discussion of our evaluation. We demonstrate effective learning in the full model for the object set selection task. We then briefly describe ablation studies and examples of learned models.. 8.1. Object Set Selection To measure set selection task performance, we di-vided the data according to attribute. To initialize the model, we used the data for six of the attributes to train supervised classifiers, and provided logical forms for the corresponding sentences to train the initial se-mantic parsing model, as described at the end of Sec. 6. Data for the remaining six attributes were used for evaluation, with 80% allocated for training and 20% held out for testing. Here, all of the visual scenes are previously unseen, the words in the sentences de-scribing the new attributes are unknown, and the only available labels are the output object set G . We report precision, recall, and F1-score on the set selection task. Results are averaged over 10 different runs with the training data presented in different ran-domized orders. The system performs well, achieving an average precision of 82%, recall of 71%, and a 76% F1-score. This level of performance is achieved rela-tively quickly; performance generally converges within five passes over the training data. 8.2. Ablation Studies To examine the need for a joint model, we measure performance of two models in which either the lan-guage or the visual component is sharply limited. In each case, performance significantly degrades. These results are summarized in Fig. 4.
 Vision In order to measure how a set of classifiers would perform on the set selection task with only a simple language model, we manually created a the-saurus of words used in the dataset to refer to target attributes containing, on average, 5 different ways of referring to each color and shape. To learn the unsu-pervised concepts for this baseline, we first extracted a list of all words appearing in the training corpus but not in the initialization data; words which appear in the thesaurus are grouped into synonym sets . To train classifiers, we collect objects from scenes in which only terms from the given synonym set appear. Any syn-onym set which does not occur in at least 2 distinct scenes is discarded. The resulting positive and neg-ative objects are used to train classifiers. To gener-ate a predicted set of objects at test time, we find all synonym sets which occur in the sentence x , and de-termine whether the classifiers associated with those words successfully identify the object.
 Averaged across our trials, the results are as follows: Precision=0.92; Recall=0.41; F1-score=0.55. These results are, on average, notably worse than the per-formance of the jointly trained model.
 Semantic Parsing As a baseline for testing how well a pure parsing model will perform when the perception model is ablated, we run the parsing model obtained during initialization directly on the test set, training no new classifiers. Since the parser is capable of gener-ating parses by skipping unknown words, this baseline is equivalent to treating the unknown concept words as if they are semantically empty.
 Averaged across our trials, the results are as follows: Precision=0.52; Recall=0.09; F1-score=0.14. Not sur-prisingly, a substantial number of parses selected no objects, as the parser has no way of determining the meaning of an unknown word.
 8.3. Discussion and Examples This section discusses typical training runs and data requirements. We present examples of learned mod-els, highlighting what is learned and typical errors, and then describe simple experiment investigating the amount of supervised data required for initialization. Classifier performance after training effects the sys-tem X  X  ability to perform the set selection task. Dur-ing a sample trial, average accuracy of color and shape classifiers for newly learned concepts are 97% and 74%, respectively. Although these values are sufficient for reasonable task performance, there are some failures X  for example, the shape attributes  X  X ube X  and  X  X ylin-der X  are sometimes challenging to differentiate. As noted in Sec. 4, the semantic parser contains lex-emes that pair words with learned classifiers, and fea-tures that indicate lexeme use during parsing. Fig. 5 shows some selected word/classifier pairs, along with the weight for their associated feature (each trial pro-duces a large number of such lexemes). The classifiers new0  X  new2 and new3  X  new5 are color and shape classifiers, respectively. As can be seen, each of the novel attributes is most strongly associated with a newly-created classifier, while irrelevant words such as  X  X hing X  tend to parse to null. The system must iden-tify which of the classifier types to use for novel words. We ran additional tests investigating whether the sys-tem is able to learn synonyms. Here, we split the data so that the training set has attributes learned during initialization, but are referred to by new, synonymous words. These runs performed comparably to those re-ported above; the approach easily learns lexemes that pair these new words with the appropriate classifiers. Finally, we briefly discuss the effects of reducing the amount of annotated data used to initialize the lan-guage and perception model (see Fig. 6). As can be seen, with fewer than 150 sentences, the learned grammar does not seem to have sufficient coverage to model unknown words in joint learning; however, be-yond that, performance is quite stable.
 This paper presents a joint model of language and perception for grounded attribute learning. Our ap-proach learns representations of the meanings of natu-ral language, using visual perception to ground those meanings in the physical world. Learning is performed via optimizing the data log-likelihood using an online, EM-like training algorithm. This system is able to learn accurate language and attribute models for the object set selection task, given data containing only language, raw percepts, and the target objects. By jointly learning language and perception models, the approach can identify which novel words are color at-tributes, shape attributes, or no attributes at all. We believe our approach has significant potential to scale to general language grounding problems. In par-ticular, our modular framework was designed to eas-ily incorporate future advances in visual classification and semantic parsing. We are also working to scale the complexity of the language and physical scenes, with the eventual goal of robust learning in completely un-constrained environments.

