 Methods that reduce the amount of labeled data needed for training have focused more on selecting which documents to label than on which queries should be labeled. One ex-ception to this [4] uses expected loss optimization (ELO) to estimate which queries should be selected but is limited to rankers that predict absolute graded relevance. In this work, we demonstrate how to easily adapt ELO to work with any ranker and show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval X  active learning Active learning, query selection
Research in information retrieval evaluation has examined how to construct minimal test collections [2], and the bal-ance between the number of queries judged and the depth of judging [3]. With respect to training rankers, most work has focused on document selection [1] or balancing number of queries with depth of documents judged using random query selection [5]. In this paper, we focus on selecting queries in order to most rapidly increase ranker retrieval performance.
In particular, we focus on the application of expected loss optimization (ELO) to query selection. Long et al. [4] used ELO to select queries for training but relied on having a ranker that estimated absolute graded relevance. We gener-alize this to work with any ranker  X  many of which induce a ranking but not absolute labels. To generalize to any ranker, we introduce a calibration phase over validation data.
In addition, although in theory ELO can be used with any performance measure for active learning, we show us-ing DCG loss (as done in [4]) leads to better performance whether DCG or NDCG is used as the final evaluation of ranker performance. We provide evidence this is because ELO using DCG loss tends toward queries that have both more relevant examples and many degrees of relevance.
ELO suggests that, given a set of candidate queries C , one pick the query q  X  X  for labeling where the expected loss is the greatest. Mathematically, we have: max where D is the given training data, and P ( Y | X q , D ) is a dis-tribution over graded relevance labels Y for the documents, X , to be ranked for the query q . M( r,y ) is a retrieval perfor-mance measure such as DCG that can evaluate the quality of a ranking, r , for a set of documents given a particular la-beling of the documents, y .  X  ( X q ) is simply a permutation of the documents and R( X q ) denotes the current ranking of the documents. For most retrieval performance measures, the inner max on the left-hand side of the difference is eas-ily found by sorting from highest relevance to the lowest. In order to estimate the label distribution P ( Y | X q , D ) Long et al. [4] relied on training an ensemble of models to predict absolute graded relevance. We generalize ELO to work with any ranker by mapping the current ranking model to a distribution over graded labels. To do so, we introduce a calibration phase where a classification model is trained over the labels of the top k documents according to the ranker in the validation data. 1 During active learning, the classifi-cation model is used to estimate the P ( Y | x q , D ) for each document x q  X  X q . The quantity in Eq. 1 is then estimated through sampling of the labels from this distribution.
Like most active learning evaluation settings, we start with some labeled data D that is randomly chosen, train the models on D , pick a number of queries to be labeled from the candidate set C , add those to D , and repeat this process for a number of iterations. The performance of the active learning strategy in augmenting D is judged at each itera-tion by evaluating the current induced rankers on held-out test data. We perform 20 iterations of labeling and based on the findings reported in [5] we label 15 documents per query (or the maximum available). We repeat this process five times, each time starting with a different set of labeled data and report averages. We report both DCG@10 and NDCG@10 (one can use DCG for selection but NDCG for evaluation and so forth).

We use the publicly available Yahoo! LETOR challenge dataset that has 3 splits: we treat the train split as the candidate data C , utilize the validation split for tuning the rankers X  parameters and training the calibration models, and use the test split for evaluation. We experiment with two rankers: one that does not produce absolute graded rele-vance (SVMRank) and one that does (Additive Regression (AR)). SVMRank labels 750 query-document pairs per it-eration where AR labels only 150. The difference is due to AR having a steeper learning curve.

We examine four query/document selection methods: (1) select queries and documents randomly (rndQ-rndD); (2) se-
We use the same validation data that is used for model parameter search  X  this ensures our method does not require any additional labeled data. lect queries randomly and select the top documents accord-ing to the current ranker (rndQ-topD); (3) select queries ac-cording to ELO with DCG@10 as the selection measure and the top documents according to the current ranker (dcgELOq); (4) same as 3 but with NDCG@10 instead (ndcgELOq).
Figure 1 shows the results for SVMRank ( solid ) and AR ( dashed ) when the evaluation measure is DCG@10 ( left ) and NDCG@10 ( right ). 2 Error bars are standard error about the mean over the five trials. As reported elsewhere [4], selecting the top documents performs as well or better than selecting documents randomly. Note that regardless of whether eval-uating by DCG or NDCG, using NDCG for selection (nd-cgELOq) leads to the worst performance. In contrast, using DCG for selection leads to the best performance across both rankers and both evaluation measures. Finally, we note that the differences between methods are less significant when evaluated by NDCG than DCG. This suggests that while the learners are more effective  X  finding more relevant results per query  X  they are contributing to the marginal relevance for each query according to NDCG. The perceived impact on user utility will likely depend on the scenario and the degree to which the task is recall-oriented. F igure 2: Rating distribution of selected queries.
Figure 2 displays the rating distribution of the training data collected at the last step of active learning as a per-
S VMRank and AR are displayed together for space and to emphasize the similarity in trends. We are interested in comparisons within each and not across the two. centage for the 15,000 labeled instances for SVMRank (the distribution trends for AR were nearly identical). Note that ndcgELOq selects far more irrelevant items than the other methods. This may seem surprising since NDCG selection is the same as DCG but normalized by the max estimate on the left-hand side of Eq. 1. However, for a poorly perform-ing query with a single relevant document, NDCG X  X  max will be 1 but current performance will be near zero. Thus, the selection method often selects queries with very few rele-vant documents. In contrast, dcgELOq not only obtains the largest percent of documents at the relevant side (labels 3,4) and fewest on the irrelevant side (label 0), it selects queries where a variety of relevance grades exist. This is consistent with the literature that biasing toward relevant documents is not sufficient in itself [1]  X  one also needs a variety of relevance grades present.
We presented a method that generalizes the applicability of ELO for query selection to any ranker. Our method also has the benefit of being less of a computational burden than training ensembles at each step prior to labeling. We also demonstrated that whether one cares about DCG or NDCG for performance, using DCG provides a more stable query selection method. This is because the nature of NDCG as a ratio pushes the selection toward queries that often have few relevant documents. In contrast, using DCG in the se-lection mechanism promotes queries that have more relevant documents, and the expected loss component ensures that there will be a variety of relevance grades  X  since current performance is far below the max. These insights may be useful in developing new query selection methods.
