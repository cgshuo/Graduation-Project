 Yi Zhang * ,1 1. Introduction
An adaptive personal information filtering system is an autonomous agent that delivers information to the user in a dy-namic environment over a period of time. The study of user profile learning is central to the filtering research. A common approach is to learn the user profile as a classifier, by adapting existing text classification/retrieval algorithms to classify incoming documents as either relevant or non relevant. New documents that are similar to relevant documents the user has seen before are usually delivered to the user. However, this approach is a very simple and limited view of user modelling.
It cannot address many practical issues such as the complex user criteria (for example, novelty ( Harman, 2003 )). Also, it is not clear how to fully take advantages of the contextual information, implicit and explicit user feedback that can be collected by a filtering system.

This study explores how to go beyond the bag of words model and topical relevance based filtering. To use rich user spe-cific information and satisfy complex user criteria, our approach is to represent the filtering system X  X  belief about each user, which is learned from multiple forms of evidence, as a probabilistic graphical user model. Our hypothesis is that the user models can be used in two directions. First, the models can provide guidance for the system analysist/designer, for example, helping the system designer to decide whether to collect or use certain user information. Second, the models can be directly used in the choice of a system action, for example, helping the system to decide whether to deliver a document to the user.
To test the hypothesis and explore the graphical modelling approach in both directions, we first carried out a user study to collect a new evaluation data set that contains thousands of extensive implicit user feedback (such as a user X  X  mouse usage, keyboard usage, document length), explicit user feedback about the news (such as novelty, relevancy, readability, authori-tativeness, and whether a user likes a document or not) along with other forms of evidence (such as news source informa-tion). We performed several experiments with the data collected. The experiments were designed to explore the potential of graphical models in the above two directions and answer the following two specific questions: (1) Can the graphical mod-elling approach help us better understand the domain? (2) Can the graphical modelling approach help us improve the per-formance of an adaptive information filtering system? The following sections report our efforts to answer the above questions. We begin with a review of some related work in
Section 2. Section 3 introduces the graphical modelling approach and how it could be used for adaptive filtering profile learn-ing. Section 4 describes our efforts towards collecting the new adaptive filtering data set and evaluating graphical models for the task of developing complex user models for filtering. Section 5 concludes our findings and discusses future work. 2. Background and related work
The goal of an information retrieval system is to find relevant information. The definition of relevance is the fundamental problem when designing and evaluating an IR system. Most of the standard evaluations are based on a narrow definition of  X  X  X opical relevance X  or aboutness (Voorhees &amp; Buckland, 2002 ). Recently, researchers have studied criteria beyond topical rel-evance. Carbonell and Goldstein (1998) studied combining query-relevance with information-novelty for retrieval and sum-marization and proposed Maximal Marginal Relevance (MMR) criterion to reduce redundancy. Varian (1999) considered the incremental value of a piece of information and argued that the standard way that presents documents  X  X  X n order of esti-mated relevance X  is not appropriate. Zhang, Callan, and Minka (2002) proposed a two stage filtering system to filter out rel-evant but redundant documents. Zhai, Cohen, and Lafferty (2003) went beyond independent relevance to model dependent relevance to retrieve documents that cover as many different subtopics as possible. Wang (1994) asked users to read aloud and think aloud while doing hard copy documents selection. After analyzing audio recording of the whole process, they pro-posed a relevance model based multiple criteria, such as personal knowledge, topicality, quality, novelty, recency, authority and author. Schamber and Bateman (1996) identified criteria underlying users X  relevance judgments and explored how users employed the criteria in making evaluations by asking users to interpret and manually sort the criteria independent of doc-uments. In previous work the word  X  X  X elevant X  was used ambiguously, either as a narrow definition of  X  X  X elated to the matter at hand X  (aboutness) or a broader definition of  X  X  X aving the ability to satisfy the needs of the user X . When the second defi-nition is used, such as in Schamber and Bateman (1996) , researchers were usually studying what this paper refers to as user _-
Despite the vocabulary difference, the work in this paper is motivated by these early works focused on  X  X  X ikability X . The work reported in this paper goes beyond relevance by (1) modelling the likability and other criteria as hidden variables; (2) quan-tifying the importance of various criteria based on probabilistic reasoning; and (3) combining these criteria with implicit and explicit user feedback in a single graphical model.

Now, IR research is moving into a context and user dependent scenario. For examples, SearchPad treated the previous information requests from the user as the context of the current query to improve retrieval results (Bharat, 2000, Shen, Tan, &amp; Zhai, 2005 ) treated the preceding queries and clicked document summaries as the context of the current query.
Not necessarily in the context of personalization, there is much prior research on using implicit feedback in the information retrieval community and user modelling community. Kelly and Teevan (2003) provided a review and classification of works in these areas according to the behavior category and minimum scope. There is also much related work on using implicit feedback to improve web retrieval performance (Anderson &amp; Horvitz, 2002; Sugiyama, Hatano, &amp; Yoshikawa, 2004; White,
Jose, &amp; Ruthven, 2006). These prior efforts suggested many possible behaviors (view, listen, scroll, find, query, print, copy, paste, quote, mark up, type and edit) on different scope (segment, object and class) for system designers to use as implicit feedback. However, much of the earlier work on personalization are incremental improvement in existing similarity based models, which will not be enough to address many practical issues such as confidence, privacy, authority, novelty, recency, long term and short term retrieval personalization ( Callan et al., 2003 ). How to use the rich personal and contextual infor-mation in a principled way to better satisfy the user X  X  information needs in a complex environment becomes an important problem.

There is much prior research on news customization (Ardissono, Console, &amp; Torre, 2001; Carreira, Crato, Goncalves, &amp; Jor-
Luparello, &amp; Roudaire, 1999). Billsus and Pazzani (1999) built a personal news agent that used time-coded feedback from the user to learn a user profile. However, their way of using time as feedback is rather heuristic. Morita et al. (1994) investigated implicit feedback for filtering news group articles. They treat reading articles for more than 20 s as positive feedback, and they found this can produce better recall and precision than user X  X  explicit rating.

Different graphical models, such as Bayesian networks, dependency networks, inference networks, and causal models, have been used to model computer software users (Horvitz, Breese, Heckerman, Hovel, &amp; Rommelse, 1998 ), car drivers (Pynadath &amp; Wellman, 1995 ), students ( Conati, Gertner, VanLehn, &amp; Druzdzel, 1997 ) and other social phenomena ( McKim &amp; Turner, 1997 ). Choosing the graphical modelling approach as a unified framework to combine multiple forms of evidence is motivated by the prior research. Recently, there has been some independent work using a different graphical modelling approach (dependency networks) to discover the relationships between implicit measures and explicit satisfaction, and using decision tree for prediction (Fox, Karnawat, Mydland, Dumais, &amp; White, 2005 ). They were focusing on predicting user satisfaction with web search results based on implicit measures gathered while users were conducting their searches and viewing results. Their findings justify the graphical modelling approach X  X  effectiveness in a closely related task. Our work differs from the previous work in that: we are focusing on the adaptive filtering task instead of web search; we carried out a detailed user study with human subjects in a news recommendation setting; we develop graphical models with very differ-ent structure and functional form, which will be discussed in detail later; we want to satisfy complex user criteria; we con-sider a very different set of explicit feedback, implicit feedback and contextual information for user modelling; and the findings and conclusions we reached are very different. 3. Graphical models for adaptive complex user modeling
The basic methodology behind the graphical modelling approach is to represent the system X  X  belief about a user as a graph that summarizes the conditional dependence relationships between user history and context. Each node in the graph represents a random variable, and each arc represents a conditional dependence between the variables. Variables that do not share an arc are conditionally independent given other variables. The graph can be either directed, such as Bayesian Net-works, or undirected, such as Markov Random Fields. We will focus on the directed graph in this paper. A graphical model includes the definition of the graph structure and a set of local conditional probability functions or potential functions. Struc-ture learning and probabilistic inference are the two key techniques while using graphical models.

Automatically learning the structure of the graph from the data can be achieved through two major approaches. The first approach assigns a score, such as the likelihood of the training data given the structure, to each candidate graph. Usually a structure with the best score is selected. The second approach finds some constraints and keeps the causal graph(s) consis-tent with these constraints are as valid. Besides the constraints automatically generated from the data, a person can also specify prior constraints based on domain knowledge.

In a graph, if there is an arc from node X to node Y if and only if X is a direct cause of Y , then we call the graph a causal graph . One of the major goals and advantages of structure learning is the ability to automatically learn the causal graph that encodes the causal relationships between variables. This will help us to understand the problem domain and answer ques-tions, such as whether liking a document causes increased reading time, or whether the authority of a page is important to the user. Some structure learning algorithms try to achieve the goal of causal discovery directly. These algorithms are new and subject to criticisms. However, because of the potential of these algorithms, their success in some domains ( McKim &amp;
Turner, 1997; Pearl, 2000; Spirtes, Glymour, &amp; Scheines, 2000 ), and the lack of causality based analysis in the information retrieval community, we decided to introduce this important technique to the IR community and apply it to the task of fil-tering in this paper.

As an example, let X  X  look at a simple constraint based causal learning algorithm that will be used later in Section 4.3: PC algorithm (Spirtes et al., 2000 ). To learn the causal structure, the PC algorithm begins with a complete graph, finds zero order conditional independence relations from the data, then removes edges that contradict the relations. The algorithm continues with first order conditional independence relations, and so on. Finally, the algorithm finds some head to head links, and ori-ents the links without producing cycles. The algorithm finds a set of models that can not be rejected on the basis of the data.
This algorithm is computationally efficient with polynomial time complexity. However, it assumes no hidden common causes, and the causal relationships are acyclic. These assumptions, especially the assumption of no hidden variables, may not hold in the real scenario.

Somealgorithmsmake fewerassumptions. For example,theFast Causal Inferencealgorithm(FCI)handles unmeasured hid-denvariables( Spirtesetal.,2000 ).Pearl(2000)andSpirtesetal.(2000) providedextensivedetaileddescriptionsaboutlearning structures with causal meanings, including hidden variables, cycles, and undirected graphs. More details of causal structure discovery are beyond the scope of this dissertation, and we refer the reader to these books for more information on this topic.
Probabilistic inference is one of the most important step in the graphical modelling approach. Probabilistic inference means computing the conditional probabilities P ( x F j x we need to estimate. There are several different algorithms for probabilistic inference, such as exact algorithms ( Pearl, 1988 ), sampling based algorithms (Tanner, 1996; Thomas et al., 1992; Mackay, 1998 ), variational algorithms ( Jaakkola &amp; Jordan, 2000; Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999 ), most likely configuration, parametric approximations (Minka, 2001;
Yedidia, Freeman, &amp; Weiss, 2000 ), and heuristic methods (Heckerman, Chickering, Meek, Rounthwaite, &amp; Kadie, 2000 ). Dif-ferent algorithms have different trade-offs between computational speed, implementation complexity, generality and accuracy.

Researchers have identified three major advantages for the graphical modelling approach. First, it provides inference tools to naturally handle situations of missing data because of the conditional dependencies encoded in the graph structure. Sec-ond, it can learn causal relationships in the domain, thus helping us to understand the problem and to predict the conse-quences of intervention. Third, it can easily combine prior knowledge (such as partial information about the causal relationship) with data. This approach has been applied to model computer software users ( Horvitz et al., 1998 ), car drivers (Pynadath &amp; Wellman, 1995 ), and students (Conati et al., 1997 ).

Because of these advantages, we hypothesize that the graphical modelling approach is a useful tool for filtering tasks, where we have complex user criteria, implicit and explicit user feedback, and contextual information. The filtering system X  X  belief about the user can be represented as a graphical model. The belief may be used in two ways, either guiding the system designer X  X  future actions (such as deciding whether to collect certain evidence), or directly guiding the choice of a system action (such as deciding whether to deliver a document to the user). 4. Experiments
To test the above hypothesis and explore the graphical modelling approach in both directions, we carried out several experiments. The experiments were designed to answer the following two specific questions:
Can the graphical modelling approach help us to better understand the domain? For examples, can the algorithm tell us what the relationships are between user actions and relevance of a document, how authority relates to the user preference for the page, whether the usage of a specific keyboard key is informative, how users differ from each other, and so on. This information may guide us in designing a better filtering system.
 Can the graphical modelling approach help us to improve the performance of an adaptive information filtering system?
For example, when a document arrives, can we better predict a user X  X  preference for the document? This prediction will be used directly in deciding whether to deliver the document to the user.

To answer the above questions, we exploit the three advantages of graphical models in the experiments. More specifi-cally, to see whether the proposed solution can help us to understand the domain better, we use the causal graph structure learning algorithms (advantage 2), together with some prior knowledge of the domain (advantage 3), to derive the causal relationships between different user feedback, actions and user context. To see whether the proposed solution can help us to improve an existing filtering system, especially in the situation of missing data, we use statistical inference tools to predict how much a user likes a document under different evidence missing conditions (advantage 1). Different graphical models are developed and evaluated for different purposes, either to understand the domain or to improve the prediction accuracy. Linear regression is also tried as an alternative approach to combine multiple forms of evidence. 4.1. Evaluation data
No existing filtering database contains the level of detail that we needed for our study, so we developed a web based news story filtering system to collect an evaluation data set (Fig. 1 ). The system included a crawler that constantly gathers infor-mation from 8000 candidate RSS news feeds (Pilgrim, 2002 ). The Lemur indexer indexed the crawled document stream incrementally ( Croft et al., 2004 ), and an adaptive filtering system constantly recommended documents to the users using a modified logistic regression algorithm (Zhang, 2004 ). Users read and evaluated the delivered documents. An example of the web interface after user login is shown in Fig. 2 .
 Twenty one paid subjects from 19 different programs at Carnegie Mellon University participated in the study for 4 weeks.
The subjects are otherwise not affiliated with our research. We expected to collect enough data for evaluation over this per-iod of time. The subjects were required to read the news for about 1 hour per day and provide explicit feedback for each page they visited. 2 Twenty eight users tried this system. However, only 21 users were official paid subjects, among which one worked only for 2 weeks and 20 worked for about 4 weeks.

We have collected 7881 feedback entries from all 28 users, among which 7839 were from the 21 official participants. Each entry contained several different forms of evidence for each news story a user clicked. was not to be exhaustive, but representative. The evidence can be roughly classified into the following five categories listed in Tables 1 X 5 . 4
News source information : For each news source (RSS feed), we collected the number of web pages that linked to it ( RSS _ link ),
Content based evidence : Three pieces of evidence were collected to represent the content of each document: the relevance 4.2. Preliminary data analysis
The means and variances of all variables are in Tables 1 X 5 . These basic descriptive statistics are very diverse. The values of some evidence may be missing; only the user actions and news source information were always collected. Out of the 7991 entries, only 4522 (57%) entries contain no missing values. The missing rate of each form of evidence is also reported in the tables. There are several reasons for the missingness. For example, the explicit feedback is missing because users did not al-ways follow instructions, the relevance score is missing for the first story in a class, and the topic _ familiar _ before values for many topics are missing because we only collected the topic specific answers for larger topics. We expect missing data to be common in operational environments.

The correlation coefficient between each piece of evidence and the explicit feedback user _ like is also listed ( corr ). The high correlation coefficients between user _ like and other forms of explicit feedback are not very interesting because we can only get explicit feedback after a user reads the document. The correlation coefficient between the relevance score and user _ like is 0.37, the highest among all forms of evidence that the system can get before delivering a document. This is not surprising since relevance is a major factor that influence user _ like judgements, while relevance score is the system X  X  estimation of how relevant a document is.

The correlation coefficients between user _ like and the topic information (Table 3 ) are relatively high. This suggests asking system would be helpful. Collecting this data requires little user effort, since a user only needs to provide information on the class level instead of document level. Section 4.4 will show how to use the information with other forms of evidence in a filtering system. The correlation coefficients between the news source information and user _ like are weaker (Table 4 ). The correlation coefficient between user _ like and each user action (Table 2 ) is even lower ( Table 1 ). Some actions, such as Tim-eOnPage , are more correlated with user _ like than other refined actions, such as NumOfPageDown . This finding agrees with (Claypool et al., 2001 &amp; Morita et al., 1994 ). 4.3. Understanding the domain using causal structure learning
In order to better understand the domain, we need to go beyond correlation and investigate the potential causal relation-ships between different variables. To accomplish this, PC algorithm (Section 3) is used to automatically learn a causal graph from the data collected. Before running the PC algorithm, we specify the prior knowledge developed by the author as tem-poral-tier constraints of variables before automatic structure learning:
Level 1, 3 nodes: Topic information (topic_info = h familiar _ topic _ before i ), news source information (RSS_info = h RSS _ link,
Level 2, 4 nodes: Hidden variables 6 , such as relevant novel authoritative and readable , that may affect a user X  X  preference for a Level 3, 2 nodes: System generated scores, such as topical relevance _ score and readability _ score ; Level 5, 12 nodes: User actions, such as milliseconds spent on a page ( TimeOnPage ) or the number of clicks on the Down
This informs the learning algorithm that a causation (indicated by ? ) prohibited. For example, TimeOnPage on the highest/latest level (5th level) could not be a cause of doc _ len on level 1. Although the prior knowledge is engineered by the author and is not guaranteed to be true, it may help the structure learning algorithms by using the constraints to make the search space smaller.

Why user _ like is on a higher level than user actions? We assume how a user likes (or will like) a document only depends on how well the document satisfies the user X  X  information need. It is a hidden variable that exists before the user reads the document. The user takes a series of actions to uncover the truth. We assume the user judgment of user _ like is the same as the hidden variable. This assumption is commonly used in the information retrieval community when collecting user assess-ments. Based on this assumption, user actions would not influence user _ like , thus we put user _ like before the actions. Similar assumptions are made for other hidden variables on level 2.

The graph structure learned is presented in Fig. 4 . This graph is a result of user data and human engineered prior knowl-edge combined. After learning from the data, the structure of the human engineered prior graph has changed greatly, with more than 80% links removed. It is very encouraging to see that the graph looks reasonable. According to Fig. 4 , whether a document is novel , relevant , authoritative , readable and whether a user is familiar with the topic before using the system familiar _ topic _ before and host _ link influence a user X  X  actions, such as the number of events on scroll bars ( EventOnScroll ).
Comparing Tables 2 X 5 with Fig. 4 , some variables are correlated with user _ like , however, there are no direct links between themand user _ like .Forexample,thecorrelationbetween relevancescore and user _ lik e is 0.39,whilethereisnodirectlinkbetween them. Does Fig.4 contradict Table5?Theanswer is X  X  X o X . Sincethereisnodirectlinkbetweenthem, relevance _ score isnota direct cause/result of user _ like . The subgraph user _ like relevant ? relevance _ score means relevance score and user likes share a com-mon cause, relevant . The correlation between relevance _ score and user _ like is due to the indirect causal relationship between refined actions, such as the number of times the page up key was pressed ( NumOfPageUp ), are several steps away from user _ like . This implies that these refined actions are less informative if we want to use the learned model to predict whether a user likes a document or not. This finding agrees with (Claypool et al., 2001 ) and Table 2 .

The node authoritative is directly linked to readability _ score and host _ link . The link between host _ link and authoritative confirms the existing approaches that use the web link structure to estimate the authority of a page ( Kleinberg, 1998 ).
The links between readability _ score , readable and authoritative are very interesting. They suggest the difficulty to understand a page may make the user feel it is not authoritative. Further investigation shows that although the percentage of not author-itative news is less than 15% in general, among the 187 news stories some users identified as  X  X  X ifficult X  using class labels, 73% were also rated not authoritative. This observation suggests that the estimation of authority of a page may be further improved using the content of a page.

There is a link among the four nodes relevant , novel , readable and authoritative . Further analysis show that the correlation between each pair is high ( Table 6 ). This suggests that the four variables influence each other one way or another. For exam-ple, the readability of a document may influence the user X  X  evaluation of authority, while whether a document is relevant or not may influence a user X  X  evaluation of novelty. There are two possible explanations: (1) This is an inherent property of the document; or (2) A user is likely to rate one aspect of the document higher than he should if the other aspects are good.
The link between readable and readability _ score is counter intuitive. To understand why it exists, we need to be aware that the causal relationships learned automatically are what the algorithm believes based on the evidence of the data, the assump-tions it makes, and the prior constraints we engineered. The relationships learned may contain error because the data is noisy, or the assumptions and the prior constraints may be wrong. For example, the PC algorithm assumes no hidden variables. How-ever, in additional to relevant, novel, authoritative, and readable , other hidden variables, such as whether a document is up-to-date, interesting, misleading, may exist and influence a user X  X  preference for a document ( Schamber &amp; Bateman, 1996 ). For another example, if the ratings influence each other, the prior constraints are no longer true. Instead of uncovering the whole truth, the causal model learned merely shed some light on the relationships between the variables. It only serves as a starting point for us, and more work is needed to better uncover the true. We delay the discussion of future work to Section 5. 4.4. Improving system performance using inference algorithms
To tell whether combining multiple forms of evidence using graphical models can improve system performance, we eval-uated the proposed solution on the task of predicting user _ like for a given a document. A reliable prediction helps the system decide whether the document should be delivered to the user.
To predict user _ like , the system learned a graphical model: the combination of a graph structure and a set of local con-ditional probability functions or potential functions. Doing inference over the causal structure learned in the previous section is difficult because of the cycles and a mixture of directed and undirected links. So, we tried the following directed acyclic graphical models.

GM _ complete an almost complete Gaussian network. In this graph, we order the nodes from top to bottom, and the par-representing the information about the news source and the topic in Tables 4 and 3 . actions =( TimeOnPage , etc.) is a 12 dimensional vector representing the user actions in Table 1 . user _ like is the target variable the system wants to predict. To make the probabilistic inference over the graphical models simple, we learned a special family of graphical models,
Gaussian networks. If the parents of node X are Y , P ( X j Y )= N ( m + W Y , R ), where N ( l , R ) is a gaussian distribution with mean l and covariance R . Using the BNT Toolbox ( Murphy, 2001 ), the maximum likelihood estimations of the parameters ( m , W , R ) were learned using the EM algorithm and junction tree inference engine (Cowell, Dawid, Lauritzen, &amp; Spiegelhalter, 1999) over the graphical models.

Baseline: We used a norm 2 regularized linear regression algorithm as our baseline. We chose this algorithm because of two major reasons. First, other researchers have compared this approach with several state of the art algorithms (e.g., logistic regression and SVM) and found it works well (Zhang &amp; Yang, 2003 ). Second, linear regression is equivalent to the maximum likelihood estimation of a conditional Gaussian model (Hastie, Tibshirani, &amp; Friedman, 2001 ), which assumes the conditional
Gaussian network, thus the major difference between LR models and the graphical models is due to the structure instead of the functional form.

We tried two special approaches to solve the missing evidence problem while using linear regression. The first approach builds a model that does not use the evidence that is missing for each missing situation ( LR _ different ). The second approach, mean substitution , replaces each missing value for an evidence with the average of the observed evidence ( LR _ mean ). For K different forms of evidence, the system may need to handle 2 regression models would have needed to be learned if we used the first approach, since K is higher than 15 in some of our experiments. Building 2 15 models is almost impossible for us, so a heuristic approach, which is discussed later, was used to make the experiments possible.

Not all 7991 cases collected in the user study were used in the experiments. We conducted two sets of experiments. For the first set of experiments, we used the 7952 cases for which user _ like was not missing. For the other set of runs, we used only the cases without missing values. In this task, the value of each variable is treated as a continuous value and is normalized to unit variance. Each model was learned from all information available on the first 2/3 cases, and tested on the remaining 1/3 cases. 4.4.1. Evaluation measures
The correlation coefficient between the predicted value of user _ like and the explicit user _ like feedback provided by the users was used as the evaluation measure. One baseline is using relevance _ score alone, which has a correlation coefficient of 0.367 with 95% confidence interval 0.33 X 0.40 on the last 1/3 of the 7952 cases. 4.4.2. Experimental results and discussions
Fig. 7 shows the effectiveness of different models at different testing conditions as indicated by the horizontal axis. From dicted the value of user _ like only given the value of RSS _ info at testing time.  X  X +actions X  means the user actions on the current and the LR _ mean model were trained with all evidence/features, and the learned models were independent of the testing conditions. LR _ different models were only trained with features provided at testing time, so there is one model per testing condition. 8
The results show that GM _ complete performs similarly to LR _ different . This is not surprising. Theoretically, if there are no LR _ different on a testing case with missing evidence.

Comparing the correlation coefficients under different testing conditions when using LR _ different or GM _ complete , we can see that as more forms of evidence are available, the performance improves. If only the news source information of a doc-ument ( RSS _ info ) is given, all models perform poorly. The readability _ score improves the system performance significantly. This is nice and interesting, because the evidence is user independent and can be estimated efficiently for each document.
The performance keeps improving as topic _ info and relevance _ score were added. To collect this data, we need user feedback on previous documents. The performance improvement is not very obvious when actions were added. This means that given ( user _ like ) much by observing these actions. However, this is only true when we use a model learned for all users and other forms of evidence are available. It does not mean the actions are useless if we learn user specific models, or if other forms of evidence (such as relevance _ score ) are not available. Meanwhile, since these actions are performed while the user is reading the current document, they could not be use for recommending the document. Instead, they may be used to predict how user likes the document, which can be for training the user model for recommending future documents or for collaborative doc-ument recommendations (Das, Datar, Garg, &amp; Rajaram, 2007 ).

The performances of LR _ mean and GM _ causal do not increase monotonically as more forms of evidence are added. They perform much worse than LR _ different and GM _ complete . Why does a structure that looks more causally reasonable perform worse than the simple GM _ complete ? We may answer this question better by comparing the underlying assumptions of these algorithms. GM _ complete only assumes the joint distribution of all variables is multivariate Gaussian. GM _ causal makes much stronger independence assumptions by removing some links between variables. As mentioned before, the causal rela-tionships learned automatically are not perfect. This may be the cause of the poor performance of GM _ causal . LR _ mean also suffers from the strong conditional independent assumptions.

Table 7 reports the performance together with the confidence intervals of all the models under the +relevance score and +actions conditions. Under both conditions, GM _ complete and LR _ different are statistically significantly ( t -test with 95% con-fidence interval) better than the baseline 0.367. LR _ mean and GM _ causal are significantly worse. It means using multiple forms of evidence either hurts or helps, depending on how they are used. Further analysis about the +actions runs shows that
LR _ mean gave explicit feedback too much weight and overlooked other less strong evidence. At testing time, it did not handle the problem of missing explicit feedback well and thus performed poorly. Although GM _ complete also gave very high weights to explicit feedback, it could infer the missing values based on other available evidence at testing time, thus performed better than LR _ mean . LR _ different did not consider explicit feedback for training, thus it did not overlook other forms of evidence, so it suffered less from the problem. LR _ mean may work reasonably well if explicit variables are not included, however missing strong evidence will still hurt the performance of LR _ mean to some extent when there is a large variance on how informative each piece of evidence. For the GM _ complete approach, a single model is needed to handle various evidence missing situa-tions. If we use the LR _ different approach, several models are needed. As we mentioned before, there are 2 missing combinations, and 2 K linear regression models are needed in order to handle all these situations using LR _ different approach. LR _ different may be preferred if K is small, while graphical modelling using GM _ complete may be a better approach to handle different data missing situations when K is large.

So far, all results are based on 7952 cases where some evidence may be missing. We also compared the models under different testing conditions using the 4522 cases that do not have any missing values ( Table 8 ). GM _ causal performed signif-icantly better than before. We need to be very careful with the structures while using the graphical modelling approach, since a structure that looks more reasonable may work poorly on the inference task. However, we could not draw any con-clusion on whether GM _ complete is better in general, because the answer may be different with different conditional prob-ability distributions, different data sets, or a better structure learning algorithm. 5. Summary
This paper describes a user study to collect an evaluation data for further research on building complex user models for recommender systems. It demonstrates that we can build a longer-term learning environment and collect a significant amount of data about a user X  X  interests with reasonably small effort. contains thousands of extensive implicit user feedback (such as a user X  X  mouse usage, keyboard usages, and document length), explicit user feedback (such as novel, relevant, readable, authoritative, and whether a user likes a document or not), and other forms of evidence (such as news source information). The basic characteristics, such as the means, for the multiple forms of evidence collected are very diverse. Most forms of evidence are correlated with user _ like . The correlation between implicit feedback and user _ like is much weaker than that between explicit feedback and user _ like . Compared with data collected by other researchers, this data set appears reasonable.

In general, the user study represented a real-world task in a more realistic setting, where users choose to create their own classes and read news using their own computers at the time and place they want. This realistic setting enables us to collect a very detailed filtering data set with ordinary people and available tools. The data is very diverse and possibly more powerful than existing filtering data sets created by NIST for TREC.

The data set is noisy, with many missing entries, and without thorough evaluation for all h document, user class, time i tuples. It would be relatively easy to create a cleaner data set later, but some of the characteristics, such as missing entries and diversity of variables, are common in the real world and unlikely to be eliminated entirely from operational system used by ordinary people.

We have analyzed the user study data using graphical models. The experimental results show that the graphical model-ling approach can help us to understand the causal relationships between multiple forms of evidence in the domain and ex-plain the real world scenario better. The results also show that the filtering system can predict user preferences more accurately with multiple forms of evidence than with a relevance model only.

In particular, we studied two problems that are important for adaptive filtering as well as user centered information re-trieval based on the graphical modelling approach. First, we studied the complex user criteria beyond topical relevance . More specifically, we have developed probabilistic user models with user _ like and other criteria as hidden variables. We have dem-onstrated how to quantify the importance of various criteria and combine these criteria with implicit and explicit user feed-back based on probabilistic reasoning. The work enables the system to go beyond relevance and develop more interesting and detailed data driven user models than prior research. This is partly because the framework has a better theory, and partly because the advantages of the proposed framework matches the task, where it is practical to collect enough training data to learn over a period of time.

Second, we explored how to solve the missing data problem faced by practical recommendation systems. Using more forms of evidence improves the system performance. However, as more forms of evidence are added, missing data is a com-mon problem because of system glitches or because the users will not behave as desired. A real system needs to handle var-ious missing data by either ignoring it or by estimating it based on what is known. The graphical modelling approach addresses this problem naturally. Simpler approaches, such as linear regression, were not designed to handle missing values.
In order to use them to combine multiple forms of evidence, extra handling of missing data is needed. LR _ different handles the problem by building many different models to be used at different data missing conditions. LR _ different and GM _ complete perform similarly. When there are few types of evidence, LR _ different probably is preferable because of the simplicity. How-ever, as more forms of evidence are added, a more powerful model, such as GM _ complete , may be preferable because of the computation and space efficiency.

In this paper, the system uses a user independent model to combine multiple forms of evidence and learns user depen-dent models to calculate some types of evidence, such as relevance scores. The major computation of the system was to learn user specific models, such as a relevance model. This means the computational complexity of using graphical models in this paper is similar to that of traditional filtering system. However, if we need to learn a user specific graphical model for infer-ence, the complexity would be higher.
 It is worth mentioning that all the inference tasks only considered documents that users clicked and assigned class labels.
The performance may be different on arbitrary h document, user class, time i tuples in a real system. A practical filtering sys-tem may ask users to create classes manually, or automatically create user classes and assign documents to classes.
We collected data only for documents clicked, and the performance of the algorithms may be different on a random sam-pled data set with both clicked and un-clicked documents. Further investigation to look at un clicked data is needed, which is a critical step to see whether the findings under the experimental setting described in this paper will help the system serve the user better in a real filtering environment.

This is the first step towards using the graphical modelling approach to build complex user models. The graphical mod-elling approach is a flexible framework. The proposed solution, especially the data analyzing methodology used in this paper, can also be used in other IR tasks where a rich user profile may help, such as context-based retrieval.

The research reported in this paper is far from the best and there is much room to improve. To mine the real cause-effects relationship underlying the filtering problem, an important future work is to iteratively use different techniques, validate recovered models, and add additional prior knowledge. To improve the prediction power of the modelling approach, an important future work is to compare different graphical models systematically. First, the prediction performance of the fil-tering system may be improved by adding model selection, variable selection, supervised variables discretization techniques.
Second, PC algorithm returns a local minimum in the space of equivalent Bayesian Network, and how to do inference based on the result is an interesting research problem. Based on Fig. 4 , other causal models for inference should also be considered besides the GM _ causal used in this paper. We can add links between explicit feedback variables or add links between RSS _ info and Topic _ info . Third, the PC algorithm used in this paper is designed to uncover the causal relationships. There are some other structure learning algorithms specially designed for optimizing prediction, such as Bayesian Network Classifiers dis-cussed in ( Friedman, Geiger, &amp; Goldszmidt, 1997 ). Comparing these different graphical models is a future work. Fourth, the missing not at random problem is not directly addressed in this paper, and it would be interesting to try structure learn-ing algorithms, such as ( Friedman, 1998 ), whose hypothesis fits better for this scenario.
 Acknowledgements This research was funded by National Science Foundation IIS-0713111, AFOSR/AFRL and the industry sponsors of the
Information Retrieval and Knowledge Management Lab at University of California Santa Cruz. Any opinions, findings, con-clusions or recommendations expressed in this paper are the author X  X , and do not necessarily reflect those of the sponsors. References
