 REGULAR PAPER Zhi-Hua Zhou  X  Min-Ling Zhang Abstract In multi-instance learning, the training set is composed of labeled bags each consists of many unlabeled instances, that is, an object is represented by a set of feature vectors instead of only one feature vector. Most current multi-instance learning algorithms work through adapting single-instance learning al-gorithms to the multi-instance representation, while this paper proposes a new solution which goes at an opposite way, that is, adapting the multi-instance rep-resentation to single-instance learning algorithms. In detail, the instances of all the bags are collected together and clustered into d groups first. Each bag is then re-represented by d binary features, where the value of the i th feature is set to one if the concerned bag has instances falling into the i th group and zero other-wise. Thus, each bag is represented by one feature vector so that single-instance classifiers can be used to distinguish different classes of bags. Through repeating the above process with different values of d , many classifiers can be generated and then they can be combined into an ensemble for prediction. Experiments show that the proposed method works well on standard as well as generalized multi-instance problems.
 Keywords Machine learning  X  Multi-instance learning  X  Classification  X  Clustering  X  Ensemble learning  X  Knowledge representation  X  Constructive induction 1 Introduction During the past decades, learning from examples became one of the most flourish-ing areas in machine learning. According to the label ambiguity , i.e., ambiguity of the labels of training examples, research in this area can be roughly categorized into three learning frameworks, i.e., supervised learning, unsupervised learning, and reinforcement learning [ 22 ]. Supervised learning attempts to learn a con-cept for correctly labeling unseen instances, where the training instances are with known labels and therefore the label ambiguity is the minimum. Unsupervised learning attempts to learn the structure of the underlying sources of instances, where the training instances are without known labels and therefore the label am-biguity is the maximum. Reinforcement learning attempts to learn a mapping from states to actions, where the instances are with no labels but with delayed rewards that can be viewed as delayed labels, and therefore its label ambiguity is between that of supervised learning and unsupervised learning.
 they were investigating the problem of drug activity prediction. Here, the training set is composed of labeled bags each consists of many unlabeled instances, and the goal is to learn some concept from the training set for correctly labeling un-seen bags. A bag is positively labeled if it contains at least one positive instance and negatively labeled otherwise. Note that a positive bag may contain hundreds of instances, among which maybe only one is really positive. This implies that the false positive noise may be overwhelmingly high if a multi-instance problem were regarded as a typical supervised learning problem through simply assigning the label of a bag to the instances in the bag. Therefore, common single-instance learning algorithms can hardly obtain good performance when being applied to multi-instance problems directly. In fact, it has been shown that learning algo-rithms ignoring the characteristics of multi-instance problems, such as the tradi-tional decision trees and neural networks, could not work well in this scenario [ 14 ].
 label ambiguity. In contrast to supervised learning where all training instances are with known labels, in multi-instance learning the labels of the training instances are unknown; in contrast to unsupervised learning where all training instances are without known labels, in multi-instance learning the labels of the training bags are known; in contrast to reinforcement learning where the labels of the training instances are delayed, in multi-instance learning there is not any time delay. Since multi-instance problems extensively exist but are unique to these addressed by previous learning frameworks, multi-instance learning has been regarded as a new learning framework [ 22 ], and attracted much attention of the machine learning community.
 that it is accompanied with an unusual representation. Actually, a bag corresponds to a real-world object while the instances in the bag are feature vectors describ-ing the object. In contrast to typical machine learning settings where an object is represented by only one feature vector, in multi-instance learning an object is represented by a set of (more than one) feature vectors.
 rithms can be adapted to multi-instance learning as long as their focuses are shifted from the discrimination on the instances to the discrimination on the bags. In fact, most current multi-instance learning algorithms can be viewed as going along this way, that is, adapting single-instance learning algorithms to the multi-instance representation.
 that is, adapting the multi-instance representation to single-instance learning algo-rithms. In detail, the bags are re-represented by features generated with the help of a clustering process such that the multi-instance problem becomes a single-instance supervised learning problem, which is then solved by an ensemble of classifiers. Since the clustering process is used to help change the representation, it can be viewed as a specific scheme of constructive induction [ 8 ]. Therefore, the proposed method is called C CE , i.e., Constructive Clustering-based Ensemble. Experiments show that C CE can work well on not only standard multi-instance problems, but also generalized multi-instance problems [ 33 ].
 dard and generalized multi-instance learning. Section 3 presents C CE . Section 4 reports on the experiments. Section 5 discusses on some related issues. Finally, Section 6 concludes. 2 Multi-instance learning Most drugs are small molecules working by binding to larger protein molecules such as enzymes and cell-surface receptors. For molecules qualified to make a drug, one of its low-energy shapes can tightly bind to the target area; while for molecules unqualified to make a drug, none of its low-energy shapes can tightly bind to the target area. In the middle of 1990s, Dietterich et al. [ 14 ] investigated the problem of drug activity prediction. The goal was to endow learning systems with the ability of predicting whether a new molecule was qualified to make some drug, through analyzing a collection of known molecules. The main difficulty of this task lies in that each molecule can have many alternative low-energy shapes, but currently biochemists only know whether a molecule is qualified to make a drug or not, instead of knowing that which of its alternative low-energy shapes re-sponses for the qualification. In order to solve this problem, Dietterich et al. [ 14 ] regarded each molecule as a bag, and regarded the alternative low-energy shapes of the molecule as the instances in the bag, thereby formulated multi-instance learning. They then proposed three axis-parallel rectangle (abbreviated as A PR ) algorithms to solve the drug activity prediction problem, which attempt to search for appropriate axis-parallel rectangles constructed by the conjunction of the fea-tures.
 under the multi-instance learning framework. They showed that if the instances in the bags are independently drawn from product distribution, then the A PR is P AC -learnable. Auer et al. [ 6 ] showed that if the instances in the bags are not indepen-dent then A PR learning under the multi-instance learning framework is NP-hard. Moreover, they presented a theoretical algorithm that does not require product distribution but with smaller sample complexity than that of Long and Tan X  X  al-gorithm, which was transformed to a practical algorithm named M ULTINST later [ 5 ]. Blum and Kalai [ 9 ] described a reduction from P AC -learning under the multi-instance learning framework to P AC -learning with one-sided random classification noise. They also presented a theoretical algorithm with smaller sample complexity than that of the algorithm of Auer et al. [ 6 ].
 gorithm, Diverse Density. This algorithm attempts to search for a point in the fea-ture space with the maximum diverse density, where the diverse density at a point in the feature space is defined to be a measure of how many different positive bags have instances near that point, and how far the negative instances are from that point. Many other practical multi-instance learning algorithms have been de-veloped during the past years, such as Wang and Zucker X  X  [ 32 ] Citation-k NN and Bayesian-k NN, Ruffo X  X  [ 28 ] multi-instance decision tree R ELIC , Chevaleyre and Zucker X  X  [ 11 ] multi-instance decision tree I D 3-MI and multi-instance rule inducer R
IPPER -MI , Zhou and Zhang X  X  [ 41 ] multi-instance neural network B P -MIP , Zhang and Goldman X  X  [ 36 ]E M -DD ,G  X  artner et al. X  X  [ 15 ] MI Kernel, Andrews et al. X  X  [ 4 ] MI S VM , Zhou and Zhang X  X  [ 42 ] multi-instance ensemble, and Xu and Frank X  X  [ 34 ]M I B OOST . It is noteworthy that almost all these algorithms attempt to adapt single-instance supervised learning algorithms to the multi-instance representa-tion, through shifting their focuses from the discrimination on the instances to the discrimination on the bags [ 42 ]. Nevertheless, multi-instance learning has already been applied to diverse applications including content-based image re-matching [ 16 , 17 ], computer security [ 28 ], web mining [ 40 ], etc.
 on multi-instance classification with discrete-valued outputs. Later, multi-instance regression with real-valued outputs was studied [ 3 , 27 ]. It is worth noting that multi-instance learning has also attracted the attention of the I LP community. It has been suggested that multi-instance problems could be regarded as a bias on inductive logic programming, and the multi-instance paradigm could be the key between the propositional and relational representations, being more expressive than the former, and much easier to learn than the latter [ 12 ]. Recently, Alphonse and Matwin [ 2 ] successfully employed multi-instance learning to help relational learning. At first, the original relational learning problem is approximated by a multi-instance problem. Then, the resulting data is passed to feature selection techniques adapted from propositional representations. Finally, the filtered data is transformed back to relational representation for a relational learner to learn. In this way, the expressive power of relational representation and the ease of feature selection on propositional representation are gracefully combined. This work con-firms that multi-instance learning can really act as a bridge between propositional and relational learning.
 sumptions of how the instances X  classifications determine their bag X  X  label, differ-ent kinds of multi-instance problems can be defined. Formally, let  X  denote the instance space and ={+ ,  X  X  denote the set of class labels. A multi-instance concept is a function on 2  X   X  . In standard multi-instance learning, this func-tionisdefinedasEq.( 1 ), where c i  X  C is a specific concept from a concept space C ,and X  X   X  is a set of instances. eralized multi-instance problems, i.e., presence-based MI , 1 threshold-based MI , and count-based MI . Presence-based MI is defined in terms of the presence of instances of each concept in a bag. For example, an MI concept of this category is  X  X nly if instances of concept c 1 and instances of concept c 2 are present in the bag, the class is positive. X  Threshold-based MI requires a certain number of in-stances of each concept to be present simultaneously. For example, an MI concept of this category is  X  X nly if more than n c 1 number of instances of concept c 1 and n 2 number of instances of concept c 2 are present in the bag, the class is positive. X  Count-based MI requires a maximum as well as a minimum number of instances of a certain concept in a bag. For example, an MI concept of this category is  X  X nly if at most max c 1 and at least min c 1 number of instances of concept c 1 and at most max c 2 and at least min c 2 number of instances of concept c 2 are present in the bag, the class is positive. X  The formal definitions of presence-based MI, threshold-based MI, and count-based MI are shown in Eqs. ( 2 ) X ( 4 ). a given set of concepts, is a counting function : 2  X   X  C  X  N which counts the number of a given concept in a bag, t i  X  Nand z i  X  N are respectively the lower and upper threshold for concept c i .
 MI, and count-based MI defined by Weidmann et al. [ 33 ], there is another setting of generalized multi-instance learning, which was defined by Scott et al. [ 29 ]. In this setting, the target concept is a set of points C ={ c 1 ,..., c k } , and the label for abag B ={ b 1 ,..., b n } is positive if and only if there is a subset of r target points C ={ c 1 ,..., c r } X  C such that each c j  X  C is near some point in B . It is evident that this setting is close to that of threshold-based MI. Scott et al. proposed the G
MIL -1 algorithm to solve this problem, which was then reformulated as a kernel algorithm [ 30 ], reducing the time complexity from exponential to polynomial. Later, this kernel was further generalized along the line of count-based MI [ 31 ]. This paper only considers Weidmann et al. X  X  [ 33 ] settings of generalized multi-instance learning since it seems they are more general. As mentioned before, since in multi-instance learning each bag is represented by a set of feature vectors, common supervised learning algorithms can hardly be ap-plied directly to obtain good performance. Actually, most current multi-instance learning algorithms were derived in nature through enabling single-instance su-pervised learning algorithms deal with objects described by feature vector sets instead of a single feature vector, which goes the way of shifting the focuses of the algorithms from the discrimination on the instances to the discrimination on the bags [ 42 ]. As such a strategy of adapting single-instance learning algorithms to meet the multi-instance representation has obtained some success, an opposite strategy, i.e., adapting the multi-instance representation to meet the requirement of existing single-instance supervised learning algorithms, can also be considered. This is really the start point of C CE .
 Since the labels of the instances are unknown, a clustering algorithm is employed to cluster the instances into d groups. Intuitively, since clustering can help find the inherent structure of a data set, the clustered d groups might implicitly encode some information on the distribution of the instances of different bags. Therefore, C
CE tries to re-represent the bags based on the clustering results. In detail, d fea-tures are generated in the way that if a bag has instance in the i th group, then the value of the i th feature is set to 1 and 0 otherwise. Thus, each bag is repre-sented by a d -dimensional binary feature vector such that common single-instance supervised classifiers can be employed to distinguish the bags.
 instances. Since there is no criterion available for judging which kind of clustering result is the best for the re-representation of the bags, a possible solution is to produce many classifiers based on different clustering results and then combine their predictions, which is adopted by C CE . Note that this is not a disadvantage but an advantage, because in this way, C CE can utilize the power of ensemble learning [ 13 ] to achieve strong generalization ability.
 as diverse as possible. In C CE , diverse classifiers can be easily obtained because they can be trained in different instance spaces. In fact, the clustering process in C
CE can be repeated many times, each time the instances are clustered into dif-ferent numbers of groups. These clustering results are then used to help represent the bags as binary feature vectors with different dimensions. Therefore, different classifiers can be trained with different dimensional feature vectors. This can be viewed as a specific process of manipulating the input features, which has been identified as an effective paradigm for generating diverse classifiers [ 13 ]. querying the clustering results, then feeds the generated feature vectors to their corresponding component classifiers, and finally obtains the classification from the ensemble. This implies that the clustering results, at least the center of each clustered group, should be stored such that the instances of the unseen bag can be assigned to appropriate groups through measuring their distances to different centers. Note that although C CE is not so efficient as multi-instance learning al-gorithms which do not query on training instances in prediction, such as R ELIC [ 28 ], it is more efficient than algorithms which require storing and querying all the training instances in prediction, such as Citation-k NN and Bayesian-k NN [ 32 ]. label of a bag is actually determined by the relationship between the feature vector set describing the bag and the target points in the instance space. In C CE ,sucha relationship is implicitly encoded in the single binary feature vector describing the bag. For example, assume that the task is to learn the problem  X  X nly if instances of concept c 1 and instances of concept c 2 are present in the bag, the class is positive, X  and assume that the instances have been clustered into a number of groups where several groups belong to concept c 1 and several belong to concept c 2 . Then, if the single binary feature vector of an unseen bag takes value 1 at a bit corresponding to any group belonging to concept c 1 as well as a bit corresponding to any group belonging to concept c 2 , this unseen bag is positive because instances of concept c and instances of concept c 2 are present in the bag. Therefore, it is evident that C
CE can be applied to generalized multi-instance problems without any modifica-tion, which is a prominent advantage, while most current multi-instance learning algorithms cannot.
 26 , 39 ] can be used to implement the clustering process and the classifier. In this paper, k -means is employed for clustering, while support vector machines are used as the classifiers. In combining the predictions of the classifiers, majority voting is used in this paper, but note that other schemes are also applicable. 4 Experiments 4.1 Musk data sets Musk data is a real-world benchmark test data for standard multi-instance learning algorithms, which was generated in the research of drug activity prediction [ 14 ]. There are two data sets, i.e., Musk1 and Musk2 , both publicly available at the UCI machine learning repository [ 7 ]. Musk1 contains 47 positive bags and 45 negative bags, and the number of instances contained in each bag ranges from 2 to 40. Musk2 contains 39 positive bags and 63 negative bags, and the number of instances contained in each bag ranges from 1 to 1044. Each instance in the bags is represented by 166 continuous attributes. Detailed information on the Musk data is tabulated in Table 2 .
 the instances into five different numbers of groups, and then five classifiers are trained and combined. The best predictive error rates of C CE are compared with the best results reported in the literatures, as shown in Table 3 . Note that these results were obtained with different experimental methodologies. For example, the results of MI Kernel 2 were average values of 1,000 runs each randomly leaves out 10 bags for testing while using the remaining bags to train the classifier [ 15 ], the results of the T LC algorithms 3 were obtained with 10 runs of 10-fold cross-validation [ 33 ], while the results of C CE were obtained with leave-one-out test. while on Musk2 it ranks the 5th. Note that among all these algorithms, besides C
CE , only MI Kernel and T LC without AS can be used to tackle generalized multi-instance problems [ 33 ]. It can be found from Table 3 that the performance of C CE is comparable to that of MI Kernel on Musk1 but worse on Musk2 , 4 while much better than that of T LC without AS on both Musk1 and Musk2 . These observations support the claim that C CE illustrates a new way to solve multi-instance problems, that is, adapting the multi-instance representation to common single-instance su-pervised learning algorithms.
 tering process employed by C CE , where the number of clusters ranges from 2 to 80. Figure 2 shows the best performance of the classifier ensembles generated by C CE , where the number of classifiers used ranges from 3 to 21 with interval 2. though the single classifiers are not strong. Actually, the best predictive accu-racy of the single classifier is 85.9% on Musk1 and 80.4% on Musk2 , both worse than that of Diverse Density (88.9% on Musk1 and 82.5% on Musk2 ); but the best predictive accuracy of the C CE ensemble is 92.4% on Musk1 and 87.3% on Musk2 , both much better than that of Diverse Density. Moreover, Fig. 2 reveals that no matter which ensemble size is used, the performance of C CE ensemble is always higher than 89.1% on Musk1 and 83.3% on Musk2 , consistently better than that of Diverse Density. These observations tell that the performance of the single classifier is relatively sensitive to the clustering process, while that of the C CE ensemble is relatively robust owing to the contribution of the ensemble process. 4.2 Generalized MI data sets Weidmann et al. [ 33 ] designed some methods for artificially generating general-ized multi-instance data sets. In every data set they generated, there are five differ-ent training sets each containing 50 positive and 50 negative bags, and a big test set containing 5000 positive and 5000 negative bags. The average test set accu-racy of the classifiers trained on each of these five training sets is recorded as the predictive accuracy of the concerned learning algorithm on that data set. Note that in this subsection, the results of MI Kernel and the T LC algorithms are from the literature [ 33 ].
 erate a positive bag, the number of instances in a concept was chosen randomly from { 1 ,..., 10 } for each concept. The number of random instances was selected with equal probability from { 10 | C | ,..., 10 | C |+ 10 } . Hence, the minimal bag size in this data set was | C |+ 10 | C | and the maximal bag size 20 | C |+ 10. In this pa-per, four presence-based MI data sets are used. The results are shown in Table 4 , where the numbers following  X   X   X  are standard deviations. Here, the name of the data set  X 2-10-5 X  means this data set was generated with 2 concepts, 10 relevant, and 5 irrelevant attributes.
 concepts. They chose thresholds t 1 = 4and t 2 = 2 for data sets with | C |= 2, and t 1 = 2, t 2 = 7and t 3 = 5 for data sets with | C |= 3. For positive bags, the number of instances of concept c i was chosen randomly from { t i ,..., 10 } . To form a negative bag, they replaced at least (( X , c i )  X  t i + 1 ) instances of a concept c i in a positive bag X by random instances. The minimal bag size in this data set is i t i + 10 | C | , the maximal size is 20 | C |+ 10. In this paper, four threshold-based MI data sets are used. The results are shown in Table 5 ,where the numbers following  X   X   X  are standard deviations. Here, the name of the data set  X 42-10-5 X  means this data set has at least 4 instances of the first concept and 2 instances of the second concept in a positive bag, with 10 relevant and 5 irrelevant attributes.
 concepts. They used the same value for both thresholds t i and z i . Hence, the num-ber of instances of concept c i is exactly z i in a positive bag. They set z 1 = 4and z with | C |= 3. A negative bag can be created by either increasing or decreasing the required number z i of instances for a particular c i . They chose a new number from { 0 ,..., z i  X  1 } X  X  z i + 1 ,..., 10 } with equal probability. If this number was less than z i , they replaced instances of concept c i by random instances; if it was greater, they replaced random instances by instances of concept c i . The minimal bagsizeinthisdatasetis i z i + 10 | C | , and the maximal possible bag size is results are shown in Table 6 , where the numbers following  X   X   X  are standard de-viations. Here, the name of the data set  X 42-10-0 X  means this data set requires exactly 4 instances of the first concept and 2 instances of the second concept in a positive bag, with 10 relevant and 0 irrelevant attributes.
 of T LC with AS, it is comparable to that of MI Kernel and T LC without AS. The fact that the performance of C CE on count-based MI is not well might be because the binary feature vectors used by C CE are not sufficient for representing the exact number of instances in a cluster. Note that the T LC methods were specifically designed for generalized multi-instance problems. Therefore, the experimental results tell that C CE can work well, at least not bad, on generalized multi-instance problems. 5 Discussion Weidmann et al. [ 33 ] proposed T LC to tackle generalized multi-instance problems, which constructs a meta-instance for each bag and then passes the meta-instance and the class label of the corresponding bag together to a common classifier. T LC uses a standard decision tree for imposing a structure on the instance space, which is trained on the set of all instances contained in all bags where the instances are labeled with their bag X  X  class label, such that a meta-instance is generated for each bag. It is obvious that the role of the decision tree in T LC canbetakenbysome other supervised learning algorithms such as rule induction algorithms. Neverthe-less, it is worth noting that T LC generates only one meta-instance for each bag. In contrast to T LC ,C CE employs clustering to impose different structures on the in-stance space, in each structure a meta-instance can be generated for each bag. That is, C CE generates multiple meta-instances for each bag, therefore it can utilize the power of ensemble learning in making predictions with an ensemble instead of a single classifier.
 multi-instance problems. In detail, they used a popular ensemble learning method to generate ensembles of multi-instance learners including Iterated-discrim A PR , Diverse Density, Citation-k NN, and E M -DD , and obtained better results than sin-gle learners. In contrast to C CE , the multi-instance ensemble method does not change the representation of the bags. Moreover, since the base learners it em-ployed were designed for standard multi-instance problems, these multi-instance ensembles could only be applied to standard multi-instance problems.
 machine learning paradigms. Although the structure of multi-instance data is not so complex as that of multimedia data, it is really more complex than traditionally used feature vector structure. In order to learn multi-instance data, a usually adopted way is to modify common learning algorithms to meet complex representations [ 42 ], while C CE goes an opposite way, i.e., simplifying complex representations to meet common learning algorithms. In fact, such an idea of changing the representation has been studied in the area of constructive induction as early as [ 25 ].
 features found in original data. Commonly, it improves the representation by constructing new features from the instance space, so that the learning tasks become easier to be performed or the learning results are improved. Roughly speaking, there are three kinds of constructive induction schemes classified according to the information used in searching for the best representation space [ 8 ], that is, data-driven constructive induction that exploits input ex-amples, hypothesis-driven constructive induction that exploits intermediate hypotheses, and knowledge-driven constructive induction that exploits domain knowledge. The clustering process employed by C CE is actually used to help construct new features from the original instances, which can be viewed as a data-driven constructive induction process. Therefore, the success of C CE also indicates that, although constructive induction is not so hot as before, this technique might be well proven useful in learning data with complex structures. 6Conclusion Most current multi-instance learning algorithms were derived from supervised learning algorithms through shifting their focuses from the discrimination on the instances to the discrimination on the bags, that is, adapting single-instance algo-rithms to the multi-instance representation. The main contribution of this paper is the illustration of the feasibility of an opposite way to the solution of multi-instance learning, that is, adapting the multi-instance representation to the single-instance algorithms.
 construct new features, on which common supervised learning algorithms can work. Besides, C CE utilizes the power of ensemble learning paradigms to achieve strong generalization ability. Experiments show that C CE can work well on stan-dard multi-instance problems. Moreover, experiments show that C CE can be ap-plied to generalized multi-instance problems without any modification, which is difficult for most current multi-instance learning algorithms.
 tion. For example, the number of instances of a bag belonging to the clustered groups can be used as feature values such that each bag is represented by an in-teger instead of a binary feature vector. Exploring other schemes for adapting multi-instance representation to single-instance algorithms is an interesting issue for future work.
 plex structures, constructive induction techniques might be useful. Trying to apply these techniques to tasks involving complex structures of data, such as multimedia stream data, is also an interesting issue to be explored in the future.
 References
