 Accurately and efficiently estimating the number of distinct values for some attribute(s) or sets of attributes in a data set is of critical importance to many database operations, such as query optimiza-tion and approximation query answering. Previous work has fo-cused on the estimation of the number of distinct values for a single attribute and most existing work adopts a data sampling approach. This paper addresses the equally important issue of estimating the number of distinct value combinations for multiple attributes which we call COLSCARD (for COLumn Set CARDinality). It also takes a different approach that uses existing statistical information (e.g., histograms) available on the individual attributes to assist estima-tion. We start with cases where exact frequency information on individual attributes is available, and present a pair of lower and upper bounds on COLSCARD that are consistent with the avail-able information, as well as an estimator of COLSCARD based on probability. We then proceed to study the case where only partial information (in the form of histograms) is available on individual attributes, and show how the proposed estimator can be adapted to this case. We consider two types of widely used histograms and show how they can be constructed in order to obtain optimal ap-proximation. An experimental evaluation of the proposed estima-tion method on synthetic as well as two real data sets is provided. H.2.4 [ Database Management ]: Systems X  Query Processing, Re-lational Databases Algorithms, Design, Experimentation Work done while visiting IBM Toronto Lab.
 Copyright 2005 ACM 1-59593-140-6/05/0010 ... $5.00.
 relational database, cardinality estimation
Most query optimizers in relational database systems require cost estimation of various candidate execution plans for a given query in order to select a good one. Good plan costing can help avoid plans that are intolerably slow compared to better access plans. Cardi-nality estimation of the intermediate results is key to getting better plan cost estimates. The number of distinct values in an attribute or the number of value combinations in a set of attributes is critical for cardinality estimation of various relational operators. For example, the textbook formula (which is also widely used in commercial sys-tems) for estimating the result size of R 1 ./ R 1 .a = R 2 .a of two relations R 1 and R 2 on a certain attribute a ,is where j R 1 j and j R 2 j are the cardinalities of the two relations, and V 1 and V 2 are the numbers of distinct values in R 1 respectively.

Besides its traditional role in query optimization, estimating the number of distinct values (or value combinations) is also of great value to fast approximate query answering [4], online aggregation [7], data exploration, among others. For instance, an online aggre-gation system might want to estimate the result size of a group-by query so as to show the users the percentage of groups that have been produced as the query is being processed.

Estimating the number of distinct values for a single attribute has been well-studied. It corresponds to the statistical problem of es-timating the number of species or classes in a population [2] and a considerable number of estimators have been proposed. Exten-sive research has also been done in the database literature, either by adopting those estimators proposed in the statistics literature to the database setting (e.g., [8, 9]), or by proposing new improved estimators (e.g., [5, 3]). Almost all previous work has taken the sampling approach, in which the estimate is derived by taking a sample of the data. The fact that useful statistical information is commonly available on the data set (stored in the catalog in the forms of histograms, etc.) has been largely overlooked.

This paper is concerned with estimating the number of distinct value combinations for a set of attributes in a table (henceforth re-ferred to as COLSCARD, which stands for COLumn Set CARDi-nality). Such estimation is required for many relational operations involving multiple attributes. For example, one might be interested in estimating the result cardinalities of the following queries be-cause these queries may be part of more complex queries or be needed for fast approximate answering purposes: and
Current commercial databases may use attribute-group statistics combinations on multiple attributes. However, these statistics are not always available, and it is not easy to decide what combinations one should collect statistics on, because the number of possible combinations can be huge. When there is no statistical information on the groups of attributes that we are interested in, naive estimators currently used in commercial systems do not work well and often produce large errors.

Instead of extending the sampling approach from the single-attribute to the multi-attribute case, we attempt to improve the qual-ity of estimation by making use of existing statistical information about individual attributes, which is usually maintained by the DBMS in the system catalog and readily available. Compared with the sampling approach, our approach does not require physical ac-cess to the data through disk I/O, and therefore tends to be much more efficient.

We start with the case where exact frequency information of all individual attributes involved is available. That is, we know exactly the number of times each value appears in each of the attributes. We first show that tighter upper/lower bounds can be achieved by ex-ploiting the frequency information. We then proceed to propose an estimator based on probability theory. We also provide a sampling method to trade off the computation cost and accuracy of estima-tion.

For the more general case where the system can only afford to maintain partial frequency information (in the forms of histograms, etc.) because of resource constraints and other considerations, we show how approximate estimation can be made, and study when the smallest approximation error can be achieved by choosing the right set of frequencies to be stored in the histograms. We present optimality results for two widely used classes of histograms.
The rest of the paper is organized as follows. Section 2 discusses the problem and introduce the notations. In Section 4, we present results for the case where exact marginal frequencies on individual attributes are available. Section 5 deals with the more general case where only partial information is available, and presents optimality results for two classes of histograms. Experimental evaluation is done in Section 6, and we conclude the paper and discuss future work in Section 7.
Virtually all previous work on estimating the number of distinct values has been focused on ways to derive an estimate through sam-pling the data. There exists a sizable bibliography in the statistics literature on this problem, where statisticians developed various es-timators for the purposes of estimating the number of species in an area or the number of classes in a population. Readers are referred to the survey by Bunge and Fitzpatrick [2] for further reference.
This problem has also been extensively studied in the database literature. Early work includes the results by Hou et al. [8, 9], Naughton and Seshadri [12], and  X  Ozsoyoglu [14], in which statisti-cal estimation methods are developed with database application in mind; see Olken [13] for an excellent survey. Haas et al. [5] did an extensive study of existing estimators from the statistical liter-ature and proposed several new sampling-based estimators, one of which is a hybrid estimator designed to take into account the degree of data skew, and is shown to outperform previous estimators over a wide range of attribute-value distributions. This result is further extended by Haas and Stokes [6], where the relationship between generalized jackknife estimators and other known estimators is re-vealed.

Estimating the number of distinct values through sampling alone is inherently difficult. Charikar [3] et al. establish a strong negative result showing that no estimator can guarantee small error across all possible attribute-value distributions with a reasonably small sam-ple. They present an estimator called GEE (Guaranteed-Error Es-timator), which is provably optimal in the sense that its worst-case error matches the lower bound of theoretical result within a small constant factor. Heuristic estimators are then presented to take ad-vantage of the knowledge of the data skewness obtained through the sample.

Gibbons [4] considers a related problem, where distinct samples of the data are obtained and stored for fast yet approximate distinct values query answering. The main objective of this approach is to get a synopsis of the data specifically tailored for distinct val-ues queries in order for such queries to be answered quickly (albeit approximately) with error guarantee without accessing the underly-ing data at query processing time. Unlike previous sampling-based methods where normally only a portion of the data is accessed, this approach requires a single scan of the data to build the synopsis.
All the aforementioned work does not consider the role of ex-isting knowledge about the data, and simply relies on sampling (or data scan in the case of distinct samples) to obtain information from the data. Such knowledge can potentially be helpful for the estima-tion and is usually abundant in modern database systems taking the forms of histograms, indexes, partial data cubes, and so on. Aside from moving the focus from single attribute to a set of attributes, our work is complementary to the existing approaches in that we consider how to utilize the knowledge in a consistent way to obtain quality estimates.
Before proceeding to discuss specific algorithms, we first for-mally define the problem of estimating the number of distinct value combinations as follows.

Definition 1. number of distinct value combinations in a set of attributes Given a selected set of attributes G =( A 1 ,A in a relation R , the number of distinct value combinations for G is defined as the number of distinct tuples in  X  G R (the projection of R on G ).

We use d i (1 i m ) to denote the number of distinct val-ues in attribute A i (1 i m ) in G , and use D G to denote the number of distinct value combinations. Assuming an (arbitrary) total ordering on the values in each attribute, we denote by vector v =( v 1 ,v 2 ,...,v m ) the value combination that takes on the th value in attribute A 1 ,the v 2 -th value in attribute We denote by V the set of all value combinations present in hence D G = jVj .Let f v be the normalized frequency of the value combination v such that
The first case we consider is when the exact marginal frequencies are known, i.e., the frequency vector f i for each attribute G ( i =1 ,...,m ) is known. Here the frequency vector f i =( f having the j -th value of attribute A i . As an example, if there are three distinct values in attribute A i , and they appear in and 140 times respectively, then f i =(0 . 1 , 0 . 2 , 0 .
When there exist functional dependencies among the attributes in G , they can be effectively utilized to obtain accurate estimates. For example, if we would like to estimate the COLSCARD for the set of attributes ( A 1 ,A 2 ) , and there happens to exist a functional dependency A 1 ! A 2 , then clearly the COLSCARD is equal to the number of distinct values in A 1 . Since such estimation is very straightforward, we will not consider it further in this paper. Our particular focus will be on the general cases where no functional dependencies can be readily exploited.
Assuming the data in different attributes are independently dis-tributed, the total number of possible value combinations would be COLSCARD. In fact, a naive estimator, which is currently used other commercial DBMS X  X , is as follows: where N is the size of the table, and serves as a sanity bound be-cause D G cannot be greater than the table size.

However, such an estimator is problematic in that it is just the maximum possible number of value combinations for G ;when j
R j is small, some less frequent value combinations may not appear in the relation at all.

E XAMPLE 1. Suppose we need to estimate the COLSCARD for three attributes A 1 ,A 2 , and A 3 , and we know that f 0 . 3 , 0 . 6) , f 2 =(0 . 01 , 0 . 99) , f 3 =(0 . 05 , 0 . 95) .
Scenario 1: N = 100 . One might want to estimate the number of possible value combinations to be 3 2 2=12 . However, the known f i  X  X  imply that the probability of occurrence of the least possible value combination is f 11 f 21 f 31 =0 . 1 0 . 01 0 0 . 00005 , which indicates that such a value combination is very unlikely to appear in a relation of size N = 100 . Therefore, the expected number of value combinations is expected to be less than 12.
 Scenario 2: When N is less than to  X  X runcate X  the estimated number of possible combinations from with a relation of size 11, we do not expect some value combina-tions with small occurrence probabilities to appear (e.g., the above combination with probability 0 . 00005 and the one with probability 0 . 3 0 . 01 0 . 05 = 0 . 00015 ). So again, the expected number of value combinations is expected to be less than the maximum possi-ble number, 11.

As shown in the preceding example, using the naive estimator is often erroneous, and better estimates are desired. In the follow-ing subsection, we first show that tighter bounds are available on COLSCARD than the trivial bounds, and then discuss means by which a good estimate can be obtained.
Let us first consider the following question: what can be a lower and an upper bound on COLSCARD? Naturally, for the set of at-tributes G =( A 1 ,A 2 ,...,A m ) , COLSCARD is lower-bounded by max f d 1 ,d 2 ,...,d m g , since it cannot be less than the number of distinct values in any of the constituent attributes. Moreover, as discussed earlier in this section, COLSCARD is upper-bounded by min trivial bounds.

Are tighter bounds available? The answer is positive for the case of two attributes. Specifically, we have the following theorems
T HEOREM 1. For attribute set G =( A 1 ,A 2 ) , we define 1 , 2; j =1 ,...,d i ) as the minimum number of different values that have to be combined with f ij given the marginals, i.e., l =min fjFj : Ff 1 ,...,d where i 0 =3  X  i , and S ( F )= is a lower bound on D G , and D ? G max f d 1 ,d 2 g .

T HEOREM 2. For attribute set G =( A 1 ,A 2 ) , we define 1 , 2; j =1 ,...,d i ) as the maximum number of different values that can be combined with f ij given the marginals, i.e., where i 0 =3  X  i .Then is an upper bound on D G , and D &gt; G min
Theorems 1 and 2 provide tighter lower and upper bounds than the trivial ones, as illustrated in the following example.
E XAMPLE 2. Consider the estimation of COLSCARD for G = (
A 1 ,A 2 ) , with N=10, f 1 =(0 . 4 , 0 . 4 , 0 . 2) , and f Clearly, the trivial lower and upper bounds are 3 and 9 respec-tively. The lower and upper bounds dictated by Theorems 1 and 2, on the other hand, are 4 and 5 respectively, which are tighter than the trivial bounds.

Computing D ? G can be done using the algorithm depicted in Al-gorithm 1, where for each f ij , we calculate the corresponding tribute have to be combined with it, and then add them up and get the final answer.

Obtaining the upper bound D &gt; G is much easier. For each compute u ij by simply comparing f ij N and d u  X  X  for each attribute and taking the lesser of the two sums give rise to the desired bound.
We now propose a method to estimate D G based on probability theory. Let f G = f 1  X  f 2  X  ...  X  f m be the Kronecker product (also known as direct product, tensor product, or outer product) of the frequency vectors of all attributes. Then f G is a vector of Algorithm 1 Computing D ? G sort f i ( i =1 , 2 )in descending order; for i =1 to 2 do end for length M = frequency f v of a specific value combination v . Clearly, the indi-vidual components of the vector f G addupto1,i.e., 1 . For instance, in the above example, f G = (0.00005, 0.00095, 0.00495, 0.09405, 0.00015, 0.00285, 0.01485, 0.28215, 0.0003, 0.0057, 0.0297, 0.5643), and it is easy to verify that they sum up to 1.

If we think of each element of f G as the probability of occurrence of the corresponding value combination, and each tuple in the re-lation R as independently drawn from M possible value combina-tions, then we have the following theorem for estimating COLSCARD:
T HEOREM 3. Under the following assumptions: 1. The data distributions of individual columns in G are inde-2. For the value combination v , its occurrence is the result of 3. The occurrences of individual possible value combinations the expected number of distinct value combinations is Proof Sketch For a value combination v 2V , let a random variate X v = 1 if the combination appears in R , and 0 otherwise. Then D dently drawn from M possible value combinations, then the prob-ability of value combination v not chosen in any particular draw is (1  X  f v ) . Therefore, under assumption 2, the probability of the combination v not chosen in any of the N draws and thus not ap-pearing in R is Pr ( X v =0)=(1  X  f v ) N . Hence the expecta-tion of X v is E [ X v ]=1  X  (1  X  f v ) N .Since X v  X  X  are inde-pendent according to assumption 3, E [ D G ]=
Applying sanity bounds, we use estimate. (The subscript G is dropped when there is no ambiguity.) Also, in the particular case of two attributes, we can use the tighter upper/lower bounds to bound the estimate.

E XAMPLE 3. Now we revisit the problem of estimating the num-ber of value combinations D G according to Theorem 3 under the settings of Example 1. It turns out that when N = 100 , D 5 . 94 ; while when N =11 ,D G =3 . 24 . These results are remark-ably different from the estimates suggested in Example 1.
It is worth pointing out that assumption 3 in Theorem 3 is ac-tually not true in theory, because the frequency vectors f actually impose constraints on the occurrences of different com-binations . For example, if we know f 11 N =2 ,then exactly two value combinations with A 1 = v 11 must appear in the table (where v 11 is the corresponding value of f 11 ). Therefore, the oc-currences of value combinations with A 1 = v 11 are not indepen-dent. Nonetheless, in practice, when the number of the possible combinations and the number of tuples are both large, individual constraints do not have a heavy impact on the occurrence frequen-cies, and assumption 3 approximately holds. This is confirmed by our experimental results.
To estimate COLSCARD, we need to first obtain f ,andthen compute the estimate using Equation 2, as illustrated in Algorithm 2. Algorithm 2 Computing the estimate 1: /* compute f G = f 1  X  f 2  X  ...  X  f m */ 2: f G = f 1 ; 3: for i =2 to m do 4: f old = f G ; 5: f G = fg ; /* empty vector */ 6: for j =1 to j f old j do 7: for k =1 to d i do 8: f G = concat( f G , f old 9: end for 10: end for 11: end for 12: /* compute the expected number of distinct value combinations 13: S =0 ; 14: for j =1 to M do 15: S = S +(1  X  f i ) N ; 16: end for 17:
When m or d i  X  X  are large, the algorithm may take a consider-able amount of time to run. Note that most of the time is spent on the computation of the Kronecker product. Therefore, we propose to use a sampling approach to reduce the cost. That is, we com-pute only a random selected portion of the f v  X  X , and obtain with proper scaling. More specifically, if we are to draw a sample of size M s , then at each step, we randomly draw an element from each f i , and include their product into the sample s , until the size of the sample s reaches M s . The sample estimate puted according to Eq. 2. The final estimate by scale only M s frequencies are computed as opposed to M in the exact version; therefore the computation time can be reduced by a factor pling scheme does work well, especially when the frequencies are relatively uniform.
Although knowing the exact marginals for each involved attribute can help us obtain accurate estimates if the assumptions hold, we often do not have such luxury. It is common practice in database systems that only concise synopses are maintained for the under-lying data because of practical considerations such as storage and computational efficiency. The synopses are used to reconstruct an approximate distribution of the data for various purposes, e.g., se-lectivity estimation, and approximate query answering. A number of choices exist for such synopses [1]; probably the most popular one is histograms [15], which have been extensively used in com-mercial database systems. Naturally, we would like to consider how to obtain the optimal histograms for our purpose of estimat-ing the number of distinct value combinations. Two broad classes of histograms, namely partition-based histograms, and end-biased histograms, are considered here. Partition-based histograms refer to the histograms that are obtained by partitioning the value do-main (with values sorted according to some sort parameter, follow-ing [15]) into buckets and approximating the frequencies in each bucket by a succinct measure (most commonly the average). Exam-ples of histograms belonging to this class include the widely used equi-depth and equi-width histograms. End-biased histograms, on the other hand, are obtained by first picking a number of values exactly, and then approximating the frequencies of the remaining values by a succinct measure (say, the average).
 We consider the following question for each class of histograms: Given the frequency vectors (either exact or approximate) of all other attributes in G , what is the optimal histogram for attribute for estimating COLSCARD? Let f  X  k be the Kronecker product of the frequency vectors excluding that of attribute A k ,i.e, f ...  X  f k  X  1  X  f k +1 ...  X  f m . Note that those frequency vectors can be either exact or obtained through reconstruction from histograms within each class is defined as the one that achieves an estimate D his that is closest to measure: where  X  f kj is the approximate frequency equal to the average of mathematical notations in the above equation: we have written brevity. Note that since the function h ( x )=(1  X  f  X  k vex, simplified as Partition-based histograms
The error incurred by the approximation made for the frequency in an interval [ a, b ] is where  X  f [ for the frequency vector ( v k 1 ,...,v ki ) using at most Then we have the following observation:
ERR ([ i, l ]) = min 1 which means that finding the optimal histogram of l buckets can be reduced to the case of finding an optimal histogram of buckets. This leads to a dynamic programming algorithm similar to that proposed for V-optimal histograms [10]. Detailed algorithmic development is omitted here because of space limitations. End-biased histograms
An end-biased histogram with B -buckets stores the exact fre-quencies of B  X  1 attribute values in B  X  1 singleton buckets, and the average of the frequencies of the remaining values in one  X  X lat X  bucket. The most widely used type of end-biased histograms keeps the exact frequencies of the most frequent values in hopes of cap-turing the majority  X  X ass X  of the frequency distribution. For our purpose of estimating COLSCARD, however, cropping the largest frequencies is no longer always the best option.

E XAMPLE 4. Suppose f  X  k =[0 . 1 , 0 . 9] , N=10. Consider the optimal end-biased histograms in the following two cases: (i) f (0 . 01 , 0 . 02 , 0 . 03 , 0 . 94) , and (ii) f k =(0 . 01 , 0 ure 1(a) shows the ERR abs associated with each choice of fre-quency when there is only one frequency (i.e., B =2 )tobestored exactly. Observe that for case (i), choosing the largest frequency (0.94) results in the smallest error among all choices. For case (ii), however, the optimal choice is the smallest frequency (0.01).
What if more than one frequency is stored exactly? Figure 1(b) shows the results for the case of B =3 ; i.e., two frequencies are chosen to be stored exactly. For illustration purpose, we also con-sider a third case, (iii) f k =(0 . 01 , 0 . 02 , 01 , 0 , storing the combination of the smallest and the largest frequencies is optimal, while for case(ii), it is the best to store the two small-est frequencies. For case (iii), on the other hand, storing the two largest frequencies leads to the smallest error.

A naive approach for identifying the optimal end-biased his-togram for attribute A k would be to consider all possible combi-nations of B  X  1 elements from the frequency vector f k . The errors incurred by storing each of the combinations exactly are computed and the one with the smallest error is chosen. This algorithm takes d .

Fortunately, the optimal choice of the set of frequencies takes a special form, which can be leveraged to safely ignore a large por-tion of the possible combinations. A rather counter-intuitive and surprising result, as we will show, is that optimality is achieved as the smallest, the largest, or a combination of the smallest and the largest frequencies are chosen to be stored exactly.
T HEOREM 4. Without loss of generality, assume that the fre-quencies of the values in attribute A k are sorted such that f 2 ... f kd k .Let ERR abs ( S ) be the error when the set of frequencies S is chosen to be stored exactly. With a space budget of B buckets, the set of frequencies to be stored exactly in an optimal end-biased histogram takes the form of S ( l )= f f k 1 ,...,f f f ku ,...,f kd k g , ( u  X  l = d k  X  B +2) , and there exists an optimal l such that ERR abs ( S ( l )) is minimized.
 Proof Sketch We first prove that the optimal set of frequencies to be stored exactly takes the form of f f k 1 ,...,f kl g[f ( u  X  l = d k  X  B +2) ; i.e., they are chosen from the smallest and largest frequencies.
 To show this, we examine the monotonicity of the error function, where T = frequencies to be stored exactly. We have and the minimum of ERR abs is reached at either end of the range of available f kj values.

Suppose there exists an optimal set of frequencies taking the form of f f k 1 ,...,f kl g[f f k  X  ,...,f k  X  g[f f ku ,...,f l +  X   X   X  = d k  X  B +3) such that  X   X  l&gt; 1 and u  X   X &gt; there are frequencies not belonging to either the set of smallest fre-quencies or the set of largest frequencies. Because of the concavity of
ERR abs , it is trivial to show that replacing any of the frequen-cies in f f k  X  ,...,f k  X  g by either f ERR abs . Therefore, the optimal set of frequencies cannot contain any frequency not belonging to the contiguous group of either the smallest or the largest frequencies. Hence, we conclude that the optimal set of frequencies to be stored exactly must take the form of f f k 1 ,...,f kl g[f f ku ,...,f kd k g ( u  X  l = d k
To obtain the optimal set of frequencies, we simply need to iden-tify the optimal l (or u ) that minimizes ERR abs , min
Theorem 4 makes efficient computation of end-biased histograms possible. To obtain the optimal end-biased histogram, one simply needs to evaluate ERR abs ( S ( l )) for 0 l B  X  1 , and choose the l corresponding to the smallest ERR abs ( S ( l )) . Clearly, the cost of such computation is linear in B ,andis independent of d k
In this section, we report the results of an experimental evalua-tion of the proposed estimation methods.
We studied the accuracy of the proposed estimator on synthetic as well as two real data sets  X  X over Type X  and  X  X ensus Income X  downloaded from UCI Machine Learning repository[16] previously used in the literature for evaluating estimators of the number of dis-tinct values [4, 3].

Synthetic data were used to study the property of the proposed estimator in a controlled manner. Specifically, the synthetic data were generated by varying the following characteristics:
The first real data set  X  X over Type X  is the Forest Covertype data from the National Forest Service. It has 581,012 tuples and 10 quantitative attributes, with the number of distinct values in indi-vidual attributes ranging from 67 to 5,827. The second real data set  X  X ensus Income X  is a data set derived from the U.S. Census database. It has 32,561 tuples and 14 attributes, with the number of distinct values in individual attributes ranging from 2 to 21,648.
The common error metric used in existing literature [4, 3] is the em ratio error metric, which is defined as error =max f D the ratio of the estimator and the true COLSCARD, where the ratio is inverted when necessary to ensure that the error is always greater than 1. We find it more intuitive to use a slightly modified error metric ERR = error  X  1 because ERR is 0 when the estimate coincides with the true value. Therefore, ERR is used in our eval-uation, and oftentimes, we use percentage representation in the text because it is more revealing.
Effect of data skew Figure 2 shows the effect of data skew on the proposed estimator as well as the naive estimator. We consider attribute pairs with different degrees of skewness, using to denote the Zipfian parameter of the two attributes respectively. The number of tuples is 100K. The first pair of attributes are both uniformly distributed ( z 1 =0 ,z 2 =0 ), with the number of dis-tinct values in both attributes being 1K. Because of the uniformity and the large possible number of combinations ( 1 K 1 K =1 which gives an estimate of 100K, does pretty well, though not as good as our proposed estimator, which produces an error of less than 0.1% and is thus almost invisible in the plot. When data be-come skewed, the naive estimator deteriorates very fast, giving er-rors in excess of 500%. The proposed estimator is highly accurate when one of the attributes involved is uniformly distributed, though tributes grows. The estimator gives larger errors for the case where both attributes are highly skewed ( z 1 =4 ,z 2 =4 ). However, in all cases, the proposed estimator produces small errors, demonstrating its robustness to data skew. Since the proposed estimator is orders of magnitudes more accurate than the naive estimator, we do not include the naive estimator in the following experiments.
Effect of number of tuples Our next experiment studies the ef-fect of the number of tuples on the estimation accuracy (Figure 3). We use three synthetic data sets with two attributes. In each data set, the two attributes follow the Zipfian distribution with the same parameter z . An interesting observation is that as N increases, the estimation error actually decreases. This can be partly explained by the fact that the  X  X andomness X  in the number of distinct value combinations appearing in the table decreases as N goes up. In the extreme case, when N approaches infinity, every possible combina-tion is expected to appear in the table, and COLSCARD therefore has a very low degree of randomness.

Effect of number of attributes We now study the effect of the number of attributes on the estimation accuracy. We estimate the COLSCARDs for groups of 2, 3, 4, 5 attributes respectively. Fig-ure 4 shows a typical result on the data set with number of attributes increases, which indicates that the proposed estimator can handle large groups of attributes as well. Therefore, for other experiments, we only present the results for the case of two attributes.
We carried out experiments on the two real data sets, and the results are reported below.

Error profile For both data sets, we experimented with all pos-sible attribute pairs (45 pairs for the Cover Type data, and 91 pairs for Census Income data), and the results are summarized in Fig-ure 5 showing the percentage of errors falling in each error range. As can be seen from Figure 5, the majority of the pairs have an esti-mation error of less than 5%. We looked closer into the cases where the error is greater than 100%, and discovered that those pairs are come data set with an error greater than 100% consists of  X  X duca-tion X (Bachelor X  X , Prof-school, etc.) and  X  X ducation-num X  (number of years of eduction), which are clearly very strongly correlated; therefore, the value combinations appearing in the data set is small (with COLSCARD being 32 while each attribute has 16 distinct values). Since our estimator assumes that the attributes are inde-pendent, it significantly overestimates the COLSCARD. If we have no knowledge available about the correlation, there is not much we can do to address this problem. However, when there exists infor-mation about correlation, it is possible to obtain better estimates. We plan to look into such cases for future work.

Sample rate In Section 3, we discussed using sampling to re-duce the computational cost. The next experiment studies the ef-fect of sample rate on the accuracy. In Figure 6, we show a typical result, obtained from estimating the COLSCARD for the first two attributes in the Cover Type data set (namely,  X  X levation X  and  X  X s-pect X ). As shown in the plot, the accuracy improves as sample rate increases. A similar trend was observed on other attribute combi-nations.

End-biased histograms We showed how to obtain an optimal end-biased histogram in Section 4. We conducted experiments to evaluate the accuracy of the histograms. Figure 7 shows the es-timation error due to the use of the end-biased histograms on the  X  X apital gain X  attribute (with 119 distinct values) in the attribute pair of  X  X ork class X  and  X  X apital gain X , as the number of buckets varies. As expected, the accuracy improves as the number of buck-ets increases. Similar results are obtained on other attributes.
Estimating the number of distinct value combinations is an im-portant task for both query optimization and approximate query answering. Current commercial systems often have to settle with naive estimates (with large errors) in the absence of important sta-tistical information on the attribute group involved. Different from previous work which studies the estimation of the number of dis-tinct values in a single attribute, we attack the more general prob-lem of estimation for multiple attributes. Instead of extending the sampling framework that has been extensively used for estimation for single attributes, we take a complimentary approach by focus-ing on utilizing existing knowledge maintained in the database cat-alog. We discussed the case where exact frequency information is available on individual attributes, and derived a pair of upper/lower bounds on the COLSCARD for attribute pairs. We also proposed an estimator by treating the frequency information as probabilities and computing the expected number of distinct combinations appearing in the table. For cases where only partial information is available, we studied how histograms can be used to get an approximate esti-mate, and we show optimality results for two widely used types of histograms. Experimental results have shown the effectiveness of the proposed estimator.

For future work, we would like to consider the case where in-formation is available on some subsets of the attribute group for which an estimate is required. Examples of such information in-clude joint indexes, multidimensional histograms, statistical views, etc. They offer the opportunity of improving the estimation ac-curacy, because they usually capture some correlation information between attributes. A key challenge here is to make consistent us-which was previously used for selectivity estimation of conjuncts of predicates[11], can be helpful in this regard. [1] D. Barbar  X  a, W. DuMouchel, C. Faloutsos, P. J. Haas, J. M. [2] J. Bunge and M. Fitzpatrick. Estimating the number of [3] M. Charikar, S. Chaudhuri, R. Motwani, and V. Narasayya. [4] P. B. Gibbons. Distinct sampling for highly-accurate answers [5] P. J. Haas, J. F. Naughton, S. Seshadri, and L. Stokes. [6] P. J. Haas and L. Stokes. Estimating the number of classes in [7] J. M. Hellerstein, P. J. Haas, and H. J. Wang. Online [8] W.-C. Hou, G.  X  Ozsoyoglu, and B. K. Taneja. Statistical [9] W.-C. Hou, G.  X  Ozsoyoglu, and B. K. Taneja. Processing [10] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, [11] V. Markl, N. Megiddo, M. Kutsch, T. M. Tran, P. Haas, and [12] J. F. Naughton and S. Seshadri. On estimating the size of [13] F. Olken and D. Rotem. Random sampling from databases -a [14] G.  X  Ozsoyoglu, K. Du, A. Tjahjana, W.-C. Hou, and D. Y. [15] V. Poosala, Y. E. Ioannidis, P. J. Haas, and E. J. Shekita. [16] C. B. S. Hettich and C. Merz. UCI repository of machine IBM, DB2, DB2 Universal Database are trademarks or registered trademarks of International Business Machines Corporation in the United States, other countries, or both.

Other company, product, or service names may be trademarks or service marks of others.
