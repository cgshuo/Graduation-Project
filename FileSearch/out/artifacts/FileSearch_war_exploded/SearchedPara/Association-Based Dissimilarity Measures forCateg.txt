 The most common similarity measures for categorical data are binary vector-based methods ([1] and references therei n). These methods transform each data object into a binary vector, at which ea ch bit indicates the presence or ab-sence of a possible attribute value. Then the similarity between two objects is estimated by the similarity between two corresponding binary vectors. These methods contain two main drawbacks: (1) the transformation of data objects into binary vectors may leave out many subtleties of the data; (2) they do not consider the correlations b etween attributes that typi cally exist in real-life data and are potentially concerned with the difference among attribute values.
Recently, Le and Ho presented the condition probability-based measure [2] based on relations among condition probability distributions of attributes. Their experiments showed that the condition probability-based measure gave better results than other tested methods. However, the method did not work properly with databases whose attributes are likely independent.

In this paper, we propose a dissimilarity measure for categorical data based on three main points. First, we employ the idea from text mining [3] and taxon-omy [4] that is to weight attribute values by its frequency in databases and in its conditional distributions. Second, the dissimilarity among weighted condition distributions of attributes are taken into account to estimate the dissimilarity between categorical values. This point is inherited from [2] but we now con-sider dissimilarity between weighted condition distributions instead of condition probability distributions, as was used in [2]. Finally, we include the dissimi-larity between weighted condition distributions and weighted distributions of attributes. The intuition behind the idea is that the dissimilarity between the weighted condition distributions conditioned on a value, and the weighted distri-butions of attributes shows reliability when considering the weighted condition distributions as the preventative of this value. The proposed measure overcomes the limitations of the condition probability-based measure [2] when applied to databases whose attributes are independent. Experiments with 30 databases also showed that the proposed measure boos ted the accuracy of Nearest Neighbor classification [5] in comparison with the condition probability-based measure, and the binary vector-based measures. Let A 1 ,...,A m be m attributes and D  X  dom ( A 1 )  X  ...  X  dom ( A m )bea database, N = | D | .Denote fr ( A i = x i ) the frequency of value x i of attribute A value x j of attribute A j given that attribute A i holds value x i . Consider value x j of an attribute A j ,itsweight w ( x j ) can simply be defined as
We restrict the term weight w ( x j )ofvalue x j of attribute A j to conditional weight w ( x j | A i = x i )ofvalue x j given that attribute A i holds value x i .
The intuition is that the less frequency fr ( A j = x j ) in database D but more conditional frequency fr ( A j = x j | A i = x i ), the greater the conditional weight w ( x j | A i = x i ). This idea is commonly used in text mining([3] and references therein) and taxonomy [4].
 x ): x j  X  dom ( A j )). We can consider these vectors as approximations of weight distribution and conditional weight distribution of values of A j , respectively, and called them hereafter weight distribution and conditional weight distribu-tion of A j .Denote Eucl ( W x i j ,W y i j ) the Euclidean distance between these two distributions.
 Definition 1. The dissimilarity of two values x i and y i of an attribute A i ,de-noted  X  ( x i ,y i ) , is defined as
It can be seen from Definition 1 that the dissimilarity between two different values x i and y i is based on three factors. The first one is the dissimilarity between the conditional weight distribution W x i j and W y i j with the assumption that the less dissimilar between the conditional weight distribution W x i j of A j when A i = x i ,and W y i j when A i = y i , the less dissimilar between x i and y i . The second (third) factor is the dissimilarity between the weight distribution W j the more dissimilar between W j and W x i j the more reliable when considering the dissimilarity between W x i j and W y i j as the dissimilarity between x i and y . The exponential function is used with the purpose to be applicable when dissimilarities between weight distributions are 0.
 Definition 2. The dissimilarity of two objects x and y is defined as the average dissimilarity of their attribute value pairs. 3.1 Characteristics Proposition 1. Given any objects x and y , it holds true for 1.  X  ( x , y )  X  0 2.  X  ( x , y ) =0ifandonlyif x = y 3.  X  ( x , y ) =  X  ( y , x ) Let J be Jaccard similarity measure J ( x , y )= a m where a is the number of identical value pairs of x and y .
 Proposition 2. If m attributes A 1 ,...,A m are all independent of each other, then  X  =1  X  J .
 Hence,  X  ( . ) can be applied to databases whose attributes are absolutely inde-pendent. This overcomes the limitations of measures based on relations among attributes, the condition probability-based measures [2]. 3.2 Algorithm and Complexity Now we present a three-step algorithm to measure the dissimilarities of all pairs of data objects of a data set D . At the first step, the weighted contributions of attributes and condition weighted contributions are estimated. Then, the dis-similarities between value pairs are c omputed based on the weighted condition contributions and the weighted contributions. Finally, dissimilarities between data objects are determined by Definition 2. Obviously, the complexity of the of the condition-based dissimilarity measure [2]. In this section we show the merit of our approach when it is applied to real data. To this end, we compared the propo sed measure with the binary vector-based measures of two families S  X  and T  X  [1], and the condition probability-based measure [2]. S  X  and T  X  include most of the popular binary-based similarity measures (see [6]). To compare similarity measures, we combined these measures with the popular distance-based data mining method, nearest neighbor classifier (NN), and analyze the accuracies of NN. 4.1 Methodology and Databases We compared the accuracy of NN in comb ination with the proposed measure (denoted  X  1 ) with the accuracy of NN when comb ined with a similarity measure of family T  X  or S  X  , or the condition probability-based measure (denoted by  X  0 ) [2], using the 10-time, 10-fold cross-validation strategy (see [6] for more detail). To avoid bias on data selection, we used 30 data sets from UCI [7] (see Table 1), for which numerical attributes are automatically discretized using the data mining system CBA [8]. 4.2 Results and Discussion Table 1 shows experimental results of 30 databases including accuracies of NN with the proposed measure (the third co lumn), accuracies of NN with binary based-measures and Z values when comparing wit h those of NN with the pro-posed measures (the fourth and fifth co lumns), and accuracies of NN with con-ditional probability-based measure and Z values when comparing with those of NN with the proposed measure (the last two columns).

Consider 95% significant level ( Z =1 . 64). As can be seen from Table 1 that, the accuracy of NN with the proposed measure  X  is higher than the accuracy of NN with a measure of S  X  or T  X  in 17 databases and only lower than in one. Similarly, 14 over 30 databases NN with the proposed measure  X  is better than with the conditional probability-based measure. However, we observed 6 cases in which NN with the conditional probability-based measure outperforms NN with the proposed measure. The number of databases for which NN with the proposed dissimilarity measures is the most accurate is 17, while that number is 3forameasureof S  X  or T  X  , and 9 for the condition probability-based measure.
The average accuracy of NN with the proposed measure  X  over 30 databases is 1.74% (1.73%) higher than NN with a measure of S  X  or T  X  (the condition probability-based measure).

In a nutshell, the proposed measure bo osts the accuracy of NN in comparison with the condition probability-based measure and binary vector-based measures. However, none of the measures outperforms completely the others. In this paper, we presented a similarity measure for categorical data based on relations between attributes. Experiments with a large amount of data showed that the nearest neighbor classification using this proposed measure achieved higher accuracy than using the condition probability-based measure and binary vector-based methods. More importantl y, the proposed method overcomes the limitations of the condition probability-based measure when applied to databases whose attributes are independent.
 We appreciate professor Judith Steeh at Japan Advanced Institute of Science and Technology, Japan for helpful comments on the manuscript.

