 Clustering text documents into different category groups is an important step in indexing, retrieval, management and mining of abundant text data on the Web or in corporate information systems. Many algorithms to address clustering problems have been developed accordin g to the specific domain requirements [1]. Usually, the distance or dissimilarity measures of these algorithms involve all features of the data set [2,3]. This is applicable only if all or most features are important to every clust er. However, in text documents, clusters are often discovered in different feature subspaces , i.e., different clusters have different subsets of important features or key words.
 rithms to be efficient, scalable and abl e to discover clusters from subspaces of the text vector space model ( VSM ). Scalable subspace clustering methods are made good candidates for text clustering [4], while other clustering algorithms often fail to produce satisfactory clustering results. Many existing subspace clus-tering algorithms [5] are designed for structured data, so they can not effectively address the following two special issues: high dimensionality and sparsity, which are problematic in text mining.
 ing algorithm with feature weighting capability was studied by Jing et al. [6]. We denoted it as FW-KMeans . This algorithm can automatically compute fea-ture weights based on the importance of the features in clustering. The more important the term for one cluster, the higher the weight. A weight calcula-tion formula that minimizes the cost function of clustering is provided. This algorithm is aimed to solve the challenging problems of big volume and high di-mensionality in text clustering. In the previous work [6], we have demonstrated that FW-KMeans outperformed the Bisection KMeans [7] and Standard KMeans algorithms. The main contribution of this work is to present some theoretical explanations and empirical tests of the FW-KMeans algorithm. The empirical tests were conducted with the real-world text data set 20-Newsgroups. We used the unified preprocessing steps for all experiments. The main results obtained can be summarized as follows:  X  Convergency: FW-KMeans quickly converges to a local optimal solution,  X  Scalability: FW-KMeans is scalable [8] to the number of documents, terms clustering with the feature weighting k -means algorithm. Section 3 analyzes the algorithm in terms of resource consumption. Following the discussion of our method, we present, in Section 4, the empirical results of our experiments in both scalability and convergency. Finally, we draw some conclusions and point out our future work in Section 5. In our research, documents are represented using a bag-of-words representation [9]. In this representation (also named as VSM ), a set of documents are repre-sented as a set of vectors X = {X 1 , X 2 ,..., X n } . Each vector X j is characterized by a set of m terms or words, ( t 1 ,t 2 ,...,t m ). Here, the terms can be considered as the features of the vector space and m as the number of dimensions represent-ing the total number of terms in the vocabulary. Assume that several categories exist in X , each category of documents is characterized by a subset of terms in the vocabulary that corresponds to a subset of features in the vector space. In this sense, we say that a cluster of documents is situated in a subspace of the vector space. 2.1 Subspace Clustering A simple example is given in Table 1 to show that it is necessary and effective to use subspace clustering on text data. Here, x j represents the j th document vector; t i represents the i th term; each cell in the table is the frequency that term t i occurs in x j .
 that the term does not appear in the related document. Table 1 shows that the number of terms covered by each document is much smaller than the total number of terms in the vocabulary. For instance, t 3 does not occur in cluster C ; and so do t 0 and t 1 in cluster C 1 , which results in the special characteristic of text data: sparsity. Meanwhile, this situation requires text clustering to be focused on subspace instead of the en tire space. For example, cluster C 0 can be represented by t 0 , t 1 , t 2 and t 4 without t 3 ;cluster C 1 represented only with t , t 3 and t 4 . Additionally, in text clustering, a good term or word will appear in the majority of the documents of a cluster with similar frequency, such as that t 4 appears two times in every document of cluster C 0 . Therefore, we cannot simply think that all terms in each cluster play an equally important role in the clustering process. For example, t 4 should be more important than t 0 , t 1 and t 2 in cluster C 0 . In order to deal with these speci al issues, we introduced feature weighting k -means for text subspace clustering [6].
  X  l =(  X  l, 1 , X  l, 2 ,..., X  l,m )betheweightsfor m features from the l th cluster. During the k -means clustering process, our method automatically calculates the feature weights which produces a k  X  m weights matrix. That is to say, in each cluster m weights are assigned to m features. According to the above analysis, a larger weight will be produced for the term that appears in the majority of the documents of one cluster with similar frequency, which means that the weight of a feature is inversely proportional to the dispersion of values of that feature. The larger the dispersion, the smaller the weight. This indicates that the values of a good feature in a cluster are very close to the value of the cluster center in that feature. For example in Table 1, a larger weight will be assigned to t 4 for cluster C . However, feature t 3 does not appear in cluster C 0 , which implicitly means that feature t 3 has a smaller dispersion and will also get a larger weight. To identify the cluster, term t 4 is apparently more important than term t 3 .Fortu-nately, the two different terms can be easily separated in post-processing. When extracting important features to represent different clusters, t 3 type features will be removed but t 4 type features will be retained.
 a parameter  X  . The problem of clustering X into k clusters can be stated as an algorithm which attempts to minimize the following cost function: subject to  X  where k (  X  n ) is a known number of clusters,  X  is an exponent greater than 1 [10]; W =[ w l,j ]isa k  X  n integer matrix; Z =[ Z 1 ,Z 2 ,...,Z k ]  X  R k  X  m represents k cluster centers; d ( z l,i ,x j,i )(  X  0) is a distance or dissimilarity measure between object j and the centroid of cluster l on the i th feature. Usually, we use Euclidean distance: much larger than d (  X  z l,i ,x j,i ), the weights will be dominated by  X  and  X  l,i will approach to 1 m . This will make the clustering process back to the standard k -means. If  X  is too small, then the gap of the weights between the zero dispersion features and other important features will be big, therefore, undermining the importance of other features. To balance we calculate  X  based on the average dispersion of the entire data set for all features as follows: where o i is the mean feature value of the entire data set. In practice we use a sample instead of the entire data set to calculate  X  . (5% sample is used according to the sampling theory [11].)  X  n is the number of documents in the sample. Exper-imental results in [6] have shown that this selection of  X  is reasonable to produce satisfactory clustering results and identify important features of clusters. In order to solve the minimization problem described in Section 2, we designed a subspace clustering algorithm, denoted as: FW-KMeans . problem, which can be solved by iterat ively executing Step 2, 3 and Step 4. The whole clustering process can be divided into three steps to optimize W , Z and  X  respectively. The usual method towards optimization of F (  X  ,  X  ,  X  )istouse partial optimization for W , Z and  X  . In this method we first fix Z and  X  ,and find necessary conditions on W to minimize F (  X  ,  X  ,  X  ). Then we fix W and  X  , and minimize F (  X  ,  X  ,  X  ) with respect to Z .Wethenfix W and Z , and minimize F (  X  ,  X  ,  X  ) with respect to  X  . The process is repeated u ntil no more improvement in the objective function value can be made.
 The proof of Eq.5 can be found in [12]. We remark that the minimum solution  X  W is not unique, so w l,j = 1 may arbitrarily be assigned to the first minimizing index l , and the remaining entries of the column are put to zero. And w l,j =1 means that the j th document belongs to the l th cluster because they have the smallest distance.
 updated by of iterations. This can be proved as follows. Firstly, we note that there are only a finite number of possible partitions W . We then show that each possible partition W appears at most once by the algorithm. Assume that W h 1 = W h 2 where h 1 = h 2 .Wenotethatgiven W h , we can compute the minimizer Z h which is Z W respectively (Step 4) a ccording to Eq.7. Again, W h 1 = W h 2 . Therefore, we have However, the sequence F (  X  ,  X  ,  X  ) generated by the algorithm is strictly decreasing. Hence the result follows.
 by adding a new step to calculate the feature weights in the iterative process, it does not seriously affect the scalability of the k -means type algorithms in clustering large data. An analysis of the run-time complexity for our algorithm shows this result. There are essentially three major computation steps in the algorithm and the complexity for each step is discussed in turn: 1. Partitioning Documents 2. Updating Cluster Centers 3. Calculating Feature Weights Therefore, the total runtime complexity of our algorithm is O ( hmnk ), where h is the total number of iterations.
 the feature weighting matrix  X  ;also O ( n (1 +  X  m )) space to store the set of n documents and their cluster labels, where  X  m is the average number of terms in each document and much smaller than m , because we only store the nonzero entries as shown in Table 1 in order to save the memory consumption. In this section, we use exper imental results to demonstrate the clustering perfor-mance of the FW-KMeans algorithm. We have done experiments on real-world text data sets to test the clustering quality in discovering clusters and identifying significant features from given data sets for each cluster [6]. Here, we conducted a scalability and convergency benchmark test of the algorithm on large text data sets.
 Clustering accuracy is defined as where C l is the number of documents occurring in both cluster l and its corre-sponding category and n is the number of documents in the data set. 4.1 Text DataSets Fourteen datasets were built from the 20-Newsgroups 1 collection with different characteristics in size, dimensionality and class distribution.
 while holding the number of documents ( n ) to 15905 and the number of clusters ( k ) to 20. In this way, we generated one data collection of 6 data subsets. When choosing the terms, we adopted the featu re selection method [13] to calculate the feature values, and sort all terms a ccording to these values. Then we selected the first m terms with the relatively higher scores.
 to 20, and then increased the number of documents from 2000 to 15905 with the rate of two times per step. Another data collection containing 4 data subsets was generated. Similarly, we only choose the first 1100 terms with the higher scores to represent the document vectors. Mean while, the two data collections overlap on the subset (15905, 1100, 20) (15905 documents, 1100 terms and 20 clusters), and cover all topics in 20-Newsgroups .
 this data collection, we made the number of clusters varying in { 3, 5, 7, 10, 12 } , while fixing the number of documents to 1500 and always extracting the first 500 terms with higher scores. For all data collections, we used the documents from the following 12 topics: alt.atheism , comp.graphics , talk.politics.guns , rec.autos , soc.religion.christian , misc.forsale , sci.crypt , comp.sys.ibm.pc.hardware , rec.sp-ort.basketball , sci.space , comp.os.windows ,and talk.politics.mideast .Byvarying the number of clusters ( k ), we selected documents from the first k topics. processing steps include removing the headers, the stop words, and the words that occur in less than three document s or greater than the average number of documents in each class, as well as stemming the left words with the Porter stemming function. The standard term scoring method was used to represent the document vector, where tf ( d, t )istheterm t frequency in document d ,and df ( d, t ) is the document frequency of term t that counts the number of documents term t appears. 4.2 Experimental Results We used the datasets described above to conduct a benchmark test on the scal-ability of FW-KMeans .Theexperimentsweredoneonamachinewitha3.2G CPU and 2G RAM. The parameter  X  was assigned to 1.5 in all experiments. The run-time and clustering accuracy of the FW-KMeans algorithm were ex-perimented with respect to the size, dimensionality and class distribution. All experiments were repeated five times and the running time represents the aver-age over five runs.
 ber of terms varied from 500 to 2000, while the number of documents was fixed to 15905 and the number of clusters to 20. We can notice that the run-time linearly increased with the number of terms, which reflexes the linear computa-tional complexity O ( h m nk ) analyzed in Section 3. The a ccuracy (i.e., clustering quality) increased a little when the nu mber of terms increased, because more terms held more information about the raw text, therefore, the clustering qual-ity was improved.
 ments varied from 2000 to 15905, while the number of terms was fixed to 1100 and the number of clusters to 20. The iteration h changed in { 9,13,13,10 } when the algorithm converged to a local optimal solution for each data set. The run-time increased linearly with the number of documents, which also verified the linear computational complexity O ( hm n k ) analyzed in Section 3. The clustering accuracy decreased a little when the number of documents increased. The rea-son was that more documents should cover much more terms, but we fixed the number of terms to 1100, which implicitly reduced the documents information and compromised the clustering quality.
 clusters. The number of clusters varied from 3 to 12, while the number of terms was fixed to 500 and the number of documents to 1500. We can see that the FW-KMeans run-time also increased linearl y with the number of clusters. The fixed number of terms resulted in the decr easing accuracy b ecause it implicitly reduced the documents information in the raw text.
 dataset (1500,500,7). The horizontal axis represents the number of iterations and the vertical axis represents the value of t he objective function (1). Each point on the curve represents a partition generated by one iteration of the k -means clus-tering process. Starting from a set of initial centroids and a set of initial weights, the algorithm first converged after 2 iterations. A new set of weights FW 1were computed. Using FW 1 as the initial weights and the current cluster centroids, the k -means process restarted again. We can see that the objective function had a significant drop after the new weights FW 1 were introduced. The k -means process converged again after 2 new iterations. Then, a set of new weights FW 2 were computed. This process continued un til the local minimal value of the ob-jective function was reached. The final set of weights FW 4 was obtained. We note that in each step, the weighted dist ance function was fixed since the weights for the features were fixed. Therefore the corresponding weighted distance func-tion space was well-defined at each step. We expected that the smaller the value of objective function value, the closer the data points under the weighted dis-tance function space. We remark that by using the similar arguments in Section 3, it can be shown that the above process is also convergent. For all the four-teen datasets, the iteration h is always less than 15, that is, our algorithm could converge quickly. In this paper we have shown the performance of the subspace clustering al-gorithm with feature weighting k-means. We analyzed this algorithm in terms of computational complexity, space consumption and convergency. A series of benchmark tests were conducted on the large real-world text datasets. The ex-perimental results have shown that this subspace clustering method is efficient and scalable to cluster large and sparse text data in subspaces.
 ground knowledge to enhance our method in text clustering and mining. The ontology will sever several purposes in the clustering process, including data preprocessing, selection of initial cluste r centers, determination of the number of clusters k, and interpretation of clustering results. We believe that ontology will provide premising solutions to the big problem in text mining: complex semantics.

