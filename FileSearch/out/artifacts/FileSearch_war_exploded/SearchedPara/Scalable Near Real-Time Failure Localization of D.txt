 Large-scale data center networks are complex X  X omprising several thousand network devices and several hundred thou-sand links X  X nd form the critical infrastructure upon which all higher-level services depend on. Despite the built-in re-dundancy in data center networks, performance issues and device or link failures in the network can lead to user-perceived service interruptions. Therefore, determining and localiz-ing user-impacting availability and performance issues in the network in near real time is crucial. Traditionally, both pas-sive and active monitoring approaches have been used for failure localization. However, data from passive monitoring is often too noisy and does not effectively capture silent or gray failures, whereas active monitoring is potent in detect-ing faults but limited in its ability to isolate the exact fault location depending on its scale and granularity.

Our key idea is to use statistical data mining techniques on large-scale active monitoring data to determine a ranked list of suspect causes, which we refine with passive monitor-ing signals. In particular, we compute a failure probability for devices and links in near real time using data from active monitoring, and look for statistically significant increases in the failure probability. We also correlate the probabilistic output with other failure signals from passive monitoring to increase the confidence of the probabilistic analysis. We have implemented our approach in the Windows Azure pro-duction environment and have validated its effectiveness in terms of localization accuracy, precision, and time to local-ization using known network incidents over the past three months. The correlated ranked list of devices and links is surfaced as a report that is used by network operators to in-vestigate current issues and identify probable root causes. C.2.3 [ Computer-Communication Networks ]: Network Operations X  Network management; Network monitoring Algorithms; Management Failure localization; Data center networks
Communications in data centers today are based on large-scale complex networks that concurrently support a variety of distinct services (e.g., search, email, cloud storage and compute). As an example, the Windows Azure network consists of over 7 thousand network devices (e.g., routers) and over 220 thousand links, which connect together all the servers in all Windows Azure data centers worldwide. All Windows Azure Services [14] depend on a healthy network for providing highly-available and scalable offerings. De-spite the built-in redundancy in data center networks, per-formance issues or failures in the network can lead to user-perceived service interruptions. Therefore, determining and localizing user-impacting availability and performance issues in the network in near real time is crucial.

Localizing network faults is done by knowledgeable net-work operations teams who work with associated on-call en-gineers to resolve problems in real time with the help of monitoring data. Such an approach can be time consuming, tedious, and is further exacerbated by monitoring noise and the increasing size of the network. As the scale of the net-work grows, automated fault localization becomes increas-ingly important since it can reduce mean-time-to-recovery and service disruption.

Network monitoring is at the heart of failure localiza-tion and is divided into two categories: passive and active monitoring. The passive approach typically involves polling the network devices periodically to collect various teleme-try data about their health and the traffic that passes by. The system will then analyze the local telemetry data and raise availability and performance alerts at the level of indi-vidual devices and links when it detects any abnormalities [4, 5, 13]. While these alerts are often useful in localizing failures, they are also noisy and not a direct signal for user-perceived failures 1 , which turns troubleshooting a particular
User-perceived network failures refer to network failures that have a direct effect on user traffic, where users are pri-marily the services running on top of the network. Figure 1: Failure localization approach: The successful, failed, and delayed ping data are overlaid on top of the net-work topology, and a statistical data mining approach is used to triangulate (i.e., localize) issues in the network. network issue into a time-consuming process that could take anywhere from minutes, to hours, to even days to resolve.
The active approach relies on the capability to inject test traffic into the network and to monitor the flow of that traf-fic. In particular, data centers typically run a ping service that generates large amounts of pings between pairs of end hosts in the network and acts as a proxy for user-perceived network availability and latency [15, 16]. In this setting, ping failures provide a strong signal that there is indeed an issue in the underlying network and, more specifically, that there is an issue somewhere along the path from the source to the destination of the ping. However, pings do not pin-point the exact device or link that has caused the pings to fail since actual network routes are typically unknown.
Even though a single ping failure cannot help in iden-tifying the culprit, the conjecture is that the combination of ping data between multiple sources and destinations will allow us to triangulate the location of an existing issue. Fig-ure 1 provides a simple visual representation of the proposed approach, which uses statistical data mining techniques to localize user-perceived network failures based on ping data. Specifically, we (i) introduce a probabilistic model for de-scribing the ping routes and failures, (ii) formulate the fail-ure localization problem as a fitting problem to compute failure probabilities for devices and links in near real time, and (iii) detect statistically significant increases in the com-puted failure probabilities.

Our algorithm produces a short ranked list of devices and links that best explain the observed ping data. Finally, we correlate this list with passive monitoring signals, such as de-vice and link alerts, in order to increase the confidence of the probabilistic analysis. The correlated ranked list of devices and links forms a starting point for investigating current is-sues in the network and can drastically reduce the time it takes for network operators to identify probable root causes and resolve network issues. Our experimental evaluation on real network incidents validates both the effectiveness and efficiency of our approach to localize network failures. In summary, our key contribution is the novel and successful application of statistical data mining techniques to local-ize user-impacting availability and performance issues in a worldwide data center network in near real time.

The rest of the paper is organized as follows. Section 2 presents background information for data center networks. Section 3 provides an overview of our approach, while the
Figure 2: Common network topologies for data centers. details are described in Section 4. The implementation and deployment details are discussed in Section 5 and the exper-imental evaluation is presented in Section 6. Finally, Section 7 discusses related work and Section 8 concludes the paper.
Data centers hosting Internet-scale services comprise of servers, direct attached storage, and networking infrastruc-ture. Over the recent years, data center networks have been on the path to commoditization [1]. For example, [7] demon-strates the performance and scale that can be achieved from a network built using low-cost Ethernet switches arranged into a Clos topology that provides multiple paths between servers. The overall network consists of a densely connected mesh network within each data center as well as a core net-work spanning the geographies. With mega-scale data cen-ters, the size of the network within each data center can be large connecting over 50-100 thousand servers.

Given the types of workloads that run in these data cen-ters, the authors in [3] present the need to treat the data center as one massive warehouse-scale computer and make the appropriate design, operation, and cost tradeoffs that best support the workloads. In this paper, we have only looked at a single aspect X  X amely the operational aspects of the network X  X ut from a holistic perspective of end-to-end user-perceived network performance and at the scale of these large data center networks.
Modern data center networks use hierarchical architec-tures reaching from a layer of servers in racks at the bottom to a layer of core routers at the top [1, 7]. We consider two common architectures within data centers, shown in Figure 2. In both cases, there are typically 20 to 40 servers per rack, each singly connected to a Top of Rack (ToR) switch. The data center backbone in the first topology consists of a layer of Aggregation switches (AS), whereas in the second topology it consists of two layers, namely the Cluster Spines (CSPs) and the Data Center Spines (DSPs). Here, each ToR is connected to a layer of CSPs, which in turn is connected to a layer of DSPs forming two bipartite graphs. The ASs and DSPs are then connected to the core of the network via Access Routers (ARs) and Border Leafs (BLs), respectively. Finally, ARs and BLs are connected to Core Routers (CRs).
The core network connects all data centers together form-ing an asymmetric wide-area network. It consists of core routers in the data centers as well as intermediate forward-ing devices between the data centers. Figure 1 shows the core network between multiple data centers and forwarding stations. For this paper, we only consider the network topol-Figure 3: Example network topology. The dotted green arrows show the represents a faulty link.
 ogy that is within the control of Microsoft and, therefore, do not consider peering devices or other ISP networks that con-nect with the core network. However, the approach can be extended beyond the Microsoft network provided we have an accurate characterization of the topology.
Large-scale data center networks typically employ multi-path routing , which involves using multiple alternative paths through the network in order to improve fault tolerance, bandwidth, and security. In this work, we consider Equal-Cost MultiPath (ECMP) routing used within data centers and Border Gateway Protocol (BGP) used in the core net-work between data centers. Routers within data centers use ECMP routing to spread the traffic along multiple paths by making per-hop decisions based on routing metric cal-culations. For routing paths, we assume that all shortest paths are equally likely and have the same cost metric, which closely resembles the ECMP protocol.

BGP is the routing protocol that determines the routes taken in the core network. BGP is responsible for setting up Multiprotocol Label Switching (MPLS) that directs traffic from one network node to the next based on short path labels rather than long network addresses. These network paths are termed Label-Switched Paths (LSPs). For this work, we assume that all shortest paths across the core are equally likely and do not take into account the LSP information available in the core network. We plan to refine the routes in the core with this information in the future.
In passive monitoring, devices are polled periodically us-ing various technologies (e.g., SNMP, NetFlow, RMON) and information is collected to assess network performance and status. Unfortunately, passive monitoring is known to be noisy as the generated alerts are largely threshold based and, in some cases, it suffers from a lack of signal when a device is actually faulty ( silent failures ) [6]. Hence, we leverage these signals to only increase the confidence and improve the pinpointing of the localization from our statistical anal-ysis performed on active monitoring signals.

Our main active monitoring signal comes from the ping service , which runs on servers in most racks in all data cen-ters. This service sends pings every minute to other ran-domly selected end hosts within the cluster, across clusters, and across data centers. Pings can fail or be delayed due to performance issues in the network or hard failures. A significant number of failed or delayed pings is an indication that there may be an issue in the physical network along the path(s) of the pings. Since the ping service runs on the same servers as the other data center services, it is a good proxy for user-perceived network availability and latency.
Another active monitoring tool is traceroute , which is used for displaying the path and measuring transit delays of pack-ets across a network. A traceroute is significantly costlier than a ping since the entire history of the route is recorded. Hence, generating traceroutes to cover large-scale data cen-ter networks is prohibitively expensive and are primarily used for targeted testing [9].
Our localization approach uses statistical data mining tech-niques on ping data to identify the links or devices that are responsible for ping failures (or significant ping delays), which typically translate into user-perceived network inter-ruptions. This section will describe the problem formulation and provide an overview of our approach that will guide the description of our solution in Section 4.
 Problem Formulation: There are two main inputs to our problem. The first input is the network topology , which we represent using a graph G = ( V , E ). The vertices V in the graph correspond to network devices, whereas edges E cor-respond to network links. The graph contains the links and devices for all compute and storage clusters within each data center, as well as the links and devices that form the global backbone network connecting all data centers together. Fig-ure 3 shows a simple graph with 10 vertices and 12 edges that we will use as a running example.

The second input is the ping data , which specifies how many pings succeeded and how many failed (or had high latency) between multiple source and destination vertices. High latency of a ping typically arises for two reasons: (a) some network device had to re-try sending the ping multiple times before succeeding or (b) the ping took a longer path than usual. In either case, significant high latency indicates an underlying issue with the network that we want to cap-ture and localize. In our case, we consider ping latency to be significant if it belongs in the 95 th percentile of the latency distribution. Table 1 shows a small example of ping data.
Our goal is to analyze the ping data overlaid on top of the topology in order to generate a short ranked list of edges and vertices associated with failure scores , whose potential fail-ure would best explain the observed ping data. This list is then combined with alerts from link-and device-level teleme-try data to further help network operators pinpoint a fault. Solution Phases: The proposed failure localization ap-proach works in 4 phases, each addressing 1 major challenge. 1. Probabilistic Routing: The exact ping routes through 2. Probabilistic Failure Modeling: Network failures are 3. Deviation Detection: There exists background noise 4. Correlation Analysis: In certain cases, some devices
Multipath routing (e.g., ECMP), which is prevalent in al-most all large networks, implies that the network route of any given ping (or packet in general) is determined by the routers using local per-hop decisions and is, therefore, un-known (recall Section 2.2). Instead, only the ping source and destination devices are known.

Given a source vertex v s and a destination vertex v d in the network graph, the goal of probabilistic routing is to compute the most likely ping routes along with their prob-abilities. The computation is based on the following two principles that govern multipath routing: Consider a ping from v 1 to v 10 in the example network graph in Figure 3. There are three possible routes indicated by the three dotted green arrows, each requiring a total of four hops. All pings from v 1 must first go through v 3 there are two equally-likely options: v 5 and v 6 . Pings that are routed to v 5 will then traverse v 7 before reaching the final destination v 10 . On the other hand, pings that reach v 6 will be routed to either v 7 or v 8 with equal probability before reaching v 10 . In summary, the three routes and the corresponding probabilities are: We use the Floyd-Warshall algorithm to compute the short-est distances from all vertices to all other vertices in the graph with worse case complexity of O ( | V | 3 ). We then build the routes of each ping opportunistically based on the short-est distances and compute the route probabilities based on the number of forwarding edges of each vertex in each route.
We use the computed routes and probabilities to also cal-culate the probability P s,d ( e ) that a given v s  X  v will pass through a particular edge e . Consider a v 1  X  v ping and edge e = ( v 7 ,v 10 ) in the example graph in Fig-ure 3. This edge is part of two different routes, namely v  X  v 3  X  v 5  X  v 7  X  v 10 and v 1  X  v 3  X  v 6  X  v 7  X  v 10 with route probabilities 0 . 5 and 0 . 25, respectively. The ping will pass through edge e if it takes one of the two aforemen-tioned routes. Therefore, the probability that the ping will pass through edge e equals the sum of the two route prob-abilities, i.e., P 1 , 10 ( e ) = 0 . 5 + 0 . 25 = 0 . 75. To simplify the discussion, we present the model in terms of network edges. A similar analysis applies to vertices.
Network devices and links that are experiencing a failure often drop only a fraction of their overall traffic, depending on the nature of the failure. In particular, issues caused dur-ing maintenance or by device malfunctions are fairly com-mon and typically cause small performance degradation or a small packet loss rate (less than 5%). On the other hand, catastrophic events like fiber cuts and complete hardware failures are fairly rare. We have seen only 5 such events out of the 73 real network incidents discussed in Section 6.
In order to address the challenge of partial failures, we em-ploy a probabilistic failure model that assigns a failure prob-ability value to each vertex and edge based on the observed ping data. We will start the discussion using a simple ex-ample before we formalize our model. Consider the network graph shown in Figure 3. Suppose edge e = ( v 6 ,v 7 ) is experi-encing an issue and is dropping some packets. Further, sup-pose the total number of pings from v 1 to v 10 is N 1 , 10 Based on the probabilistic routing discussed in Section 4.1, we calculate the probability that any ping will go through the problematic edge e . Specifically, P 1 , 10 ( e ) = 0 . 25. Fi-nally, assume that this edge is dropping 20% of its traffic. In other words, the probability that any ping going through e will fail is X e = 0 . 2. Therefore, out of the 50 pings, 25% of them will go through the problematic edge, out of which 20% will fail. Multiplying the 3 numbers together will give us the expected number of ping failures, F 1 , 10 = 5. Table 2 summarizes the notation used in this section. Figure 4: Distribution of failure scores for the top-100 edges for three different least squares formulations.
 Equation 1 provides the basic unit of our model: The number of failed pings F i,j and total pings N i,j between vertices v i and v j are observed. The probability P i,j a v i  X  v j ping will go through a particular edge e is com-puted by the routing model. Finally, X e , the failure score of edge e , is the unknown value we need to compute. This model assumes there is at most one failed edge (or vertex) on the paths from v i to v j , which is common in practice. However, concurrent failures on independent network paths are captured by the model. The fact that we have a long list of pings between multiple sources and destinations results in the creation of an overdetermined system of equations that can be numerically solved.

Consequently, we need to solve a data fitting problem by computing the failure score X e for each edge e that best fits the observed data. There are several well-known techniques for solving such problems and we have chosen to use the method of Least Squares because it produces a linear unbi-ased estimator and it is computationally efficient to solve. Least squares formulation: The best fit in the least-squares sense minimizes the sum of squared residuals; a residual being the difference between the observed value and the fitted value provided by the model. The model from Equation 1 results in the following formulation: Solving the above least squares problem yields the equation: The main drawback of this solution is that it typically as-signs a wide range of failure scores to multiple edges during a network issue. In layman X  X  terms, it is spreading the blame around, increasing the size of the output list that a network operator would have to investigate. Figure 4 shows the dis-tribution of failure scores for the top-100 edges computed by three different least squares solutions during a real network incident. The solution from Equation 3 assigns a failure score higher than 0 . 5 to 25% of the edges. This behavior is partly due to the presence of the N 2 i,j term in the denomi-nator; the failure score is thus very sensitive to the number of total pings between each pair of vertices, which can vary between tens to hundreds of pings every 5 minutes.

Solving for F i,j /N i,j rather than F i,j in the original model (Equation 1) results in a second formulation and solution: The solution in Equation 5 has the desired property of high contrast in the distribution of failure scores, as can be seen in Figure 4. Only 4 edges get a failure score over 0 . 5. However, this solution completely ignores the presence of successful pings between two vertices when there are no failed pings, i.e., N i,j is ignored when F i,j = 0.

The above observation has lead to our final formulation and solution: Figure 4 shows that the solution from Equation 7 offers a similar contrast in failure scores as Equation 5, while the presence of N i,j in the denominator ensures the use of suc-cessful pings in the calculations. We use Equation 7 to com-pute the failure scores for all edges and vertices that are part of a possible route of any failed ping, and produce a ranked list of edges and vertices based on that score.
The presence of background noise in large-scale network monitoring systems is common [6]. In our case, there exists background noise in both the input ping data as well as the generated failure scores.

Noise in the ping data typically appears in the form of false ping failures or delays, even though the underlying network is not experiencing any issues. In some cases, inflated ping latency measurements are caused by increased load on the servers running the ping service. In other cases, pings might get dropped by a router X  X ven though regular applications do not exhibit any packet loss X  X ue to the combination of two factors: (a) traffic saturation at a link and (b) pings having the lowest priority in network traffic.

Noise in the generated failure scores is primarily attributed to the unknown nature of network routes. Since the routes for failed pings are unknown and computed using probabili-ties, any edge or vertex across alternative routes can poten-tially have a non-zero failure score.

Background noise typically manifests as low failure scores for healthy edges and vertices. A simple solution would be to introduce a threshold for filtering out the low scores. Apart from the obvious drawback of introducing an ad-hoc thresh-old value, we would risk filtering out low failure scores that resulted from true partial failures. By definition, partial fail-ures cause low packet drop rates and naturally lead to low failure scores. By simply looking at the value of a failure score, we cannot distinguish these two cases.
 Statistical hypothesis testing: The key idea behind our solution is to look at failure scores over time in order Figure 5: Detection of a spike and a gradual increase in a time sequence of failure scores. to establish a score baseline for each edge and vertex, and then use statistical hypothesis testing to determine whether a current failure score is significantly different than its his-torical values. This approach allows us to filter out the false positives due to noise and only output edges and vertices with statistically significant failure scores.

Given a time sequence of failure scores S = { s 0 ,s 1 ,...,s for a particular edge (or vertex), we use a series of Stu-dent X  X  t -tests to determine whether the addition of the latest score s 0 introduces a statistically significant change in the sequence of scores. In particular, we are interested in de-tecting two different score patterns over time, namely spikes and gradual increases, seen in Figure 5. Spikes are detected using one-sample Student X  X  t -tests while gradual increases using two-sample Student X  X  t -tests.

The one-sample Student X  X  t -test is used to determine whether the latest score s 0 is likely to belong in the distribution of historical scores S h = { s 1 ,...,s n } , which forms the null hy-pothesis. Let n h ,  X  h , and  X  2 h be the size, mean, and vari-ance of S h , respectively. According to the t -test, the null hypothesis is rejected (i.e., the score is deemed significantly different) when where  X  is a scaling parameter and t a,n h  X  1 is the t value obtained from the t -distribution table using significance level  X  = 99% and degree of freedom equal to ( n h  X  1). Currently, we use a dynamic scaling parameter  X  = 1 / set based on empirical evidence.

In the two-sample Student X  X  t -test, we divide the set of scores S into two independent sets, the current set { s 0 ,...,s c } and the historical set S h = { s h ,...,s n } . Locations c and h in the sequence are configurable and control the size and gap between the two sets.
 Let n c ,  X  c , and  X  2 c be the size, mean, and variance of respectively. Similarly, n h ,  X  h , and  X  2 h are the size, mean, and variance of S h . The null hypothesis is that  X   X  , for some scaling parameter  X  . We use  X  = 1 / discussed above. We account for the presence of  X  in the null hypothesis by revising  X  c =  X  c / X  and  X  2 c =  X  2 According to the t -test, we compute the t -statistic T as: The degree of freedom df is computed using: Finally, the null hypothesis is rejected (and the latest score is deemed significantly different) when where t  X ,df is the t value obtained from the t -distribution table using significance level  X  = 99%.

Both the one-sample and two-sample Student X  X  t -tests are invoked each time a new failure score is computed. If any of the t -tests determines the score to be significant, the corre-sponding edge (or vertex) is added in the final ranked list.
The algorithms discussed so far generate a ranked list of links and devices suspected to have caused ping failures. However, it is possible for a small set of links or devices to share the same failure score due to network redundancy and multipath routing. As an example, consider the case were three physical links are connecting two network devices for redundancy purposes. Multipath routing (and our proba-bilistic routing) postulates that all traffic between the two devices will be equally divided among the three links. There-fore, the probability that a failed ping will go through any of the three links is the same, which in turn implies that the three links will have the same failure score.

We address this issue by taking advantage of the availabil-ity and performance alerts produced by passive monitoring (recall Section 2.3). In particular, we perform a left outer join between our ranked list L and the set of device-and link-level network alerts A that are triggered during the time period of the analysis. This join retains all devices and links in
L while appending the alerting information from A to the matching records. We do not remove the records in L that do not match A in order to capture the cases were a device or link is dropping packets without issuing any alerts. (We discuss such cases in the experimental evaluation in Section 6.) Instead, devices and links with alerting information are pushed to the top of the list since there is more evidence that they may be responsible for the observed ping failures.
We have implemented the fault localization approach dis-cussed in Section 4 and incorporated it into the Windows Azure network monitoring infrastructure. Figure 6 shows the relevant components of the monitoring system: The Ping Service and Device Data Collectors are continu-ously collecting large amounts of data that need to be ana-lyzed. In particular, there are about 400 thousand pings and several hundreds to thousands of network events being gen-erated every 15 minutes by the existing framework. In order to produce results in near real time, we process the data in-crementally. Every five minutes, the Analysis Worker will (a) get the latest topology information from the Data Store and build the network graph, (b) stream in the ping and telemetry data from the last 15 minutes, (c) run the fault localization analysis presented in Section 4, and (d) stream out the ranked list of suspect links and devices, whose fail-ures best explain the observed data.
In this section, we evaluate the effectiveness and efficiency of our approach to correctly localize issues in the entire Win-dows Azure network. The experimental evaluation is based on 73 real network incidents that were investigated by the Windows Azure network operations team during a period of three months. After sifting through large amounts of data and reports compiled primarily by network operators, we identified the (most likely) root cause for each incident. Root causes are divided into the following 10 categories: Figure 7: Derivation of localization accuracy classifications. Table 3 lists the total count of incidents for each root cause.
For the purpose of our evaluation, we define a real phys-ical network issue (PNI) to be any issue experienced by a network device or link that resulted in dropped traffic and, therefore, has affected higher-level users. For example, a fiber cut or a device failure is considered a real physical network issue; these are the type of issues our algorithm is trying to localize. On the other hand, issues with DNS or software issues are not considered physical issues, and thus our algorithm should exclude them from localization.
In this section, we evaluate the ability of our approach to accurately localize real physical network issues (PNIs) and to correctly exclude non-physical network issues. Unfortu-nately, in many cases, we do not have the ground truth of whether a network incident was caused by a real PNI or not. One option would be to consider the final ruling of the network operator as the ground truth. However, issues such as transient failures can, by definition, cause loss of traffic at the time of the incident but for the network to appear healthy by the time the operator investigates the in-cident. Hence, to evaluate the localization accuracy of our approach, we combine two sources of truth: (a) the exper-tise of the network operator in marking an issue as a real PNI or not, and (b) traffic data showing whether there was a significant loss of traffic near the location and around the time of the investigated incident.

When the two sources of truth are in agreement, local-ization accuracy is easily established. First, suppose the operator marked the incident as a real PNI and there was significant loss of traffic at the time of the incident. Us-ing conventional terminology from binary classification, we declare the result of our localization approach to be true positive (TP) when our approach correctly identifies and lo-calizes the issue and false negative (FN) when it failed to identify the issue. On the other hand, suppose the oper-ator marked the incident as not a PNI and there was no observable loss of traffic. In this case, the localization re-sult is considered true negative (TN) if the algorithm did Root Cause TP TN FP FN LP LN NA Total Device Issue 4  X   X   X   X  4 1 9 Fiber Cut 3  X   X   X   X   X  1 4 Maintenance 5 5  X   X   X   X   X  10 Transient Failure 3  X   X   X  11  X   X  14 DNS Issue  X  10  X   X   X   X   X  10 BGP Issue  X  5  X   X   X   X   X  5 Software  X  7  X   X   X   X   X  7 False Alarm  X  10  X   X   X   X   X  10 DDoS Attack  X   X  2  X   X   X   X  2 Peering Issue  X   X   X   X   X   X  2 2
Total 15 37 2 0 11 4 4 73 not produce any devices or links in the topology near the investigated incident; otherwise, it is a false positive (FP) .
When the two sources of truth are in disagreement (i.e., the operator believes there is a real PNI but there is no sig-nificant loss of traffic, or the operator believes there is no PNI but there is loss of traffic), we define two new localiza-tion accuracy classifications. The localization result is con-sidered a likely positive (LP) when the algorithm identifies an issue and either the operator has marked it as a real PNI or there was significant loss of traffic. Conversely, the result is considered a likely negative (LN) when the algorithm does not identify an issue and either the operator has marked it as a non-issue or there was no substantial loss of traffic. Finally, if the issue is not located within the network graph, then we classify it as not applicable (NA) . For example, peering links that connect Windows Azure to the outside world are not part of our topology. Figure 7 summarizes the derivation of our localization accuracy classifications.

Table 3 lists the localization accuracy of our approach for each root cause for the 73 incidents. Overall, our approach was able to correctly identify and localize 15 real physical network issues (TPs) while correctly excluding 37 non-issues (TNs). The root causes for the TPs were device issues, fiber cuts, issues during planned maintenance, and transient fail-ures. All DNS, BGP, and software issues as well as all false alarms and some maintenance issues were correctly classi-fied as TNs. There were only 2 cases of FPs where our algorithm incorrectly output devices and links as suspects of causing failures; both incidents were likely the result of DDoS attacks. There were no cases of FNs.

There were also 15 incidents where the network operator and the traffic data were in disagreement. In particular, there were 11 cases of transient failures where the operator did not find a physical network issue even though there was significant loss of traffic observed for a short period of time (typically 5-10 minutes). In fact, in all cases there was ad-ditional evidence in the form of device-and link-level alerts suggesting there was indeed a physical network issue. We classify these cases as likely positives (LPs) even though we believe they are probably TPs. The remaining 4 cases are classified as likely negatives (LNs) and are attributed to de-vice misconfigurations that did not affect traffic (and by ex-tension, did not cause any user-perceived failures). Finally, there were 4 incidents that occurred outside the Windows Azure network and are thus classified as NA.

In the absence of our algorithm, the network operators must rely on their expertise as well as device-and link-level Figure 8: Localization accuracy of our approach compared to the conventional approach based on network alerts. alerts from passive monitoring in order to localize network issues. Figure 8 compares the localization accuracy of the approach based on network alerts (we call this the conven-tional approach) and our approach. We observe that the conventional approach yields a large number of false posi-tives; 20 compared to just 2 from our approach. It is fairly common for BGP, DNS, or software issues to cause device-and link-level alerts even though the actual devices and links are healthy and the traffic is not affected. In comparison, our approach is able to correctly determine that the issue is not related to physical devices or links.

Furthermore, the conventional approach suffered from 5 false negatives. There were 5 network incidents caused by a device (or link) malfunction but that device (or link) had no alerts associated with it (i.e., they were silent failures). We have seen this happen during device hardware failures, device misconfiguration, or planned maintenance. However, since the problematic device was causing traffic loss (and by extension ping failures or delays), our approach was able to detect and localize it correctly.
While Section 6.1 demonstrated the ability of our ap-proach to accurately localize network failures, it does not give any indication of how precise the localization is. In this section, we evaluate the precision of our approach using two metrics: one based on the size of the ranked list L of devices and links produced by the algorithm, and another based on the placement (i.e., rank) of the problematic device or link that caused the network issue in L .

The first metric is termed localization precision and is in-spired by a similar metric defined in [9]. The localization precision for a given network incident is defined as the ratio of the number of suspect devices and links after localization (i.e., the size of list L ) to that before localization. In other words, it is the fraction of devices and links that are likely to explain a particular network issue using our algorithm out of all the devices and links that can potentially cause that issue. We call the latter set the potential set P .
The size of P depends on the scope of the investigation, which typically starts with some higher-level performance or availability alert for some part of the network. For example, suppose there is an availability alert for a particular cluster. Any ToR switch or CSP and their links could be potentially causing the issue. Table 4 lists the number of potential de-vices and links that might need to be checked for the 15 TP Figure 9: Localization precision of our approach compared to the conventional approach based on network alerts. incidents. Of course, using domain knowledge and expertise, an operator will only check a subset of P ; but the estimate underscores the challenge faced by operators today.

Figure 9 shows the cumulative distribution functions (CDFs) of the localization precision of our approach and the conven-tional approach 2 . We observe that our approach localizes network issues to less than 2% for more than 50% of the failures and to less than 5% for more than 80% of the fail-ures. On the contrary, only 33% of the failures are localized to less than 5% using the conventional approach. Table 4 further supports the poor localization precision of the con-ventional method as the number of distinct devices or links associated with network alerts is often much larger (up to an order of magnitude larger) than the total size of our ranked list L . We conclude that our approach is able to localize the true issue very precisely from a large set of possible root causes for a given failure.

The second metric for evaluating the algorithm X  X  precision is the diagnostic rank , which is the number of network de-vices or links deemed more likely culprits than the device or link that actually failed [11]. Assuming operators investigate failures in the order listed by our algorithm, the diagnostic rank reflects the overhead of identifying and resolving the issues. Table 4 lists the average diagnostic rank computed during the duration of each incident. On average, the op-erator will have to check less than 5 devices or links (and almost always less than 10) before identifying the real cul-prit, compared to 6 X 305 for conventional diagnosis. Hence, our approach can significantly reduce diagnosis time.
In this section, we evaluate the efficiency of our approach in terms of how quickly a particular network issue is local-ized. As discussed in Section 5, our algorithm is running continuously in real time and generates a ranked list of de-vices and links every 5 minutes (assuming it has identified a network issue). We compare the time the ranked list con-tained the problematic device or link for the first time with the starting time of the incident. Note that in most cases the exact starting time of an incident is not known, so we are using the starting time recorded in the incident report. Figure 10 shows the CDF of the time to localize real network incidents in Windows Azure. We observe that 50% of the
The localization precision for the conventional approach is based on the number of distinct devices and links with network alerts within the scope of the investigation. incidents are localized within 12 minutes while the longest time to localize is 23 minutes. Typically, the time to localize depends on the severity of the incident; the higher the ping failure rate, the shorter the time to localize since the sta-tistical hypothesis testing will quickly detect the deviation in the failure score. For example, the incident that took 23 minutes to localize had a ping failure rate between 1-2%.
It is interesting to note that in 3 incidents the device or link causing the issue was localized by our algorithm well before the starting time recorded by the operator X  X nd pre-sumably, before the time the operator started the investi-gation. Therefore, there is potential for our approach to be used in a more proactive way for localizing user-perceived failures. We plan to investigate this approach in the near future. Overall, our localization approach can identify and localize real physical network issues in a very short time period, significantly improving the ability of the operations team to localize and resolve network incidents.
In network tomography, link-level properties like link loss and packet latency are inferred using statistical approaches from end-to-end measurements [5]. In particular, BAD-ABING [12] and Tulip [10] measure per-path characteris-tics to identify problems that impact network performance. These methods, along with some commercial products as well, use active testing (e.g., traceroute) to pinpoint faulty links. However, they have only been applied to smaller net-works since applying an active testing technique that will sufficiently cover a worldwide network is prohibitively ex-pensive. Our approach, on the other hand, does not require any controlled testing infrastructure and relies only on ping data, which is typically available as a connectivity signal in data center networks.

There exists a large body of work on detecting and lo-calizing performance problems across network and services. Sherlock [2] captures the dependencies between various com-ponents of the IT infrastructure (e.g., DNS service, load bal-ancers, network components) and tries to localize failures to a small number of components. Similarly, Shrink [8] and SCORE [9] have their own dependency model, which they use to find the most likely root causes of faults in wide-area networks. In contrast, our approach localizes failures on the underlying physical network, both within and across data centers, and as such, can complement the aforementioned tools. Furthermore, we have shown that our approach is scalable, can produce results in near real time, and can han-dle densely connected topologies in addition to loosely con-nected ones such as wide-area networks.
In this paper, we described a novel and successful applica-tion of statistical data mining techniques for localizing user-impacting availability and performance issues in a worldwide data center network in near real time. A unique aspect of our work is that it is agnostic to the routing intricacies and for-warding aspects of the network and is, therefore, a scalable solution. We have deployed our solution on the Windows Azure network monitoring infrastructure with a special fo-cus on producing a high confidence, real-time signal that can be used by operations teams to detect and localize physical network issues. Our experimental evaluation on real network incidents has validated both the efficiency and effectiveness of our approach.
We would like to thank the Windows Azure Network teams that collaborated with us, especially Albert Greenberg for introducing us to this problem as well as Monika Machado and Tina Stewart for providing operational support.
