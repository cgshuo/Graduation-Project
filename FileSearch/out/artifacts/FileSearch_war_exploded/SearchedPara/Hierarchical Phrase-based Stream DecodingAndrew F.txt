 Statistical machine translation traditionally oper-ates on sentence segmented input. This technol-ogy has advanced to the point where it is becom-ing capable enough to be useful for many applica-tions. However, this approach may be unsuitable for simultaneous interpretation where the machine translation system is required to provide transla-tions within a reasonably short space of time after words have been spoken. Under this type of con-straint, it may not be possible to wait for the end of the sentence before translating, and segmenta-tion at the sub-sentential level may be required as a consequence. This segmentation process is difficult, even for skilled human interpreters, and presents a major challenge to a machine since in addition to the translation process, decisions need to be made about when to commit to outputting a partial translation. Such decisions are critical since once such an output is made it can be dif-ficult and highly undesirable to correct it later if it is in error. In order to automatically perform segmentation for interpretation, two types of strategy have be proposed. In the first, which we will call pre-segmentation, the stream is segmented prior to the start of the machine translation decoding process, and the machine translation system is constrained to translate using the given segmentation. This approach has the advantage that it can be imple-mented without the need to modify the machine translation decoding software. In the second type of strategy, which we will call incremental decod-ing, the segmentation process is performed during the decoding of the input stream. In this approach the segmentation process is able to exploit seg-mentation cues arising from the decoding process itself. That is to say, the order in which the de-coder would prefer to generate the target sequence is taken into account.

A number of diverse strategies for pre-segmentation were studied in (Sridhar et al., 2013). They studied both non-linguistic tech-niques, that included fixed-length segments, and a  X  X old-output X  method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigation.

In (Oda et al., 2014) a strategy for segmentation prior to decoding based on searching for segmen-tation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmenta-tion could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical de-coder (Watanabe et al., 2006) to achieve a consid-erably faster decoder in return for a small cost in terms of BLEU score.

A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper (Kolss et al., 2008a) for a complete description of the stream decoding process; in this section we pro-vide a brief summary.

Figure 1 depicts a stream decoding process, and the figure applies to both the original phrase-based technique, and the proposed hierarchical method. The input to the stream decoder is a stream of to-kens (it is also possible for the decoder to oper-ate on tuples of confusable token sequences from a speech recognition decoder). As new tokens ar-rive, states in the search graph are extended with the new possible translation options arising from the new tokens. Periodically the stream decoder will commit to outputting a sequence of target to-kens. At this point a state from the search graph is selected, the search graph leading from this state is kept, and the remainder discarded. The search then continues using the pruned search graph. The language model context is preserved at this state for use during the subsequent decoding. In this manner the stream decoder is able to jointly seg-ment and translate a continuous stream of tokens that contains no segment boundary information; the segmentation occurs as a natural by-product of the decoding process. Re-ordering occurs in ex-actly the same manner as the sentence-by-sentence hierarchical decoder, and word re-ordering within segments is possible. 2.1.1 Latency Parameters The stream decoding process is governed by two parameters L max and L min . These parameters are illustrated in Figure 1. The L max parameter con-trols the maximum latency of the system. That is, the maximum number of tokens the system is per-mitted to fall behind the current position. If in-terpreting from speech, the parameter represents the number of words the system is allowed to fall behind the speaker, before being required to pro-vide an output translation. This parameter is a hard constraint that guarantees the system will always be within L max tokens of the current last token in the stream of input tokens. The parameter L min represents the minimum number of words the sys-tem will lag behind the last word spoken. It serves as a means of preventing the decoder from com-mitting to a translation too early.

Both the phrase-based and hierarchical phrase-based stream decoders maintain a sequence of to-kens that represent the sequence of untranslated tokens from the input stream (see Figure 1). As new tokens arrive from the input stream, they are added to the end of the sequence. When the length of this sequence reaches L max , the decoder is forced to provide an output. 2.1.2 Phrase-based Segmentation When forced to commit to a translation, the phrase-based decoder rolls back the best hypoth-esis state by state, until the remaining state se-quence translates a contiguous sequence of source words starting from beginning of the sequence of untranslated words, and the number of words that would remain in the sequence of untranslated words after the translation is made, is at least L min . It is possible that no such state exists, in which case since the stream decoder is required to make an output, it must use an alternative strategy.
In this alternative strategy, the stream decoder will undertake a new decoding pass in which it is forced to make a monotonic step as the first step in the decoding process. Then, a state is selected from the best hypothesis using the roll-back strat-egy above. This process may also fail if the mono-tonic step would lead to the violation of L min . In the implementation of (Finch et al., 2014), the decoder is permitted to violate L min only in this case. 2.1.3 The Proposed Method Figure 2: Selecting a segmentation point during hierarchical decoding.

The proposed hierarchical method attempts to capture the spirit of the phrase-based method. When forced to commit to a translation of a se-quence of n words, the segmentation process is simple and guided direcly by the chart.

As in the phrase-based approach, the best hy-pothesis at the top of the chart is used to pro-vide the partial translation and segmentation point. This hypothesis has a span of [1 ,n ] over the source words. The left child of the rule (defined in accor-dance with the binarized grammar used by the de-coder) that was applied to create this hypothesis is examined; let its span be [1 ,k ] . If n  X  k  X  L min , then this partial hypothesis represents a translation of the first k words of the sentence that leaves at least L min words untranslated, and therefore the target word sequence from this partial hypothesis is output, and the associated source words are re-moved from the sequence of untranslated words. If this hypothesis is not able to meet the constraint, the parse tree traversal continues in the same man-ner: depth first along the left children until either a translation can be made, or no further traversal is possible.

Following the translation of of word sequence, similar to the phrase-based stream decoder of (Finch et al., 2014), the hierarchical stream de-coder proceeds from an initial state in which the language model context is preserved. The decod-ing process relies on an implicit application of the glue grammar to connect the past and future nodes. An visual example of this selection pro-cess is given in Figure 2. In this example, the neither the root of the tree (spanning t 1 t 2 t 3 t 4 t 5 nor its left child (spanning t 1 t 2 t 3 t 4 ) are not able to generate an output since they both span sequences of words that would violate L min , which is 3 in this example. The left child two levels down from the root node spans only t 1 t 2 and would leave 4 words untranslated, therefore it defines an accept-able segmentation point.

Instead of forcing a monotonic decoding step in the event of a failure to find a segmentation point during the decoding, the hierarchical stream de-coder directly eliminates hypotheses that would lead to such a failure. The search process is con-strained such that all parse trees that cover the first word of the source sentence, must contain a sub-tree that can give rise to a translation that does not violate L min (constituents that can produce translations cannot span more than L max  X  L min words). Any search state that would violate this constraint is not allowed to enter the chart. This property is recursively propagated up the chart during the parsing process ensuring that each entry placed into the first column of the chart contains a constituent that could be used to produce a trans-lation.

This approach is more appealing than the forced monotonic step in that it will also allow non-monotonic translations that are guaranteed to be usable. Similar to the phrase-based approach, in some circumstances it may not be possible to pro-duce a parse that does not violate L min , and only in this rare case is the decoder allowed to violate L min in order to guarantee maximum latency. 3.1 Corpora data sets from the IWSLT2014 campaign. We evaluated on English-to-Spanish, and English-to-Chinese translation using the same data sets that were used in (Finch et al., 2014). These pairs were chosen to include language pairs with a relatively monotonic translation process (English-Spanish) and (English-French), and also language pairs that required a greater amount of word re-ordering for example (English-Chinese). The Chinese corpus was segmented using the Stanford Chinese word segmenter (Tseng et al., 2005) according to the Chinese Penn Treebank standard. 3.2 Experimental Methodology Our stream decoder was implemented within the framework of the AUGUSTUS decoder, a hierar-chical statistical machine translation decoder (Chi-ang, 2007) that operates in a similar manner to the moses-chart decoder provided in the Moses ma-chine translation toolkit (Koehn et al., 2007). The training procedure was quite typical: 5-gram lan-guage models were used, trained with modified English input stream: Sequence of translated segments: Kneser-Ney smoothing; MERT (Och, 2003) was used to train the log-linear weights of the models; the decoding was performed with a distortion limit of 20 words.

To allow the results to be directly comparable to those in (Finch et al., 2014), the talk level BLEU score (Papineni et al., 2001) was used to evaluate the machine translation quality in all experiments. 3.3 Results The results for decoding with various values of the latency parameters are shown in Figure 3 for English-French, English-Spanish, English-Arabic, English-Hebrew, English-Russian and English-Chinese. Overall the behavior of the system was quite similar in character to the published results for phrase-based stream decoding for English-Spanish (Kolss et al., 2008b; Finch et al., 2014). The hierarchical system seemed to be more sen-sitive to small values of minimum latency, and less sensitive to larger values. The results for the more challenging English-Chinese pair were more surprising. In (Finch et al., 2014), the per-formance of the phrase-based decoder suffered as expected in comparison to pairs of European lan-guages. This was in line with the increase in dif-ficulty of the task due to word order differences. However, in comparison to prior results published on the phrase-based stream decoder, the hierarchi-cal stream decoder seems less affected by the dif-ferences between these languages; the curves are higher at the optimal values of minimum latency, and seem less sensitive to its value. The character of the results appears to be very similar to those from English-Spanish. This result is encouraging and suggests that the hierarchical method may be better suited to interpreting between the more dif-ficult language pairs. Figure 4 shows the segmen-tation given by the system with L max = 8 and L min = 4 , on a sequence of English words which is a subsequence of an unseen test stream of words being decoded. In this paper we propose and evaluate the first hi-erarchical phrase-based steam decoder. The stan-dard hierarchical phrase-based decoding process generates from the source in left-to-right order, making it naturally suited for incremental decod-ing. The hierarchical decoder organizes the search process in a chart which can be directly exploited to perform stream decoding. The proposed hier-archical stream decoding process only searches a subset of the search space that is capable of gen-erating useful partial translation hypothesis. This eliminates the necessity for the forced monotonic step necessary in the phrase-based counterpart. Hypotheses that are not useful are discarded, and are therefore not able to compete with useful hy-potheses in the search. Additionally, a benefi-cial side-effect of the pruning of the search space is that decoding speed increased by a factor of approximately 8 over the baseline sentence-by-sentence decoder. Looking to the future, one im-portant benefit of taking a hierarchical approach is that the re-ordering process is made explicit, and in further research we wish to explore the possi-bility of introducing of new interpretation-oriented rules into the stream decoding process.
 We would like to thank the reviewers for their valuable comments.
