 If you were given Fig. 1 (c) and (d) to memorize, which would you find easier to memorize? If you were a zoology teacher, how would you come up with a lesson plan to teach the facts in Table 1 , containing animals and their properties? In our formulation, our facts consist of simple (object, property) pairs (Table 1 a). Informally, our problem can be stated as follows: Informal Problem 1 (Transmission/Teaching Rate Problem). Given a large, sparse binary matrix whose rows represent objects, columns represent prop-erties, in which ones represent facts, how do we measure how good a particular metric? Table 1 illustrates our intuition behind the solution: most people would agree that randomly stating facts ( X  X almons have fins X ) would be painful for the student. A good teacher would group animals and properties, as in Table 1 b, and use analogies and comparison, such as  X  X igers are like lions, but have stripes. X  Our contributions are as follows:  X  Problem Formulation : we formulate the Transmission Rate Problem for- X  Optimization Goal : we formulate a new metric which prioritizes consistent  X  Algorithm :wepropose groupNteach , an algorithm for encoding and Reproducibility : All datasets and code we use are publicly available http:// www.cs.cmu.edu/  X  hyunahs/tol .
 A snapshot of our results is shown in Fig. 1 . The proposed outperforms the baseline (Dot-by-Dot) in terms of the student X  X  lost utility, and total encoding length. Also, groupNteach automatically reorders and groups facts of the matrix as a by-product. A brief flowchart of in Fig. 2 , which will be discussed in more detail in later sections. Support from Learning Theory. Improving student learning has been a great interest in various domains including psychology, education [ 7 , 10 ], as well as data mining [ 11 , 14 ]. In our study, we find that algorithms that do well under explained in Table 2 . In the last column, we show the keyword of (Table 4 ) that reflects the principles.
 Matrix Compression. There are several methods for efficiently compressing a we want metrics that prioritize consistent learning rather than pure encoding length; (b) we want to encode new information based on a learner X  X  existing knowledge.
 Bipartite Clustering and Binary Matrix Reordering. Various bipartite clustering and matrix reordering algorithms [ 5 , 17 ] can be plugged in to our reordering stage as shown in Fig. 2 . In Table 3 , we compare related algorithms in terms of the functions they perform such as ordering, com-pression, and grouping, as well as their relation to educational and human com-puter interaction (HCI) principles.
 Minimum Spanning Trees. In one of our proposed plugins, groupNteach Tree, we adopt Euclidean-MST [ 13 ]. How can we assign a numerical score for comparing different ways of teaching a collection of facts to students? Definition 1 Performance-for-Price curve p ( n ) . Given an encoding algorithm, define p ( n ) as the number of nonzero entries of the matrix that are decodable based on the first n bits output by the encoding algorithm (e.g. see Fig. 1 ; top). Definition 2 Area Left of Curve ( ALOC ) . The ALOC metric is the area left of the curve p ( n ). Lower ALOC is better.
 Utility interpretation. Assuming the students gain utility at each time step according to how much they know, the total utility gained by students is the area under the curve p ( n ). Then ALOC corresponds to the utility lost by a student over time compared to having known all the information in advance. ALOC uses the number of bits transmitted as the units along the understand the lesson content. Hence transmitting the message efficiently lowers the amount of effort students need to understand the material. In this section, we describe groupNteach ; it finds efficient and interpretable encodings for a collection of facts, and as a by-product, it sequences the facts so as to find groupings of related facts, and a good teaching order for the groups. All-engulfing approach. Since real-world datasets differ greatly in their under-datasets. Hence, we propose four plugins, each designed to perform well on a particular type of dataset. Our all-engulfing approach tries each plugin to encode a dataset, and chooses the encoding with lowest ALOC . The complete algorithm is given in Algorithm 1 .

Algorithm 1. groupNteach encoding method that also returns grouped and ordered data Table 4 summarizes the encoding structure and keywords each plugin uses to encode information.
 In this section, we give detailed explanations on each of the plugins. 5.1 G ROUP N TEACH -Block groupNteach -Block, explained in Algorithm 2 , is designed to encode block-structured data highly efficiently. It does this by clustering the rows and columns to produce dense blocks, then re-orders the block regions, and encodes the block information (starting point, row and column length of the block), along with the missing elements in the block, and the additional elements outside the defined blocks. 5.2 G ROUP N TEACH -Tree groupNteach -Tree encodes a row by describing its differences from a similar they have stripes. Since real-world datasets typically have many similar items, most rows end up having very short encodings.
 binary string. All subsequent rows are encoded using statements like  X  X ow like row j except in positions k,l,...  X . This means row i starting with row j and flipping the bits in positions k,l,... encoding, groupNteach -Tree first constructs a distance function to the number of differences between row i and row j . Then finding an encoding is equivalent to constructing a spanning tree: for example, if we encoded row i based on similarity to row j , then ( i, j ) is an edge of weight corresponding tree. It is a tree because each row has exactly one ancestor (the row used to encode it), except the root. Then, we can minimize the number of Algorithm 2. groupNteach-Block .

Algorithm 3. groupNteach-Tree : fast approximate minimum spanning tree-based encoding method differences we need to encode by minimizing the weight of the spanning tree. We could do this by constructing a distance matrix d ( i, j ) between the rows, then finding the MST using e.g. Kruskal X  X  algorithm. Since Kruskal X  X  algorithm would require quadratic time, however, we instead use the Euclidean MST algorithm which takes O ( n log n ) time, as given in Algorithm 3 . 5.3 G ROUP N TEACH -Chain groupNteach -Chain is similar to groupNteach -Tree: we also encode each row based on a similar row. However, unlike the tree pattern of Tree, here each row is encoded based on comparison to the last encoded row. For example, we may encode lions based on tigers , then jaguars based on lions , and so on, forming a chain. This allows the encoding of lions to not encode its parent tigers , since its parent tigers can be deduced from being the last animal encoded. This encodes each row more cheaply and is more efficient for sparse data.
 method as groupNteach -Tree to obtain feature representations the rows. We then use the Euclidean distance between x i and for the number of differences between rows i and j . Starting from the row with the most ones, we repeatedly find the next row to encode as follows: randomly sample a fixed k of the remaining unused rows; choose the closest of these as the next row to encode; then continue this until all rows are encoded. 5.4 G ROUP N TEACH -Fishbone groupNteach -Fishbone aims to efficiently encode data with uneven number of ones in each row, such as power-law degree distributions which are common in online communities [ 2 ]. groupNteach -Fishbone rearranges as many ones as possible to the top-left of the matrix, then takes advantage of the density of that region to encode the data efficiently. To do this, groupNteach first reorders the rows and columns in descending order of their row or column sum. Then, it encodes the top row by encoding a number k followed by a list of exceptions p,q,r,... . This indicates that except at positions row contains k ones, then n  X  k zeroes. k is chosen by trying all possible using the shortest encoding. For efficiency, we terminate the search for if the current encoding is some fixed constant C bits worse than the best found encoding. Having encoded the top row, we then encode the first column of the remaining matrix in the same way, and so on, as shown in Algorithm 4 . Algorithm 4. groupNteach-Fishbone .
 5.5 Extensibility groupNteach canbebrokendownintotwoparts: reorganization (reordering of rows and columns) ,and encoding of the reorganized matrix . can be easily extended by plugging in any matrix reorganization method, such as Cross Association [ 4 ]orMETIS[ 8 ]. As a by-product, groupNteach produces an intuitive ordering and grouping of the objects in the dataset, as was shown in Fig. 1 .
 The process of grouping is described in Algorithm 5 .

Algorithm 5. GroupingCode : groupings of the related facts on the reordered matrix In this section we demonstrate the efficiency and effectiveness of using real and synthetic datasets. We implemented groupNteach all experiments were carried out on a 2.4 GHz Intel Core i5 Macbook Pro, 16 GB RAM, running OS X 10.9.5. Our code and all our datasets are publicly available at http://www.cs.cmu.edu/  X  hyunahs/tol . We used 100 features ( 100) for groupNteach -Chain and groupNteach -Tree, and threshold for groupNteach -Fishbone. The real datasets used are shown in Table 5 .The synthetic datasets used are: 1. Kronecker : a 256  X  256 Kronecker graph [ 12 ], 2.

Blocks : two 50  X  70 blocks of ones in a matrix, 3. Hyperbolic matrix containing 3 overlapping communities of sizes 20, 8 and 4, each resembling a scale-free network.
 Scalability, Q2. Effectiveness, Q3. Discoveries.
 our algorithms. The algorithms are run on random matrices of varying number of rows, fixed to 1000 columns and an average of 10 ones per row. teach allow it to do well on diverse types of data. Figure 3 (b) shows that the various plugins of groupNteach do well on different types on data: Nteach -Block for block-wise, groupNteach -Fishbone Hyperbolic Nteach -Chain and groupNteach -Tree datasets with similar rows or columns (which is the case for the real datasets). No one method dominates the others. and groups the data. We analyze this property using the Drug-Bank groupNteach finds a teaching order with several desirable characteristics, as shown in Fig. 4 . In this paper, we considered the problem of teaching a collection of facts while minimizing student effort. Our contributions are as follows:  X  Problem Formulation : we define the problem of transmitting a matrix of objects and properties adhering to principles from the theory of (human) learning.  X  Optimization Goal : We define an appropriate optimization goal; minimiz-ing ALOC (maximizing student utility).  X  Algorithm :Wepropose groupNteach ,an all-engulfing method that encodes the data while reordering and grouping the data. We evaluate Nteach on synthetic and real datasets, showing that it encodes data more efficiently than a naive encoding approach, measured using both ALOC and total encoding length.  X  Ordering of Groups : When applying groupNteach on real datasets, we find that the orderings and groupings it produces are meaningful.
