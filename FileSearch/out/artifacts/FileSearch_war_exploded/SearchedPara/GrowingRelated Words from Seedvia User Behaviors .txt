 What is the relationship between  X   X  X  X  X  X   X   X  (N atural L anguage P rocessing ) and  X   X  X  X  X   X   X  ( A rtificial I ntelligence ) ? We may regard NL P as a r e search branch of AI . Problems arise when we want to find more words related to th e input query/ seed word. For example, i f seed word  X   X  X  X  X  X  X  X   X  ( N atural L anguage P rocessing ) is entered into Google Sets (Google, 20 10 ) , Google Sets returns an ordered list of r e-lated words such as  X   X  X  X  X  X  X   X  ( A rtificial I n-te l ligence ) and  X   X  X  X  X   X  (Computer) . Gener ally speaking, it performs a large -scale clustering a l-gorithm that can gather related words.

I n this paper, we want to investigate the a d-vantage of user behaviors and re -ranking fram e-work in related words retrieval task using Ch i-nese input method user reco rds . We construct a User -Word bipartite graph to represent the i n-formation hiding in user records. The bipartite graph keep s users on one side and words on the other side. The u n derlying idea is that the more frequently two words co -occur in user records, the more related they are. For e x ample,  X   X  X  X  X   X   X  (Machine Translation) is quite r e lated to  X   X   X  X  X  X   X  (Chinese Word Segmentation) b e cause the two words are usually used together by r e-searchers in natural language processing co m-munity. As a result , u ser behaviors offer a new perspective for measuring relatedness b e tween words. On the other hand, we can also reco m-mend related words to users in order to enhance user experiences. R esearchers are a l ways willing to accept related terminologies in their research fields.

However, t he method is purely statistic s based if we only consider co -occurrence aspect. We want to add semantic features. Sahami and He l-man (2006) utilize search engine to supply web queries with more semantic context and gains better results for query s uggestion task . We bo r-row their idea in this paper. U ser behaviors pr o-vide statistic information to generate cand i date words. Then , w e can enrich candidate words with additional s e mantic feature s using search engine to retrieve more relevant candidates ear l i-er. Statistical and semantic feature s can compl e-ment each other . Therefore, we can gain better performance if we consider them t o gether.
 T he contributions of this paper are three fold. First, we introduce user behaviors in related word retrieval task and construct a U s er -Word bipartite graph from user behaviors . Words are used by users, and i t is reasonable to measure relatedness between words by analyzing user behaviors. Second, we take the advantage of s e-mantic features using search engine to reorder can didate words. We aim to return more r e levant candi dates earlier . F inally, our method is uns u-pervised and language indepe n dent , which means that we do not require any training set or manual labeling e f forts.
 The rest of the paper is organized as follows. So me related works are discussed in S ection 2. Then we introduce our method for related words retrieval in S ection 3. Experiment results and discussions are showed in S ection 4. Finally, S ection 5 concludes the whole paper and gives some f u ture works. F or related words retrieval task, Google Sets (Google , 20 10 ) provides a remarkably interesting tool for finding word s related to an input word. A s stated in (Zheng et al., 2009), Google Sets performs poor results for input word s in Chinese language . Bayesian Sets (Gh a hramani and Heller, 2006) offers an alternative method for related words retrieval under the framework of Bayesian inference. It computes a score for each cand i date word by comparing the posterior probabi l i ty of that word given the input , to the prior probability of that candidate word. Then , it returns a ranked list of candidate words according to the ir co m-puted scores .

Recently, Zheng et al. (2009) introduce user behaviors in new word detection task via a coll a-borative filtering manner. T hey extend their m e-thod to related word retrieval task. More o ver, they prove that u ser behaviors provide a new point for new word detection and r e lated word retri e val task s . However, t heir method is pure ly statistical method without considering semantic features .

W e can regard related word retrieval task as problem of measuring the semantic rel a tedness between pairs of very short texts. Sahami and Helman ( 2006 ) introduce a web kernel function for measur ing semantic similarit ies using sni p-pets of search re sult s . T his work is followed by Metzler et al., ( 2007) , Yih and Meek , ( 2007) . They combine the web kernel with other metrics of similarity between word vectors, such as Ja c-card Coeff i cient and KL Divergence to enhance the result .

In this paper , we follow t he similar idea of u s-ing search engine to enrich semantic features of a query word. We regard the returned snippets as the context of a query word. And then w e reor d er candidate words and expect more relevant cand i-date words can be r e trieved earlier. More details are given in Section 3. In this section, we will introduce how to find related words from a single seed word via user behaviors and re -ranking framework .

First, we introduce the dataset utilized in this paper. All the resour ce used in this paper comes from Sogou Chinese pinyin input method (Sogou, 2006 ) . We use S o gou for abbreviation hereafter. Users can install Sogou on their computers and the word lists they have used are kept in their user records. Volunteers are encourage d to up l-oad their anonymous user records to the ser v er side. In order to preserve user privacy, use r-names are hidden using MD5 hash algorithm .
 Then we de m onstrate how to build a User -Word bipartite graph based on the dataset. The construction can be accomp lish ed while traver s-ing the dataset with linear time cost. We will give more details in S ection 3.1.

Second , we adopt conditional probability ( Deshpande and Karypis , 2004 ) to measure the relatedness of two words. Intu i tively, two words are supposed to be r elated if there are a lot of users who have used both of them. In other words, the two words a lways co -occur in user records. Starting from a single seed word, we can generate a set of candidate words . This is the candidate generation step.

Third , in order to take the advantage of sema n-tic features , we carry out feature extraction tec h-niques to represent generated candidate words with enriched semantic context. In this paper, we generally make use of search e n gine to conduct the feature extraction step. Aft er this step, input seed word and candidate word s are represente d as feature vector s in the vector space .

Fi nally, we can reorder generated candidate words according to their semantic relate d ness of the input seed word. We expect to r e trieve more relevant cand i date words earlier. We will make further expl a nations about the mentioned steps in the next subse c tions. 3.1 Bipartite Graph Construction As stated before, we first construct a User -Word bipartite graph from the dataset. The bipartite graph has two layers , with users on one side and the words on the other side. We travers e the user records , and add a link b e tween user u and word w if w appears in the user record of u . Thus this procedure can be ac co m plished in linear time.
In order to give better explanati on s of bipa r tite graph construction step , we show some user records in Figure 1 and the corresponding bipa r-tite graph in Figure 2 .
 From Fig ure 1, we can see that Word 1 and Word 2 appear in User 1  X  s record , which indicates that User 1 has used W ord 1 and W ord 2 . As a result, in Fig ure 2, node U ser 1 is linked with node W ord 1 and W ord 2 . The rest can be done in the same manner. 3.2 Candidates Gen eration After the construction of bipartite graph, we can measure the relatedness of words from the bipa r-tite graph. Intuitively, if two words always co -occur in user records, they are related to each other. Inspired by ( Deshpande and Karypis , 2004) , we ad opt conditional probability to mea s-ure the relatedness of two words.

In particular, the conditional probability of word j occurs given that word i has already a p-peared is the number of users that used both word i and word j divided by the total nu m ber of u sers that used word i .
In formula 1, Freq(X) is the number of users that have used words in the set X . We can clear ly see that P(j|i)  X  P(i|j) , which means that cond i-tional probability leads to asymmetr ic relations. T he disadvantage is that each word i tend s to have a close relationship with stop words that are used quite frequently in user records , such as  X   X   X  (of) and  X   X  X   X  (a) .

In order to alleviate this problem, we co n sider the conditional probabiliti es P(j|i) and P(i|j) t o-gether. Word i and word j is said to be quite r e-lated if conditional prob a bilities P(j|i) and P(i|j) are both relatively high. We borrow the idea pr o-posed in ( Li and Sun, 2007 ) . In their paper, a weighted harmonic averaging is used t o define the relatedness score between word i and word j because either P(j|i) or P(i|j) being too small is a s e vere detri ment.
In formula 2, p arameter [0,1]  X   X  is the weight for P(i|j) , which denotes how much P(i|j) should be emphasized. We carry out some compar a tive experiments when parameter  X  v a ries from 0 to 1 stepped by 0.1 . W e also tried other co -occurrence based measures like mutual inform a-tion, Euclidean and Ja c card distance, and found that weight harm onic averaging gives relatively be t ter results. Due to space limitation, we are not able to report detailed results .

So far, we have introduced how to calculate the rel a tedness Score(i, j) between word i and word j . W hen a user enters an input seed word w , we can compute Score(w,c) between seed word w and each candi date word c , and then sort ca n-didate words in a descending order. Top N ca n-didate words are kept for re -ranking, we aim to reorder top N candidate words and return the more related candidate word s earlier. Altern a-tively , we can also set a threshold for Score(w,c) , which keeps the candidate word c with Score(w,c) larger than the threshold. We argue that this thr e-shold is difficult to set because different seed words have different score threshold s .

Note that this candidate generation step is completely statistical method as w e only consi d-er the co -occurrence of words. We argue that semantic features can be a complement of stati s-tical method. 3.3 Semantic Feature Representation and As stated b efore, we utilize search engine to enrich semantic features of the input seed word and top N candidate words. To be more sp e cific , we issue a word to a search engine (S o gou, 2004) and get top 20 r eturned snippets . We regard snippets as the context and the semantic repr e-sentation of this word.
 For an input seed word w , w e can generat e top N candidate words using formula (2) . We issue each word to search engine and get returned snippets . Then, each word is represented as a feature vector using bag -of -words mo del . Fo l-low ing the conventional approach , w e c alc u late the relatedness b e tween the input seed word w and a candidate word c as the cosine similarity between their fe a ture vectors . Intuitively , if we introduce more candidate words, we are more likely to fin d r e lated words in the candidate sets. However, noisy words are inevi t ably included. We will show how to tune parameter N in the exper i ment part .

As a result, candidate words with higher s e-mantic similarit ies can be returned earlier with enriched semantic features . Re -ranking can be regarded as a complement ary step after candidate generation. W e can improve the performance of related word retrieval task if we consider user behaviors and re -ranking t o gether . In this section, we demonstrate our exp er i ment results. First, we introduce the dataset used in this paper and some statistics of the dat a set. T hen, we build our ground truth for related word r e-trieval task using Baidu e ncyclop e dia . Third , we give some example of related word r e trieval task . We show that more related words can be r e-turned earlier if we consider s e mantic features. F inally, we make further analysis of the param e-ter tuning mentioned before. 4.1 Experiment Settings W e carry out our experiment on Sogou Chinese input method dataset. The d ataset contains 10,000 users and 183 , 870 words, and the number of edges in the constructed bipartite graph is 42 , 250 , 718 . As we can see, t he dataset is quite sparse, because most of the users tend to use only a small number of words.

For related word retri eval task, we need to judge whether a candidate word is related to the input seed word. We can ask domain experts to answer this question . However, it needs a lot of m a nual efforts . To alleviate this problem, we adopt Ba i du encyclopedia (Baidu, 2006) as ou r ground truth. In Ba i du encyclopedia , v olunteers give a set of words that are related to the partic u-lar seed word. As related words are pr o vided by human, we are confident enough to use them as our ground truth .

We randomly select 2,000 seed words as our validation set. However, whet h er two words are related is quite subjective . In this paper, Baidu encyclopedia is only used as a relatively accurate standard for evaluation. W e just want to invest i-gate whether user behaviors and re -ranking framework is help ful in the related word retrieval task under various evaluation metrics .
 We give a simple example of our method in T able 1 . The input seed word is  X   X  X  X  X  X  X   X  ( Machine Learning ) . Generally speaking , all these returned candidate words are relevant to the seed word to certain degree, which indicates the effectiveness of our m e thod.
Table 1 . Words Related to  X  Machine Learning  X  4.2 Evaluation Metric s In this paper, we use th ree evaluation metrics to validate the performance of our m e thod : 1. Precision@N ( P@N ) . P@N measures how 2. Binary preference measure ( Bpref ) (Buc k-3. Mean reciprocal rank of the first r e trieved 4.3 Candidate Re -ranking In order to show the effect iveness of semantic features and re -ranking framework, we give an example in T able 2 . The input seed word is  X   X   X  X  X   X  ( Ericsson ) , and if we only take user beh a-viors into consideration , top 5 words returned are shown on the left side. After using search engine and semantic representation , we reorder the ca n-didate words as shown on the right side.

As shown i n T able 2 , we can clearly see that we return the most related candidate words such as  X   X  X  X  X  X  X  X   X  (Sony Ericsson) and  X   X  X  X   X  (the a b breviation of Sony Ericsson in Chinese) in the first two places. More o ver, after re -ranking, top candidate words are some famous b rand s that are quite related to query word  X   X  X  X  X   X  ( Eric s-son ) . Some words like  X   X  X  X  X   X  (Core Network) that are not quite r e lated to the query word are removed from the top list. From this observation, we can see that s emantic fe a tures and re -ranking framework can improve the pe r formance. 4.4 Parameter Tuning As discussed in S ection 3, we have introduced two parameters in this paper. The first is the p a-rameter  X  in the candidate generation step, and the other is the parameter N in the re -ranking step. We show how th e se two parameters affect the performance. In addition, we should empha s-ize that the ground truth is not a complete answer, so all the results are only us e ful for comparison s . The absolute value is not very meaningful.

As we have shown in S ection 3.2, para meter  X  adjust s the weight of conditional probability b e-tween two word i , j . The parameter  X  is va ried from 0 to 1 stepped by 0.1 . We record the co r-responding values of P@5, P@10, Bpref and MRR . The results are shown in Fig ure 3.

We can clearly see that al l the values increase when  X  increases first . And then all the values decrease dr a matically when  X  is close to 1. This indicates that e ither P( j | i ) or P(i|j) being too small is a severe detr i ment. The result reaches peak value when  X  =0.5, i.e. we should tr eat P( j | i ) and P(i|j) equally to get the best result. Therefore , we use  X  =0.5 to generate ca n didate words, those candidates are used for re -ranking.
 Fig. 3. Parameter  X  for Candidate Generation
We also carry out the comparisons with Bay e-sian Sets, which is shown in T able 3 . It is clear that our method gains better results than Bay e-sian Sets with different values of parameter  X  . Results of Google Sets ar e omitted here because Zheng et al. (2009) have already showed that Google Sets performs worse than Bay esian Sets with query words in Chinese.

To investigate the effectiveness of re -ranking framework, we also conduct exper i ments on the parameter N that is used for re -ranking. The e x-peri mental results are shown in Fig ure 4.

We can observe that more candi dates tend to harm the performance as noisy words are intr o-duced inevitab ly . For example, B p ref drops to less than 0.25 when N = 100. More comparative result s are shown in T a ble 4 . We can see that N = 20 gives relatively best results, which indicates that we should select Top 20 candidate words for re -ranking.

Table 4 . Comparisons with Re -ranking Method In this paper, we have propose d a novel method for r e lated word retrieval task. Different from other method, we consider user behaviors, s e-mantic features and re -ranking framework t o-gether. We make a reasonable assumption that if two words always co -occur in user records , then they tend to have a close relationship with each other. Based on this assumption, we first gene r-ate a set of candidate words that are related to an input seed word via user behaviors. Secon d , we utilize search engine to enrich candidates with semantic features. Finally, we can reorder the candidate words to return more related cand i-dates earlier. Experiment results show that our method is effective and gains better r e sults.
However , we also observed some noisy words in the returned results. As our dataset is genera t-ed from Chinese input method, users can type whatever they want, which will bring some noise in the dataset. We plan to r e move noisy words in the future. Furthermore, we want to ta ke the a d-vantage of learning to rank liter a ture (Liu, 2009) to further improve the performance of related word retrie v al task. We may need to extract more features to represent the word pairs and build a labeled training set. Then various machine lear n-ing techniques can be used in this task.

Another important issue is how to build a complete and accurate ground truth for related word retrieval task . People may have different opinions about whether two words are related or not, which makes this problem compl icate.

Thirdly, our method can only process a single seed word , so we aim to extend our method to process multi ple seed words. In addition, we want to build a network of Chinese word associ a-tion . We can discover how words are o r ganized and connected within this network. And this word association network will be quite useful for foreigners to learn Ch i nese.

Fourthly, how to deal with ambiguous query word is also left as our future work. For example, query word  X  apple  X  can refer to a kind of fruit or an IT co mpany. As a result, w e are expected to return two groups of related words instead of mixing them together.

F inally, our dataset provide s a new perspective for many interesting research tasks like new word detection, social network analysis , user b e-havior a nalysis, and so on . W e are trying to r e-lease our dataset for research use in the f u ture. We thank Xiance Si and Wufeng Ke for provi d-ing the Baidu e ncyclopedia corpus for evaluat i on . We also thank the anonymous reviewers for their helpful co mments and suggestions . This work is supported by a Tsinghua -Sogou joint research project .

