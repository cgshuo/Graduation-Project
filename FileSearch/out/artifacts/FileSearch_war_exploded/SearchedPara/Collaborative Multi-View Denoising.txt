 In multi-view learning applications, like multimedia analysis and information retrieval, we often encounter the corrupted view problem in which the data are corrupted by two differ-ent types of noises, i.e. , the intra-and inter-view noises. The noises may affect these applications that commonly acquire complementary representations from different views. There-fore, how to denoise corrupted views from multi-view data is of great importance for applications that integrate and analyze representations from different views. However, the heterogeneity among multi-view representations brings a sig-nificant challenge on denoising corrupted views. To address this challenge, we propose a general framework to jointly denoise corrupted views in this paper. Specifically, aim-ing at capturing the semantic complementarity and distri-butional similarity among different views, a novel Heteroge-neous Linear Metric Learning (HLML) model with low-rank regularization, leave-one-out validation, and pseudo-metric constraints is proposed. Our method linearly maps multi-view data to a high-dimensional feature-homogeneous space that embeds the complementary information from differen-t views. Furthermore, to remove the intra-and inter-view noises, we present a new Multi-view Semi-supervised Collab-orative Denoising (MSCD) method with elementary trans-formation constraints and gradient energy competition to establish the complementary relationship among the hetero-geneous representations. Experimental results demonstrate that our proposed methods are effective and efficient.  X  Information systems  X  Data mining;  X  Computing methodologies  X  Machine learning; Multi-view; denoising; heterogeneity; metric learning  X  Co rresponding Author.

Figure 1: Intra-View Noise and Inter-View Noise.
With the rapid development of modern information tech-nology, a large number of high-tech digital products appear in real world. The multi-view data produced by these elec-tronic equipments become available in various fields, includ-ing medical diagnosis, webpage classification, and multime-dia analysis. These multi-view data show heterogeneous characteristics of low-level features and the correlation of high-level semantics.

Generally, due to inappropriate data processing, man-made mistakes, random events, and the like, not all instances are a prefect reflection of objective reality, resulting in the corrupted views of multi-view data. Rather, the corrupted view problem in multi-view learning is essentially different from single-view one. The reason is that multi-view data are always corrupted by two different types of noise. One refers to the intra-view noise that makes the instances from different categories in the same view grouped together while keeping the samples of the same class away from each oth-er simultaneously. The other represents the noise existing among different views, i.e., the inter-view noise, leading to false complementary relationship among the heterogeneous representations of the same object. For example, as shown in Fig. 1, the existence of intra-view noise causes the zebra pho-t o is incorrectly grouped with the tiger images; additionally, the unmatched white tiger picture is wrongly correlated with the Siberian tiger image from man-made mistakes, leading to intra-view noise.

More notably, these noise levels are high enough to affect the performance of multi-view data, leading to false classi-fication, clustering, retrieval and analysis. Thus before ex-tracting vital information from multi-view data or next level of processing, it is essential to denoise them to improve the quality of multi-view data for a more accurate and rigorous assessment [ 1, 2, 3, 4, 5]. Furthermore, to the best of our k-Figure 2: Complementarity and Distributivity Re-s traints on Multi-View Data. nowledge, no existing efforts have focused on denoising the corrupted views of multi-view data. Consequently, the above-mentioned applications face great challenge in the real world. Thus, it is necessary to develop an effective denoising method for multi-view corrupted data.

However, it is a challenging task to denoise the corrupted views of multi-view data. First of all, since different views span heterogeneous low-level feature spaces, there is no ex-plicit correspondence among the heterogeneous representa-tions from different views. For example, in the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) [ 6] database, ob-j ects not only have Positron Emission Tomography (PET) scan, but also own Magnetic Resonance Imaging (MRI) mea-surement. Therefore, to denoise the corrupted views of multi-view data, an issue to be first addressed is to learn a couple of heterogeneous metrics through multi-view un-corrupted data to capture the semantic complementarity a-mong different views.

Meanwhile, for multi-view data, it can be assumed as illus-trated in Fig. 2 that they are under both complementarity a nd distributivity constraints. The complementarity con-straint refers to the semantic complementarity among differ-ent views that makes much more the complementary infor-mation from different views fully contained in the multi-view data. Unlike the complementarity constraint, the distribu-tivity constraint takes high distributional similarity which can group the samples of the same class from the same view together. Hence, another issue we need further to deal with for denoising corrupted views is to refine multi-view corrupt-ed data under both the complementarity and distributivity constraints.
The key contributions of this paper are highlighted as fol-lows:
The remainder of this paper is organized as follows: We present a general framework for denoising the corrupted views of multi-view data in Section 2.1 . In Section 2.2 , a n ovel Heterogeneous Linear Metric Learning (HLML) mod-el is developed for correlating different views. We build a new Multi-view Semi-supervised Collaborative Denoising (MSCD) method to remove the intra-and inter-view noises under both complementarity and distributivity constraints in Section 2.3 . Furthermore, Section 3 provides two efficien-t algorithms to solve the proposed framework. Section 4 gives a broad overview of some related works. Experimental r esults and analyses are reported in Section 5. Section 6 concludes this paper.
We establish some notations to be used throughout this paper in Table 1.

Here we propose a general framework to denoise the cor-rupted views of multi-view data. To facilitate the under-standing of our proposed framework, Fig. 3 gives an overall i llustration of the proposed framework. More details are presented in the following subsections. x
We provide an overview of the proposed formulations by using the example in Fig. 3. In this example, a set of multi-v iew data consists of View X and View Y . There are a cer-tain amount of multi-view uncorrupted data such as ( x 1 , y However, some multi-view data are corrupted. For instance, the zebra representations x 9 and y 10 are wrongly grouped into the tiger category, and the co-occurring heterogeneous representations in the multi-view data ( x 7 , y 7 ), ( x ( x 9 , y 9 ) have incorrect complementary relationships.
To denoise the corrupted views of multi-view data, multi-ple heterogeneous linear metrics are learned by HLML model to build a high-dimensional feature-homogeneous subspace among multiple heterogeneous low-level feature spaces in the proposed framework. Specifically, to fully exploit the semantic complementarity and distributional similarity a-mong different views, multiple heterogeneous linear metrics A and B are learned using the multi-view uncorrupted da-ta X U and Y U to eliminate the heterogeneity across them. Thus, a feature-homogeneous subspace is obtained, in which the correlated representations from different views are cou-pled together to capture much more complementary infor-mation from different views. At the same time, the samples of the same class from the same view can be grouped to-gether while keeping the instances from different categories away from each other simultaneously. For example, the ze-bra heterogeneous representations x 6 and y 6 are matched together to capture much more complementary information from different views. Furthermore, the tiger co-occurring heterogeneous representations ( x 1 , y 1 ), ( x 2 , y 2 ), and ( x and the zebra co-occurring heterogeneous representations ( x tively to mine the distributional similarity among different views.

Meanwhile, by exploiting multiple heterogeneous metrics learned by HLML model, the intra-view noises existing in the multi-view corrupted data X C and Y C are removed by MSCD model to a large extent in the feature-homogeneous space on the basis of both semantic complementarity and distributional similarity among different views. Moreover, the MSCD method utilizes the proposed elementary trans-formation constraints based on GEC criterion to establish the complementary relationship among the heterogeneous representations of the same object according to the learned multiple heterogeneous metrics. The constraints will switch the positions of corresponding representations in the cor-rupted matrix X C and Y C to eliminate the inter-view noises. For instance, the zebra representation x 9 in the View X is pulled out the group composed of the tiger representations x 7 and x 8 ; besides, the zebra representation y 10 in the View Y is pushed closer to the cluster consisting of the zebra rep-resentations to remove intra-view noise; and the zebra repre-sentations y 7 and y 9 in the View Y are switched respectively to match the appropriate representations to eliminate inter-view noise effectively. After denoising, the heterogeneous representations from different views are correctly matched and grouped together in the feature-homogeneous space.
In the following, a novel multi-view metric learning method is developed for capturing both semantic complementarity and distributional similarity among different views in this subsection. Our work is motivated by a few prior stud-ies. Recently, Sun et al [ 7] have proved that the homo-g eneous transformations can significantly capture the com-plementarity among different views. Moreover, Weinberg-e r et al [ 8] have pointed out that the pseudo-metric based o n Mahalanobis distance can be used to effectively elimi-nate the intra-view noise. Furthermore, Goldberger et al [9] have proved that the Mahalnobis distance metric based o n leave-one-out validation can exploit the characteristics of sample distribution to improve the performance of classifi-cation. Additionally, Liu et al [ 10 ] have pointed out that the r ank is a powerful tool to capture between-class differences in the matrix case. Nevertheless,  X  rank (  X  ) X  is not a convex function, which leads to the difficulty in finding the optimal solution. Fortunately, Cand`es and Recht [ 11 ], Recht et al [12 ], and Cand`es and Tao [ 13 ] have theoretically justified t hat the trace norm of a matrix can be used to approximate the rank of the matrix.

Following the above-mentioned strong theoretical support-s [ 7, 8, 9, 10 , 11 , 12 , 13 ], we propose a novel HLML model w ith low-rank regularization, leave-one-out validation, and pseudo-metric constraints to learn multiple heterogeneous linear transformations for multi-view data, as shown in Fig. 4. P articularly, the existing uncorrupted heterogeneous repre-sentations X U and Y U are utilized in HLML model to learn multiple well-defined pseudo-metrics A and B to mine the distributional similarity among different views. Then, to make transformed data M U and R U maximally linearly sep-arable, it is essential to impose the low-rank regularization on the transformed data. As a consequence, the heteroge-neous representations are linearly projected into a feature-homogeneous space, in which the correlated representations from different views are coupled together to capture the se-mantic complementarity among different views.

More specifically, the new distance metrics are defined as follows to learn a Mahalanobis distance: where M X = A T A and M Y = B T B are two positive semi-definite matrices. Thus, the linear transformations A and B can be applied to each pair of co-occurring heterogeneous representations ( x i , y i ).

Assuming C t X and C t Y be the sample sets of t -th class from the views V x and V y , respectively. We define each sample x or y i selects another sample y j or x j in another view as its neighbor with the probability p ij or q ij . By using a softmax under the Euclidean distance in the transformed feature-homogeneous space, p ij and q ij are defined as follows: Under this definition, we can compute the probabilities p and q i that the sample i will be correctly classified: Figure 4: Heterogeneous Linear Metric Learning.
 T hen the proposed approach can be formulated as follows: where A  X  R d x  X  k , B  X  R d y  X  k , k is the dimensionality of the feature-homogeneous subspace, the positive semidefinite constraints A T A 0 and B T B 0 are added into the opti-mization to ensure a well-defined pseudo-metric, and  X  and  X  are two trade-off parameters. The first term in the ob-jective function is used to capture the semantic complemen-tarity among different views. The motivation of introducing the leave-one-out validation g ( A, B ) consisting of the classification accuracies of different views is to mine the distributional similarity among different views. In addition, the third term h ( A, B ) in the objective function is a low-rank regularization based on trace norm to make transformed data M U and R U carrying more between-class differences information.

It is worth to note that no similar numerical method has been yet proposed. Our proposed HLML model is greatly d-ifferent from well-known kernel methods [ 14 , 15 ] without an e xplicit high-dimensional projection and classical linear al-gorithms [ 7, 16 ] to reduce dimensionality. HLML can linear-l y project the multi-view data into a feature-homogeneous space of even higher dimensions. That is to say, k may be greater than both d x and d y , i.e., k  X  max ( d x , d y
Furthermore, the Parallel Feature Fusion Strategy (PFFS) [17 , 18 ] is adopted to establish the common representations. T he details is as follows: for the i -th pair of heterogeneous representations ( x i , y i ), we can obtain their own Homoge-neous Correlated Representations (HCR) with the optimal A  X  and B  X  by: Consequently, we can obtain a Complex Representations (CR) i in the feature-homogeneous subspace based on x i
In Section 3.1 , an efficient algorithm is proposed to solve t he problem  X  1 . Figure 5: Multi-view Semi-supervised Collaborative D enoising.
In the above subsection, we have built a high-dimensional feature-homogeneous space by multiple learned heteroge-neous linear metrics to capture both semantic complemen-tarity and distributional similarity among different views. Furthermore, to eliminate the intra-and inter-view noise, it is essential to recover the complementary relationship a-mong the heterogeneous representations of the same object in the multi-view corrupted data on the basis of the learned heterogeneous metrics.

In [ 19 ], it has been pointed out that an elementary row t ransformation matrix can be used to exchange any rows of a matrix. Additionally, Rubinstein et al [ 20 ] have proposed r ecently a forward-looking energy function that measures the effect of seam carving on the retargeted image, not the original one. They have shown how the new measure can be used in either graph cut or dynamic programming and demonstrated the effectiveness of their contributions on sev-eral images and video sequences. Moreover, it has been tes-tified in [ 21 ] by Muslea et al that the robust performance o f multi-view learning depends on interleaving active and semi-supervised learning on the basis of view correlation. Furthermore, Blum and Mitchell [ 22 ] have proved that for a p roblem with two views the target concept can be learned based on a few labeled and many unlabeled examples, pro-vided that the views are compatible and heterogeneous.
Based on the above-mentioned strong theoretical supports [19 , 20 , 21 , 22 ], we propose a new MSCD model with elemen-t ary transformation constraints and GEC criterion to elimi-nate the intra-and inter-view noises according to the learned semantic complementarity and distributional similarity a-mong different views in Section 2.2 . As shown in Fig. 5, M SCD model eliminates the intra-and inter-view noises by means of semi-supervised learning. It firstly makes use of the uncorrupted linearly-separable representations M U and R
U with labels to learn a decision matrix W . Then through the learned elementary row transformation matrices T and H , MSCD switches the positions of the noise-corrupted sam-ples in the matrices M C and R C ; meanwhile, the decision matrix W is applied to predict the classification of the un-labeled noisy representations M C and R C to establish the complementary relationship among the heterogeneous rep-resentations of the same object.

Specifically, let ( A  X  , B  X  ) be the optimal solutions of the problem  X  1 . Then the proposed approach can be formulated as follows: transformation matrices, W  X  R k  X  m is a decision matrix, M
C = X C A  X  and R C = Y C B  X  are the noise-corrupted ma-trices in View X and View Y , respectively, M U = X U A  X  R
U = Y U B  X  are the uncorrupted linearly-separable repre-sentations in the feature-homogeneous space, E n 2  X  R n 2 is a set of elementary row transformation matrices, and  X  and  X  are two regularization parameters. The first term in the objective function takes advantage of the learned W , T , and H to recover the complementary relationship among the heterogeneous representations of the same object. The second term in the objective function is a linear least square loss to learn an excellent decision matrix W using the uncor-rupted linearly-separable representations M U and R U with labels. The goal of imposing the orthogonal constraints on W is to effectively remove the correlations among differ-ent classes. The motivation of introducing the elementary transformation constraints is to ensure the matrices T and H be two standard elementary row transformation matrices to switch the positions of the noise-corrupted samples in the matrices M C and R C .

Recently, the gradient energy measure has been widely used in dynamic programming and demonstrated its effec-tiveness on graph cut or seam carving[ 20 ]. Based on the a bove strong theoretical supports, we propose a Gradient Energy Competition (GEC) criterion to build an elemen-tary row transformation matrix.

In detail, in a gradient matrix G obtained by gradient de-scent method, every internal element G ij is connected to its the  X  1 -norm gradient magnitude energy [ 20 ], we define the b etween-sample energy E bs of G ij in the vertical direction as and the within-sample energy E ws in the horizontal direction as The global energy of G ij can be obtained via E bs and E ws where  X  is a trade-off parameter. We use Eq.( 15 ) to compute Algorithm 1: H eterogeneous Linear Metric Learning ( HLML ) Input: F ( ) , D ( ), h ( ), Z 0 =[ A Z 0 B Z 0 ],  X  , X U Output: Z  X  . 1: Define F  X ,S ( Z )= D ( S )+ h  X  D ( S ) , Z  X  S i +  X  k Z  X  S k 3: for i =1,2, , max  X  iter do 5: Compute A S i = (1 +  X  i ) A Z i  X   X  i A Z i 6: Compute B S i = (1 +  X  i ) B Z i  X   X  i B Z i 7: Set S i = [ A S i B S i ]. 8: Compute  X  A S D ( A S i ) and  X  B S D ( B S i ). 9: while (true) 10: Compute c A S = A S i  X   X  A S D ( A S i ) / X  i . 11: Compute [ A Z i +1 ] = PSP ( c A S ). 12: Compute c B S = B S i  X   X  B S D ( B S i ) / X  i . 13: Compute [ B Z i +1 ] = PSP ( c B S ). 15: if F ( Z i +1 )  X  F  X  i ,S i ( Z i +1 ), then break; 16: else Update  X  i =  X  i  X  2. 17: end-if 18: end-while 19: Update t i = 1+ 20: end-for 21: Set Z  X  = Z i +1 . global energy of every element in the matrix G , and then an energy matrix E can be obtained. Furthermore, the global energies of every element in the matrix E are compared. As shown in Fig. 6, the winner which owns the greatest energy w ill be set to 1, the rest of the elements in the same row and column to 0. And the cycle repeats until a standard elementary transformation matrix T is established. It is worth to note that no similar method has been yet proposed.
Section 3.2 presents an efficient algorithm to compute the o ptimum for the problem  X  1 .
Here we provide two efficient algorithms to solve the pro-posed framework. Specifically, an iterative algorithm for solving the HLML model  X  1 (see Section 2.2 ) is presented i n the Section 3.1 . Moreover, the Section 3.2 shows how to s olve the MSCD model  X  1 proposed in Section 2.3 .
For notational simplicity, we denote the optimization prob-lem  X  1 in Eq.( 7) by: where D ( ) = k k 2 F  X   X g ( ) is a smooth objective function, Z =[ A Z B Z ] symbolically represents the optimization vari-ables, and C is the closed and separately convex domain with respect to each variable: As D ( ) is continuously differentiable with Lipschitz contin-uous gradient L [23 ]: k  X  D ( Z x )  X   X  D ( Z y ) k F  X  L k Z x  X  Z y k F ,  X  Z x thus it is appropriate to adopt the Accelerated Projected Gradient (APG) [ 23 , 24 , 25 ] method to solve Eq.( 16 ).
T he APG algorithm is a first-order gradient method, which can accelerate each gradient step on the feasible solution to obtain an optimal solution when minimizing a smooth function [ 26 ]. This method will construct a solution point s equence { Z i } and a searching point sequence { S i } , where each Z i is updated from S i .

Note that, in the APG algorithm, the Euclidean projec-tion of a given point s onto the convex set C can be defined by: Weinberger et al. proposed a Positive Semi-definite Projec-tion (PSP) [ 8] to minimize a smooth function while remain-i ng positive semi-definite constraints. Then we can use the PSP to solve the problem in Eq.( 19 ).

F inally, when applying the APG method for solving the problem in Eq.( 16 ), the projection Z = [ A Z B Z ] of a given point S = [ A S B S ] onto the set C is defined by: By combining APG and PSP, we can solve the problem in Eq.( 20 ). The details are given in Algorithm 1. Algorithm 2: Energy Input: G  X  R n ,  X  .
 O utput: an energy matrix E  X  R n . 1: for i =1,2, , n do 2: for j =1,2, , n do 26: end-if 27: Compute E ij =  X   X  E bs + (1  X   X  )  X  E ws . 28: end-for 29: end-for
T his subsection provides an efficient algorithm to solve the model  X  1 proposed in Section 2.3 . Similarly, the opti-m ization problem  X  1 can be simplified as: where Q ( ) = k k 2 F is a smooth objective function, Z = [ T
Z H Z W Z ] symbolically represents the optimization vari-ables, and Q is the closed domain set with respect to each variable: Similarly, as Q ( ) is continuously differentiable with Lips-chitz continuous gradient L [23 ] in the Eq.( 18 ), it is also a ppropriate to adopt the Accelerated Projected Gradient (APG) [ 23 ] method to solve the problem in Eq.( 21 ).
I n like manner, we can define the Euclidean projection of a given point s onto the closed set Q in the APG algorithm as: To solve the Eq.( 23 ), we use the proposed GEC criterion in S ection 2.3 to project the approximate solution of the prob-l em into the elementary transformation constraint Q . Two new functions Energy ( ) and Competition ( ) are designed in this subsection to implement the GEC criterion.
The proposed function Energy ( ) in Algorithm 2 will com-p ute the global energy of every internal element in the gra-dient matrix G obtained by gradient descent method on the basis of the position of each element according to E-q.( 13 ,14 ,15 ). Thus, an energy matrix E c an be obtained. In addition, we also develop a Competition ( ) function in Algorithm 3 to establish a standard elementary transforma-t ion matrix according to the energy matrix E produced by Algorithm 2.
 Algorithm 3: Competition Input: E  X  R n .
 O utput: Z  X  R n . 1: Build a zero matrix Z  X  R n . 2: for i =1,2, , n do 3: Find the row and column coordinates r and c of 4: Set Z r,c = 1. 5: Replace the other components of r -th row and 6: end-for
Note that the orthogonality constraints are included in E-q .( 21 ). Recently, the Gradient Descent Method with Curvi-l inear Search (GDMCS) [ 27 ] proposed by Wen and Yin can e ffectively deal with these difficulties. Thus, we can use the GDMCS to preserve the orthogonality of a given point s in Eq.( 21 ). By combining APG, Algorithm 2 and 3, and GDM-C S, we can solve the problem in Eq.( 21 ). The details are g iven in Algorithm 4, where the function S chmidt ( ) [ 19 ] d enotes the GramSchmidt process.
This section reviews some related works. We begin by discussing some prior methods for mining the correlation between different views in multi-view learning. And then s-Algorithm 4: M ulti-view Semi-supervised Collaborative Denoising ( MSCD ) Input: Q ( ) , T Z 0 = I n , H Z 0 = I n , W Z 0 , Z 0 =[ T Output: Z  X  . 1: Define Q  X ,S ( Z )= Q ( S )+ h  X  Q ( S ) , Z  X  S i +  X  k Z  X  S k 2: Compute [ W Z 0 ] = Schmidt ( W Z 0 ). 4: for i =1,2, , max  X  iter do 6: Compute T S i = (1 +  X  i ) T Z i  X   X  i T Z i 7: Compute H S i = (1 +  X  i ) H Z i  X   X  i H Z i 8: Compute W S i = (1 +  X  i ) W Z i  X   X  i W Z i 9: Set S i = [ T S i H S i W S i ]. 10: Derive  X  T S Q ( T S i ),  X  H S Q ( H S i ), and  X  W S 11: while (true) 12: Compute c T S =  X   X  T S Q ( T S i ) / X  i . 13: Compute [ \ T Z i +1 ] = Energy ( c T S ,  X  ). 14: Compute [ T Z i +1 ] = Competition ( \ T Z i +1 ). 15: Compute c H S =  X   X  H S Q ( H S i ) / X  i . 16: Compute [ \ H Z i +1 ] = Energy ( c H S ). 17: Compute [ H Z i +1 ] = Competition ( \ H Z i +1 ). 18: Compute d W S = W S i  X   X  W S Q ( W S i ) / X  i . 19: Compute [ \ W Z i +1 ] = Schmidt ( d W S ). 20: Compute [ W Z i +1 ] = GDMCS ( \ W Z i +1 ,  X  1 ,  X  ). 22: if Q ( Z i +1 )  X  Q  X  i ,S i ( Z i +1 ), then break; 23: else Update  X  i =  X  i  X  2. 24: end-if 25: end-while 26: Update t i = 1+ 27: end-for 28: Set Z  X  = Z i +1 . ome representative linear metric learning technologies for m ono-view data are investigated to show the correlation be-tween them and our proposed work. Finally, some multi-view semi-supervised learning algorithms to bootstrap clas-sifiers in each view are studied.
To eliminate the heterogeneity across different views, many feature homogeneous techniques based on Subspace Learn-ing (SL) [ 7, 16 , 14 , 15 ] have been proposed recently.
C CA (Canonical Correlation Analysis) [ 7] and PLS (Par-t ial Least Squares) [ 16 ] are two classical statistical analysis t echniques for modeling correlation between sets of observed variables. They both compute low-dimensional embedding of sets of variables simultaneously. The main difference of them is that CCA maximizes the correlation between vari-ables in the embedded space, while PLS maximizes their covariance. Kernel CCA (KCCA) [ 14 ] offers a nonlinear a lternative solution for CCA by implicitly mapping multi-view data into a high-dimensional feature-homogeneous s-pace. Unlike KCCA, Deep CCA (DCCA) in [ 15 ] does not r equire an inner product, which provides a flexible nonlinear alternative to KCCA.
Many representative metric learning algorithms, such as Large Margin Nearest Neighbors (LMNN) [ 8], Information T heoretic Metric Learning (ITML) [ 28 ], Neighborhood Com-p onent Analysis (NCA) [ 9], Logistic Discriminative Metric L earning (LDML) [ 29 ], and Linear Discriminant Analysis ( LDA) [ 30 ], are based on linear transformation and distance m etric.

Weinberger et al. [ 8] proposed a linear metric learning al-g orithm called LMNN based Mahalanobis distance [ 8] for k-N earest Neighbors (kNN) [ 31 ] classification. ITML [ 28 ] uses a one to one correspondence between the Mahalanobis dis-tance to minimize the differential relative entropy between two multivariate Gaussians under constraints on a distance function. NCA [ 9] is another linear metric learning method t o find a distance metric that maximizes the performance of kNN classification, measured by Leave-One-Out (LOO) validation. A logistic discriminant approach based marginal probability named LDML was presented by Guillaumin et al. [ 29 ] to learn a metric from a set of labelled image pairs. L DA [ 30 ] is widely used as a form of linear preprocessing f or pattern classification, which is operated in a supervised setting and uses the class labels of the inputs to derive in-formative linear projections.
Recently, some researchers have investigated many semi-supervised learning methods [ 21 , 32 , 33 , 34 ] to deal with v arious multi-view problems.

Muslea et al. pointed out in [ 21 ] that the robustness o f multi-view learning came from the combination of semi-supervised and active learning. In [ 32 ], Qian et al. presented a joint learning framework based on reconstruction error, namely Semi-Supervised Dimension Reduction for Multi-label and Multi-view Learning (SSDR-MML), to perform optimization for dimension reduction and label inference in multi-label and multi-view learning settings. Yan and Naphade [ 33 ] proposed a novel multi-view semi-supervised l earning algorithm called Semi-supervised Cross Feature Learn-ing (SCFL) for detecting the video semantic concepts, which can handle additional views of unlabeled data even when these views were absent from the training data. A Multi-View Vector-Valued Manifold Regularization (MV3MR) al-gorithm was developed in [ 34 ] to integrate multiple features f rom different views in the learning process of the vector-valued function.
In this section, we evaluate and analyze the effectiveness of the proposed formulations and algorithms for denoising the corrupted views of multi-view data.
Our experiments are conducted on three publicly available multi-view datasets, namely, UCI Multiple Features (UCI MFeat) [ 35 ], COREL 5K [ 36 ], and Alzheimer X  X  Disease Neu-r oimaging Initiative (ADNI) [ 6].
Note that all the data are normalized to unit length. Each dataset is randomly separated into a training set and a test set. The training samples account for 80 percent of each original dataset, and the remaining ones act as the test data. Such a partition of each dataset is repeated five times and the average performance is reported. In the training and test sets, 10 percent of multi-view data have corrupted view (We rearrange the corresponding relationships among these multi-view data in random order, and corrupt them by white Gaussian noise.).

Some key parameters of all the methods in our experi-ments are tuned using the 5-fold cross-validation based on the AUC (area under the receiver operating characteristic curve) on the training set. Particularly, the LIBSVM clas-sifier serves as the benchmark for the tasks of classification in the experiments.
Since the proposed HLML model and other classical fea-ture homogeneous methods such as CCA [ 7], PLS [ 16 ], KC-C A [ 14 ], and DCCA [ 15 ] are based on subspace learning, w e compare their classification performance to show the im-portance of mining the distributional similarity among d-ifferent views. Here, the dimensionality k of the feature-homogeneous space is specified by min ( d x , d y ) for CCA and PLS. For KCCA and DCCA, we tune the dimensionality k on the candidate set { i  X  200 | i = 1 , 2 , 3 , , 10 } , and Gaussian kernel is used in KCCA. The dimensionality k of the feature-homogeneous subspace is set to max ( d x , d y HLML, and the trade-off parameters  X  and  X  are tuned on the sets { 10 i | i =  X  2 ,  X  1 , 0 , 1 , 2 } ..

Due to their inherent limitations, PLS and CCA can on-ly project the multi-view data into a low-dimensional space according to Eq.( 11 ) without the full consideration of dis-t ributional similarity among different views. Therefore, the feature-homogeneous spaces learned by PLS and CCA may contain much more noise, which groups the instances from different categories together while keeping the samples of the same class away from each other simultaneously. Ad-ditionally, KCCA and DCCA offer an alternative solution by nonlinearly mapping the multi-view data into a feature-homogeneous space. However, it is very difficult for KCCA and DCCA to capture much distributional information with-out leave-one-out validation and low-rank regularization. Table 2: Classification Performance of Feature Ho-mogeneous Methods in terms of AUC
The proposed HLML model linearly maps multiple het-e rogeneous low-level feature spaces to a high-dimensional feature-homogeneous one using pseudo-metric constraints. As shown in Table 2, the superiority of HLML over CCA, P LS, KCCA, and DCCA in the classification performance is quite clear. For example, nearly 20 percent gain is achieved for the COREL 5K dataset. It means that HLML can learn the distributional similarity among different views more ef-fectively than CCA, PLS, KCCA, and DCCA. Figure 7: Comparisons of Classification Perfor-m ance of Metric Learning Methods.
To validate the heterogeneous metrics learned by the pro-posed HLML method, we analyze HLML on the task of clas-sification with other four representative and state-of-the-art metric learning methods such as LMNN [ 8], ITML [ 28 ], N-C A [ 9], and LDA [ 30 ]. This experiment is conducted in the l arger COREL 5K dataset. We randomly sample data in the ratio { 20% , 40% , 60% , 80% , 100% } from the training set as the training instances and fix the testing set. For LMNN, ITML, NCA, and LDA, the experiment settings follow the original works [ 8, 28 , 9, 30 ], respectively. ITML uses iden-t ity matrix as initial metric matrix. Moreover, we used the codes provided by the authors for LMNN, ITML, and NCA.
Similar to LMNN and ITML, the proposed HLML mod-el is also a metric method based on Mahalanobis distance. But the major difference of HLML with the other model-s lies in that it fully takes into account the distributional similarity among different views. In addition, though NCA also use leave-one-out validation to exploit the characteris-tics of sample distribution, the correlation among hetero-geneous representations in multi-view data is not utilized fully. Moreover, since LDA is originally developed for han-dling mono-view problem, it can only learn some limited distributional information among different views.
We can see from Fig. 7 that HLML is superior to other m etric learning methods in classification performance. This observation further confirms that HLML can effectively cap-ture both semantic complementarity and distributional simi-larity among different views. Furthermore, with the increas-ing of training sample, the performance of HLML will im-prove. Thus, HLML also has some limitations that it need a certain number of existing samples to learn a set of excellent metrics.
In essence, like SSDR-MML [ 32 ], SCFL [ 33 ], and MV3MR [34 ], the proposed MSCD model is also a multi-view semi-s upervised learning method using both labeled and unla-beled data simultaneously. But the explicit difference of MSCD from the former models lies in that it fully takes in-to account the semantic complementarity among different views. So the latter will be more favorable to reduce intre-view noise for reestablishing the complementary relationship among heterogeneous representations.

To validate this point, we first use HLML to project the multi-view data into a feature-homogeneous space and then apply SSDR-MML, SCFL, MV3MR, and MSCD to denoise corrupted view. The performances of the classifiers learned by MSCD and other methods are compared in three multi-view dataset. For MSCD, the elementary row transforma-tion matrices T and H are set to identity matrices. We tune the regularization parameters  X  and  X  on the set { 10 i | =  X  2 ,  X  1 , 0 , 1 , 2 } . The parameter  X  in GEC criterion (see E-q.( 15 )) is specified by 0.1. The decision matrix W  X  R k  X  m i s randomly initialized. The regularization parameter  X  in SSDR-MML is set to 1, and following the original work [ 32 ], t he maximization of learning success measure is adopted to determine the importance of each label. For SCFL [ 33 ], S V M Light [37 ] serves as the underlying classifier where the l inear kernel is applied for View V x and the RBF kernel for View V y . The parameter setting in MV3MR is the same as in its original reference [ 34 ]. Additionally, we also compare t he classification performances of the methods in each itera-tion round in COREL 5K dataset to verify the convergence of the proposed MSCD algorithm.
 Table 3: Classification Performance of SSDR-MML, SCFL, MV3MR and MSCD in terms of AUC As shown in Table 3, the superiority of MSCD over SSDR-M ML, SCFL, and MV3MR in the classification performance is quite clear. This result shows that, in contrast to the compared approaches, MSCD is effective on reducing intre-view noise. Moreover, it can be observed from Fig. 8 that M SCD shows an obvious advantage over the other methods in every iteration and converges as well. This observation indicates that the MSCD is superior to other multi-view semi-supervised learning methods in rebuilding the semantic complementarity among different views.
In this paper, we have investigated the corrupted views problem in multi-view learning. We developed a gener-al framework to denoise corrupted views to obtain CR for multi-view data. Within this framework, multiple hetero-geneous linear metrics are learned by the proposed HLML model with pseudo-metric constraints, leave-one-out valida-tion, and low-rank regularization to unfold the shared infor-
Figure 8: Comparison in each iteration round. mation from different views. Meanwhile, we also proposed a M SCD method with elementary transformation constraints and GEC criterion to remove the intra-and inter-view noises by using the semantic complementarity and distributional similarity among different views.
 This work was supported in part by the National Natural Science Foundation of China (No.61271275 and No.61202067), the National High Technology Research and Development Program of China (No.2013AA013204), and National Sci-ence Foundation (DBI-1147134 and DBI-1350258). [1] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, [2] A. Sharma, A. Kumar, H. Daume III, and D. W.
 [3] M. Xiao and Y. Guo. Semi-supervised matrix [4] D. Tao, X. Li, X. Wu, and S. J. Maybank. Geometric [5] D. Tao, X. Li, X. Wu, and S. J. Maybank. General [6] M. Liu, D. Zhang, and D. Shen. Ensemble sparse [7] L. Sun, S. Ji, and J. Ye. Canonical correlation analysis [8] K. Q. Weinberger and L. K. Saul. Distance metric [9] J. Goldberger, G. E. Hinton, S. T. Roweis, and [10] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor [11] E. J. Cand`es and B. Recht. Exact matrix completion [12] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed [13] E. J. Cand`es and T. Tao. The power of convex [14] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor. [15] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep [16] H. Wold. Partial least squares. Encyclopedia of [17] J. Yang, J. Yang, D. Zhang, and J. Lu. Feature fusion: [18] Q. Sun, S. Zeng, Y. Liu, P. Heng, and D. Xia. A new [19] C. D. Meyer. Matrix Analysis and Applied Linear [20] M. Rubinstein, A. Shamir, and S. Avidan. Improved [21] I. Muslea, S. Minton, and C. A. Knoblock. Active + [22] A. Blum and T. Mitchell. Combining labeled and [23] Y. Nesterov. Introductory Lectures on Convex [24] S. Ji and J. Ye. An accelerated gradient method for [25] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [26] Y. Nesterov. Smooth minimization of non-smooth [27] Z. Wen and W. Yin. A feasible method for [28] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. [29] M. Guillaumin, J. Verbeek, and C. Schmid. Is that [30] R. A. Fisher. The use of multiple measures in [31] T. M. Cover and P. E. Hart. Nearest neighbor pattern [32] B. Qian, X. Wang, J. Ye, and I. Davidson. A [33] R. Yan and M. Naphade. Semi-supervised cross [34] Y. Luo, D. Tao, C. Xu, D. Li, and C. Xu.
 [35] R. P. Duin. UCI repository of machine learning [36] M. Guillaumin, J. Verbeek, and C. Schmid.
 [37] T. Joachims. Making large scale svm learning
