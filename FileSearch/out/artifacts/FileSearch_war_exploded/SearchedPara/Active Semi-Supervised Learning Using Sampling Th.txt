 We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas un-labeled data is easily available. The data points are repre-sented by the vertices of an undirected graph with the sim-ilarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function de-fined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for ac-tive learning based on sampling set selection which aims at maximizing the frequency of the signals that can be recon-structed from their samples on the set. Experiments show the effectiveness of our method.
 I.5.2 [ Pattern Recognition ]: Design Methodology X  Clas-sifier design and evaluation ; I.5.3 [ Pattern Recognition ]: Clustering X  Algorithms Active semi-supervised learning; Graph signal processing; Sampling theory; Graph signal filtering
In many real-life machine learning tasks, labeled data is scarce whereas unlabeled data is easily available. Active semi-supervised learning is an effective approach for such scenarios. A semi-supervised learning technique must not only learn from the labeled data but also from the inherent clustering present in the unlabeled data [29]. Further, when the labeling is expensive, it is better to let the learner choose the data points to be labeled so that it can pick the most informative and representative labels. Thus, in an active learning scenario, the goal is to achieve the maximum gain in terms of learning ability for a given, and small, number of label queries. In this paper, we propose a novel approach to active semi-supervised learning based on recent advances in sampling theory for graph signals.

Active learning has been studied in different problem sce-narios such as online stream-based sampling, adaptive sam-pling etc. (see [23] for a review). We focus on the problem of pool-based batch-mode active semi-supervised learning, where there is a large static collection of unlabeled data from which a very small percentage of data points have to be se-lected in order to be labeled. Batch operation (i.e., selecting a set of data points to be labeled) is more realistic in scenar-ios such as crowdsourcing where it would not be practical to submit for labeling one data point at a time. Further, in this paper we focus on the problem of optimizing batches of any size without using any label information, which would be the case when selecting the first batch of data points to be labeled. We leave for future work the problem of incor-porating labeled data, which would allow labels obtained for the first batch to be used to optimize data point selection for the second batch, and so on.

Applying a graph perspective to semi-supervised learning is not new. In a graph-based formulation, the data points are represented by nodes of a graph and the edges capture the similarity between the nodes they connect. For example, the weight on an edge might be a function of the distance between the two points in the feature space chosen for the classification task. The membership function of a given class can be thought of as a  X  X raph signal X , which has a scalar value at each of the nodes (e.g., 1 or 0 depending on whether or not the data point belongs to the class). Since features have been chosen to be meaningful for the classification task, it is reasonable to expect that nodes that are close together in the feature space will be likely to have the same label. Conversely, nodes that are far away in the feature space are less likely to have the same label. Thus, we expect the membership function to be smooth on the graph, i.e., moving from a node to its neighbors in the graph is unlikely to lead to changes in the membership. Thus, the semi-supervised learning problem can be viewed as a problem of interpolating a smooth graph signal. This view has led to many effective techniques such as MinCut [4], Gaussian random fields and harmonic functions [30], local and global consistency [28], manifold regularization [3] and spectral graph kernels [25].
Active learning has also benefited from this graph based-view. Many active learning approaches use the graph to quantify the quality of sampling sets [7, 9, 10]. One method-ology is to try and pick a subset of nodes which captures the underlying low-dimensional manifold represented by the graph. Another is to pick the nodes to be labeled in such a way that unlabeled nodes are strongly connected to them. Some methods pick those samples which lead to minimiza-tion of generalization error bound. We discuss some of these methods in Section 4.

Many of the semi-supervised methods mentioned above are global , in the sense that they require inversion or eigen-decomposition of large matrices associated with the underly-ing graph. This poses a problem in scalable and distributed implementation of these algorithms. Most graph-based ac-tive learning methods suffer from the same problem. An-other issue with these methods is that they do not give con-ditions under which the graph signal can be uniquely and perfectly interpolated from its samples on the chosen subset.
In recent years, there has been a significant amount of work devoted to the study of graph signal processing. The focus of this work has been to extend to the context of graph signals, theoretical results and tools that are well established in the context of conventional signal processing [24]. In par-ticular, there have been contributions to the design of graph wavelets [11], graph filterbanks [17], etc. A key challenge in graph signal processing is to design localized algorithms that scale well with graph sizes, i.e., the output at each vertex should only depend on its local neighborhood.

In this paper we leverage our recent work on graph signal sampling and interpolation [18, 1]. We show that the newly developed theoretical results provide a rigorous and unified framework to select points to be labeled and subsequently perform semi-supervised learning. Our framework provides conditions under which a graph signal can be uniquely re-covered from its values on a subset of vertices. These con-ditions lead to a powerful greedy algorithm for choosing the best nodes for labeling. The proposed algorithm is well mo-tivated through a compelling graph theoretic interpretation. We give a numerically efficient way to implement the pro-posed algorithm which makes it scalable. We also give an effective and efficient semi-supervised learning method that is closely tied to the label selection algorithm and is theo-retically well-justified. Both our algorithms are well-suited for a large-scale distributed implementation. We show that our method outperforms several state of the art methods by testing on multiple real datasets.

The rest of the paper is organized as follows. Section 2 reviews our recent work on sampling theory for graph sig-nals. In Section 3 we apply the framework of sampling the-ory to derive the proposed active semi-supervised learning approach. Section 4 summarizes the related prior work. Ex-periments are presented in Section 5. Finally, we provide some concluding remarks in Section 6.
We begin by briefly describing the theory of sampling for graph signals formulated in our previous work [18, 1].
Throughout this paper, we consider simple, connected, undirected, and weighted graphs G = ( V ,E ) with nodes numbered from the set V = { 1 , 2 ,...,N } , and edges E = { ( i,j,w ij ) } ,i,j  X  V , where ( i,j,w ij ) denotes an edge of weight w ij between nodes i and j , with w ii = 0. In the present context, the weights denote similarity between the respective nodes. The degree d i of a node i is defined as the sum of the weights of edges connected to node i , and the degree matrix of the graph is a diagonal matrix defined as D = diag { d 1 ,d 2 ,...,d N } . The adjacency matrix W of the graph is an N  X  N matrix with W ij = w ij and the com-binatorial Laplacian matrix is defined as L = D  X  W . We shall use the symmetric normalized form of the adjacency and the Laplacian matrices defined as W = D  X  1 / 2 WD  X  1 / 2 and L = D  X  1 / 2 LD  X  1 / 2 respectively. L is a symmetric pos-itive semi-definite matrix and has a set of real eigenvalues 0 =  X  1  X   X  2  X   X  X  X   X   X  N  X  2 and a corresponding orthog-onal set of eigenvectors denoted as U = { u 1 , u 2 ,..., u A subset of nodes of the graph is denoted as a collection of indices S  X  X  , with S c = V\S denoting its complement set. A restriction of a matrix A to rows in set S 1 and columns in set S 2 is denoted by the submatrix A S 1 , S 2 and for the sake of brevity A S , S = A S . Also, 0 and 1 denote all-zeroes and all-ones vectors of appropriate sizes.

A graph signal is defined as a scalar-valued discrete map-ping f : V  X  R , such that f ( i ) is the value of the signal on node i . For ease of notation, it can also be represented as a vector f  X  R N with indices corresponding to the node indices in the graph. In this paper, the signals of interest will be the membership functions associated with the various labels of interest in the classification problem. Sampling a graph signal f onto a subset of nodes S , known as the sampling set , is realized by retaining the signal X  X  values on the nodes in S . The sampled signal is denoted by f ( S ), which is a vector of reduced length |S| . In our context, a sampled graph signal will include the membership information for the data points that have been labeled.
The classical Nyquist-Shannon sampling theorem estab-lishes an upper limit on the bandwidth of signals that can be uniquely reconstructed when sampled at a given sam-pling rate. To have an analogous result in the realm of graphs, one needs a notion of frequency for graph signals. Such a spectral interpretation is provided by the eigenval-ues and eigenvectors of the Laplacian matrix L , similar to the Fourier transform in traditional signal processing. The eigenvalues can be thought of as frequencies and indicate the variation in the eigenvectors: a high eigenvalue implies higher variation in the corresponding eigenvector [24]. Since the eigenvectors are orthogonal, they form a basis in Thus, the Graph Fourier Transform (GFT) of a signal f is defined as its projection onto the eigenvectors of the graph Laplacian, i.e.  X  f (  X  i ) =  X  f , u i  X  , or more compactly,
In this context, a smooth or low-pass graph signal can be obtained by forcing high frequency GFT coefficients to vanish. More formally, an  X  -bandlimited signal on a graph is defined to have zero GFT coefficients for frequencies above its bandwidth  X  , i.e. its spectral support is restricted to the set of frequencies [0 , X  ]. The space of all  X  -bandlimited signals is known as the Paley-Wiener space and is denoted by PW  X  ( G ) [20]. Note that PW  X  ( G ) is a subspace of
With the notion of frequency introduced via the GFT, one can frame an adequate sampling theory for graph signals using the following ingredients:
P1: Cutoff frequency -For a given subset of nodes S , find
P2: Optimal sampling set -For a given cut-off frequency
P3: Reconstruction algorithm -Given samples f ( S ) of a Note that for regular sampling in the traditional signal pro-cessing, problems P1 and P2 are reciprocal, i.e., knowing one automatically leads to the solution of the other. How-ever, this does not hold for irregular sampling, as in the case of graph signals. Next, we briefly describe the solution to each of the problems above, and refer to [18, 1] for the details.
Let L 2 ( S c ) denote the space of all graph signals that are zero everywhere except possibly on the nodes in S c  X   X   X  L 2 ( S c ) , X  ( S ) = 0. Also, let  X  (  X  ) denote the bandwidth of a graph signal  X  , i.e., the value of the maximum non-zero frequency of that signal. Then the following theorem can be proved [1]:
Theorem 1 (Sampling Theorem). For a graph G , with normalized Laplacian L , any signal f  X  PW  X  ( G ) can be per-fectly recovered from its values on a subset of nodes S  X  V if and only if where  X  c ( S ) is the cut-off frequency.
 The theorem leads to a cut-off frequency that is lower than the minimum bandwidth of any signal in L 2 ( S c ). Intuitively, a signal  X   X  L 2 ( S c ) can be added to any input signal f without affecting its sampled version (since  X  is identically zero for all vertices that are sampled, i.e., those in S ). Thus, if there existed a  X   X  L 2 ( S c ) such that  X   X  PW  X  would have that both f and  X  + f belong to PW  X  ( G ) and lead to the same set of samples on S . So clearly it would not be possible to recover them both, and thus sampling of such signals in PW  X  ( G ) would not be possible. The condition in Theorem 1 ensures that PW  X  ( G )  X  L 2 ( S c ) = { 0 } and thus no such  X  exists.

From Theorem 1, finding the maximum cut-off frequency for a set S requires finding the bandwidth  X  (  X   X  ) of the smoothest possible signal  X   X   X  L 2 ( S c ). A brute-force ap-proach to this would entail computing the GFT of all sig-nals in L 2 ( S ) and exhaustively searching for  X   X  . We instead devise a computationally efficient way to approximate the bandwidth of any signal  X  for a given integer parameter k &gt; 0 as follows: We then replace  X  (  X  ) in Theorem 1 by  X  k (  X  ) in the objective function to obtain our estimated bandwidth: Then, the smoothest possible signal  X   X  in L 2 ( S c ) can be approximated by the minimizer  X   X  k in (3). Numerically,  X  ( S ) and  X   X  k can be determined from the smallest eigen-pair (  X  1 ,k , X  1 ,k ) of the reduced matrix ( L k ) S c This approach does not require complete eigen-decomposition of L and is computationally tractable. One can show that k controls the accuracy of the cut-off estimate (refer to [1] for details). As we increase the value of k ,  X  k ( S ) tends to give a better estimate of the cut-off frequency. Thus, there is a trade-off between accuracy of the estimate on the one hand, and complexity and numerical stability on the other that arise due to the power k in L k . Moreover,  X  k ( S ) can be proven to be always less than the actual cut-off  X  c ( S ), i.e. the Sampling Theorem still holds for the subset S ex-cept that the class of recoverable signals is determined to be narrower as a penalty for the cut-off approximation.
We now describe the framework for the converse ques-tion: given a cut-off frequency  X  c for PW  X  ( G ), what is the smallest sampling set S opt so that a signal f  X  PW  X  ( G ) is uniquely represented by f ( S opt ). If K c represents the num-ber of eigenvalues of L below  X  c , then by dimensionality considerations |S opt |  X  K c . Also, note that S opt be unique. Formally, one can use Theorem 1 and relax the true cut-off  X  c ( S ) by  X  k ( S ), then S opt can be found from the following optimization problem: This is a combinatorial problem because we need to compute  X  ( S ) for every possible subset S .

However, this problem can be solved using a greedy heuris-tic to get an estimate S est of the optimal sampling set. Starting with an empty sampling set S (with corresponding  X  ( S ) = 0) we keep adding nodes to S (from S c ) one-by-one while trying to ensure maximum increase in  X  k ( S ) at each step. The hope is that  X  k ( S ) reaches the target cut-off  X  with minimum number of node additions to S . To under-stand which nodes should be included in S , we introduce a binary relaxation of our cut-off formulation by defining the following matrix where D ( t ) is a diagonal matrix with t on its diagonal. Let (  X   X  k ( t ) , x  X  k ( t )) denote the smallest eigen-pair of M Then, if 1 S : V  X  { 0 , 1 } denotes the indicator function for the subset S (i.e. 1 ( S ) = 1 and 1 ( S c ) = 0 ), one has Note that the right hand side of the equation above is simply an unconstrained regularization of the constrained optimiza-tion problem in (3). When  X  1, the components x ( S ) are highly penalized during minimization. Thus, if x  X  k the minimizer in (8), then [ x k  X  ( 1 S )]( S )  X  0 , i.e. the values on nodes S tend to be very small. Therefore, for  X  1, we have From the above equation, we observe that the problem of greedily maximizing  X  k ( S ) is equivalent to maximizing  X  and thus, we simply need to study the variation of  X  with t , a real-valued vector in R N , at t = 1 S . This relax-ation circumvents the combinatorial nature of our problem and has been used earlier to study graph partitioning based on Dirichlet eigenvalues [19]. The gradient of  X   X  respect to t ( i ) is given by This equation forms the basis of our greedy heuristic: start-ing with an empty S (i.e., 1 S = 0 ), if at each step, we include the node on which the smoothest signal  X   X  k  X  L 2 ( S maximum energy (i.e., 1 S ( i )  X  1 ,i = arg max j (  X   X  k then the cut-off estimate  X  k ( S ) tends to increase maximally.
While the algorithm in [1] has a goal of finding an S of smallest possible size that satisfies a target cut-off fre-quency, we can easily adapt it for our cut-off frequency maximization-based active learning algorithm. This will be discussed in detail in Section 3.1.
A graph signal f  X  PW  X  ( G ) can be written as a linear combination eigenvectors of L with eigenvalues less than  X  , i.e., f = U V , K  X  where K is the index set of those eigenvec-tors and  X  is a vector containing the corresponding GFT coefficients. When the unique recovery conditions of The-orem 1 are satisfied,  X  and the signal f , can be recovered from its subsampled version f ( S ) by solving the following least squares problem: Note that if the original signal f is not bandlimited, i.e., f /  X  PW  X  ( G ), then the least squares solution corresponds to an approximation of f in PW  X  ( G ) (in l 2 sense).
The least squares solution requires eigen-decomposition of L which is computationally expensive and may not be practical for large graphs. We now describe the iterative, distributed algorithm developed in [18] based on projection onto convex sets (POCS). The proposed method is similar to the Papoulis-Gerchberg algorithm [22] in classical signal processing which is used to reconstruct a bandlimited signal from irregular samples. The convex sets of interest in this case are where D S is the downsampling operator such that D S f = f ( S ). The unique solution f to the least squares problem satisfies the following two constraints: (1) the signal equals the known values on the sampling set (i.e., f  X  C 1 ), (2) the signal is  X  -bandlimited, where  X  is computed using (4) (i.e., f  X  C 2 ). The projector for C 2 is P  X  : R N  X  PW  X  ( G ) which is a low-pass graph filter such that P  X  can be written in graph spectral domain as P  X  = H ( L ) = P We define the projection operator for C 1 as P S : R N  X  C which replaces the samples on S by the known values.
With this notation the proposed iterative algorithm can be written as: At each iteration the algorithm resets the signal samples on S to the actual given samples and then projects the signal onto the low-pass space PW  X  ( G ). Figure 1 depicts this pro-cedure graphically. It can be shown that T = P  X  P S is a non-expansive and asymptotically regular operator. Hence, the iterations in (18) converge to the unique point f  X  C 1  X  C 2 which is the desired solution.

The low pass filter P  X  above is a spectral graph filter with an ideal brick-wall type spectral response. Thus, the exact computation of P  X  would require knowledge of the GFT, which we would like to avoid due to high computa-tional complexity for large graphs. However, it is possible to approximate the ideal filtering operation as a matrix polyno-mial in terms of L , that can be implemented efficiently using only matrix vector products. Thus we replace P  X  in (18) with an approximate low pass filter P poly  X  given by: We specifically use the truncated Chebychev polynomial ex-pansion of any spectral kernel h (  X  ), as proposed in [11], in our experiments. It is easy to show that an operator which is a p -degree polynomial in L is p -hop localized on the graph and can be implemented in a distributed fashion. In order to ensure that the Chebyshev polynomial approximation is good, we first approximate the ideal spectral kernel by a Figure 2: Spectral response of an approximate polynomial filter of degree 10.  X  = 1 , X  = 8. smooth, continuous sigmoid-like function (see Figure 2) Due to these approximations in the filter, the reconstructed signal obtained via POCS is different from the true band-limited signal. However, in semi-supervised learning appli-cations we do not expect the signals (i.e., class membership functions) to be exactly bandlimited anyway. So using a filter with slowly decaying spectral response ends up im-proving the classification accuracy slightly.
We now relate the sampling theory developed for graph signals to active semi-supervised learning and propose our solution to the problem. As noted earlier, if the edges of the graph represent similarity between the nodes, then a graph signal defined using the membership functions of a particular class tends to be smooth. This is illustrated ex-perimentally in Figure 3. In Section 2.3 we showed how to estimate the sampling cut-off frequency for a set of vertices. In practice, class membership signals are not strictly ban-dlimited (see Figure 3). Thus we will be approximating a non-bandlimited signal with one that is bandlimited to the cut-off frequency of the chosen vertex set. The key observa-tion in our work is that, even though we cannot recover the  X  X rue X  membership signal exactly from its samples, an active learning approach should aim at selecting the sampling set with maximum cut-off frequency . This is obviously true since PW  X  ( G )  X  PW  X  0 ( G ) if  X   X   X  0 and thus, for any signal, its best approximation with a signal from PW  X  0 ( G ) can be no worse (in terms of l 2 error) than its best approximation with a signal from PW  X  ( G ).

In this setting, predicting the labels of the unknown data-points using the labeled data amounts to reconstructing a bandlimited graph signal from its values on the sampling set. Thus, based on the above reasoning the active learning strategy, given a target number of datapoints to be labeled, should be to find a set S , with that size, so that the cut-off frequency of S is maximized.
Now, we present the details of our method. We target a multi-class active semi-supervised learning problem with C classes. The true membership function for class j is de-noted as f j : V 7 X  { 0 , 1 } , where f j ( i ) = 1 indicates that node i belongs to class j . These membership functions are taken to be the graph signals for our setting. The predicted membership functions for each class take real values and are Algorithm 1 Greedy heuristic for finding S  X  L Input: G = {V ,E } , L , target size m , parameter k  X  Z + Initialize: S = { X  X  . 1: while |S| X  m do 2: For S , compute the smoothest signal  X   X  k  X  L 2 ( S 3: v  X  arg max i (  X   X  k ( i )) 2 . 4: S  X  X  X  v . 5: end while 6: S  X  L  X  X  . denoted as  X  f j : V 7 X  R . The predicted label of node i is given by arg max j  X  f j ( i ). We denote the labeled set as S and the unlabeled set as S U = V \S L . Then, our solution to the active semi-supervised learning task can be formally summarized as follows: 1. Given a size m and parameter k , we first find the opti-2. Next, we query the labels of nodes in S  X  L . 3. Finally, we determine the predicted membership func-
In this section, we will provide an intuitive interpretation for our node selection algorithm in terms of connected-ness among the nodes. To simplify the exposition, we consider the maximization problem (21) for k = 1: This expression appears more commonly as part of discrete Dirichlet eigenvalue problems on graphs. Specifically, it is equal to the Dirichlet energy of the subset S c [6, 19]. The sampling set selection problem seeks to identify the subset S that maximizes this objective function. To give an intuitive interpretation of our goal, we expand the objective function for any x with constraint x ( S ) = 0 as follows: x t L x = X The minimizer in the equation above is the first Dirichlet eigenvector which is guaranteed to have strictly positive val-ues on S c [19]. Therefore, the contribution of the second region. term is expected to be negligible as compared to the first one due to differencing, and we get where, p j = P i  X  X  w ij is defined as the  X  X artial out-degree X  of node j  X  X  c , i.e., it is the sum of weights of edges crossing over to the set S . Therefore, given a current selected S , the greedy algorithm selects the next node, to be added to S , that maximizes the increase in Due to the constraint || x || = 1, the expression being mini-mized is essentially an infimum over a convex combination of the fractional out-degrees and its value is largely deter-mined by nodes j  X  S c for which p j /d j is small. In other words, we must worry about those nodes that have a low ratio of partial degree to the actual degree. Thus, in the simplest case, our selection algorithm tries to remove those nodes from the unlabeled set that are weakly connected to nodes in the labeled set. This makes intuitive sense as, in the end, most prediction algorithms involve propagation of labels from the labeled to the unlabeled nodes. If an unla-beled node is strongly connected to various numerous points, its label can be assigned with greater confidence.

Note that using a higher power k in the cost function, i.e., finding  X  k ( S ) for k &gt; 1 involves x L k x which, loosely speaking, takes into account higher order interactions be-tween the nodes while choosing the nodes to label. In a sense, we expect it to capture the connectivities in a more global sense, beyond local interactions, taking into account the underlying manifold structure of the data.
We now comment on the time and space complexity of our algorithm. The most complex step in the greedy proce-dure for maximizing  X  k ( S ) is computing the smallest eigen-pair of ( L k ) S c . This can be accomplished using an iterative Rayleigh-quotient minimization based algorithm. Specifi-cally, the locally-optimal pre-conditioned conjugate gradi-ent (LOPCG) method [14] is suitable for this approach. Note that ( L k ) S c can be written as I S c , V . L . L ... L . I hence the eigenvalue computation can be broken into atomic matrix-vector products: L . x . Typically, the graphs encoun-tered in learning applications are sparse, leading to efficient implementations of L . x . If | L | denotes the number of non-zero elements in L , then the complexity of the matrix-vector product is O ( | L | ). The complexity of each eigen-pair com-putation for ( L k ) S c is then O ( k | L | r ), where r is a constant equal to the average number of iterations required for the LOPCG algorithm ( r depends on the spectral properties of L and is independent of its size |V| ). The complexity of the label selection algorithm then becomes O ( k | L | mr ), where m is the number of labels requested.

In the iterative reconstruction algorithm, since we use polynomial graph filters (Section 2.5), once again the atomic step is the matrix-vector product L . x . The complexity of this algorithm can be given as O ( | L | pq ), where p is the order of the polynomial used to design the filter and q is the av-erage number of iterations required for convergence. Again, both these parameters are independent of |V| . Thus, the overall complexity of our algorithm is O ( | L | ( kmr + pq )). In addition, our algorithm has major advantages in terms of space complexity: Since, the atomic operation at each step is the matrix-vector product L . x , we only need to store L and a constant number of vectors. Moreover, the structure of the Laplacian matrix allows one to perform the afore-mentioned operations in a distributed fashion. This makes it well-suited for large-scale implementations using software packages such as GraphLab [16].
As discussed in Section 2.5, given the samples f S of the true graph signal on a subset of nodes S  X  V , its estimate on S c is obtained by solving the following problem:  X  f ( S c ) = U S c , K  X   X  where,  X   X  = arg min Here, K is the index set of eigenvectors with eigenvalues less than the cut-off  X  c ( S ). If the true signal f  X  PW  X  c then the prediction is perfect. However, this is not the case in most problems. The prediction error k f  X   X  f k roughly equals the portion of energy of the true signal in [  X  c ( S ) , X  frequency band. By choosing the sampling set S that max-imizes  X  c ( S ), we try to capture most of the signal energy and thus, reduce the prediction error.

An important question in the context of active learning is determining the minimum number of labels required so that the prediction error k f  X   X  f k is less that some given tolerance  X  . To find this we first characterize the smoothness  X  ( f ) of a signal f as The following theorem gives a lower bound on the minimum of number of labels required in terms of  X  ( f ).

Theorem 2. If  X  f is obtained by solving (26) , then the minimum number of labels l required to satisfy k f  X  is greater than p , where p is the number of eigenvalues of L less than  X  ( f ) .

Proof. In order for (26) to have a unique solution, U S , K needs to have full column rank, which implies that l = |S| X  |K| . Now, for k f  X   X  f k X   X  to hold the bandwidth of  X  be at least  X  ( f ), or in other words, |K|  X  p . This gives us the desired result as l  X |K| X  p .
Different frameworks have been proposed for pool-based batch-mode active semi-supervised learning including opti-mal experiment design [27, 26], generalization error bound minimization [7, 8] and submodular optimization [9, 10, 12]. We now point out connections between some of the graph based approaches in the above categories and our graph sig-nal sampling theory based framework.

The notion of frequency given by GFT is closely related to Laplacian eigenmaps which is a well known dimensional-ity reduction technique [2]. GFT can be viewed as a way of measuring the signal variation on the manifold repre-sented by Laplacian eigenmaps. By selecting nodes that maximize the bandwidth of the space of recoverable sig-nals, we are trying to capture as many dimensions of the manifold structure of the data with as few samples as pos-sible. A related active learning method proposed by Zhang et al. [27] uses optimal experiment design while consider-ing local structure of the data in a way which is similar to local linear embedding (LLE) for approximating the under-lying low-dimensional manifold [21]. This approach tries to choose the most representative data points from which one can recover the whole data set by local linear reconstruction. It is interesting to note that under certain conditions LLE and Laplacian eigenmaps are equivalent [15].

Gu and Han [7] propose a method based on minimizing the generalization error bound for learning with local and global consistency (LLGC) [28]. Their formulation boils down to choosing subset S that minimizes Tr (  X  L S + I )  X  2 . To re-late this formulation to our proposed method, note that where,  X  1  X  ...  X   X  | S | denote the eigenvalues of L S . Loosely speaking, minimizing the above objective function is equiv-alent to maximizing the smallest eigenvalue  X  1 of L S . So, this method essentially tries to ensure that the labeled set is well-connected to the unlabeled set whereas our method en-sures that the unlabeled set is well-connected to the labeled set (cf. Section 3.2).

Submodular functions have been used for active semi-supervised learning on graphs by Guillory and Bilmes [10, 9]. In this work, the subset of nodes S  X  V is chosen to maximize where  X ( T ) denotes the cut function P i  X  T,j/  X  T itively, maximizing  X ( S ) ensures that no subset of unla-beled nodes is weakly connected to the labeled set S . This agrees with the graph theoretic interpretation of our method given in Section 3.2. They also provide a bound on the prediction error in terms  X ( S ) and a smoothness function  X ( f ) = P i,j w ij | f i  X  f j | . This bound gives a theoretical justification for semi-supervised learning using min-cuts [4]. It also motivates a graph partitioning-based active learning heuristic [9] which says that to select l nodes to label, the graph should be partitioned into l clusters and one node should be picked at random from each cluster.
We compare our method against three active semi-supervised learning approaches mentioned in the previous section, namely, LLR [27], LLGC error bound minimization [7], METIS graph partitioning based heuristic [9] and  X -max [10]. The details of implementation of each method are as follows: 1. The LLR approach [27] allows any prediction method 2. In our implementation of the LLGC bound method [7], 3. The normalized cut based active learning heuristic of In the implementation of our proposed method, we use ap-proximate polynomial filters of degree 10 with  X  = 8. The parameter k in our method is fixed as 8 for these exper-iments. Its effect on classification accuracy is studied in Section 5.4. In addition to the above methods, we also com-pare with the random sampling strategy. We use LapRLS to predict the unknown labels from the randomly queried samples and report the average error rates over 30 trials.
To intuitively demonstrate the effectiveness of our method, we first test it on a two circles toy data as shown in Fig-ure 4. The data is comprised of 200 nodes from which we would like to select 8 nodes to query. We construct a weighted sparse graph by connecting each node to its 10 nearest neighbors while ensuring that the connections are symmetric. The edge weights are computed with the Gaus-where the graph is unweighted). It can be seen from Figure 4 that all the methods choose 4 points from each of the two circles. Additionally, the proposed approach selects evenly
In our experiments, we observed that the greedy algorithm given in [7] did not converge to a good solution. So we use Monte-Carlo simulations to minimize the objective function. spaced data points within one circle, while at the same time maximizing the spacing between the selected data points in different circles. This is in accordance with the requirement of choosing points which are most representative of the data.
We tested our method in three application scenarios: Hand-written digit recognition, text classification and spoken let-ters recognition. In these experiments, we do not compare with  X -max since the method has computational complex-ity of O ( N 6 ) and, to the best of our knowledge, is not scal-able. Next, we provide the details of each experiment. Both the datasets and the graph construction procedures used are typical of what has been used in the literature.
In this experiment, we used our proposed active semi-supervised learning algorithm to perform a classification task on the USPS handwritten digits dataset 2 . This dataset con-sists of 1100 16  X  16 pixel images for each of the digits 0 to 9. We used 100 randomly selected samples for each digit class to create one instance of our dataset. Thus each in-stance consists of 1000 feature vectors (100 samples/class  X  10 digit classes) of dimension 256.

The graph is constructed using Gaussian kernel weights w ij = exp  X  feature vector composed of pixel intensity values for each image. The parameter  X  is chosen to be 1 / 3-rd of the aver-age distance to the K -th nearest neighbor for all datapoints. This heuristic has been suggested in [5]. We fix K = 10. Additionally, the graph is sparsified approximately by re-stricting the connectivity of each datapoint to its K nearest neighbors, i.e., an edge between nodes i and j is removed unless node i is among the K -nearest neighbors of node j or vice-versa. This results in a symmetric adjacency ma-trix for the graph. Using the graph constructed, we select the points to label and report prediction error after recon-struction using our semi-supervised learning algorithm. We repeat the classification over 10 such instances of the dataset and report the average classification error. The results are illustrated in Figure (5a). We observe that our proposed method outperforms the others. A notable feature of our method is that we show very good classification results even for very few labeled samples. This is due to our inherent criterion for active learning that tries to select those points that maximize the recoverable dimensions of the underlying data manifold. http://www.cs.nyu.edu/~roweis/data.html
For our text classification example, we use the 20 news-groups dataset 3 . It contains around 20,000 documents, par-titioned in 20 different newsgroups. For our experiment, we consider 10 groups of documents, namely, { comp.graphics, comp.os.ms-windows.misc, comp.sys.ibm.pc.hardware, comp. sys.mac.hardware, rec.autos, rec.motorcycles, sci.crypt, sci. electronics, sci.med, sci.space } , and randomly choose 100 datapoints from each group. We generate 10 such instances of 1000 data points each and report the average errors. We clean the dataset by removing the words that appear in fewer than 20 documents and then select only the 3000 most fre-quent ones from the remaining words. To form the feature vectors representing the documents, we use the tf-idf statis-tic of these words. The tf-idf statistic captures the relative importance of a word in a document in a corpus: where, tf is the frequency of a word in a document, idf is the number of documents in which the word appears and N is the total number of documents. Thus, we get 1000 feature vectors in 3000 dimensional space. To form the graph of doc-uments, we compute the pairwise cosine similarity between their feature vectors. Each node is connected to the 10 nodes that are most similar to it and the resultant graph is then symmetrized. The classification results in Figure (5b) show that our method performs very well compared to others. However, the absolute error rates are not very good. This is due to the high similarity between different newsgroups which makes the problem inherently difficult.
For the spoken letters classification example, we consid-ered the Isolet dataset 4 . It consists of letters of the English alphabet spoken in isolation twice by 150 different subjects. The speakers are grouped into 5 sets of 30 speakers each, with the groups referred to as isolet1 through isolet5. Each alphabet utterance has been pre-processed beforehand to create a 617-dimensional feature vector.

For this experiment, we considered the task of active semi-supervised classification of utterances into the 26 alphabet categories. To form an instance of the dataset, 60 utter-ances are randomly selected out of 300 for each alphabet. Thus, each instance consists of 60  X  26 = 1560 datapoints http://qwone.com/~jason/20Newsgroups/ http://archive.ics.uci.edu/ml/datasets/ISOLET accuracies for different percentages of labeled data. of dimension 617. As in the hand-written digits classifica-tion problem, the graph is constructed using Gaussian ker-nel weights between nodes, with  X  taken as 1 / 3-rd of the average distance to the K -th nearest neighbor for each dat-apoint. We select K = 10 for our experiment. Sparsification of the graph is carried out approximately using K -nearest neighbor criterion. With the constructed graph, we perform active semi-supervised learning using all the methods. The experiment is repeated over 10 instances of the dataset and average prediction error is reported in Figure (5c). Note that we start with 2% labeled points to ensure that each method gets a fair chance of selecting at least one point to label from each of the 26 classes. We observe that our method outperforms the others.
To study the effect of parameter k in the proposed method on classification accuracy we repeat the above experiments for different values of k . Figure 6 shows the results. For the USPS and Isolet datasets, the classification accuracies remain largely unchanged for different values of k . For the 20 Newsgroups dataset, a slight improvement in classifica-tion accuracies is observed for higher values of k . This result agrees with the distribution of GFT coefficients of the class membership functions in each dataset shown in Figure 3. In USPS and Isolet datasets, most of the energy of the graph signal (i.e., the class membership functions) is contained in the first few frequencies. Thus, increasing the value of k , so that a better estimate of cut-off frequency is maximized during the choice of sampling set, is not necessary. In other words, maximizing a loose estimate of the cut-off frequency is sufficient. However, the membership functions in the 20 Newsgroups dataset have a significant fraction of their en-ergy spread over high frequencies as shown in Figure 3. Due to this, maximizing a tighter estimate of the the cut-off al-lows the sampling set selection algorithm to pick nodes that capture more signal energy, resulting in higher accuracies.
In this paper, we introduce a novel framework for batch mode active semi-supervised learning based on sampling the-ory for graph signals. The proposed active learning frame-work aims to select the subset nodes which maximizes the di-mension of the space of uniquely recoverable signals. In the context of sampling theory, this translates to selecting the subset with the maximum cut-off frequency. This interpreta-tion leads to a very efficient greedy algorithm. We provide intuition about how the method tries to choose the nodes which are most representative of the data. We also present an efficient semi-supervised learning method based on ban-dlimited interpolation. We show, through experiments on real data, that our two algorithms, in conjunction, perform very well compared to state of the art methods.

In the future, we would like to provide bounds on the pre-diction error of the proposed method (when the true signal is not exactly bandlimited) in terms of signal smoothness and the cut-off frequency. We also hope to have tighter bounds on the number of labels required for desired prediction ac-curacy. It would be useful to consider an extension of the proposed framework to a partially batch setting so that we can incorporate the label information from previous batches to improve the choice of sampling sets. [1] A. Anis, A. Gadde, and A. Ortega. Towards a [2] M. Belkin and P. Niyogi. Semi-supervised learning on [3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [4] A. Blum and S. Chawla. Learning from labeled and [5] O. Chapelle, B. Sch  X  olkopf, A. Zien, et al. [6] F. R. K. Chung. Spectral graph theory , volume 92. [7] Q. Gu and J. Han. Towards active learning on graphs: different percentages of labelled data. [8] Q. Gu, T. Zhang, C. Ding, and J. Han. Selective [9] A. Guillory and J. Bilmes. Label selection on graphs. [10] A. Guillory and J. Bilmes. Active semi-supervised [11] D. Hammond, P. Vandergheynst, and R. Gribonval. [12] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode [13] G. Karypis and V. Kumar. A fast and high quality [14] A. V. Knyazev. Toward the optimal preconditioned [15] D. Kong, C. H. Ding, H. Huang, and F. Nie. An [16] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, [17] S. Narang and A. Ortega. Perfect reconstruction [18] S. K. Narang, A. Gadde, E. Sanou, and A. Ortega. [19] B. Osting, C. D. White, and E. Oudet. Minimal [20] I. Pesenson. Sampling in Paley-Wiener spaces on [21] S. T. Roweis and L. K. Saul. Nonlinear dimensionality [22] K. Sauer and J. Allebach. Iterative reconstruction of [23] B. Settles. Active learning literature survey. Computer [24] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, [25] A. J. Smola and R. Kondor. Kernels and [26] K. Yu, J. Bi, and V. Tresp. Active learning via [27] L. Zhang, C. Chen, J. Bu, D. Cai, X. He, and T. S. [28] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [29] X. Zhu. Semi-supervised learning literature survey. [30] X. Zhu, Z. Ghahramani, and J. Lafferty.

