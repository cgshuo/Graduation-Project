 There exists a vast amount of kno wledge sources and ontologies in the medical domain. Such in-formation is also gro wing and changing extremely quickly , making the information dif cult for peo-ple to read, process and remember . The combi-nation of recent developments in information ex-traction and the availability of unparalleled medi-cal resources thus offers us the unique opportunity to develop new techniques to help healthcare pro-fessionals overcome the cogniti ve challenges the y face in clinical decision making.

Relation extraction plays a key role in informa-tion extraction. Using question answering as an example (W ang et al., 2012): in question analy-sis, the semantic relations between the question focus and each term in the clue can be used to identify the weight of each term so that better search queries can be generated. In candidate an-swer generation, relations enable the background kno wledge base to be used for potential candidate answer generation. In candidate answer scoring, relation-based matching algorithms can go beyond explicit lexical and syntactic information to detect implicit semantic relations shared across the ques-tion and passages.

To construct a medical relation extraction sys-tem, several challenges have to be addressed:  X  The rst challenge is how to identify a set of  X  The second challenge is how to efciently de- X  The third challenge is that the labeled rela-
The contrib utions of this paper on medical rela-tion extraction are three-fold:  X  The problem setup is new. There is a  X  From the perspecti ve of relation extraction  X  From the perspecti ve of relation extraction
The experimental results sho w that our relation detectors are fast and outperform the state-of-the-art approaches on medical relation extraction by a lar ge mar gin. We also apply our model to build a new medical relation kno wledge base as a comple-ment to the existing kno wledge bases. 2.1 Medical Ontologies and Sour ces Medical domain has a huge amount of natural lan-guage content found in textbooks, enc yclopedias, guidelines, electronic medical records, and man y other sources. It is also gro wing at an extremely high speed. Substantial understanding of the med-ical domain has already been included in the Uni-ed Medical Language System (UMLS) (Lind-ber g et al., 1993), which includes medical con-cepts, relations, denitions, etc. The 2012 version of the UMLS contains information about more than 2.7 million concepts from over 160 source vocab ularies. Softw ares for using this kno wledge also exist: MetaMap (Aronson, 2001) is able to identify concepts in text. SEMREP (Rindesch and Fiszman, 2003) can detect some relations us-ing hand-crafted rules. 2.2 Relation Extraction To extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) emplo y a number of linguistic rules to capture relation patterns. Feature-based methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a lar ge amount of linguistic features lik e lex-ical, syntactic and semantic features, and capture the similarity between these feature vectors. Re-cent results mainly rely on kernel-based meth-ods. Man y of them focus on using tree kernels to learn parse tree structure related features (Collins and Duf fy, 2001; Culotta and Sorensen, 2004; Bunescu and Moone y, 2005).

Other researchers study how dif ferent ap-proaches can be combined to impro ve the extrac-tion performance. For example, by combining tree kernels and con volution string kernels, (Zhang et al., 2006) achie ved the state of the art performance on ACE data (A CE, 2004). Recently ,  X distant su-pervision X  has emer ged to be a popular choice for training relation extractors without using manually labeled data (Mintz et al., 2009; Jiang, 2009; Chan and Roth, 2010; Wang et al., 2011; Riedel et al., 2010; Ji et al., 2011; Hof fmann et al., 2011; Sur -deanu et al., 2012; Takamatsu et al., 2012; Min et al., 2013).

Various relation extraction approaches have been adapted to the medical domain, most of which focus on designing heuristic rules tar geted for diagnosis and inte grating the medical ontology in the existing extraction approaches. Results of some of these approaches are reported on the i2b2 data (Uzuner et al., 2011). 3.1 Super Relations in Medical Domain The rst step in building a relation extraction sys-tem for medical domain is to identify the relations that are important for clinical decision making.
Four main clinical tasks that physicians eng age in are discussed in (Demner -Fushman and Lin, 2007). The y are Therapy -select treatments to of-fer a patient, taking consideration of effecti veness, risk, cost and other factors (pre vention is under the general cate gory of Therap y), Diagnosis (includ-ing dif ferential diagnosis based on ndings and di-agnostic test), Etiology -identify the factors that cause the disease and Pr ognosis -estimate the pa-tient' s lik ely course over time. These acti vities can be translated into  X search tasks X . For example, the search for therap y is usually the therap y selection given a disease.

We did an independent study regarding what clinical questions usually ask for on a set of 5,000 Doctor Dilemma (DD) questions from the Ameri-can Colle ge of Ph ysicians (A CP). This set includes questions about diseases, treatments, lab tests, and general facts 1 . Our analysis sho ws that about 15% of these questions ask for treatments, pre ventions or contraindicated drugs for a disease or another way around, 4% are about diagnosis tests, 6% are about the causes of a disease, 1% are about the lo-cations of a disease, 25% are about the symptoms of a disease, 8% are asking for denitions, 7% are about guidelines and the remaining 34% questions either express no relations or some relations that are not very popular .
 Based on the analysis in (Demner -Fushman and Lin, 2007) and our own results, we decided to fo-cus on seven key relations in the medical domain, which are described in Table 1. We call these re-lations  X super relations X , since the y cover most questions in the DD question set and align well with the analysis result in (Demner -Fushman and Lin, 2007). 3.2 Collect Training Data This section presents how we collect training data for each relation. The overall procedure is illus-trated in Figure 1.
Our medical corpus has incorporated a set of medical books/journals 2 and MEDLINE ab-stracts. We also complemented these sources with Wikipedia articles. In total, the corpus contains 80M sentences (11 gig abyte pure text).

The UMLS 2012 Release contains more than 600 relations and 50M relation instances under around 15 cate gories. The RO cate gory (RO stands for  X has Relationship Other than synon y-mous, narro wer , or broader X ) is the most inter -esting one, and covers relations lik e  X may treat X ,  X has nding site X , etc. Each relation has a certain number of Concept Unique Identier (CUI) pairs that are kno wn to bear that rela-tion. In UMLS, some relation information is redundant. Firstly , half of these relations are simply inverse of each other (e.g. the relation  X may treat X  and  X may be treated by X ). Secondly , there is a signicant amount of redundanc y even among non-in verse relations (e.g. the relation  X has manifestation X  and  X disease has nding X ).
From UMLS relations, we manually chose a subset of them that are directly related to the su-per relations discussed in Section 3.1. The cor -respondences between them are given in Table 1. One thing to note is that super relations are more general than the UMLS relations, and one super relation might inte grate multiple UMLS relations. Using the CUI pairs in the UMLS relation kno wl-edge base, we associate each super relation with a set of CUI pairs.

To collect the training data for each super re-lation, we need to collect sentences that express the relation. To achie ve this, we parsed all 80M sentences in our medical corpus, looking for the sentences containing the terms that are associated with the CUI pairs in the kno wledge base. This (distant supervision) approach resulted in a huge amount of sentences that contain the desired rela-tions, but also brought in a lot of noise in the form of false positi ves. For example, we kno w from the kno wledge base that  X antibiotic drug X  may treat  X Lyme disease X . Ho we ver the sentence  X This paper studies the relationship between antibiotic drug and Lyme disease X  contains both terms but does not express the  X treats X  relation.

The most reliable way to clean the training data is to ask annotators to go through the sentences and assign the sentences with positi ve/ne gative la-bels. Ho we ver, it will not work well when we have millions of sentences to vet. To minimize the hu-man labeling effort, we ran a K-medoids clustering on the sentences associated with each super rela-tion and kept the cluster centers as the most rep-resentati ve sentences for annotation. Depending on the number of the sentences we collected for each relation, the #clusters was chosen from 3,000 -6,000. The similarity of two sentences is dened as the bag-of-w ords similarity of the dependenc y paths connecting arguments. Part of the resulting data was manually vetted by our annotators, and the remaining was held as unlabeled data for fur -ther experiments.

Our relation annotation task is quite straightfor -ward, since both arguments are given and the de-cision is a Yes-or -No decision. The noise rate of each relation (#sentences expressing the relation / #sentences) is reported in Table 1 based on the annotation results. The noise rates dif fer signi-cantly from one relation to another . For  X treats X  relation, only 16% of the sentences are false posi-tives. For  X contraindicates X  relation, the noise rate is 97%.

To gro w the size of the negative training set for each super relation, we also added a small amount of the most representati ve examples (also coming from K-medoids clustering) from each unrelated UMLS relation to the training set as negative ex-amples. This resulted in more than 10,000 extra negative examples for each relation. 3.3 Parsing and Typing The most well-kno wn tool to detect medical en-tity mentions is MetaMap (Aronson, 2001), which considers all terms as entities and automatically associates each term with a number of concepts from UMLS CUI dictionary (Lindber g et al., 1993) with 2.7 million distinct concepts.

The parser used in our system is Medi-calESG, an adaptation of ESG (English Slot Grammar) (McCord et al., 2012) to the medical domain with extensions of medical lexicons inte-grated in the UMLS 2012 Release. Compared to MetaMap, MedicalESG is based on the same med-ical lexicons, 10 times faster and produces very similar parsing results.
 We use the semantic types dened in UMLS (Lindber g et al., 1993) to cate gorize argument types. The UMLS consists of a set of 133 subject cate gories, or semantic types, that pro vide a consistent cate gorization of more than 2M concepts represented in the UMLS Metathesaurus. Our system assigns each relation argument with one or more UMLS semantic types through a two step process. Firstly , we use Med-icalESG to process the input sentence, identify segments of text that correspond to concepts in the UMLS Metathesaurus and associate each of them with one or more UMLS CUIs (Concept Unique Identier). Then we do a CUI lookup in UMLS to nd the corresponding semantic types for each CUI.

Most relation arguments are associated with multiple semantic types. For example, the term  X tetrac ycline hydrochloride X  has two types:  X Or -ganic Chemical X  and  X Antibiotic X . Sometimes, the semantic types are noisy due to ambiguity of terms. For example, the term  X Hepatitis b X  is asso-ciated with both  X Pharmacologic Substance X  and  X Disease or Syndrome X  based on UMLS. The rea-son for this is that people use  X Hepatitis b X  to rep-resent both  X the disease of Hepatitis b X  and  X Hep-atitis b vaccine X , so UMLS assigns both types to it. This is a concern for relation extraction, since two types bear opposite meanings. Our current strat-egy is to inte grate all associated types, and rely on the relation detector trained with the labeled data to decide how to weight dif ferent types based upon the conte xt.

Here is an illustrati ve example. Consider the sentence:  X Antibiotics are the standar d ther apy for Lyme disease X  : MedicalESG rst generates a dependenc y parse tree (Figure 2) to represent grammatical relations between the words in the sentence, and then associates the words with CUIs. For example,  X Antibiotics X  is associated with CUI  X C0003232 X  and  X Lyme disease X  is associated with two CUIs:  X C0024198 X  and  X C0717360 X .
 CUI lookup will assign  X Antibiotics X  with a se-mantic type  X Antibiotic X , and  X Lyme disease X  with three semantic types:  X Disease or Syndrome X ,  X Pharmacologic Substance X  and  X Immunologic Factor X . This sentence expresses a  X treats X  rela-tion between  X Antibiotics X  and  X Lyme disease X . 4.1 Moti vations Given a few labeled examples and man y unlabeled examples for a relation, we want to build a re-lation detector leveraging both labeled and unla-beled data. Follo wing the manifold regularization idea (Belkin et al., 2006), our strate gy is to learn a function that assigns a score to each example. Scores are t so that examples (both labeled and unlabeled) with similar content get similar scores, and scores of labeled examples are close to their labels. Inte gration of the unlabeled data can help solv e overtting problems when the labeled data is not suf cient. 4.2 Featur es We use 8 groups of features to represent each rela-tion example. These features are commonly used for relation extraction.  X  (1) Semantic types of argument 1, such as  X  (2) Semantic types of argument 2.  X  (3) Syntactic features representing the depen- X  (4) Features modeling the incoming and out- X  (5) Topic features modeling the words in  X  (6) Topic features modeling the words in the  X  (7) Bag-of-w ords features modeling the de-Notations:
The input dataset X = { x 1 ,  X  X  X  , x m } is repre-sented as a feature-instance matrix.

The desired label vector Y = { y 1 ,  X  X  X  , y l } repre-sents the labels of { x 1 ,  X  X  X  , x l } , where l  X  m . models the similarity of x i and x j . k x i  X  x j k stands for the Euclidean distance be-tween x i and x j in the vector space.
 D is a diagonal matrix: D i,i = graph Laplacian matrix.  X  is a user dened l  X  l diagonal matrix, where  X  i represents the weight of label y i .
 A =
V = [ y 1 ,  X  X  X  y l , 0 ,  X  X  X  , 0] is a 1  X  m matrix.  X  is a weight scalar . () + represents pseudo inverse.

Algorithm: Figure 3: Notations and the Algorithm to Train a Manifold Model for Relation Extraction  X  (8) Bag-of-w ords features modeling the
In relation extraction, man y recent approaches use non-linear kernels to get the similarity of two relation examples. To classify a relation exam-ple, a lot of dot product computations are required. This is very time consuming and becomes a bottle-neck in using relation extraction to facilitate clin-ical decision making. To speed up the classier during the apply time, we decided to use a linear classier instead of non-linear classiers.

We represent all features in a single feature space. For example, we use a vector of 133 en-tries (UMLS contains 133 semantic types) to rep-resent the types of argument 1. If argument 1 is associated with two types:  X Or ganic Chemical X  and  X Antibiotic X , we set the two corresponding en-tries to 1 and all the other entries to 0. Similar ap-proaches are used to represent the other features. 4.3 The Main Algorithm The problem we want to solv e is formalized as fol-lows: given a relation dataset X = { x 1 ,  X  X  X  , x m } , and the desired label Y = { y 1 ,  X  X  X  , y l } for { x 1 ,  X  X  X  , x l } , where l  X  m , we want to construct a mapping function f to project any example x i to a new space, where f T x i matches x i 's desired la-bel y i . In addition, we also want f to preserv e the manifold topology of the dataset, such that similar examples (both labeled and unlabeled) get simi-lar scores. Here, the label is `+1' for positi ve ex-amples, and `-1' for negative examples. Notations and the main algorithm to construct f for each re-lation are given in Figure 3. 4.4 Justication The solution to the problem dened in Section 4.3 is given by the mapping function f to minimize the follo wing cost function: The rst term of C ( f ) is based on labeled ex-amples, and penalizes the dif ference between the mapping result of x i and its desired label y i .  X  i is a user specied parameter , representing the weight of label y i . The second term of C ( f ) does not tak e label information into account. It encourages the neighborhood relationship (geometry of the man-ifold) within X to be preserv ed in the mapping. When x i and x j are similar , the corresponding W i,j is big. If f maps x i and x j to dif ferent posi-tions, f will be penalized. The second term is use-ful to bound the mapping function f and pre vents overtting from happening. Here  X  is the weight of the second term. When  X  = 0 , the model dis-regards the unlabeled data, and the data manifold topology is not respected.

Compared to manifold regularization (Belkin et al., 2006), we do not include the RKHS norm term. Instead, we associate each labeled example with an extra weight for label condence. This weight is particularly useful when the training data comes from  X Cro wdsourcing X , where we ask multiple work ers to complete the same task to correct errors. In that scenario, weights can be as-signed to labels based upon annotator agreement. Theor em 1: f = ( X ( A +  X  L ) X T ) + X A V T minimizes the cost function C ( f ) .
 Pr oof: Given the input X , we want to nd the optimal mapping function f such that C ( f ) is minimized: It can be veried that We can also verify that So C ( f ) can be written as Using the Lagrange multiplier trick to dif ferentiate C ( f ) with respect to f , we have This implies that So where  X + X  represents pseudo inverse. 4.5 Adv antages Our algorithm offers the follo wing adv antages:  X  The algorithm exploits unlabeled data, which  X  The algorithm pro vides users with the ex- X  Dif ferent from man y approaches in this area,  X  The algorithm is computationally efcient at 5.1 Cr oss-V alidation Test We use a cross-v alidation test 3 with the relation data generated in Section 3.2 to compare our ap-proaches against the state-of-the-art approaches. The task is to classify the examples into positi ve or negative for each relation. We applied a 5-fold cross-v alidation. In each round of validation, we used 20% of the data for training and 80% for test-ing. The F 1 scores reported here are the average of all 5 rounds. We used MedicalESG to process the input text for all approaches. 5.1.1 Data and Parameters This dataset includes 7 relations. We do not con-sider the relation of  X contraindicates X  in this test, since it has too few positi ve examples. On average, each relation contains about 800 positi ve examples and more than 13,000 negative examples. To elim-inate the examples that are trivial to classify , we remo ved the negative examples that do not bear the valid argument types. This remo ved the exam-ples that can be easily classied by a type lter , resulting in 3,000 negatives on average per rela-tion. For each relation, we also collected 5,000 unlabeled examples and put them into two sets: unlabeled set 1 and 2 (2,500 examples in each set).
No parameter tuning was tak en and no relation specic heuristic rules were applied in all tests. In all manifold models,  X  = 1 . In SVM implemen-tations, the trade-of f parameter between training error and mar gin was set to 1 for all experiments. 5.1.2 Baseline Appr oaches We compare our approaches to three state-of-the-art approaches including SVM with con volution tree kernels (Collins and Duf fy, 2001), linear re-gression and SVM with linear kernels (Sch  X  olk opf and Smola, 2002). To adapt the tree kernel to med-ical domain, we follo wed the approach in (Nguyen et al., 2009) to tak e the syntactic structures into consideration. We also added the argument types as features to the tree kernel. In the tree kernel im-plementation, we assigned the tree structure and the vector corresponding to the argument types with equal weights. The SVM with linear kernels and the linear regression model used the same fea-tures as the manifold models. 5.1.3 Settings for the Manif old Models We tested our manifold model for each relation un-der three dif ferent settings: (1) Manifold Unlabeled : We combined the la-beled data and unlabeled set 1 in training. We set  X  i = 1 for i  X  [1 , l ] . (2) Manifold Predicted Labels : We combined labeled data and unlabeled set 2 in training.  X  i = 1 for i  X  [1 , l ] . Dif ferent from the pre vious set-ting, we gave a label estimation to all the exam-ples in the unlabeled set 2 based on the noise rate (Noise%) from Table 1. The label of all unla-beled examples was set to  X +1 X  when 100%  X  2  X  N oise % &gt; 0 , or  X -1 X  otherwise. Two weighting strate gies were applied:  X  With Weights: We let label weight  X  i =  X  Without Weights:  X  i is always set to 1. (3) Manifold UnLabeled+Pr edicted Labels : a combination of setting (1) and (2). In this setting, the data from unlabeled set 1 was used as unla-beled data and the data from unlabeled set 2 was used as labeled data (W ith Weights). 5.1.4 Results The results are summarized in Table 2.

The tree kernel-based approach and linear re-gression achie ved similar F 1 scores, while linear SVM made a 5% impro vement over them. One thing to note is that the results from these ap-proaches vary signicantly . The reason for this is that the labeled training data is not suf cient. So the approaches that completely depend on the la-beled data are lik ely to run into overtting. Linear SVM performed better than the other two, since the lar ge-mar gin constraint together with the lin-ear model constraint can alle viate overtting.
By inte grating unlabeled data, the manifold model under setting (1) made a 15% impro vement over linear regression model on F 1 score, where the impro vement was signicant across all rela-tions.

Under setting (2), the With Weights strate gy achie ved a slightly worse F 1 score than the pre vi-ous setting but much better result than the baseline approaches. This tells us that estimating the label of unlabeled examples based upon the sampling result is one way to utilize unlabeled data and may help impro ve the relation extraction results. The results also sho w that the label weight is important for this setting, since the Without Weights strate gy did not perform very well.

Compared to setting (1) and (2), setting (3) made use of 2,500 more unlabeled examples, and achie ved the best performance among all ap-proaches. On one hand, this result sho ws that using more unlabeled data can further impro ve the result. On the other hand, the insignicant impro vement over (1) and (2) strongly indicates that how to utilize more unlabeled data to achie ve a signicant impro vement is non-tri vial and de-serv es more attention. To what extensions the un-labeled data can help the learning process is an open problem. Generally speaking, when the ex-isting data is suf cient to characterize the dataset geometry , adding more unlabeled data will not help (Singh et al., 2008).

We tested the tree kernel-based approach with-out inte grating the medical types as well. That re-sulted in very poor performance: the average F 1 score was belo w 30%. We also applied the rules used in SEMREP (Rindesch and Fiszman, 2003) to this dataset. Since the relations detected by SEMREP rules cannot be perfectly aligned with super relations, we cannot directly compare the re-sults. Ov erall speaking, SEMREP rules are very conserv ative and detect very few relations from the same text. 5.2 Kno wledge Base (KB) Construction The UMLS Metathesaurus (Lindber g et al., 1993) contains a lar ge amount of manually extracted re-lation kno wledge. Such kno wledge is invaluable for people to collect training data to build new relation detectors. One downside of using this KB is its incompleteness. For example, it only contains the treatments for about 8,000 diseases, which are far from suf cient. Further , the medical kno wledge is changing extremely quickly , making people hard to understand it, and update it in the kno wledge base in a timely manner .

To address these challenges, we constructed our own relation KB as a complement to the UMLS relation KB. We directly ran our relation detec-tors (trained with all labeled and unlabeled exam-ples) on our medical corpus to extract relations. Then we combined the results and put them in a new KB. The new KB covers all super relations and stores the kno wledge in the format of (rela-tion name, argument 1, argument 2, condence), where the condence is computed based on the re-lation detector condence score and relation pop-ularity in the corpus. The most recent version of our relation KB contains 3.4 million such entries.
We compared this new KB against UMLS KB using an answer generation task on a set of 742 Doctor Dilemma questions. We rst ran our rela-tion detectors to detect the relation(s) in the ques-tion clue involving question focus (what the ques-tion asks for). Then we searched against both KBs using the relation name and the non-focus argu-ment for the missing argument. The search re-sults were then generated as potential answers. We used the same relations to do KB lookup, so the results are directly comparable. Since most ques-tions only have one correct answer , the precision number is not very important in this experiment.
If we detect multiple relations in the question, and the same answer is generated from more than one relations, we sum up all those condence scores to mak e such answers more preferable. Sometimes, we may generate too man y answers from KBs. For example, if the detected relation is  X location of X  and the non-focus argument is  X skin X , then thousands of answers can be gener -ated. In this scenario, we sort the answers based upon the condence scores and only consider up to p answers for each question. In our test, we considered three numbers for p : 20, 50 and 3,000.
From Table 3, we can see that the new KB out-performs the most popularly-used UMLS KB at all recall levels by a lar ge mar gin. This result in-dicates that the new KB has a much better kno wl-edge coverage. The UMLS KB is manually cre-ated and thus more precise. In our experiment, the UMLS KB generated fewer answers than the new KB. For example, when up to 20 answers were generated for each question, the UMLS KB gen-erated around 4,700 answers for the whole ques-tion set, while the new KB generated about 7,600 answers.

Construction of the new KB cost 16 machines (using 4  X  2.8G cores per machine) 8 hours. The reported computation time is for the whole corpus with 11G pure text.
 In this paper , we identify a list of key relations that can facilitate clinical decision making. We also present a new manifold model to efciently extract these relations from text. Our model is developed to utilize both labeled and unlabeled examples. It further pro vides users with the exibility to tak e label weight into consideration. Effecti veness of the new model is demonstrated both theoretically and experimentally . We apply the new model to construct a relation kno wledge base (KB), and use it as a complement to the existing manually cre-ated KBs.
 We thank Siddharth Patw ardhan for help on tree kernels, Sug ato Bagchi and Dr. Herbert Chase' s team for cate gorizing the Doctor Dilemma ques-tions. We also thank Anthon y Le vas, Karen In-graf fea, Mark Mer gen, Katherine Modzele wski, Jonathan Hodax, Matthe w Schoenfeld and Adarsh Thak er for vetting the training data.
