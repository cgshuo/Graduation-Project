 Information retrieval (IR) systems such as search eng ines receive queries from users, and aim to provide the most relevant information available in their data-bases in response. Search engines can use a central index for retrieval, but this strategy has several drawbacks. Due to hardware limitations it may not be easy to keep all the documents indexed on a sing le machine. Also, a centralized search engine for web data relies on documents being provided by a crawl, and thus can-not index the hidden web. For example, the query  X  X ireless and network X  returns 28013 answers (as of May 27, 2004) from the USPTO (the US Patent and Trend-mark database, patft.upsto.gov/netahtml/search-adv.htm ), while Google ( google.com ) reports no answer for searching for the same keywords in the same site [Ipeirotis, 2004].

Distributed information retrieval (DIR) addresses such problems. In DIR sys-tems, information is held in separate collections, which might be in different physical locations or on separate servers . The query is first passed to a central broker . The broker then sends this query to all or some of the servers. The servers provide the broker with their best answers, which the broker merges into a single list that is returned to the user. Thus the documents do not have to be gathered into a single location, and the constraints imposed by machine capacity are much more relaxed; however, DIR does introduce query-time costs of networking, and a query may be sent to collections where there are unlikely to be answers.
DIR systems therefore need to address two major issu es, how to select collec-tions and how to merge the results return ed from each collection. In this paper, we investigate the first of these: which collections should be selected for each query? Brokers typically compare each query to summaries  X  also called rep-resentation sets  X  of each collection [Ipeirotis and Gravano, 2004], and choose the collections whose summaries have the greatest similarity to the query. In most previous work, and in this paper, each summary contains statistics about the lexicon of the corresponding collection. If the lexicon of the collections is provided to the central broker  X  X hat is, if the servers are cooperative  X  X hen complete and accurate information can be used for collection selection. In an un-cooperative environment such as the hi dden web, however, the collections need to be probed to establish a sample of th eir topic coverage. This technique is known as query probing. In previous work, it has been claimed that a probe that returns 300 documents is sufficient t o characterize a co llection. However, we dispute this claim.

We propose an adaptive query probing technique that uses statistics of term occurrences in returned documents to exa mine whether further probing is justi-fied. Our results show that, with only 300 documents, coverage of the lexicon is small and query effectiveness is impaired. By use of larger samples, and by use of our thresholding technique that determines when sampling can terminate, we obtain much greater effectiveness. While the number of documents that must be probed is substantially increased, the method is free of an arbitrary choice of cut-off and is expected to adapt to collections with different characteristics. In a cooperative DIR environment, servers provide the broker with global in-formation about their collections. This information is usually about the terms they contain [Callan et al., 1995; Gravano et al., 1999; Yuwono and Lee, 1997] or the similarity function they use for the ranking. The advantage of this type of environment is that broker usually has comprehensive knowledge about each collection, allowing relatively accurate selection. However, many servers do not provide such information. Approaches to cooperative DIR differ in terms of the type of information that is provided and in the merging functions that are used. In spite of implementation differences, the performance these methods are re-ported to be similar [D X  X ouza et al., 2004a;b].

In an environment such as the Web, collections are usually non-cooperative and do not publish their index information. In non-cooperative environments, brokers try to obtain information about the collections that are to be searched by send-ing them artificial queries and evaluating the returned answers. These queries are known as probes , and the whole procedure is usually called query-probing [Craswell et al., 2000] or query-based sampling [Callan and Connell, 2001]. For ex-ample, suppose that the series of single-t erm probe queries  X  X occer X ,  X  X asketball X ,  X  X ealth X ,  X  X omputer X ,  X  X BM X , and  X  X an cer X  has been sent to a collection and the following numbers of answers have been returned: 1500, 1730, 200, 0, 0, and 2. Then the collection is more likely to include coverage of sports than compared to computer science.

Callan and Connell [2001] applied query-based sampling for iteratively dis-covering the language model of the collections in non-cooperative environments. Their algorithm starts by selecting an initial query that returns at least one answer from the collection, and then retrieves the first N results returned. The language model is updated according to t he new terms found in the retrieved doc-uments. The next probe queries are selected from the obtained language model; probing continues until a stopping criterion is met. Callan et al. tested different values of N and various stopping conditions, and reported that using N =4and 75 queries, thus obtaining about 300 documents, leads to a good summary of the sampled collection. They also examined di fferent strategies for query selection and conclude that these do not have a significant effect on final performance. Variants explored included choosing the queries from the terms that have the highest document frequency, collectio n frequency, and average term frequency in the current language model, with randomly generated queries as a baseline. In all these cases, the reported results are similar, with random queries having small advantage over other methods.

Using random queries is now a widely a ccepted method for query-based sam-pling [Gravano et al., 2003; Callan and Connell, 2001; Craswell et al., 2000]. We also use this strategy in our proposed approach. The main focus of our work is to examine different s topping criteria.

Callan and Connell [2001] proposed use of the ctf ratio ,representingthefrac-tion of term occurrences in the total coll ection that are covered by distinct terms in the sampled documents. For example, consider a collection that includes only two documents. The first consists of 98 occurrences of the term  X  X ar X  and one occurrence of a single term,  X  X ook X . The other document consists of a single term,  X  X ar X . By finding the second document, the sampled language model will contain 99% of term occuren ces in the collection and the ctf ratio will be 0 . 99. Callan et al. [1999] report that after sampling about 300 documents the ctf ratio becomes smooth. Later, we examine the effectiveness of using the ctf ratio for estimating the quality of the obtained language model.
 Craswell et al. [2000] investigated query probing for server selection on the web. They used the sampling approach of Callan and Connell [2001] to estimate the server effectiveness, and use d this estimation for server selection. They report that a system that chooses the top 10 collections out of 956, based on summaries ob-tained by query probing, can outperform a central index that has indexed 25% of the total documents. Query-based samp ling has recently been applied for different purposes, such as estimating the size of uncooperative collections [Si and Callan, 2003] or classification of hidden web databases [Gravano et al., 2003]. In all of these experiments, fixed sample sizes were used. Ipeirotis and Gravano [2004] used query expansion techniques to overcome the poor quality of the samples but their investigation was limited to topical collections. As can be seen from the work surveyed abo ve, most of the prior research uses fixed parameters in query based sampling, and there is no clear stopping condi-tion and termination point for the process. To our knowledge, none of the pro-posed methods for non-cooperative DIR involves an adaptive choice of stopping point. Yet the usual stopping point seems low; 300 documents seem unlikely to be sufficient for representing many current collections, such as digital libraries. In-tuitively, larger collections with diverse topics need more samples while smaller, topic-specific ones might need less. Williams and Zobel [2005] have shown that vocabulary growth after indexing about 45 GB of web data does not converge to zero, and, the rate of discovery of new unique terms is about one in every 400 term occurrences. For query-based samp ling, the question is, therefore, when to stop sampling. In the following section we explore the following hypotheses: 1. As long as we keep sampling, the vocabulary continues to grow. 2. The rate of vocabulary growth is not a good way to estimate collection size. 3. The risk of missing significant terms is high with traditional sampling. Test Environment. We tested query-based samplin g on five collections of differ-entsizesandcontents,showninTable 1. The first two collections are from the UDC-39 testbed (discussed in detail in the next section), each containing 17 , 352 documents of TREC newswire data. DATELINE 509 and DATELINE 325 are two managed collections used by D X  X ouza et al. [2004a]. Documents in each col-lection of this testbed are TRE C newswire data split by the DATELINE field. They reported that gathering the data in managed collections improves the over-all performance of document retrieval fro m distributed collect ions. Since docu-ments in these collections are usually from the same organization and authors, we would expect them to have a more lim ited vocabulary compared to collec-tions of similar size with various authors. The fifth collection is composed of 2 GB of data from a 1997 web crawl (the first two gigabytes of TREC WT10g collection). The WT10g collection was co nstructed to be representative of the web [Bailey et al., 2003].

To evaluate our approach we also extracted the most significant terms from each collection by gatheri ng all terms with a Cosine tf  X  idf [Baeza-Yates and Ribeiro-Neto(1999)] factor greater than a certain threshold (  X  ) using Zettair. 1 We used 0 . 5for  X  ; other values of  X  do not affect the approach and could be used. This information is used after termination of query-based sampling, as a measure of the effectiveness of the collect ed summaries and of the risk of missing significant terms.

Two testbeds are used in our selection experiments. SYM236 includes 236 col-lections of varying size. It has been used in previous related work [French et al., 1999; Powell and French, 2003]. It includes collections made from documents on TREC CDs 1 to 4. UDC39 is made from UDC236 [French et al., 1999; Powell and French, 2003], which contains 236 collections that include the same number of documents each and has been constructed from the same documents as SYM236 (from TREC CDs 1 to 4). The difference is only the methodology used to assign documents to collections. In SYM236, collections are of varying sizes, while in UDC236 collections contain the similar number of documents. UDC39 has 39 collections each made from concatenating six consecutive collec-tions in UDC236. Therefore, UDC-1 in this testbed contains documents in the first six collections of UDC236. The total numbers of documents in both testbeds are the same.

We used the titles of TREC topics 51  X  150 as queries, whose average length is 2  X  3 terms, which is similar to web queries [Jansen et al., 2000]. A thousand answer documents are retrieved in response to each query from each selected collection. The assumption is that collections only return a limited number of documents for any given query. If a coll ection does not return a relevant doc-ument in the top 1000 results, the DIR system can never use that document. We used Lemur 2 for query-based sampling, and CORI [Callan et al., 1995] for collection selection and re sult merging because, although it may not be the most effective method, our results can then b e directly compared to those in most previous work. We leave the testing of ot her collection select ion methods, such as those which are discussed in Meng et al. [2002] as future work. For each collec-tion (other than WEB) we gathered samples of different sizes, from 100 to 3000 documents. Each sample n contains all of the documents from sample n  X  1, plus 100 new documents. The initial sample always extracts 100 distinct documents. At each point, the system calculates the number of unique and significant terms available in the samples. We show results for 3000 documents because this num-ber was sufficient according to our expe riments; other collections might need greater sample sizes to meet the stoppi ng criteria. We use a recall metric to measure completeness o f the sampled term set: Experimental Results. Figure 1 shows the number of unique and significant terms in each sample provided by query-based sampling UDC-1 and UDC-2 collections. The rate at which new unique terms are found slows as the number of sampled documents increases. The slope of each curve is large at 300 documents, the recom-mended size for query-based sampling [Callan et al., 1999]. As sampling continues, the slope becomes flatter. Based on previ ous work [Williams and Zobel, 2005], con-tinued sampling will always continue to find new words but the rate will decrease. Note that the rate for significant terms drops more rapidly than for terms.
A key contribution in this paper is that convergence to a low rate of vo-cabulary increase is indicative of good coverage of vocabulary by the sampled documents. In other words, query samplin g reaches a good coverage of the collec-tion vocabulary when the slope becomes les s than a certain threshold; empirical tests of this hypothesis are discussed below. In these charts, when the trends for the number of unique terms starts smoothing, the curves for the number of significant terms found are nearly flat, which means that by continuing sampling we are unlikely to receive many new signi ficant terms, and it is unlikely to be efficient to keep probing. The recall curve c onfirms that the number of new signif-icant terms hardly increa ses after sampling a certain amount of documents. The recall value for a sample of 300 document is less than 15%, while for summaries including more than 2000 documents this amount is greater than 45% (three times more) in both graphs. These trends strongly indicate that a sample size of 300 documents is insufficient for maki ng effective summaries. As the slopes for significant terms are not negligible after sampling 300 documents, the risk of losing significant terms is high at this point. Figure 2 shows similar trends for the DATELINE managed collections. Again, the samples made from 300 documents do not appear to be a good representation of the collection language model. Cu-riously, although we were expecting the graphs to get smooth sooner than the previous collections (because of the documents should have similar topics), the results are very similar. The reason might be that all the collections so far are based on the TREC newswire data and contain similar documents. Trends for discovery of new terms and recall values for summaries obtained by sampling our WEB collection are shown in Figure 3. As the collection is significantly larger, we extended our range of sampling to 6000 documents. The slope is sharply upward, not only after sampling 300 documents, but also in all the other points lower than 1000. At this point, the curve for significant terms is already fairly smooth. In other words, we are unlikely to receive si gnificant terms wit h the previous rate by continuing probing. Interestingly, while the system has downloaded less than 2% of total documents, the trend for discovering new terms is getting smooth. Recall values start converging after do wnloading nearly 900 documents. Based on these experiments, we conclude that:  X  Hypothesis 1 is clearly confirmed, since the accumulation of new vocabulary  X  Hypothesis 2 is confirmed, because collections that were significantly dif- X  Hypothesis 3 is confirmed; if probing is halted after sampling 300 documents, Given that a sample size of 300 is inadequate, but that some condition is needed to terminate sampling, we need to investigate when sampling should cease. In this section, we test the effect of varying the sample size on retrieval effective-ness. Table 2 shows the mean average precision (MAP) for different sample sizes. We use the TITLE field of TREC topics 51  X  150 as queries. Values for precision at 10 and 20 documents retrieved are provided because these include the documents that users are most likely to look at [Jansen et al., 2000]. Cut-off values represent the number of collections that will be searched for each query. The results show that, by using samples of more than 300 documents, the overall performance increases. The previously recommended number of 300 documents is not in general a sufficient sample size. Previous work uses ctf as an indication of vocabulary coverage, and shows that curves become smooth af-ter downloading a limited number of documents from a collection [Callan et al., 1999; Callan and Connell, 2001]. However, our results show ctf is not an indi-cation of achieving good vocabulary coverage. Terms that are more frequent in the collection are more likely to be extra cted by query probing. Once the system finds such a term, the ctf ratio increases more than when system finds a word with lower frequency. However, these te rms are not necessarily more important than the other terms [Luhn, 1958] in the collection, and indeed are unlikely to be significant in queries; downloading them does not mean that the coverage of the vocabulary is sufficient. Given that 300 documents is insufficient, and that the appropriate number is not consistent from collection to collection, the question is: how big a sample should be chosen from a given collection?
We propose that an appropriate method is to keep sampling until the rate of occurrence of new unique terms (the slope in previous figures) becomes less than a predefined threshold. Specifically, we propose that query probing stop when, for  X  subsequent samples, the rate of growth in vocabulary becomes less than a threshhold  X  . Based on the empirical experiments discussed in the previous section, we suggest initial parameter choices of  X  =3and  X  = 2%; that is, probing stops once three consecutive probes all s how growth rate of less than 2%. These convergence points are indicated by arrows in previous figures. In our approach, these points indicate when sampling is  X  X nough X . According to the observations,  X  X nough X  varies drastically from collection to collectio n. Increasing the value for  X  or decreasing  X  delay reaching the stopping condition and increase the number of samples that should be gathered from the collection.
 SYM236. The performance of a central index for document retrieval for both collections is shown in Table 3. Since both testbeds include exactly the same documents, the central index for both of them is the same. We used the val-ues in this table as the baseline. Central indexes are usually reported as being more effective than distributed systems [Craswell et al., 2000]. The first column is the number of relevant documents retrieved for TREC topics 51  X  150; the last column is the precision of the sys tem after as many documents have been retrieved as there are relevant documents in the collection. A comparison of the effectiveness of two systems using traditio nal and adaptive query-based sampling techniques is shown in Table 4. The numbers above the middle line represent the values obtained from the traditional method, while those below specify the same factor using our adaptive method. For cutoff = 1, only the best collection  X  that whose sampled lexicon has the greatest similarity to the query  X  will be searched. For cutoff = 118, half of the collections will be searched. It can be seen that our method outperforms the traditional query probing technique in all of the parameters and for all cutoff values 3 . Sanderson and Zobel [2005] demonstrated that a significant improvement in performance requires statistical tests. We applied the t-test for comparing the outputs of traditional and adaptive systems. Values shown with an asterisk (*) are significantly different at P&lt; 0 . 05 while those with double asterisks (**) differ significantly at P&lt; 0 . 01.
Table 5 gives more information about the number of terms and documents that have been sampled using the traditional and adaptive techniques. The small-est and largest samples in each testbed are specified in the last two columns. It is clear that our new approach collects a much more comprehensive set of terms and documents during sampling, and that different collections require samples of greatly varying size.
 UDC39. Similar experiments using the UDC39 testbed are shown in Table 6. The same query set is used for experiments on this testbed. Table 6 confirms that our new method outperforms the traditional query based sampling approach; furthermore, our approach is more effective than a central index in many cases. Central index performance has often been viewed as an ideal goal in previous work [Craswell et al., 2000]. Developing a distributed system that outperforms the central index in all cases is still on e of the open questions in distributed information retrieval but has been reported as achieveable [French et al., 1999]. According to these results, the perform ance of our DIR system was greater than the central index for cutoffs 10, 20, and 30 for precision-oriented metrics. For cutoff = 10, for example, the system only searches the top 10 collections for each query. This means that it searches only about a quarter of the collections and documents used by the central index, but shows greater effectiveness. Again, values flagged with (*) and (**) indicate statistical significant using the t-test . Changing  X  and  X  . In the results discussed above, we used values for  X  and  X  obtained from our initial experiments. Decreasing  X  or increasing  X  leads to faster termination of query probing, with less effective summaries. In Table 7, we have decreased the threshold  X  to 1%  X  thus increasing the sample sizes  X  for SYM236. In most cases, the effectiveness is greater than for the same parameters in Table 4, that uses the old  X  and  X  values. Although the results are better, they are more costly. Table 5 shows that the number of documents sampled with  X  = 1% is about twice that with  X  = 2%. The results for the UDC39 were also tested and found to be similar ( but are not presented here). We have proposed a novel sampling strategy for query probing in distributed information retrieval. In almost all previous work on query probing, the sample size was 300 documents; we have shown that such small samples lead to consid-erable loss of effectiveness. In contrast to these methods, our system adaptively decides when to stop probing, according to the rate of which new unique terms are received. Our results indicate that once the rate of arrival of new terms has become constant, relatively few new sig nificant terms  X  those of high impact in retrieval  X  are observed. We compared our new approach and traditional model for query-based sampling on two different testbeds. We found that collections have different characteristics, and that the sample size will vary between collec-tions. The effectiveness of the new approach was not only significantly better than the fixed-size sampling approach, but also outperformed a central index in some cases. While the use of larger sample s leads to greater initial costs, there is a significant benefit in effectiveness for subsequent queries.
