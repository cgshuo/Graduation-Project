 In light of the proliferation in available data in general and text documents, which are usually unlabeled, in particular, there is a growing necessity for Unsupervised Learning. Unsupervised Learning in Machine Learning (ML) refers to the method used for seeking out similarities between pieces of data in order to determine whether they can be characterized as forming a group (cluster). Many methods employed in unsupervised ML are based on data mining methods used to preprocess data. Approaches to unsupervised ML include clustering methods, such as K-means, Expectation Maximization (EM), and hierarchical clustering. Surveys of various clustering methods can be seen at [13, 22]. 
Text clustering is an automatic unsupervised grouping of text documents into clusters. Clustering text documents is the process of detecting groups of documents with similar content. Clustering typically results in a set of clusters, where each cluster consists of documents. The quality of clustering is considered to be superior when more similarities appear within one cluster, in the contents of the documents, and when more dissimilarities can be found between the various clusters. However, it applications and might even differ between users. Researchers can influence the results of a clustering algorithm by using subsets of features only or by using specific similarity measures. 
There is a widespread variety of clustering applications. The most popular ones are: (1) document clustering related to classification and information retrieval, and (2) word clustering to produce groups of similar words or concept hierarchies. Examples of clustering applications are presented below. 
He and Hui [10] propose a mining process to automate the author co-citation analysis (ACA) based on the Web Citation Database. Their mining process uses agglomerative hierarchical clustering as th e mining technique for author clustering and multidimensional scaling for displayi ng author cluster maps. The clustering results and author cluster map have been incorporated into a citation-based retrieval system known as PubSearch to support author retrieval of Web publications. 
A graph-clustering algorithm, called Chinese Whispers [1], is used for various natural language processing (NLP) tasks such as language separation, acquisition of syntactic word classes, and word sense disambiguation. Document clustering is used to produce an overview of the recent trends in data mining activities [20]. Production of domain clusters for domestic and international news in four languages: English, German, Chinese and Russian, was done by Sharoff [21]. Integration of clustering and ranking for heterogeneous information network analysis is presented by Sun et al. [23]. Chan at el. [2] developed a system that visualizes relationships and groupings of authors using keyword clustering and author clustering. They found that authors tend to be grouped together correctly because the co-citation and joint publications imply an extent of collaboration between the authors in the same clusters. 
This study focuses on a special form of documents called responsa. The responsa are usually unlabeled answers written by foremost Jewish rabbis in response to various questions submitted to them. These documents are taken from a widespread variety of Jewish domains, e.g.: customs, holidays, kosher food, and laws. Each responsa is based on responsa are written in two Semitic languages: Hebrew and Aramaic. from the viewpoints of their authors, the places (countries) in which the documents were function words), (2) FW (most frequent words including function words), and (3) VFW tasks because function words have little lexical meaning and words with the highest variance values are not considered good features. 
This paper is organized as follows: Section 2 presents several previous text clustering systems that use words. Section 3 presents our model. Section 4 describes the examined corpora. Section 5 details the experimental results and analyzes them. Section 6 summarizes and proposes future directions for research. Text clustering presents challenges due to the large number of training documents, the vast amount of features present in the documents set, and the dependencies between the features. Effective feature selection is essential to make the learning task efficient and higher in accuracy. In text clustering one typically uses a  X  X ag of words X  model, where each position in the input feature vector corresponds to a given word or phrase. efficient -----conserving computation, storage and network resources for the training phase and for every future use of the classifier. Moreover, appropriate features and an appropriate number of features can improve clustering capability substantially, or equivalently, reduce the amount of training data necessary in order to obtain a desired level of performance. 
Supervised classification of Hebrew-Aramaic documents has been presented in a few previous works. Several works presented by HaCohen-Kerner et al. [7, 8] use stylistic feature sets. Other works presented by Koppel et al., [14, 15] use a few hundreds of single words. HaCohen-Kerner et al. [6] investigate as a supervised ML task the use of  X  X ag of words X  for the classification of Hebrew-Aramaic documents according to their historical period and the ethnic origin of their authors. Hotho et al. [11, 12] implement text documents clustering based on ontologies. Their method includes the following stages: (1) representation of the original text document as a  X  X ag of words X , (2) removal of 571 stopwords taken from a standard list, (3) stemming, (4) pruning rare terms, (5) TF-IDF weighting, and (6) integrating sets of synonyms as background knowledge achieved through the Wordnet ontology. They found that the best background knowledge method always improves performance compared to the best baseline. 
Miao et al. [18] implement three methods for document clustering: (1) using words (after removing stopwords, stemming, pruning rare terms and tfidf weighting), (2) using terms (based on their C Value , i.e., a frequency-based weight that accounts for nested terms), and (3) using frequent character n-grams. They found that the n-gram-based representation gives the best performance with the lowest dimensionality. 
Li and Chung [16] and Li et al. [17] implement text clustering based on frequent word sequences and frequent word meaning sequences instead of  X  X ag of words X . A word meaning is the concept expressed by synonymous word forms. Using these two kinds of sequences they reduce the high dimensionality of the documents and measure the closeness between documents. 
Yu [25] uses function words 1 to solve the Chinese authorship attribution problem (C-FWAA) in three different genres: the novel, essay, and blog. Her system is able to distinguish three authors in each genre with various levels of success. C-FWAA is the most effective in distinguishing authors of novels (averaged accuracy 90%), followed by essays (85%), and with blogs being the most difficult (68%). 
With the exception of Yu [25], all studies mentioned above that use  X  X ags of words X , inter alia process removal of stop words from their  X  X ags of words X . Moreover, they neither use the most frequent words nor words with the highest variance values. 
Forman [4] presents an empirical study of twelve feature selection metrics, which were evaluated on a benchmark of 229 text classification problem instances that originated from Reuters. Forman claims that overly common words (stopwords), such as  X  X  X  and  X  X f X , may be removed on the grounds that they are ambiguous and occur so common word was identified if it occurred in over 50% all documents. As mentioned above, Forman claims that stopwords should be removed when utilizing the feature selection methods. We chose to examine his claim by conducting two sets of experiments: one uses all the words including stopwords and the other without the stopwords. In both sets, we used a normalized frequency for each feature. In addition, we examined a third word-list: words with the highest variance values. 
We have defined the following terms: 1. Common Features (CF)  X  a word is identified as a common feature if it occurs in 2. Frequent Words (FW)  X  Most frequent words including CF. 3. Filtered Frequent Words (FFW)  X  Most frequent words excluding CF. 4. Variance Features (VFW) -Words with the highest variance values. We have defined the following algorithm: For each experiment E done for task T on corpus C For each word list: FW, FFW and VFW select: 100, 200, ..., 1000 as the number of its features Measure the performance of the EM algorithm 
The EM algorithm [3] is a general appro ach to iterative computation of maximum-likelihood estimates when the observations are viewed as incomplete data. Each algorithm's iteration consists of an expectation step followed by a maximization step. The EM method was chosen by us, due to its simplicity, generality and relatively quick run time, and due to the wide range of examples which fall under its definition. 
In our work, the EM algorithm has been applied on a widespread variety of clustering tasks: clustering of documents to their authors or to countries where the documents were written or the historical period when they were written. For each task, various experiments (depending of the number of chosen words) have been performed. Each experiment has been applied for each one of the 3 word lists: FW, FFW, and VFW. The accuracy measured in all experiments is the fraction of the number of documents correctly clustered to the total number of possible documents to be clustered. We applied the EM implementation of Weka [24, 9] using the default values as done by Forman [4]. Model tuning is left for future research. The application domain contains responsa written in Hebrew and Aramaic. The responsa are answers written by foremost Jewish rabbis in response to various questions submitted to them. These documents are taken from a widespread variety of Jewish domains, e.g.: customs, holidays, kosher food, and laws. Each responsa is based on both ancient writings and answers given by previous rabbis. The examined corpora were downloaded from The Global Jewish Database (The Responsa Project 2 ) at Bar-Ilan University. These corpora include 1,370 documents (responsa), which contain 1,769,072 words composed by six authors who lived in two consecutive generations (the first starting around 1800 and the second starting around 1850) from 3 countries (Germany, Lithuania and Morocco). Tables 1-3 present full statistical information concerning these corpora. In sub-section 5.1, we detail the results and the analyses of the clustering task according to countries or periods of composition. In sub-section 5.2, we detail the results and the analyses of the clustering task according to the documents' authors. 5.1 Clustering Experiments Accord ing to Countries or Periods We have performed eight clustering experiments (combinations of 2 or 3 countries of composition and 2 periods of composition). Tables 4-6 present the clustering results for the 3 word lists: FW, FFW, and VFW, respectively. 
Almost all superior clustering results for the first two word lists: FW and FFW (Tables 4-5) have been achieved by 100 or 200 or 300 words, i.e., there is no need to use more words. However, most of the superior clustering results for VFW (Table 6) have been achieved by 800 or 900 words. Table 7 presents the best clustering results versus the baseline results for all 8 experiments. The baseline classifier is the "majority" method, which assumes that every document belongs to the larger of the two or three categories (depending on the experiment). This baseline is reasonable since almost all the experiments are for two categories and the categories do not include the same number of documents, i.e. most of the baseline values are relatively classification tasks. 
The improvement rates for the clustering tasks according to countries or periods presented in Table 7 are significant. The improvement rates from the "majority" (the baseline result) to the best result vary from 11.53% to 39.43%. 
When considering all 8 experiments (Tables 4-6), FFW has been found as the superior set with 4 best results (one of them tied with FW), VFW with 3 best results, and FW with 2 best results (one of them tied with FW). When considering the clustering experiments according to period, FFW has been found as the superior set with 3 best results. FFW contains the most frequent words excluding function words, i.e. the set of the most frequent content wo rds is the best set for clustering according to periods. A possible explanation is that di fferent content words are used in different periods. 
However, a surprising finding has been discovered when considering the clustering experiments according to countries. VFW has been found as the best set with 3 best results (out of 4). VFW contains the words with the highest variance values. A possible explanation is that frequent words that their distribution is non-uniform over the documents are the best set for clustering according to countries. 
The FFW set was better than the FW set in the clustering experiments performed in sub-section 5.1. This finding strengthens Forman's claim, which stopwords do not support the classification task. Nevertheless, FW achieved some surprising results. Moreover, VFW achieved even more surprising results. Another interesting finding is as follows: the greater the distance between the countries, the greater the success rate of clustering results according to countr ies in comparison to those according to periods (Germany-Lithuania 1300 km, Germany-Morocco 3300 km and Lithuania-Morocco 4600 km). The clustering result for Germany versus Lithuania, which are relatively close geographically, is quite poor (74.50%) with a rather small improvement rate (11.53%), while the clustering results for Germany or Lithuania versus Morocco are much higher (93.23% and 93.95% respectively) with higher improvement rates. 5.2 Clustering Experiments According to Authors Document clustering according to authors is one of the most popular clustering tasks. A few relevant works have been presented in the first two sections. In our work, we have performed clustering experiments for all combinations of 2 or 3 authors. All six examined authors are the same authors mentioned in the previous sub-section with the same serial numbers. 
Tables 8-10 present the clustering results for all 15 possible combinations of 2 authors (out of 6 authors) for the 3 word lists: FW, FFW, and VFW, respectively. When taking into account the number of optimal results (100%), FW is the superior set with 7 optimal results, VFW is second best with 6 optimal results, and FFW is least successful with only 3 optimal results. When taking into account the average of the best 10 results (one for each row), for each one of the word lists, we observe that VFW is the best set with an average of 98.58%, FW is the second with a very close result of 98.49%, and FFW is least successful with an average of only 95.41%. 
FW includes function words, which are co mmon to many authors and therefore are usually regarded as words that have little lexical meaning. Thus, these results are rather surprising and contrast Forman X  X  opinion that common words should not be used for classification tasks. A more surprising result is the success of VFW, which represents words that are unusually distributed over the documents. Improvement function words and words with the highest variance values are relevant for such clustering tasks. 
Tables 11-13 present the clustering results for all 20 possible combinations of 3 authors (out of 6 authors) for the 3 word lists: FW, FFW, and VFW, respectively. When taking into account the number of optimal results (100%), FW is again the best set with 2 optimal results, VFW is in the second place with 1 optimal result, and FFW is in the last place with no optimal results. Additional results concerning clustering to 3 authors are given in Tables 13 and 15. 
When taking into account the average of the best 10 results (one for each row), for each of the word lists (Tables 11-13), we observe again that VFW is the best set with an average of 96.38%, FFW is in the second place with 95.85%, and FW is in the last place with an average of 95.41%. Again, VFW appears to be better than the other word lists for clustering according to 3 authors. The improvement rates for the clustering tasks according to 2 authors presented in Table 14 vary from 15.61% to 48.97%. The improvement rates for the clustering tasks according to 3 authors presented in Table 15 vary from 24.39% to 56.51%. 
When considering all 15 experiments according to 2 authors (rows 1-15 in Table 14), FW has been found to be the best set with 11 best results, VFW with 8 best results, and FFW with only 4 best results. When considering all 20 experiments according to authors (rows 1-20 in Table 15), FW has been found again as the best set with 9 best results, FFW with 8 best results, and VFW with only 6 best results. clustering of documents according to their authors, to the countries in which the documents were written or the historical period in which they were written. To the best of our knowledge, performing clustering tasks according to countries or periods are novel. 
The clustering tasks for 2 countries are rather successful (74.50%, 93.23%, and 93.95%). An interesting finding is that as the countries are geographically farther apart, the clustering results achieved were more successful. For the more complex clustering tasks (3 tasks were reasonable (between 82.70% and 92.71%). 
The clustering tasks according to 2 or 3 authors have been highly successful (above 95%), especially when using FW and VFW. A possible explanation is that function words and words with the highest variance values are relevant for such clustering tasks. 
Future directions for research are: (1) tuning of the model, (2) defining and applying additional types of features such as: special content word lists, n-grams, morphological features (e.g.: nouns, verbs and adjectives), syntactic features adverb), key-phrases [5], collocations and references unique to each class, and (3) applying various kinds of models in other domains (especially those which are important for historians or other humanities researchers), applications and languages. 
