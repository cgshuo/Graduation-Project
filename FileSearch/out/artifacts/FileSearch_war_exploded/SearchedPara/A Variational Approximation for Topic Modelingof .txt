 Do-kyum Kim dok027@cs.ucsd.edu Geoffrey M. Voelker voelker@cs.ucsd.edu Lawrence K. Saul saul@cs.ucsd.edu The basic steps to prove theorem 3.1 are contained in two lemmas.
 Lemma S1.1. Let f ( x ) = log  X ( x ) + log( x )  X  x log( x ) . Then f ( x ) is a concave function of x &gt; 0 . Proof. We prove concavity by showing f 00 ( x ) &lt; 0 for all x &gt; 0. Taking derivatives, we find: where  X ( x ) denotes the digamma function and  X  0 ( x ) its derivative. A useful identity for this deriva-tive (Abramowitz &amp; Stegun, 1964) is the infinite series representation: The lemma follows by substituting this series repre-sentation into eq. (S1). In particular, we have: f ( x ) =  X  This completes the proof, but we gain more intuition by plotting f ( x ) as shown in Fig. S1. Note that log  X ( x ), which contains only the first term in f ( x ), is a convex function of x . Thus it is the other terms in f ( x ) that flip the sign of its second derivative. Essentially, the concavity of f ( x ) is established by adding log x at small x and by subtracting x log x at large x . Lemma S1.2. Let x be a nonnegative random variable with bounded E[log(1 /x )] &lt;  X  . Then: Proof. Let f ( x ) denote a concave function on x &gt; 0. From Jensen X  X  inequality, we have that E[ f ( x )]  X  f (E[ x ]). The result follows by setting f ( x ) = log  X ( x ) + log x  X  x log x as in Lemma S1.1. Note that a naive application of Jensen X  X  inequality to the left hand side of eq.(S3) yields the lower bound E[log  X ( x )]  X  log  X (E[ x ]). Thus it is the additional terms on the right hand side of eq. (S3) that establish the upper bound. The direction of this inequality is crucial in the context of variational inference, where the upper bound in eq. (S3) is needed to maintain an overall lower bound on the log-likelihood. Equipped with this lemma, we can now prove our main result. Proof of Theorem 3.1. Let  X   X  Dirichlet(  X  ), and also let  X  &gt; 0. Setting x =  X  X  i in eq. (S3) gives:
E[log  X (  X  X  i )]  X  log  X (  X  E[  X  i )]) + log E[  X  i ]  X  E[log  X  All the expected values on the right hand side of this inequality can be computed analytically for Dirichlet random variables. In particular, let  X  0 = P i  X  i . Then: The theorem follows from substituting these statistics into eq. (S4).
 How tight is the bound in Lemma S1.2? The question is important because we use this inequality in conjunc-tion with the variational approximation in eq. (3) to generate a lower bound on the log-likelihood. Here we make two useful observations.
 First, we note that the bound in Lemma S1.2 is exquisitely tuned to the shape of the function log  X ( x ) and the location of the expected value E[ x ]. To see this, we provide an alternate derivation of the result in eq. (S3). We begin by appealing to the concavity of f ( x ), established in Lemma S1.1; from this we obtain the upper bound which holds for all values x 0 &gt; 0. Now we recall the definition of f ( x ) in Lemma S1.1 to obtain an upper bound on log  X ( x ). Specifically we have: log  X ( x ) = f ( x )  X  log x + x log x, (S9) Figure S2 illustrates this upper bound on log  X ( x ) for different values of x 0 ; note especially its tightness in the vicinity of x 0 . The upper bound on E[log  X ( x )] in eq. (S3) is based on choosing the best approximation from this family of upper bounds; it is easy to show that this occurs at x 0 = E[ x ]. Thus we obtain the bound in Lemma S1.2 by taking expectations of both sides of eq. (S10) and setting x 0 = E[ x ].
 Second, we note that the upper bound in eq. (S3) re-duces to an equality in the limit of vanishing variance. In particular, this is the limit in which E[log x ]  X  log E[ x ] and also E[ x log x ]  X  E[ x ] log E[ x ]. In this limit, the last four terms on the right hand side of eq. (S3) vanish, and we recover the result E[log  X ( x )] = log  X (E[ x ]). In general, we expect factorized approx-imations such as eq. (3) to work well in the regime where the true posterior is peaked around its mean value. In this regime, we also expect the bound in eq. (S3) to be tight. Put another way, if it is sufficiently accurate to proceed with the factorized approximation in eq. (3), then we do not expect to incur much addi-tional loss from the inequality in Lemma S1.2. Here we briefly describe our scheme for parallelizing the recursive procedures in Algorithm 1. In practice, we obtain a significant speedup from this parallel im-plementation of tiLDA. This parallelization was nec-essary, for example, to obtain the results in section 4. One naive manner of parallelization would simply be to allocate the inference for different top-level categories to different threads of execution. This approach, how-ever, has two obvious limitations. First, inference in different categories may require different amounts of time; if the goal is to minimize idle CPU cycles, then we must more intelligently distribute the overall work-load across different threads. Second, the number of parallel threads at our disposal may greatly exceed the number of top-level categories. (For example, the BlackHatWorld corpus has only three top-level cate-gories.) In this case, the naive approach to paralleliza-tion hardly makes the best use of available resources. In the following, we describe a parallel implementation of tiLDA that overcomes both these limitations. Our parallel implementation is based on two key ideas. The first is to partition the algorithm into three types of tasks X  X TART, DOCUMENT, and REPEAT X  which we explain below. The second is to maintain a queue of these tasks and create multiple threads that execute tasks from this queue.
 A START task is associated with every internal node in the corpus hierarchy. The task begins by initializ-ing the node X  X  parameters  X  t and  X  t . After this ini-tialization, the task then enqueues a new START task for each subcategory of the node and a DOCUMENT task for each document of the node. In Algorithm 1, the START task corresponds to lines 5 X 10.
 A DOCUMENT task is associated with each document in the corpus. This task is responsible for optimizing the variational parameters  X  d and  X  dn for documents given their observed words and (currently inferred) pa-rameters of their parents. In Algorithm 1, the DOC-UMENT task corresponds to the procedure called in line 10.
 A REPEAT task is issued at each internal node in the corpus whenever all the tasks for the node X  X  chil-dren complete. The REPEAT task is responsible for maximizing the lower bound on the log-likelihood L 0 with respect to the node X  X  parameters. We mark the node as complete if the lower bound does not improve over its value from the previous REPEAT task at the node. Otherwise, we enqueue START and DOCU-MENT tasks again for the node X  X  children. The RE-PEAT task corresponds to executing lines 11 X 13 and then lines 6 X 10.
 The overall algorithm begins with a START task at the root node and ends in a REPEAT task at the root node when the lower bound L 0 can no longer be improved. The Freelancer corpus collects seven years of job post-ings from Freelancer.com, one of the largest crowd-sourcing sites on the Internet. The postings can be grouped by advertiser to form the three-level hierar-chy shown in Fig. 1. In this hierarchy, tiLDA models the advertisers as second-level interior nodes and the job postings as third-level leaf nodes.
 The BlackHatWorld corpus collects over two years of postings from the  X  X lackHatWorld X  Internet forum. This data set was collected as part of a larger ef-fort (Motoyama et al., 2011) to examine the social networks that develop in underground forums among distrustful parties. The BlackHatWorld forum evolved to discuss abusive forms of Internet marketing, such as bulk emailing (spam). The discussions are organized into the multi-level hierarchy shown in Fig. 2. We treat the threads in these subforums as documents for topic modeling. (We do not consider individual posts within threads as documents because they are quite short.) We preprocessed these two corpora in the same way, removing stopwords from a standard list (Lewis et al., 2004), discarding infrequent words that appeared in fewer than 6 documents, and stemming the words that remain. In both data sets, we also pruned  X  X arren X  branches of the hierarchy: specifically, in the Free-lancer corpus, we pruned advertisers with fewer than 20 job postings, and in the BlackHatWorld corpus, we pruned subforums with fewer than 60 threads.
 The multi-level tiLDA models can also be used to an-alyze hierarchical corpora in ways that go beyond the discovery of global topics. Recall that each tiLDA model yields topic proportions  X  t and a concentration parameter  X  t for each category of the corpus. We can analyze these proportions and parameters for further insights into hierarchical corpora. In general, they pro-vide a wealth of information beyond what can be dis-cerned from (say) ordinary LDA.
 Consider for example the Freelancer corpus. In this corpus, the categories of tiLDA represent advertisers, and the topic proportions of these categories can be used to profile the types of jobs that advertisers are trying to crowdsource. Summing these topic propor-tions over the corpus gives an estimate of the number of advertisers for each job type. Table S1 shows the results of this estimate: it appears that nearly one-third of advertisers on Freelancer.com are commission-ing abuse-related jobs, and of these jobs, the majority appear to involve some form of SEO.
 We gain further insights by analyzing the concentra-tion parameters of individual advertisers. For exam-ple, the advertiser with the maximum concentration parameter (  X  t = 4065 . 00) on Freelancer.com commis-sioned 34 projects, among which 32 have nearly the exact same description. We also observe that adver-tisers with lower concentration parameters tend to be involved in a wider variety of projects.
 On the BlackHatWorld corpus, the topic proportions and concentration parameters of categories generally reflect the titles of their associated subforums. For ex-ample, the highest topic proportion (0.48) for  X  X mail Marketing X  belongs to the subforum on  X  X mail Mar-keting and Opt-In Lists, X  and the highest topic pro-portion (0.59) for  X  X logging X  belongs to the  X  X log-ging X  subforum. The highest concentration parameter (29 . 62) belongs to the  X  X oney, and Mo Money X  subfo-rum. This is not surprising as this subforum itself has only four subforums as children, all of which are nar-rowly focused on specific revenue streams; see Fig. 2. Abramowitz, M. and Stegun, I. A. Handbook of Math-ematical Functions with Formulas, Graphs, and
Mathematical Tables . Dover, New York, ninth dover printing, tenth GPO printing edition, 1964.
 Lewis, D. D., Yang, Y., Rose, T. G., and
Li, F. SMART stopword list. http: //jmlr.csail.mit.edu/papers/volume5/ lewis04a/a11-smart-stop-list/english.stop , April 2004.
 Motoyama, M., McCoy, D., Levchenko, K., Savage, S., and Voelker, G. M. An Analysis of Underground Fo-rums. In Proceedings of the 2011 ACM SIGCOMM
Internet Measurement Conference (IMC) , pp. 71 X 
 Do-kyum Kim dok027@cs.ucsd.edu Geoffrey M. Voelker voelker@cs.ucsd.edu Lawrence K. Saul saul@cs.ucsd.edu In the last decade, probabilistic topic models have emerged as a leading framework for analyzing and organizing large collections of text (Blei &amp; Lafferty, 2009). These models represent documents as  X  X ags of words X  and explain frequent co-occurrences of words as evidence of topics that run throughout the corpus. The first properly Bayesian topic model was latent Dirich-let allocation (LDA) (Blei et al., 2003). A great deal of subsequent work has investigated hierarchical ex-tensions of LDA, much of it stemming from interest in nonparametric Bayesian methods (Teh et al., 2006). In these models, topics are shared across different but related corpora (or across different parts of a single, larger corpus). One challenge of topic models is that exact inference is intractable. Thus, it remains an ac-tive area of research to devise practical approximations for computing the statistics of their latent variables. In this paper we are interested in the topic modeling of corpora whose documents are organized in a multi-level hierarchy. Often such structure arises from prior knowledge of a corpus X  X  subject matter and readership. For example, news articles appear in different sections of the paper (e.g., business, politics), and these sec-tions are sometimes further divided into subcategories (e.g., domestic, international). Our goal is to explore the idea that prior knowledge of this form, though nec-essarily imperfect and incomplete, should inform the discovery of topics.
 We explore a parametric model of such corpora, as-suming for simplicity that the number of topics is known or can be estimated by (say) cross-validation. The models that we consider assign topic proportions to each node in a corpus X  X  hierarchy X  X ot only the leaf nodes that represent documents, but also the ancestor nodes that reflect higher-level categories. Conditional dependence ensures that nearby nodes in the hierarchy have similar topic proportions. In particular, the topic proportions of lower-level categories are on average the same as their parent categories. However, useful vari-ations naturally arise as one descends the hierarchy. As we discuss later, these models can also be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs) (Teh et al., 2006). Our main contributions are two. First, we devise a new variational approximation for inference in these models. Based on a previously unexploited inequality, the approximation enables us to compute a rigorous lower bound on the likelihood in Bayesian networks where Dirichlet random variables appear as the chil-dren of other Dirichlet random variables. We believe that this simple inequality will be of broad interest. Our second contribution is to demonstrate the large-scale viability of our approach. Our interest in this subject arose from the need to analyze two sprawling, real-world corpora from the field of computer secu-rity. The first is a seven-year collection of over 350,000 job postings from Freelancer.com , a popular Web site for crowdsourcing. We view this corpus as a three-layer tree in which leaf nodes represent the site X  X  job postings and interior nodes represent the active buy-ers (over 6,000 of them) on the site; see Fig. 1. The second corpus is derived from the BlackHatWorld In-ternet forum, in which users create and extend threads in a deep, content-rich hierarchy of pre-defined subcat-egories; see Fig. 2. Our results break new ground for hierarchical topic models in terms of both the breadth (i.e., number of interior nodes) and depth (i.e., number of levels) of the corpora that we consider. Moreover, it is our experience that sampling-based approaches for HDPs (Teh et al., 2006) do not easily scale to corpora of this size, while other variational approaches (Teh et al., 2008; Wang et al., 2011; Wang &amp; Blei, 2012; Bryant &amp; Sudderth, 2012) have not been demonstrated (or even fully developed) for hierarchies of this depth. The organization of this paper is as follows. In sec-tion 2, we describe our probabilistic models for hi-erarchical corpora and review related work. In sec-tion 3, we develop the variational approximation for inference and parameter estimation in these models. In section 4, we evaluate our approach on several cor-pora and compare the results to existing implemen-tations of HDPs. Finally, in section 5, we conclude and discuss possible extensions of interest. The sup-plementary material for our paper contains a full proof of the key inequality for variational inference, a brief description of our large-scale (parallelized) implemen-tation, and additional background and results on the Freelancer and BlackHatWorld corpora. Figs. 1 and 2 illustrate the types of structure we seek to model in hierarchical corpora. This structure is most easily visualized as a tree in which the root node rep-resents the corpus as a whole, the children of the root node represent top-level categories, the interior nodes represent subcategories of their parents, and the leaf nodes represent individual documents. In this section we describe a probabilistic generative model for cor-pora of this form and discuss its relation to previous work in topic modeling. 2.1. Model Our model is essentially an extension of LDA to ac-count for the tree structure in Figs. 1 and 2. In LDA, each document d is modeled by topic proportions  X  d , which are mixture weights over a finite set of K topics. In our approach, we model not only the documents in this way X  X he leaves of the tree X  X ut also the cate-gories and subcategories that appear at higher levels in the tree. Thus for each (sub)category t , we model its topic proportions by a latent Dirichlet random vari-able  X  t , and we associate one of these variables to each non-leaf node in the tree. We use  X  0 to denote the topic proportions of the root node in the tree (i.e. the corpus-wide topic proportions), and we sample these from a symmetric Dirichlet prior  X  .
 The topic proportions of the corpus, its categories, subcategories, and documents are the latent Dirich-let variables in our model. It remains to specify how these variables are related X  X n particular, how topic proportions are inherited from parent to child as one traverses the trees in Figs. 1 and 2. We parameterize this conditional dependence by associating a (scalar) concentration parameter  X  t to each category t . The parameter  X  t governs how closely the topic propor-tions of category t are inherited by its subcategories and documents; in particular, small values of  X  t al-low for more variance, and large values for less. More formally, let  X  ( t ) denote the parent category of the category t . Then we stipulate: Likewise, documents inherit their topic proportions from parent categories in the same way: where  X  ( d ) in the above equation denotes the parent category of document d .
 The final assumption of our model is one of conditional independence: namely, that the topic proportions of subcategories are conditionally independent of their  X  X ncestral X  categories given the topic proportions of their parent categories. With this assumption, we ob-tain the simple generative model of hierarchical cor-pora shown in Fig. 3. To generate an individual docu-ment, we begin by recursively sampling the topic pro-portions of its (sub)categories conditioned on those of their parent categories. Finally, we sample the words of the document, conditioned on its topic proportions, in the same manner as LDA. In what follows we refer to this model as tree-informed LDA, or simply tiLDA. In general, it is a bit unwieldy to depict the Bayesian network for topic models of this form. However, a special case occurs when the corpus hierarchy has uni-form depth X  X hat is, when all documents are attached to subcategories at the same level. Fig. 4 shows the graphical model when all documents in the corpus are attached (for example) to third-level nodes. 2.2. Related Work Our model can be viewed as a generalization of certain previous approaches and a special instance of others. Consider, for example, the special case of tiLDA for a  X  X lat X  corpus, where all the documents are attached directly to its  X  X oot. X  This case of tiLDA corresponds to LDA with an asymmetric Dirichlet prior over topic proportions. Wallach et al. (2009a) showed how to perform Gibbs sampling in such models and demon-strated their advantages over LDA with a symmetric Dirichlet prior.
 Our approach also draws on inspiration from hierar-chical Dirichlet processes (HDPs) (Teh et al., 2006). In tiLDA, as in HDPs, the sample from one Dirich-let distribution serves as the base measure for another Dirichlet distribution. HDPs are a nonparametric gen-eralization of LDA in which the number of topics is potentially unbounded and can be learned from data. We can view the generative model of tiLDA as a spe-cial case of multi-level HDPs whose base measure is finite (thus only allowing for a finite number of top-ics). Though tiLDA does not possess the full richness of HDPs, our results will show that for some applica-tions it is a compelling alternative.
 Gibbs sampling is perhaps the most popular strategy for inference and learning in hierarchical topic models. The seminal work by Teh et al. (2006) developed a Gibbs sampler for HDPs of arbitrary depth and used it to learn a three-level hierarchical model of 160 papers from two distinct tracks of the NIPS conference. We note also that Du et al. (2010) developed a collapsed Gibbs sampling algorithm for a three-level hierarchical model similar in spirit to ours. The drawback to Gibbs sampling is its slowness; it is not currently a viable approach for large corpora.
 Many researchers have pursued variational inference in HDPs as a faster, cheaper alternative to Gibbs sam-pling. Teh et al. (2008) developed a framework for collapsed variational inference in two-level (but not arbitrarily deep) HDPs, and later Sato et al. (2012) proposed a related but simplified approach. Yet an-other framework for variational inference was devel-oped by Wang et al. (2011), who achieved speedups with online updates. While the first variational meth-ods for HDPs truncated the number of possible topics, two recent papers have investigated online approaches with dynamically varying levels of truncation (Wang &amp; Blei, 2012; Bryant &amp; Sudderth, 2012). There have been many successful applications of variational HDPs to large corpora; however, we are unaware of any ac-tual applications to hierarchical corpora (i.e., involving HDPs that are three or more levels deep). It seems fair to say that variational inference in nonparamet-ric Bayesian models involves many complexities (e.g., auxiliary variables, stick-breaking constructions, trun-cation schemes) beyond those in parametric models. We note that even for two-level HDPs, the variational approximations already require a good degree of clev-erness (sometimes just to identify the latent variables). The above considerations suggest regimes where an approach such as tiLDA may compete favorably with nonparametric HDPs. In this paper, we are interested in topic models of large corpora with known hierar-chical structure, sometimes many levels deep. In ad-dition, the corpora are static, not streaming; thus we are not attempting to model the introduction of new (or a potentially unbounded number of) topics over time. We seek a model richer than LDA, one that can easily corporate prior knowledge in the form of Figs. 1 and 2, but with a minimum of additional complexity. (Here it bears reminding that LDA X  X n its most ba-sic form X  X till remains a wildly popular and successful model.) We shall see that tiLDA fits the bill perfectly in this regime. In this section we develop the algorithms for inference and learning in tiLDA. Our large-scale implementation is described in the paper X  X  supplementary material. 3.1. Variational Inference The problem of inference in tiLDA is to compute the posterior distribution over the model X  X  latent variables given the observed words in the corpus. In tiLDA, the latent variables are the topic proportions  X  t of each category (or subcategory) t , the topic proportions  X  d of each document d , the topic z dn associated with each word w dn , and the multinomial parameters  X  k for each topic. Exact inference is not possible; approximations are required. Here we pursue a variational method for approximate inference (Jordan et al., 1999) that gen-eralizes earlier approaches to LDA (Blei et al., 2003). The variational method is based on a parameterized approximation to the posterior distribution over the model X  X  latent variables. The approximation takes the fully factorized form: where the parameters  X  t ,  X  d ,  X  dn , and  X  k are varied to make the approximation as accurate as possible. The component distributions in this variational approxima-tion are the exponential family distributions: Figs. 4 and 5 contrast the graphical models for the true posterior and its variational approximation.
 The variational parameters  X  t ,  X  d ,  X  dn , and  X  k are found by attempting to minimize the Kullback-Leibler divergence between the approximation in eq. (3) and the true posterior distribution of the model. It can be shown that this is equivalent to maximizing a lower bound L  X  log p ( w |  X , X , X  ) on the marginal log-likelihood of the corpus. This lower bound is given by: where E q denotes the expectation with respect to the variational distribution and H ( q ) denotes its entropy. So far we have developed the variational approxima-tion for our model by following exactly the same ap-proach used in LDA. The lower bound in eq. (4), however, cannot be computed analytically, even for the simple factorized distribution in eq. (3). In particular, new terms arise from the expectation E q [log p (  X ,z,w, X  |  X , X , X  )] that are not present in the variational approximation for LDA.
 Let us see where these terms arise. Consider the model X  X  prior distribution over latent topic proportions for each subcategory t and document d in the corpus: p (  X  |  X , X  )  X  Y In eq. (5), we have again used  X  ( t ) and  X  ( d ) to denote the parent categories of t and d , respectively, in the tree. Note that both terms in this prior distribution express conditional dependencies between Dirichlet variables at adjacent levels in the tree. In eq. (4), they give rise to averages such as E q [log p (  X  t |  X   X  ( t ) that cannot be analytically computed.
 In this paper, we do not have space for a complete derivation of the log-likelihood bound in our model. However, the extra steps beyond LDA are essentially applications of the following theorem.
 Theorem 3.1. Let  X   X  Dirichlet(  X  ) , and let  X  &gt; 0 . As shorthand, let  X  0 = P i  X  i . Then: E [log  X (  X  X  i )]  X  log  X (  X  E[  X  i ]) +  X  (1  X  E[  X  i ]) / X  where E[  X  i ] =  X  i / X  0 and  X (  X  ) and  X (  X  ) are respectively the gamma and digamma functions.
 A proof of this theorem is given in the paper X  X  sup-plement. Note especially the direction of the bound. The function log  X (  X  ) is convex, and hence a naive ap-plication of Jensen X  X  inequality to the left hand side of the equation yields the lower bound E[log  X (  X  X  i )]  X  log  X (  X  E[  X  i ]). It is the additional terms on the right hand side of the equation that establish the theorem X  X  upper bound. The direction of inequality is crucial in the context of variational inference, where the upper bound is needed to maintain an overall lower bound on the log-likelihood. Thus it can be used to compute a looser (but still rigorous) lower bound L 0  X  L on the log-likelihood in terms of the model X  X  variational parameters. We shall see that this surrogate bound remains highly effective for inference and learning. We obtain the best approximation in the form of eq. (3) by maximizing L 0 with respect to the vari-ational parameters  X  ,  X  and  X  . In practice, we per-form the optimization by coordinate ascent in repeated bottom-up sweeps through the corpus hierarchy. Each sweep begins by updating the parameters  X  d and  X  d attached to individual documents; these updates take essentially the same form as in LDA. Then, once these parameters have converged, we turn to updating the variational parameters  X  t attached to different-level categories; these maximizations are performed using variants of Newton X  X  method. The bottom-up sweep continues through the different levels of the corpus un-til we reach the root of the corpus. Finally, the whole procedure repeats until L 0 converges. 3.2. Variational Learning We can either fix the model parameters  X  ,  X  and  X  or learn them from data. For the latter, we use the lower bound from section 3.1 as a surrogate for maximum likelihood estimation. The variational EM algorithm alternates between computing the best factorized ap-proximation in eq. (3) and updating the model param-eters to maximize the lower bound L 0 . The first of these steps is the variational E-step; the second is the variational M-step. In the M-step we update the model parameters by block coordinate ascent. In particular, we use Newton X  X  method to update the concentration parameter  X  t associated to each category t as well as  X  and  X  at the root of the corpus.
 It is useful to view the variational EM algorithm as a double-optimization over both the variational pa-rameters (E-step) and the model parameters (M-step). This view naturally suggests an interweaving of the two steps, and, in fact, this is how we implement the algorithm in practice; see Algorithm 1. In this section we evaluate tiLDA on several corpora and compare its results where possible to existing im-plementations of HDPs. We followed more or less stan-dard procedures in training. The variational EM algo-rithms for tiLDA and HDPs were iterated until conver-Algorithm 1 The variational EM algorithm for tiLDA. The algorithm begins in main, then invokes OPT SUBTREE recursively for each category. At the deepest level of recursion, OPT DOCUMENT infers the hidden variables of documents given their words and prior on topic proportions (just as in LDA). gence in their log-likelihood bounds. HDPs estimated by Gibbs sampling were trained for 5,000 iterations. Since the log-likelihood of held-out data cannot be computed exactly in topic models, we use a method known as document completion (Wallach et al., 2009b) to evaluate each model X  X  predictive power. First, for each trained model, we estimate a set of topics  X  and category topic proportions  X  t on this set of topics. Then we split each document in the held-out set into two parts; on the first part, we estimate document topic proportions  X  d , and on the second part, we use these proportions to compute a per-word likelihood. This approach permits a fair comparison of different (or differently trained) models.
 The topic proportions of held-out documents were computed as follows. For variational inference, we sim-ply estimated  X  d by E q [  X  d ]. In the HDPs trained by Gibbs sampling, we sampled topic assignments z dn for each word in the first part of the document and com-of tokens assigned to k th topic in s th sample. Finally, we averaged  X  s dk over 2,000 samples after 500 iterations of burn-in. 4.1. Comparison to HDPs HDPs have been evaluated on several  X  X lat X  corpora, which in the manner of Figs. 1 X 2 we can visual-ize as two-level trees in which all documents are di-rectly attached to a single root node. In this sec-tion we compare the results from tiLDA and HDPs on three such corpora from the UCI Machine Learn-ing Repository (Frank &amp; Asuncion, 2010). These cor-pora are: (1) KOS X  X  collection of 3,430 blog arti-cles with 467,714 tokens and a 6,906-term vocabulary; (2) Enron X  X  collection of 39,861 email messages with roughly 6 million tokens and a 28,102-term vocabu-lary; (3) NYTimes X  X  collection of 300K news articles with a 102,660-term vocabulary. The full NYTimes corpus was too large for our experiments on (batch) HDPs so we extracted a subset of 80K articles with 26 million tokens.
 On the KOS, Enron, and NYTimes corpora we com-pared tiLDA to two publicly available batch imple-pling (Teh et al., 2006), the other based on variational methods (Wang et al., 2011). We denote the former by HDP-Gibbs and the latter by HDP-Variational. For all algorithms we used the same hyperparame-ters (  X  =  X  0 = 1) and the same symmetric Dirichlet prior on topics. We initialized HDP-Gibbs with 100 topics, and we experimented with three settings of the truncation parameters ( K,T ) in HDP-Variational, where K is the number of topics per corpus and T is the number of topics per document. These set-tings were ( K = 150 ,T = 15) as reported in previous work (Wang et al., 2011) as well as ( K = 300 ,T = 50) and ( K = 300 ,T = 100). For each corpus we only report the results from HDP-Variational for the best of these settings. In our experience, however, HDP-Variational was sensitive to these settings, exhibiting the same or more variance than tiLDA over widely different choices for its fixed number of topics.
 Figure 6 summarizes our experimental results. The error bars for tiLDA show the standard deviation in per-word log-likelihood over five different folds of each corpus. (In each experiment, one fold was held out for testing while the other four were used for train-ing.) Also shown are the range of results on these folds for HDP-Gibbs and HDP-Variational. On the smaller KOS and Enron corpora, we obtain our best HDP-Gibbs was too slow to train even on our subset of the NYTimes corpus. Comparing tiLDA and HDP-Variational, we find that the former does significantly better on the KOS and NYTimes corpora. On En-ron, the corpus which appears to contain the most topics, the order is reversed (but only provided that one explores the space of truncation parameters for HDP-Variational). Though one cannot conclude too much from three corpora, these results certainly es-tablish the viability and scalability of tiLDA. We now turn to the sorts of applications for which tiLDA was explicitly conceived. 4.2. Hierarchical Corpora In this section we demonstrate the benefits of tiLDA when it can exploit known hierarchical structure in corpora. We experimented on three corpora with such structure. These corpora are: (1) NIPS X  X  collec-with over 2 million tokens and a 13,649-term vocabu-lary; (2) Freelancer X  X  collection of 355,386 job post-ings by 6,920 advertisers, with over 16M tokens and a 27,600-term vocabulary, scraped from a large crowd-sourcing site; (3) BlackHatWorld X  X  collection of 7,143 threads from a previously underground Internet forum, with roughly 1.4M tokens and a 7,056-term vocabu-lary. More details and results on the Freelancer and BlackHatWorld corpora can be found in the supple-mentary material. We previously analyzed the Free-lancer corpus using LDA (Kim et al., 2011), but this earlier work did not attempt to model the authorship of job postings as we do here. The BlackHatWorld corpora was collected as part of a larger effort (Mo-toyama et al., 2011) to examine the social networks among distrustful parties in underground forums. We evaluated three-level tiLDA models on the NIPS and Freelancer corpora (Fig. 1) and five-level tiLDA models on the BlackHatWorld corpus (Fig. 2). For comparison we also evaluated two-level models of tiLDA that ignored the internal structure of these cor-pora. We adopted the same settings as in the previous section except that we also learned the models X  con-centration parameters  X  . Note that we do not have comparative results for multi-level HDPs on these cor-pora. We know of no Gibbs samplers for HDPs that would scale to corpora of this size and depth. Like-wise we know of no variational HDPs that have been implemented for general (multi-level) hierarchies. Figure 7 shows the results of these evaluations. The plot for the Freelancer corpus ( middle ) shows the av-erage and standard deviation of the per-word log-likelihood over five folds. The plots for the NIPS and BlackHatWorld corpora ( left and right ) show the average and standard deviation over five runs, where each run averaged the test results over folds. (We did this because the NIPS and BlackHatWorld corpora are much smaller, and the folds themselves exhibit large variance regardless of the settings.) The results in Figure 7 paint a consistent picture over a wide range of choices for the number of topics, K . In every set of experiments, the deep tiLDA models of hierarchical corpora outperform the flat ones. Over-all the results support the notion that deep tiLDA generalizes better for two reasons: first, because it can model different categories with different topic pro-portions, and second, because it shares information across different categories. These abilities guard, re-spectively, against the challenges of underfitting and overfitting the data.
 We also examined the topics learned by the deep tiLDA models with the highest held-out likelihoods. On the Freelancer corpus, which consists of job post-ings, these topics can be interpreted as different job types (Kim et al., 2011). Table 1 shows four of the more pernicious job types on Freelancer.com identified by discovered topics. The  X  X SN (Online Social Net-work) Linking X  topic describes jobs to generate friends and fans on sites such as Facebook and Twitter. The  X  X d Posting X  topic describes jobs to post classified ads on sites such as Craigslist. Many other jobs are re-lated to search engine optimization (SEO). The  X  X EO Content Generation X  topic describes jobs to generate keyword-rich articles that drive traffic from search en-gines. Likewise, the  X  X EO Link Building X  topic de-scribes jobs to increase a Web site X  X  PageRank (Brin &amp; Page, 1998) by adding links from blogs and forums. On the BlackHatWorld corpus, the topics discovered by tiLDA relate to different types of Internet market-ing. Table 2 shows four particularly interpretable top-ics. The  X  X mail Marketing X  topic describes strategies for bulk emailing (spam). The  X  X oogle Adsense X  topic describes ways for online publishers (e.g., bloggers) to earn money by displaying ads suggested by Google on their Web sites. The  X  X ffiliate Program X  topic de-scribes ways to earn commissions by marketing on be-half of other merchants. Finally, the  X  X logging X  topic describes the use of blogs for Internet marketing. In this paper we have explored a generalization of LDA for hierarchical corpora. The parametric model that we introduce can also be viewed as a finite-dimensional HDP. Our main technical contribution is theorem 3.1, which has many potential uses in graphical models with latent Dirichlet variables. Our main empirical contribution is a parallel implementation that scales to very large corpora and deep hierarchies. Our re-sults on the Freelancer and BlackHatWorld corpora illustrate two real-world applications of our approach. Unlike tiLDA, nonparametric topic models can infer the number of topics from data and grow this number as more data becomes available. But this advantage of HDPs does not come without various complexities. Variational inference in tiLDA does not require stick-breaking constructions or truncation schemes, and it generalizes easily to hierarchies of arbitrary depth. For many applications, we believe that tiLDA provides a compelling alternative to the full generality of HDPs. The approximations we have developed for tiLDA may also be useful for truncated versions of nonparametric models (Kurihara et al., 2007).
 We note one potential direction for future work. In this paper, we have studied a batch framework for variational inference. Online approaches, like those recently explored for LDA (Hoffman et al., 2010) and HDPs (Wang et al., 2011; Wang &amp; Blei, 2012; Bryant &amp; Sudderth, 2012), also seem worth exploring for tiLDA. Such approaches may facilitate even larger and more diverse applications.
 We thank the reviewers for helpful comments. This work was supported in part by ONR MURI grant N000140911081 and NSF grant NSF-1237264.
 Asuncion, A., Welling, M., Smyth, P., and Teh, Y. W. On smoothing and inference for topic models. In
Proceedings of the Twenty-Sixth Conference on Un-certainty in Artificial Intelligence (UAI) , 2009. Blei, D. M. and Lafferty, J. Topic models. In Text
Mining: Theory and Applications . Taylor and Fran-cis, London, UK, 2009.
 Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent
Dirichlet allocation. The Journal of Machine Learn-ing Research , 3:993 X 1022, March 2003.
 Brin, S. and Page, L. The anatomy of a large-scale hypertextual web search engine. In Proceedings of the Seventh International Conference on the World Wide Web , pp. 107 X 117, 1998.
 Bryant, M. and Sudderth, E. Truly nonparametric online variational inference for hierarchical Dirichlet processes. In Bartlett, P., Pereira, F. C. N., Burges, C. J. C., Bottou, L., and Weinberger, K. Q. (eds.),
Advances in Neural Information Processing Systems 25 , pp. 2708 X 2716. 2012.
 Du, L., Buntine, W., and Jin, H. A segmented topic model based on the two-parameter Poisson-Dirichlet process. Machine Learning , 81:5 X 19, 2010.
 Frank, A. and Asuncion, A. UCI machine learning repository, 2010.
 Hoffman, M., Blei, D., and Bach, F. Online learn-ing for latent Dirichlet allocation. In Lafferty, J.,
Williams, C. K. I., Shawe-Taylor, J., Zemel, R. S., and Culotta, A. (eds.), Advances in Neural Infor-mation Processing Systems 23 , pp. 856 X 864. 2010. Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and
Saul, L. K. An Introduction to variational methods for graphical models. Machine Learning , 37(2):183 X  233, November 1999.
 Kim, D.-k., Motoyama, M., Voelker, G. M., and Saul,
L. K. Topic modeling of freelance job postings to monitor Web service abuse. In Proceedings of the 4th ACM Workshop on Security and Artificial In-telligence (AISec-11) , pp. 11 X 20, 2011.
 Kurihara, K., Welling, M., and Teh, Y. W. Col-lapsed variational Dirichlet process mixture models. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence , 2007.
 Motoyama, M., McCoy, D., Levchenko, K., Savage, S., and Voelker, G. M. An analysis of underground fo-rums. In Proceedings of the 2011 ACM SIGCOMM
Internet Measurement Conference (IMC-11) , pp. 71 X 80, 2011.
 Sato, I., Kurihara, K., and Nakagawa, H. Practical collapsed variational Bayes inference for hierarchi-cal Dirichlet process. In Proceedings of the 18th
ACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD-12) , August 2012.
 Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei,
D. M. Hierarchical Dirichlet processes. Journal of the American Statistical Association , 101(476): 1566 X 1581, December 2006.
 Teh, Y. W., Kurihara, K., and Welling, M. Col-lapsed variational inference for HDP. In Platt, J. C.,
Koller, D., Singer, Y., and Roweis, S. (eds.), Ad-vances in Neural Information Processing Systems 20 , pp. 1481 X 1488. 2008.
 Wallach, H., Mimno, D., and McCallum, A. Rethink-ing LDA: why priors matter. In Bengio, Y., Schu-urmans, D., Lafferty, J., Williams, C. K. I., and Culotta, A. (eds.), Advances in Neural Information Processing Systems 22 , pp. 1973 X 1981. 2009a.
 Wallach, H., Murray, I., Salakhutdinov, R., and
Mimno, D. Evaluation methods for topic mod-els. In Bottou, L. and Littman, M. (eds.), Proceed-ings of the 26th International Conference on Ma-chine Learning (ICML-09) , pp. 1105 X 1112, Mon-treal, June 2009b. Omnipress.
 Wang, C., Paisley, J., and Blei, D. Online variational inference for the hierarchical Dirichlet process. In
Proceedings of the Fourteenth International Confer-ence on Artificial Intelligence and Statistics , 2011. Wang, C. and Blei, D. Truncation-free online varia-tional inference for Bayesian nonparametric models. In Bartlett, P., Pereira, F. C. N., Burges, C. J. C.,
Bottou, L., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 25 , pp.
