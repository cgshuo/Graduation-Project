 Dimensionality reduction plays an important role in many data mining applications involving high-dimensional data . Many existing dimensionality reduction techniques can be formulated as a generalized eigenvalue problem, which does not scale to large-size problems. Prior work transforms the generalized eigenvalue problem into an equivalent leas t squares formulation, which can then be solved efficiently. However, the equivalence relationship only holds under cer -tain assumptions without regularization, which severely l im-its their applicability in practice. In this paper, an efficie nt two-stage approach is proposed to solve a class of dimen-sionality reduction techniques, including Canonical Corr e-lation Analysis, Orthonormal Partial Least Squares, Linea r Discriminant Analysis, and Hypergraph Spectral Learning. The proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality. The main contributions of this paper include (1) we rigorously estab -lish the equivalence relationship between the proposed two -stage approach and the original formulation without any as-sumption; and (2) we show that the equivalence relationship still holds in the regularization setting. We have conducte d extensive experiments using both synthetic and real-world data sets. Our experimental results confirm the equivalence relationship established in this paper. Results also demon -strate the scalability of the proposed two-stage approach. H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithm Dimensionality reduction, generalized eigenvalue proble m, least squares, regularization, scalability
Recent technological innovations have allowed us to col-lect massive amounts of data with a large number of features. One of the key issues in such data analysis is the curse of dimensionality [5], i.e., an enormous number of samples is required to perform accurate prediction on problems with large numbers of features. Dimensionality reduction, whic h extracts a small number of features by removing the irrele-vant, redundant, and noisy information, is an effective way to overcome the curse of dimensionality. Many dimension-ality reduction algorithms have been proposed in the past, including Canonical Correlation Analysis (CCA) [15], Par-tial Least Squares (PLS) [21], Linear Discriminant Analysi s (LDA) [6] and Hypergraph Spectral Learning (HSL) [24]. A common characteristic of these algorithms is that they can be formulated as a generalized eigenvalue problem. Al-though well-established algorithms in numerical linear al ge-bra exist to solve generalized eigenvalue problems [12, 22] , they are in general computationally expensive to solve and hence may not scale to large-size problems.

There have been several recent attempts to improve the scalability of dimensionality reduction techniques [8, 9, 16, 24, 25, 26, 32, 33]. The key idea is to transform the gen-eralized eigenvalue problem into an equivalent least squar es formulation, which can be solved efficiently using existing algorithms such as the iterative conjugate gradient algo-rithm [12, 19, 20]. In particular, an equivalent least squar es formulation has been developed for a class of dimensional-ity reduction techniques in [26]. However, it suffers from several drawbacks which limits its applicability in practi ce. First, the equivalent transformation relies on a key assump -tion that all the data points are linear independent. This assumption tends to hold for high-dimensional data, but it i s likely to fail for (relatively) low-dimensional data. Seco ndly, the equivalence relationship between the least squares for -mulation and the original formulation does not hold when the regularization is employed. However, regularization h as been shown to be critical in many data mining and machine learning algorithms including support vector machines [23 ].
In this paper, we propose an efficient two-stage approach to solve a class of dimensionality reduction techniques, in -cluding CCA, PLS, LDA and HSL. In the first stage we solve a least squares problem using the iterative conjugate gradient algorithm [12, 19, 20]. The distinct property of this stage is its low time complexity. In the second stage, the original data is projected onto a low-dimensional space , and then we solve a generalized eigenvalue problem with a significantly reduced size. The proposed two-stage approac h scales linearly in terms of both the sample size and data di-mensionality, thus applicable for large-size problems. Th e main contributions of this paper include: We have conducted extensive experiments to evaluate the proposed two-stage approach using both synthetic and real-world benchmark data sets. Our experimental results con-firm the equivalence relationship established in this paper . Results also demonstrate the scalability of the proposed tw o-stage approach.
 Organization The rest of the paper is organized as fol-lows. Section 2 briefly reviews the class of dimensionality reduction techniques discussed in the paper. In Section 3, we present the two-stage approach, and establish the equiv-alence relationship. We further extend the two-stage ap-proach to the regularization setting in Section 4. A compre-hensive empirical study is reported in Section 5. Finally, w e conclude in Section 6.
 Notations Throughout the paper, all matrices are bold-face uppercase, and vectors are boldface lowercase. n is the number of samples, d is the data dimensionality, and k is the number of classes (or labels). The i th sample is denoted as x  X  R d , and its corresponding label is denoted as y i  X  R X = [ x 1 , x 2 , , x n ]  X  R d  X  n represents the data matrix, and Y = [ y 1 , y 2 , , y n ]  X  R k  X  n is the matrix representa-tion for label information. S  X  R n  X  n is a symmetric and positive semi-definite matrix. I p is the p -by-p identity ma-trix, and 1 p is a vector of all ones with length p . Note that the subscript p may be omitted when the size is clear from the context.
The class of generalized eigenvalue problems considered in this paper exhibit the following form: where S  X  R n  X  n is a symmetric and positive semi-definite matrix. In general, we are interested in the principal eigen -vectors corresponding to nonzero eigenvalues. The general -ized eigenvalue problem in Eq. (1) is often reformulated as the following eigenvalue problem where ( XX T )  X  is the Moore-Penrose pseudoinverse of XX In addition, the generalized eigenvalue problem in Eq. (1) can also be formulated as an optimization problem: In the following derivation, we generally use the formulati on in Eq.(2).

Many existing dimensionality reduction techniques exhibi t the form in Eq. (1) or (2). In particular, the matrix S can be represented in the following form: where H  X  R n  X  k is often constructed from the label infor-mation in supervised learning.

To control the model complexity and avoid the singularity of XX T , a regularization term  X  I d with  X  &gt; 0 is added to XX T in Eq. (1), leading to the following generalized eigenvalue problem:
We briefly review several algorithms involving the general-ized eigenvalue problem in the form of Eq. (1). Specifically, they include Canonical Correlation Analysis, Orthonormal -ized Partial Least Squares, Hypergraph Spectral Learning, and Linear Discriminant Analysis. For supervised learn-ing methods, the label information is encoded in the matrix Y = [ y 1 , y 2 , , y n ]  X  R k  X  n , where y i ( j ) = 1 if x to class j and y i ( j ) = 0 otherwise.
In Canonical Correlation Analysis (CCA) [4, 13, 15], two different representations, X and Y , of the same set of objects are given, and a projection is computed for each representa-tion such that the correlation coefficient is maximized in the dimensionality-reduced space, where w x  X  R d and w y  X  R k are projection vectors for X and Y , respectively. Assume that YY T is nonsingular. It can be verified that w x is the first principal eigenvector of the following generalized eigenvalue problem: Multiple projection vectors can be obtained simultaneousl y by computing the first  X  principal eigenvectors of the gener-alized eigenvalue problem in Eq. (7). It can be observed that CCA is in the form of the generalized eigenvalue problem in Eq. (1) with S = Y T ( YY T )  X  1 Y and H = Y T ( YY T )  X  1 / 2
Partial least squares (PLS) [29] is a family of methods for modeling relations between two sets of variables. In this pa -per, the Orthonormalized Partial Least Squares (OPLS) [2, 30], a popular variant of PLS, is studied. In contrast to CCA, OPLS computes orthogonal score vectors by maximiz-ing the covariance between X and Y . It solves the following generalized eigenvalue problem: It follows from Eq. (8) that orthonormalized PLS involves a generalized eigenvalue problem in Eq. (1) with S = Y T Y and H = Y T .
Hypergraph Spectral Learning (HSL) [24] is a dimension-ality reduction technique for multi-label classification. A hypergraph [1] is a generalization of the traditional graph in which the edges (a.k.a. hyperedges) are arbitrary non-empty subsets of the vertex set. HSL employs a hypergraph Algorithm 1 The Two-Stage Approach without Regular-ization Input: X , H Output: W
Stage 1: Solve the following least squares problem:
Stage 2: Compute  X  X = W T 1 X , and solve the following optimization problem:
Compute W = W 1 W 2 as the final solution. to capture the correlation information among different labe ls for improved classification performance in multi-label lea rn-ing. Specifically, HSL constructs a hyperedge for each la-bel, and includes all instances annotated with a common la-bel into one hyperedge, thus capturing their joint similari ty. Three different Laplacians have been proposed to capture the spectral properties of the hypergraph, including cliqu e expansion [1], star expansion [1] and Zhou X  X  Laplacian [34] . It has been shown that given the normalized Laplacian L for the constructed hypergraph, HSL involves the following generalized eigenvalue problem: It has been shown that for all three Laplacian matrices, the resulting matrix S is symmetric and positive semi-definite, and it can be represented as S = HH T , where H  X  R n  X  k Note that H can be constructed from the label information in Y explicitly without the matrix decomposition.
As a supervised dimensionality reduction technique, Lin-ear Discriminant Analysis (LDA) attempts to minimize the within-class variance while maximizing the between-class variance after the linear projection. It has been shown that the optimal linear projection consists of the top eigenvect ors of S  X  t S b corresponding to nonzero eigenvalues [11, 31], where S is the total covariance matrix and S b is the between-class covariance matrix. The matrices S t and S b are defined as follows: where c ( j ) is the centroid of the j th class and we assume that P n i =1 x i = 0. We also assume that the data matrix X is partitioned into k classes as X = [ X 1 , , X k ], where X class, n j is the size of the j th class, and P k j =1 n j length n j . Thus, we have where S j = 1 n for LDA. Therefore, S  X  t S b can also be formulated in the fol-lowing form: It can be verified that S = HH T , where
H = diag 1  X 
In this section, we present our two-stage approach and show that the two-stage approach is equivalent to the direct approach which solves the generalized eigenvalue problem i n Eq. (1) directly.
In the two-stage approach, we first solve a least squares problem by regressing X on H T . In other words, H T can be considered as the  X  X atent target X  encoded by the label information Y . After projecting the data matrix X onto the subspace, we solve the resulting generalized eigenvalu e problem by replacing the data matrix in Eq. (1) with the projected data matrix. Note that the data dimensionality is reduced dramatically after the projection, thus the result ing generalized eigenvalue problem in the second step can be solved efficiently. The two-stage approach is summarized in Algorithm 1.
In our implementation, we apply the LSQR algorithm [19, 20], a conjugate gradient method for solving the least squar es problem in the first stage. Previous studies have shown that LSQR is reliable even for ill-conditioned problems [20 ]. In addition, when the data matrix X is sparse, the least squares problem can be solved very efficiently using LSQR. Note that the computational cost of each iteration of LSQR is O (3 n + 5 d + 2 dn ) when X is dense or O (3 n + 5 d + 2 z ) when X is sparse, where z is the number of nonzero entries in X [20]. Since H T  X  R n  X  k , k least squares problems are solved simultaneously in the first stage of Algorithm 1, whic h implies that the total computational cost of the first stage i s O ( Nk (3 n +5 d +2 dn )) using LSQR when X is dense, where N is the total number of iterations. When the data matrix X is sparse, the cost of LSQR is reduced to O ( Nk (3 n +5 d +2 z )). In the second stage, the cost of computing  X  X is O ( ndk ) when X is dense or O ( kz ) when X is sparse. Since the size of is significantly reduced, the cost of solving the optimizati on problem is O ( nk 2 ) in the second stage. The cost of combin-ing W 1 and W 2 is O ( dk X  ), where  X  (  X   X  k ) is the number of final projection vectors. Therefore, the total computation al cost is O ( Nk (3 n + 5 d + 2 z ) + kz + nk 2 + dk 2 ) when X is sparse.
Next we rigorously prove the equivalence relationship be-tween the two-stage approach and the direct approach which solves the original eigenvalue problem in Eq.(2) directly.
Using the standard technique in linear algebra, the solu-tion to the least squares problem in Eq. (10) is Let the singular value decomposition (SVD) [12] of X be where U  X  R d  X  d and V  X  R n  X  n are orthogonal, U 1  X  R and V 1  X  R n  X  r have orthonormal columns,  X   X  R d  X  n and  X  1  X  R r  X  r are diagonal, and r = rank( X ). Then W 1 can be represented as Then  X  X can be represented as  X  and Thus, the optimization problem in Eq. (11) can be simplified into the following form: Denote Then the optimization problem in the second stage can be reformulated as follows: Let the compact SVD of A be rank( A ).

Based on the above definitions, the solution to the two-stage approach is summarized in the following theorem.
Theorem 1. The top  X  (  X   X  rank( A )) projection vectors computed by Eq. (11) are given by where ( U A  X   X  1 A )  X  consists of the first  X  columns of ( U Thus, the projection vectors computed by the two-stage ap-proach are When  X  = rank( A ) , W can be simplified as
Proof. Using the Lagrange dual function in optimiza-tion theory, the optimization problem in Eq. (20) can be reformulated as the following eigenvalue problem: Next we derive the eigendecomposition of the matrix AA T  X  AA T AA T as follows: where [ U A , U  X  A ]  X  R k  X  k is orthogonal. Thus, the eigenvec-tors corresponding to the top  X  eigenvalues are given by where U A  X  consists of the first  X  columns of U A . To ensure that the constraint in Eq. (20) is satisfied, we normalize the columns of W 2 without affecting the range space of W T 2 Combing Eqs. (17) and (22), we have When  X  = rank( A ), we have V A  X  = V A and W = U 1  X   X  1 This completes the proof of this theorem.

Note that the solution to the generalized eigenvalue prob-lem in Eq. (1) consists of the principal eigenvectors of matr ix ( XX T )  X  ( XHH T X T ). We follow [25] to derive the eigende-composition of ( XX T )  X  ( XHH T X T ) and show the equiva-lence relationship between the two-stage approach and the direct approach. The results are summarized in the follow-ing theorem:
Theorem 2. The eigenvectors corresponding to the top  X  (  X   X  rank( A )) eigenvalues of ( XX T )  X  ( XHH T X T ) are where V A  X  consists of the first  X  columns of V A . Thus, the two-stage approach produces the same solution as the di-rect approach which solves the original generalized eigenv alue problem directly.

Proof. Given V A  X  R r  X  t with orthonormal columns, an orthogonal matrix [12]. Hereafter, we denote [ V A , V V
We can decompose ( XX T )  X  ( XHH T X T ) as follows: It is clear that the eigenvectors corresponding to the top  X  eigenvalues of ( XX T )  X  ( XHH T X T ) are given by The equivalence relationship follows from Eqs. (23) and (26 ). This completes the proof of the theorem.

A consequence of Theorem 1 is that solving the optimiza-tion problem in Eq. (11) in the second stage amounts to computing the SVD of the matrix A . Note that A  X  R k  X  r where r = rank( X )  X  min { d, n } , thus the computational cost of the SVD of A is quite low. In practice we can per-form SVD on A directly instead of solving the optimization problem in Eq. (11).

Remark 1. A least squares formulation is proposed in [26] for a class of dimensionality reduction techniques wit h the same computational cost as the proposed two-stage ap-proach. However, the analysis in [26] assumes that the data matrix X is of full rank (before centering). This tends to fail for (relatively) low-dimensional data. In particular, whe n the number of data points is larger than the number of di-mensions, this assumption is likely to be violated. The two-stage algorithm proposed in this paper significantly improv es previous work by relaxing this assumption.
Regularization is commonly employed to control the model complexity and avoid overfitting. In this section we present the two-stage approach with regularization. We rigorously prove the equivalence relationship between the two-stage a p-proach and the direct approach which solves the regularized generalized eigenvalue problem in Eq. (5).
To handle the regularization in the generalized eigenvalue problem in Eq. (5), we solve a penalized least squares prob-lem, or ridge regression [14] in the first step. Note that the  X  X atent target X  is the same as the one used in Algorithm 1; the difference is the regularization term included in the lea st squares problem. After projecting the data matrix X onto the subspace, we compute an auxiliary matrix D  X  R k  X  k and its SVD. Intuitively, the SVD computation of D amounts to solving the original optimization problem by replacing X with  X  X . Note that the size of D is very small, thus the cost of computing the SVD of D is relatively low. The algorithm outline is summarized in Algorithm 2.
Similar to Algorithm 1, the least squares problem in the first stage is solved using the LSQR algorithm [20] with the same cost. In the second stage, since k  X  d , the most expensive part is the computation of  X  X with a cost of O ( kdn ) if X is dense or O ( kz ) if X is sparse where z is the number of nonzero entries in the data matrix X . In addition, the cost of computing D and W 2 is O ( nk 2 ) and O ( k 3 ), respectively. The cost of combining W 1 and W 2 is O ( dk X  ), where  X  (  X   X  k ) is the number of final projection vectors. Therefore, the total computational cost is O ( Nk (3 n +5 d +2 z )+ kz + nk dk 2 ) if X is sparse.
Next we show that the two-stage approach with regular-ization produces the same solution as the direct approach which solves the regularized generalized eigenvalue probl em in Eq. (5) directly.
 Algorithm 2 The Two-Stage Approach with Regulariza-tion Input: X , H ,  X  .
 Output: W
Stage 1: Solve the following least squares problem: Stage 2: Compute  X  X = W T 1 X and D =  X  XH .
 Compute the compact SVD of D = U D  X  D U T D and set Compute W = W 1 W 2 as the final solution.

Following the standard techniques in linear algebra, the solution to the least squares problem with regularization i n Eq. (27) is W 1 = ( XX T +  X  I )  X  XH = U 1  X  2 1 +  X  I  X  1  X  1 V T 1 Then  X  X can be represented as Thus the matrix D  X  R k  X  k can be represented as:
D =  X  XH = H T V 1  X  1  X  2 1 +  X  I  X  1  X  1 V T 1 H = BB T where the matrix B  X  R k  X  r is defined as
The solution to the two-stage approach in Algorithm 2 is summarized in the following theorem: Theorem 3. Let the compact SVD of B be rank( B ) . The top  X  (  X   X  rank( B )) projection vectors com-puted by Algorithm 2 are given by where and V B  X  consists of the first  X  columns of V B , ( U B  X  consists of the first  X  columns of U B  X   X  1 B . When  X  = rank( B ) , W can be simplified as
Proof. Note that D = BB T . The SVD of D can be obtained from the SVD of B as follows: It follows from Algorithm 2 that Recall that W 1 can also be represented using B as: W We can thus derive W as follows: If only the first  X  projection vectors are required, then the resulting W is given by This completes the proof.
 We follow [27] for the eigendecomposition of the matrix
XX T +  X  I  X  1 ( XHH T X T ). The eigendecomposition is sum-marized in the following theorem, based on which we also obtain the equivalence relationship between the two-stage approach and the direct approach.

Theorem 4. The eigenvectors corresponding to the top  X  eigenvalues of matrix ( XX T +  X  I )  X  1 ( XHH T X T ) are given by where V B  X  consists of the first  X  (  X   X  rank( B )) columns of V
B . Thus, the two-stage approach in Algorithm 2 is equiva-lent to the direct approach which solves the generalized eig en-value problem with regularization directly.

Proof. Given V B  X  R r  X  q with orthonormal columns, orthogonal matrix. Hereafter, we denote [ V B , V  X  B ] = V
We can diagonalize ( XX T +  X  I )  X  1 ( XHH T X T ) as follows: = U (  X  = U (  X  Thus, the eigenvectors corresponding to the top  X  eigenval-ues of ( XX T +  X  I )  X  1 ( XHH T X T ) are given by U 1 (  X   X  I )  X  1 / 2 V B  X  . The equivalence between the two-stage approach and the direct approach follows from Eqs. (33) and (37). This completes the proof of the theorem.

Remark 2. The least squares algorithm proposed in [26] only works for dimensionality reduction techniques withou t regularization. This drawback limits its applicability in prac-tice since the regularized algorithms are expected to be mor e effective in practice due to its better generalization perfo r-mance. The two-stage algorithm proposed in this paper sig-nificantly improves previous work by extending the equiva-lence to the regularization setting.
 Table 1: Statistics of the data sets: n is the number of samples, d is the data dimensionality, and k is the number of labels (classes).
 Data Set Type n d k Syn1 Multi-class 1000 100 5 Syn2 Multi-class 1000 5000 5 Syn3 Multi-label 1000 100 5 Syn4 Multi-label 1000 5000 5 Ionosphere Multi-class 351 34 2 Optical digits Multi-class 5620 64 10 Satimage Multi-class 6435 36 6 USPS Multi-class 9298 256 10 Wine Multi-class 178 13 3 Scene Multi-label 2407 294 6
Yeast Multi-label 2417 103 14 news20 Multi-class 15935 62061 20 rcv1v2 Multi-label 3000 47236 101
Remark 3. The two-stage approach can be extended to the kernel-induced feature space. The equivalence relatio n-ship still holds for all dimensionality reduction techniqu es discussed above with and without regularization. Due to the space constraint, we skip the detailed proof in this paper.
We have performed extensive experiments to verify the es-tablished equivalence relationship and demonstrate the sc al-ability of the proposed two-stage approach. All the experi-ments are performed on a PC with Intel Core 2 Duo T9500 2.6G CPU and 4GB RAM. We implement all algorithms in Matlab. All Matlab codes and synthetic data sets are avail-able at www.public.asu.edu/  X  lsun27/Code/TwoStage.html.
The dimensionality reduction techniques discussed in this paper can be divided into two categories: 1) LDA for multi-class classification; 2) HSL, CCA, and OPLS for multi-label classification. We utilize both multi-class and multi-labe l data sets, including synthetic and real-world data sets, in the experiments. Two synthetic data sets for multi-class classification as well as two synthetic data sets for multi-label classification are generated. In the synthetic data se ts, each entry of the data matrix X is generated independently from the standard Gaussian distribution N (0 , 1). The num-ber of classes is k = 5, and the labels are generated uniformly with random. Five real-world data sets from the UCI ma-chine learning repository [3] and two benchmark data sets in multi-label classification [7, 10] are used in our experimen ts. To investigate the scalability of the two-stage approach, t wo large-scale data sets news20 [17] and rcv1v2 [18] are used. The statistics of all data sets are summarized in Table 1.
To distinguish different techniques tested in the experi-ments, we name the regularized techniques using a prefix X  X  X  before the corresponding technique, e.g.,  X  X LDA X . The two-stage versions are named using a prefix X 2S X ( X 2S X  X eans two stage) such as  X 2SLDA X  and  X 2SrLDA X , which are the two-stage versions of LDA and regularized LDA, respectively.
In this experiment, we compare the performance of differ-ent approaches for all techniques using both synthetic and
AUC
AUC data set as the regularization parameter  X  varies. real-world data sets. Denote W 0 as the solution of the gen-eralized eigenvalue problem in Eq. (2) or (5) by solving it directly, and W as the solution of Eq. (2) or (5) by solving it using the two-stage approach.

To verify whether both approaches produce equivalent projections, we compute k W 0 W T 0  X  WW T k 2 under dif-ferent values of the regularization parameter  X  . We vary the value of  X  from 0 to 1e6. It follows from [27] that k W 0 W T 0  X  WW T k 2 = 0 if and only if W 0 = WR , where R is an orthogonal matrix. Thus, both W and W 0 project the original data onto the same low-dimensional space. Note that a direct comparison between W and W 0 is possible only when the generalized eigenvalue problem in Eq. (2) or (5) admits a unique solution. This is not always the case, e.g., when two eigenvalues coincide.

The values of k W 0 W T 0  X  WW T k 2 for all data sets are summarized in Table 2. Note that two different variants of HSL, HSL-Clique and HSL-Star, which compute the Lapla-cian using different expansion schemes, are tested. From Table 2 it can be observed that for all values of the regu-larization parameter  X  , k W 0 W T 0  X  WW T k 2 is always very small, which confirms the equivalence relationship between W and W 0 for projection.

Next, we investigate the classification performance of dif-ferent techniques. We compare the performance of differ-ent approaches on the multi-label data set Yeast [10]. The data set is randomly partitioned into a training set and a test data set with equal size. After the projection matrix is learned from the training set, the test data set is projected onto the low-dimensional space. In our experiments, the linear support vector machines (SVM) is applied for classifi -cation. The average Area Under ROC Curve (AUC) over all labels are summarized in Figure 1 for all techniques. The regularization parameter  X  varies from 1e-6 to 1e6. From Figure 1 we conclude that the proposed two-stage approach always produces the same classification results as the direc t approach in all cases.

A similar experiment is performed on the multi-class data set wine [3] for LDA. The classification accuracies of LDA under different values of  X  are summarized in Figure 2, and similar observations can be made.
In this experiment, we study the scalability of the two-stage approach in comparison with the direct approach. Sinc e regularization is commonly used in practice, we compare parameter  X  .
 the scalability of different algorithms with regularizatio n. In terms of the implementation, the least squares problem in the first stage of the two-stage approach is solved us-ing the LSQR algorithm [19, 20]. For the direct approach which solves the generalized eigenvalue problem directly, the Lanczos algorithm [12] is applied. It is well-known that sol v-ing large-scale generalized eigenvalue problems is much mo re difficult than the regular eigenvalue problems [22, 28]. In or -der to transform the generalized eigenvalue problem into th e regular eigenvalue problem, we can factor XX T or apply the standard Lanczos algorithm for the matrix ( XX T )  X  1 XSX using the XX T inner product [22]. Due to the issue of singu-larity for high-dimensional data set with small regulariza tion for the second method, in this paper we follow the procedure in [26], which factors XX T and solves a symmetric eigen-value problem using the Lanczos algorithm.

The computation time (in log scale) of different techniques on the large-scale data set rcv1v2 is shown in Figures 4 and 5. In Figure 4, we increase the sample size from 500 to 3000 with a step size 500, and the dimensionality is fixed at 5000. The computation time of both approaches increases as the sample size increases. However, it can be observed that the computation time of the two-stage approach is significantly less than that of the direct approach. In Figure 5 we fix the sample size at 3000 and increase the dimensionality from 500 to 5000 with a step size 500. Similar observations can be made from Figure 5.

We perform a similar experiment on the news20 data set for LDA (multi-class classification). The experimental re-sults are summarized in Figure 3. In Figure 3 (left), we fix the dimensionality at 5000 and increase the sample size from 500 to 3000 with a step size 500. In Figure 3 (right), the dimensionality is increased from 500 to 3000 with a step size 500 while the sample size is fixed at 5000. It can be observed from these figures that the two-stage approach is much more efficient than the direct approach.
In this paper we propose an efficient two-stage approach for a class of dimensionality reduction techniques, includ -ing Canonical Correlation Analysis, Orthonormalized Par-tial Least Squares, Hypergraph Spectral Learning and Lin-ear Discriminant Analysis. We rigorously prove the equiva-lence relationship between the two-stage approach and the Figure 2: Comparison of different approaches in terms of classification accuracy for LDA on the wine data set as the regularization parameter  X  varies. Figure 3: Scalability comparison on the news20 data set as the sample size (left) and dimensionality (right) increase. The horizontal axis is the sample size (left) or the dimensionality (right), and the ver-tical axis is log( t ) , where t is the computation time (in seconds). direct approach which solves the generalized eigenvalue pr ob-lem directly. Compared with previous work, one appealing feature of the two-stage approach is that no assumption is required for the equivalence relationship. In addition, th e two-stage approach can be further extended to the regu-larization setting. We show that the proposed two-stage approach scales linearly in terms of both the sample size and data dimensionality. We have performed extensive ex-periments on both synthetic and real-world data sets. Our experimental results confirm the established equivalence r e-lationship. Results also demonstrate the scalability of th e proposed two-stage approach.

The current two-stage approach assumes that the com-plete data set for training is given in advance, and learning is carried out in one batch. However, in many real appli-cations the data come as a stream. We plan to explore the online algorithm for the class of dimensionality reduction techniques.
 This work was supported by NSF IIS-0612069, IIS-0812551, IIS-0953662, NGA HM1582-08-1-0016, the Office of the Di-rector of National Intelligence (ODNI), Intelligence Adva nced Research Projects Activity (IARPA), through the US Army. [1] S. Agarwal, K. Branson, and S. Belongie. Higher order [2] J. Arenas-Garcia and G. Camps-Valls. Efficient kernel [3] A. Asuncion and D. J. Newman. UCI machine [4] F. R. Bach and M. I. Jordan. Kernel independent [5] R. E. Bellman. Adaptive Control Processes: A Guided [6] C. M. Bishop. Pattern Recognition and Machine [7] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown. [8] D. Cai. Spectral Regression: A Regression Framework [9] D. Cai, X. He, and J. Han. SRDA: An efficient [10] A. Elisseeff and J. Weston. A kernel method for [11] K. Fukunaga. Introduction to Statistical Pattern [12] G. H. Golub and C. F. Van Loan. Matrix [13] D. R. Hardoon, S. R. Szedmak, and J. R.
 [14] T. Hastie, R. Tibshirani, and J. Friedman. The [15] H. Hotelling. Relations between two sets of variables. [16] P. Howland and H. Park. Two-stage methods for [17] K. Lang. Newsweeder: Learning to filter netnews. In [18] D. Lewis, Y. Yang, T. Rose, and F. Li. Rcv1: A new [19] C. C. Paige and M. A. Saunders. Algorithm 583: [20] C. C. Paige and M. A. Saunders. LSQR: An algorithm [21] R. Rosipal and N. Kr  X  amer. Overview and recent [22] Y. Saad. Numerical Methods for Large Eigenvalue [23] B. Sch  X  olkopf and A. J. Smola. Learning with kernels: [24] L. Sun, S. Ji, and J. Ye. Hypergraph spectral learning [25] L. Sun, S. Ji, and J. Ye. A least squares formulation [26] L. Sun, S. Ji, and J. Ye. A least squares formulation [27] L. Sun, S. Ji, S. Yu, and J. Ye. On the equivalence [28] D.S. Watkins. Fundamentals of matrix computations . [29] S. Wold and et al. Chemometrics, mathematics and [30] K. Worsley, J.-B. Poline, K. J. Friston, and A.C. [31] J. Ye. Characterization of a family of algorithms for [32] J. Ye and Q. Li. A two-stage linear discriminant [33] Z. Zhang, G. Dai, and M. Jordan. A flexible and [34] D. Zhou, J. Huang, and B. Sch  X  olkopf. Learning with
