 Learning a sequence classifier means learning to predict a sequence of output tags based on a set of input data items. For example, recognizing that a handwritten word is  X  X at X , based on three images of handwritten letters and on gen-eral knowledge of English letter combinations, is a sequenc e classification task. This paper describes a new two-stage approach to learning a sequence classifier that is (i) highly accurate, (ii) scalable, and (iii) easy to use in data mining ap-plications. The two-stage approach combines support vecto r machines (SVMs) and conditional random fields (CRFs). It is (i) highly accurate because it benefits from the maximum-margin nature of SVMs and also from the ability of CRFs to model correlations between neighboring output tags. It is (ii) scalable because the input to each SVM is a small training set, and the input to the CRF has a small num-ber of features, namely the SVM outputs. It is (iii) easy to use because it combines existing published software in a straightforward way. In detailed experiments on the task of recognizing handwritten words, we show that the two-stage approach is more accurate, or faster and more scalable, or both, than leading other methods for learning sequence clas -sifiers, including max-margin Markov networks (M3Ns) and standard CRFs.
 H.2.8 [ Database management ]: Database applications X  data mining.
 Algorithms.
 Conditional random fields, support vector machines, sequen ce learning.

A highly active and successful direction of research in ma-chine learning in the last seven years concerns methods for what is called structured learning [11, 16, 18, 17, 15, 5]. Structured learning means learning to predict outputs that have internal structure. This structure can be modeled, and, to achieve high predictive accuracy, it must be mod-eled. Learning to predict a sequence of output tags, given a sequence of input data items, is an example of a struc-tured learning problem. Specifically, suppose the input is a sequence of images where each image is a bitmap of a hand-written letter. A traditional supervised learning approac h is to train a function that can recognize the letter encoded by each image separately. In this traditional approach, the trained classifier recognizes each letter in isolation, bas ed only on the information available in the corresponding im-age. In a structured learning approach, given the sequence of images representing the letters in a word, a single traine d model recognizes all the letters of the word, using all the input images and using knowledge learned about which let-ters tend to be adjacent in English. For example, suppose the word to be recognized is  X  X ern. X  The handwritten third and fourth letters may well be almost identical, so a tradi-tional classifier might recognize this word as  X  X enn X  or  X  X er r X  or  X  X enr X . A sequence classifier would use probabilistic con -straints between neighboring output letters to know that  X  X ern X  is more likely than the alternatives, even though the alternatives are an equally good fit to the input data at the level of individual letters.

Research on structured learning has seen great progress, with sequence classification as its most important and suc-cessful subfield. Indeed, the original paper on conditional random fields (CRFs) has been cited over 1100 times since it was published in 2001 [11]. However, technology transfer from basic research to applications has been limited so far. Accelerating this technology transfer is the goal of this pa -per. We show that existing software that is high in quality and easy to use, specifically the well-known SVM package named LIBSVM [3] and a new CRF package named CRF-SGD [1], can be used together to achieve high accuracy and high speed on a sequence classification task that so far has been addressed only using complex custom methods that are effectively out of reach for practitioners.

In other words, the goal of the work described here is to show how to benefit from state-of-the-art methods in ma-chine learning by combining them in an uncomplicated way. Frank Lloyd Wright once wrote  X  X  X hink simple X  as my old master used to say X  X eaning reduce the whole of its parts into the simplest terms, getting back to first principles. X  O ur goal in this paper is to combine multiple theoretical ideas i n order to obtain one easy-to-use high-performance method. Following the principle of reducing the whole of its parts into the simplest terms, we reduce the problem of learning a sequence classifier into two subproblems.

The new learning framework is called a two-stage SVM/CRF method. It simplifies ideas introduced previously under the name max-margin Markov networks (M3Ns) [16]. Essen-tially, we first use SVMs to learn to predict the labels of in-dividual input sequence data items. Then, we use a CRF to predict the sequence of all output labels, where the input to the CRF is the outputs of the SVMs applied to the inputs. The two-stage method gains high accuracy from two comple-mentary strengths: margin-maximization approaches can be more accurate than likelihood-maximization approaches as discriminative classifiers, and learning correlations bet ween neighboring output labels helps resolve ambiguities.
Because our goal is to present a method that practition-ers can use easily in multiple other applications, our exper -iments use off-the-shelf software. As an implementation of SVMs, we use the LIBSVM package [3]. As an implemen-tation of CRFs, we use the very recent CRFSGD package [1]. The latter software is especially interesting, and fas t, because it solves the numerical optimization problem at the core of CRFs by stochastic gradient descent, following but simplifying much recent research [19].

In experiments we compare the two-stage method against three baseline methods. The first two baselines treat the problem as unstructured; they are standard logistic regres -sion (LR) and SVMs [3]. The third baseline does not use the margin-maximization idea; it is a standard CRF classifier. In addition to the two-stage SVM/CRF approach, we also investigate a similar two-stage LR/CRF method. Previous studies have shown that different sets of feature-functions lead to widely varying accuracy for CRFs [10]. Hence we investigate a range of alternative sets of feature-functio ns.
The M3N method combines maximum-margin and output-correlation constraints into a single quadratic programmi ng optimization problem [16]. In addition to the mathematical challenges of combining these two types of constraints, thi s approach is computationally intensive [13], although algo -rithms that are faster than the original M3N method have been proposed [17]. The two-stage approach that we in-troduce has an intuitive rationale that is similar to that of previous max-margin sequence prediction methods, but the new approach is notably simpler mathematically and com-putationally.

In our approach, first SVMs are trained to predict the la-bel of each input sequence element; this is a standard multi-class supervised learning task. Second, one CRF is trained to predict the output sequence of labels using as its input th e outputs from the previously trained SVMs. The intuition is that both learning approaches are somewhat orthogonal in their advantages, so a combination of them can yield supe-rior results.
In previous work, the outputs of other learning methods have been used as the input to an SVM [6, 12], but our approach is the opposite: the output of multiple SVMs is used as the input to another learning method.

During SVM training, the goal is to learn each class based on each sequence element (i.e. data item or data point) and its label in the training set, by maximizing the separation between data points with labels in the same class and other data points. Many studies have shown that SVMs tend to obtain superior results, compared to other classifiers, for p re-dicting individual labels. This advantage of SVMs stems from their ability to use high-dimensional feature spaces v ia kernels, and from theoretical guarantees on generalizatio n ability [16]. However, an important drawback is that it is typically hard to choose the settings for an SVM (in partic-ular, the best value for the soft-margin penalty C ) that will yield obtain optimum results. The most common way to choose settings is to use a validation set that is independen t from the training and testing sets.
 Given a data point in the test set, the output of the trained SVMs is a vector of scores. In the second stage of our approach, this vector is used as the input attributes for a CRF classifier. Traditionally, a feature-function for a CRF is based on one or more data points, and one label or two adjacent labels. Our proposed new type of feature-function is based on a prediction vector of scores for a data point, instead of directly on the attributes of the data point. Es-sentially, the two-stage approach uses SVMs as a feature induction method, in order to allow a CRF to learn a better overall classifier.

Let X be a set of input sequences and let Y be the corre-sponding set of sequences of labels. The data ( X, Y ) consist of samples (  X  x i ,  X  y i ) for i = 1 , ..., n . Each sample (  X  x sists of L ( i ) data points and their labels. That is A label y ij can belong to one of c different classes, and each input data point x ij can have p dimensions, where p is the number of pixels in the image of one character for example. We assume that each dimension can have one of v values.
Our experiments use an optical character recognition (OCR) dataset compiled by Kassel [9] and standardized by Taskar [16], who performed image segmentation to separate the characters in each word, rasterization, and normalization o f each character. Previous papers do not mention any further data manipulation such as dimensionality reduction. It is well known that dimensionality reduction can be very im-portant in image processing, but we do not investigate it here.
For multiclass classification SVMs can be used in either one-against-all or one-against-one fashion. With the one-against-all technique, each class is trained separately ag ainst the union of all other classes. Applying the trained SVMs on a test data point ( x ij , y ij ) yields a vector of prediction scores ( g 1 , g 2 , ..., g c ) ij , where c is the number of classes. With the one-against-one technique, each class is trained sepa-rately against each other class. Applying the trained SVMs to the test data point yields a vector of prediction scores ( g 1 , g 2 , ..., g b ) ij where b = c ( c  X  1) / 2.
Previous work [16] has indicated that the one-against-one approach yields slightly more accurate results for the OCR data. There are two additional advantages of using this approach as part of the two-stage SVM/CRF method: it yields faster SVM training, and it increases the bandwidth of information passed to the CRF. Although one-against-one training is conducted c ( c  X  1) / 2 times, each time only the data points in two classes are involved. SVM training time is typically superlinear in the number of training examples, s o learning more classifiers each with a smaller training set is a net win. This improvement in running time is proportional to the number of alternative labels ( c = 26 if labels are letters in the alphabet), so it is considerable. The increas e in communication bandwidth between the SVMs and the CRF can potentially improve the accuracy achievable by the CRF. However, the larger number of inputs for the CRF tends to increase its training time.

When used for multiclass classification, logistic regressi on classifiers produce similar vectors of scores, which can als o be used as inputs to a CRF in a second stage. For LR train-ing we use another off-the-shelf tool, the MATLABArsenal package [20]. With logistic regression, each vector of scor es is a non-normalized vector of probabilities. With support vector machines, each vector is a collection of scores with numerical values between  X  12 . 0 and 5 . 0.
Given a dataset of input and output sequences ( X, Y ), the training objective for a CRF model is to choose parameters W (also called weights) that maximize the conditional log likelihood log P ( Y | X ; W ), which is Here there are d different fixed feature-functions denoted F for z = 1 , . . . , d . There is one trainable parameter w each F z . Each feature-function F z is actually a sum over output sequence positions of a lower-level feature-functi on f . That is, each high-level feature-function F z has the form where j ranges over the elements of  X  y i and y i 0 is a special token to represent the beginning of a sequence.

Although the lower-level functions f z can in general be real-valued, all the f z functions we use are binary, i.e. they have value 0 or 1. Each f z function can depend on any or all of the input sequence, and/or on up to two adjacent labels in the output sequence  X  y i . The reason why only at most two adjacent output labels can be used is that making predictions efficiently with a trained CRF model depends on the Viterbi algorithm to compute and this algorithm cannot handle lower-level feature-func tions that involve more than two adjacent elements of  X  y i .
We investigate multiple alternative CRF designs that dif-fer in which feature-functions they use. The alternative CRFs that we consider use various combinations of the fol-lowing six types of feature-function, which are all special cases of the general form above.
 Feature-functions of the first type have the form There are c v p functions of this type, because there are c possible values for y ij , v attributes of x ij , and p possible values for each attribute.
 Feature-functions of the second type have the form The number of functions of this type is c 2 vp .

When dealing with the OCR dataset, previous work sug-gests that using features that depend only on output labels is beneficial. In particular, the best results of [10, Sectio n 3, Table 2] are obtained using F (1) z features in addition to features that use just a single label, and just two adjacent labels. We represent these feature types as follows: and There are c features of the former type, and c 2 of the latter type.

Our contribution is to introduce features for the two-stage approach that depend on the data point x ij only indirectly, through prediction scores g z ( x ij ) assigned by SVM classi-fiers. We formalize this idea as follows: and where g z ( x ij ) is one element of the score vector produced by the multiclass SVM classifier applied to x ij .
Real-valued SVM scores are discretized, in order to allow the f (5) z and f (6) z feature-functions to be binary. Specifi-cally, only the most significant digit is taken into account. Given a real-valued score g z ( x ij ), the integer value that is used as input to the feature-function is Each different integer value, for each of the binary SVM clas-sifiers, then gives rise to a different binary feature-functi on. When logistic regression is used instead of SVMs, scores are probabilities between 0 and 1, so we use instead.

As is customary with CRFs, we in fact maximize a regu-larized version of the conditional log likelihood, that is where log P ( W ) =  X  k W k 2 2  X  2 . Often the regularization param-eter  X  is set using a validation dataset, but in our experi-ments it is fixed at  X  = 1.
 The objective function is maximized by gradient descent. The gradient (  X / X  X  ( l ) z ) J ( X, Y ) is for l  X  { 1 , 2 , 3 , 4 , 5 , 6 } . The gradient, for each weight and for each training example (  X  x,  X  y ), is essentially the difference between the feature-function value for (  X  x,  X  y ) and the average value of the feature-function averaging over each  X  y  X  with probability given by the current model p (  X  y  X  |  X  x ; w ). The CRF software we use, called CRFSGD, does stochastic gradient descent [1]. Our experiments confirm that this approach achieves the same accuracy as a sophisticated quasi-Newton method (L-BFGS, [14]) but is about 10 times faster.
Our hypothesis is that the two-stage combined SVM/CRF method just described performs as well as more mathemat-ically and computationally complex methods, in particular the M3N method. In previous papers, Taskar et al. and Perez-Cruz et al. measure accuracy as the average error per character, but Nguyen et al. and Keerthi et al. measure ac-curacy as the average over words of the average error per character in each word. In this paper, we report both mea-surements, since this is the only way to establish a direct correspondence with previous results. As expected, both definitions of accuracy yield very similar results. The first definition is where N is the total number of characters in the test set, y ij is the true value of the j th character of the i th word in the test set, and  X  y ij is the predicted value of this character. The second definition is where M is the total number of words and L ( i ) is the total number of characters in the i th word.
The specific dataset used for experiments here is a subset containing 6876 words from the OCR dataset of [9]. This subset was compiled by Ben Taskar, and is precisely the same dataset used previously [16, 13, 10, 15]. Each characte r image in the dataset is of size of 8 16 = 128 pixels and is labeled with one of 26 letters. Each pixel has value 0 or 1. To the best of our knowledge, this is the preferred dataset fo r comparing the performance of classifiers where margin and sequential based approaches are combined, given previousl y published results that study the matter.

In previous work, Taskar et al. used an unusual 10-fold cross-validation technique where they divided the data int o training sets of about 610 words and test sets of about 5500 words. This approach is unusual because in each fold, a small set is used for training versus a large set for testing. In standard cross-validation, in each fold a large set is use d for training and a small set for testing. Nguyen et al. applied a similar nonstandard technique, but they used about 600 words for training, about 5400 words for testing, about 100 words for validation. The precise cardinalities of the subs ets used in this previous work is not known.

It seems that the reason previous authors used small train-ing sets is time limitations for training. It has been report ed [13, Section 4] that the M3N method needed to be halted after 10 iterations of the optimization algorithm for a singl e fold. The two-stage approach proposed here is much faster. Therefore traditional 10-fold cross-validation can be use d, as done also by Perez-Cruz et al. This is desirable because standard cross-validation gives a better idea of the ultimat e accuracy that can be achieved by different methods, since it is based on larger training sets.
For the standard unstructured classifiers, logistic regres -sion and SVMs, each input data point is separate and is one array of pixels. Both methods are trained in a one-against-one fashion for solving the multi-class problem, which is th e same as done previously by Taskar [16, Section 3]. For lo-gistic regression the regularization constant is set to 1. F or soft-margin SVMs, three different kernels are tried: linear , quadratic and cubic.

Changing the soft-margin penalty parameter C typically yields significantly different results for different kernels [2]. In our experiments C is set to be 150, 250, and 450, for the linear, quadratic, and cubic kernels respectively. Oth er training parameters are set to the defaults from LIBSVM. Notice that the CGM experiments also use LIBSVM [15]. Perez-Cruz et al. pick C to be 5, and use a radial basis function kernel.

Standard CRF classifiers are trained using two different sets of feature-functions. The first set consists of the F and F (2) z features. Following Keerthi et al. , the second set consists of the F (1) z , F (3) z and F (4) z feature-functions. In the first set there are 128 2 26 = 6656 F (1) z functions and 128 2 26 26 = 173056 F (2) z functions. In the second set there are 26 F (3) z functions and 26 26 = 676 F (4) z functions in addition to the F (1) z functions.

Two-stage SVM/CRF classifiers are trained using three different sets of feature-functions. The first set includes F z and F (6) z feature-functions, and thus corresponds to the M3N approach. The second set contains F (3) z , F (4) and F (5) z feature-functions, so it is analogous to the set of CRF feature-functions that performs best in recent experi-ments [10]. Finally, the third set combines the original CRF feature-functions F (1) z and F (2) z with the novel F (5) feature-functions. After discretization, each SVM score i s one of at most 17 unique values. Given the one-against-one approach, each score vector has length (26 25) / 2 = 325. Thus, there are at most 325 17 26 = 143 , 650 F (5) z func-tions, and at most 325 17 26 26 = 3 , 734 , 900 F (6) z functions. The CRFSGD software only keeps features that occur more than three times in the training set, so these feature set cardinalities are upper bounds on the number of features actually used.
Tables 1 and 2 show accuracy results using nonstandard cross-validation, that is with a small 10% training set in each fold, while Tables 3 and 4 show results using standard cross-validation, with a large 90% training set in each fold . Results are presented as mean accuracy plus/minus stan-dard deviation over ten folds. Rows in italics are results taken from previous papers. If a method from a previous paper does not appear in a table, it is because the previous paper did not report the corresponding performance metric, or did not use the corresponding type of cross-validation. Standard deviations are given where available. Results fro m Taskar et al. appear with two places of accuracy only since they are obtained from a figure in that paper. Finally, Table 5 presents the number of seconds needed to run one fold of cross validation for each method.

The first unstructured baseline, the logistic regression cl as-sifier, performs better than previously reported. The im-provement may be due to the fact that we use the one-against-one approach. In results not shown, when running logistic regression in one-against-all fashion, our resul ts are the same as previously found by Taskar.

The SVM classifiers based on LIBSVM produce interest-ing results compared to previous experiments. They yield slightly better accuracy than has been reported by Taskar et al. , Nguyen et al. , and Perez-Cruz et al. The differences may be due to the challenge of setting the soft-margin penalty pa -rameter adequately. In Taskar X  X  work, a multiclass kernel-vector machine [4] is used for the linear, quadratic and poly -nomial kernels. The results from that method closely match the performance obtained here using LIBSVM.

Nguyen et al. use two types of SVM, called SV M struct [8] and SV M multiclass , which are both based on the SV M light quadratic optimizer [7]. Notice that Nguyen et al. only show results for SVMs with linear kernels, which perform worse than SVMs with polynomial kernels in this domain. SV M struct performs better than SV M multiclass in their ex-periments; its accuracy is close to the accuracy we can obtai n using polynomial kernels. Perez-Cruz et al. use the same LIBSVM package that we do; their results using a radial basis function kernel are similar to ours using a linear ker-nel. Clearly, so far polynomial kernels are the best known for this domain.

Our first baseline method for structured learning, a CRF classifier with feature types F (1 , 2) z , performs better than the CRF of Taskar et al. by around 3 percentage points, and much better than the CRF of Nguyen et al. , beating it by 10 percentage points. This big difference in accuracy is likely due to differences choosing features for the CRF. The CRFSGD software lets us efficiently use a large number of feature-functions, which is known to be beneficial for the success of this type of classifier.

Our second CRF baseline uses the feature-functions sug-gested by Keerthi et al. , namely the types F (1 , 3 , 4) are token-dependent first-order and token-independent firs t-order and second-order according to their nomenclature. Th e results in this case are similar to previous findings.
Last but not least, the results for the novel two-stage ap-proach are very promising. Overall this approach does bette r than logistic regression, SVM, and CRF methods separately, and offers accuracy similar to that of the more complex M3N and CGM methods. Using feature-functions that are token-obtaining a good two-stage classifier.

Results with the two-stage logistic regression/CRF method are better than results with either method alone, and almost as good as the best results obtained with the M3N method. Although both logistic regression and CRFs are based on maximizing the conditional log-likelihood of a linear mode l, supplying the logistic regression vector of probability es ti-mates to the CRF appears to enhance its ability to solve the problem. Presumably the vector of scores makes explicit information that is only implicit in the original data. Table 1: Small training sets: average accuracy per character.
 Table 2: Small training sets: average accuracy per character per word.
 Table 3: Large training sets: average accuracy per character.
 Table 4: Large training sets: average accuracy per character per word.

The performance of the two-stage SVM/CRF method is good. Its accuracy is comparable to that of the M3N method when using features based on the vector of scores and on adjacent labels ( F (5 , 6) z ). The two-stage SVM/CRF also per-forms as well as the CGM method with cliques of size 2, which is the fair comparison. The CGM method with cliques of size 3 obtains the best overall results. This make sense because there is definitely useful information in tripl es of letters over and above the information in pairs of letters. For example, while  X  X t X  and  X  X h X  are both common letter pairs in English, the triplet  X  X th X  is rare.

Tables 3 and 4 show that using traditional cross-validation , with a large training set in each fold, leads to significantly improved accuracy. With this setup, all methods do 5 to 10 percentage points better than with a smaller training set. In summary, Tables 1 to 4 together show that the two-stage approach, with information from either logistic regressio n or SVMs provided as input to a CRF, yields the same accuracy as mathematically more complex methods.
Previous studies do not mention the time required to con-duct experiments. Table 5 shows the number of seconds needed to run one fold of cross-validation for each of our methods, with small and with big training sets. The en-tries in the table for LR/CRF and SVM/CRF are the time needed by the CRF stage for these approaches. Thus, the total time for the SVM/CRF two-stage approach is the sum of the SVM and SVM/CRF entries. The computers used for Table 5 are quite standard and inexpensive (Redhat Linux EL4, dual P4 3.2GHz, single CPU used, 2GB memory). As expected, logistic regression training is fastest, whil e SVM training is slowest. Given that the larger training set is 9 times bigger, a ratio of running times of 9 or less can be considered reasonable. The observed ratio is reasonable fo r all methods, except for SVM training with a linear kernel. Table 5: Time in seconds for one fold of training and testing.
 It is an unfortunate drawback of SVMs that training time often increases more than linearly as the number of training examples increases. This phenomenon is observed here for SVM training with the linear kernel. In future work, we plan to use one of the more recent SVM implementations that tend to be much faster because they use stochastic gradient descent.
Structured learning is a new research area in machine learning that has not yet seen wide usage in data mining or knowledge discovery. Within the field of structured learn-ing, the most studied task has been how to learn a classifier that maps a sequence of inputs into a sequence of output labels. Above, we have described a practical new approach to training a sequence classifier. Our experiments show that the proposed method achieves high accuracy, and is faster and more scalable than competitors.

The proposed method combines support vector machines and conditional random fields in a two-stage approach. It achieves high accuracy because of the maximum-margin na-ture of SVMs, and because CRFs can model correlations between neighboring output labels. The SVM stage of the new method is scalable because the input for training each SVM is only a small subset of the entire training data. The CRF stage of the new method is scalable because the CRF uses only a limited number of features, namely the outputs of the SVMs trained in the first stage.

We report the results of detailed experiments on the task of recognizing handwritten words. Our results provide a lot of detail concerning just one dataset, rather than being les s detailed but involving multiple datasets. The reason for th is choice is partly that a previous comparison paper in this are a [13] has been controversial. The results of this particular previous paper show CRFs and the M3N method performing much worse than in the experience of other researchers. The reason for some of the poor results in [13] was uncovered by [10]. Now, we have performed careful and systematic experiments whose results, reported here, supersede those of [13], and will resolve the controversy, we hope.
We feel confident that the good performance obtained on the handwritten word recognition problem by the two-stage method will carry over to other sequential prediction problems. The reason is the orthogonal strengths of the two phases of the two-stage method. In general, a margin-based approach can extract the most important information about individual data points, while a sequential approach can augment the learning process by exposing the sequen-tially structured nature of the problem. The results above show that the two-stage SVM/CRF method yields greater accuracy than its component individual methods, which are the current practical state of the art. The two-stage method matches closely the accuracy achievable with the M3N and CGM methods, which are more complex mathematically and computationally. For practical purposes, what is most im-portant is that the good SVM/CRF results are obtained using robust off-the-shelf software. This fact means that the proposed SVM/CRF combination is usable immediately by other researchers and practitioners in their applicatio n areas.

Acknowledgments: This work was performed while the first author was a student at UCSD and employed by Qual-comm, Inc. [1] L. Bottou. CRFSGD software , 2008. Available at [2] C. J. C. Burges. A tutorial on support vector [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [4] K. Crammer and Y. Singer. On the algorithmic [5] V. Franc and B. Savchynskyy. Discriminative learning [6] T. Jaakkola, M. Diekhans, and D. Haussler. Using the [7] T. Joachims. SV M light Support Vector Machine , [8] T. Joachims. SVM-hmm sequence tagging with [9] R. H. Kassel. A comparison of approaches to on-line [10] S. S. Keerthi and S. Sundararajan. CRF versus [11] J. Lafferty, A. McCallum, and F. Pereira. Conditional [12] L. Liao and W. S. Noble. Combining pairwise [13] N. Nguyen and Y. Guo. Comparisons of sequence [14] J. Nocedal and S. J. Wright. Limited memory BFGS. [15] F. Perez-Cruz, Z. Ghahramani, and M. Pontil. [16] B. Taskar, C. Guestrin, and D. Koller. Max-margin [17] B. Taskar, S. Lacoste-Julien, and M. I. Jordan. [18] I. Tsochantaridis, T. Joachims, T. Hofmann, and [19] S. V. N. Vishwanathan, N. N. Schraudolph, M. W. [20] R. Yan. MATLABArsenal: A Matlab package for
