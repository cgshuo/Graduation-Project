 This paper studies a family of decision-making problems in which discrete events occur on a contin-uous time scale. The time intervals between events are governed by a Poisson process. Each event has to be met by a decision to either suppress or allow it. The optimization criterion is allowed to depend on the rate of decision outcomes within a time interval; the criterion is not necessarily a sum of a loss function over individual decisions.
 The problems that we study cannot adequately be modeled as Mavkov or semi-Markov decision problems because the probability of transitioning from any value of decision rates to any other value depends on the exact points in time at which each event occurred in the past. Encoding the entire backlog of time stamps in the state of a Markov process would lead to an unwieldy formalism. The learning formalism which we explore in this paper models the problem directly as a Poisson process with a throttling policy that depends on an explicit data-dependent rate limit, which allows us to refer to a result from queuing theory and derive a convex optimization problem that can be solved efficiently.
 Consider the following two scenarios as motivating applications. In order to stage a successful denial-of-service attack, an assailant has to post requests at a rate that exceeds the capacity of the service. A prevention system has to meet each request by a decision to suppress it, or allow it to be processed by the service provider. Suppressing legitimate requests runs up costs. Passing few abusive requests to be processed runs up virtually no costs. Only when the rate of passed abusive requests exceeds a certain capacity, the service becomes unavailable and costs incur. The following second application scenario will serve as a running example throughout this paper. Any email service provider has to deal with a certain fraction of accounts that are set up to disseminate phishing messages and email spam. Serving the occasional spam message causes no harm other than consuming computational ressources. But if the rate of spam messages that an outbound email server discharges triggers alerting mechanisms of other providers, then that outbound server will become blacklisted and the service is disrupted. Naturally, suppressing any legitimate message is a disruption to the service, too. Let x denote a sequence of decision events x 1 , . . . , x n ; each event is a point x i  X  X  in an instance space. Sequence t denotes the time stamps t i  X  R + of the decision events with t i &lt; t i +1 . We define an episode e by the tuple e = ( x , t , y ) which includes a label y  X  X  X  1 , +1 } . In our application, an episode corresponds to the sequence of emails sent within an observation interval from a legitimate ( y =  X  1 ) or abusive ( y = +1 ) account e . We write x i and t i to denote the initial sequence of the first i elements of x and t , respectively. Note that the length n of the sequences can be different for different episodes.
 Let A = { X  1 , +1 } be a binary decision set, where +1 corresponds to suppressing an event and  X  1 corresponds to passing it. The decision model  X  gets to make a decision  X  ( x i , t i )  X  X  at each point in time t i at which an event occurs.
 The outbound rate r ( t  X  | x , t ) at time t  X  for episode e and decision model  X  is a crucial concept. It counts the number of events that were let pass during a time interval of lengh  X  ending before t  X  . throttling,  X  corresponds to the time interval that is used by other providers to estimate the incoming spam rate.
 We define an immediate loss function  X  : Y  X A X  R + that specifies the immediate loss of deciding a  X  X  for an event with label y  X  Y as where c + and c  X  are positive constants, corresponding to costs of false positive and false negative decisions. Additionally, the rate-based loss  X  : Y  X  R +  X  R + is the loss that runs up per unit of time. We require  X  to be a convex, monotonically increasing function in the outbound rate for y = +1 and to be 0 otherwise. The rate-based loss reflects the risk of the service getting blacklisted based on the current sending behaviour. This risk grows in the rate of spam messages discharged and the duration over which a high sending rate of spam messages is maintained.
 The total loss of a model  X  for an episode e = ( x , t , y ) is therefore defined as The first term penalizes a high rate of unsuppressed events with label +1  X  X n our example, a high rate of unsuppressed spam messages X  X hereas the second term penalizes each decision individually. For the special case of  X  = 0 , the optimization criterion resolves to a risk, and the problem becomes a standard binary classification problem.
 An unknown target distribution over p ( x , t , y ) induces the overall optimization goal E x ; t ;y [ L (  X  ; x , t , y )] . The learning problem consists in finding  X  from a training sample of tuples D = { ( x 1 n 1 , t 1 n 1 , y 1 ) , . . . , ( x m n m , t m n m , y m ) } . We assume the following data generation process for episodes e = ( x , t , y ) that will allow us to derive an optimization problem to be solved by the learning procedure. First, a rate parameter  X  , parameter of a Poisson process p ( t |  X  ) which now generates time sequence t . The expected loss of decision model  X  is taken over all input sequences x , rate parameter  X  , label y , and over all possible sequences of time stamps t that can be generated according to the Poisson process.
 2.1 Derivation of Empirical Loss In deriving the empirical counterpart of the expected loss, we want to exploit our assumption that time stamps are generated by a Poisson process with unknown but fixed rate parameter. For each input episode ( x , t , y ) , instead of minimizing the expected loss over the single observed sequence of time stamps, we would therefore like to minimize the expected loss over all sequences of time stamps generated by a Poisson process with the rate parameter that has most likely generated the observed sequence of time stamps. Equation 4 introduces the observed time sequence of time stamps t  X  into Equation 3 and uses the fact that the rate parameter  X  is independent of x and y given t  X  . Equation 5 rearranges the terms, and Equation 6 writes the central integral as a conditional expected value of the loss given the rate  X  . Finally, Equation 7 approximates the integral over all values of  X  by a single summand with value  X   X  for each episode.
 We arrive at the regularized risk functional in Equation 8 by replacing p ( x , t  X  , y ) by 1 m for all ob-servations in D and inserting MAP estimate  X   X  e as parameter that generated time stamps t e . The influence of the convex regularizer  X  is determined by regularization parameter  X  &gt; 0 . Minimizing this risk functional is the basis of the learning procedure in the next section. As noted in Section 1, for the special case when the rate-based loss  X  is zero, the problem reduces to a standard weighted binary classification problem and would be easy to solve with standard learning algorithms. However, as we will see in Section 4, the  X  -dependent loss makes the task of learning a decision function hard to solve; attributing individual decisions with their  X  X air share X  of the rate loss X  X nd thus estimating the cost of the decision X  X s problematic. The Erlang learning model of Section 3 employs a decision function that allows to factorize the rate loss naturally. In the following we derive an optimization problem that is based on modeling the policy as a data-dependent rate limit. This allows us to apply a result from queuing theory and approximate the empirical risk functional of Equation (8) with a convex upper bound. We define decision model  X  events, where  X  is some feature mapping of the initial sequence ( x i , t i ) and  X  is a parameter vector. The throttling model is defined as the current instance, would exceed rate limit f ( x i , t i ) . We will now transform the optimization goal of Equation 8 into an optimization problem that can be solved by standard convex optimization tools. To this end, we first decompose the expected loss of an input sequence given the rate parameter in Equation 8 into immediate and rate-dependent loss terms. Note that t e denotes the observed training sequence whereas t serves as expectation variable for the expectation E t [  X |  X  e  X  ] over all sequences conditional on the Poisson process rate parameter  X  e  X  as in Equation 8.
 Equation 10 uses the definition of the loss function in Equation 2. Equation 11 exploits that only We will first derive a convex approximation of the expected rate-based loss E [ model allows us to factorize the expected rate-based loss into contributions of individual rate limit decisions. The convexity will be addressed by Theorem 1.
 Since the outbound rate r increases only at decision points t i , we can upper-bound its value with the value immediately after the most recent decision in Equation 12. Equation 13 approximates the actual outbound rate with the rate limit given by f ( x e i , t e i ) . This is reasonable because the outbound rate depends on the policy decisions which are defined in terms of the rate limit. Because t is generated by a Poisson process, E t [ t i +1  X  t i |  X   X  e ] = 1  X  We have thus established a convex approximation of the left side of Equation 11.
 the loss functional in Equation 11. Queuing theory provides a convex approximation: The Erlang-B formula [5] gives the probability that a queuing process which maintains a constant rate limit of f within a time interval of  X  will block an event when events are generated by a Poisson process with given rate parameter  X  . Fortet X  X  formula (Equation 15) generalizes the Erlang-B formula for non-integer rate limits.
 The integral can be computed efficiently using a rapidly converging series, c.f. [5]. The formula requires a constant rate limit, so that the process can reach an equilibrium. In our model, the rate limit f ( x i , t i ) is a function of the sequences x i and t i until instance x i , and Fortet X  X  formula therefore serves as an approximation.
 Unfortunately, Equation 17 is not convex in  X  . We approximate it with the convex upper bound  X  illustration). This is an upper bound, because  X  log p  X  1  X  p for 0  X  p  X  1 ; its convexity Combining the two components of the optimization goal (Equation 11) and adding convex regular-izer  X (  X  ) and regularization parameter  X  &gt; 0 (Equation 8), we arrive at an optimization problem for finding the optimal policy parameters  X  .
 Optimization Problem 1 (Erlang Learning Model) . Over  X  , minimize Next we show that minimizing risk functional R amounts to solving a convex optimization problem. Theorem 1 (Convexity of R ) . R (  X  ) is a convex risk functional in  X  for any  X   X  e &gt; 0 and  X  &gt; 0 . Proof. The convexity of  X  and  X  follows from their definitions. It remains to be shown that both  X  tion 18 is independent of  X  . It is known that Fortet X  X  formula B ( f,  X  e  X   X  )) is convex, monotically Next, we show that  X  log(1  X  B ( f (  X  ) ,  X   X  e  X  ))) is convex and monotonically decreasing. From the above it follows that b ( f ) = 1  X  B ( f,  X   X  e  X  )) is monotonically increasing, concave and positive. follows that  X  log(1  X  B ( f (  X  ) ,  X   X  e  X  ))) is convex in  X  due to the definition of f . We will now discuss how the problem of minimizing the expected loss,  X   X  = argmin E x ; t ;y [ L (  X  ; x , t , y )] , from a sample of sequences x of events with labels y and observed rate parameters  X   X  relates to previously studied methods. Sequential decision-making problems are commonly solved by reinforcement learning approaches, which have to attribute the loss of an episode (Equation 2) to individual decisions in order to learn to decide optimally in each state. Thus, a crucial part of defining an appropriate procedure for learning the optimal policy consists in defin-ing an appropriate state-action loss function. Q ( s, a ) estimates the loss of performing action a in state s when following policy  X  for the rest of the episode.
 Several different state-action loss functions for related problems have been investigated in the litera-ture. For example, policy gradient methods such as in [4] assign the loss of an episode to individual decisions proportional to the log-probabilities of the decisions. Other approaches use sampled esti-of states of the episode is known [7]. Such general purpose methods, however, are not the optimal choice for the particular problem instance at hand. Consider the special case  X  = 0 , where the problem reduces to a sequence of independent binary decisions. Assigning the cumulative loss of the episode to all instances leads to a grave distortion of the optimization criterion. As reference in our experiments we use a state-action loss function that assigns the immediate loss  X  ( y, a i ) to state s i only. Decision a i determines the loss incurred by  X  only for  X  time units, in the interval [ t i , t i +  X  ) . The corresponding rate loss is deciding a i =  X  1 instead of a i = +1 is the difference in the corresponding  X  -induced loss. Let x  X  i , t  X  i denote the sequence x , t without instance x is the sum of immediate loss and  X  -induced loss; it serves as our first baseline.

Q it ( s i , a ) =  X  ( y, a ) +  X  ( a =  X  1) By approximating function of a second plausible state-action loss that, instead of using the observed loss to estimate the loss of an action, approximates it with the loss that would be incurred by the current outbound rate r ( t i | x  X  i , t  X  i ) for  X  time units.

Q ub ( s i , a ) =  X  ( y, a ) +  X  ( a =  X  1) The state variable s has to encode all information a policy needs to decide. Since the loss crucially depends on outbound rate r ( t  X  | x , t ) , any throttling model must have access to the current outbound rate. The transition between a current and a subsequent rate depends on the time at which the next event occurs, but also on the entire backlog of events, because past events may drop out of the interval  X  at any time. In analogy to the information that is available to the Erlang learning model, function, different approaches for defining the policy  X  and optimizing its parameters have been investigated. For our baselines, we use the following two methods.
 Policy gradient. Policy gradient methods model a stochastic policy directly as a parameterized decision function. They perform a gradient descent that always converges to a local optimum [8]. The gradient of the expected loss with respect to the parameters is estimated in each iteration k for the distribution over episodes, states, and losses that the current policy  X  k induces. However, in order to achieve fast convergence to the optimal polity, one would need to determine the gradient for the distribution over episodes, states, and losses induced by the optimal policy. We implement two policy gradient algorithms for experimentation which only differ in using Q it and Q ub , respectively. They are denoted PG it and PG ub in the experiments. Both use a logistic regression function as decision function, the two-class equivalent of the Gibbs distribution which is used in the literature. Iterative Classifier. The second approach is to represent policies as classifiers and to employ methods for supervised classification learning. A variety of papers addresses this approach [6, 3, 7]. We use an algorithm that is inspired by [1, 2] and is adapted to the problem setting at hand. Blatt and Hero [2] investigate an algorithm that finds non-stationary policies for two-action T-step MDPs by solving a sequence of one-step decisions via a binary classifier. Classifiers  X  t for time step t are algorithm iteratively learns weighted support vector machine (SVM) classifier  X  k +1 in iteration k +1 on the set of instances and losses Q k ( s, a ) that were observed after classifier  X  k was used as policy on the training sample. The weight vector of  X  k is denoted  X  k . The weight of misclassification of s is given by Q k ( s,  X  y ) . The SVM weight vector is altered in each iteration as  X  k +1 = (1  X   X  k )  X  k +  X   X  , where  X   X  is the weight vector of the new classifier that was learned on the observed losses. In the experiments, two iterative SVM learner were implemented, denoted It-SVM it and It-SVM ub , corresponding to the used state-action losses Q it and Q ub , respectively. Note that for the special case  X  = 0 the iterative SVM algorithm reduces to a standard SVM algorithm.
 All four procedures iteratively estimate the loss of a policy decision on the data via a state-action loss function and learn a new policy  X  based on this estimated cost of the decisions. Convergence guarantees typically require the Markov assumption; that is, the process is required to possess a on the entire backlog of time stamps and the duration over which state s i has been maintained, the Markov assumption is violated to some extent in practice. In addition to that,  X  -based loss estimates are sampled from a Poisson process. In each iteration  X  is learned to minimize sampled and inherently random losses of decisions. Thus, convergence to a robust solution becomes unlikely. In contrast, the Erlang learning model directly minimizes the  X  -loss by assigning a rate limit. The rate limit implies an expectation of decisions. In other words, the  X  -based loss is minimized without explicitely estimating the loss of any decisions that are implied by the rate limit. The convexity of the risk functional in Optimization Problem 1 guarantees convergence to the global optimum. The goal of our experiments is to study the relative benefits of the Erlang learning model and the four reference methods over a number of loss functions. The subject of our experimentation is the problem of suppressing spam and phishing messages sent from abusive accounts registered at a large email service provider. We sample approximately 1,000,000 emails sent from approximately Figure 1: Average loss on test data depending on the influence of the rate loss c for different immediate loss constants c  X  and c + . 10,000 randomly selected accounts over two days and label them automatically based on information passed by other email service providers via feedback loops (in most cases triggered by  X  X eport spam X  buttons). Because of this automatic labeling process, the labels contain a certain smount of noise. Feature mapping  X  determines a vector of moving average and moving variance estimates of several attributes of the email stream. These attributes measure the frequency of subject changes and sender address changes, and the number of recipients. Other attributes indicate whether the subject line or the sender address have been observed before within a window of time. Additionally, a moving average estimate of the rate  X  is used as feature. Finally, other attributes quantify the size of the message and the score returned by a content-based spam filter employed by the email service. We implemented the baseline methods that were descibed in Section 4, namely the iterative SVM methods It-SVM ub and It-SVM it and the policy gradient methods PG ub and PG it . Additionally, we used a standard support vector machine classifier SVM with weights of misclassification corre-sponding to the costs defined in Equation 1. The Erlang learning model is denoted ELM in the plots. Linear decision functions were used for all baselines.
 In our experiments, we assume a cost that is quadratic in the outbound rate. That is, overall loss. The time interval  X  was chosen to be 100 seconds. Regularizer  X (  X  ) as in Optimization problem 1 is the commonly used squared l 2 -norm  X (  X  ) =  X   X   X  2 2 .
 We evaluated our method for different costs of incorrectly classified non-spam emails ( c  X  ), incor-rectly classified spam emails ( c + ) (see the definition of  X  in Equation 1), and rate of outbound spam messages ( c ). For each setting, we repeated 100 runs; each run used about 50%, chosen at random, as training data and the remaining part as test data. Splits where chosen such that there were equally many spam episodes in training and test set. We tuned the regularization parameter  X  for the Erlang learning model as well as the corresponding regularization parameters of the iterative SVM methods and the standard SVM on a separate tuning set that was split randomly from the training data. 5.1 Results Figure 1 shows the resulting average loss of the Erlang learning model and reference methods. Each of the three plots shows loss versus parameter c which determines the influence of the rate loss on the overall loss. The left plot shows the loss for c  X  = 5 and c + = 1 , the center plot for ( c  X  = 10 , c + = 1) , and the right plot for ( c  X  = 20 , c + = 1) .
 We can see in Figure 1 that the Erlang learning model outperforms all baseline methods for larger values of c  X  X ore influence of the rate dependent loss on the overall loss X  X n two of the three settings. For c  X  = 20 and c + = 1 (right panel), the performance is comparable to the best baseline method It-SVM ub ; only for the largest shown c = 5 does the ELM outperform this baseline. The iterative classifier It-SVM ub that uses the approximated state-action loss Q ub performs uniformly better than It-SVM it , the iterative SVM method that uses the sampled loss from the previous it-eration. It-SVM it itself surprisingly shows very similar performance to that of the standard SVM method; only for the setting c  X  = 20 and c + = 1 in the right panel does this iterative SVM method show superior performance. Both policy gradient methods perform comparable to the Erlang learn-ing model for smaller values of c but deteriorate for larger values. As expected, the iterative SVM and the standard SVM algorithms perform better than the Erlang learning model and policy gradient models if the influence of the rate pedendent loss is very small. This can best be seen in Figure 2(a). It shows a detail of the results for the setting c  X  = 5 and c + = 1 , for c ranging only from 0 to 1. This is the expected outcome following the considerations in Section 4. If c is close to 0, the problem approximately reduces to a standard binary classifi-cation problem, thus favoring the very good classification performance of support vector machines. However, for larger c the influence of the rate dependent loss rises and more and more dominates the immediate classification loss  X  . Consequently, for those cases  X  which are the important ones in this real world application  X  the better rate loss estimation of the Erlang learning model compared to the baselines leads to better performance.
 The average training times for the Erlang learning model and the reference methods are in the same order of magnitude. The SVM algorithm took 14 minutes in average to converge to a solution. The Erlang learning model converged after 44 minutes and the policy gradient methods took approxi-mately 45 minutes. The training times of the iterative classifier methods were about 60 minutes. We devised a model for sequential decision-making problems in which events are generated by a Poisson process and the loss may depend on the rate of decision outcomes. Using a throttling policy that enforces a data-dependent rate-limit, we were able to factor the loss over single events. Applying a result from queuing theory led us to a closed-form approximation of the immediate event-specific loss under a rate limit set by a policy. Both parts led to a closed-form convex optimization problem. Our experiments explored the learning model for the problem of suppressing abuse of an email service. We observed significant improvements over iterative reinforcement learning baselines. The model is being employed to this end in the email service provided by web hosting firm STRATO. It has replaced a procedure of manual deactivation of accounts after inspection triggered by spam reports.
 Acknowledgments We gratefully acknowledge support from STRATO Rechenzentrum AG and the German Science Foundation DFG.
 [1] J.A. Bagnell, S. Kakade, A. Ng, and J. Schneider. Policy search by dynamic programming. [2] D. Blatt and A.O. Hero. From weighted classification to policy search. Advances in Neural [3] C. Dimitrakakis and M.G. Lagoudakis. Rollout sampling approximate policy iteration. Machine [4] M. Ghavamzadeh and Y. Engel. Bayesian policy gradient algorithms. Advances in Neural [5] D.L. Jagerman, B. Melamed, and W. Willinger. Stochastic modeling of traffic processes. Fron-[6] M.G. Lagoudakis and R. Parr. Reinforcement learning as classification: Leveraging modern [7] J. Langford and B. Zadrozny. Relating reinforcement learning performance to classification [8] R.S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforce-

Motivation: Filtering outbound email spam: Sequential decision-making problems Loss does not factorize over individual decisions. Common problem in IT security applications: Attacks incur costs if number of unsuppressed hostile events per time exceeds certain capacity.
  X  that takes sequence of events as input,  X  and outputs upper limit on passing (negative) decisions per 
