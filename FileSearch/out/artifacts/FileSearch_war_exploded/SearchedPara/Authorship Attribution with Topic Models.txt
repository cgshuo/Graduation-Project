 Monash University Monash University Monash University more attention has been given to texts generated by on-line users, such as e-mails and blogs.
Authorship attribution of such on-line texts is a more challenging task than traditional author-ship attribution, because such texts tend to be short, and the number of candidate authors is obtain author representations. In addition to exploring novel ways of applying two popular topic models to this task, we test our new model that projects authors and documents to two disjoint polarity of texts, and predicting the ratings that users would give to items such as movies. 1. Introduction
Authorship attribution has attracted much attention due to its many applications in, for example, computer forensics, criminal law, military intelligence, and humanities research (Juola 2006; Stamatatos 2009; Argamon and Juola 2011). The traditional prob-a set of known candidate authors, whose training texts are supplied in advance (i.e., supervised classification). Whereas most of the early work on authorship attribution focused on formal texts with only a few candidate authors, researchers have recently turned their attention to scenarios involving informal texts and tens to thousands of authors (Koppel, Schler, and Argamon 2011; Luyckx and Daelemans 2011). In parallel, topic models have gained popularity as a means of discovering themes in such large text corpora (Blei 2012). This article explores authorship attribution with topic models, extending the work presented by Seroussi and colleagues (Seroussi, Zukerman, and
Bohnert 2011; Seroussi, Bohnert, and Zukerman 2012) by reporting additional experi-mental results and applications of topic-based author representations that go beyond traditional authorship attribution.
 of corpora through latent factors called topics , which are commonly associated with distributions over words (Blei 2012). For example, in the popular Latent Dirichlet
Allocation (LDA) topic model, each document is associated with a distribution over topics, and each word in the document is generated according to its topic X  X  distribution over words (Blei, Ng, and Jordan 2003). The word distributions often correspond to a human-interpretable notion of topics, but this is not guaranteed, as interpretability depends on the corpus used for training the model. Indeed, when we ran LDA on a data set of movie reviews and message board posts, we found that some word distributions correspond to authorship style as reflected by authors X  vocabulary, with netspeak words such as  X  X anna, X   X  X lot, X  and  X  X aha X  assigned to one topic, and words such as  X  X ompelling X  and  X  X eautifully X  assigned to a different topic. This finding motivated our use of LDA for authorship attribution (Seroussi, Zukerman, and Bohnert 2011). use Rosen-Zvi et al. X  X  (2004) Author-Topic (AT) model to obtain improved authorship attribution results (Seroussi, Bohnert, and Zukerman 2012). However, AT is also limited in that it does not model documents. We addressed this limitation through the Disjoint Author-Document Topic (DADT) model X  X  topic model that draws on the strengths of LDA and AT, while addressing their limitations by integrating them into a single model.
Our DADT model extends the model introduced by Seroussi, Bohnert, and Zukerman (2012), which could only be trained on single-authored texts. In this article, we provide a detailed account of the extended model. In addition, we offer experimental results for five data sets, extending the results by Seroussi, Bohnert, and Zukerman (2012), which were restricted to two data sets of informal texts with many authors. Our experiments show that DADT yields state-of-the-art performance on these data sets, which contain either formal texts written by a few authors or informal texts where the number of candidate authors ranges from 62 to about 20,000.
 also be used to model authors based on multi-authored texts, such as research papers. To demonstrate the potential utility of this capability of the models, we present the results of a preliminary study, where we use AT and DADT to identify anonymous reviewers based on publicly available information (reviewer lists and the reviewers X  publications, which are often multi-authored). Our results indicate that reviewers may be identified with moderate accuracy, at least in small conference tracks and workshops. We hope that these results will help fuel discussions on the issue of reviewer anonymity. indicates that they capture aspects of authorship style, which is known to be indicative of author characteristics such as demographic information and personality traits (Argamon et al. 2009). This is in addition to the well-established result that topic models can be used to represent authors X  interests (Rosen-Zvi et al. 2004). An implication of these results is that topic models may be used to obtain text-based representations of users in scenarios where user-generated texts are available. We demonstrate this by 270 showing how topic models can be utilized to improve the performance of methods we developed to address the popular tasks of polarity inference and rating prediction. discusses LDA, AT, and DADT and the author representations they yield. Section 4 introduces authorship attribution methods, which are evaluated in Section 5. Section 6 presents applications of our topic-based approach, and Section 7 concludes the article. 2. Related Work
Authorship attribution has a long history that predates modern computing. For exam-ple, Mendenhall (1887) suggested that word length can be used to distinguish works by different authors. Modern interest in authorship attribution is commonly traced back to Mosteller and Wallace X  X  (1964) study on applying Bayesian statistical analysis of function word frequencies to uncover the authors of the Federalist Papers (Juola 2006;
Koppel, Schler, and Argamon 2009; Stamatatos 2009). The interest in authorship attri-bution is due to its many applications in areas such as computer forensics, criminal law, military intelligence, and humanities research. In recent years, authorship attribution research has been fuelled by advances in natural language processing, text mining, machine learning, information retrieval, and statistical analysis. This has motivated the organization of workshops and competitions to facilitate the development and comparison of authorship attribution methods (Juola 2004; Argamon and Juola 2011). attribute the text to the correct author out of the candidate authors (Argamon and Juola 2011). Related tasks include open-set attribution, where some test texts may not have been written by any of the candidate authors, and verification , where texts by only one candidate author are supplied in advance, and the task is to verify whether test texts were written by the candidate author (Koppel and Schler 2004; Sanderson and Guenter 2006; Koppel, Schler, and Argamon 2011).
 addressing scenarios with many candidate authors and varying amounts of data per author (Argamon and Juola 2011; Koppel, Schler, and Argamon 2011; Luyckx and
Daelemans 2011). This challenge is illustrated by the corpus chosen for the PAN X 11 competition (Argamon and Juola 2011), which contains short e-mails by tens of authors.
Other examples are Koppel, Schler, and Argamon X  X  (2011) work on a corpus of blog posts by thousands of authors, and Luyckx and Daelemans X  X  (2011) study of the effect of the number of authors and training set size on authorship attribution performance on data sets of student essays. Our approach to authorship attribution addresses this challenge by using topic models, which are known to successfully deal with varying amounts of text (Blei 2012).
 attribution of single-authored texts: Rajkumar et al. (2009) reported preliminary results on using LDA topic distributions as feature vectors for support vector machines (SVM), but they did not compare the results obtained with LDA-based SVM to those obtained with SVM trained on tokens only (we present the results of such a comparison in
Section 5). We know of two related studies that followed the publication of our initial LDA-based results (Seroussi, Zukerman, and Bohnert 2011): Wong, Dras and Johnson X  X  (2011) work on native language identification with LDA, and Pearl and
Steyvers X  X  (2012) study of authorship verification where some of the features are topic distributions. Although Wong, Dras and Johnson reported only limited success (perhaps because an author X  X  native language may manifest itself in only a few words, or maybe due to data-set-specific issues), Pearl and Steyvers found that topical representations helped them achieve state-of-the-art verification accuracy. Pearl and Steyvers X  X  findings further strengthen our hypothesis that topic models yield meaningful author representations. We take this observation one step further by defining our DADT model, and applying it to several authorship attribution scenarios, where it yields better performance than LDA-based approaches and methods based on the AT model (Section 5).
 generic topic models that incorporate metadata labels (Blei 2012). These models can be divided into two types: upstream models, which use the labels to constrain the topics, and downstream models, which generate the labels from the topics (Mimno and McCallum 2008). Generic models have the appealing advantage of obviating the need to define a new model for each new task (e.g., they may be used to obtain author representations by defining a metadata label for each author). However, this advantage may come at the price of increased computational complexity or poorer performance than that of task-specific models (Mimno and McCallum 2008). As the focus of our work is on modeling authors, we experimented only with LDA and with the task-specific topic models discussed in Section 3 (AT and DADT, which model authors explicitly). The applicability of generic models to authorship attribution is an open question that would be interesting to investigate in the future. Nonetheless, most of the generic models surveyed here have properties that make them unsuitable for our purposes.

Jordan 2008), Labeled LDA (Ramage et al. 2009), and DMR (Mimno and McCallum 2008). The former two dedicate at least one topic to each metadata label, making them too computationally expensive to use on data sets with thousands of authors, such as the Blog and IMDb1M data sets (Section 5.1). In contrast to DiscLDA and Labeled
LDA, DMR uses less topics by sharing them between labels. Mimno and McCallum (2008) showed that DMR outperformed AT on authorship attribution of multi-authored documents. Despite this, we decided to use AT, because we found in preliminary exper-iments that AT performs better than DMR on authorship attribution of single-authored texts. Such texts are the main focus of this article. Nonetheless, it is worth noting that
Mimno and McCallum X  X  experiments were performed on a data set of research papers where stopwords were filtered out. We do not discard stopwords in most experiments, because they are known to be indicators of authorship (Koppel, Schler, and Argamon 2009). 1
McAuliffe 2007), which generates labels from each document X  X  topic assignments via a generalized linear model. This model was extended by Zhu, Ahmed, and Xing (2009), who introduced MedLDA, where training is done in a way that maximizes the margin between labels, which is  X  X rguably more suitable X  for inferring document labels. Zhu 272 and Xing (2010) further extended that work by introducing sCTRF, which combines sLDA with conditional random fields to accommodate arbitrary types of features. Zhu and Xing applied these models to the polarity inference task, and found that support vector regression outperformed sLDA and performed comparably to MedLDA (these three models used only unigrams), whereas sCTRF yielded the best performance by incorporating additional feature types (e.g., part-of-speech tags and a lexicon of positive and negative words). Based on these results, we decided to leave experiments with downstream models for future work, as it seems unlikely that we would obtain good results on the authorship attribution task without considering other feature types in addition to token unigrams (which is beyond the scope of this article). 3. Topic Models and Author Representations
This section introduces notation, provides a discussion of the meaning of the parameters used by the topic models, and describes the three topic models considered in this article (LDA, AT, and DADT), focusing on the author representations that they yield. 3.1 Notation and Preliminaries of a matrix M M M is denoted m ij , and vector elements are denoted in lowercase italics with a subscript index (e.g., v i ). Sets are denoted with calligraphic font (e.g., S ). In addition, Dir and Cat denote the Dirichlet and categorical distributions, respectively. mined by the corpus (Section 3.1.1) or configured when using the models (Sections 3.1.2 and 3.1.3). Table 1 shows the models X  corpus-dependent and configurable parameters with their lengths and meanings (scalars have length 1). Corpus-dependent parameters are at the top, configurable document-related parameters are in the middle (for LDA and DADT), and configurable author-related parameters are at the bottom (for AT and DADT). 3.1.1 Corpus-Dependent Parameters. The following parameters depend on the corpus, and are thus considered to be observed: A : Number of authors. We use a  X  { 1, ... , A } to denote an author identifier. D : Number of documents. We use d  X  { 1, ... , D } to denote a document identifier. V : Vocabulary size. We use v  X  { 1, ... , V } to denote a unique word identifier.
N d : Number of words in document d . We use i  X  { 1, ... , N
A A A : Document authors. This is a D -dimensional vector of vectors, where the d -th
W W W : Document words. This is a D -dimensional vector of vectors, where the d -th 3.1.2 Number of Topics. We make a distinction between document topics and author topics.
In both cases,  X  X opics X  describe distributions over all the words in the vocabulary. The difference is that document topics are word distributions that arise from documents, while author topics are word distributions that characterize the authors. LDA uses only document topics, whereas AT uses only author topics. DADT, our hybrid model, uses both document topics and author topics.
 by T ( D ) for the number of document topics and by T topics. Although the models have other configurable parameters (introduced subse-quently), we found that the number of topics has the largest impact on model perfor-mance because it controls the overall model complexity. For example, setting T in LDA means that all the words in all the documents are drawn from the same topic (i.e., a single distribution for all the words), whereas setting T much more freedom to adapt to the corpus, as each word can be drawn from one of 200 distributions.
 have been suggested. For example, Teh et al. (2006) used hierarchical Dirichlet processes to learn the number of topics while inferring the LDA model. We did not experiment with such techniques as they tend to complicate model inference, and we found that using a constant number of topics yields good performance. Nonetheless, we note that utilizing such techniques may be a worthwhile future research direction, especially to determine the balance between document topics and author topics for DADT. 3.1.3 Distribution Priors. The following parameters are the priors of the Dirichlet and beta distributions used by the models. In contrast to the number of topics, which controls model complexity, the priors allow users of the models to specify their prior knowledge and beliefs about the data. In addition, the number of topics imposes a rigid constraint on the inferred model, whereas the effect of the priors on the model is expected to dimin-ish as the amount of observed data increases (Equations (3), (7), and (11)). Indeed, we found in our experiments that varying prior values had a small effect on performance compared to varying the number of topics (Figure 8b, Section 5.3.4). 274 represented by only a few topics. This choice follows the recommendations from
LingPipe X  X  documentation ( alias-i.com/lingpipe ), which are based on empirical evidence from several corpora.
 encode our beliefs about the word distributions. As for the topic distribution priors, ments (yielding sparse word distributions, as indicated earlier), meaning that each topic is expected to assign high probabilities to only a few top words (Steyvers and Griffiths 2007). In contrast to the topic distribution priors, Wallach, Mimno, and McCallum (2009) found in their experiments on LDA that using an asymmetric
Figure 1c). For the same reason, using a symmetric  X   X   X  ( A ) contrast to LDA and AT, our DADT model distinguishes between document words and author words, and thus uses both  X   X   X  ( D ) and  X   X   X  ( A ) prior knowledge that stopword use is indicative of authorship. Thus, for DADT we set  X  v = 0 . 01  X  e and  X  to obtain symmetric priors).
 document words and author words in a given document. Document words (drawn from document topics) are expected to be representative of the documents in the corpus, whereas author words (drawn from author topics) characterize the authors in the corpus. For example, if we asked two different authors to write a report about but the frequencies of non-content words (i.e., function words and other indicators of authorship style) are likely to vary across the reports. In this case, the content words are expected to be allocated to document topics, and the non-content words whose usage varies across authors would be allocated to author topics. In cases where the authors write about different issues, DADT may allocate some content words to author topics (i.e., the meaning of DADT X  X  topics is expected to be corpus-specific). According to DADT X  X  definition (Section 3.4.1), which uses the beta distribution, the prior expected value of the portion of each document that is composed of author words is with a variance of
In our experiments, we chose values for  X  ( D ) and  X  ( A ) and variance, and solving these equations for  X  ( D ) and document (without looking at the actual words in the document). This prior is only used on documents with unobserved authors (i.e., when attributing authors to anonymous texts). Because we have no reason to favor one author over the other, we use a uniform prior, setting  X  a = 1 for each author a . 276 3.2 LDA 3.2.1 Model Definition. LDA was originally defined by Blei, Ng, and Jordan (2003). Here we describe Griffiths and Steyvers X  X  (2004) extended version. The idea behind LDA is that each document in a corpus is described by a distribution over topics, and each word in the document is drawn from its topic X  X  word distribution. Figure 2 presents LDA in plate notation, where observed variables are in shaded circles, unobserved variables are in unshaded circles, and each box represents repeated sampling, with the number of repetitions at the bottom-right corner. Formally, the generative process is: (1) for each topic t , draw a word distribution  X   X   X  ( D ) t  X  Dir  X   X   X  topic z di  X  Cat (  X   X   X  ( D ) d ) , and the word w di  X  Cat ( 3.2.2 Model Inference. Topic models are commonly inferred using either collapsed Gibbs sampling (Griffiths and Steyvers 2004; Rosen-Zvi et al. 2004) or methods based on vari-ational inference (Blei, Ng, and Jordan 2003). We use collapsed Gibbs sampling to infer all models due to its efficiency and ease of implementation. This involves repeatedly sampling from the conditional distribution of the latent parameters, which is obtained analytically by marginalizing over the topic and word distributions, and using the prop-erties of conjugate priors. This conditional distribution is given in Equation (3) (Griffiths and Steyvers 2004; Steyvers and Griffiths 2007): where W W W is the corpus; Z Z Z  X  di contains all the topic assignments, excluding the assign-ment for the i -th word of the d -th document; c ( DT ) dt the di -th topic assignment (i.e., z di ).
 from each chain after a burn-in period, which allows the chain to reach its stationary distribution. For each sample, the topic distributions and the word distributions are estimated using their expected values, given the topic assignments Z Z Z . These expected values are given in Equations (4) and (5): where in this case the counts are over the full topic and author assignments. The the conjugate prior of the categorical distribution (Griffiths and Steyvers 2004). Note that these values cannot be averaged across samples due to the exchangeability of the topics (Steyvers and Griffiths 2007) (e.g., topic 1 in one sample is not necessarily the same as topic 1 in another sample).
 setup, where training texts with known candidate authors are given in advance. Test texts are classified one by one, and the goal is to attribute each test text to one of the candidate authors. As the word distributions of the LDA model inferred in the training phase are unlikely to change much due to the addition of a single test document, in the classification phase we consider each topic X  X  word distribution to be observed, setting it to its expected value according to Equation (5). This yields the following sampling equation for a given test text  X  w  X  w  X  w (  X  w  X  w  X  w is a word vector of length assignments except for the i -th assignment, and  X  c ( DT ) topic t , excluding the i -th assignment.
 expected value according to Equation (4), where c ( DT ) dt exchangeable. This means that we can average the E [  X   X  obtained from the same sampling chain. 3.2.3 Author Representations. LDA does not directly model authors, but it can still be used to obtain valuable information about them. The output of LDA consists of distributions over topics  X   X   X  ( D ) d for each document d . As the number of topics T smaller than the size of the vocabulary V , these topical representations form a lower-dimensional representation of the corpus. The LDA-based author representation we consider in this article is LDA-M (LDA with multiple documents per author), where each author a is represented as the set of distributions over topics of their documents, namely, the set {  X   X   X  ( D ) d | a d = a } , where a d approach is LDA-S (LDA with a single document per author), where each author X  X  documents are concatenated into a single document in a preprocessing step, LDA is run on the concatenated documents, and each author is represented by a single distribution over topics (the distribution of the concatenated document).
 author representation than LDA-M, especially for authors who wrote many documents.
However, this compactness may come at the price of accuracy, as markers that may be present only in a few short documents by one author may lose their prominence 278 if these documents are concatenated with longer documents. It is worth noting that concatenating each author X  X  documents into one document has been named the profile-based approach in previous authorship attribution studies, in contrast to the instance-based approach, where each document is considered separately (Stamatatos 2009).
 authored documents, and there is no straightforward way of extending them to consider multi-authored documents. This limitation is addressed by AT, which we present in the next section. Note that when analyzing single-authored documents, the author representations yielded by AT are equivalent to LDA-S X  X  representations. Therefore, we do not report results obtained with LDA-S. Nonetheless, practitioners may find it easier to use LDA-S than AT due to the relative prevalence of LDA implementations (in fact, our initial modeling approach was LDA-S for exactly this reason). 3.3 AT 3.3.1 Model Definition. AT was introduced by Rosen-Zvi et al. (2004) to model author interests in corpora of multi-authored texts (e.g., research papers). The main idea behind AT is that each document is generated from the topic distributions of its observed authors, rather than from a document-specific topic distribution. Figure 3 presents AT in plate notation. Formally, the generative process is: (1) for each topic t , draw a word distribution  X   X   X  ( A ) t  X  Dir  X   X   X  ( A ) an author x di uniformly from the document X  X  set of authors a a a and the word w di  X  Cat  X   X   X  ( A ) z 3.3.2 Model Inference. As for LDA, we use collapsed Gibbs sampling to infer AT. This involves repeatedly sampling from Equation (7) (Rosen-Zvi et al. 2004, 2010): the assignment for the i -th word of the d -th document; c assignments to author a ; and c ( ATV ) tw all the counts exclude the di -th assignments (i.e., x jointly because this yields faster convergence than separate sampling (Rosen-Zvi et al. 2010).
 values given the author assignments X X X and the topic assignments Z Z Z : where in this case the counts are over the full author and topic assignments. assume that test texts are single-authored). If we did, no sampling would be required to obtain  X  a  X  X  topic distribution because it is already inferred in the training phase (Equa-tion (8)). Hence, we assume that  X  a is a  X  X ew, X  previously unknown author, and utilize
Gibbs sampling to infer this author X  X  topic distribution from Equation (10) (as for LDA, the word distributions are assumed to be observed and set to their expected values according to Equation (9)): assignments except for the i -th assignment, and  X  c ( AT ) to author  X  a (excluding the i -th assignment). Similarly to LDA, we then set expected value according to Equation (8), where c ( AT ) at assignment vector  X  z  X  z  X  z . 3.3.3 Author Representations. AT naturally yields author representations in the form of are generated in an identical manner (Section 3.3.1). To address this limitation, Rosen-
Zvi et al. (2010) introduced  X  X ictitious X  authors, adding a unique  X  X uthor X  to each document. This allows AT to adapt itself to each document without changing the model specification. Therefore, we consider the two following variants: (1) AT :  X  X ure X 
AT, without fictitious authors; and (2) AT-FA : AT, when run with the additional pre-processing step of adding a fictitious author to each document. 3.4 DADT 3.4.1 Model Definition. Our DADT model can be seen as a combination of LDA and
AT, which is meant to address the weaknesses of both models while retaining their strengths. The main idea behind DADT is that words are generated from two disjoint sets of topics: document topics and author topics. Words generated from document topics follow the same generation process as in LDA, whereas words generated from 280 author topics are generated in an AT-like fashion. This approach has the potential ben-efit of separating  X  X ocument X  words from  X  X uthor X  words. That is, words whose use varies across documents are expected to be found in document topics, whereas words whose use varies between authors are expected to be assigned to author topics. Figure 4 presents the graphical representation of the model, where the document-dependent parameters appear on the left-hand side, and the author-dependent parameters appear on the right-hand side. Formally, the generative process is as follows (we mark each step as coming from either L DA or A T , or as new in D ADT ).
 Corpus level:
L. For each document topic t , draw a word distribution  X   X   X 
A. For each author topic t , draw a word distribution  X   X   X 
A. For each author a , draw an author topic distribution  X   X   X  D. Draw an author distribution  X   X   X   X  Dir  X   X   X  .
 Document level. For each document d :
L. Draw d  X  X  document topic distribution  X   X   X  ( D ) d  X  Dir
D. Draw d  X  X  author set a a a d by repeatedly sampling without replacement from Cat (
D. Draw d  X  X  author/document topic ratio  X  d  X  Beta  X  ( A ) Word level. For each word index i  X  { 1, ... , N d } :
D. Draw the author/document topic indicator y di  X  Bernoulli(
L. If y di = 0, use document topics: draw a topic z di  X  Cat
A. If y di = 1, use author topics: Draw an author x di uniformly from a a a sampling from Wallenius X  X  noncentral hypergeometric distribution (Fog 2008) with a weight vector  X   X   X  and a parameter vector whose elements are all equal to 1. In this article, we consider only situations where a a a d is observed when the model is inferred. When handling documents with unknown authors in our authorship attribution experiments, we assume that all anonymous texts are single-authored. 3.4.2 Model Inference. We infer DADT using collapsed Gibbs sampling, as done for LDA and AT. This involves repeatedly sampling from the following conditional distribution of the latent parameters: where Y Y Y  X  di contains the topic indicators, excluding the di -th value; and c are the counts of words assigned to document or author topics in document d , respec-tively. The other variables are defined as for LDA and AT. Here, all the counts exclude the di -th assignments (i.e., x di , y di , and z di ).

Equation (3) is contained in the y = 0 case, where the word is drawn from document topics, and AT X  X  Equation (7) is contained in the y = 1 case, where the word is drawn from author topics. However, Equation (11) also demonstrates the main difference between DADT and its building blocks, as DADT considers both documents and authors during the inference process by assigning each word to either a document topic or an author topic, where document topics and author topics come from disjoint sets.
For each sample, the topic and word distributions are estimated using their expected values given the latent variable assignments. The expected values for the topic and word distributions are the same as for LDA and AT, and the expected values for the author/document ratio and the corpus author distribution are: where the counts are now over the full assignments X X X , Y Y Y , and Z Z Z . As in LDA and AT, these equations were straightforward to obtain, because the Dirichlet distribution is the conjugate prior of the categorical distribution and the beta distribution is the conjugate prior of the Bernoulli distribution. It is worth noting that because we assume that the documents X  authors are observed during model inference, the expected value of each element of the corpus distribution over authors  X  a does not vary across samples, as it only depends on the prior  X  a and on author a  X  X  count of documents in the corpus c
AT, we assume that  X  a is a previously unknown author. We set the word distributions to their expected values from the training phase, and infer  X  a  X  X  author topic distribution 282 together with the test text X  X  document topic distribution topic ratio  X   X  by repeatedly sampling from where  X  y i is the topic indicator for the i -th word,  X  y  X  y  X  y except for the i -th indicator, and  X  c ( DD ) and  X  c to document and author topics, respectively, excluding the i -th assignment (the other variables are defined as in Equations (6) and (10)). The expected values of are the same as for LDA and AT, respectively. The expected value of  X  replacing c ( DA ) d and N d with  X  c ( DA ) and  X  N in Equation (12) (where  X  c counts over the full vector of indicators  X  y  X  y  X  y ). 3.4.3 Author Representations and Comparison to LDA and AT. DADT can be seen as a gen-eralization of LDA and AT X  X etting DADT X  X  number of author topics T a model that is equivalent to LDA, and setting the number of document topics T zero yields a model that is equivalent to AT. An advantage of DADT over LDA and
AT is that both documents and authors are accounted for in the model X  X  definition, and are represented via distributions over document and author topics, respectively. Hence, preprocessing steps such as concatenating each author X  X  documents or adding fictitious authors X  X s done in LDA-S and AT-FA to obtain author and document representations, respectively X  X re unnecessary.
 seem most similar to AT-FA. However, there are several key differences between DADT and AT-FA.
 each topic set. Thus, the number of author topics T ( A ) of document topics T ( D ) , which enables us to vary the number of author and document topics according to the number of authors and documents in the corpus. For example, in the judgment data set (Section 5.1.1), which includes only a few authors that wrote many long documents, we expect that small values of T suffice to get good author representations. By contrast, modeling the 19,320 authors of the Blog data set (Section 5.1.5) is expected to require many more author topics. On such large data sets, using more than a few hundred topics may become too computationally expensive, because adding topics increases model complexity and thus adds to the runtime of the inference algorithm. Hence, being able to specify the balance between document and author topics in such cases is beneficial (Section 5.4).
 words are strong indicators of authorship (Koppel, Schler, and Argamon 2009). Our model allows us to encode this prior knowledge by giving elements that correspond to stopwords in  X   X   X  ( A ) higher weights than such elements in property of DADT has practical benefits, as it improved the accuracy of DADT-based authorship attribution methods in our experiments (Section 5). document basis, and makes it possible to specify a prior belief of what this ratio should be. We show that this has practical benefits in our authorship attribution document is composed of author words can yield better results than using AT X  X  fic-titious author approach that evenly splits each document into author and document words.
 number of texts by each author when performing authorship attribution. In addition, this enables the use of DADT in a semi-supervised setup by training on documents with unknown authors X  X n extension that is left for future work. 4. Authorship Attribution Methods This section introduces the authorship attribution methods considered in this article. In
Section 4.1, we discuss our baseline method (SVM trained on tokens), and Sections 4.2, 4.3, 4.4, and 4.5 introduce methods based on LDA, AT, AT-FA, and DADT, respectively. These methods are summarized in Table 2.
 dimensionality reduction and probabilistic.
 verted to topic distributions, and the topic distributions are used as input to a classifier.
Generally, this approach makes it possible to use classifiers that are too computationally expensive to use with a large feature set, e.g., Webb, Boughton and Wang X  X  (2005) AODE reduced document representations as input to SVM, and compare their performance with the performance obtained with SVM trained directly on tokens (denoted Token
SVM ). This allows us to roughly gauge how much information is lost by converting texts from token representations to topic representations. However, this approach ignores the probabilistic nature of the underlying topic model, and thus does not fully test the utility of the author representations yielded by the model X  X hese are better tested by the next approach.
 284 underlying model X  X  definitions directly to estimate the probability that a given author wrote a given test text. These methods require the model to be aware of authors, which means that LDA cannot be used in this case. We expect this approach to outperform the dimensionality reduction approach because the probabilistic approach considers the structure of the topic model.
 distance) to find the author whose topic distributions are closest to the distributions inferred from the test text. We do not describe distance-based methods in this article because we found that they yield poor results in most cases (Seroussi 2012), probably because they do not fully consider the underlying structure of the topic model. 4.1 Baseline: Token SVM
Our baseline method is SVM trained on token frequency features (i.e., token counts divided by the total number of tokens in the document). This method is known to yield state-of-the-art authorship attribution performance on this feature set; that is, when comparing methods without any further feature engineering, Token SVM is expected to yield good performance with minimal tuning (Koppel, Schler, and Argamon 2009). We use the one-versus-all setup to handle non-binary authorship attribution scenarios. effective as other multi-class SVM approaches in many cases (Rifkin and Klautau 2004). with the goal of maximizing the authorship attribution accuracy, which may give Token
SVM an advantage over topic-based methods. Further, as a discriminative classification approach, SVM may yield better performance than probabilistic topic-based methods, which are generative classifiers (Ng and Jordan 2001). However, as demonstrated by
Ng and Jordan X  X  comparison of discriminative and generative classifiers, this better performance may only be obtained in the presence of  X  X nough X  training data (just how much data is  X  X nough X  depends on the data set). 4.2 Methods Based on LDA 4.2.1 Dimensionality Reduction: LDA-SVM. Using LDA for dimensionality reduction is topic distributions as described in Section 3.2.2, and using these topic distributions as classifier features. Because we use SVM, it is possible to directly compare the results obtained with the LDA-SVM method to the baseline results obtained by running SVM trained directly on token frequencies.
 the dimensionality reduction capabilities of LDA on the task of classifying articles according to a set of predefined categories. To the best of our knowledge, only Rajkumar et al. (2009) have previously applied LDA-SVM to authorship attribution X  X hey pub-lished preliminary results obtained by running LDA-SVM, but did not compare their results to a Token SVM baseline. In Section 5, we present the results of more extensive experiments on the applicability of this approach to authorship attribution. 4.3 Methods Based on AT 4.3.1 Dimensionality Reduction: AT-SVM. We cannot use AT to obtain document topic distributions, because AT only infers author topic distributions (Section 3.3). Hence, we train the SVM component on the author topic representations (each document is written by a previously unknown author, infer this author X  X  topic distribution tion 3.3.2), and classify this distribution. This may be seen as very radical dimensionality reduction, because each author X  X  entire set of training documents is reduced to a single author topic distribution. 4.3.2 Probabilistic: AT-P. For each author a , AT-P calculates the probability of the test text words given the AT model inferred from the training texts, under the assumption that highest:
This method does not require any topic inference in the classification phase, because the author topic distributions  X   X   X  ( A ) and topic word distributions training time. It is worth noting that we use the log of this probability for reasons of numerical stability.
 AT-SVM because AT-P relies directly on the probabilistic structure of the AT model.
In addition, AT-P has the advantage of not requiring any topic inference in the classifi-cation phase.
 the test text was co-written by all the candidate authors, (2) infers the word-to-author assignments for the test text, and (3) returns the author that was attributed the most words. However, we found that this method performs poorly in comparison with other
AT-based approaches in three-way authorship attribution. In addition, this method was too computationally expensive to run in cases with many authors, as it requires iterating through all the authors for every test text word in each sampling iteration. 4.4 Methods Based on AT-FA an additional fictitious author to each training document. Hence, different constraints with a fictitious author, since we do not know who wrote the test text. Hence, if we were to assume that the real author is a previously unknown author, as done for AT, we would have no way of telling the previously unknown author from the fictitious author, because they are both unique to the test text. We consider two possible ways of addressing this: 1. Assume that the test text was written only by a real, previously unknown, 2. For each training author a , assume that the test text was written by a together with 286
Although the second alternative may appear more attractive because it does not violate the fictitious author assumption of AT-FA, we cannot use it with the dimensionality reduction method (AT-FA-SVM, as described in the following section), as this method requires inferring the topic distribution of the previously unknown author 4.4.1 Dimensionality Reduction: AT-FA-SVM. AT-FA yields a topic distribution for each training document (i.e., the topic distribution of the fictitious author associated with the document), and a topic distribution for each real author (all the distributions are over the same topic set). We convert each training document to the concatenation of these two distributions, and use this concatenation as input to the SVM component. In the classification phase, we assume that the test text was written by a single previously unknown author, and represent the test text as the concatenation of the inferred topic distribution  X   X   X   X   X   X  ( A ) to itself.
 catenating the same distribution to itself, because DADT differentiates between author topics and document topics X  X  distinction that AT-FA attempts to capture through fictitious authors. Hence, we expect the DADT-SVM approach, which we define in Section 4.5, to perform better than AT-FA-SVM. Nonetheless, we also experiment with
AT-FA-SVM for the sake of completeness. 4.4.2 Probabilistic: AT-FA-P. For the probabilistic approach, we consider two variants, matching the two alternatives outlined earlier. 4.5 Methods Based on DADT 4.5.1 Dimensionality Reduction: DADT-SVM. DADT yields a document topic distribu-tion  X   X   X  ( D ) d for each document d , and an author topic distribution
Similarly to AT-FA-SVM, we convert each training document to the concatenation of these two distributions, and use this concatenation as input to the SVM component. that the test text was written by a previously unknown author, and obtain the test phase, test texts are represented as the concatenation of these two distributions. tain the assumptions of DADT in the classification phase, which we cannot do in AT-FA-SVM. Further, DADT-SVM should perform better than AT-SVM, because DADT-
SVM accounts for differences between individual documents, whereas AT-SVM repre-sents each author using a single training instance. Hypothesizing about the expected performance of DADT-SVM in comparison to LDA-SVM is harder: We expect per-formance to be corpus-dependent to a certain degree X  X n data sets where differences between individual documents are important, LDA-SVM may have an advantage, as all the words are allocated to document topics. On the other hand, in data sets where the differences between authors are more important, DADT-SVM may outperform LDA-
SVM because it represents the authors explicitly. 4.5.2 Probabilistic: DADT-P. This method assumes that the test text was written by a previously unknown author, infers the test text X  X  document topic distribution the author/document topic ratio  X   X  , and returns the most probable author according to the following equation: approach that avoids sampling  X   X  and  X   X   X   X   X   X  ( D ) probably because it  X  X orces X  all the words to be author words, including words that are very likely to be document words. In addition, we found that following an approach comparable performance to sampling only once by following the previously-unknown author assumption. However, the former approach is too computationally expensive to run on data sets with many candidate authors. Hence, we present only the results obtained with the approach that performs sampling only once. 5. Evaluation
This section presents the results of our evaluation. We first describe the data sets we used (Section 5.1) and our experimental setup (Section 5.2), followed by the results of our experiments on the Judgment and PAN X 11 data sets (Section 5.3). Then, we present the results of a more restricted set of experiments on the larger IMDb62, IMDb1M, and
Blog data sets (Section 5.4) and summarize our key findings (Section 5.5). 288 5.1 Data Sets We experimented with five data sets: Judgment, PAN X 11, IMDb62, IMDb1M, and
Blog. Judgment, IMDb62, and IMDb1M were collected and introduced by us, and are freely available for research use (Judgment can be downloaded from www.csse. monash.edu.au/research/umnl/data , and IMDb62 and IMDb1M are available upon request). The two other data sets were introduced by other researchers, are publicly available, and were used to facilitate comparison between our methods and previous work. Table 3 presents some data set statistics. 5.1.1 Judgment. The Judgment data set contains judgments by three judges who served on the Australian High Court from 1913 to 1975: Dixon, McTiernan, and Rich (ab-breviated to D, M, and R, respectively, in Table 3). We created this data set to verify rumors that Dixon ghost-wrote some of the judgments attributed to McTiernan and
Rich (Seroussi, Smyth, and Zukerman 2011). This data set is an example of a traditional authorship attribution data set, as it contains only three authors who wrote relatively long texts in a formal language. In this article, we only use judgments with undisputed authorship, which were written in periods when only one of the three judges served on the High Court (Dixon X  X  1929 X 1964 judgments, McTiernan X  X  1965 X 1975 judgments, and Rich X  X  1913 X 1928 judgments). We removed numbers from the texts to ensure that dates cannot be used to discriminate between judges. We also removed quotes to ensure that the classifiers take into account only the actual authors X  language use (removal was done automatically by matching regular expressions for numbers and text in quotation marks). Because all three judges dealt with various topics, it is likely that successful methods would have to consider each author X  X  style, rather than rely solely on content features in the texts.
 the data sets we considered, but these authors wrote more texts than the average author in PAN X 11, IMDb1M, and Blog. Judgments are also substantially longer than the texts in all the other data sets, which should make authorship attribution on the Judgment data set relatively easy.
 5.1.2 PAN X 11. The PAN X 11 data sets were introduced as part of the PAN 2011 compe-tition (available from pan.webis.de ) (Argamon and Juola 2011). These data sets were extracted from the Enron e-mail corpus ( www.cs.cmu.edu/ ~ to emulate closed-class and open-class authorship attribution and authorship verifica-tion scenarios (Section 2). These data sets represent authorship attribution scenarios that may arise in computer forensics, such as the case noted by Chaski (2005), where an employee who was terminated for sending a racist e-mail claimed that any person with access to his computer could have sent the e-mail.

Unlike the other data sets we used, this data set is split into training, validation, and testing subsets (abbreviated to Trn, Vld, and Tst, respectively, in Table 3). We focused on the closed-class problem, using the validation and testing sets that contain texts only by training authors. The only change we made to the original data set was dropping two training and two validation texts that were automatically generated, which were detected by length and content. This had a negligible effect on method accuracy, but made the statistics in Table 3 more representative of the data (e.g., the mean count of tokens per text is 65.3 in the validation set without the two automatically generated texts, compared with 338.3 in the full validation set).
 more authors than in traditional authorship attribution. As Table 3 shows, the PAN X 11 data set contains the shortest texts of the data sets we considered. This fact, together with the training/validation/testing structure of the data set, make it possible to run many experiments on this data set before moving on to larger data sets. 5.1.3 IMDb62. IMDb62 contains 62,000 movie reviews and 17,550 message board posts by 62 prolific users of the Internet Movie database (IMDb, www.imdb.com ). We intro-duced this data set (Seroussi, Zukerman, and Bohnert 2010) to test our author-aware polarity inference approach (Section 6.2). Each user wrote 1,000 reviews (sampled from mostly movie-related, but may also be about television, music, and other topics. This themes, and the number of authors is relatively small, but is already much larger than the number of authors considered in traditional authorship attribution settings. Unlike the other data sets of informal texts, IMDb62 consists only of prolific authors, allowing us to test our approach in a scenario where training data is plentiful. 5.1.4 IMDb1M. Although the IMDb62 data set is useful for testing our methods on small-to medium-scale problems, it cannot be seen as an adequate representation of large-typical data sets contain thousands of users (Section 6.3). Hence, we created IMDb1M by randomly generating one million valid IMDb user IDs and downloading the reviews and message board posts written by these users (Seroussi, Bohnert, and Zukerman 2011). Unfortunately, most of the randomly generated IDs led to users who submit-ted neither reviews nor posts X  X e found that about 5% of the entire user population submitted posts, and less than 3% wrote reviews. After filtering out users who have not submitted any rated reviews, we were left with 22,116 users. These users, who make up the IMDb1M data set, submitted 204,809 posts and 66,816 rated reviews.
 us to test scenarios in which the user population is made up of prolific users, whereas 290
IMDb1M contains a more varied sample of the population. However, because we did not impose a minimum threshold on the number of reviews or posts, the IMDb1M 56% of the users in IMDb1M wrote only one text). It is worth noting that three users appear in both IMDb62 and IMDb1M. In IMDb62 these three users authored 3,000 reviews and 268 posts in total (about 4.8% of the total number of reviews and 1.5% of the posts), and in IMDb1M they authored 5,695 reviews and 358 posts (about 8.5% of the re-sampling we performed when we created IMDb62, and the difference in the number of posts is due to the time difference between the creation of the two data sets. blog posts by 19,320 authors (available from u.cs.biu.ac.il/ by Schler et al. (2006) to learn about the relation between language use and demographic characteristics, such as age and gender. We use this data set to test how our authorship attribution methods scale to handle thousands of authors. As blog posts can be about any topic, this data set is less restricted than the Judgment, PAN X 11, and IMDb data sets.
Further, the large number of authors ensures that every topic is likely to interest at least several authors, meaning that methods that rely only on content are unlikely to perform as well as methods that also take author style into account. 5.2 Experimental Setup
We used different experimental setups, depending on the data set. PAN X 11 experiments followed the setup of the PAN X 11 competition (Argamon and Juola 2011): We trained all the methods on the given training data set, tuned the parameters according to results obtained for the given validation data set, and ran the tuned methods on the given testing data set. For all the other data sets we utilized ten-fold cross validation. In all cases, we report the overall classification accuracy, that is, the percentage of test texts correctly attributed to their author. Statistically significant differences are reported when p &lt; 0 . 05 according to McNemar X  X  test (when reporting results in a table, the best result for each column is in boldface, and several boldface results mean that the differences between them are not statistically significant).

LIBLINEAR (Fan et al. 2008), which is well suited for large-scale text classification. We experimented with cost parameter values from the set { ... , 10 accuracy improvement was obtained (starting from 10 0 = 1 and going in both direc-tions). We report the results obtained with the value that yielded the highest accuracy, which gives an optimistic estimate for the performance of the Token SVM baseline. with a burn-in of 1,000 iterations. In the Judgment, PAN X 11, and IMDb62 experiments, we retained eight samples per chain with a spacing of 100 iterations. In the IMDb1M and Blog experiments, we retained one sample per chain due to runtime constraints.
Because we cannot average topic distribution estimates obtained from training samples due to topic exchangeability (Steyvers and Griffiths 2007), we averaged the probabilities calculated from the retained samples. In the dimensionality reduction experiments, we used the topic distributions from a single training sample to ensure that the number of features is substantially reduced (an alternative approach would be to use the concate-nation of all the samples, but this may result in a large number of features, and em-ploying this alternative approach did not improve results in preliminary experiments).
For test text sampling, we used a burn-in of 10 iterations and averaged the parameter estimates over the next 10 iterations in a similar manner to the procedure used by Rosen-
Zvi et al. (2010). We found that these settings yield stable results across different random seed values.
 baseline, all methods were trained on the same token representations of the texts. In most experiments, we did not apply any filters and simply used all the tokens as they appear in the text. In some cases, as indicated throughout this section, we either retained only stopwords or discarded the stopwords in a preprocessing step that was applied before running the methods. This allowed us to obtain rough estimates of the effect of considering only style words, considering only content words, and considering both style and content. However, we note that this is only a crude way of separating style and content, because some stopwords may contain content clues, whereas some words that do not appear in the stopword list may be seen as indicators of personal style, regardless of content.
 the effect of other configurable parameters is smaller (Section 3.1). Hence, we used symmetric topic priors, setting all the elements of and min { 0 . 1, 5 / T ( A ) } , respectively. For all models, we set the base measure for the prior of words in topics. Because DADT allows us to encode our prior knowledge that stopword use is indicative of authorship, we set and  X  ( A ) w = 0 . 01 + e for all w , where w is a stopword. Unless otherwise specified, we set e = 0 . 009, based on tuning experiments on Judgment and PAN X 11 (Section 5.3). Sim-ilarly, we set  X  ( D ) = 1 . 222 and  X  ( A ) = 4 . 889 for DADT, based on the same experiments.
In addition, we set  X  a = 1 for each author a , yielding smoothed estimates for the corpus distribution of authors  X   X   X  . 5.3 Experiments on Small Data Sets which contains judgments by three judges, and on the PAN X 11 data set, which con-tains e-mails by 72 authors. Authorship attribution on the PAN X 11 data set is more challenging than on the Judgment data set, because PAN X 11 texts are shorter than judgments, and some of the PAN X 11 authors wrote only a few e-mails. We first present the results obtained with LDA, followed by the results obtained with AT (with and without fictitious authors), and with our DADT-based methods, which yielded the best performance. We end this section with experiments that explore the effect of applying stopword filters to the corpus in a preprocessing step. These experiments demonstrate that our DADT-based approach models authorship indicators other than content words. data set. On PAN X 11, we tuned the methods on the validation subset and report the results obtained on the testing subset with the settings that yielded the best validation results (i.e., each method was run multiple times on the validation subset and only once on the testing subset). We present some tuning results together with testing results to illustrate the behavior of the various methods. It is worth noting that for most methods,
PAN X 11 testing results are better than the best validation results. This may be because on average testing texts are about 10% longer than validation texts (Section 5.1.2). 5.3.1 LDA. Figure 5 presents the results of the LDA experiment, with Judgment results in
Figure 5a, and PAN X 11 validation and testing results in Figures 5b and 5c, respectively. 292 fier on LDA topic distributions (LDA-SVM with 100 topics) was somewhat worse than that obtained by training directly on tokens (Token SVM), but was still much better than a majority baseline (the differences between LDA-SVM and both the Token SVM and majority baselines are statistically significant in all cases). This indicates that although some authorship indicators are lost when using LDA for dimensionality reduction, many are retained despite the fact that LDA X  X  document representations are much more compact than the raw token representations.
 set, though on Judgment the difference between LDA-SVM and Token SVM is much smaller. The reason for this difference may be that LDA does not consider authors in the model-building stage. Although this had a relatively small effect on performance in the three-way judgment attribution scenarios, it appears that accounting for authors is important in scenarios with many authors. As the rest of this article deals with such scenarios, we decided not to use LDA for modeling authors in subsequent sections. 5.3.2 AT. Figure 6 presents the results of the AT experiment, with Judgment results in Figure 6a and PAN X 11 validation and testing results in Figures 6b and 6c, respectively. is probably because AT-SVM X  X  dimensionality reduction is more radical than LDA-
SVM X  X : In AT-SVM, each document is reduced to the same distribution over author topics because AT does not model individual documents (Section 4.3). Notably, AT-
SVM X  X  performance was very poor when 200 and 400 topics were used, possibly because the more fine-grained topic distributions yielded by using more topics resulted in sparser author representations (where some topics were allocated only a few words), which may have caused the SVM component to overfit. This trend is more pronounced in the Judgment results than in the PAN X 11 results: On Judgment, AT-SVM with 200 and 400 topics yielded poorer results than the majority baseline, probably because the effect of sparsity is larger when considering three authors than when modeling 72 authors. jority baseline on both data sets. Although AT-P performed comparably to Token SVM on the PAN X 11 data set, it was significantly outperformed by Token SVM on the
Judgment data set. Nonetheless, these results indicate that AT captures many of the indicators required for authorship attribution. This is despite the fact that AT was not designed with authorship attribution in mind. Hence, it represents each author with a single distribution over topics while ignoring differences and similarities between documents (which may be important for the authorship attribution task). This stands in contrast to the Token SVM baseline, which attempts to build a document-based model that is optimized for the classification goal of authorship attribution (Section 4.1). 5.3.3 AT-FA. Figure 7 presents the results of the AT-FA experiment, with Judgment respectively.
 significantly lower than that obtained by the corresponding methods in the AT case without fictitious authors (AT-SVM and AT-P, respectively). This may seem surprising, because the only difference between AT and AT-FA is the addition of a fictitious author for each document, which was shown to improve AT X  X  ability to predict unseen por-tions of documents (Rosen-Zvi et al. 2010). However, the reason for AT-FA-SVM and
AT-FA-P1 X  X  poor performance may be that they do not conserve the underlying as-sumption of fictitious authors in the classification stage, i.e., they do not assume that the test text was written by a fictitious author together with a previously unseen author (Section 4.4). This is probably also the reason why the probabilistic AT-FA-P2 signifi-294 cantly outperformed AT-FA-P1 by a large margin on the Judgment data set X  X T-FA-P2 conserves the fictitious author assumption, whereas AT-FA-P1 ignores it (we did not run the AT-FA-P2 method on PAN X 11 because it requires running a separate sampling chain for each candidate author and test text, which makes it too computationally expensive to run in cases with many candidate authors and test texts).
 significantly outperformed by Token SVM for all topic numbers, but yielded signifi-cantly better performance than the majority baseline. Despite the fact that AT-FA-P2 was outperformed by Token SVM, the margin was not large when enough topics were used (AT-FA-P2 yielded its best accuracy of 89.60% with 100 topics, in comparison with
Token SVM X  X  accuracy of 91.15%). This indicates that representing both documents and authors in the topic model may have advantages in terms of authorship attribution. This further motivates the use of our DADT model, which considers documents and authors without requiring the preprocessing step of adding fictitious authors. 5.3.4 DADT. Figure 8a presents the results of the DADT experiment on the Judg-ment data set, obtained with 10 author topics, 90 document topics, and prior settings subsequently). These results are compared to the baselines (majority and Token SVM), and to the best topic-based result obtained on this data set thus far (by AT-FA-P2 with 100 topics). As we can see, the best DADT-based result was obtained with the proba-bilistic DADT-P method, which significantly outperformed all the other methods. This demonstrates the effectiveness of our DADT model in capturing author characteristics that are relevant to authorship attribution.

SVM X  X  relatively weak performance may be because its use of document topics introduces noise that causes the SVM component to underperform, as DADT X  X  document topics are not expected to be indicative of authorship.

DADT on the Judgment data set is illustrated by Figure 9, which presents three docu-ment topics and three author topics in word-cloud form. The top 50 tokens from each topic are shown, where the size and shade of each token indicates its frequency in the topic. This anecdotal sample of topics reflects the general trend that we noticed in this data set, where document topics represent different types of cases, and the top tokens in author topics do not carry content information and are dominated by stopwords 296 and punctuation (LDA and AT topics were similar to DADT X  X  author topics due to the prevalence of stopwords and the lack of document X  X uthor separation in these models). of different types, and thus content words are unlikely to carry enough information to adequately represent the judges. As discussed in Section 3.1.3, this separation of content and style is corpus-dependent and is expected to occur only in cases where content is independent of author identity. Indeed, we did not observe such a clear separation in our experiments on other data sets.

Somewhat surprisingly, these settings did not have a large effect on the performance of the methods in most cases. This is demonstrated by the results presented in Figure 8b, which were obtained by varying the values of these parameters and running the DADT-
P method. As Figure 8b shows, the results obtained with a setting of which can be seen as encoding no strong prior belief about the document/author word balance in each document (it is equivalent to setting a uniform prior on this balance), were comparable to the results obtained with  X  ( D ) = 1 . 222 and changing e from 0 to 0 . 009 only had a minor effect on the results. The only setting that made a relatively large difference is the document/author topic split: Changing it from 90/10 to 10/90 yielded poorer results. However, the 50/50 split yielded close results to the 90/10 split, which shows that in this case, the document/author topic split setting is only sensitive to relatively large variations.
 settings for each method would allow us to obtain somewhat improved results. How-ever, such a search would be computationally expensive, as the model needs to be retrained and tested for each fold, parameter set, and method. Therefore, we decided to present the results obtained with the non-optimized settings, which are sufficient to demonstrate the merits of our DADT approach, as DADT-P outperformed all the other methods discussed so far.
 of topics yielded the best topic-based results of the models and methods whose results based on LDA, AT, and AT-FA). Figure 10b shows the results of tuning DADT X  X  settings and running DADT-P on the PAN X 11 validation set. The PAN X 11 tuning experiment shows a clearer picture in terms of accuracy differences between different parameter settings than the Judgment experiments. Specifically, when we used uninformed uni-form priors on the document/author word split (  X  ( D ) = in-topic priors for both document and author words ( e = 0), the obtained accuracy was comparable to AT-P X  X  accuracy. On the other hand, setting which encodes our prior belief that on average 80% (with a standard deviation of 15%) of each document is composed of author words, significantly improved performance.
Setting e = 0 . 009 to encode our prior knowledge that stopwords are indicators of au-thorship yielded an additional improvement. Finally, the last two results in Figure 10b demonstrate the importance of having enough topics to model the authors: Accuracy dropped by about 4 percentage points when we used 50 author topics and 50 document topics, and by about 24 percentage points when we used only 10 author topics and 90 document topics, rather than 90 author topics and 10 document topics. This leads us to conjecture that it would be beneficial to pursue a future extension that learns the topic balance automatically, e.g., in a similar manner to Teh et al. X  X  (2006) method of inferring the number of topics in LDA.
 using the best setting from Figure 10b: 10 document topics, 90 author topics, 1 . 222,  X  ( A ) = 4 . 889, and e = 0 . 009. As Figure 10a shows, DADT-P, which obtained the outperformed Token SVM. This implies that our DADT model is the most suitable of the models we considered for capturing patterns in the data that are important for authorship attribution, at least in scenarios that are similar to the PAN X 11 case. in the PAN X 11 competition (Argamon and Juola 2011) (competitors were ranked accord-ing to macro-averaged and micro-averaged precision, recall, and F1; the micro-averaged measures are all equivalent to the accuracy measure in this case, because each of the test texts is assigned to a single candidate author). However, to the best of our knowledge,
DADT-P obtained the best accuracy for a fully supervised method that uses only uni-gram features. Specifically, Kourtis and Stamatatos (2011), who obtained the highest ac-curacy (65.8%), assumed that all the test texts are given to the classifier at the same time and used this additional information with a semi-supervised method, whereas Kern et al. (2011) and Tanguy et al. (2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies, respectively, used various feature types (e.g., features obtained from parse trees). In addition, preprocessing differences make it hard to compare the methods on a level playing field. Nonetheless, we note that extending DADT to enable semi-supervised classification and additional feature types are promising directions for future work. 5.3.5 Testing the Effect of Stopwords. The results reported up to this point were all obtained by running the methods on document representations that include all the tokens. As discussed in Section 5.2, discarding or retaining stopwords provides a crude way of separating style from content. We ran a set of experiments where we either discarded 298 stopwords in a preprocessing step or retained only stopwords, and then ran the Token
SVM baseline and the DADT-P method, which obtained the best performance when all the tokens were used (DADT was run with the same settings used to obtain the tuned results from the previous section).
 carding stopwords caused the Token SVM baseline to yield poorer performance than when all the tokens were used, but retaining only stopwords significantly improved
Token SVM X  X  performance on Judgment and yielded a substantial drop in performance on PAN X 11. Interestingly, this was not the case with DADT-P, where either discarding or retaining stopwords caused a statistically significant drop in performance in compari-son with using all the tokens. The reason why DADT-P X  X  performance dropped when only stopwords were used may be that DADT was designed under the assumption that all the tokens in the corpus are retained. However, we are encouraged by the fact that
DADT-P X  X  performance drop on Judgment was not very large when only stopwords were retained, as it indicates that DADT captures stylistic elements in the authors X  texts. mance than Token SVM when using feature sets that included all the tokens or all the tokens without stopwords. DADT-P appears to harness the extra information from non-stopword tokens more effectively than Token SVM, despite the fact that such tokens tend to occur less frequently in the texts than stopwords. Further, the vocabulary size of these two feature sets is larger than that of the stopword-only feature set, which suggests that DADT-P is more resilient to noise than Token SVM.
 stopwords are retained. For example, the phrase  X  X n my opinion X  appears in texts by all three authors in the Judgment data set, but is used more frequently by McTiernan (it occurs in about 82% of his judgments) than by Dixon (69%) or Rich (58%). As the frequency of this phrase is apparently dependent on author style and independent of the specific content of a given judgment, it is probably safe to assume that it would be beneficial to retain the word  X  X pinion X  (this is also evidenced by the dominance of this word in the third author topic in Figure 9). However, this word does not appear in our stopword list. This problem is more pronounced in the PAN X 11 data set, where it appears that other words beyond stopwords are also indicative of authorship. For instance, Tanguy et al. (2011) used the openings and closings of the e-mails in the data set as separately weighted features. Openings can start with words such as  X  X ello, X   X  X i, X   X  X ey, X  and  X  X ear, X  but only the first two words appear in our stopword list, meaning that even when only stopwords are retained some stylistic features are lost. These exam-ples highlight the difficulties in extracting words that are truly content-independent X  X  problem that would be especially relevant when trying to adapt an authorship classifier trained on texts from one domain to texts from a completely different domain (this specific stopwords X  X or example, by extracting a list of frequent words X  X ut this gives rise to new problems, such as determining a frequency threshold. We decided not to pursue such a solution because the PAN X 11 results show that improved performance is not guaranteed when only stopwords are retained, even when Token SVM is used.
Hence, in the remainder of this article we use all the words, that is, we neither discard stopwords nor retain only stopwords. 5.4 Experiments on Large Data Sets In this section, we report the results of our experiments on the IMDb62, IMDb1M, and
Blog data sets. Both IMDb data sets contain movie reviews and message board posts, with IMDb62 consisting of texts by 62 prolific authors (with at least 1,000 texts each), and IMDb1M consisting of texts by 22,116 authors, who are mostly non-prolific. The
Blog data set contains blog posts by 19,320 authors, and is the largest of the data sets we considered in terms of token count X  X t contains about 168 million tokens, whereas
IMDb62 and IMDb1M contain about 22 and 34 million tokens, respectively. In addition to running experiments on the full Blog data set, we considered a subset that contains all the texts by the 1,000 most prolific authors (this subset contains about 69 million tokens overall in 332,797 posts X  X bout 49% of the posts in the full Blog data set). on IMDb62, IMDb1M, and Blog than on the Judgment and PAN X 11 data sets (which contain about 3 and 0.74 million tokens, respectively). We ran only the Token SVM baseline, AT-P, and DADT-P, as these methods yielded the best performance in the
PAN X 11 experiments. We set the overall number of topics of AT and DADT to 200 topics for IMDb62, and 400 topics for IMDb1M and Blog. We set DADT X  X  document/author topic split to 50/150 for IMDb62 and 50/350 for IMDb1M and Blog, and used the prior setting that yielded the best PAN X 11 results (  X  ( D ) = 1 . 222, As in the PAN X 11 experiments, we determined the overall number of topics based on
AT-P X  X  performance with 25, 50, 100, 200, and 400 topics. The document/author topic splits we tested were 10/190, 50/150, and 100/100 for IMDb62, and 10/390, 50/350, and 100/300 for IMDb1M and Blog.

DADT-P consistently outperformed AT-P, which indicates that using disjoint sets of document and author topics yields author representations that are more suitable for authorship attribution than using only author topics. In contrast to the previous exper-iments, Token SVM outperformed DADT-P in one case: the IMDb62 data set. This may be because discriminative methods (such as Token SVM) tend to outperform generative 300 methods (such as DADT-P) in scenarios where training data is abundant (Ng and Jordan 2001), which is the case with IMDb62 X  X t contains at least 900 texts per author in each training fold.
 on the full Blog data set, the topic-based methods experienced a larger drop in accuracy than Token SVM when transitioning from the prolific author subset to the full data set. This may be because topic-based methods use a single model, making them more sensitive to the number of authors than Token SVM X  X  one-versus-all setup that uses one model per author (this sensitivity may also explain why DADT-P outperformed
Token SVM by a relatively small margin on IMDb1M). This result suggests a direction for future work in the form of an ensemble of Token SVM and DADT-P. The potential of this direction is demonstrated by the fact that a perfect oracle, which chooses the correct answer between Token SVM and DADT-P when they disagree, yields an accuracy of 37.36% on the full Blog data set. 5.5 Summary of Key Findings
In summary, we found that the DADT-based probabilistic approach (DADT-P) yielded strong performance on the five data sets we considered, outperforming the Token
SVM baseline in four out of the five cases. We showed that DADT-P is more suitable for authorship attribution than methods based on LDA and AT (with or without fic-titious authors), and than using DADT for dimensionality reduction. Although our results demonstrate that separating document words from author words is a good approach to authorship attribution, relying only on unigrams is a limitation (which is shared by LDA, AT, and DADT). We discuss ways of addressing this limitation in Section 7.
 comes at a price of more parameters to tune. However, the most important parameter is the number of topics X  X e found that the prior values that yielded good results on the small data sets also obtained good performance on the large data sets without further tuning. We offered a simple recipe to determine the number of topics for DADT-P: First run AT-P to find the overall number of topics (which is equivalent to running DADT-
P without document topics), and then tune the document/author topic balance. As mentioned in Section 3.1.2, this procedure can be obviated by automatically learning the topic balance, which is left for future work. 6. Applications
This section presents three applications of topic-based author representations: identi-fying anonymous reviewers (Section 6.1), author-aware polarity inference (Section 6.2), and text-aware rating prediction (Section 6.3). 6.1 Reviewer Identification
AT and DADT can potentially be used to identify anonymous reviewers based on pub-licly available data X  X he reviewer list (which is commonly available), and the reviewers X  published papers. The main question in this case is whether authorship markers learned from (often multi-authored) texts in one domain (the papers) can be used to classify single-authored texts from a related domain (the reviews). attracted 18 submissions that were each reviewed by two reviewers. We collected the bodies of 10 papers (without references, author names, acknowledgments, etc.) by each of the 18 reviewers that were listed in the proceedings, which resulted in a training cor-pus of 171 documents with 196 authors overall (some of the reviewers have co-authored papers with other reviewers). We omitted authors with only one paper, because their presence is equivalent to having fictitious authors, which may hurt performance reviews by the 9 reviewers who gave us permission to use their reviews.

Section 5.2, and used AT-P and DADT-P to classify the reviews. The best accuracy, 8/19, was obtained by DADT-P with 10 document topics and 90 author topics. The accuracy of AT-P (with 100 topics) was slightly worse, at 7/19. In addition, the correct reviewer appeared in the top-five list of probable authors for 15/19 of the reviews with
DADT-P and 11/19 with AT-P (the list of probable authors included all 18 reviewers X  we considered all the reviewers as candidates because this did not require using any private information and it made our experimental setup more realistic). We obtained better results by eliminating non-reviewers from the training corpus (thus training on the 171 documents with 18 authors overall). DADT-P required only 25 document topics and 25 author topics in this case, and its accuracy rose to 10/19 (AT-P again performed worse with an accuracy of 7/19). In 16/19 of the cases the correct reviewer appeared in DADT-P X  X  top-five list, compared to 12/19 with AT-P.
 viewer identification is feasible (note that it is unlikely that DADT-P X  X  performance
To verify this, a fully fledged study should be done on a corpus of reviews from a large conference, with a training corpus that includes each author X  X  full body of publications (perhaps dropping very old publications, which we did not do). As far as we know, such a study is yet to be performed. The closest work we know of is by
Nanavati et al. (2011), who considered the question of whether  X  X nsiders, X  who served as program committee members and thus had access to non-anonymous reviews, can use these reviews as training data to identify reviewers. Although they found that they could identify reviewers with high accuracy, the main limitation of their approach is that it relies on private data.
 approach is to use tools that obfuscate author identity, as developed by, for example,
Kacmarcik and Gamon (2006) and Brennan and Greenstadt (2009). However, as this may lead to an  X  X rms race X  between such tools and authorship analysis methods, perhaps the best approach is to forgo anonymity completely, as advocated by some researchers and editors (Groves 2010). This is an open question with no simple answers, but we hope that our results will help motivate the search for solutions. 6.2 Author-Aware Polarity Inference
Sentiment analysis deals with inferring people X  X  sentiments and opinions from texts (Pang and Lee 2008; Liu and Zhang 2012). One of the main tasks in this field is polarity inference , where the goal is to infer the degree of positive or negative sentiment of texts (Pang and Lee 2008). Even though the way polarity is expressed often depends on the author, most of the work in this field ignores authors. We addressed this gap (Seroussi, Zukerman, and Bohnert 2010; Seroussi 2012) by introducing a framework 302 that considers authors when performing polarity inference, by combining the outputs of author-specific inference models in a manner that makes it possible to consider author similarity. We showed that our framework outperforms two state-of-the-art baselines introduced by Pang and Lee (2005): one that ignores authorship information, and another that considers only the model learned for the author of the text whose polarity we want to infer. These results support our hypothesis that the way sentiment is ex-pressed is often author-dependent, and shows that our approach successfully harnesses this dependency to improve polarity inference performance.
 tween authors based on their texts, which can be used by our polarity inference frame-work. Such measures are expected to capture authors X  interests and aspects of their authorship style, which is indicative of demographic attributes and personality traits.
We hypothesize that compact representation of authors using topic distributions would losing much information, as it did on the authorship attribution task.
 inference framework, which infers the polarity rating of a sentiment-bearing text q written by author a according to a weighted average 2 where N a is the set of neighbors of author a , w aa 0 is a non-negative similarity weight for q (each inferrer is a support vector regression model trained on the labeled texts by a 0 ). The neighborhood N a is obtained for each author a by learning a threshold on the number of similar authors to consider. This is done by performing five-fold cross validation on a  X  X  set of labeled texts to find the threshold that minimizes the root mean squared error (RMSE) out of a set of candidate thresholds.
 weighted average) and token frequency similarity with those obtained with similarity measures based on the AT and DADT topic models. The token frequency similarity measure is the cosine similarity of the frequency vectors of all the tokens in the authors X  vocabularies. The AT and DADT similarity measures are calculated as one minus the Hellinger distance between the author topic distributions.
 of scenarios, we utilized the GivenX protocol, where each target author has exactly X training samples (Breese, Heckerman, and Kadie 1998). Specifically, we performed ten-fold cross validation over authors , where we partitioned the authors into ten folds and iterated over the folds, using nine folds as the training folds and the remaining fold as the test fold. The model was trained on all the labeled texts (IMDb62 reviews, each with a polarity rating assigned by its author) by the authors in the training folds, and exactly X labeled texts by each target author in the test fold. The model was then tested on the remaining samples by each target author. This process was repeated five times with different random seeds, and the RMSE was averaged across folds (here and in the next section we use a paired two-tailed t-test to measure statistical significance, as polarity inference and rating prediction are regression problems). Note that the GivenX protocol cannot be used to reliably compare the performance of the same method across different X values (e.g., testing how the performance of a method varies from
Given1 to Given100), because the test samples vary across X values. Rather, we use this protocol to compare different methods under the same conditions, e.g., by comparing the performance of using DADT to that of employing equal weights under the Given10 scenario.

DADT similarity measures outperformed the baselines and performed comparably to each other (the differences between either AT or DADT and the baselines are statisti-cally significant in all cases except for Given5 and Given10 with the token frequency measure, whereas the differences between DADT and AT are not statistically significant
Instead, we used the settings that yielded the best authorship attribution performance on the IMDb62 data set (Section 5.4). It appears that in this case DADT X  X  approach of de-noising the author representations by modeling authors and documents over two disjoint sets of topics is of little benefit in comparison with AT X  X  approach of using only attribution experiments (Section 5), but it could be because the similarity measures do not require the models X  full discriminatory power, which is where DADT X  X  strengths lie (Section 3.4.3). Nonetheless, we are encouraged by the fact that using either AT or
DADT yielded better results than both the equal weights and token frequency baselines dimensionality than the token frequency measure, which demonstrates the strength of topic-based approaches for author representation. 304 6.3 Text-Aware Rating Prediction
Recommender systems help users deal with information overload by finding and recommending items of personal interest (Resnick and Varian 1997). Rating prediction is a core component of many recommender systems (Herlocker et al. 1999). Recently, rating prediction algorithms that are based on matrix factorization have become increasingly popular, due to their high accuracy and scalability (Koren, Bell, and
Volinsky 2009). However, such algorithms often deliver inaccurate rating predictions for users with few ratings (this is known as the new user problem ). We introduced an extension to the matrix factorization algorithm that considers user attributes when generating rating predictions (Seroussi, Bohnert, and Zukerman 2011; Seroussi 2012). We showed that using either demographic attributes or text-based attributes extracted with the LDA-S model, which is equivalent to AT (Section 3.2.3), outperforms state-of-the-art baselines that consider only ratings, thereby enabling more accurate predictions are generated without requiring users to explicitly supply any information about themselves and their preferences.
 our attribute-based model and Koren, Bell, and Volinsky X  X  (2009) ratings-only model according to an empirically set threshold n on the size of user u  X  X  known rating set R where  X  is the global rating mean; b ( U ) u , b ( I ) i , and b biases, respectively; and x x x  X  u , y y y  X  i , and z z z user u having one of the T attributes t is denoted by p ( t | u ), which in the case of AT and DADT is the user X  X  probability of using the author topic t , i.e., matrices using gradient descent in two stages (all the available ratings are used in both part of the model, assuming that the item biases and factor matrix are given (Seroussi, Bohnert, and Zukerman 2011).
 set consisted of message board posts and rated reviews, and calculated the RMSE on the test ratings (the reviews associated with these ratings were hidden from the models).
The baseline methods were non-personalized prediction ( equivalent to item i  X  X  rating mean), and the personalized, ratings-only model, which could only be used in the Given1 case. We set the number of author topics to 75 for
Given0 and 125 for Given1, as this yielded the best results for AT (out of 5, 10, 25, 50, 75, 100, 125, and 150). In DADT X  X  case, we used additional five document topics, which yielded the best results (out of 1, 5, 10, and 25). As the attribute-based model is sensitive only to the number of author topics, this enabled us to perform a fair comparison between the two models.
 performed the baselines, which supports our hypothesis that considering user texts by using topic-based author representations can yield personalized and accurate rating predictions, potentially leading to improved recommendations. The reason DADT did not outperform AT may be that DADT tends to yield user representations that help discriminate between texts by individual users (as shown in our authorship attribution experiments), but such representations are not as useful when utilized as attributes, be-cause the attribute-based model requires a representation that captures commonalities between users. 7. Conclusion and Future Work In this article, we extended and added detail to the work of Seroussi, Zukerman, and
Bohnert (2011) and Seroussi, Bohnert, and Zukerman (2012) by reporting additional experimental results and applications of topic-based author representations that go beyond traditional authorship attribution. We provided experimental results for authorship attribution methods that are based on three topic models (LDA, AT, and
DADT) for several scenarios where the number of authors varies from three to about 20,000. Specifically, we showed that in most cases, a probabilistic approach that is based on our DADT model (DADT-P) yields the best results, outperforming methods based on LDA and AT, as well as a Token SVM baseline. This indicates that our topic-based approach successfully captures indicators of authors X  style (which is indicative of author characteristics such as demographic attributes and personality
DADT to uncover the authors of anonymous reviews where the training texts are multi-authored, improve performance when measuring similarity between authors based on their texts in our polarity inference framework, and obtain compact representations of users for our rating prediction framework.
 would be to address the limitation of relying only on unigrams as features by consider-ing word order. This can possibly be pursued by adding author-awareness to Griffiths et al. X  X  (2004) HMM-LDA model, which considers word order by combining LDA with a
Hidden Markov Model. Author awareness can also be introduced into the models sug-gested by Wallach (2006) and Wang, McCallum, and Wei (2007), who made each word dependent on both its topic and on the previous word (at a considerable computational cost). A more general alternative would be to enable the use of various feature types, for example, by incorporating conditional random fields into DADT in a manner similar to
Zhu and Xing X  X  (2010) model. This direction can also be pursued by using DADT-P in an ensemble with SVMs that can be trained on feature types other than token unigrams, which may also have the added value of combining the strengths of DADT with those of the SVM approach (Section 5.4). Testing these approaches with character n -grams would be of particular interest, as they often deliver strong performance, sometimes outperforming token unigrams (Koppel, Schler, and Argamon 2009). 306 have not been written by any of the candidate authors (i.e., open-set attribution and ver-ification, described in Section 2). A fairly straightforward approach consists of setting a threshold on the probability assigned to the selected author based on performance on held-out data X  X f the probability of the selected author is below the threshold, then  X  X nknown author X  is returned. This approach was successfully used by Tanguy et al. (2011) in conjunction with a maximum entropy classifier.
 number of topics had the largest impact on performance among the parameters con-sidered in our experiments (Section 5.5). In addition, our models can be extended to address semi-supervised authorship attribution, and may potentially be applied to any scenario where user-generated texts are available, going beyond the applications presented in Section 6.
 Acknowledgments References 308
