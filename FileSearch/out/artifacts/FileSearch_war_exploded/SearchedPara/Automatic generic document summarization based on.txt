 1. Introduction
As the Internet sees increasingly wider use, the amount of information in the public domain continues to increase, ren-users. Document summarization is an essential technology to overcome this obstacle in technological environments.
Automatic document summarization is the process of reducing the size of documents while presenting the important semantic content. Its purpose is to identify a summary of a document without reading the entire document. Content is ex-ner appropriate for the user X  X  or application X  X  needs. This process can be used in many applications such as information Mani, 2001 ).

Document summarization methods can be classified into generic summarization and query-based summarization. While generic summarization distills the summarized text and presents the important semantic content of given documents, query-based summarization presents the summaries that are closely related to the query ( Marcu, 1999; Mani &amp; Maybury, 1999; Mani, 2001 ).

Generally, a document is comprised of major and minor topics. A large number of the sentences in a document are related to the major topics. Some sentences, then, are related to minor topics, which supplement the major topics. A good generic summary should contain the major topics of the document and minimize redundancy ( Gong &amp; Liu, 2001 ).
Generally, automatic generic document summarization methods can be divided into two categories: supervised and unsupervised methods ( Mani, 2001; Mani &amp; Maybury, 1999 ). Supervised methods are based on algorithms that use a large amount of human-made summaries, and as a result, are most useful for documents that are relevant to the summarizer mod-when users change the purpose of summarization or the characteristics of documents, it becomes necessary to reconstruct
Meng, 2005 ). Unsupervised methods do not require training data such as human-made summaries to train the summarizer (Nomoto &amp; Yuji, 2001; Buckley &amp; Walz, 1999; Gong &amp; Liu, 2001 ).

Recently, many generic document summarization methods using Latent Semantic Analysis (LSA) have been proposed tures obtained from LSA are composed of a great number of negative and positive weighted terms. Also, the meaning of the semantic features cannot be captured intuitively, and the scope of their meaning may be obscure. Furthermore, the weights of semantic features that are the elements of a linear combination representing a sentence can have both positive and neg-2002).

In this paper, we propose a new unsupervised generic document summarization method using Non-negative Matrix Fac-torization (NMF). The proposed method has the following advantages: First, it is an unsupervised method and does not re-quire training summaries for the summarizer and the training step. Second, the semantic feature vectors extracted from NMF can be interpreted more intuitively than those extracted from LSA-related methods, since the components of the former have and there are few zero values. Moreover, a sentence can be represented as a linear combination of a few intuitive semantic topics of a document can be identified more successfully. Therefore, there is greater possibility of extracting important sentences.

The remainder of this paper is organized as follows: Section 2 describes related work regarding generic document sum-marization and the LSA method; Section 3 describes NMF; Section 4 presents a comparison of summarization methods using
NMF and LSA; Section 5 describes NMF summarization; Section 6 describes the performance evaluation. Finally, in Section 7, we conclude the paper with directions for future research. 2. Related work
Kupiec, Pedersen, and Chen (1995) proposed a trainable summarizer, a kind of supervised method. statistical classifier is constructed using Bayes X  rule and hand-selected training summaries. Chuang and Yang (2000) proposed a supervised meth-od that uses a machine learning technique to extract sentences. Their method divides sentences into segments that are rep-marization. Their method uses classification expectation maximization (CEM) as a semi-supervised learning method, and it a new trainable summarizer for document summaries. Their summarizer uses several kinds of document features. Sentence posed a document summarization method using conditional random fields; this is a supervised method that has benefits of an unsupervised method.

Mihalcea (2005) proposed TextRank, which is used for unsupervised extractive summarization. It relies on an iterative graph-based ranking algorithm. Sentences that are highly recommended by other sentences are extracted as a summary.
However, it does not identify disjointed subtopics. Nomoto and Yuji (2001) proposed an unsupervised text summarization method. It uses modified X-means to find diverse topic areas in text and a simple sentence weighting model to identify the most important sentence from each topic area.

Kruengkrai and Jaruskulchai (2003) exploited both local and global properties (LGP) of sentences in order to find clusters tence having the largest combination score with respect to the local clustering score and the global connectivity score.
Xu, Liu, and Gong (2003) proposed two generic document summarization methods. The first method uses a relevance measure (RM). It splits a document into a set of candidate sentences. It then extracts high score sentences from the set by using the RM. The second method uses a latent semantic analysis (LSA) to semantically identify important sentences for summary creations. The sentence that has the largest index value with respect to the important singular vector is ex-gular value can have both positive and negative components, making sentence ranks by singular vector component values less meaningful (Zha, 2002 ).
 Zha (2002) proposed generic summarization using sentence clustering and the mutual reinforcement principle (MRP).
Their method clusters sentences of documents into several topical groups. Sentences are extracted from each topical group by their saliency scores, which are computed using the MRP; this is a modified LSA method. This method guarantees that the features do not necessarily identify subtopics. Yeh et al. (2005) proposed a document summarization method using LSA and a text relationship map (TRM). Their method uses LSA to derive the semantic matrix of a document and uses semantic repre-sentation of a sentence to construct a semantic text relationship map. TRM is constructed using the similarity between semantic representations, and important sentences are extracted by using the number of links in the TRM. This method does not consider subtopics for document summarization. Li et al. (2006) extended generic multi-document summarization using
LSA to query-based document summarization. 2.1. Document summarization using latent semantic analysis
In this paper, we define the matrix notation as follows: Let X * vector, and X ij be the element of the i th row and j th column.

The document summarization method using LSA applies singular value decomposition (SVD), delineated in Eq. (1),to summarize documents. This method decomposes matrix A into three matrices, U , D , and V where A is a m n terms-by-sentences matrix, m is the number of terms, and n is the number of sentences in a document. vectors of A T A (right singular vectors). D = diag ( r 1 negative eigenvalues sorted in descending order. e U is an m r matrix, where matrix, where e D ij  X  D ij if 1 6 i , j 6 r . e V is an n r matrix, where than n . In the method using LSA, the i  X  X h column vector A * resented as a linear combination of the left eigenvectors U * the weight of the j th semantic vector U * j corresponding to the sentence vector A * singular vector is extracted, to be included in summarized sentences (Gong &amp; Liu, 2001 ).
 Example 1. We illustrate an example using LSA algorithm. Let r be 3. LSA decomposes Matrix A into U,D, and V, as shown in
Fig. 1 a. Fig. 1 b shows an example of sentence representation. The column vector A * represented as a linear combination of feature vectors U * 3. Non-negative matrix factorization
Unlike LSA, NMF represents individual objects as a non-negative linear combination of part information extracted from a large volume of object. Studies on human cognition show that we use only the summation of non-negative data when we two non-negative matrices (Lee &amp; Seung, 1999; Xu, Liu, &amp; Gong, 2003 ).

NMF is employed to decompose a given m n non-negative matrix A into a multiplication of an m r non-negative semantic feature matrix (NSFM), W , and an r n non-negative semantic variable matrix (NSVM), H , as shown in Eq. (3) where r is usually chosen to be smaller than m or n so that the total sizes of W and H are smaller than that of matrix A .
We use the Frobenius norm as an objective function to satisfy the approximation condition is shown in Eq. (4) (Lee &amp; Seung, 1999, 2001; Wild, Curry, &amp; Dougherty, 2003 ): This is lower bounded by zero, and clearly vanished if and only if A = WH . W and H are continuously updated until H ( W , H ) converges under the predefined threshold or exceeds the number of repetitions. The update rules are as follows:
A column vector corresponding to the j  X  X h sentence, A * j vectors W * l and the semantic variable H lj as follows:
Example 2. We illustrate the example using the NMF algorithm: Let r be 2, the number of repetitions 50, and the tolerance 0.001. When the initial elements of the W and H matrices are 0.5, the non-negative matrix A is decomposed into two non-
In contrast to LSA, NMF decomposes a sparse matrix into two sparse matrices. Fig. 3 shows this property of the NMF. Here, as 0.5%, 1%, 2%, 3%, 5%, 7%, 10%, 30%, 60%, and 99%. The matrices W and H are obtained by using NMF. The matrices U and V are obtained by using LSA.

As shown in Fig. 3 , NMF decomposition produces sparse matrices while LSA produces non-sparse matrices. 4. Comparison of LSA and NMF as summarization methods Table 1 shows some extracted sentences from 20 documents related to the topic  X  X  X ourism in Great Britain X  ( Hoa, 2005 ).
Table 2 shows the terms-by-sentences matrix A obtained by preprocessing a set of sentences in Table 1 . Matrix A is com-posed of 396 terms and 57 sentences. Tables 3 and 4 illustrate the cases of applying LSA and NMF to matrix A . As shown in Table 2 , the terms-by-sentences matrix A is very sparse. Table 3 shows 10 semantic feature vectors 1 ... , U * 10 obtained from SVD decomposition of matrix A , the weight values r semantic feature vectors.
The method using LSA indicates that the first semantic feature vector U * umn value of V is multiplied by the largest eigenvalue. Therefore, we extract the sentence having the largest value among weights of U * 1 to summarize documents.

Table 4 presents 10 semantic feature vectors W * 1 , ... , W * vector calculated from the weight values, and the semantic feature vectors.

Comparing the semantic feature vectors in Tables 3 and 4 , it is seen that there are many negative values and few zero obtained by using the NMF are sparser than those obtained using LSA. Thus, the scope of the meaning of the semantic fea-tures obtained by using the NMF is clearer and narrower than that obtained by using LSA. This indicates that the method using NMF provides better performance in identifying subtopics of a document, as compared with the methods using LSA. have non-negative values and the latter have both negative and positive values. A more intuitive explanation is provided through Examples 3 and 4.

The weight values of semantic variable vectors ( H j 20 ) obtained using NMF are also sparser than those ( r using LSA. This indicates that the methods using LSA represent a sentence as a linear combination of many non-intuitive and less important semantic feature vectors, whereas the method using NMF represents a sentence as a linear combination of only a few intuitive and directly related-semantic feature vectors.

The method using LSA extracts the sentence from the first row of V corresponding to the semantic feature vector having the largest weight, and extracts the next sentence from the second row V corresponding to the semantic feature vector hav-ing the next largest weight. However, the method using NMF extracts the sentence having the largest weight with respect to how much the sentence reflects major topics, which are represented as semantic features. Therefore, the method using NMF has a greater likelihood of extracting semantically important sentences, which are closer to the major topics, as compared with the method using LSA.

Example 3. We analyzed examples of semantic features using LSA and NMF. Fig. 4 shows the term weights included in the of non-zero weights of the semantic feature vectors from LSA and NMF are 385 and 52, respectively. The average term
However, the maximum value of the term weights in the semantic feature vector from NMF (2.52) is much larger than that from LSA (0.187). The number of non-zero values greater than a tenth of the maximum values of NMF (17) is considerably smaller that of LSA (231). The standard deviation for term weights from NMF (0.217) is also much greater than that of LSA
In other words, NMF can more intuitively find comprehensible semantic features. These semantic features can be used more appropriately for subtopics of target documents.

Example 4. Fig. 5 shows an example of NMF and LSA representations for a CNN news story. We randomly selected 10 CNN victory X  for McCain in New Hampshire X . The NMF representation of this news contains 4 semantic features ( W * tures. The terms in the news are also shown in semantic features. Each semantic feature has a few terms. Their weights are all non-negative. Each semantic feature includes partial information of the news. With this information, we can roughly grasp the meaning of the semantic features. In contrast, the semantic features from the LSA representation have almost all terms found in the 10 CNN news articles whose values are positive or negative. Most of the terms in semantic features fore, it is not possible to grasp the meaning of the semantic features obtained from LSA representation through partial information of the news. 5. Generating generic summaries
In this section, we propose a method to create generic document summaries by selecting sentences using NMF. The pro-posed method consists of a preprocessing step and a summarization step. In the following subsections, the two steps are described in detail.

We give a full explanation of the two phases in Fig 6 . 5.1. Preprocessing
In the preprocessing step of generating generic document summaries, after a given English document is decomposed into individual sentences, all stopwords are removed by using Rijsbergen X  X  stopwords list and word stemming is performed by
Porter X  X  stemming algorithm (Frankes &amp; Baeza-Yates, 1992 ). A term-frequency vector for each sentence in the document is then constructed by Eq. (7).
 Let element A ji be the weighted term-frequency of term j in sentence i in a m n terms-by-sentences matrix A . where Wgt ( j , i ) is the weight for term j in sentence i . Several weighting schemes are when constructing the weighted
Baeza-Yates, 1992 ). In Section 6.3, we evaluate several weighting schemes and explain how these weighting schemes affect the summarization results. 5.2. Generic document summarization by non-negative matrix factorization
The summarization step selects sentences for generic summarization by using NMF. We perform NMF on A to obtain the non-negative semantic feature matrix W while the non-negative semantic variable matrix H is obtained using Eqs. (4) and (5).

We propose a novel method to select sentences based on NMF and define the Generic Relevance of a Sentence (GRS )as follows:
The weight ( H i * ) is the relative relevance of the i  X  X h semantic feature ( W * evance of a sentence refers to how much the sentence reflects major topics, which are represented as semantic features. 5.3. The proposed generic document summarization algorithm
The proposed algorithm for generic document summarization is as follows: 1. Decompose the document D into individual sentences, and let k be the number of sentences for generic document summarization. 2. Perform stopwords removal and word-stemming operations. 3. Construct the terms-by-sentences matrix A . 4. Perform NMF on matrix A to obtain matrix H . 5. For each sentence, calculate its generic relevance. 6. Select k sentences with the highest generic relevance values.

Example 5. We illustrate an example of sentence extraction using the proposed method as follows: Table 5 shows four sentences. We generate matrix A by preprocessing a set of sentences in Table 5 and decomposing matrix A into a semantic having the largest GRS value (0.53). 6. Experiments 6.1. Data set and evaluation system
We used the DUC2006 data set as test documents. The Document Understanding Conference (DUC conference for performance evaluations of proposed system by comparing manual summaries by experts with summaries of the proposed system. The systems participating in DUC2006 are constrained to query-based multi-document summarization.
However, our proposed system is based on generic summarization. Therefore, we implemented 4 kinds of generic summariza-tion methods (RM, LSA, MRP, LGP) using the DUC2006 data set. To compare the performances, we used the ROUGE evaluation software package, which compares various summary results from several summarization methods with summaries generated by humans.

Fig. 8 illustrates how the evaluations in this experiment are performed. As test data, we randomly selected 50 documents from the DUC2006 data set. Each document has a human-produced summary. Our methods (NMF) and 4 other methods pro-duce summaries using test documents. These summaries are input to ROUGE software to yield the ROUGE evaluation values.
We use ROUGE (recall-oriented understudy for gisting evaluation) to evaluate the proposed method. ROUGE has been ap-ing data consists of 50 topics and 25 documents related to each topic (Hoa, 2005 ). 6.2. Performance evaluation measure We conducted a performance evaluation of the document summarization methods using 50 given topics from DUC 2006. ROUGE includes five automatic evaluation methods, ROUGE-N , ROUGE-L , ROUGE-W , ROUGE-S , and ROUGE-SU (Lin, 2004 ).
Each method estimates recall, precision, and f -measure between human written reference summaries and candidate sum-maries of the proposed system. ROUGE-N uses n -gram recall between a candidate summary and a set of reference summa-ries. ROUGE-N is computed as follows: where n is the length of the n -gram , gram n , and Count didate summary and a set of reference summaries (Lin, 2004 ). ROUGE-L computes the ratio between the length of the sum-maries X  longest common subsequence (LCS) and the length of the reference summary, as delineated by Eq. (11):
LCS of X and Y . R lcs is a recall of LCS ( X , Y ), P lcs weighted LCS that favors LCS with consecutive matches. ROUGE-S uses the overlap ratio of the skip-bigram between a can-didate summary and a set of reference summaries, as given by Eq. (12): where SKIP2 ( X , Y ) is the number of skip-bigrams between X and Y , and b is the relative importance of P extension of ROUGE-S with the addition of unigram as the counting unit (Lin, 2004 ).
 6.3. Weighting schemes We evaluated the summarization performances for the eight weighting schemes ( Frankes &amp; Baeza-Yates, 1992; Baeza-system ( Gong &amp; Liu, 2001; Buckley &amp; Walz, 1999 ): the document, and n ( i ) is the number of sentences that contain term i . 6.4. Results and discussion
In this paper, we conducted two experiments to evaluate the performance of the proposed method. In the first experi-ment, we conducted a performance evaluation ( t -test) using the ROUGE measure with respect to five document summari-method (NMF) is superior to RM in ROUGE-1 recall X ,  X  X  X ur proposed method (NMF) is superior to LSA in ROUGE-W F-mea-sure X , etc. The significance level is 5% and the number of samples is 50. The acceptance region of t is t &gt; t ences of different weighting schemes on the proposed method.

Experiment 1. We evaluated the five different summarization methods: RM, LSA, MRP, LGP, and NMF. In Table 6 ,RM denotes Gong and Liu X  X  (2001) method using the relevance measure. In Table 7 , LSA denotes their method using latent semantic analysis. In Table 8 , MRP denotes Zha X  X  (2002) method using the mutual reinforcement principle and sentence clustering. In Table 9 , LGP denotes Kruengkrai and Jaruskulchai X  X  (2003) method using the local and global properties of sentences. NMF denotes the proposed generic document summarization algorithm using NMF. The t -test evaluation results are presented in Tables 6 X 9 .

In this experiment, most hypotheses were accepted except a few hypotheses regarding precision. However, the F-mea-sure is more important, because it is the synthesis of recall and precision. Most F-measure tests were accepted. RM showed the poorest performance, because it only uses the cosine similarity between sentences. LGP, however, showed better performance than LSA, because it reflects the local and global properties in documents while LSA cannot consider subtopics successfully. MRP showed better performance than LGP, because it can select sentences that contain major topics by using the most important singular vector. NMF showed the best performance, because it uses semantic features that are more intu-itively interpretable than those of any of the aforementioned document summarization methods. NMF X  X  use of semantic fea-tures allows it to identify subtopics of documents more successfully than the LSA-related methods.
Experiment 2. We compared the influences of eight different weighting schemes described in Section 6.3 for the proposed method. Fig. 9 shows the ROUGE -1 comparison results for the eight weighting schemes on the proposed method. The ROUGE -
L comparison results, the ROUGE -W comparison results, and the ROUGE -SU comparison results are shown in Figs. 10 X 12 , respectively. In this experiment, the No weight for the recall evaluation results showed the best performance among the ROUGE -SU . In the precision evaluation results, the Binary weight showed the best performance among all ROUGE measures.
In the F-measure evaluation results, the Modified binary showed the best performance among all ROUGE measures. 7. Conclusions and future research This paper presents a novel generic document summarization method using the generic relevance of a sentence based on
NMF. The proposed method has the following advantages: NMF selects more meaningful sentences than the LSA-related of documents. As such, it provided superior representation of the subtopics of documents. In Experiment 1 , the proposed technique achieved significant improvement over RM, LSA, MRP, and LPG in terms of recall, precision, and F-measure. In
Experiment 2 , the No weight showed the highest performance in the recall evaluation, the Binary weight showed the highest performance in the precision evaluation, and the Modified binary weight showed the highest performance in the F-measure evaluation.

In future research, automatic relevance feedback and pseudo relevance feedback will be considered in order to enhance the methodology. Furthermore, we plan to investigate the factors that determine which terms are more important than oth-ers in summarization.
 Acknowledgement This work was supported by Inha University Research Grant.
 References
