 Missing data commonly occur in ma ny applications. While many data imputation methods exist to handle the missing data problem for databases, when applied to c oncept drifting data streams, these methods share some common difficulties. First, due to large and continuous data volumes, we are unable to maintain all stream records to form a candidate pool for missing value estimation, as most existing methods commonly do. Second, even if we could maintain all complete stream records using a summary structure, the concept drifting problem would make some information obsolete, and thus deteriorate the imputation accuracy. Third, in data streams, it is necessary to develop a fast yet accurate algorithm to find most similar data for imputation. Fourth, due to dynamic and sophisticated data collection environments, the missing rate of most stream data may be much higher than that in databases, so the imputation met hod should be able to handle high missing rate in the data. To tackle these challenges, we propose a S treaming k -Nearest-Neighbors I mputation F ramework (SKIF) for concept drifting data streams. To handle concept drifting and large volume problems in data st reams, SKIF first summarizes historical complete records in some micro-resources (which are high-level statistical data stru ctures), and maintains these micro-resources in a candidate pool as benc hmark data. After that, SKIF employs a novel hybrid-kNN imputation procedure, which uses a hybrid similarity search mechanism, to find the most similar micro-resources from the large scale candidate pool efficiently. Experimental results demonstrate the effectiveness of the proposed SKIF framework for data stream imputation tasks. H.2.8 [ Programming Languages ]: Database Applications I.2.6 [ Artificial Intelligence ]: Learning Algorithms. Data streams, Data imputation, Concept drifting. Recent advances in large scale data storage and network transmission have made it possibl e to collect and model dynamic data streams for rapid response a nd real time decision making [1]. Nevertheless, due to the inherent uncertainty, complexity, and sophistication of the data coll ection environments, many real-world stream applications co mmonly encounter missing data issues which may cause bias or lead to inefficient analysis [2]. As a result, it is urgent to propose an effective stream data imputation method to handle large volumes st ream data with missing values. Compared to traditional database imputation methods [3], stream data imputation raises a number of new challenges: (1) for data streams, it is impractical to maintain all complete records for imputation; (2) unlike generic imput ation methods that assume the data distribution is invariant, the probability distributions underlying stream data are continuously changing ( i.e. , the concept drifting); (3) given an incomplete record, finding its k most similar neighbors from the candidate pool in a fast yet accurate way is essential for stream based applications; (4) due to the dynamic data nature, in data stream applications, the data missing rate, compared to the static databases, may be relatively high, which imposes special re quirements for solutions in handling data with high rate missing values. To address these new challenges, in this paper, we present a S treaming k -Nearest-Neighbors I mputation F ramework (SKIF) for data stream imputation tasks. The SKIF first summaries the statistical information of the stream data as some high-level micro-resources , and maintains these micro-resources in a candidate pool as benchmark data. For each up-to-date incomplete record, SKIF uses a novel hybrid-kNN imputation method to estimate its missing values based on a hybrid distance metric mechanics. More specifically, hybrid-kNN first uses Euclidean distance to approximately select a small portion of neighbors from the candidate pool, and then uses a sophisticated Reliable Distance to precisely assign a weight to each selected neighbor for imputation. As a result, SKIF can achieve good imputation results with significantly computational cost reduction. 
The rest of the paper is stru ctured as follows. Section 2 introduces the SKIF framework. S ection 3 discusses the candidate pool and the micro-resource struct ure. Section 4 discusses the proposed hybrid-kNN imputation mode l. Experimental results are reported in Section 5, and we c onclude the paper in Section 6. Consider a data stream S consisting of a set of data records R ; X  X  X ;R i ; X  X   X  arriving at time stamps T 1 ; X  X  X ;T i record contains r -dimensional attributes V=(v i 1 ; X  X  X ;v i and a class label Y2fc 1 ; X  X  X ;c l g . Assume that the current time stamp is T t , and the corresponding up-to-date record R incomplete record with both missing class la bel and missing attribute values. The goal of the stream data imputation is to estimate all missing values of R t based on some historical information. Formally, we use O to represent R t attributes O= fo 1 ; X  X  X ;o r 1 g , use U to represent its r with missing values U= fu 1 ;  X  X  X  ; u r 2 g , and use L to represent its missing class label. Besides, assume that we X  X e summarized all historical complete records in some statistical structures, called micro-resources , and maintained these micro-resources in a candidate pool P= fm 1 ;  X  X  X  ; m n g as the benchmark data, where m is the i th micro-resource, and there are n micro-resources in PP . Therefore, our goal is to estimate most probable class label L and missing attribute values in U , that maximizes Eq.(1) where  X  is a set containing all the option values of each attribute in U . Estimating P(L;UjO;P) is, however, by no means an easy task, considering that using all micro-resources in to impute L and U is computationally expensive or even infeasible, because of the considerable size of micro-resources m in the candidate pool . Beside, even if computational cost is not a concern, not all micro-resources in P are beneficial for estimating missing values due to the concept drifting reality. To alleviate the problem, we introduce a bridge variable M which denotes R t  X  X  k most similar micro-resources in P , and try to estimate P(M ;L;UjO;P) instead. In order to calculate th is probability, we decompose the estimation of P(M ;L;UjO;P) into three sub-parts as follows: P(M ;L;UjO;P) = P(O;P;M ;L;U) P(M ;L;UjO;P) = P(O;P;M ;L;U) Because P\ M = M , the above derivation can be simplified as: Consequently, estimating missing class label L and missing values step aims to find the k nearest micro-resources from the candidate pool P according to R t  X  X  observed attributes O . The second step imputes missing class label L by only using its k neighbors in M (instead of the whole set). The last step estimates missing values in U according to a subset in M which shares the same class label with the estimated label L . Note that we only consider the most general situation where both L and U are missing. In this section, we propose a micro-resource structure to summary statistical information of the historical data records. The micro-resource structure, inspired by the micro-cluster [4], consists of a set of additive entries that can incrementally summarize the statistical information of a set of complete records, including the records' cluster center, weight, class distribution, the dominant class label, and the distances that to other clusters. More specifically, the definition of th e micro-resource is as follows: Definition 1. A micro-resource for a set of data records R ;  X  X  X  ; R k arriving at time stamps T 1 ;  X  X  X  ; T k m= ( X; D; C; r adi us; number; wei ght; label) , where the dimensional vector which corres ponds to the distance from the current cluster to the remaining n-1 micro-resources (indeed, we only need to maintain a small portion of the smallest distances), the third entry C is a l -dimensional vector which corresponds to the probability distribution of all the records that have been absorbed in this structure, and each of the remaining four entries corresponds to a one-dimensional variable. They maintain the micro-resource X  X  radius, number of records that have been absorbed, weights, and cl ass label respectively. resource X  X  cluster center. This is the most important part. It can not only be used to describe mi cro-resource X  X  location, but also used to estimate missing attribute values for surrounding incomplete records. The second entry D maintains the distances from the current micro-resource to the remaining micro-resources, which is used to accelerate the si milarity search. The third entry C describes the class distribution of the micro-resource. This is because each micro-resource contains data records from different classes, where the class distribu tion factor should be considered for accurate imputation. The fourth entry controls the radius of each micro-resource. Similar to the micro-cluster [4], for a newly arrival complete record, if it lies within the radius of the micro-resource, it will be absorbed into the micro-resource; otherwise, a new micro-resource will be created. The fifth entry maintains the total number of data records that have been absorbed. The sixth entry corresponds to the weight of the micro-resource. Assume that a micro-resource is created at a specific time stamp T the current time stamp is T k . If this micro-resource has not been activated ( i.e. , no records fall into this micro-resource) during the time period (T k  X T 1 ) , then its weight will decrease from 1 ( i.e. , dynamic changing weight is im portant for handling concept drifting in data stream. If a micro-resource hasn X  X  been updated for a long time, it is likely to be an outdated pattern and should, therefore, be eliminated from the candidate pool. For example, given the time limit  X 2(0;1) , if a micro-resource has not been activated in  X   X 1 ln X  time, it will be taken as an outdated point, and deleted from the candidate pool. This will also provide memory space for new micro-resources to capture emerging concepts in data streams. The last entry describes the micro-resource X  X  class label, which co rresponds to the dominant class label in C . It should be noted that the micro-resource structure, like the micro-cluster structure, sa tisfies the additive property, and thus can be maintained efficiently. In this section, we propose a hybrid-kNN model which uses a hybrid distance metric for data stream imputation. For an incomplete record q , the hybrid-kNN model first uses a simple Euclidean based distance metric to approximately select a small portion of neighbors from the candidate pool, and then uses a more sophisticated Reliable Distance metric to assign a precise weight to each selected neighbor to do imputation. 
In the first step, we aim at finding a small portion of nearest neighbors, denoted by M , from the large candidate pool P under a simple Euclidean distance metric. In order to find the k nearest neighbors for query q , an intuitive approach is the full similarity search algorithm [5], which first evaluates the distances for all points in the candidate pool, and then selects the k minimal ones. For a candidate pool with large size or high dimensional data, calculating the distance between query q and all points in the candidate pool P may involve significant computational overhead. A popularly used estimate function is the triangle inequality . However, in our problem formulation, the triangle inequality may face a heterogeneous distance metric problem. More specifically, the query distance (the distance between the query point and the point under es timation, which is calculated on the observed r 1 dimensions), and the pre-computed distance (the distance between micro-resources that maintained in the entry D , which has r dimensions) are not in the same distance metric space. To address this issue, we propose a new estimate function derived from the original triangle inequality as follow. 
Theorem 1. For points q; a; b 2 V with a Euclidean-based distance metric, if distances d r 1 (q; a) and d r priori, and r 1  X r 2 , then d r 1 (q; c)  X  d r where d  X  (u; v) represents the Euclidean distance between two points u and v in the  X  -dimensional space.
 Due to the space limitation, we omit the proof here. After finding a small portion of neighbors M under the simple Euclidean distance metric, we then define a more sophisticated Reliable Distance metric to assign a proper weight to each neighbor of query q in M for imputing q  X  X  class label. A point is defined as reliable if it is close to the query point and points with the same class label, but far away from the points with different class labels . For example, for two points p; q 2 R candidate pool P , the Reliable Distance R(pjq) between them should satisfy the following three rules: (i) R(pjq) / f (p;q) (ii) R(pjq) / (iii) R(pjq) / where f(p;q)= expf X  1  X  jjp  X  qjj 2 g with  X &gt;0 , P + is a subset of P containing the same class label with p , and P  X  Rules (ii) and (iii) are independent of the query point q , they mean that p should be close to the points with the same class label yet far away from the points with different class labels. By combining the above rules, we finally formulate the Reliable Distance as follow: where 0 X   X  1 ;  X  2 ;  X  3  X 1 , and  X  1 +  X  2 +  X  3 =1 . The normalization factor ensures that 0 X  R(pjq)  X  1 . The Reliable distance is able to precisely measure the similarity between the query q and each point in M . When estimating q  X  X  class label, points with smaller Re liable distances will take larger weight values. More specifically, for a given query q , we can estimate its class label by using a weighted average mechanism, with each weight reversely proportional to the Reliable distance, 
After estimating q  X  X  class label L , the last step is to estimate q  X  X  missing attribute values U . To impute U , we first select a subset M 0 from M which shares the same class label with q  X  X  estimated class label, denoted by M 0 =fp 0 1 ; X  X  X ;p 0 k 0 g M 0 =fp number of points in M 0 . Then we combine all points p 0 for imputation through a weighted average mechanism, with each weight being reversely proportional to its reliable distance to the query point q . Benchmark Methods we compare SKIF with some existing wireless sensor network imputati on models, which is similar to our problem setting. These models include (1) I mputation based on T wo C orrelated A ttributes (ITCA). In this method, if attributes v and v 2 are correlated variables and the value of v 1 will use v 2 to impute v 1 . This method is a variation of the WARM [6] model. In ITCA, we empirica lly assign their correlation values, whereas WARM uses association rule mining to mine such relationships; (2) I mputation based on M ulti-C orrelated A ttributes (IMCA), which is an extension of the CARM [7] model. In IMCA, if ( v 1 , v 2 , v 3 ) are correlated variables and the value of v it will use v 1 and v 2 to do imputation; and (3) I mputation based on R ecent S liding W indows (IRSW) derived from the FARM [8] model, which uses the complete data in the most recent data chunk to do imputation. Benchmark Data Streams We use three real-world data streams (Web monitoring stream [9], Se nsor stream [10], and Power stream [11]) as the benchmark. The web monitoring stream contains both malicious and be nign URLs. For every incoming URL, 3231961 features are obtained by querying the DNS, WHOIS, blacklist and geographic in formation servers, as well as processing IP address-related a nd lexical-related features. For simplicity, we will analyze the first week data. Besides, for each type of attributes, we only extr act the former 20 attributes for analysis. The Sensor stream contains information collected from 54 sensors, as shown in Figure 2. In order to generate the class label , four regions (the ellipses) are selected from the total 54 sensors. The Power stream contains hourly power supply of an electricity company from two sources: power supply from main grid and power transformed from other grids. The learning task of this stream is to predict whic h hour (one out of the 12 periods from (0,2),..., (22,24)) the current power supply belongs to. The whole stream contains 29, 928 records, each of which has four dimensions. In the experiments, the chunk size is set to be 500, the parameter  X  in Table 1 denotes the missing rate. For example, if  X = 0:7 , it means that each record will have 70% attribute values missing. The correlations among different attributes are given a priori. For example, in sensor stream, we take spatially adjacent nodes to be correlated nodes ( e.g. , node 7 and node 5 are the correlated nodes). If readings from node 7 are missing, we will use node 5 to impute its value (ITCA). For IMCA, to impute missing readings from node 7, we will use its surrounding nodes 4, 5, 6 and 8 as references. For IRSW, we will separate the data stream into chunks, and only use the complete data in the current data chunk to predict missing data in the same chunk. 
From Table 1, we can obser ve that, the proposed SKIF imputation framework, on average, performs the best. In addition, our SKIF method is much more st able for high level missing data rate than its peers. The reason is that all three benchmark frameworks aim to use local info rmation for imputation (which will, of course, make these methods computationally efficient and require very little resource consum ption). However, using local information for imputation can not tackle data with high level missing rate. For example, when the missing rate increases, there is very little useful information in the local data which will eventually lead to poor imput ation results. This limitation motivates the utilization of the global information for imputation. In order to make proper use of th e global information, SKIF first Table 1. Comparisons among different imputation frameworks 
Web 
Sensor 0.7 53.30
Power 0.7 40.74 maintains all past historical info rmation in the candidate pool, and then uses an efficient hybrid-kNN to find correlated historical information for imputation. By doing so, the SKIF framework, compared to its peers, receives the best performance gain for stream data with high level missing rate. Due to dynamic, complex, and uncertain data collection and transmission environments, most stream data are vulnerable to corruptions or missing values. Wh ile some data stream based imputation models [6, 7, 8] ha ve been recently proposed, these models have traditionally focuse d on using local information ( i.e. , correlated attributes or some local data readings) for imputation, which makes them ineffective for concept drifting data streams and incapable of handling stream data with high level missing rate. In this paper, we proposed a new Streaming k-NN imputation framework (SKIF) for concept dr ifting data streams. Different from existing imputation methods , SKIF aims to use global information built from the historical data to estimate missing values for each incomplete record. To achieve the goal, SKIF first maintains the statistical information ( i.e. , the micro-resources ) of the historical data, and then uses an efficient hybrid-kNN method which combines a simple distance metric (the Euclidean distance) and a complex distance metric (the Reliable distance) for fast and accurate imputation. Experimental results and comparisons, with several benchmark methods, on th ree real-world data streams demonstrated that the proposed SK IF method is able to provide accurate and fast imputation results for concept drifting data stream with high level missing rate. This research was partially supported by the National Science Foundation of China (NSFC) under Grant No. 61003167, Basic Research Program of China (973 Program) under Grant No.2007CB311100, and Australian Re search Council's Discovery Projects funding scheme (project number DP1093762). [1] C.Aggarwal, Data Streams: Models and Algorithms, [2] P. Haghani, S. Michel, K. Ab erer, Evaluating Top-K Queries [3] X. Zhu, S. Zhang, Z. Jin, Z. Zhang, Z. Xu, Missing Value [4] C. Aggarwal, J. Han, J. Wang, Philip S. Yu, A Framework [5] P. Zezula, G. Amato, M. Batko, Similarity Search: The [6] M. Halacthev, L. Gruenwald, Estimating Missing Values in [7] N. Jiang, L. Gruenwald, Es timating Missing Data in Data [8] L. Gruenwald et al., Using Data Mining to Estimate Missing [9] J. Ma et al., Identify Suspicious URLs: An Application of [10] X. Zhu, X. Wu, C. Zhang. Vague One-Class Learning for [11] P. Zhang et al., Mining Data Streams with Labeled and 
