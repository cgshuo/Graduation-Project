 Similarity functions assign scores to documents in response to queries. These functions require as input statistics about the terms in the queries and documents, where the intention is that the statistics are estimates of the relative informa-tiveness of the terms. Common measures of informativeness use the number of documents containing each term (the doc-ument frequency) as a key measure. We argue in this paper that the distribution of within-document frequencies across a collection is also pertinent to informativeness, a measure that has not been considered in prior work: the most in-formative words tend to be those whose frequency of occur-rence has high variance. We propose use of relative standard deviation (RSD) as a measure of variability incorporating within-document frequencies, and show that RSD compares favourably with inverse document frequency (IDF), in both in-principle analysis and in practice in retrieval, with small but consistent gains.
The effectiveness of a retrieval system hinges on its abil-ity to retrieve relevant documents in response to a query. The core of retrieval methods are similarity functions, whose scores are used to construct document rankings and are, in effect, proxies for statistical estimates of the likelihood of document relevance. Term weighting schemes are input to these functions; these are used to allocate values to terms that represent their significance within the collection and each document. That is, they represent the informativeness of words. A common approach to calculation of the infor-mativeness of a term is measurement of the term specificity .
In their development of the Robertson X  X p  X  arck Jones rel-evance weight (RSJ) [10], the authors argued that, if any  X 
This work was conducted while the author was employed by Microsoft, Melbourne, Australia.
 proportional to the variability. In the context of document retrieval, the theory implies that the words that exhibit the most uncertainty carry the most information and thus should be the most informative. In information retrieval, en-tropy has previously been investigated for term weighting, feature selection, and the building of probabilistic models of information retrieval [2, 4, 6, 8]. We explore statistical dis-persion as a novel method of calculating variability for use in information retrieval.

Dispersion as used in this paper is the median or mean of the deviations of the random variable X t = { x 1 ,x 2 ,...,x n } from the the expected value E [ X t ] for some term t with term frequencies x i over n documents. Here x i are f t,d , the frequency of term t in document d , or the relative term frequency rf t,d = f t,d | d | , where | d | is the length of d . Equa-tion (1) is thus a measure of the extent to which the actual distribution differs from the expected (normal) distribution. where E is the median or mean and  X  ,  X  and  X  together determine the statistical deviation being calculated.
The difference between the predictor (expected value) and an observed value is an observation of the predictors uncer-tainty: the error in the prediction. The mean error of the predictor is thus a measure of the predictor X  X  uncertainty.
The uncertainty inherent in dispersion suggests a connec-tion with probability theory and Shannon X  X  theory of infor-mation. Shannon X  X  entropy determines the value of informa-tion by calculating the average self-information of a term t ; the self-information of an item k is the log of the inverse probability 1 /P ( k ) or the non-likelihood of item k .
As the inverse probability of an event is proportional to the uncertainty of the event, we may define 1 /P ( k ) as the uncertainty of an event U t . Thus Equation (2) gives the formulation of a slightly different method of self-information, based on uncertainty: Equation (3) defines the informativeness of IDF as the self-information of the uncertainty of the terms frequency distri-bution U t = N / n t + 1.
 Here, N is the total number of documents and n t is the num-ber of documents containing t . Similarly RSD is a measure of informativeness calculated from the self-information of the uncertainty of the terms distribution as given by Equa-tion (1).
 Here,  X  X t is the mean of the distribution of term frequencies X . We now examine the potential for RSD to act as a measure of term specificity and compare to IDF.
Our investigation of RSD is in two parts. The second part is its value as a component of similarity measures; our values as generated by IDF that are one of the motivations for use of within-document term frequency. RSD compares favourably, showing a lack of gaps in the range of values for the same sample size, suggesting a finer-grained measure of informativeness.

The shape of the RSD scattergram is strongly similar to that of IDF. Both exhibit a similar growth in values, par-ticularly at the right-hand side. For Figures 1 and 2, these values from approximately 4 to 12 and 2 to 6 within a total value range of 0 to 12 and 0 to 6 respectively. This repre-sents 66% of the total value range of both IDF and RSD, further demonstrating the similarity between the two. that produce weights similar to IDF are likely to also be useful measures of informativeness.

Figure 3 was produced plotting the weights as determined by the IDF and RSD weighting functions. Then comparative analysis determined the similarities in the visual properties between IDF and RSD plot values. Each graph is generated from a random sample of 1000 documents of the OHSUMED text collection.

The IDF weighting scattergrams clearly show that RSD correlates strongly with IDF, supporting results from Sec-tion 3. The strong correlation supports the hypothesis that RSD, like IDF, is a measure of term specificity. This further supports the hypothesis that IDF and RSD are similar mea-sures of uncertainty, and that these measures can be used interchangeably.
 crease in the space between like documents in the vector space and a corresponding increase in the space between un-like documents increases the precision of a retrieval system [12, p. 617-619].

We define the  X  X iscriminating power of a function X  as the distance between term weights of two opposing classes of documents as generated by a given function. Figures 4 and 5 depict the term weights as determined by IDF and RSD respectively, of two samples of opposing classes. Then for IDF and RSD the discriminating power of the weighting function is determined by analysing the differences between the weights of the document sample for each class.

Figure 4 depicts the differences between the weights of the rarest terms for the positive set relative to the same terms in the negative set. The average difference between at the 0.05 and 0.01 levels are indicated by  X  and  X  , respectively. ument rankings are not as impacted and thus this is likely not to affect the user. TF-RSD calculated from relative term frequencies also shows significant improvement in the MAP for title-only queries, relative to description-only queries.
These results support the claims that RSD is a useful mea-sure of term specificity, and show that TF-RSD is more ef-fective than TF-IDF for the data sets and queries used.
We have proposed that the informativeness of a word is proportional to the self-information of the uncertainty of the word X  X  occurrence in a set of documents, and defined new measures of informativeness and corresponding term weighting schemes, by calculating the self X  X nformation of the statistical dispersion (variability) of a terms frequency across a set of documents.

We have shown that RSD, a new measure of informative-ness, correlates strongly with IDF on essential qualitative properties of measures of informativeness. We then demon-strated that TF-RSD, a new term weighting scheme for in-formation retrieval, achieves improved results compared to TF-IDF in 12 of 16 retrieval experiments, seven of which were statistically significant. The retrieval results for the top 10 ranked documents were equally impressive  X  where only two of the 16 experiments failed to perform better than TF-IDF. Thus TF-RSD is an effective term weighting scheme that is a plausible replacement for TF-IDF.

