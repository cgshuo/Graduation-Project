 In this paper we investigate a novel and important problem in multi-document summarization, i.e., how to extract an easy-to-understand English summary for non-native readers. Existing summarization systems extract the same kind of English summaries from English news doc uments for both native and non-native readers. However, the non-native readers have different English reading skills because they have different English education and learning backgrounds. An English summary which can be easily understood by native readers may be hardly understood by non-native readers. We propose to add the dimension of reading easiness or difficulty to multi-document summarization, and the proposed EUSUM system can produce easy-to-understand summaries accord ing to the English reading skills of the readers. The sentence-level reading easiness (or difficulty) is predicted by usi ng the SVM regression method. And the reading easiness score of each sentence is then incorporated into the summarization process. Empirical evaluation and user study have been performed and th e results demonstrate that the EUSUM system can produce more easy-to-understand summaries for non-native readers than existing summarization systems, with very little sacrifice of the summary X  X  informativeness. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing  X  abstracting methods ; I.2.7 [ Artificial Intelligence ]: Natural Language Processing  X  text analysis Algorithms, Experimentation, Design, Human Factors. EUSUM, multi-document summarization, reading easiness Document summarization is a ta sk of producing a condensed version of a document or document set. A summary is usually required to be informative and fluent. Users can easily understand the main content of the document or document set by reading the summary. To date, various summarizati on methods and a number of summarization systems have been developed, such as MEAD, NewsInEssence and NewsBlaster. These methods and systems focus on how to improve the inform ativeness, diversity or fluency of the English summary, and they usually produce the same English summaries for all users, including native readers and non-native readers. However, different users usually have different English reading levels because they have different English education backgrounds and learning environments. And native readers usually have higher English reading levels than non-native readers. In particular, Chinese readers usually have less ability to read English summaries than native English readers. For example, Chinese college students usually have passed the National English Test Band 4 (CET-4), and they ha ve learned English for several years, but they still have more or less difficulty to read original English news and summaries. The difficulty lies in unknown or difficult English words (e.g.  X  X ei smographs X ,  X  X oodbine X ), or the complex sentence structure (e.g.  X  X he chairman of the House Agriculture Committee says hearings are planned next year into how the U.S. Forest Service handled last summer's stubborn Yellowstone National Park. X ). Ther efore, they have to slow down the reading speed in order to understand the news text or summary, or give up the reading process. In this study, we argue that the English summaries produced by existing methods and systems are not fit for non-native readers (i.e. Chinese readers). We examine a new factor -reading easiness (or difficulty) 1 for document summarization, and the factor can indicate whether the summary is easy to understand by non-native readers or not. The reading easiness of a summary is dependent on the reading easiness of each sentence in the summary. And we propose a novel summarization system  X  EUSUM (Easy-to-Understand Summarization) for incorporating the reading easiness factor into the final summary. Th e proposed system first predicts the reading easiness score for each sentence, and then incorporates the reading easiness score into the final sentence ranking process. Both informative and easy-to-unde rstand sentences are selected into the summary. Both automatic evaluation and user study have been performed and the evaluation results verify the effectiveness of the proposed EUSUM system. The contribution of this paper is summarized as follows: 1) We examine a new factor of reading easiness for document summarization. 2) We propose a novel summarization system  X  EUSUM for incorporating the new factor and producing easy-to-In this paper,  X  X eading easiness X  and  X  X eading difficulty X  refer to the same factor, and we use them interchangeably. understand summaries for non-native readers. 3) We conduct both automatic evaluation and user study to verify the effectiveness of the proposed system. The paper is organized as follo ws: Section 2 introduces related work. Section 3 describes the details of the EUSUM system. Sections 4 and 5 present experi mental results and discussions. Lastly we conclude our paper in Section 6. Document summarization methods can be generally categorized into extraction-based methods a nd abstraction-based methods. In this paper, we focus on extr action-based methods. Extraction-based summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combinati on of a number of statistical and linguistic feature values, such as term frequency, sentence position, topic signature [19, 22]. The summary sentences can also be selected by using machine learning methods [1, 17] or graph-based methods [8, 23]. Othe r methods include mutual reinforcement principle [33]. For multi-document summarization, the centroid-based method [27] is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS [20] makes use of new features such as topic signature to select important sentences. Machine Learning-based approaches have also been proposed for combining various sentence featur es [34]. Themes (or topics, clusters) discovery in documents has been used for sentence selection [10]. The influences of input difficulty on summarization performance have been investigated in [25]. Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau [24] extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences [32]. Senten ce ordering in summaries has been investigated in [2, 3] to improve the summary fluency. Other summarization tasks include topic-focused (query-biased) document summarization [31], upda te summarization [18]. All these summarization tasks do not consider the reading easiness factor of the summary for non-native readers. A reading difficulty measure can be originally described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level [12]. And reading difficulty prediction can be viewed as a regression of difficulty grade level based on a set of featur es derived from the text. Earlier work on reading difficulty prediction is conducted for the purpose of education or language learning. For example, one purpose is to find appropriate reading material s of the appropriate difficulty level, in terms of both vocabular y and grammar, for English as a First or Second Language students. And almost all earlier work focuses on document-level reading difficulty prediction. A variety of features have been investigated in reading difficulty measures. Average sentence length and word length are simple proxies for grammatical and lexical complexity of a text, as in the Dale-Chall model [5]. The Flesch-Kincaid meaure [15] is probably the most common reading difficulty in use in earlier days. The Lexile Framework [29] uses individual word frequency estimates as a measure of lexical difficulty, and it uses a Rasch model based on the features of word frequency and sentence length. In recent years, more sophi sticated features and models are used. Smoothed unigram language modeling has been developed to capture the predictive ability of individual words based on their frequency at each reading difficulty level [7]. A statistical approach is proposed to infer the distribution of a word X  X  likely acquisition age automatically from authentic texts collected from the Web, and then an effective semantic component for predicting reading difficulty of news texts is provided by combining the acquisition age distributions for all words in a document [14]. Schwarm and Ostendorf [28] incorporate syntactic features derived from syntactic parses of text, and their system performs better than the Flesch-Kincaid and Lexile measures. The frequency of grammatical constr uctions has been used as a measure of grammatical difficulty, and the final prediction function is a linear function of the lexical and grammatical components [11, 12]. Pitler and Nenkova [26] combine lexical, syntactic and discourse features to produce a highly predictive model of text readability. In addition to English language, Fran X ois [9] presents an appro ach to assessing the readability of French texts. More recently, a machine learning approach is used for predicting the readability of we b search summaries or snippets [13]. In this study, we investigate the reading difficulty (or easiness) prediction of English sentences for Chinese readers, i.e. whether an English sentence is easy to understand by Chinese readers or not. Note that sentence ordering in a long summary also has influences on the reading difficulty or readability of the summary, and proper order of extracted sentences can improve their readability [2]. However, sentence ordering is a nother research problem and we do not take into account this factor in this study. The main idea of the proposed EU SUM system is to incorporate the sentence-level reading easiness factor into the summary extraction process. Each sentence is associated with two factors: informativeness and reading easiness. The informativeness of a sentence is computed by using previous summarization methods. The reading easiness of a sentence is measured by an EU (easy-to-understand) score, which is pr edicted by using statistical regression methods. The two scores are then combined and both informative and easy-to-understand sentences are chosen into the summary. The three steps of the EUSUM system will be described in details in next two sections. As mentioned in Section 2.2, we do not consider the fluency factor of the whole summary in this study, which has been investigated in related research areas (e.g. sentence ordering [2, 3]). In this study, reading easiness refers to how easily a text can be understood by non-native readers. Reading easiness prediction is a task of mapping a text to a numerical value corresponding to a reading easiness. The larger the value is, the more easily the text can be understood. We focus on predicting the reading easiness score of an English sentence for Chinese college students. As mentioned earlier, Chinese college students usually have studied English for several years and they usually have passed the CET-4 test 2 or above, which means that they have some ability to read ordinary English articles. However, because of different English learning environments and different learning abilities, these students may have different English reading levels. Many students have some difficulty to read original English news or summaries. The two factors most influencing the reading process are as follows: 1) Unknown or difficult English words: for example, most Chinese college students do not know the words such as  X  X eismographs X ,  X  X oodbine X . 2) Complex sentence structure: for example, a sentence with two or more clauses introduced by a subordinating conjunction is usually difficult to read. As introduced in Section 2.2, various regressi on methods have been used for reading difficulty prediction. In this study, we adopt the  X  -support vector regression (  X  -SVR) method [30] for the reading easiness prediction task. The SVR algorithm is firmly grounded in the framework of statistical learning theory (VC theory). The goal of a regression algorithm is to fit a flat function to the given training data points. In the experiments, we use the LIBSVM tool [6] with the RBF kernel for the regression task, a nd we use the parameter selection tool of 10-fold cross validation via grid search to find the best parameters with respect to mean square error (MSE), and then use the best parameters to train the whole training set. We use the following two groups of features for each sentence: the first group includes surface features, and the second group includes parse based features. The four surface features are as follows: 1) Sentence length : It refers to the number of words in the 2) Average word length : It refers to the average length of 3) CET-4 word percentage : It refers to the percentage of how CET-4 is College English Test Band 4, which is a national 
English level test in China, a nd all college students are required to pass this test before graduation. 4) Number of peculiar words : It refers to the number of We use the Stanford Lexicalized Parser [16] with the provided English PCFG model to parse a sentence into a parse tree. The output tree is a context-free phrase structure grammar representation of the sentence. The four parse features are as follows: 1) Depth of the parse tree : It refers to the depth of the 2) Number of SBARs in the parse tree : SBAR is defined as a 3) Number of NPs in the parse tree : It refers to the number of 4) Number of VPs in the parse tree : It refers to the number of All the above feature values are scaled by using the provided svm-scale program. At this step, each sentence s i can be associated with a reading easiness score EaseScore(s i ) predicted by the  X  -SVR method. The larger the score is, the more eas ily the sentence is understood. The score is finally normalized by dividing by the maximum score. In this study, we adopt two typical methods for evaluating the informativeness of each sentence in a document set. The two methods are described briefly in the following sections. The centroid-based method is the algorithm used in the MEAD system. The method uses a heuristic and simple way to sum the sentence scores computed based on different features. In our combination of the weights computed based on the following three features: 1) Centroid-based Weight. The weight C ( s sentence s i is calculated as the cosine similarity between the sentence text and the concatenated text for the whole document set D . The weight is then normalized by dividing by the maximal weight. 2) Sentence Position. The weight P ( s i sentence s i to reflect its position priority as P ( s where pos i is the position number of sentence s document and n i is the total number of sentences in the document. Obviously, pos i ranges from 1 to n i . 3) First Sentence Similarity. The weight F ( s i ) is computed as the cosine similarity value between sentence s i and the corresponding first sentence in the same document. After all the above weights are calculated for each sentence, we sum the three weights and get the overall score InfoScore(s sentence s i . After the scores for all sentences are computed, the score of each sentence is normalized by dividing by the maximum score. The basic idea of the graph-based method is that of  X  X oting X  or  X  X ecommendation X  between sentences. Formally, given a document set D , let G =( V , E ) be an undirected graph to reflect the relationships between senten ces in the document set. V is the set set. E is the set of edges. Each edge e ij in E is associated with an affinity weight f ( s i , s j ) between sentences s weight is computed using the st andard cosine measure between the two sentences. Here, we have f ( s i , s j )= f ( s j to avoid self transition. We use an affinity matrix M to describe G with each entry corresponding to the weight of an edge in the graph. M = ( M to make the sum of each row equal to 1 .
 Based on matrix M ~ , the saliency score InfoScore(s i ) for sentence s can be deduced from those of all other sentences linked with it and it can be formulated in a recursive form as in the PageRank algorithm: where  X  is the damping factor usually set to 0.85, as in the PageRank algorithm. After the scores for all sentences are computed, the score of each sentence is normalized by dividing by the maximum score. After we obtain the reading easiness score and the informativeness score of each sentence in the document set, we linearly combine the two scores to get the comb ined score of each sentence. Formally, let EaseScore ( s i )  X  [0,1] and InfoScore ( s the reading easiness score and the informativeness score of sentence s i , the combined score of the sentence is: where  X   X  0 is a parameter controlling the influences of the reading considering the reading easiness factor. Usually,  X  is not set to a large value because we must maintain the content informativeness in the extracted summary. Therefore, we choose the parameter value empirically in order to balance the two factors of content informativeness and reading easiness. For multi-document summarization, some sentences are highly overlapping with each other, and thus we apply the same greedy algorithm in [31] to penalize the sentences highly overlapping with other highly scored sentences , and finally the informative, novel, and easy-to-understand sent ences are chosen into the summary. In the algorithm, the final rank score RankScore ( s sentence s i is initialized to its combined score CombinedScore ( s And at each iteration, the highly ranked sentence (e.g. s selected into the summary, and the rank score of each remaining sentence s j is penalized by using the following formula: where  X  &gt;0 is the penalty degree factor. The larger  X  is, the penalty is imposed at all. Th e iteration is stopped after the summary length limit is reached. In the experiments, we first cons tructed the gold-standard dataset in the following way. DUC2001 provided 309 news articles for document summarization tasks, and the articles were grouped into 30 document sets. The news articles were selected from TREC-9. We chose five document sets (d04, d05, d06, d08, d11) with 54 news articles out of the DUC2001 test set. The documents were then split into sentences and there were totally 1736 sentences. Two college students (one undergra duate student and one graduate student) manually labeled the reading easiness score for each sentence separately. The score ranges between 1 and 5, and 1 means  X  X ery hard to understand X , and 5 means  X  X ery easy to understand X , and 3 means  X  X ostly understandable X . The final reading easiness score was the average of the scores provided by the two annotators. After annotation, we randomly se parated the labeled sentence set into a training set of 1482 sentences and a test set of 254 sentences. We then used the LIBSVM tool for training and testing. Two standard metrics were used for evaluating the prediction results. The two metrics are as follows: Mean Square Error (MSE) : This metric is a measure of how correct each of the prediction values is on average, penalizing more severe errors more heavily. Pearson X  X  Correlation Coefficient (  X  ) : This metric is a measure of whether the trends of prediction values matched the trends for human-labeled data. Table 1 shows the prediction resu lts. For comparison, the result for the Flesch-Kincaid measure 3 is also reported in the table. We can see that the overall results of our method are very promising. And the correlation is high. The re sults guarantee that the use of reading easiness scores in the summarization process is feasible. 
SVR (Surface features + Parse features) 0.112 0.931 The reading easiness scores based on the FK measure are directly obtained by accessing the following web service: http://www.standards-schmandard s.com/exhibits/rix/index.php We can also see that either the surface feature set or the parse feature set can achieve good prediction result, and the two feature sets can contribute to the overall prediction results. However, the Flesch-Kincaid measure doe s not perform well. In this study, we used the multi-document summarization task (task 2) in DUC2001 for evaluati on. As mentioned in Section 4.1.1, DUC2001 provided 30 documen t sets. Because we have used five document sets (d04-d11) for training and testing in the task of reading easiness prediction, we used the remaining 25 document sets for summarization evaluation, and the average document number per document set is 10. The sentences in each article have been separated and the sentence information has been stored into files. A summary was required to be created for each document set and the summary length was 100 words. Generic reference summaries were provi ded by NIST annotators for evaluation. We used the LIBSVM tool with the learned model to predict the reading easiness score for each sentence in the documents, and then used the scores for summary extraction.
 Different from traditional summariza tion tasks, our task is to incorporate the reading easiness factor into multi-document summary, and the easy-to-understand summarization can be considered as a novel summarization task. Therefore, we evaluate a summary from the following two aspects: Content Informativeness : This aspect is widely evaluated in almost all traditional summarization tasks. It refers to how much a summary reflects the major content of the document set. Usually, it can be measured by comparing the system summary with the reference summary. We used the ROUGE-1.5.5 toolkit for automatic evaluation of the content informativeness, and the t oolkit was officially adopted by DUC for automatic summarization evaluation. The toolkit measures summary quality by counting overlapping units such as the n-gram, word sequences and wo rd pairs between the candidate summary and the reference summary [21]. The ROUGE-1.5.5 toolkit reports separate F-measure scores for 1, 2, 3 and 4-gram, and also for longest common subs equence co-occurrences. In this study, we show four ROUGE F-measure scores in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), ROUGE-W (based on weighted longest common subsequence, weight=1.2), and ROUGE-SU* (based on skip Reading Easiness : This aspect is not evaluated by previous summarization tasks. We aim to evaluate the reading easiness level of the whole summary. Because the reading easiness level of a summary is dependent on the reading easiness scores of the sentences in the summary, we use the average reading easiness score of the sentences in a summary as the summary X  X  reading easiness level. The overall reading easiness score is the average across all 25 document sets. In addition to the above automatic evaluation procedures, we also performed pilot user studies for evaluation. Four Chinese college students participated in the user studies. We have developed a user study tool for facilitating the subjects to evaluate each summary from the two aspects of content informativeness and reading easiness. Each subject can assign a score from 1 to 5 on each aspect for each summary. For reading easiness, 1 means  X  X ery hard to understand X , and 5 means  X  X ery easy to understand X . For content informativeness, 1 means  X  X east informative X , and 5 means  X  X ery informative X . During each user study procedure, we compared two summarization systems X  results. And the two summaries produced by the two systems for the same document set were presented in the same interface, and then the four subjects assigned scores to each summary after they read and compared the two summaries. The final score of a summary on one aspect was the average of the scores assigned by the four subjects. And the overall scores were averaged across all subjects and all 25 document sets. In this section, we report the automatic evaluation results of EUSUM from both two aspects. Though the combination weight  X  in EUSUM can be set to any non-negative value, it ranges from 0 to 1 in our experiments, because a much larger  X  will lead to a big sacrifice of the content informativeness for the summary. The penalty degree factor  X  for EUSUM is set to 10, as in [31]. Table 2 shows the ROUGE scores and the reading easiness score of the EUSUM system with the centroid-based method, which is denoted as EUSUM(Centroid). Table 3 shows the ROUGE scores and the reading easiness score of the EUSUM system with the graph-based method, which is denoted as EUSUM(Graph). Seen from the tables, the ROUGE scores of EUSUM(Centroid) and EUSUM(Graph) are decreased with the increase of the combination weight  X  , and the reading easiness scores of them are increased with the increase of  X  . And we can see that with the increase of  X  , the summary X  X  reading easiness can be more quickly becoming significantly different from that of the summary with  X  =0, while the summary X  X  content informativeness is not significantly affected when  X  is set to a small value. Moreover, For EUSUM(Graph), even the ROUGE scores with  X  =0.1 are set to a small value, the content informativeness aspect of the extracted summary are almost not affected, but the reading easiness aspect of the extracted summary can be significantly improved. By comparing the performance values in the two tables, we can see that when  X  is fixed, the ROUGE-1, ROUGE-W and ROUGE-SU* scores of EUSUM(Graph) are higher than the corresponding scores of EUSUM(Centroid), whic h verifies the effectiveness of the graph-based summarization me thod. We can also see that when  X  is fixed, the reading easiness scores of EUSUM(Graph) EUSUM(Centroid), which demonstr ates that EUSUM(Graph) can EUSUM(Centroid). We explain the results by that the graph-based sentence extraction method tends to extract sentences with good feature values for indicating reading easiness. For example, sentence length is one of the important features for reading easiness prediction, and a shorter sentence is more likely to be easy to understand. We compare the average sentence length (average word number per sentence) in the summaries extracted by EUSUM(Graph) and EUSUM(Centroid) in Figure 1. We can see that EUSUM(Graph) usually extracts shorter sentences than EUSUM(Centroid), which veri fies the results from one perspective. Overall, the resu lts show that EUSUM(Graph) is more suitable than EUSUM(Centroid) for extracting easy-to-understand summaries 5 . In the above experiments, the penalty weight  X  is fixed to 10. We now take EUSUM(Graph) as an ex ample to show how the penalty weight  X  influences the two aspects of the proposed summarization system. Figures 2 and 3 show the reading easiness score curves and the ROUGE-SU* F-score curves of EUSUM(Graph) with different settings, respectively. We can see that the reading easiness scores of EUSUM(Graph) with different settings have a tendency to increase with the increase of  X  . And after  X  is larger than 10, the reading easiness scores for most settings do not change any more, which shows that the penalty weight has no significant influences on the reading easiness of the summaries when the weight is set to a moderately large value. The ROUGE-SU* scores are firstly increasing with the increase of Actually, we can improve the centroid-based method by incorporating some useful features such as sentence length, but it is not the focus of this paper.  X  and then decreased with the increase of  X  , which demonstrates that less or much penalty will lower the performance instead of content informativeness. 
Figure 1. EUSUM average sentence length (sentence word Figure 2. EUSUM(Graph) reading easiness vs. penalty weight. Figure 3. EUSUM(Graph) content informativeness vs. penalty In order to validate the effectiveness of the system by real non-native readers, two user study procedures were performed: User study 1 : The summaries extracted by EUSUM(Centroid) (  X  =0) and EUSUM(Graph) (  X  =0.2) are compared and scored by subjects. Seen from Tables 2 and 3, most ROUGE scores of the two systems are very similar, and the reading easiness score of EUSUM(Graph) (  X  =0.2) is higher than that of EUSUM(Centroid) (  X  =0). Table 4 gives the averaged subjective scores of the two systems. The user study results verify that the summaries by EUSUM(Graph) (  X  =0.2) are indeed significantly easy to understand by non-native readers, while the content informativeness of the two systems are not significantly different. User study 2 : The summaries extracted by EUSUM(Graph) (  X  =0) and EUSUM(Graph) (  X  =0.3) are compared and scored by subjects. Seen from Tables 2 and 3, most ROUGE scores of the two systems are not significantly different, and the reading easiness score of EUSUM(Graph) (  X  =0.3) is higher than that of EUSUM(Graph) (  X  =0). Table 5 gives the averaged subjective scores of the two systems. The user study results verify that the summaries by EUSUM(Graph) (  X  =0.3) is indeed significantly easy to understand by non-native readers, while the content informativeness of the two systems are not significantly different. In order to better compare the results, we give several typically extracted summaries for two doc ument sets D14 and D59. The predicted reading easiness score of each sentence is also given in brackets. In this study, the experiments we re performed by Chinese college students. Because college students in different countries may have different English reading levels, the experimental results may be slightly changed if we use non-na tive students in other countries for evaluation. Even for Chinese readers, college students and high school students may have different English reading levels, and thus the experimental results may be slightly changed if we use high school students for evaluati on. That X  X  to say, the reading easiness level of a summary should be adjusted with the particular reader. In practice, we can let readers to tune the combination weight  X  in the proposed EUSUM system, and they can select the best weight for extracting summari es best suitable for reading. Similarly, for native English read ers, different persons may have different English reading levels, and thus the framework proposed in this paper is also applicab le. However, the reading easiness score of each sentence may be different because of the differences between the English reading abilities and behaviors of Chinese readers and native English readers. In this study, we investigate the new factor of reading easiness for document summarization, and we propose a novel summarization system -EUSUM for producing easy-to-understand summaries for non-native readers. We performed automatic evaluation and user study to verify the effectiveness of the proposed system. In future work, we will further improve the summary X  X  reading easiness in the following two ways: 1) The summary fluency (e.g. sentence ordering in a summary) has influences on the reading easiness of a summary, and we will consider the summary fluency factor in the summarization system. 2) More sophisticated sentence reduction and sentence si mplification techniques will be investigated for improving the summary X  X  readability. This work was fully supported by NSFC (60873155), and partially supported by RFDP (20070001059), Beijing Nova Program (2008B03), NCET (NCET-08-0006) and National High-tech R&amp;D Program (2008AA01Z421). [1] M. R. Amini, P. Gallinari. The Use of Unlabeled Data to [2] R. Barzilay, N. Elhadad and K. McKeown, Inferring strategies [3] D. Bollegala, N. Okazaki and M. Ishizuka. A bottom-up [4] T. Brants, A. Franz. Web 1T 5-gram Version 1 . Linguistic Data [5] J. S. Chall and E. Dale. Readability revisited: the new Dale-[6] C.-C. Chang and C.-J. Lin. LIBSVM : a library for support [7] K. Collins-Thompson and J. Callan. Predicting reading [8] G. ErKan, D. R. Radev. LexPageRank: Prestige in Multi-[9] T. L. Fran X ois. Combining a st atistical language model with [10] S. Harabagiu and F. Lacatusu. Topic themes for multi-[11] M. Heilman, K. Collins-Thompson, J. Callan and M. Eskenazi. [12] M. Heilman, K. Collins-Thompson and M. Eskenazi. An [13] T. Kanungo and D. Orr. Predicting the readability of short web [14] P. Kidwell, G. Lebanon and K. Collins-Thompson. Statistical [15] J. Kincaid, R. Fishburne, R. Rodgers and B. Chissom. [16] D. Klein and C. D. Manning. Fast Exact Inference with a [17] J. Kupiec, J. Pedersen, F. Chen. A.Trainable Document [18] W. Li, F. Wei, Q. Lu and Y. He. PNR2: ranking sentences with [19] C. Y. Lin, E. Hovy. The Automated Acquisition of Topic [20] C..-Y. Lin and E.. H. Hovy. From Single to Multi-document [21] C.-Y. Lin and E.H. Hovy. Auto matic Evaluation of Summaries [22] H. P. Luhn. The Automatic Creation of literature Abstracts. [23] R. Mihalcea, P. Tarau. TextRank: Bringing Order into Texts. In [24] R. Mihalcea and P. Tarau. A language independent algorithm [25] A. Nenkova and A. Louis. Can you summarize this? [26] E. Pitler and A. Nenkova. Revisiting readability: a unified [27] D. R. Radev, H. Y. Jing, M. St ys and D. Tam. Centroid-based [28] S. Schwarm and M. Ostendorf. Reading level assessment using [29] A. J. Stenner. Measuring reading comprehension with the [30] V. Vapnik. The Nature of Statistical Learning Theory . Springer, [31] X. Wan, J. Yang and J. Xiao . Using cross-document random [32] X. Wan and J. Yang. Multi-document summarization using [33] X. Wan, J. Yang and J. Xiao. Towards an Iterative [34] K.-F. Wong, M. Wu and W. Li. Extractive summarization 
