 This paper shows how citation-based information and struc-tural content (e.g., title, abstract) can be combined to im-prove classification of text documents into predefined cate-gories. We evaluate different measures of similarity  X  five derived from the citation information of the collection, and three derived from the structural content  X  and determine how they can be fused to improve classification effectiveness. To discover the best fusion framework, we apply Genetic Programming (GP) techniques. Our experiments with the ACM Computing Classification Scheme, using documents from the ACM Digital Library, indicate that GP can dis-cover similarity functions superior to those based solely on a single type of evidence. Effectiveness of the similarity functions discovered through simple majority voting is bet-ter than that of content-based as well as combination-based Support Vector Machine classifiers. Experiments also were conducted to compare the performance between GP tech-niques and other fusion techniques such as Genetic Algo-rithms (GA) and linear fusion. Empirical results show that GP was able to discover better similarity functions than GA or other fusion techniques.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval; I.5.4 [Pattern Recognition]: Applications X  Text pro-cessing General Terms: Algorithms, Measurement, Performance, Experimentation Keywords: Classification, Genetic Programming In recent years, automated classification of text into pre-defined categories has attracted considerable interest, due to the increasing volume of documents in digital form and the ensuing need to organize them. However, traditional content-based classifiers are known to perform poorly when documents are noisy (e.g., digitized through speech recog-nition or OCR) and/or contain scarce textual content (e.g., metadata records in digital library (DL) catalogs) [3].
DLs offer both (1) the opportunity to explore the com-plex internal structured nature of documents and metadata records in the classification task; and (2) the social net-works occuring in specific communities, as expressed for ex-ample by citation patterns in the research literature. On the other hand, many DLs, which are created by aggrega-tion of other sub-collections/catalogs, suffer from problems of quality of information. One such problem is incomplete-ness (e.g., missing information). This makes it very hard to classify documents using traditional content-based classifiers like SVM, or Naive Bayes. Another quality problem is im-precision. For example, citation-based information is often obtained with OCR, a process which produces a significant number of errors. In this work we try to overcome these problems by applying automatically discovered techniques for fusion of the available evidence. Particularly, we investi-gate an inductive learning method  X  Genetic Programming (GP)  X  for the discovery of better fused similarity functions to be used in the classifiers, and explore how this combina-tion can be used to improve classification effectiveness.
One motivation of this work is to enhance teaching and learning in the computing field, by improving CITIDEL [5], part of the National Science Digital Library (NSDL). Many of the records in CITIDEL lack classification information. That makes it difficult for users to browse, even with the support of our advanced multischeming approach [26]. It also limits the scope of advanced interfaces to CITIDEL, which make use of category information [21]. Further, the lack of classification information reduces broader downstream impact that could result from use of records harvested from CITIDEL and similar systems, such as into NSDL or NDLTD (e.g., to support browsing of computing dissertations [37]).
This paper provides background in Section 2, especially about GP. Section 3 highlights challenges in classification. Section 4 describes our experiments. Section 5 discusses related work, while Section 6 gives conclusions and describes plans for ongoing research.
Genetic algorithms (GAs) [18] and genetic programming (GP) [25] are a set of artificial intelligence search algorithms designed following the principles of biological inheritance and evolution. The solution to a problem is represented as an individual (i.e., a chromosome) in a population pool. The population of individuals evolves generation by generation through genetic transformation operations  X  such as repro-duction, crossover, and mutation  X  with the aim of creating more diverse and better performing individuals with better fitness values in subsequent generations. A fitness function is available to assign the fitness value for each individual.
The difference between GA and GP is the internal repre-sentation  X  or data structure  X  of the individual. In GAs, each individual is commonly (though not always) represented by a fixed-length bit string, like (1101110 . . . ) or a fixed-length sequence of real numbers (1.2, 2.4, 4, ...).InGP,morecomplexdatastructures,e.g.,trees, linked lists, or stacks, are used [27]. Moreover, the length or size of the data structure is not fixed, although it may be constrained by implementation to within a certain size limit. GAs are often used to solve difficult optimization problems, while GP is typically used to approximate complex, non-linear functional relationships [25]. Because of the intrinsic parallel search mechanism and powerful global exploration capability in a high-dimensional space, both GA and GP have been used to solve a wide range of hard optimization problems that oftentimes have no best known solutions.
Both GP and GA have been applied to the information retrieval field [12,17] as well as for data classification. In [4], Cheang et al. proposed a Genetic Parallel Programming classifier to evolve parallel programs for data classification problems. Eggermont et al. [10] proposed several meth-ods using techniques from the field of machine learning to refine and reduce search space sizes for evolving decision trees for data classification. They showed how classification performance improves when shrinking the space in which a tree-based Genetic Programming (GP) algorithm searches. Kishore et al. [23] proposed a methodology for GP-based n -class pattern classification. They modelled the given n -class problem as n two-class problems; further, a GP classifier expression was evolved as a discriminant function for each class. In [24], they integrated the GP classifier with feature space partitioning for localized learning to improve pattern classification. Genetic Programming also has been applied to text classification through the use of a parse-tree. In [6], Clack et al. used GP to route in-bound documents to a central classifier which autonomously sent documents to in-terested research groups with in a large organization. The central classifier used a parse tree to match the aspects of a document to nodes of the tree, which ultimately leads to a single numerical value  X  the classification or  X  X onfidence value X   X  during evaluation.

Castillo et al. [9] developed a multistrategy classifier sys-tem for document classification. They applied different types of classifiers (e.g., Naive Bayes, Decision Trees) to differ-ent parts of the document (e.g., titles, references). Genetic algorithms are applied for feature selection as well as for combining the output of the different classifiers.
To fuse multiple sources of evidence for text classification, we could use techniques like manual selection and major-ity voting. However, manual selection and combing is very tedious and time-consuming, while simple majority voting may not work in all contexts. A better way is to use ma-chine learning techniques to intelligently combine evidence, by optimizing a given objective function.

The decision to use GP for fusing multiple sources of ev-idence to discover similarity functions for text classification was motivated by four factors: 1. The large size of the search space; 2. The characteristics of the objective function; 3. Previous success on the application of GP in the IR 4. Little prior work on applying GP to large scale text
In order to apply GP to the problem of classification, several required key components of a GP system need to be defined: Terminals are the leaf nodes in the tree struc-ture. Functions are the non-leaf nodes combining the leaf nodes. Numericaloperationslike+,-,*,/,andlogare commonly used. A fitness function is the objective function GP aims to optimize. Reproduction is a genetic operator to breed new individuals, while mutations are deviations during this reproduction process. Crossover takes two in-dividuals (parents) to breed one new individual that shares some attributes with each parent.

When setting up the configurations of the GP system for similarity function discovery, we combined features re-garding content-based structural information and features regarding citation-based information to use as the terminals.
To determine the similarity between two documents we used three different similarity measures applied to the con-tent of abstract, title, and abstact-plus-title (denoted as full) of documents separately: Bag-of-Words (BOW), Cosine and Okapi. Also, we used five different citation-related similar-ity measures: co-citation [32], bibliographic coupling [22], Amsler [1], and Companion (authority and hub) [8]. More information about these measures can be obtained in [36]. This gave us fourteen similarity measures, represented as document  X  document matrices. Through GP, we intend to discover a single similarity function, for each class, that combines all or several of the similarity measures described here. Table 1 shows the mapping of all these measures to their abbreviations which are used later in the paper. Table 1: Measures and corresponding abbreviations.
The functions used are +, *, /, sqrt. These functions are selected because they provide meaningful operations on rela-tions. For example, the + and * operations are summing and reinforcing the relations, respectively. On the other hand, the / operation accomodates inverse relationships, while the sqrt operation serves to scale the values. The algorithm be-low details our fitness evaluation function which GP should optimize within a particular class.
 LetR=0 For each document D in class C end for Let F = R/|C|
A good similarity function, i.e., a similarity function with a high fitness value, is one that, when applied to a document d of class C ,ranksdocumentsfromclass C in such a way that those with greater similarities to d i are top-ranked. The higher the value of F, the better the function. It is worth noticing that the choice of fitness function can have a huge impact on the final classification performance [11]. Experiments with different fi tness functions are currently underway.
Along with the settings discussed in the previous section, the overall classification framework has 5 steps, as follows: 1. For each class, generate an initial population of ran-2. For each class, perform the following sub-steps on 3. Apply the recorded ( N gen * N top ) candidate  X  X imilar-4. For each class C, use b C as similarity function in a kNN 5. Combine the output of each classifier through a simple
Steps 1, 2, and 3 concern the t raining process within GP which intends to discover better similarity functions for each class. However, the discovered functions only can be used to calculate the similarity between a pair of documents. In order to evaluate the performance of those functions in the classification task, we used a strategy based on a nearest neighbor classifier  X  kNN [34]. This classifier assigns a cat-egory label to a test document, based on the categories at-tributed to the k most similar documents in the training set. The kNN algorithm was chosen since it is simple and makes direct use of similarity information.

In the kNN algorithm, to a given test document d is as-signed a relevance score s c i ,d associating d with each candi-date category c i . This score is defined as: where N k ( d )arethe k nearest neighbors (the most similar documents) of d in the training set. In Step 4 of our frame-work the generic similarity function of kNN is replaced by the functions discovered by GP for each class.

In multi-classification problems with n classes, we effec-tivelyendupwith n kNN classifiers using the described framework. In order to produce a final classification result, we combine the output of all n classifiers using a simple ma-jority voting scheme, whereby the class of a document d i is decided by the most common class assigned by all the n classifiers. In case of ties, we assign d i to the larger class. Because of its simplicity we chose to use majority voting in our framework (Step 5) to: 1) help alleviate the common problem of overfitting found in GP training [12] and; 2) help boost performance by allowing kNN classifiers to apply dif-ferent similarity functions which explore and optimize the characteristics of each particular class in different ways.
The classification framework suffers from scalability, a common problem with GP. That is, it takes a relatively long time to find an optimal solution. In fact, the time complexity of an experiment based on the above framework is O( N gen * N ind * T eval ), where N gen is the number of gen-erations for evolution, N ind is the number of individuals in a population pool, and T eval is the fitness evaluation time for an individual. Since T eval is determined by the complexity of an individual (Size) and the number of training samples ( N samples ), then the total time complexity of an experiment is O( N gen * N ind *Size* N samples ) [11]. Nevertheless, scal-ability and computing efficiency are not the main issue in our experiments; we focus more on the effectiveness of the classifiers. The speed of the l earning process can improve through parallel processing as well as through optimization, e.g., as described in Section 4.1. Further, in a practical ap-plication, the learning process need occur only occasionally; the frequent operation will be the actual classification of incoming new documents.
To test the hypotheses that GP is able to adapt itself to find the best similarity functions, we ran two sets of experiments following the framework of the previous sec-tion on both the first level (11 categories, A to K) and the second level (44 categories, though some small cate-gories were removed, see Section 4.1) of the ACM Com-puting Classification System (CCS, http://www.acm.org/ class/1998/ ). For the first level experiments, a subset of the ACM collection with 30K metadata records correspond-ing to those classified under only one category in the first level was used. Correspondingly, for the second level exper-iments, another ACM collection subset was used, with 19K metadata records drawn from those classified under only one second level category.

The ACM Digital Library suffers from most of the prob-lems we mentioned in the Introduction. For example, only 42.3% of the records in the first level collection and only 41.7% of the records in the second level collection have abstracts, which makes it very hard to classify them us-ing traditional content-based classifiers. For these records, the only available textual content is title. But titles con-tain normally only 5 to 10 words. Citation information was created with OCR and had a significant number of errors. A very imprecise process of matching between the citation text and the documents, using adaptations of techniques de-scribed in [29], had to be performed. This introduced noise and incompleteness in the citation-based similarity matrices computed with measures such as co-citation or bibliographic coupling. Other concerns were the large search space and skewed distributions observed for some categories.
Stratified random sampling (cf. Section 4.1) was used to create training and test collections. The combination of these experiments provides us with insights about the capability of the GP-based discovery framework.
The collection used in our experiments has 30,022 docu-ments belonging to 11 categories for the first level and 18,664 documents belonging to 44 categories for the second level. Figure 1 shows the distributions for the first level and sec-ond level categories, respectively. Each terminal or feature described is a similarity matrix which contains the similar-ity between each pair of documents. Using half or more of the whole collection as our training data, the required re-sources  X  in CPU time and amount of memory  X  would be enormous. The time required to discover a proper classifi-cation framework also would be significant. To reduce the high cost of resources and at the same time improve effi-ciency, sampling was used. So, we started by considering the documents belonging to each category of CCS as dif-ferent population strata. We then randomly selected from each stratum a given number of units based on a proportion, like 15%, for each category. However, special attention was paid to skewed categories. For example, category E in the first level only has 94 documents while the average size of the other categories is in the range of thousands. 15% of 94 only gives us 14 documents and this was too small to serve as the sample to discover the whole category X  X  characteristics. The number of documents for some second level categories is too small ( &lt; 50) for classification purposes, so those categories were dropped from the experiments. Also, because of its size, category E was not subdivided in the second level and all of its ACM sub-categories were considered as a unique second-level one (denoted E.x). Classification statistics for both the first level and the second level collection were used to control the sampling procedure. That is, baselines from the samples for each level were compared with baselines for that level X  X  corresponding whole collection, to ensure that the samples mirror that level X  X  whole collection as well as possible. Figure 1: Compared distribution for the first and the second level ACM collections.
We follow a three data-sets design [12, 30] in our experi-ment. We randomly split the data into training, validation, and test parts. The introduction of the validation data-set is to help alleviate the problem of overfitting of GP on the training data and select the best generalizable similarity function. We generate two sets of training samples using a stratified sample strategy for both the first level and the second level in the CCS. The first set used a random 15% sample for large classes and 50% for skewed classes. The second set used a random 30% sample for large classes and 50% for skewed classes 1 . In the 30% sample, one specific strategy was used for the biggest class D in the first level to reduce time. Since class D is huge, even using 30% for training in GP would require a long time. Thus, instead of a 30% sample, we save time by using 20% for training for class D; we think that sample is enough for GP to discover this class X  best fusion of features for classification purposes. For the 15% (50% for skewed classes) training sample, a ran-dom 10% (25% for skewed classes) sample of the collection is used for validation and the rest of the samples are used for testing. Correspondingly, for the 30% (20% for class D
In the remainder, we use 15% to refer to the first sample set and 30% to refer to the second sample set for each level, respectively. and 50% for skewed classes) training sample, a random 20% (15% for class D and 25% for skewed classes) sample of the collection is used for validation and the rest of the samples are used for testing. All approaches reported in later sec-tions use the same training and test sets. Results are based on test data sets only.
In order to demonstrate that the combination of differ-ent features by GP is able to provide better classification results, we need to compare it with the classification statis-tics of each feature in isolation (baselines). We used F1 as our comparison criteria. F1 is a combination of precision and recall. Precision p is defined as the proportion of cor-rectly classified records in the set of all records assigned to the target class. Recall r is defined as the proportion of correctly classified records out of all the records having the target class. F 1= 2 pr p + r . It is worth noting that F1 is a balanced combination of precision and recall. It reduces the risk that you can get perfect precision by always assigning zero categories, or perfect recall by always assigning every category. The result we want is to assign the correct cate-gories and only the correct categories, maximizing precision and recall at the same time, thereby maximizing F1. Macro-averaging (F1 results are computed on a per-category basis, then averaged over categories) was applied to get a single performance value over all categories. Tables 2 and 3 show the type of evidence that performs the best, according to macro F1, when applied to a kNN algorithm for a specific category in isolation in the test collections for the first level and the second level, respectively. Table 4 shows the average macro F1 over all categories for both the first level and the second level for each similarity evidence, also in isolation. Table 2: Best baseline for each category (first level).
From Tables 2 and 3 it can be seen that full-based evi-dence is most important for the majority of the classes. For those classes whose best performer was citation-based evi-dence, 1) Amsler had the highest value for category I.3 for both the 15% and 30% samples; 2) Amsler, Bib Coup, and Comp Hub were the most frequent evidence for the 15% sample; and 3) Amsler and Comp Aut were the most fre-quent evidence for the 30% sample. From Table 4 it can be seen that the best types of evidence are the full-based ones, followed by title-based, citation-based, and abstract-based evidence, respectively. This should be expected since: full is the evidence which appears in all the documents and con-tains the most complete information, the titles are relatively short, many documents are missing abstracts, and the infor-mation provided by the citation structure is very incomplete and imprecise.
 Table 3: Best baseline for each category (second level).
We ran experiments on the two training samples using different parameters. We noticed that a larger population size and different rate for the genetic transformation oper-ations like crossover, reproduction, and mutation produce better results. On the other hand, they have a huge effect on the training time. We used 400 as population size, 65% crossover, 30% reproduction, 5% mutation, and 30 genera-tions as our GP system experimental setting. In the next section, we only report performance of the best tree in the validation sets applied to the test sets.
We demonstrate the effectiveness of our classification framework in several ways: 1) by comparing its performance against the best baselines per class in isolation; 2) by com-paring it against a majority voting of classifiers using those best baselines as similarity functions; 3) by comparing our experimental results with the results achieved through a lin-ear combination of both the content-based and structure-based information through SVM; and 4) by comparing our experimental results with the results achieved through a content-based SVM classifier 2 . While the last comparison may seem inappropriate since the classifiers are trained and applied to different types of content, it does provide a good idea of the core performance of our method, clearly showing it as a valid alternative in classification tasks similar to the ones used in this paper.

The SVM classifier has been extensively evaluated for text classification on reference collections, thus offering a strong baseline for comparison. A content-based SVM classifier was first used in text classification by Joachims [19]. It works over a vector space, where the problem is to find a hyperplane with the maximal margin of separation between two classes. This hyperplane can be uniquely constructed by solving a constrained quadratic optimization problem, by means of quadratic programming techniques.

Joachims et al. [20] also have shown how to combine dif-ferent similarity measures by means of composite kernels. Their approach consists of combining simple and well-under-stood kernels by a series of  X  X ernel preserving X  operations, hence constructing an increasingly matching feature space S . In order to apply their method, we started by noticing that each one of our types of evidence can be represented as pos-itive document-by-document matrices, like those presented in Section 2.4. So, we can represent each type of evidence as a kernel matrix K evidence with elements K evidence ( d where d i and d j are document vectors. The kernel matrix for our final feature space S is obtained by means of a linear combination of our initial kernel matrices, as described in Eq. 2. For content we used a concatenation of title + abstract. where N is the number of distinct kinds of evidence used. Finally, the classification decisions for the combined kinds of evidence are obtained by applying a linear SVM classifier in our final feature space S .

For completeness, we also compare our classification framework against a classifier using a simple linear fusion of evidence as the similarity function and against a clas-sifier using the similarity function discovered through GA. The similarity function used in linear fusion is simply rep-resented through a summation of all the evidence, while the similarity function discovered through GA is represented by a weighted linear combination of all evidence, where the weights are assigned by the GA training process.

In a comparison, class by class, for the first level, between the majority GP (Table 5 columns 8 and 9) and the best evidence in isolation (Table 2), the majority GP outperforms the best evidence in most of the classes in both samples (only the performance for class A and E are worse). It also can be seen from Table 6 (columns 8 and 9) and Table 3 that this is true for most of the classes in the second level. There are two major reasons that the majority GP didn X  X  excel in a few classes (like classes A.1, B.8, C.0, C.4, D.1, F.3, J.1, and K.1) in the second level. One is that the numbers of documents belonging to these classes are too small, so GP could not discover enough characteristics of those classes for classification purposes. For example, class A.1 only has 52 documents while class C.0 only has 53 documents. The other reason is that the best baselines for those classes (see Table 3) are poor, as compared with other classes. This means that we do not have good types of evidence for those classes.

When comparing the majority GP against the majority using the best evidence on the first level (Table 5 between columns 2,3 and 8,9 ) it is clear that majority GP presents better performance: we obtain a gain of 12.86% in the 15% sample and 7.90% in the 30% sample. This is also true for the second level: we obtain a gain of 10.05% in the 15% sample and 10.42% in the 30% sample.

When majority GP is compared with the combination-based SVM classifiers, the gains are even higher: we obtain a gain of 14.50% in the 15% sample and 10.03% in the 30% sample for the first level and a gain of 23.73% in the 15% sample and 18.78% in the 30% sample for the second level. The performance of content-based SVM is also worse than that of the majority GP, which suggests that we have a better classification method.

Finally, when we compare majority GP against both linear fusion and GA, we see that GP is able to discover better similarity functions.

We did a pair-wise t-test comparing GP with all other columns in Tables 5 and 6, respectively. Majority GP is statistically significantly different from all the others, with p &lt; 0.05.
In the World Wide Web environment, several works have successfully used link information to improve classification performance. Different information about links, such as an-chor text describing the links, text from the paragraphs sur-rounding the links, and terms extracted from linked docu-ments, has been used to classify documents. For example, Furnkranz et al. [15], Glover et al. [16], and Sun et al. [33] show that anchor text and the paragraphs and headlines that surround the links helped improve the classification result. Similarly, Yang et al. [35] claimed that the use of terms from linked documents works better when neighboring documents are all in the same class.

Other researchers applied learning algorithms to handle both the text components of the Web pages and the links between them. For example, Joachims et al. [20] studied the combination of support vector machine kernel functions representing co-citation and content information. Cohn et al. [7] show that a combination of link-based and content-based probabilistic methods improved classification perfor-mance. Fisher and Everson [14] extended this work by show-ing that link information is useful when the document col-lection has a sufficiently high density in the linkage matrix and the links are of high quality.

Chakrabarti et al. [3] estimate the category of test docu-ments by studying the known classes of neighboring training documents. Oh et al. [31] improved on this work by using a filtering process to further refine the set of linked docu-ments to be used. Calado et al. [2] proposed a Bayesian network model to combine the output of a content-based classifier and the information provided by the documents X  link structure.
In this paper, we considered the problem of classification in the context of document collections where textual con-tent is scarce and imprecise citation information exists. A framework for tackling this problem based on Genetic Pro-gramming has been proposed and tested. Our experimental results on two different sets of documents from each level of the ACM Computing Classification System have demon-strated that the GP framework can be used to discover bet-ter similarity functions that, when applied to a kNN algo-rithm, can produce better classifiers than ones using indi-vidual evidence in isolation. Our experiments also showed that the framework achieved re sults better than both tra-ditional content-based and combination-based SVM classi-fiers. Comparison between th e GP framework and linear fusion as well as GA also showed that GP has the ability to discover better similarity functions.

Future work will include an extensive and comprehensive analysis of the reasons why GP-based fusion works and out-performs other similar techniques. We want to explore par-allel computation to address the scalability issue. We also want to test this framework with different document collec-tions (e.g., the Web) to assess its applicability. Besides that, we want to improve our current evidence, for example, us-ing better methods for citation matching, by trying to fix some OCR errors and using diffe rent matching strategies. Finally, new terminals (features) representing additional ev-idence may be explored; one example is further evidence such as anchor/citation text or patterns of authorship. This work was funded in part by NSF through grants DUE-0136690, DUE-0121679, and IIS-0086227. Additional support was provided by AOL, CNPq, MCT/FCT (grant SFRH/BD/4662/2001) , and Fucapi Techno logy Founda tion.
