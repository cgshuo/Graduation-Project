 1. Introduction
Measuring relative performance of information retrieval (IR) systems such as Web search engines is essential for research and development and for monitoring search quality in dynamic environments.
However, due to the size and dynamic nature of document collections and users, evaluating or comparing the retrieval performance of search engines in regular intervals is difficult. Automatic evaluation of retrieval systems is the ultimate solution to this problem.

Assessing IR effectiveness normally requires a test collection, a set of queries, and relevance information about each document with respect to each query. However, for very large databases creating relevance judgment is a difficult and extremely time-consuming task, since all documents need to be judged for rel-evance to each query. Text REtrieval Conference (TREC) uses the pooling approach to overcome such dif-ficulties ( Voorhees, 2001 ). Pooling is the selection of a fraction of documents with high similarity to the query for assessment, assuming that the pooled documents are a representative sample of the relevant por-tion of the whole collection. Zobel (1998) examined the effect of pooling on the effectiveness assessment in very large databases and showed its reliability. To reduce the manual effort involved in pooling, Cormack,
Lhotak, and Palmer (1999) proposed some new algorithms and compared them with the TREC pooling method. They found that it is possible to build an effective pool with fewer human judgments.
Another difficulty in creating relevance judgments is that people usually disagree about the relevance judgments. Harter (1996) examined the variations in relevance assessments and the measurement of retrie-val effectiveness using small databases. He found that the disagreement of assessors has little influence on the relative effectiveness of retrieval systems. Voorhees (2000) studied the effect of assessor disagreement using much larger collections of TREC and showed that with a little overlap in the relevance judgments, the relative effectiveness of IR systems is close to each other for different assessors. Her results show that differences in human relevance judgments do not affect the relative performance of IR systems.
Based on the observations of Voorhees (2000) and Soboroff, Nicholas, and Cahan (2001) proposed to use randomly selected documents from official TREC pools as the relevant documents to rank the retrieval systems. Their experiments revealed statistically significant correlations with the actual TREC rankings.
Recently, Wu and Crestani (2003) used the reference count method for automatic ranking of retrieval sys-tems. The assessment of various versions of this method was presented and compared with the random selection method and shown that it outperformed the random selection method in automatic ranking of retrieval systems.
 In this study, we present and assess the application of three different data fusion methods (Rank Position,
Borda Count, and Condorcet X  X efined later) in ranking retrieval systems in terms of their effectiveness with-out using human relevance judgments. Combining different rankings using data fusion, or selecting docu-ments based on multiple criteria is also referred to as rank aggregation ( Dwork, Kumar, Naor, &amp;
Sivakumar, 2001 ). In IR, data fusion merges the retrieval results of multiple systems and aims at achieving matic evaluation determines the (pseudo) relevant documents for evaluating the relative performance of a set of retrieval systems. For this purpose, the retrieval results of the systems to be ranked are merged following various techniques and the top-ranked documents in the merged result are considered as  X  X  X SEUDO
RELevant documentS X  X  ( Pseudorels ) and used to evaluate the relative effectiveness of retrieval systems. Note that we refer to these documents as  X  X (pseudo) relevant documents X  X , since their relevance is decided automat-ically and we do not know whether they are actually relevant or not. In this study, we also introduce a novel method for the selection of the retrieval systems to be used in data fusion. This method is based on the bias concept ( Mowshowitz &amp; Kawaguchi, 2002 ). Our experiments show that the use of bias in the selection of systems to be used in determining the Pseudorels improves the effectiveness of automatic ranking.
In our experiments, we use four TREC collections (TREC-3, -5, -6, and -7). The correlations between our methods and the actual TREC rankings are strong and statistically significant in all of the variations of our methods. We also compare the effectiveness of our methods with the results of Soboroff et al. (2001) and Wu and Crestani (2003) and show that we outperform their best cases.

It should be noted that all of these studies, including ours, use a complete query set. This may not be of much use in comparing different systems on individual queries ( Cronen-Townsend, Zhou, &amp; Croft, 2002 ).
Furthermore, our study, like the other automatic methods, concerns the relative performance of systems with respect to each other, but not the real performance of the individual systems. When analyzing systems, we may be interested in which relevant documents a given system was able to find and rank highly, which it was able to find but could not rank highly, which it was unable to find at all, and which of the documents it retrieved and ranked highly were not relevant. Answers to such questions are beyond the scope of this study.
The rest of the paper is organized as follows. In the following section, we review related works on auto-matic performance evaluation. In Section 3, we give the details of the data fusion techniques we used for determining the Pseudorels and the bias concept. In Section 4, we explain our experimental design in terms of the data sets and measures used. We present the experimental results and compare our methods with the previous automatic ranking techniques in Section 5. Finally, we conclude the paper in Section 6 with some future research pointers. 2. Related works
The first study in ranking retrieval systems without relevance judgments is the work of Soboroff et al. (2001) . Their methodology replaces human relevance judgments with a number of randomly selected doc-uments from a pool generated in the TREC environment. The consistency of the random selection (RS) method with human relevance judgment is measured by varying factors such as the pool depth, number of relevant documents, and allowing/disallowing duplicated documents in the pool. Ranking of retrieval systems with RS correlates positively and significantly with official TREC rankings.

Wu and Crestani (2003) use the reference count (RC) method for automatic ranking. In this method, for a given query, they first consider the list of documents returned by a system, take each document of the list, and find references. More specifically, they count the occurrences of that document in the lists provided by other systems. They take each list one by one, find the summation of these reference counts, and rank doc-uments using the total reference count sum for each document. In the variations of the RC method, they; for example, consider the rank positions of documents by assigning higher weights to documents that ap-pear in upper rank positions. The experiments show that the proposed methods are effective and in many cases more effective than RS; however, neither of them is good at predicting the performance of top per-forming systems ( Wu &amp; Crestani, 2003 ).

A recent work by Amitay, Carmel, Lempel, and Soffer (2004) uses a list of terms believed to be relevant ( onTopic terms) and a separate list of terms believed to be irrelevant ( offTopic terms) to a particular query.
For the top documents returned for the query, a relevance score is calculated using these terms. For this purpose they use two different score calculation approaches. In this process onTopic terms increase the scores and offTopic terms decrease the scores. They show that their results are consistent with the human-based results. The approach involves some human expertise in the selection of queries and terms and this process can be complicated. For example, in TREC-8 they were able to use 27 of the 5 0queries. The method is semi-automatic.
 In our recent work ( Can, Nuray, &amp; Sevdik, 2004 ), we introduce a new methodology called Automatic
Web Search Engine Evaluation Methodology (AWSEEM) to replace human-based relevance judgments with a set of automatically generated relevance judgments. In AWSEEM for each query, the top b docu-ments from each search engine are collected to form a pool of documents. Then these documents are indexed and ranked using the vector space model ( Salton &amp; McGill, 1983 ) and are sorted in descending order according to their similarity to the query. A constant number of top documents in this ordering are treated as Pseudorels . Statistical experiments involving eight Web search engines show that the AWSEEM approach is good at predicting both the best and worst performing systems.

In another recent study we introduce the concept of the imperfect environment (in such environments some documents judged to be relevant may not be available during the evaluation process due to various reasons such as network problems). For automatic ranking of retrieval systems in imperfect environments, we use an approach similar to AWSEEM and obtain promising results that are significantly consistent with that of human judgments ( Nuray &amp; Can, 2003 ).

Similar to the AWSEEM approach; Beitzel, Jensen, Chowdhury, and Grossman (2003) consider Web search engines and use the Open Directory Project (ODP, http://dmoz.org/ ) categories to determine (pseu-do) relevant documents for their evaluation. ODP is a volunteer (net-citizens) edited directory of the Web.
In the Beitzel et al. study, it is assumed that taxonomy entries are relevant to a query if their editor-entered titles or category titles exactly match the query. They evaluate six Web search engines by using a sample of real Web queries. Their semi-automatic method (if we ignore the volunteer time component in ODP it can be regarded as an automatic method) results are consistent with the human-based results in terms of deter-mining the top-3 and bottom-3 Web search engines. 3. Data fusion for ranking systems without relevance judgments
In general, a data fusion algorithm accepts two or more ranked lists and merges these lists into a single ranked list with the aim of providing a better effectiveness than all systems used for data fusion ( Croft, 2000, Chapter 1; Meng et al., 2002 ). Another aim of the data fusion is to group existing search services under one umbrella, as the number of existing search services increases ( Selberg &amp; Etzioni, 1996 ). In this regard, the CombSum and CombMNZ are two well-known data fusion algorithms designed by Fox and
Shaw (1994) . These algorithms are commonly used for testing and comparison purposes ( Lee, 1995, 1997 ). Aslam and Montague (2001) and Montague and Aslam (2002) developed two different merging algo-rithms based on the social welfare functions, Borda Fuse and Condorcet  X  s Fuse, and showed that the use of social welfare functions ( Roberts, 1976 ) as the merging algorithms in data fusion generally outperforms the CombMNZ algorithm.

Meng and his co-workers (2002) indicate that metasearch (data fusion) software involves four components: 1. Database/search engine selector: the search engines (databases) to be fused selected using some system selection methods. 2. Query dispatcher: the queries are submitted to the underlying search engines. 3. Document selector: documents to be used from each search engine are determined. The simplest way is the use of the top b documents. 4. Result merger: the results of search engines are merged using some merging techniques.

In our experiments, we deal with three of these components. We skip the query dispatcher component, because we have the results for each query: in TREC, each retrieval system for each query returns the top 1000 documents.

Our method of ranking retrieval systems without relevance judgments works as follows: First, we select k systems to be fused. As explained in detail later, for system selection we use three different approaches and they are referred to as best, normal, and bias. The maximum number of systems that can be selected is the number of systems ( n ) in the test environment and; therefore, k 6 n . Then using one of the data fusion methods described in this section, we combine the top b documents from each selected system. The top s% of the merging result is selected and treated as Pseudorels . For the value of s we use a percentage of documents instead of a constant number, because for some queries and pool depths the constant number for s may be higher than the number of documents returned as a response to that query from all of the systems. Finally, the performance of each retrieval system is evaluated and ranked using Pseudorels .
Fig. 1 provides a graphical description of our method.
In the following, we first present the data fusion algorithms we used for the selection of Pseudorels and then explain selection of systems to be used in data fusion. 3.1. Data fusion methods for determining pseudorels
In our experiments, we employ three data fusion methods for determining the Pseudorels : the Rank Po-sition, Borda Count, and Condorcet methods. 3.1.1. Rank position (reciprocal rank) method
In this approach, to merge the documents into a unified list only the rank positions of retrieved docu-ments are used. Retrieval systems determine the rank positions. When a duplicated document is found the inverse of its rankings are summed up, since the documents returned by more than one retrieval system might be more likely to be relevant. The following equation shows the computation of the rank score of document i using the position information of this document in all of the systems ( j =1 ... n ). Note that in this summation, systems not ranking a document are skipped.

In this approach, first Rank Position score of each document to be combined is evaluated, then using these rank position scores, documents are sorted in non-decreasing order. A portion (e.g., a certain percent-age) of the top documents is treated as Pseudorels .

Example. Suppose that we have four different retrieval systems A , B , C ,and D with a document collection ranked as follows:
Now, we compute the rank position of each document in the document list, and the rank scores of the doc-uments are as follows: i.e., it is the top most document; b is the second document, etc. 3.1.2. Borda count method
The first method taken from social theory of voting and used in the data fusion is Borda count method, which is introduced by Jean-CharlesdeBorda . Borda count and our next data fusion method, Condorcet  X  s algorithm, are based on democratic election strategies ( Roberts, 1976 ). The highest ranked individual (in an n -way vote) gets n votes and each subsequent gets one vote less (so the number two gets n 1 and the num-ber three gets n 2 and so on). If there are candidates 1eft unranked by the voter, the remaining points are divided evenly among the unranked candidates. Then, for each alternative, all the votes are added up and the alternative with the highest number of votes wins the election.

Example. Suppose that we evaluate the performance of three retrieval systems A , B , and C . The systems return the following ranked list of documents for a given query.
 (BC) of each document is computed by summing their Borda count values in individual systems ( BC system A , etc.) as follows:
Finally, the documents are ranked using their Borda counts. The final ranked list of documents is c &gt; a &gt; b &gt; e &gt; d .
In Borda count, the deletion of a document may reverse the rank positions of other documents ( Roberts, 1976; Sen, 1979 ). We ignore such cases (if any). Doing so simulates current Web search engine ranking practices. For example, some Web search engines, such as Google, use undisclosed algorithms that exploit page linking information among Web pages for final ranking of response URLs; deletion of a page may change the association among the remaining pages and reverse the rankings of some documents. 3.1.3. Condorcet method
The second method from social theory of voting, Condorcet  X  s algorithm, is named after the French mathematician MarieJeanAntoineNicolasdeCaritatCondorcet . In the Condorcet election method, voters rank the candidates in the order of preference. The vote counting procedure then takes into account each preference of each voter for one candidate over another. The Condorcet voting algorithm is a majoritarian method that specifies the winner as the candidate, which beats each of the other candidates in a pair wise comparison.
 Example. Suppose that we have three candidates (documents) a , b ,and c with five voters (systems) A , B , C , D , and E . (Note that in system C , the documents b and c have the same original rank.)
In the first stage, we use an N  X  N matrix for the pair wise comparison, where N is the number of candi-the number of wins, loses, and ties of document a over document b , respectively). In a system while count-ing votes, a document loses to all other retrieved documents if it is not retrieved by that system.
After that, we determine the pair wise winners. Each complimentary pair is compared, and the winner pair wise election is a tie, both receive one point in the  X  X  X ie X  X  column.

To rank the documents we use their win and lose values. If the number of wins that a document has is higher than the other one, then that document wins. Otherwise if their win property is equal we consider their lose scores, the document which has smaller lose score wins. If both win and lose scores are equal then a  X  4, 1, 04, 1, 0 b 1, 4, 0 X  2, 2, 1 c 1, 4, 02, 2, 1  X  a 2 0 0 b 011 c 011 both documents are tied. The final ranking of the documents in the example is a &gt; b = c . In our implementation, the documents b and c will be assigned the rank of 2 and 3 in a random fashion.
The original Condorcet method can lead to a condition referred to as the Condorcet paradox, also known as the voting paradox ( Roberts, 1976; Sen, 1979 ). In a paradoxical case, there would be an equiv-alence class of winners, and one would be unable to pick the top winner or rank them. (A commonly used is a , etc.) Our interpretation of the Condorcet method is similar to the one defined by Montague and Aslam (2002) , where equivalent sources are considered tied, and resolves this problem. 3.2. System selection methods and bias concept We consider three approaches for the selection of IR systems to be used in data fusion.

Best: a certain percentage of the top performing systems (obtained with human-based results) are used in data fusion to examine the effect of employing such systems for automatic ranking. Merging of the best systems to generate pseudo relevance judgments provides a motivation to find a system selection algo-rithm that improves the automatic performance prediction of retrieval systems. It also helps us intui-tively explain the conditions that affect the performance of our automatic ranking methods. Normal: all systems to be ranked are used in data fusion.

Bias: retrieval systems that behave differently from the norm or majority of all systems are used in data fusion. 3.2.1. Bias concept
Bias in IR is the balance or representativeness of a set of documents retrieved in response to a set of queries. In IR, undue inclusion or exclusion of certain documents in query response sets implies bias ( Mow-showitz &amp; Kawaguchi, 2002 ). A response set may display bias whether or not the retrieved documents are relevant to the users  X  need. A system is defined to be biased if its query responses are different from the norm, i.e., the majority of the documents returned by all systems. We hypothesize that systems that would return documents different from the majority could improve the data fusion effectiveness. The use of such systems would eliminate ordinary systems from data fusion, and this could provide better discrimination among documents and systems. In return, this could lead to a more accurate Pseudorels set and more accu-rate (automatic) ranking. Based on this hypothesis, we introduce a new method based on the bias concept for the selection of IR systems for data fusion.

The distribution of documents in the norm is obtained by computing the occurrence frequencies of doc-uments in the collection retrieved by several information retrieval systems for a given set of queries. For a particular information retrieval system, the distribution of items is obtained in a similar fashion. To com-pute the bias of a particular system, we first calculate the similarity of the vectors of the norm and the re-trieval system using a metric, e.g., their dot product divided by the square root of the product of their lengths, i.e., the cosine similarity measure. The bias value is obtained by subtracting this similarity value from 1, i.e., the similarity function for vectors v and w is the following: The bias between these two vectors is defined as follows ( Mowshowitz &amp; Kawaguchi, 2002 ):
Two variant measures of bias, one that ignores the order of the documents in the retrieved result set, and one that takes account of order, are formulated in the study of Mowshowitz and Kawaguchi (2002) .To ignore position, frequency of document occurrence is used to calculate bias. To take the order of documents into account, we may increment the frequency count of a document with a value different from 1. One pos-sibility is to increment frequency of a document by m/i where m is the number of positions and is the posi-tion of the document in the retrieved result set. We believe that order is important in the calculation of bias, since users usually just look at the documents of higher rank. Therefore, in the experiments, in our bias calculations we pay attention to the order of the documents in the response set of queries by incrementing the frequency of documents by m/i .

Example. To illustrate the computation of bias, suppose that we use two hypothetical retrieval systems A and B to define the norm, and three queries processed by each retrieval system. The documents retrieved by A and B for three queries are as follows (first row corresponds to the first query, etc.): vectors for A , B and the norm are: X A = (3,3,3,2,1,0,0), X respectively.
 The similarity of vector X A to X is 49/[(32)(96)] 1/2 = 0.8841, where the similarity of vector X [(30)(96)] 1/2 = 0.8758. The bias values for each system are the following:
If we repeat the calculations by taking order of documents into account, the response vector for A , B and norm are: X A =(10,8,4,2,1,0,0), X B = (0,8,22/3,0,2,8/3,7/3), and X = (10,16,34/3,2,8/3,7/3), respectively. The bias of A is 0.0.087 and the bias of B is 0.1226.

In our experiments, we first evaluate the bias of all of the retrieval systems used in the TREC year of concern. The retrieval systems are sorted in decreasing order of their bias values. Then the top 50% of the retrieval systems are used in the fusion process. We also tried the top 25%, but the 50% version gives better results ( Nuray, 2003 ). The Pseudorels are obtained from the result list of this fusion. Our expectation is that if most of the documents used in the fusion are rare and unique relevant documents then they will be at the top of the fusion result. Thus, automatic ranking of retrieval systems with these top (pseudo relevant) documents will have a strong correlation with the official TREC rankings. 4. Experimental design: data sets and measures
We used the ad hoc tasks of TREC-3, -5, -6, and -7. Table 1 gives the number of runs for each TREC used in this task and in our experiments. TREC evaluates systems using different variants of precision. (The effectiveness measure precision is the proportion of the retrieved documents that are relevant.) One of the measures used by TREC is mean non-interpolated average precision (MAP). The average precision for a single topic is the average of the precision after each relevant document is retrieved and using zero as the precision for not retrieved relevant documents. We use mean average precision where the meanaverage precision is the mean of the average precision for multiple topics (queries). We prefer MAP, since average precision is based on much more information than other effectiveness measures such as R -precision or P (10) and known to be a more powerful and more stable effectiveness measure ( Buckley &amp; Voorhees, 2000 ).
The correlation of the ranking with our proposed methods to the TREC official rankings is measured using the Spearman  X  s rank correlation coefficient. Two rankings are uncorrelated when the measure is 0, identical when the coefficient is 1, and in reverse order when the coefficient is 1. Another alternative for rank consistency comparisons is Kendall  X  s tau correlation coefficient (a function of the number of pair wise swaps needed to make two ranked lists equal). Both of these measures are popular rank correlation measures. However, some statisticians use Kendall  X  s tau when the rankings of the raters are based on ordi-nal data ( Shannon &amp; Davenport, 2001, p. 182 ). Our set of numbers, MAP values, is interval or ratio. (In our case, Spearman  X  s correlations are equivalent to the Pearson correlations calculated by using ranks.)
Furthermore, by using Spearman  X  s measure, we can directly compare our results with the RS and RC results (again based on Spearman  X  s coefficient) provided by Wu and Crestani (2003) . They prefer Spearman  X  s coefficient, since it punishes heavier for big ranking differences than Kendall  X  s tau.
We also assess the effectiveness of our methods for predicting the top and bottom performing retrieval systems using the average accuracy ( AA ) measure proposed by Wu and Crestani (2003) . Suppose we have two different rankings, R 1 and R 2 , for the same set of IR systems. By using the AA measure, we compare the consistency of the top n systems in these two rankings. We first use accuracy A ( n ) to measure the proportion of the top n systems in R 1 that appears in the top n systems of R
R 2 = {1,2,3,4,5, ... }; then A (1) = 1, A (2) = 1/2, A (3) = 2/3 and so on. The average accuracy is defined as follows:
For the above example AA (3) becomes (1 + 1/2 + 2/3)/3 0.72. For the bottom systems, the AA calcula-tions are performed in a similar fashion. 5. Experimental results
We performed the experiments using different pool depths ( b = 10, 20, and 30) and a certain percentage ( s = 10%, 20%, 30%, 40%, and 50%) of merged documents as relevant for each query. For a summative evaluation, the average correlations of various numbers of relevant documents for each pool depth and
TREC year are provided. In our experiments, all of the correlations are statistically significant at a 99% confidence level.

First we fuse the 25% of the best systems X  X etermined using the actual TREC rankings X  X o see the effect of using best systems in the fusion component of the automatic ranking. Our hope was that the automatic rankings would be highly similar to the actual TREC rankings, since best systems improve the effectiveness approach), then (2) the 50% of the systems that show the highest bias (bias approach). Our aim with the use of normal and bias versions is to get correlations that are competitive with the best case.
To prevent any possible confusion we have to state that the best system experiments are for motivational purposes. Intuitively data fusion with  X  X  X est systems X  X  provides the  X  X  X aximum base line X  X  standard, i.e., the probable best performance with our data fusion methods. If we could not obtain a desirable performance using the best systems then there would be almost no sense to continue with the methods. Note that the use of the best systems in real data fusion applications is impossible, since in a real environment the  X  X  X est sys-tem X  X  information is unavailable; however, in our controlled TREC environment we know the actual per-formance of each retrieval system. 5.1. Results with the best system selection approach
The correlations of ranking with the best to the actual TREC rankings for the pool depth ( b ) 3 0are given in Table 2 . (For the best case, we provide output with only one b value since it is a good representative of other b values.) All of the correlations are the averages of the correlations of various numbers of Pseudorels (i.e., as indicated before for s = 10%, 20%, 30%, 40%, and 50%). The highest correlations are observed with the use of the Rank Position method. The correlations show that the rankings with the best version of our methods are highly similar to the actual TREC rankings (as high as 0.876). For all cases standard deviation values are small and therefore the mean values are good representative for each case.

Note that the Rank Position and the Borda count methods make the assumption that all (input) retrieval systems used for data fusion are doing equally good job in ranking; i.e., the i th rank means the same thing to all systems. However, in practice, the 2nd document may not be relevant in one system whereas the first five documents may all be relevant in another system. We are relying on this (equally good) assumption in the Rank Position and Borda count methods. However, the Condorcet method does not use an assumption like that (i.e., it does not pay attention to exact rank positions of the documents). This may be the reason why the Borda count and Rank Position methods appear to do somewhat better when using the best sys-tems (note that the best systems are  X  X  X ore X  X  equal to each other). However, as will be illustrated in the next subsection, when this equality condition is not satisfied with normal and bias approaches, these rank-based systems may be disadvantaged and Condorcet method appears to do somewhat better.

Our best system experiments provide a simple, yet important observation: better or more effective sys-tems (in real life this could be interpreted as highly credible or reliable systems) have the potential of pro-viding superior performance in automatic ranking when a rank-based data fusion method is used. 5.2. Results with the normal and bias system selection approaches The correlations results with the normal and bias versions are shown in Tables 3 X 5 . (Like the values of Table 2 , standard deviation (Std.) values are small and therefore the mean values are good representative for each case.) The correlations of the normal are not as strong as the correlations of the best. Although for some of the TRECs the bias version  X  s correlations are very close to the correlations of the best, this is not true for all of the methods and all of the TRECs. The bias version of Condorcet is an interesting case, since with TREC-6 ( Tables 3 X 5 ) that version is better than that of the Condorcet results obtained with the best systems given in Table 2 .

For b = 1 0(see Table 3 ) the Condorcet method is better than the other methods in terms of the aver-age correlations for all TRECs, although the rank position method is the winner when we count the number of wins in each row of the table using only the Normal columns. Moreover, for b = 2 0and b =30 most of the time the Condorcet method has the highest correlations (see Tables 4 and 5 ). The superiority of the Condorcet method is easier to see by looking at the average values given in the last row of each table.

The italicised scores in Tables 3 X 5 indicate the version of the method (normal or bias) that give higher correlation for the corresponding data fusion method. As it can be seen the bias version of Borda count and
Rank Position method are not as strong as their normal version with the pool b =10and b = 2 0(see the averages given in Tables 3 and 4 for Borda and Rank versions). The bias version of those methods are stronger than their normal version in the pool with b = 30. The highest average correlations (shown in the last row of each table) are observed with the bias version of the Condorcet method in Tables 3 X 5 .
Of the 12 tests, Condorcet with bias wins in five cases, Borda with bias wins 4, and Condorcet normal wins 3. Note that the number of wins is obtained by finding the best performance in each row of Tables 3 X 5 (we have three tables and in each table there are four cases/rows, so we have total of 12 test cases). For example, the first row of Table 3 provides the first case and in this row the best performance is provided by the bias version of Condorcet which is 0.862. In these tables for easier identification the winner of each row (case) is underlined. Average correlation values corresponding to the bias version of Condorcet are 10 X 14% better than the second best case in all experiments.

A one-way analysis of variance (ANOVA) using a Scheffe multiple comparison was conducted to deter-mine whether Condorcet with bias provides the best performance in ranking. The average Spearman cor-relation values for each pool X  X ethod X  X ersion combination were used as the response variable. For example, for b = 1 0the six average correlations, one each for Borda normal, Borda bias, Rank normal,
Rank bias, Condorcet normal, and Condorcet bias, were used. Thus, a total of 18 observations were used, six for each of the three pool levels. The six method version levels, Borda normal, Borda bias, Rank normal,
Rank bias, Condorcet normal, and Condorcet bias, made up the classification variable. The significance of the ANOVA was very strong, F (5,12) = 12.3 0( p &lt; 0.001), and the mean correlation for each method and version, averaged over all the pool levels, is given in the following table: Method Average correlation Condorcet bias 0.635 Condorcet normal 0.560 Rank normal 0.555 Borda normal 0.554 Borda bias 0.551 Rank bias 0.548
The Scheffe multiple comparison test indicated that, at a significance level of 0.05, the Concorcet with bias has a significantly higher correlation than each of the other five methods. Thus, based on these tests, we can conclude that (on the average and in terms of being the most frequent winner) the Condorcet method with bias provides the best performance in automatic ranking. In the rest of the paper, when appropriate, we use the bias version of Condorcet as our representative method.

Our methods were especially good at distinguishing the systems with poor performance. Fig. 2 shows the rankings obtained using normal and bias version of the Condorcet method with respect to the actual rank-ings in the TRECs for a pool of depth ( b =) 2 0and when the ( s =) 10% of the documents are assumed to be relevant. In this figure, the left vertical axis indicates the MAP scores using actual (human-based) TREC evaluations, and the right axis indicates the same using the automatic evaluations. The figure shows that for the given b and s values and for TREC-3 and -5 the bias version of the method is better than its normal version in predicting the worst, best and middle performing systems.
 The AA measure observations for the top and bottom 1 0systems of TREC-3 also support the same claim. For the top 1 0documents, the AA values are 0.201 and 0.405 for the normal and bias versions, respectively.
For the bottom 10 systems it is 0.741 (normal) and 0.778 (bias) with the same conditions of pool depth and number of relevant documents given in Fig. 2 (the detailed AA values are given in Tables 6 and 7 ).
The AA measure of the Condorcet method is calculated for various pool depths using the 10% of the documents as relevant ( s = 10, chosen as a representative case). We compute the AA values for the top and bottom 1 0systems prediction. As it can be seen in Tables 6 and 7 , the AA values are significantly high for the bottom performing systems. This means that data fusion is more effective in determining the ranking of the systems with poor (bottom) performance and less effective in determining the ranking of the systems with good (top) performance. The AA value for the top and bottom performing systems increases with the use of the bias version of the Condorcet method (see Table 7 ). The bias version of the Condorcet algorithm generally improves the ability of our method in discriminating the top and bottom performing systems. For example, for b = 2 0the average AA values for the normal version for the top 1 0and bottom 1 0systems are 0.143 and 0.768 (see Table 6 , the last row). The same values for the bias version are 0.236 and 0.846, respec-tively (see Table 7 ).

For the bottom performing systems, the normal version of Condorcet had an AA value of 0.7713 and the bias version  X  s value was 0.7187. A one-way ANOVA indicated no significant difference between these AA values at a 0.05 significance level. However, for top performing systems, the AA values for the normal and biased versions of Condorcets  X  were respectively 0.1173 and 0.206. A one-way ANOVA indicated a very strong significant difference between these value AA s at a 0.01 significance level.

There is one interesting pattern about the AA values. As we go from TREC-3 to TREC-7, the AA values for the top systems tend to decrease, and the AA values for the bottom systems tend to increase.
For example, in Table 7 , the middle column that corresponds to b = 20, for the top systems the AA values decrease from 0.405 to 0.0, and for the bottom systems the AA values increase from 0.778 to 0.932 in a consistent manner. In the other columns of Tables 6 and 7 we observe the same trend. This perhaps can be explained by a detailed study of the query characteristics as we go from TREC-3 to
TREC-7 and such an investigation may open a gateway for better querying techniques for automatic system ranking.
 5.3. Comparison with previous automatic ranking methods
So far in this paper we show that the Condorcet method provides the best performance in terms of auto-matic ranking. In this section, we compare the normal and bias versions of the Condorcet method with the random selection, RS, method of Soboroff et al. (2001) and different versions of the reference count, RC, method of Wu and Crestani (2003) . In this study our concern is data fusion using only the  X  X  X ank informa-tion X  X  provided by the retrieval systems involved in comparison. Our previous automatic ranking method
AWSEEM uses the actual contents of the top documents for determining the Pseudorels . For this reason we do not examine the performance of AWSEEM nor do we include it in our comparison experiments.
The performance scores for the RC and RS methods are taken from ( Wu &amp; Crestani, 2003 ). The RS method used for comparison is slightly different from its original definition. Soboroff and his co-workers first calculate the average number and standard deviation of relevant documents appearing in the pool per query by checking TREC official figures, then randomly select the documents from the pool at a per-centage value. This value is drawn from a normal distribution by using the corresponding TREC mean and standard deviation values. On the other hand, Wu and Crestani (2003) select a certain percentage (10%) of the documents as relevant documents for obtaining the RS performance scores. This provides a fair com-parison environment since our approach and the RC method of Wu and Crestani do not use such infor-mation (i.e., TREC mean and standard deviation values); furthermore, such information would not be available in real applications.

The correlations obtained with the use of mean average precision (MAP) are presented for normal and bias versions of the Condorcet method, while the MAP, R -precision (RP), and precision at 100 (P100) cor-relations of both RC (RC MAP ,RC RP ,RC P100 ) and RS (RS 8 . In each column of the RC and RS methods, we use the bestperformance scores provided in Wu and Cre-stani (2003) . In this table, our italicised scores for a given TREC show the correlations higher than that of the best case of RC (underlined), where our bold face scores indicate the correlations higher than that of the best case RS (underlined). For each method we also provide the average correlation for all TRECs used in the experiments. The averages and the individual scores indicate that our results are significantly better than that of previous automatic approaches.

In order to measure the improvements with our Condorcet approach we use the average correlation val-ues of all TRECs for each method given in Table 8 (the last row). For this purpose, percentage improve-ments are calculated: if x is the mean correlation with our (Condorcet) method and y is the mean correlation with a previously proposed method (RS or RC), then the percent improvement with our method is ( x / y 1) * 100. The percentage improvements with our method are shown in Fig. 3 . The highest improvements are observed with the Condorcet-Bias version with respect to RC ments are always positive except the improvements made with respect to RC the Condorcet method, which is 1.9%. Fig. 3 also shows that the bias version of the Condorcet method with a pool depth b = 3 0(the last bar of each group) is the best method used in the ranking of information retrieval systems without relevance judgments.

Wu and Crestani (2003) show that their reference count (RC) method provides better AA values than that of Soboroff et al.  X  s random selection (RS) method. The AA measures obtained with the use of our Con-dorcet method are higher than the AA measures for RC given in Wu and Crestani (2003) . Our AA measures are between 0 and 0.543 for the top performing systems and between 0.357 and 0.966 for the bottom per-forming systems. However, the AA measure of the RC methods are 0and 0.1 for the top systems and be-tween 0.28 and 0.58 for the bottom systems ( Wu &amp; Crestani, 2003 ).

A one-way ANOVA with a Scheffe multiple comparison test was conducted to determine whether the bias version of Condorcet was significantly better than the normal version of Condorcet, RS, and RC.
The average correlation of each method averaged over all values of the last row of Table 8 are given in the following table: Method Mean Condorcet (bias) 0.653 Condorcet (normal) 0.560 RC 0.506 RS 0.493
At a 0.05 significant level, the Scheffe test indicated that the bias version of Cordorcet had a signifi-cantly higher correlation than RC and RS, but not significantly higher than the normal version of
Condorcet. 6. Conclusions and future work
In this paper, we propose new methods for automatic ranking of retrieval systems without relevance judgments using three different data fusion techniques: the Rank Position, Borda count, and Condorcet methods. We compare the effectiveness of these three methods in automatic ranking. We consider the effec-tiveness of ranking by using all of the systems for determining Pseudorels and by using some of the systems that behave differently from the norm. The norm and being different from the norm are defined by using the bias concept ( Mowshowitz &amp; Kawaguchi, 2002 ). We demonstrate that the bias concept improves the auto-matic ranking of retrieval systems. The experiments show a high level of statistically significant consistency between automatic and human-based rankings. Our methods are robust and remarkably good in determin-ing the poor systems. We show that systems with better performance, in real life this could be interpreted as more reliable or highly credible systems , have the potential of providing superior effectiveness in perfor-mance estimation when a rank-based data fusion method is used.

We compare our Condorcet results with the random selection (RS) method of Soboroff et al. (2001) and various versions of the reference count (RC) method of Wu and Crestani (2003) . We show that our method outperforms the best cases of both the RS and RC approaches.

The practical implications of our study and in general automatic ranking of retrieval systems have a wide spectrum ( Can et al., 2004 ). Possible ways of extending our study include the following: (1) The data fusion methods Rank Position and Borda count are based on simple weighting. One may also consider other weighting schemes such as those based on utility functions. However, selection of such functions and the parameters needed for their definitions may require a great deal of exploratory analysis ( Fan, Fox, Pathak, &amp; Wu, 2004 ). (2) A weighted version of the Borda count and Condorcet methods may be used in Pseudorels selection ( Montague &amp; Aslam, 2002 ). (3) Methods other than bias can be used for system selection or can be integrated with the bias concept. For example, iterative or recursive data fusion is one possibility for this purpose: systems selected by an initial data fusion process can be used in subsequent data fusion steps for better results. We explored the iterative version of the Rank Position method on one of the TREC data with certain conditions; the results are promising ( Nuray, 2003 ). (4) Superior automatic ranking performance with Rank Position and Borda count using best systems warrants further research on rank aggregation and automatic evaluation of search engines by using search engine credibility or accuracy (cf. Maynard-
Zhang &amp; Lehmann, 2003 ). (5) Effects of queries (very short, very long, etc.) on automatic ranking can be investigated. Query types that work better may be used to get better automatic ranking results without changing the data fusion methods, and; of course, this idea can be combined with other improvement ideas.
Similarly, discovering the reasons of better performance with some TRECs or with bottom systems may lead us to find more suitable query types for automatic ranking. (6) In the ranking experiments, we cover a wide range of number of systems (between 4 0for TREC-3 and 1 03 for TREC-7). However, inverse-sca-lability or downsizability of our approach for minimal number of systems needs further investigation. (7) In the Web environment if the purpose is to measure the relative performance of a few  X  X  X elect X  X  search engines, then assuming that systems are good and rankings provided by these systems are equally good, a rank-based method may give a superior performance as shown by our  X  X  X est system X  X  experiments. This needs further investigation.
 Acknowledgement
We are grateful to Jon M. Patton of Miami University for his valuable comments on the paper and his helps in the statistical experiments. We are also grateful to Ben Carterette of University of Massachusetts,
Amherst and Petrito Maynard-Zhang of Miami University for their numerous insightful comments for an initial version of this paper. We thank two anonymous referees for their remarks which significantly im-proved the paper and NIST ( http://www.nist.gov/ ) for making the TREC data available.
 References
