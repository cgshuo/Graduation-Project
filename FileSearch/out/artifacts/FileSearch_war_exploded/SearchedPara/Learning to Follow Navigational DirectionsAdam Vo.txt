 Spatial language usage is a vital component for physically grounded language understanding sys-tems. Spoken language interfaces to robotic assis-tants (Wei et al., 2009) and Geographic Informa-tion Systems (Wang et al., 2004) must cope with the inherent ambiguity in spatial descriptions.
The semantics of imperative and spatial lan-guage is heavily dependent on the physical set-ting it is situated in, motivating automated learn-ing approaches to acquiring meaning. Tradi-tional accounts of learning typically rely on lin-guistic annotation (Zettlemoyer and Collins, 2009) or word distributions (Curran, 2003). In con-trast, we present an apprenticeship learning sys-tem which learns to imitate human instruction fol-lowing, without linguistic annotation. Solved us-ing a reinforcement learning algorithm, our sys-tem acquires the meaning of spatial words through 1. go vertically down until you X  X e underneath eh 2. then eh go right until you X  X e 3. you X  X e between springbok and highest view-Figure 1: A path appears on the instruction giver X  X  map, who describes it to the instruction follower. grounded interaction with the world. This draws on the intuition that children learn to use spatial language through a mixture of observing adult lan-guage usage and situated interaction in the world, usually without explicit definitions (Tanz, 1980).
Our system learns to follow navigational direc-tions in a route following task. We evaluate our approach on the HCRC Map Task corpus (Ander-son et al., 1991), a collection of spoken dialogs describing paths to take through a map. In this setting, two participants, the instruction giver and instruction follower , each have a map composed of named landmarks. Furthermore, the instruc-tion giver has a route drawn on her map, and it is her task to describe the path to the instruction follower , who cannot see the reference path. Our system learns to interpret these navigational direc-tions, without access to explicit linguistic annota-tion.

We frame direction following as an apprentice-ship learning problem and solve it with a rein-forcement learning algorithm, extending previous work on interpreting instructions by Branavan et al. (2009). Our task is to learn a policy, or mapping from world state to action, which most closely fol-lows the reference route. Our state space com-bines world and linguistic features, representing both our current position on the map and the com-municative content of the utterances we are inter-preting. During training we have access to the ref-erence path, which allows us to measure the util-ity, or reward, for each step of interpretation. Us-ing this reward signal as a form of supervision, we learn a policy to maximize the expected reward on unseen examples. Levit and Roy (2007) developed a spatial seman-tics for the Map Task corpus. They represent instructions as Navigational Information Units, which decompose the meaning of an instruction into orthogonal constituents such as the reference object, the type of movement, and quantitative as-pect. For example, they represent the meaning of  X  X ove two inches toward the house X  as a reference object (the house), a path descriptor (towards), and a quantitative aspect (two inches). These represen-tations are then combined to form a path through the map. However, they do not learn these rep-resentations from text, leaving natural language processing as an open problem. The semantics in our paper is simpler, eschewing quantitative as-pects and path descriptors, and instead focusing on reference objects and frames of reference. This simplifies the learning task, without sacrificing the core of their representation.

Learning to follow instructions by interacting with the world was recently introduced by Brana-van et al. (2009), who developed a system which learns to follow Windows Help guides. Our re-inforcement learning formulation follows closely from their work. Their approach can incorpo-rate expert supervision into the reward function in a similar manner to this paper, but is also able to learn effectively from environment feedback alone. The Map Task corpus is free form conversa-tional English, whereas the Windows instructions are written by a professional. In the Map Task cor-pus we only observe expert route following behav-ior, but are not told how portions of the text cor-respond to parts of the path, leading to a difficult learning problem.

The semantics of spatial language has been studied for some time in the linguistics literature. Talmy (1983) classifies the way spatial meaning is Figure 2: The instruction giver and instruction fol-lower face each other, and cannot see each others maps. encoded syntactically, and Fillmore (1997) studies spatial terms as a subset of deictic language, which depends heavily on non-linguistic context. Levin-son (2003) conducted a cross-linguistic semantic typology of spatial systems. Levinson categorizes the frames of reference, or spatial coordinate sys-tems 1 , into 1. Egocentric : Speaker/hearer centered frame 2. Allocentric : Speaker independent. Ex:  X  X he
Levinson further classifies allocentric frames of reference into absolute, which includes the cardi-nal directions, and intrinsic, which refers to a fea-tured side of an object, such as  X  X he front of the car X . Our spatial feature representation follows this egocentric/allocentric distinction. The intrin-sic frame of reference occurs rarely in the Map Task corpus and is ignored, as speakers tend not to mention features of the landmarks beyond their names.

Regier (1996) studied the learning of spatial language from static 2-D diagrams, learning to distinguish between terms with a connectionist model. He focused on the meaning of individual terms, pairing a diagram with a given word. In contrast, we learn from whole texts paired with a path, which requires learning the correspondence between text and world. We use similar geometric features as Regier, capturing the allocentric frame of reference.

Spatial semantics have also been explored in physically grounded systems. Kuipers (2000) de-veloped the Spatial Semantic Hierarchy, a knowl-edge representation formalism for representing different levels of granularity in spatial knowl-edge. It combines sensory, metrical, and topolog-ical information in a single framework. Kuipers et al. demonstrate its effectiveness on a physical robot, but did not address the learning problem.
More generally, apprenticeship learning is well studied in the reinforcement learning literature, where the goal is to mimic the behavior of an ex-pert in some decision making domain. Notable ex-amples include (Abbeel and Ng, 2004), who train a helicopter controller from pilot demonstration. The HCRC Map Task Corpus (Anderson et al., 1991) is a set of dialogs between an instruction giver and an instruction follower . Each participant has a map with small named landmarks. Addition-ally, the instruction giver has a path drawn on her map, and must communicate this path to the in-struction follower in natural language. Figure 1 shows a portion of the instruction giver X  X  map and a sample of the instruction giver language which describes part of the path.

The Map Task Corpus consists of 128 dialogs, together with 16 different maps. The speech has been transcribed and segmented into utterances, based on the length of pauses. We restrict our attention to just the utterances of the instruction giver, ignoring the instruction follower. This is to reduce redundancy and noise in the data -the in-struction follower rarely introduces new informa-tion, instead asking for clarification or giving con-firmation. The landmarks on the instruction fol-lower map sometimes differ in location from the instruction giver X  X . We ignore this caveat, giving the system access to the instruction giver X  X  land-marks, without the reference path.

Our task is to build an automated instruction follower . Whereas the original participants could speak freely, our system does not have the ability to query the instruction giver and must instead rely only on the previously recorded dialogs.
 Figure 3: Sample state transition. Both actions get credit for visiting the great rock after the indian country. Action a1 also gets credit for passing the great rock on the correct side. We frame the direction following task as a sequen-tial decision making problem. We interpret ut-terances in order, where our interpretation is ex-pressed by moving on the map. Our goal is to construct a series of moves in the map which most closely matches the expert path.

We define intermediate steps in our interpreta-tion as states in a set S , and interpretive steps as actions drawn from a set A . To measure the fi-delity of our path with respect to the expert, we define a reward function R : S  X  A  X  R + which measures the utility of choosing a particular action in a particular state. Executing action a in state s carries us to a new state s 0 , and we denote this tran-sition function by s 0 = T ( s,a ) . All transitions are deterministic in this paper. 2 For training we are given a set of dialogs D . Each dialog d  X  D is segmented into utter-ances ( u 1 ,...,u m ) and is paired with a map, which is composed of a set of named landmarks ( l 1 ,...,l n ) . 4.1 State The states of our decision making problem com-bine both our position in the dialog d and the path we have taken so far on the map. A state s  X  S is composed of s = ( u i ,l,c ) , where l is the named landmark we are located next to and c is a cardinal direction drawn from { North , South , East , West } which determines which side of l we are on. Lastly, u i is the utterance in d we are currently interpreting. 4.2 Action An action a  X  A is composed of a named land-mark l , the target of the action, together with a cardinal direction c which determines which side to pass l on. Additionally, a can be the null action, with l = l 0 and c = c 0 . In this case, we interpret an utterance without moving on the map. A target l together with a cardinal direction c determine a point on the map, which is a fixed distance from l in the direction of c .

We make the assumption that at most one in-struction occurs in a given utterance. This does not always hold true -the instruction giver sometimes chains commands together in a single utterance. 4.3 Transition Executing action a = ( l 0 ,c 0 ) in state s = ( u i ,l,c ) leads us to a new state s 0 = T ( s,a ) . This tran-sition moves us to the next utterance to interpret, and moves our location to the target of the action. If a is the null action, s = ( u i +1 ,l,c ) , otherwise s = ( u i +1 ,l 0 ,c 0 ) . Figure 3 displays the state tran-sitions two different actions.

To form a path through the map, we connect these state waypoints with a path planner 3 based on A  X  , where the landmarks are obstacles. In a physical system, this would be replaced with a robot motion planner. 4.4 Reward We define a reward function R ( s,a ) which mea-sures the utility of executing action a in state s . We wish to construct a route which follows the expert path as closely as possible. We consider a proposed route P close to the expert path P e if P visits landmarks in the same order as P e , and also passes them on the correct side.

For a given transition s = ( u i ,l,c ) , a = ( l 0 ,c 0 ) , we have a binary feature indicating if the expert path moves from l to l 0 . In Figure 3, both a 1 and a 2 visit the next landmark in the correct order.
To measure if an action is to the correct side of a landmark, we have another binary feature indi-cating if P e passes l 0 on side c . In Figure 3, only a 1 passes l
In addition, we have a feature which counts the number of words in u i which also occur in the name of l 0 . This encourages us to choose poli-cies which interpret language relevant to a given landmark.

Our reward function is a linear combination of these features. 4.5 Policy We formally define an interpretive strategy as a policy  X  : S  X  A , a mapping from states to ac-tions. Our goal is to find a policy  X  which max-imizes the expected reward E  X  [ R ( s, X  ( s ))] . The expected reward of following policy  X  from state s is referred to as the value of s , expressed as When comparing the utilities of executing an ac-tion a in a state s , it is useful to define a function which measures the utility of executing a , and fol-lowing the policy  X  for the remainder. A given Q function implicitly defines a policy  X  by
Basic reinforcement learning methods treat states as atomic entities, in essence estimating V  X  as a table. However, at test time we are following new directions for a map we haven X  X  previously seen. Thus, we represent state/action pairs with a feature vector  X  ( s,a )  X  R K . We then represent the Q function as a linear combination of the fea-tures, and learn weights  X  which most closely approxi-mate the true expected reward. 4.6 Features Our features  X  ( s,a ) are a mixture of world and linguistic information. The linguistic information in our feature representation includes the instruc-tion giver utterance and the names of landmarks on the map. Additionally, we furnish our algo-rithm with a list of English spatial terms, shown in Table 1. Our feature set includes approximately 200 features. Learning exactly which words in-fluence decision making is difficult; reinforcement learning algorithms have problems with the large, sparse feature vectors common in natural language processing.

For a given state s = ( u,l,c ) and action a = ( l ,c 0 ) , our feature vector  X  ( s,a ) is composed of the following: above, below, under, underneath, over, bottom, top, up, down, left, right, north, south, east, west, on  X  Coherence : The number of words w  X  u that  X  Landmark Locality : Binary feature indicat- X  Direction Locality : Binary feature indicat- X  Null Action : Binary feature indicating if l 0 =  X  Allocentric Spatial : Binary feature which  X  Egocentric Spatial : Binary feature which Given this feature representation, our problem is to find a parameter vector  X   X  R K for which Q ( s,a ) =  X  T  X  ( s,a ) most closely approximates E [ R ( s,a )] . To learn these weights  X  we use SARSA (Sutton and Barto, 1998), an online learn-ing algorithm similar to Q -learning (Watkins and Dayan, 1992).

Algorithm 1 details the learning algorithm, which we follow here. We iterate over training documents d  X  D . In a given state s t , we act ac-cording to a probabilistic policy defined in terms of the Q function. After every transition we up-date  X  , which changes how we act in subsequent steps.
 Exploration is a key issue in any RL algorithm. If we act greedily with respect to our current Q function, we might never visit states which are ac-Input : Dialog set D
Output : Feature weights  X 
Initialize  X  to small random values until  X  converges do 3 foreach Dialog d  X  D do 4 Initialize s 0 = ( l 1 ,u 1 ,  X  ) , 5 for t = 0 ; s t non-terminal; t ++ do 6 Act: s t +1 = T ( s t ,a t ) 8 Update: 9  X   X  R ( s t ,a t ) +  X  T  X  ( s t +1 ,a t +1 ) 10  X   X  T  X  ( s t ,a t ) 11  X   X   X  +  X  t  X  ( s t ,a t ) X  12 end 13 end end return  X 
Algorithm 1: The SARSA learning algorithm. tually higher in value. We utilize Boltzmann ex-ploration, for which The parameter  X  is referred to as the tempera-ture , with a higher temperature causing more ex-ploration, and a lower temperature causing more exploitation. In our experiments  X  = 2 .

Acting with this exploration policy, we iterate through the training dialogs, updating our fea-ture weights  X  as we go. The update step looks at two successive state transitions. Suppose we are in state s t , execute action a t , receive reward r t = R ( s t ,a t ) , transition to state s t +1 , and there choose action a t +1 . The variables of interest are ( s t ,a t ,r t ,s t +1 ,a t +1 ) , which motivates the name SARSA.
 Our current estimate of the Q function is Q ( s,a ) =  X  T  X  ( s,a ) . By the Bellman equation, for the true Q function After each action, we want to move  X  to minimize the temporal difference , solid red line is the path the system follows. For each feature  X  i ( s t ,a t ) , we change  X  i propor-tional to this temporal difference, tempered by a learning rate  X  t . We update  X  according to  X  =  X  +  X  t  X  ( s t ,a t )( R ( s t ,a t )
Here  X  t is the learning rate, which decays over time 4 . In our case,  X  t = 10 10+ t , which was tuned on the training set. We determine convergence of the algorithm by examining the magnitude of updates to  X  . We stop the algorithm when We evaluate our system on the Map Task corpus, splitting the corpus into 96 training dialogs and 32 test dialogs. The whole corpus consists of approx-imately 105,000 word tokens. The maps seen at test time do not occur in the training set, but some of the human participants are present in both. 6.1 Evaluation We evaluate how closely the path P generated by our system follows the expert path P e . We mea-sure this with respect to two metrics: the order in which we visit landmarks and the side we pass them on.

To determine the order P e visits landmarks we compute the minimum distance from P e to each landmark, and threshold it at a fixed value.
To score path P , we compare the order it visits landmarks to the expert path. A transition l  X  l 0 which occurs in P counts as correct if the same transition occurs in P e . Let | P | be the number of landmark transitions in a path P , and N the number of correct transitions in P . We define the order precision as N/ | P | , and the order recall as N/ | P e | .

We also evaluate how well we are at passing landmarks on the correct side. We calculate the distance of P e to each side of the landmark, con-sidering the path to visit a side of the landmark if the distance is below a threshold. This means that a path might be considered to visit multiple sides of a landmark, although in practice it is usu-ally one. If C is the number of landmarks we pass on the correct side, define the side precision as C/ | P | , and the side recall as C/ | P e | . 6.2 Comparison Systems The baseline policy simply visits the closest land-mark at each step, taking the side of the landmark which is closest. It pays no attention to the direc-tion language.

We also compare against the policy gradient learning algorithm of Branavan et al. (2009). They parametrize a probabilistic policy Pr( s | a ;  X  ) as a log-linear model, in a similar fashion to our explo-ration policy. During training, the learning algo-rithm adjusts the weights  X  according to the gradi-ent of the value function defined by this distribu-tion.

Reinforcement learning algorithms can be clas-sified into value based and policy based. Value methods estimate a value function V for each state, then act greedily with respect to it. Pol-icy learning algorithms directly search through the space of policies. SARSA is a value based method, and the policy gradient algorithm is pol-icy based.
 Table 2: Experimental results. Visit order shows how well we follow the order in which the answer path visits landmarks.  X  X ide X  shows how success-fully we pass on the correct side of landmarks. Table 2 details the quantitative performance of the different algorithms. Both SARSA and the policy gradient method outperform the baseline, but still fall significantly short of expert performance. The baseline policy performs surprisingly well, espe-cially at selecting the correct side to visit a land-mark.

The disparity between learning approaches and gold standard performance can be attributed to several factors. The language in this corpus is con-versational, frequently ungrammatical, and con-tains troublesome aspects of dialog such as con-versational repairs and repetition. Secondly, our action and feature space are relatively primitive, and don X  X  capture the full range of spatial expres-sion. Path descriptors, such as the difference be-tween around and past are absent, and our feature representation is relatively simple.

The SARSA learning algorithm accrues more reward than the policy gradient algorithm. Like most gradient based optimization methods, policy gradient algorithms oftentimes get stuck in local maxima, and are sensitive to the initial conditions. Furthermore, as the size of the feature vector K in-creases, the space becomes even more difficult to search. There are no guarantees that SARSA has reached the best policy under our feature space, and this is difficult to determine empirically. Thus, some accuracy might be gained by considering different RL algorithms. Examining the feature weights  X  sheds some light on our performance. Figure 5 shows the relative strength of weights for several spatial terms. Re-call that the two main classes of spatial features in  X  are egocentric (what direction we move in) and allocentric (on which side we pass a landmark), combined with each spatial word.

Allocentric terms such as above and below tend to be interpreted as going to the north and south of landmarks, respectively. Interestingly, our sys-tem tends to move in the opposite cardinal direc-tion, i.e. the agent moves south in the egocen-tric frame of reference. This suggests that people use above when we are already above a landmark. South slightly favors passing on the south side of landmarks, and has a heavy tendency to move in a southerly direction. This suggests that south is used more frequently in an egocentric reference frame.

Our system has difficulty learning the meaning of right . Right is often used as a conversational filler, and also for dialog alignment, such as Furthermore, right can be used in both an egocen-tric or allocentric reference frame. Compare which utilizes an allocentric frame, with which uses an egocentric frame of reference. It is difficult to distinguish between these meanings without syntactic features. We presented a reinforcement learning system which learns to interpret natural language direc-tions. Critically, our approach uses no semantic annotation, instead learning directly from human demonstration. It successfully acquires a subset of spatial semantics, using reinforcement learning to derive the correspondence between instruction language and features of paths. While our results are still preliminary, we believe our model repre-sents a significant advance in learning natural lan-guage meaning, drawing its supervision from hu-man demonstration rather than word distributions or hand-labeled semantic tags. Framing language acquisition as apprenticeship learning is a fruitful research direction which has the potential to con-nect the symbolic, linguistic domain to the non-symbolic, sensory aspects of cognition.
 This research was partially supported by the Na-tional Science Foundation via a Graduate Re-search Fellowship to the first author and award IIS-0811974 to the second author and by the Air Force Research Laboratory (AFRL), under prime contract no. FA8750-09-C-0181. Thanks to Michael Levit and Deb Roy for providing digital representations of the maps and a subset of the cor-pus annotated with their spatial representation.
