 Clustering is one of the most popular techniques in the field of data mining [9]. The ba-sic idea of clustering is to partition the data in such a way that the members of a partition are closer to each other and the members of di fferent partitions are far apart. But many real-world applications often suffer from the  X  curse of dimensionality  X  and the mea-sure of nearness becomes meaningless. In su ch scenarios, many feature selection and dimensionality reduction methods have been pr oposed to aid clustering [7]. However, these methods reduce the dimensionality by op timizing a certain criterion function and do not address the problem of data clustering directly. The result of using dimension-ality reduction techniques for clustering high-dimensional data is far from satisfactory and rarely used in such scenarios. Moreover, it is possible that different clusters lie in different subspaces and thus cannot be identifi ed using any dimensionality reduction or feature selection method (see Fig. 1(a)). To deal with such cases, subspace clustering methods such as CLIQUE [4], ENCLUS [6], SUBCLU [10] etc. have been proposed in the literature (see Fig. 1(b)). These met hods attempt to find clusters in different sub-spaces. Searching every possible subspace is a computationally intensive task due to exponentially large search space. Thus, most of these methods use some form of an Apriori-based approach to iden tify the most interesting subspaces. Projected clustering [3,12] is one form of subspace clustering that uses the concept of projection. How-ever, these subspace clustering or projected clustering methods have the following two limitations: 1. They are only able to separate clusters t hat are oriented in axis-parallel manner. 2. They can only find clusters in the sense of locality. In subspace clustering, the clusters are fo rmed in the subspaces ra ther than the full-dimensional space [4,6]. Finding arbitrary-oriented subspace cluster (Fig. 1(c)) involves exponential search space. To efficiently addres s the problem, most correlation based ap-proaches use PCA to avoid searching unw anted regions of the search space. ORCLUS [2], which uses the same idea of axis-parallel PROCLUS [3], is the first PCA based method to find correlation clusters. 4C [5] also uses PCA, however it incorporates a density-based approach and hence, the num ber of clusters need not be pre-specified. Although these methods are able to keep the computational complexity low and do not suffer from a potentially infinite search space, their success in finding correlation clus-ters is highly dependent on the initial choice of the seeds. In addition, they usually do not produce optimal results in the presence of noise. Another problem with these meth-ods is that the dimensionality of the correlation has to be pre-defined which is usually difficult for the end-user from the practical viewpoint. Yip et al. [13] proposed an algo-rithm called HARP [13], which exploits the data to adjust the internal threshold values dynamically at the runtime. However, this me thod faces difficulties in obtaining a low-dimensional cluster [11]. To avoid the problem associated with PCA-based methods such as choosing the initial set of seeds and susceptibility to noise, CASH algorithm [1] uses Hough transform to find correlations. Though this method does not use any ini-tial seeds, its worst case time complexity is exponential, thus making it impractical for high-dimensional data. To solve these issues, we propose a novel PCA based algorithm which eliminates the problem of susceptibility to noise and the need for initial seeds to find correlation clusters. It can also simultaneously find correlation clusters of different dimensionality (see Fig. 1(d)) and the computational complexity is relatively low.
In this paper, we propose RObust SEedless Correlation Clustering (ROSECC) algo-rithm to find the correlation clusters in high-dimensional data. The main advantages of the ROSECC algorithm compared to the state-of-the-art methods proposed in the literature are: 1. It does not require initial seeds for the clustering and hence it is deterministic. 2. It can simultaneously identify correlation clusters with different number of 3. It is robust to handle noise in the data and can obtain the desired correlation clusters The rest of this paper is organized as follows: Section 2 describes the necessary defi-nitions and notations. The proposed algorithm along with its computational complexity is described in Section 3. Section 4 outlines the experimental results on both synthetic and real-world datasets. Finally, Section 5 concludes our discussion. In this section, we will introduce some defi nitions that are needed to comprehend our algorithm. Our method is based on the projection distance of a point onto the principal vectors. Let dp  X  D is a datapoint and v is the principal vector of a member P i of Par-titionset P , then the projection distance of dp and P i is given by the following equation: where dist ( dp, x )= j ( dp j  X  x j ) 2 . dp j and x j corresponds to the j th feature of the data points dp and x respectively. At the beginning of the generation of parti-tions, there will be only one single point x without any principal component. In such cases, the Euclidean distance from the point x will be used as the projection distance. A X  P artitionset  X  (denoted by P i ) is defined as a set of datapoints which have lower projection distance to a particular component compared to the projection distance to any other component. A  X  P artition  X  X sdefinedasasetof P artitionsets .
 Definition 1. (Nearest Component): Let dp  X  D and P = { P i } where P i  X  X  are components, the nearest component of dp from P , denoted by NComp ( dp, P ) ,is defined as the component for which the projection distance from dp is minimum. NComp ( dp, P ) = argmin  X  X inimum projection X , denoted by MProj ( dp, P )= min Definition 2. (Farthest Candidate): Let P = { P i } where P i is a Partitionset. The far-thest candidate of P , denoted by FCand ( P ) , is defined as the data point for which the minimum projection distance is maximum. FCand ( P ) = argmax Theorem 1. Let a new component consisting of a single element { dp } is added to the component list P to become P i.e.if P = P  X  X  dp } .If MProj ( dp, P ) &gt;dist ( dp ,dp ) then MProj ( dp, P )= dist ( dp ,dp ) Proof From the definition of the projection distance, we have PDist ( dp, { dp } )= dist ( dp ,dp ) . Therefore, MProj ( dp, P ) &gt;dist ( dp ,dp )  X  MProj ( dp, P ) &gt;PDist ( dp, { dp } ) . Also, from the definition of the Minimum Projection, we get,  X  PDist ( dp, { dp } )  X  X  X  Hence, from the definition of the projection distance, we get MProj ( dp, P )= PDist ( dp, { dp } )= dist ( dp ,dp ) .
 This theorem helps in reducing some of the unnecessary computations during the par-tiotionset assignment step after obtaining a farthest candidate and the corresponding new component of a single datapoint. We can use the minimum projection value di-rectly from the last iteration and compare it to the Euclidean distance from the farthest candidate. This is an important step that saves a lot of computation time.
 Definition 3. (Dimensionality of a cluster): Let P = { P i } where P i is a Partition-set and  X  1 , X  2 ... X  m be the eigen values of the Eigen decomposition of that partition set in descending order. The dimensionality of the cluster is defined as Dim ( P i )= Min k ( k 1  X  i &gt; X  ) where  X  is a threshold.
 When  X  =0 . 9 , 90% of the original variance of the data is preserved and the corre-sponding Partitionset is obtained as a cluste r. The dimensionality of the cluster is the number of eigenvalues that cover most of the variance in the data of the Partitionset. Definition 4. (Acceptable Partitionset): A Partitionset P i is said to be an acceptable Partitionset, if | P i | X  n 0 and 1 | P For a Partitionset to be acceptable, it must satisfy the following two conditions: 1. There must be sufficient number of data points ( n 0 ) in the Partitionset. 2. The points in the correlation cluster must be closer ( &lt; ) to its principal component. In this section, we will describe the details of the proposed ROSECC algorithm and also analyze the effect of different parameter va lues in the algorithm. The overall procedure is shown in Algorithm 1. 3.1 Algorithm Description The five different steps in the ROSECC algorithm are described below.
 Step 1: Finding k 0 principal components: For a given set of data points ( Data ), this step will find k 0 number of components in the data by iteratively generating a sufficient number of partitions and their corresponding pr incipal components. The details of this step are described in Algorith m 2. Initially, the principal components are computed with the entire dataset. Then, the F arthest Candidate from the first principal component is taken and the dataset is partitioned into two different subsets depending on the distance of the data point to the previous principal component vector and the new seedpoint. PCA is independently applied on these two subs ets, thus generating the two corresponding principal component vectors. The membership is updated and this process is repeated until k 0 number of components are obtained (see Fig. 2(a)). At this point, one might encounter any of these following scenarios: 1. Some principal components are obtained for data points that are not correlated at 2. There might be some components that contain more than 1-dimensional correlation. 3. More than one component and corresponding points might be obtained for the same In the next three steps, these three situations are handled.
 Step 2: Removing non-informative principal components: The components that may not represent a correlation cluster are removed in this step. A component is accepted for further consideration only if it satisfies the definition of acceptability given in the previous section. When the unacceptable c omponents are removed, the corresponding set of points become members of the outlier set ( O ), which can then become a member of any other cluster in the later stages (see Fig. 2(b)).
 Algorithm 1. ROSECC ( Data , k 0 , k 1 , n 0 ) Step 3: Finding the dimensionality of the correlation: Though Step 1 successfully produces the set of components that possibly represent some correlation, identifying the dimensionality of the correlation is one of the key aspects of the algorithm. Prac-tically, the correlation cluster lies in a lo w-dimensional space. Hence, we can assume that a high-dimensional correlation in the da ta will contain a one dimensional correla-tion with at least a fewer number of those datapoints. Correlation dimensionality of an acceptable component P (denoted by Dim ( P ) ) is the number of eigenvectors needed for preserving the desired amount of variance.
 Step 4: Adding points to the correlation cluster: This step assumes that there are some acceptable components and a set of ou tliers obtained from step 1. It will also consider each outlier and tries to assign them to any one of the current components. If a datapoint cannot be assigned to any of the correlation clusters, then it remains to be an outlier. A data point dp in the outlier set O is includable to P i ,if PDist ( dp, P i ) is less than a threshold  X  (see Fig. 2(c)). At this point, the mutual exclusiveness of the component does not hold and hence the algorithm will produce overlapping clusters. Step 5: Merging the correlation cluster: In this step, each component is checked for any possibility of merging with another component. Two components will be merged if Algorithm 2. Generate Components ( Data , k 0 , k 1 ) the coefficient of their representative vectors does not differ significantly. i.e. two com-ponents P i and P j are merged, if | c il  X  c jl | &lt; X  ,  X  l (see Fig. 2(d)). The time complexity of our algorithm is O( k 3 0 + Nd 2 ), which is typically dominated by the Nd 2 term for large-scale and/or high-dimensional data. This time complexity of the ROSECC algorithm is competitive to the other state-of-the-art techniques. 3.2 Tuning of the Parameters The parameter k 0 is the number of components that are to be generated in the first step of the algorithm. This value should be at least twice the number of correlation clusters present in the data. The second parameter k 1 is the number of iterations that the algorithm runs for convergence with the new partition. From our experiments, we observed that very few iterations (fewer than 6) are necessary for convergence. The third parameter n 0 is required to check the acceptability of a component and should be atleast 1% of the number of data points. In Table 1, we show the effect of different parameter values for a synthetic dataset (DS1). Since we know the ground truth, we presented the accuracy as the percentage of co rrectly clustered datapoints. We can see that k 1 is good with value as low as 5, while the value of n 0 can be between 1% and 3%. Results are not optimal when k 0 =5 ,but k 0 =15 or more gives an accurate result. Hence, it is important to generate more number of components in the first step of the algorithm. 4.1 Synthetic Datasets Our algorithm was tested successfully on various synthetic datasets that were gener-ated to see the different aspects of the RO SECC algorithm. We discuss two different synthetic datasets and explain the data gene ration along with the corresponding results. (1) DS1 : In this dataset, two one-dimensional correlations containing 200 data points generated from the origin with a slope of 1 and -1. Another correlation cluster (with 100 data points) is generated from the origin with a slope of 1.4. 100 random noise points are also added. The ROSECC algorithm found all the correlation clusters, even in the presence of sufficient noise an d significant overlapping of the clusters (see Fig. 2(d)). Thus it shows that our algorithm is robust and only starts to break when the noise is greater than 20% in the data (see Table 2). (2) DS2 : In this dataset, one two-dimensional correlation cluster (circular shape) with 300 data points is generated. Two 1-dimensional correlation clusters that start from the edge of that cluster in two different directions, one with 150 datapoints and the other with 100 datapoints is added to the first cluster (see Fig. 3). This dataset is created to test the ability of the ROSECC algorithm to simultaneously identify the correlation clusters with different dimensionality which is one of key challenges in the correlation clustering problem.

We compared the result of the ROSECC algorithm with the well-studied DBSCAN [8] and 4C [5] algorithms. In this dataset, all the datapoints are density connected (see Fig. 3). There are two 1-dimensional and one 2-dimensional correlation cluster in this dataset. As shown in the result, DBSCAN recognizes all the data points as a single cluster (Fig. 3(a)). Algorithm 4C recognizes either 1-dimensional clusters (Fig. 3(b)) or the 2-dimensional cluster (Fig. 3(c)) depending on the choice of the parameter corre-sponding to the number of dimensionality. O n the other hand, the ROSECC algorithm was able to successfully identify all the three clusters simultaneously in a single run (Fig. 3(d)). 4.2 Real-World Datasets We also show the performance of the ROSECC algorithm in finding correlation clusters in the following three real-world datasets. (1) Breast Cancer dataset: 1 This dataset measures nine biomedical parameters char-acterizing breast cancer type in 683 humans. The algorithm found one 3-dimensional cluster and two 2-dimensional clusters. When the result is projected in a 3-D space using PCA, we found that the 3-dimensional correlation cluster is a pure cluster of be-nign cancer (see Fig. 4) and no malignant cancerous patient belongs to any correlation cluster and are considered to be outliers by our algorithm. (2) Wages dataset : The wages dataset contains the statistics of the determinants of Wages from the 1985 Current Population Survey. It contains 534 observations on 11 features sampled from the original Current Population Survey of 1985 and can be downloaded from StatLib Data archive 2 .ROSECC gives one 2-dimensional correlation (see Fig. 5) which basically gives YE(Years of Education) + WE(Years of work experience) + 6 = Age. It also gives four 1-dimensional correlation clusters; (i) Age = WE + 18, YE=12 (ii) Age = WE + 20, YE=16 (iii) Age = WE + 24, YE=18 (iv) Age = WE + 24, YE=14.

However, all of the data points of these four clusters are also the members of the 2-dimensional clusters, which suggests that the se four 1-dimensional correlations are ac-tually on the 2-dimensional plane of the 2-dimensional cluster. This is an interesting example because there are quite a few data poi nts in the 1-dimensi onal clusters and our algorithm identifies those as s eparate clusters as well. (3) Glass Identification dataset : The glass identification dataset 3 consists of 214 sam-ples and 9 attributes measuring the refractive index and weight percentage of different metals in the corresponding oxide in the glass. We ran our algorithm to find the clusters in this dataset and the wdbc dataset. We measure the performance of the ROSECC al-gorithm by using the class label as ground truth and calculating the F1 measure. Table 3 shows the results of the ROSECC algorithm in both the datasets are better than the other state-of-the-art methods.
 We proposed ROSECC, a novel PCA based robust seedless correlation clustering algo-rithm. ROSECC does not require the initial set of seeds for clustering and is robust to noise in the data compared to the other PCA based approaches which typically require initial seeds and a pre-defined dimensionality parameter. It incrementally partitions the data space and eventually finds the correlation clusters in any arbitrary subspace even in the presence of overlapping clusters. Using several synthetic and real-world datasets, we demonstrated the advantages of the ROSECC algorithm compared to the other methods available in the literature for identifying correlation clusters.

