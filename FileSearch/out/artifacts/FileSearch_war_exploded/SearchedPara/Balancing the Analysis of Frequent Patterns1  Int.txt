 For twenty years, the pattern mining algorithms have gained performance and now arrive to quickly extract patterns from large amounts of data. However, evaluate and ensure the quality of extracted patterns remains a very open issue. In general, a pattern is considered to be relevant if it deviates from what was expected from a knowledge model. In the literature, there are two broad cate-gories of knowledge models [1,2,3]: user-driven and data-driven ones. User-driven approaches discover interesting pattern s with subjective models based on user oriented information, such as domain knowledge, beliefs or preferences. Data-driven approaches discover interesting pa tterns with objective models based on the statistical properties applied to data, such as frequency of patterns. Most often these methods neglect how the user will analyze the collection of patterns. In this paper, we present a novel approach, named analysis-driven , to evaluate discovered patterns by foreseeing ho w the collection will be analyzed.
Before presenting in depth our motivations, we first recall the context of fre-quent itemset mining [4]. Let I be a set of distinct literals called items ,anitem-set corresponds to a non-null subset of I . A transactional dataset is a multi-set of itemsets, named transactions . Table 1 (a) presents such a dataset D where 4 transactions t 1 ,...,t 4 are described by 4 items A,...,D .The support of an itemset X in a dataset D is the fraction of transactions containing X .Anitemset is frequent if it appears in a database with support no less than a user specified threshold. For instance, Table 1 (b) shows the frequent itemsets of D (with 0 . 25 as minimal support) that are ranked according to the support.

In a post-analysis process based on the scoring of patterns, we assume that an analyst examines each pattern with a diligence proportional to an interest-ingness measure (here, the support) for a nalyzing the dataset (we will justify our proposal of analysis model in Section 3). Hence, it is not difficult to see that the attention paid to A is twice that for AB since their support is respectively 0.5 and 0.25. We also assume that the analysis proportion of each transaction is proportional to the sum of the time spent on each pattern covering it. The transaction t 1 covered by A , B and AB is then analyzed for a length of 1.25 (= 0 . 5+0 . 5+0 . 25). Therefore, according to this observation, the individual analysis of t 1 , t 2 ,or t 3 in the analysis is 5 times greater than that of t 4 as shown by the third column of Table 1 (c). Assuming that the user considers all trans-actions equally interesting (see the preference vector  X  in the fourth colmun), we say that such an analysis is unbalanced . It means that some transactions are understudied while others are overstudied! For us, a good way to balance the analysis is to increase the score (here the support) of the pattern D .Itsscore increase will also increase the analysis proportion of t 4 and decrease that of the other three transactions. More interestingly, we think that D is peculiar and deserves a higher valuation because it describes a singular transaction that is described by no other pattern.

In this work, we seek to balance the analysis of the dataset by proposing a new interestingness measure to rate the patte rns. More precisely, a preference vector  X  : D X  (0 , 1] indicates the intensity of his/her interest for each transaction such that t  X  X   X  ( t ) = 1. Indeed, the user does not always devote the same interest in all transactions as in our example above. Sometimes he/she prefers to focus on rare data as it is often the case in fraud detection or with biomedical data. An analysis is balanced wh en each transaction t is studied with an acuity  X  ( t, M ) corresponding to that specified by a user-preference vector  X  ( t ):  X  ( t, M )=  X  ( t )where  X  ( t, M ) is the analysis proportion of the transaction t given an analysis model M that simulates analysis sessions conducted to understand the data. To the best of our knowledge, we propose the first model, called scoring analysis model , to simulate the sessions of analyzing pattern sets according to an interestingness measure. Its strength is to rely on a stochastic model successfully used in Information Retrieval [5,6] while integrating the preferences given by the user. As main contribution, we then introduce the balanced support that induces balanced analysis under our model. This measure removes the equalized axiom of support saying that every transaction has the same weight in its calculation. It gives a higher weight to the most singular transactions in the calculation of the balanced support. For instance, in Table 1, the transaction t 4 will be weighted 5 times more than others so th at it receives the same attention as other transactions. We also develop an algorithm, Absolute , to compute the balanced support. Our exper iments show its effectiveness for balancing frequent itemsets and compare the balanced support with the traditional support.
The rest of this paper is organized as follows. Section 2 introduces the basic notations. In Section 3, we introduce the scoring analysis model allowing us to simulate the behavior of an analyst. Under this model, we propose in Section 4 the balanced support and the algorithm Absolute to compute it. Section 5 presents experiments demonstrating its e fficiency and the interest of the balanced support. Section 6 reviews some rel ated work. We conclude in Section 7. For the sake of clarity, we illustrate our definitions with the notion of itemsets but, our problem is not limited to a particular type of pattern. We consider a language L and a dataset D that is a multiset of L (or another language). A specialization relation is a partial order relation on L [7]. Given a specialization relation on L , l l means that l is more general than l ,and l is more specific than l . For instance, A is more general than AB w.r.t  X  .

Given two posets ( L 1 , 1 )and( L 2 , 2 ), a binary relation  X   X  X  1  X L 2 is a (resp. l 2 2 l 2 ). The relation l 1  X l 2 means that l 1 covers l 2 ,and l 2 is covered by l 1 . The cover relation is useful to relate different languages together (e.g., for linking patterns to data). Note that a specialization relation on L is also a cover relation on L X L . For instance, the set inclusion is used for determinating which patterns of P cover a transaction of D . Given two pattern sets L  X  X  , L  X  X  and a cover relation  X   X  X  X L ,the covered patterns of L by l  X  L is the set of patterns of L covered by the pattern l : L l = { l  X  L | l X l } . Dually, the covering patterns of L for l  X  L is the set of patterns of L covering the pattern l : L l = { l  X  L | l X l } . With Table 1, we obtain that D  X  A = { t 1 ,t 2 } and P
Pattern discovery takes advantage of interestingness measures to evaluate the relevancy of a pattern. The support of a pattern  X  in the dataset D can be considered as the proportio n of transactions covered by  X  [4]: Supp (  X , D )= |D specified minimal threshold. For instance, with a minimal threshold 0 . 25, the function f : L X  R is extended to any pattern set P  X  X  by considering f ( P )=  X   X  P f (  X  ). For instance, Supp ( P, with P = { A, B, C, D, AB, AC, BC } in Table 1. 3.1 Scoring Analysis Model In this section, we propose the scoring analysis model to simulate an analyst faced with a set of scored patterns. This model generates sessions by randomly picking patterns taking into account the scoring of patterns. More precisely, the  X  X imulated analyst X  randomly draws a pattern by favoring those with the highest measure, and then studies each transa ction covered by this pattern during a constant period weighted by its prefe rence vector. Indeed, it is important to benefit from these preferences for better approximating the user behavior. After each pattern analysis, the session can be interrupted (if the analyst is satisfied, no longer has time to pursue, etc.) or con tinued (if the analyst is dissatisfied, wants more information, etc). This interruption of the session of analysis can be modeled by a halting probability. We now formalize this model: Definition 1 (Scoring analysis model). Let D be a dataset, P  X  X  a pattern set, m : L X  [0 , 1] an interestingness measure and  X  a preference vector.
The scoring analysis model with a halting probability  X   X  (0 , 1) andaunit length  X &gt; 0 , denoted by S  X , X , X  , generates sessions with the following process: 1. Pick (with replacement) a pattern  X  of P with probability distribution p (  X  )= 2. Study each transaction t  X  X  covered by  X  during a length  X   X   X  ( t ) . 3. Stop the session with probability  X  or then, continue at Step 1.

Basically, Step 1 favors the analysis of patterns having the highest measure (with replacement because the end-user can re-analyze a pattern in the light of another). Step 2 takes into account the user-preferences for the analysis of transactions. Simulating a data expert by randomly picking patterns may seem strange and unrealistic at first. However, this mechanism has been successfully used in other high-level tasks such as web browsing [5] and text analysis [6]. We think that the strength of our stochastic model is to describe the average behavior of users. By analogy with the random surfer model, each pattern would be a web page. The web pages would then be completely interconnected where each link is weighted by the support of the destination page. In this context, the probability  X  would correspond to the probability of interrupting navigation. 3.2 Analysis Proportion of a Transaction under S  X , X , X  Starting from the scoring analysis model, we desire to derive the analysis pro-portion of each transaction.
 Theorem 1 (Analysis proportion). The analysis proportion  X  ( t, S  X , X , X  ) of the transaction t is:
Theorem 1 (proofs are omitted due to lack of space) means that the analysis proportion of a pattern is independent of the parameters  X  and  X  . Therefore, in the following,  X  ( t, S  X , X , X  ) is simply denoted  X  ( t, S  X  ). Let us consider the analysis proportion of each transaction of Table 1 in light of Theorem 1 us-ing a uniform preference vector. As the itemsets A , B and AB cover t 1 ,we obtain that Supp ( P t 1 , D )=0 . 5+0 . 5+0 . 25=1 . 25. The same result is ob-tained for t 2 and t 3 and similarly, Supp ( P t 4 , D )=0 . 25. Finally,  X  ( t 1 , S  X  )= and  X  ( t 4 , S  X  )=0 . 25 / 4=1 / 16 = 0 . 0625. It means that under the scoring anal-ysis model, the transaction t 4 will be less analyzed than the transactions t 1 , t 2 or t 3 asindicatedinTable1(c). 3.3 Balanced Analysis under S  X , X , X  We now deduce what a balanced analysis with respect to  X  is under the scoring analysis model: Property 1 (Balanced analysis). The analysis of D by the pattern set P with m is balanced with respect to  X  under the scoring analysis model iff for any transaction t  X  X  , the following relations holds:
The crucial observation highlighted by Property 1 is that the balance of an analysis is independent of the preference vector specified by the user, under the scoring analysis model. Indeed, the preference vector  X  involved in the right side of the equation  X  ( t, M )=  X  ( t ) is canceled by the one appearing in the analysis proportion (see Theorem 1). Consequently, if the analysis of a dataset D by a pattern set P with a measure m is balanced with respect to a given preference vector  X  , then it is also balanced with respect to any other preference vector. However, note that the analysis length of a transaction will take into account the considered preference vector.
 Let us compute whether the analysis of the dataset D by the pattern set P with the measure Supp (see Table 1) is balanced under the model S  X  using Property 1. First, the transaction t 1 is too much studied because Supp ( P t 1 , D )= Supp ( { A, B, AB } , D )=1 . 25 and 1 / |D|  X  t  X  X  Supp ( P t , D )=1 / 4  X  (3  X  transaction t 4 is not studied enough. In Sectio n 5, we observe that the use of frequent patterns with the support for the analysis of datasets coming from the UCI repository always leads to an unbalanced analysis. 4.1 Axiomatization of Support Under the scoring analysis model, we aim at balancing the analysis of the dataset by proposing a new interestingness measure that satisfies the equation of Prop-erty 1. At this stage, the right question is  X  X hat characteristics should satisfy this measure? X  Unfortunately we found that the support does not lead to balanced analysis. However, this extremely popular measure is both intuitive for experts and useful in many applications. Moreover, it is an essential atomic element to build many other interestingness measures. For all these reasons, we desire a measure that leads to balanced analysis while maintaining the fundamental properties of the support. To achieve this goal, we first dissect the support by means of its axiomatization (we only focus on the support measure and not on the frequent itemset mining as proposed in [8]).
 Property 2 (Support axioms). The support is the only interestingness measure m that simultaneously satisfies the three below axioms for any dataset D : 1. Normalized: If a pattern  X  covers no transaction (resp. all transactions), 2. Cumulative: If patterns  X  1 and  X  2 cover respectively the set of transactions 3. Equalized: If two patterns cover the same number of transactions, then
Clearly the first axiom does not constitute the keystone of support, since similar normalizations are widely used by other measures (e.g., confidence or J-measure). Furthermore, it has no impact on the fact that an analysis is balanced or not, since Step 1 of scoring analysis model performs another normalization. Conversely, we believe that the other tw o axioms (not verified by other measures) are the main characteristics of the support. If we do not find reason to reconsider the cumulative axiom, we think the third is not fair. Ideally, an interestingness measure should favor the patterns covering the least covered transactions as explained in the introduction. Thus, the value of a measure should not only depend on the number of transactions covered but also on the singularity of these transactions. To this end, we propose to retain the first two axioms and to substitute the equalized axiom by the axiom of balance: a measure of interest must lead to the balanced analysis of the dataset by the pattern set . 4.2 Balanced Support We first introduce a relaxation of the support by removing the constraint due to the equalized axiom: Definition 2 (Weighted support). Given a function w : D X  + ,the weighted support of a pattern  X  in the dataset D is defined as: Supp w (  X , D )=
It is not difficult to see that the weighted support satisfies the normalized axiom and the cumulative axiom. Now it only remains to choose the right vec-tor w to get a balanced analysis. A naive idea would be to use the preference vector  X  to weight the support. That does not work in the general case: Supp  X  u (where  X  u : t  X  1 / |D| ) corresponds exactly to the support Supp whichleadto an unbalanced analysis as shown by our running example (see Section 3.3) or observed in experimental study (see Section 5). In fact, to find the right weights, it is necessary to solve the equation of Property 1 by using the definition of the weighted support. Then, the weighted support induced by these weights defines the optimal balanced support : Definition 3 (Optimal balanced support). If it exists, the optimal balanced support of a pattern  X   X  P in the dataset D with the pattern set P , denoted by BS equation for all transactions t  X  X  : Interestingly, this definition underlines that the whole set of mined patterns P is necessary to compute the optimal balanced support of any individual pattern. Let us illustrate the above equation with the example given by Ta-ble 1. With the weight w bal where t 1 ,t 2 ,t 3  X  1 / 8et t 4  X  5 / 8, we obtain Supp w bal ( A, D )= Supp w bal ( B, D )=1 / 8+1 / 8=2 / 8; Supp w bal ( AB, D )=1 / 8 and Supp w bal ( D, D )=5 / 8. Then, we can check that the equation of Definition 3 (similar for t 2 and t 3 )and Supp w bal ( P t 4 , D )= Supp w bal ( { D } , D )=5 / 8. In other words, Supp w bal corresponds exactly to the optimal balanced support BS Theorem 2. The optimal balanced support (if it exists) is the single interest-ingness measure that satisfies the normalized and cumulative axioms, and that leads to a balanced analysis.

Theorem 2 achieves our main goal as stated in introduction. However, the equation of Definition 3 can admit no solution and then the optimal balanced support is not defined. For instance, it is impossible to adjust the weighted support for balancing the analysis of D = { A, B, AB } by P = { A, B, AB } . Indeed, whatever the weighted support, the transaction AB is still more analyzed than the other two since it is covered by all patterns. So, the next section proposes an algorithm to approximate the optimal balanced support by minimizing the deviation between Supp w ( P t , D )and t  X  X  Supp w ( P t , D ) / |D| . 4.3 Approximating the Balanced Support Absolute (for an anagram of bal ance dsu ppo rt ) returns the weights w such that the analysis S  X , X , X  ( D ,P,Supp w ) is balanced as better as possible. Its in-put parameters consist in a pattern set P ,adataset D and a threshold .The latter is the maximal differ ence expected between two weight vectors stemming from consecutive iterations before terminating the algorithm. The weights out-putted by Absolute enable us to define the (approximated) balanced support BS (  X , D ,P ).
 Algorithm 1. Absolute
Note that in Algorithm 1, w i are symbol tables where the keys are transac-tions. Lines 1-3 initialize all the weigths with 1 / |D| . The main loop (Lines 5-17) adjusts the weigths until the sum of differences between w i +1 and w i is less than . More precisely, Lines 7-10 correct the weight of each transaction. Using Definition 3, Line 8 computes the new weight w i +1 [ t ] by multiplying the pre-vious weight w i [ t ] by the ratio between the average coverage (i.e., a constant 1 / |D|  X  (i.e., Supp w i ( P t , D )). For instance, if the coverage of t is below the average cov-erage, the ratio is above 1 and the new weig ht is stronger. Thus, it increases the support of all the patterns covering this transaction. This operation therefore operates a local balance for each transaction. Nevertheless, there is also a global modification since a normalization is performed on these weights at Line 13 (where W is computed Line 9). Line 14 updates diff (initialized Line 11) ac-cumulating the difference between w i +1 and w i for all the transactions. Finally, Line 18 returns the last weights that correspond to a balanced analysis. This section evaluates the effectiveness o f the algorithm for balancing the analy-sis and to compare the quality of the balanced support with respect to the usual one. All experiments reported below were conducted with a difference thresh-old =10  X  5 on datasets coming from the UCI Machine Learning Repository ( www.ics.uci.edu/ ~ mlearn/MLRepository.html ). Given a minimal support thresh-old minsupp , we select all the frequent itemsets for P . Increasing the weight of singular transactions does not cause the extraction of random noise patterns because the final patterns are selected fr om the collection of frequent patterns. For simplicity, we use the uniform preference vector  X  u : t  X  1 / |D| for  X  . Efficiency of Absolute Table 2 (columns 2-3) presents the number of patterns and the number of iterations required by Absolute for balancing all the fre-quent patterns. Note that we do not provide running times because they are very low. Indeed, the worst case is the balancing time for all the frequent patterns on zoo , but it does not exceed 16 seconds perf ormed on a 2.5 GHz Xeon processor with the Linux operating system and 2 GB of RAM memory ( Absolute is im-plemented in C++). Table 2 shows that the number of iterations varies between 13 and 52. No simple relationship was found between the number of iterations and the features of datasets.

Table 2 also reports the Kullback-Leibler divergence for support and BS (columns 4-6). Let us recall that Kullback-Leibler divergence defined by D bility distributions P and Q [9]. For any transaction t ,wefix P ( t )=  X  u ( t )as reference and Q ( t )=  X  ( t, M  X  u ) as model. Table 2 shows that Absolute reaches its goal since the Kullback-Leibler divergence is always significantly reduced by benefiting from the balanced support. This divergence is at least divided by 5 and it is even divided by more than 10 in 13 datasets. The average gain is 17.56 for frequent itemsets. Similar experiments conducted on collections of free and closed itemsets [10] gave respectively an average gain of 12.36 and 11.94. Effectiveness of Balanced Support. We desire to quantify the number of non-correlated patterns (i.e., the number of extracted patterns that are spurious) with a usual/balanced support. Unfortunately, the pattern discovery process is unsupervised and the (ir)relevant patterns are unknown. We tackle this issue by using an experimental protocol inspired by [11]. The idea is to make the assumption that a pattern is non-correlated if this pattern is also extracted (by the same method) in a random dataset D  X  having the same characteristics as D (i.e., the same dimensions and the same support for each item).

Figure 1 depicts the ratio of non-correlated patterns (averaged from 10 ran-dom datasets D  X  )for abalone and anneal for frequent itemsets with a minimal usual/balanced support varying between 0 and 0 . 5. This ratio is the number of non-correlated patterns divided by the total number of patterns. For the bal-anced support, we use three collections of frequent patterns P obtained with minsupp =0 . 01 / 0 . 05 / 0 . 10 independently of the second threshold applied to balanced support. Given a minimal threshold (see horizontal axis), the ratio of non-correlated patterns for Supp is always higher than that of BS and most of times, with a significant difference. Interestingly, the change of minsupp for the collection of patterns has a marginal impact on the ratio of non-correlated patterns. Recall that balanced support only differs from the traditional one by replacing the equalized axiom by the axiom of balance (see Section 4.1). So it is this axiom that enables our measure to keep out uncorrelated patterns. More generally, this experience justifies the interest of a balanced analysis and even the usefulness of the scoring analysis model for simulating an analysis. As mentioned in the introduction, many interestingness measures have been proposed for evaluating the pattern interest as alternative to the support [2,12,3]. They can be categorized into two sets [1] : user-driven measures and data-driven ones. Among the data-driven approaches, the statistical models are often based on the null hypothesis. A pattern is interesting if it covers more transactions than what was expected. Some models sim ply require the frequency of items forming the itemset [12], others rely on its subsets [13,14] or even, patterns already extracted [15]. These methods consider that all transactions have the same weight. However, in practice, th e user tends to attach more importance to information that describes the least common facts. Thus, the most singular transactions should have an important weight in the evaluation of patterns that describe such transactions. In this sense, this paper proposes another alternative resting on the integration of the analysis method into the metric. To the best of our knowledge, this way has not yet been explored in the literature. A major and original consequence of our approach lies in the fact that each transaction contributes with a different weight in t he balanced support and this weight depends on the entire extracted collection.

However, the problem of unbalance induced by a pattern set is indirectly ad-dressed by several approaches removing the patterns that describe transactions covered by other patterns. For instance, the condensed representations [10] which remove redundant patterns, often decrease the unbalanced of the analysis. But, empirical experiments have shown that the unbalance remains important (see Section 5). In the same way, global models based on patterns [16,17,18] favor balanced analyses of the dataset. Indeed, one goal of these approaches is to de-scribe all the data by choosing the smallest set of patterns. The overlap between the coverings of the different patterns is v ery reduced (ideally each transaction should be described by a unique pattern as it is the case with a decision tree). Un-fortunately, relevant patterns may be removed from such models. Our approach balances the analysis of the dataset by preserving the whole set of patterns to avoid losing information.

Rather than modifying the collection of mined patterns, it would be possible to modify the initial dataset in order to satisfy user preferences. Sampling meth-ods [19,20] are widely used in machine learning and data mining in particular to correct a problem of unbalance betw een classes. There is no reason that the change of the dataset with a usual sampling method leads to a balanced analysis. We think that our approach is complementary to those of sampling. In this paper, we introduce the scoring analysis model for simulating analysis sessions of a dataset by means of a pattern set. Under this model, we define the balanced support that induces a balanced analysis of the dataset for any user-specified preference vector. We propose the algorithm Absolute to iteratively calculate transaction weights leading to the balanced support. This new inter-estingness measure strongly balances the analysis and in parallel, it enables us to filter-out non-correlated patterns. The originality of our work is to show that the integration of the analysis method to drive the data mining is profitable.
In future work, we are interested in examining our approach on real-world data for better understanding the semantic of the balanced support: what are the patterns which balanced support is much higher than traditional support? What are domains and datasets where the balanced support is most appropri-ate? Dually, we must also study the properties of the weights resulting from Absolute that could be interesting to identify the outliers. Furthermore, the prospects of using the scoring analysis model are manifold. For instance, this model could be used to balance other measures of interest like the confidence.
