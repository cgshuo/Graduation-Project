 The need for mining structured data has increased in the past few years. One of the best studied data structures in computer science and discrete mathematics are graphs. It can therefore be no surprise that graph based data mining has become quite popular in the last few years.
 This article introduces the theoretical basis of graph based data mining and surveys the state of the art of graph-based data mining. Brief descriptions of some represen tativ e ap-proac hes are provided as well.
 Graph-based Data Mining graph, tree, path, structured data, data mining During the past decade, the  X eld of data mining has emerged as a novel  X eld of researc h, investigating interesting researc h issues and developing challenging real-life applications. The objectiv e data formats in the beginning of the  X eld were limited to relational tables and transactions where each in-stance is represen ted by one row in a table or one transac-tion represen ted as a set. However, the studies within the last several years began to extend the classes of considered data to semi-structured data such as HTML and XML texts, symbolic sequences, ordered trees and relations represen ted by advanced logics. Many papers on data mining of semi-structured data have been presen ted in major international journals and conferences [19]. The researc h on data mining and machine learning of symbolic sequences also became ac-tive in the last several years. Many papers appeared in ILP, ALT and DS conferences [17]. Furthermore, in the last few years, many researc hers started working on mining ordered tree structures [26]. One of the most recen t researc h topics associated with structured data is multi-relational data min-ing whose main scope is to  X nd patterns in expressiv e logical and relational languages from complex, multi-relational and structured data [1]. The main aim of mining semi-structured data, symbolic sequences and ordered trees is to extract pat-terns from structured data. Within this framew ork, the pat-terns mined are characterized by some measures such as fre-quency and information entropy are mined. The classes of the patterns handled in the multi-relational data mining are more expressiv e than the aforemen tioned data structures. Recen tly, a novel  X eld of data mining emerged from a topo-logical view of the data structure. In mathematics, one of the most generic topological structures are graphs. Semi-structure represen ted by text tags, symbolic sequence and tree including ordered and unordered trees are subclasses of general graphs. The earliest studies to  X nd subgraph pat-terns characterized by some measures from massiv e graph data were conducted by Cook and Holder (S UBDUE ) [3] and Yoshida and Moto da (GBI) [25] in the middle of the 1990's. Their approac hes used greedy searc h to avoid high com-plexit y of the graph isomorphism problem, which resulted in an incomplete set of characteristic subgraphs. In 1998, Dehasp e and Toivonen prop osed an ILP-based algorithm, WARMR, enabling a complete searc h for frequen t subgraphs from graph data [6]. Subsequen t work done by Nijssen and Kok prop osed a faster algorithm, FARMER [21]. In 2000, Inokuc hi et al. prop osed an approac h called AGM to com-bine Apriori algorithm and mathematical graph theory [11]. In 2001, De Raedt and Kramer prop osed the version space based approac h called MolF ea to  X nd characteristic paths from the graph data [4].
 Based on these pioneering studies, the number of papers on graph mining is rapidly increasing. The total number of papers related to graph and tree mining in SIGMOD, SIGKDD, IJCAI/AAAI, ICML, ECML/PKDD and IEEE ICDM was 10 in 2001, whereas the number increased to 18 in 2002 in our coun t. In addition, a considerable number of papers on this topic appeared in other international con-ferences and workshops. Thus, the researc h  X eld of graph mining just started to emerge. Since graph topology is one of the most fundamen tal structures studied in mathemat-ics, and has a strong relation with logical languages, graph mining is expected to contribute to the developmen t of new principles in data mining and machine learning. Further-more, graph mining has a high potential to provide practical applications because the graph structured data widely oc-curs in various practical  X elds including biology , chemistry , material science and comm unication networking.
 Graph mining has a strong relation with the aforemen tioned multi-relational data mining. However, the main objectiv e of graph mining is to provide new principles and e X cien t algorithms to mine topological substructures embedded in graph data, while the main objectiv e of multi-relational data mining is to provide principles to mine and/or learn the re-lational patterns represen ted by the expressiv e logical lan-guages. The former is more geometry orien ted and the latter more logic and relation orien ted.
 In this review article, the theoretical basis of graph-based data mining is explained in the following section. Second, the approac hes to graph-based data mining are review ed, and some represen tativ e approac hes are brie X  X  describ ed. The theoretical basis of graph-based data mining is not lim-ited to one principle although the history of this researc h  X eld is still young. This is because researc h on graphs has a long history in mathematics. In this section, the  X ve theoretical bases of graph-based data mining approac hes are review ed. They are subgraph categories, subgraph iso-morphism, graph invarian ts, mining measures and solution metho ds. The subgraphs are categorized into various classes, and the approac hes of graph-based data mining strongly de-pend on the targeted class. Subgraph isomorphism is the mathematical basis of substructure matc hing and/or coun t-ing in graph-based data mining. Graph invarian ts provide an importan t mathematical criterion to e X cien tly reduce the searc h space of the targeted graph structures in some approac hes. Furthermore, the mining measures de X ne the characteristics of the patterns to be mined similarly to con-ventional data mining. In this paper, the theoretical basis is explained for only undirected graphs without labels but with/without cyclic edges and parallel edges due to space limitations. But, an almost identical discussion applies to directed graphs and/or labeled graphs. Most of the searc h algorithms used in graph-based data mining come from arti- X cial intelligence, but some extra searc h algorithms founded in mathematics are also used. Various classes of substructures are targeted in graph-based data mining. This is because the graph is one of the most generic data structures and includes characteristic substruc-tures in various views.
 Mathematically , a graph G is represen ted as G ( V; E; f ) where V is a set of vertices, E a set of edges connecting some vertex pairs in V , f a mapping f : E ! V  X  V . In the graph of Fig. 1 (a), V = f v 1 ; v 2 ; v 3 ; v 4 ; v E = f e 1 ; e 2 ; e 3 ; e 4 ; e 5 ; e 6 ; e 7 ; e 8 ; e 9 a relation represen ted as f ( e h ) = ( v i ; v j ) in which v v are in V , for example, f ( e 1 ) = ( v 1 ; v 2 ), f ( e 2 most generic class of the substructure of G is a \ gener al sub-graph " where V s  X  V , E s  X  E and v i ; v j 2 V s for all edges eral subgraph in which a vertex v 5 and edges e 4 ; e 6 ; e are missed. Another importan t and generic class of the sub-structure is an \ induc ed subgraph " where V s  X  V , E s and 8 v i ; v j 2 V s , e h = ( v i ; v j ) 2 E s , f ( e An induced subgraph G i s of a graph G has a subset of the vertices of G and the same edges between pairs of vertices as in G . Fig. 1 (c) is an example of the induced subgraph in which a vertex v 5 is missed. In this case, only the edges e and e 9 are also missed, and e 4 ; e 6 ; e 7 are retained since they exist among v 1 ; v 3 and v 4 in the original G . The third im-portan t and generic class of the substructure is a \ conne cted subgraph " where V s  X  V , E s  X  E and all vertices in V s mutually reachable through some edges in E s . Fig. 1 (d) is an example where v 6 is further missed from (c). Moreo ver, (d) is an example of an \ induc ed and conne cted subgraph " since it satis X es both conditions of the induced subgraph and the connected subgraph. An acyclic sub-graph is called a \ tree ".
 Though the labels of ver-tices and edg-es are not considered in the aforemen-tioned graph form ulation, if we intro-duce the la-bels of edges in the tree, and if they are ordered in a way that the label of an edge is al-ways young-er than the labels of its lower (upp er) and right (left) edges, the tree is de X ned as an \ ordered tree ". If the edge is not ordered or does not have labels, the tree is called an \ unordered tree ". Fig. 1 (e) is an example of the ordered tree in terms of the edge indices which has a root node v 1 , and is included as a substructure in the original graph (a). The graph (f) is an example of the unordered tree. If the substructure does not include any branc hes, it is called a \ path "of the original G . The graph (g) is an example path. Given two graphs G x ( V x ; E x ; f x ) and G y ( V y ; E \ subgraph isomorphism " problem is to  X nd the subgraphs G g xy between the vertices in V sx and the vertices in V sy such that G sx and G sy are identical, i.e. , f x ( e xh ) = ( v E sx i X  f y ( e yh ) = ( v yi ; v yj ) 2 E sy where v yi = g v yj = g xy ( v xj ). The existence of g xy ensures the topological identity between G sx and G sy . For example, the graphs (b) and (d) in Fig. 1 commonly share the subgraph consisting of the vertices f v 1 ; v 2 ; v 3 g and the edges f e 1 ; e 2 mapping is a \ subgraph isomorphism " between the graphs (b) and (d).
 In graph-based data mining, the subgraph isomorphism prob-lem is further extended to cover multiple graphs. Given is to  X nd the subgraph G s ( V s ; E s ; f s ), a set of subgraphs f G between the vertices of G s and every G sk for all k = 1 ; :::; n . When f s satisfying this condition exists, G s ( V s ; E s mon subgraph of the given set of graphs. This de X nition of the subgraph isomorphism provides the basis for matc hing and coun ting of topologically identical parts of the given graphs.
 The graph isomorphism problem (i.e. the problem of decid-ing whether two graphs have identical topological structure) has an unkno wn computational complexit y. It is either NP-complete or polynomial and all attempts to classify it in one of these two categories have so far failed. On the other hand, the subgraph isomorphism problem (i.e. the problem of deciding whether one graph is a subgraph of another one) is known to be NP-complete. Graph invarian ts are the quan tities to characterize the topo-logical structure of a graph. If two graphs are topologically identical, i.e. , isomorphic, they also have identical graph invarian ts, though the reverse prop erty does not hold. Ex-amples of graph invarian ts are the number of vertices, the degree of each vertex, i.e. , the number of edges connected to the vertex, and the number of cyclic loops. Isomorphic graphs always have identical values of all graph invarian ts, while the identical values of given graph invarian ts does not imply the isomorphism of the graphs. Accordingly the use of graph invarian ts is not equiv alent to complete isomorphic subgraph matc hing and coun ting. However, graph invari-ants can be used to reduce the searc h space to solve the subgraph isomorphism problem. If any of the graph invari-ants show di X eren t values between two subgraphs, the sub-graphs are not isomorphic.
 In discrete mathematics, many studies to solve the graph iso-morphism problem between two graphs by introducing the graph invarian ts have been made in the last decades. One of the represen tativ e works is the NAUTY algorithm which is known to be one of the fastest algorithms for graph isomor-phism [18]. It focuses the graph invarian ts on each vertex in the graphs, e.g., its degree and the numbers of its adjacen t vertices having certain degrees, to reduce the searc h space drastically . The mapping of vertices having di X eren t values of the graph invarian ts between the given two graphs never exist in the graph isomorphism problem, because such ver-tices locate at mutually di X eren t positions of the graphs in the sense of topology . Accordingly , it partitions the set of vertices V of a given graph into its subsets where every ver-tex has mutually identical values of graph invarian ts. Then, it checks the graph isomorphism between the subsets having identical values of the graph invarian ts for the two graphs in a brute force manner. If all subsets are isomorphic be-tween the two graphs, the isomorphism of the two graphs is concluded. This divide and conquer approac h based on the graph invarian ts signi X can tly enhances the computational e X ciency in most of the practical problems.
 More direct represen tation and handling of the graph struc-ture can be made in an algebraic framew ork. The \ adjac ency matrix " is one such example [11]. The i -th row and the i -th column corresp ond to the i -th vertex v i . The i; j -elemen t of the matrix is the set of the edges f f ( e h ) = ( v i ; v ing the vertices. To be rigorous, it is not a matrix because its elemen ts are not real numbers, but here we call it matrix for convenience. If no edge exists between the two vertices, the elemen t is nil or 0. For example, the adjacency matrix of Fig. 1 (a) is represen ted as follows. All graph invarian ts are deriv ed from this direct represen-tation of a graph. A drawbac k is the complexit y on the memory consumption and the processing time.
 One of the most generic and importan t graph invarian ts is \ canonic al label " and \ canonic al form " [11; 4]. A graph can be represen ted by multiple forms. For example, all adja-cency matrices obtained from an adjacency matrix through permutations of rows and columns represen t an identical graph. This ambiguit y is commonly seen in various graph represen tations, and induces a combinatorial increase of the searc h space in the graph isomorphism problem. The canon-ical label is the most e X ectiv e remedy for this issue. Various de X nitions of the canonical label are possible, but it has to uniquely represen t a graph. For example, the n  X  n adja-cency matrix can be labeled by a code generated from the matrix elemen ts in the following order. where only the upper right columns are used because of the diagonal symmetry of the matrix for the undirected graph. Similar coding is applicable to the case of directed graphs. We can uniquely de X ne the canonical label as the lexico-graphically minim um (or maxim um) code and the canonical form of the adjacency matrix as the matrix corresp onding to the canonical label. The introduction of the canonical label and the canonical form signi X can tly reduces the graph represen tation ambiguit y and the searc h space. Recen tly, a new researc h direction to use graph invarian ts for the construction of a high dimensional feature space char-acterizing a graph has been prop osed [13]. Various machine learning, data mining and statistical approac hes can be ap-plied if the graph is transformed into a feature vector. The newly emerging approac h collects many graph invarian ts on a graph G , and forms a feature vector X G consisting of the graph invarian ts. When the graph G is very complex, the dimension of X G required to appro ximate the graph topol-ogy closely can be very large. This causes computational problems for many mining approac h. To alleviate these is-sues, the new approac h introduces a kernel function K ( X X
G y ) and a mapping  X  : X G ! H enabling the represen ta-tion of K by the inner product &lt;  X  ( X Gx ) ;  X  ( X Gy represen ts a similarit y between two graphs G x and G y , and H is usually the Hilbert space. Because the value of the in-ner product can be deriv ed without directly computing the vectors in H , this approac h can avoid the tractabilit y issue. Moreo ver, the issue of the sparse distribution is also alle-viated because the similarit y K is given by a scalar value. A drawbac k of this approac h is that these kernels cannot be computed e X cien tly. Some appropriate alternativ es and similarit y measures have been prop osed in recen t works [7; 13]. Various measures to mine substructures of graphs are used similarly to conventional data mining. The selection of the measures depends on the objectiv e and the constrain ts of the mining approac h. The most popular measure in the graph-based data mining is the following \ support " whose de X ni-tion is identical with that of Bask et Analysis [2]. Given a graph data set D , the supp ort of the subgraph G s , sup ( G is de X ned as This measure has an anti-monotonic prop erty that sup ( G  X  sup ( G sy ) if G sy is a subgraph of G sx . By specifying a \ minimum support " value minsup , subgraphs f G s g whose supp ort values are more than the minsup are mined in some approac hes.
 The anti-monotonicit y of the supp ort is insu X cien t for some mining objectiv es. For example, an analysis to  X nd sub-graphs G s s which appear more than a minim um supp ort minsup but also less than a maxim um supp ort maxsup ( minsup &lt; maxsup ) may be needed in some application domains. The former constrain t c 1 is anti-monotonic, i.e. , where G s &lt; G c 1 means that G s is more general than G ( G s is a subgraph of G c 1 ) and sol ( c 1 ) a solution set of c i.e. , a set of all subgraphs satisfying c 1 in D . The latter constrain t c 2 is monotonic, i.e. , where sol ( c 2 ) a solution set of c 2 . The negation of a mono-tonic constrain t is anti-monotonic, and the negation of an anti-monotonic constrain t is monotonic. The version space algorithm of graph-based data mining can handle these mea-sures as describ ed later [4].
 Many other mining measures which are very commonly used in the machine learning  X eld are also used in some graph-based data mining approac hes, especially , information en-tropy, information gain, gini-index and minim um descrip-tion length (MDL) [25; 3]. The aforemen tioned subgraph isomorphism problem among many graphs must be solved by using e X cien t searc h meth-ods in graph-based data mining. Roughly speaking,  X ve types of searc h metho ds are curren tly used. These are cat-egorized into heuristic searc h metho ds and complete searc h metho ds in terms of the completeness of searc h. They are also categorized into direct and indirect matc hing metho ds from the view point of the subgraph isomorphism matc hing problem. The indirect matc hing does not solve the subgraph isomorphism problem but subgraph similarit y problem un-der some similarit y measure.
 The  X rst type of the searc h metho d is the conventional greedy searc h which has been used in the initial works in graph-based data mining [25; 3]. This type belongs to heuris-tic searc h and direct matc hing. The greedy searc h is further categorized into depth- X rst searc h (DFS) and breadth- X rst searc h (BFS). DFS was used in the early studies since it can save memory consumption. Initially the mapping f s 1 from a vertex in a candidate subgraph to a vertex in the graphs of a given data set is searc hed under a mining measure. Then another adjacen t vertex is added to the vertex mapp ed by f s 1 , and the extended mapping f s 2 to map these two vertices to two vertices in the graphs of a given data set is searc hed under the mining measure. This process is repeated until no more extension of the mapping f sn is available where n is the maximal depth of the searc h of a DFS branc h. A draw-back of this DFS approac h is that only an arbitrary part of the isomorphic subgraphs can be found when the searc h must be stopp ed due to the searc h time constrain ts if the searc h space is very large. Because of the recen t progress of the computer hardw are, more memory became available in the searc h. Accordingly , the recen t approac hes to graph-based data mining are using BFS. An advantage of BFS is that it can ensure deriv ation of all isomorphic subgraphs within a speci X ed size of the subgraphs even under greedy searc h scheme. However, the searc h space is so large in many applications that it often does not  X t in memory . To alleviate this di X cult y, beam searc h metho d is used in the recen t greedy searc h based approac h [3; 25] where the max-imum number of the BFS branc hes is set, and the searc h proceeds downward by pruning the branc hes which do not  X t the maxim um branc h number. Since this metho d prunes the searc h paths, the searc h of the isomorphic subgraphs  X nishes within tractable time while the completeness of the searc h is lost.
 The second type of searc h metho d is to apply the framew ork of \ inductive logic programming (ILP) " [20]. The \ induc-tion " is known to be the combination of the \ abduction " to select some hypotheses and the \ justi X c ation " to seek the hy-potheses to justify the observ ed facts. Its main advantage is the abilities to introduce background knowledge associ-ated with the subgraph isomorphism and the objectiv e of the graph-based data mining. It can also deriv e knowledge represen ted by \  X rst order predicate logic " from a given set of data under the background knowledge. The general graph is known to be represen ted by \  X rst order predicate logic ". The  X rst order predicate logic is so generic that generalized patterns of graph structures to include variables on the la-bels of vertices and edges are represen ted. ILP is formalized as follows [20]. Given the background knowledge B and the evidence (the observ ed data) E where E consists of the pos-itive evidence E + and the negativ e evidence E  X  , ILP  X nds a hypothesis H such that the following \ normal semantics " conditions hold. 1. Posterior Satis X abilit y: B ^ H ^ E  X  j = = 2 , 2. Posterior Su X ciency: B ^ H j = E + , where 2 is \ false ", and hence j = = 2 means that the theory is satis X able. In case of ILP, intentional de X nitions are de-rived from the given data represen ted by instan tiated  X rst order predicates, i.e. , extensional de X nitions. For ILP, the advantage is not limited to the knowledge to be disco vered but the abilit y to use the positiv e and the negativ e exam-ples in the induction of the knowledge. A disadv antage is the size of the searc h space which is very huge in general and computational intractabilit y. The ILP metho d can be any of heuristic, complete, direct and indirect searc h accord-ing to the background knowledge used to control the searc h process. When control knowledge is used to prune some searc h paths having low possibilit y to  X nd isomorphic sub-graphs under a given mining measure, the metho d is heuris-tic. Otherwise, it is complete. When some knowledge on predetermined subgraph patterns are introduced to matc h subgraph structures, the metho d is indirect since only the subgraph patterns including the predetermined patterns or being similar to the predetermined patterns are mined. In this case the subgraph isomorphism is not strictly solved. The third type is to use \ inductive datab ase " [10]. Given a data set, a mining approac h such as inductiv e decision tree learning, basket analysis [2] and ILP is applied to the data to pregenerate inductiv e rules, relations or patterns. The induced results are stored in a database. The database is queried by using a query language designed to concisely express query conditions on the forms of the pregenerated results in the database. This framew ork is applicable to graph-based mining. Subgraphs and/or relations among subgraphs are pregenerated by using a graph-based min-ing approac h, and stored in an inductiv e database. A query on the subgraphs and/or the relations is made by using a query language dedicated to the database. An advantage of this metho d is the fast operation of the graph mining, because the basic patterns of the subgraphs and/or the re-lations have already been pregenerated. Potential drawbac k is large amoun t of computation and memory to pregenerate and store the induced patterns. This searc h metho d is used in conjunction with the following complete level-wise searc h metho d in some works [4].
 The fourth type is to apply \ complete level-wise search " which is popularly used in the basket analysis and \ inductive datab ase ". These are complete searc h and direct metho ds. In case of Apriori algorithm which is the most represen-tativ e for the basket analysis [2], all frequen t items which appear more than a speci X ed minim um supp ort \ minsup " in the transaction data are enumerated as frequen t item-sets of size 1. This task is easily conducted by scanning the given transaction data once. Subsequen tly, the frequen t itemsets are joined into the candidate frequen t itemsets of size 2, and their supp ort values are checked in the data. Only the candidates having the supp ort higher than minsup are retained as the frequen t itemsets of size 2. This process to extend the searc h level in terms of the size of the fre-quen t itemsets is repeated until no more frequen t itemsets are found. This searc h is complete since the algorithm ex-haustiv ely searc hes the complete set of frequen t item sets in a level-wise manner. In case of the graph-based data min-ing, the data are not the transactions, i.e. , sets of items, but graphs, i.e. , combinations of a vertex set V ( G ) and an edge set E ( G ) which include topological information. Accord-ingly , the above level-wise searc h is extended to handle the connections of vertices and edges [11; 4]. Similarly to the Apriori algorithm, the searc h in a given graph data starts from the frequen t graphs of size 1 where each consists of only a single vertex. Subsequen tly, the candidate frequen t graphs of size 2 are enumerated by combining two frequen t vertices. Then the supp ort of each candidate is coun ted in the graph data, and only the graphs having higher supp ort than the minsup are retained. In this coun ting stage, the edge information is used. If the existence and the label of the edge between the two vertices do not matc h, the graph of size 2 is not coun ted as an identical graph. This process is further repeated to incremen tally extend the size of the frequen t graphs in a level wise manner, and  X nishes when the frequen t graphs are exhaustiv ely searc hed. In induc-tive database and constrain t-based mining, the algorithm can be extended to introduce monotonic measures such as \ maxsup " [4].
 The  X fth type is \ Supp ort Vector Machine (SVM) " [23]. This is a heuristic searc h and indirect metho d in terms of the subgraph isomorphism problem and used in the graph classi X cation problem. It is not dedicated to graph data but to feature vector data. Given feature and class vectors where L is the total number of data, i = 1 ; :::; L , Z a set of vectors and y i a binary class labels, each sample feature vector x 1 in the data is classi X ed by Here  X  : Z ! H where H is the Hilbert space,  X  i ; b 2 R and  X  i positiv e  X nite. By extending the feature space to far higher dimension space via  X  , SVM can prop erly classify the samples by a linear hyper plane even under complex nonlin-ear distributions of the samples in terms of the class in Z . The product  X  ( x i )  X   X  ( x ) can be represen ted by the afore-mentioned kernel function K ( X G x ; X G y ) for graphs where X
G x = x i and X G y = x . Accordingly , SVM can provide an e X cien t classi X er based on the set of graph invarian ts. The approac hes to graph-based data mining are categorized into  X ve groups. They are greedy searc h based approac h, inductiv e logic programming (ILP) based approac h, induc-tive database based approac h, mathematical graph theory based approac h and kernel function based approac h. In this section, some major studies in each category are describ ed, and represen tativ e metho ds among the studies are sketched. Two pioneering works appeared in around 1994, both of which were in the framew ork of greedy searc h based graph mining. Interestingly both were originated to disco ver con-cepts from graph represen tations of some structure, e.g. a conceptual graph similar to seman tic network and a physical system such as electric circuits.
 One is called \S UBDUE " [3]. S UBDUE deals with concep-tual graphs which belong to a class of connected graph. The vertex set V ( G ) is R [ C where R and C are the sets of labeled vertices represen ting relations and concepts respec-tively. The edge set E ( G ) is U which is a set of labeled edges. Though the original S UBDUE targeted the disco very of repeatedly appearing connected subgraphs in this speci X c type of graph data, i.e. , concept graph data, the principle can be applied to generic connected graphs.
 S UBDUE starts looking for a subgraph which can best com-press an input graph G based on Minim um Description Leng-th (MDL) principle. The found subgraph can be considered a concept. This algorithm is based on a computationally-constrained beam searc h. It begins with a subgraph com-prising only a single vertex in the input graph G , and grows it incremen tally expanding a node in it. At each expansion it evaluates the total description length (DL), I ( G s )+ I ( G j G of the input graph G which is de X ned as the sum of the two: DL of the subgraph, I ( G s ), and DL of the input graph, I ( G j G s ), in which all the instances of the subgraph are re-placed by single nodes. It stops when the subgraph that minimizes the total description length is found. The searc h is completely greedy , and it never backtrac ks. Since the maxim um width of the beam is predetermined, it may miss an optim um G s . One of the good features of S UBDUE is that it can perform appro ximate matc hing to allow slight varia-tions of subgraphs. It can also embed background knowl-edge in the form of prede X ned subgraphs. After the best substructure is found and the input graph is rewritten, the next iteration starts using the rewritten graph as a new in-put. This way, S UBDUE  X nds a more abstract concept at each round of iteration. As is clear, the algorithm can  X nd only one substructure at each iteration. Furthermore, it does not main tain strictly the original input graph struc-ture after compression because its aim is to facilitate the global understanding of the complex database by forming hierarc hical concepts and using them to appro ximately de-scrib e the input data. Recen tly, a new technique to induce a graph grammar has been developed by the same group [12]. The other one is called \ Graph Based Induction (GBI)" [25]. GBI was originally intended to  X nd interesting concepts from inference patterns by extracting frequen tly appearing patterns in the inference trace. GBI was form ulated to de-rive a graph having a minimal size similarly to S UBDUE by replacing each found subgraph with one vertex that it re-peatedly compress the graph. It used an empirical graph size de X nition that re X  X cted the sizes of extracted patterns as well as the size of compressed graph. This prevented the algorithm from continually compressing, which mean t the graph never became a single vertex. GBI can handle both directed and undirected labeled graph with closed paths (in-cluding closed edges). An opportunistic beam searc h similar to genetic algorithm was used to arriv e at local minim um so-lutions. In this algorithm, the primitiv e operation at each step in the searc h was to  X nd a good set of linked pairs of vertices by an edge to chunk, i.e. , pairwise chunking. The idea of pairwise chunking in case of a directed graph is given in Fig. 2. GBI chunks the triplet ( A k ; f i ; B j ) which mini-mizes the above graph size where A k and B j are the vertices, and f i is a link directing from B j to A k . This chunking is repeated until the size of the graph reaches a local mini-mum. It is possible that either one or both of the paired nodes have already been chunked. Chunking can be nested. GBI remem bers the link information and can reconstruct the original graph at any time of the searc h. Later an im-provemen t was made to use other measures than frequency .
 Because the searc h is lo-cal and step-wise, an in-direct measure rather than a direct estimate of the graph size is adopted to  X nd the most promising pairs. Such mea-sures include information gain, information gain ratio and gini index, all of which can be evaluated from the frequency of the pair selected to chunk. By using such measures it is possible to extract substructures that are discriminating if a class label is attac hed to each input graph, and construct a classi X er. This idea was applied to induce a classi X er for command prediction. It is known that a simple command sequence is not enough to accurately predict the next com-mand. The  X le I/O relations across the issued commands together with the command sequence information form a di-rected tree, and the prediction task is form ulated to predict the root node of a tree from its subtree. Figure 3 depicts the process of the subgraph (subtree in this case) disco very in the commands and I/O history data indicated in the table. Unlab eled edges indicate command sequences and labeled edges indicate  X le I/O relations. As shown in steps (A), (B) and (C), the  X le paper.dvi is processed by three di X eren t commands: xtex , xdvi and dvi2ps . The corresp onding di-rected graphs that are inputs to GBI are also given as (A), (B) and (C) in the  X gure. The GBI algorithm  X rst chooses and chunks the triplets ( xdvi, dvi, latex ) in (B) and ( dvi2ps, dvi, latex ) in (C) in accordance with the information gain. Next, the algorithm chooses the unlab eled edge and its start-ing vertex xdvi for testing and chunks the triplet (( dvi2ps, dvi, latex ) , unlab el, xdvi ). This separates (C) from (B) and the induction stops. The parts surrounded by round rectangles are the typical (discriminating) patterns. It is straigh tforw ard to construct a decision tree from these pat-terns. The most re-cent version of GBI can han-dle two di X er-ent measures, one for chunk-ing and the other for ex-tracting patters.
 This is to over-come the prob-lem caused by non monotonic nature of dis-criminating mea-sure. The fact that a subgraph A is a good discriminator does not necessarily mean that its subgraph B is also a good discriminator. However, successiv e pairwise chunking re-quests that B must be extracted in an earlier step in order to obtain A later. Curren t GBI emplo ys canonical labeling to see if two chunks obtained by di X eren t histories are iso-morphic or not. Recen tly GBI is also being used as a feature constructor in a decision tree classi X er for graph structured data [8]. To our knowledge, the  X rst system to try complete searc h for the wider class of frequen t substructure in graphs named WARMR was prop osed in 1998 [6]. They combined ILP metho d with Apriori-lik e level wise searc h to a problem of carcinogenesis prediction of chemical comp ounds. The structures of chemical comp ounds are represen ted by the  X rst order predicates such as atomel ( C; A 1 ; c ), bond ( C; A 1 ; A 2 ; BT ), aromatic ring ( C; S 1) and alcohol ( C; S 2). The  X rst two state that A 1 which is a carbon atom bond to A 2 where the bond type is BT in a chemical comp ound C . The third represen ts that substructure S 1 is an aromatic ring in a chemical comp ound C , and the last represen ts that S 2 is an alcohol base in C . Because this approac h allows variables to be introduced in the argumen ts of the predicates, the class of structures which can be searc hed is more general than graphs. However, this approac h easily faces the high com-putational complexit y due to the equiv alence checking under  X  -subsumption (an NP-complete operation) on clauses and the generalit y of the problem class to be solved. To alleviate this di X cult y, a new system called FARMAR has recen tly been prop osed [21]. It also uses the level wise searc h, but applied less strict equiv alence relation under substitution to reduced atom sets. FARMAR runs two orders of magni-tudes faster. However, its result includes some prop ositions having di X eren t forms but equiv alent in the sense of the  X  -subsumption due to the weaker equiv alence criterion. A major advantage of these two systems is that they can dis-cover frequen t structures in high level descriptions. These approac hes are expected to address many problems, because many context dependen t data in the real-w orld can be rep-resen ted as a set of grounded  X rst order predicates which is represen ted by graphs. A work in the framew ork of inductiv e database having prac-tical computational e X ciency is MolF ea system based on the level-wise version space algorithm [4]. This metho d performs the complete searc h of the paths embedded in a graph data set where the paths satisfy monotonic and anti-monotonic measures in the version space. The version space is a searc h subspace in a lattice structure. The monotonic and anti-monotonic mining measures describ ed in subsection 2.4 de- X ne borders in the version space. To de X ne the borders, the minimal and the maximal elemen ts of a set in terms of gen-eralit y are introduced. Let F be a set of paths included in graph data, then de X ne where f  X  q indicates that q is more general than or equally general to f . Then, given a set sol ( c ) of all paths satisfying a primitiv e constrain t c in a graph data set D , the borders S ( c ) and G ( c ) are de X ned as When c is a prop er anti-monotonic constrain ts, G ( c ) = f&gt;g and S ( c ) 6 = f?g holds, and when c is a prop er monotonic constrain ts, S ( c ) = f?g and G ( c ) 6 = f&gt;g holds. Under these de X nitions, the two borders in the version space have the following relation with sol ( c ). The set of solutions sol ( c i ) to each primitiv e constrain t c is a version space de X ned by the borders S ( c i ) and G ( c The computation of S ( c i ) and G ( c i ) for a constrain t c conducted by the commonly used level-wise algorithm men-tioned in subsection 2.5. Furthermore, sol ( c 1 ^ ::: ^ c a conjunctiv e constrain t c 1 ^ ::: ^ c n is also characterized by S ( c 1 ^ ::: ^ c n ) and G ( c 1 ^ ::: ^ c n ). This sol ( c deriv ed by the level wise version space algorithm [4]. Based on this framew ork, MolF ea system can perform a com-plete searc h for the solutions, w.r.t , a conjunctiv e constrain t consisting of monotonic and anti-monotonic primitiv e con-strain ts. For example, given a set of molecule structure graph data of chemical comp ounds, the following constrain t: ( c-o  X  f ) ^ : ( f  X  c-o-s-c-o-s ) ^ sup ( f )  X  100 queries for all paths f s embedded in the molecule structure that in-clude (being more speci X c than or equal to) the subpath c  X  o but do not include (not being more general than or not equal to) the subpath of c-o-s-c-o-s and have a frequency larger than 100. MolF ea system has applied to the Pre-dictiv e Toxicology Evaluation challenge data set [22]. This data set consists of over 300 comp ounds. The goal of the data mining here is to disco ver molecular fragmen ts that are frequen t in carcinogenetic comp ounds and infrequen t in non-carcinogenetic comp ounds which are called \ structur al alerts " in toxicology . The frequency and the infrequency which are anti-monotonic and monotonic measures respec-tively are speci X ed to certain numbers as indicated below and a solution for each pair of the frequency and the infre-quency bounds are enumerated. Single bond and aromatic bond between two atoms are represen ted by  X  and  X  re-spectiv ely. The mathematical graph theory based approac h mines a complete set of subgraphs under mainly supp ort measure. The initial work is AGM (Apriori-based Graph Mining) sys-tem [11]. The basic principle of AGM is similar to the Apri-ori algorithm for basket analysis. Starting from frequen t graphs where each graph is a single vertex, the frequen t graphs having larger sizes are searc hed in bottom up man-ner by generating candidates having an extra vertex. An edge should be added between the extra vertex and some of the vertices in the smaller frequen t graph when searc hing for the connected graphs. One graph constitutes one trans-action. The graph structured data is transformed without much computational e X ort into an adjacency matrix men-tioned in subsection 2.3. Let the number of vertecies con-tained in a graph be its \ size ", an adjacency matrix of a graph whose size is k be X k , the ij -elemen t of X k , x its graph, G ( X k ). AGM can handle the graphs consisting of labeled vertices and labeled edges. The vertex labels are de X ned as N p ( p = 1 ;  X  X  X  ;  X  ) and the edge labels, L ( q = 1 ;  X  X  X  ;  X  ). Labels of vertices and edges are indexed by natural numbers for computational e X ciency . The AGM system can mine various types of subgraphs including gen-eral subgraph, induced subgraph, connected subgraph, or-dered subtree, unordered subtree and subpath. Because of the space limitation, the case to mine induced subgraph is shown here. This algorithm generates association rules hav-ing supp ort and con X dence higher than user speci X ed thresh-olds. In the actual implemen tation, the adjacency matrices are represen ted by the codes de X ned as in subsection 2.3. According to the code, the canonical label and the canoni-cal form are also introduced.
 The candidate generation of frequen t induced subgraph is done as follows. It is also outlined in Fig. 4. Two frequen t graphs are joined only when the following conditions are satis X ed to generate a candidate of frequen t graph of size k +1. Let X k and Y k be adjacency matrices of two frequen t graphs G ( X k ) and G ( Y k ) of size k . If both G ( X k have equal elemen ts of the matrices except for the elemen ts of the k -th row and the k -th column, then they are joined to generate Z k +1 as follows where X k  X  1 is the adjacency matrix represen ting the graph vectors. The elemen ts z k;k +1 and z k +1 ;k represen t an edge label between k -th vertices of X k and Y k . Their values are mutually identical because of the diagonal symmetry of the undirected graph. Here, the elemen ts z k;k +1 and z k +1 ;k the adjacency matrix Z k +1 are not determined by X k and Y . In case of an undirected graph, two possible cases are considered in which 1) there is an edge labeled L q between the k -th vertex and the k + 1-th vertex of G ( Z k +1 ) and 2) there is no edge among them. This is indicated in Fig. 4. Ac-cordingly  X  + 1 adjacency matrices whose ( k; k + 1)-elemen t and ( k + 1 ; k )-elemen t are \0" and \ L q " are generated. X and Y k are called the  X rst matrix and the second matrix to generate Z k +1 respectiv ely. Because the labels of the k -th nodes of X k and Y k are the same, switc h-ing X k and Y k , i.e. , taking Y k as the  X rst ma-trix and X k as the second ma-trix, produces redundan t ad-jacency matri-ces. In order to avoid this redundancy , the two adjacency matrices are joined only when the following condition is satis X ed. The adjacency matrix generated under these constrain ts is a " normal form ". The graph G of size k +1 is a candidate frequen t graph only when adjacency matrices of all induced subgraphs whose size are k are con X rmed to be frequen t graphs. If any of the induced subgraphs of G ( Z k +1 ) is not frequen t, Z k +1 is not a candidate frequen t graph, because any induced subgraph of a frequen t graph must be a frequen t graph due to the anti-monotonicit y of the supp ort. This check to use only the former result of the frequen t graph mining is done without accessing the graph data set. Af-ter the generation of candidate subgraphs, their supp ort is coun ted by accessing the data set. To save computation for the coun ting, the graphs in the data set are represen ted in normal form matrices, and each subgraph matc hing is made between their normal forms. This technique signi X can tly increases the matc hing e X ciency . The process continues in level-wise manner until no new frequen t induced subgraph is disco vered.
 This approac h has been applied to the analysis of the associ-ation of the molecule substructures of chemical comp ounds with their mutagenesis activit y. The data was drawn from a chemistry journal of A. K. Debnath et al. [5]. The data con-tains 230 aro-matic and het-eroaromatic ni-tro comp ounds.
 The percen tages of the molecule having the class-es of high, medi-um, low and inactiv e muta-genicit y are 15.2%, 45.7%, 29.5% and 9.6% respectiv ely. In the data prepro cessing stage, numerical at-tributes of the chemical comp ounds of LogP and LUMO were discretized into symbolic magnitudes, converted to an isolated vertex, and added to each molecule structure. Figure 5 is the disco vered molec-ular substruc-ture indicating low activit y, and Fig. 6 contains the ones indi-cating high ac-tivity. The de-viations of the activit y distri-butions from the original distri-bution are clear for both cases, and their signi X cances has been con X rmed via  X  2 -test. After the prop osal of AGM, a family of graph-based data mining based on similar principles has been prop osed. A work is FSG (Frequen t SubGraph disco very) system [15] which also takes similar de X nition of canonical labeling of graphs based on the adjacency matrix. To increase the e X ciency of de-riving the canonical labels, the approac h uses some graph vertex invarian ts such as the degree of each vertex in the graph. FSG also increases the e X ciency of the candidate generation of frequen t subgraphs by introducing the trans-action ID (TID) metho d. Furthermore, FSG limits the class of the frequen t subgraphs to connected graphs. Under this limitation, FSG introduces an e X cien t searc h algorithm us-ing \ core "whic h is a shared part of the size k  X  1 in the two frequen t subgraphs of the size k . FSG increases the joining e X ciency by limiting the common part of the two frequen t graphs to the core. Once the candidate set is obtained, their frequency coun ting is conducted by checking the cardinalit y of the intersection of both TID lists. FSG runs fast due to the introduction of many techniques, but it consumes much memory space to store TID lists for massiv e graph data. More recen tly, DFS based canonical labeling approac h called gSpan (graph-based Substructure pattern mining) has been prop osed [24]. This approac h also uses the idea of canonical labeling which is deriv ed from a coding scheme of a graph represen tation. The main di X erence of the coding from the other approac h is that it uses a tree represen tation of each graph instead of the adjacency matrix to de X ne the code of the graph as depicted in Fig. 7. Given a graph (a), various quasi-tree expressions of the graph exist depending on the way to take a root vertex among the vertices. (b), (c) and (d) are the examples where the least number of edges to re-move all cyclic paths are represen ted by dashed lines. Upon this represen tation, starting from the root vertex, the code is generated by following the lexicographical order of the labels for the combinations of vertices and the edge bound by the vertices. For example, the combinations of ( v 0 ; v 1 ) with the label ( v 0 ; v 1 ; X; a; Y ) comes  X rst in the code because this is starting from v 1 which is the last vertex of the previous code elemen t, the youngest code elemen t ( v 1 ; v 2 ; Y; b; X ) is cho-chosen. When this trace returns to a vertex which is in-volved in the previous code elemen t, the trace backtrac ks by one step, and the next younger elemen t starting from v ( v manner. The sequence of these elemen ts is called a code of this quasi-tree expression. The other quasi-tree expres-sions including (c) and (d) have their own codes respectiv ely. Among the codes, the quasi-tree expression having the mini-mum code in terms of the lexicographical order is the canon-ical form, and the corresp onding code is the canonical label. Because the code is deriv ed in the DFS algorithm, this code is called DFS code. Every graph in a data set is represen ted by the multi-ple codes in this manner.
 Then all codes are sorted ac-cording to as-cending lexico-graphical order, and the matc h-ing of the codes starting from the  X rst ele-ments among the codes are conducted by using DFS in the sorted order. This means that the searc h trees of the DFS matc hing are ordered trees where the left branc h is always younger than the right branc h. Accordingly , when the branc h represen ting a subgraph which is identical to the subgraph previously vis-ited in the ordered tree searc h are found, the further DFS in the searc h tree can be pruned. By applying this DFS coding and DFS searc h, gSpan can deriv e complete set of frequen t subgraphs over a given minsup in a very e X cien t manner in both computational time and memory consumption. As explained in the last paragraph in subsection 2.3, a kernel function K de X nes a similarit y between two graphs G x and G . For the application to graph-based data mining, the key issue is to  X nd the good combinations of the feature vector X
G and the mapping  X  : X G ! H to de X ne appropriate sim-ilarit y under abstracted inner product &lt;  X  ( X Gx ) ;  X  ( X A recen t study prop osed a comp osition of a kernel function characterizing the similarit y between two graphs G x and G based on the feature vectors consisting of graph invarian ts of vertex labels and edge labels in the certain neigh bor area of each vertex [13]. This is used to classify the graphs into binary classes by SVM mentioned in subsection 2.5. Given training data consisting of graphs having binary class, the SVM is trained to classify each graph. Though the simi-larity is not complete and sound in terms of the graph iso-morphism, the graphs are classi X ed prop erly based on the similarit y de X ned by the kernel function.
 Another framew ork of kernel function related with graph structures is called \ di X usion kernel " [14]. Though this is not dedicated to graph-based data mining, each instance is assigned to a vertex in a graph structure, and the similarit y between instances is evaluated under the di X usion process along the edges of the graph. Some experimen ts report that the similarit y evaluation in the structure characterizing the relations among the instances provides better performance in classi X cation and clustering tasks than the distance based similarit y evaluation. This type of work is supp osed to have some theoretical relation with graph-based data mining [7]. There are many other studies related to graph mining. Geib el and Wysotzki prop osed a metho d to deriv e induced sub-graphs of graph data and to use the induced subgraphs as attributes on decision tree approac hes [9]. Their metho d can be used to  X nd frequen t induced subgraphs in the set of graph data. However, the upper limit number of the ver-tices to be included in the subgraph must be initially speci- X ed to avoid the exponen tial explosion of the computational time, and thus the searc h is not complete. Liquiere and Sal-lantin prop osed a metho d to completely searc h homomor-phically equiv alent subgraphs which are the least general over a given set of graphs and do not include any identical triplet of the labels of two vertices and the edge direction between the vertices within each subgraph [16]. They show that the computational complexit y to  X nd this class of sub-graphs is polynomial for 1/2 locally injectiv e graphs where the labels of any two vertices pointing to another common node or pointed from another common vertex are not iden-tical. However, many graphs appearing in real-w orld prob-lems such as chemical comp ound analysis are more general, and hence the polynomial characteristics of this approac h do not apply in real cases. In addition, this approac h may miss many interesting and/or useful subgraph patterns since the homomorphically equiv alent subgraph is a small subclass of the general subgraph.
 In this review article, the theoretical basis of the graph-based data mining was explained from multiple point of views such as subgraph types, subgraph isomorphism prob-lem, graph invarian ts, mining measures and searc h algo-rithms. Then, represen tativ e graph-based data mining ap-proac hes were shown in the latter half of this article. Even from theoretical perspectiv e, many open questions on the graph characteristics and the isomorphism complexit y re-main. This researc h  X eld provides many attractiv e topics in both theory and application, and is expected to be one of the key  X elds in data mining researc h. The authors express our deep appreciation to the review ers and editors of this article. [1] MRDM'01: Workshop multi-relational data mining. [2] R. Agra wal and R. Srikant. Fast algorithms for mining [3] J. Cook and L. Holder. Substructure disco very using [4] L. De Raedt and S. Kramer. The levelwise version space [5] A. Debnath, R. De Compadre, G. Debnath, A. Schus-[6] L. Dehasp e and H. Toivonen. Disco very of frequen t dat-[7] T. Gaertner. A survey of kernels for structured data. [8] W. Geamsakul, T. Matsuda, T. Yoshida, H. Moto da, [9] P. Geib el and F. Wysotzki. Learning relational concepts [10] T. Imielinski and H. Mannila. A database perspectiv e [11] A. Inokuc hi, T. Washio, and H. Moto da. Complete min-[12] I. Jonyer, L. Holder, and D. Cook. Concept formation [13] H. Kashima and A. Inokuc hi. Kernels for graph classi-[14] R. Kondor and J. La X ert y. Di X usion kernels on graphs [15] M. Kuramo chi and G. Karypis. Frequen t subgraph dis-[16] M. Liquiere and J. Sallan tin. Structural machine learn-[17] H. Mannila and H. Toivonen. Disco vering generalized [18] B. Mckay. Naut y users guide (version 1.5). Technical [19] A. Mendelzon, A. Mihaila, and T. Milo. Querying the [20] S. Muggleton and L. De Raedt. Inductiv e logic pro-[21] S. Nijssen and J. Kok. Faster association rules for mul-[22] A. Sriniv asan, R. King, and D. Bristol. An assessmen t [23] V. Vapnik. The Natur e of Statistic al Learning Theory . [24] X. Yan and J. Han. gspan: Graph-based substructure [25] K. Yoshida, H. Moto da, and N. Indurkh ya. Graph-[26] M. Zaki. E X cien tly mining frequen t trees in a forest. In
