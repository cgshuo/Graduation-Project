 Electrical and Computer Eng.
 rabbat@cae.wisc.edu colonies.
 throughput measurement techniques such as microarrays have successfully been used to identify inferring pathway orders is a largely unexplored research area [3].
 when a subject performs different tasks may lead to a similar network inference problem. feasible network over the others. See the supplementary document [7] for further discussion. incorporation of side information.
 In this paper, we model co-occurrences as randomly permuted samples of a random walk on the process as the shuffled Markov model. In this framework, network inference amounts to maximum pathway/observation, so that the computational complexity of the EM algorithm is determined by paths, we propose a Monte Carlo E-step based on a simple, linear complexity importance sampling breaks the curse of dimensionality using randomness.
 structure of a directed graphical model or Bayesian network [9, 10]. The aim of graphical mod-observations: co-occurring vertices must lie along a path in the network. 2.1 The Shuffled Markov Model z ( z matrix is determined by the adjacency structure of the graph; i.e. , A drawn uniformly from S Under this model, the log-likelihood of the set of observations Y is where P [ y |  X  , A ,  X  ] =  X  maximum likelihood (ML) estimates ( A ML ,  X  ML ) = arg max treating the permutations as missing data. 2.2 EM Algorithm ( x 1 , ..., x permutations are independent and equiprobable.
 The EM algorithm proceeds by (the E-step) computing Q A ,  X  ; A k ,  X  k = R r the E-step reduces to computing the conditional expectations of r ( m ) Q A ,  X  ; A k ,  X  k .
 Since the permutations are (a priori) equiprobable, we have P [ r ( m ) ] = ( N ( N m  X  1)! /N m ! = 1 /N m independence among different observations, and Bayes law, it is not hard to show that The computation of  X   X  ( m ) Computing {  X  r ( m ) Section 3, we describe a sampling approach for computing approximations to  X  r M-step: A sequence { ( A k ,  X  k ) } converges monotonically to a local maximum of the likelihood. 2.3 Handling Known Endpoints r are simply given by  X  matrices satisfying r For long paths, the combinatorial nature of the exact E-step  X  summing over all permutations of convergence properties of the EM algorithm; i.e. , monotonic convergence to a local maximum. 3.1 Monte Carlo E-Step by Importance Sampling for the current parameter estimates. Moreover, since the statistics  X   X  ( m ) y = ( y 1 , y 2 , . . . , y N ) and drop the superscript ( m ) .
 A na  X   X ve Monte Carlo approximation would be based on random permutations sampled from the uniform distribution on S place is that S vertex set, V , denote by p | f the restriction of p to those elements i  X  V for which f Our sampling scheme proceeds as follows: Step 1: Initialize f so that f Step 2: Let A Step 3: While i &lt; N , update v  X  v 0 and i  X  i + 1 and repeat Step 2; otherwise, stop. where the superscript now identifies the sample number; the corresponding permutation matrices distribution R [  X  | x , A ,  X  ] on S sample estimates correct for this disparity and are given by the expressions where the correction factor (or weight) for sample r ` is given by in the product (8) are readily available as a byproduct of Step 2 (denominator of A 3.2 Monotonicity and Convergence  X  antees that the exact EM iterates converge monotonically to a local maximum of log P [ Y|  X  ] . When the Monte Carlo E-step is used, we no longer have monotonicity since now the M-step solves b  X  by EM algorithm (MCEM) converges, the number of importance samples, L , must be chosen carefully so that for automatically adapting the number of Monte Carlo samples used at each EM iteration. They that Q ( Rather than resorting to asymptotic approximations, we take advantage of the specific form of Q in our problem to obtain the finite-sample PAM result below. Because log b A k i,j and log blow up. Specifically, we assume a small positive constant  X  Theorem 1 Let ,  X  &gt; 0 be given. There exist finite constants b if importance samples are used for the m th observation, then Q ( probability greater than 1  X   X  .
 the importance sample estimates showing, e.g. , that exponential in the number of importance samples used. These bounds are based on rather novel of b A update is probably monotonic ( i.e. , (0 ,  X  ) -PAM, not approximately monotonic) if L 0 samples are used for the m th observation, where L 0 the supplementary document [7] for further discussion and for the full proof of Theorem 1. Recall that exact E-step computation requires N PAM update is on the order of N 4 procedure described above requires N having polynomial computational complexity, and, in this sense, the MCEM meaningfully breaks the curse of dimensionality by using randomness to preserve the monotonic convergence property. The performance of our algorithm for network inference from co-occurrences (NICO, pronounced vations. Because the EM algorithm is only guaranteed to converge to a local maximum, we rerun We simulate co-occurrence observations in the following fashion. A random graph on 50 vertices weight per edge ( X  X hortest path X ) or a random weight on each edge ( X  X andom routing X ), and then co-occurrence observations are formed from each path. We keep the number of origins fixed at 5 performance. NICO performance is compared against the frequency method (FM) described in [4]. Figure 1: Edge symmetric differences between inferred networks and the network one would obtain compare with both the sparsest (fewest edges) and clairvoyant best (lowest error) FM solution. izations. Exact E-step calculation is used for observations with N is used for longer paths. The longest observation in our data has N initializations. We plot FM performance for two schemes; one based on choosing the sparsest FM with lowest error. NICO consistently outperforms even the clairvoyant best FM solution. production of SAPK/JNK or NF  X  B proteins. The reconstructed network (combined SAPK/JNK and sible edge ( e.g. ,  X  X APK X  and  X  X F- X  B Signaling X  on http://www.cellsignal.com ). structure from incomplete  X  X o-occurrence X  measurements. Co-occurrences are modelled as samples EM algorithms for calculating maximum likelihood estimates of the Markov chain parameters (ini-Standard results for the EM algorithm guarantee convergence to a local maximum. Although our exact EM algorithm has exponential computational complexity, we provide finite-sample bounds guaranteeing convergence of the Monte Carlo EM variation to a local maximum with high probabil-ity and with only polynomial complexity. Our algorithm is easily extended to compute maximum a Markov transition matrix. Figure 2: Inferred topology of the combined SAPK/JNK and NF  X  B signal transduction pathways. and then network is inferred using NICO.
 Acknowledgments part by the Portuguese Foundation for Science and Technology grant POSC/EEA-SRI/61924/2004, CCR-0350213.
 References
