 User votes are important signals in community question-answering (CQA) systems. Many features of typical CQA systems, e.g. the best answer to a question, status of a user, are dependent on rat-ings or votes cast by the community. In a popular CQA site, Ya-hoo! Answers, users vote for the best answers to their questions and can also thumb up or down each individual answer. Prior work has shown that these votes provide useful predictors for content quality and user expertise, where each vote is usually assumed to carry the same weight as others. In this paper, we analyze a set of possible factors that indicate bias in user voting behavior  X  these factors encompass different gaming behavior, as well as other ec-centricities, e.g., votes to show appreciation of answerers. These observations suggest that votes need to be calibrated before being used to identify good answers or experts. To address this prob-lem, we propose a general machine learning framework to calibrate such votes. Through extensive experiments based on an editorially judged CQA dataset, we show that our supervised learning method of content-agnostic vote calibration can significantly improve the performance of answer ranking and expert ranking.
 Categories and Subject Descriptors: H.2.8 [ Database Manage-ment ]: Database Applications  X  Data Mining.
 Keywords: Reputation, user modeling, crowdsourcing, commu-nity question-answering.
Community question answering (CQA) systems form the crowd-sourced alternative to search engines for providing information. Popular and effective CQA systems such as Yahoo! Answers  X  This work was conducted when all authors were affiliated with Yahoo! answers.yahoo.com provide an environment for people to share knowledge and experi-ence, which complement search engines by allowing questions to be posed in natural language and be answered by human beings through active content (i.e., answers) creation, instead of mining existing Web pages. Since the basis of CQA systems is the wide participation, the hurdle for users to proffer answers is typically minimal. Hence, CQA systems are also prone to spam and other kinds of abuse. Commercial spam itself is a widespread problem, and one that can mostly be tackled by conventional machine learn-ing. Other forms of abuse, e.g., low quality content, willfully wrong answers are difficult for machines to detect, and again crowdsourc-ing quality content identification is an engaging, scalable way of ensuring that high quality questions and answers are identified and the creators appropriately rewarded. Different CQA systems have thus implemented different kinds of voting mechanisms coupled with user reputation systems in order to surface high quality con-tent and discourage undesired behavior.

In this paper, we focus on votes in CQA sites. Take Yahoo! An-swers for example, which is heavily dependent on the best answer voting scheme. An asker (user who asks a question) can vote for the best answer to his/her question. If the asker does vote, the best answer is declared and the question is labeled as resolved. If the asker does not vote for the best answer within certain period of time, other users in the community can vote for the best answer to that question. The answer that receives the largest number of best-answer votes within a certain period of time is then declared as the best answer to this question, and the question is declared re-solved. Other than best-answer votes, users can also demonstrate their quality or opinion preferences by casting thumb-up or thumb-down votes on each individual answer.

Although such voting mechanisms are designed with the intu-ition that they should be able to identify high quality answers and the corresponding answerers, there has been little study on the ac-tual effectiveness of these systems in achieving this goal. On the other hand, many studies [1, 6, 15, 3, 8, 17, 10] have treated the user-voted best answers as the ground truth source of high quality answers and have developed models to predict whether an answer would be voted as the best answer based on features extracted from the answer, past activities of the answerer, etc. As a counter-point, other studies, e.g., [14, 9], report best answers not to be entirely high quality ones. In fact, as part of this work, we also observe that user-voted best answers do not always have high quality, pos-sibly because of bias in users X  voting behavior. A few examples of potential bias are:  X  Users may vote more positively for their friends X  answers.  X  Users may use votes to show their appreciation to answerers  X  Users who are trying to game the system in order to obtain  X  For questions about opinions, users tend to vote for the an-Thus, instead of treating users X  best-answer votes as the ground truth for answer quality, we ask trained human editors to judge answers based on a set of well-defined guidelines. Our first ob-servation is that raw user votes have low correlation with editorial judgment. Based on this finding, we ask  X  is it possible to calibrate the votes to mitigate the effects of potential bias, in order to in-crease the correlation between votes and editorially judged answer quality?  X  Contributions: We make the following contributions.  X  We propose the novel problem of vote calibration in CQA  X  In Section 5, based on exploratory data analysis we identify a  X  We develop a novel model for vote calibration based on super- X  In Section 7, we experimentally show that the proposed vote We note that since our focus is on vote calibration, we take a content-agnostic approach (i.e., no use of features extracted from answer content) similar to [12]. This makes our model applicable to votes on other types of content. Also, although we use Yahoo! Answers as the example CQA system throughout the paper, our model can be easily applied to other CQA systems. In fact we believe that our approach has the following useful properties  X  (i) it is robust to vote gaming, having incorporated a variety of such gaming behavioral features, and (ii) it provides a general framework for calibrating user votes in general crowdsourced rating platforms.
Although there have been many studies on CQA, to our knowl-edge, the problem of how to calibrate user votes to improve the correlation between votes and answer quality has not been system-atically studied before. The related work can broadly be catego-rized into three threads.
 Predicting user-voted best answers: There is a large body of work on building models to predict user-voted best answers (e.g., [1, 6, 15, 3, 8, 17, 10]). The assumption behind this line of work is that quality judgments of answers are expensive to obtain and it seems to be reasonable to use the readily available user-voted best answers as the target to predict. Different from this line of work, we do not try to predict user-voted best answers, but show that user-voted best answers may not be high quality ones because of potential bias in users X  voting behavior. Our focus is on how to calibrate votes to minimize the discrepancy between votes and quality.
 Predicting editorial judgments: Along with [2, 7, 16, 9, 14, 12], we define answer quality by a set of guidelines and obtain the ground-truth quality scores through an editorial process. For exam-ple, Agichtein et al. [2] used features extracted from text, user in-teraction graphs and votes to predict editorial judgments. Suryanto et al. [16] developed a number of graph-based methods to esti-mate user expertise, and evaluated these methods using editorially judged quality and relevance. Sakai et al. [9, 14] noticed bias in users X  voting behavior that can cause user-voted best answers to be unsuitable for model evaluation, and proposed a number of evalu-ation methods based on editorial judgments. Similar to these stud-ies, we also want to predict editorially labeled quality scores. Al-though user votes have commonly been used as features, calibration of each individual vote has not be studied.
 Content-agnostic user expertise estimation: User expertise score estimation is an important use case of vote calibration. In the Section 7, we will show that using a simple average of the cal-ibrated votes of a user as his/her expertise score can outperform the best methods reported in Liu et al. [12], where a variety of methods, such as PageRank-based method, HITS-based methods, competition-based methods, are empirically compared based on ed-itorial judgments. Earlier work on user expertise is described in the following. Zhang et al. [18] proposed a PageRank-style algorithm for online forums. Jurczyk et al. [11] leveraged the HITS algorithm. Pal et al. [13] exploited the observation that experts usually answer questions that have not yet received good answers. Bouguessa et al. [4] developed a method to identify the number of users to be considered as experts based on a mixture of two Gamma distribu-tions.
Our dataset consists of editorial data and voting data.
To determine the quality of an answer, we designed a set of guidelines together with a set of carefully selected examples for each quality grade and ask a human editor to judge it. We ran-domly sampled around 10,000 resolved questions in five popular categories (4.2K from each of  X  X ports X  and  X  X ntertainment and Music X , 1.2k from each of  X  X usiness and Finance X  and  X  X onsumer Electronics X , and 500 from  X  X ews and Events X ) from US Yahoo! Answers and, for each question, sampled three answers. These questions were selected by a diffusion process first starting with a random seed of  X  X ctive users  X  those who have answered at least 10 questions and provided 5 best answers; we then sampled re-solved questions that these users answered, and then again from all answers to that question. For construction of features when model-ing, we however looked at our entire dataset.

When judging an answer, an editor first read the question and answer, and then gave a quality grade to the answer according to a pre-determined set of editorial guidelines. In the interest of space, we only give a high-level description of the guidelines.  X  Excellent : The answer provides a significant amount of useful  X  Good : The answer provides a partial but not entirely persua- X  Fa i r : The answer fails to add much value to the knowledge  X  Bad : The answer is either abusive, not germane to the question Each answer was judged by one editor only. We ignored all the non-English questions and answers and also ignored all the answers that the editors could not make a confident judgement on. In total, we obtain 21,525 editorially judged answers on 7,372 questions (note that some questions do not have three answers).
 User-voted best answers vs. quality: Table 1 shows the distribu-tion of editorial quality grades for three types of answers: ABA de-notes the asker-voted best answer, while CBA denotes the community-voted best answer through the majority rule. On Yahoo! Answers, CBA votes happen only when the asker of a question does not pick the best answer within certain period of time. Non-Best denote the answers that are not best answers. Notice that these three types of answers are disjoint. Each row of Table 1 sums up to one. The numbers of answers of the three types and the four editorial grades are reported in the last column and row. We make the following observations.  X  The distribution of editorial grades for best answers are not  X  A significant percentage (&gt; 70%) of best answers are not even  X  Many non-best answers are actually good or excellent. This is Numeric quality scores: To build our vote calibration model, we give a numeric score to each editorial grade: Excellent = 1, Good = 0.5, Fair = 0, Bad = -0.5. We note that slight changes of assign-ments of numeric scores to editorial grades do not make a signif-icant difference for the results as long as the order is preserved. They also do not affect the evaluation metrics in any way, since we use ranking metrics.
We collect the voting data in the following manner. We start with the set of answerers of the editorially judged answers and collect all of the resolved questions that were answered by this set of answer-ers in a one-year period. We then collect all of the answers to this set of questions, together with all of the votes on these answers. As a result, our voting data includes 1.3M questions, 7.0M answers, 0.5M asker best answer votes, 2.1M community best answer votes and 9.1M thumb up/down votes.
 Uniqueness of our data: Voters X  identities are important to cal-ibrate their votes. To our knowledge, no public CQA dataset re-leases the identities of the voters. Thus, we were not able to eval-uate our methods based on public datasets. Instead, we collect our own data, where each vote is associated with the anonymized IDs of the voter and recipient. Due to user confidentiality agreements, our data is not publicly available.
We present our vote calibration model in this section. Let z { +1 ,  X  1 } denote the value of vote k . The basic idea is simple. We associate each vote k with an importance weight w k so that the calibrated value z k w k of a vote would have higher correlation with answer quality. In order to determine these importance weights, we define a set of features to capture potential bias in users X  vot-ing behavior. Let x k denote the feature vector of vote k . Then, we predict importance weight w k using its corresponding features; i.e., w k = f ( x k  X  ) ,where  X  is a vector of regression coefficients, is the inner product of the two column vectors ( x k is the transpose of x k ), and f (  X  ) is a monotone function to calibrate the value (for example, to constrain w k to be between 0 and 1). The param-eter that needs to be determined is  X  . Based on a set of answers labeled with ground-truth quality scores and the votes on these an-swers, we estimate  X  by minimizing the error of predicting quality scores using the calibrated votes.

In the rest of this section, we first introduce the notations and describe how to generate +1 and  X  1 vote values from the three kinds of votes on Yahoo! Answers, then explain how to predict quality scores using calibrated votes, and finally present a training algorithm for estimating the model parameters. We u s e q to denote a question ID and u to denote an answerer ID. Since each user can provide at most one answer to each question in Yahoo! Answers, ( q,u ) form the ID of the answer to q provided by answerer u .Weuse v to denote a voter ID and t to denote a vote type. In Yahoo! Answers, there are three types of votes:  X  Asker votes: These are the best answer votes by the asker of  X  CBA votes: These are the best answer votes by users in the  X  Thumb votes: These are the thumb-up and thumb-down votes Since a user can provide at most one vote for each answer and vote type, we use z ( t ) quv  X  X  +1 ,  X  1 } to denote the value of the type-t vote that voter v gives the answer provided by answerer u to ques-tion q .Let x quv denote the feature vector associated with the vote that voter v gives answer ( q,u ) . Notice that we did not add a su-perscript t on x quv to reduce notational cumbersome; our method can easily handle features defined specifically for a particular vote type. Let y qu denote the ground-truth numeric quality score of an-swer ( q,u ) . We discuss how we convert editorial grades into nu-meric scores in Section 3.1. Finally, we use V ( t ) qu to denote the set of all the voters who cast type-t vote on answer ( q,u ) ,and denote the set of questions to which answerer u provides answers that receive type-t votes.
In this paper, we focus on binary votes. Extension to numeric ratings is future work. We generate binary votes for the three vote types in the following way.  X  Asker votes: If the asker of a question votes for an answer  X  CBA votes: Similar to asker votes, if a voter votes for an  X  Thumb votes: If a voter thumbs up an answer, then the an-
We now describe how to predict the quality scores using cali-brated votes. Intuitively, we predict the quality score y swer by a weighted sum of (1) the average vote value of the answer ( q,u ) and (2) the average vote value of the answerer u . Because there are multiple types of votes, we compute average vote value for each type separately and then do a weighted average, where the weight on each type will be learned from a training dataset. Calibrated vote value: Using the notations defined in Section 4.1, the value of calibrated type-t vote that voter v gives answer ( q,u ) x quv is the feature vector associated with this vote and tor of regression coefficients for type t . Notice that we have a type-specific  X  t , which gives us the flexibility to calibrate votes of dif-ferent types differently. Although f (  X  ) can be any differentiable monotone function, for concreteness, in this paper we use the sig-moid function: f ( x )= 1 1+ e  X  x . For comparison purposes, we also consider f (  X  ) as the identity function; i.e., f ( x )= x . We call the votes calibrated through the sigmoid function sigmoid-calibrated votes and call the votes calibrated through the identity function linearly-calibrated votes .

It is instructive to compare sigmoid calibration with linear cali-bration based on their definitions.  X  Sigmoid calibration is based on a nonlinear function of the  X  The range of the sigmoid function is between 0 and 1. Thus, In Section 7, we will compare these two kinds of calibration exper-imentally and show the benefit of using the sigmoid function. Average vote value of an answer: When there are multiple votes on an answer, we use the average calibrated vote value on the an-swer to predict its quality. The choice of average instead of the sum is prompted by the fact that the total number of votes received is of-ten biased severely by the age of the answer, how prominently it is displayed etc. To make this average stable, we add some number  X  of  X  X seudo votes X  to the average. Specifically, using the nota-tions defined in Section 4.1, the average type-t vote value of answer ( q,u ) is computed as where  X  t and  X  t are two tuning parameters. In this paper, we set  X  =1 and  X  t = the average uncalibrated vote value over all type-t votes, which is computed by the sum of all type-t uncalibrated vote values divided by the total number of type-t votes in the system. Notice that  X  t is computed based on a large number of votes and should be a quite stable constant. If an answer receives no vote, its AnsValue would be exactly  X  t . Thus,  X  t can be thought of as the  X  X rior mean X  of AnsValue . If an answer receives a small number of votes, its AnsValue would still be close to  X  t , where how close they are depends on  X  t  X  X helarger  X  t is, the closer they are. However, if an answer receives many votes, then its AnsValue would not be influenced much by  X  t .
 Average vote value of an answerer/user: When an answer re-ceives no vote or a small number of votes, we can also use the average value of the votes received by the answerer to predict the quality of the answer. Similar to Equation 1, the average type-t vote value of user u is computed as
UsrValue ( t ) u (  X  t )= Notice that this formula is almost the same as Equation 1, except that here we sum over all of the type-t votes received by answerer u in the numerator and compute the total number of type-t votes received by answerer u in the denominator.
 Quality prediction function: Given the answer-level and user-level average vote values of all types on an answer, its quality is predicted by a weighted sum of these average vote values. Specifi-cally, we predict the quality score y qu of answer ( q,u ) by where b is a bias term,  X  = {  X  t, 0 , X  t, 1 }  X  t consists of the weights of different components, and  X  = {  X  t }  X  t consists of all of the regression-coefficient vectors. Since answer quality is predicted by aggregating calibrated votes, we call this model calibrated vote aggregation model. When sigmoid calibration is used, we call it sigmoid-calibrated vote aggregation (SVA) model. When linear calibration is used, we call it linearly-calibrated vote aggregation (LVA) model. Notice that b ,  X  and  X  are the model parameters that need to be learned from a training dataset.
 User expertise: To estimate the expertise score of a user, we sim-ply use the average calibrated vote values of that user. Specifi-cally, the expertise score of user u is Expertise u ( b,  X  model, we do not specifically discuss this special case.
Given a training dataset consisting a set of answers together with ground-truth quality scores and votes on them, we determine the model parameters  X  =( b,  X  ,  X  ) by minimizing the following loss function (
 X  )= 1 where  X  2 denotes the sum of squares of individual elements in  X  , which serves as a regularizer to prevent potential overfitting.  X  ,  X  2 and  X  3 are the regularization weights of each regularizer. We minimize this loss function using gradient descent.
 Gradients: The formulas of the gradients are as follows. Let e qu (  X  )= y qu  X  Score qu (  X  ) denote the difference between the ground-truth quality score and the predicted quality score. where t AnsValue t UsrValue If f (  X  ) is the sigmoid function in sigmoid calibration, its derivative f ( x ) is f ( x )(1  X  f ( x )) . For linear calibration, f ( x )=1 always. However, it can be shown that linear calibration can be transformed into a regular linear regression problem. Thus, we solve linear cal-ibration by using a standard linear regression package.
 Loss minimization: There are many gradient descent algorithms, which can be easily applied to minimize our loss function. In our implementation, we use L-BFGS [5].
Before we define the features to be used in our calibration model, we discuss a range of potential indicators of voting bias in this section. In the interest of space, we mainly use community best-answer (CBA) votes to show our analysis. Many of the insights also apply to other types of votes.
 Characteristics of CBA votes: First, the importance of CBA votes is seen in Figure 1, which plots the fraction of questions resolved by CBA votes as a function of the number of answers to the question. On average around 61% of the total number of questions in our corpus get resolved by CBA votes, and this fraction reaches its peak for questions with smaller number of answers (being near 70% for questions with less than three answers). One possible explanation for the U-shaped curve of Figure 1 could be that the number of answers being too small indicates that the question is probably not interesting, likely posed by a casual user who is less inclined to give a best answer vote herself  X  and thus the community has to step in.
Furthermore, CBA votes are rarely unanimous. Figure 2 shows how many answers to a question on average receive at least one CBA vote as a function of the number of answers to the question. The y-axis shows the number of answers that receive at least one CBA vote, averaged over all questions that have the same number of answers. There is a clear linear trend up for questions with answers. Beyond that, the trend is almost flat and noisier. Spar-sity of data is likely the main reason behind the noise. The flatness of the trend can be possibly explained by the fact that a commu-nity member looking to vote an answer as CBA is likely only to look at the top few answers in the page; some answers can also be submitted after the CBA deadline passes.

In the following, we discuss our observations about some po-tential sources of bias that we see in our dataset. For each of these sources, it is important to note that we are not claiming that this bias immediately implies that the corresponding CBA votes provide no signal to predict answer quality. Our goal here is to identify a vari-ety of indicators of potential bias, so that our model can use them to calibrate votes.
 Self Voting: Figure 3 shows the extent of self voting (where a user votes for his/her own answers) in our dataset. Self votes contribute to 33% of the total CBA votes, and when we consider users who cast at least 20 votes, the percentage of self votes goes even above 40%. Self votes are certainly unlikely to represent any quality judg-ment, they can at most be taken as a signal of the user X  X  familiarity with the voting mechanism of the CQA site, which in itself could provide a useful signal.
 Vote Spread and Reciprocity: We d e fi n e in/out spread of a user u as the ratio between the number of distinct users who cast/receive a vote to/from u and the number of votes that go into/out of u ,which measures the spread of u  X  X  incoming/outgoing votes over differ-ent users. This measure could capture potential biased behavior in which users are either receiving most of their votes from a small number of users, or are themselves casting the votes in a concen-trated manner. This phenomenon might be due to gaming, a user X  X  Figure 3: Number of self CBA votes by a user, along with fitted curve showing the trend. Figure 4: (a) In Spread and (b) Out Spread of CBA votes. The rightendpointofthe x -axis includes all users with &gt; 20 votes narrow interests, or a scenario where only a small number users provide high quality answers. We report out spread and in spread in the Figure 4. The red curve together with the left hand y-axis represents in/out spread as a function of the number of votes that go into/out of a user. The green curve with the right hand y-axis shows, in log-scale, the number of users who received/cast a cer-tain number of CBA votes. The spread decreases with increasing number of CBA votes cast/received. It is lowest on average for users who have cast/received more than 20 votes  X  which could indicate gaming by a small number of active users.

One interesting user-user interaction, almost a  X  X uid pro quo X  action, is when two users cast CBA votes on each other thereby creating a reciprocal relation between the two. We call a vote a re-ciprocal vote if the recipient also vote back for an answer authored by the voter. We define in/out reciprocity of user u as the fraction of incoming/outgoing votes that are reciprocal. Figure 5 shows in/out reciprocity as a function of the total number of incoming and outgo-ing CBA votes of a user. Both the curves are concave, and stabilize around 0.46 for users with large number of votes  X  again indicating potential gaming by heavy users. The number of users involved in such a reciprocal voting pattern is also seen in Figure 6  X  over 20K users in our dataset are involved in at least 10 reciprocal votes. Considering this pattern as a feature when calibrating CBA votes certainly seems advisable.
 Other Interaction Bias: In the remaining part of the section, we investigate whether CBA votes are independent of previous inter-action between the voter and recipient. Our strategy is to use a hypothesis testing framework to analyze whether the null hypoth-esis  X  that CBA voting is independent of previous interaction  X  holds true individually for different interaction types. The different types of interaction we consider are the following: Whether one user answered another X  X  question, whether they thumbed up/down each other, and whether they gave CBA/ABA votes to each other (recall that ABA denote asker best-answer). We represent each of these by a formal relation as follows. Let t be a time point. ANS t denotes the set of all pairs ( u, v ) of users such that u an-swered a question posed by v before time t . TU t denotes the set Figure 5: (a) Out Reciprocity and (b) In Reciprocity of CBA votes.
Figure 6: Number of users having at least x reciprocal votes of the pairs ( u, v ) such that u thumbed up an answer by v be-fore time t . TD t , CBA t , ABA t are similarly defined based on thumb-down votes, CBA votes and ABA votes that user u gives user v . Our goal is to investigate whether a CBA vote that v gives to u at time t +1 is independent of whether ( u, v )  X  R  X  X  ANS , TU , TD , CBA , ABA } . Before describing our obser-vation, we briefly describe our methodology.
 Chi-squared Statistic: Let CBA ( u ) denote the set of recipients of the CBA votes cast by user u .Let R be any of the relation described above. Then, the chi-squared statistic for measuring independence of R and CBA ( u ) can be computed based on the following counts, show in Table 2. Let N = a + b + c + d . Then, the chi-squared statistic X 2 ( CBA,R ) is defined as For measuring the effect of previous interaction on a CBA vote, we will compute this statistic based on the dynamic interaction stream. We go through the list of user actions performed in increasing or-der of time, and maintain the current set of user pairs who are in relation R . At any point, when we are processing a CBA vote, we use the relation R constructed up until this point to find which of the above cells we should increment the counts of. The X 2 is computed after the entire stream is processed  X  this is then used for the test.
 Randomization Tests: In order to get the confidence interval of the TU 66941.65 35718.70 35263.40 261.76 TD 5830.44 7369.88 7046.85 -18.81 CBA 55515.30 29976.85 29360.74 168.97 ANS 13391.79 8677.06 8496.51 103.60 ABA 9616.96 5921.43 5755.67 82.56 Table 3: Effect of previous interaction (row indicates type) on CBA. above X 2 value under the null hypothesis, we resort to randomiza-tion of the data. The way the data is randomized is as follows  X  for each question, we first decide on a list of candidate answers who are eligible to receive CBA votes. We select these to be the answerers who have received at least one CBA vote for this ques-tion. The randomized version of the X 2 statistic is then computed by the following process  X  we again traverse the list of interactions in increasing order of time. Each CBA vote is now replaced by a random choice from the list of candidates to this question  X  the updates to the counts in Table 2 are now made according to this random choice. Finally, the X 2 statistic is computed at the end. Effect of Interaction: The above randomization was performed 20 times. In Table 3, we present the statistics obtained. The four columns present the X 2 statistic obtained on our data, the maxi-mum and minimum over the 20 randomizations, and the normal-ized value of the statistic based on the empirical mean and vari-ance. Based on the 20 randomizations, the null hypothesis of the CBA being independent of the relation was rejected with probabil-ity 0.1 for each of the relation TU , TD , CBA , ANS and ABA .Note that this only means that there is some correlation between CBA votes and past interaction of any of the above types  X  it does not es-tablish a direct bias. Such correlation could, for instance, occur if active users are also experts. Nevertheless, this result suggest past interaction could be useful features for vote calibration.
Based on the exploratory analysis, in this section we define the features that we use to calibrate votes. We consider two types of features: voter features, which try to capture a user X  X  voting behav-ior, and relation features, which try to capture potential bias asso-ciated with the relation between the voter and the answerer (vote recipient).
 Definitions: We say that user v gives user u avoteif v gives a vote to an answer authored by u . In this relation, v is the voter and u is the answerer. A vote from v to u is a reciprocal vote if u has given v a vote before. A vote from user v on an answer is a majority vote if the answer receives at least one additional vote and has the largest number of votes among other answers to the same question. Notations: We will use the following notations. Voter features: We define the following features for voter v ,for each vote type separately (when appropriate). Relation features: We define the following features for each pair ( v,u ) of users, where v is the voter and u is the answerer, for each vote type separately. Feature transformation: For each of the features C that are counts, we consider log(1 + C ) as an additional feature. For ratio features R , we also include a quadratic term R 2 .
 Intercept: Same as in linear regression, we always include a fea-ture, called intercept, which value is always 1.
With the above features, we evaluate our methods based on our editorially labeled dataset described in Section 3.
We evaluate our models at two different levels:  X  User-level expert ranking : User expertise score estimation is  X  Answer ranking : Our models can be used to predict the qual-Note that our focus is on vote calibration. Thus, we take a content-agnostic approach to the above two ranking problems. Since the most related work [12] on content-agnostic methods is in the ex-pert ranking setting, this forms our main comparison setting. Next, we compare variants of our methods based on both user-level and answer-level score estimation.

An ideal way to evaluate user-level scores is to collect a ground-truth data which has the expertise grade for each user. As noted by a few research studies, such as [12], obtaining such a dataset by human judgment is very costly because all the activities of a user need to be examined in order to assess her expertise. An alternative approach is to use certain heuristics to define experts (e.g., users with the  X  X op contributor X  badge on Yahoo! Answers), which are also prone to have biases or be abused. Thus in our paper, we take an indirect approach similar to [12] in using the editorially judged (question, answer) pairs to evaluate user-level expertise. The evalu-ation hypothesis is that the answers provided by a user with higher expertise should have higher quality. Thus, we rank all the answers to a question using the user-level expertise scores of the answers and use the standard ranking metrics for evaluation.
 Data: We try to predict the editorial quality scores on answers us-ing either user-level or answer-level scores. We obtain 21,525 ed-itorially judged answers as described in Section 3.1. All of the methods to be compared have access to the voting data described in Section 3.2.
 Cross-validation: We use 10-fold cross validation to evaluate our models. We randomly split users into 10 groups. We use the edito-rially judged answers authored by users in 9 groups to train a model and then use this model to predict the user-level scores of the users in the remaining group and the answer-level scores for the answers they authored. This process is repeated 10 times to obtain the pre-dicted scores for all the users and answers. It is important to note that the score of each user is predicted by a model that does not use any label information about that user.
 Evaluation Metrics: In Yahoo! Answers, a resolved question has 3.6 answers on average. When creating our editorial dataset, we sampled at most 3 answers for each question. Thus, the ranked list per question is short in our evaluation and the ability to dis-tinguish the performance of different methods is limited. In order to overcome this limitation, we proposed three types of evaluation schemes as follows.  X  Question Level : For each question, we rank the  X   X  Global Level : In the global level evaluation, we pool all the  X  Category Level : Between the above two levels, we have the Methods for Comparison: The variants of our methods to be com-pared include (1) SVA.Full : SVA model with both voter and rela-tion features; (2) SVA.Voter : SVA model with only voter features; (3) LVA.Full : LVA model with both voter and relation features; (4) LVA.Voter : LVA model with only voter features; (5) NoCalib : SVA model that only uses the intercept feature (which value is al-ways one, thus no calibration for individual votes; however, differ-ent vote types are weighted differently). All these methods can be applied for either answer ranking or user-level expert ranking. We compare with the following baseline methods:  X  Smoothed Best Answer Rate ( BAR ) estimates the probability  X  Smoothed Competition Win Rate ( CWR ) is based on the com- X  SVM is the model proposed in [12]. It computes a single score  X  BAR+BA : All the above methods are for user-level scores,
We compare our methods with all the baselines in Table 4. In this table, we show the three types of evaluation setting for both user-level expert ranking and answer-level ranking. All the parameters,  X  for BAR and CWR, C for SVM, and a coefficient for BAR+BA, are tuned and we report their optimal results. In our methods, all the regularization parameters are set to 1.
 Significance test: We use the paired t-test to assess the significance of the performance difference in NDCG@k, P@10 and MAP be-tween two methods. Each query (i.e., question or category) gives a point in the significance test. For Kendall X  X  rank correlation, each cant improvement over NoCalib (p-value &lt; 0.05).
 + 0.701 + 0.690 0.701 + 0.712 +  X  + 0.765 + 0.760 0.766 + 0.769 + + 0.811 + 0.807 0.811 + 0.814 +  X  + 0.407 + 0.365 0.475 + 0.501 + + 0.396 +  X  0.381 0.425 + 0.444 +  X  method generates a single number which makes t-test inapplicable. In this case, we use bootstrap sampling to construct the distribution of Kendall X  X  correlation and assess significance based on that. We analyze our results from the following perspectives: Supervised vs Unsupervised: For either expert ranking or an-swer ranking methods, we have supervised methods (NoCalib and SVA.Full) and unsupervised methods (all others). In Table 4, the best results are highlighted. SVA.Full outperforms all other meth-ods by a large margin. For example, for expert ranking, SVA.Full improves over BAR by 3.8% on NDCG@1, 8.0% on MAP and 38% on Kendall; for answer ranking, SVA.Full improve over BAR+BA by 3.2% on NDCG@1, 16% on MAP and 37% on Kendall. All these improvement are statistically significant. Among the unsu-pervised methods, BAR is slightly worse than SVM and CWR on NDCG metrics but consistently better on MAP and Kendall met-rics. This shows that BAR is a robust baseline to for expert rank-ing. Also, NoCalib (which uses three types of votes) consistently outperforms BAR. This shows that in CQA systems, considering all types of voting information is helpful, which has been unfor-tunately ignored in previous work. Our methods are effective to combine all the votes to improve the utility of CQA.
 Effect of Calibration: We analyze the effect of calibration in Ta-ble 4 by comparing NoCalib and SVA.Full. From this table, we can see that SVA.Full is significantly better than NoCalib on most of the metrics, especially for answer ranking. For example, the improvement is 1.5% for NDCG@1 and 4.5% on MAP for the an-swer ranking. All these improvement are statistically significant at p-value &lt; 0.05. Together with Section 5, this result confirms the existence of bias in the current voting data and shows that vote cal-ibration using our methods is effective.
In this section, we compare different models of vote calibration and different sets of features. In Figure 7(a), we compare variants of SVA and LVA together with the NoCalib using the category-level metrics on two sets of users: a set with all users and a set with heavy users who have more than 50 answers in our voting data (there are 11 such users). We again see SVA and LVA outperform NoCalib. Furthermore, we have the following observations: SVA vs LVA: From both figures, we can that SVA (SVA.Full and SVA.Voter) is consistently better than LVA (LVA.Full and LVA.Voter) on both expert ranking and answer ranking. The difference is more significant in the answer ranking setting. This shows that proper normalization is important for the vote calibration in our models. Full vs Voter: By comparing different set of features, full and voter, we see that the models with full features are also better than those with only voter features. This shows that the relation features are useful in vote calibration.
We conducted a stratified study on heavy users by selecting them according to their level of activities in our dataset. Specifically, we set a threshold t and select only those users who have at least t an-swers in our voting data set. We vary t from 2 to 50 and plot the results in Figure 8. In this figure, the larger t is, the more active the set of users are. Clearly, both figures show that accuracy increases as user activity level increases. This makes intuitive sense because we have more information about heavy users and thus their exper-tise scores and the quality of the answers they provide can be better estimated. On both expert ranking and answer ranking, our models are consistently better than NoCalib and the relative order of dif-ferent methods stay the same. Furthermore, for expert ranking, we see a larger margin between models with calibration and NoCalib. This suggests that calibration is more important for heavy users. Feature importance: We now investigate the importance of dif-ferent features defined in Section 6. To assess the importance of a set of features, we use that set of features alone to build our vote calibration model and compute the Kendall X  X  correlation as the im-portance score of that set of features. The top 5 sets of features in the order of their importance are: Majority vote, vote spread, self vote, vote reciprocity and voting probability.
 Tuning parameters: Recall that our model has two sets of tuning parameters  X  t and  X  k . To prevent overfitting, all the above results are based on simply setting  X  t =1 and  X  k =1 without tuning. Here, we investigate the sensitivity of SVA.Full to these tuning pa-rameters. We use Kendall X  X  correlation as the evaluation metric. As can be seen, our model is not sensitive to tuning parameter set-tings.
In this paper, we introduce vote calibration to CQA systems. By analyzing potential bias in users X  voting behavior we propose a set of features to capture such bias. Using supervised models we show that our calibrated models are better than the non-calibrated ver-sions on both user expertise and answer quality estimation.
