
This paper studies the problem of categorical data clustering, especially for transactional data characterized by high dimensionality and large volume. Starting from a heuristic method develop a novel algorithm -CLOPE, which is very fast and scalable, while being quite effective. We demonstrate the performance of our algorithm on two real world datasets, and compare CLOPE with the state-of-art algorithms, data mining, clustering, categorical data, scalability 
Clustering is an important data mining technique that groups together similar data records [12, 14, 4, 1]. Recently, more 13], where records are made up of non-numerical attributes. 
Transactional data, like market basket data and web usage data, clustering of transactional data has many potential applications in retail industry, e-commerce intelligence, etc. extremely difficult because of the high dimensionality, sparsity, and huge volumes often characterizing these databases. Distance-based approaches like k-means [11] and CLARANS [12] are effective for low dimensional numerical data. Their performances on high dimensional categorical data, however, are often unsatisfactory [7]. Hierarchical clustering methods like ROCK [7] have been demonstrated to be quite effective in categorical data clustering, but they are naturally inefficient in processing large databases. requires prior specific permission and/or a fee. SIGKDD '02, July 23-26, 2002, Edmonton, Alberta, Canada. Copyright 2002 ACM 1-58113-567-X/02/0007...$5.00. 
For simplicity, transaction (apple, banana) is abbreviated to etc. For this small database, we want to compare the following 
H=2.0, W=4 H=1.67, W=3 H=1.67, W=3 H=I.6, W=5 
H/W=0.5), but the one for cluster {acd, de, def} the same number of blocks (H=I.6, H/W=0.32). Clearly, clustering (1) is better since we prefer more overlapping among transactions in the same cluster. From the above example, we can see that a larger height-to-width ratio of the histogram means better intra-cluster similarity. We algorithm and define the global criterion function using the geometric properties of the cluster histograms. We call this new algorithm CLOPE -Clustering with sLOPE. While being quite effective, CLOPE is very fast and scalable when clustering large transactional databases with high dimensions, such as market-basket data and web server logs. the categorical clustering problem more formally and presents our criterion function. Section 3 details the CLOPE algorithm and its implementation issues. In Section 4, experiment results of CLOPE and LargeItem on real life datasets are compared. After some discussion of related works in Section 5, Section 6 concludes the paper. Notations Throughout this paper, we use the following called a cluster. Unless otherwise stated, n, m, k are used respectively for the number of transactions, the number of items, and the number of clusters. A good clustering should group together similar transactions. Most clustering algorithms define some criterion functions and optimize them, maximizing the intra-cluster similarity and the inter-cluster dissimilarity. The criterion function can be defined locally or pair-wise similarity between transactions. This has been widely measures for categorical data are the Jaccard coefficient number of common items between two transactions [ 10]. However, for large databases, the computational costs of these local approaches are heavy, compared with the global approaches. Pioneered by Wang et.al, in their Largeltem algorithm [13], global In global approaches, no pair-wise similarity measures between individual transactions are required. Clustering quality is measured small items in the clustering. Since the computations of these global approaches are very efficient for the clustering of large categorical databases. Compared with Largeltem, CLOPE uses a much simpler but effective global metric for transactional data clustering. A better with their respective occurrences, that is, the number of decrreasingly ordered by their occurrences, and occurrence as the Y-axis. We define the size S(C) and width W(C) of a cluster C below: The height of a cluster is defined as H(C)=S(C)/W(C). We will important or can be inferred from context. 1 below. Please note that, geometrically in Figure 2, the histogram size S. Figure 2. The detailed histogram of cluster {acd, de, def}. It's straightforward that a larger height means a heavier overlap of the two clusterings are the same. However, to define our criterion function, height alone is not enough. Take a very simple database {abc, def}. There is no the clustering {{abc}, {de.[}} have the same height 1. Another choice works better for this example. We can use gradient G(C) = gradient of cluster {abc, deJ}. following as a straightforward definition of the criterion function. power r instead of 2 as follows. within the same cluster must share a large portion of common items. Otherwise, separating these transactions into different clustering for database (abc, abcd, bcde,cde}: (1) { {abc, abcd, bcde, cde}} and (2) {{abc, abcd}, {bcde, cde}}. In order to (2), 4r 4' 5" , must be greater than that of (1), This means that a repulsion greater than ln(14/7)/ln(5/4) = 3.106 must be used. On the contrary, small repulsion can be used to group sparse databases. Transactions sharing few common items may be put in = 1.357. Now we state our problem of clustering transactional data below. Problem definition Given D and r, find a clustering C that maximize Profitr(C). 2: read the next transaction (t, unknown); 4: write (t, i) back to database; Like most partition-based clustering approaches, we approximate our criterion function is defined globally, only with easily computable metrics like size and width, the execution speed is much faster than the local ones. Our implementation requires a first scan of the database to build transactions sharing no common item can be put in the same cluster. that, a few more scans are required to refine the clustering and made in a previous scan, the algorithm will stop, with the final every transaction, indicating the cluster id that the transaction belongs to. The sketch of the algorithm is shown in Figure 3. RAM data structure In the limited RAM space, we keeps only cluster. The information, called cluster features 2, includes the number of transactions N, the number of distinct items (or width) 
W, a hash of (item, occurrence) pairs occ, and a pre-computed the occurrence of item i in cluster C, etc. Remark In fact, CLOPE is quite memory saving, even array representation of the occurrence data is practical for most transactional databases. The total memory required for item occurrences is approximately MxK X 4 bytes using array of 4-byte integers, where M is the number of dimensions, and K the number of clusters. Databases with up to 10k distinct items with a clustering of lk clusters can be fit into a 40M RAM. The computation of profit It is easy to update the cluster feature data When adding or removing a transaction. The computation of profit through cluster features is also the clusters (including an empty one). Although computing the same but much faster judgement. 4: for (i = 0; i &lt; t.ltemCount; i++) change of value --after adding transaction t to cluster C. The following theorem guarantees the correctness of our implementation. Theorem IfDeltaAdd(Ci, t) is the maximum, then putting t to Ci will maximize Profit,. Proof: Observing the profit function, we find that the profits of the formula. Assume that the numerator of the clustering profit before adding t is X. Subtracting the constant X from these new numerators, we get exactly the values returned by the DeltaAdd function. Time and space complexity From Figure 4, we know that the time complexity of DeltaAdd is O(t.ItemCount). Suppose the 2 Named after BIRCH [14]. average length of a transaction is A, the total number of transactions is N, and the maximum number of clusters is K, the the execution speed of CLOPE is affected linearly by the number only one transaction is kept in memory at any time, the space requirement for CLOPE is approximately the memory size of the the maximum number of clusters K. For most transactional databases, it is not a heavy requirement. In this section, we analyze the effectiveness and execution speed of CLOPE with two real-life datasets. For effectiveness, we compare the clustering quality of CLOPE on a labeled dataset (mushroom from the UCI data mining repository) with those of LargeItem [13] and ROCK [7]. For execution speed, we compare CLOPE with LargeItem on a large web log dataset. All the experiments in this Section are carried out on a PIII 450M Linux machine with 128M memory. The mushroom dataset from the UCI machine learning repository (http://www.ics.uci.edu/-,.mlearn/MLRepository.html) has been used by both ROCK and Largeltem for effectiveness tests. It contains 8,124 records with two classes, 4,208 edible mushrooms and 3,916 poisonous mushrooms. By treating the value of each attributes as items of transactions, we converted all the 22 categorical attributes to transactions with 116 distinct items (distinct attribute values). 2480 missing values for the stalk-root attribute are ignored in the transactions. 9000 8000 .~,7000 ~-6000 5000 4000 Figure 5. The result of CLOPE on mushroom. A few of the results are shown in Figure 5. the larger one of the number of edibles and the number of poisonous in every cluster. It has a maximum of 8124, the total number of transactions. The number of clusters should be as few will surely achieve a maximum purity. When r=2.6, the number of clusters is 27, and there is only one clusters with mixed records: 32 poisonous and 48 edibles (purity=8092). When r reaches 3.1, there are 30 clusters with perfect classification (purity=8124). Most of these results require Costa.w (C) = w x Intra + Inter 9000 8000 6000 5000 4000 9000 8000 5000 4000 
Figure 6. The result of Largeltem on mushroom. 
Our experiment results on the mushroom dataset show that with very simple intuition and linear complexity, CLOPE is quite effective. The result of CLOPE on mushroom is better than that of 
Largeltem and close to that of ROCK, which has quadratic complexity to the number of transactions. The comparison with 
Largeltem also shows that the simple idea behind CLOPE works quite well even without any explicit constraint on inter-cluster dissimilarity. Sensitivity to data order We also perform sensitivity test of CLOPE on the order of input data using mushroom. The result in 
CLOPE with randomly ordered mushroom data. The results are of reaching purity=8124 with 45 clusters, at r=3.9. It shows that 
CLOPE is not very sensitive to the order of input data. However, our experiment results on randomly ordered mushroom data show that Largeltem is more sensitive to data order than CLOPE. 
Apart from market basket data, web log data is another typical category of transactional databases. We choose the web log files from http://www.cs.berkeley.edu/logs/ as the dataset for our second experiment and test the scalability as well as performance of CLOPE. We use the web logs of November 2001 and preprocess it with methods proposed in [3]. There are about 7 after non-html 3 entries removed. Among these 2 million entries, 15 minutes, 613,555 sessions are identified. The average session length is 3.34. with w=l) on 10%, 50% and 100% of the sessions respectively. 
The average per-iteration running time is shown in Figure 7. ,~, [ ~ CLOPE r=l.0 Figure 7. The running time of CLOPE and Largeltem on the 
From Figure 7, we can see that the execution time of both CLOPE and Largeltem are linear to the database size. For non-integer repulsion values, CLOPE runs slower for the float point 3 Those non-directory requests having extensions other than ".[s]htm[l]". /~lazzaro/sa/book/simple/index.html, occ=426 /~lazzaro/sa/index.htrnl, occ=332 /~lazzaro/sa, occ=170 /~lazzaro/sa/book/index.html, occ=120 /~lazzaro/sa/video/index.html, occ=26 /~lazzaro/sa/sfman/user/network/index.html, occ=9 /'russell/aima.html, occ=388 /~russell/code/doc/install.htrnl, occ=231 /~russell/code/doc/overview.html, oec= 184 /'-xussell/code/doc/user.html, occ=158 /-.,russell/intro.html, occ=150 /'-'russell/aima-bib.html, oct=61 /,occ=19517 /StudentaClasses, occ=2726 * number aferpage name isthe occurrenceinthe cluster 
The most similar work to CLOPE is Largeltem [13]. However, our experiments show that CLOPE is able to find better clusters, even at a faster speed. Moreover, CLOPE requires only one parameter, repulsion, which gives the user much control over the approximate number of the resulting clusters, with little domain knowledge. 
The minimal support 0 and the weight w of Largeltem are more of the input data. 
Moreover, many works on document clustering are quite related with transactional data clustering. In document clustering, each document is represented as a weighted vector of words in it. 
Clustering is carried out also by optimizing a certain criterion function. However, document clustering tends to assume different weights on words with respect to their frequencies. See [15] for some common approaches in document clustering. 
Also, there are some similarities between transactional data clustering and association analysis [2]. Both of these two popular data mining techniques can reveal some interesting properties of item co-occurrence and relationship in transactional databases. 
Moreover, current approaches [9] for association analysis needs only very few scans of the database. However, there are differences. On the one hand, clustering can give a general overview property of the data, while association analysis only finds the strongest item co-occurrence pattern. On the other hand, transactional data is not enough, and are mostly used as preprocessing phrase for other data mining tasks like association analysis. 
In this paper, a novel algorithm for categorical data clustering the height-to-width ratio of the cluster histogram. The idea is generalized with a repulsion parameter that controls tightness of 
The simple idea behind CLOPE makes it fast, scalable, and memory saving in clustering large, sparse transactional databases with high dimensions. Our experiments show that CLOPE is quite specify explicitly any inter-cluster dissimilarity metric. Moreover, 
CLOPE is not very sensitive to data order, and requires little domain knowledge in controlling the number of clusters. These features make CLOPE a good clustering as well as preprocessing algorithm in mining transactional data like market basket data and web usage data. 
We are grateful to Rajeev Rastogi, Vipin Kumar for providing us thank the providers of the UCI ML Repository and Web log files of http://www.cs.berkeley.edu/. We also wish to thank the authors of [13] for their help. Comments from the three anonymous referees are invaluable for us to prepare the final version. Automatic subspace clustering of high dimensional data for data mining applications. In Proc. SIGMOD'98, Seattle, Washington, June 1998. association rules between sets of items in large databases. In Proc. SIGMOD'93, Washington, D.C., 1993. for mining world wide web browsing patterns. Knowledge and Information Systems, 1(1):5-32, 1999. based algorithm for discovering clusters in large spatial databases with noise. In Proc. KDD'96, Portland, Oregon, 1996. Clustering categorical data using summaries. In Proc. KDD'99, San Diego, CA, 1999. data: an approach based on dynamical systems. In Proc. VLDB'98, New York, NY, 1998. clustering algorithm for categorical attributes. In Proc. ICDE'99, Sydney, Australia 1999. Clustering based on association rule hypergraphs. In Proc. SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 1997. candidate generation. In Proc. SIGMOD '00, Dallas, TX, 2000. categorical data sets in data mining. In Proc. SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 1997. of multivariate observations. In Proc. 5 th Berkeley Symposium on Math. Stat. and Prob., 1967. methods for spatial data mining. In Proc. VLDB'94, Santiago, Chile, 1994. large items. In Proc. CIKM'99, Kansas, Missouri, 1999. efficient data clustering method for very large databases. In Proc SIGMOD'96, Montreal, Canada, 1996. clustering: experiments and analysis. Tech. Report #01-40, Department of Comp. Sci. &amp; Eng., U. Minnesota, 2001. Avaliable as: http://www-users'itlabs'tmm'edu/~karypis/ publications/Papers/Postscript/vscluster.p s 
