 Significant amount of literature is available on compound splitting of long words albeit for non-English languages-es-pecially European. Not surprisingly, there has been not much work for English as it is not a compounding language like some of its European counterparts. However, Internet domain names in general are compound English words, e.g.  X  X ankofamerica.com X . Compound splitting can be effectively employed to extract information from domain names. In this paper, an data-driven learning technique for splitting English compound words is described which among others uses features like normalized frequency, length of parts and n-gram. The splitting F-measure is higher than the pub-lished approaches. We applied this technique on a real life web search application where the queries are mistyped do-main names routed through sources like ISPs and browsers. Relevant and meaningful keywords were extracted out and shown to the user as a value added search option. Results show a very high click-through rate and increased commer-cial value.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval; H.3.1 [ Information Systems ]: Information Stor-age and Retrieval X  content analysis and indexing ; H.3.5 [ Information Systems ]: Online Information Ser-vices; H.2.8 [ Information Systems ]: Database Applica-tions X  data mining Algorithms, Languages, Experimentation, Measurement Domain names, Compound splitting
Decompounding compound words is an interesting and challenging proposition. It has found wide usage in informa-tion retrieval, speech recognition and information extraction applications. The English language is not considered a com-pounding language unlike German, Greek, Finnish, Danish, Norwegian, Korean etc. However, Internet domain names in general are compound English words.

The first thing a user does on the web is to access a do-main name. These domains are typically compound English words e.g.  X  X ankofamerica.com X ,  X  X iscoverychannel.com X ,  X  X heaptickets.com X  etc. One can visit a desired web page by a variety of ways viz. accessing stored bookmarks, typ-ing the actual domain name on the address bar, searching for partial or full domain name in a search engine, follow-ing a link on the current page or copying and pasting a link on the browser from a message. All these actions can lead to certain errors like typing mistakes, miscommunication, se-lecting obsolete bookmarks or page links. In such situations, the DNS resolution for the domain name results in a NX-DOMAIN (non-existent domain) response and the browsers display a  X  X annot display page X  error page. These errors can be captured at the DNS resolution level by either the Internet Service Providers or in certain cases the browsers.
The mistyped or non-existent domain name if redirected to a search provider, the user can be provided with relevant web search results. The application does not aim to provide the correct domain name given the NXDomain. It gives rel-evant search results based on the extracted keywords. This along with value to users, gives the search provider opportu-nity to create revenue through paid clicks on search results. In this paper, we describe an application called Domain Keyword Extraction , which uses compound splitting along with other techniques to pick relevant keywords from mistyped or non-existent domain names. We present an data-driven approach to compound splitting, which is tested against En-glish domain names to yield very high recall, precision and F-measure. We use several features like length of parts, n-gram, stop words and frequency. We did an extensive anal-ysis on the frequency distribution of words in a corpus and derived a frequency normalization function based on word length. Results show dramatic improvement with the nor-malized frequency value.

In the following sections we describe the related work, application, techniques, and finally the experimental results. [11] used compound splitting for web page classification and splog filtering. [9] worked on compound splitting in de-tail for German language. They have used various features like word frequency, parallel text corpus and POS tags. POS tagging is a CPU intensive NLP application which has its own complexities and dependencies. Parallel corpora for dif-ferent languages are also not readily available. [3] also uses parallel corpus in breaking cognates and words found in a translation lexicon. This is relevant from machine trans-lation perspective. [10] describe a method for compound splitting used in speech recognition. [2] gives a SVM model which uses a corpus of annotated compounds in German and [1] show that the model is language independent. [7] uses an unsupervised corpus driven method for compound split-ting to get periphrases for German. The performance of this approach is not impressive. [5] exploits n-gram techniques in compound splitting for machine translation from various European languages to English.

Major work in Asian languages include Korean, Chinese and Japanese. [6] has used specifier-headword model and statistical preference rules to de-compound Korean com-pound nouns. [4] used a matching algorithm with 6 differ-ent heuristic rules to resolve ambiguities in decompounding mandarin Chinese sentences. A significant portion of the DNS resolutions are NXDO-MAINs (non-existent domains), which are mostly mistyped or non-existent domain names. Much of this traffic is routed to search engines through DNS servers of ISPs or through browsers. The web search application receives the mistyped domain name as query. This can be used as a search element and when clicked, the user be presented with search results or further search options. In our case, we extract keywords out of the input using domain sanitization and compound splitting to show them as a valid search option to the user. Searching on it will produce relevant web search results for the users and revenue for search engines in the form of paid-clicks. Figure 1 gives an overview of the architecture.
It should be noted that the user is not provided with the corrected domain name, but with relevant web search results based on extracted keywords. This provides value to users with an opportunity for search providers to generate revenue through paid clicks. We will now talk about the components in detail.
We call the process of removing the prefix and suffix from domain typos as Domain Sanitization. For instance,  X  X ww.bankofamerica.cmo X  X ill be sanitized as X  X ankofamer-ica X . The prefix  X  X ww. X  and the suffix  X .cmo X  are removed. As another example X  X w.ibm.co.uk X  X ill be sanitized to X  X bm X .
First, all the domain typos are aggregated and analyzed for prefix and suffix patterns. A list of prefix strings and another list of suffix patterns are prepared. For the suffix rules, we also use the list of Top Level Domains maintained by IANA. [8]. If there are any special characters present that are not allowed in a URL are also removed. A few examples are listed in Table 1.

It is a method of splitting a compound word into its con-stituent words. The process might seem straight-forward to some, but complications arise when the compound or its sub-parts can be broken into several ways to yield valid words. Figure 2 shows an example to split a compound word X  X ome-store X .
 Among the many possible splits, the two interesting splits can be  X  X ome store X  and  X  X omes tore X . Both of them are formed using valid words. Deciding between them requires more than just knowing the vocabulary. This gives birth to using different features like frequency of the word, length of word, n-grams etc. We will stick to features which are easy to get and almost no manual work is required, which increases the applicability of the solution to different lan-guages and scenarios.
For all experiments and approaches mentioned hereon (un-less otherwise specified), we have used the publicly avail-able Wikipedia English corpus [12]. Only titles of the pages were used. It consists of approximately 1.1M unique words including proper nouns like person names, places and com-pany names etc. This largely avoids the problem with proper nouns as they are not in the language dictionary.
We will discuss several features in the following sections and their impact to the process of compound splitting. A few parameters will be tuned based on a small set of de-velopment data (100 domain names and their correct split). Finally, we present a hybrid approach which uses all the out-lined features together.
Frequency of a word in a reasonably large corpus, is one of the first features which comes to mind when you are on a tie-breaker situation. Frequency not completely, but gives a fair value for the popularity of the word. [9] used frequency g-mean method for German. The formulation of the frequency g-mean (geometric mean) problem is as follows:
Let W = { w 1 , w 2 , w 3 ...w n } be unique words in the corpus and f w i the frequency of word w i . Let S = {  X  s 1 ,  X  s all possible splits for a given compound word. Then objec-tive function for frequency g-mean method [9] is,
Section 7 shows results of frequency g-mean on a domain test data. We get a F-measure of 0.785 which is reasonable and proves that frequency is one of the features to consider, but with a significant space for improvement.
In the context of compound splitting, if we are able to split out a part which is a long word, the probability that the compound word actually had that word as a part is high, since the longer the word, the probability of the word to make a even longer word is less. Keeping this in mind we go ahead to formulate a compound splitting strategy based on length of parts as follows,
Let l w i be the length (number of characters) in the word w . We define a sequence of length weight increments called  X , such that P  X  l =1  X  l  X  1. Then mathematically length norm score  X  l is, The objective function based on length of parts is,
After running a few experiments on the development data works pretty well.

Figure 3 shows the length norm score plotted against word length for the chosen  X . The smaller values chosen for length  X  2 avoids the noise caused by smaller words. Also, after a limit, the length factor ceases to have its impact on the process, therefore the value 1 at length  X  7.
 The results for the above strategy yielded a handsome F-Measure of 0.859, which establishes length of parts as a prominent feature in the making of our compound splitter.
Results for frequency g-mean showed that frequency fea-ture can be improved. We then analyzed the data to figure out why things are not working with frequency alone. Fig-ure 4 shows the frequency distribution of words in an En-glish corpus. More than 90% of the words have frequency less than 10. If we see the curve C-1 , most of the words are scored less than 0.01 (scaled frequency method, equivalent to appraoch mentioned in Section 5.2) and therefore fails to discriminate words effectively.

We then define a new frequency score called the positional frequency score ( C-4 ). It appears to be a better discrimina-tor as its growth rate is higher at the dense part of frequency Figure 4: Frequency and Score distribution, C1: Fre-range. The formal definition is as follows,
Positional frequency score of a word w i is defined as the number of unique frequencies in the corpus less than f Mathematically positional frequency score for w i is,
Though the scoring helped, we were still not able to get results as expected. We then investigated further into the data. Figure 5 gives a closer insight into the frequency dis-tribution of words. As we can see, words of different lengths have a radically different frequency distribution. This moti-vated us to put the word frequency distribution in the same scale using word-lengths. We then arrived to a holistic ap-proach to score word-frequency which we named unified fre-quency normalized score. The formal definition follows,
Let  X  f w i be the unified frequency for word w i . C is a big constant so that  X  f w i is atleast f w i .
We then define the unified frequency normalized score  X   X  as follows,
On applying Equation 6 we get the curves SC on Figure 5, which justifies the frequency distribution better. Figure 5: Frequency and Score distribution, C-3: frac-
To verify the superiority of the normalization, we modified the frequency g-mean objective function to include the new normalized function. Mathematically,
We got an amazing F-measure of 0.885 . The following example adds light to the situation. Consider two words,  X  X rand X  with frequency 564, and  X  X ole X  with frequency 606. The unified frequency normalized score for them are 0.47 and 0.41 respectively, which brings them in a common platform for discrimination in a better way.
This feature helps us to identify compound patterns which have appeared in the corpus as N-grams. It can be used directly to enrich the corpus dictionary, where compressed form of N-grams are stored as words themselves. Consider having seen the bi-gram  X  X ome store X  in the corpus. Now, we can store the compressed form  X  X omestore X  in a n-gram dictionary. When we encounter the compound  X  X omestore-furniture X , getting to the split  X  X ome store furniture X  will be straight-forward as the competitor  X  X omes tore furniture X  falls a lot short in total score.

Mathematically, N-gram word score for word w i is defined as,
Any N-gram word w i is finally expanded to the N-gram format. Eg., the word  X  X hisis X  will be expanded to  X  X his is X .
Only bi-grams and tri-grams were taken as the splitting application generally encounters compounds containing 2 to 3 words. Also as we grow to quadra-grams and more, the space complexity to store them will increase dramatically without adding much value.
Stop words are generally small in length but do have a significant say in the splitting process. The score  X  l does not work well for stop words as they are placed together with non-popular small words. Even  X   X  doesn X  X  seem to work well with stop words as they are too abundant and their  X   X  scores compete with long and popular words. Hence, we need a different scoring function for stop words. To facilitate that, we define some parameters viz. minimum stop-word score ST w and stop-word length weight factor ST  X  .
Stop-word scoring function  X  ST is,
ST w ensures that a stop word gets at least this much score regardless of its length whereas ST  X  acts as a boost factor for long stop words. We used ST w = 0 . 4 and ST  X  = 0 . 15.
We have discussed several features in the preceding sec-tions like length of words, unified frequency normalized score, n-grams and stop words. We also evaluated the length of words and unified frequency normalized score features on our golden data set for recall, precision and F-measure. The n-grams and stop words features are primarily supporting features as they cannot be standalone. Considering the suc-cess of hybrid approaches we were motivated to use all the above features together and come up with a new hybrid ap-proach.

Now, the final hybrid scoring function for a word is defined as,  X  W is the norm frequency weight which acts like a boost factor. We chose  X   X  W = 0 . 35.

The corresponding objective function then becomes,
We chose the additive form of the objective function over the g-mean form to rule out the possibility of zero prod-ucts in case the compound is a mistype and generates non-existing words.
Figure 6 gives an overview of the splitting process. The preprocessing stage has two main derivatives, n-gram dictio-nary creation and generation of unified frequency normalized score for the words. Generation of n-grams is trivial.
For the score generation, we first calculate the  X  f w i  X  X . Cal-culation of  X  f w i  X  X  is preceded by the calculation of average frequencies for all word lengths. Then based on their av-erage values, a value for C is chosen which is at least the largest average calculated. Once we choose the value for C , we can easily derive the  X  f w i for any word w i . Then we cre-ate a long array  X  F of length max {  X  f w i  X  s } . For each word w put a true flag at  X  F [  X  f w i ]. We then make a pass on the ar-ray  X  F and replace every true value with the number of true values before it. The modified  X  F is the final scores, i.e, for a a hashtable of words with their  X   X  scores.

The online part of splitter is mainly lookups to the n-gram table, word dictionary and the stop word list.  X  l w internal function to the splitter as it can be calculated online given the word. The splitter needs to evaluate all possible splits for a given compound word as mentioned in Equation 11. Theoretically, there exists an exponential number of splits for a particular compound word. We use dynamic programming to limit the complexity to O ( l w i 2 ) with a multiplicative factor of O ( l w i ) for string operations. The dynamic table D is of length l w i where D [ j ] gives the best split with its score for the compound w i [ j...l w i ]. If we make a pass from D [ l w i ], we can calculate D [ j ] at any j by looking at values of all D [ j + 1 ...l w i ].
In the previous sections we discussed how we can deal with compound splitting for correct domain names. In our application we deal with erroneous domain redirects. Most of the errors are because the user did not formulate the domain properly, eg.,  X  X w.disneychannel X . Many are not domain names but sort of a search query, eg.,  X  X ww.harry potter birthday partys.com X . Typos exist in the form of deletion, addition, transposition, miscommunication etc.
Surprisingly, 84.3% data after domain sanitization are correctly spelled compound words ( based on manual eval-uation of a random sample of 1000 NXDomain redi-rects ). These are effectively handled by the compound split-ter to generate relevant keywords for search. It should be noted that for these cases the correct domain could also have been easily shown by affixing  X  X ww X  and  X  X om X . However it would have been of no value to the search provider. Search on keywords generates revenue for the provider through paid clicks.

For the cases like  X  X arnesndnoble X  which is not a proper compound word, we get  X  X arnes nd noble X  from the com-pound splitter. Now we need a mechanism to weed out words like  X  X d X . When the words are completely scrambled like  X  X eirzonwirelss X , then the process will yield  X  X ei r zon wire lss X , which hardly contains relevant keywords. In such cases, we just send the sanitized query to the search engine.
We remove all words with individual parts having score less than 0.2, and if the overall splitting score is less than 0.6, domain sanitized keyword is taken. A confidence less than 0.6 is generally a random split of a mistyped word. Table 2 shows some examples of keyword extraction.
Even though many mistyped cases are not handled, statis-tically we lose a little as majority of the cases are correctly spelled compounds.
In this section we present the various results which gives the quality of compound splitting method developed. Rele-vancy and commerciality of the end application is also dis-cussed and illustrated.
English is not a compounding language like many Euro-pean languages. Therefore the search logs failed to produce a good set of compound words. To evaluate the compound splitter we then used 500 domain names from a database of existing domain names. Prefix and suffixes were removed. The correct splits were evaluated manually. For the purpose of evaluation, notion of recall, precision and accuracy was taken from [9].

Most of the methods have used accuracy as the overall evaluation score for compound splitting, but it fails to do so as it depends a lot on the compound to non-compound distribution of the test data set. Eg., [9] used a data set which had 94.5% non-compound words. It gives 0.945 accu-racy even without processing. Therefore we use F-measure which is the harmonic mean of recall and precision. Appar-ently, it is not much affected by the test data distribution.
Various methods including frequency g-mean (Equation 1), length of parts (Equation 3), unified frequency normal-ization (Equation 6) and hybrid approach (Equation 10) were evaluated. All the above mentioned methods were trained automatically using the language dataset (mentioned in Sec-tion 5.1.1) and then tested on the generated domain names test data. Table 3 lists the numbers obtained by the exper-iments. Hybrid approach stands out in among all.
 Table 3: Evaluation of compound splitting methods
Best results from [1] and [9] have also been listed. These approaches were not evaluated on the above mentioned test data. Highest accuracy of [9] is driven by the high ratio of non-compounds (94.5%) in their test data. The hybrid ap-praoch gives a much higher F-measure, therefore we believe that it will give competetive results in their respective lan-guages even though it doesn X  X  use NLP features or labelled dataset for training like the above two approaches.
In this section we will evaluate the quality of keyword extraction when used in the application of mistyped domain redirect search.
We have a application as described in Section 3, getting a small percentage of our total live user traffic where NXDo-mains are redirected by an ISP to us. Initially, it only had Domain Sanitization in place. Given a NXDomain, domain sanitized compound word was passed to our search engine and search results were presented to the user. We log the user behavior. Then we replace the domain sanitization ser-vice with the keyword extraction service as mentioned in Section 6 and log the user behavior again. Relevancy of keyword extraction service, can now be measured by the number of clicks on search results by the user. Table 4 pro-vides the details.
 Domain Sanitization 363302 Keyword Extraction 400014 Table 4: Evaluation of approaches based on click-through rate
The data was recorded for 7 days each (Mon-Sun) to cover user trends. We saw a 10% improvement in the click-through rate. It is evident that keyword extraction results are more relevant as they are able to retrieve better search results and hence the higher click through rate.
Here we evaluate the commercial value of keyword extrac-tion as discussed in Section 3 in the context of search applica-tion. The application is focused to provide search keywords than directly spell correcting the NXDomains because of the commercial value it generates. To test the commerciality, we take a set of 1000 random domains names which are redi-rected (only NXDomains are redirected) by an ISP to our search engine. We then check the number of paid-links gen-erated on the first page if searched for the mistyped domain name itself, domain sanitized version and also the keyword extracted one. Commerciality of the keyword selection can be measured by the number of paid-links generated. Table 5 lists the details of the results. We see that the keyword Table 5: Evaluation of approaches based on paid-link counts extraction method developed gives more paid links than all of the approaches. Even at granularity of 5 and 10 results, we see that the paid-results are more. This clearly says that the approach is more commercial and hence is of importance to search engines.
A robust compound splitter for English that uses a new corpus-driven technique was presented. The normalization function for word frequency derived here gave significant im-provement to the overall split quality. We have not used any NLP features like POS tags, or assumed any special data like parallel corpus which is not readily available. The F-Measure of 0.968 which incidentally is the highest of all approaches published so far. Because of the method be-ing corpus driven, it can be effectively applied to most lan-guages.

This technique was successfully applied in our real life web search application handling mistyped and non-existent domain names. We presented our users with the generated (splitted) keywords as a search query. Corrected domain names were not shown as they have no commercial value to the provider. Clicks on search results showed increased rele-vancy with higher number of paid links showing commercial value.

In the future, we would like to investigate if the super-vised version of the splitting algorithm can increase quality further. Also NLP features can be added and their impact be analyzed. From the application X  X  perspective, we plan to explore the design of a domain name spell corrector us-ing compound splitting which can further be used to gener-ate better keywords for the cases where domain sanitization gives mistyped compound words. [1] E. Alfonseca, S. Bilac, and S. Pharies.
 [2] E. Alfonseca, S. Bilac, and S. Pharies. German [3] R. D. Brown. Corpus-driven splitting of compound [4] K.-J. Chen and S.-H. Liu. Word identification for [5] T. Hedlund, H. Keskustalo, A. Pirkola, E. Airio, and [6] B.-H. Y. Ho, H. Lee, and H. chang Rim. Analysis of [7] F. Holz and C. Biemann. Unsupervised and [8] IANA. List of TLDs . IANA, [9] P. Koehn and K. Knight. Empirical methods for [10] M. Larson, D. Willett, J. K  X  lohler, and G. Rigoll. [11] F. Salvetti and N. Nicolov. Weblog classification for [12] Wikipedia. Wikipedia, the free encyclopedia .
