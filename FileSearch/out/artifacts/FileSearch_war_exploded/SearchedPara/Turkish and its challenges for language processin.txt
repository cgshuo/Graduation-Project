 Kemal Oflazer Abstract We present a short survey and exposition of some of the important aspects of Turkish that have proven challenging for natural language processing. Most of the challenges stem from the complex morphology of Turkish and how morphology interacts with syntax. We also provide a short overview of the major tools and resources developed for Turkish natural language processing over the last two decades.
 Keywords Turkish Natural language processing Language resources 1 Introduction Turkish is a language in the Turkic family of Altaic languages which also includes Mongolic, Tungusic, Korean and Japonic families. Modern Turkish is spoken mainly by about 60 million people in Turkey, Middle East and in Western European countries. Turkic languages comprising about 40 languages some of which are extinct, are spoken as a native language by 165 X 200 M people in a much wider geography, shown in Fig. 1 . 1 The right side of Fig. 1 shows the distribution of Turkic speakers to prominent members of the Turkic family. 2 Turkish and other languages in the Turkic family have certain features that pose interesting challenges for language processing. Turkish is usually used as a textbook example while discussing concepts such as agglutinating morphology or vowel harmony in morphophonology, or free constituent order in syntax. But there are many other issues that need to be addressed for robust handling of language processing tasks.
In this paper, we present a bird X  X  eye view of these aspects, and then present the state-of-the-art in Turkish language processing and computational resources for Turkish. 3 2 Turkish morphology Morphologically, Turkish is an agglutinative language with morphemes attaching to a root word like  X  X  X eads-on-a-string X  X . There are no prefixes and no productive compounding (e.g., as found in German) and most lexicalized compounds have noncompositional semantics (e.g., acemborusu , literally persian pipe , actually denotes a flower.)
Words are formed by very productive affixations of multiple suffixes to root words from a lexicon of about 30 K root words (not counting proper names.) Nouns do not have any classes nor are there any markings of grammatical gender in morphology and syntax. The content word root lexicons have been heavily influenced by Arabic, Persian, Greek, Armenian, French, Italian, German and commercial and temporal proximity. Literally overnight, the alphabet used for writing the language was switched from the Arabic alphabet to a Latin alphabet in 1928, and this was followed by a systematic purging of words of Arabic, Persian and sometimes western origins, but many such words still survive.

When used in context in a sentence, Turkish words can take many inflectional and derivational suffixes. It is quite common to construct words which correspond to almost a sentence in English: yap ? abil ? ecek ? se ? k ! if we will be able to do (it) Almost all morphemes have systematic allomorphs that vary in respective vowels and sometimes in boundary consonants: paket ? ten (from the package) vs. araba ? dan (from the car) where we see a consonant assimilating to the root-final consonant, while at the same time, the vowel agreeing with the last vowel in the root (hence vowel harmony). Whereas, in the right example, there is no consonant assimilation but a different instance of a vowel harmony. Vowel harmony in fact operates from left-to-right in a cascaded fashion as shown in Fig. 2 . Oflazer ( 1994 ) presents details of Turkish morphophonology as implemented in a two-level morphology setting.

Multiple derivations in a given word are not an uncommon occurrence. Ar  X  soy ( 2009 ) cites the word ruhsatland  X  r  X  lamamas  X  ndaki as a word with nine morphemes that was found in a large collection of text. This word roughly means existing at the time of (something) not being able to acquire certification , and is used as a modifier of some noun in context. Internal to the word, there are five derivations as shown in Fig. 3 , where we start with a root word ruhsat (certification) and after five derivations end up as a modifier.

But in general, things are saner: The average number of bound and unbound morphemes in a word in running text is about three, but this is heavily skewed. Also, on the average each word has about two different morphological interpretations due to different parts-of-speech for the roots, homography of some suffixes and different segmentations of a given word into morphemes.

Table 1 shows the top twenty most frequent words in a large Turkish corpus, along with the number of morphemes in the word and morphological ambiguity for each. We can estimate from these numbers that, since the more frequent words have just one morpheme, that many of the lower frequency words have more than three or more morphemes. Also most of the high-frequency words have relatively high morphological ambiguity, which for words with one morpheme corresponds to having different root parts-of-speech. Hence, an average of two morphological interpretations mentioned above means that morphological ambiguity for words with many morphemes (owing usually to, for example, segmentation ambiguity) is actually less.

Another aspect of Turkish morphology is the heavy use for derivational morphemes in word formation as exemplified in Fig. 3 . Table 2 shows the number of possible words that can be generated from only one noun or a verb root using zero, one, two and three derivational morphemes, with the zero case counting only the inflectional variants. The total column shows the cumulative sum of words with less of equal to the number of derivations on the same row. 4 It is certain that many of these derived words are never used but nevertheless the generative capacity of the morphological processes can generate this. The fact that a given verb root can give rise to about 1.5 M different word forms is rather amazing (see Wickwire 1987 for an interesting take on this.) To tame this generative capacity, these derivational processes need to be semantically constrained but this is extremely hard to do in a morphological analyzer.

Sak et al. ( 2011 ), present statistics from a relatively large corpus of Turkish text of close to 500 M Turkish words collected from mainly news text. They find about 4.1 M unique words in this corpus, with the most frequent 50 K/300 K words covering 89 %/97 % of the words respectively; 3.4M words appear \ 10 times and 2 M words appear once. The most crucial finding is that when increasing the corpus size from 490 to 491 M by adding 1 M new words, they report encountering 5,539 new word forms not found amongst the first 490 M words!
Figure 4 from Sak et al. ( 2011 ), shows the number of distinct stems and the number of distinct morpheme combinations that have been observed in this corpus. One can see that at around 360 M words in the corpus, the number of distinct morpheme combination observed reaches to around 46 K and exceeds the number of distinct stems observed.
 This essentially infinite lexicon size brings numerous challenges in many tasks: 5
Spelling checking and correction : Methods that rely on a finite list of words or a list of root words with some fixed number of affixes, can not capture lexicon of Turkish. We have developed efficient spelling correction algorithms for languages like Turkish based on error tolerant finite state recognition, operating on a finite state recognizer model of morphology which can encode an infinite number of words (Oflazer 1996 ).
Tagset design : It is not possible to fully represent the morphosyntactic information encoded in morphology with a finite set of tags. The data in Fig. 4 already hints at this. There are of course a small number of root part-of-speech categories, but with multiple inflectional and derivational affixes, the word may end up having many morphological features including multiple parts-of-speech, all of which may have syntactic implications. See Hakkani-Tu  X  r et al. ( 2002 ) for statistics on the number of different possible tags.

Statistical language modeling : A large vocabulary size almost always leads to a data sparseness problem in word-based language modeling. Figure 5 , from Ar  X  soy X  X  Ph.D. thesis (Ar  X  soy 2009 ), shows the OOV percentage for a speech recognizer using different vocabularies with different sizes. Table 3 , also from this thesis, shows the OOV rates for several languages using a vocabulary of about 60K words. Turkish along with Czech (an inflectional language) has about 8 % OOV with other agglutinative languages actually having significantly higher rates. Ar  X  soy has also investigated using sub-lexical units in language modeling for Turkish and has found that using 76K morphemes (roots and bound morphemes) gives a very good coverage in a speech recognition task and the lowest word error rate percentage in a test set.
Syntactic modeling : As we will shortly see, derivational morphemes have interesting implications in syntactic modeling using either constituency based discusses how derivational morphology interfaces with syntax.

Statistical machine translation : Just as with statistical language modeling, a large vocabulary implies sparseness in statistical machine translation, which is compounded by the fact that no really large parallel corpora involving Turkish exist to offset this. Thus approaches exploiting morphology in various ways have been proposed with good improvements over word-based baseline. 3 Constituent order and morphology X  X yntax interface The unmarked constituent order in Turkish is Subject X  X bject X  X erb with adjuncts going in more or less freely anywhere. However all six constituent orders are possible with minimal constraints. 6 As is usual with other free constituent order languages, the freeness comes with the availability of case marking on the nominal arguments of the verbs.

The following are examples of constituent order variations along with the contextual assumptions when they are used. In all cases, the main event being mentioned is Ekin saw  X ag  X  lae. , with the variations encoding the discourse context and assumptions.  X  Ekin C  X ag  X  la X  X   X  go  X  rdu  X  . (Ekin saw  X ag  X  la.)  X  C  X ag  X  la X  X   X  Ekin go  X  rdu  X  . (It was Ekin who saw  X ag  X  la.)  X  Go  X  rdu  X  C  X ag  X  la X  X   X  Ekin. (Ekin saw  X ag  X  la (and I was expecting that.)) Handling these variations in the usual CFG-based formalisms is possible (though not necessarily trivial or clean). C  X  etinog  X  lu X  X  large scale LFG grammar for Turkish handled these variations in a principled way but did not have a good way to encode the additional information provided by the constituent order variations. 3.1 Morphology X  X yntax interface A more interesting impact of complex morphology, especially derivational morphology, is on modeling syntactic relationships between the words. Before elaborating on this let X  X  describe an abstraction that has helped us to model these relationships.

The morphological analysis of a word can be represented as a sequence of tags corresponding to the morphemes. In our morphological analyzer output, the tag ^ DB marks derivation boundaries. We call the set of morphological features encoded between two derivations (or before the first of after the last, if any) as an inflectional group (IG). We represent the morphological information in Turkish in the following general form: root  X  IG 1  X  ^ DB  X  IG 2  X  ^ DB  X  X  ^ DB  X  IG n . where each IG i denotes the relevant sequence of inflectional features including the part-of-speech for the root (in IG 1 ) and for any of the derived forms. A given word may have multiple such representations depending on any morphological ambiguity brought about by alternative segmentations of the word, and by ambiguous interpretations of morphemes.

For instance, the morphological analysis of the derived modifier that will be made to be far/distant X  X , would be: 7 uzak ? Adj The five IGs in this word are: 1. ? Adj 2. ? Verb ? Become 3. ? Verb ? Caus 4. ? Verb ? Pass ? Pos 5. ? Adj ? FutPart ? Pnon The first IG indicates that the root is a simple adjective. The second IG indicates a derivation into a verb whose semantics is  X  X  X o become X  X  the preceding adjective (equivalent to  X  X  X o move away X  X  in English). The third IG indicates that a causative verb (equivalent to  X  X  X o send away X  X  in English) is derived from the previous verb. The fourth IG indicates the derivation of a passive verb with positive polarity, from the previous verb. Finally, the last IG represents a derivation into future participle which will function as a modifier in the sentence.

The one interesting observation that we can make about IGs is that (1) the syntactic relations are NOT between words, but rather between IGs of different words; and (2) the role of a given word in the sentence is determined by its last IG! To further motivate this, we present the example in Fig. 6 . The second word in spor araban  X  zdayd  X  (it was in your sports car) has a second/final IG which happens to have the part-of-speech of a verb. However there is also the noun-noun compound spor araba-(sports car), where the word spor acts as a modifier of araba . So the word (which has a part-of-speech of noun) and not with the whole word whose final part-of-speech is a verb. In fact, different IGs of a word can be involved in multiple relations with different IGs of multiple words as depicted in a more comprehen-sively annotated sentence in Fig. 7 . 8 In this figure, the solid lines denote the words and the broken lines denote the IGs in the words. Note that in each case, a relation from a dependent emanates from the last IG of a word, but may land on any IG as the head. The morphological features encoded in the IGs are listed vertically under each IG with different IGs X  features separated by vertical dashed lines. For instance, if we zoom into the three words in the middle of the sentence (shown in Fig. 8 ), we can note the following: The word ak  X  ll  X  s  X  is composed of three IGs; it starts as noun ak  X  l (intelligence) and with the derivational suffix ? l  X  , becomes an adjective (with intelligence/intelligent) and then through a zero derivation becomes again a noun (one who is intelligent). The word  X g  X  rencilerin (of the students) and this final IG of ak  X  ll  X  s  X  , have the necessary morphological markings and agreement features to form a possessor/possessee noun compound, and this is indicated by the relation by Poss . The more interesting example is the adverbial intensifier en (most) modifying the intermediate IG with the adjective part-of-speech  X  it can not have any other relationship, adverbials modify adjectives and not nouns. Thus we get a noun phrase meaning  X  X  X he most intelligent of the students X  X .

We have used IGs as a convenient abstraction in both statistical and rule-based contexts: Hakkani-Tu  X  r et al. ( 2002 ) model morphological disambiguation in terms Treebank (Oflazer et al. 2003 ) has been encoded in terms of relations between IGs. 4 Statistical machine translation At this point, it should be clear that morphology is bound to create problems for the components of a statistical machine translation systems for Turkish. Let X  X  look at a rather contorted but not that unreasonable example of a hypothetical process of how an English phrase becomes a Turkish word in the ideal case. Figure 9 shows how different parts of the English phrase (mostly function words) are scrambled around and then translated into morphemes which when concatenated, gives us a single word saglamlastirabileceksek . One can immediately see that the process of alignment X  X he starting point for training SMT systems is bound to have problems (discontinuous) sequence of nine words ideally has to align with one Turkish word if word-to-word alignment is done. This clearly stresses the alignment process and will lead to further problems. This is especially due to the data sparseness problem commonalities between variants of the same word will not be exploited. Another problem is that, if one gets a single morpheme wrong, the Turkish word can not be constructed properly and this hurts in evaluation (even if the rest of the word is OK.)
An obvious thing to do is to make Turkish like English: segment Turkish words in their overt morphemes (and maybe even do some segmentation on the English side separating suffixes like the plural or verb suffixes). For example, we would use the following representations on the two sides of a parallel sentence pair: E: I would not be able to read ... T: ...yap ? ama ? yacak ? t  X  ? m
This approach was experimented with by Oflazer ( 2008 ), Durgar-El-Kahlout ( 2009 ) and Durgar-El-Kahlout and Oflazer ( 2010 ) in a phrase-based SMT setting, using Moses (Koehn et al. 2007 ), with some additional techniques. Significant improvements over a word-based baseline was obtained but clearly other fundamental problems came to surface:  X  When Turkish words are segmented, the number of tokens per (Turkish)  X  The decoder of the SMT tool is now tasked with getting both the word and the An alternative approach is to make English side more like Turkish based on the observation that many kinds of specific phrases in English actually correspond solely to morphology on the Turkish side. Yeniterzi and Oflazer ( 2010 ), experimented with a so called syntax-to-morphology scheme in which English sentences were syntactically processed to identify various phrases or parts of phrases and transform those so that the resulting words  X  X  X ook like Turkish X  X . So if the original English sentence had a phrase like ... in their economic relations ... a parser would identify the preposition in and possessive pronoun their are related to the word relations and convert this to a representation like ... economic relation ? s ? their ? in ... essentially treating these function words as if they were morphemes on relation . Morphological preprocessing on the Turkish side also gives ... ekonomik ilis  X ki ? ler ? i ? nde ...

Using factored phrase-based SMT, Yeniterzi and Oflazer concatenated  X  X  X or-phemes X  X  on both sides into a single morphology factor. The alignment was then performed only based on the root words  X  X orphology on the Turkish side and the function words tacked on to the words as pseudo-morphology on the English-side were assumed to align if the associated root words aligned. These transformations reduced the number of tokens on the English side by about 30 % hence alignment could be run on much shorter sentence (of just the remaining root words on both sides). Again, significant improvements over a baseline were obtained. This method is probably a bit more promising but the recall for finding English side patterns to map to Turkish morphological structures was not very high, as the patterns and the corresponding rules for extracting them were hand-crafted. An automatic scheme for extracting these syntax-to-morphology correspondences could improve applicability. 5 State-of-the-art in tools and resources for Turkish Many tools and resources for Turkish language processing have been developed over the last two decades starting essentially from scratch. In this section, we present brief overview of these, with pointers to literature and web (whenever applicable) for researchers interested in following-up. 1. Morphological Analysis :Oflazer( 1994 ) describes a wide-coverage morpholog-2. Morphological Disambiguation : In addition to early work by Oflazer and 3. Statistical Dependency Parsers : There have been several dependency parsers for 4. A LFG-based parser : A large scale wide-coverage deep grammar for Turkish 5. The Turkish Treebank : A treebank of 5,635 sentences using a dependency 6. The Turkish Discourse Bank : A Turkish discourse bank with annotations of 7. Turkish WordNet : A WordNet for Turkish of about 15 K synsets (Bilgin et al. 8. Miscellaneous Corpora and Resources : There have been numerous other efforts 6 Conclusions Despite being the native language of over 60 M speakers in a wide geography, Turkish has been a relative late-comer into natural language processing and the development of tools and resources for Turkish natural language processing has only been attempted in the last two decades. Yet Turkish presents unique problems for almost all tasks in language processing ranging from tagset design, to statistical language modeling to syntactic modeling to statistical machine translation, among many others. On the other hand, solutions to problems observed for Turkish when appropriately abstracted, turn out to be applicable to a much wider set of languages. Over the years many tools and resources have been developed but many more challenges remain: For example, there are no natural sources of parallel texts where one side is Turkish (akin to, say, Europarl parallel corpora), so researchers working on statistical machine translation can only experiment with rather limited data which will not increase to the levels used for pairs such as English-Chinese or English-Arabic any time soon. Other more mundane issues such as drifting away from a one-to-one correspondence between letters and sounds, by the recent wholesale import of words from other languages such as English, with their native orthography and pronunciation, which now have to tack on the morphology, cause rather nasty problems even for the basic stages of lexical processing such as morphology. For example, one usually sees words like serverlar (servers) where as written, the vowels violate the harmony constraints but as pronounced, they don X  X , because of a bizarre assumption by the writers of such words that the readers will know the English pronunciation of the root words for the vowel harmony to go through!
Nevertheless, despite these difficulties the last several years have seen and significant increase of researchers and research groups in and out of Turkey who have dedicated efforts into building new resources and addressing new problems and the future should be quite bright moving forward.
 References
 Kemal Oflazer Abstract We present a short survey and exposition of some of the important aspects of Turkish that have proven challenging for natural language processing. Most of the challenges stem from the complex morphology of Turkish and how morphology interacts with syntax. We also provide a short overview of the major tools and resources developed for Turkish natural language processing over the last two decades.
 Keywords Turkish Natural language processing Language resources 1 Introduction Turkish is a language in the Turkic family of Altaic languages which also includes Mongolic, Tungusic, Korean and Japonic families. Modern Turkish is spoken mainly by about 60 million people in Turkey, Middle East and in Western European countries. Turkic languages comprising about 40 languages some of which are extinct, are spoken as a native language by 165 X 200 M people in a much wider geography, shown in Fig. 1 . 1 The right side of Fig. 1 shows the distribution of Turkic speakers to prominent members of the Turkic family. 2 Turkish and other languages in the Turkic family have certain features that pose interesting challenges for language processing. Turkish is usually used as a textbook example while discussing concepts such as agglutinating morphology or vowel harmony in morphophonology, or free constituent order in syntax. But there are many other issues that need to be addressed for robust handling of language processing tasks.
In this paper, we present a bird X  X  eye view of these aspects, and then present the state-of-the-art in Turkish language processing and computational resources for Turkish. 3 2 Turkish morphology Morphologically, Turkish is an agglutinative language with morphemes attaching to a root word like  X  X  X eads-on-a-string X  X . There are no prefixes and no productive compounding (e.g., as found in German) and most lexicalized compounds have noncompositional semantics (e.g., acemborusu , literally persian pipe , actually denotes a flower.)
Words are formed by very productive affixations of multiple suffixes to root words from a lexicon of about 30 K root words (not counting proper names.) Nouns do not have any classes nor are there any markings of grammatical gender in morphology and syntax. The content word root lexicons have been heavily influenced by Arabic, Persian, Greek, Armenian, French, Italian, German and commercial and temporal proximity. Literally overnight, the alphabet used for writing the language was switched from the Arabic alphabet to a Latin alphabet in 1928, and this was followed by a systematic purging of words of Arabic, Persian and sometimes western origins, but many such words still survive.

When used in context in a sentence, Turkish words can take many inflectional and derivational suffixes. It is quite common to construct words which correspond to almost a sentence in English: yap ? abil ? ecek ? se ? k ! if we will be able to do (it) Almost all morphemes have systematic allomorphs that vary in respective vowels and sometimes in boundary consonants: paket ? ten (from the package) vs. araba ? dan (from the car) where we see a consonant assimilating to the root-final consonant, while at the same time, the vowel agreeing with the last vowel in the root (hence vowel harmony). Whereas, in the right example, there is no consonant assimilation but a different instance of a vowel harmony. Vowel harmony in fact operates from left-to-right in a cascaded fashion as shown in Fig. 2 . Oflazer ( 1994 ) presents details of Turkish morphophonology as implemented in a two-level morphology setting.

Multiple derivations in a given word are not an uncommon occurrence. Ar  X  soy ( 2009 ) cites the word ruhsatland  X  r  X  lamamas  X  ndaki as a word with nine morphemes that was found in a large collection of text. This word roughly means existing at the time of (something) not being able to acquire certification , and is used as a modifier of some noun in context. Internal to the word, there are five derivations as shown in Fig. 3 , where we start with a root word ruhsat (certification) and after five derivations end up as a modifier.

But in general, things are saner: The average number of bound and unbound morphemes in a word in running text is about three, but this is heavily skewed. Also, on the average each word has about two different morphological interpretations due to different parts-of-speech for the roots, homography of some suffixes and different segmentations of a given word into morphemes.

Table 1 shows the top twenty most frequent words in a large Turkish corpus, along with the number of morphemes in the word and morphological ambiguity for each. We can estimate from these numbers that, since the more frequent words have just one morpheme, that many of the lower frequency words have more than three or more morphemes. Also most of the high-frequency words have relatively high morphological ambiguity, which for words with one morpheme corresponds to having different root parts-of-speech. Hence, an average of two morphological interpretations mentioned above means that morphological ambiguity for words with many morphemes (owing usually to, for example, segmentation ambiguity) is actually less.

Another aspect of Turkish morphology is the heavy use for derivational morphemes in word formation as exemplified in Fig. 3 . Table 2 shows the number of possible words that can be generated from only one noun or a verb root using zero, one, two and three derivational morphemes, with the zero case counting only the inflectional variants. The total column shows the cumulative sum of words with less of equal to the number of derivations on the same row. 4 It is certain that many of these derived words are never used but nevertheless the generative capacity of the morphological processes can generate this. The fact that a given verb root can give rise to about 1.5 M different word forms is rather amazing (see Wickwire 1987 for an interesting take on this.) To tame this generative capacity, these derivational processes need to be semantically constrained but this is extremely hard to do in a morphological analyzer.

Sak et al. ( 2011 ), present statistics from a relatively large corpus of Turkish text of close to 500 M Turkish words collected from mainly news text. They find about 4.1 M unique words in this corpus, with the most frequent 50 K/300 K words covering 89 %/97 % of the words respectively; 3.4M words appear \ 10 times and 2 M words appear once. The most crucial finding is that when increasing the corpus size from 490 to 491 M by adding 1 M new words, they report encountering 5,539 new word forms not found amongst the first 490 M words!
Figure 4 from Sak et al. ( 2011 ), shows the number of distinct stems and the number of distinct morpheme combinations that have been observed in this corpus. One can see that at around 360 M words in the corpus, the number of distinct morpheme combination observed reaches to around 46 K and exceeds the number of distinct stems observed.
 This essentially infinite lexicon size brings numerous challenges in many tasks: 5
Spelling checking and correction : Methods that rely on a finite list of words or a list of root words with some fixed number of affixes, can not capture lexicon of Turkish. We have developed efficient spelling correction algorithms for languages like Turkish based on error tolerant finite state recognition, operating on a finite state recognizer model of morphology which can encode an infinite number of words (Oflazer 1996 ).
Tagset design : It is not possible to fully represent the morphosyntactic information encoded in morphology with a finite set of tags. The data in Fig. 4 already hints at this. There are of course a small number of root part-of-speech categories, but with multiple inflectional and derivational affixes, the word may end up having many morphological features including multiple parts-of-speech, all of which may have syntactic implications. See Hakkani-Tu  X  r et al. ( 2002 ) for statistics on the number of different possible tags.

Statistical language modeling : A large vocabulary size almost always leads to a data sparseness problem in word-based language modeling. Figure 5 , from Ar  X  soy X  X  Ph.D. thesis (Ar  X  soy 2009 ), shows the OOV percentage for a speech recognizer using different vocabularies with different sizes. Table 3 , also from this thesis, shows the OOV rates for several languages using a vocabulary of about 60K words. Turkish along with Czech (an inflectional language) has about 8 % OOV with other agglutinative languages actually having significantly higher rates. Ar  X  soy has also investigated using sub-lexical units in language modeling for Turkish and has found that using 76K morphemes (roots and bound morphemes) gives a very good coverage in a speech recognition task and the lowest word error rate percentage in a test set.
Syntactic modeling : As we will shortly see, derivational morphemes have interesting implications in syntactic modeling using either constituency based discusses how derivational morphology interfaces with syntax.

Statistical machine translation : Just as with statistical language modeling, a large vocabulary implies sparseness in statistical machine translation, which is compounded by the fact that no really large parallel corpora involving Turkish exist to offset this. Thus approaches exploiting morphology in various ways have been proposed with good improvements over word-based baseline. 3 Constituent order and morphology X  X yntax interface The unmarked constituent order in Turkish is Subject X  X bject X  X erb with adjuncts going in more or less freely anywhere. However all six constituent orders are possible with minimal constraints. 6 As is usual with other free constituent order languages, the freeness comes with the availability of case marking on the nominal arguments of the verbs.

The following are examples of constituent order variations along with the contextual assumptions when they are used. In all cases, the main event being mentioned is Ekin saw  X ag  X  lae. , with the variations encoding the discourse context and assumptions.  X  Ekin C  X ag  X  la X  X   X  go  X  rdu  X  . (Ekin saw  X ag  X  la.)  X  C  X ag  X  la X  X   X  Ekin go  X  rdu  X  . (It was Ekin who saw  X ag  X  la.)  X  Go  X  rdu  X  C  X ag  X  la X  X   X  Ekin. (Ekin saw  X ag  X  la (and I was expecting that.)) Handling these variations in the usual CFG-based formalisms is possible (though not necessarily trivial or clean). C  X  etinog  X  lu X  X  large scale LFG grammar for Turkish handled these variations in a principled way but did not have a good way to encode the additional information provided by the constituent order variations. 3.1 Morphology X  X yntax interface A more interesting impact of complex morphology, especially derivational morphology, is on modeling syntactic relationships between the words. Before elaborating on this let X  X  describe an abstraction that has helped us to model these relationships.

The morphological analysis of a word can be represented as a sequence of tags corresponding to the morphemes. In our morphological analyzer output, the tag ^ DB marks derivation boundaries. We call the set of morphological features encoded between two derivations (or before the first of after the last, if any) as an inflectional group (IG). We represent the morphological information in Turkish in the following general form: root  X  IG 1  X  ^ DB  X  IG 2  X  ^ DB  X  X  ^ DB  X  IG n . where each IG i denotes the relevant sequence of inflectional features including the part-of-speech for the root (in IG 1 ) and for any of the derived forms. A given word may have multiple such representations depending on any morphological ambiguity brought about by alternative segmentations of the word, and by ambiguous interpretations of morphemes.

For instance, the morphological analysis of the derived modifier that will be made to be far/distant X  X , would be: 7 uzak ? Adj The five IGs in this word are: 1. ? Adj 2. ? Verb ? Become 3. ? Verb ? Caus 4. ? Verb ? Pass ? Pos 5. ? Adj ? FutPart ? Pnon The first IG indicates that the root is a simple adjective. The second IG indicates a derivation into a verb whose semantics is  X  X  X o become X  X  the preceding adjective (equivalent to  X  X  X o move away X  X  in English). The third IG indicates that a causative verb (equivalent to  X  X  X o send away X  X  in English) is derived from the previous verb. The fourth IG indicates the derivation of a passive verb with positive polarity, from the previous verb. Finally, the last IG represents a derivation into future participle which will function as a modifier in the sentence.

The one interesting observation that we can make about IGs is that (1) the syntactic relations are NOT between words, but rather between IGs of different words; and (2) the role of a given word in the sentence is determined by its last IG! To further motivate this, we present the example in Fig. 6 . The second word in spor araban  X  zdayd  X  (it was in your sports car) has a second/final IG which happens to have the part-of-speech of a verb. However there is also the noun-noun compound spor araba-(sports car), where the word spor acts as a modifier of araba . So the word (which has a part-of-speech of noun) and not with the whole word whose final part-of-speech is a verb. In fact, different IGs of a word can be involved in multiple relations with different IGs of multiple words as depicted in a more comprehen-sively annotated sentence in Fig. 7 . 8 In this figure, the solid lines denote the words and the broken lines denote the IGs in the words. Note that in each case, a relation from a dependent emanates from the last IG of a word, but may land on any IG as the head. The morphological features encoded in the IGs are listed vertically under each IG with different IGs X  features separated by vertical dashed lines. For instance, if we zoom into the three words in the middle of the sentence (shown in Fig. 8 ), we can note the following: The word ak  X  ll  X  s  X  is composed of three IGs; it starts as noun ak  X  l (intelligence) and with the derivational suffix ? l  X  , becomes an adjective (with intelligence/intelligent) and then through a zero derivation becomes again a noun (one who is intelligent). The word  X g  X  rencilerin (of the students) and this final IG of ak  X  ll  X  s  X  , have the necessary morphological markings and agreement features to form a possessor/possessee noun compound, and this is indicated by the relation by Poss . The more interesting example is the adverbial intensifier en (most) modifying the intermediate IG with the adjective part-of-speech  X  it can not have any other relationship, adverbials modify adjectives and not nouns. Thus we get a noun phrase meaning  X  X  X he most intelligent of the students X  X .

We have used IGs as a convenient abstraction in both statistical and rule-based contexts: Hakkani-Tu  X  r et al. ( 2002 ) model morphological disambiguation in terms Treebank (Oflazer et al. 2003 ) has been encoded in terms of relations between IGs. 4 Statistical machine translation At this point, it should be clear that morphology is bound to create problems for the components of a statistical machine translation systems for Turkish. Let X  X  look at a rather contorted but not that unreasonable example of a hypothetical process of how an English phrase becomes a Turkish word in the ideal case. Figure 9 shows how different parts of the English phrase (mostly function words) are scrambled around and then translated into morphemes which when concatenated, gives us a single word saglamlastirabileceksek . One can immediately see that the process of alignment X  X he starting point for training SMT systems is bound to have problems (discontinuous) sequence of nine words ideally has to align with one Turkish word if word-to-word alignment is done. This clearly stresses the alignment process and will lead to further problems. This is especially due to the data sparseness problem commonalities between variants of the same word will not be exploited. Another problem is that, if one gets a single morpheme wrong, the Turkish word can not be constructed properly and this hurts in evaluation (even if the rest of the word is OK.)
An obvious thing to do is to make Turkish like English: segment Turkish words in their overt morphemes (and maybe even do some segmentation on the English side separating suffixes like the plural or verb suffixes). For example, we would use the following representations on the two sides of a parallel sentence pair: E: I would not be able to read ... T: ...yap ? ama ? yacak ? t  X  ? m
This approach was experimented with by Oflazer ( 2008 ), Durgar-El-Kahlout ( 2009 ) and Durgar-El-Kahlout and Oflazer ( 2010 ) in a phrase-based SMT setting, using Moses (Koehn et al. 2007 ), with some additional techniques. Significant improvements over a word-based baseline was obtained but clearly other fundamental problems came to surface:  X  When Turkish words are segmented, the number of tokens per (Turkish)  X  The decoder of the SMT tool is now tasked with getting both the word and the An alternative approach is to make English side more like Turkish based on the observation that many kinds of specific phrases in English actually correspond solely to morphology on the Turkish side. Yeniterzi and Oflazer ( 2010 ), experimented with a so called syntax-to-morphology scheme in which English sentences were syntactically processed to identify various phrases or parts of phrases and transform those so that the resulting words  X  X  X ook like Turkish X  X . So if the original English sentence had a phrase like ... in their economic relations ... a parser would identify the preposition in and possessive pronoun their are related to the word relations and convert this to a representation like ... economic relation ? s ? their ? in ... essentially treating these function words as if they were morphemes on relation . Morphological preprocessing on the Turkish side also gives ... ekonomik ilis  X ki ? ler ? i ? nde ...

Using factored phrase-based SMT, Yeniterzi and Oflazer concatenated  X  X  X or-phemes X  X  on both sides into a single morphology factor. The alignment was then performed only based on the root words  X  X orphology on the Turkish side and the function words tacked on to the words as pseudo-morphology on the English-side were assumed to align if the associated root words aligned. These transformations reduced the number of tokens on the English side by about 30 % hence alignment could be run on much shorter sentence (of just the remaining root words on both sides). Again, significant improvements over a baseline were obtained. This method is probably a bit more promising but the recall for finding English side patterns to map to Turkish morphological structures was not very high, as the patterns and the corresponding rules for extracting them were hand-crafted. An automatic scheme for extracting these syntax-to-morphology correspondences could improve applicability. 5 State-of-the-art in tools and resources for Turkish Many tools and resources for Turkish language processing have been developed over the last two decades starting essentially from scratch. In this section, we present brief overview of these, with pointers to literature and web (whenever applicable) for researchers interested in following-up. 1. Morphological Analysis :Oflazer( 1994 ) describes a wide-coverage morpholog-2. Morphological Disambiguation : In addition to early work by Oflazer and 3. Statistical Dependency Parsers : There have been several dependency parsers for 4. A LFG-based parser : A large scale wide-coverage deep grammar for Turkish 5. The Turkish Treebank : A treebank of 5,635 sentences using a dependency 6. The Turkish Discourse Bank : A Turkish discourse bank with annotations of 7. Turkish WordNet : A WordNet for Turkish of about 15 K synsets (Bilgin et al. 8. Miscellaneous Corpora and Resources : There have been numerous other efforts 6 Conclusions Despite being the native language of over 60 M speakers in a wide geography, Turkish has been a relative late-comer into natural language processing and the development of tools and resources for Turkish natural language processing has only been attempted in the last two decades. Yet Turkish presents unique problems for almost all tasks in language processing ranging from tagset design, to statistical language modeling to syntactic modeling to statistical machine translation, among many others. On the other hand, solutions to problems observed for Turkish when appropriately abstracted, turn out to be applicable to a much wider set of languages. Over the years many tools and resources have been developed but many more challenges remain: For example, there are no natural sources of parallel texts where one side is Turkish (akin to, say, Europarl parallel corpora), so researchers working on statistical machine translation can only experiment with rather limited data which will not increase to the levels used for pairs such as English-Chinese or English-Arabic any time soon. Other more mundane issues such as drifting away from a one-to-one correspondence between letters and sounds, by the recent wholesale import of words from other languages such as English, with their native orthography and pronunciation, which now have to tack on the morphology, cause rather nasty problems even for the basic stages of lexical processing such as morphology. For example, one usually sees words like serverlar (servers) where as written, the vowels violate the harmony constraints but as pronounced, they don X  X , because of a bizarre assumption by the writers of such words that the readers will know the English pronunciation of the root words for the vowel harmony to go through!
Nevertheless, despite these difficulties the last several years have seen and significant increase of researchers and research groups in and out of Turkey who have dedicated efforts into building new resources and addressing new problems and the future should be quite bright moving forward.
 References
