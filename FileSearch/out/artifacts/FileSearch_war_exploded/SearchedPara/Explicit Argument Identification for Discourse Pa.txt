 Units within a piece of text are not meant to be un-derstood independently but understood by linking them with other units in the text. These units may be clauses, sentences or even complete paragraphs. Establishing relations between units present in a text allows the text to be semantically well structured and understandable. Understanding the internal struc-ture of text and the identification of discourse rela-tions is called discourse analysis.

A fully automated shallow discourse parser would greatly aid in discourse analysis and improve the performance of Text summarization and Question answering systems. Given a text, a shallow dis-course parser would identify discourse relations, consisting of two spans of text exhibiting some kind of relationship between each other. Discourse re-lations whose presence is marked explicitly by dis-course connectives are called Explicit discourse re-lations and those which are not are called Implicit discourse relations.

At present, complete shallow discourse parsers are only available for English (Lin et al., 2014; Wang and Lan, 2015; Ali and Bayer, 2015). The ongo-ing CoNLL 2016 shared task on Shallow Discourse Parsing has included Chinese as well. Work towards a complete shallow discourse parser in Hindi has also begun. Jain et al. (2016) reported state-of-the-art results for discourse connective identification in Hindi. Our work focuses on the next part towards a shallow discourse parser for Hindi i.e. argument identification for Explicit discourse relations.
In this paper, we discuss current approaches for this task and also propose a hybrid pipeline incorpo-rating many of these approaches. We report high ac-curacies of 93.28% for Arg2 identification, 71.09% for Arg1 identification and 66.3% for Arg1-Arg2 identification.

The rest of the paper is organized as follows. Sec-tion 2 briefly introduces the Hindi Discourse Rela-tions Bank(HDRB). Related work carried out in En-glish is discussed in Section 3. In section 4, we de-scribe in detail our approach to argument identifica-tion of Explicit discourse relations. Section 5 dis-cusses the performance of the proposed pipeline and we conclude in Section 6. The Hindi Discourse Relation Bank(HDRB) was created broadly following the lines of Penn Dis-course TreeBank(PDTB) (Miltsakaki et al., 2004; Prasad et al., 2008) X  X  lexically grounded approach along with a modified annotation workflow, addi-tional grammatical categories for explicit connec-tives, semantically driven Arg1/Arg2 labelling and modified sense hierarchies.(Oza et al., 2009; Ko-lachina et al., 2012) HDRB was annotated on a subset of the Hindi TreeBank (Begum et al., 2008) which includes part-of-speech, chunk and dependency parse tree anno-tations. HDRB contains 1865 sentences and a word count of 42K. Furthermore HDRB contains 650 ex-plicit discourse relations and 1200 implicit discourse relations.

In HDRB, one of the arguments occurs after the discourse connective and the other occurs before the connective. Discourse relations not adhering to this rarely occur in the corpus. However, due to the se-mantic labelling of Arg1 and Arg2, Arg2 does not always occur after the connective. For example:  X  c\dFgY m\ b h-pEtvAr kF s Bh BArF vfA kF  X  Heavy rains have occurred in Chandigarh be-
The relation sense is  X  X ontingency cause rela-tion X , where the situation described in Arg2 (itali-cized) is the cause of the the situation described in Arg1 (bolded). Due to this fact, Arg2 occurs before Arg1. However, for the purpose of argument iden-tification we refer to the argument occurring before the connective as Arg1 and the argument occurring after the connective as Arg2. We believe changing the labels later on during sense identification to be the simpler approach.

In the corpus, Arg1 can occur in the same sen-tence as the connective ( SS ) or in the sentence pre-ceding that of the connective ( PS ) with proportions of 46% and 54% respectively, whereas Arg2 only occurs in the same sentence as the connective.
Arg1 can cover 1,2,3 or even more than 4 sen-tences with proportions of 89.2%, 5.4%, 2.6% and 2.8% respectively. As such in this paper, we only consider the sentence containing the connective and the sentence immediately preceding it for Arg1 iden-tification. Argument identification for Hindi has not been ex-plored before, therefore we discuss some of the ap-proaches adopted for English.

Ghosh et al. (2011) proposed a linear tagging ap-proach for argument identification using Conditional random fields and n-best results.

Lin et al. (2014) proposed a sub-tree extraction approach for argument identification. Firstly an ar-gument position classifier was employed to decide the location of Arg1(PS/SS). In the case of PS, Arg1 was labelled as the entire preceding sentence. For tagging Arg1(SS) and Arg2, a argument node iden-tifier was employed to decide which nodes were part of Arg1(SS) or Arg2. Next sub-tree extraction was used to extract Arg1(SS) and Arg2. However, since it is not necessary that arguments may be dominated entirely by a single node as pointed out by Dinesh et al. (2005), this method has inherent shortcomings.
Kong et al. (2014) proposed a constituent based approach where, similar to Lin, an argument iden-tifier is employed to decide which constituents are Arg1 and Arg2. Previous sentence was consid-ered as a special constituent to handle Arg1(PS). A constituent pruner was also employed to reduce the number of candidate constituents considered for Arg1 and Arg2. In addition, Integer Linear Program-ming(ILP) with language specific constraints, was employed to ensure the argument identifier made le-gitimate global predictions.

Approaches in English can be summed up as two sub-tasks: (1) Considering the possible con-stituents/nodes/words to be identified as Arg1 or Arg2 by use of subtree extraction (Lin et al., 2014), constituent pruning (Kong et al., 2014) or simple baseline (Ghosh et al., 2011) approaches. (2) Clas-sification of selected constituents/nodes/words as Arg1/Arg2/None by use of CRF(Ghosh et al., 2011) or classifier(Lin et al., 2014; Kong et al., 2014) based approaches. We base our pipeline on the two sub tasks discussed in the previous section. We use a method similar to subtree extraction to extract possible candidates for Arg1/Arg2 and use CRF tagging to further refine the extent of the extracted arguments.

We approach the task of Arg1 and Arg2 identi-fication seperately since tagging Arg1 is inherently more difficult. We first discuss Arg2 identification and then Arg1 identification. Features used are listed in Table 1. 4.1 Arg2 Identification Doing a simple analysis on HDRB, we find that Arg2 largely lies in two positions in the dependency tree. Arg2 can either occur in the subtree of the con-nective node(Conn-SubTree) or in the subtree of the first verb group node occurring as parent to the con-nective node(Parent-VG-SubTree) as shown in Im-age 1.

To decide the position of Arg2, we make use of a classifier with Conn-St r, Conn-Pos-Sentence , Is-Leaf-Node , VG-In-SubTree , VG-In-Parent-SubTree and Right-Word-Location as features. Once we have the position of Arg2, all the nodes present in the sub-tree are extracted as Arg2. Henceforth, we refer to this step as SubTree-Extraction .

Although Arg2 lies in either in  X  X onn-SubTree X  or  X  X arent-VG-SubTree X , it does not necessarily cover the entire subtree. Thus we need to re-fine the extent of Arg2 extracted from the SubTree-Extraction . We approach this as a linear tagging task, allowing us to capture the local dependency between nodes. We use Conn-Rel-Pos , Node-Tag , Clause-End , Is-Conn and Part-of-Conn-Sub-Tree as features. Henceforth, we refer to this step as Partial-SubTree .

We find that Arg2 sometimes extends further up into the dependency tree. For example:  X  isk alAvA , uho\n r{lF BF EnkAlF aOr  X  In addition, he also took out rallies and also or-isk alAvA (In addition) X  X  Arg2 lies in  X  X arent-VG-SubTree X . However, the presence of and in-dicates some kind of correlation between  X  X e also took out rallies X  and  X  X lso organized rallies X . This correlation is also indicated in the dependency tree where both VG groups are children of and . To han-dle these and other similar cases we employ a classi-fier to decide whether extension beyond the current node is necessary. The current node is either con-nective node or the parent VG node of the connec-tive node depending upon the decision made in the SubTree-Extraction step. We use Conn-Str , Node-Tag of current node, Node-Tag of parent of current node, Conn-Rel-Pos for parent of current node as features for this step. Henceforth we refer to this step as SubTree-Extension .
 SubTree-Extraction , Partial-SubTree and SubTree -Extention complete the pipeline for Arg 2 identifi-cation. 4.2 Arg1 Identification Following the approach adopted for English (Kong et al., 2014; Lin et al., 2014; Wang and Lan, 2015), we approach Arg1 as two distinct problems: Arg1(SS) and Arg1(PS) identification. We employ a classifier to decide the position of Arg1. We use Conn-Str , Node-Tag of connective, Conn-Pos-Sentence , Chunk-Before-Conn as features for this step. Henceforth we refer to this step as Arg1 Pos Identification .

The position of Arg1(SS) in the dependency tree, similar to Arg2, shows strong correlation with the position of the connective in the dependency tree. In addition to Conn-SubTree and Parent-VG-SubTree , Arg1(SS) also lies in the subtree of the first verb group node occurring as parent to Parent-VG (pParent-VG-SubTree). This happens when Arg2 lies in the Parent-VG-SubTree .

To identify Arg1(SS), we use the same pipeline used for Arg2 identification, with certain differ-ences in choice of features. SubTree-Extraction uses Conn-Str , Is-Leaf-Node , Arg2-Pos , Node-Tag of par-ent node of connective, Node-Tag of parent of parent node of connective, Conn-Two-Clause as features. Both Patial-SubTree and SubTree-Extension use the same set of features used for Arg2 identification. SubTree-Extraction , Partial-SubTree and SubTree -Extention complete the pipeline for Arg 1 (SS) identification.

A similar pipeline for Arg1(PS) identification cannot be used, since both Arg2 and Arg1(SS) showed a strong correlation to the connective node in the dependency tree. No such anchor node exists for Arg1(PS).

We divide the dependency tree of previous sen-tence into smaller verb group subtrees(VG Sub-Tree). We consider each of them as candidates to be labelled as Arg1(SS). In the case of nested verb group sub trees, we treat them as two separate verb group subtrees ensuring no overlap of nodes be-tween them. We refer to this step as VG-SubTree-Extaction .
 We make use of a classifier to decide whether each VG SubTree candidate is part of Arg1(PS) or not. We use Verb-Group , Verb-Group-Compact , Verb-Root-Inflection as features. All the nodes present in the VG SubTrees labelled as Arg1(PS) are ex-tracted to form Arg1(PS). We refer to this step as VG-SubTree-Labelling .
 VG-SubTree-Extraction and VG-SubTree-Labelling complete the pipeline for Arg1 (PS) identification. The entire pipeline for argument identification is shown below in Image 3. Firstly, we discuss the experimental setup, baselines and performance metrics we have considered to put the performance of our pipeline in perspective. Later on, we discuss, in detail, the performance of Arg2 and Arg1 identification pipelines. 5.1 Experimental Setup Maximum Entropy (Fan et al., 2008) for classifier based steps and Conditional Random Fields (Laf-ferty et al., 2001) for linear tagging based steps were our choice of algorithms. L2 regularized Stochas-tic Gradient Descent (SGD) was used while train-ing the CRF model and LibLinear solver (Fan et al., 2008) with L2 penalties was used to train the Max-imum Entropy model. Maximum Entropy was im-Conditional Random Fields was implemented using validation to arrive at the results. 5.2 Baseline and Performance metrics As discussed in Section 2, Arg2 is the argument occurring after the connective and Arg1 is the ar-gument occurring before the connective. Therefore Arg2 baseline is computed by labelling Arg2 as the text span between the connective and the beginning of the next sentence. Similarly Arg1(SS) baseline is computed by labelling Arg1(SS) as the text span between the connective and the end the of the pre-vious sentence. Arg1(PS) baseline is computed by labelling the entire previous sentence as Arg1(PS).
Ghosh et al. (2011), kong et al. (2014) and Lin et al. (2014) have reported performance using exact match metric. In addition to reporting performance using exact match metric, we introduce a new metric for measuring performance-Partial match:
Where ArgResult is the argument labelled by the system, ArgGold is the argument labelled in the cor-pus. Partial match scores between 0-1 and incorpo-rates a penalty for each missed or erroneously la-belled node/chunk. Partial match is thus a more le-nient scoring metric than exact match, however the penalty ensures the leniency is limited. Partial match allows us to measure minor performance improve-ments that are not captured by exact match metric. 5.3 Arg2 Results We report Arg2 identification results in Table 2
We report a baseline score of 63.2 and 77.95 for exact and partial matches respectively. SubTree-Extraction does not reach the performance of the baseline with scores of 58.28 for exact match and 69.10 for partial match. With an increase of 33.28 for exact match and 23.78 for partial match, Partial-SubTree step results in the largest performance gains. SubTree-Extension further improves perfor-mance by 1.72 and 2.49 for exact and partial re-spectively. For Arg2 identification, we report a final score of 93.28 for exact match and 95.37 for partial match. 5.4 Arg1 Results Coming to arg1 identification, we report a high ac-curacy of 99.1 % for Arg1 Pos Identification step. This is similar to the performance reported by Lin et al. (2014). We find that Conn-Pos-Sentence is suf-ficient to decide between Arg1(PS) and Arg1(SS). Other features used result in minor improvements. We report Arg1(SS) results in Table 3. For Arg1(SS), we report a baseline score of 43.38 and 71.57 for exact and partial matches respectively. SubTree-Extraction performs poorly with a score of 2.05 for exact match and 22.63 for partial match. Similar to Arg2, we find that Partial-Subtree re-sults in a large increase in performance of 68 for exact match and 56.93 for partial match. SubTree-Extension yields minor improvements of 1.13 and 0.56 for exact and partial respectively. For Arg1(SS) we report a final score of 70.84 for exact match and 80.12 for partial match.

For Arg1(PS) we report a baseline of 71.05 and 72.38 for exact and partial matches respectively. We find that our system does not exceed the baseline scores with 38.55 for exact match and 62.07 for par-tial match. We believe more work is needed to suc-cessfully extract Arg1(PS).

We thus report an accuracy of 93.28% for Arg2 identification, 71.09% for Arg1 identification and 66.3% for Arg1-Arg2 identification. In this paper, we focus on argument identification for explicit discourse relations in Hindi. In particu-lar we propose a hybrid pipeline using both subtree extraction and linear tagging approaches. This is the first such work carried out in Hindi.
 Evgeny A Stepanov Giuseppe Riccardi Ali and Orkan
Bayer. 2015. The unitn discourse parser in conll 2015 shared task: Token-level sequence labeling with argument-specific models. CoNLL 2015 , page 25. Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-pendency annotation scheme for indian languages. In IJCNLP , pages 721 X 726. Citeseer.
 Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-tribution and the (non-) alignment of syntactic and dis-course arguments of connectives. In Proceedings of the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky , pages 29 X 36. Association for Compu-tational Linguistics.
 Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine
Learning Research , 9:1871 X 1874. Sucheta Ghosh, Richard Johansson, and Sara Tonelli. 2011. Shallow discourse parsing with conditional ran-dom fields. In In Proceedings of the 5th Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP 2011 . Citeseer.
 Rohit Jain, Himanshu Sharma, and Dipti Misra Sharma. 2016. Using lexical and dependency features to dis-ambiguate discourse connectives in hindi. In LREC . Sudheer Kolachina, Rashmi Prasad, Dipti Misra Sharma, and Aravind K Joshi. 2012. Evaluation of dis-course relation annotation in the hindi discourse rela-tion bank. In LREC , pages 823 X 828.
 Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014. A constituent-based approach to argument labeling with joint inference in discourse parsing. In EMNLP , pages 68 X 77.
 John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data. Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A pdtb-styled end-to-end discourse parser. Natural Lan-guage Engineering , 20(02):151 X 184.
 Eleni Miltsakaki, Rashmi Prasad, Aravind K Joshi, and
Bonnie L Webber. 2004. The penn discourse tree-bank. In LREC .
 Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs).
 Umangi Oza, Rashmi Prasad, Sudheer Kolachina,
Dipti Misra Sharma, and Aravind Joshi. 2009. The hindi discourse relation bank. In Proceedings of the third linguistic annotation workshop , pages 158 X 161. Association for Computational Linguistics.
 Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC . Citeseer.
 Jianxiang Wang and Man Lan. 2015. A refined end-to-end discourse parser. CoNLL 2015 , page 17.
