 Mining of frequent patterns is one of the popular knowledge discovery and data mining (KDD) tasks. It also plays an essential role in the mining of many other patterns such as correlation, sequences, and association rules. Hence, it has been the subject of numerous studies since its introduction. Most of these studies find all the frequent patterns from col-lection of precise data, in which the items within each datum or transaction are definitely known and precise. However, there are many real-life situations in which the user is inter-ested in only some tiny portions of these frequent patterns. Finding all frequent patterns would then be redundant and waste lots of computation. This calls for constrained min-ing , which aims to find only those frequent patterns that are interesting to the user. Moreover, there are also many real-life situations in which the data are uncertain. This calls for uncertain data mining . In this article, we propose algorithms to efficiently find constrained frequent patterns from collections of uncertain data . The research problem of frequent pattern mining has been the subject of numerous studies [7; 15; 16; 23; 26; 27] since its introduction. These studies can be broadly divided into the following two categories, those focused on functionality and those focused on performance.
 With respect to functionality, the central question consid-ered is what kind of patterns to compute. Although frequent pattern mining was first introduced as a fundamental step in mining association rules [3], studies in this category have shown that frequent pattern mining also plays an important role in the mining of various other patterns including cor-relation, sequences, episodes, emerging patterns, maximal itemsets, as well as closed itemsets [4; 8; 18; 19; 20; 21]. With respect to performance, the central question consid-ered is how to compute the frequent patterns as efficiently as possible. Most studies in this category focused on Apriori-based algorithms [2], which depend on a generate-and-test paradigm. They find frequent patterns from the transac-tion database (TDB) by first generating candidates and then checking their support (i.e., their occurrences) against the TDB.  X  Corresponding author: C.K.-S. Leung.
 To improve efficiency of the mining process, Han et al. [17] proposed an alternative framework, namely a tree-based framework, for finding frequent patterns. The algorithm they proposed in this framework constructs an extended prefix-tree structure, called Frequent Pattern tree ( FP-tree ), to capture the contents of the TDB. Rather than employing the generate-and-test strategy of Apriori-based algorithms, such a tree-based algorithm focuses on frequent pattern growth X  X hich is a restricted test-only approach (i.e., does not generate candidates, and only tests for support). Basically, a majority of the aforementioned studies mainly considered the data mining exercise in isolation. They did not explore how data mining can best interact with the hu-man user X  X  key component in the broader picture of knowl-edge discovery in databases. In other words, they rely on a computational model, in which the mining system does almost everything and the user is un-engaged in the mining process. Consequently, such a model provides little or no support for user focus (e.g., for limiting the computation to what interests the user). However, the support for user fo-cus is needed in many real-life applications where the user may have some particular phenomena in mind on which to focus the mining. For example, the user may want to find some specific events occurring only on a sunny day. Without user focus, the user often needs to wait for a long time for numerous frequent patterns, out of which only a tiny frac-tion may be interested to the user. This calls for constrained mining [5; 13; 14; 22; 24; 29; 30; 31].
 A common characteristic among the above frequent pat-tern mining algorithms (regardless whether they are for con-strained mining or not) is that they handle precise data, such as databases of market basket transactions, Web logs, and click streams. When mining precise data, the user definitely knows whether an item (or an event) is present in, or is ab-sent from, a transaction in the databases. However, there are situations in which the user is uncertain about the pres-ence or absence of some items or events [9; 12; 32; 33]. For example, a physician may highly suspect (but cannot guar-antee) that a patient suffers from flu. The uncertainty of such suspicion can be expressed in terms of existential proba-bility . So, in this uncertain database of patient records, each transaction t i represents a patient X  X  visit to a physician X  X  of-fice. Each item within t i represents a potential disease, and is associated with an existential probability expressing the likelihood of a patient having that disease in t i . For instance, in t i , the patient has a 90% likelihood of having the flu, and a 70% likelihood of having a cold regardless of having the flu or not. With this notion, each item in a transaction t i in traditional databases containing precise data can be viewed as an item with a 100% likelihood of being present in t i . This calls for uncertain data mining [1; 6; 11; 25; 26; 28]. In recent years, uncertain data mining has become one of the popular research topics in knowledge discovery and data mining (KDD). Many of the developed algorithms for uncer-tain mining have been focused on data mining tasks like clus-tering and classification of uncertain data [9]. With respect to frequent pattern mining or association rule mining of un-certain data, Chui et al. [11] proposed an Apriori-based algo-rithm called U-Apriori and introduced a trimming strategy to reduce the number of candidates that need to be counted by U-Apriori. They [10] also proposed a decremental prun-ing technique to speed up the mining process. However, as an Apriori-based algorithm, U-Apriori relies on the candi-date generate-and-test paradigm.
 Knowing that tree-based algorithms for mining precise data (e.g., FP-growth [17]) are usually faster than their Apriori-based counterparts (e.g., Apriori [2]), we previously pro-posed a tree-based algorithm called UF-growth [28] for min-ing uncertain data. We then further proposed two per-formance enhancements [26] to this UF-growth algorithm. However, like its counterpart for mining frequent patterns from precise data (i.e., FP-growth), the UF-growth algo-rithm for mining frequent patterns from uncertain data also suffers from the following problem: It has not yet explored how data mining can best interact with the human user. It relies on a computational model that provides little or no support for user focus. Consequently, the algorithm finds from uncertain data all the frequent patterns, out of which only a tiny fraction may be interested to the user. Hence, lots of time and computation are wasted. It is important to note that the support for user focus is needed in many real-life applications where the user may have some particular phenomena in mind on which to focus the mining. For ex-ample, when analyzing laboratory test data, there are some known factors (e.g., human reaction time, measuring errors) contributing to the uncertainty of the data. Among these uncertain data, the user may be interested in some partic-ular subsets of the data (e.g., those participated in Tests 1 and 2, those observed to have Symptoms 3 and 4). As an-other example, for an election, an analyst may be uncertain about which candidates the voter has cast his vote for, but the analyst may be interested in only those candidates who run for some particular posts (e.g., interested in those run-ning for the Secretary/Treasurer position but not for the Chair position, interested in those who belong to a particu-lar party).
 In this article, our key contribution is the proposal and de-velopment of efficient algorithms for mining constrained fre-quent patterns (i.e., frequent patterns that satisfy user con-straints) from uncertain data. Advantages of mining with our proposed algorithms include the following. First, the search space of frequent patterns from precise data is known to be huge, and that from uncertain data is even much big-ger. By pushing the user constraints deep inside the uncer-tain mining process, we effectively reduce the correspond-ing search space. Hence, we reduce and avoid unnecessary computation. Consequently, the computation for mining is proportional to the selectivity of user constraints. Second, the algorithms generate all and only those frequent patterns that satisfy the constraints. Thus, the number of patterns generated is proportional to the selectivity of constraints. Third, although there are algorithms for constrained mining and algorithms for uncertain mining, they either mine from precise data for the frequent patterns that satisfy the user constraints or mine from uncertain data for all the frequent patterns (regardless whether they satisfy the constraints or not). In other words, they do not find constrained frequent patterns from uncertain data. In contrast, the algorithms we proposed here are capable of mi ning from uncertain data for all those frequent patterns that satisfy the user constraints. This article is organized as follows. The next section presents background and related work. We propose algorithms for mining constrained frequent patterns from uncertain data in Section 3. Section 4 shows experimental results. Finally, conclusions are given in Section 5. Recall that our key contribution of this article is the pro-posal and development of efficient algorithms for mining constrained frequent patterns from uncertain data, which involves user constraints and uncertain data . Lakshmanan, Ng and their colleagues [22; 30] proposed a constrained frequent pattern mining framework, within which the user can use a rich set of SQL-style constraints to guide the mining process to find only those frequent patterns X  containing market basket items X  X hat satisfy the user con-straints. Examples of these constraints include the follow-ing: C 1  X  min ( S . Price )  X  $10 and C 2  X  S . Type = snack . Here, constraint C 1 says that the minimum price of all items in a pattern/set S is at least $10; constraint C 2 says that all items in a pattern S are snack. It is important to note that, besides these market basket items, the set of constraints can also be imposed on individuals, events, or objects in max ( S.T emperature )  X  36 . 8  X  C, C 4  X  S.Symptom  X  { fever, runny nose } , C 5  X  S.Day  X  X  Saturday, Sunday } , C 6  X  min ( S.W eight )  X  23kg, and C 7  X  avg ( S.W eight ) 23kg. Here, constraint C 3 says that the maximum (body) temperature of all individuals in a pattern/set S is at most 36.8  X  C; constraint C 4 says that individuals in S suffer only from fever or runny nose. Similarly, constraint C 5 say that all the events in a pattern S must span over the weekend (Saturday and Sunday). Constraints C 6 and C 7 ,respec-tively, say that the minimum and the average weights of all the objects in S is at most 23kg.
 The above constraints can be categorized into several over-lapping classes according to the nice properties that they possess. One of these classes is the succinct constraint , and its formal definition is given below.
 Definition 1 (Succinct Constraint [22; 24; 30]).
 Let Item be the set of domain items, and let 2 Item denote the powerset of Item .Then,the succinct constraint can be de-fined in several steps, as follows: (i) An itemset SS j  X  Item is a succinct set if SS j can be (ii) Apowersetofitems SP  X  2 Item is a succinct powerset (iii) Aconstraint C is succinct provided that the set of pat-With the above definition, constraints C 1  X  C 6 are succinct . For example, the set of patterns/itemsets satisfying C 1 can cinct powerset. Similarly, the sets of itemsets satisfying the other five succinct constraints ( C 2  X  C 6 ) are also succinct powersets. Practically, one can directly generate precisely all and only those patterns satisfying these constraints by using precise  X  X ormulas X  X  X alled member generating func-tions [22; 24; 30] X  X hat do not require generating and ex-cluding patterns not satisfying the constraints. For exam-ple, patterns satisfying C 6 can be precisely generated by combining at least one object of weight  X  23kg with some optional objects (of any weight), thereby avoiding the sub-stantial overhead of the generation and exclusion of invalid patterns. It is important to note that a majority of con-straints are succinct and many non-succinct constraints can be induced into weaker constraints that are succinct (e.g., non-succinct constraint C 7 can be induced into succinct con-straint C 6 because all frequent patterns satisfying C 7 must satisfy C 6 ).
 Succinct constraints can be further divided into subclasses X  such as succinct anti-monotone (SAM) constraints and succinct non-anti-monotone (SUC) constraints  X  X ased on additional properties they possessed. For in-stance, among the above succinct constraints, C 1  X  C 4 are SAM constraints because they possess an additional prop-erty of anti-monotonicity. With such a property, supersets of any pattern violating the SAM constraints also violate the constraints (e.g., if a pattern S contains an item with price lower than $10, then S violates C 1 and so do any su-persets of S ). In contrast, succinct constraints C 5  X  C 6 are SUC constraints because they do not possess such an anti-monotonicity property. For instance, if the minimum weight of all objects within a pattern S is heavier than 23kg, then S violates C 6 but there is no guarantee that all supersets of S would violate C 6 .Asanexample,let x.Weight be 30kg and y.Weight be 20kg. Then, S  X  X  x } and S  X  X  y } are both supersets of S . Between them, the former violates C 6 but the latter does not violate the constraint. A key difference between precise and uncertain data is that each transaction of the latter contains items and their exis-tential probabilities . The existential probability P ( x, t an item x in a transaction t i indicates the likelihood of x being present in t i .Usingthe  X  X ossible world X  interpreta-tion of uncertain data [11; 12; 26; 28], there are two possible worlds for an item x and a transaction t i : (i) the possible world W 1 where x  X  t i and (ii) the possible world W 2 where x the true world, the probability of W 1 being the true world is P ( x, t i ) and that of W 2 is 1  X  P ( x, t i ). To a further extent, there is usually more than one transac-tion in a transaction database (TDB). For instance, for an item x and a TDB consisting of two transactions t 1 and t 2 , there are four possible worlds: (i) W 1 where x is in both t 1 and t 2 , (ii) W 2 where x is in t 1 but not t 2 , (iii) W 3 where x is in t 2 but not t 1 ,and(iv) W 4 where x is neither in t 1 nor in t 2 . Let prob ( W j ) denote the probability of W j to be the true world. Then, prob ( W 1 )= P ( x, t 1 )  X  P ( x, t 2 ), prob ( W 2 )= P ( x, t 1 )  X  [1  X  P ( x, t 2 )], prob ( W 3 )=[1  X  P ( x, t 1 )] and prob ( W 4 )=[1  X  P ( x, t 1 )]  X  [1  X  P ( x, t 2 )]. Similarly, there is usually more than one domain item in each transaction in a TDB. For instance, for two indepen-dent items x &amp; y and a transaction t i , there are also four possible worlds: (i) W 1 where both x, y  X  t i , (ii) W 2 where x  X  t i but y  X  t i , (iii) W 3 where x  X  t i but y  X  t i , and (iv) W 4 where both x, y  X  t i . Then, prob ( W 1 )= P ( x, t i )  X  P ( y, t i ), prob ( W 2 )= P ( x, t i )  X  prob ( W 3 )=[1  X  P ( x, t i )]  X  P ( y, t i ), and prob ( W 4 )= [1  X  P ( x, t i )]  X  [1  X  P ( y, t i )].
 To generalize, there are many items in each of the n trans-actions in a TDB (where | TDB | = n ). Hence, the expected support of a pattern/itemset S intheTDBcanbecomputed by summing the support of S in a possible world W j (while taking in account the probability of W j to be the true world) over all possible worlds: where (i) sup ( S )in W j denotes the support of S in a possible world W j and (ii) prob ( W j ) denotes the probability of W j to be the true world. Note that (i) sup ( S )in W j can be com-puted by counting the number of transactions that contain S inthepossibleworld W j and (ii) prob ( W j ) can be computed by where x and y are items within a pattern/itemset S . Recall from the previous section that existing frequent pat-tern mining algorithms either find constrained frequent pat-terns from precise data or find the all frequent patterns from uncertain data. However, these algorithms do not find con-strained frequent patterns fro m uncertain data. Here, in this section, we propose algorithms that mine from uncertain data those frequent patterns that satisfy the user-specified succinct constraints. When using the  X  X ossible world X  interpretation of uncer-tain data, a naive algorithm for finding constrained frequent patterns from uncertain data is to find all frequent pat-terns first, and then checks these frequent patterns against the user constraints X  X s a post-processing step X  X o filter out the patterns that do not satisfy the constraints. Here, a pattern S is considered frequent if its expected support expSup ( X )  X  user-specified minimum support threshold minsup . Recall that the expected support of a pattern can be computed by summing the weighted support of S over all possible worlds. The weight reflects the probability of each possibleworldtobethetrueworld. Theskeletonforthis algorithm can be described as follow: 1. For each possible world W j , do the following: 2. Compute the expected support expSup ( S )ofapat-3. For each pattern S ,checkif S is frequent (i.e., check 4. For each frequent pattern, check if it satisfies the user A problem/weakness associated with the above naive algo-rithm is that one needs to compute the expected support of each pattern S , regardless whether S satisfies the user constraints or not. It is important to note that support counting is orthogonal to constraint checking. As we aim to find constrained frequent patterns (i.e., frequent patterns that satisfy the user constraints), we should not waste our time and resources in computing the support of a pattern if the pattern does not satisfy the user constraints. Hence, we should check to see if the pattern satisfies the user con-straints before computing its support. Pushing constraint checking earlier in the algorithm could save lots of computa-tion, especially when the user constraints are selective (i.e., when not many frequent patterns satisfy the constraints). Another problem/weakness associated with the naive algo-rithm is that one needs to compute the expected support of each pattern S by summing the support of S over all pos-sible worlds. However, the number of possible words is well known to be huge. Fortunately, given independent items in pattern S (i.e., s  X  S ), Equation (1) can be simplified [12] to become the following: With this setting, we no longer need to compute the ex-pected support of a (constrained) pattern S (i.e., sup ( S ) in W j ) and the probability prob ( W j ) for every possible world W j . Here, a (constrained) pattern S is considered frequent if its expected support equals or exceeds the user-specified support threshold minsup .
 In order to count the support of a constrained pattern, one needs to represent and store the data. As with many tree-based mining algorithms, a key challenge here is how to represent and store data X  X n this case, uncertain data X  X n a tree? For precise data, each item in a TDB is implic-itly associated with a definite certainty of its presence in the transaction. In contrast, for uncertain data, each item is explicitly associated with an existential probability rang-ing from a positive value close to 0 (indicating that the item has an insignificantly low chance to be present in the TDB) to a value of 1 (indicating that the item is definitely present). Moreover, the existential probability of the item can vary from one transaction to another. Different items may have the same existential probability. Inspired by the key modification made to the Apriori algorithm [2] by the U-Apriori algorithm [11] (i.e., incrementing the support val-ues of candidate patterns by their expected support instead of the actual support), we propose two efficient algorithms X  called U-FPS(SAM) and U-FPS(SUC) ,or U-FPS for short X  X o mine from u ncertain data for f requent p atterns that satisfy s uccinct constraints. More specifically, both U-FPS(SAM) and U-FPS(SUC) algorithms mine uncertain data, but the former finds frequent patterns that satisfy SAM constraints whereas the latter finds those that satisfy SUC constraints. In general, the U-FPS algorithms consist of three key operations: (i) the constraint checking of trans-actions in the database of uncertain data, (ii) the construc-tion of a tree structure that we call a UF-tree , and (iii) the mining of frequent patterns from the UF-tree. Note that our proposed UF-tree is a variant of the FP-tree, in which each node stores an item, its expected support, and the number of occurrences of such expected support for such an item. Recall from Section 2.1 that a succinct constraint can be a succinct anti-monotone ( SAM ) constraint or a succinct non-anti-monotone ( SUC ) constraint . Let us first consider aSAMconstraint C SAM , which possesses two nice proper-ties: (i) anti-monotonicity (i.e., if a pattern violates C SAM , then all its supersets also violate C SAM ) and (ii) succinctness (i.e., one can easily enumerate all and only those patterns that are guaranteed to satisfy C SAM ). Hence, any pattern S satisfying C SAM must consist of only items that individu-ally satisfy C SAM .Inotherwords, S  X  Item M (where Item M is the set of items that individually satisfy C SAM ). Due to succinctness, items in Item M can be efficiently enumerated. To mine uncertain data for frequent patterns that satisfy TDB of uncertain data once, finds all the items that satisfy the user-specified SAM constraint (i.e., Item M ), and captures these items in a UF-tree. Note that any domain item not belonging to Item M can be safely discarded because any pat-tern containing a non-Item M item does not satisfy C SAM . (Recall that, if a pattern violates C SAM , then all of its su-persets also violate C SAM .) Once all the Item M items are found, U-FPS accumulates the expected support of each of the Item M items. Then, it finds all frequent items (i.e., items having expected support  X  minsup ), and sorts them in de-scending order of accumulated expected support. The algo-rithm then scans the TDB the second time and inserts each transaction of the TDB of uncertain data into the UF-tree in a similar fashion as in the construction of an FP-tree ex-cept that (i) the new transaction is merged with a child (or descendant) node of the root of the UF-tree (at the highest support level) only if the same item and the same expected support exist in both the transaction and the child (or de-scendant) nodes and (ii) only those Item M items in the trans-action are added. With this tree construction process, the UF-tree possesses a nice property that the occurrence count of a node is at least the sum of occurrence counts of all its children nodes.
 Once the UF-tree is constructed, our proposed U-FPS(SAM) algorithm recursively mines frequent patterns that satisfy SAM constraints from this tree in a similar fashion as in the FP-growth algorithm except for the following: By doing so, U-FPS effectively mines from uncertain data all and only those frequent patterns that satisfy the user-specified SAM constraint. To get a better understanding of this algorithm, let us consider the following example.
Example 1. Consider the following database transactions consisting of uncertain data: with the following auxiliary information: In the above TDB of uncertain data, each transaction con-tains items and their corresponding existential probabili-ties. For instance, there are five items a, b, c, d and e in the first transaction t 1 , and the existential probabilities of these items are 0.9, 0.8, 0.7, 0.6 and 0.2 respectively. Note that the existential probabilities of the same item may vary from one transaction to another (e.g., the existential probability of item b in transaction t 1 is 0.8 whereas that in t 5 is 0.9). Different items may have the same existential probabilities (e.g., the existential probabilities of items c and f in t 5 are the same X  X ith a value of 0.5).
 Constraint checking and identification of the domain items that satisfy C SAM . Let constraint C SAM be the SAM constraint C 1  X  min ( S . Price )  X  $10. Our proposed U-FPS(SAM) algorithm checks each of the six domain items against the constraint C SAM ; it enumerates from the domain those valid items a, b, c, d &amp; e (each having price  X  i.e., Item M = { a, b, c, d, e } . (In other words, item f Once U-FPS identified the domain items that satisfy C SAM , these items would serve as a building block of all frequent patterns satisfying C SAM because all constrained frequent patterns must comprise only those items.
 Construction of the UF-tree. Let the user-specified support threshold minsup be set to 1.0. Our U-FPS(SAM) algorithm constructs the UF-tree as follows. First, the al-gorithm scans the TDB once and accumulates the expected support of each Item M item. Hence, it finds all frequent Item M items and sorts them in descending order of (accumu-lated) expected support. Specifically, it finds frequent Item M items a, b, c and d (with their corresponding accumulated ex-pected support of 2.7, 2.6, 2.6 and 1.8), which are sorted in descending order of their expected support values. These Item M items and their expected support are represented as a :2.7, b :2.6, c :2.6 and d :1.8. The expected support of each of these frequent Item M items  X  minsup =1 . 0. On the other hand, the Item M item e having accumulated expected sup-port of 0.9 &lt;minsup is removed because it is infrequent. Item f can be ignored as it does not belong to Item M . Then, our U-FPS algorithm scans the TDB the second time and inserts each transaction into the UF-tree. The algo-rithm first inserts the content of the first transaction t 1 into the tree, and results in a tree branch ( a :0.9):1, ( b :0.8):1, ( c :0.7):1, ( d :0.6):1 . It then inserts the content of the sec-ond transaction t 2 into the UF-tree. Since the expected support of a in t 2 is the same as the expected support of a in an existing branch (i.e., the branch for t 1 ), this node can be shared. So, the algorithm increments the occur-rence count for the tree node ( a :0.9) to 2, and adds the remainder of t 2  X  X amely, ( c :0.7):1, ( d :0.6):1  X  X s a child of the node ( a :0.9):2. As a result, we get the tree branch ( a :0.9):2, ( c :0.7):1, ( d :0.6):1 . Afterwards, our algorithm in-serts the content of the third transaction t 3 as a new branch ( b :0.9):1 because the node ( b :0.9):1 cannot be shared with thenode( a :0.9):2. Transactions t 4 and t 5 are then inserted into the UF-tree in a similar fashion. For t 4 ,nodes( a :0.9):2, ( c :0.7):1 and ( d :0.6):1 are all incremented by 1; for t 5 ,the node ( b :0.9) is incremented to 2, and ( c :0.5):1 is added to this branch to get ( b :0.9):2, ( c :0.5):1 . Consequently, at the end of the tree construction process, we get the UF-tree shown in Figure 1(a) capturing the content of the above TDB of uncertain data.
 Mining of constrained frequent patterns from the UF-tree. Once the UF-tree is constructed, our proposed U-FPS(SAM) algorithm recursively mines constrained fre-quent patterns from this tree with minsup =1 . 0 as follows. It starts with item d (with expSup ( { d } ) = 1.8). The al-gorithm extracts from two tree paths X  X amely, (i) ( a :0.9), ( b :0.8), ( c :0.7) with the occurrence count of ( d : 0.6 )equalto 1 (implying that a, b, c &amp; d occur together once in the origi-nal database) and (ii) ( a :0.9), ( c :0.7) with the occurrence count of ( d : 0.6 ) equal to 2 (implying that a, c &amp; d occur to-gether twice) X  X nd forms the { d } -projected database. The expected support of { a, d } =(1  X  0.6  X  0.9) + (2  X  0.6  X  0.9) = 1.62. Similarly, the expected support of { c, d } =(1  X  0.6  X  0.7) + (2  X  0.6  X  0.7) = 1.26. So, both constrained patterns { a, d } and { c, d } are frequent. How-ever, { b, d } is infrequent because expSup ( { b, d } )=1  X  0.8 = 0.48 &lt;minsup .Thus, b is removed from the { d } -projected database, which then consists of a single path 0.6  X  ( a :0.9):3, ( c :0.7):3 . The UF-tree for such a projected database is shown in Figure 1(b).
 Then, the algorithm extracts from the UF-tree for the { d projected database to form the { c, d } -projected database, which consists of { a } (representing the constrained frequent pattern { a, c, d } )with expSup ( { a, c, d } )=3  X  0.42  X  1.13, where 0.42 represents expSup ( { c, d } ) for each of the 3 occurrences of { c, d } . See Figure 1(c).
 Next, U-FPS deals with item c . It extracts from three tree paths X  X amely, (i) ( a :0.9), ( b :0.8) :1, (ii) ( a :0.9) :2 and (iii) ( b :0.9) :1 X  X nd forms the { c } -projected database. Note that items in the first two paths are both associated with the same item and the same existential probability (i.e., c : 0.7 ), whereas items in the last path are associated with c : 0.5 . All this information is captured by a UF-tree shown in Fig-ure 1(d). From this tree, U-FPS finds frequent patterns { a, c } and { b, c } ,where expSup ( { a, c } )=3  X  0.7  X  0.9 = 1.89 and expSup ( { b, c } =(1  X  0.7  X  0.8) + (1  X  0.5  X  0.9) = 1.01. The constrained pattern { a, b, c } is infrequent. Finally, the U-FPS algorithm deals with item b by extracting from tree paths and forming the { b } -projected database con-sisting of 0.8  X  ( a :0.9) :1. The constrained pattern { is infrequent because expSup ( { a, b } )=1  X  0.8  X  0.9 = 0.72 &lt;minsup .
 To summarize, by applying our proposed U-FPS algorithm to the UF-tree (shown in Figure 1) that captures the content of uncertain data in Example 1, we found constrained fre-quent patterns { a } :2.7, { b } :2.6, { c } :2.6, { d } :1.8, { a, d } :1.62, { a, c, d } :1.13, { b, c } :1.01 and { c, d Recall from Section 2.1 that succinct constraints can be fur-ther divided into two subclasses: SAM constraints and SUC constraints. So far, we have discussed how our proposed U-FPS(SAM) algorithm mines from uncertain data for those frequent patterns that satisfy SAM constraints. Here, let us discuss how our proposed U-FPS(SUC) algorithm mines from uncertain data for those frequent patterns that satisfy SUC constraints. Note that, although SUC constraints pos-sess the succinctness property (i.e., one can easily enumerate all and only those patterns that are guaranteed to satisfy So, if a pattern violates C SUC , there is no guarantee that all or any of its supersets would violate C SUC . Hence, not all valid patterns are composed of only mandatory items (as for SAM constraints). Instead, any pattern S satisfying C SUC is composed of mandatory items (i.e., items satisfying C SUC ) and possibly some optional items (i.e., items not satisfying C SUC ). In other words, a valid pattern S is usually of the form  X   X   X  where (i)  X   X  Item M (where Item M is the set of mandatory items) such that  X  =  X  and (ii)  X   X  Item O (where Item O is the set of optional items). Due to succinctness, items in Item M and in Item O can be efficiently enumerated. To handle C SUC , our proposed U-FPS(SUC) algorithm can-not apply the same procedure as we did for C SAM .Some modification is needed; otherwise, we may only get a subset of (e.g., missing some of) valid frequent patterns. Specifi-cally, the algorithm first applies constraint checking to all the domain items and divides them into two sets X  Item M consisting of all mandatory items and Item O consisting of all optional items. Then, U-FPS captures items belonging to these two sets in a global UF-tree in such a way that mandatory items appear below optional items (i.e., manda-tory items are closer to the leaves, and optional items are closer to the root). Once the global UF-tree is constructed with this item-ordering scheme, the algorithm extracts ap-propriate paths to form the projection of each x  X  Item M Note that U-FPS does not need to form projections for any y  X  Item O because all patterns satisfying C SUC must be  X  X xtensions X  of items from Item M (i.e., all valid patterns must be grown from Item M items). When forming each { x } -projection and constructing its UF-tree, U-FPS does not need to distinguish those Item M items from Item O items. Such a distinction is only needed for the global UF-tree but not the projected UF-trees because optional items can come from Item M or Item O once we found at least one mandatory item for C SUC . After constructing the projected UF-tree for each x  X  Item M , our proposed U-FPS(SUC) algorithm mines from uncertain data for all frequent patterns that sat-isfy C SUC inthesamemannerasU-FPS(SAM)minesfor those satisfying C SAM . To get a better understanding of how U-FPS(SUC) handles SUC constraints, let us consider the following example.

Example 2. Reconsider the uncertain TDB shown in Ex-ample 1 with the following auxiliary information: Constraint checking and identification of the manda-tory &amp; optional items for C SUC . Let constraint C SUC be the SUC constraint C 6  X  min ( S . Weight )  X  23kg. Our proposed U-FPS(SUC) algorithm checks each of the six do-main items against the constraint C SUC ; it enumerates from the domain those mandatory items d &amp; e (each having weight  X  23kg) as well as optional items a, b, c &amp; f (each having weight &gt; 23kg), i.e., Item M = { d, e } and Item O = { Once U-FPS divided the domain items into Item M and Item O for C SUC , these items would play different roles in form-ing frequent patterns satisfying C SUC because constrained frequent patterns must contain some Item M items and may contain some Item O items.
 Construction of the UF-tree. Let the user-specified support threshold minsup be set to 1.0. Our U-FPS(SUC) algorithm constructs the UF-tree in a similar fashion as U-FPS(SAM) does for C SAM except the following. After scanning the TDB and accumulating the expected support of each domain item, U-FPS(SUC) finds the frequent Item M item d and frequent Item O items a, b &amp; c .(Items e and f are removed because they are infrequent.) Then, the algorithm scans the TDB the second time and inserts each transaction into the UF-tree. Note that, along each tree path, the Item M item is placed below any Item O items (i.e., Item M item is closer to the leaves and Item O items are closer to the root). Among Item O items, they are arranged in descending or-der of (accumulated) expected support. At the end of this UF-tree construction process, we get the UF-tree shown in Figure 1(a) capturing the content of the above TDB of un-certain data.
 Mining of constrained frequent patterns from the UF-tree. Once the UF-tree is constructed, our proposed U-FPS(SUC) algorithm recursively mines constrained fre-quent patterns from this tree with minsup =1 . 0inafashion similar X  X ut not identical X  X o that in Example 1. The key difference is that, for C SUC , our proposed U-FPS(SUC) al-gorithm only forms projected databases for Item M items (i.e., the item d in this example). Hence, U-FPS only needs to form the { d } -projected database (as shown in Figure 1(b)) and its subsequent { c, d } -projected database (as shown in Figure 1(c)). Note that U-FPS does not form the { projected database as c  X  Item M . By applying our proposed U-FPS algorithm, we found constrained frequent patterns { d } :1.8, { a, d } :1.62, { a, c, d } :1.13 and { c, d } :1.26. The experimental results cited below are based on data generated by the program developed at IBM Almaden Re-search Center [2]. The database contains 1M records with an average transaction length of 10 items, and a domain of 1,000 items. We assigned an existential probability from the range (0,1] to every item in each transaction. In addi-tion, we also conducted experiments using other databases including real-life databases from the UC Irvine Machine Learning Depository (e.g., mushroom data) as well as those from the Frequent Itemset Mining Implementation (FIMI) Dataset Repository. The results were consistent with those using the IBM synthetic data. So, to avoid distraction, we only show the results for the IBM data below.
 All experiments were run in a time-sharing environment in a1GHzmachine. Thereportedfiguresarebasedontheav-erage of multiple runs. Runtime includes CPU and I/Os; it includes the time for constraint checking, UF-tree construc-tion, and frequent pattern mining steps (if appropriate). In the experiments, we measured different aspects of our pro-posed U-FPS algorithm, which was implemented in C. In the first set of experiments, we evaluated the functional-ity of our proposed U-FPS algorithm (when compared with unconstrained algorithms). We used a database of uncertain data and a constraint with 100% selectivity (which selects every item). We compared the frequent patterns returned by our U-FPS algorithms with those returned by existing uncertain mining algorithms (e.g., UF-growth [28]). The ex-perimental results showed that both U-FPS and UF-growth returned the same set of frequent patterns. However, it is important to notice that UF-growth is confined to finding frequent patterns from uncertain data when using the user-specified constraint of 100% selectivity. In contrast, our U-FPS algorithms are capable of finding frequent patterns from uncertain data when using user-specified constraints of any selectivity.
 In the second set of experiments, we also evaluated the functionality of our proposed U-FPS algorithm (but com-paring with algorithms that mine precise data). Here, we used a user-specified constraint of some non-100% selectiv-ity (which selects some but not all items) and a database of uncertain data containing items with existential proba-bility of 1. We compared the frequent patterns returned by our U-FPS algorithms with those returned by existing constrained mining algorithms (e.g., FPS [29]). The exper-imental results showed that both U-FPS and FPS returned the same set of frequent patterns. However, it is important to notice that FPS is confined to finding frequent patterns from data with all items having existential probability of 1 (i.e., all items are guaranteed to be present). In contrast, our U-FPS algorithms are capable of finding frequent patterns from uncertain data containing items with various existen-tial probability values (which can range from 0 to 1). In the third set of experiments, we evaluated the effective-ness of constrained mining ( selectivity of constraints ). We compared the two U-FPS algorithms mentioned in this arti-cle. The y -axisofFigure2showstheruntime. The x -axis of Figure 2(a) shows the selectivity of the succinct constraints. A constraint with pct % selectivity means pct %ofitemsis selected. The higher the pct value,themoreisthenumber of selected items. As the selectivity of the SAM constraint remained unchanged for the naive algorithm. It is because such an algorithm ignores C SAM at the early stage of the mining process and finds all (valid and invalid) frequent pat-terns. In contrast, the runtimes for our U-FPS(SAM) algo-rithm decreased gradually as the selectivity decreased. This shows that the runtimes required by U-FPS(SAM) depend on selectivity. To elaborate, the number of valid patterns de-pends on the selectivity of C SAM . The computation for min-ing is proportional to the selectivity of the SAM constraint. As for the the SUC constraint C SUC , our U-FPS(SUC) al-gorithm also checks the constraints early at the initial step instead of as a post-processing step. Hence, the computa-tion for mining is also proportional to the selectivity of the SUC constraint as well.
 In the fourth set of experiments, we tested the effect of the distribution of item existential probability . Theoretically, when items take on many different existential probability values, UF-trees (for the original TDB , projected databases for singletons as well as for non-singletons) become larger and times for both UF-tree construction and frequent pat-tern mining become longer. On the other hand, when items take on a few unique existential probability values, the run-time becomes shorter. This is confirmed by experimental results shown in Figure 2(b).
 In the fifth set of experiments, we tested the effect of minsup . Theoretically, the runtime decreases when minsup increases. Experimental results shown in Figure 2(b) confirmed that, when minsup increased, fewer patterns had expected sup-port  X  minsup , and thus shorter runtimes were required. In the sixth set of experiments, we tested scalability of our proposed U-FPS algorithms. Theoretically, the algorithms should be scalable with respect to the number of transac-tions. Experimental results confirmed that mining with our proposed algorithms had linear scalability.
 To summarize, the above results showed the importance and the benefits of using our algorithms for efficient mining of frequent patterns that satisfy the user-specified succinct con-straints from databases of uncertain data. Frequent pattern mining plays an essential role in various knowledge discovery and data mining (KDD) tasks such as the mining of patterns like correlation, sequences, and as-sociation rules. Hence, it has been the subject of numerous studies since its introduction. Most of these studies find all the frequent patterns from collection of precise data, in which the items within each datum or transaction are defi-nitely known and precise. However, there are many real-life situations in which the user is interested in only some tiny portions of these frequent patterns. Finding all frequent patterns would then be redundant and waste lots of compu-tation. This calls for constrained mining , which aims to find only those frequent patterns that are interesting to the user. Moreover, there are also many real-life situations in which the data are uncertain. This calls for uncertain data mining . A key contribution of this article is our design and devel-opment of efficient and effective algorithms, called U-FPS, to mine uncertain data for frequent patterns that satisfy the user-specified succinct constraint. By pushing succinct constraints deep inside the mining process, the amount of computation and item storage in the UF-tree is proportional to the selectivity of constraints. The U-FPS algorithms ef-ficiently find constrained frequent patterns from uncertain data .
 As ongoing work, we are developing algorithms for mining constrained frequent patterns when items in the uncertain database are not fully independent (e.g., some correlated items). Moreover, we are also developing algorithms to mine uncertain data for frequent patterns that satisfy other user-specified constraints (i.e., other than the succinct ones). Acknowledgements. This project is partially supported by NSERC (Canada) in the form of research grants. [1] C.C. Aggarwal et al. Frequent pattern mining with un-[2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] R. Agrawal et al. Mining association rules between sets [4] J. Bailey et al. Fast algorithms for mining emerging [5] R.J. Bayardo Jr. (ed.) Special issue on constraints in [6] T. Bernecker et al. Probabilistic frequent itemset [7] G. Buehrer et al. Out-of-core frequent pattern mining [8] L. Cerf et al. Data-Peeler: constraint-based closed pat-[9] R. Cheng et al. Probabilistic verifiers: evaluating con-[10] C.-K. Chui and B. Kao. A decremental approach for [11] C.-K. Chui et al. Mining frequent itemsets from uncer-[12] X. Dai et al. Probabilistic spatial queries on existen-[13] R. Ge et al. Constraint-driven clustering. In Proc. KDD [14] G. Grahne et al. Efficient mining of constrained corre-[15] R. Gupta et al. Quantitative evaluation of approx-[16] J. Han and J. Pei. Mining frequent patterns by pattern-[17] J. Han et al. Mining frequent patterns without candi-[18] J. Hipp et al. Algorithms for association rule mining X  X  [19] M. Hu et al. Permu-pattern: discovery of mutable per-[20] Y. Ke et al. Mining quantitative correlated patterns [21] A.J. Knobbe and E.K.Y. Ho. Maximally informative [22] L.V.S. Lakshmanan, C.K.-S. Leung, and R. Ng. Ef-[23] L.V.S. Lakshmanan, C.K.-S. Leung, and R.T. Ng. The [24] C.K.-S. Leung. Frequent itemset mining with con-[25] C.K.-S. Leung and B. Hao. Mining of frequent itemsets [26] C.K.-S. Leung et al. A tree-based approach for frequent [27] C.K.-S. Leung et al. CanTree: a tree structure for effi-[28] C.K.-S. Leung et al. Efficient mining of frequent pat-[29] C.K.-S. Leung et al. Exploiting succinct constraints us-[30] R.T. Ng et al. Exploratory mining and pruning op-[31] J. Pei and J. Han. Constrained frequent pattern mining: [32] C. Wang and S. Parthasarathy. Summarizing item-[33] Q. Zhang et al. Finding frequent items in probabilistic
