 1. Introduction analysis of user intent within an IR task has underlined the increasing need of collaboration to answer multifaceted is expected. Indeed, for such queries and more generally, for complex and exploratory ones ( Denning &amp; Yaholkovsky, results ( Shah, 2012b ).

The issue of collaboration has given rise to the need of revisiting search interfaces, IR techniques and IR models that  X  be found in four application domains: (1) the medical domain ( McMullan, 2006; ECDPC, 2011 ) in which patients and phy-the assessment of privileged documents is performed by experts from different trades, namely lawyers, reviewers and lead previous work surrounding user search behavior domain ( Allen, 1991; Hembrooke, Granka, Gay, &amp; Liddy, 2005; White, session.

With this in mind, we aim to address in this paper the issue of designing a system-mediated CIR framework that considers of our knowledge, this is the first attempt for the design of a CIR ranking model built upon domain expertise-based roles search between the symmetric domain expertise-based roles with respect to a shared information need, i.e., novice and edge about the query topic by focusing on specific documents, while the goal of the user with the lowest domain expertise level towards the query topic is to get a better understanding of the query topic by exploring documents with a generic a document, ranks a list of those documents that have not been previously selected. The document ranking takes into novelty gain with respect to his/her domain knowledge.

A two-step collaborative document-ranking model is proposed for ranking documents according to the domain expertise-ument specificity and novelty according to user roles within a language model smoothing. Then, the Expectation X  X aximi-zation (EM) learning method ( Dempster, Laird, &amp; Rubin, 1977 ) applied on the document relevance scoring, assigns documents. In order to evaluate our proposed model, we carry out a thorough experimental evaluation for measuring the retrieval effectiveness of our model and analyzing the impact of user roles on the ranking effectiveness. More particularly, the underlying research questions are:
How to adapt language model smoothing to realize user domain expertise based document relevance scores? As dis-ing the domain expertise level of the user. For this purpose, we propose to integrate the user domain expertise level within the smoothing parameter of a language model-based document scoring.

How to utilize user expertise-based document relevance scores in a document ranking framework for collaborative search? Considering a CIR search session, the challenge remains on how to optimize the collaboration and satisfying information need and their own domain expertise and interest levels. Therefore, we focus on determining which user could be more satisfied by a document according to his/her knowledge expertise and the query topic.
In the following section, we review previous work surrounding CIR domain to put our work in context. Section 3 presents our future work.
 2. Related work 2.1. Collaborative information retrieval 2.1.1. Definition and basic notions mation need ( Hansen &amp; J X rvelin, 2005 ). A CIR setting is characterized by two main dimensions:
The human activity dimension represents collaboration as a process that encapsulates four human behavior activities, from the highest to the smallest granularity level: cooperation, coordination, contribution and communication ( Shah, 2012b ).
 tiation between synchronous and asynchronous collaboration depends upon whether or not user activities take place at the same time.

Moreover, CIR is surrounded by three principles, namely the division of labor, awareness and sharing of knowledge ( Morris &amp; Teevan, 2009; Foley &amp; Smeaton, 2010 ). All of these enable avoiding undesired redundant work.
The division of labor aims at splitting up work among users. More particularly, two main lines of approaches can be highlighted:
A task-based approach which assigns distinct search tasks among collaborators, such as looking for diversity or analyzing more in-depth document relevance ( Pickens et al., 2008; Shah, Pickens, &amp; Golovchinsky, 2010 ).
A document-based approach which (1) splits the search results in order to display to users distinct document lists result lists those documents currently seen by collaborators ( Foley &amp; Smeaton, 2009; Soulier et al., 2013 ).
The awareness alerts users of already seen documents or previous submitted queries. Collaborative interfaces may sup-port the awareness principle by means of shared workspace ( Shah, 2012a ), enabling users to be informed on the selected documents by the other collaborators, or shared interactive tabletops ( Morris, Paepcke, &amp; Winograd, 2006; Smeaton, Foley, Gurrin, Lee, &amp; McGivney, 2006 ) enabling to synchronously see other users X  actions.
The sharing of knowledge enables the information flow among users by means of shared workspaces including annotation or bookmark facilities or adapted tools favoring brainstorming among users, such as instant messaging ( Shah, 2012a;
Gianoutsos &amp; Grundy, 1996 ). 2.1.2. Previous work
The second line of works, more system-mediated oriented and close to our contribution, suggests revisiting traditional single-IR ranking techniques and models in the light of collaborative IR involving a group of users searching together of works, Foley and Smeaton (2009) and Foley (2008) have proposed the implementation of the two main collaborative a relevance term-weighting formula which combines the relevance statistics of each user through users X  authority express-ing the users X  expertise towards the search task. Experiments on simulated TREC based collaborative search scenarios show alized document rankings, in contrast to Foley and Smeaton (2009) who estimate the global relevance of documents by lin-of documents at the collaboration group level. Both of the underlying methods are based on personalized scores estimated using a profile BM25-based weighting model. As well as work in Morris et al. (2008) and Foley and Smeaton (2009) ,we ting results among users by means of the learning Expectation X  X aximisation algorithm which assigns documents to the most likely suitable users by taking into account its role within the collaborative search process. weighting and document-ranking functions have been proposed using relevance and freshness scores. Experiments using the uments. The authors propose to merge selected document sets provided by both users and then to assign documents to user tion. Experiments using TREC ad hoc dataset show promising results but emphasize the need of improving the surveyor role propose a collaborative ranking model devoted to solving a multi-faceted information need by a group of multiple users, method. Accordingly, both documents and experts are modeled by a multi-topical representation where each element rep-documents within displayed document lists. The latter are built according to a learning algorithm which assigns documents posed model considers a vertical distinction among users with a hierarchy between expertise levels. More generally, we attempt to consider the difference in the domain expertise level of the collaborators, including expert/novice users, in between a domain expert and a domain novice for any types of information needs. Accordingly, the model has been adapted in two main aspects: (1) users and documents are modeled by a term-based representation within the proposed model and (2) the document scoring with respect to each user has been tuned with respect to the characteristics of users X  roles.

To the best of our knowledge, we address in this paper a new pair of domain expertise-based roles, namely domain expert and preferences while exploring information. 2.2. Collaboration between domain expert and domain novice shared information need. Therefore, within a collaborative setting, one collaborator can have more knowledge toward the topic compared to the other ones. Moreover, the domain expertise level difference between users can be explicitly defined within an application domain. Users are also labeled as domain experts or domain novices considering the extremums of the spectrum of their domain expertise levels.

More particularly, previous work surrounding search behavior analysis ( Allen, 1991; Hembrooke et al., 2005; Hsieh-yee, main difference remains on the fact that domain experts are more familiar with technical vocabulary ( Allen, 1991;
Sontag, 2011 ). As well as query terms are more sophisticated for experts, their queries are longer and more keyword-ori-search strategies and success are different considering the knowledge expertise of users ( White et al., 2009; Hembrooke evance of retrieved documents ( Hembrooke et al., 2005 ).

Considering their collaboration need and their search behavior differences, Filho et al. (2010) have already intro-duced the pair of domain expert and domain novice within their experimental protocol. Their collaborative interface has been evaluated by a user-study involving Linux experts and Linux novices where the former help the latter to solve an information need. In addition, the pair of domain expert and domain novice roles can be found in two main catego-ries of collaboration: ergic effect and therefore, to get mutual benefits within the solving process of the information need. The whole set of users are active and complementary within the collaboration process. Below, we discuss two application domain exam-ples of this type of collaboration.
 one user which gets the benefits of other collaborators X  knowledge in order to solve his/her information need. In most of the cases, users who ask for collaboration are information consumers, whereas the other collaborators are information producers, as shown in the two following examples of application domains.
 3. The model duce the retrieval setting and then detail the ranking model. 3.1. The retrieval setting 3.1.1. Framework
We focus, here, on the retrieval aspect aiming at supporting a synchronous collaborative search setting between users with symmetric roles, namely domain expert and domain novice . The aim is to provide different documents to each user in expertise levels, and, for convenience, we characterize each user u
R X f domain expert ; domain novice g , representative of his/her knowledge expertise. The search session S is launched by documents. At the initialization step S init , the model provides for each user u each search iteration k 2 S iter is launched by the user X  X  feedback through selection of document d l  X  u ; D k ns  X  of documents that were not previously selected D vided at the different search iterations are split, ensuring empty intersections. 3.1.2. Division of labor
Our model also ensures the division of labor principle among collaborators according to three main aspects: (a) Documents are allocated according to an Expectation Maximization-based method which assigns documents to users (b) The intersection of document lists currently displayed to the whole set of collaborators is empty. (c) Documents already seen by at least one member of the collaboration group are not considered for the collaborative 3.1.3. User modeling
We formalize users by two main components, namely their roles and their profiles, respectively connected to their rel-ative domain expertise level and their own domain expertise level towards the information need. by the user through past works. Characteristics of user roles are based on the following assumptions: Vakkari, Pennanen, &amp; Serola, 2003; White et al., 2009 ).

Previous work adopted search strategies applied on non-experts, namely novices, for educating them to detect relevant documents, such as query suggestion, and allowing to gain domain knowledge on the query topic ( Hsieh-yee, 1993; H X lscher &amp; Strube, 2000; White et al., 2009 ).

One challenge of document relevance is to take into consideration its novelty in addition to it similarity regarding the query topic ( Harman, 2002; Soboroff &amp; Harman, 2005 ). The novelty need is even more important within an iterative search process in order to avoid retrieving documents similar to already selected ones.

Accordingly, domain experts represent problems at deep structural levels and are generally interested in discovering new ument novelty and document specificity, described in what follows.
 user u j according to the already selected document set D X  u
Item Novelty X  X  ( Castells et al., 2011 ) applied to the selected document set D X  u document d i given a previously selected document set D X  u where distance d depends on the Jaccard similarity function sim  X  d d  X  d i ; d i 0  X  X  1 sim  X  d i ; d i 0  X  .

Document specificity level L s  X  d i  X  estimates the level of description of terms used in document d is specific if its frequency in the collection C is low. For this purpose, the specificity level L ment frequency of term t in the whole collection, computed as follows: where df t is the number of documents including term t ; N is the collection size. rank and split the lists of candidate documents in response to the shared query. With this in mind, a user profile p  X  u assigned to each user u j depending on his/her domain expertise, based on term-frequency modeling. Considering our model user u j is extracted from the collaborative query q and its selected documents D X  u until iteration search k . For a particular term t v within user profile p  X  u within query q and each selected document d i 0 2D X  u j  X  where z is the total number of terms included in both query q and selected documents D X  u weight w k v j of term t v for user profile p  X  u j  X  k is estimated as follows: where w v q and w v i 0 denote respectively the weight of term t the tf idf scores. The number of terms included in document d vector q  X f X  t 1 ; w 1 q  X  ; ... ;  X  t v ; w v q  X  ; ... ;  X  t sidered as evidence source of the user profile. 3.2. Collaborative document ranking over an iteration The collaborative document-ranking model over search iterations includes two main steps, described in Fig. 2 :
Step 1: A role-based document scoring in order to estimate the document relevance probability for both users given their respective roles.

Step 2: A user document allocation in order to assign to each document the most likely suited collaborator according to his/her knowledge expertise modeled through his/her profile. 3.2.1. Role-based document scoring
Considering our CIR model framework, the document relevance depends upon the query topic and the users X  character-istics. First, the document relevance probability P k  X  d ditional probabilities, as shown in Eq. (5) . Then, we estimate this probability assuming that the probability P discriminant for estimating the document relevance (Eq. (6) ) and user u
On the one hand, assuming that the document scoring with respect to the query topic is invariant regardless of the search iteration and the probability P  X  d i  X  is not discriminant for measuring the probability P rewritten as follows:
In order to estimate the probability P  X  q j d i  X  , we assume that document d terms h d i . Therefore, the probability P  X  q j d i  X  corresponds to the probability P  X  q j h approach ( Jelinek &amp; Mercer, 1980 ): where tf  X  t v ; d i  X  and tf  X  t v ; C  X  are respectively the frequency of term t of terms within document d i and document collection C is noted j d
P  X  t j h C  X  are combined using a tuning parameter k .
 On the other hand, the probability P k  X  u j j d i  X  is estimated using the language-based modeling by the probability
P  X  p  X  u j  X  k j h d i  X  where user u j is estimated by its term distribution over its profile p  X  u k probability of the user profile at iteration k according to the language model h nek X  X ercer approach ( Jelinek &amp; Mercer, 1980 ): where the tuning parameter k in Eq. (9) is replaced with k d and user role R X  u j  X  at iteration k . It is estimated dynamically according to the role R X  u described in Section 3.1.3 . Therefore, the tuning parameter k according to knowledge profile p  X  u j  X  k of user u j at iteration k .

For the expert user u j and a given document d i , the more specific and the newer the document d expertise, the larger k ij . Thus, we estimate k ij as follows: where D expresses the document dataset.

For the novice user u j , a given document d i and an iteration k , we estimate k a specific vocabulary. The less specific and the newer the document d
Therefore, we propose to estimate k k ij as follows: 3.2.2. Document allocation to user roles based on the Expectation X  X aximization algorithm Here, we aim at optimizing the document relevance function over role-based scores, computed in the previous step using most likely suited collaborator considering his/her domain knowledge. For this purpose, we use the Expectation Maximiza-
Step , estimates probability P  X  R j  X  Rel j x k ij  X  of the relevance of document d sponds to the document relevance probability according to the role of user u the relevance of the document for the user role and, on the other hand, an exponential probability law to model the non-relevance of the same document for the user role. The second step, called M-Step , updates the parameters of the mixture overview of our document allocation method. The latter is detailed in what follows. 1. Learning the document-user mapping.

We aim, here, at learning how users are likely to assess the relevance of a document through an EM-based algorithm, involving two steps: follows: where P  X  x k ij ; R j  X  Rel j h k j  X  X  a k j / k j  X  x k
We build for each user u j a ranked list of documents d k
Algorithm 1. EM-based collaborative document ranking 2. Allocating documents to users.

The objective, here, is to determine which user is the most likely to assess the relevance of a document. For this pur-pose, we use each user X  X  document list d k j ranked according to the EM-based probabilities P  X  R ticularly, we focus on the rank of each document within both users X  lists d rank of a document is higher within document list d k j of user u document should be more likely assessed by user u j . Thus, we model the rank-based allocation function r k document d i as follows: where rank  X  d i ; d k j  X  and rank  X  d i ; d k j 0  X  represent the rank of document d assigning the list l k  X  u j ; D k ns  X  to the user u j which has selected a document, launching iteration k . ensuring that a document is seen/selected by only one user. 4. Experimental evaluation
Shah, 2012a ). To tackle this lack, Foley and Smeaton (2009) have proposed an experimental framework which simulates col-follow this framework in order to assess the validity of our model using a standard IR collection issued from TREC campaign. The objectives of our experimental evaluation are threefold:
In what follows, we describe the experimental setup and analyze the obtained results. 4.1. Experimental setup 4.1.1. Dataset
In our experiments, we rely on the TREC 6-7-8 interactive track, previously used within the simulation-based framework aspects, related to the information need ( Over, 2001 ). The used document dataset is the TREC Financial Times of London proposes additional details such as the time-stamp, the submitted queries, and seen documents. For our experiments, we expresses how many seconds after the beginning of the search session this document has been selected. The evaluated par-ticipants are: Berkeley TREC 6 (bkl6), rmitMG TREC 6 (rmitMG6), rmitZ TREC 6 (rmitZ6), Berkeley TREC 7 (bkl7), Toronto A
TREC 7 (torontoA7), Toronto B TREC 7 (torontoB7) and Berkeley TREC 8 (bkl8). Fig. 3 illustrates meta-data described above each user. 4.1.2. Collaboration simulation
Our experimental evaluation rests upon the simulation-based framework detailed in Foley and Smeaton (2009) adapted in two ways (1) user roles are identified through the computation of the domain-expertise levels of users involved in the hand. We review in what follows the main components of the evaluation setting. 4.1.2.1. Collaborators. Collaboration involves a pair of users u domain expertise difference between users: level, in compliance with roles X  assumptions detailed in Section 3.1.3 . The expertise level Expertise  X  u topic s is estimated according to two different methods: 1. The specificity-based expertise Expertise S  X  u j ; s  X  depends on the average of the specificity level L selected documents d i 2D d s  X  u j  X  within a search session d authority-based expertise Expertise A  X  u j ; s  X  for user u identified for TREC topic s and documents d i 2D d s  X  u j these two categories of documents.
 based domain expertise level.
Smeaton, 2010 ). We therefore consider the assumption expressed by Foley (2008) that collaborators formulate collabora-within the TREC interactive dataset, queries include generally 3 terms and 94% have at least one word in common ( Foley, 2008 ). We exploit the meta-data of TREC participants to extract the query q of collaborators c  X  u j ; u j 0  X  , the collaborative query q is obtained by merging the queries q only one collaborative query q is generated. We notice that several queries q 2 Q on the participants X  information needs formulations. 4.1.2.4. Search session. Given a pair of collaborators c  X  u D synchronization of the two lists of documents l S  X  u j  X  and l obtain the timeline L S  X  u j ; u j 0  X  of selected documents D trated in Fig. 4 .

As done in ( Foley &amp; Smeaton, 2009 ), we assume that the maximum level of relevance recall is 30 since that users may examine around 30 documents in a list. Accordingly, we only used relevance feedback expressed at the top 30 to provide the successive document rankings. During the search session S , the document list L ing to the ranked lists l k  X  u j ; D k ns  X  displayed to each user u chronized document list L S  X  u j ; u j 0  X  , two conditions must be fulfilled for enabling a user u iteration step k : (1) document d i must be included in the user X  X  currently displayed list l document d i must not have been previously selected by the other collaborator u principle.
 u a ranked list l k  X  u j ; D k ns  X  of not already selected documents D collaborative sessions d s .

Let X  X  introduce a small example illustrated on Fig. 4 . User u 5773 and FT931-8485 at respectively timestamps 89, 149 and 253. On another individual search log, we identify that user u has also selected three documents, namely FT931-5947, FT944-5773 and FT931-8485 at timestamps 151, 185 and 238. We, therefore built the following timeline: L S  X  u 1 ; u -5773 ; u 2  X  ;  X  FT931-8485 ; u 2  X  ;  X  FT931-8485 ; u 1 15,661 is currently displayed within the document list of user u
FT944-5773 is displayed within the document list of user u in this case, this document would already has been selected by user u 4.1.3. Metrics
Considering the collaborative search setting aiming at retrieving relevant documents and avoiding useless effort through the sharing of knowledge and the division of labor principles, we introduce two categories of metrics: 1. The precision-based measures aim at measuring the retrieval effectiveness of the collaborative search sessions. It is important to point out here that the length of the sessions in terms of the number of search iterations is irregular over the course of the whole search sessions. Indeed, the number of iterations varies between 3 and 12 according to collabo-rative search sessions. Highlighted irregularities of session lengths may induce a bias in basic precision measures by favoring long search sessions; to avoid this, we propose an evaluation measure for aggregating precision at the search-this in mind, we define three levels of analysis: topic s . L S denotes the set of ranked lists displayed to collaborators within a search session S . Drel relevant documents in the list and Dsel l the number of selected, namely retrieved, documents in list l . where L S ; k denotes the set of ranked lists displayed to collaborators within a search session S at iteration k . 2. The collaboration-based measures aim at estimating the collaboration optimization through the diversification results over displayed document lists throughout the whole search session ( Shah, 2012b ): where h is the set of TREC topics and L S is the set of displayed lists during search session S . Co number of distinct documents displayed during the whole search session S . The total number of documents displayed throughout the same session is noted j l j .

For evaluating the retrieval effectiveness of our model, we use the evaluation measure at rank 30, this measure fits the assumptions of the maximum relevance recall measure and allows us to compare our results to Foley and Smeaton X  X  model ( Foley &amp; Smeaton, 2009 ). Therefore, metrics are respectively noted P @ 30 ; P @ 30 4.1.4. Evaluation scenarios We defined four settings considering the different ways of building pairs of domain expert and domain novice :
Setting S Exh Spec in which collaborator groups are built using an exhaustive pooling and the users X  roles are assigned according to the specificity-based expertise Expertise S  X  u
Setting S Exh Auth in which collaborator groups are built using an exhaustive pooling and the users X  roles are assigned according to the authority-based expertise Expertise A  X  u to the specificity-based expertise Expertise S  X  u j ; s  X  .
 to the authority-based expertise Expertise A  X  u j ; s  X  .

Table 3 introduces the number of collaborative queries (and groups) within each considered search scenarios. 4.1.5. Baselines
We compare our model to one individual-based baseline and three collaborative-based baseline scenarios. Below are all the scenarios tested in our experiments. w/oDoL . This scenario includes all the components of our model, detailed in Section 3.2.1 and 3.2.2 except the second document score with respect to the domain expertise level of the whole set of users involved in the collaborative query. w/oEM . This scenario includes only the first component of our model which estimates a user-oriented document score principle by removing from the displayed lists documents displayed to other users, which leads to consider this scenario as a collaborative-based search setting. w/oEMDoL . This scenario includes only the first component of our model which estimates a user-oriented document score based search setting.

Smeaton (2009) , and designed for a relevance feedback process. Authors propose both a document relevance scoring extend-setting. The authors propose a unique ranking expressing the global relevance of documents by taking into account user ments selected by the user. The similarity sim  X  d i ; q  X  between document d partial user relevance weight purw , as follows: r uments, which contain term t . The number of documents identified by user u is noted R collection is N . The parameter a u expresses the user X  X  authority.

The query is expanded with top terms according to a term-weight, namely partial-user offer weight puow , based on the partial user relevance weight purw , computed as follows: experiments, as detailed in Section 4.1.2.1 . Foley and Smeaton (2009) propose two versions of the authority measure: the novice . 4.2. Results and discussion 4.2.1. Analyzing the ranking effectiveness at the search session level S study of domain expert and domain novice groups created from a selective pooling. This second analysis concerns settings S precision-based measures and the collaborative-based ones. We do not observe distinct different statements between both model reaches significant improvements, namely between 17% and 49% regardless of the metrics and the settings, over relevant coverage of displayed document subsets. However, we observe that the scenario w/oEMDoL without any of these to 142%. We conclude that, even if the scenario w/oEMDoL provides more effective document lists, it seems that documents search session is lower. Thus, our model is more particularly oriented to residual precision. general trend of the curves with a noticeably improvement between 115% and 119% respectively for each setting S scores depend on both users X  roles and relevance judgement-based user profiles enable to refine the shared information need, and, therefore, get a better insight of what relevant documents might be. However, we underline that the dramatic ation, namely 4 over 243 for both settings S Exh Spec and S viously obtained for the collaboration groups built upon an exhaustive pooling presented in Table 4 . More particularly, we observe a global improvement in terms of precision of our model in contrast to scenarios w/oDoL, w/oEM and FS and in terms of coverage-based ratios between 29% and 123% for the whole set of scenarios. We also notice that the expertise of the collaboration between domain expert and domain novice , identified through the selective pooling methodology. We also analyze the evolving micro-precision measure of our model throughout the whole session at the iteration level. similar to those illustrated in Fig. 5 , except a drop at iteration 8 for setting S ering that only one collaborative group among the 95 ones got more than 8 iterations.

These results lead to different conclusions. First, our model allows retrieving more distinct and relevant documents than the other scenarios throughout the whole search sessions. Second, the significant improvement of our model with respect to scenario w/oEM emphasizes the importance of the document allocation step using the EM-algorithm. Indeed, this learning method enables to estimate the user preference for documents with an expertise level as close as his own domain expertise. Third, considering scenarios w/oDoL and w/oEMDoL , we suspect that applying the division of labor
However, coverage and relevant coverage-based ratios counterbalance this statement by highlighting that our model pro-vides more distinct and relevant documents throughout the whole search sessions in contrast to models without division of labor which are likely to display redundant documents. Thus, we advocate that collaboration may improve the retrieval DoL , which only provides to users relevant documents but do not avoid redundancy between displayed document lists.
Fourth, our model retrieval effectiveness significantly improves over the collaborative document ranking FS  X  one. It can be explained by the fact that our model takes into account users X  domain expertise for assigning documents to the most on the whole users X  relevance judgments. Finally, our model can be applied in different cases. Collaborative groups can rely (1) on a relative domain expertise difference level, namely an exhaustive pooling, in which we consider that a col-laborator is more willing to know the domain than the other one or (2) on a selective pooling which clearly identifies expert and novice users of the domain. Moreover, the expertise level of users can be estimated in different ways and results show the robustness of our model regardless of the difference in the domain expertise levels between the collaborators.
 4.2.2. Analyzing the ranking effectiveness at the user role level tiveness of our model towards each role throughout the whole set of search sessions.

Table 6 gives a comparison between the retrieval effectiveness of different used ranking models regarding our four sce-playing to the a user the same top-ranked documents throughout the whole search session. In contrast, our model is more is included in the set of pairs built upon the exhaustive one S comes from the non-personalized rankings towards users having close expertise levels. This highlights another reason that probably explains these results: poor relevance feedback collected upon non-personalized document rankings, particularly ged between 19 % and 24 % with respect to the baseline FS for the domain expert users, but our model enhancement, between model applies a smoothing of documents rankings leading to an outcome that fits the users X  expertise at the average level effective as our model is for novices: we note not significant improvements, from 3 : 86 % to 23 : 81 % , for our model. and domain novice roles are significant, we propose a mean comparison of precisions between domain expert and domain nov-particular user X  role in terms of displayed document ranking effectiveness.
 Our last focus aims at analyzing whether our model enables to improve the search experience of users. In the same spirit, appropriate vocabulary towards the information need. For this purpose, we aim, here, at analyzing whether domain novice users improve their knowledge about the shared information need, and more specifically, whether they leverage displayed document lists for enhancing the specificity of selected documents. To avoid the bias underlying the specificity-based domain expertise measure used for building collaborative groups, we only performed this analysis for settings relying on displayed documents in comparison to selected documents at each iteration.

Iteration 0 refers to the initialization step S init , and the following ones represent iterations k 2 S average specificity of displayed document lists over search sessions for evaluating to what extent our model supports the domain expertise development for domain novices. For both exhaustive and selective pooling-based collaborative groups, an increasing specificity level. Second, we aim at highlighting whether domain novice users X  knowledge is improved rather than the displayed ones. We can see from Fig. 7 that the average specificity of selected documents by novice users decreases over iterations for collaborative groups built upon an exhaustive pooling method while this measure increases tive pooling-based collaborative groups might involve two domain experts with slightly different domain expertise levels. 5. Discussion and concluding remarks
In this paper, we presented a novel collaborative ranking model based on roles taking into account users X  domain exper-tise level. Our model includes a document scoring based on user roles and an Expectation Maximization-based learning method for document allocation to user roles. Collaboration is generally supported by a system-mediated approach in which the division of labor principle avoids redundancy among users X  actions, and assigns documents to the most likely suitable user. Our evaluation drawn on a TREC-based simulation framework shows that a collaborative search built on our model and diversity improvements are significant. By comparing our model to a state-of-the-art collaborative ranking model, we also underlined that the proposed model is more adapted for collaboratively ranking documents within a CIR task which assumes that users have different domain expertise levels.

Another interesting contribution of our work presented in this paper is that we highlight a generic approach for collab-within the document relevance smoothing toward the collaborator with respect to the shared information need. However, our collaborative ranking approach has some limitations: Accordingly, we plan to enhance our model by: Acknowledgement collaborative context motivation. We thank Janet Silver Ghent for her proofreading of the manuscript. References
