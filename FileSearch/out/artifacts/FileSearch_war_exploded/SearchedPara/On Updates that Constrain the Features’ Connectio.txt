 In many multiclass learning scenarios, the number of classes is relatively large (thousands,...), or the space and time efficiency of the learning system can be crucial. We investigate two online up-date techniques especially suited to such problems. These updates share a sparsity preservation capacity: they allow for constraining the number of prediction connections that each feature can make. We show that one method, exponential moving average, is solv-ing a  X  X iscrete X  regression problem for each feature, changing the weights in the direction of minimizing the quadratic loss. We de-sign the other method to improve a hinge loss subject to constraints, for better accuracy. We empirically explore the methods, and com-pare performance to previous indexing techniques, developed with the same goals, as well as other online algorithms based on proto-type learning. We observe that while the classification accuracies are very promising, improving over previous indexing techniques, the scalability benefits are preserved.
 I.2 [ Artificial Intelligence ]: Learning General Terms Algorithms, Performance, Theory Keywords Multiclass Learning, Many Classes, Index Learning, Online Learning
Many prediction and classification tasks, such as large-scale top-ical text categorization, predicting words in text, and visual classifi-cation, can be viewed as large-scale many-class learning problems, i.e. , problems with large numbers of classes (in the thousands and beyond) in addition to large numbers of features and instances (mil-lions and beyond) [16, 18, 17, 7]. In the text classification domain, text fragments, such as queries, advertisements, news articles, or web pages can be mapped (classified) into a large collection of cat-egories, for example, the topics in the Yahoo! topic hierarchy or the Open Directory Project (http://dir.yahoo.com and http://dmoz.org) ( e.g. , [5, 16, 19]), or Wikipedia topics. In these problems, the num-ber of classes can range in the hundreds of thousands. Once a search query or some piece of text is sufficiently accurately clas-sified into one or more informative classes (from thousands of pos-sible candidates), a personalization system can then take effective actions, for instance, recommend relevant ads or related items. In other scenarios, such as personalization on a desktop [3, 13], the number of classes can be relatively small (tens or hundreds), but efficiency in space and time during learning and classification re-mains crucial in practical applications.

Recently, index learning was introduced to address the challenges posed by many-class tasks, in particular the problems of how to quickly classify given an instance, and how to efficiently learn such an efficient classification system [18, 19]. A characteristic aspect of the indexing approach making it efficient is constraining the num-ber of prediction connections that each feature makes: on average, a feature should connect to only a small number of classes ( e.g. , 10s or 100s). The general update technique was termed feature-focus (FF) (as opposed to prototype focused) to emphasize this aspect. FF updating easily allows for controlling the number of connec-tions of a feature. In that work, the primary goal was assessing the viability of the general indexing approach. It was observed that in-dexing was much more efficient than the commonly used methods of one-versus-rest and top-down hierarchical classification using a taxonomy [16], while the accuracies were often better, in particular, as the number of classes is increased. The goal of this paper is to begin a systematic study of feature-focus update methods. We also compare to two other online methods (based on prototype learn-ing), namely multiclass perceptron and passive-aggressive (PA) al-gorithms [1, 2]. These algorithms also learn to rank classes given an instance.

We explore two FF update methods. One is simpler and involves exponential weighted averaging . We refer to it as EMA (pronounce it  X  X mma X ). We derive a number of properties of EMA, and in par-ticular show that it is changing the weights of a feature in the di-rection of minimizing the quadratic loss, between the probabilities that the active features assign to the classes and the true probabili-ties (as we explain). We also present the version of EMA that han-dles multiple true classes (labels) per instance. Our second update is designed for the goal of obtaining a lower 0/1 error by minimiz-ing a hinge loss. This approach, which we call OOZ (pronounced  X  X oze X ), involves a linear optimization task and requires more book keeping. Our experiments show that both EMA and OOZ updates can yield significant improvements in accuracy over the previous indexing method, while trading off some speed. In accuracy, OOZ is best over all. OOZ also compares favorably to Passive Aggres-sive and multiclass perceptron algorithms [1, 2]. These indexing methods however, as we observe in our experiments, scale to many classes, providing substantial advantages in training and classifi-cation times. The algorithms offer a novel approach even in the binary class setting.
 We next begin with preliminaries and an overview of indexing. We then describe the EMA and OOZ algorithms. Section 4 presents our empirical findings, Section 5 discusses related work, and Sec-tion 6 concludes. An expanded version of the paper with further details and proofs appears in our technical report [20].
Our setting is standard multiclass supervised learning, but with many classes. A learning problem consists of a set S of instances, each training instance specified by a column vector of feature val-ues, x , as well as a set of classes that the instance belongs to C We use x to refer to the instance itself as well. A positive class, given an instance x is any class c  X  C x ,anda negative class is any class c  X  C x . We also use the expression x  X  c to denote that instance x belongs to category c ( c is a category of x ). We say a problem is multilabeled if | C x | &gt; 1 for some instances. F and C denote respectively the set of all features and categories. Our current indexing algorithms ignore features with nonpositive value, and in our data sets, features do not have negative value. x notes the value of feature f in the vector x , x f  X  0 .If x we say feature f is active (in instance x ), and denote this aspect by f  X  x . The number of active features is denoted by | x | .Weuse o to denote the i th member in a sequence of values or observations. Good references on machine learning for text classification include Sebastiani [25] and Lewis et. al. [15].

Various existing multiclass algorithms suffer from a mix of draw-backs in large scale many-class settings. For instance, the strategy of training and using binary classifiers in a one-versus-rest man-ner becomes increasingly prohibitive as the number of classes in-creases. Plain nearest neighbors becomes impractical when S is a stream of instances of indefinite length. A plausible approach to ef-ficient classification (and updating or learning) is to try to  X  X ecall X  (or retrieve) a relatively small number of classes as each (active) feature of a given instance is examined, and then to score the re-trieved classes, akin to the use of the inverted index for document retrieval in IR. A difference is that we learn this index. We next formalize the concepts. An index is a directed weighted bipartite graph and equivalently a weight matrix W . The index maps or connects each feature to zero or more classes. An edge connecting feature f to category c has a positive weight denoted by w w i,j for feature i and category j , i  X  1 ,j  X  1 . On presentation of an instance, the index is used to score categories. The (non-negative) score of a category is determined by the active features: s those whose score exceeds a threshold can be reported (assigned to the instance). In the matrix interpretation, the rows can cor-respond to features, the columns to classes, and w i,j denotes the entryinrow i and column j . Scoring is computing the dot prod-uct:  X  C x = x T W .The outdegree of a feature is simply the number of (outgoing) edges (connections) of the feature in the index, and equivalently, it is the number of nonzeros in its corresponding row. The total outgoing weight of a feature, denoted w f ,is The vector of weights for feature f in W is denoted by W f
On presentation of instance x , since there can be at most classes scored, scoring (a dot product) takes O ( | x | d forward index implementation. Thus, if a randomly picked feature of a randomly picked instance has relatively low d , scoring is ef-ficient. d is in the order of 10s in our experiments. We will see that updates that we cover require number of operations in one or a few multiples of | x | d . In our experiments, during scoring, only the highest 25 weighted connections for each active feature are used for scoring.

To evaluate classification quality, we will mainly use the stan-dard accuracy measure R 1 (one minus zero-one error) and a more relaxed R 5 , defined as follows [18]. We sort the scored classes  X  C x in decreasing order to obtain a ranking. Let k x be the rank of the highest ranked true category for instance x in a given ranking. Thus k x  X  X  1 , 2 , 3 ,  X  X  X } . If no positive appears in the ranking, then k x =  X  .Weuse R k to denote recall at (rank) k , R k = E x k ] , where E x denotes expectation over the instance distribution and [ k x  X  k ]=1 iff k x  X  k , and 0 otherwise (Iverson bracket). In both index learning methods, the weight matrix is 0 initially. This means an empty index (no edges) in the implementation. Both methods learn nonnegative weights, motivated by efficiency con-siderations, as we explain. We begin by describing EMA.
It was shown in [18] that for the purpose of classification, fea-tures X   X  X otes X  for various classes can simply be aggregated. Fea-tures X  votes are the proportion of concepts, or class conditional probabilities, in their own stream (the stream of concepts observed whenever the feature is active). Computing simple proportions was motivated by efficiency considerations in many-class learning: fea-tures should not use complex processing, nor keep much space to track history. For efficiency, features would drop proportions under a threshold without affecting overall accuracy significantly. That work gave two proportion estimation methods, one assuming a static distribution (proportions do not change over time), and a second one appropriate for nonstationarity. The authors focused on the single label setting and used the first update in experiments with indexing. Here, we derive the properties of the latter update method, the exponential moving average updating (EMA), shown in Figure 1 in a general form handling real-valued features and mul-tilabel problems. For simplicity and for starters, imagine features are Boolean, x f  X  X  0 , 1 } ,and | C x | =1 (the single label case).
A moving average tracks the average of a time varying target value of interest, with applications in financial statistics, time se-ries prediction, and signal analysis [6]. Observations are made at discrete time points. Let o ( t ) denote the observation at time t .The exponential (or exponentially weighted) moving average, is a con-vex combination of the last average and current observation, in its where w ( t ) is the (moving) average at time t ,and  X  is a mix-ing rate, 0 &lt; X   X  1 . Equivalently, we may express w ( t ) w Figure 1: An exponential moving average version of feature updating. Therefore, the latest observation value has the highest impact on the moving average, and more generally the observation at time t is down weighted by (1  X   X  ) k , hence the name.

We can interpret the EMA of Figure 1 as computing a moving average W f of a vector valued target for each feature: Each feature in an update  X  X bserves X  a Boolean vector T of concept occurrence observations of size | C | , most entries being zero, one or a few being 1.0, indicating the positive classes. The EMA_Update performs: All the individual moving average quantities (entries), in this case the concept proportions, are updated. However, most concept pro-portions (connections) are zero and have 0 value in the observation vector, so will remain zero: nothing need be done for them. For the ones with nonzero average, they are first weakened by (1  X  for the observed ones (with value of 1.0 in the observed vector), a boost x f  X  is added. For efficiency, tiny weights are dropped (ze-roed). The exponential moving average is attractive in our setting as it is particularly simple to implement and efficient: it does not require extra memory for keeping track of history.

Note that (1  X   X  ) w +  X  = w +(1  X  w )  X &gt;w ,when w&lt; and with 0 &lt; X  . Thus, weakening and boosting a weight increases the weight w as long as w&lt; 1 . Furthermore, the total outgoing weight of a feature remains bounded, always converging to 1.0 in the single-label setting, as the following shows.

L EMMA 3.1. Let 0 &lt; X   X  1 , and assume the Boolean case. 1. The update w ( i +1)  X  (1  X   X  ) w ( i ) +  X w  X  , has a unique fixed 3. In the EMA update method of Figure 1, when  X  x, | C x Proof (sketch). The update is a linear function with non-negative slope. The second part follows from the properties of convex com-binations. The third part follows from the first and second parts, once we realize that the total outgoing weight w f is itself a moving average.

Therefore, though certain connections may be zeroed in a given update when w min &gt; 0 , the lost weight can be recaptured in future updates. There is always a push for the sum to approach 1.0. The row vector W f corresponding to feature f always remains within the unit simplex. For the multilabel setting, since the weakening step is performed only once but there can be multiple boosts, a features X  outgoing total weight may exceed 1.0. This is fine under the interpretation of the moving average: the proportions need not add up to at most 1.0, as given that a feature is active, there can be multiple positives. A feature that is always associated with the same two positive classes should obtain a weight of 1.0 to each class, its total outgoing weight should converge to 2.0.
In general w min should be set as a function of the desired max-imum outdegree, the learning rate, and the feature values. We set w min to min(0 . 005 , X / 5) . Note that a low  X  can lead to high out-degree, and slow convergence and slow adaptation to changes (slow learning). On the other hand, it allows for a more refined weight estimation, and thus potentially higher accuracy. See experiments (Section 4.2). Incorporating the degree of activity of a feature into the update affects the extent of the update. Updating proportional to activity level, or  X  X olume X , has been referred to as volume weight-ing . In addition to the moving average view for tracking possibly nonstationary concept proportions, EMA updating has a gradient interpretation. This further justifies the particular update expres-sion given in Figure 1. Observe that, as an example, if a feature always has an activity level of x f =0 . 5 , and is repeatedly updated for the same class c , w f,c would converge to 2.0, so that the prod-uct x f w f,c would converge to the desired 1 . 0 (more scenarios are explored in [20]).
Observe that as the connection strength to the true class c proaches 1.0, the strengthening amount in the update decreases. EMA tries to correct most for values farthest from 1.0. Squared loss shares this sensitivity, and we verify that EMA updating changes the weight vector of a feature in the direction of the gradient of the objective of lowering quadratic loss. Given the current connection weights of an active feature, the quadratic loss of feature f is: The partial derivatives are: the loss, a small step along the negative gradient vector is taken. With a step size of  X / 2 , W f  X  W f  X   X  2  X  l, we obtain the EMA update given in Equation 1. We note that we may naturally want to minimize quadratic loss over the class scores (not individual fea-ture X  X  votes, but their aggregation). In this case, the updates can lead to negative weights, which can make the control of outdegree difficult. We do not explore that avenue here. Figure 2: EMA updating when a margin threshold is used. First Figure 3: An example scenario showing how the scores are changed
If we always invoke EMA, the scores that each feature computes best approximate the (moving average of) class conditional prob-abilities (given the feature is active). However, in many tasks, we care about simple accuracy. In this context, we want the positive class to be ranked highest most often, and we may not mind un-calibrated (inconsistent) probabilities. For instance, at the expense of added implementation complexity, the scores can be mapped to probabilities afterwards [20]. In [18], it is argued and shown in experiments that mistake driven updating via use of a margin can significantly improve accuracy. For example, mistake driven up-dating can down weigh the effects of redundant features. Given an instance x , after scoring, the current margin is defined as: If the margin  X  falls below a threshold  X  m &gt; 0 , we invoke the EMA version given in Figure 2. In this version, we first weaken all (active) connections, then re-score concepts, so that if an additional positive class falls below the margin, it X  X  updated (the re-scoring is not necessary in the single-label setting).
In every update, EMA weakens the weights of all negative classes, while for improving ranking and thus accuracy, we need only lower those negative classes that scored higher than the positive(s). Low-ering others X  weights may unnecessarily break what has been learned before ( e.g. , can lead to misclassifying previously correctly classi-fied instances) It has been shown, in particular in the setting of support vector machine learning, that optimizing hinge loss, with proper regularization, yields superior generalization performance in terms of 0/1 accuracy compared to some other approaches. Here, we explore an online technique in which a hinge loss is improved in each update. The update becomes more complex than the EMA, and in particular requires information about the scores of (a subset of) the negative categories. Furthermore, to allow for controlling the outdegree, the update is constrained further, as we describe. This aspect helps avoid overfitting as well. We call the algorithm OOZ ( X  X oze X ) (for Online Optimization of z ( i.e. , slack) variables). The high level difference between EMA and OOZ is shown in Fig-ure 3. For the OOZ technique, we will only develop the single label setting, although we will briefly mention ideas for extending to multiple labels. Finally, there are a number of variations possi-ble that will become apparent as we describe the approach. We are able to explore only a few here. OOZ is shown in Figure 4.
Slacks. The slack (or  X  X elper X ) variable for instance i , denoted z ,isdefinedas: for a choice of a margin threshold  X  m , X  m &gt; 0 . We motivated the use of a margin threshold for improved accuracy in [18]. The slack variable is just high enough for the inequality to hold. The value of the slack variable is the same as the hinge loss on a given instance. Ideally, we want every slack variable to stay at the min-imum value of 0, and in general, we seek to minimize the sum of the values over the training set, equivalent to minimizing the hinge loss over the training set: Objective: minimize to certain constraints that include:  X  i, z x  X  0 ,  X  i, j, w and  X  i, optimize the above linear formulation.

In the online setting, on a training instance x , we compute the current value of its corresponding slack variable, which amounts to scoring concepts, or computing  X  C x = x T W . If the slack is posi-tive 1 , we update (invoke OOZ). To lower the value, we can either increase the value of the positive class, and/or lower the value of the negative classes that have scored too high, in particular, the highest scoring negative class(es). We need to make sure a single update is not so extensive as to  X  X orget X  what has been learned before (a gen-eral constraint in online learning, e.g. , [12]), Or, another way put, we need to make sure the step taken along the gradient is not too big, but substantial enough to lead to learning at an adequate rate. Equally important, a new feature should not commit all its weight to the first concept in which it is invoked to update. The learning rate (or the step amount) controls this.

Sources of Weight. We want to constrain the total weight that any feature uses, in order to constrain the feature X  X  out-degree, so the increase in the weight of the true positive class must come from some limited source. There are several choices. One candidate is the negative classes, in particular those that exceed the thresh-old point (the  X  X ffenders X ). Shifting weights from these negative
In case of multiple positive classes, the highest scoring class de-fines the slack, and gets all the boost in the update. If all have 0 score (initially), the boost is spread over all the positive classes. Figure 4: Pseudo-code for OOZ and its subroutines. OOZ shifts classes yields a kind of weight conservation or more accurately weight bounding. However, for instance initially, there may not be a positively scoring negative class at all. The solution is to as-sume each feature also has access to a  X  X ree X  but limited source of weight, w f, 0 . A good way to think about it is that each fea-ture is connected to a dummy always negative class, concept 0, and the first time the feature is seen during training, the connection has weight w f, 0 =1 . Thus in every update of a given feature f ,we can make the following distinctions regarding the source of weight to channel to the connection to the positive class w f,c x 1. The free weight source (if connected, i.e. ,if w f, 0 &gt; 2. Concepts that scored too high (if connected) 3. Any negative concept (if connected)
Intuitively, it is best to first take from the free source, when avail-able. This will have lowest impact on what has been learned before as the dummy concept is never a true positive. After that, if there still remains room for updating, the feature should deduct weights from those concepts that scored too high, if the feature is connected to them. And finally, the feature could take from any other negative concepts it X  X  connected to. OOZ currently implements the first two options. We need to determine how much each feature may shift, and how much to remove from each type of negative concept.
Extent of Change and Improvement. As seen in Figure 4, each feature X  X  connections is updated proportional to its activity level x . Given the overall desired improvement step  X  , we can verify that the change  X  f,c in any connection of feature f is at most  X x once OOZ is done:  X  f,c  X   X x f . Thus, when vector x is l ized, the improvement in score of the positive class,  X  s P value may improve by at most 2  X  . Furthermore, again with every connection change  X  f,c  X   X x f , and with l 2 normalization, it fol-lows that the change in  X  W in weight matrix W in l 2 norm is also at most  X  : ||  X  W || 2 = Therefore, OOZ as given obeys a norm constraint in changing the weight matrix.

Taking from the Free Source. As the free source is  X  X ree X , one question is why should the weight amount taken from it be con-strained in an update, at most x f  X  as given in OOZ? While taking a larger amount may have little impact on the relative ordering of other classes on previous instances (in the beginning, there are no previous instances to worry about), the intuition is that we don X  X  want to over commit to a single update and instance. We don X  X  want to give all the weight available to a feature to predict a single class. We want to see the feature active in several updates to best allocate its weight distribution. On a related consideration, we want to limit the influence of infrequent or newly seen features when predicting (see Section 3.2.1 below on further budgeting).

Weight Shifting. After taking weights from the free source (pos-sibly 0), OOZ computes the remaining overall  X  left in the update, as well as  X  f for each feature f .If  X  =0 or margin is met, the up-date is over. Otherwise, in the spirit of changing as little as possible while getting the most slack reduction, OOZ computes the offend-ing (negative) concepts, those scoring above s c x  X   X  m . It also com-putes the target (desired) deduction amounts for each. An example explains the process best. Let c 1 , c 2 and c 3 be negative classes with scores s c 1 =0 . 4 ,s c 2 =0 . 3 and s c 3 =2 . 1 , and let  X  target deduction amounts are D ( c 1 )=0 . 15 , D ( c 2 )=0 D ( c 3 )=0 : as first the score of c 1 needs to be lowered to tie c ( 0 . 1 ), then both are lowered, for 0 . 05 each, at which point  X  is ex-hausted. This manner lowers the slack the quickest. Observe that some offending classes may not be assigned a deduction. Next each active feature is sequentially processed, and deducts the minimum of x f D ( c ) ,w f,c ,R ( c ) , and r from w f,c ,wherein R D ( c ) , but goes down as features deduct weights from their con-nection to c . In these deductions, some connections can be zeroed, limiting the feature degrees.

With our current implementation, focusing on sources 1 and 2, a feature may still have some allowed change left. For instance, one feature may only be connected to offending c 1 and anther to c Multiple calls to further deduct can help reduce the allowed change (or revisiting the same instance in multiple passes). However, we can verify that OOZ as implemented makes positive progress (strictly lowers the slack in an update): if no feature is connected to a free source, then there must be offending classes with positive deduc-tion target amounts, and at least one feature connected to one.
One budget constraint is that the free source for every feature begins at 1.0 and can only decrease over time, thus total outgoing weight w f  X  1 throughout training. On finite sets, and subsequent passes on the same training set, infrequent features can develop strong connections by taking weight from the free source in every pass, leading to overfitting. Imagine every instance having a unique feature. To avoid this, we can impose stricter budget constraints. After pass k , for some choice of k  X  1 , we can remove the free source: setting  X  f, w f, 0  X  0 , effectively limiting the remaining optimization to weight shifting. We will see that this option can improve accuracy on some data sets.
OOZ takes time in  X  O ( | x | d ) , involving several passes over the connections of active features, and some sorting and hashing for efficiency. As long as the average outdegree of features d remain relatively low, OOZ will be fast. Since the learning rate  X  may be lowered in an update (when desired margin is almost met, or after the use of free source), it is possible that the weight increments and deductions may become very low in some updates. Explicit weight removal under a minimum threshold, and their transfer to the connection of the true positive, may help efficiency.
For the multilabel setting, one approach is to introduce multiple slack variables per instance, one for each positive class, and attempt to minimize the slack sum over all instances as before. Just as in the case of EMA, it is intuitive that features that tend to be in instances with multiple labels should be able to develop total weights greater than 1.0 ( e.g. , when compared to features that only occur in single-label instances). An ideas to handle this is allow access to k free sources (each initially at 1.0) when an instance has k  X  1 classes in an update. We leave exploring this avenue to future work.
Figure 5 displays our data sets. We use the same data sets Except for Advertisements, which is a Yahoo! internal data set. Figure 5: Data sets: | S | is number of instances, | F | is the number the same preprocessing and experimental set up as in [18]. Web refers to web page classification into Yahoo! directory of top-ics. The others are benchmark categorization [15, 22]: Reuters are news articles, and news groups is news postings [14]. The last data set, Austen, is a word prediction task [24], the concatenation of 6 online novels of Jane Austen (obtained from project Gutenberg (http://www.gutenberg.org/). Here, each word is its own class, and there is no class hierarchy. Each word X  X  surrounding neighborhood of words, 3 on one side, 3 on the other, and their conjunctions con-stituted the features (about 15 many). We include this prediction task to further underscore the potential of scalable learning.
All instances are l 2 (cosine) normalized (unless otherwise men-tioned), using standard features (unigrams, bigrams,..), obtained from publicly available sources ( e.g. , [15, 22]). To simplify eval-uation for the case of the taxonomy, we used the lowest true cat-egory(ies) in the hierarchy the instance was categorized under. In many practical applications such as personalization, topics at the top level are too generic to be useful. All but one data set (Reuters RCV1) is single-label per instance.

In the comparisons, we report on the average performance over ten trials. In each trial a random 10% of the data is held out. The exception is the newsgroup data set where we use the ten 80-20 train-test splits provided by Rennie et. al. [22].

In our comparisons, we report on some of the results of [18] as well. On the first 3 small sets, we report their results when com-paring to the one-versus-rest technique ( e.g. , [23]) employing bi-nary classifier learning: perceptrons, committee of 10 perceptrons, and linear SVMs (a fast algorithm/implementation [11]) using at least two regularization parameters, C =1 and C =10 . 3 On the two larger ones, the binary classifiers are deployed in a top-down method ( e.g. , [16]). The original feature-focus explored in [18] is similar to EMA updating, except that it assumes stationar-ity, and an effective rate of 1.0 was used. See [18] for details. In addition to the comparison with previous results, we also compare with other methods that stem from recent advances in efficient on-line learning. Multi-class Multi-label Perceptron (MMP) [2] and Passive Aggressive (PA) [1] are both families of prototype-based classification algorithms, wherein each category has its own asso-ciated prototype weight vector. MMP updates all the prototypes in learning, while PA only updates two and thus can be faster. To compare with strong baseline methods, we intentionally selected
Often these two suffice for text, and we have further experimented with others such as C =100 ,C =20 , and so on, on the smaller data sets, and have observed no improvement in accuracy the variant of each that demonstrated the best result in the origi-nal work. Specifically, we used the uniform update scheme and the IsErr ranking loss for MMP. PA-II was chosen for its capability to better handle label noise, with the introduction of slack variables and aggressive parameter C . For OOZ and EMA, during classi-fication, the top 25 connected classes of every feature are scored. For EMA, w min was set to 0 . 005 . For each algorithm we tested, we experimented with a few parameter settings (margin threshold and learning rate) to obtain good accuracy results. Experiments were run on an Intel quad-core Q6600 (2.4GHz) CPU with 4GB of memory.

Table 4 gives the results. We observe that OOZ improves on ac-curacy across the data sets, while EMA performs better than (orig-inal) FF in some cases. This gain in accuracy is achieved at cost of some loss in efficiency. In general, the original FF quickly con-verges (within very few passes), but the accuracy achieved may not be the best ( e.g. , Figure 6). The standard deviations are relatively low (third to second decimal point), and representatives are given in subsequent figures and tables. OOZ tends to outperform PA-II, which in turn outperforms MMP. OOZ also scores significantly higher R 1 and R 5 than the one-versus-rest and top-down binary classifier based methods using linear SVM or perceptron methods in several data sets. As explained in [18], binary classifier train-ing is often more appropriate for ranking instances with respect to a fixed class, versus our problem of scoring classes given a single instance.

We are particularly interested in comparing the accuracy with the original FF algorithm. We boldfaced the cells in Table 4 where OOZ/EMA win over FF in at least 9 out of the 10 trials (95% con-fidence from the sign test). Particularly in the large datasets, we observe substantial gain in classification accuracy of OOZ com-pared to FF. For instance, there is relatively 14% improvement in R 1 in the web dataset and 5% in the RCV1 dataset.

Finally, in terms of efficiency, we observe the benefits of index-ing. Top-down method using SVMs took more than a day and it was stopped on the Web data [18]. Similarly the PA and MMP methods take hours on Web, and we stopped them. We also see that the space used by the index is often substantially less than other methods. 4
Figure 6 shows classification accuracy of the three indexing al-gorithms OOZ, EMA and FF as a function of the number of passes in the RCV1 dataset. FF reaches its peak of performance with R1 approaching 0.79 at pass 2 and 3, but slowly declines afterwards exhibiting a possible sign of overfitting. On the other hand, the accuracy of EMA and OOZ levels off once they reach the peaks. OOZ converges slower than EMA, but achieves 2% higher clas-sification accuracy. Similar learning patterns are also observed in other datasets, though we omit the figures for space constraints.
On the smaller datasets, the space consumption of the SVMs or perceptron committees (when one-versus-rest was used) were es-timated optimistically by reporting the size of a perceptron imple-mented via a sparse space-efficient method [18] (the plus signs de-note estimates). Otherwise, if all features are explicitly represented, space consumption would simply be | F || C | , i.e. , the product of the total number of features in the problem and the number of classes. Classifier R 1 R 5 T tr e |I| OOZ (  X  :0 . 01 , X  m :0 . 2 ,p :8 ) 0.912 0.999 1.6s 5 87k EMA (  X  :0 . 1 , X  m :1 . 0 ,p :5 ) 0.892 0.996 2.5s 5 87k FF (  X  m :0 . 5 ,p :1 ) 0.884 0.997 0.7s 5 73k MMP 0.879 0.995 1s 7 116k PA-II ( C :1 ) 0.900 0.994 0.7s 8 62k SVM ( C :1 ) 0.906 0.998 11s 10 74k+ OOZ (  X  :0 . 02 , X  m :0 . 4 ,p :15 ) 0.862 0.986 72s 14 226k EMA (  X  :0 . 6 , X  m :7 . 0 ,p :30 ) 0.867 0.988 85s 13 205k FF (  X  m :0 . 5 ,p :1 ) 0.865 0.987 3.7s 10 171k MMP 0.803 0.958 6.2s 17 386k PA-II ( C :1 ) 0.840 0.966 5.7s 18 163k SVM ( C :1 ) 0.852 0.975 92s 20 189k+ OOZ (  X  :0 . 01 , X  m :0 . 2 ,p :30 ) 0.903 0.952 180s 21 230k EMA (  X  :0 . 1 , X  m :1 . 0 ,p :30 ) 0.873 0.933 150s 20 199k FF (  X  m :0 . 5 ,p :3 ) 0.886 0.949 16s 16 196k MMP 0.776 0.893 135s 93 1.6M PA-II ( C :1 ,p :10 ) 0.862 0.930 230s 87 266k SVM ( C :10 ) 0.872 0.933 235s 104 330k+ OOZ (  X  :0 . 02 , X  m :0 . 1 ,p :8 ) 0.855 0.975 72s 21 163k EMA (  X  :0 . 02 , X  m :0 . 05 ,p :20 ) 0.815 0.958 400s 23 285k FF (  X  m :0 . 1 ,p :4 ) 0.787 0.952 24s 13 220k MMP 0.840 0.979 143s 375 383k PA-II ( C :1 ) 0.847 0.966 67s 228 281k Perceptron 0.621 0.815 70s 38 760k Committee 0.769 0.918 750s 36 760+ SVM ( C :1 ) 0.783 0.939 520s 36 4M OOZ (  X  :0 . 05 , X  m :0 . 05 ,p :4 ) 0.402 0.601 31m 23 1.9M EMA (  X  :0 . 1 , X  m :0 . 1 ,p :20 ) 0.335 0.496 2h 22 1.7M FF (  X  m :0 . 0 ,p :2 ) 0.352 0.576 128s 8 1.5M Perceptron 0.098 0.224 1h+ 250 14M Committee 0.207 0.335 12h+ 190 14M+ OOZ (  X  :0 . 1 ,  X  m :0 . 1 ,p :2 ) 0.275 0.477 192s 22 1.6M EMA (  X  :0 . 1 ,  X  m :0 . 1 ,p :20 ) 0.284 0.485 6m 22 1.6M FF (  X  m :0 . 0 ,p :1 ) 0.272 0.480 40s 8 1.5M Table 1: Performance of different the learners on six datasets over 10 trials. T tr is the training time (s=seconds, m=minutes and h=hours), e is the average number of edges touched per feature of a test instance (during classification), |I| denotes the number of (nonzero) weights in the classifier ( k =10 3 M =10 6 ). Parameters used for each classifier are given in parentheses after the corresponding method X  X  name. Figure 6: RCV1: Accuracy (R1) of OOZ, EMA and FF against the number of passes.
We investigate the impact of the learning rate on prediction per-formance of EMA and OOZ. Figure 7 shows the learning curve of EMA on the Industry dataset with  X  set to 0.02, 0.05, 0.1 and 0.2 (margin of 0.3). As we increase  X  , R 1 converges faster, and train-ing time is faster (not shown). However, the excessively aggressive learning rate  X  =0 . 2 leads to inferior performance. Setting  X  to 0.1 in this dataset, on the other hand, yields both faster convergence and best R 1 score. We observed similar learning patterns for OOZ on the RCV1 dataset (Figure 8,  X  = { 0 . 01 , 0 . 02 , 0 . gin set to 0.1). Setting  X  to 0.05 in this dataset, R 1 peaks at 0.855 in the fourth pass while the aggressive (  X  =0 . 1 )andslow(  X  learning rates result in worse accuracy.
 Figure 7: Accuracy as a function of the number of passes with different learning rate  X  for EMA in the Industry dataset. The tails of the curves are shown in the inset.
Here, we investigate the effects, on the OOZ algorithm, of shift-ing weights from negative classes, lowering the learning rate, and as an extension, imposing a stricter budget than 1.0, i.e. , removing the free source after a certain pass (setting w f, 0 to 0) to avoid po-tential overfitting. As illustrated in Table 4.3, weight shifting is im-portant, improving R 1 by 7.5% and R 5 by 5.2%. If the free weight source is removed after two passes, R 1 and R 5 can be further im-proved by 2-3%. The option to lower the learning rate (when de-sired margin is smaller) is also important for OOZ, without which the accuracy degraded by 9%. Figure 8: Accuracy as a function of the number of passes with different learning rate  X  for OOZ in the RCV1 dataset.
 Table 2: Effects of weight shifting (WS), weight source re-moval (SR), lowering learning rate (LLR) of OOZ in the RCV1 dataset. Standard deviation is shown in square brackets.
The work in [19, 18] motivated many-class learning problems and provided evidence that very efficient linear learning methods, via  X  X oncept indexing X , exist. Our work further explores the ap-proach and expands and improves the methods.

There are a variety of multiclass techniques, but past research has not focused on large-scale many-class learning, and in particular the problem of efficient classification. The multiclass perceptron and related algorithms ( e.g. , [4, 2, 1]) have similar ranking goals. These methods are best viewed as learning  X  X rototypes X  (weight-vectors), one for each class 5 . On an update, certain prototypes X  weights for the active features get promoted or demoted. We will refer to them as prototype methods , in contrast to our predictor-based or feature-focus methods. In terms of the weight matrix, where features are rows and concepts are columns, indexing or feature-focus approaches work on the rows, while prototype meth-ods operate on the columns (Figure 9). L1 regularization in partic-ular yields prototypes with fewer nonzero weights compared to L2 regularization ( e.g. , see LASSO in [10]). For the setting of many-class learning, it is possible that scalable discriminative methods other than predictor-based ones exist as well. It is conceivable that some kind of prototype regularization may render prototype methods efficient for large-scale many-class learning, but here are some difficulties that we see: Computing prototypes while simul-taneously preserving efficiency may be more challenging as con-straining the indegree of classes may not guarantee small feature outdegrees (important for efficient classification and updates) and different classes can require widely different and large indegrees for good accuracy, as some classes are more typical or generic than others [18]. Furthermore, updates that involve prototype weight normalization or sparsity control require extra data structures ( e.g. ,
One method studied theoretically normalizes the whole matrix. Figure 9: In learning a weight matrix, in which rows are features and pointers) for efficient prototype processing, in addition to the space required for the index (the feature to category mapping), complicat-ing the approach. Note however that in developing OOZ we used similar general techniques developed previously, in for instance op-timizing a hinge loss in multiclass setting ( e.g. , [12, 1]), but subject to different constraints (feature outdegree budgets). Variations of OOZ, including batch optimization, and allowing for learning neg-ative weights, may also prove fruitful.

Moving averages are used in time series analysis and prediction, for instance in ARMA and ARIMA models [6]. A Boolean single-feature version of EMA has also been used for the task of Unix command prediction for personalization [3, 13], to adapt to non-stationarities, wherein it was called  X  -updating. Temporal differ-ence learning of state values utilized in reinforcement learning also has the form of EMA [26]. Feature-focus algorithms, as predictor-based methods, have similarities with additive models and tree-induction [10]. They may be thought to be in the family of so-called expert (opinion or forecast) aggregation and learning algo-rithms ( e.g. , [21, 8, 9])). Previous work has focused on learning mixing weights for the experts, while we have focused on how the experts may efficiently update their votes.
We developed two feature focus methods for multiclass learn-ing, and demonstrated their potential for improved accuracy while maintaining efficiency in many-class settings. We plan to further study and advance the techniques, and explore applications. These algorithms extend the repertoire of tools available for large-scale learning and data mining.

