 and fully-grouped methods. We demonstrate th is performance improvement by running various experiments on two "real-world" datasets. Finally, we examine head/tail splitting strategies reducing error rates of recommendati ons and demonstrate that this partitioning often outperforms clustering of the whole itemset. systems has been addressed previously in [3] and [4]. In particular, [3] analyzed the impact of recommender systems on sales concentration and devel oped an analytical model of consumer purchases that follow product recommendations provided by a recommender system. The recommender system follows a popularity rule, recommending the bestselling products to all consumers, and they show that the process tends to increase the concentration of sales. As a result, the treatment is somewhat akin to providing product popularity information. The model in [3] does not account for consumer preferences and their incentives to follow recommendations or not. Also [3] studied the effects of recommender systems on sales concentration and did not address the problem of improving recommendations for the items in the Long Tail, which constitutes the focus of this paper. In [4], a related question has been studied: to which extent recommender systems account for an increase in the Long Tail of the sales distribution. [4] shows that recommender systems increase firm X  X  profits and affect sales concentration. This is the case because our appro ach can be viewed as a solution to the cold start problem for the items in the Long Tail that have only very few ratings. A popular solution to the cold start problem utilizes content-based methods when two items with no or only few ratings are inferred to be sim ilar based on their content [1]. In our work, we use grouping of items in the long tail, rather than the content-based methods to identify similar items and to leverage their combined ratings to provide better recommendations. recommender systems. In particular, [9] clusters similar users into the same cluster to overcome th e data sparsity problem for collaborative filtering. Also in [8], item clustering is used to improve the prediction accuracy of collaborative filtering where items were divided into sm aller groups, and existing CF algorithms were applied to each group category separately. We use related clustering ideas but in the context of the Long Tail phenomenon to leverage few ratings of the items in the Long Tail. In this section, we provide some background information about the Long Tail problem in recommender systems and its solutions. We assume that there is a set of items I, a set of customers C and the set of known ratings R = {r ij } provided by the customers in C movie Toy Story had 272 ratings, we can build a linear regression model to predict the unknown ratings for that movie, use RMSE to measure performance of the mode l, and apply 10-fold cross validation to compute RMSE for that movie. This process was repeated 1682 times for each movie in MovieLens. As Figure 1 demonstrates, the movies in its long tail have only few ratings, and predictive models are learned from only few training examples using the EI method. Finally, since we used 10-fold cross validation, the minimal number of ratings needed for the model-building purposes was 10, which was the case with MovieLens. methods to the whole itemset I, we call it Total Clustering ( TC ) recommendation method. In other words, TC clusters the whole itemset I into different groups and builds rating predictive models for each resulting group. Finally, if we split itemset I into the head H and tail T, apply clustering only to the tail T while leaving head items un-clustered, and build data mining models for each cluster in tail T and individual models in head H, we call it Clustered Tail ( CT ) recommendation method. method is that only few ratings ar e available in the Long Tail, and the performance rates of these mode ls deteriorate in the Long Tail of the itemset I. We describe this problem in Section 2.2 and present various ways to address it in the rest of the paper. We used the Each Item ( EI ) method to build rating estimation models for individual items in I, as described in Section 2.1. We have used Weka [7] to repeat th is model building process across two datasets (MovieLens and BookCrossing), two types of performance measures (MAE and RMSE) and ni ne types of predictive models: (1) SimpleLinearRegression ( SLR ), (2) Gaussian radial basis function network ( RBF ), (3) Support vector machines ( SVM ), (4) K-nearest neighbours ( KNN ), (5) Locally-weighted learning ( LWL ), (6) Bagging classifier ( Bagging ), (7) DecisionStump tree ( DecisionStump ), (8) M5P model tree ( M5P ) and (9) 0-R Rule Induction classifier ( ZeroR ). Furthermore, we have build these individual item models using five sets of derived variables that served as independent variables in the model. Therefore, the total number of experiments for Each Item method are 90 (2  X  9  X  5) for each data set. presented in Figures 2 and 3. Figures 2(a, b) show the MAE and RMSE error rates respectively for each of the aforementioned nine predictive models for the MovieLen s dataset. The movies in the graphs are ordered according to the number of ratings (ranks) that are plotted on the x-axis. Figures 3(a, b) provide the same measurements in formation, but for the BookCrossings dataset. increase in the tail of the figur es (for the items with only few ratings). To demonstrate this effect more clearly, we performed the correlation analysis and computed Pearson X  X  correlation coefficients between the rating numbers and error rates. The results are presented in Table 1, and it show s that all the 90 models for the BookCrossing dataset have signi ficant negative correlations between the rating numbers and the error rates and 70 (out of 90) models have significant negative correlation for the MovieLens method, the error rates tend to increase for the low-ranked items in the tail of itemset I . We call this problem the Long Tail Recommendation Problem (LTRP). It occurs because rating prediction models do not have enough data for the less popular items in the Long Tail. We address th is problem in the rest of this paper. One way to do it is to cluster items in I so that predictive models are learned using more da ta, thus, decreasing error rates for the less popular items. We describe this approach next. Since the LTRP problem is caused by the lack of data to build good predictive models in the tail, clustering items using the Total Clustering ( TC ) method from Section 2.1 can be a reasonable solution since it can provide more data for the less popular items. experimental settings for the TC method are the same as for EI , as described in Section 2.2; however the TC method has an additional factor  X  the number of clusters. Thus, for the TC models, we consider nine data mining methods, five sets of the independent variables, two perfo rmance measurements and five clustering groups having 10, 20, 30, 40 and 50 clusters in total. We cluster items in these groups using the Expectation-Maximization ( EM ) clustering method [7]. Thus, the number of experiments for the TC case becomes 450. 1682 movies into 10 group using the EM method [7] and build a predictive model, e.g., a Suppor t Vector Machine for each group (10 SVM models in total). If we want to predict the unknown rating of movie The Other Boleyn Girl for customer C then the TC method, first, determines into wh ich of these 10 groups that movie belongs. If the movie belongs to group G5 , consisting of 30 other movies having 10000 transac tions among them, then TC applies SVM method to group G5 and computes RMSE error rates using 10-fold cross validation on these 10000 ratings. This process was repeated 10 times on MovieLens for each cluster. methods across the MovieLen s and BookCrossing datasets respectively. Figure 4 uses the Simple Linear Regression ( SLR ) method and 10 clusters for the TC method. Figure 5 uses the Locally-Weighted Learning ( LWL ) method and also 10 clusters for the TC method. These two graphs clearly show that the TC outperforms the EI method, especially in the tail of the distribution, where the gap between the two lines is clearly visible. In order to formally ve rify this visual observation, we performed the paired t-test. Tabl e 2 presents the paired t-test results and shows that for the MovieLens data in 448 (out of 450) cases the error rates of the TC models are significantly smaller than the EI error rates at the 95% confidence levels. Likewise, in cutting strategies. However, in th is section, we assume some arbitrary partitioning of the itemset I into the head and the tail. T as follows. First, for each item, we compute several derived variables from the transactional variables for that item. For example, for the movie items, examples of these derived variables are the average rating of a movie ( I_aver_rating ), the popularity of the movie ( I_popularity ) and how much customers liked the movie ( I_likablility ). As a result, each item becomes a point in the space of the derived variables. For example, if we have 10 derived variables and 100,000 movies, then each movie becomes a point in the 10-dimensional space that has 100,000 da ta points in total. Next, we apply standard clustering methods to identify particular clusters in that space (e.g. cluster these 100,000 points in the 10-dimensional space). In this paper we used the EM clustering method [7]. individual predictive models for each item in H for the following reason. As Figures 4 and 5 show, the error rates for the TC and EI methods are relatively close near the origin of these two graphs, which is in contrast to the error ra tes in the right parts of the graphs. This can be explained by observing that the popular items in the head have already considerable ratings data, unlike the less popular items in the tail. Thus, clusteri ng items in the head should not contribute significantly to the performance of the corresponding data mining models. Therefore, we cluster items only in tail T and build individual data mining models for each item in the head . We call this approach the Clustered Tail (CT) method. estimation models on the resulting clusters. For example, if we clustered movies A , B and C into one cluster, we take the ratings assigned to these three movies, in formation about the movies and customers and use linear regressi on to estimate unknown ratings of the three movies. of experimental settings and m easured by how much it improves performance and solves the LTRP problem. We present our experimental settings in Section 4 and the results in Section 5. In this section, we explain th e experimental settings used for validating the Clustered Tail ( CT ) method, including an overview of the data used, selected variable s, data mining methods, performance measurements and statistical tests. Data. We used two popular datasets in our study MovieLens [5] and BookCrossing [6]. The MovieLens dataset contains 100,000 ratings on the scale of 1 to 5 from 943 customers on 1682 movies. The BookCrossing dataset contains 1,149, 780 ratings on the scale of 1 to 10 from 278,858 customers on 271,379 books. Variables. In order to predict unknown ratings, we used the following derived variables ( DV ) as independent variables in our data mining models. Custome r-related derived variables DV are : measures. After building the model, we predict the unknown rating on the holdout sample and calculate the error rates as: next section we present the results of our experiments. In this section we present the re sults of the experiments described in Section 4. In Section 5.1, we will focus on comparing the Clustered Tail ( CT ) and the Each Item ( EI ) methods and demonstrate that CT outperforms EI . In Section 5.2, we will focus on finding the most appropriate head/tail cutting point and determining the appropriate clustering number for the CT method. To compare the Clustered Tail ( CT ) and the Each Item ( EI ) methods, we performed the paired t-tests across various experimental settings described in Section 4. In particular, we perform these tests across five se ts of variables, five cutting points, six clustering groups, nine data mining methods and two measurements. This results in 2700 comparisons of the CT and EI methods for each dataset. particular predictive model for each item in the itemset I for the CT and the EI methods. To check for the statistically significant differences between them, we perfo rmed paired t-tests, but only for the items in the tail T of the distribution. In other words, if E = (e the tail T for methods CT and EI respectively, then we perform the paired t-test of sets E and E ' to detect the statistically significant differences between the two. The reason for considering the errors only in the tail T, is that the errors in the head H for the two methods CT and EI are the same, and there is no point in including them in the test. shows that for the MovieLens dataset the CT model outperforms the EI model in 2464 cases at a 95% c onfidence interval out of the total 2700 comparisons. Similarly, for the BookCrossing dataset, the CT model outperforms the EI model in 2525 cases. 
Table 3 . Paired T-test Result (Statistically significant at the 95% the CT and the EI methods, without showing any specifics for the individual comparisons. Since there are 2700 of them in total, it is impossible to present the specifics on all of them. Therefore, we decided to examine the performance differences between the CT and the EI models for some selected individual setting (out of the total of 2700 of them). presents the histogram of the average improvement rate of CT vs. the EI methods for the RMSE errors for the BookCrossing dataset. outperforms the EI method in most of the cases. Only in 49 cases (out of total of 1350) the differences between the CT and EI performances are negative for the MovieLens and in 53 cases for the BookCrossing datasets. Also, the performance improvements go as high as 11.78% for the MovieLens and 72.45% for the BookCrossing datasets. Figure 8. Histogram of the average RMSE improvement rate of Figure 9. Histogram of the average RMSE improvement rate of results across extensive experimental conditions clearly show that the CT method produces significant performance improvements vs. the EI method. All this means that clustering each item in the Long Tail T indeed produces better recommendations. into head H and tail T parts and how to cluster the items in tail T. As was observed earlier in the paper, error rates in the tail T depend on where we cut the itemset into the head and tail. In this section we empirically study where the best cutting points are and whether it first place. items with the ratings frequency more than i belong to the head H and the items with frequency less than i to the tail T. In addition, we also consider the special case of a vacuous head H, where all the items appear only in tail T (and nothing in the head). Note that this is the Total Clustering ( TC ) case described and discussed in Section experimental conditions for each of the two datasets (while only 4 of them are presented in Figur es 10(a, b) and 11(a, b)). (representing the best model case) in Figure 12 for the MovieLens and in Figure 13 for the BookCrossing datasets. Both Figures 12 and 13 show that in a signifi cant number of cases, the TC solution constitutes the best case scenario producing the minimal error rates. For example, Figure 11(a) clearly demonstrates this point since the minimal error rate is achieved for the Total_50 case. difference between the smallest e rror rate achieved for Total_20 and the second best point of 110-30 is highly insignificant (1.0218 vs. 1.0227 in this case). This means that, even though the Total Clustering (TC) is theoretically the best solution, in practice it may not be the case since it is usually computationally very expensive, while it achieves highly insignifican t improvements in error rates. Therefore, it may be better to replace such TC model with the second best, but much cheaper CT model. We next compare performances of the Best vs. Sec ond-best models by applying the paired t-tests to the overall performances of the corresponding models. If the differences are statistically insignificant at the 95% confidence level, we replace the best-performing TC with the second-best-performing CT model. Then the Practical best performing model ( Practical solution) is: select the best-performing model TC if the second-best one is significantly worse. Alternatively, if the performance differences are statistically insignificant, then select the second best CT model. The histograms of the Practical solutions are show n with black bars in Figures 12 and 13 for MovieLens and BookCro ssing datasets respectively. 
Figure 11. Average RMSE according to the cutting point and the fewer best-performing Practical models for the Total Clustering ( TC ) case. In fact, most of the practically best-performing models fall within the middle region in both figures (Figures 12 and 13). itemset I (no head/tail partitioning) results in the best performance results. However, in many cases in our study, it turns out that the best cutting point  X  lies somewhere in the middle of the itemset distribution histogram and therefore the TC method does not always constitute the best approach. This also means that a good cutting point  X  and the right number of tail clusters need to be selected carefully since these parameters affect the performance of recommender systems significantly and the good choices of their values depends on various parameters that vary across different datasets and recommendation approaches. a) the item-based Long Tail of the ratings distribution does matter; b) the items in the Long Tail can be used productively by clustering them into various gr oups; c) the practically best head/tail cutting points often lie in the middle of the range, as Figure 13 shows, and empirically finding such cutting points. of our experiments took a long time to run, especially when the head/tail cutting point  X  was skewed more towards the head and the number of clusters in the tail was small. We would also like to develop incremental algorithm for determining optimal splitting points when new rating and other data about items and users is added or changed dynamically. We would also like to combine our CT method with other recommendation approaches, such as collaborative filtering, and see if this combined method improves performance even further. Finally, we studied the binary splitting problem of the itemset into the Head and Tail. In the future, we would like to consider multiple (non binary) partitioning of the item base, each partition having its own grouping methods. [1] Schein, A., Popescul, A., Ungar, L. and Pennock, D. 2002. [2] Anderson, C. 2006. Th e Long Tail. Hyperion press. [3] Fleder, D.M., and Hosana gar, K. 2008. Blockbuster Cultures [4] Hervas-Drane, A. 2007. Word of Mouth and Recommender [5] http://movielens.umn.edu. [6] http://www.bookcrossing.com. [7] Witten, I.H., and Frank, E. 2005. Data Mining: Practical [8] Truong, K.Q., Ishikawa, F., Honiden, S. 2007. Improving [9] Ungar, L.H. and Foster, D.P. 1998. Clustering Methods for 
