 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval-Selection process.
 Algorithms, Design, Experimentation. Opinion Summarization, Sentence Retrieval, Topic Detection. newly discovered opinions is important for governments to improve their services and companies to improve their products [1, 3]. Because no queries are posed beforehand, detecting opinions is similar to the task of topic de tection on sentence level. Besides telling which opinions are positive or negative, identifying which events correlated with such opini ons are also important. This paper proposes a major topic detec tion mechanism to capture main concepts embedded implicitly in a relevant document set. Opinion summarization further retrie ves all the relevant sentences related to the major topic from the document set, determines the opinion polarity of each relevant sentence, and finally summarizes positive and negative sentences, respectively. main concepts of a relevant document set is the spirit of our major topic detection. A term is cons idered to be representative if it appears frequently across documents or appears frequently in each document [2]. Such terms form the major topic of the relevant document set. How to choose th e major topic is described as follows. We assign weights to each word both at document level and paragraph level. In the following formulas, W denotes term frequency, and N is word count. In the subscripts, symbol i is the document index, symbol j is the paragraph index, and symbol t is the word index. Formulas (1) and (2) compute respectively. Formulas (3) a nd (4) denote how frequently term t appears across documents and paragraphs. Formulas (5) and (6) denote how frequently term t appears in each document and in each paragraph. A term is thought as representative if it satisfies either Formulas (7) or (8). Terms satisfying Formula (7) tend to appear in few paragraphs of many documents, while terms satisfying Formula (8) appear in many paragraphs of few documents. The score of a term, defined as the absolute value of minus , measures how significant it is to represent the main concepts of a relevant document set. scoring function in paragraph level. All documents in the same corpus are concatenated into a bigger one, i.e., the original boundaries between documents are neglected, so that words which repeat frequently among paragraphs will be chosen. We also use threshold TH to control the numbe r of representative terms in a relevant corpus. The larger th e threshold TH is, the more the number of terms will be included. The value of this parameter is trained in the experiments. satisfying a specific topic from a document collection. In our experiment, those sentences that contain a sufficient number of representative terms are considered as relevant sentences to the major topic. WordNet synonyms are adopted to expand the coverage of representative terms. All relevant sentences are collected in a topical set and are called topical sentences hereafter. Conference) 2003 and 2004 [4] are used as the experiment corpus to verify the performance of our relevant sentence retrieval. There are 50 document sets in 2003 TREC novelty corpus, and each set has 25 documents. All documents in the same set are relevant. That meets the assumption of our ma jor topic detection. Take set 2 ( X  X lone Dolly sheep X ) as an example. It discussed the feasibility of the gene cloning and the perspectives of the authority. The extracted terms and the related scores to represent the major topic of this set are shown in the left part of Table 1. For comparison, the original topic narrative is listed in the right part. 
Table 1. Major Terms Extracted vs. Original Topic Narrative adult 232.64 aging 174.52 dolly 168.08 
DNA 153.73 cell 153.19 nucleus 131.25 genetic 106.91 human 99.24 medical 82.58 lamb 63.89 clone 27.92 Section 2 rather than the topic given in TREC novelty corpus, we can examine the performance of the relevant sentence identification to verify the accuracy of the major topic detection. The more accurate the relevant sentence retrieval is, the more precise the major topic detection is. Sentences with more qualified terms are considered as relevant. Table 2 shows the experiment results. The aver age F-measure is 0.617 for 2003 TREC novelty corpus, which is bette r than most runs submitted by the participants [4]. It shows that our major topic identification can capture the concepts embedded in documents. performance depends on the quantity of relevant documents. Experiments on 2004 TREC novelty corpus demonstrate the point. This corpus contains relevant and non-relevant documents in the same set. If all documents ar e used, F-measure is only around 0.38 (TREC 2004 A). However, if non-relevant documents are filtered in advance, F-measure 0.46 is achieved (TREC 2004 B). and their tendencies are determined. The procedure is as follows. orientation. If the score is positive/negative, the sentence is positive/negative-oriented. Beside s, we also consider opinion keywords, e.g.,  X  X ay X ,  X  X re sent X ,  X  X how X ,  X  X uggest X , etc . If a sentence contains such opinion keywords which follow a named entity with zero opinion score, it is regarded as a neutral opinion. opinion-oriented degree. We distinguish positive and negative documents. A document is positive if it has more positive-topical sentences than negative-topical ones; and vice versa. Among positive and negative documents, two types of opinion summarizations are proposed, that is, brief and detailed opinion summary. For brief summary, we pick up the document with the largest number of positive or ne gative sentences and use its headline to represent the overall summary of positive-topical or negative-topical sentences. For detailed summary, we list positive-topical and negative-topical sentences with higher opinion-oriented degree. Examples are shown in Tables 3 and 4. Table 2. TREC 2003 &amp; 2004 Relevant Sentences Retrieval TREC 2003 0.56 0.85 0.617 TREC 2004 A 0.28 0.87 0.38 TREC 2004 B 0.34 0.86 0.46 Positive Chinese Scientists Sugge st Proper Legislation for Clone Negative UK Government Stops Funding for Sheep Cloning Positive Ahmad Rejai Al-Jundi, Assistant Secretary General of Negative Dolly the cloned sheep is only 3, but her genes are this paper, including major topi c detection, relevant sentence summarization. Our system ach ieved 61.7% F-measure using 2003 TREC novelty corpus, and 46% in 2004 TREC novelty corpus if non-relevant documents are filtered in advance. Major topics and the summarization of the corresponding opinions from a large quantity of documents are very useful for the government, institutes, companies, and the concerned public. [1] Dave, K., Lawrence, S., and Pennock, D.M. Mining the [2] Fukumoto, F. and Suzuki , Y. Event Tracking based on [3] Morinaga, S., Yamanishi, K., Tateishi, K. and Fukushima, T. [4] Soboroff, I. and Harman, D. Overview of the TREC 2003 
