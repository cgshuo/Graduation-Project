 1. Introduction
Ranking items is a core task in several information retrieval fields such as spam webpage detection ( Dwork, Kumar, Naor,
A few approaches have been developed to solve ranking aggregation problem, e.g., Borda Count ( Aslam &amp; Montague, 2001 ) and Markov Chain based methods ( Dwork et al., 2001 ). In Borda Count, the ranking score is assigned to the items
Chain based methods obtain the final ranking list based on the pairwise comparisons of items to be ranked (denoted by four Markov Chain based methods in which the definitions of transaction matrix are different.  X  proaches are unsupervised. In these methods, only ranking lists are available and no other additional information  X  such 2005, 2006a, 2006b ). For example, Klementiev and Small (2007) proposed an unsupervised learning algorithm for rank aggregation (ULARA) which assigns different weights to different rankers. In their paper, a mean ranking is calculated by ilarity to it cannot reflect the real reliability of each ranker.

In order to improve the quality of ranking aggregation, Liu, Liu, Qin, Ma, and Li (2007) proposed a supervised learning ing score can be calculated by making use of the learned weights. However, the main problem of this method is that people our knowledge, we are the first one who consider the semi-supervised problem in ranking aggregation. There are several advantages to use the semi-supervised methods. First, we can use the preference constraints to guide the aggregation pro-cess. Second, the preference constraints are easier to obtain than the whole ranking of the item set in practice.
In our semi-supervised ranking aggregation method, we assumed that all the items are on an intrinsic data manifold. The which is similar to more ranking lists should be weighted more than others. The third hypothesis is added because it is quadratic programming problem which is easy to solve by existing techniques.
 the experiment results on toy data and OSHUMED data which show that the proposed method have better performance than Markov Chain based ranking aggregation methods, the Borda Count method, RankSVM and supervised rank aggregation methods especially in the unreliable case. 2. The algorithm initions and preliminary. 2.1. Preliminary satisfies the following constraints: where e is an all-one vector. 2.2. Weight disagreement it can be measured by the agreement between this ranking list and the ground truth. However, in the unsupervised ranking each ranking list r i . S  X  r i ; R  X  is given by: scribe the dissimilarity between two ranking lists, such as Spearman distance ( Crichton, 1999 ) and Kendall tau distance list is described by the similarity between this list and the mean one in the ULARA. Let the distance between the weight vector w which we want to learn and the internal weight vector p by case, it is apparent to find that the optimal w * = p .
 In the semi-supervised case, we suppose the preference constraints on a few items are given as side information ( Xing, agreement between the output of aggregation function and the ground truth is given by the inequality:
N , the first and second items in t -th constraints by t 1 and t 2 respectively, the Eq. (6) can be rewritten by: where
The learned weights should minimize the weight disagreement and satisfy the inequality in Eq. (7) simultaneously. Then we can get the semi-supervised optimization problem: 2.3. Regularization framework manifold should have similar ranking scores, i.e., we should find an aggregation function f the value of which changes smoothly on the data manifold. Here the smooth function is defined as: imizing the following optimization problem: where L is the Laplacian matrix. The Laplacian matrix is defined as follows: if W is defined to be a N N matrix with W ij where the similarity between items is usually calculated by Gaussian function: to increase the performances of manifold based methods Incorporating the Eq. (1) we can rewrite the smooth regularizer S  X  f  X  by S  X  w  X  : The goal is to find the optimal w that minimizes the regularized version of g ( w , p ): disagreement, smoothness regularizer and the function complexity regularizer. The hard margin semi-supervised optimiza-tion problem is then given by: where a , b are both tradeoff parameters with a &gt;0, b &gt; 0 and 1 a b &gt;0.
 optimization problem: C is a constant which is used to control the trade off.

Let us describe the solution of the optimization problem for our semi-supervised ranking aggregation method. Without the first and second terms in the objective function, our methods degenerates to Ranking SVM. We can use a similar opti-mizing method as Ranking SVM. On introducing the Langrangian multipliers to the hard margin optimization problem, problem can be reduced to the dual problem: Let The Q , c and a are defined as follows: where 0 is a zero vector. And the solution of the original problem is w  X  1 2 A 1 x where i.e. A is positive define. Therefore the inverse matrix A 1 exist. It is also positive define.

We also can obtain the duality of the soft margin problem: (18) and (19) are convex quadratic programming problems which can be solved by existing techniques. If we get the optimal x * =[ s * T , q * T , r * ] T , the weights w can be calculated by
Our hard margin semi-supervised ranking aggregation method can be summarized in Table 1 . 3. Experiments
In this section, we illustrate the experiment results of our method on synthetic toy data and TREC data. Our method is compared with the Markov Chain based methods (MC1, MC2, MC3, MC4), the Borda Count (BC), RankSVM (RS) ( Joachims, 2002 ) and supervised rank aggregation method (SRA)(Liu et.al. (2007b)). In the RankSVM and SRA, we give the same pref-erence constraints as we give in our semi-supervised methods. The document pairs with preference constraints are used as training data while other pairs as testing data. We also experiment on the proposed method without manifold smoothness hypothesis (SSRA2). Let us represent several settings in the experiments below. 1. Trade off parameters: a and b are set to receive the best results of our semi-supervised ranking aggregation method. variate from 0 to 1. For each r , do ranking aggregation and estimate the performance by some measures. Then we select a r which results in the best performance. For different data set, the r is assigned different values. 3. Ranking distance measure: when we estimate the quality vector p of the ranking lists, we use the Spearman distance ( Crichton, 1999 ). 4. Performance measures: In our paper, the performance of ranking aggregation is measured by the ranking accuracy eval-uations: Mean Average Precision (MAP) ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ) and Normalized Discount Cumulative Gain els of relevance judgements while for MAP it is assumed there are only two relevance levels. Assume the ranking order the MAP is defined as: of d j : The NDCG is given by where n is a normalized constant chosen to make that the perfect ranking X  X  NDCG score is 1. 3.1. Toy data
In this section we will use one toy example to illustrate the effectiveness of our semi-supervised ranking aggregation method. We create a data set with 400 data points in two dimensions. The 400 data points are on two moon-like manifolds. points on the lower moon should be higher than the points on the upper moon. We construct the ranking aggregation prob-lem as follows: (1) Ground truth: the data points on the lower moon are labeled as relevant while the data points on the upper moon are (3) Preferenceconstraints: we randomly select a data point on the lower moon and the upper moon respectively. The lower
We run the above mentioned 9 methods. In SSRA, RS, SRA and SSRA2, we select 50 preference constraints by the tech-nique mentioned above (the number of preference constraints is denoted by PC ). Fig. 3 shows the MAP and NDCG values of these 9 methods on the toy data. We find that the SSRA outperforms other 8 methods under both MAP and NDCG mea-sures. And the ranking accuracies of SSRA are very close to 1. 3.2. TREC data In this section, we will focus on the ranking aggregation of the OSHUMED data set in the filtering track in TREC 2000. we use it in our semi-supervised ranking aggregation method.

OSHUMED data set is a collection of queries and documents on medicine. There are totally 348,566 documents and 106 queries. We choose the 16,140 query-document pairs for which relevance judgements are made according to Liu, Qin, et al. (2007). For each query, there is a set of judged documents. We treat the documents corresponding to the same query as an individual data set: (1) Ground truth: the relevance to the query is given by the TREC committee with three degrees: definitely relevant, pos-(3) Preference constraints: select the pairs which are with different relevant degrees randomly. (4) Data feature: for each document, we extract features from its abstract. The TD.IDF is calculated as the features of 3.2.1. Experiments with reliable ranking lists
In this set of experiments, we want to find to which extent the semi-supervised ranking aggregation can improve the by naive ranking algorithm) in this experiments.

We choose the first 50 queries and their corresponding documents as our 50 experimental data sets. We run above men-tioned 9 ranking aggregation methods on these 50 data sets. The results are shown in Fig. 4 . 3.2.2. The number of preference constraints
In this section, we will focus on how the change of preference constraints can affect the ranking accuracy of our semi-supervised ranking aggregation approach. When the number is 0, the semi-supervised problem is reduced to an unsuper-vised problem. In the unsupervised case, the cost function is simplified to the trade off between smoothness regularizer and the weight disagreement. The optimization problem is defined as follows: It is an convex quadratic programming problem which can be easily solved by the tool. Besides the unsupervised method, we also make experiments with our semi-supervised method with different amounts of preference constraints. In this set of experiments, we also select the first 50 queries and the corresponding documents as our data sets. Since the constraints constraints. We find that both NDCG and MAP increase a lot when the number is 10 and enhance less relatively as the num-ingful in reality because the number we need to label is limited.
 3.2.3. Experiments in unreliable case
In this section, we try to show the effectiveness of our method in case the unreliable ranking lists are involved in. The unreliable ranking lists are generated by two methods. If we want to construct it l unreliable ranking lists, the method erating l permutations randomly. (3) Transforming the ranking orders to ranking scores by some monotony decreasing func-by the random ranking lists.
 od always performs better than other eight methods. 3.3. Result analysis
The proposed method has been compared with five unsupervised methods and three supervised methods. In the reliable case, the SSRA and SSRA2 methods outperform the RS methods. If we compare SSRA2 with RS, we can find that SSRA2 is dif-involving manifold smoothness assumption in unreliable case. 4. Conclusions
In this paper, we propose a new ranking aggregation method: semi-supervised ranking aggregation. We extend the reg-the algorithm insensitive to the noise. We provide many experiments to show the effectiveness of our method, from which, we find that our method can achieve good result when very few constraints are added. It makes the algorithm more mean-ingful in reality.
 References
