 Akik o Takeda taked a@ae.keio.a c.jp Masashi Sugiy ama sugi@cs.titech.a c.jp Supp ort vector classi c ation (SV C) is one of the most successful classi cation algorithms in mo dern mac hine learning (Sc h X olkopf &amp; Smola, 2002). SVC nds a hy-perplane that separates training samples in di eren t classes with maxim um margin (Boser et al., 1992). The maxim um margin hyperplane was sho wn to min-imize an upp er bound of the generalization error ac-cording to the Vapnik-Cherv onenkis theory (Vapnik, 1995). Thus the generalization performance of SVC is theoretically guaran teed.
 SVC was extended to be able to deal with non-separable data by trading the margin size with the data separation error (Cortes &amp; Vapnik, 1995). This soft-margin form ulation is commonly referred to as C -SVC since the trade-o is con trolled by the parameter C . C -SV C was sho wn to work very well in a wide range of real-w orld applications (Sc h X olkopf &amp; Smola, 2002).
 An alternativ e form ulation of the soft-margin idea is -SVC (Sc h X olkopf et al., 2000)|instead of the parameter C , -SV C involves another trade-o parameter that roughly speci es the fraction of supp ort vectors (or sparseness of the solution). Thus, the -SV C form ula-tion pro vides us richer interpretation than the original C -SV C form ulation, whic h would be poten tially useful in real applications.
 Since the parameter corresp onds to a fraction, it should be able to be chosen between 0 and 1. How-ever, it was sho wn that admissible values of are ac-tually limited (Crisp &amp; Burges, 2000; Chang &amp; Lin, 2001). To cop e with this problem, Perez-Cruz et al. (2003) introduced the notion of negativ e margins and prop osed extended -SV C (E -SV C) whic h allo ws to tak e the entire range between 0 and 1. They also exp erimen tally sho wed that the generalization perfor-mance of E -SV C is often better than that of original -SV C. Thus the extension con tributes not only to elu-cidating the theoretical prop erty of -SV C, but also to impro ving its generalization performance.
 However, there remain two open issues in E -SV C. The rst issue is that the reason why a high general-ization performance can be obtained by E -SV C was not completely explained yet. The second issue is that the optimization problem involved in E -SV C is non-con vex and theoretical con vergence prop erties of the E -SV C optimization algorithm have not been stud-ied yet. The purp ose of this pap er is to pro vide new theoretical insigh ts into these two issues.
 After reviewing existing SVC metho ds in Section 2, we elucidate the generalization performance of E -SV C in Section 3. We rst sho w that the E -SV C form ulation could be interpreted as minimization of the conditional value-at-risk (CV aR), whic h is often used in nance (Ro ckafellar &amp; Ury asev, 2002; Gotoh &amp; Takeda, 2005). Then we give new generalization error bounds based on the CV aR risk measure. This theoretical result jus-ti es the use of E -SV C.
 In Section 4, we address non-con vexit y of the E -SV C optimization problem. We rst give a new optimiza-tion algorithm that is guaran teed to con verge to one of the local optima within a nite num ber of itera-tions. Based on this impro ved algorithm, we further sho w that the global solution can be actually obtained within nite iterations even though the optimization problem is non-con vex.
 Finally , in Section 5, we give concluding remarks and future prosp ects. Pro ofs of all theorems and lemmas are sketched in App endix unless men tioned. In this section, we form ulate the classi cation problem and brie y review supp ort vector algorithms. 2.1. Classi cation Problem Let us address the classi cation problem of learning a decision function h from X ( IR n ) to f 1 g based on training samples ( x i ; y i ) ( i 2 M := f 1 ; :::; m g ). We assume that the training samples are i.i.d. follo wing the unkno wn probabilit y distribution P ( x ; y ) on X f 1 g .
 The goal of the classi cation task is to obtain a clas-si er h that minimizes the generalization error (or the risk): whic h corresp onds to the misclassi cation rate for un-seen test samples.
 For the sak e of simplicit y, we generally focus on linear classi ers, i.e., where w ( 2 IR n ) is a non-zero normal vector, b ( 2 IR) is a bias parameter, and sign( ) = 1 if 0 and 1 otherwise.
 Most of the discussions in this pap er can be directly applicable to non-linear kernel classi ers (Sc h X olkopf &amp; Smola, 2002). Thus we may not lose generalit y by restricting ourselv es to linear classi ers. 2.2. Supp ort Vector Classi cation The Vapnik-Cherv onenkis theory (Vapnik, 1995) sho wed that a large margin classi er has a small gen-eralization error. Motiv ated by this theoretical result, Boser et al. (1992) dev elop ed an algorithm for nding the hyperplane ( w ; b ) with maxim um margin: This is called (hard-margin) supp ort vector classi ca-tion (SV C) and valid when the training samples are linearly separable. In the follo wing, we omit \ i 2 M " in the constrain t for brevit y. 2.3. C -Supp ort Vector Classi cation Cortes and Vapnik (1995) extended the SVC algo-rithm to non-separable cases and prop osed trading the margin size with the data separation error (i.e., \soft-margin"): where C ( &gt; 0) con trols the trade-o . This form ulation is usually referred to as C -SV C, and was sho wn to work very well in various real-w orld applications (Sc h X olkopf &amp; Smola, 2002). 2.4. -Supp ort Vector Classi cation -SV C is another form ulation of soft-margin SVC (Sc h X olkopf et al., 2000): where ( 2 IR) is the trade-o parameter.
 Sch X olkopf et al. (2000) sho wed that if the -SV C solu-tion yields &gt; 0, C -SV C with C = 1 = ( m ) pro duces the same solution. Thus -SV C and C -SV C are equiv-alen t. However, -SV C has additional intuitiv e inter-pretations, e.g., is an upp er bound on the fraction of margin errors and a lower bound on the fraction of supp ort vectors (i.e., sparseness of the solution). Thus, the -SV C form ulation would be poten tially more use-ful than the C -SV C form ulation in real applications. 2.5. E -SV C Although has an interpretation as a fraction, it can-not alw ays tak e its full range between 0 and 1 (Crisp &amp; Burges, 2000; Chang &amp; Lin, 2001). 2.5.1. Admissible Range of For an optimal solution f C i g m i =1 of dual C -SV C, let Then, Chang and Lin (2001) sho wed that for 2 ( min ; max ], the optimal solution set of -SV C is the same as that of C -SV C with some C (not necessarily unique). In addition, the optimal objectiv e value of -SV C is strictly negativ e. However, for 2 ( max ; 1], -SV C is unbounded, i.e., there exists no solution; for 2 [0 ; min ], -SV C is feasible with zero optimal ob-jectiv e value, i.e., we end up with just having a trivial solution ( w = 0 and b = 0). 2.5.2. Increasing Upper Admissible Range It was sho wn by Crisp and Burges (2000) that where m + and m are the num ber of positiv e and negativ e training samples. Thus, when the training samples are balanced (i.e., m + = m ), max = 1 and therefore can reac h its upp er limit 1. When the training samples are imbalanced (i.e., m + 6 = m ), Perez-Cruz et al. (2003) prop osed mo difying the opti-mization problem of -SV C as s : t : y i ( h w ; x i i + b ) i ; i 0 ; 0 ; i.e., the e ect of positiv e and negativ e samples are bal-anced. Under this mo di ed form ulation, max = 1 holds even when training samples are imbalanced. For the sak e of simplicit y, we assume m + = m in the rest of this pap er; when m + 6 = m , all the results can be simply extended in a similar way as above. 2.5.3. Decreasing Lower Admissible Range When 2 [0 ; min ], -SV C pro duces a trivial solution ( w = 0 and b = 0) as sho wn in Chang and Lin (2001). To prev ent this, Perez-Cruz et al. (2003) prop osed allo wing the margin to be negativ e and enforcing the norm of w to be unit y: s : t : y i ( h w ; x i i + b ) i ; i 0 ; k w k 2 = 1 : (3) By this mo di cation, a non-trivial solution can be ob-tained even for 2 [0 ; min ]. This mo di ed form ula-tion is called extended -SV C (E -SV C).
 The E -SV C optimization problem is non-con vex due to the equalit y constrain t k w k 2 = 1. Perez-Cruz et al. (2003) prop osed the follo wing iterativ e algorithm for computing a solution. First, for some initial e w , solv e the problem (3) with k w k 2 = 1 replaced by h e w ; w i = 1. Then, using the optimal solution b w , update e w by for = 9 = 10, and iterate this pro cedure until con ver-gence.
 Perez-Cruz et al. (2003) exp erimen tally sho wed that the generalization performance of E -SV C with 2 [0 ; min ] is often better than that with 2 ( min ; max implying that E -SV C is a promising classi cation al-gorithm. However, it is not clear how the notion of negativ e margins in uences on the generalization per-formance and how fast the above iterativ e algorithm con verges. The goal of this pap er is to give new theo-retical insigh ts into these issues. In this section, we give a new interpretation of E -SV C and theoretically explain why it works well. 3.1. New Interpretation of E -SV C as CV aR Let f ( w ; b ; x ; y ) be the mar gin error for a sample ( x ; y ): Let us consider the distribution of margin errors over all training samples: For 2 [0 ; 1), let ( w ; b ) be the 100 -percen tile of the margin error distribution: Thus only the fraction (1 ) of the margin error ure 1). ( w ; b ) is commonly referred to as the value-at-risk (VaR) in nance and is often used by securit y houses or investmen t banks to measure the mark et risk of their asset portfolios (Ro ckafellar &amp; Ury asev, 2002; Gotoh &amp; Takeda, 2005). Let us consider the -tail distribution of f ( w ; b ; x i Let ( w ; b ) be the mean of the -tail distribution of f ( w ; b ; x i ; y i ) (see Figure 1 again): where E denotes the exp ectation over the distri-bution . ( w ; b ) is called the conditional VaR (CV aR). By de nition, the CV aR is alw ays larger than or equal to the VaR: Let us consider the problem of minimizing the CV aR ( w ; b ) (whic h we refer to as minCV aR): Then we have the follo wing theorem.
 Theorem 1 The solution of the minCV aR problem (6) is equivalent to the solution of the E -SV C problem (3) with Theorem 1 sho ws that E -SV C actually minimizes the CV aR 1 ( w ; b ). Thus, E -SV C could be in-terpreted as minimizing the mean margin error over a set of \bad" training samples. In con trast, the hard-margin SVC problem (2) can be equiv alen tly expressed in terms of the margin error as Thus hard-margin SVC minimizes the margin error of the single \worst" training sample. This analysis sho ws that E -SV C can be regarded as an extension of hard-margin SVC to be less sensitiv e to an outlier (i.e., the single \worst" training sample).
 3.2. Justi cation of E -SV C We have sho wn the equiv alence between E -SV C and minCV aR. Here we deriv e new bounds of the general-ization error based on the notion of CV aR and try to justify the use of E -SV C.
 When training samples are linearly separable, the mar-Then, at the optimal solution ( w ; b ), the CV aR 1 ( w ; b ) is alw ays negativ e. However, in non-separable cases, 1 ( w ; b ) could be positiv e par-ticularly when is close to 0. Regarding the CV aR, we have the follo wing lemma.
 Lemma 2 1 ( w ; b ) is continuous with respect to and is strictly decreasing when is incr eased. Let be suc h that if suc h exists; we set = max if 1 ( w ; b ) &gt; 0 for all and we set = 0 if 1 ( w ; b ) &lt; 0 for all . Then we have the follo wing relation (see Figure 2): Belo w, we analyze the generalization error of E -SV C dep ending on the value of . 3.2.1. Justifica tion When 2 ( ; max ] Theorem 3 Let 2 ( ; max ] . Supp ose that supp ort X is in a ball of radius R around the origin. Then, for all ( w ; b ) such that k w k = 1 and 1 ( w ; b ) &lt; 0 , ther e exists a positive constant c such that the following bound hold with probability at least 1 : wher e
G ( ) = The gener alization error bound in (7) is furthermor e upp er-b ounde d as G ( ) is monotone decreasing as j j increases. Thus, the above theorem sho ws that when 1 ( w ; b ) &lt; 0, the upp er bound + G ( 1 ( w ; b )) is lowered if the CV aR 1 ( w ; b ) is reduced. Since E -SV C minimizes 1 ( w ; b ) (see Theorem 1), the upp er bound of the generalization error is also minimized. 3.2.2. Justifica tion When 2 (0 ; ] Our discussion below dep ends on the sign of 1 ( w ; b ). When 1 ( w ; b ) &lt; 0, we have the fol-lowing theorem.
 Theorem 4 Let 2 (0 ; ] . Then, for all ( w ; b ) such that k w k = 1 and 1 ( w ; b ) &lt; 0 , ther e exists a posi-tive constant c such that the following bound holds with probability at least 1 : A pro of of the above theorem is omitted since the pro of follo ws a similar line to the pro of of Theorem 3. This theorem sho ws that when 1 ( w ; b ) &lt; 0, the up-per bound + G ( 1 ( w ; b )) is lowered if 1 ( w ; b ) is reduced. On the other hand, Eq.(5) sho ws that the VaR 1 ( w ; b ) is upp er-b ounded by the CV aR 1 ( w ; b ). Therefore, minimizing 1 ( w ; b ) by E -SVC may have an e ect of lowering the upp er bound of the generalization error.
 When 1 ( w ; b ) &gt; 0, we have the follo wing theorem. Theorem 5 Let 2 (0 ; ] . Then, for all ( w ; b ) such that k w k = 1 and 1 ( w ; b ) &gt; 0 , ther e exists a posi-tive constant c such that the following bound hold with probability at least 1 : Mor eover, the lower bound of R [ h ] is bounde d from above as A pro of of the above theorem is also omitted since the pro of resem bles to Theorem 3. Theorem 5 implies that the lower bound G ( 1 ( w ; b )) of the gener-alization error is upp er-b ounded by G ( 1 ( w ; b )). On the other hand, Eq.(5) and 1 ( w ; b ) &gt; 0 yields 1 ( w ; b ) &gt; 0. Thus minimizing 1 ( w ; b ) by E -SVC may con tribute to lowering the lower bound As review ed in Section 2.5, E -SV C involves a non-con vex optimization problem. In this section, we give a new ecien t optimization pro cedure for E -SV C. Our prop osed pro cedure involves two optimization al-gorithms dep ending on the value of . We rst de-scrib e the two algorithms and then sho w how these two algorithms are chosen for practical use. 4.1. Optimization When 2 ( ; max ] Lemma 6 When 2 ( ; max ] , the E -SV C problem (3) is equivalent to s : t : y i ( h w ; x i i + b ) i ; i 0 ; k w k 2 1 : (8) This lemma sho ws that the equalit y constrain t k w k 2 = 1 in the original problem (3) can be replaced by k w k 2 1 without changing the solution. Due to the con vexit y of k w k 2 1, the above optimization prob-lem is con vex and therefore we can easily obtain the global solution by a standard optimization soft ware. 4.2. Optimization When 2 (0 ; ] If 2 (0 ; ], the E -SV C optimization problem is es-sen tially non-con vex and therefore we need a more elab orate algorithm. 4.2.1. Local Optimum Sear ch Here, we prop ose the follo wing iterativ e algorithm for nding a local optim um.
 Algorithm 7 (The E -SV C local optim um searc h algorithm for 2 (0 ; ] ) Step 1: Initialize e w .

Step 2: Solv e the follo wing linear program: s : t : y i ( h w ; x i i + b ) i ; i 0 ; h e w ; w i = 1 ; and let the optimal solution be ( b w ; b b; b ; b ).
Step 3: If e w = b w , terminate and output e w . Oth-erwise, update e w by e w b w = k b w k .
 Step 4: Rep eat Steps 2{3.
 The linear program (9) is the same as the one pro-posed by Perez-Cruz et al. (2003), i.e., the equalit y constrained k w k 2 = 1 of the original problem (3) is replaced by h e w ; w i = 1. The updating rule of e w in Step 3 is di eren t from the one prop osed by Perez-Cruz et al. (2003) (cf. Eq.(4) ).
 We de ne a \corner" (or \0-dimensional face") of E -SVC (3) as the intersection of an edge of the polyhedral cone formed by linear constrain ts of (3) and k w k 2 = 1. Under the new update rule, the algorithm visits a corner of E -SV C (3) in eac h iteration. Since E -SV C has nite corners, we can sho w that Algorithm 7 with the new update rule terminates in a nite num ber of iterations, i.e., less than or equal to the num ber of corners of E -SV C.
 Theorem 8 Algorithm 7 terminates within a nite numb er of iter ations of Steps 2{3. Furthermor e, a solution of the modi e d E -SV C algorithm is a local minimizer if it is unique and non-de gener ate. 4.2.2. Global Optimum Sear ch Next, we sho w that the global solution can be actu-ally obtained within nite iterations, despite the non-con vexit y of the optimization problem.
 A naiv e approac h to searc hing for the global solution is to run the local optim um searc h algorithm man y times with di eren t initial values and choose the best local solution. However, there is no guaran tee that this naiv e approac h can nd the global solution. Belo w, we give a more systematic way to nd the global solution based on the follo wing lemma.
 Lemma 9 When 2 (0 ; ] , the E -SV C problem (3) is equivalent to s : t : y i ( h w ; x i i + b ) i ; i 0 ; k w k 2 1 : (10) Lemma 9 could be pro ved in a similar way as Lemma 6, so we omit the pro of. This lemma sho ws that the equalit y constrain t k w k 2 = 1 in the original E -SV C problem (3) can be replaced by k w k 2 1 without changing the solution if 2 (0 ; ].
 The problem (10) is called a line ar reverse convex pro-gram (LR CP), whic h is a class of non-con vex prob-lems consisting of linear constrain ts and one conca ve inequalit y ( k w k 2 1 in the curren t case). The feasi-ble set of the problem (10) consists of a nite num-ber of faces . For LR CPs, Horst and Tuy (1995) sho wed that the local optimal solutions corresp ond to 0-dimensional faces (or corners). This implies that all the local optimal solutions of the E -SV C problem (10) can be traced by chec king all the faces. Let D be the feasible set of E -SV C (3). Belo w, we summarize the E -SV C training algorithm based on the cutting plane metho d , whic h is an ecien t metho d of tracing faces.
 Algorithm 10 (The E -SV C global optim um searc h algorithm for 2 (0 ; ] ) Step 1: e D D .
 Step 2: Find a local solution by Algorithm 7.
Step 3: Iden tify a face of D in e D that corresp onds the local solution.

Step 4a: If the face is a corner, construct a \con-cavity cut".

Step 4b: If the face is a prop er face, construct a \facial cut".
 Step 5: Add the cut to the problem (9) and e D .
Step 6: Rep eat Steps 2{5 until e D includes no face of D .

Step 7: Output the best local optimal solution as the global solution.
 If the local solution obtained in Step 2 is a corner of D (i.e., the local solution is not on any cutting plane as (a) in Figure 3), a conc avity cut (Horst &amp; Tuy, 1995) is constructed. The conca vity cut has a role of remo ving the local solution, i.e., a 0-dimensional face of D and its neigh borho od. Otherwise, a facial cut (Ma jtha y &amp; Whinston, 1974) is constructed to eliminate the prop er face (see (b) in Figure 3).
 Since the total num ber of distinct faces of D is nite in the curren t setting and a facial cut or a conca vity cut eliminates at least one face at a time, Algorithm 10 is guaran teed to terminate within nite iterations (pre-cisely , less than or equal to the num ber of all dimen-sional faces of E -SV C). Furthermore, since the addi-tion of a conca vity cut or a facial cut does not remo ve local solutions whic h are better than the best local solution found so far, Algorithm 10 is guaran teed to trace all sucient local solutions. Thus we can alw ays nd a global solution within nite iterations by Algo-rithm 10. A more detailed discussion on the conca vity cut and the facial cut is sho wn in Horst and Tuy (1995) and Ma jtha y and Whinston (1974), resp ectiv ely. 4.3. Choice of Tw o Algorithms We have two con vergen t algorithms when 2 ( ; max ] and 2 (0 ; ]. Thus, choosing a suitable algorithm dep ending on the value of would be an ideal pro ce-dure. However, the value of the threshold is dicult to explicitly compute since it is de ned via the opti-mal value 1 ( w ; b ) (see Figure 2). Therefore, it is not straigh tforw ard to choose a suitable algorithm for a given .
 When we use E -SV C in practice, we usually com-pute the solutions for sev eral di eren t values of and choose the most promising one based on, e.g., cross-validation. In suc h scenarios, we can prop erly switc h two algorithms without explicitly kno wing the value of |our key idea is that the solution of the problem (8) is non-trivial (i.e., w 6 = 0 ) if and only if 2 ( ; max Thus if the solutions are computed from large to small , the switc hing point can be iden ti ed by chec k-ing the trivialit y of the solution. The prop osed algo-rithm is summarized as follo ws.
 Algorithm 11 (The E -SV C algorithm for ( max ) 1 &gt; 2 &gt; &gt; k &gt; 0 ) Step 1: i 1.
 Step 2: Compute ( w ; b ) for i by solving (8).
Step 3a: If w 6 = 0 , accept ( w ; b ) as the solution for i , incremen t i , and go to Step 2.
 Step 3b: If w = 0 , reject ( w ; b ).
 Step 4: Compute ( w ; b ) for i by Algorithm 10.
Step 5: Accept ( w ; b ) as the solution for i , incremen t i , and go to Step 4 unless i &gt; k . We characterized the generalization error of E -SV C in terms of the conditional value-at-risk (CV aR, see Fig-ure 1) and sho wed that a good generalization perfor-mance is exp ected by E -SV C. We then deriv ed a glob-ally con vergen t optimization algorithm even though the optimization problem involved in E -SV C is non-con vex.
 We introduced the threshold based on the sign of the CV aR (see Figure 2). We can chec k that the prob-lem (8) is equiv alen t to -SV C in the sense that they share the same negativ e optimal value in ( ; max ] and ( min ; max ], resp ectiv ely (Gotoh &amp; Takeda, 2005). On the other hand, the problem (8) and -SV C have the zero optimal value in (0 ; ] and [0 ; min ], resp ectiv ely. Thus, although the de nitions of and min are di er-ent, they would be essen tially the same. We will study the relation between and min in more detail in the future work.

