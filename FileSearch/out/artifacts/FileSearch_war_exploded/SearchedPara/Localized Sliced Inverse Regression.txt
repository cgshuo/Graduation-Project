 The importance of dimension reduction for predictive model ing and visualization has a long and central role in statistical graphics and computation In the modern context of high-dimensional data analysis this perspective posits that the functional depen dence between a response variable y and a Characterizing this predictive subspace, supervised dime nsion reduction, requires both the response and explanatory variables. This problem in the context of li near subspaces or Euclidean geometry has been explored by a variety of statistical models such as s liced inverse regression (SIR, [10]), sliced average variance estimation (SAVE, [3]), principal Hessian directions (pHd, [11]), (condi-tional) minimum average variance estimation (MAVE, [18]), and extensions to these approaches. To extract nonlinear subspaces, one can apply the aforementio ned linear algorithms to the data mapped into a feature space induced by a kernel function [13, 6, 17].
 In machine learning community research on nonlinear dimens ion reduction in the spirit of [19] has mapping (ISOMAP, [16]), local linear embedding (LLE, [14]) , Hessian Eigenmaps [5], and Lapla-cian Eigenmaps [1]. Two key differences exist between the pa radigm explored in this approach and that of supervised dimension reduction. The first differ ence is that the above methods are un-supervised in that the algorithms take into account only the explanatory variables. This issue can be addressed by extending the unsupervised algorithms to us e the label or response data [7]. The bigger problem is that these manifold learning algorithms d o not operate on the space of the ex-planatory variables and hence do not provide a predictive su bmanifold onto which the data should be projected. These methods are based on embedding the obser ved data onto a graph and then us-ing spectral properties of the embedded graph for dimension reduction. The key observation in all of these manifold algorithms is that metrics must be local an d properties that hold in an ambient Euclidean space are true locally on smooth manifolds.
 This suggests that the use of local information in supervise d dimension reduction methods may be of use to extend methods for dimension reduction to the setti ng of nonlinear subspaces and subman-ifolds of the ambient space. In the context of mixture modeli ng for classification two approaches have been developed [9, 15]. Though the predictive directions obtained by LSIR are linea r ones, they coded nonlinear informa-dimension reduction analysis  X  semi-supervised learning.
 The paper is arranged as follows. LSIR is introduced in Secti on 2 for continuous and categorical and real data in Sections 4 and 5. We close with discussions in Section 6. We start with a brief review of SIR method and remark that the f ailure of SIR in some situations is caused by ignoring local structures. Then we propose a gener alization of SIR, called localized SIR, by incorporating some localization idea from manifold lear ning. Connection to some existing work is addressed at the end. 2.1 Sliced inverse regression Assume the functional dependence between a response variab le Y and an explanatory variable X  X  R p is given by where  X  the L -dimensional subspace spanned by  X  onto space B , provides a sufficient summary of the information in X relevant to Y . Estimating B or  X   X  X  becomes the central problem in supervised dimension redu ction. Though we define B here via a heuristic model assumption (1), a general definition based on conditional independence between Y and X given P (d.r.) subspace and  X  Slice inverse regression (SIR) model is introduced [10] to e stimate the d.r. directions. The idea for details. According to this result the d.r. directions  X  covariance matrix  X  = Cov( E ( X | Y )) . In general when the covariance matrix of X is  X  , the  X  can be obtained by solving a generalized eigen decompositio n problem A simple SIR algorithm operates as the following on a set of sa mples { ( x Though SIR has been widely used for dimension reduction and y ielded many useful results in prac-restricted its use in binary classification problems where o nly one direction can be obtained. The a summary of the information in each slice, which apparently is not enough. Generalizations of SIR include SAVE [3], SIR-II [12] and covariance inverse regres sion estimation (CIRE, [2]) that exploit the information from the second moment of the conditional di stribution of X | Y . However in some similar to the multimodal situation considered by [15], the data in a slice may form two clusters, then a good description of the data would not be a single numbe r such as any moments, but the two cluster centers. Next we will propose a new algorithm that is a generalization of SIR based on local structures of X in each slice. 2.2 Localization meaningful locally. Under this principle, it is dangerous t o calculate the slice average m Motivated by this idea we introduce a localized SIR (LSIR) me thod for dimension reduction. Here is the intuition for LSIR. Let us start with the transfor med data set where the empirical co-data point x different groups well, the group means projected to that dir ection would be very close, therefore ization idea into this approach is to shift each data point x neighborhood is often chosen by k nearest neighborhood ( k -NN). Different from manifolds learning supervised learning will also incorporate information fro m the response variable y . Here is the mathematical description of LSIR. Recall that th e group average m where m equal to some local average, and then use the corresponding s ample covariance matrix to replace  X   X  in equation (2). Below we give the details of our LSIR algorit hm: The neighborhood size k in LSIR is a tuning parameter specified by users. When k is large enough, say, larger than the size of any group, then  X   X  With a moderate choice of k , LSIR uses the local information within each slice and is exp ected to retrieve directions lost by SIR in case of SIR fails due to deg eneracy.
 classes is C , then the estimate  X   X  from the original FDA is of rank at most C  X  1 , which means FDA can only estimate at most C  X  1 directions. This is why FDA is seldom used for binary clas-sification problems where C = 2 . In LSIR we use more points to describe the data in each class. Mathematically this is reflected by the increase of the rank o f  X   X  and hence produces more directions. Moreover, if for some cl asses the data is composed of several sub-clusters, LSIR can automatically identify these sub-c luster structures. As showed in one of our examples, this property of LSIR is very useful in data analys is such as cancer subtype discovery using genomic data. 2.3 Connection to Existing Work before. For example, the local discriminant information (L DI) introduced by [9] is one of the early work in this area. In LDI, the local information is used to com pute the between-group covariance matrix  X  the top eigenvector of the averaged between-group matrix 1 analysis (LFDA) introduced by [15] can be regarded as an impr ovement of LDI with the within-class covariance matrix also being localized.
 Comparing to these two approaches, LSIR utilizes the local i nformation directly at the point level. One advantage of this simple localization is computation. F or example, for a problem of C classes, LDI needs to compute nC local mean points and n between-group covariance matrices, while LSIR computes only n local mean points and one covariance matrix. Another advant age is LSIR can be easily extended to handle unlabeled data in semi-supervi sed learning as explained in the next covariance matrices instead of data points. Regularization. When the matrix  X   X  is singular or has a very large condition number, which is com -mon in high-dimensional problems, the generalized eigen-d ecomposition problems (3) is unstable. Regularization techniques are often introduced to address this issue [20]. For LSIR we adopt the following regularization: Semi-supervised learning. In semi-supervised learning some data have y  X  X  ( labeled data) and some do not ( unlabeled data). How to incorporate the information from unlabeled da ta has been the main focus of research in semi-supervised learning. Our LSIR alg orithm can be easily modified to take the unlabeled data into consideration. Since y of an unlabeled sample can take any possible values, we put the unlabeled data into every slice. So the neighborho od s any point in the k -NN of x same slice as x In this section we apply LSIR to several synthetic data sets t o illustrate the power of LSIR. The performance of LSIR is compared with other dimension reduct ion methods including SIR, SAVE, pHd, and LFDA. Table 1: Estimation accuracy (and standard deviation) of va rious dimension reduction methods for semisupervised learning in Example 1. Figure 1: Result for Example 1. (a) Plot of data in the first two dimensions, where  X + X  corresponds to y = 1 while  X  X  X  corresponds to y =  X  1 . The data points in red and blue are labeled and the ones in green are unlabeled when the semisupervised setting is considered. (b) Projection of data n = 400 data points are labeled. (d) Projection of the data to the firs t two LSIR directions when only 20 points as indicated in (a) are labeled.
 Let  X  B = (  X   X  estimated d.r. directions. We introduce the following metr ic to measure the accuracy: study, we found it usually good enough to choose k between 10 to 20, except for the semisupervised setting (e.g. Example 1 below). But further study and a theor etical justification are necessary. two dimensions and the remaining eight dimensions are Gauss ian noise. The data in the first two relevant dimensions are plotted in Figure 1(a) with sample s ize n = 400 . For this example SIR same for the first two dimensions, due to the symmetry in the da ta. Using local average instead of group average, LSIR can find both directions, see Figure 1(c) . But so do SAVE and pHd since the high-order moments also behave differently in the two group s.
 Next we create a data set for semi-supervised learning by ran domly selecting 20 samples, 10 from each group, to be labeled and setting others to be unlabeled. The directions from PCA where one ignores the labels do not agree with the discriminant direct ions as shown in Figure 1(b). So to We evaluate the accuracy of LSIR (the semi-supervised versi on), SAVE and pHd where the latter different random set to be labeled. The averaged accuracy is reported in Table 1. The result for one out-performs the other two supervised dimension reduction methods.
 Example 2. We first generate a 10-dimensional data set where the first t hree dimensions are the Swiss roll data [14]: where t = 3  X  sions are independent Gaussian noises. Then all dimensions are normalized to have unit variance. Consider the following function: We randomly choose n samples as a training set and let n change from 200 to 1000 and compare the estimation accuracy for LSIR with SIR, SAVE and pHd. The resu lt is showed in Figure 2. SAVE and pHd outperform SIR, but are still much worse comparing to LSIR.
 where the goal is to  X  X nroll X  the data into the intrinsic two d imensional space. Since LSIR is a linear dimension reduction method we do not expect LSIR to un roll the data, but expect to retrieve Example 3. (Tai Chi) The Tai Chi figure is well known in Asian culture wh ere the concepts of Yin-Yang provide the intellectual framework for much of anc ient Chinese scientific development. A 6-dimensional data set for this example is generated as fol lows: X structure as shown in Figure 3(a) where the Yin and Yang regio ns are assigned class labels Y =  X  1 and Y = 1 respectively. X The Tai Chi data set was first used as a dimension reduction exa mple in [12, Chapter 14]. The correct d.r. subspace B is span ( e [12], we generate n = 1000 samples as the training data, then run LSIR with k = 10 and repeat 100 times. The average accuracy is 98 . 6% and the result from one run is shown in Figure 3. For comparison we also applied LFDA for this example. The averag e accuracy is 82% which is much better than SIR, SAVE and pHd but worse than LSIR.
 data projected onto the first two LSIR directions; (c) An inde pendent test data projected onto the first two LSIR directions. In this section we apply our LSIR methods to two real data sets . 5.1 Digits recognition The MNIST data set (Y. LeCun, http://yann.lecun.com/exdb/mnist/ ) is a well known benchmark data In our simulations, we randomly sampled 1000 images ( 100 samples for each digit) as training set. We apply LSIR and computed d = 20 e.d.r. directions. Then we project the training data and 10000 data, we report the classification error over 100 iterations in Table 2. Compared with SIR method, much significant. 5.2 Gene expression data Cancer classification and discovery using gene expression d ata becomes an important technique in modern biology and medical science. In gene expression data number of genes is huge (usually up to thousands) and the samples is quite limited. As a typical l arge p small n problem, dimension reduction plays very essential role to understand the data s tructure and make inference. ples and 34 test samples. The training sample has two classes , AML and ALL, and the class ALL has two subtypes. We apply SIR and LSIR to this data. The class ification accuracy is similar by subtype discovery while SIR cannot. By project the training data onto the first two directions (Fig-are T-cell ALL and the 19 -samples cluster is B-cell ALL samples. Note that there are t wo samples (which are T-cell ALL) cannot be assigned to each subtype onl y by visualization. This means LSIR only provides useful subclass knowledge for future researc h but itself may not a perfect clustering method. We developed LSIR method for dimension reduction by incorpo rating local information into the original SIR. It can prevent degeneracy, increase estimati on accuracy, and automatically identify as well as real data sets.
 can be helpful to realize nonlinear dimension reduction dir ections and to reduce the computational complexity in case of p  X  n.
 Further research on LSIR and its kernelized version include s their asymptotic properties such as borhoods, and L , the dimensionality of the reduced space.
 [1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensi onality reduction and data repre-[2] R. Cook and L. Ni. Using intra-slice covariances for impr oved estimation of the central sub-[3] R. Cook and S. Weisberg. Disussion of li (1991). J. Amer. Statist. Assoc. , 86:328 X 332, 1991. [4] R. Cook and X. Yin. Dimension reduction and visualizatio n in discriminant analysis (with [5] D. Donoho and C. Grimes. Hessian eigenmaps: new locally l inear embedding techniques for [6] K. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel dimensio n reduction in regression. Annals [8] T. Golub, D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J . Mesirov, H. Coller, M. Loh, [12] K. C. Li. High dimensional data analysis via the sir/phd approach, 2000. [14] S. Roweis and L. Saul. Nonlinear dimensionality reduct ion by locally linear embedding. Sci-[15] M. Sugiyam. Dimension reduction of multimodal labeled data by local fisher discriminatn [17] Q. Wu, F. Liang, and S. Mukherjee. Regularized sliced in verse regression for kernel models. [18] Y. Xia, H. Tong, W. Li, and L.-X. Zhu. An adaptive estimat ion of dimension reduction space. [19] G. Young. Maximum likelihood estimation and factor ana lysis. Psychometrika , 6:49 X 53, 1941. [20] W. Zhong, P. Zeng, P. Ma, J. S. Liu, and Y. Zhu. RSIR: regul arized sliced inverse regression
